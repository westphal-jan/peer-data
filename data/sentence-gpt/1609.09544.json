{"id": "1609.09544", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Algorithms for item categorization based on ordinal ranking data", "abstract": "We present a new method for identifying the latent categorization of items based on their rankings. Complimenting a recent work that uses a Dirichlet prior on preference vectors and variational inference, we show that this problem can be effectively dealt with using existing community detection algorithms, with the communities corresponding to item categories. In particular we convert the bipartite ranking data to a unipartite graph of item affinities, and apply community detection algorithms. In this context we modify an existing algorithm - namely the label propagation algorithm to a variant that uses the distance between the nodes for weighting the label propagation - to identify the categories. We propose and analyze a synthetic ordinal ranking model and show its relation to the recently much studied stochastic block model. We test our algorithms on synthetic data and compare performance with several popular community detection algorithms. We also test the method on real data sets of movie categorization from the Movie Lens database. In all of the cases our algorithm is able to identify the categories for a suitable choice of tuning parameter. To find the most commonly used and fastest-known categorization algorithm, we use a Bayesian Monte Carlo method to find the best classification algorithm, and then run it on real data sets of movies. In general we use Bayesian Monte Carlo methods to find the best classification algorithm, and then run it on real data sets of movies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 29 Sep 2016 22:59:45 GMT  (2706kb,D)", "http://arxiv.org/abs/1609.09544v1", "To appear in IEEE Allerton conference on computing, communications and control, 2016"]], "COMMENTS": "To appear in IEEE Allerton conference on computing, communications and control, 2016", "reviews": [], "SUBJECTS": "cs.LG cs.SI", "authors": ["josh girson", "shuchin aeron"], "accepted": false, "id": "1609.09544"}, "pdf": {"name": "1609.09544.pdf", "metadata": {"source": "CRF", "title": "Algorithms for Item Categorization Based on Ordinal Ranking Data", "authors": ["Josh Girson", "Shuchin Aeron"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTION\nIn this paper we consider the problem of item categorization based on choice, preference or ranking data by a number of voters. So far using the voter-rating matrix, the literature has focused on voter categorization instead of item categorization [1]. We are partly motivated by a recent work, [2] where item categorization based on choice statistics was considered. Using a Dirichlet prior on the preferences for each user coupled with a random utility model for making choices, the authors use a variational algorithm to infer the categories. In contrast to these approaches, in this paper a new ranking (choice) model among the categories is presented followed by the use of community detection algorithms [3], [4] for category discovery by converting the bipartite graph of ratings to the unipartite similarity graph of item similarities.\nIn this context our contribution is two-fold - (a) We analyze the expected connectivity in the similarity graph and link our model to the recently much studied stochastic block model. This implies that one can understand information theoretic limits on the discovery of categories by directly using recent results in [5]1; (b) We propose a variation of the standard label propagation algorithm [6], which we refer to as the weighted label propagation algorithm where the path distance in label aggregation is incorporated to avoid false inclusions. This can be thought of as nodes having\n1We note that this conversion can also be applied to other latent category choice/ranking models and similarity measures, but the ensuing analysis, at present, seems quite complicated.\nmore influence on their neighbors than distant nodes in the graph, which slows the ability of one label to consume an entire graph. We show that this simple modification is useful in avoiding misclustering in some cases where the standard label propagation algorithm fails. We also compare the performance of this algorithm with other algorithms such as Clauset-Neuman-Moore (CNM) [7] algorithm that maximizes the modularity metric for clustering. Synthetic data experiments show that the weighted label propagation algorithm is competitive with these existing methods.\nThe rest of the paper is organized as follows. In the following section we introduce the generative ranking model used in the analysis of the algorithm and the relation to the standard block model is explored through the analysis of the chosen similarity function. In section 3, we present a modification of the label propagation algorithm that introduces a weighting of the labels that are to be selected at each round. The new algorithm aims to give larger weight to labels that are closer to their source vertices and less strength to labels that are farther. Finally, in section 4 we present the experimental results of the algorithm, both on the synthetic data produced by the generative model and on real data from the Movie Lens database [8]."}, {"heading": "II. GENERATION OF A SYNTHETIC RATING MODEL", "text": "Algorithm 1 outlines the model used to generate the ranking data among categories. The parameters for this data generation are C: the number of categories; S: the number of items in each category; p: the expected number of items each category will swap with every other category (referred to as the mixing parameter) to allow for choice variability; and V : the number of voters.\nThe main idea behind the synthetic ranking generation algorithm (Algorithm 1) is as follows. For each voter, the following process is repeated. Each of the categories are ordered randomly from 1 to C. Then, each of the S items of the C categories are given a unique random integer value in the range [CiS, (Ci + 1)S) where Ci is the location of the category in the ordering. This process produces an ordinal ranking system of the N = SC items with items in the same category initially given close proximity to one another.\nOnce all of the items are placed as such, the mixing process begins. In a new random order, generated separately from the\nar X\niv :1\n60 9.\n09 54\n4v 1\n[ cs\n.L G\n] 2\n9 Se\np 20\n16\nAlgorithm 1 Synthetic Ranking Generation Input: S: Category size; C: Number of Categories; p: Mixing parameter; V: Number of voters for all voters in range(V) do O \u2190 random permutation of the C categories i\u2190 1 for all j \u2208 O do Ri \u2190 random permutation of [jS, (j + 1)S \u2212 1] i\u2190 i+ 1\nend for O2 \u2190 random permutation of the C categories for category c1 \u2208 O2 do\nfor all other categories c2 \u2208 O2 \u2212 {c1} do Swap p random unique items between c1 and c2\nend for end for R\u2190 collapse [R1, . . . , RC ]\nend for\nordering above, the C categories each swap p items with each of the other categories. These p items are chosen randomly and uniformly from the S items of the category. It is worth noting that if 3 categories swap in the order A, B C, then the item that swapped into B from A could be the same ones that B swaps with C. This is to say that the p items swapped from a category do not have to have originated in that category.\nAs mentioned, the above ranking generation is repeated for each of the V voters to create a complete dataset of ordinal rankings. Once this is completed, the bipartite ranking data is converted into a unipartite graph using Algorithm 2. This algorithm is based around the idea of collapsing the edges between voters and items into direct edges between items. For each voter, call it v, each pair of item is compared using a similarity function, S. The values of this similarity function affect whether or not there is to be an edge between these two items.\nAs edges in the final graph are to represent a strong relationship between elements, the similarity function should have a higher value for items that are rated similarly and a lower rating for those with a larger difference. The reasoning behind this is that users typically prefer similar things and will thus give similar ratings to items of the same category\nwhile items of different categories should be rated differently due to preference. There are a variety of functions that can be chosen such as the Cosine Similarity Measure [1] or the Pearson Correlation Coefficient [9]. In this paper, a different function, introduced below, is used that is more amenable to analysis as shown in the next section.\nAlgorithm 2 Conversion of Bipartite Ranking Graph to Unipartite\nInput: An M \u00d7 N matrix B where the columns are the elements and the rows are the ratings of the voters. A similarity function S that maps pairs of rankings to a weight. A threshold, , to cut off the weights. Initialize: A weight matrix, W, of all zeros of size N\u00d7N . An empty graph G for Voter v in range(M) do\nfor i in range(N) do for j in range(i + 1, N) do W[i][j]+ = S(B[v][i],B[v][j])\nend for end for\nend for for i in range(N) do\nfor j in range(N) do if W[i][j] > then\nAdd edge eij to G end if\nend for end for"}, {"heading": "A. Relation to the Stochastic Block Model (SBM)", "text": "In this section, we aim to discover the relationship between this model and the stochastic block model (SBM)2 recently used for deriving information theoretic bounds [5]. In the standard block model, the input parameters are the community size, number of communities, intracommunity density (pin or \u03b1) and the intercommunity density (pout or \u03b2). The output is a graph of size community size \u00d7 number of communities where the chance of an edge between nodes of the same community is \u03b1 and the chance of an edge between nodes of different communities is \u03b2. It is clear that in this ranking model, S and C are directly representative of the community size and number of communities parameters in the standard block model. It is necessary to examine Algorithm 2, which actually introduces the edges, to determine how \u03b1 and \u03b2 relate to the standard block model. Algorithm 2 includes a threshold, , that serves as a cutoff for when an edge will and will not connect two elements, a and b. \u03b1 and \u03b2 are discovered by calculating the probability that the average similarity value between the two elements across all voters is above this threshold.\n2Also equivalent to the planted partition model.\nDefinition II.1. Alpha: The probability of an edge between elements of the same category in the generative model\n\u03b1(a,b) = P  \u2211 v\u2208V Sv(a, b,N) |V | \u2265  (1) Definition II.2. Beta: The probability of an edge between elements of different categories in the generative model\n\u03b2(a,b) = P  \u2211 v\u2208V Sv(a, b,N) |V | \u2265  (2) Since these probabilities are implicit, in the following sections we will analyze their expected behaviors."}, {"heading": "B. Analysis of Similarity Function", "text": "As the calculation of \u03b1 and \u03b2 is an implicit calculation, it is useful to calculate the expected values of the similarity function for elements in the same category and those in different categories as a proxy for examining these measures. We first introduce the similarity function we are using in the algorithm and analysis:\nSim(a, b,N) = 1\u2212 |a\u2212 b| N\n. As discussed above, this function is chosen as it gives higher weights to elements that are more closely ranked, such as those in the same category, than elements that are ranked very differently. As the threshold, , should lead to the addition of only edges that represent strong relationships, the expected value of our similarity function is used. In this calculation it is hypothesized that there is an equal chance of choosing any pair of elements a and b.\nLemma II.1. Expected Value of Similarity Function:\nE[|a\u2212 b|] = N + 1 3 =\u21d2 E[S(a, b,N)] = 1\u2212 N + 1 3N (3)\nProof. Please see Appendix A"}, {"heading": "1) Analysis of Similarity Function for Elements of the Same", "text": "Category: As mentioned above, as the calculation of \u03b1 is implicit, it is useful to consider what the expected value for the similarity function would be between elements in the same category. All that must be considered is the value of the distance, as the value of the similarity function is inversely proportional to the distance. The expected distance is calculated by averaging the distance between all possible combinations of elements.\nSituations involving mixing must also be considered. The total distance of all possible pairs of elements can be calculated regardless of where a new element, X , swaps in; X can swap in with any of the other elements already in the category. In order to calculate the number of combinations, the S possible locations swapped in the category are multiplied by the ( S 2 )\ndifferent element pairs to yield the total S \u2217 ( S 2 ) possible distance combinations.\nTo get the total distance, we recognize that the new element, X , will be paired with each other element already in the category S \u2212 1 times, every round except that in which X will have swapped with the element itself. Similarly, all of the standard intracategory pairings will still happen S \u2212 2 times: the two exceptions being when either of the two elements is the one swapped. Thus we multiply the standard\nsum of intracategory distances, S\u22121\u2211 i=1\ni(S\u2212 i) by S\u2212 2. These values are combined to get the expected distance expressed below.\nLemma II.2. Expected Intracategory Distance with 1 swap:\nE [|a\u2212 b|same] = (S \u2212 1)\nS\u2211 i=1 |X \u2212 i|+ (S \u2212 2) S\u22121\u2211 i=1 i(S \u2212 i)\nS \u2217 ( S 2 ) (4)\nThis idea is extended to the general case of p swaps. Let X1, . . . , Xp be the p elements that are swapping into the category.\nLemma II.3. Expected Intracategory Distance with p swaps\nE [|a\u2212 b|same] =( S p ) p\u2211 i=1 p\u2211 j=i+1 |Xi \u2212Xj |+ ( S\u22121 p ) S\u2211 i=1 p\u2211 j=1 |Xj \u2212 i|+ ( S\u22122 p ) S\u22121\u2211 i=1\ni(S \u2212 i)( S 2 )( S p ) (5)\nProof. This lemma follows from the extension of the above ideas discussed in the formation of Lemma II.2\nClearly these two equations are equivalent if the value 1 is substituted for p. One thing worth noting is that when p = 0, E [|a\u2212 b|same] = S+13 ."}, {"heading": "2) Analysis of Similarity Function for Elements in Different", "text": "Categories: The analysis of the distance function for elements in different categories is slightly more difficult. As categories can have different distances from each other, a new variable, D, is introduced to signify this distance. To give an example of this measurement, if two elements are in adjacently ranked categories, D = 0, if they are two away, i.e. category 1 and category 3, then D = 1.\nLemma II.4. Expected Intercategory Distance with 1 swap\nE [|a\u2212 b|diff ] =\n( 1 + (S \u2212 1)2 ) S3(D + 1) + 4S S\u22121\u2211 i=1 i(S \u2212 i)\nS4 (6)\nProof. Please See Appendix B\nWe can also extend this formulation to the situation where there are p elements swapping. As all the elements swapping in come from the other category, there is no need to introduce the Xi variables seen in the intracategory comparison.\nLemma II.5. Expected Intercategory Distance with p swaps\nE [|a\u2212 b|diff ]\n=\n(( S\u22121 p )2 + ( S\u22121 p\u22121 )2) S3(D + 1) + 4 ( S\u22122 p\u22121 )( S p ) S\u22121\u2211 i=1 i(S \u2212 i)\nS2 \u2217 ( S p )2 (7)\nProof. Please See Appendix B\nFor the majority of our comparisons, the case where C = 2 is considered, which causes D to always be zero as a greater distance is impossible. By fixing D = 0, the terms involving D can be disregarded as they become the multiplicative identity.\n3) Comparisons: In order to achieve a graph in which the community structure is discernible, we would like to see that\n\u03b2 < < \u03b1\nso that edges within a category are more likely to be added than edges joining elements between different categories. Recalling that as a proxy we can instead use the relationship between their expected distances (which is the inverse of the relationship between \u03b1 and \u03b2), we require E [|a\u2212 b|same] < E[|a\u2212 b|] < E [|a\u2212 b|diff ]. As a base case, we compare the expected distance values without any mixing:\nS + 1\n3\n? < N + 1\n3\n? < S(D + 1)\nRemembering that we are primarily considering the case when D = 0 allows us to equate the right side to simply S. Similarly, the middle term is equivalent to 2S+13 . Clearly, then, this inequality simplifies to the true inequality S+1 3 < 2S+1 3 < S.\nIt is now important to consider the cases where p is not zero. In Figure 2 the results seen in Lemmas II.1 - II.5 are combined to see the values of the expected distance functions as a function of p, the number of elements swapped.\nAs p increases, the expected distance between elements of different categories decreases and the expected distance between elements of the same category increases. It is only once p gets large enough (p > 8) that the expected distance between elements of different categories dips beneath the threshold. Regardless of the value of p, the expected intracategory distance is lower than the intercategory distance. In the next section, we introduce the Weighted Label Propagation algorithm which is used to examine the community structure on the created graph."}, {"heading": "III. WEIGHTED LABEL PROPAGATION ALGORITHM", "text": "In this section we introduce a modification of the traditional label propagation algorithm seen in [6]. As with the original, this algorithm starts with each node having its own unique label. At each iteration, a node\u2019s label is updated to be the most common label of all its neighboring nodes, with ties broken uniformly and randomly. As iterations continue, most of the labels disappear as many nodes take on the same label. The convergence point in this algorithm is when the label of every node does not change from iteration to iteration. This is equivalent to every node having the most common label of all of its neighbors. At the conclusion, all nodes sharing the same label are grouped into communities.\nIn the original version, all neighboring labels have equal weight regardless of their location in the graph. The distance of a label from its source vertex is not considered at all. In the weighted version of the algorithm, we incorporate this distance function in order to allow labels that occur close to their source label to have more weight than labels that are very far from their source. This modification serves to better localize the clustering. A secondary benefit of this modification is that it prevents the entire graph from being classified as one community purely due to an increase in the frequency of a label.\nAs stated, this algorithm is very similar to the non-weighted version except that before assigning the label with the largest count, the label counts are re-weighted based on their distance from their source vertex. This weighting should be a function of the distance, d, and can be done in a variety of ways. In this paper, two such distance functions chosen were the linear W1(d) = 1d and the exponential W2(d) = 1 2d .\nFig. 5. Here we can see the results of the algorithm on the categorization of 30 elements split into 3 categories. The left graph shows the true categorization while the right graph shoes the categorization resulting from the proposed algorithm. As can be seen, the results are nearly identical except for one orange node that is categorized as yellow.\nAlgorithm 3 Weighted Label Propagation Input: A graph, G. A weighting function, W : R 7\u2192 [0, 1].\nInitialize: t = 0, \u2200v \u2208 G, Lv(t) = v. Create a random ordering of the nodes, X . while labels have not converged do\nfor v \u2208 X do Initialize a map, C, of all zero values to represent the counts of each label for z \u2208 Nv do\nLet L = Lz(t) C[L] = C[L] +W (dist(L, z))\nend for Lv(t+ 1) = argmax\nz\u2208Nv C[Lz(t)]\nend for t = t+ 1\nend while Split the vertices into communities based on common labels\nThe weighted algorithm can be performed in either a synchronous or asynchronous manner. The synchronous manner is the one described above and the asynchronous version differs only in the update step. Rather than always updating based on the label from the previous time step, the asynchronous version uses the new label of any nodes that have already been updated at the current time step.\nThe time complexity of this algorithm is also linear per iteration. The only difference is that it requires a preprocessing step to find the distances from all vertices to all other vertices\nin the graph. This is done using Dijkstra\u2019s algorithm for each vertex O(V ) \u2217 O(E log V ) = O(V E log V ). However, since the graphs we are considering are relatively sparse, we have E = O(V ), so this reduces to O(V 2 log V ). We have omitted the analysis of the convergence of the weighted label propagation algorithm and intend to explore it in future work. Please see Appendix C for a comparison of the weighted label propagation algorithm with other algorithms."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": ""}, {"heading": "A. Synthetic Data", "text": "In order to judge the success of the algorithm with the synthetic data, we compared the resulting community labeling to\nFig. 9. Here we can see the results of the algorithm on the categorization of 30 elements split into 3 categories. The left graph shows the true categorization while the right graph shoes the categorization resulting from the proposed algorithm. As can be seen, the results are nearly identical except for one orange node that is categorized as yellow.\nthe initial categorization from the beginning of the algorithm. The metric we used to judge this comparison was normalized mutual information (NMI) [10]. This metric ranges from 0 to 1, where a 1 signifies that we have a perfect match with the true community structure and a 0 signifies no relationship with the true structure. Definition IV.1. Normalized Mutual Information\nNMI(A,B) = 2I(A,B)\nH(A) +H(B)\nwhere A and B are two partitionings of elements, I denotes the mutual information between the partitions and H denotes the entropy of a partition.\nWe can examine Figure 6 to see the effects of the voting multiplier and the mixing parameter on the accuracy of the model. Here the voting multiplier signifies the ratio of the number of voters to the total number of elements. As we would expect, the larger the voting multiplier, the higher the accuracy, regardless of the mixing parameter. An increase in the mixing parameter does show a general trend of decreasing the NMI with the true categorization."}, {"heading": "B. Real Data", "text": "In order to test the algorithm, we used the MovieLens data [8], which consists of user ratings on a variety of movies. Users rate the movies from 0-5 and may leave some of the movies unranked. We explored different thresholds for cutting off the similarity function and present the results below. In every situation, the entire dataset chosen consists of the union of all the categories.\n\u2022 Comparing Amityville Horror Movies and Kid\u2019s Movies = 0.94\u2192 Edge Ratio = 0.440\n1) Category 1: Toy Story (1995), Lion King, The (1994), Aladdin (1992), Snow White and the Seven Dwarfs (1937), Alice in Wonderland (1951) 2) Category 2: Aladdin and the King of Thieves (1996), Jungle Book, The (1994), Pocahontas (1995) 3) Category 3: Amityville 1992: It\u2019s About Time (1992), Amityville 3-D (1983), Amityville: A New Generation (1993), Amityville II: The Possession (1982), Amityville Horror, The (1979), Amityville Curse, The (1990)\n\u2022 Comparing Amityville Horror Movies and Star Trek Movies = 0.94\u2192 Edge Ratio = 0.359\n1) Category 1: Star Trek VI: The Undiscovered Country (1991), Star Trek: The Wrath of Khan (1982), Star Trek III: The Search for Spock (1984), Star Trek IV: The Voyage Home (1986), Star Trek: Generations (1994), Star Trek: The Motion Picture (1979) 2) Category 2: Star Trek V: The Final Frontier (1989), Amityville 1992: It\u2019s About Time (1992), Amityville 3-D (1983), Amityville: A New Generation (1993), Amityville II: The Possession (1982), Amityville Horror, The (1979), Amityville Curse, The (1990)\n\u2022 Comparing Star Wars and Star Trek Movies = 0.92\u2192 Edge Ratio = 0.266\n1) Category 1: Star Wars (1977), Empire Strikes Back, The (1980), Return of the Jedi (1983) 2) Category 2: Star Trek VI: The Undiscovered Country (1991), Star Trek: The Wrath of Khan (1982), Star Trek III: The Search for Spock (1984), Star Trek IV: The Voyage Home (1986), Star Trek: Generations (1994), Star Trek: The Motion Picture (1979) 3) Category 3: Star Trek V: The Final Frontier (1989) = 0.915\u2192 Edge Ratio = 0.333\n1) Category 1: Star Wars (1977), Empire Strikes Back, The (1980), Return of the Jedi (1983), Star Trek: The Wrath of Khan (1982) 2) Category 2: Star Trek VI: The Undiscovered Country (1991), Star Trek III: The Search for Spock (1984), Star Trek IV: The Voyage Home (1986), Star Trek: Generations (1994), Star Trek: The Motion Picture (1979), Star Trek V: The Final Frontier (1989)\nWe are able to form a categorization of the movies chosen by examining the relationship between the ratings of all of the voters. In each of the above situations, a clear distinction can be seen between the groups of movies we are examining. Although at some points there is an additional group introduced, the groups reflect the separation that is implied based on the chosen films."}, {"heading": "V. APPENDIX", "text": ""}, {"heading": "A. Calculation of Expected Value of Similarity Function:", "text": "We are calculating the expected value of the similarity function\nSim(a, b,N) = 1\u2212 |a\u2212 b| N\nE[S(a, b,N)] = E [ 1\u2212 |a\u2212 b|\nN ] = E [1]\u2212 E [ |a\u2212 b| N ] = 1\u2212 1 N E [|a\u2212 b|]\n(8)\nE [|a\u2212 b|] = 1( N 2 ) N\u22121\u2211 i=1 i(N \u2212 i) (9)\n= 2 N(N \u2212 1) (N \u2212 1)N(N + 1) 6 = N + 1\n3 (10)\nE[S(a, b,N)] = 1\u2212 1 N (N + 1) 3 = 1\u2212 N + 1 3N (11)"}, {"heading": "B. Calculation of Expected Intercategory Distance", "text": "Below we present the formulation of the expected intercategory distance with p swaps. The proof of the equation with 1 swap can be seen by simply substituting p = 1. Similarly to the derivation of the intracategory distance, the intercategory\ndistance was derived by examining the possible combinations of the elements that are going to be compared.\nE [|a\u2212 b|diff ] =(( S\u22121 p )2 + ( S\u22121 p\u22121 )2) S3(D + 1) + 4 ( S\u22122 p\u22121 )( S p ) S\u22121\u2211 i=1 i(S \u2212 i)\nS2 \u2217 ( S p )2 (12)\n(( S\u22121 p )2 + ( S\u22121 p\u22121 )2)\nS3(D + 1): This represents the total sum of distances when the two elements are in separate categories. S3(D + 1) is the cumulative distance for all possible pairs of elements in each arrangement of this type and (( S\u22121 p )2 + ( S\u22121 p\u22121 )2)\nis the total number of times we will have comparisons between elements of different categories. The first term represents arrangements in which neither of the elements swaps from its own category. We can think of this as fixing the element we are considering and picking the p elements to swap from the other S \u2212 1 elements of the category for both of the categories. The second term represents arrangements when the elements actually swap with each other and neither is in its true category. We can calculate the number of ways of doing this as requiring the element to swap and picking the other p \u2212 1 elements from the S \u2212 1 remaining elements of the category for each category. 4 ( S\u22122 p\u22121 )( S p ) S\u22121\u2211 i=1\ni(S\u2212i): This term represents the total sum of distances when the two elements are in the same category.\nWe know that S\u22121\u2211 i=1\ni(S \u2212 i) is the total distance for each occurrence of this comparison (as seen in Appendix A). Here 4 ( S\u22122 p\u22121 )( S p ) represents the arrangement of categories in which this comparison arises. We know that we are then fixing two elements from the category (1 to swap and 1 to stay), so we then pick the other p\u22121 elements to swap from the remaining S \u2212 2 elements. Now we know we will have an occurrence of this type regardless of what happens in the other category, leading to the factor of ( S p ) (the number of possible outcomes from the second category). We then multiply by 2 twice: once because either element can be the one that stays and once because we must consider this summation of distances for both of the categories.\nFinally, we see the denominator is formed from the product of the number of possible swaps at each arrangement. We know that there are ( S p )2 possible arrangements resulting from each cluster picking p elements to swap. Similarly, we know that each arrangement has S2 swaps just by simple examination."}, {"heading": "C. Comparison of Weighted Label Propagation with Other Algorithms:", "text": "In order to judge the success of the Weighted Label Propagation algorithm, we compared its success in determining the community structure of a series of planted partition models:\np = 0.7, q = 0.01, number categories = 10, size = 5\nCNM Regular Weighted Avg Num of Categories: 8.81 9.35 9.99 Avg NMI with Truth: 0.9568 0.9764 0.9940 Avg Modularity: 0.6998 0.6992 0.7020\np = 0.75, q = 0.01, number categories = 10, size = 5\nCNM Regular Weighted Avg Num of Categories: 8.97 9.29 10.02 Avg NMI with Truth: 0.9644 0.9754 0.9970 Avg Modularity: 0.7003 0.6969 0.7024\np = 0.8, q = 0.01, number categories = 10, size = 5\nCNM Regular Weighted Avg Num of Categories: 8.99 9.61 10.01 Avg NMI with Truth: 0.9646 0.9873 0.9976 Avg Modularity: 0.7066 0.7069 0.7089\nAs is seen in all of the above examples, the weighted label propagation algorithm performs better than the CNM algorithm and the regular label propagation algorithm in terms of the correct number of categories found, the normalized mutual information (NMI) and the modularity of the resulting partition."}, {"heading": "VI. ACKNOWLEDGEMENTS", "text": "This research was supported by NSF Research Experiences for Undergraduate (REU) program via the grant NSF:CCF:1319653."}], "references": [{"title": "Leveraging community detection for accurate trust prediction", "author": ["G. Beigi", "M. Jalili", "H. Alvari", "G. Sukthankar"], "venue": "Academy of Science and Engineering (ASE), USA, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "On ranking and choice models", "author": ["S. Agarwal"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJ- CAI), 2016, http://www.shivani-agarwal.net/Publications/2016/ijcai16ranking-choice-models-invited.pdf.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Community detection in graphs", "author": ["S. Fortunato"], "venue": "Physical Reports, vol. 486, pp. 75\u2013174, Feb. 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Community structure in social and biological networks", "author": ["M. Girvan", "M.E.J. Newman"], "venue": "Proceedings of the National Academy of Sciences, vol. 99, no. 12, pp. 7821\u20137826, 2002. [Online]. Available: http://www.pnas.org/content/99/12/7821.abstract", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Exact recovery in the stochastic block model", "author": ["E. Abbe", "A.S. Bandeira", "G. Hall"], "venue": "IEEE Transactions on Information Theory, vol. 62, no. 1, pp. 471\u2013487, Jan 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Near linear time algorithm to detect community structures in large-scale networks", "author": ["U.N. Raghavan", "R. Albert", "S. Kumara"], "venue": "Physical Review E, vol. 76, no. 3, p. 036106, Sep. 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Finding community structure in very large networks", "author": ["A. Clauset", "M.E.J. Newman", "C. Moore"], "venue": "Physical Review E, vol. 70, no. 6, p. 066111, Dec. 2004.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "The movielens datasets: History and context", "author": ["F.M. Harper", "J.A. Konstan"], "venue": "ACM Trans. Interact. Intell. Syst., vol. 5, no. 4, pp. 19:1\u201319:19, Dec. 2015. [Online]. Available: http: //doi.acm.org/10.1145/2827872", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Community detection for correlation matrices", "author": ["M. MacMahon", "D. Garlaschelli"], "venue": "ArXiv e-prints, Nov. 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Comparing community structure identification", "author": ["L. Danon", "A. Daz-Guilera", "J. Duch", "A. Arenas"], "venue": "Journal of Statistical Mechanics: Theory and Experiment, vol. 2005, no. 09, p. P09008, 2005. [Online]. Available: http://stacks.iop.org/1742-5468/2005/i=09/a=P09008", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "So far using the voter-rating matrix, the literature has focused on voter categorization instead of item categorization [1].", "startOffset": 120, "endOffset": 123}, {"referenceID": 1, "context": "We are partly motivated by a recent work, [2] where item categorization based on choice statistics was considered.", "startOffset": 42, "endOffset": 45}, {"referenceID": 2, "context": "In contrast to these approaches, in this paper a new ranking (choice) model among the categories is presented followed by the use of community detection algorithms [3], [4] for category discovery by converting the bipartite graph of ratings to the unipartite similarity graph of item similarities.", "startOffset": 164, "endOffset": 167}, {"referenceID": 3, "context": "In contrast to these approaches, in this paper a new ranking (choice) model among the categories is presented followed by the use of community detection algorithms [3], [4] for category discovery by converting the bipartite graph of ratings to the unipartite similarity graph of item similarities.", "startOffset": 169, "endOffset": 172}, {"referenceID": 4, "context": "This implies that one can understand information theoretic limits on the discovery of categories by directly using recent results in [5]1; (b) We propose a variation of the standard label propagation algorithm [6], which we refer to as the weighted label propagation algorithm where the path distance in label aggregation is incorporated to avoid false inclusions.", "startOffset": 133, "endOffset": 136}, {"referenceID": 5, "context": "This implies that one can understand information theoretic limits on the discovery of categories by directly using recent results in [5]1; (b) We propose a variation of the standard label propagation algorithm [6], which we refer to as the weighted label propagation algorithm where the path distance in label aggregation is incorporated to avoid false inclusions.", "startOffset": 210, "endOffset": 213}, {"referenceID": 6, "context": "We also compare the performance of this algorithm with other algorithms such as Clauset-Neuman-Moore (CNM) [7] algorithm that maximizes the modularity metric for clustering.", "startOffset": 107, "endOffset": 110}, {"referenceID": 7, "context": "Finally, in section 4 we present the experimental results of the algorithm, both on the synthetic data produced by the generative model and on real data from the Movie Lens database [8].", "startOffset": 182, "endOffset": 185}, {"referenceID": 0, "context": "There are a variety of functions that can be chosen such as the Cosine Similarity Measure [1] or the Pearson Correlation Coefficient [9].", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "There are a variety of functions that can be chosen such as the Cosine Similarity Measure [1] or the Pearson Correlation Coefficient [9].", "startOffset": 133, "endOffset": 136}, {"referenceID": 4, "context": "In this section, we aim to discover the relationship between this model and the stochastic block model (SBM)2 recently used for deriving information theoretic bounds [5].", "startOffset": 166, "endOffset": 169}, {"referenceID": 5, "context": "In this section we introduce a modification of the traditional label propagation algorithm seen in [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "A weighting function, W : R 7\u2192 [0, 1].", "startOffset": 31, "endOffset": 37}, {"referenceID": 9, "context": "The metric we used to judge this comparison was normalized mutual information (NMI) [10].", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": "In order to test the algorithm, we used the MovieLens data [8], which consists of user ratings on a variety of movies.", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "= E [1]\u2212 E [ |a\u2212 b|", "startOffset": 4, "endOffset": 7}], "year": 2016, "abstractText": "We present a new method for identifying the latent categorization of items based on their rankings. Complimenting a recent work that uses a Dirichlet prior on preference vectors and variational inference, we show that this problem can be effectively dealt with using existing community detection algorithms, with the communities corresponding to item categories. In particular we convert the bipartite ranking data to a unipartite graph of item affinities, and apply community detection algorithms. In this context we modify an existing algorithm namely the label propagation algorithm to a variant that uses the distance between the nodes for weighting the label propagation to identify the categories. We propose and analyze a synthetic ordinal ranking model and show its relation to the recently much studied stochastic block model. We test our algorithms on synthetic data and compare performance with several popular community detection algorithms. We also test the method on real data sets of movie categorization from the Movie Lens database. In all of the cases our algorithm is able to identify the categories for a suitable choice of tuning parameter.", "creator": "LaTeX with hyperref package"}}}