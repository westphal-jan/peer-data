{"id": "1609.09864", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2016", "title": "Technical Report: Graph-Structured Sparse Optimization for Connected Subgraph Detection", "abstract": "Structured sparse optimization is an important and challenging problem for analyzing high-dimensional data in a variety of applications such as bioinformatics, medical imaging, social networks, and astronomy. Although a number of structured sparsity models have been explored, such as trees, groups, clusters, and paths, connected subgraphs have been rarely explored in the current literature. One of the main technical challenges is that there is no structured sparsity-inducing norm that can directly model the space of connected subgraphs, and there is no exact implementation of a projection oracle for connected subgraphs due to its NP-hardness. In this paper, we explore efficient approximate projection oracles for connected subgraphs, and propose two new efficient algorithms, namely, Graph-IHT and Graph-GHTP, to optimize a generic nonlinear objective function subject to connectivity constraint on the support of the variables. Our proposed algorithms enjoy strong guarantees analogous to several current methods for sparsity-constrained optimization, such as Projected Gradient Descent (PGD), Approximate Model Iterative Hard Thresholding (AM-IHT), and Gradient Hard Thresholding Pursuit (GHTP) with respect to convergence rate and approximation accuracy. We apply our proposed algorithms to optimize several well-known graph scan statistics in several applications of connected subgraph detection as a case study, and the experimental results demonstrate that our proposed algorithms outperform state-of-the-art methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 30 Sep 2016 19:26:26 GMT  (671kb,D)", "http://arxiv.org/abs/1609.09864v1", "11 pages in 2016 IEEE International Conference of Data Mining"]], "COMMENTS": "11 pages in 2016 IEEE International Conference of Data Mining", "reviews": [], "SUBJECTS": "cs.AI cs.DS", "authors": ["baojian zhou", "feng chen"], "accepted": false, "id": "1609.09864"}, "pdf": {"name": "1609.09864.pdf", "metadata": {"source": "CRF", "title": "Technical Report: Graph-Structured Sparse Optimization for Connected Subgraph Detection", "authors": ["Baojian Zhou", "Feng Chen"], "emails": ["bzhou6@albany.edu", "fchen5@albany.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nIn recent years, structured sparse methods have attracted much attention in many domains such as bioinformatics, medical imaging, social networks, and astronomy [2], [4], [14], [16], [17]. Structured sparse methods have been shown effective to identify latent patterns in high-dimensional data via the integration of prior knowledge about the structure of the patterns of interest, and at the same time remain a mathematically tractable concept. A number of structured sparsity models have been well explored, such as the sparsity models defined through trees [14], groups [17], clusters [16], and paths [2]. The generic optimization problem based on a structured sparsity model has the form\nmin x\u2208Rn\nf(x) s.t. supp(x) \u2208M (1)\nwhere f : Rn \u2192 R is a differentiable cost function, the sparsity model M is defined as a family of structured supports: M = {S1, S2, \u00b7 \u00b7 \u00b7 , SL}, where Si \u2286 [n] satisfies a certain structure property (e.g., trees, groups, clusters), [n] =\n{1, 2, \u00b7 \u00b7 \u00b7 , n}, and the support set supp(x) refers to the set of indexes of non-zero entries in x. For example, the popular k-sparsity model is defined as M = {S \u2286 [n] | |S| \u2264 k}.\nExisting structured sparse methods fall into two main categories: 1) Sparsity-inducing norms based. The methods in this category explore structured sparsity models (e.g., trees, groups, clusters, and paths) [4] that can be encoded as structured sparsity-inducing norms, and reformulate Problem (1) as a convex (or non-convex) optimization problem\nminx\u2208Rn f(x) + \u03bb \u00b7 \u2126(x) (2) where \u2126(x) is a structured sparsity-inducing norm of M that is typically non-smooth and non-Euclidean and \u03bb is a trade-off parameter. 2) Model-projection based. The methods in this category rely on a projection oracle of M:\nP(b) = arg minx\u2208Rn \u2016b\u2212 x\u201622 s.t. supp(x) \u2208M, (3) and decompose the problem into two sub-problems, including unconstrained minimization of f(x) and the projection problem P(b). Most of the methods in this category assume that the projection problem P(b) can be solved exactly, including the forward-backward algorithm [42], the gradient descent algorithm [38], the gradient hard-thresholding algorithms [6], [18], [40], the projected iterative hard thresholding [5], [7], and the Newton greedy pursuit algorithm [41]. However, when an exact solver of P(b) is unavailable and we have to apply approximate projections, the theoretical guarantees of these methods do not hold any more. We note that there is one recent approach named as GRAPH-COSAMP that admits inexact projections by assuming \u201chead\u201d and \u201ctail\u201d oracles for the projections, but is only applicable to compressive sensing or linear regression problems [15].\nWe consider an underlying graph G = (V,E) defined on the coefficients of the unknown vector x, where V = [n] and E \u2286 V \u00d7 V. We focus on the sparsity model of connected subgraphs that is defined as\nM(G, k) = {S \u2286 V | |S| \u2264 k, S is connected}, (4) where k refers to the allowed maximum subgraph size. There are a wide array of applications that involve the search of interesting or anomalous connected subgraphs in networks. The connectivity constraint ensures that subgraphs reflect changes due to localized in-network processes. We describe\nar X\niv :1\n60 9.\n09 86\n4v 1\n[ cs\n.A I]\n3 0\nSe p\n20 16\na few applications below.\n\u2022 Detection in sensor networks, e.g., detection of traffic bottlenecks in road networks or airway networks [1]; crime hot spots in geographic networks [22]; and pollutions in water distribution networks [27]. \u2022 Detection in digital signals and images, e.g., detection of objects in images [15]. \u2022 Disease outbreak detection, e.g., early detection of disease outbreaks from information networks incorporating data from hospital emergency visits, ambulance dispatch calls and pharmacy sales of over-the-counter drugs [36]. \u2022 Virus detection in a computer network, e.g., detection of viruses or worms spreading from host to host in a computer network [24]. \u2022 Detection in genome-scale interaction network, e.g., detection of significantly mutated subnetworks [21]. \u2022 Detection in social media networks, e.g., detection and forecasting of societal events [8], [9].\nTo the best of our knowledge, there is no existing approach to Problem (1) for M(G, k) that is computationally tractable and provides performance bound. First, there is no known structured sparsity-inducing norm for M(G, k). The most relevant norm is fused lasso norm [39]: \u2126(x) = \u2211 (i,j)\u2208E |xi\u2212xj |, where xi is the i-th entry in x. This norm is able to enforce the smoothness between neighboring entries in x, but has limited capability to recover all the possible connected subsets as described by M(G, k) (See further discussions in Section V Experiments). Second, there is no exact solver for the projection oracle of M(G, k):\nP(x) = arg min x\u2208Rn \u2016b\u2212 x\u201622 s.t. supp(x) \u2208M(G, k), (5) as this projection problem is NP-hard due to a reduction from classical Steiner tree problem [19]. As most existing modelprojection based methods require an exact solution to the projection oracle P(x), these methods are inapplicable to the problem studied here. To the best of our knowledge, there is only one recent approach named as GRAPH-COSAMP that admits inexact projections for M(G, k) by assuming \u201chead\u201d and \u201ctail\u201d oracles for the projections, but is only applicable to compression sensing and linear regression problems [15]. The main contributions of our study are summarized as follows:\n\u2022 Design of efficient approximation algorithms. Two new algorithms, namely, GRAPH-IHT and GRAPH-GHTP, are developed to approximately solve Problem (1) that has a differentiable cost function and a sparsity model of connected subgraphs M(G, k). GRAPH-GHTP is required to minimize f(x) over a projected subspace as an intermediate step, which could be too costly in some applications and GRAPH-IHT could be considered as a fast variant of GRAPH-GHTP. \u2022 Theoretical guarantees and connections. The convergence rate and accuracy of our proposed algorithms are analyzed under a smoothness condition of f(x) that is more general than popular conditions such as Restricted Strong Convexity/Smoothness (RSC/RSS) and Stable\nMode Restricted Hessian (SMRH). We prove that under mild conditions our proposed GRAPH-IHT and GRAPHGHTP enjoy rigorous theoretical guarantees. \u2022 Compressive experiments to validate the effectiveness and efficiency of the proposed techniques. Both GRAPH-IHT and GRAPH-GHTP are applied to optimize a variety of graph scan statistic models for the connected subgraph detection task. Extensive experiments on a number of benchmark datasets demonstrate that GRAPHIHT and GRAPH-GHTP perform superior to state-of-theart methods that are designed specifically for this task in terms of subgraph quality and running time.\nReproducibility: The implementation of our algorithms and the data sets is open-sourced via the link [11].\nThe remaining parts of this paper are organized as follows. Sections II introduces the sparsity model of connected subgraphs and statement of the problem. Sections III presents two efficient algorithms and their theoretical analysis. Section IV discusses applications of our proposed algorithms to graph scan statistic models. Experiments on several real world benchmark datasets are presented in Section V. Section VI discusses related work and Section VII concludes the paper and describes future work."}, {"heading": "II. PROBLEM FORMULATION", "text": "Given an underlying graph G = (V,E) defined on the coefficients of the unknown vector x, where V = [n], E \u2286 V \u00d7 V, and n is typically large (e.g., n > 10, 000). The sparsity model of connected subgraphs in G is defined in (4), and its projection oracle P(x) is defined in (5). As this projection oracle is NP-hard to solve, we first introduce efficient approximation algorithms for P(x) and then present statement of the problem that will be studied in the paper.\nA. Approximation algorithms for the projection oracle P(x)\nThere are two nearly-linear time approximation algorithms [15] for P(x) that have the following properties: \u2022 Tail approximation (T(x)): Find a S \u2286 V such that\n\u2016x\u2212 xS\u20162 \u2264 cT \u00b7 min S\u2032\u2208M(G,kT ) \u2016x\u2212 xS\u2032\u20162, (6)\nwhere cT = \u221a\n7, kT = 5k, and xS is the restriction of x to indices in S: we have (xS)i = xi for i \u2208 S and (xS)i = 0 otherwise.\n\u2022 Head approximation (H(x)): Find a S \u2286 V such that \u2016xS\u20162 \u2265 cH \u00b7 max\nS\u2032\u2208M(G,kH) \u2016xS\u2032\u20162, (7) where cH = \u221a\n1/14 and kH = 2k. It can be readily proved that, if cT = cH = 1, then T(x) = H(x) = P(x), which indicates that these two approximations (T(x) and H(x)) stem from the fact that cT > 1 and cH < 1."}, {"heading": "B. Problem statement", "text": "Given a predefined cost function f(x) that is differentiable, the input graph G, and the sparsity model of connected\nsubgraph M(G, k), the problem to be studied is formulated as:\nmin x\u2208Rn\nf(x) s.t. supp(x) \u2208M(G, k). (8)\nProblem (8) is difficult to solve as it involves decision variables from a nonconvex set that is composed of many disjoint subsets. In this paper, we will develop nearly-linear time algorithms to approximately solve Problem (8). The key idea is decompose this problem to sub-problems that are easier to solve. These sub-problems include an optimization sub-problem of f(x) that is independent on M(G, k) and projection approximations for M(G, k), including T(x) and H(x). We will design efficient algorithms to couple these subproblems to obtain global solutions to Problem (8) with good trade-off on running time and accuracy."}, {"heading": "III. ALGORITHMS", "text": "This section first presents two efficient algorithms, namely, GRAPH-IHT and GRAPH-GHTP, and then analyzes their time complexities and performance bounds.\nA. Algorithm GRAPH-IHT\nThe proposed GRAPH-IHT algorithm generalizes the traditional algorithm named as projected gradient descent [5], [7] that requires a exact solver of the projection oracle P(x). The high-level summary of GRAPH-IHT is shown in Algorithm 1 and illustrated in Figure 1 (b). The procedure generates a sequence of intermediate vectors x0, x1, \u00b7 \u00b7 \u00b7 from an initial approximation x0. At the i-th iteration, the first step (Line 5) first calculates the gradient \u201c\u2207f(xi)\u201d, and then identifies a subset of nodes via head approximation that returns a support set with the head value at least a constant factor of the optimal head value: \u201c\u2126 \u2190 H(\u2207f(xi))\u201d. The support set \u2126 can be interpreted as the subspace where the nonconvex set \u201c{x | supp(x) \u2208M(G, k)}\u201d is located, and the projected gradient in this subspace is: \u201c\u2207\u2126f(xi)\u201d. The second step (Line 6) calculates the projected gradient descent at the point xi with step-size \u03b7: \u201cb\u2190 xi\u2212 \u03b7 \u00b7\u2207\u2126f(xi)\u201d. The third step (Line 7)\nidentifies a subset of nodes via tail approximation that returns a support set with tail value at most a constant times larger than the optimal tail value: \u201cSi+1 \u2190 T(b)\u201d. The last step (Line 8) calculates the intermediate solution xi+1: xi+1 = bSi+1 . The previous two steps can be interpreted as the projection of b to the nonconvex set \u201c{x | supp(x) \u2208 M(G, k)}\u201d using the tail approximation.\nAlgorithm 1 GRAPH-IHT 1: Input: Input graph G, maximum subgraph size k, and step\nsize \u03b7 (1 by default). 2: Output: The estimated vector x\u0302 and the corresponding\nconnected subgraph S. 3: i\u2190 0, xi \u2190 0; Si \u2190 \u2205; 4: repeat 5: \u2126\u2190 H(\u2207f(xi)); 6: b\u2190 xi \u2212 \u03b7 \u00b7 \u2207\u2126f(xi); 7: Si+1 \u2190 T(b); 8: xi+1 \u2190 bSi+1 ; 9: i\u2190 i+ 1;\n10: until halting condition holds 11: return x\u0302 = xi and S = GSi ;\nB. Algorithm GRAPH-GHTP\nThe proposed GRAPH-GHTP algorithm generalizes the traditional algorithm named as Gradient Hard Threshold Pursuit (GHTP) that is designed specifically for the k-sparsity model: M = {S \u2286 [n] | |S| \u2264 k} [40]. The high-level summary of GRAPH-GHTP is shown in Algorithm 2 and illustrated in Figure 1 (c). The first two steps (Line 5 and Line 6) in each iteration is the same as the first two steps (Line 5 and Line 6) of GRAPH-IHT, except that we return the support of the projected gradient descent: \u201c\u03a8 \u2190 supp(xi \u2212 \u03b7 \u00b7 \u2207\u2126f(xi))\u201d, in which pursuing the minimization will be most effective. Over the support set S, the function f is minimized to produce an intermediate estimate at the third step (Line 7):\nAlgorithm 2 GRAPH-GHTP 1: Input: Input graph G, maximum subgraph size k, and step\nsize \u03b7 (1 by default). 2: Output: The estimated vector x\u0302 and the corresponding\nconnected subgraph S. 3: i\u2190 0, xi \u2190 0; Si \u2190 \u2205; 4: repeat 5: \u2126\u2190 H(\u2207f(xi)) 6: \u03a8\u2190 supp(xi \u2212 \u03b7 \u00b7 \u2207\u2126f(xi)); 7: b\u2190 arg minx\u2208Rn f(x) s.t. supp(x) \u2286 \u03a8; 8: Si+1 \u2190 T(b); 9: xi+1 \u2190 bSi+1 ;\n10: i\u2190 i+ 1; 11: until halting condition holds 12: return x\u0302 = xi and S = GSi ;\n\u201cb\u2190 arg minx\u2208Rn f(x) s.t. supp(x) \u2286 \u2126\u201d. The fourth and fifth steps (Line 8 and Line 9) are the same as the last two steps (Line 7 and Line 8) of GRAPH-IHT in each iteration.\nC. Relations between GRAPH-IHT and GRAPH-GHTP\nThese two algorithms are both variants of gradient descent. In overall, GRAPH-GHTP converges faster than GRAPH-IHT as it identifies a better intermediate solution in each iteration by minimizing f(x) over a projected subspace {x | supp(x) \u2286 \u2126}. If the cost function f(x) is linear or has some special structure, this intermediate step can be conducted in nearlylinear time. However, when this step is too costly in some applications, GRAPH-IHT is preferred.\nD. Theoretical Analysis of GRAPH-IHT\nIn order to demonstrate the accuracy of estimates using Algorithm 1, we require that the cost function f(x) satisfies the Weak Restricted Strong Convexity (WRSC) condition as follows:\nDefinition III.1 (Weak Restricted Strong Convexity Property (WRSC)). A function f(x) has the (\u03be, \u03b4, M)-model-WRSC if \u2200x,y \u2208 Rn and \u2200S \u2208M with supp(x) \u222a supp(y) \u2286 S, the following inequality holds for some \u03be > 0 and 0 < \u03b4 < 1:\n\u2016x\u2212 y \u2212 \u03be\u2207Sf(x) + \u03be\u2207Sf(y)\u20162 \u2264 \u03b4\u2016x\u2212 y\u20162. (9)\nThe WRSC is weaker than the popular Restricted Strong Convexity/Smoothness (RSC/RSS) conditions that are used in theoretical analysis of convex optimization algorithms [40]. The RSC condition basically characterizes cost functions that have quadratic bounds on the derivative of the objective function when restricted to model-sparse vectors. The RSC/RSS conditions imply condition WRSC, which indicates that WRSC is no stronger than RSC/RSS [40]. In the special case where f(x) = \u2016y\u2212Ax\u201622 and \u03be = 1, the condition (\u03be, \u03b4, M)-model-WRSC reduces to the well known Restricted Isometry Property (RIP) condition in compressive sensing.\nTheorem III.1. Consider the sparsity model of connected subgraphs M(G, k) for some k \u2208 N and a cost function f : Rn \u2192 R that satisfies the (\u03be, \u03b4,M(G, 5k))-model-WRSC\ncondition. If \u03b7 = cH(1 \u2212 \u03b4) \u2212 \u03b4 then for any x \u2208 Rn such that supp(x) \u2208M(G, k), with \u03b7 > 0 the iterates of Algorithm 2 obey\n\u2016xi+1 \u2212 x\u20162 \u2264 \u03b1\u2016xi \u2212 x\u20162 + \u03b2\u2016\u2207If(x)\u20162 (10) where\n\u03b10 = cH(1\u2212 \u03b4)\u2212 \u03b4, \u03b20 = \u03b4(1 + cH),\n\u03b1 =\n\u221a 2(1 + cT )\n1\u2212 \u03b4\n(\u221a 1\u2212 \u03b120 + ( (2\u2212 \u03b7\n\u03be )\u03b4 + 1\u2212 \u03b7 \u03be\n)) ,\n\u03b2 = 1 + cT 1\u2212 \u03b4\n( (1 + 2 \u221a 2)\u03be + (2\u2212 2 \u221a 2)\u03b7 +\n\u221a 2\u03b20 \u03b10 + \u221a 2\u03b10\u03b20\u221a 1\u2212 \u03b120\n) ,\nand I = argmaxS\u2208M(G,8k) \u2016\u2207Sf(x)\u20162\nBefore we prove this result, we give the following two lemmas III.2 and III.3.\nLemma III.2. [40] Assume that f is a differentiable function. If f satisfies condition (\u03be, \u03b4,M)-WRSC, then \u2200x,y \u2208 Rn with supp(x) \u222a supp(y) \u2282 S \u2208 M, the following two inequalities hold 1\u2212 \u03b4 \u03be \u2016x\u2212 y\u20162 \u2264 \u2016\u2207Sf(x)\u2212\u2207Sf(y)\u20162 \u2264 1 + \u03b4 \u03be \u2016x\u2212 y\u20162\nf(x) \u2264 f(y) + \u3008\u2207f(y),x\u2212 y\u3009+ 1 + \u03b4 2\u03be \u2016x\u2212 y\u201622\nLemma III.3. Let \u03b10 = cH(1 \u2212 \u03b4) \u2212 \u03b4, \u03b20 = \u03be(1 + cH), ri = xi \u2212 x, and \u2126 = H(\u2207f(xi)). Then\n\u2016ri\u0393c\u20162 \u2264 \u221a 1\u2212 \u03b120\u2016r i\u20162 + [ \u03b20 \u03b10 + \u03b10\u03b20\u221a 1\u2212 \u03b120 ] \u2016\u2207If(x)\u20162\nwhere I = argmaxS\u2208M(G,8k) \u2016\u2207Sf(x)\u20162. We assume that cH and \u03b4 are such that \u03b10 > 0.\nProof: Denote \u03a6 = supp(x) \u2208 M(G,k),\u2126 = H(\u2207f(xi)) \u2208 M(G, 2k), ri = xi \u2212 x, and \u039b = supp(ri) \u2208 M(G, 6k). The component \u2016\u2207\u0393f(xi)\u20162 can be lower bounded as \u2016\u2207\u0393f(xi)\u20162 \u2265 cH\u2016\u2207\u03a6f(xi)\u20162\n\u2265 cH(\u2016\u2207\u03a6f(xi)\u2212\u2207\u03a6f(x)\u20162 \u2212 \u2016\u2207\u03a6f(x)\u20162 \u2265 cH(1\u2212 \u03b4) \u03be \u2016ri\u20162 \u2212 cH\u2016\u2207If(x)\u20162,\nwhere the first inequality follows from the definition of head approximation and the last inequality follows from Lemma III.2 of our paper. The component \u2016\u2207\u0393f(xi)\u20162 can also be upper bounded as \u2016\u2207\u0393f(xi)\u20162 \u2264 1\n\u03be \u2016\u03be\u2207\u0393f(xi)\u2212 \u03be\u2207\u0393f(x)\u20162 + \u2016\u2207\u0393f(x)\u20162\n\u2264 1 \u03be \u2016\u03be\u2207\u0393f(xi)\u2212 \u03be\u2207\u0393f(x)\u2212 ri\u0393 + ri\u0393\u20162 +\n\u2016\u2207\u0393f(x)\u20162\n\u2264 1 \u03be \u2016\u03be\u2207\u0393\u222a\u2126f(xi)\u2212 \u03be\u2207\u0393\u222a\u2126f(x)\u2212 ri\u0393\u222a\u2126\u20162 +\n\u2016ri\u0393\u20162 + \u2016\u2207\u0393f(x)\u20162\n\u2264 \u03b4 \u03be \u00b7 \u2016ri\u20162 + 1 \u03be \u2016ri\u0393\u20162 + \u2016\u2207If(x)\u20162,\nwhere the fourth inequality follows from condition (\u03be, \u03b4,M(G, 8k))-WRSC and the fact that ri\u0393\u222a\u2126 = ri. Combining the two bounds and grouping terms, we obtain\nthe inequality:\n\u2016ri\u0393\u2016 \u2265 \u03b10\u2016ri\u20162 \u2212 \u03be(1 + cH)\u2016\u2207If(x)\u20162. We have \u2016ri\u0393\u2016 \u2265 \u03b10\u2016ri\u20162\u2212\u03b20\u2016\u2207If(x)\u20162. After a number of algebraic manipulations, we obtain the inequality\n\u2016ri\u0393c\u20162 \u2264 \u221a 1\u2212 \u03b120\u2016r i\u20162 + [ \u03b20 \u03b10 + \u03b10\u03b20\u221a 1\u2212 \u03b120 ] \u2016\u2207If(x)\u20162, which proves the lemma. We give the formal proof of III.1.\nProof: From the traingle inequality, we have\n\u2016ri+1\u20162 = \u2016xi+1 \u2212 x\u20162 = \u2016b\u03a8 \u2212 x\u20162 \u2264 \u2016b\u2212 x\u20162 + \u2016b\u2212 b\u03a8\u20162 \u2264 (1 + cT )\u2016b\u2212 x\u20162 = (1 + cT )\u2016xi \u2212 \u03b7\u2207\u2126f(xi)\u2212 xi\u20162 = (1 + cT )\u2016ri \u2212 \u03b7\u2207\u2126f(xi)\u20162,\nwhere \u2207\u2126f(xi) is the projeceted vector of f(xi) in which the entries outoside \u2126 are set to zero and the entries in \u2126 are unchanged. \u2016ri \u2212 \u03b7\u2207\u2126f(xi)\u20162 has the inequalities\n\u2016ri \u2212 \u03b7\u2207\u2126f(xi)\u20162 = \u2016ri\u2126c + ri\u2126 \u2212 \u03b7\u2207\u2126f(xi)\u2016 \u2264 \u2016ri\u2126c\u20162 + \u2016ri\u2126 \u2212 \u03b7\u2207\u2126f(xi) + \u03b7\u2207\u2126f(x)\u2212 \u03b7\u2207\u2126f(x)\u2016 \u2264 \u2016ri\u2126c\u20162 + \u2016ri\u2126 \u2212 \u03b7\u2207\u2126f(xi) + \u03b7\u2207\u2126f(x)\u2016+ \u2016\u03b7\u2207\u2126f(x)\u2016 \u2264 \u2016ri\u2126c\u20162 + \u2016ri\u2126 \u2212 \u03be\u2207\u2126f(xi) + \u03be\u2207\u2126f(x)\u2016+\n(\u03be \u2212 \u03b7)\u2016\u2207\u2126f(xi)\u2212\u2207\u2126f(x)\u20162\u2016+ \u2016\u03b7\u2207\u2126f(x)\u20162 \u2264 \u2016ri\u2126c\u20162 + (1\u2212 \u03b7/\u03be + (2\u2212 \u03b7/\u03be)\u03b4)\u2016ri\u20162 + \u03b7\u2016\u2207If(x)\u20162\nwhere the last inequality follows from condition (\u03be, \u03b4,M)WRSC and Lemma III.2. From Lemma III.3, we have\n\u2016ri\u0393c\u20162 \u2264 \u221a 1\u2212 \u03b120\u2016ri\u20162 + [\u03b20 \u03b10 + \u03b10\u03b20\u221a 1\u2212 \u03b120 ] \u2016\u2207If(x)\u20162 Combining the above inequalities, we prove the theorem. Our proposed GRAPH-IHT generalizes several existing sparsity-constrained optimization algorithms: 1) Projected Gradient Descent (PGD) [30]. If we redefine H(b) = supp(b) and T(b) = supp(P(b)), where P(b) is the projection oracle defined in Equation (5), then GRAPH-IHT reduces to the PGD method; 2) Approximated Model-IHT(AMIHT) [13]. If the cost function f(x) is defined as the least square cost function f(x) = \u2016y \u2212Ax\u201622, then \u2207f(x) has the specific form \u2212AT(y \u2212A) and GRAPH-IHT reduces to the AM-IHT algorithm, the state-of-the-art variant of IHT for compressive sensing and linear regression problems. In particular, let e = y \u2212Ax. The component \u2016\u2207f(xi)\u20162 = \u2016ATe\u20162 is upper bound by bounded by \u221a 1 + \u03b4\u2016e\u20162 [13], Assume that \u03be = 1 and \u03b7 = 1. Condition (\u03be, \u03b7,M)-WRSC then reduces to the RIP condition in compressive sensing. The convergence inequality (10) then reduces to\n\u2016xi+1 \u2212 x\u20162 \u2264 \u03b1\u2032\u2016xi \u2212 x\u20162 + \u03b2\u2032\u2016e\u20162, (11) where \u03b1\u2032 = (1 + cT ) [ \u03b4 + \u221a 1\u2212 \u03b120 ] and\n\u03b2\u2032 = (1 + cT ) [ (\u03b10 + \u03b20)\u221a1 + \u03b4\n\u03b10 + \u03b10\u03b20(\n\u221a 1 + \u03b4)\u221a\n1\u2212 \u03b120\n] .\nSurprisingly, the above convergence inequality is identical to the convergence inequality of AM-IHT derived in [13] based on the RIP condition, which indicates that GRAPH-IHT has the same convergence rate and approximation error as AM-IHT, although we did not make any attempt to explore the special properties of the RIP condition. We note that the convergence properties of GRAPH-IHT hold in fairly general setups beyond compressive sensing and linear regression. As we consider GRAPH-IHT as a fast variant of GRAPH-GHTP, due to space limit we ignore the discussions about the convergence condition of GRAPH-IHT. The theoretical analysis of GRAPH-GHTP to be discussed in the next subsection can be readily adapted to the theoretical analysis of GRAPH-IHT.\nE. Theoretical Analysis of GRAPH-GHTP\nTheorem III.4. Consider the sparsity model of connected subgraphs M(G, k) for some k \u2208 N and a cost function f : Rn \u2192 R that satisfies the (\u03be, \u03b4,M(G, 5k))-model-WRSC condition. If \u03b7 = cH(1 \u2212 \u03b4) \u2212 \u03b4 then for any x \u2208 Rn such that supp(x) \u2208M(G, k), with \u03b7 > 0 the iterates of Algorithm 2 obey\n\u2016xi+1 \u2212 x\u20162 \u2264 \u03b1\u2016xi \u2212 x\u20162 + \u03b2\u2016\u2207If(x)\u20162 (12) where\n\u03b10 = cH(1\u2212 \u03b4)\u2212 \u03b4, \u03b20 = \u03b4(1 + cH),\n\u03b1 =\n\u221a 2(1 + cT )\n1\u2212 \u03b4\n(\u221a 1\u2212 \u03b120 + ( (2\u2212 \u03b7\n\u03be )\u03b4 + 1\u2212 \u03b7 \u03be\n)) ,\n\u03b2 = 1 + cT 1\u2212 \u03b4\n( (1 + 2 \u221a 2)\u03be + (2\u2212 2 \u221a 2)\u03b7 +\n\u221a 2\u03b20 \u03b10 + \u221a 2\u03b10\u03b20\u221a 1\u2212 \u03b120\n) ,\nand I = argmaxS\u2208M(G,8k) \u2016\u2207Sf(x)\u20162\nProof: Denote \u2126 = H(\u2207f(xi)) and \u03a8 = supp(xi \u2212 \u03b7 \u00b7 \u2207\u2126f(xi)). Let ri+1 = xi+1 \u2212 x. \u2016ri+1\u20162 is bounded as\n\u2016ri+1\u20162 = \u2016xi+1 \u2212 x\u20162 \u2264 \u2016xi+1 \u2212 b\u20162 + \u2016x\u2212 b\u20162 \u2264 cT \u2016x\u2212 b\u20162 + \u2016x\u2212 b\u20162 \u2264 (1 + cT )\u2016x\u2212 b\u20162, (13)\nwhere the second inequality follows from the definition of tail approximation. The component \u2016(x\u2212 b)\u03a8\u201622 is bounded as\n\u2016(x\u2212 b)\u03a8\u201622 = \u3008b\u2212 x, (b\u2212 x)\u03a8\u3009 = \u3008b\u2212 x\u2212 \u03be\u2207\u03a8f(b) + \u03be\u2207\u03a8f(x), (b\u2212 x)\u03a8\u3009 \u2212\n\u3008\u03be\u2207\u03a8f(x), (b\u2212 x)\u03a8\u3009 \u2264 \u03b4\u2016b\u2212 x\u20162\u2016(b\u2212 x)\u03a8\u20162 + \u03be\u2016\u2207\u03a8f(x)\u20162\u2016(b\u2212 x)\u03a8\u20162,\nwhere the second equality follows from the fact that \u2207Sf(b) = 0 since b is the solution to the problem in the third Step (Line 7) of GRAPH-GHTP, and the last inequality can be derived from condition (\u03be, \u03b4,M(G, 8k))-WRSC. After simplification, we have\n\u2016(x\u2212 b)\u03a8\u20162 \u2264 \u03b4\u2016b\u2212 x\u20162 + \u03be\u2016\u2207\u03a8f(x)\u20162. It follows that\u2016x\u2212 b\u20162 \u2264 \u2016(x\u2212 b)\u03a8\u20162 + \u2016(x\u2212 b)\u03a8c\u20162\n\u2264 \u03b4\u2016b\u2212 x\u20162 + \u03be\u2016\u2207\u03a8f(x)\u20162 + \u2016(x\u2212 b)\u03a8c\u20162.\nAfter rearrangement we obtain\n\u2016b\u2212 x\u20162 \u2264 \u2016(b\u2212 x)\u03a8c\u20162 1\u2212 \u03b4 + \u03be\u2016\u2207\u03a8f(x)\u20162 1\u2212 \u03b4 , (14)\nwhere this equality follows from the fact that supp(b) \u2286 S. Let \u03a6 = supp(x) \u2208M(G, k).\n\u2016(xi \u2212 \u03b7\u2207\u2126f(xi))\u03a6\u20162 \u2264 \u2016(xi \u2212 \u03b7\u2207\u2126f(xi))\u03a8\u20162,\nas \u03a8 = supp(xi\u2212\u03b7\u00b7\u2207\u2126f(xi)). By eliminating the contribution on \u03a6 \u2229\u03a8, we derive \u2016(xi \u2212 \u03b7\u2207\u2126f(xi))\u03a6\\\u03a8\u20162 \u2264 \u2016(xi \u2212 \u03b7\u2207\u2126f(xi))\u03a8\\\u03a6\u20162\nFor the right-hand side, we have \u2016(xi \u2212 \u03b7\u2207\u2126f(xi))\u03a8\\\u03a6\u20162 \u2264 \u2016(xi \u2212 x\u2212 \u03b7\u2207\u2126f(xi) + \u03b7\u2207\u2126f(x))\u03a8\\\u03a6\u20162 + \u03b7\u2016\u2207\u2126\u222a\u03a8f(x)\u20162,\nwhere the inequality falls from the fact that \u03a6 = supp(x). From the left-hand side, we have\n\u2016(xi \u2212 \u03b7\u2207\u2126f(xi))\u03a6\\\u03a8\u20162 \u2264 \u2212\u03b7\u2016\u2207\u2126\u222a\u03a6f(x)\u20162 + \u2016(xi \u2212 x\u2212 \u03b7\u2207\u2126f(xi) + \u03b7\u2207\u2126f(x))\u03a6\\\u03a8 + (x\u2212 b)\u03a8c\u20162\nwhere the inequality follows from the fact that b\u03a8c = 0, x\u03a6\\\u03a8 = x\u03a8c , and \u2212x\u03a6\\\u03a8 + (x \u2212 b)\u03a8c = 0. Let \u03a6\u2206\u03a8 be the symmetric difference of the set \u03a6 and \u03a8. It follows that\n\u2016(b\u2212 x)\u03a8c\u20162 \u2264 \u221a 2\u2016(xi \u2212 x\u2212 \u03b7\u2207\u2126f(xi) + \u03b7\u2207\u2126f(x))\u03a6\u2206\u03a8\u20162 + 2\u03b7\u2016\u2207If(x)\u20162\n\u2264 \u221a 2\u2016(xi \u2212 x\u2212 \u03be\u2207\u2126f(xi) + \u03be\u2207\u2126f(x))\u03a6\u2206\u03a8\u20162 +\u221a\n2(\u03be \u2212 \u03b7)\u2016(\u2207\u2126f(xi) +\u2207\u2126f(x))\u03a6\u2206\u03a8\u2016+ 2\u03b7\u2016\u2207If(x)\u20162 \u2264 \u221a 2\u2016(ri\u2126c + ri\u2126 \u2212 \u03be\u2207\u2126f(xi) + \u03be\u2207\u2126f(x))\u03a6\u2206\u03a8\u20162 +\u221a\n2(\u03be \u2212 \u03b7)\u2016(\u2207\u2126f(xi)\u2212\u2207\u2126f(x))\u03a6\u2206\u03a8\u2016+ 2\u03b7\u2016\u2207If(x)\u20162 \u2264 \u221a 2\u2016ri\u2126c\u2016+\n\u221a 2\u2016(ri\u2126 \u2212 \u03be\u2207\u2126f(xi) + \u03be\u2207\u2126f(x))\u03a8\u2206\u03a8\u20162 +\u221a\n2(\u03be \u2212 \u03b7)\u2016(\u2207\u2126f(xi)\u2212\u2207\u2126f(x))\u03a8\u2206\u03a8\u2016+ 2\u03b7\u2016\u2207If(x)\u20162 \u2264 \u221a 2\u2016ri\u2126c\u2016+\n\u221a 2\u2016ri \u2212 \u03be\u2207\u2126\u222a\u03a8\u222a\u03a6f(xi) + \u03be\u2207\u2126\u222a\u03a8\u222a\u03a6f(x)\u20162 +\u221a\n2(\u03be \u2212 \u03b7)\u2016(\u2207\u2126\u222a\u03a8\u222a\u03a6f(xi)\u2212\u2207\u2126\u222a\u03a8\u222a\u03a6f(x))\u03a8\u2206\u03a8\u2016+ 2\u03b7\u2016\u2207If(x)\u20162\n\u2264 \u221a 2\u2016ri\u2126c\u20162 + \u221a 2 (( 2\u2212 \u03b7\n\u03be\n) \u03b4 + 1\u2212 \u03b7\n\u03be\n) \u2016ri\u2016+\n2 (\u221a 2\u03be + (1\u2212 \u221a 2)\u03b7 ) \u2016\u2207If(x)\u20162,\nwhere the first inequality follows from the fact that\n\u03b7\u2016\u2207\u2126\u222a\u03a6f(x)\u20162 + \u03b7\u2016\u2207\u03a8\u222a\u03a6\u222a\u2126f(x)\u20162 \u2264 2\u03b7\u2016\u2207If(x)\u20162, the third inequality follows as xi \u2212 x = ri = ri\u2126c + ri\u2126, the fourth inequality follows from the fact that \u2016(ri\u2126c)\u03a6\u2206\u03a8\u20162 \u2264 \u2016ri\u2126c\u20162, the fifth inequality follows as ri \u2286 \u2126\u222a\u03a8\u222a\u03a6, and the last inequality follows from condition (\u03be, \u03b4,M(G, 8k))-WRSC and Lemma III.2. From Lemma III.3, we have\n\u2016ri\u2126c\u20162 \u2264 \u221a 1\u2212 \u03b72\u2016ri\u20162 +\n[ \u03be(1 + cH)\n\u03b7 + \u03be\u03b7(1 + cH)\u221a\n1\u2212 \u03b72\n] \u2016\u2207If(x)\u20162\nCombining (14) and above inequalities, we prove the theorem.\nTheorem III.4 shows the estimator error of GRAPH-GHTP is determined by the multiple of \u2016\u2207Sf(x)\u20162, and the convergence rate is geometric. Specifically, if x is an uncontrained minimizer of f(x), then \u2207f(x) = 0. It means GRAPH-GHTP is guaranteed to obtain the true x to arbitrary precision. The estimation error is negligible when x is sufficiently close to an unconstrained minimizer of f(x) as \u2016\u2207Sf(x)\u20162 is a small value. The parameter\n\u03b1 =\n\u221a 2(1 + cT )\n1\u2212 \u03b4\n(\u221a 1\u2212 \u03b120 + ( (2\u2212 \u03b7\n\u03be )\u03b4 + 1\u2212 \u03b7 \u03be\n)) < 1,\ncontrols the convergence rate of GRAPH-GHTP. Our algorithm allows an exact recovery if \u03b1 < 1. As \u03b4 is an arbitrary\nconstant parameter, it can be an arbitrary small positive value. Let \u03b7 be \u03be and \u03b4 be an arbitrary small positive value, the parameters cH and cT satisfy the following inequality\nc2H > 1\u2212 1/(1 + cT )2. (15) It is noted that the head and tail approximation algorithms described in [15] do not meet the inequality (15). Nonetheless, the approximation factor cH of any given head approximation algorithm can be boosted to any arbitrary constant c\u2032H < 1, which leads to the satisfaction of the above condition as shown in [15]. Boosting the head-approximation algorithm, though strongly suggested by [13], is not empirically necessary.\nOur proposed GRAPH-GHTP has strong connections to the recently proposed algorithm named as Gradient Hard Thresholding Pursuit (GHTP) [40] that is designed specifically for the k-sparsity model: M = {S \u2286 [n] | |S| \u2264 k}. In particular, if we redefine H(b) = supp(b) and T(b) = supp(P(b)), where P(b) is the projection oracle defined in Equation (5), and assume that there is an algorithm that solves the projection oracle exactly, in which the sparsity model does not require to be the k-sparsity model. It then follows that the upper bound of \u2016ri\u2126c\u20162 stated in Lemma III.2 in Appendix is updated as \u2016ri\u2126c\u20162 \u2264 0, since supp(ri) = \u2126 and ri\u2126c = 0. In addition, the multiplier (1 + cT ) is replaced as 1 as the first inequality (13) in the proof of Theorem III.4 in Appendix is updated as \u2016ri+1\u20162 \u2264 \u2016x\u2212b\u20162, instead of the original version \u2016ri+1\u20162 \u2264 (1 + cT )\u2016x \u2212 b\u20162. After these two changes, the shrinkage rate \u03b1 is updated as\n\u03b1 =\n\u221a 2\n1\u2212 \u03b4\n( (2\u2212 \u03b7\n\u03be )\u03b4 + 1\u2212 \u03b7 \u03be\n) , (16)\nwhich is the same as the shrinkage rate of GRAPH-GHTP as derived in [40] specifically for the k-sparsity model. The above shrinkage rate \u03b1 (16) should satisfy the condition \u03b1 < 1 to ensure the geometric convergence of GRAPH-GHTP, which implies that\n\u03b7 > ((2 \u221a 2 + 1)\u03b4 + \u221a 2\u2212 1)\u03be/( \u221a 2 + \u221a 2\u03b4). (17)\nIt follows that if \u03b4 < 1/( \u221a\n2 + 1), a step-size \u03b7 < \u03be can always be found to satisfy the above inequality. This constant condition of \u03b4 is analogous to the constant condition of stateof-the-art compressive sensing methods that consider noisy measurements [23] under the assumption of the RIP condition. We derive the analogous constant using the WRSC condition that weaker than the RIP condition.\nAs discussed above, our proposed GRAPH-GHTP has connections to GHTP on the shrinkage rate of geometric convergence. We note that the shrinkage rate of our proposed GRAPH-GHTP stated in Theorem III.4 is derived based on head and tail approximations of the sparsity model of connected subgraphs M(G, k), instead of the k-sparsity model that has an exact projection oracle solver. Our convergence properties hold in fairly general setups beyond k-sparsity model, as a number of popular structured sparsity models such as the \u201cstandard\u201d k-sparsity, block sparsity, cluster sparsity, and tree sparsity can be encoded as special cases of M(G, k).\nTheorem III.5. Let x \u2208 Rn such that supp(x) \u2208 M(G, k), and f : Rn \u2192 R be cost function that satisfies condition (\u03be, \u03b4,M(8k, g))-WRSC. Assuming that \u03b1 < 1, GRAPH-GHTP (or GRAPH-IHT) returns a x\u0302 such that, supp(x\u0302) \u2208 M(5k, g) and \u2016x\u2212 x\u0302\u20162 \u2264 c\u2016\u2207If(x)\u20162, where c = (1 + \u03b21\u2212\u03b1 ) is a fixed constant. Moreover, GRAPH-GHTP runs in time\nO ( (T + |E| log3 n) log(\u2016x\u20162/\u2016\u2207If(x)\u20162) ) , (18)\nwhere T is the time complexity of one execution of the subproblem in Step 6 in GRAPH-GHTP (or Step 5 in GRAPHIHT). In particular, if T scales linearly with n, then GRAPHGHTP (or GRAPH-IHT) scales nearly linearly with n.\nProof: The i-th iterate of GRAPH-GHTP (or GRAPHIHT) satisfies\n\u2016x\u2212 xi\u20162 \u2264 \u03b1i\u2016x\u20162 + \u03b2\n1\u2212 \u03b1\u2016\u2207If(x)\u20162. (19) After t = \u2308 log (\n\u2016x\u20162 \u2016\u2207If(x)\u20162 ) / log 1\u03b1 \u2309 iterations, GRAPH-\nGHTP (or GRAPH-IHT) returns an estimate x\u0302 satisfying \u2016x\u2212 x\u0302\u20162 \u2264 (1 + \u03b21\u2212\u03b1 )\u2016\u2207If(x)\u20162. The time complexities of both head approximation and tail approximation are O(|E| log3 n). The time complexity of one iteration in GRAPH-GHTP (or GRAPH-IHT) is (T + |E| log3 n), and the total number of iterations is \u2308 log (\n\u2016x\u20162 \u2016\u2207If(x)\u20162 ) / log 1\u03b1 \u2309 , and the overall time\nfollows. GRAPH-GHTP and GRAPH-IHT are only different in the definition of \u03b1 and \u03b2 in this Theorem.\nAs shown in Theorem 18, the time complexity of GRAPHGHTP is dependent on the total number of iterations and the time cost (T ) to solve the subproblem in Step 6. In comparison, the time complexity of GRAPH-IHT is dependent on the total number of iterations and the time cost T to calculate the gradient \u2207f(xi) in Step 5. It implies that, although GRAPHGHTP converges faster than GRAPH-IHT, the time cost to solve the subproblem in Step 6 is often much higher than the time cost to calculate a gradient \u2207f(xi), and hence GRAPHIHT runs faster than GRAPH-GHTP in practice."}, {"heading": "IV. APPLICATIONS ON GRAPH SCAN STATISTICS", "text": "In this section, we specialize GRAPH-IHT and GRAPHGHTP to optimize a number of well-known graph scan statistics for the task of connected subgraph detection, including elevated mean scan (EMS) statistic [29], Kulldorff\u2019s scan statistic [25], and expectation-based Poisson (EBP) scan statistic [26]. Each graph scan statistic is defined as the generalized likelihood ratio test (GLRT) statistic of a specific hypothesis testing about the distributions of features of normal and abnormal nodes. The EMS statistic corresponds to the following GLRT test: Given a graph G = (V,E), where V = [n] and E \u2286 V \u00d7 V, each node i is associated with a random variable xi:\nxi = \u00b5 \u00b7 1(i \u2208 S) + i, i \u2208 V, (20) where |\u00b5| represents the signal strength and i \u2208 N (0, 1). S is some unknown anomalous cluster that forms as a connected subgraph. The task is to decide between the null hypothesis (H0): ci \u2208 N (0, 1),\u2200i \u2208 V and the alternative (H1(S)): ci \u2208 N (\u00b5, 1),\u2200i \u2208 S and ci \u2208 N (0, 1),\u2200i /\u2208 S. The EMS statistic\nis defined as the GLRT function under this hypothesis testing:\nF (S) = Prob(Data|H1(S))\nProb(Data|H0) = 1\u221a |S| \u2211 i\u2208S ci. (21)\nThe problem of connected subgraph detection based on the EMS statistic is then formulated as\nmin S\u2286V \u2212 1 |S| ( \u2211 i\u2208S ci) 2 s.t. S \u2208M(G, k), (22)\nwhere the square of the EMS scan statistic is considered to make the function smooth, and this transformation does not infect the optimum solution. Let the {0, 1}-vectors form of S be x \u2208 {0, 1}n, such that supp(x) = S. Problem (22) can be reformulated as\nmin x\u2208{0,1}n\n\u2212(cTx)2/(1Tx) s.t. supp(x) \u2208M(G, k), (23)\nwhere c = [c1, \u00b7 \u00b7 \u00b7 , cn]T. To apply our proposed algorithms, we relax the input domain of x and maximize the strongly convex function [3]:\nmin x\u2208Rn \u2212(cTx)2/(1Tx) + 1 2 xTx s.t. supp(x) \u2208M(G, k). (24) The connected subset of nodes can be found as the subset of indexes of positive entries in x\u0302, where x\u0302 refers to the solution of the Problem (24). Assume that c is normalized and ci \u2264 1, \u2200i. Let c\u0302 = max{c1, \u00b7 \u00b7 \u00b7 , cn}. The Hessian matrix of the above objective function satisfies the following conditions\n(1\u2212 c\u03022) \u00b7 I I\u2212 (c\u2212 c Tx 1Tx 1)(c\u2212 c Tx 1Tx 1)T 1 \u00b7 I. (25) According to Lemma 1 (b) in [40]), the objective function f(x) satisfies condition (\u03be, \u03b4,M(G, 8k))-WRSC that\n\u03b4 = \u221a\n1\u2212 2\u03be(1\u2212 c\u03022) + \u03be2, for any \u03be such that \u03be < 2(1\u2212 c\u03022). The geometric convergence of GRAPH-GTHP as shown in Theorem III.4 is guaranteed.\nDifferent from the EMS statistic that is defined for numerical features based on Gaussian distribution, the Kulldorff\u2019s scan statistic and Expectation Based Poisson statistic (EBP) are defined for count features based on Poisson distribution. In particular, each node i is associated with a feature ci, the count of events (e.g., crimes, flu infections) observed at the current time, and a feature bi, the expected count (or \u2018baseline\u2019) of events by using historical data. Let c = [c1, \u00b7 \u00b7 \u00b7 , cn]T and b = [b1, \u00b7 \u00b7 \u00b7 , bn]T. The Kulldorff\u2019s scan statistic and EBP scan statistics are described Table I. We note that these two scan statistics do not satisfy the WRSC condition, but as demonstrated in our experiments, our proposed algorithms perform empirically well for all the three scan statistics, and in particular, our proposed GRAPH-GHTP converged in less than 10 iterations in all the settings."}, {"heading": "V. EXPERIMENTS", "text": "This section evaluates the performance of our proposed methods using four public benchmark data sets for connected subgraph detection. The experimental code and data sets are available from the Link [11] for reproducibility."}, {"heading": "A. Experiment Design", "text": "Datasets: 1) BWSN Dataset. A real-world water network is offered in the Battle of the Water Sensor Networks (BWSN) [28]. That has 12,527 nodes and 14,323 edges. In order to simulate a contaminant sub-area, 4 nodes with chemical contaminant plumes, which were distributed in this sub-area, were generated. We use the water network simulator EPANET [31] that was employed in BWSN for a period of 3 hours to simulate the spreads for contaminant plumes on this graph. If a node is polluted by the chemical, then its sensor reports 1, otherwise, 0, in each hour. To test the tolerance of noise of our methods, K \u2208 {2, 4, 6, 8, 10} percent vertices were selected randomly, and their sensor reports were set to 0 if their original reports were 1 and vice versa. Each hour has a graph snapshot. The snapshots corresponding to the 3 hours that have 0% noise are considered for training, and the snapshots that have 2%, \u00b7 \u00b7 \u00b7 , 10% noise reports for testing. The goal is to detect a connected subgraph that is corresponding to the contaminant sub-area. 2) CitHepPh Dataset. We downloaded the high energy physics phenomenology citation data (CitHepPh) from Stanford Network Analysis Project (SNAP) [20]. This citation graph contains 11,897 papers corresponding to graph vertices and 75,873 edges. An undirected edge between two vertices (papers) exists , if one paper is cited by another. The period of these papers published is from January 1992 to April 2002. Each vertex has two attributes for each specific year (t = 1992, \u00b7 \u00b7 \u00b7 , t = 2002). We denote the number of citations of each specific year as the first attribute and the average citations of all papers in that year as the second attribute. The goal is to detect a connected subgraph where the number of citations of vertices (papers) in this subgraph are abnormally high compared with the citations of vertices that are not in this subgraph. This connected subgraph is considered as a potential emerging research area. Since the training data is required for some baseline methods, the data before 1999 is considered as the training data, and the rest from 1999 to 2002 as the testing data. 3) Traffic Dataset. Road traffic speed data from June 1, 2013 to Mar. 31, 2014 in the arterial road network of the Washington D.C. region is collected from the INRIX database (http://inrix.com/publicsector.asp), with 1,723 nodes and 5,301 edges. The database provides traffic speed for each link at a 15-minute rate. For each 15-minute interval, each method identities a connected subgraph as the most congested region. 4) ChicagoCrime Dataset. We collected crime data from City of Chicago [https://data.cityofchicago.org/PublicSafety/Crimes-2001-to-present/ijzp-q8t2] from Jan. 2001 to Dec. 2015. There are 46,357 nodes (census blocks) and 168,020 edges (Two census blocks are connected with each other if they are neighbours). Specifically, we collected all records of burglaries from 2001 to 2015. The data covers burglaries in the period from Jan. 2001 to Dec. 2015. Each vertex has an attribute denoting the number of burglaries in sepcific year and average number of burglaries over 10 years. We aim to detect connected census areas which has anomaly high burglaries accidents. The data before 2010 is considered as training data, and the data from 2011 to 2015 is considered as testing data. Graph Scan Statistics: As shown in Table I, three graph scan statistics were considered as the scoring functions of connected subgraphs, including Kulldorff\u2019s scan statistic [25], expectation-based Poisson (EBP) scan statistic [26], and elevated mean scan (EMS) statistic [29]. The first two statistic functions require that each vertex v has a count cv representing the count of events observed at that vertex, and an expected count (\u2018baseline\u2018) bv . For EMS statistic, only cv is used. We need to normalize cv for EMS as it is defined based on the assumptions of standard normal distribution for normal values and shifted-mean normal distribution for abnormal values.\nTable II provides details about the calculations of cv and bv for each data set.\nComparison Methods: We compared our proposed methods with four state-of-the-art baseline methods that are designed specifically for connected subgraph detection, namely, GraphLaplacian [33], EventTree [32], DepthFirstGraphScan [36] and NPHGS [8]. DepthFirstGraphScan is an exact search algorithm based on depth-first search and takes weeks to run on graphs that have more than 1000 nodes. We imposed a maximum limit on the depth of the search to 10 to reduce its time complexity.\nThe basic ideas of these baseline methods are summarized as follows: NPHGS starts from random seeds (nodes) as initial candidate clusters and gradually expends each candidate cluster by including its neighboring nodes that could help improve its BJ statistic score until no new nodes can be added. The candidate cluster with the largest BJ statistic score is returned. DepthFirstGraphScan adopts a similar strategy to NPHGS but expands the initial clusters based on depth-first search. GraphLaplacian uses a graph Laplacian penalty function to replace the connectivity constraint and converts the problem to a convex optimization problem. EventTree reformulates the connected subgraph detection problem as a prize-collecting steiner tree (PCST) problem [19] and apply the Goemans-Williamson (G-W) algorithm for PCST [19] to detect anomalous subgraphs. We also implemented the generalized fused lasso model (GenFusedLasso) for graph scan statistics using the framework of alternating direction method of multipliers (ADMM). GenFusedLasso method solves the following minimization problem\nmin x\u2208Rn\n\u2212f(x) + \u03bb \u2211\n(i,j)\u2208E |xi \u2212 xj |, (26)\nwhere f(x) is a predefined graph scan statistic and the trade-off parameter \u03bb controls the degree of smoothness of neighboring entries in x. We applied the heuristic rounding step proposed in [29] to x to generate connected subgraphs.\nParameter Tunning: We strictly followed strategies recommended by authors in their original papers to tune the related model parameters. Specifically, for EventTree, we tested the set of \u03bb values: {0.02, 0.04, \u00b7 \u00b7 \u00b7 , 2.0, 3.0, \u00b7 \u00b7 \u00b7 , 20}. For Graph-Laplacian, we tested the set of \u03bb values: {0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1} and returned the best result. For GenFusedLasso, we tested the set of \u03bb values: {0.02, 0.04, \u00b7 \u00b7 \u00b7 , 2.0, 3.0, \u00b7 \u00b7 \u00b7 , 20}. For NPHGS, we set the suggested parameters by the authors: \u03b1max = 0.15 and K = 5. Our proposed methods GRAPH-IHT and GRAPH-GHTP have a single parameter k, an upper bound of the subgraph size. We tested the set of k values: {50, 100, \u00b7 \u00b7 \u00b7 , 1000}. As the BWSN dataset has the ground truth about the contaminated nodes, we identified the best parameter for each method that has the largest F-measure in the training data. For the other data sets, as we do not have ground truth about the true subgraphs, for each specific scan statistic, we identified the best parameter for each method that was able to identify the connected subgraph with the largest statistic score.\nPerformance Metrics: 1) Optimization Power. The over-\nall scores of the three graph scan statistic functions of the connected subgraphs returned by the comparison methods are compared and analyzed. The objective is to identify methods that could find the connected subgraphs with the largest graph scan statistic scores. 2) Precision, Recall, and F-Measure. For the BWSN dataset, as the true anomalous subgraphs are known,we use F-measure that combines precision and recall to evaluate the quality of detected subgraphs by different methods. 3) Run Time. The running times of different methods are compared."}, {"heading": "B. Evolving Curves of Graph Scan Statistics", "text": "Figure 3 compares our methods Graph-IHT and GRAPHGHTP with GenFusedLasso on the scores of two graph scan statistics (Kulldorff\u2019s scan statistic and elevated mean scan statistic (EMS)) based on the best connected subgraphs identified by both methods in different iterations. Note that, a heuristic rounding process as proposed in [29] is applied to continuous vector xi estimated by GenFusedLasso at each iteration i in order to identify the best connected subgraph at the current iteration. As the setting of the parameter \u03bb will influence the quality of the detected connected subgraph, the results under different \u03bb values are also shown in Figure 2. The results indicate that our method GRAPH-GHTP converges in less than 10 iterations, and GRAPH-IHT converges in more steps. The qualities (scan statistic scores) of the connected subgraphs identified at different iterations by our two methods are consistently higher than those returned by GenFusedLasso."}, {"heading": "C. Optimization Power", "text": "The comparisons between our method and the other baseline methods are shown in Table III and Table IV. The scores of the three graph scan statistics based on the connected subgraphs returned by these methods are reported in these two tables. The results in indicate that our method outperformed all the baseline methods on the three graph scan statistics, except that EventTree achieved the highest Kulldorff score (16738.43) on the CitHepPh dataset, but is only 2.71% larger than the returned score of our method GRAPH-GHTP. We note EventTree is a heuristic algorithm and does not provide theoretical guarantee on the quality of the connected subgraph returned, as measured by the scan statistic scores."}, {"heading": "D. Water Pollution Detection", "text": "Figure 4 shows the precision, recall, and F-measure of all the comparison methods on the detection of polluted nodes in the water distribution network in BWSN with respect to different noise ratios. The results indicate that our proposed method GRAPH-GHTP and DepthFirstGraphScan were the best methods on all the three measures for most of the settings. However, DepthFirstGraphScan spent 5929 seconds to finish, and GRAPH-GHTP spent only 166 seconds, 35.8 times faster than DepthFirstGraphScan. EventTree achieved high recalls but low precisions consistently in different settings. In contrast, GraphLaplacian and NPHGS achieved high precisions but low recalls in most settings."}, {"heading": "E. Scalability Analysis", "text": "Table III and Table IV also show the comparison between our proposed method GRAPH-GHTP and other baseline methods on the running time. The results indicate that our proposed method GRAPH-GHTP ran faster than all the baseline methods in most of the settings, except for EventTree. EventTree was the fastest method but was unable to detect subgraphs with high qualities. As our method has a parameter on the upper bound (k) of the subgraph returned, we also conducted the scalability of our method with respect to different values of k as shown in Figure 2. The results indicate that the running time of our algorithm is insensitive to the setting of k, which is consistent with the time complexity analysis of GRAPH-GHTP as discussed in Theorem III.5."}, {"heading": "VI. RELATED WORK", "text": "A. Structured sparse optimization. The methods in this category have been briefly reviewed in the introduction section. The most relevant work is by Hegde et al. [15]. The authors present GRAPH-COSAMP, a variant of COSAMP [23], for compressive sensing and linear regression problems based on head and tail approximations of M(G, k). B. Connected subgraph detection. Existing methods in this category fall into three major categories:1) Exact algorithms. The most recent method is a brunch-and-bounding algorithm DepthFirstGraphScan [36] that runs in exponential time in the worst case; 2) Heuristic algorithms. The most recent methods in this category include EventTree [32], NPHGS [8], AdditiveScan [37],\nGraphLaplacian [33], and EdgeLasso [34]; 3) Approximation algorithms that provide performance bounds. The most recent method is presented by Qian et al.. The authors reformulate the connectivity constraint as linear matrix inequalities (LMI) and present a semi-definite programming algorithm based on convex relaxation of the LMI [18, 19] with a performance bound. However, this method is not scalable to large graphs (\u2265 1000 nodes). Most of the above methods are considered as baseline methods in our experiments and are briefly summarized in Section V-A."}, {"heading": "VII. CONCLUSION", "text": "This paper presents, GRAPH-IHT and GRAPH-GHTP, two efficient algorithms to optimize a general nonlinear optimization problem subject to connectivity constraint on the support of variables. Extensive experiments demonstrate the effectiveness and efficency of our algorithms. For the future work, we plan to explore graph-structured constraints other than connectivity constraint and extend our proposed methods such that good theoretical properties of the cost functions that do not satisfy the WRSC condition can also be analyzed."}], "references": [{"title": "Non-recurrent traffic congestion detection on heterogeneous urban road networks", "author": ["B. Anbaro\u011flu", "T. Cheng", "B. Heydecker"], "venue": "TRANS- PORTMETRICA, 11(9):754\u2013771", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning with submodular functions: A convex optimization perspective", "author": ["F. Bach"], "venue": "arXiv:1111.6453", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Structured sparsity through convex optimization. Stat Sci, 27(4):450\u2013468", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning model-based sparsity via projected gradient descent", "author": ["S. Bahmani", "P.T. Boufounos", "B. Raj"], "venue": "IT", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Greedy sparsity-constrained optimization", "author": ["S. Bahmani", "B. Raj", "P.T. Boufounos"], "venue": "JMLR, 14(1):807\u2013841", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Compressed sensing with nonlinear observations and related nonlinear optimization problems", "author": ["T. Blumensath"], "venue": "IT, 59(6):3466\u20133474", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Non-parametric scan statistics for event detection and forecasting in heterogeneous social media graphs", "author": ["F. Chen", "D.B. Neill"], "venue": "ACM SIGKDD, pages 1166\u20131175", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Human rights event detection from heterogeneous social media graphs", "author": ["F. Chen", "D.B. Neill"], "venue": "Big Data, 3(1):34\u201340", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Geographic analysis of forest health indicators using spatial scan statistics", "author": ["J.W. Coulston", "K.H. Riitters"], "venue": "EM, 31(6):764\u2013773", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Early warning system for temporary crime hot spots", "author": ["W.L. Gorr", "Y. Lee"], "venue": "Journal of Quantitative Criminology, 31(1):25\u201347", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Approximation-tolerant modelbased compressive sensing", "author": ["C. Hegde", "P. Indyk", "L. Schmidt"], "venue": "SODA, pages 1544\u20131561. SIAM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "A fast approximation algorithm for tree-sparse recovery", "author": ["C. Hegde", "P. Indyk", "L. Schmidt"], "venue": "ISIT, pages 1842\u20131846. IEEE", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "A nearly-linear time framework for graph-structured sparsity", "author": ["C. Hegde", "P. Indyk", "L. Schmidt"], "venue": "ICML, pages 928\u2013937", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning with structured sparsity", "author": ["J. Huang", "T. Zhang", "D. Metaxas"], "venue": "JMLR, 12:3371\u20133412", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Group lasso with overlap and graph lasso", "author": ["L. Jacob", "G. Obozinski", "J.-P. Vert"], "venue": "ICML, pages 433\u2013440. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "On iterative hard thresholding methods for high-dimensional m-estimation", "author": ["P. Jain", "A. Tewari", "P. Kar"], "venue": "NIPS, pages 685\u2013693", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "The prize collecting steiner tree problem: theory and practice", "author": ["D.S. Johnson", "M. Minkoff", "S. Phillips"], "venue": "SODA, pages 760\u2013769", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2000}, {"title": "Graphs over time: densification laws, shrinking diameters and possible explanations", "author": ["J. e. a. Leskovec"], "venue": "In KDD,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Supervised feature selection in graphs with path coding penalties and network flows", "author": ["J. Mairal", "B. Yu"], "venue": "JMLR, 14(1):2449\u20132485", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Hotspot detection with bivariate data", "author": ["R. Modarres", "G. Patil"], "venue": "Journal of Statistical planning and inference, 137(11):3643\u20133654", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Cosamp: Iterative signal recovery from incomplete and inaccurate samples", "author": ["D. Needell", "J.A. Tropp"], "venue": "ACHA, 26(3):301\u2013321", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Scan statistics for the online detection of locally anomalous", "author": ["H J. Neil"], "venue": "subgraphs. Technometrics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "An empirical comparison of spatial scan statistics for outbreak detection", "author": ["D.B. Neill"], "venue": "IJHG, 8(1):1", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast subset scan for spatial pattern detection", "author": ["D.B. Neill"], "venue": "JRSS: Series B (Statistical Methodology), 74(2):337\u2013360", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Detection of patterns in water distribution pipe breakage using spatial scan statistics for point events in a physical network. JCCE", "author": ["D. Oliveira", "D. P"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "The battle of the water sensor networks (bwsn): A design challenge for engineers and algorithms. JWRPM", "author": ["A. Ostfeld", "J.G. e. a. Uber"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Connected sub-graph detection", "author": ["J. Qian", "V. Saligrama", "Y. Chen"], "venue": "AISTATS", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Monotone operators and the proximal point algorithm", "author": ["R.T. Rockafellar"], "venue": "SIAM journal on control and optimization, 14(5):877\u2013898", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1976}, {"title": "Event detection in activity networks", "author": ["P. Rozenshtein", "A. Anagnostopoulos", "A. Gionis", "N. Tatti"], "venue": "SIGKDD, pages 1176\u20131185", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Changepoint detection over graphs with the spectral scan statistic", "author": ["J. Sharpnack", "A. Rinaldo", "A. Singh"], "venue": "arXiv:1206.0773", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparsistency of the edge lasso over graphs", "author": ["J. Sharpnack", "A. Rinaldo", "A. Singh"], "venue": "AISTATS, pages 1028\u20131036", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Near-optimal anomaly detection in graphs using lov\u00e1sz extended scan statistic", "author": ["J.L. Sharpnack", "A. Krishnamurthy", "A. Singh"], "venue": "NIPS, pages 1959\u20131967", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic pattern detection with temporal consistency and connectivity constraints", "author": ["S. Speakman", "Y. Zhang", "D.B. Neill"], "venue": "ICDM, pages 697\u2013706. IEEE", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Greedy algorithms for structurally constrained high dimensional problems", "author": ["R A. Tewari"], "venue": "In NIPS,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Efficient generalized fused lasso and its application to the diagnosis of alzheimers disease", "author": ["K B. Xin"], "venue": "In AAAI,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Gradient hard thresholding pursuit for sparsity-constrained optimization", "author": ["X. Yuan", "P. Li", "T. Zhang"], "venue": "ICML, pages 127\u2013135", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Newton greedy pursuit: A quadratic approximation method for sparsity-constrained optimization", "author": ["X.-T. Yuan", "Q. Liu"], "venue": "CVPR, pages 4122\u20134129. IEEE", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive forward-backward greedy algorithm for sparse learning with linear models", "author": ["T. Zhang"], "venue": "NIPS, pages 1921\u20131928", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "In recent years, structured sparse methods have attracted much attention in many domains such as bioinformatics, medical imaging, social networks, and astronomy [2], [4], [14], [16], [17].", "startOffset": 166, "endOffset": 169}, {"referenceID": 11, "context": "In recent years, structured sparse methods have attracted much attention in many domains such as bioinformatics, medical imaging, social networks, and astronomy [2], [4], [14], [16], [17].", "startOffset": 171, "endOffset": 175}, {"referenceID": 13, "context": "In recent years, structured sparse methods have attracted much attention in many domains such as bioinformatics, medical imaging, social networks, and astronomy [2], [4], [14], [16], [17].", "startOffset": 177, "endOffset": 181}, {"referenceID": 14, "context": "In recent years, structured sparse methods have attracted much attention in many domains such as bioinformatics, medical imaging, social networks, and astronomy [2], [4], [14], [16], [17].", "startOffset": 183, "endOffset": 187}, {"referenceID": 11, "context": "A number of structured sparsity models have been well explored, such as the sparsity models defined through trees [14], groups [17], clusters [16], and paths [2].", "startOffset": 114, "endOffset": 118}, {"referenceID": 14, "context": "A number of structured sparsity models have been well explored, such as the sparsity models defined through trees [14], groups [17], clusters [16], and paths [2].", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "A number of structured sparsity models have been well explored, such as the sparsity models defined through trees [14], groups [17], clusters [16], and paths [2].", "startOffset": 142, "endOffset": 146}, {"referenceID": 2, "context": ", trees, groups, clusters, and paths) [4] that can be encoded as structured sparsity-inducing norms, and reformulate Problem (1) as a convex (or non-convex) optimization problem", "startOffset": 38, "endOffset": 41}, {"referenceID": 37, "context": "Most of the methods in this category assume that the projection problem P(b) can be solved exactly, including the forward-backward algorithm [42], the gradient descent algorithm [38], the gradient hard-thresholding algorithms [6], [18], [40], the projected iterative hard thresholding [5], [7], and the Newton greedy pursuit algorithm [41].", "startOffset": 141, "endOffset": 145}, {"referenceID": 33, "context": "Most of the methods in this category assume that the projection problem P(b) can be solved exactly, including the forward-backward algorithm [42], the gradient descent algorithm [38], the gradient hard-thresholding algorithms [6], [18], [40], the projected iterative hard thresholding [5], [7], and the Newton greedy pursuit algorithm [41].", "startOffset": 178, "endOffset": 182}, {"referenceID": 4, "context": "Most of the methods in this category assume that the projection problem P(b) can be solved exactly, including the forward-backward algorithm [42], the gradient descent algorithm [38], the gradient hard-thresholding algorithms [6], [18], [40], the projected iterative hard thresholding [5], [7], and the Newton greedy pursuit algorithm [41].", "startOffset": 226, "endOffset": 229}, {"referenceID": 15, "context": "Most of the methods in this category assume that the projection problem P(b) can be solved exactly, including the forward-backward algorithm [42], the gradient descent algorithm [38], the gradient hard-thresholding algorithms [6], [18], [40], the projected iterative hard thresholding [5], [7], and the Newton greedy pursuit algorithm [41].", "startOffset": 231, "endOffset": 235}, {"referenceID": 35, "context": "Most of the methods in this category assume that the projection problem P(b) can be solved exactly, including the forward-backward algorithm [42], the gradient descent algorithm [38], the gradient hard-thresholding algorithms [6], [18], [40], the projected iterative hard thresholding [5], [7], and the Newton greedy pursuit algorithm [41].", "startOffset": 237, "endOffset": 241}, {"referenceID": 3, "context": "Most of the methods in this category assume that the projection problem P(b) can be solved exactly, including the forward-backward algorithm [42], the gradient descent algorithm [38], the gradient hard-thresholding algorithms [6], [18], [40], the projected iterative hard thresholding [5], [7], and the Newton greedy pursuit algorithm [41].", "startOffset": 285, "endOffset": 288}, {"referenceID": 5, "context": "Most of the methods in this category assume that the projection problem P(b) can be solved exactly, including the forward-backward algorithm [42], the gradient descent algorithm [38], the gradient hard-thresholding algorithms [6], [18], [40], the projected iterative hard thresholding [5], [7], and the Newton greedy pursuit algorithm [41].", "startOffset": 290, "endOffset": 293}, {"referenceID": 36, "context": "Most of the methods in this category assume that the projection problem P(b) can be solved exactly, including the forward-backward algorithm [42], the gradient descent algorithm [38], the gradient hard-thresholding algorithms [6], [18], [40], the projected iterative hard thresholding [5], [7], and the Newton greedy pursuit algorithm [41].", "startOffset": 335, "endOffset": 339}, {"referenceID": 12, "context": "We note that there is one recent approach named as GRAPH-COSAMP that admits inexact projections by assuming \u201chead\u201d and \u201ctail\u201d oracles for the projections, but is only applicable to compressive sensing or linear regression problems [15].", "startOffset": 231, "endOffset": 235}, {"referenceID": 0, "context": ", detection of traffic bottlenecks in road networks or airway networks [1]; crime hot spots in geographic networks [22]; and pollutions in water distribution networks [27].", "startOffset": 71, "endOffset": 74}, {"referenceID": 19, "context": ", detection of traffic bottlenecks in road networks or airway networks [1]; crime hot spots in geographic networks [22]; and pollutions in water distribution networks [27].", "startOffset": 115, "endOffset": 119}, {"referenceID": 24, "context": ", detection of traffic bottlenecks in road networks or airway networks [1]; crime hot spots in geographic networks [22]; and pollutions in water distribution networks [27].", "startOffset": 167, "endOffset": 171}, {"referenceID": 12, "context": ", detection of objects in images [15].", "startOffset": 33, "endOffset": 37}, {"referenceID": 21, "context": ", detection of viruses or worms spreading from host to host in a computer network [24].", "startOffset": 82, "endOffset": 86}, {"referenceID": 18, "context": ", detection of significantly mutated subnetworks [21].", "startOffset": 49, "endOffset": 53}, {"referenceID": 6, "context": ", detection and forecasting of societal events [8], [9].", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": ", detection and forecasting of societal events [8], [9].", "startOffset": 52, "endOffset": 55}, {"referenceID": 34, "context": "The most relevant norm is fused lasso norm [39]: \u03a9(x) = \u2211 (i,j)\u2208E |xi\u2212xj |, where xi is the i-th entry in x.", "startOffset": 43, "endOffset": 47}, {"referenceID": 16, "context": "as this projection problem is NP-hard due to a reduction from classical Steiner tree problem [19].", "startOffset": 93, "endOffset": 97}, {"referenceID": 12, "context": "To the best of our knowledge, there is only one recent approach named as GRAPH-COSAMP that admits inexact projections for M(G, k) by assuming \u201chead\u201d and \u201ctail\u201d oracles for the projections, but is only applicable to compression sensing and linear regression problems [15].", "startOffset": 266, "endOffset": 270}, {"referenceID": 12, "context": "There are two nearly-linear time approximation algorithms [15] for P(x) that have the following properties:", "startOffset": 58, "endOffset": 62}, {"referenceID": 3, "context": "The proposed GRAPH-IHT algorithm generalizes the traditional algorithm named as projected gradient descent [5], [7] that requires a exact solver of the projection oracle P(x).", "startOffset": 107, "endOffset": 110}, {"referenceID": 5, "context": "The proposed GRAPH-IHT algorithm generalizes the traditional algorithm named as projected gradient descent [5], [7] that requires a exact solver of the projection oracle P(x).", "startOffset": 112, "endOffset": 115}, {"referenceID": 35, "context": "The proposed GRAPH-GHTP algorithm generalizes the traditional algorithm named as Gradient Hard Threshold Pursuit (GHTP) that is designed specifically for the k-sparsity model: M = {S \u2286 [n] | |S| \u2264 k} [40].", "startOffset": 200, "endOffset": 204}, {"referenceID": 35, "context": "The WRSC is weaker than the popular Restricted Strong Convexity/Smoothness (RSC/RSS) conditions that are used in theoretical analysis of convex optimization algorithms [40].", "startOffset": 168, "endOffset": 172}, {"referenceID": 35, "context": "The RSC/RSS conditions imply condition WRSC, which indicates that WRSC is no stronger than RSC/RSS [40].", "startOffset": 99, "endOffset": 103}, {"referenceID": 35, "context": "[40] Assume that f is a differentiable function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Our proposed GRAPH-IHT generalizes several existing sparsity-constrained optimization algorithms: 1) Projected Gradient Descent (PGD) [30].", "startOffset": 134, "endOffset": 138}, {"referenceID": 10, "context": "If we redefine H(b) = supp(b) and T(b) = supp(P(b)), where P(b) is the projection oracle defined in Equation (5), then GRAPH-IHT reduces to the PGD method; 2) Approximated Model-IHT(AMIHT) [13].", "startOffset": 189, "endOffset": 193}, {"referenceID": 10, "context": "The component \u2016\u2207f(x)\u20162 = \u2016Ae\u20162 is upper bound by bounded by \u221a 1 + \u03b4\u2016e\u20162 [13], Assume that \u03be = 1 and \u03b7 = 1.", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "Surprisingly, the above convergence inequality is identical to the convergence inequality of AM-IHT derived in [13] based on the RIP condition, which indicates that GRAPH-IHT has the same convergence rate and approximation error as AM-IHT, although we did not make any attempt to explore the special properties of the RIP condition.", "startOffset": 111, "endOffset": 115}, {"referenceID": 12, "context": "It is noted that the head and tail approximation algorithms described in [15] do not meet the inequality (15).", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "Nonetheless, the approximation factor cH of any given head approximation algorithm can be boosted to any arbitrary constant cH < 1, which leads to the satisfaction of the above condition as shown in [15].", "startOffset": 199, "endOffset": 203}, {"referenceID": 10, "context": "Boosting the head-approximation algorithm, though strongly suggested by [13], is not empirically necessary.", "startOffset": 72, "endOffset": 76}, {"referenceID": 35, "context": "Our proposed GRAPH-GHTP has strong connections to the recently proposed algorithm named as Gradient Hard Thresholding Pursuit (GHTP) [40] that is designed specifically for the k-sparsity model: M = {S \u2286 [n] | |S| \u2264 k}.", "startOffset": 133, "endOffset": 137}, {"referenceID": 35, "context": "which is the same as the shrinkage rate of GRAPH-GHTP as derived in [40] specifically for the k-sparsity model.", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "This constant condition of \u03b4 is analogous to the constant condition of stateof-the-art compressive sensing methods that consider noisy measurements [23] under the assumption of the RIP condition.", "startOffset": 148, "endOffset": 152}, {"referenceID": 26, "context": "In this section, we specialize GRAPH-IHT and GRAPHGHTP to optimize a number of well-known graph scan statistics for the task of connected subgraph detection, including elevated mean scan (EMS) statistic [29], Kulldorff\u2019s scan statistic [25], and expectation-based Poisson (EBP) scan statistic [26].", "startOffset": 203, "endOffset": 207}, {"referenceID": 22, "context": "In this section, we specialize GRAPH-IHT and GRAPHGHTP to optimize a number of well-known graph scan statistics for the task of connected subgraph detection, including elevated mean scan (EMS) statistic [29], Kulldorff\u2019s scan statistic [25], and expectation-based Poisson (EBP) scan statistic [26].", "startOffset": 236, "endOffset": 240}, {"referenceID": 23, "context": "In this section, we specialize GRAPH-IHT and GRAPHGHTP to optimize a number of well-known graph scan statistics for the task of connected subgraph detection, including elevated mean scan (EMS) statistic [29], Kulldorff\u2019s scan statistic [25], and expectation-based Poisson (EBP) scan statistic [26].", "startOffset": 293, "endOffset": 297}, {"referenceID": 1, "context": "To apply our proposed algorithms, we relax the input domain of x and maximize the strongly convex function [3]:", "startOffset": 107, "endOffset": 110}, {"referenceID": 35, "context": "According to Lemma 1 (b) in [40]), the objective function f(x) satisfies condition (\u03be, \u03b4,M(G, 8k))-WRSC that", "startOffset": 28, "endOffset": 32}, {"referenceID": 22, "context": "Kulldorff\u2019s Scan Statistic [25] cTx log c Tx bTx \u2212 1Tc log 1 Tc 1Tb + (1Tc\u2212 cTx) log 1 Tc\u2212cTx 1Tb\u2212bTx The statistic is used for anomalous pattern detection in graphs with count features, such as detection of traffic bottlenecks in sensor networks [1], [22], detection of anomalous regions in digitals and images [10], detection of attacks in computer networks [24], disease outbreak detection [36], and various others.", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "Kulldorff\u2019s Scan Statistic [25] cTx log c Tx bTx \u2212 1Tc log 1 Tc 1Tb + (1Tc\u2212 cTx) log 1 Tc\u2212cTx 1Tb\u2212bTx The statistic is used for anomalous pattern detection in graphs with count features, such as detection of traffic bottlenecks in sensor networks [1], [22], detection of anomalous regions in digitals and images [10], detection of attacks in computer networks [24], disease outbreak detection [36], and various others.", "startOffset": 247, "endOffset": 250}, {"referenceID": 19, "context": "Kulldorff\u2019s Scan Statistic [25] cTx log c Tx bTx \u2212 1Tc log 1 Tc 1Tb + (1Tc\u2212 cTx) log 1 Tc\u2212cTx 1Tb\u2212bTx The statistic is used for anomalous pattern detection in graphs with count features, such as detection of traffic bottlenecks in sensor networks [1], [22], detection of anomalous regions in digitals and images [10], detection of attacks in computer networks [24], disease outbreak detection [36], and various others.", "startOffset": 252, "endOffset": 256}, {"referenceID": 8, "context": "Kulldorff\u2019s Scan Statistic [25] cTx log c Tx bTx \u2212 1Tc log 1 Tc 1Tb + (1Tc\u2212 cTx) log 1 Tc\u2212cTx 1Tb\u2212bTx The statistic is used for anomalous pattern detection in graphs with count features, such as detection of traffic bottlenecks in sensor networks [1], [22], detection of anomalous regions in digitals and images [10], detection of attacks in computer networks [24], disease outbreak detection [36], and various others.", "startOffset": 312, "endOffset": 316}, {"referenceID": 21, "context": "Kulldorff\u2019s Scan Statistic [25] cTx log c Tx bTx \u2212 1Tc log 1 Tc 1Tb + (1Tc\u2212 cTx) log 1 Tc\u2212cTx 1Tb\u2212bTx The statistic is used for anomalous pattern detection in graphs with count features, such as detection of traffic bottlenecks in sensor networks [1], [22], detection of anomalous regions in digitals and images [10], detection of attacks in computer networks [24], disease outbreak detection [36], and various others.", "startOffset": 360, "endOffset": 364}, {"referenceID": 23, "context": "Expectation-based Poisson Statistic (EBP) [26] cTx log c Tx bTx + bTx\u2212 cTx This statistic is used for the same applications as above [1], [12], [25], [36], [37], but has different assumptions on data distribution [25].", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": "Expectation-based Poisson Statistic (EBP) [26] cTx log c Tx bTx + bTx\u2212 cTx This statistic is used for the same applications as above [1], [12], [25], [36], [37], but has different assumptions on data distribution [25].", "startOffset": 133, "endOffset": 136}, {"referenceID": 9, "context": "Expectation-based Poisson Statistic (EBP) [26] cTx log c Tx bTx + bTx\u2212 cTx This statistic is used for the same applications as above [1], [12], [25], [36], [37], but has different assumptions on data distribution [25].", "startOffset": 138, "endOffset": 142}, {"referenceID": 22, "context": "Expectation-based Poisson Statistic (EBP) [26] cTx log c Tx bTx + bTx\u2212 cTx This statistic is used for the same applications as above [1], [12], [25], [36], [37], but has different assumptions on data distribution [25].", "startOffset": 144, "endOffset": 148}, {"referenceID": 32, "context": "Expectation-based Poisson Statistic (EBP) [26] cTx log c Tx bTx + bTx\u2212 cTx This statistic is used for the same applications as above [1], [12], [25], [36], [37], but has different assumptions on data distribution [25].", "startOffset": 156, "endOffset": 160}, {"referenceID": 22, "context": "Expectation-based Poisson Statistic (EBP) [26] cTx log c Tx bTx + bTx\u2212 cTx This statistic is used for the same applications as above [1], [12], [25], [36], [37], but has different assumptions on data distribution [25].", "startOffset": 213, "endOffset": 217}, {"referenceID": 26, "context": "Elevated Mean Scan Statsitic (EMS) [29] cTx/1Tx This statistic is used for anomalous pattern detection in graphs with numerical features, such as event detection in social networks, network surveillance, disease outbreak detection, biomedical imaging [29], [35]", "startOffset": 35, "endOffset": 39}, {"referenceID": 26, "context": "Elevated Mean Scan Statsitic (EMS) [29] cTx/1Tx This statistic is used for anomalous pattern detection in graphs with numerical features, such as event detection in social networks, network surveillance, disease outbreak detection, biomedical imaging [29], [35]", "startOffset": 251, "endOffset": 255}, {"referenceID": 31, "context": "Elevated Mean Scan Statsitic (EMS) [29] cTx/1Tx This statistic is used for anomalous pattern detection in graphs with numerical features, such as event detection in social networks, network surveillance, disease outbreak detection, biomedical imaging [29], [35]", "startOffset": 257, "endOffset": 261}, {"referenceID": 25, "context": "A real-world water network is offered in the Battle of the Water Sensor Networks (BWSN) [28].", "startOffset": 88, "endOffset": 92}, {"referenceID": 17, "context": "We downloaded the high energy physics phenomenology citation data (CitHepPh) from Stanford Network Analysis Project (SNAP) [20].", "startOffset": 123, "endOffset": 127}, {"referenceID": 22, "context": "Graph Scan Statistics: As shown in Table I, three graph scan statistics were considered as the scoring functions of connected subgraphs, including Kulldorff\u2019s scan statistic [25], expectation-based Poisson (EBP) scan statistic [26], and elevated mean scan (EMS) statistic [29].", "startOffset": 174, "endOffset": 178}, {"referenceID": 23, "context": "Graph Scan Statistics: As shown in Table I, three graph scan statistics were considered as the scoring functions of connected subgraphs, including Kulldorff\u2019s scan statistic [25], expectation-based Poisson (EBP) scan statistic [26], and elevated mean scan (EMS) statistic [29].", "startOffset": 227, "endOffset": 231}, {"referenceID": 26, "context": "Graph Scan Statistics: As shown in Table I, three graph scan statistics were considered as the scoring functions of connected subgraphs, including Kulldorff\u2019s scan statistic [25], expectation-based Poisson (EBP) scan statistic [26], and elevated mean scan (EMS) statistic [29].", "startOffset": 272, "endOffset": 276}, {"referenceID": 29, "context": "Comparison Methods: We compared our proposed methods with four state-of-the-art baseline methods that are designed specifically for connected subgraph detection, namely, GraphLaplacian [33], EventTree [32], DepthFirstGraphScan [36] and NPHGS [8].", "startOffset": 185, "endOffset": 189}, {"referenceID": 28, "context": "Comparison Methods: We compared our proposed methods with four state-of-the-art baseline methods that are designed specifically for connected subgraph detection, namely, GraphLaplacian [33], EventTree [32], DepthFirstGraphScan [36] and NPHGS [8].", "startOffset": 201, "endOffset": 205}, {"referenceID": 6, "context": "Comparison Methods: We compared our proposed methods with four state-of-the-art baseline methods that are designed specifically for connected subgraph detection, namely, GraphLaplacian [33], EventTree [32], DepthFirstGraphScan [36] and NPHGS [8].", "startOffset": 242, "endOffset": 245}, {"referenceID": 16, "context": "EventTree reformulates the connected subgraph detection problem as a prize-collecting steiner tree (PCST) problem [19] and apply the Goemans-Williamson (G-W) algorithm for PCST [19] to detect anomalous subgraphs.", "startOffset": 114, "endOffset": 118}, {"referenceID": 16, "context": "EventTree reformulates the connected subgraph detection problem as a prize-collecting steiner tree (PCST) problem [19] and apply the Goemans-Williamson (G-W) algorithm for PCST [19] to detect anomalous subgraphs.", "startOffset": 177, "endOffset": 181}, {"referenceID": 26, "context": "We applied the heuristic rounding step proposed in [29] to x to generate connected subgraphs.", "startOffset": 51, "endOffset": 55}, {"referenceID": 26, "context": "Note that, a heuristic rounding process as proposed in [29] is applied to continuous vector x estimated by GenFusedLasso at each iteration i in order to identify the best connected subgraph at the current iteration.", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "The authors present GRAPH-COSAMP, a variant of COSAMP [23], for compressive sensing and linear regression problems based on head and tail approximations of M(G, k).", "startOffset": 54, "endOffset": 58}, {"referenceID": 28, "context": "The most recent methods in this category include EventTree [32], NPHGS [8], AdditiveScan [37],", "startOffset": 59, "endOffset": 63}, {"referenceID": 6, "context": "The most recent methods in this category include EventTree [32], NPHGS [8], AdditiveScan [37],", "startOffset": 71, "endOffset": 74}, {"referenceID": 32, "context": "The most recent methods in this category include EventTree [32], NPHGS [8], AdditiveScan [37],", "startOffset": 89, "endOffset": 93}, {"referenceID": 29, "context": "GraphLaplacian [33], and EdgeLasso [34]; 3) Approximation algorithms that provide performance bounds.", "startOffset": 15, "endOffset": 19}, {"referenceID": 30, "context": "GraphLaplacian [33], and EdgeLasso [34]; 3) Approximation algorithms that provide performance bounds.", "startOffset": 35, "endOffset": 39}, {"referenceID": 15, "context": "The authors reformulate the connectivity constraint as linear matrix inequalities (LMI) and present a semi-definite programming algorithm based on convex relaxation of the LMI [18, 19] with a performance bound.", "startOffset": 176, "endOffset": 184}, {"referenceID": 16, "context": "The authors reformulate the connectivity constraint as linear matrix inequalities (LMI) and present a semi-definite programming algorithm based on convex relaxation of the LMI [18, 19] with a performance bound.", "startOffset": 176, "endOffset": 184}], "year": 2016, "abstractText": "Structured sparse optimization is an important and challenging problem for analyzing high-dimensional data in a variety of applications such as bioinformatics, medical imaging, social networks, and astronomy. Although a number of structured sparsity models have been explored, such as trees, groups, clusters, and paths, connected subgraphs have been rarely explored in the current literature. One of the main technical challenges is that there is no structured sparsity-inducing norm that can directly model the space of connected subgraphs, and there is no exact implementation of a projection oracle for connected subgraphs due to its NP-hardness. In this paper, we explore efficient approximate projection oracles for connected subgraphs, and propose two new efficient algorithms, namely, GRAPH-IHT and GRAPH-GHTP, to optimize a generic nonlinear objective function subject to connectivity constraint on the support of the variables. Our proposed algorithms enjoy strong guarantees analogous to several current methods for sparsity-constrained optimization, such as Projected Gradient Descent (PGD), Approximate Model Iterative Hard Thresholding (AM-IHT), and Gradient Hard Thresholding Pursuit (GHTP) with respect to convergence rate and approximation accuracy. We apply our proposed algorithms to optimize several well-known graph scan statistics in several applications of connected subgraph detection as a case study, and the experimental results demonstrate that our proposed algorithms outperform state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}