{"id": "1610.00054", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2016", "title": "Outlier Detection from Network Data with Subnetwork Interpretation", "abstract": "Detecting a small number of outliers from a set of data observations is always challenging. This problem is more difficult in the setting of multiple network samples, where computing the anomalous degree of a network sample is generally not sufficient. In fact, explaining why the network is exceptional, expressed in the form of subnetwork, is also equally important. In this paper, we develop a novel algorithm to address these two key problems. We treat each network sample as a potential outlier and identify subnetworks that mostly discriminate it from nearby regular samples. The algorithm is developed in the framework of network regression combined with the constraints on both network topology and L1-norm shrinkage to perform subnetwork discovery. Our method thus goes beyond subspace/subgraph discovery and we show that it converges to a global optimum. Evaluation on various real-world network datasets demonstrates that our algorithm not only outperforms baselines in both network and high dimensional setting, but also discovers highly relevant and interpretable local subnetworks, further enhancing our understanding of anomalous networks.\n\n\nThe approach presented in this paper is consistent with the principle of model-based model analysis (PPAR) that allows a model to examine the observed observed network network performance. In this approach, models can include a set of features of an individual network, including individual network features and the probability distribution of the observed network performance (PAN). This is known as the \u2021prevalence\u201d model, which provides a prediction and analysis of a network performance, and a predictive model for predictive model performance.\nModel-based model inference is a powerful technique for inference and inference when possible. The most recent paper, \u2021prevalence\u201d , shows that a given network performance has an average PAN in comparison with the previous dataset. This model is used by analyzing the median PAN, which was derived from several high-resolution visualizations from three studies published by the Stanford Computational Analysis Laboratory. This model is used by a different model (CAS), for model selection, but the results of the CAS are comparable to the observed data from different sources. In this paper, we examine the prediction and analysis of PAN, because there are multiple studies in this area, the probability distribution of the observed PAN is similar to that from the previous dataset.\nThe models we describe have shown a highly significant correlation between PAN performance and performance. This means that when we test our model, it appears that it is indeed not possible to predict the observed PAN performance", "histories": [["v1", "Fri, 30 Sep 2016 23:13:28 GMT  (2051kb,D)", "http://arxiv.org/abs/1610.00054v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["xuan-hong dang", "arlei silva", "ambuj singh", "ananthram swami", "prithwish basu"], "accepted": false, "id": "1610.00054"}, "pdf": {"name": "1610.00054.pdf", "metadata": {"source": "CRF", "title": "Outlier Detection from Network Data with Subnetwork Interpretation", "authors": ["Xuan-Hong Dang", "Arlei Silva", "Ambuj Singh", "Ananthram Swami", "Prithwish Basu"], "emails": ["xdang@cs.ucsb.edu", "arlei@cs.ucsb.edu", "ambuj@cs.ucsb.edu", "ananthram.swami.civ@mail.mil", "pbasu@bbn.com"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nDetecting and characterizing exceptional patterns is an important task in many domains ranging from fraud detection, environmental surveillance, to various health care applications [4,37]. This problem is often referred to as outlier or anomaly detection in the literature. In contrast to other popular data mining tasks like clustering, classification or frequent patterns mining that all discover prevalent patterns, outlier identification aims at uncovering a small set of inconsistent objects (outliers) that deviate significantly from the larger number of regular objects (inliers) in the data.\nAlthough identifying anomalous subjects has been widely studied in high dimensional data [37] and recently extended to the network context [4], the problem remains very challenging. One of the most challenging issues lies in the fact that the number of anomalous objects is considerably smaller than the large population of regular ones, which limits the learning capability of most data mining algorithms. Another challenge comes from the notion of \u201cinconsistency\u201d which is hard to precisely define, quantify and interpret, especially when entities are connected in a network. In the network setting, most existing works focus on searching individual nodes [19], or groups of linked nodes [15] whose structures or behaviors are irregular. Though these studies have provided intuitive concepts about outlying patterns defined in the respect of network connectivity, most results are\nlimited to the setting of a single static network. Other recent studies have extended the scope of analysis to evolving networks [7,17], but the focus is on event/change detection where the temporal dimension is a key factor for defining outliers.\nIn this paper, we address the problem of identifying anomalous networks from a database of multiple network samples while at the same time investigating why a network is exceptional. An outlier is defined at the global level of an entire network sample but we use local subnetworks to explain its exceptionality. Although the outlierness of a network sample can be quantified via the outlier degree, such a single measure only bears limited explanatory information [12,26] since it lacks the capability of showing in what data view, i.e. local subnetworks, an anomalous network is most exceptional. Moreover, although two networks may have similar outlier degrees, the local subnetworks that make them abnormal might be quite different since the anomalous networks themselves are usually not homogeneous. For example, exploring a database of gene networks for outliers can lead to the isolation of subjects suffering from cancer. However, the gene pathway (local subnetwork) that causes the disease can vary from subject to subject due to the complexity of the disease [23], or even depending on different stages of the disease. Spotting an unhealthy subject is generally not sufficient. Figuring out what abnormal gene subnetwork leads to the disease is usually more important since it helps to develop possible and effective treatments.\nWe develop a novel algorithm that exploits network regression models combined with network topology regularization to concurrently address the two important problems mentioned above. Specifically, we treat each network sample as a potential outlier and determine local subnetworks that help discriminate it from nearby regular network samples. Our objective function is formulated under the framework of network regression where we first upsample the outlier candidate network in order to make the binary regression problem balanced. The objective function is then regularized by the network topology and further penalized by L1norm shrinkage to perform subnetwork discovery. It can be shown that the combined objective function has a form closely related to the dual SVM [18,20], which can be further optimized in the primal form using Newton\u2019s method. The objective function is proven to be convex, which ar X\niv :1\n61 0.\n00 05\n4v 1\n[ cs\n.A I]\n3 0\nSe p\n20 16\nis key to guaranteeing the convergence of the algorithm. Our algorithm, therefore, goes beyond the simple strategy of subspaces/subgraphs examination by directly learning the most discriminative subnetworks with respect to each network sample. Consequently, the outlier degree can be appropriately computed within the space spanned by these selected subnetworks and, collectively, they form a ranking of all network samples based on the outlier scores.\nIn summary, we make the following contributions in this work: (i) We address a challenging problem of both identifying and explaining anomalous networks from a database of network samples. The explanations are expressed in the form of local subnetworks, which play a key role in understanding the abnormal properties behind the observed network data; (ii) We formulate the problem under the regression framework with network regularization for subnetwork discovery, and develop a novel algorithm to efficiently mine most relevant subnetworks to discriminate and explain network outliers from their nearby network inliers; (iii) We demonstrate the effectiveness of our algorithm against typical techniques developed for both dynamic network data and high dimensional data using various real world datasets. Experimental results show that our algorithm is not only competitive in producing outlier ranking quality but further outputs highly relevant and interpretable local subnetworks, leading to better understanding of why the outlier networks are exceptional."}, {"heading": "II. PROBLEM SETTING", "text": "Definition 1: A network sample is a triple Nk = (Vk, Ek,F), where Vk = {v1, v2, . . . , vn} is a set of nodes, Ek \u2286 Vk \u00d7 Vk is a set of undirected edges, and F is a function labeling each node with a real number.\nLet DB = {N1,N2, . . . ,Nm} be a network dataset that consists of m network samples. We focus on a family of networks whose topologies are relatively stable across different network instances. For example, human subjects usually have similar gene networks with the same number of genes. However, the expression level of each individual gene may differ from subject to subject. Likewise, various snapshots captured from a traffic network often have the same network topology while traffic conditions on each road segment may vary from snapshot to snapshot. In mining outlying networks from a database of network samples DB, we aim to compute an anomaly score for each network sample and at the same time, to uncover subnetworks that show the most exceptional properties of the network under examination. Collectively, an outlier ranking is generated for the entire dataset and those network samples having the highest anomaly scores will be brought up to the user for further investigation."}, {"heading": "III. REGRESSION ON NETWORKS", "text": "As mentioned in the previous section, our objective is not only to compute the outlier degree for each network sample but also to discover a small set of subnetworks as explanations for each outlier candidate network. We explore the regression model for our problem since it allows us to formulate outlier detection as a binary prediction. In this section, we formulate the regression problem solely based on the values associated with network\u2019s nodes while the network topology will be taken into account in the next section.\nWe view each network sample as a potential candidate outlier while comparing its properties against its K nearby networks (based on some network distance measures, e.g. cosine distance between node values [13]). Therefore, a network sample can be a local outlier rather than a global one [10,37], as both network distribution and the outliers themselves can be heterogeneous and one should not presume any canonical form for the distribution. Let us denote No as an outlier network candidate, and Nk as one of its K neighboring networks (we use the same index k as in Def.1 for simplification, but here k only ranges over the K nearest neighbors of No). We can capture the node-values of a network sample Nk by a vector xk in a high dimensional space Rn. Under the vector format, we aim to optimize the following regression function for each No:\nargmin w\nL(w)=(xTo w\u2212zo)2+ K\u2211\nk=1\n(xTk w\u2212zk)2 s.t. |w|1\u22641 (1)\nwhere xo is the vector of local node values for network No; and zo = \u22121 while zk = 1 if Nk is among the K neighboring networks of No; |w|1 is the L1-norm of vector w. The main role of |w|1 is to set many coefficients in w to zero if the corresponding nodes are less predictive. It is worth mentioning that in a conventional case, one can constrain |w|1 \u2264 c [18] for a non-negative constant c. However, it is easy to see that c is only a scalar and can be replaced by 1 by dividing both w and the predicted labels zo, zk\u2019s by c. For simplicity, we thus directly use the constraint |w|1 \u2264 1.\nIt is possible to see that our Eq.(1) resembles the form of Lasso regression [18]. However, there are two challenging issues in optimizing Eq.(1). First, our regression model is highly imbalanced since we have only a single outlier candidate but a large number of neighboring inliers. In dealing with this issue, we adopt a simple approach of upsampling the outlier candidate in order to ensure that the data become balanced [6]. Essentially, (K \u2212 1) new samples will be generated (for the outlier class) following the normal distribution with xo as the mean vector, and the covariance matrix as the one computed from the statistics of K neighboring networks. By doing so, we assure that variations at each node/dimension of the outlier class are not generated randomly but resemble the ones from the\ninlier class, and thus minimize the impact on the explanation quality of the outlier.\nThe second, more challenging, issue in optimizing Eq.(1) is that the function is not directly differentiable\u2014it is not smooth due to the appearance of L1-norm imposed on w. The solution is at best only suboptimal using methods like sub-gradient [30], in which each component of w is optimized individually and in sequence. Moreover, such a solution is less efficient given the large number of nodes in the networks. We thus handle the L1-norm in a more general setting [30] by representing w using two nonnegative variables w+ and w\u2212, that are respectively defined as w+ = max(0,w) and w\u2212 = \u2212min(0,w). Hence, it is easy to see that w = w+ \u2212 w\u2212. We denote the new variable w\u0303 = [w+;w\u2212] \u2208 R2n. Coefficients in w\u0303 are thus all non-negative. Now, in combination with the upsampling reasoning above, Eq.(1) can be reformulated in the matrix form as follows:\narg min w\u0303i\u22650\nL(w\u0303) = \u2225\u2225\u2225[X(o),\u2212X(o)]w\u0303 \u2212 z\u2225\u2225\u22252\n2 s.t. 2n\u2211 i=1 w\u0303i \u2264 1 (2)\nwhere X(o) is the matrix with the first K rows as the vectors xk\u2019s, and the last K rows as xo and its (K \u2212 1) sampling vectors. Correspondingly, the first K entries of vector z are +1, predicting xk\u2019s as inliers, while the last K entries are \u22121, predicting xo and its upsampling samples as outliers."}, {"heading": "IV. ROLE OF NETWORK TOPOLOGY", "text": "Our formulation in Eq.(2) gives us a regression form to predict No as an outlier candidate based on the local state values associated with network nodes. It, however, has not taken into account the network information, and may lack essential information in learning the most relevant subnetworks that make No exceptional. Therefore, we add the network structure information as a constraint in learning w\u2019s coefficients. Intuitively, if two nodes are connected in the network, their behaviors will mutually impact each other and, consequently, their coefficients reflected in w\u2019s entries should be similar. For example, if a congestion happened at a road segment (node), it is likely that the nearby road segments will also be impacted, causing low speed over a region of the network. Towards modeling this network influence, we first define a graph that generalizes the network topology of both No and its K neighboring networks as follows:\nDefinition 2: Let DBo = {No,Np, . . . ,Nq} be the set of networks that involves the outlier candidate network No and its K neighboring networks {Np, . . . ,Nq}. We define G(o) = (V, E,A)1 as a graph summarizing the network topology of DBo, where V is the union of Vo and Vp, . . . , Vq; E \u2286V \u00d7V and Ek \u2286 E \u2200Nk \u2208 DBo. Each\n1The superscript (o) is used for G(o) only but it should be understood that it also applies to V, E and A since we define G(o) for each outlier candidate network No.\nedge E(i, j)\u2208E is associated with a positive weight A(i, j) defined as the popularity of the corresponding edge in either No or in its neighboring networks Nk\u2019s, i.e., A(i, j) = max(Eo(i, j), (1/K) \u00d7 \u2211 k Ek(i, j)) with Ek(i, j) = 1 if vi connects vj in a network Nk \u2208 DBo. We will regularize w using G(o)\u2019s topology in order to favor subnetworks that are frequently seen in No and/or in its K neighboring networks, and not favor subnetworks that appear occasionally in Nk \u2019s and that absent in No (i.e., (1/K)\u00d7 \u2211 k Ek(i, j)) is small while Eo(i, j) = 0). Values for entries in matrix A are thus constrained between 0 and 1. Moreover, since all Nj \u2208 DBo are undirected networks, A\u2208Rn\u00d7n is a symmetric matrix, with n as the total number of nodes in V .\nIn searching for the subnetworks that explain the abnormal properties of network sample No, we impose a smoothness constraint on w\u2019s coefficients with respect to the graph topology captured by G(o). In combination with the L1norm imposed on w (ref. Eq(1)), they will together perform group/subgraph selection that predicts No as an outlier network.\nEssentially, let us define the degree of a vertex vi in the graph G(o) as deg(i) = \u2211 vi\u223cvj A(i, j), i.e. sum over all unordered pairs {vi, vj} for which vi and vj are linked in G(o). We assume that G(o) is connected (if not, each of its disconnected component will be considered separately) and thus the degree of every node is non-zero. Accordingly, the matrix L(o) is defined as follows:\nL (o) ij = L (o) ji  1\u2212A(i, j)/deg(i) if vi = vj \u2212A(i, j)/ \u221a deg(i)deg(j) if vi connects vj\n0 otherwise (3)\nIt is not hard to show that L(o) is positive semidefinite and it is the normalized Laplacian matrix of G(o). Thus, the network topology can be taken as the regularization constraint imposed on the w via minimizing the following quadratic form:\nwTL(o)w = \u2211 vi \u2211 vj ( wi\u221a deg(i) \u2212 wj\u221a deg(j) )2 A(i, j) \u2265 0 (4) It can be seen that if vi and vj are connected in G(o) with a large value A(i, j), the function will incur a large penalty wherever wi\u221a\ndeg(i) and wj\u221a deg(j) are different from\neach other. Thus, these coefficients should be similar/smooth in order to minimize this penalty. For example, if node vi is highly explanatory for the abnormal property of No, then there is a high possibility that vj is also related to the abnormality of No if both nodes are strongly connected (i.e., wi \u2248 wj 6= 0). Likewise, if vi is less explanatory for No, its non-selection (wi = 0) will make vj also less likely to be selected. However, in order to appropriately incorporate this network-constrained penalty into our objective function formulated in Eq.(2), we need the following lemma.\nLemma 1: Given the definition w\u0303 = [w+;w\u2212], the following equation is satisfied:\nwTL(o)w = w\u0303T [ L(o) \u2212L(o) \u2212L(o) L(o) ] w\u0303 (5)\nProof: The proof of this lemma is straightforward with the expansion over the quadratic forms in both sides of Eq.(5).\nLemma 1 ensures that the network constraint penalty can also be represented using the transformed variable w\u0303. Following this, we recast our objective function in Eq.(2):\narg min w\u0303i\u22650\nL(w\u0303) = \u2225\u2225\u2225[X(o),\u2212X(o)]w\u0303 \u2212 z\u2225\u2225\u22252\n2 (6)\n+ \u03bb1w\u0303 T [ L(o) \u2212L(o) \u2212L(o) L(o) ] w\u0303 s.t. 2n\u2211 i=1 w\u0303i \u2264 1\nNotice that if the inequality constraint in Eq.(6) is not equal to 1, i.e., |w\u0303|1 < 1, then the upper bound is inactive and in this case, coefficients in w\u0303 will be widely non-zero. In other words, the majority of nodes in the graph will be selected. This solution is obviously undesirable. Therefore, in order to ensure that only subnetworks with the most explanatory information are used for No, this constraint should always be tight [9,36]. This means that we can safely use the equality constraint \u22112n i=1 w\u0303i = 1, or with 1 as the vector of all 1, we have 1T w\u0303 = 1. Upon this setting, the first term in Eq.(6) can be rewritten as:\n\u2225\u2225\u2225[X(o),\u2212X(o)]w\u0303 \u2212 z\u2225\u2225\u22252 2 = \u2225\u2225\u2225[X(o),\u2212X(o)]w\u0303\u2212z1T w\u0303\u2225\u2225\u22252 2 (7)\n= \u2225\u2225\u2225[X(o) \u2212 z1T ,\u2212(X(o) + z1T )]w\u0303\u2225\u2225\u22252\n2 = \u2016[X1,\u2212X2]w\u0303\u201622\nin which we use X1 and X2 to respectively denote (X(o)\u2212 z1T ) and (X(o)+z1T ). Consequently, we can combine two terms in Eq.(6) into a single quadratic form by using the following lemma.\nLemma 2: Let L(o) be decomposed into L(o) = STS and\nX\u0303 = ([ X1\u221a \u03bb1S ] [ \u2212X2 \u2212 \u221a \u03bb1S ]) . Then:\n\u2016[X1,\u2212X2]w\u0303\u201622 + \u03bb1w\u0303 T [ L(o) \u2212L(o) \u2212L(o) L(o) ] w\u0303 = w\u0303T X\u0303T X\u0303w\u0303 (8)\nProof: On one hand, the expansion of the first term gives us:\n\u2016[X1,\u2212X2]w\u0303\u201622 = w\u0303 T [ XT1 X1 \u2212XT1 X2 \u2212XT2 X1 XT2 X2 ] w\u0303 (9)\nOn the other hand, as L(o) is a normalized Laplacian matrix, it can be eigen-decomposed into L(o) = U\u03a3UT = STS where S = \u03a31/2UT where U and \u03a3 are respectively the matrices of eigenvectors and non-negative eigenvalues of\nL(o). Therefore: w\u0303T [ XT1 X1 \u2212XT1 X2 \u2212XT2 X1 XT2 X2 ] w\u0303 + \u03bb1w\u0303 T [ L(o) \u2212L(o) \u2212L(o) L(o) ] w\u0303\n=w\u0303T [ XT1 X1 + \u03bb1S\nTS \u2212XT1 X2 \u2212 \u03bb1STS \u2212XT2 X1 \u2212 \u03bb1STS XT2 X2 + \u03bb1STS\n] w\u0303\n=w\u0303T  [ X1\u221a \u03bb1S ]T [ \u2212X2 \u2212 \u221a \u03bb1S ]T \u00d7 [[ X1\u221a\u03bb1S ] [ \u2212X2 \u2212 \u221a \u03bb1S ]] w\u0303 =w\u0303T X\u0303T X\u0303w\u0303 (10)\nFrom the 1st row to the 2nd row, we have used the fact that both X1 and X2 have the same size of 2K \u00d7 n while L(o) has the size of n\u00d7n. So, the pairwise addition between the two matrices in the 2nd row is clearly matched.\nGiven Lemma 2 in combination with the previous results, we can rewrite Eq.(6) as follows:\narg min w\u0303i\u22650 L(w\u0303) = w\u0303T X\u0303T X\u0303w\u0303 + \u03bb2w\u0303T w\u0303 s.t. 2n\u2211 i=1 w\u0303i = 1 (11)\nwhere, like the classical ridge regression [18], we add a small amount of L2-norm regularization in order to improve the stability of solutions when n m."}, {"heading": "V. OPTIMIZATION", "text": "In solving the objective function in Eq.(11), it is possible to note that it is closely related to the dual form of the SVM with the squared loss function [20,34]:\narg min w\u0303i\u22650\nf(w\u0303) = w\u0303T X\u0303T X\u0303w\u0303 + 1\n2C \u2211 i=1 w\u03032i \u2212 1T w\u0303 (12)\nfor any general dataset {x\u0303i}|DS|i=1 of |DS| samples, where X\u0303 = [x\u03031, . . . , x\u0303|DS|] \u00d7 diag(y), in which diag(y) is the diagonal matrix whose entries are class labels (i.e., yi \u2208 {\u22121, 1}) for the corresponding samples x\u0303i\u2019s, and C is the margin parameter. 2\nIt is easy to see that our X\u0303 in Lemma 2 can also be represented in this format. Specifically,\nX\u0303 =\n([ X1\u221a \u03bb1S ] [ X2\u221a \u03bb1S ]) \u00d7 diag(y), where\ny = (1, . . . , 1,\u22121, . . . ,\u22121)T is the vector in which the first n entries are 1\u2019s and the last n entries are -1\u2019s. Our \u03bb2 in Eq.(11) has a similar role as 1/(2C) in Eq.(12). The only difference between the two objective functions is that our optimization (Eq.(11)) further requires the constraint\u22112n\ni=1 w\u0303i = 1 T w\u0303 = 1. However, it also can be seen that if such a constraint is applied to Eq.(12), then its last term becomes a constant. Indeed, this constraint simply rescales our optimal solution for w\u0303 to be of unit L1-length. The sparseness property of w\u0303 is obviously unchanged\n2Note that we use the same notation w\u0303 in both Eq.(12) and (11) for easy explanation. However, w\u0303 in Eq.(12) should be understood as the Lagrange multipliers (often denoted by \u03b1 in [20,34]). Likewise, X\u0303\u2019s in Eq.(12) and (11) are not necessarily the same.\nby such a normalization step. Similar to the dual-form SVM, we can solve Eq.(11) using several available techniques like coordinate descent [20], internal point [31] or active set method [29]. However, the computation often involves dealing with 2n inequality constraints directly. Therefore, a more practical approach is to consider such a quadratic programming problem in the primal form of an unconstrained problem [21,34] as follows:\nL\u0303(w\u0303)= 2n\u2211 i=1 2n\u2211 j=1 w\u0303iw\u0303j x\u0303 T i x\u0303j+\u03bb2 2n\u2211 i=1 max(0, 1\u2212yi \u2211 j w\u0303j x\u0303 T i x\u0303j) 2\n(13)\nwhere, with the introduction of vector y above, we have redefined X\u0303 \u2190\u2212 ([\nX1\u221a \u03bb1S ] [ X2\u221a \u03bb1S ]) with x\u0303i\u2019s as its\ncolumn vectors, and w\u0303\u2190\u2212 diag(y)\u00d7 w\u0303. In this representation, one can view the first quantity in Eq.(13) as the regularization term while the second one as the loss function. Since there is a flat part in this loss function (i.e., the 2nd term in Eq.(13) is 0 if 1 < yi \u2211 j w\u0303jx\u0303 T i x\u0303j), w\u0303 is usually sparse. Moreover, the function is continuously differentiable, which is a great advantage. Hence, in optimizing Eq.(13), we resort to Newton\u2019s method. Note that the function is doubly differentiable. In particular, let us denote Q = X\u0303T X\u0303 and vector Qi as the i-th column of matrix Q. The gradient of L\u0303 can be written as follows:\ng = \u2202L\u0303\n\u2202w\u0303 = 2Qw\u0303 \u2212 2\u03bb2 \u2211 i Qiyi(1\u2212 yiQTi w\u0303) (14)\nin which the summation in the second term is applied to x\u0303i\u2019s for which yi \u2211 j w\u0303jx\u0303 T i x\u0303j < 1. The Hessian is therefore:\nH = \u2202L\u0303\n\u2202w\u0303\u2202w\u0303T = 2Q+ 2\u03bb2 \u2211 i y2iQiQ T i (15)\nAt each iteration of Newton\u2019s method, we update w\u0303 to w\u0303 \u2212 \u03b7H\u22121g where \u03b7 is the learning rate found through the line search technique [9]. Given the convergence of w\u0303 (thus also w), the final subnetworks that are used as the explanations for the exceptionality of No can be identified via the non-zero entries of w. For the outlier score of No, denoted by OS(No), we follow a similar approach as [10] but computing it only in the subspace spanned by the explanatory subnetworks. The higher the value of OS(No), the more No deviates from its neighboring networks."}, {"heading": "VI. ANALYSIS AND DISCUSSION", "text": "Algorithm Complexity: We name our algorithm ODeSM that stands for Outlier Detection with Subgraph Mining. Its complexity is briefly analyzed as follows. Searching for neighboring networks and upsampling takes O(nm2) given m as the number of network samples and n as the number of nodes. The computation of S and inversion of H both depend on the number of non-zero entries in w\u0303 that is significantly reduced after each iteration. Let d denote that number, then computing S takes O(d2 log d) due to\nthe eigen-decomposition, while the inversion of H takes similar time. The checking step (1\u2212yi \u2211 j w\u0303jx\u0303 T i x\u0303j)\n2 > 0 in Eq.(13)\u2019s 2nd term takes O(nd). Due to its reliance on the Newton\u2019s method, ODeSM requires only a few iterations (usually \u2264 10) to reach its converged solution. As this whole process is applied to each network sample, the overall complexity is therefore O(nm2 +m\u00d7 (d2 log d+ nd)).\nConvergence: It is straightforward to show that our Hessian matrix derived in Eq.(15) is positive semi-definite. For any given non-negative vector \u03b1, we have \u03b1TH\u03b1 \u2265 0. This is given by the fact that Q = X\u0303T X\u0303 , as the first term in Eq.(15), is a symmetric matrix. So its quadratic form \u03b1TQ\u03b1 is always non-negative. Likewise, for the second term, each of its component\u2019s quadratic form \u03b1TQiQiT\u03b1 \u2265 0, while \u03bb2 is a non-negative parameter and y2i can be omitted as it always equals 1 by definition. These characteristics are of key importance since they collectively ensure the convexity of the objective function in Eq.(13), making our optimization procedure always converge to the global optimal solution. Also, note that our solution lies in the general family of quadratic programming solutions, often used in both dual and primal SVM. However, unlike SVM that works in the original data space, our algorithm works in the feature space. Nodes (or features) in the final subnetworks thus can be loosely interpreted as the (support) vectors falling inside the discriminative margin. Hence, one can control the subnetwork sizes through adjusting \u03bb2.\nParameter setting: Other than \u03bb2, our algorithm requires two parameters to be set: K determining neighboring networks, and \u03bb1 measuring the impact of network constraint. Without any prior knowledge regarding the network distribution, it is hard to choose the right values for both parameters since outlier detection is an unsupervised learning problem. We therefore employ the best-effort-approach that follows the strategy developed in [10,37]. The essential idea is to try on a parameter range, rather than a single value, and use an object-wise maximum ensemble to combine the final outlier score. We set K = {10 . . . 30}, similar to the range chosen in [10], and \u03bb1 = {0.1 . . . 10}. Regarding K, it is also noticed that, among neighbors of an outlier candidate, there may exist other outliers with the likelihood that they possess similar anomalous properties. In dealing with this case, one can either exclude closest neighboring samples (with assumption that outliers are closest neighbors), or increase the K value. We have empirically tested both approaches and the results are quite similar. Indeed, since the number of outliers within a database is usually small, the probability of having one within the K neighbors of a sample is usually low. The quality of the outlier detection and explanation is thus not much compromised and still determined by the majority of regular neighbors."}, {"heading": "VII. EXPERIMENTS", "text": ""}, {"heading": "A. Methodology", "text": "We compare the performance of our algorithm ODesM against techniques in both network studies and high dimensional studies. Specifically, it is compared against the following techniques: (1) Netspot [7] without temporal constraint so allowing it to uncover network regions from each individual network; (2) HiCS [22] that seeks outliers through contrast subspaces for high dimensional data; (3) ABOD [25] which discovers outliers via variance of angles between vector triples; (4) ODesMw/o, a variant of our method that does not exploit network regularization. The parameter setting for ODesM and ODesMw/o follows the discussion in Section VI, while for Netspot, we set the number of failures h = 10 as suggested in [7]. For HiCS, we choose all settings as suggested by the authors [22] and adopt LOF as its core algorithm. ABOD is a parameter-free technique, so we use its exact version with a polynomial kernel of degree 2.\nIn evaluating algorithm performance, we use the well established Receiver Operating Characteristic (ROC) curve computed based on the outlier ranking returned by an algorithm against the ground truth labels of normal and outlying networks. A ROC curve provides a visualization over the relationship between the true positive rate (y-axis) and the false positive rate (x-axis). This curve can be numerically comparable via a single value, when desired, known as the area-under-curve (AUC)."}, {"heading": "B. CMUFace graph data", "text": "Since most network datasets (presented next) lack groundtruth subnetworks, we conduct an experiment on the CMUFace image data (http://archive.ics.uci.edu) since it allows us to evaluate the relevance of uncovered subnetworks via visualization. Though images do not originally involve explicit network structures, studying them as graphs has been extensively studied and deemed advantageous [32]. In particular, it enables the discovery of local image properties, especially in the studies of image denoising and image forensics where pixels can be missing or purposely tampered. Following [32], we first down-sample the number of pixels to 50% and construct a common network topology relying on the remaining pixels. Within each image, a pixel corresponds to a node and has edges connected to the 5 nearest pixels. The value associated with a node is the grey level of the corresponding pixel. In order to evaluate whether\nany method can deal with the heterogeneity in the network dataset, we select all networks with open-eye images from each person as inliers, and randomly select one with sunglass from any of 4 poses (straight/up/left/right) as an outlier (images from a random person is depicted in Fig.1). This results in 303 regular network samples and 20 anomalous ones, each containing 1, 920 nodes and 11, 172 edges. Subnetworks extracted from sun-glass areas are therefore the ground truths.\nOutlier identification: In Fig.2, we plot the ROC curve performance of all algorithms. As seen from this figure, both techniques HiCS and ABOD designed for high dimensional data perform moderately well on this dataset. ABOD explores the variance over angles between an outlier candidate and every pair of other two samples, so its approach explores global outliers deviated from a single distribution of inliers. For this dataset, however, we have multiple distributions. These local outliers are thus harder to be explored by solely relying on the variances of high dimensional vectors\u2019 angles. This might explain for the low success rate of ABOD. HiCS, on the other hand, while being designed to find outliers based on contrast subspaces, also does not perform well in this dataset. HiCS attempts to find most information subspace from bottom-up approach and it starts with those of 2-dimension (from a pool of ( 1920 2 ) = 1844160 possible subspaces). If such low dimensional subspaces are not well sampled, it becomes much harder to ensure the most contrast subspaces will be found in higher dimensional subspaces. This is because HiCS retains only 100 to 1000 subspaces in order to avoid the exponential complexity. Netspot performs better than these two techniques by relying on the pvalue defined at each node in order to explore significant anomalous regions. However, by converting to a p-value, Netspot also removes the contrast among node\u2019s values and thus is less successful in seeking the most potential seednodes. Over all techniques, ODesM\u2019s performance yields the best with its AUC achieving 0.84, as compared to 0.78 obtained by the second best ODesMw/o. This large gap in AUC clearly confirms the key role of network topology exploited by ODesM, which not only helps it to narrow down the search space of all subgraphs, but also converges to the\nmost explanatory subnetwork structures. Explanatory subnetworks: We further explore the set of subnetworks discovered by ODesM as the explanation for top ranking outliers. Out of top 20 anomalous networks, 8 are true outliers. We plot in Fig.3(a-c) the three top ranked networks that are also truly labeled as outliers and their corresponding images from different poses. In each picture, the full image is shown in background (with dimmed color to boost visualization). The entire network topology is plotted in grey while we color the explanatory subnetworks discovered in blue. As observed, despite coming from different poses, the outlier networks are still well-identified and the subnetworks located around the sunglasses are appropriately selected by ODesM. By visualization, these discriminative substructures clearly explain why an anomalous network is exceptional from regular ones, though they can vary across different outlier networks. We plot in Fig.3(d) a network sample that is also ranked high by ODesM but not a true outlier according to the sunglasses\u2019 labeling. However, by inspecting its discovered substructures, they still reflect some exceptional property of this image, where all subnetworks have been selected at the curve of the face. Generally, such kind of substructures are quite typical for each individual person.\nRecall that ABOD, ODesMw/o and HiCS are not network-based techniques. While ABOD identifies outliers based on variance of vector angles, ODesMw/o selects individual nodes and does not explore subneworks. HiCS generates multiple subspaces for a single outlier candidate and there is no obvious way to derive subnetworks from all of them. Hence, we select Netspot for comparison based on its discovered anomalous subnetwork regions. In Fig.3(e-f), we plot two typical true outliers found from 20 top networks ranked by Netspot based on the anomalous score of the selected subnetwork regions. It can be seen that, unlike the subnetworks discovered by our method, it is hard to justify why the corresponding images are exceptional though they\nare strongly connected. In both figures, the substructures from entire faces have been selected. This performance probably comes from the fact that, other than p-value, Netspot also relies on the adjacency of network samples to derive the time interval at which significant anomalous regions can appear. However, once the interval is set to 1 (i.e. for each individual network), it has limited information to justify the relevance of a network region since there is no temporal development among network samples. Thus, the p-value computed at each node is likely playing the key role. And as long as its values do not change abruptly, Netspot tends to select all of them, forming a large subnetwork structures as shown in Fig.3(e-f). The patterns discovered between Netspot and our ODesM are thus fundamentally different. For this reason, we do not attempt to compare their uncovered subnetworks in the subsequent experiments."}, {"heading": "C. Biological PPI network", "text": "The second dataset we use for evaluation is the Liver metastasis in human [23] with the gene network derived from the protein-protein interaction. Values associated with nodes are the gene expression values. The dataset contains 7, 383 genes and 251, 916 edges collected from 101 healthy subjects viewed as inlying network samples, and 15 diseased subjects labeled as outliers.\nOutlier identification: We show in Fig.4 the ROC curve of all algorithms on the Liver dataset. The performance of our ODesM method is competitive to that of HiCS and both are better than the remaining techniques. Netspot also performs well on this dataset as indicated by its 0.76 AUC value and slightly better than ODesMw/o. Recall that each network sample of this dataset also contains a large number of nodes. However, unlike the CMUFace graph data where we have multiple data distributions (each representing images from an individual person), here we have only a single network distribution of healthy subjects. The outlier prediction rates of all techniques are thus not as diverse as those we have seen in the CMUFace graph dataset. However, the results still indicate that our ODesM algorithm yields the highest outlier prediction rate.\nExplanatory subnetworks: There are no obvious ground truths for the gene pathways (subetworks) associated with the liver cancer. However, as an attempt to investigate how relevant and explanatory are the subnetworks discovered by ODesM, we compute the most frequent subnetworks found in the top 15 ranked outlying networks. In Fig.5, we plot 3 subgraphs that have the highest frequency. The first subnetwork is found in 6 networks and out of these, 4 are anomalous networks. The second subnetwork is found in 4 networks with 3 as true outliers. Among these, two discovered subnetworks, the genes REG1A, MMP1, MMP2 and TIMP1 (shaded in the Fig.5) are particularly interesting since they are in agreement with the ones found in [23] and have been reported to be involved in liver metastasis. The last subnetwork is found in 5 network samples and among them, only one is a true outlier. Though the genes forming the above subnetworks are not all related to the liver cancer and not all diseased subjects are ranked at the top (7 true outliers are found out of top 15), an important observation from these results is that, the frequent involvement of diseased genes in the discovered subnetworks can signal the appearance of the disease. Moreover, since diseased subjects can suffer from different stages or subtypes of the cancer, the disease-related gene pathways can possibly vary from one subject to another. These uncovered subnetworks thus do carry explanatory information to justify why an unhealthy subject is an outlier."}, {"heading": "D. Road traffic networks", "text": "The last dataset we use for evaluation is LATraffic\u2014 the highway traffic network data of Los Angeles, California (http://pems.dot.ca.gov) during April 2011. LATraffic contains multiple network snapshots of size of 100/128 nodes/edges. Each node in the network corresponds to a road segment and its associated value is the average vehicle speed within 5-minute resolution. In generating outlier labels for the network samples, we rely on the distribution of the average speed computed for each snapshot. Specifically, 300 snapshots are randomly selected around the mean of this distribution and labeled as regular networks. Other 30 snapshots are randomly selected from two extreme tails (15 each) of this distribution and labeled as anomalous networks.\nOutlier identification: The ROC curve performance of all algorithms on the LATraffic is shown in Fig.6. For this relatively small network, HiCS handles the subspace candidates\nwell and its Monte-Carlo sampling based approach tends to select high contrast subspaces. Regarding the performance of Netspot, recall that the dataset contains two types of outliers, one with high average speed and the other with low speed. By relying on the notion of network fraction in computing the p-value for each node, Netspot may not be able to find both types of outliers. Among all examined techniques, ODesM is still the best performer with its AUC score at 0.9. Deeper investigation on its outlier ranking further shows that ODesM predicts 16 out of top 20 networks as true outliers and they are from both low and high average speeds.\nExplanatory subnetworks: We further explore the set of subnetworks discovered by ODesM for its top ranking network snapshots. In Fig.7, we plot the uncovered subnetworks for top four outlier networks. The networks in (a) and (d) are the true outliers with low speed while the ones in (b) and (c) are the true outliers with high speed. The sets of discovered subnetworks in both cases are quite consistent. Taking a closer look of these explanatory substructures, there is an interesting point to highlight. Apparently, we would expect the explanatory subnetworks for two types of outliers to be different since one was chosen from the low speed distribution while the other one was selected from the high speed distribution. However, it turns out that they share one large subnetwork spanned by the nodes 11,6,9,12 and 25. The common selection of this subnetwork in both kinds of outliers may suggest that such a set of adjacent road segments is highly sensitive to the traffic congestion. For monitoring purposes, these road segments should be the top candidate to be selected since they are likely to reflect the overall condition of the entire traffic network.\nE. Impact of parameters\nODesM requires three parameters to be set: K determining the number of network neighbors, \u03bb1 deciding the influence of network topology, while \u03bb2 controlling the discovered subnewtork size. As discussed in Section VI, we select a range of values for K and \u03bb1 and apply the best-effort-approach [10,37] to compute outlierness for each network sample. In this experiment, we thus only report the impact of varying \u03bb2 on the performance of outlier detection. Since our three datasets are vastly different in network size, we will use specific values to limit the size of selected\nA general trend can be observed from Fig.8. As the total number of nodes for subnetworks becomes larger, the outlier detection rate tends to increase. However, for Liver and CMUFace datasets, when the discovered subnetworks are larger than 70 nodes, the outlier detection rates get reduced. This might happen since choosing larger values for the subnetworks may further include irrelevant substructures, which leads to a higher rate of false positive prediction."}, {"heading": "VIII. RELATED WORK", "text": "Outlier detection from network data can generally be divided into two categories: those addressing plain networks [3,14] and those focusing on attributed networks [16, 28]. In the first category, only information about the network topology is available and most studies adopt structurebased [3,14] and community-based methods [33] to spot nodes or small groups of nodes that have abnormal connectivity patterns. In the second category, attributes associated with nodes and edges are also available. Discovering outlying patterns therefore seeks not only abnormal connectivity structure but also coherence of network attributes [16,28]. Network properties like normality [27], conductance [5] and Oddball [3] are often employed to quantify the internal consistency and external separability (collectively anomalous degree) of a set of nodes (local communities). Most of these studies focus on searching outlying patterns from a single network, which contrasts with our work that addresses the problem in a more general setting of multiple networks. Several recent studies [1,7,11,17] developed for dynamic networks are closer to ours. In [11], the authors present 6 types of community-based outliers including shrink, grow,\nmerge, split, born and vanish. Such types of anomalous communities can be identified via tracking the evolution of communities over time. In [1], the temporal distribution of the number of messages exchanged in a social network (like Twitter) is used as means to detect abnormal events. More specifically, if the fraction of edges added to a community with the current time window is significantly larger than the previous one, then it can signal that a special event is within that community. Authors in [8] introduce a novel problem of mining a heaviest dynamic subgraph (HDS) in a time evolving network. The problem is shown NP-hard, and a heuristic algorithm named MEDEN is developed based on the filter-and-verify framework. This study is recently extended in [7] to the NetSpot technique that enables the mining of multiple HDSs. NetSpot approximates HDSs via a local search approach and it alleviates the local optimal solutions via exploring a large range of neighborhood search [7]. Other studies [2] monitor global network parameters/probabilities to detect events/changes while those developed in [17] attempt to spot anomalous nodes and edges. They are thus less relevant to our studies. In contrast, we do not focus on searching for outlying patterns from a single dynamic evolving network but from multiple network samples. Moreover, our focus is on discovering outliers as entire network samples but localizing subnetworks to explain why such network samples are exceptional.\nOutlier detection in high dimensional spaces [37] can also be conceptually related to our studies. Two popular approaches to deal with this problem are from subspace sampling [22] and subspace projection [24]. Techniques from subspace sampling generally assume that outliers only show up in low dimensional subspaces and such subspaces can be discovered via sampling combined with relevant statistical tests. In contrast, methods based on space transformation directly search for a single subspace, often a linear combination of all original features, that maintains certain properties, e.g. variance, of the data. Outliers can be found from this induced low dimensional subspace. Though these techniques are effective in ranking and finding anomalous objects, directly applying them to network data often lacks domain relevance since the nature of mutual interaction\namong network entities is completely ignored. Additionally, while a novel subspace is effective in computing outlier scores, it barely provides qualitative explanation for each individual outlier."}, {"heading": "IX. CONCLUSIONS", "text": "In this paper, we addressed an important problem of identifying and explaining outlier network samples. A novel algorithm was developed to identify subnetworks that discriminate outlier networks from their neighboring regular network samples. The algorithm was designed in the framework of network regression combined with the constraint on the network topology and the L1-norm shrinkage to perform subnetwork discovery. Our algorithm thus goes beyond both subspace learning and subgraph discovery methods by directly learning the most discriminative subnetworks to justify the exceptional properties of an anomalous network. Evaluation on various real-world network datasets demonstrated that our novel algorithm not only outperformed existing techniques, but also uncovered highly relevant and interpretable local subnetworks.\nAs future work, we would like to extend our research to handle databases with very large networks. Obviously, directly applying ODesM might not be highly scalable as analyzed in Section VI. To deal with very large networks, we could apply network compression [35] that allows us to summarize both network topology and signals on the nodes. This is equivalent to representing a large network at different scales/resolutions. The open research issues are therefore: (i) How can we trade-off between the size of compressed networks, in exchange for scalability, and the quality of outlier detection? (ii) How can we ensure that the most exceptional information (explaining for an outlier network) is not compromised by such a compression approach?"}], "references": [{"title": "Event detection in social streams", "author": ["C.C. Aggarwal", "K. Subbian"], "venue": "SDM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Outlier detection in graph streams", "author": ["C.C. Aggarwal", "Y. Zhao", "P.S. Yu"], "venue": "ICDE, pages 399\u2013409", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "oddball: Spotting anomalies in weighted graphs", "author": ["L. Akoglu", "M. McGlohon", "C. Faloutsos"], "venue": "PAKDD", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Graph based anomaly detection and description: a survey", "author": ["L. Akoglu", "H. Tong", "D. Koutra"], "venue": "DMKD", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Local graph partitioning using pagerank vectors", "author": ["R. Andersen", "F.R.K. Chung", "K.J. Lang"], "venue": "FOCS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "A study of the behavior of several methods for balancing machine learning training data", "author": ["G. Batista"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Netspot: Spotting significant anomalous regions on dynamic networks", "author": ["P. Bogdanov"], "venue": "In SDM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Mining heavy subgraphs in time-evolving networks", "author": ["P. Bogdanov", "M. Mongiov\u00ec", "A.K. Singh"], "venue": "ICDM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "LOF: identifying density-based local outliers", "author": ["M.M. Breunig"], "venue": "In SIGMOD,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Community-based anomaly detection in evolutionary networks", "author": ["Z. Chen", "W. Hendrix", "N.F. Samatova"], "venue": "JIIS", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Discriminative features for identifying and interpreting outliers", "author": ["X.H. Dang"], "venue": "In ICDE,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Discriminative subnetworks with regularized spectral learning for global-state network data", "author": ["X.H. Dang", "A.K. Singh", "P. Bogdanov", "H. You", "B. Hsu"], "venue": "ECML", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Intrusion as (anti)social communication: characterization and detection", "author": ["Q. Ding"], "venue": "In KDD,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Discovering structural anomalies in graph-based data", "author": ["W. Eberle", "L.B. Holder"], "venue": "ICDM workshop", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "On community outliers and their efficient detection in information networks", "author": ["J. Gao"], "venue": "In KDD,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Integrating community matching and outlier detection for mining evolutionary community outliers", "author": ["M. Gupta", "J. Gao", "Y. Sun", "J. Han"], "venue": "KDD", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "The Elements of Statistical Learning. Data Mining, Inference, and Prediction", "author": ["T. Hastie"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "It\u2019s who you know: graph mining using recursive structural features", "author": ["K. Henderson"], "venue": "In KDD,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "A dual coordinate descent method for largescale linear SVM", "author": ["C. Hsieh"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Building support vector machines with reduced classifier complexity", "author": ["S. Keerthi", "O. Chapelle", "D. DeCoste"], "venue": "JMLR", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Hics: High contrast subspaces for density-based outlier ranking", "author": ["F. Keller", "E. M\u00fcller", "K. B\u00f6hm"], "venue": "ICDE", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Whole genome analysis for liver metastasis gene signatures in colorectal cancer", "author": ["D.H. Ki"], "venue": "Int J Cancer,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Outlier detection in arbitrarily oriented subspaces", "author": ["H. Kriegel", "P. Kr\u00f6ger", "E. Schubert", "A. Zimek"], "venue": "ICDM", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Angle-based outlier detection in high-dimensional data", "author": ["H. Kriegel", "M. Schubert", "A. Zimek"], "venue": "SIGKDD", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Explaining outliers by subspace separability", "author": ["B. Micenkov\u00e1", "R.T. Ng", "X.H. Dang", "I. Assent"], "venue": "ICDM", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Scalable anomaly ranking of attributed neighborhoods", "author": ["B. Perozzi", "L. Akoglu"], "venue": "SDM", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Focused clustering and outlier detection in large attributed graphs", "author": ["B. Perozzi"], "venue": "In KDD,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "An efficient implementation of an active set method for svms", "author": ["K. Scheinberg"], "venue": "JMRL", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "Fast optimization methods for L1 regularization: A comparative study and two new approaches", "author": ["M.W. Schmidt", "G. Fung", "R. Rosales"], "venue": "ECML", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning with Kernels: Support Vector Machines", "author": ["B. Scholkopf", "A.J. Smola"], "venue": "Regularization, Optimization, and Beyond. MIT Press, Cambridge, MA, USA", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "The emerging field of signal processing on graphs", "author": ["D.I. Shuman"], "venue": "IEEE Signal Process. Mag.,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Neighborhood formation and anomaly detection in bipartite graphs", "author": ["J. Sun"], "venue": "In ICDM,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2005}, {"title": "A review of optimization methodologies in support vector machines", "author": ["J. Taylor", "S. Sun"], "venue": "Neurocomput.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient aggregation for graph summarization", "author": ["Y. Tian"], "venue": "In SIGMOD,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "A reduction of the elastic net to SVM with an application to GPU computing", "author": ["Q. Zhou"], "venue": "In AAAI,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "A survey on unsupervised outlier detection in high-dimensional numerical data", "author": ["A. Zimek"], "venue": "Statistical Analysis and Data Mining,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "Detecting and characterizing exceptional patterns is an important task in many domains ranging from fraud detection, environmental surveillance, to various health care applications [4,37].", "startOffset": 181, "endOffset": 187}, {"referenceID": 36, "context": "Detecting and characterizing exceptional patterns is an important task in many domains ranging from fraud detection, environmental surveillance, to various health care applications [4,37].", "startOffset": 181, "endOffset": 187}, {"referenceID": 36, "context": "Although identifying anomalous subjects has been widely studied in high dimensional data [37] and recently extended to the network context [4], the problem remains very challenging.", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "Although identifying anomalous subjects has been widely studied in high dimensional data [37] and recently extended to the network context [4], the problem remains very challenging.", "startOffset": 139, "endOffset": 142}, {"referenceID": 18, "context": "In the network setting, most existing works focus on searching individual nodes [19], or groups of linked nodes [15] whose structures or behaviors are irregular.", "startOffset": 80, "endOffset": 84}, {"referenceID": 14, "context": "In the network setting, most existing works focus on searching individual nodes [19], or groups of linked nodes [15] whose structures or behaviors are irregular.", "startOffset": 112, "endOffset": 116}, {"referenceID": 6, "context": "Other recent studies have extended the scope of analysis to evolving networks [7,17], but the focus is on event/change detection where the temporal dimension is a key factor for defining outliers.", "startOffset": 78, "endOffset": 84}, {"referenceID": 16, "context": "Other recent studies have extended the scope of analysis to evolving networks [7,17], but the focus is on event/change detection where the temporal dimension is a key factor for defining outliers.", "startOffset": 78, "endOffset": 84}, {"referenceID": 11, "context": "Although the outlierness of a network sample can be quantified via the outlier degree, such a single measure only bears limited explanatory information [12,26] since it lacks the capability of showing in what data view, i.", "startOffset": 152, "endOffset": 159}, {"referenceID": 25, "context": "Although the outlierness of a network sample can be quantified via the outlier degree, such a single measure only bears limited explanatory information [12,26] since it lacks the capability of showing in what data view, i.", "startOffset": 152, "endOffset": 159}, {"referenceID": 22, "context": "However, the gene pathway (local subnetwork) that causes the disease can vary from subject to subject due to the complexity of the disease [23], or even depending on different stages of the disease.", "startOffset": 139, "endOffset": 143}, {"referenceID": 17, "context": "It can be shown that the combined objective function has a form closely related to the dual SVM [18,20], which can be further optimized in the primal form using Newton\u2019s method.", "startOffset": 96, "endOffset": 103}, {"referenceID": 19, "context": "It can be shown that the combined objective function has a form closely related to the dual SVM [18,20], which can be further optimized in the primal form using Newton\u2019s method.", "startOffset": 96, "endOffset": 103}, {"referenceID": 12, "context": "cosine distance between node values [13]).", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": "Therefore, a network sample can be a local outlier rather than a global one [10,37], as both network distribution and the outliers themselves can be heterogeneous and one should not presume any canonical form for the distribution.", "startOffset": 76, "endOffset": 83}, {"referenceID": 36, "context": "Therefore, a network sample can be a local outlier rather than a global one [10,37], as both network distribution and the outliers themselves can be heterogeneous and one should not presume any canonical form for the distribution.", "startOffset": 76, "endOffset": 83}, {"referenceID": 17, "context": "It is worth mentioning that in a conventional case, one can constrain |w|1 \u2264 c [18] for a non-negative constant c.", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "(1) resembles the form of Lasso regression [18].", "startOffset": 43, "endOffset": 47}, {"referenceID": 5, "context": "In dealing with this issue, we adopt a simple approach of upsampling the outlier candidate in order to ensure that the data become balanced [6].", "startOffset": 140, "endOffset": 143}, {"referenceID": 29, "context": "The solution is at best only suboptimal using methods like sub-gradient [30], in which each component of w is optimized individually and in sequence.", "startOffset": 72, "endOffset": 76}, {"referenceID": 29, "context": "We thus handle the L1-norm in a more general setting [30] by representing w using two nonnegative variables w and w\u2212, that are respectively defined as w = max(0,w) and w\u2212 = \u2212min(0,w).", "startOffset": 53, "endOffset": 57}, {"referenceID": 8, "context": "Therefore, in order to ensure that only subnetworks with the most explanatory information are used for No, this constraint should always be tight [9,36].", "startOffset": 146, "endOffset": 152}, {"referenceID": 35, "context": "Therefore, in order to ensure that only subnetworks with the most explanatory information are used for No, this constraint should always be tight [9,36].", "startOffset": 146, "endOffset": 152}, {"referenceID": 17, "context": "where, like the classical ridge regression [18], we add a small amount of L2-norm regularization in order to improve the stability of solutions when n m.", "startOffset": 43, "endOffset": 47}, {"referenceID": 19, "context": "(11), it is possible to note that it is closely related to the dual form of the SVM with the squared loss function [20,34]:", "startOffset": 115, "endOffset": 122}, {"referenceID": 33, "context": "(11), it is possible to note that it is closely related to the dual form of the SVM with the squared loss function [20,34]:", "startOffset": 115, "endOffset": 122}, {"referenceID": 19, "context": "(12) should be understood as the Lagrange multipliers (often denoted by \u03b1 in [20,34]).", "startOffset": 77, "endOffset": 84}, {"referenceID": 33, "context": "(12) should be understood as the Lagrange multipliers (often denoted by \u03b1 in [20,34]).", "startOffset": 77, "endOffset": 84}, {"referenceID": 19, "context": "(11) using several available techniques like coordinate descent [20], internal point [31] or active set method [29].", "startOffset": 64, "endOffset": 68}, {"referenceID": 30, "context": "(11) using several available techniques like coordinate descent [20], internal point [31] or active set method [29].", "startOffset": 85, "endOffset": 89}, {"referenceID": 28, "context": "(11) using several available techniques like coordinate descent [20], internal point [31] or active set method [29].", "startOffset": 111, "endOffset": 115}, {"referenceID": 20, "context": "Therefore, a more practical approach is to consider such a quadratic programming problem in the primal form of an unconstrained problem [21,34] as follows:", "startOffset": 136, "endOffset": 143}, {"referenceID": 33, "context": "Therefore, a more practical approach is to consider such a quadratic programming problem in the primal form of an unconstrained problem [21,34] as follows:", "startOffset": 136, "endOffset": 143}, {"referenceID": 8, "context": "At each iteration of Newton\u2019s method, we update w\u0303 to w\u0303 \u2212 \u03b7H\u22121g where \u03b7 is the learning rate found through the line search technique [9].", "startOffset": 134, "endOffset": 137}, {"referenceID": 9, "context": "For the outlier score of No, denoted by OS(No), we follow a similar approach as [10] but computing it only in the subspace spanned by the explanatory subnetworks.", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "We therefore employ the best-effort-approach that follows the strategy developed in [10,37].", "startOffset": 84, "endOffset": 91}, {"referenceID": 36, "context": "We therefore employ the best-effort-approach that follows the strategy developed in [10,37].", "startOffset": 84, "endOffset": 91}, {"referenceID": 9, "context": "30}, similar to the range chosen in [10], and \u03bb1 = {0.", "startOffset": 36, "endOffset": 40}, {"referenceID": 6, "context": "Specifically, it is compared against the following techniques: (1) Netspot [7] without temporal constraint so allowing it to uncover network regions from each individual network; (2) HiCS [22] that seeks outliers through contrast subspaces for high dimensional data; (3) ABOD [25] which discovers outliers via variance of angles between vector triples; (4) ODesMw/o, a variant of our method that does not exploit network regularization.", "startOffset": 75, "endOffset": 78}, {"referenceID": 21, "context": "Specifically, it is compared against the following techniques: (1) Netspot [7] without temporal constraint so allowing it to uncover network regions from each individual network; (2) HiCS [22] that seeks outliers through contrast subspaces for high dimensional data; (3) ABOD [25] which discovers outliers via variance of angles between vector triples; (4) ODesMw/o, a variant of our method that does not exploit network regularization.", "startOffset": 188, "endOffset": 192}, {"referenceID": 24, "context": "Specifically, it is compared against the following techniques: (1) Netspot [7] without temporal constraint so allowing it to uncover network regions from each individual network; (2) HiCS [22] that seeks outliers through contrast subspaces for high dimensional data; (3) ABOD [25] which discovers outliers via variance of angles between vector triples; (4) ODesMw/o, a variant of our method that does not exploit network regularization.", "startOffset": 276, "endOffset": 280}, {"referenceID": 6, "context": "The parameter setting for ODesM and ODesMw/o follows the discussion in Section VI, while for Netspot, we set the number of failures h = 10 as suggested in [7].", "startOffset": 155, "endOffset": 158}, {"referenceID": 21, "context": "For HiCS, we choose all settings as suggested by the authors [22] and adopt LOF as its core algorithm.", "startOffset": 61, "endOffset": 65}, {"referenceID": 31, "context": "Though images do not originally involve explicit network structures, studying them as graphs has been extensively studied and deemed advantageous [32].", "startOffset": 146, "endOffset": 150}, {"referenceID": 31, "context": "Following [32], we first down-sample the number of pixels to 50% and construct a common network topology relying on the remaining pixels.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "The second dataset we use for evaluation is the Liver metastasis in human [23] with the gene network derived from the protein-protein interaction.", "startOffset": 74, "endOffset": 78}, {"referenceID": 22, "context": "5) are particularly interesting since they are in agreement with the ones found in [23] and have been reported to be involved in liver metastasis.", "startOffset": 83, "endOffset": 87}, {"referenceID": 9, "context": "As discussed in Section VI, we select a range of values for K and \u03bb1 and apply the best-effort-approach [10,37] to compute outlierness for each network sample.", "startOffset": 104, "endOffset": 111}, {"referenceID": 36, "context": "As discussed in Section VI, we select a range of values for K and \u03bb1 and apply the best-effort-approach [10,37] to compute outlierness for each network sample.", "startOffset": 104, "endOffset": 111}, {"referenceID": 2, "context": "Outlier detection from network data can generally be divided into two categories: those addressing plain networks [3,14] and those focusing on attributed networks [16, 28].", "startOffset": 114, "endOffset": 120}, {"referenceID": 13, "context": "Outlier detection from network data can generally be divided into two categories: those addressing plain networks [3,14] and those focusing on attributed networks [16, 28].", "startOffset": 114, "endOffset": 120}, {"referenceID": 15, "context": "Outlier detection from network data can generally be divided into two categories: those addressing plain networks [3,14] and those focusing on attributed networks [16, 28].", "startOffset": 163, "endOffset": 171}, {"referenceID": 27, "context": "Outlier detection from network data can generally be divided into two categories: those addressing plain networks [3,14] and those focusing on attributed networks [16, 28].", "startOffset": 163, "endOffset": 171}, {"referenceID": 2, "context": "In the first category, only information about the network topology is available and most studies adopt structurebased [3,14] and community-based methods [33] to spot nodes or small groups of nodes that have abnormal connectivity patterns.", "startOffset": 118, "endOffset": 124}, {"referenceID": 13, "context": "In the first category, only information about the network topology is available and most studies adopt structurebased [3,14] and community-based methods [33] to spot nodes or small groups of nodes that have abnormal connectivity patterns.", "startOffset": 118, "endOffset": 124}, {"referenceID": 32, "context": "In the first category, only information about the network topology is available and most studies adopt structurebased [3,14] and community-based methods [33] to spot nodes or small groups of nodes that have abnormal connectivity patterns.", "startOffset": 153, "endOffset": 157}, {"referenceID": 15, "context": "Discovering outlying patterns therefore seeks not only abnormal connectivity structure but also coherence of network attributes [16,28].", "startOffset": 128, "endOffset": 135}, {"referenceID": 27, "context": "Discovering outlying patterns therefore seeks not only abnormal connectivity structure but also coherence of network attributes [16,28].", "startOffset": 128, "endOffset": 135}, {"referenceID": 26, "context": "Network properties like normality [27], conductance [5] and Oddball [3] are often employed to quantify the internal consistency and external separability (collectively anomalous degree) of a set of nodes (local communities).", "startOffset": 34, "endOffset": 38}, {"referenceID": 4, "context": "Network properties like normality [27], conductance [5] and Oddball [3] are often employed to quantify the internal consistency and external separability (collectively anomalous degree) of a set of nodes (local communities).", "startOffset": 52, "endOffset": 55}, {"referenceID": 2, "context": "Network properties like normality [27], conductance [5] and Oddball [3] are often employed to quantify the internal consistency and external separability (collectively anomalous degree) of a set of nodes (local communities).", "startOffset": 68, "endOffset": 71}, {"referenceID": 0, "context": "Several recent studies [1,7,11,17] developed for dynamic networks are closer to ours.", "startOffset": 23, "endOffset": 34}, {"referenceID": 6, "context": "Several recent studies [1,7,11,17] developed for dynamic networks are closer to ours.", "startOffset": 23, "endOffset": 34}, {"referenceID": 10, "context": "Several recent studies [1,7,11,17] developed for dynamic networks are closer to ours.", "startOffset": 23, "endOffset": 34}, {"referenceID": 16, "context": "Several recent studies [1,7,11,17] developed for dynamic networks are closer to ours.", "startOffset": 23, "endOffset": 34}, {"referenceID": 10, "context": "In [11], the authors present 6 types of community-based outliers including shrink, grow, 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "In [1], the temporal distribution of the number of messages exchanged in a social network (like Twitter) is used as means to detect abnormal events.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "Authors in [8] introduce a novel problem of mining a heaviest dynamic subgraph (HDS) in a time evolving network.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "This study is recently extended in [7] to the NetSpot technique that enables the mining of multiple HDSs.", "startOffset": 35, "endOffset": 38}, {"referenceID": 6, "context": "NetSpot approximates HDSs via a local search approach and it alleviates the local optimal solutions via exploring a large range of neighborhood search [7].", "startOffset": 151, "endOffset": 154}, {"referenceID": 1, "context": "Other studies [2] monitor global network parameters/probabilities to detect events/changes while those developed in [17] attempt to spot anomalous nodes and edges.", "startOffset": 14, "endOffset": 17}, {"referenceID": 16, "context": "Other studies [2] monitor global network parameters/probabilities to detect events/changes while those developed in [17] attempt to spot anomalous nodes and edges.", "startOffset": 116, "endOffset": 120}, {"referenceID": 36, "context": "Outlier detection in high dimensional spaces [37] can also be conceptually related to our studies.", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "Two popular approaches to deal with this problem are from subspace sampling [22] and subspace projection [24].", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "Two popular approaches to deal with this problem are from subspace sampling [22] and subspace projection [24].", "startOffset": 105, "endOffset": 109}, {"referenceID": 34, "context": "To deal with very large networks, we could apply network compression [35] that allows us to summarize both network topology and signals on the nodes.", "startOffset": 69, "endOffset": 73}], "year": 2016, "abstractText": "Detecting a small number of outliers from a set of data observations is always challenging. This problem is more difficult in the setting of multiple network samples, where computing the anomalous degree of a network sample is generally not sufficient. In fact, explaining why the network is exceptional, expressed in the form of subnetwork, is also equally important. In this paper, we develop a novel algorithm to address these two key problems. We treat each network sample as a potential outlier and identify subnetworks that mostly discriminate it from nearby regular samples. The algorithm is developed in the framework of network regression combined with the constraints on both network topology and L1-norm shrinkage to perform subnetwork discovery. Our method thus goes beyond subspace /subgraph discovery and we show that it converges to a global optimum. Evaluation on various real-world network datasets demonstrates that our algorithm not only outperforms baselines in both network and high dimensional setting, but also discovers highly relevant and interpretable local subnetworks, further enhancing our understanding of anomalous networks.", "creator": "LaTeX with hyperref package"}}}