{"id": "1610.00479", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2016", "title": "Nonsymbolic Text Representation", "abstract": "We introduce the first generic text representation model that is completely nonsymbolic, i.e., it does not require the availability of a segmentation or tokenization method that attempts to identify words or other symbolic units in text, and that requires the use of the expression 'C' to denote a symbol. The basic concept of 'C' is that the function (C) can contain a representation of an individual words in one form, and it is a function that performs the same job for another. In the present example, we will use the function C: [C] ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )", "histories": [["v1", "Mon, 3 Oct 2016 10:30:13 GMT  (31kb)", "http://arxiv.org/abs/1610.00479v1", null], ["v2", "Sat, 29 Oct 2016 11:51:10 GMT  (33kb)", "http://arxiv.org/abs/1610.00479v2", null], ["v3", "Mon, 1 May 2017 14:30:00 GMT  (48kb)", "http://arxiv.org/abs/1610.00479v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hinrich schuetze", "heike adel", "ehsaneddin asgari"], "accepted": false, "id": "1610.00479"}, "pdf": {"name": "1610.00479.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["inquiries@cislmu.org"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n00 47\n9v 1\n[ cs\n.C L\n] 3\nO ct\n2 01\n6"}, {"heading": "1 Introduction", "text": "Character-level models are attracting increasing interest. We group them into three classes. (i) Character-level models of words derive a word representation from the character string, but they are symbolic in that they need text segmented into tokens as input. (ii) Bag-of-ngram models discard the order of character ngrams on the assumption that relevant order information is coded in the ngrams, so the order of ngrams can be neglected. (iii) End-to-end models learn a separate model on the raw character (or byte) input for each task; these models estimate task-specific parameters, but no representation of text that would be usable across tasks is computed.\nTurning to group (iii) first, one premise of this paper is that text representations are needed in NLP. A large body of work on the use of word embeddings like SENNA (Collobert et al., 2011), word2vec (Mikolov et al., 2013), HLBL (Mnih and Hinton, 2009) and GloVe (Pennington et al., 2014) demonstrates that a generic text representation, trained in an unsupervised fashion on large corpora, is useful for many\nNLP applications. Thus, we take the view that radical end-to-end learning is not a good approach for NLP.\nIn this paper, we separate training and utilization of the text representation model. We use \u201ctraining\u201d to refer to the method by which the model is learned and \u201cutilization\u201d to refer to the application of the model to a piece of text to compute a representation of the text. In many text representation models, utilization is trivial. For example, for word embedding models, utilization amounts to a simple lookup of a word to get its precomputed embedding. However, for the models we consider, utilization is not trivial and we will discuss different approaches.\nBoth training and utilization can be either symbolic or nonsymbolic. We define a symbolic approach as one that is based on tokenization, i.e., a segmentation of the text into tokens. Symbol identifiers (i.e., tokens) can have internal structure, e.g., if a tokenizer recognizes tokens like \u201cto and fro\u201d and \u201cLondon-based\u201d that contain delimiters or if a token is morphologically analyzed downstream.1\nWe define a nonsymbolic approach as one that is tokenization-free, i.e., no assumption is made that there are segmentation boundaries and that each segment (e.g., a word) should be represented (e.g., by a word embedding) in a way that is independent of the representations (e.g., word embeddings) of neighboring segments.\nMethods for training text representation models that require tokenized text include word embedding models like SENNA, HLBL, Word2vec and GloVe and character-level models like fast-\n1Some define symbols as identifiers that have no internal structure, but uses of the term that allow for the possibility of internal structure are also frequent. For example, the Wikipedia entry for \u201csymbol\u201d states: \u201cNumerals are symbols for numbers.\u201d Most numerals have internal structure.\nText skipgram (Bojanowski et al., 2016). Examples of nonsymbolic text representation utilization models are bag-of-ngram models that compute the representation of a text as the sum of the embeddings of all character ngrams occurring in it, e.g., CHARAGRAM (Wieting et al., 2016)). CHARAGRAM is an example of a mixed trainingutilization model: training requires tokenized text (words and phrases), utilization is nonsymbolic. The model is trained on words and phrases, i.e., tokens that are the output of tokenization, but it can be applied to any untokenized character sequence.\nWe make two contributions in this paper. First, we propose the first generic method for training a text representation model without the need for tokenization and address the challenging sparseness issues that make this a difficult problem. Second, we propose the first nonsymbolic utilization method that fully represents sequence information \u2013 in contrast to utilization methods like bag-ofngrams that discard sequence information that is not directly encoded in the character ngrams themselves.\nWe show that our models perform better than prior work on an information extraction and a text denoising task."}, {"heading": "2 Motivation for nonsymbolic representation", "text": "Chung et al. (2016) discuss two main motivations for their work on character-level models. First, segmentation/tokenization algorithms make many mistakes and are brittle: \u201cwe do not have a perfect word segmentation algorithm for any one language\u201d. Tokenization errors then propagate throughout the NLP pipeline, potentially doing great damage.\nSecond, there is currently no general solution for morphology in statistical NLP. For many languages, high-coverage and high-quality morphological resources are not available. Even for well resourced languages, problems like ambiguity make morphological processing difficult; e.g., \u201crung\u201d is either the singular of a noun meaning \u201cpart of a ladder\u201d or the past participle of \u201cto ring\u201d. In many languages, e.g., in German, syncretism, a particular type of systematic morphological ambiguity, is pervasive. Thus, there is no simple morphological processing method that would produce a representation in which all inflected forms of \u201cto ring\u201d are marked as having a common lemma;\nand no such method in which an unseen form like \u201caromatizing\u201d is reliably analyzed as a form of \u201caromatize\u201d whereas an unseen form like \u201cantitrafficking\u201d is reliably analyzed as the compound \u201canti+trafficking\u201d.\nOf course, it is an open question whether nonsymbolic methods can perform better than morphological analysis, but the foregoing discussion motivates us to investigate them.\nChung et al. (2016) focus on problems with processing the tokens that are output by text segmentation algorithms. Equally important is the problem that tokenization fails to capture structure across multiple tokens. The job of dealing with multi-token structure is often given to downstream components of the pipeline, e.g., components that recognize multiwords and named entitites in English or in fact any word in a language like Chinese that uses no overt delimiters. However, there is no linguistic or computational reason in principle why we should treat the recognition of a unit like \u201celectromechanical\u201d (containing no space) as fundamentally different from the recognition of a unit like \u201celectrical engineering\u201d (containing a space). Character-level models offer the potential of uniform treatment of linguistic units that are relevant for NLP."}, {"heading": "3 Training of the text representation model", "text": ""}, {"heading": "3.1 Methodology", "text": "Many text representation learning algorithms can be understood as estimating the parameters of the model from a unit-context matrix C where each row corresponds to a unit ui, each column to a context cj and each cell Cij is a quantity that measures how strongly ui and cj are associated. For example, the skipgram model is closely related to an SVD factorization of a positive pointwise mutual information matrix (Levy and Goldberg, 2014); in this case, both units and contexts are words. Text representation learning algorithms explicitly formalized as matrix factorization include (Deerwester et al., 1990; Hofmann, 1999; Stratos et al., 2015), but there may not be a big difference between methods that explicitly factorize and those that approximate the factorization through iterative methods (e.g., (Pennington et al., 2014)); see also (Mohamed, 2011; Rastogi et al., 2015).\nOur goal in this paper is not to develop new\nparameter estimation methods, e.g., a new matrix factorization algorithm. Instead, we will focus on defining the unit-context matrix in such a way that no symbolic assumption has to be made. This unitcontext matrix can then be processed by any existing or still to be invented algorithm."}, {"heading": "3.2 Definition of units and contexts", "text": "How can we define units and contexts without relying on segmentation boundaries? In initial experiments, we simply generated all character ngrams of length up to kmax (where kmax is a parameter), including character ngrams that cross token boundaries; i.e., no segmentation is needed. We then used an objective similar to the skipgram objective for learning ngram embeddings, i.e., we used an iterative training procedure each instance of which consisted of selecting an ngram g1 and trying to predict an ngram g2 in its context based on g1. Our first naive implementation of this idea did not work well because many training instances consist of pairs (g1, g2) in which g1 and g2 overlap or one is a subsequence of the other. So the objective encourages trivial predictions of ngrams that have high string similarity with the input and nothing interesting is learned.\nIn this paper, we propose an alternative way of defining units and contexts that supports wellperforming nonsymbolic text representation learning: multiple random segmentation. A pointer moves through the training corpus. The current position i of the pointer defines the left boundary of the next segment. The length l of the next move is uniformly sampled from [kmin, kmax] where kmin and kmax are the minimum and maximum segment lengths. The right boundary of the segment is then i+ l. Thus, the segment just generated is ci,i+l, the subsequence of the corpus between (and including) positions i and i+ l. The pointer is positioned at i+ l+1, the next segment is sampled and so on.\nThe corpus is segmented this way m times (where m is a parameter) and the m random segmentations are concatenated. The unit-context matrix is derived form this concatenated corpus.\nMultiple random segmentation has two advantages. First, there is no redundancy since, in any given random segmentation, two ngrams do not overlap and are not subsequences of each other. Second, a single random segmentation would only cover a small part of the space of possible ngrams. For example, a random segmentation of \u201ca rose\nis a rose is a rose\u201d might be \u201c[a ros][e is a ros][e is][a rose]\u201d. This segmentation does not contain the segment \u201crose\u201d and this part of the corpus can then not be exploited to learn a good embedding for the fourgram \u201crose\u201d. However, if we produce multiple random segmentations of the corpus (and concatenate them), then it is likely that this part of the corpus does give rise to the segment \u201crose\u201d in one of the segmentations and can contribute information to learning a good embedding for \u201crose\u201d."}, {"heading": "3.3 The permutation premise and ngram equivalence classes", "text": "Nonsymbolic representation learning does not preprocess the training corpus by means of tokenization and considers many ngrams that would be ignored in tokenized approaches because they span token boundaries. As a result, the number of ngrams is much larger and sparseness is greatly increased. Figure 1 shows that the number of ngrams that occur in a corpus is an order of magnitude larger for tokenization-free approaches than for tokenization-based approaches.\nWe will see below that this sparseness impacts performance of nonsymbolic text representation negatively. We address sparseness by defining\nngram equivalence classes. All ngrams in an equivalence class will receive the same embedding.\nThe learning procedure for learning equivalence classes is based on two premises.\nPermutation premise. Let A be the alphabet of a language, i.e., its set of characters, \u03c0 a permutation on A, C a corpus and P (\u03c0,C) the transformed corpus after the permutation has been applied to C . For example, if \u03c0(a) = e, then all characters \u201ca\u201d in C are replaced with \u201ce\u201d in P (\u03c0,C). The learning procedure should learn identical equivalence classes on C and P (\u03c0,C). So, if g1 \u223c g2 after runing the learning procedure on C , then P (\u03c0, g1) \u223c P (\u03c0, g2) after running the learning procedure on P (\u03c0,C).\nThis premise is motivated by our desire to come up with a general method that does not rely on specific properties of a language or genre; e.g., the premise rules out exploiting the fact through feature engineering that in many languages and genres, \u201cc\u201d and \u201cC\u201d are related. Such a relationship has to be learned from the data.\nForm-meaning homomorphism premise. The relationship between form and meaning is mostly arbitrary, but there are substructures of the ngram space and the embedding space that are systematically related by homomorphism. In this paper, we will assume that the following homomorphism holds:\ng1RT g2 \u21d4 ~v(g1)R=~v(g2)\nwhere g1RT g2 iff T (g1) = T (g2) for a string transduction T and ~v(g1)R=~v(g2) iff |~v(g1) \u2212 ~v(g2)|2 < \u01eb.\nWe will propose a learning procedure for the transduction below. As a simple example for a meaning-preserving string operation consider \u201cdelete a space at the beginning of an ngram\u201d. For the transductor T we will learn below, we have: T (@Mercedes) = T (Mercedes). This is a good example of what a transducer can do because we can reasonably assume that the embeddings of \u201c@Mercedes\u201d and \u201cMercedes\u201d are the same.\nWe define R= as \u201ccloseness\u201d \u2013 not as identity \u2013 because of estimation noise when embeddings are learned. We assume that there are no true synonyms and therefore the direction g1RT g2 \u21d0 ~v(g1)R=~v(g2) also holds. For example, \u201ccar\u201d and \u201cautomobile\u201d are considered synonyms, but we assume that their embeddings are different (e.g., because \u201ccar\u201d has the literary sense \u201cchariot\u201d that\n\u201cautomobile\u201d does not have). If they were identical, then the homomorphism would not hold since \u201ccar\u201d and \u201cautomobile\u201d cannot be converted into each other by any plausible meaning-preserving string operation.\nLearning procedure. To learn the transductor automatically, we define three templates that transform one ngram into another: (i) replace character a1 with character a2, (ii) delete character a1 if its immediate predecessor is character a2, (iii) delete character a1 if its immediate successor is character a2. The learning procedure takes a set of ngrams and their embeddings as input. It then exhaustively searches for all pairs of ngrams, for all pairs of characters a1/a2, for each of the three templates.2 When two matching embeddings exist, we compute their cosine. For example, for the operation \u201cdelete space before M\u201d, an ngram pair from our embeddings that matches is \u201c@Mercedes\u201d / \u201cMercedes\u201d and we compute the cosine of these two embeddings. As the characteristic statistic of an operation we take the average of all cosines; e.g., for \u201cdelete space before M\u201d the average cosine is 0.7435. We then rank operations according to average cosine and take the first No as the definition of the transductor where No is a parameter. For characters that are replaced by each other (e.g., 1, 2, 3 in Table 1), we compute the eqivalence class and then replace the learned operations with ones that replace a character by the canonical member of its equivalence class (e.g., 2 \u2192 1, 3 \u2192 1)."}, {"heading": "3.4 Experiments", "text": "We run experiments on C , a 3 gigabyte English Wikipedia corpus. Based on the permutation premise (Section 3.3), we first apply a randomly generated permutation \u03c0 to C and then conduct all experiments on P (\u03c0,C), the permuted form of C . We train three text representation models: FASTTEXT, NONSYMBOLIC and TRANSDUCED. We use a dimensionality of 200 throughout this paper for all models.\nFASTTEXT. We train fastText skipgram (Bojanowski et al., 2016), with default parameters, on P (\u03c0,C).\nNONSYMBOLIC. We train word2vec skipgram on a multiple random segmentation corpus that is generated from P (\u03c0,C). As we discussed in Section 3.1, our focus in this paper is not to de-\n2This takes about 10 hours on a 20-core compute server for 4,000,000 ngram embeddings.\nvelop new methods for implicit matrix factorization. We therefore selected word2vec because it is a well understood and widely used method.\nWe use default parameters, except that we train for only one iteration since we found this was sufficient. Parameters for the segmentation were m = 50 (number of copies) and kmin = 3, kmax = 9 (min/max length of character ngrams). The average length of words in this type of setting is between 7 and 8,3 the average length of ngrams of the model we learned with word2vec is 6.9, which is roughly comparable. If we used the fastText maximum kmax = 6 for our model, this would clearly not be a fair comparison.\nFor the nonsymbolic experiments (i.e., not for the experiments with fastText), we replace white spaces in C with \u201c@\u201d. The segmenter then simply marks segment boundaries by white space. This trick avoids the need for any change to the implementation of word2vec and we can run it as is.\nTRANSDUCED. We repeat experiment NONSYMBOLIC with a version of the multiple random segmentation corpus that has been transformed by the transductor T .\nTo learn T , we first run the learning procedure described in Section 3.3 on the ngram embeddings that are the output of NONSYMBOLIC. We then rank all operations by the average cosine of the input embedding and output embedding of all pairs that match the operation (see Section 3.3) and select the top No = 200 to define the transductor.\nTable 1 shows a selection of the No operations.4 The two uppercase/lowercase conversions shown in the table were the only ones that were learned (we had hoped for more). The postdeletion rule \u201cml\u201d \u2192 \u201cm\u201d was learned because of \u201chtml\u201d \u2192 \u201chtm\u201d type ngram pairs. This is good for \u201chtml\u201d, but dangerous as a general rule. We inspected all 200 rules and, with a few exceptions like \u201chtml\u201d \u2192 \u201chtm\u201d, they looked reasonable to us. T applies all 200 operations.\nEVALUATION. We evaluate the three models on an entity typing task based on an entity dataset released by Xie et al. (2016) in which each entity has been assigned one or more types from a set of\n3It is not easy to check the actual average length for FASTTEXT since it only outputs a model in binary format, not a list of embeddings. For word2vec GoogleNews embeddings, the average word length is 7.6.\n4Recall that our experiments are run on a permuted version of Wikipedia (Section 3.3). All examples in the tables and the body of the paper were converted back to the original for better readability.\n50 types. For example, the entity \u201cHarrison Ford\u201d has the types \u201cactor\u201d, \u201ccelebrity\u201d and \u201caward winner\u201d among others. We extract mentions from FACC (URL, 2016; Gabrilovich et al., 2013) if an entity has a mention there or we use the Freebase name as the mention otherwise. This gives us a data set of 54,334, 6085 and 6747 mentions in train, dev and test, respectively. Each mention is annotated with the types that its entity has been assigned by Xie et al. (2016). We will release this dataset at time of publication.\nWe perform entity typing by representing each mention as the sum of the embeddings of all ngrams that occur in the mention. Linear SVMs (Chang and Lin, 2011) were then trained, one for each of the 50 types, on train and applied to dev and test. Our evaluation measure is micro F1 on all typing decisions; e.g., one typing decision is: \u201cHarrison Ford\u201d is a mention of type \u201cactor\u201d. We tune thresholds on dev to optimize F1 and then use these thresholds on test.\nTable 2 shows that NONSYMBOLIC outperforms FASTTEXT decisively. We attribute this to two reasons. First, cross-word ngrams are important for entity typing, but these cross-word ngrams are missing from the FASTTEXT model. Sec-\nond, FASTTEXT seems to have many preprocessing heuristics built in that help with text processing in general and also with entity typing. In a small experiment, we trained FASTTEXT on lowercased text and then applied the learned model to uppercase words. Even though no uppercase character occurred in training, FASTTEXT still computed meaningful ngram embeddings. The permutation we use disables this preprocessing and limits the capabilities of the model to what can be learned from distributional information without feature-engineered preprocessing.\nTRANSDUCED gives us an additional improvement. This shows that the operations learned have identified string variations that affect meaning not at all or only slightly."}, {"heading": "3.5 Analysis of ngram embeddings", "text": "To analyze the NONSYMBOLIC text embedding model further, Table 3 shows nearest neighbors of ten character ngrams. The search space was limited to 9-grams. Queries were chosen to contain only alphanumeric characters. To highlight the difference to symbol-based representation models, we restricted the search to those 9-grams that contained a delimiter somewhere in the middle of the word (i.e., the first two and last two positions do not count).\nLines 1\u20134 show that \u201cdelimiter variation\u201d, i.e., cases where a word has two forms, one with a delimiter, one without a delimiter, is handled well: \u201cAbdulaziz\u201d / \u201cAbdul Azi\u201d, \u201ccodenamed\u201d / \u201ccode name\u201d, \u201cQuarterfinal\u201d / \u201cQuarter-Final\u201d, \u201cworldrecord\u201d / \u201cworld-record\u201d.\nLines 5\u20139 are cases of ambiguous or polysemous words that are disambiguated through \u201ccharacter context\u201d. \u201cstem\u201d, \u201ccell\u201d, \u201crear\u201d, \u201cwheel\u201d, \u201ccrash\u201d, \u201cland\u201d, \u201cscripts\u201d, \u201cthrough\u201d, \u201cdowntown\u201d all have several meanings. In contrast, the meanings of \u201cstem cell\u201d, \u201crear wheel\u201d, \u201ccrash land\u201d,\n\u201c(write) scripts for\u201d and \u201cthrough downtown\u201d are specific and much less ambiguous. A multiword recognizer might be able to find the phrases \u201cstem cell\u201d and \u201ccrash land\u201d automatically. But the examples of \u201cscripts for\u201d and \u201cthrough downtown\u201d show that what is accomplished here is not multiword detection, but a more general use of character context for disambiguation.\nLine 10 shows that a 9-gram of \u201cface-to-face\u201d is the closest neighbor to a 9-gram of \u201cfacilitating\u201d. This demonstrates that form and meaning sometimes interact in surprising ways. Facilitating a meeting is most commonly done face-to-face. It is not inconceivable that form \u2013 the shared trigram \u201cfac\u201d or the shared fourgram \u201cfaci\u201d in \u201cfacilitate\u201d / \u201cfacing\u201d \u2013 is influencing meaning here in a way that also occurs historically in cases like \u201cear\u201d \u2018organ of hearing\u2019 / \u201cear\u201d \u2018head of cereal plant\u2019, originally distinct and unrelated words that many speakers of English today intuit as a single word."}, {"heading": "4 Utilization: Tokenization-free representation of text", "text": ""}, {"heading": "4.1 Methodology", "text": "The main text representation model that is based on ngram embeddings similar to ours is the bagof-ngram model. A sequence of characters is represented by a single vector that is computed as the sum of the embeddings of all ngrams that occur in the sequence. In most work on bagof-ngram models, the sequences considered are words or phrases (see Section 5 for citations). In fact, that is what we did in our first experiment. In a few cases, the model is applied to longer sequences, including sentences and documents (e.g., (Wieting et al., 2016)).\nThe basic assumption of the bag-of-ngram model is that sequence information is encoded in the character ngrams and therefore a \u201cbag-of\u201d approach (which usually throws away all sequence information) is sufficient. The assumption is not implausible: for most bags of character sequences, there is only a single way of stitching them together to one coherent sequence, so in that case information is not necessarily lost (although this is likely when embeddings are added). But the assumption has not been tested experimentally.\nEnd-to-end systems that use RNNs (e.g., LSTMs) for sequence processing are not bag-ofngram. But these systems have not been widely used for representation learning. See our discus-\nsion of related work (Section 5). Here, we propose position embeddings, character-ngram-based embeddings that more fully preserve sequence information. The simple idea is to represent each position as the sum of all ngrams that contain that position. For the settings kmin = 3, kmax = 9 in our NONSYMBOLIC model, this means that the position is the sum of \u2211 3\u2264k\u22649 k ngram embeddings (if all of these ngrams have embeddings, which generally will be true for some, but not for most positions). A sequence of n characters is then represented as a sequence of n such position embeddings."}, {"heading": "4.2 Experiments", "text": "We use the NONSYMBOLIC model from the previous section. As a small illustrative example of the potential of position embeddings, we devised the following experiment. We randomly selected 2,000,000 contexts of size 40 characters from our Wikipedia corpus. We then created a noise context for each of the 2,0000,000 contexts by replacing one character at position i (15 \u2264 i \u2264 25, uniformly sampled) with space (p = .5) or a randomly chosen character (p = .5). This was repeated until a sequence was produced that was different from the original context. Finally, we selected 1000 noise contexts randomly and computed their nearest neighbors among the 4,000,000 contexts (excluding the noise query). We did this in two different conditions: for a bag-of-ngram representation of the context and for a sequence of 11 position embeddings, those between 15 and 25. Our evaluation measure is mean reciprocal rank of the clean context corresponding to the noise context. This simulates a text denoising experiment since if the clean context has rank 1, then the noisy context can be corrected.\nTable 4 shows that sequence-preserving position embeddings perform better than bag-ofngram representations.\nTable 5 shows an example of a context in which position embeddings did better than bag-ofngrams. This example demonstrates that impor-\ntant sequence information can be lost by bag-ofngram representations, in this case the exact position in the string of the word \u201cSeahawks\u201d.\nTo give further intuition about what type of information position embeddings contain, we show the ngram embeddings that are closest to position embeddings for an example in Figure 6; e.g., \u201cestseller\u201d (the first 9-gram on the line numbered 3 in the table) is closest to the embedding of position 3 (the letter \u201cs\u201d of the word \u201cbest-selling\u201d). The kNN search space is restricted to alphanumeric ngrams."}, {"heading": "5 Related work", "text": "We attempt to provide a comprehensive review of recent literature, but have no space to discuss individual papers. We therefore group related work into clusters and tag each cluster. The reader can find the relevant papers by scanning the reference section for papers with the tag of interest.\nOur main goal in this paper is to overcome the limitations of tokenization both in estimating the parameters of a text representation model and in applying it to computing the representation of a new text. Most prior work on characterlevel models has been based on tokenization or, more generally, has addressed tasks that involve processing of words. This class of paper is tagged TOKENIZATION-BASED-MODEL. We also include here models that learn one embedding per Chinese character (which is conceptually and architecturally similar to word embeddings) and models that learn character sequences that are phrases (e.g., named entities). The criterion for inclusion in this group is not that segmentation is identical to standard tokenization; instead, the criterion is that a segmentation is computed and each segment boundary is a traditional token boundary. Note that many rule-based tokenizers produce tokens that contain spaces (e.g., Kaplan (2006)).\nThere have been two main approaches that are not tokenization-based. The first one learns character ngram embeddings (from tokenized text or short text segments in all prior work that we have reviewed), but then computes the representation of new text segments without the need for tokenization by summing the embeddings of all occurring ngrams. We have shown above that important sequence information is lost in this approach. This class of paper is tagged BAG-OFNGRAM-MODEL. We also include sparse repre-\nsentation spaces whose dimensionality is the number of ngrams, i.e., non-embedding approaches. Many methods compute token representations in a bag-of-ngram fashion. We only tag papers as bag-of-ngram that go beyond word and token representations.\nEnd-to-end approaches to speech and natural language processing (Eyben et al., 2009; Graves and Jaitly, 2014; Gillick et al., 2015; Bahdanau et al., 2016; Chan et al., 2016) are the second main tokenization-free approach. They dispense with intermediate representations entirely. This class of papers is tagged END-TO-END-MODEL.5\nOur premise is that text representations are needed in NLP. A large body of work on word embeddings like SENNA (Collobert et al., 2011), word2vec (Mikolov et al., 2013), HLBL (Mnih and Hinton, 2009) and GloVe (Pennington et al., 2014) demonstrates that a generic text representation, trained in an unsupervised fashion on large corpora, is useful for solving NLP tasks. Thus, we take the view that radical end-to-end learning is not a good approach for NLP; note that this argument does not apply to variants of end-to-end learning that use text repre-\n5We have also included a few papers that are heuristically optimized (not truly trained end-to-end), but that share the property of end-to-end models that no generic, taskindependent representation of text is learned.\nsentations, e.g., pretrained word embeddings, that are then modified in an end-to-end setup (e.g., (Collobert et al., 2011)).\nOur work bears some similarity to the autoencoder introduced by Dai and Le (2015), but the authors do not couch their approach as representation learning and do not analyze the properties of their representation beyond showing that it performs well on a number of tasks.\nThe hidden states of a character language model can also be interpreted as a nonsymbolic text representation. There seems little work that has exploited this, but see (Chrupala, 2013; Evang et al., 2013; Chrupala, 2014).\nWe took the idea of random segmentation from work on biological sequences (Asgari and Mofrad, 2015; Asgari and Mofrad, 2016). Biological sequences have no delimiters, so they provide a good model if one believes that delimiter-based segmentation is problematic for text."}, {"heading": "6 Conclusion and future work", "text": "We have introduced the first generic text representation model that is completely nonsymbolic, i.e., it does not require the availability of a segmentation or tokenization method that attempts to identify words or other symbolic units in text. This is true for the training of the model as well as for\napplying it when computing the representation of a new text. We showed that our model performs better than prior work on an information extraction and a text denoising task.\nFuture work. Gillick et al. (2015) write: \u201cIt is worth noting that noise is often added . . . to images . . . and speech where the added noise does not fundamentally alter the input, but rather blurs it. [bytes allow us to achieve] something like blurring with text.\u201d \u201cBlurring\u201d is not a completely accurate description: a byte can only be altered by flipping a bit, a discrete process that turns, e.g., \u201cB\u201d (100010) into \u201cC\u201d (100011). In contrast, the text representation we have introduced in this paper can be blurred in a way that is completely analogous to images and speech. Each embedding of a position is a vector that can be smoothly changed in every direction. We plan to exploit this property in future work.\nThe most important challenge that we need to address is how to use nonsymbolic text representation for tasks that are word-based like partof-speech tagging. This may seem like a contradiction at first, but Gillick et al. (2015) have shown how character-based methods can be used for \u201csymbolic\u201d tasks. We are currently working on creating an analogous evaluation for our nonsymbolic text representation."}], "references": [{"title": "An unsupervised system for identifying english inclusions in german text. In Annual Meeting of the Association for Computational Linguistics. TOKENIZATIONBASED-MODEL", "author": ["Beatrice Alex"], "venue": null, "citeRegEx": "Alex.,? \\Q2005\\E", "shortCiteRegEx": "Alex.", "year": 2005}, {"title": "Continuous distributed representation of biological sequences for deep proteomics and genomics", "author": ["Asgari", "Mofrad2015] Ehsaneddin Asgari", "Mohammad R.K. Mofrad"], "venue": "PLoS ONE,", "citeRegEx": "Asgari et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Asgari et al\\.", "year": 2015}, {"title": "Comparing fifty natural languages and twelve genetic languages using word embedding language divergence (WELD) as a quantitative measure of language distance", "author": ["Asgari", "Mofrad2016] Ehsaneddin Asgari", "Mohammad R.K. Mofrad"], "venue": null, "citeRegEx": "Asgari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Asgari et al\\.", "year": 2016}, {"title": "End-to-end attentionbased large vocabulary speech recognition", "author": ["Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio"], "venue": "In IEEE International Conference on Acoustics,", "citeRegEx": "Bahdanau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2016}, {"title": "Language identification: The long and the short of the matter", "author": ["Baldwin", "Lui2010] Timothy Baldwin", "Marco Lui"], "venue": "In Conference of the North American Chapter of the Association for Computational Linguistics / Human Language Technologies,", "citeRegEx": "Baldwin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baldwin et al\\.", "year": 2010}, {"title": "Improved transitionbased parsing by modeling characters instead of words with lstms", "author": ["Chris Dyer", "Noah A. Smith"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Enriching word vectors with subword information", "author": ["Edouard Grave", "Armand Joulin", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1607.04606. TOKENIZATION-BASED-MODEL", "citeRegEx": "Bojanowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Botha", "Blunsom2014] Jan A Botha", "Phil Blunsom"], "venue": "In International Conference on Machine Learning. TOKENIZATION-BASED-MODEL", "citeRegEx": "Botha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Botha et al\\.", "year": 2014}, {"title": "A joint model for word embedding and word morphology", "author": ["Cao", "Rei2016] Kris Cao", "Marek Rei"], "venue": "Computing Research Repository (arXiv.org),", "citeRegEx": "Cao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2016}, {"title": "Using an n-grambased document representation with a vector processing retrieval model", "author": ["W Cavnar"], "venue": "NIST SPECIAL PUBLICATION SP,", "citeRegEx": "Cavnar.,? \\Q1995\\E", "shortCiteRegEx": "Cavnar.", "year": 1995}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["Chan et al.2016] William Chan", "Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals"], "venue": "In IEEE International Conference on Acoustics,", "citeRegEx": "Chan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2016}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "Lin2011] Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM TIST,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Joint learning of character and word embeddings", "author": ["Chen et al.2015] Xinxiong Chen", "Lei Xu", "Zhiyuan Liu", "Maosong Sun", "Huan-Bo Luan"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Named entity recognition with bidirectional lstm-cnns. Transactions of the Association for Computational Linguistics, 4:357\u2013370", "author": ["Chiu", "Nichols2016] Jason P.C. Chiu", "Eric Nichols"], "venue": null, "citeRegEx": "Chiu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chiu et al\\.", "year": 2016}, {"title": "Text segmentation with character-level text embeddings. CoRR, abs/1309.4628", "author": ["Grzegorz Chrupala"], "venue": null, "citeRegEx": "Chrupala.,? \\Q2013\\E", "shortCiteRegEx": "Chrupala.", "year": 2013}, {"title": "Normalizing tweets with edit scripts and recurrent neural embeddings", "author": ["Grzegorz Chrupala"], "venue": "In Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Chrupala.,? \\Q2014\\E", "shortCiteRegEx": "Chrupala.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Chung et al.2016] Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Annual Meeting of the Association", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Combining distributional and morphological information for part of speech induction", "author": ["Alexander Clark"], "venue": "In Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Clark.,? \\Q2003\\E", "shortCiteRegEx": "Clark.", "year": 2003}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Character-based neural machine translation", "author": ["Costa-Juss\u00e0", "Jos\u00e9 A.R. Fonollosa"], "venue": "In Annual Meeting of the Association for Computational Linguistics. TOKENIZATION-BASED-MODEL", "citeRegEx": "Costa.Juss\u00e0 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Costa.Juss\u00e0 et al\\.", "year": 2016}, {"title": "Multitask learning of morphological word-embeddings", "author": ["Cotterell", "Sch\u00fctze2015] Ryan Cotterell", "Hinrich Sch\u00fctze"], "venue": "In Conference of the North American Chapter of the Association for Computational Linguistics / Human Language Tech-", "citeRegEx": "Cotterell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cotterell et al\\.", "year": 2015}, {"title": "A joint model of orthography and morphological segmentation", "author": ["Tim Vieira", "Hinrich Sch\u00fctze"], "venue": "In Conference of the North American Chapter of the Association", "citeRegEx": "Cotterell et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cotterell et al\\.", "year": 2016}, {"title": "Semi-supervised sequence learning", "author": ["Dai", "Le2015] Andrew M. Dai", "Quoc V. Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Gauging similarity with n-grams. language-independent categorization of text", "author": ["M. Damashek"], "venue": null, "citeRegEx": "Damashek.,? \\Q1995\\E", "shortCiteRegEx": "Damashek.", "year": 1995}, {"title": "Indexing by latent semantic analysis", "author": ["Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman"], "venue": "Journal of the American Society for Information", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "A hybrid neural model for type classification of entity mentions", "author": ["Dong et al.2015] Li Dong", "Furu Wei", "Hong Sun", "Ming Zhou", "Ke Xu"], "venue": "In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["dos Santos", "Maira Gatti"], "venue": "In International Conference on Computational Linguistics,", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["dos Santos", "Victor Guimar\u00e3es"], "venue": "In Fifth Named Entity Workshop,", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["dos Santos", "Bianca Zadrozny"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Statistical identification of language", "author": ["Ted Dunning"], "venue": "Technical Report MCCS 940273,", "citeRegEx": "Dunning.,? \\Q1994\\E", "shortCiteRegEx": "Dunning.", "year": 1994}, {"title": "Elephant: Sequence labeling for word and sentence segmentation", "author": ["Evang et al.2013] Kilian Evang", "Valerio Basile", "Grzegorz Chrupala", "Johan Bos"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Evang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Evang et al\\.", "year": 2013}, {"title": "From speech to letters - using a novel neural network architecture for grapheme based ASR", "author": ["Eyben et al.2009] Florian Eyben", "Martin W\u00f6llmer", "Bj\u00f6rn W. Schuller", "Alex Graves"], "venue": "In IEEE Workshop on Automatic Speech Recognition", "citeRegEx": "Eyben et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Eyben et al\\.", "year": 2009}, {"title": "ASOBEK at SemEval-2016 task 1: Sentence representation with character n-gram embeddings for semantic textual similarity", "author": ["Eyecioglu", "Keller2016] Asli Eyecioglu", "Bill Keller"], "venue": null, "citeRegEx": "Eyecioglu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Eyecioglu et al\\.", "year": 2016}, {"title": "Morphological inflection generation using character sequence to sequence learning", "author": ["Yulia Tsvetkov", "Graham Neubig", "Chris Dyer"], "venue": "In Conference of the North American Chapter of the Association", "citeRegEx": "Faruqui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Facc1: Freebase annotation of clueweb corpora", "author": ["Michael Ringgaard", "Amarnag Subramanya"], "venue": null, "citeRegEx": "Gabrilovich et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gabrilovich et al\\.", "year": 2013}, {"title": "Multilingual language processing from bytes. Computing Research Repository (arXiv.org). END-TO-ENDMODEL", "author": ["Gillick et al.2015] Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "venue": null, "citeRegEx": "Gillick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Character-level question answering with attention", "author": ["Golub", "He2016] David Golub", "Xiadong He"], "venue": "In Conference on Empirical Methods in Natural Language Processing. END-TO-ENDMODEL", "citeRegEx": "Golub et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Golub et al\\.", "year": 2016}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Jaitly2014] Alex Graves", "Navdeep Jaitly"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Finding nonarbitrary form-meaning systematicity using stringmetric learning for kernel regression", "author": ["Roger Levy", "Benjamin Bergen"], "venue": "In Annual Meeting of the Association", "citeRegEx": "Guti\u00e9rrez et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Guti\u00e9rrez et al\\.", "year": 2016}, {"title": "Codex: Combining an SVM classifier and character n-gram language models for sentiment analysis on twitter text", "author": ["Han et al.2013] Qi Han", "Junfei Guo", "Hinrich Sch\u00fctze"], "venue": "In Second Joint Conference on Lexical and Computational Seman-", "citeRegEx": "Han et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Han et al\\.", "year": 2013}, {"title": "Probabilistic latent semantic indexing", "author": ["Thomas Hofmann"], "venue": "In ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Hofmann.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann.", "year": 1999}, {"title": "Hierarchical character-word models for language identification", "author": ["Jaech et al.2016] Aaron Jaech", "George Mulcaire", "Shobhit Hathi", "Mari Ostendorf", "Noah A. Smith"], "venue": null, "citeRegEx": "Jaech et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaech et al\\.", "year": 2016}, {"title": "MED: The LMU system for the SIGMORPHON 2016 shared task on morphological reinflection", "author": ["Kann", "Sch\u00fctze2016a] Katharina Kann", "Hinrich Sch\u00fctze"], "venue": "In SIGMORPHON Workshop. TOKENIZATION-BASED-MODEL", "citeRegEx": "Kann et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kann et al\\.", "year": 2016}, {"title": "2016b. Single-model encoder-decoder with explicit morphological representation for reinflection", "author": ["Kann", "Sch\u00fctze2016b] Katharina Kann", "Hinrich Sch\u00fctze"], "venue": "In Annual Meeting of the Association for Computational Linguistics. TOKENIZATION-", "citeRegEx": "Kann et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kann et al\\.", "year": 2016}, {"title": "Neural morphological analysis: Encoding-decoding canonical segments", "author": ["Kann et al.2016] Katharina Kann", "Ryan Cotterell", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Kann et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kann et al\\.", "year": 2016}, {"title": "A method for tokenizing text", "author": ["Ronald M. Kaplan"], "venue": "Kaius Sinnema\u0308ki, editors, A Man of Measure", "citeRegEx": "Kaplan.,? \\Q2006\\E", "shortCiteRegEx": "Kaplan.", "year": 2006}, {"title": "Using syllables as indexing terms in full-text information retrieval. In Human Language Technologies - The Baltic Perspective ", "author": ["Paul McNamee", "Feza Baskaya"], "venue": "Proceedings of the Fourth International", "citeRegEx": "Kettunen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kettunen et al\\.", "year": 2010}, {"title": "Characteraware neural language models", "author": ["Kim et al.2016] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Named entity recognition with character-level models", "author": ["Klein et al.2003] Dan Klein", "Joseph Smarr", "Huy Nguyen", "Christopher D. Manning"], "venue": "In Computational Natural Language Learning,", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Automatic labelling of topic models using word vectors and letter trigram vectors", "author": ["Kou et al.2015] Wanqiu Kou", "Fang Li", "Timothy Baldwin"], "venue": "In Asia Information Retrieval Societies Conference (AIRS),", "citeRegEx": "Kou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kou et al\\.", "year": 2015}, {"title": "Neural architectures for named entity recognition", "author": ["Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": "In Conference of the North American Chapter of the Association", "citeRegEx": "Lample et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "NAVER machine translation system for WAT", "author": ["Lee et al.2015] Hyoung-Gyu Lee", "Jae-Song Lee", "JunSeok Kim", "Chang-Ki Lee"], "venue": "In Workshop on Asian Translation (WAT2015). END-TOEND-MODEL", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Machine Translation 4 Microblogs", "author": ["Wang Ling"], "venue": "Ph.D. thesis, Instituto Superior Te\u0301cnico/CMU. http://www.cs.cmu.edu/ \u0303lingwang/papers/thesis.pdf, TOKENIZATION-BASED-MODEL", "citeRegEx": "Ling.,? \\Q2015\\E", "shortCiteRegEx": "Ling.", "year": 2015}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Luong", "Manning2016] Minh-Thang Luong", "Christopher D. Manning"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Luong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Richard Socher", "Christopher D. Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong et al.2015] Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "In Annual Meeting of the Association", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "End-to-end sequence labeling via bidirectional lstm-cnns-crf", "author": ["Ma", "Hovy2016] Xuezhe Ma", "Eduard H. Hovy"], "venue": "In Annual Meeting of the Association for Computational Linguistics. ENDTO-END-MODEL", "citeRegEx": "Ma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "Character n-gram tokenization for european language text retrieval", "author": ["McNamee", "Mayfield2004] Paul McNamee", "James Mayfield"], "venue": "Information Retrieval,", "citeRegEx": "McNamee et al\\.,? \\Q2004\\E", "shortCiteRegEx": "McNamee et al\\.", "year": 2004}, {"title": "Subword language modeling with neural networks. TOKENIZATION-BASEDMODEL", "author": ["Ilya Sutskever", "Anoop Deoras", "Hai-Son Le", "Stefan Kombrink", "Jan Cernocky"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean"], "venue": "In Advances in Neural Information Processing", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Gated word-character recurrent language model. CoRR, abs/1606.01700. END-TO-END-MODEL", "author": ["Miyamoto", "Cho2016] Yasumasa Miyamoto", "Kyunghyun Cho"], "venue": null, "citeRegEx": "Miyamoto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miyamoto et al\\.", "year": 2016}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2009] Andriy Mnih", "Geoffrey E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "author": ["Plank et al.2016] Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg"], "venue": "In Annual Meeting of the Association", "citeRegEx": "Plank et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Deep learning for character-based information extraction", "author": ["Qi et al.2014] Yanjun Qi", "Sujatha G. Das", "Ronan Collobert", "Jason Weston"], "venue": "In European Conference on Information Retrieval,", "citeRegEx": "Qi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qi et al\\.", "year": 2014}, {"title": "Multiview LSA: Representation learning via generalized CCA", "author": ["Benjamin Van Durme", "Raman Arora"], "venue": "In North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Rastogi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rastogi et al\\.", "year": 2015}, {"title": "Weighting finite-state transductions with neural context", "author": ["Ryan Cotterell", "Jason Eisner"], "venue": "In Conference of the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Rastogi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastogi et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "In Annual Meeting of the Association for Computational Linguistics. TOKENIZATION-BASED-MODEL", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Feature-rich sub-lexical language models using a maximum entropy approach for german LVCSR", "author": ["Amr El-Desoky Mousa", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "In Annual Conference of the International Speech", "citeRegEx": "Shaik et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shaik et al\\.", "year": 2013}, {"title": "Letter n-gram-based input encoding for continuous space language models", "author": ["Sperr et al.2013] Henning Sperr", "Jan Niehues", "Alex Waibel"], "venue": "In Workshop on Continuous Vector Space Models and their Compositionality,", "citeRegEx": "Sperr et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sperr et al\\.", "year": 2013}, {"title": "Model-based word embeddings from decompositions of count matrices", "author": ["Stratos et al.2015] Karl Stratos", "Michael Collins", "Daniel J. Hsu"], "venue": "In Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Stratos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stratos et al\\.", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["James Martens", "Geoffrey E. Hinton"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Analyzing the use of character-level translation with sparse and noisy datasets", "author": ["Tiedemann", "Nakov2013] J\u00f6rg Tiedemann", "Preslav Nakov"], "venue": "In Recent Advances in Natural Language Processing,", "citeRegEx": "Tiedemann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tiedemann et al\\.", "year": 2013}, {"title": "Sharing network parameters for crosslingual named entity recognition", "author": ["V et al.2016] Rudra Murthy V", "Mitesh M. Khapra", "Pushpak Bhattacharyya"], "venue": null, "citeRegEx": "V et al\\.,? \\Q2016\\E", "shortCiteRegEx": "V et al\\.", "year": 2016}, {"title": "Can we translate letters", "author": ["Vilar et al.2007] David Vilar", "Jan-T. Peter", "Hermann Ney"], "venue": "In Workshop on Statistical Machine Translation. END-TOEND-MODEL", "citeRegEx": "Vilar et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Vilar et al\\.", "year": 2007}, {"title": "Word representation models for morphologically rich languages in neural machine translation", "author": ["Trevor Cohn", "Xuanli He", "Gholamreza Haffari"], "venue": null, "citeRegEx": "Vylomova et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vylomova et al\\.", "year": 2016}, {"title": "Morphological segmentation with window LSTM neural networks", "author": ["Wang et al.2016] Linlin Wang", "Zhu Cao", "Yu Xia", "Gerard de Melo"], "venue": "In AAAI Conference on Artificial Intelligence. TOKENIZATION-BASED-MODEL", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Charagram: Embedding words and sentences via character n-grams", "author": ["Wieting et al.2016] John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Wieting et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2016}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["W. Wang", "C. Young", "J. Smith", "J. Riesa", "A. Rudnick", "O. Vinyals", "G. Corrado", "M. Hughes", "J. Dean."], "venue": "arXiv preprint. END-TO-END-MODEL.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Efficient character-level document classification by combining convolution and recurrent layers. CoRR, abs/1602.00367. END-TO-END-MODEL", "author": ["Xiao", "Cho2016] Yijun Xiao", "Kyunghyun Cho"], "venue": null, "citeRegEx": "Xiao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2016}, {"title": "Representation learning of knowledge graphs with entity descriptions", "author": ["Xie et al.2016] Ruobing Xie", "Zhiyuan Liu", "Jia Jia", "Huanbo Luan", "Maosong Sun"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Xie et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Multi-task crosslingual sequence tagging from scratch", "author": ["Yang et al.2016] Zhilin Yang", "Ruslan Salakhutdinov", "William W. Cohen"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["Zhang et al.2015] Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "A large body of work on the use of word embeddings like SENNA (Collobert et al., 2011), word2vec (Mikolov et al.", "startOffset": 62, "endOffset": 86}, {"referenceID": 60, "context": ", 2011), word2vec (Mikolov et al., 2013), HLBL (Mnih and Hinton, 2009) and GloVe", "startOffset": 18, "endOffset": 40}, {"referenceID": 63, "context": "(Pennington et al., 2014) demonstrates that a generic text representation, trained in an unsupervised fashion on large corpora, is useful for many NLP applications.", "startOffset": 0, "endOffset": 25}, {"referenceID": 6, "context": "Text skipgram (Bojanowski et al., 2016).", "startOffset": 14, "endOffset": 39}, {"referenceID": 78, "context": ", CHARAGRAM (Wieting et al., 2016)).", "startOffset": 12, "endOffset": 34}, {"referenceID": 24, "context": "learning algorithms explicitly formalized as matrix factorization include (Deerwester et al., 1990; Hofmann, 1999; Stratos et al., 2015), but there may not be a big difference between methods that explicitly factorize and those that approx-", "startOffset": 74, "endOffset": 136}, {"referenceID": 40, "context": "learning algorithms explicitly formalized as matrix factorization include (Deerwester et al., 1990; Hofmann, 1999; Stratos et al., 2015), but there may not be a big difference between methods that explicitly factorize and those that approx-", "startOffset": 74, "endOffset": 136}, {"referenceID": 71, "context": "learning algorithms explicitly formalized as matrix factorization include (Deerwester et al., 1990; Hofmann, 1999; Stratos et al., 2015), but there may not be a big difference between methods that explicitly factorize and those that approx-", "startOffset": 74, "endOffset": 136}, {"referenceID": 63, "context": ", (Pennington et al., 2014)); see also (Mohamed, 2011; Rastogi et al.", "startOffset": 2, "endOffset": 27}, {"referenceID": 66, "context": ", 2014)); see also (Mohamed, 2011; Rastogi et al., 2015).", "startOffset": 19, "endOffset": 56}, {"referenceID": 6, "context": "We train fastText skipgram (Bojanowski et al., 2016), with default parameters, on P (\u03c0,C).", "startOffset": 27, "endOffset": 52}, {"referenceID": 81, "context": "released by Xie et al. (2016) in which each entity has been assigned one or more types from a set of", "startOffset": 12, "endOffset": 30}, {"referenceID": 34, "context": "We extract mentions from FACC (URL, 2016; Gabrilovich et al., 2013) if an entity has a mention there or we use the Freebase name as the mention otherwise.", "startOffset": 30, "endOffset": 67}, {"referenceID": 81, "context": "assigned by Xie et al. (2016). We will release this dataset at time of publication.", "startOffset": 12, "endOffset": 30}, {"referenceID": 78, "context": ", (Wieting et al., 2016)).", "startOffset": 2, "endOffset": 24}, {"referenceID": 45, "context": ", Kaplan (2006)).", "startOffset": 2, "endOffset": 16}, {"referenceID": 31, "context": "End-to-end approaches to speech and natural language processing (Eyben et al., 2009; Graves and Jaitly, 2014; Gillick et al., 2015; Bahdanau et al., 2016; Chan et al., 2016) are the second main tokenization-free approach.", "startOffset": 64, "endOffset": 173}, {"referenceID": 35, "context": "End-to-end approaches to speech and natural language processing (Eyben et al., 2009; Graves and Jaitly, 2014; Gillick et al., 2015; Bahdanau et al., 2016; Chan et al., 2016) are the second main tokenization-free approach.", "startOffset": 64, "endOffset": 173}, {"referenceID": 3, "context": "End-to-end approaches to speech and natural language processing (Eyben et al., 2009; Graves and Jaitly, 2014; Gillick et al., 2015; Bahdanau et al., 2016; Chan et al., 2016) are the second main tokenization-free approach.", "startOffset": 64, "endOffset": 173}, {"referenceID": 10, "context": "End-to-end approaches to speech and natural language processing (Eyben et al., 2009; Graves and Jaitly, 2014; Gillick et al., 2015; Bahdanau et al., 2016; Chan et al., 2016) are the second main tokenization-free approach.", "startOffset": 64, "endOffset": 173}, {"referenceID": 18, "context": "embeddings like SENNA (Collobert et al., 2011), word2vec (Mikolov et al.", "startOffset": 22, "endOffset": 46}, {"referenceID": 60, "context": ", 2011), word2vec (Mikolov et al., 2013), HLBL (Mnih and Hinton, 2009) and GloVe", "startOffset": 18, "endOffset": 40}, {"referenceID": 63, "context": "(Pennington et al., 2014) demonstrates that a generic text representation, trained in an unsupervised fashion on large corpora, is useful for solving NLP tasks.", "startOffset": 0, "endOffset": 25}, {"referenceID": 18, "context": ", (Collobert et al., 2011)).", "startOffset": 2, "endOffset": 26}, {"referenceID": 14, "context": "that has exploited this, but see (Chrupala, 2013; Evang et al., 2013; Chrupala, 2014).", "startOffset": 33, "endOffset": 85}, {"referenceID": 30, "context": "that has exploited this, but see (Chrupala, 2013; Evang et al., 2013; Chrupala, 2014).", "startOffset": 33, "endOffset": 85}, {"referenceID": 15, "context": "that has exploited this, but see (Chrupala, 2013; Evang et al., 2013; Chrupala, 2014).", "startOffset": 33, "endOffset": 85}, {"referenceID": 35, "context": "Gillick et al. (2015) write: \u201cIt is worth noting that noise is often added .", "startOffset": 0, "endOffset": 22}, {"referenceID": 35, "context": "This may seem like a contradiction at first, but Gillick et al. (2015) have shown how character-based methods can be used for \u201csymbolic\u201d tasks.", "startOffset": 49, "endOffset": 71}], "year": 2017, "abstractText": "We introduce the first generic text representation model that is completely nonsymbolic, i.e., it does not require the availability of a segmentation or tokenization method that attempts to identify words or other symbolic units in text. This applies to training the parameters of the model on a training corpus as well as to applying it when computing the representation of a new text. We show that our model performs better than prior work on an information extraction and a text denoising task.", "creator": "LaTeX with hyperref package"}}}