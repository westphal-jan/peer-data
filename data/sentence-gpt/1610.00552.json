{"id": "1610.00552", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2016", "title": "FPGA-Based Low-Power Speech Recognition with Recurrent Neural Networks", "abstract": "In this paper, a neural network based real-time speech recognition (SR) system is developed using an FPGA for very low-power operation. The implemented system employs two recurrent neural networks (RNNs); one is a speech-to-character RNN for acoustic modeling (AM) and the other is for character-level language modeling (LM). The system also employs a statistical word-level LM to improve the recognition accuracy. The results of the AM, the character-level LM, and the word-level LM are combined using a fairly simple N-best search algorithm instead of the hidden Markov model (HMM) based network. The RNNs are implemented using massively parallel processing elements (PEs) for low latency and high throughput. The weights are quantized to 6 bits to store all of them in the on-chip memory of an FPGA. The proposed algorithm is implemented on a Xilinx XC7Z045, and the system can operate much faster than real-time.\n\n\n\n\n\n\nThe paper is supported in part by the Office of Management and Budget and the IEEE Technology Conference.", "histories": [["v1", "Fri, 30 Sep 2016 10:44:32 GMT  (471kb,D)", "http://arxiv.org/abs/1610.00552v1", "Accepted to SiPS 2016"]], "COMMENTS": "Accepted to SiPS 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["minjae lee", "kyuyeon hwang", "jinhwan park", "sungwook choi", "sungho shin", "wonyong sung"], "accepted": false, "id": "1610.00552"}, "pdf": {"name": "1610.00552.pdf", "metadata": {"source": "CRF", "title": "FPGA-based Low-power Speech Recognition with Recurrent Neural Networks", "authors": ["Minjae Lee", "Kyuyeon Hwang", "Jinhwan Park", "Sungwook Choi", "Sungho Shin", "Wonyong Sung"], "emails": ["shshin}@dsp.snu.ac.kr,", "wysung@snu.ac.kr"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nSpeech recognition has long been studied, and most of the algorithms employ hidden Markov models (HMMs) or its variants as inference and information combining tools [1], [2]. Recently, deep neural networks are employed for acoustic modeling (AM) of state of the art speech recognition systems which, however, are not free from the HMM [3]. HMM modeling for speech recognition demands a vast amount of memory access operations on a large size network, whose memory capacity usually exceeds a few hundred megabytes [4]. Thus, speech recognition algorithms are usually implemented on GPUs or multi-core systems that equip large DRAM-based memory, which are hardly power efficient.\nRecently, fully neural recurrent network based speech recognition algorithms are actively investigated [5], [6]. The RNN is end-to-end trained with connectionist temporal classification (CTC) [7] to directly transcribe the input utterance to characters. The RNN has also been used for language modeling (LM), which shows much better capability than tri-gram based statistical algorithms [8]. Recently, complete speech recognition algorithms have been developed by combining the CTC RNN and the RNN LM [5], [6]. These RNN based algorithms do not employ a conventional HMM that needs a large search space. However, neural network algorithms, including RNNs, demand a very large number of arithmetic operations, thus they are mostly implemented using GPUs [9], [10].\nIn this work, a low-power real-time speech recognition (SR) system is developed using an FPGA. The developed system employs two long-short term memory (LSTM) RNNs [11];\none for acoustic modeling and the other for character-level language modeling. A statistical word-level LM is also used to further improve the recognition performance. The overall algorithm is shown in Fig. 1. The information generated from the RNNs and the word-level LM is combined using a tree structured N -best beam search algorithm. The beam search employing the beam width of 128 only requires about 197 KB of data structure, while the conventional HMM based network demands a few hundred megabytes of memory. The SR system employs a unidirectional RNN based acoustic model, causing a slight disadvantage in the recognition performance when compared to a bidirectional one, but is more appropriate for online real-time applications where immediate reaction to utterance is desired.\nThe RNNs for acoustic modeling and character-level LM are implemented on a mid-sized FPGA, Xilinx XC7Z045, which contains 2.18 MB on-chip memory. To store all the weights of the RNNs in the on-chip memory, the weights are quantized to 6 bits using the retraining based fixed-point optimization algorithm [12]. The RNN for the character-level LM stores 128 contexts in the on-chip memory, where each context is assigned to each beam in the N -best search. All of the weights and the contexts are stored in the on-chip memory of the FPGA, and thus the RNNs do not need DRAM accesses which require a large amount of energy [13], [14]. As a result, this speech recognition system only uses DRAM accesses for tri-gram based language modeling, and consumes very small power compared to GPU based systems or other off-chip memory based architectures. The RNNs in the FPGA are implemented using highly parallel arithmetic arrays.\nThe paper is organized as follows. In Section II, recent related works are revisited. Section III describes the implemented SR algorithm. The FPGA based implementation of the algorithm is shown in Section IV. The system is evaluated in Section V. Concluding remarks are in Section VI."}, {"heading": "II. RELATED WORKS", "text": ""}, {"heading": "A. Large Vocabulary Continuous Speech Recognition", "text": "Most state-of-the-art large vocabulary continuous speech recognition (LVCSR) systems employ a DNN-HMM hybrid acoustic model [3] or a weighted finite state transducer (WFST) decoder [2]. The WFST network is composed by integrating the HMM acoustic model, a pronunciation lexicon model, and a word-level n-gram back-off language model.\nar X\niv :1\n61 0.\n00 55\n2v 1\n[ cs\n.C L\n] 3\n0 Se\np 20\n16\nTherefore, the resulting decoding network becomes huge, which is usually over a few hundred megabytes [4], and hinders small-footprint low-power implementations.\nA traditional LVCSR performs Viterbi decoding [15] on the WFST network using senone-level likelihoods computed by the acoustic model. Efficient hardware based implementation of the LVCSR [16] is difficult because of the large amount of search operations needed for Viterbi decoding. Specifically, the network cannot be embedded in the on-chip memory due to its size and is usually stored on an off-chip DRAM module. The energy cost of a DRAM access is large since static power is required to keep the I/O active and data must travel a long distance [13]. Therefore, the decoding procedure on WFST using DRAM consumes a large amount of power.\nRecently, several RNN based end-to-end speech recognizers have been developed [17], [9], [10]. A phoneme-level CTCtrained RNN for acoustic modeling can reduce the size of a WFST network to about a half of that needed for DNNHMM hybrid models [10]. Also, character-level RNN language models and prefix beam search decoding greatly reduce the complexity of the decoding stage [5], [6]. Especially, a tree-based online decoding algorithm is proposed for lowlatency speech recognition [6]."}, {"heading": "B. FPGA-Based Neural Network Implementation", "text": "Neural networks demand many multiply and add operations, but they are hardware-friendly in nature due to their massive parallelism. However, many previous implementations store the network parameters on an external DRAM, since the networks usually demand more than millions of parameters. Note that the weights for fully connected layers or recurrent neural networks are used only once when fetched, thus their accesses show very low temporal locality. There have been efforts to reduce the size of parameters by quantization. The bit-width of DNNs can be reduced to only two bits by retraining the quantized parameters with a modified backpropagation algorithm [12]. This approach was successfully applied to CNNs and RNNs [18], [19]. RNNs also demand a large number of parameters. Thus, it is helpful to quantize the parameters in low bits. A study on weight quantization of RNNs was presented in [19]. The retrain-based quantization method led to an efficient VLSI implementation of DNNs that store all the quantized parameters on the on-chip SRAM [20]. Also, a similar architecture was employed for a DNN implementation on an FPGA [21]."}, {"heading": "III. SPEECH RECOGNITION WITHOUT HMM", "text": ""}, {"heading": "A. Algorithm Overview", "text": "The speech recognition algorithm implemented in this paper consists of an RNN for acoustic modeling (AM), an RNN for character-level LM and a statistical word-level LM as illustrated in Fig. 1. The RNN AM employs the online CTC algorithm [22] and generates the probabilities of characters by analyzing each frame of input utterance. The character-level RNN LM outputs the probabilities of the following characters, while the statistical word-level tri-gram back-off LM shows\nthat of the following words. The information generated from these three modules are integrated to find the best hypothesis using an N -best search algorithm.\nThe acoustic model has a deep LSTM network structure and is end-to-end trained with online CTC algorithm [22]. Although some recent RNN-based end-to-end speech recognition algorithms [17], [9], [10] employ the bidirectional structure for recognition performance improvement, we use a unidirectional structure for real-time operation, where it is not allowed to access the future contexts.\nThe proposed SR system also employs a deep unidirectional LSTM RNN for character-level LM [23]. Since the character-level LM does not utilize any lexicon information, it can dictate out of vocabulary (OOV) words but is slightly disadvantaged in recognizing vocabularies in the dictionary. When compared to widely used HMM or RNN based speech recognition algorithms, the implemented one has the capability of low-latency decoding and OOV dictation, but these characteristics also mean slight weakness in the recognition accuracy. The structures of the RNNs for the AM and character-level LM are described in [6].\nIn our work, conventional statistical tri-gram back-off model is also employed for the word-level LM to complement the RNN based character-level LM. For better backing-off, we use improved Kneser-Ney smoothing [24]. The word-level LM is integrated for the N -best beam search in a similar manner as the character-level LM [6], except that the rescoring is performed on the fly, only when the active node represents a blank or the end of sentence (EOS) symbol. Also, the word insertion bonus is considered when the word-level LM is applied. Note that the number of DRAM accesses for the word-level LM is not very large."}, {"heading": "B. Beam Search Algorithm", "text": "In this work, the beam search decoding is conducted with a simple prefix tree structure. The N -best hypotheses are generated using the RNN AM and the RNN for characterlevel LM, and rescored by the statistical word-level LM on the fly.\nLet L be the set of all output labels in the RNN AM except for the CTC blank. The input feature vector from time 1 to t is denoted as x1:t. Given x1:t, the goal of the beam search decoding is to find the label sequence with the maximum posterior probability generated by the RNN AM.\nThe hypotheses are represented by a simple tree, where each node in the tree represents labels in L. To deal with CTC state\ntransitions, state-based networks that are represented with CTC states, L\u2032 = L\u222a{CTC blank}, are employed in low level by decomposing a tree node into two CTC states; a state that corresponds to a label in L and a following state that represents the CTC blank label.\nSince the tree grows indefinitely as the beam search proceeds, it is necessary to prune the search tree periodically. The tree is pruned both in depth and width as explained in [6]."}, {"heading": "C. Retraining Based Fixed-Point Optimization", "text": "Since the LSTM RNN contains millions of weights, an FPGA based implementation demands large on-chip memory space to store the parameters. It is not efficient to store the weights on the external DRAM because the fetched weights are used only once for each output computation. In our implementation, the retraining based method [12], [19] is applied to reduce the word-length of weights. The algorithm groups the weights and signals by layer, applies direct quantization to each group, and retrains the whole network in the quantized domain. In our work, the weights and the internal signals are quantized to 6 and 8 bits, respectively. We find that the internal LSTM cells demand high precision, and thus, they are represented in 16 bits."}, {"heading": "IV. FPGA-BASED IMPLEMENTATION", "text": ""}, {"heading": "A. Overview of the FPGA System", "text": "The proposed algorithm is implemented on a Xilinx ZC706 evaluation board that equips an XC7Z045 FPGA. The FPGA embeds an ARM CPU in addition to configurable logic circuits. Fig. 2 shows the hardware architecture for implementing RNNs. Although the SR algorithm employs two RNN algorithms, our FPGA design implements only one LSTM tile and one output tile, which operate intermittently when the control signal is given. Note that the RNN operation for the acoustic model is needed only once for each input speech frame whose length is normally 10-ms, but the characterlevel LM operates much more frequently to generate N -best hypotheses for different search paths."}, {"heading": "B. Architecture and Algorithm", "text": "The standard LSTM with peephole connections is described in Algorithm 1. The equations show that one LSTM RNN layer requires eight matrix-vector multiplications in each time step.\nThe LSTM tile in Fig. 3 consists of two main processing modules; the processing element (PE) array calculates matrixvector multiplications and the LSTM extra processing unit\nAlgorithm 1 LSTM equations with peephole connections: x is the input vector of the input layer, h is the output vector of the layer. The vector i, f and o are activations of the input gate, forget gate and the output gate processed by the logistic sigmoid function \u03c3, respectively. c represents the activation of the cell and c\u0303t is the candidate memory cell. The vector b stands for the bias. The subscript t is the current data where t \u2212 1 denotes the data from the previous time step. W is the model parameter matrix and W\u0303 is the diagonal model parameter matrix. The operator is an element-wise multiplication, and tanh is a hyperbolic tangent.\nit = \u03c3(Wxi xt +Whi ht\u22121 + W\u0303ci ct\u22121 + bi) ft = \u03c3(Wxf xt +Whf ht\u22121 + W\u0303cf ct\u22121 + bf ) ot = \u03c3(Wxo xt +Who ht\u22121 + W\u0303co ct + bo) c\u0303t = tanh(Wxc xt +Whc ht\u22121 + bc) ct = ft ct\u22121 + it c\u0303t ht = ot tanh(ct)\n(LSTM EPU) conducts the rest of the calculations, such as applying element-wise products for peephole connections and evaluating activation functions.\nAs shown in Fig. 4, the PE array consists of 512 PEs. The PE in Fig. 5 multiplies the input Din with the weight W and adds the result with the partial sum stored in the accumulator where the bias values are preloaded [21]. The results of eight matrix-vector multiplications are stacked in the PE output buffer. We use four PE buffers, PEi, PEf , PEo and PEc.\nThe LSTM EPU shown in Fig. 6 is implemented to manage the rest of the LSTM operations. The input ct\u22121 represents the cell activation of the previous time step.\nTo implement the peephole connections in the LSTM, ct\u22121 is multiplied with the peephole weights and added to PEi and PEf while ct is multiplied with the weights and added to PEo. Since the matrix-vector multiplication results are already stored in the PE buffers, the LSTM EPU and the PE array can operate independently. The activation functions in the LSTM EPU are implemented using lookup tables. In the proposed system, only one LSTM EPU is used because one output data is transmitted in each clock and all the operations in the LSTM EPU are element-wise ones.\nThe output vector of the LSTM EPU is stored in the context memory. The stored contexts are used in the following\noperations and the beam search decoding. The number of stored contexts is the same as that of hypotheses in the beam search.\nThe output tile is a fully connected layer that employs the same structure in [21]. The input of the output tile is the data stored in the context memory."}, {"heading": "C. Throughput of the LSTM tile", "text": "As shown in Fig. 4, there are two PE arrays in the PE array block. Since there are eight matrix-vector multiplications, one RNN layer demands four matrix-vector multiplication cycles. Each PE array has 256 PEs and conducts a matrix-vector multiplication using the outer product method. The processing time of the LSTM depends on the dimension of the input vector because the outer product method supplies one input element at each clock. The input size of the first level RNN AM is 123 and that of the next layers is 256. Thus, the first layer processing of the RNN AM requires 246 (= 123\u00d74\u00f72) and 512 (= 256\u00d74\u00f72) clock cycles to conduct matrix-vector multiplications related with xt and ht\u22121. The number of clock cycles for the next layer is 1,024. Note that there exists a small overhead to synchronize the system. The number of required clock cycles to process the RNN AM with three LSTM layers is 2,806 (= 758 + 1, 024 + 1, 024) and that of the RNN LM containing two LSTM layers is 1,596 (= 572 + 1, 024), respectively."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": ""}, {"heading": "A. Recognition Performance", "text": "To train the RNN AM, we use the standard WSJ SI-284 training set. The utterances with verbalized punctuations are removed and odd transcriptions are filtered out. The final size of the training set is roughly 71 hours. For evaluation, the WSJ eval92 (Nov\u201992 20k evaluation set) is used. The utterances\nin the evaluation set are sequentially concatenated to generate a single 42-minute input speech stream.\nThe RNN AM is trained using the stochastic gradient descent (SGD) with 8 parallel input streams on a GPU [25].\nThe RNN AM uses a 40-dimensional log mel frequency filterbank feature with energy and their delta and double-delta, resulting in a 123-dimensional vector. The feature vector is computed every 10 ms over a 25 ms Hamming window and element-wisely normalized based on the statistics obtained from the training set. A centered sliding-window with 300- frame size is used to reduce the amplitude distortion effect from silence intervals. The RNN AM outputs a 31-dimensional vector representing the probabilities of 26 upper case alphabet characters, 3 special characters for punctuation marks, the end of sentence symbol, and the CTC blank label.\nThe RNN LM is trained with a text stream generated by concatenating randomly selected sentences in the WSJ nonverbalized punctuation text corpus where the EOS label is inserted between the sentences. The RNN LM is trained with AdaDelta [26] based SGD. The RNN LM uses a 30- dimensional vector where the current character-label is one-hot encoded and outputs a 30-dimensional vector which represents the probabilities of the following character-labels.\nThe statistical tri-gram LM is generated with the IRSTLM [27] toolkit included in the KALDI speech recognition tool [28]. build-lm.sh and compile-lm in IRSTLM toolkit is used to generate a standard advanced research project agency (ARPA) file while applying the improved Kneser-Ney method [24] for higher performance. We use the WSJ nonverbalized punctuation text corpus that contains 165 K words to build the LM. The generated 578-MB ARPA file is stored in the off-chip DRAM.\nThe word error rate (WER) and character error rate (CER) performances of the proposed system with respect to the size of the RNNs and the beam width are shown in TABLE I. The small-model represents the system with 3\u00d7256 RNN AM and 2\u00d7256 RNN LM while the large-model employs 4\u00d7512 RNN AM and 2\u00d7512 RNN LM. The table shows that the performance improves when the beam width or the network size increases. Also, combining the word-level LM improves the performance especially when the network size is small.\nThe best floating-point performance of our algorithm in TABLE I shows the WER of 8.79 % which is higher than the state of the art result, 7.34 % [10], but ours supports delay free real-time SR. Of course, the best advantage we expect is the energy efficiency since we do not employ a WFST network which demands a large amount of computation and memory accesses. Note that the algorithm in [10] is not for real-time speech recognition task, and employs a bidirectional structure that shows better performance over the unidirectional structure. The algorithm also uses the WFST decoding network to combine the results of acoustic modeling, lexicon, and the word-level LM. Note that the compared system does not use the character-level RNN because the WFST network embeds the lexicon. However, the WFST-based decoding demands a large memory space to search, and thus the algorithm is hard to be power efficient. On the other hand, our algorithm employs the character-level LM in addition to the word-level LM, and uses simple beam-search in decoding that requires far less memory. The RNNs of the proposed algorithm are implemented using only on-chip memory for energy efficiency. Note that the recognition performance of our system can be further improved by employing larger RNNs or increasing the beam width.\nThe SR algorithm is implemented on an XC7Z045 FPGA that has 2.18 MB on-chip memory. In our experiment, the number of parameters for the small-model is 2.3 M while that of the large-model is 15.1 M. The retraining based fixed-point optimization is applied to reduce the precision of weights. TABLE II shows the performance of the systems that employ fixed-point weights, where the precision of the signal and the LSTM cells are fixed to 8 and 16 bits, respectively. The table shows that rescoring with the word-level LM is also effective for the systems that employ fixed-point weights. The FPGA can only accommodate up to 6-bit weights, which demands\nonly 1/5 of the memory space required for floating-point implementations with about 1.5% WER increase. The size of the parameters with 6-bit precision is about 1.1 MB, which can be stored in the on-chip memory of Xilinx XC7Z045."}, {"heading": "B. FPGA Implementation Performance", "text": "The FPGA implements the small-model with the beam width of 128. Note that the large-model based system can be implemented using an ultra-scale FPGA [29]. In our implementation, the programmable hardware operates at 100- MHz and the CPU runs with a 800-MHz clock to conduct the N -best search. The FPGA resource utilization result is shown in TABLE III.\nThe implemented system requires one RNN AM operation for each 10 ms speech frame (100 times per second). However, the RNN for character-level LM is needed only when character transition occurs, whose frequency is usually no more than 30 times per second in our experiments. Assuming 128 beams, this translates about 3,840 RNN LM operations per second. Thus, the number of clock cycles for achieving a real time with conservative estimation is about 6.4 M (= 100\u00d72, 806+ 3, 840\u00d7 1, 596) per second. Note that silence period does not generate any transition, thus no RNN LM is demanded.\nTABLE IV shows the power consumption measured by the Xilinx simulation tool. The actual power consumption of the small-model based SR measured on the evaluation board is 9.24 W including that in the DRAM and peripherals, while achieving \u00d7 4.12 real-time speed. Our implementation consumes some extra cycles for communication.\nWe compare our FPGA implementation with that of a highend GPU, NVIDIA GeForce Titan X. In the GPU based implementation, the time to evaluate the 42-minute WSJ eval92\nevaluation set is 12.5 minute, which means \u00d73.36 real-time speed, while utilizing about 30 % of GPU resource. Note that the throughput of the GPU can be increased by processing multiple input speech utterances. However, our FPGA based system shows better recognition speed by efficiently utilizing hardware resources even when processing a single speech stream. The power consumption of the GPU based system is about 80 W which is much higher than ours."}, {"heading": "VI. CONCLUDING REMARKS", "text": "In this paper, an RNN based real-time speech recognition system is implemented on an FPGA. The algorithm employs the RNNs for acoustic modeling and character-level language modeling, and is optimized for real-time operations using unidirectional RNNs. The vocabulary size of the speech recognition is unlimited since the character-level RNN can dictate out of vocabulary words. A statistical word-level language model is also employed to improve the recognition performance. The models are integrated using a simple tree-based search algorithm without employing a hidden Markov model or weighted finite state transducers. The weights of the RNNs are quantized to 6 bits. The RNNs are implemented using an array of processing elements for high throughput matrix-vector multiplications. The RNNs implemented on the FPGA only use on-chip memory. The implemented speech recognition system on Xilinx XC7Z045 can achieve approximately 4.12 times of the real-time speed when 100 MHz clock is used while consuming only 9.24 W of power. When compared to a high-end GPU based system, the power efficiency is considered about 10 times higher."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported in part by the Brain Korea 21 Plus Project and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No. 2015R1A2A1A10056051)."}], "references": [{"title": "Foreword By-Reddy, Spoken language processing: A guide to theory, algorithm, and system development", "author": ["X. Huang", "A. Acero", "H.-W. Hon"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Weighted finite-state transducers in speech recognition", "author": ["M. Mohri", "F. Pereira", "M. Riley"], "venue": "Computer Speech & Language, vol. 16, no. 1, pp. 69\u201388, 2002.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Parallel scalability in speech recognition", "author": ["K. You", "J. Chong", "Y. Yi", "E. Gonina", "C.J. Hughes", "Y.-K. Chen", "W. Sung", "K. Keutzer"], "venue": "IEEE Signal Processing Magazine, vol. 26, no. 6, pp. 124\u2013135, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Lexicon-free conversational speech recognition with neural networks", "author": ["A.L. Maas", "Z. Xie", "D. Jurafsky", "A.Y. Ng"], "venue": "The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT), 2015, pp. 345\u2013354.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Character-level incremental speech recognition with recurrent neural networks", "author": ["K. Hwang", "W. Sung"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 5335\u2013 5339.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Connectionist Temporal Classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "International Conference on Machine Learning (ICML), 2006, pp. 369\u2013376.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "From feedforward to recurrent LSTM neural networks for language modeling", "author": ["M. Sundermeyer", "H. Ney", "R. Schl\u00fcter"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), vol. 23, no. 3, pp. 517\u2013529, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos"], "venue": "International Conference on Machine Learning (ICML), 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "EESEN: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Y. Miao", "M. Gowayyed", "F. Metze"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015, pp. 167\u2013174.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Fixed-point feedforward deep neural network design using weights +1, 0, and -1", "author": ["K. Hwang", "W. Sung"], "venue": "IEEE Workshop on Signal Processing Systems (SiPS), 2014, pp. 1\u20136.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "1.1 computing\u2019s energy problem (and what we can do about it)", "author": ["M. Horowitz"], "venue": "IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC), 2014, pp. 10\u201314.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "EIE: efficient inference engine on compressed deep neural network", "author": ["S. Han", "X. Liu", "H. Mao", "J. Pu", "A. Pedram", "M.A. Horowitz", "W.J. Dally"], "venue": "arXiv preprint arXiv:1602.01528, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "The Viterbi algorithm", "author": ["G.D. Forney Jr"], "venue": "Proceedings of the IEEE, vol. 61, no. 3, pp. 268\u2013278, 1973.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1973}, {"title": "An FPGA implementation of speech recognition with weighted finite state transducers", "author": ["J. Choi", "K. You", "W. Sung"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2010, pp. 1602\u20131605.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "International Conference on Machine Learning (ICML), 2014, pp. 1764\u20131772.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["S. Anwar", "K. Hwang", "W. Sung"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 1131\u20131135.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Fixed-point performance analysis of recurrent neural networks", "author": ["S. Shin", "K. Hwang", "W. Sung"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 976\u2013980.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "X1000 real-time phoneme recognition VLSI using feed-forward deep neural networks", "author": ["J. Kim", "K. Hwang", "W. Sung"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2014, pp. 7510\u20137514.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Fpga based implementation of deep neural networks using on-chip memory only", "author": ["J. Park", "W. Sung"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 1011\u2013 1015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence to sequence training of ctc-rnns with partial windowing", "author": ["K. Hwang", "W. Sung"], "venue": "International Conference on Machine Learning (ICML), 2016, pp. 2178\u20132187.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G.E. Hinton"], "venue": "International Conference on Machine Learning (ICML), 2011, pp. 1017\u20131024.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Improved backing-off for m-gram language modeling", "author": ["R. Kneser", "H. Ney"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), vol. 1, 1995, pp. 181\u2013184.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "Single stream parallelization of generalized LSTM-like RNNs on a GPU", "author": ["K. Hwang", "W. Sung"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 1047\u2013 1051.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "IRSTLM: an open source toolkit for handling large scale language models.", "author": ["M. Federico", "N. Bertoldi", "M. Cettolo"], "venue": "in Interspeech,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "IEEE workshop on automatic speech recognition and understanding, no. EPFL-CONF-192584, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1925}, {"title": "Xilinx ultrascale architecture for high-performance, smarter systems", "author": ["N. Mehta"], "venue": "Xilinx White Paper WP434, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Speech recognition has long been studied, and most of the algorithms employ hidden Markov models (HMMs) or its variants as inference and information combining tools [1], [2].", "startOffset": 165, "endOffset": 168}, {"referenceID": 1, "context": "Speech recognition has long been studied, and most of the algorithms employ hidden Markov models (HMMs) or its variants as inference and information combining tools [1], [2].", "startOffset": 170, "endOffset": 173}, {"referenceID": 2, "context": "Recently, deep neural networks are employed for acoustic modeling (AM) of state of the art speech recognition systems which, however, are not free from the HMM [3].", "startOffset": 160, "endOffset": 163}, {"referenceID": 3, "context": "HMM modeling for speech recognition demands a vast amount of memory access operations on a large size network, whose memory capacity usually exceeds a few hundred megabytes [4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 4, "context": "Recently, fully neural recurrent network based speech recognition algorithms are actively investigated [5], [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 5, "context": "Recently, fully neural recurrent network based speech recognition algorithms are actively investigated [5], [6].", "startOffset": 108, "endOffset": 111}, {"referenceID": 6, "context": "The RNN is end-to-end trained with connectionist temporal classification (CTC) [7] to directly transcribe the input utterance to characters.", "startOffset": 79, "endOffset": 82}, {"referenceID": 7, "context": "The RNN has also been used for language modeling (LM), which shows much better capability than tri-gram based statistical algorithms [8].", "startOffset": 133, "endOffset": 136}, {"referenceID": 4, "context": "Recently, complete speech recognition algorithms have been developed by combining the CTC RNN and the RNN LM [5], [6].", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "Recently, complete speech recognition algorithms have been developed by combining the CTC RNN and the RNN LM [5], [6].", "startOffset": 114, "endOffset": 117}, {"referenceID": 8, "context": "However, neural network algorithms, including RNNs, demand a very large number of arithmetic operations, thus they are mostly implemented using GPUs [9], [10].", "startOffset": 149, "endOffset": 152}, {"referenceID": 9, "context": "However, neural network algorithms, including RNNs, demand a very large number of arithmetic operations, thus they are mostly implemented using GPUs [9], [10].", "startOffset": 154, "endOffset": 158}, {"referenceID": 10, "context": "The developed system employs two long-short term memory (LSTM) RNNs [11]; one for acoustic modeling and the other for character-level language modeling.", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "To store all the weights of the RNNs in the on-chip memory, the weights are quantized to 6 bits using the retraining based fixed-point optimization algorithm [12].", "startOffset": 158, "endOffset": 162}, {"referenceID": 12, "context": "All of the weights and the contexts are stored in the on-chip memory of the FPGA, and thus the RNNs do not need DRAM accesses which require a large amount of energy [13], [14].", "startOffset": 165, "endOffset": 169}, {"referenceID": 13, "context": "All of the weights and the contexts are stored in the on-chip memory of the FPGA, and thus the RNNs do not need DRAM accesses which require a large amount of energy [13], [14].", "startOffset": 171, "endOffset": 175}, {"referenceID": 2, "context": "Most state-of-the-art large vocabulary continuous speech recognition (LVCSR) systems employ a DNN-HMM hybrid acoustic model [3] or a weighted finite state transducer (WFST) decoder [2].", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "Most state-of-the-art large vocabulary continuous speech recognition (LVCSR) systems employ a DNN-HMM hybrid acoustic model [3] or a weighted finite state transducer (WFST) decoder [2].", "startOffset": 181, "endOffset": 184}, {"referenceID": 3, "context": "Therefore, the resulting decoding network becomes huge, which is usually over a few hundred megabytes [4], and hinders small-footprint low-power implementations.", "startOffset": 102, "endOffset": 105}, {"referenceID": 14, "context": "A traditional LVCSR performs Viterbi decoding [15] on the WFST network using senone-level likelihoods computed by the acoustic model.", "startOffset": 46, "endOffset": 50}, {"referenceID": 15, "context": "Efficient hardware based implementation of the LVCSR [16] is difficult because of the large amount of search operations needed for Viterbi decoding.", "startOffset": 53, "endOffset": 57}, {"referenceID": 12, "context": "The energy cost of a DRAM access is large since static power is required to keep the I/O active and data must travel a long distance [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 16, "context": "Recently, several RNN based end-to-end speech recognizers have been developed [17], [9], [10].", "startOffset": 78, "endOffset": 82}, {"referenceID": 8, "context": "Recently, several RNN based end-to-end speech recognizers have been developed [17], [9], [10].", "startOffset": 84, "endOffset": 87}, {"referenceID": 9, "context": "Recently, several RNN based end-to-end speech recognizers have been developed [17], [9], [10].", "startOffset": 89, "endOffset": 93}, {"referenceID": 9, "context": "A phoneme-level CTCtrained RNN for acoustic modeling can reduce the size of a WFST network to about a half of that needed for DNNHMM hybrid models [10].", "startOffset": 147, "endOffset": 151}, {"referenceID": 4, "context": "Also, character-level RNN language models and prefix beam search decoding greatly reduce the complexity of the decoding stage [5], [6].", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "Also, character-level RNN language models and prefix beam search decoding greatly reduce the complexity of the decoding stage [5], [6].", "startOffset": 131, "endOffset": 134}, {"referenceID": 5, "context": "Especially, a tree-based online decoding algorithm is proposed for lowlatency speech recognition [6].", "startOffset": 97, "endOffset": 100}, {"referenceID": 11, "context": "The bit-width of DNNs can be reduced to only two bits by retraining the quantized parameters with a modified backpropagation algorithm [12].", "startOffset": 135, "endOffset": 139}, {"referenceID": 17, "context": "This approach was successfully applied to CNNs and RNNs [18], [19].", "startOffset": 56, "endOffset": 60}, {"referenceID": 18, "context": "This approach was successfully applied to CNNs and RNNs [18], [19].", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "A study on weight quantization of RNNs was presented in [19].", "startOffset": 56, "endOffset": 60}, {"referenceID": 19, "context": "The retrain-based quantization method led to an efficient VLSI implementation of DNNs that store all the quantized parameters on the on-chip SRAM [20].", "startOffset": 146, "endOffset": 150}, {"referenceID": 20, "context": "Also, a similar architecture was employed for a DNN implementation on an FPGA [21].", "startOffset": 78, "endOffset": 82}, {"referenceID": 21, "context": "The RNN AM employs the online CTC algorithm [22] and generates the probabilities of characters by analyzing each frame of input utterance.", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "The acoustic model has a deep LSTM network structure and is end-to-end trained with online CTC algorithm [22].", "startOffset": 105, "endOffset": 109}, {"referenceID": 16, "context": "Although some recent RNN-based end-to-end speech recognition algorithms [17], [9], [10] employ the bidirectional structure for recognition performance improvement, we use a unidirectional structure for real-time operation, where it is not allowed to access the future contexts.", "startOffset": 72, "endOffset": 76}, {"referenceID": 8, "context": "Although some recent RNN-based end-to-end speech recognition algorithms [17], [9], [10] employ the bidirectional structure for recognition performance improvement, we use a unidirectional structure for real-time operation, where it is not allowed to access the future contexts.", "startOffset": 78, "endOffset": 81}, {"referenceID": 9, "context": "Although some recent RNN-based end-to-end speech recognition algorithms [17], [9], [10] employ the bidirectional structure for recognition performance improvement, we use a unidirectional structure for real-time operation, where it is not allowed to access the future contexts.", "startOffset": 83, "endOffset": 87}, {"referenceID": 22, "context": "The proposed SR system also employs a deep unidirectional LSTM RNN for character-level LM [23].", "startOffset": 90, "endOffset": 94}, {"referenceID": 5, "context": "The structures of the RNNs for the AM and character-level LM are described in [6].", "startOffset": 78, "endOffset": 81}, {"referenceID": 23, "context": "For better backing-off, we use improved Kneser-Ney smoothing [24].", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "The word-level LM is integrated for the N -best beam search in a similar manner as the character-level LM [6], except that the rescoring is performed on the fly, only when the active node represents a blank or the end of sentence (EOS) symbol.", "startOffset": 106, "endOffset": 109}, {"referenceID": 5, "context": "The tree is pruned both in depth and width as explained in [6].", "startOffset": 59, "endOffset": 62}, {"referenceID": 11, "context": "In our implementation, the retraining based method [12], [19] is applied to reduce the word-length of weights.", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "In our implementation, the retraining based method [12], [19] is applied to reduce the word-length of weights.", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": "5 multiplies the input Din with the weight W and adds the result with the partial sum stored in the accumulator where the bias values are preloaded [21].", "startOffset": 148, "endOffset": 152}, {"referenceID": 0, "context": "PE0[1] Din W Bias Dout", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "PE1[1] Din W Bias Dout", "startOffset": 3, "endOffset": 6}, {"referenceID": 20, "context": "The output tile is a fully connected layer that employs the same structure in [21].", "startOffset": 78, "endOffset": 82}, {"referenceID": 24, "context": "The RNN AM is trained using the stochastic gradient descent (SGD) with 8 parallel input streams on a GPU [25].", "startOffset": 105, "endOffset": 109}, {"referenceID": 25, "context": "The RNN LM is trained with AdaDelta [26] based SGD.", "startOffset": 36, "endOffset": 40}, {"referenceID": 26, "context": "The statistical tri-gram LM is generated with the IRSTLM [27] toolkit included in the KALDI speech recognition tool [28].", "startOffset": 57, "endOffset": 61}, {"referenceID": 27, "context": "The statistical tri-gram LM is generated with the IRSTLM [27] toolkit included in the KALDI speech recognition tool [28].", "startOffset": 116, "endOffset": 120}, {"referenceID": 23, "context": "sh and compile-lm in IRSTLM toolkit is used to generate a standard advanced research project agency (ARPA) file while applying the improved Kneser-Ney method [24] for higher performance.", "startOffset": 158, "endOffset": 162}, {"referenceID": 9, "context": "34 % [10], but ours supports delay free real-time SR.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "Note that the algorithm in [10] is not for real-time speech recognition task, and employs a bidirectional structure that shows better performance over the unidirectional structure.", "startOffset": 27, "endOffset": 31}, {"referenceID": 28, "context": "Note that the large-model based system can be implemented using an ultra-scale FPGA [29].", "startOffset": 84, "endOffset": 88}], "year": 2016, "abstractText": "In this paper, a neural network based real-time speech recognition (SR) system is developed using an FPGA for very low-power operation. The implemented system employs two recurrent neural networks (RNNs); one is a speech-tocharacter RNN for acoustic modeling (AM) and the other is for character-level language modeling (LM). The system also employs a statistical word-level LM to improve the recognition accuracy. The results of the AM, the character-level LM, and the word-level LM are combined using a fairly simple N -best search algorithm instead of the hidden Markov model (HMM) based network. The RNNs are implemented using massively parallel processing elements (PEs) for low latency and high throughput. The weights are quantized to 6 bits to store all of them in the on-chip memory of an FPGA. The proposed algorithm is implemented on a Xilinx XC7Z045, and the system can operate much faster than real-time.", "creator": "LaTeX with hyperref package"}}}