{"id": "1610.00634", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2016", "title": "Orthographic Syllable as basic unit for SMT between Related Languages", "abstract": "We explore the use of the orthographic syllable, a variable-length consonant-vowel sequence, as a basic unit of translation between related languages which use abugida or alphabetic scripts. We show that orthographic syllable level translation significantly outperforms models trained over other basic units (word, morpheme and character) when training over small parallel corpora. Although phonetic orthographic syllables are not considered a complete unit of translation, the total phonetic and orthographic syllable level translation is also superior over model training when training large parallel corpora. In addition, orthographic syllables are highly superior to model training when training large parallel corpora. In a second, orthographic syllables are slightly superior than model training when training large parallel corpora. Moreover, the orthographic syllables are much more detailed when training large parallel corpora. The results of our study demonstrate that orthographic syllables are superior to model training when training large parallel corpora. For example, the orthographic syllable level translation in the sub-par English language of the French was 5 times greater than for model training when training large parallel corpora. However, the orthographic syllable level translation was significantly superior to model training when training large parallel corpora. These results are consistent with prior research demonstrating that orthographic syllables are superior to model training when training large parallel corpora. For example, the orthographic syllable level translation in the sub-par English language of the French was 5 times greater than for model training when training large parallel corpora. The orthographic syllable level translation in the sub-par English language of the French was 5 times greater than for model training when training large parallel corpora. Similarly, the orthographic syllable level translation in the sub-par English language of the French was 5 times greater than for model training when training large parallel corpora. However, the orthographic syllable level translation in the sub-par English language of the French was 5 times greater than for model training when training large parallel corpora. Because orthographic syllables are often compared with model training when training large parallel corpora, it is possible to assess orthographic syllable level translation by assessing the relative magnitude of the relative magnitude of the degree of orthographic syllables. Because orthographic syllables are often compared with model training when training large parallel corpora, it is possible to assess orthographic syllable level translation by examining the relative magnitude of the orthographic syllables.\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 3 Oct 2016 16:53:10 GMT  (160kb)", "http://arxiv.org/abs/1610.00634v1", "7 pages, 1 figure, compiled with XeTex, to be published at the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016"]], "COMMENTS": "7 pages, 1 figure, compiled with XeTex, to be published at the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["anoop kunchukuttan", "pushpak bhattacharyya"], "accepted": true, "id": "1610.00634"}, "pdf": {"name": "1610.00634.pdf", "metadata": {"source": "CRF", "title": "Orthographic Syllable as basic unit for SMT between Related Languages", "authors": ["Anoop Kunchukuttan", "Pushpak Bhattacharyya"], "emails": ["anoopk@cse.iitb.ac.in", "pb@cse.iitb.ac.in"], "sections": [{"heading": null, "text": "We explore the use of the orthographic syllable, a variable-length consonant-vowel sequence, as a basic unit of translation between related languages which use abugida or alphabetic scripts. We show that orthographic syllable level translation significantly outperforms models trained over other basic units (word, morpheme and character) when training over small parallel corpora."}, {"heading": "1 Introduction", "text": "Related languages exhibit lexical and structural similarities on account of sharing a common ancestry (Indo-Aryan, Slavic languages) or being in prolonged contact for a long period of time (Indian subcontinent, Standard Average European linguistic areas) (Bhattacharyya et al., 2016). Translation between related languages is an important requirement due to substantial government, business and social communication among people speaking these languages. However, most of these languages have few parallel corpora resources, an important requirement for building good quality SMT systems. Modelling the lexical similarity among related languages is the key to building good-quality SMT systems with limited parallel corpora. Lexical similarity implies that the languages share many words with the similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages. Translation for such words can be\nachieved by sub-word level transformations. For instance, lexical similarity can be modelled in the standard SMT pipeline by transliteration of words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014). A different paradigm is to drop the notion of word boundary and consider the character n-gram as the basic unit of translation (Vilar et al., 2007; Tiedemann, 2009a). Such character-level SMT bas been explored for closely related languages likeBulgarian-Macedonian, Indonesian-Malaywith modest success, with the short context of unigrams being a limiting factor (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). In this work, we present a linguistically motivated, variable length unit of translation \u2014 orthographic syllable (OS)\u2014which provides more context for translation while limiting the number of basic units. The OS consists of one or more consonants followed by a vowel and is inspired from the akshara, a consonant-vowel unit, which is the fundamental organizing principle of Indic scripts (Sproat, 2003; Singh, 2006). It can be thought of as an approximate syllable with the onset and nucleus, but no coda. While true syllabification is hard, orthographic syllabification can be easily done. Atreya et al. (2016) and Ekbal et al. (2006) have shown that the OS is a useful unit for transliteration involving Indian languages. We show that orthographic syllable-level trans-\nlation significantly outperforms character-level and strong word-level and morpheme-level baselines over multiple related language pairs (Indian as well as others). Character-level approaches have been previously shown to work well for language pairs with high lexical similarity. Ourmajor finding is that OS-level translation outperforms other approaches even when the language pairs have relatively less lexical similarity or belong to different language families (but have sufficient contact relation)."}, {"heading": "2 Orthographic Syllabification", "text": "The orthographic syllable is a sequence of one or more consonants followed by a vowel, i.e a C+V unit. We describe briefly procedures for orthographic syllabification of Indian scripts and nonIndic alphabetic scripts. Orthographic syllabification cannot be done for languages using logographic and abjad scripts as these scripts do not have vowels.\nIndic Scripts: Indic scripts are abugida scripts, consisting of consonant-vowel sequences, with a consonant core (C+) and a dependent vowel (matra). If no vowel follows a consonant, an implicit schwa vowel [IPA: \u0259] is assumed. Suppression of schwa is indicated by the halanta character following a consonant. This script design makes for a straightforward syllabification process as shown in the following example. e.g. \u0932\u0915\u094d\u0937\u092e\u0940 ( lakShamI\nCV CCV CV\n) is\nsegmented as \u0932 \u0915\u094d\u0937 \u092e\u0940 (\nla kSha mI CV CCV CV\n) . There are two\nexceptions to this scheme: (i) Indic scripts distinguish between dependent vowels (vowel diacritics) and independent vowels, and the latter will constitute an OS on its own. e.g. \u092e\u0941\u092e\u094d\u092c\u0908 (mumbaI) \u2192 \u092e\u0941 \u092e\u094d\u092c \u0908 (mu mba I) (ii) The characters anusvaara and chandrabindu are part of the OS to the left if they represents nasalization of the vowel/consonant or start a new OS if they represent a nasal consonant. Their exact role is determined by the character following the anusvaara. See Appendix A for details.\nNon-Indic Alphabetic Scripts: We use a simpler method for the alphabetic scripts used in our experiments (Latin and Cyrillic). The OS is identified by a C+V+ sequence. e.g. lakshami\u2192la ksha mi, mumbai\u2192mu mbai. The OS could contains multiple terminal vowel characters representing long vowels (oo in cool) or diphthongs (ai inmumbai). A vowel start-\ning a word is considered to be an OS."}, {"heading": "3 Translation Models", "text": "We compared the orthographic syllable level model (O) with models based on other translation units that have been reported in previous work: word (W), morpheme (M), unigram (C) and trigram characters. Table 1 shows examples of these representations. The first step to build these translation systems is to transform sentences to the correct representation. Each word is segmented as the per the unit of representation, punctuations are retained and a special word boundary marker character (_) is introduced to indicate word boundaries as shown here:\nW: \u0930\u093e\u091c\u0942 , \u0918\u0930\u093e\u092c\u093e\u0939\u0947\u0930 \u091c\u093e\u090a \u0928\u0915\u094b . O: \u0930\u093e \u091c\u0942 _ , _ \u0918 \u0930\u093e \u092c\u093e \u0939\u0947 \u0930 _ \u091c\u093e \u090a _ \u0928 \u0915\u094b _ .\nFor all units of representation, we trained phrasebased SMT (PBSMT) systems. Since related languages have similar word order, we used distance based distortionmodel andmonotonic decoding. For character and orthographic syllable level models, we use higher order (10-gram) languages models since data sparsity is a lesser concern due to small vocabulary size (Vilar et al., 2007). As suggested by Nakov and Tiedemann (2012), we used word-level tuning for character and orthographic syllable level models by post-processing n-best lists in each tuning step to calculate the usual word-based BLEU score. While decoding, the word and morpheme level systems will not be able to translate OOV words. Since the languages involved share vocabulary, we transliterate the untranslated words resulting in the post-edited systems WX and MX corresponding to the systems W and M respectively. Following decoding, we used a simple method to regenerate words from sub-word level units: Since we represent word boundaries using a word boundary marker, we\nsimply concat the output units between consecutive occurrences of the marker character."}, {"heading": "4 Experimental Setup", "text": "Languages: Our experiments primarily concentrated on multiple language pairs from the two major language families of the Indian sub-continent (Indo-Aryan branch of Indo-European and Dravidian). These languages have been in contact for a long time, hence there are many lexical and grammatical similarities among them, leading to the subcontinent being considered a linguistic area (Emeneau, 1956). Specifically, there is overlap between the vocabulary of these languages to varying degrees due to cognates, language contact and loanwords from Sanskrit (throughout history) and English (in recent times). Table 2 lists the languages involved in the experiments and provides an indication of the lexical similarity between them in terms of the Longest Common Subsequence Ratio (LCSR) (Melamed, 1995) between the parallel training sentences at character level. All these language have a rich inflectional morphology with Dravidian languages, and Marathi and Konkani to some degree, being agglutinative. kok-mar and pan-hin have a high degree of lexical similarity.\nDataset: We used the multilingual ILCI corpus for our experiments (Jha, 2012), consisting of a modest number of sentences from tourism and health domains. The data split is as follows \u2013 training: 44,777, tuning 1K, test: 2K sentences. Language models for word-level systems were trained on the target side of training corpora plus monolingual corpora from various sources [hin: 10M (Bojar et al., 2014), tam: 1M (Ramasamy et al., 2012), mar: 1.8M (news websites), mal: 200K (Quasthoff et al., 2006) sentences]. We used the target language side of the\nparallel corpora for character, morpheme and OS level LMs.\nSystem details: PBSMT systems were trained using the Moses system (Koehn et al., 2007), with the grow-diag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram LMs withKneser-Ney smoothing for word andmorpheme level models and 10-gram LMs for character and OS level models. We used the BrahmiNet transliteration system (Kunchukuttan et al., 2015) for postediting, which is based on the transliteration Module in Moses (Durrani et al., 2014). We used unsupervised morphological segmenters trained with Morfessor (Virpioja et al., 2013) for obtaining morpheme representations. The unsupervised morphological segmenters were trained on the ILCI corpus and the Leipzig corpus (Quasthoff et al., 2006).The morph-segmenters and our implementation of orthographic syllabification are made available as part of the Indic NLP Library1.\nEvaluation: We use BLEU (Papineni et al., 2002) and Le-BLEU (Virpioja and Gr\u00f6nroos, 2015) for evaluation. Le-BLEU does fuzzy matches of words and hence is suitable for evaluating SMT systems that perform transformation at the sub-word level."}, {"heading": "5 Results and Discussion", "text": "This section discusses the results on Indian and nonIndian languages and cross-domain translation.\nComparison of Translation Units: Table 3 compares the BLEU scores for various translation systems. The orthographic syllable level system is clearly better than all other systems. It significantly outperforms the character-level system (by 46% on an average). The character-based system is competitive only for highly lexically similar language pairs like pan-hin and kok-mar. The system also outperforms two strong baselines which address data sparsity: (a) a word-level system with transliteration of OOV words (10% improvement), (b) amorph-level systemwith transliteration of OOV words (5% improvement). The OS-level representation is more beneficial when morphologically rich\n1http://anoopkunchukuttan.github.io/indic_ nlp_library\nlanguages are involved in translation. Significantly, OS-level translation is also the best system for translation between languages of different language families. The Le-BLEU scores also show the same trend as BLEU scores (See Appendix B). There are a very small number of untranslated OSes, which we handled by simple mapping of untranslated characters from source to target script. This barely increased translation accuracy (0.02% increase in BLEU score).\nWhy is OS better than other units?: The improved performance of OS level representation can be attributed to the following factors: One, the number of basic translation units is limited and small compared to word-level and\nmorpheme-level representations. For word-level representation, the number of translation units can increase with corpus size, especially for morphologically rich languages which leads to many OOVs. Thus, OS-level units address data sparsity. Two, while character level representation too does not suffer from data sparsity, we observe that the translation accuracy is highly correlated to lexical similarity (Table 4). The high correlation of character-level system and lexical similarity explains why character-level translation performs nearly as well other methods for language pairs which have high lexical similarity, but performs badly otherwise. On the other hand, the OSlevel representation has lesser correlation with lexical similarity and sits somewhere between characterlevel and word/morpheme level systems. Hence it is able to make generalizations beyond simple character level mappings. We observed that OS-level representation was able to correctly generate words whose translations are not cognate with the source language. This is an important property since function words and suffixes tend to be less similar lexically across languages. Can improved translation performance be explained by longer basic translation units? To verify this, we trained translation systemswith character trigrams as basic units. We chose trigrams since the average length of the OS was 3-5 characters for the languages we tested with. The translation accuracies were far less than even unigram representation. The number of unique basic units was about 8-10 times larger than orthographic syllables, thus making data sparsity an issue again. So, improved translation performance cannot be attributed to longer n-gram units alone.\nRobustness to Domain Change: We also tested the translation models trained on tourism & health domains on an agriculture domain test set of 1000 sentences. In this cross-domain translation scenario too, the OS level model outperforms most units of representation. The only exceptions are the pan-hin and tel-mal language pairs for the systemMX (accuracies of the OS-level system are within 10% of the MX system). Since the word leve model depends on coverage of the lexicon, it is highly domain dependent, whereas the sub-word units are not. So, even unigram-level models outperform word-level models in a cross-domain setting.\nExperiments with non-Indian languages: Table 6 shows the corpus statistics and our results for translation between some related non-Indic language pairs (Bulgarian-Macedonian, Danish-\nSwedish, Malay-Indonesian). OS level representation outperforms character and word level representation, though the gains are not as significant as Indic language pairs. This could be due to short length of sentences in training corpus [OPUS movie subtitles (Tiedemann, 2009b)] and high lexical similarity between the language pairs. Further experiments between less lexically related languages on general parallel corpora will be useful.\nEffect of training data size: For different training set sizes, we trained SMT systems with various representation units (Figure 1 shows the learning curves for two language pairs). OS level models are consistently better than character level models even for small training corpora. While the OSlevel models benefit from larger training corpora, the character-level models show only small benefits. For smaller corpora, the OS level model shows even larger performance gains over the corresponding word and morph level models. The performance gap between word/morph-level and OS-level shows a decrease with increase in training data."}, {"heading": "6 Conclusion & Future Work", "text": "We focus on the task of translation between related languages. This aspect of MT research is important to make available translation technologies to language pairs with limited parallel corpus, but huge potential translation requirements. We propose the use of the orthographic syllable, a variablelength, linguistically motivated, approximate syllable, as a basic unit for translation between related languages. We show that it significantly outperforms other units of representation, over multiple language pairs, spanning different language families, with varying degrees of lexical similarity and is robust to domain changes too. This opens up the possibility of further exploration of sub-word level translation units e.g. segments learnt using byte pair encoding (Sennrich et al., 2016)."}, {"heading": "Acknowledgments", "text": "We thank Arjun Atreya for inputs regarding orthographic syllables. We thank the Technology Development for Indian Languages (TDIL) Programme and the Department of Electronics & Information Technology, Govt. of India for their support."}, {"heading": "A. Pseudo-code for Orthographic Syllabification of Indic Scripts", "text": "Aminimal pseudo-code for orthographic syllabification of Indic scripts is shown in Algorithm 1 ."}, {"heading": "B. Le-BLEU Scores", "text": "In this appendix, we report and briefly analyse the Le-BLEU scores for all the experiments. Tables 7 shows the Le-BLEU scores for Indian language pairs. Table 8 shows the Le-BLEU scores for crossdomain translation. We see that the Le-BLEU score also clearly shows that the OS level representation is better than other representations. Especially in the cross-domain scenario, compared to the BLEU scores the Le-BLEU scores clearly indicate that superiority of the OS level representation.\nW WX M MX C O\nben-hin 67.63 71.03 70.77 71.18 67.22 71.66 pan-hin 86.81 90.13 89.95 90.34 90.45 90.66 kok-mar 63.97 63.74 65.95 65.85 63.22 67.05\nmal-tam 31.87 40.72 40.92 43.43 31.11 44.19 tel-mal 31.35 37.92 38.33 39.34 34.62 43.79\nhin-mal 40.48 43.89 43.55 44.05 32.41 46.65\nmal-hin 46.03 52.28 52.8 54.17 44.38 55.5"}], "references": [{"title": "Value the vowels: Optimal transliteration unit selection for machine", "author": ["Arjun Atreya", "Swapnil Chaudhari", "Pushpak Bhattacharyya", "Ganesh Ramakrishnan."], "venue": "InUnpublished, private communication with authors.", "citeRegEx": "Atreya et al\\.,? 2016", "shortCiteRegEx": "Atreya et al\\.", "year": 2016}, {"title": "Statistical machine translation between related languages", "author": ["Pushpak Bhattacharyya", "Mitesh Khapra", "Anoop Kunchukuttan."], "venue": "NAACL Tutorials.", "citeRegEx": "Bhattacharyya et al\\.,? 2016", "shortCiteRegEx": "Bhattacharyya et al\\.", "year": 2016}, {"title": "HindEnCorp \u2013 Hindi-English and Hindi-only Corpus for Machine Translation", "author": ["Ond\u0159ej Bojar", "Vojt\u011bch Diatka", "Pavel Rychl\u00fd", "Pavel Stra\u0148\u00e1k", "V\u00edt Suchomel", "Ale\u0161 Tamchyna", "Daniel Zeman."], "venue": "Proceedings of the 9th International Conference on Lan-", "citeRegEx": "Bojar et al\\.,? 2014", "shortCiteRegEx": "Bojar et al\\.", "year": 2014}, {"title": "Batch tuning strategies for statistical machine translation", "author": ["Colin Cherry", "George Foster."], "venue": "Proceedings of the 2012Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "Cherry and Foster.,? 2012", "shortCiteRegEx": "Cherry and Foster.", "year": 2012}, {"title": "Hindi-to-Urdu machine translation through transliteration", "author": ["Nadir Durrani", "Hassan Sajjad", "Alexander Fraser", "Helmut Schmid."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Durrani et al\\.,? 2010", "shortCiteRegEx": "Durrani et al\\.", "year": 2010}, {"title": "Integrating an unsupervised transliteration model into Statistical Machine Translation", "author": ["Nadir Durrani", "Hieu Hoang", "Philipp Koehn", "Hassan Sajjad."], "venue": "EACL 2014.", "citeRegEx": "Durrani et al\\.,? 2014", "shortCiteRegEx": "Durrani et al\\.", "year": 2014}, {"title": "A modified joint source-channel model for transliteration", "author": ["Asif Ekbal", "Sudip Kumar Naskar", "Sivaji Bandyopadhyay."], "venue": "Proceedings of the COLING/ACL on Main conference poster sessions.", "citeRegEx": "Ekbal et al\\.,? 2006", "shortCiteRegEx": "Ekbal et al\\.", "year": 2006}, {"title": "India as a lingustic area", "author": ["Murray B Emeneau."], "venue": "Language.", "citeRegEx": "Emeneau.,? 1956", "shortCiteRegEx": "Emeneau.", "year": 1956}, {"title": "The TDIL program and the Indian Language Corpora Initiative", "author": ["Girish Nath Jha."], "venue": "Language Resources and Evaluation Conference.", "citeRegEx": "Jha.,? 2012", "shortCiteRegEx": "Jha.", "year": 2012}, {"title": "Moses: Open source toolkit for Statistical Machine Translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "The IIT Bombay SMT System for ICON 2014 Tools contest", "author": ["Anoop Kunchukuttan", "Ratish Pudupully", "Rajen Chatterjee", "AbhijitMishra", "PushpakBhattacharyya."], "venue": "NLP Tools Contest at ICON 2014.", "citeRegEx": "Kunchukuttan et al\\.,? 2014", "shortCiteRegEx": "Kunchukuttan et al\\.", "year": 2014}, {"title": "Brahmi-Net: A transliteration and script conversion system for languages of the Indian subcontinent", "author": ["Anoop Kunchukuttan", "Ratish Puduppully", "Pushpak Bhattacharyya"], "venue": null, "citeRegEx": "Kunchukuttan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kunchukuttan et al\\.", "year": 2015}, {"title": "Automatic evaluation and uni", "author": ["I Dan Melamed"], "venue": null, "citeRegEx": "Melamed.,? \\Q1995\\E", "shortCiteRegEx": "Melamed.", "year": 1995}, {"title": "BLEU: a method for automatic evalu", "author": ["Jing Zhu"], "venue": null, "citeRegEx": "Zhu.,? \\Q2002\\E", "shortCiteRegEx": "Zhu.", "year": 2002}, {"title": "News from opus-a collection", "author": ["Translation. J\u00f6rg Tiedemann"], "venue": null, "citeRegEx": "Tiedemann.,? \\Q2009\\E", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "under-resourced languages and domains", "author": ["EACL. David Vilar", "Jan-T Peter", "Hermann Ney"], "venue": null, "citeRegEx": "Vilar et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Vilar et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 1, "context": "Related languages exhibit lexical and structural similarities on account of sharing a common ancestry (Indo-Aryan, Slavic languages) or being in prolonged contact for a long period of time (Indian subcontinent, Standard Average European linguistic areas) (Bhattacharyya et al., 2016).", "startOffset": 255, "endOffset": 283}, {"referenceID": 4, "context": "For instance, lexical similarity can be modelled in the standard SMT pipeline by transliteration of words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al.", "startOffset": 121, "endOffset": 143}, {"referenceID": 10, "context": ", 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014).", "startOffset": 27, "endOffset": 81}, {"referenceID": 15, "context": "A different paradigm is to drop the notion of word boundary and consider the character n-gram as the basic unit of translation (Vilar et al., 2007; Tiedemann, 2009a).", "startOffset": 127, "endOffset": 165}, {"referenceID": 0, "context": "Atreya et al. (2016) and Ekbal et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Atreya et al. (2016) and Ekbal et al. (2006) have shown that the OS is a useful unit for transliteration involving Indian languages.", "startOffset": 0, "endOffset": 45}, {"referenceID": 15, "context": "For character and orthographic syllable level models, we use higher order (10-gram) languages models since data sparsity is a lesser concern due to small vocabulary size (Vilar et al., 2007).", "startOffset": 170, "endOffset": 190}, {"referenceID": 14, "context": "As suggested by Nakov and Tiedemann (2012), we used word-level tuning for character and orthographic syllable level models by post-processing n-best lists in each tuning step to calculate the usual word-based BLEU score.", "startOffset": 26, "endOffset": 43}, {"referenceID": 7, "context": "These languages have been in contact for a long time, hence there are many lexical and grammatical similarities among them, leading to the subcontinent being considered a linguistic area (Emeneau, 1956).", "startOffset": 187, "endOffset": 202}, {"referenceID": 12, "context": "Table 2 lists the languages involved in the experiments and provides an indication of the lexical similarity between them in terms of the Longest Common Subsequence Ratio (LCSR) (Melamed, 1995) between the parallel training sentences at character level.", "startOffset": 178, "endOffset": 193}, {"referenceID": 8, "context": "Dataset: We used the multilingual ILCI corpus for our experiments (Jha, 2012), consisting of a modest number of sentences from tourism and health domains.", "startOffset": 66, "endOffset": 77}, {"referenceID": 2, "context": "Language models for word-level systems were trained on the target side of training corpora plus monolingual corpora from various sources [hin: 10M (Bojar et al., 2014), tam: 1M (Ramasamy et al.", "startOffset": 147, "endOffset": 167}, {"referenceID": 9, "context": "System details: PBSMT systems were trained using the Moses system (Koehn et al., 2007), with the grow-diag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters).", "startOffset": 66, "endOffset": 86}, {"referenceID": 3, "context": ", 2007), with the grow-diag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters).", "startOffset": 87, "endOffset": 112}, {"referenceID": 11, "context": "We used the BrahmiNet transliteration system (Kunchukuttan et al., 2015) for postediting, which is based on the transliteration Module in Moses (Durrani et al.", "startOffset": 45, "endOffset": 72}, {"referenceID": 5, "context": ", 2015) for postediting, which is based on the transliteration Module in Moses (Durrani et al., 2014).", "startOffset": 79, "endOffset": 101}], "year": 2016, "abstractText": "We explore the use of the orthographic syllable, a variable-length consonant-vowel sequence, as a basic unit of translation between related languages which use abugida or alphabetic scripts. We show that orthographic syllable level translation significantly outperforms models trained over other basic units (word, morpheme and character) when training over small parallel corpora.", "creator": " XeTeX output 2016.09.23:2106"}}}