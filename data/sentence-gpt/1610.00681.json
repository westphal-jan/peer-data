{"id": "1610.00681", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2016", "title": "Team-Optimal Distributed MMSE Estimation in General and Tree Networks", "abstract": "We construct optimal estimation algorithms over distributed networks for state estimation in the mean-square error (MSE) sense. Here, we have a distributed collection of agents with processing and cooperation capabilities. These agents continually observe a noisy version of a desired state of the nature through a linear model and seek to learn this state by interacting with each other. Although this problem has attracted significant attention and extensively been studied in several different fields including machine learning theory to signal processing, all the well-known strategies achieve suboptimal learning performance in the MSE sense. To this end, we provide algorithms that achieve distributed minimum MSE (MMSE) performance over an arbitrary network topology based on the aggregation of information at each agent. This approach differs from the diffusion of information across network, i.e., exchange of local estimates per time instance. Importantly, we show that exchange of local estimates is sufficient only over the certain network topologies. By inspecting these network structures, we also propose strategies that achieve the distributed MMSE performance also through the diffusion of information such that we can substantially reduce the communication load while achieving the best possible MSE performance. For practical implementations we provide approaches to reduce the complexity of the algorithms through the time-windowing of the observations. Finally, in the numerical examples, we demonstrate the superior performance of the introduced algorithms in the MSE sense due to optimal estimation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 3 Oct 2016 19:09:01 GMT  (2369kb)", "http://arxiv.org/abs/1610.00681v1", "Submitted to Digital Signal Processing"], ["v2", "Sun, 8 Jan 2017 19:28:02 GMT  (410kb,D)", "http://arxiv.org/abs/1610.00681v2", "Submitted to Digital Signal Processing"]], "COMMENTS": "Submitted to Digital Signal Processing", "reviews": [], "SUBJECTS": "cs.SY cs.LG", "authors": ["muhammed o sayin", "suleyman s kozat", "tamer ba\\c{s}ar"], "accepted": false, "id": "1610.00681"}, "pdf": {"name": "1610.00681.pdf", "metadata": {"source": "CRF", "title": "Network Structures and Fast Distributed MMSE Estimation", "authors": ["Muhammed O. Sayina", "Suleyman S. Kozatb"], "emails": ["sayin2@illinois.edu", "kozat@ee.bilkent.edu.tr"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n00 68\n1v 1\n[ cs\n.S Y\n] 3\nO ct\n2 01\nWe construct optimal estimation algorithms over distributed networks for state estimation in the mean-square error (MSE) sense. Here, we have a distributed collection of agents with processing and cooperation capabilities. These agents continually observe a noisy version of a desired state of the nature through a linear model and seek to learn this state by interacting with each other. Although this problem has attracted significant attention and extensively been studied in several different fields including machine learning theory to signal processing, all the well-known strategies achieve suboptimal learning performance in the MSE sense. To this end, we provide algorithms that achieve distributed minimum MSE (MMSE) performance over an arbitrary network topology based on the aggregation of information at each agent. This approach differs from the diffusion of information across network, i.e., exchange of local estimates per time instance. Importantly, we show that exchange of local estimates is sufficient only over the certain network topologies. By inspecting these network structures, we also propose strategies that achieve the distributed MMSE performance also through the diffusion of information such that we can substantially reduce the communication load while achieving the best possible MSE performance. For practical implementations we provide approaches to reduce the complexity of the algorithms through the time-windowing of the observations. Finally, in the numerical examples, we demonstrate the superior performance of the introduced algorithms in the MSE sense due to optimal estimation.\nKeywords: Distributed networks, distributed Kalman filter, MMSE estimation, tree networks."}, {"heading": "1. Introduction", "text": "Over a distributed network of agents with measurement, processing and communication capabilities, we can have enhanced processing performance, e.g., fast response time, relative to the centralized networks by distributing the processing power over the networks [1, 2, 3]. Mainly, distributed agents observe a true state of the system through noisy measurements from different viewpoints, process the observation data in order to estimate the state, and communicate with each other to alleviate the estimation process in a fully distributed manner. Notably, the agents\n\u2217Corresponding author Email addresses: sayin2@illinois.edu (Muhammed O. Sayin), kozat@ee.bilkent.edu.tr (Suleyman S.\nKozat)\nPreprint submitted to Elsevier October 4, 2016\ncan respond to streaming data in an online manner by disclosing information among each other at certain instances. This framework is conveniently used to model highly complex structures from defense applications to social and economical networks [4, 5, 6, 7]. As an example, say that we have radar systems distributed over an area and seeking to locate hostile missiles, i.e., the location of the missile is the underlying state of the system. In that respect, distributed processing approach has vital importance in terms of detecting the missiles and reacting as fast as possible. In particular, even if the viewpoints of a few radar systems are blocked due to environmental obstacles, through the communication among the radar systems, each system can still locate the missiles. Additionally, since each radar system not only collects measurements but also process them to locate the missiles, the overall system can react against the missiles faster than a centralized approach in which measurements of all the radar systems are collected at a centralized unit and processed together.\nAlthough there is an extensive literature on this field, e.g., [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents. Prior art focuses on the computationally simple algorithms that aim to achieve certain performance criterion asymptotically, e.g., diffusion or consensus based estimation algorithms [2, 8, 11, 12], due to processing power related practical concerns. However, there is a trade-off in terms of computational complexity and estimation performance. Correspondingly, for certain network scenarios, as exemplified in the following, computationally demanding yet fast algorithms can be preferable to achieve desired performance. In particular, commonly, at each iteration of the algorithms, agents make a noisy measurement, process the data, and exchange information with each other. In this chain of steps, each step, i.e., measurement, processing, and communication steps, plays significant roles for the overall performance of the algorithms as follows:\nSpeed of reliable communication: The reliable communication rate among the agents, i.e., communication step, should be as fast as the processing step for fast estimation performance. Otherwise, the communication rate creates a burden for the overall speed of the algorithm. In chemical kinetics, this phenomena is called as the rate determining step (RDS) such that speed of a chain of reactions can be approximately determined by the speed of the slowest reaction [13]. This implies that we can employ fast algorithms in spite of high complexity in the network scenarios with relatively slow communication step due to time flexibility.\nCommunication power: The low complexity algorithms have relatively slow learning rate such that agents disclose similar information with the previously disclosed ones. However, communication between the agents also requires significant power resources. To this end, in the design of distributed networks, we should also consider the trade-off in terms of the communication power and the estimation performance [4, 14, 12]. In [14, 12], the authors study this problem by compressing the data before the transmission and extracting the compressed information adaptively. We point out that in a fast estimation algorithm, exchanged data carries relatively more, i.e., new, information. This implies a communication flexibility such that through certain protocols, we can reduce the number of information exchange due to the disclosure of fertile information, and correspondingly this can reduce the communication load of the overall system.\nNumber of samples: Finally, in certain network scenarios, the number of measurements can also be too low, e.g., due to small sampling frequency, that the low complexity approaches aiming to achieve the performance asymptotically cannot yield an acceptable result. Particularly, some applications can require the agents to process the measurements as much as they can, i.e., optimally with respect to certain performance measure, due to the limited number of measurements.\nAs noted above, computationally demanding yet fast algorithms have applications for certain network scenarios. Hence, formulating the optimal distributed estimation algorithms with respect to certain performance criteria is a significant and unexplored challenge. To this end, instead of computationally simple yet slow algorithms, we seek to formulate the optimal estimation algorithms over distributed networks in the mean-square-error (MSE) sense. We design which information to disclose and how to utilize them for the minimum MSE (MMSE) performance. In addition to the aforementioned scenarios, we also analyze the network topologies in which the optimal algorithms can also be computationally simple and require less communication load. In the single agent systems, the well-known Kalman filter is extensively used for the MMSE estimation. Due to its iterative nature and superior estimation performance, Kalman filter appeals significant attention in both control theory and signal processing literatures. Correspondingly, there have been abundant attempts to formulate distributed Kalman filter [15, 16, 17, 18]. However, while extending Kalman filter for distributed networks, these approaches develop asymptotically optimal or sub-optimal approaches in the MSE sense due to practical concerns. Particularly, the cooperation among the agents results in complex MMSE algorithms for arbitrary network topologies. As explained in [19], once agent-1 transmits an information to agent-2, this information becomes common knowledge between the agents. This implies that agent-1 knows that agent-2 knows it, agent-2 knows that agent-1 knows it, agent-1 knows that agent-2 knows that agent-1 knows it, and so on. Therefore, in the design of the distributed MMSE estimator, we take these into consideration and utilize the exchanged information based on its content rather than a blind approach in which all exchanged information is handled irrespective of the content as in the diffusion or consensus based approaches.\nIn diffusion or consensus based approaches, the agents utilize the exchanged information generally through certain static combination rules, e.g., the uniform rule [20], the Laplacian rule [21] or the Metropolis rule [22]. However, if the statistical profile of the measurement data varies over the network, i.e., each agent observes diverse signal-to-noise ratios, by ignoring the variation in noise, these rules yield severe decline in the estimation performance [2]. In such cases the agents can even perform better without cooperation [2]. Hence, we also seek to formulate the optimal utilization of the exchanged information in the MSE sense.\nConsider distributed networks of the agents that observe a noisy version of an underlying state and can exchange information with only certain agents at each time instant. Note that information is transmitted over the hops since certain agents might not be connected directly. To this end, we introduce a comprehensive cost measure considering the transmission of information over the hops and the corresponding content. We formulate the MMSE estimator, called the optimal distributed online learning (ODOL) algorithm, using common knowledge about the statistical profiles and the network topology for the jointly Gaussian state and noise signals. We point out that the ODOL algorithm achieves the linear MMSE (LMMSE) performance for other statistical profiles. Notably, the ODOL algorithm utilizes the aggregation of information at each agent via time-stamped information exchange and is different from the conventional algorithms that benefit from the diffusion of local estimates [2, 8, 11]. The aggregation of information at each agent requires excessive communication load relative to the diffusion of information in general. In particular, the ODOL algorithm is not practical for real life applications due to the excessive communication load, however, we utilize this algorithm to analyze the performance of proposed approaches since the ODOL algorithm achieves the oracle performance, i.e., the distributed MMSE performance, for any network scenario.\nFurthermore, we modify the ODOL algorithm by exploiting the network topology to reduce the communication load such that agents disclose their local estimates only yet achieve the\nMMSE performance. Over tree networks excluding multiple transmission paths, the diffusion of local estimates is sufficient to achieve the distributed MMSE performance. We analytically show that for sufficiency of diffusion of estimates, tree networks could also involve cell structures that we define as the sub-networks in which all agents are connected with each other. Additionally, we formulate the optimal and efficient distributed online learning (OEDOL) algorithm, which is practical for real life applications and achieves the MMSE performance over the tree networks. Note that for an arbitrary network topology, we can construct such network connections by eliminating certain communication links. Through numerical examples, we examine the impact of constructing the spanning tree of a network on the estimation performance and observe that the impact is negligible. Finally, we propose the time windowing of the observation set in order to reduce the complexity of the algorithms. In these algorithms, agents combine the received information linearly with time-invariant combination weights.\nWe can list our main contributions as follows: 1) We introduce a thorough cost measure considering the transmission of information over hops across networks. 2) We derive the ODOL algorithm achieving the oracle performance over arbitrary networks through the aggregation of information at each agent. 3) For practical applications, we study the network structures in which we can achieve the MMSE performance through the disclosure of local estimates. 4) We propose the OEDOL algorithm achieving the MMSE performance over certain network topologies with tremendously reduced communication load. 5) We also formulate sub-optimal versions of the algorithms with reduced complexity. 6) We provide numerical examples demonstrating the significant gains due to the introduced algorithms.\nThe remainder of the paper is organized as follows. We introduce the distributed MMSE framework in Section II. We study the tree networks, exploit the network topology to formulate the OEDOL algorithm that reduces the communication load and introduce cell structures enhancing the learning rate further under communication constraints in Section III. We propose the sub-optimal versions of the ODOL and OEDOL algorithms for practical implementations in Section IV. In Section V, we provide numerical examples demonstrating significant gains due to the introduced algorithms compared to the conventional algorithms. We conclude the paper in Section VI with several remarks.\nNotation: Bold lower (or upper) case letters denote column vectors (or matrices). For a vector a (or matrix A), aT (or AT ) is its ordinary transpose. The terms of the vector 1 (and 0) are all 1s (and 0s) and the size of the vector will be understood from the context. We use calligraphic letters for random variables and underlined calligraphic letters for random vectors, e.g., X and X. Bold calligraphic letters, e.g., Z , denote a set of random variables. For a random variable X (or vector X), E[X] (or E[X]) represents its expectation. We work with real data for notational simplicity. The operator col{\u00b7} produces a column vector or a matrix in which the arguments of col{\u00b7} are stacked one under the other. For a matrix argument, diag{A} operator constructs a diagonal matrix with the diagonal entries of A. For a given set N, diag{N} creates a diagonal matrix whose diagonal block entries are elements of the set. The operator \u2297 denotes the Kronecker product."}, {"heading": "2. Distributed-MMSE Estimation Framework", "text": "Consider a distributed network of m agents with processing and communication capabilities. In Fig. 1, we illustrate this network through an undirected graph, where the vertices and the edges correspond to the agents and the communication links across the network, respectively.\nFor each agent i, we denote the set of agents whose information could be received at least after k hops, i.e., k-hop neighbors, by N(k)i defined as\nN(k)i \u25b3 =\n{\nj1, \u00b7 \u00b7 \u00b7 , j\u03c0(k)i\n}\nand \u03c0(k)i = \u2223 \u2223 \u2223N(k)i \u2223 \u2223 \u2223 is the cardinality of N(k)i (See Fig. 1b). We assume that N (0) i = {i} and N (k) i = \u2205 for k < 0. Here, the agents observe a noisy version of a time-invariant and unknown state vector x \u2208 Rp which is a realization of the Gaussian random vector process X with mean x\u0304 and autocovariance \u03a3x. Each agent i observes the state as\nyi,t = Hix + ni,t,\nwhere Hi \u2208 Rq\u00d7p is a known matrix, ni,t \u2208 Rq is a realization of a zero-mean white Gaussian vector process N i,t with auto-covariance \u03a3ni and correspondingly the observation yi,t \u2208 R\nq is a realization of the random process Y\ni,t = HiX +N i,t. The noise ni,t is also independent from the\nstate and the other noise parameters. We assume that the variance of the noise signals are known, which can also be readily estimated from the data [23]. At each instant, the agents observe the underlying state through yi,t, diffuse information to the neighboring agents and receive information from them as seen in Fig. 1a. Naturally, the agents can learn the true state, under certain regularity conditions [23], irrespective of cooperation among them, provided that their own measurements are not biased and sufficient to estimate the true state in time. However, through the cooperation of agents, we can increase the learning rate significantly [2]. To this end, we aim to find the optimal learning strategy over distributed networks in the MSE sense.\nAs a lower bound on the MSE performance over distributed networks, we first consider the case when there is no limitation on the amount of the disclosed information. Agents disclose their own measurements and transmit the received information from the neighboring agents (with time stamps, i.e., the information is ordered). Through this implementation, each agent can obtain the measurements of the other agents separately in a connected network. We point\nout that the information on the non-neighboring agents could only be received after certain hops due to the sparsely connected structure. As an example, the disclosed information of j \u2208 N(2)i reaches to i by passing through two communication links as seen in Fig. 1b. In particular, this case assumes that each agent has access to full information from the other agents, albeit with certain hops, and corresponds to the direct aggregation of all information across the network at each agent.\nRemark 2.1: The aggregation of all information directly at each agent and processing them optimally yield excessive communication load in general. Since the communication load is crucial for the applicability of the distributed learning algorithms [12, 14], we also seek to reduce the communication load yet achieve the MMSE performance. In the consensus and diffusion implementations, the agents disclose the local estimates, which requires relatively reduced communication load [2, 24, 11, 8]. However, later in this paper we show that the disclosure of the local estimates is sufficient to achieve the MMSE performance only over certain network topologies.\nAt time t, all the information aggregated at ith agent is given by { {\nyi,\u03c4 }\u03c4\u2264t , { y j,\u03c4 }\u03c4\u2264t\u22121\nj\u2208N(1)i , \u00b7 \u00b7 \u00b7 ,\n{ y j,\u03c4 }\u03c4\u2264t\u2212\u03bai\nj\u2208N (\u03bai ) i\n}\n,\nwhere the information from the furthest agent is received at least after \u03bai hops1. Here, the collection set { y j,\u03c4 }\u03c4\u2264t\u2212k\nj\u2208N(k)i includes the observations from the k-hop neighbors N(k)i . Particularly, the\ncollection is explicitly defined as {\ny j,\u03c4 }\u03c4\u2264t\u2212k\nj\u2208N(k)i\n\u25b3 =\n{\ny j1,t\u2212k, ..., y j1,0,\n..., y j \u03c0 (k) i ,t\u2212k, ..., y j \u03c0 (k) i ,0\n}\n. (1)\nThen, we have the comprehensive cost measure in the distributed-MMSE framework as\nx\u0302i,t = minx E [ \u2225 \u2225 \u2225Xt \u2212 x \u2225 \u2225 \u2225 2 \u2223 \u2223 \u2223 \u2223 { Y i,\u03c4 = yi,\u03c4 }\u03c4\u2264t ,\n{\nY j,\u03c4 = y j,\u03c4\n}\u03c4\u2264t\u22121\nj\u2208N(1)i , \u00b7 \u00b7 \u00b7 ,\n{\nY j,\u03c4 = y j,\u03c4\n}\u03c4\u2264t\u2212\u03bai\nj\u2208N (\u03bai ) i\n]\n(2)\nand the MMSE estimate for each agent i is given by the expectation of x conditioned all the accessed information:\nxi,t = E [ X \u2223 \u2223 \u2223 \u2223 { Yi,\u03c4 = yi,\u03c4 }\u03c4\u2264t , { Y j,\u03c4 = y j,\u03c4 }\u03c4\u2264t\u22121\nj\u2208N(1)i , \u00b7 \u00b7 \u00b7 ,\n{\nY j,\u03c4 = y j,\u03c4\n}\u03c4\u2264t\u2212\u03bai\nj\u2208N (\u03bai ) i\n]\n. (3)\nCorrespondingly, we define the random variable X\u0302i,t as follows\nX\u0302i,t \u25b3 = E\n[\nX \u2223 \u2223 \u2223 \u2223 {\nY i,\u03c4\n}\u03c4\u2264t , \u00b7 \u00b7 \u00b7 , {\nY j,\u03c4\n}\u03c4\u2264t\u2212\u03bai\nj\u2208N (\u03bai ) i\n]\n.\n1For notational simplicity, we denote Ni \u25b3 = N(1)i and \u03c0i \u25b3 = \u03c0 (1) i ."}, {"heading": "2.1. ODOL Algorithm", "text": "Importantly, since the state and the observation noise are independent Gaussian random parameters, we propose the ODOL algorithm constructing the distributed-MMSE estimator (3) in an iterative way. To this end, we collect all received information at time t as\nzi,t = col {\nyi,t, yi1,t\u22121, ..., yi\u03c0i ,t\u22121,\n..., y j1,t\u2212\u03bai , ..., y j \u03c0\n(\u03bai) i\n,t\u2212\u03bai\n}\n.\nThen, the iterations of the ODOL algorithm are given by\nKi,t = \u03a3\u0302i,t\u22121H\u0304 T i\n(\nH\u0304i\u03a3\u0302i,t\u22121H\u0304 T i + \u03a3\u0304ni )\u22121 ,\nx\u0302i,t = ( I \u2212 Ki,tH\u0304i ) x\u0302i,t\u22121 + Ki,tzi,t, \u03a3\u0302i,t = ( I \u2212 Ki,tH\u0304i ) \u03a3\u0302i,t\u22121,\nwhere2 H\u0304i \u25b3 =\n( Pi \u2297 Iq ) H, H \u25b3 = col {H1, \u00b7 \u00b7 \u00b7 ,Hm}, \u03a3\u0304ni \u25b3 = ( Pi \u2297 Ip ) \u03a3n ( Pi \u2297 Ip )T , \u03a3n \u25b3 =\ndiag { \u03a3n1 , \u00b7 \u00b7 \u00b7 ,\u03a3nm } , and Pi is the corresponding permutation matrix."}, {"heading": "3. Distributed-MMSE Estimation with Disclosure of Local Estimate", "text": "Since the underlying state and the observation noises are independent Gaussian random parameters, the conditional expectation of the state given the observations, i.e., the MMSE estimate, is an affine combination of the observations. Hence, in this section we aim to exploit the affine transformation in order to reduce the disclosed amount of information so that each agent exchanges the necessary information in an efficient way. We point out that, as shown in the following example, disclosure of the local estimates is not sufficient to achieve the distributed MMSE performance in general. Consider a cycle network of 4 agents where agent-1 is directly connected with agents 2 and 3. At time t = 1, we obtain\nx\u03022,1 = E [ X \u2223 \u2223 \u2223Y\n2,1 = y2,1,Y2,0 = y2,0,Y1,0 = y1,0,Y4,0 = y4,0\n]\n,\nx\u03023,1 = E [ X \u2223 \u2223 \u2223Y\n3,1 = y3,1,Y3,0 = y3,0,Y1,0 = y1,0,Y4,0 = y4,0\n]\n.\nSince x\u03022,1 and x\u03023,1 are conditioned on Y4,0 = y4,0 commonly, the MMSE estimator x\u03021,2 could not be obtained by just processing the current estimates, i.e.,\nx\u03021,2 , E [ X \u2223 \u2223 \u2223Y1,2 = y1,2, X\u03021,1 = x\u03021,1, X\u03022,1 = x\u03022,1,\nX\u03023,1 = x\u03023,1 ] .\nIn particular, for i.i.d. observations Y1, Y2 and Y3 we have\nE [ X \u2223 \u2223 \u2223Y1,Y2,Y3 ] , E [ X \u2223 \u2223 \u2223E [ X|Y1,Y2 ] , E [ X|Y2,Y3 ]] .\nHence, optimal processing of estimates should be elaborately considered. In the following, we analytically show that the MMSE estimate could be obtained through the disclosure of local estimates over \u201ctree networks\u201d but not in general.\n2If the inverse fails to exist, a pseudo inverse can replace the inverse [25]."}, {"heading": "3.1. Tree Networks", "text": "A network has a \u201ctree structure\u201d if its corresponding graph is a tree, i.e., connected and undirected without any cycles [26]. As an example, the conventional star or line networks have tree structures. We remark that for an arbitrary network topology we can also construct the spanning tree of the network and eliminate the cycles. In the literature, there exists numerous distributed algorithms for minimum spanning tree construction [27, 28, 29, 30, 31].\nImportantly, the following theorem shows that over tree networks we can achieve the performance of the oracle algorithm through the disclosure of the local estimates only.\nTheorem 3.1: Consider the distributed estimation framework over a tree network. Then, the distributed-MMSE estimator (3) could also be obtained by\nx\u0302i,t = E [ X \u2223 \u2223 \u2223 \u2223 \u2223 { Yi,\u03c4 = yi,\u03c4 }\u03c4\u2264t , { X\u0302 j,\u03c4 = x\u0302 j,\u03c4 }\u03c4\u2264t\u22121\nj\u2208Ni\n]\n, (4)\ni.e., each agent can only disclose its own local estimate to achieve the MMSE performance.\nProof: Initially, agent i has access to yi,0 only and the MMSE estimator is x\u0302i,0 = E[X|Yi,0 = yi,0]. At time t = 1, the MMSE estimator is given by\nx\u0302i,1 = E [ X \u2223 \u2223 \u2223 {\nY i,\u03c4 = yi,\u03c4 }\u03c4\u22641 , { Y j,0 = y j,0 }\nj\u2208Ni\n]\n, (5)\nwhich can be written as\nx\u0302i,1 = E [ X \u2223 \u2223 \u2223 {\nY i,\u03c4 = yi,\u03c4 }\u03c4\u22641 , { E[X \u2223 \u2223 \u2223Y j,0 = y j,0] }\nj\u2208Ni\n]\n,\n= E [ X \u2223 \u2223 \u2223 {\nY i,\u03c4 = yi,\u03c4 }\u03c4\u22641 , { X\u0302 j,0 = x\u0302 j,0 }\nj\u2208Ni\n]\n. (6)\nAs seen in Fig. 2, over a tree network, for k \u2208 {1, \u00b7 \u00b7 \u00b7 , \u03bai} we have\nN(k)i = \u22c3\nj\u2208Ni\n(\nN(k)i \u2229 N (k\u22121) j\n)\n. (7)\nNote that the sets in (7) are disjoint as (\nN(k)i \u2229 N (k\u22121) j1\n) \u2229 (\nN(k)i \u2229 N (k\u22121) j2\n)\n= \u2205 (8)\nfor all j1, j2 \u2208 Ni and j1 , j2. Notably, over a tree network, by (8), we can partition the collection set defined in (1) as follows\n{ y j,\u03c4 }\nj\u2208N(k)i =\n{ {\ny j,\u03c4 }\nj\u2208N(k)i \u2229N (k\u22121) j1\n,\n\u00b7 \u00b7 \u00b7 , { y j,\u03c4 }\nj\u2208N(k)i \u2229N (k\u22121) j\u03c0i\n}\n. (9)\nLet the set of all accessed information by the ith agent at time t = 2 be\nZi,2 \u25b3 =\n{{\nyi,\u03c4 }\u03c4\u22642 , { y j,\u03c4 }\u03c4\u22641 j\u2208Ni , { yk,0 } k\u2208N(2)i\n}\n.\nWe also define\nZ\u0302 j,i,1 \u25b3 =\n{ =y j,1 \ufe37 \ufe38\ufe38 \ufe37 {\nyk,1 }\nk\u2208N(1)i \u2229N (0) j\n, { yk,0 }\nk\u2208N(2)i \u2229N (1) j\n}\nsuch that by (9) we have Zi,2 = { yi,2, Z\u0302 j1,i,1, \u00b7 \u00b7 \u00b7 , Z\u0302 j\u03c0i ,i,1, Zi,1 }\n(10)\nand\nZ\u0302 j,i,1 = Z j,1 \\ {\n=Z j,0 \ufe37\ufe38\ufe38\ufe37\ny j,0 , yi,0 } . (11)\nHence, by (10) and (11), we obtain\nx\u0302i,2 = E [ X \u2223 \u2223 \u2223 { Yi,\u03c4 = yi,\u03c4 }\u03c4\u22642 , { Z j,0 = Z j,0 }\nj\u2208Ni ,\n{\nZ j,1 \\ {\nZ j,0,Yi,0\n} = Z j,1 \\ { Z j,0, yi,0 } }\nj\u2208Ni\n]\n. (12)\nAfter some algebra, (12) could also be obtained by\nx\u0302i,2 = E [ X \u2223 \u2223 \u2223 \u2223 {\nY i,\u03c4 = yi,\u03c4 }\u03c4\u22642 , { E [ X \u2223 \u2223 \u2223Z j,\u03c4 = Z j,\u03c4 ]}\u03c4\u22641\nj\u2208Ni\n]\n,\n= E [ X \u2223 \u2223 \u2223 {\nY i,\u03c4 = yi,\u03c4 }\u03c4\u22642 , { X\u0302 j,\u03c4 = x\u0302 j,\u03c4 }\u03c4\u22641\nj\u2208Ni\n]\n. (13)\nAt time t = 3, (10) yields\nZ\u0302 j,i,2 = Z j,2 \\ { Z j,1 \u222a\n=Z\u0302i, j,1 \ufe37 \ufe38\ufe38 \ufe37 {\nZi,1 \\ { Zi,0 \u222a Z j,0 } } } . (14)\nCorrespondingly, (14) leads to\nZ\u0302 j,i,t = Z j,t \\ { Z j,t\u22121 \u222a Z\u0302i, j,t\u22121 }\nimplying that Z\u0302 j,i,t is constructible from Zi,\u03c4 and Z j,\u03c4 for \u03c4 \u2264 t. Hence, for t > 0 the MMSE estimator over tree networks is given by (4) and the proof is concluded.\nRemark 3.1: When the expectation of the state is conditioned on the infinite number of observations over even a constructed spanning tree, only a finite number of the observations is devoid compared to the case over a fully connected network. Hence, even if we construct the spanning tree of that network, we would still achieve the distributed MMSE performance over a fully connected (or centralized) network asymptotically. As an illustrative example, in Fig. 8, we observe that the MMSE performance over the fully connected, star and line networks are asymptotically the same. Similarly, in [32, 33, 34], the authors show that the performance of the diffusion based algorithms could approach the performance of a fully connected network under certain regularity conditions.\nIn the sequel, we propose the optimal and efficient distributed online learning (OEDOL) algorithm that achieves the distributed-MMSE performance over tree networks iteratively."}, {"heading": "3.2. OEDOL Algorithm", "text": "We remark that the MMSE estimate x\u0302i,t linearly depends on previous estimate x\u0302i,t\u22121. In order to extract the new information, we need to eliminate the previously received information at each instant on the neighboring agents. This brings in additional computational complexity. On the contrary, agents can just disclose the new information, i.e., z\u0302i,t, after eliminating all the previously exchanged information. Since we are conditioning on the linear combinations of the conditioned variables without effecting their spanned space, i.e., z\u0302i,t is computable from x\u0302i,\u03c4 for \u03c4 \u2264 t and vice versa, we can still achieve the MMSE performance by reduced computational load, yet.\nAt time t, agent i observes yi,t and receives z\u0304i,t = col { z\u0302 j1,t\u22121, \u00b7 \u00b7 \u00b7 , z\u0302 j\u03c0i ,t\u22121 }\n. Here, we determine the content of the received information to extract the innovation within them and utilize this innovation in the update of the local estimate. In particular, the OEDOL algorithm is given by\nx\u0302i,t = Ai,tx\u0302i,t\u22121 + Bi,tyi,t + Ci,twi,t\u22121, (15)\n\u03a3\u0302i,t = Ai,t\u03a3\u0302i,t\u22121, (16)\nwhere wi,t\u22121 is the innovation term extracted from the received information. Ai,t, Bi,t, and Ci,t are the corresponding weighting matrices defined as\n[ Bi,t Ci,t ] = \u03a3\u0302i,t\u22121H\u0303 T i,t\u22121\u00d7\n(\nH\u0303i,t\u22121\u03a3\u0302i,t\u22121H\u0303 T i,t\u22121 + G\u0303i,t\u22121\n)\u22121 , (17)\nAi,t = I \u2212 Bi,tHi \u2212 Ci,tH\u0304i,t\u22121, (18)\nwhere H\u0303i,t = col{Hi, H\u0304i,t}, G\u0303i,t = diag{\u03a3ni , G\u0304i,t} and G\u0304i,t = diag{Gi,t}. The intermediate parame-\nters H\u0304i,t and Gi,t evolve according to\nH\u0304i,t =\n  B j1,tH j1 + C j1,tH\u0304 j1,t\u22121 ...\nB j\u03c0i ,tH j\u03c0i + C j\u03c0i ,tH\u0304 j\u03c0i ,t\u22121\n  \u2212\n  C j1,tH\u0304 (i) j1,t\u22121 ...\nC j\u03c0i ,tH\u0304 (i) j\u03c0i ,t\u22121\n  , (19)\nGi,t =\n  B j1,t\u03a3n j1 B T j1,t + C j1,tG\u0304 j1,t\u22121C T j1,t ...\nB j\u03c0i ,t\u03a3n j\u03c0i B T j\u03c0i ,t + C j\u03c0i ,tG\u0304 j\u03c0i ,t\u22121C T j\u03c0i ,t\n  \u2212\n  C(i)j1,tG (i) j1,t\u22121 ( C(i)j1,t )T ...\nC(i)j\u03c0i ,t G(i)j\u03c0i ,t\u22121\n(\nC(i)j\u03c0i ,t\n)T\n \n(20)\nand we initialize the parameters as H\u0304 j,\u03c4 = 0 and G j,\u03c4 = 0 for \u03c4 < 0. The recursion of the innovation parameter wi,t is given by\nwi,t = z\u0304i,t \u2212 Di,tz\u0302i,t\u22121 + Ti,twi,t\u22122, (21)\nwhere Di,t = col {\nC(i)j1,t, \u00b7 \u00b7 \u00b7 ,C (i) j\u03c0i ,t\n}\n(22)\nand\nTi,t \u25b3 =\n  C(i)j1,tC ( j1) i,t\u22121 \u00b7 \u00b7 \u00b7 0 ... . . . ...\n0 \u00b7 \u00b7 \u00b7 C(i)j\u03c0i ,t C ( j\u03c0i ) i,t\u22121\n  . (23)\nThen, the agents disclose z\u0302i,t = x\u0302i,t \u2212 Ai,tx\u0302i,t\u22121.\nThe detailed description of the algorithm is provided in Table 1.\nRemark 3.2: In (15), the combination matrices Ai,t,Bi,t and Ci,t are independent from the streaming data although they are time-varying. Hence they can be computed before-hand. In that case, as an example for p = q = 1, the computational complexity of the iterations is O ( \u03c0\u03042 ) where \u03c0\u0304 is the average of the cardinalities of the first-order neighborhoods across the network. Otherwise, the computational complexity of the algorithm is given by O ( m(\u03c0\u0304)3 ) , while it is O ( m3 )\nfor the oracle algorithm. Note that over the tree we have m \u2212 1 edges and correspondingly \u03c0\u0304 is expected to be small. Hence, the optimal diffusion strategy over tree networks also reduces the computational complexity in general (in addition to the substantial reduction in communication). In Table 2, we tabulate the computational complexities of the introduced algorithms.\nWhile constructing the spanning tree, we cancel certain communication links in order to avoid multi-path information propagation. However, we also observe that in a fully connected network we can achieve the performance of the ODOL algorithm, i.e., the distributed-MMSE performance, through the disclosure of local observations. In particular, since all of the agents are connected, each agent can receive the observations across the network directly. Correspondingly, in a fully connected network, we can achieve identical performance with the\nODOL algorithm only through the disclosure of the local estimates as stated in the following corollary formally.\nCorollary 3.1: Consider the distributed estimation framework over a fully connected network. Then, the distributed-MMSE estimator (3) could also be obtained by (4), where agents disclose local estimates only.\nProof: Over a fully connected network, \u03bai = 1 and the MMSE estimator is given by\nx\u0302i,t = E [ X \u2223 \u2223 \u2223 \u2223 \u2223 { Y i,\u03c4 = yi,\u03c4 }\u03c4\u2264t , { Y j,\u03c4 = y j,\u03c4 }\u03c4\u2264t\u22121\nj\u2208Ni\n]\n(24)\nand we can also obtain (24) by (6). Here, we have\nZ\u0302 j,i,t = Z j,t \\ {Zi,t \\ {yi,t}},\nwhich yields\nx\u0302i,t = E [ X \u2223 \u2223 \u2223 {\nY i,\u03c4 = yi,\u03c4 }\u03c4\u2264t , { Z j,\u03c4 = Z j,\u03c4 }\u03c4\u2264t\u22121\nj\u2208Ni ,\n{\nZ j,t\u22121 \\ { Zi,t\u22121 \\ Yi,t\u22121 }\n= Z j,t\u22121 \\ { Zi,t\u22121 \\ {yi,t\u22121} }}\nj\u2208Ni\n]\n(25)\nand (25) is also given by (4) and the proof is concluded.\nWe point out that the optimal algorithms can achieve the desired performance provided that the model assumptions are satisfied. If a link or node failure occurs, the algorithms may not achieve the optimal performance. However, once we detect a link failure, we can redesign the algorithms by eliminating this link in the new reconfiguration. Hence, through such strategies, we can increase the robustness of the introduced algorithms.\nIn the following, we enhance the learning rate further for an arbitrary network topology by exploiting the structure of the fully connected sub-networks that we call as \u201ccell\u201d."}, {"heading": "3.3. Tree Networks Involving Cell Structures", "text": "We define a \u201ccell structure\u201d as a sub-network in which all agents are connected to each other. Intuitively, considering a cell structure as a \u201csingle\u201d agent, we can involve the cell (i.e., all the agents in the cell) in the tree such that we can still achieve the distributed-MMSE performance through the disclosure of local estimates (although we may have loops in the cell). We list the features of the cell structures, e.g., seen in Fig. 3, as follows:\n\u2022 Agents out of a cell can connect to at most one of the agents within that cell.\n\u2022 A cell structure consists of at least 2 agents.\n\u2022 An agent can belong to more than one cell.\n\u2022 Two different agents cannot belong to more than one cell at the same time.\n\u2022 All of the agents belong to at least a cell in a connected network.\n\u2022 Each agent has also the knowledge of the cells of the other agents.\n\u2022 Each agent labels its cells from its own and its first order neighbor\u2019s point of view. As an example, for the agent i, Ci,i1 denotes the cell involving both i and i1. Note that if the same cell also includes i2, Ci,i1 = Ci,i2 .\nThe following theorem shows that we can achieve the performance of the oracle algorithm over tree networks involving cell structures through the disclosure of the local estimates only.\nTheorem 3.2: Consider the distributed estimation framework over tree networks involving cell structures. Then, the distributed-MMSE estimator (3) could also be obtained by (4).\nProof: We point out that Z\u0302 j,i,t denotes the set of new information received by i over j at time t. Initially, we have Z\u0302 j,i,0 = Z j,0 = {y j,0} and the MMSE estimator is also given by (6) over this network topology. Note that the information received by j at t = 1 is given by Z j,1 = {\ny j,1, {\n=Z\u0302k, j,0 \ufe37\ufe38\ufe38\ufe37\nyk,0 }\nk\u2208N j , Z j,0\n}\n, which yields\nZ j,i,1 = Z j,1 \\\n   Z j,0 \u222a \u22c3\nk\u2208Ci, j\n{ yk,0 }\n   ,\n= Z j,1 \\\n   Z j,0 \u222a \u22c3\nk\u2208Ci, j\nZ\u0302k, j,0\n  \nand Z\u0302 j, j,t = \u2205 by definition. Due to the cell structure, we have\n\u22c3\nk\u2208C j,i\nZ\u0302k, j,0 = Z\u0302i, j,0 \u222a \u22c3\nk\u2208Ci, j\\ j\nZ\u0302k,i,0,\n= Zi,0 \u222a \u22c3\nk\u2208Ci, j\\ j\nZk,0,\nwhich leads to\nx\u0302i,2 = E [ X \u2223 \u2223 \u2223 \u2223 {\nY i,\u03c4 = yi,\u03c4 }\u03c4\u22642 , { Z j,0 = Z j,0 }\nj\u2208Ni ,\n{\nZ j,1 \\\n{\nZ j,0 \u222aZi,0 \u222a \u22c3\nk\u2208Ci, j\\ j\nZk,0\n}}\n= Z j,1 \\ { Z j,0 \u222a Zi,0 \u222a \u22c3\nk\u2208Ci, j\\ j\nZk,0 }}\nj\u2208Ni\n]\n.\nWe point out that Ci, j \u2282 Ni and we obtain (13). Correspondingly, for t > 0 we have\nZ\u0302 j,i,t = Z j,t \\\n   Z j,t\u22121 \u222a Z\u0302i, j,t\u22121 \u222a \u22c3\nk\u2208Ci, j\\ j\nZ\u0302k,i,t\u22121\n  \nand Z\u0302 j,i,t is constructible by the sets Z j,\u03c4 for j \u2208 Ni and \u03c4 \u2264 t. Hence, for t > 0 we obtain (4) and the proof is concluded.\nNote that we can have loops within the cell structures and still achieve the optimal performance through the diffusion of the local estimates only. In addition to the enhanced estimation rate, this will also increase the robustness of the introduced strategies against the link failures. In the sequel, we provide the sub-optimal extensions of the algorithms for practical applications."}, {"heading": "4. Sub-optimal Approaches", "text": "Minimization of the cost measure (2) optimally requires relatively excessive computations. We aim to mitigate the problem sub-optimally yet in a computationally efficient approach while achieving comparable performance with the optimal case. As an example, we can approximate the cost measure (2) through time-windowing as follows\nx\u0303i,t = minx E [ \u2225 \u2225 \u2225X \u2212 x \u2225 \u2225 \u2225 2 \u2223 \u2223 \u2223 \u2223 { Y i,\u03c4 = yi,\u03c4 }t\u2212\u03ba<\u03c4\u2264t ,\n{\nY j,\u03c4 = y j,\u03c4\n}t\u2212\u03ba<\u03c4\u2264t\u22121\nj\u2208N(1)i , \u00b7 \u00b7 \u00b7 ,\n{\nY j,\u03c4 = y j,\u03c4\n}t\u2212\u03ba<\u03c4\u2264t\u2212\u03bai\nj\u2208N (\u03bai ) i\n]\n.\nIn Fig. 4, we illustrate the time-windowing approach over the aggregated observations. We define a memory element mt denoting the expectation of the zero-mean state conditioned on all observation data time-stamped at t, i.e., mt \u25b3 = E [ X \u2223 \u2223 \u2223 {\nY i,t = yi,t\n}\ni\u2208N\n]\n. We have mt = Myt\nand M \u25b3 = \u03a3xHT\n( H\u03a3xHT + \u03a3n )\u22121 , where H \u25b3 = col{H1, \u00b7 \u00b7 \u00b7 ,Hm}, \u03a3n \u25b3 = diag{\u03a3n1 , \u00b7 \u00b7 \u00b7 ,\u03a3nm} and\nyt \u25b3 = col{y1,t, \u00b7 \u00b7 \u00b7 , ym,t}. Correspondingly, x(e)i,t\u22121 represents the extracted information from the estimate x\u0303i,t\u22121 via the memory element mt\u2212\u03ba. In particular, x (e) i,t\u22121 is defined as follows\nx(e)i,t\u22121 \u25b3 = E\n[\nX \u2223 \u2223 \u2223 {\nY i,\u03c4 = yi,\u03c4 }t\u2212\u03ba+1<\u03c4\u2264t\u22121 ,\n\u00b7 \u00b7 \u00b7 , {\nY j,\u03c4 = y j,\u03c4\n}t\u2212\u03ba+1<\u03c4\u2264t\u2212\u03bai\u22121\nj\u2208N (\u03bai ) i\n]\n.\nThis yields x(e)i,t\u22121 = Niy (c) i,t\u22121 where y (c) i,t\u22121 is the vector collecting all observation data\nwithin that window (See Fig. 4) and Ni \u25b3 = \u03a3xH\u0302 T i\n(\nH\u0302i\u03a3xH\u0302 T i + \u03a3\u0302ni )\u22121 . We define H\u0302i \u25b3 =\ncol {\nHi,Hi,H (1) i , \u00b7 \u00b7 \u00b7 ,Hi,H (1) i , \u00b7 \u00b7 \u00b7 ,H (\u03bai) i\n}\nand H( j)i \u25b3 = col\n{\nH j1 , \u00b7 \u00b7 \u00b7 ,H j \u03c0\n( j) i\n}\n. Correspondingly, \u03a3\u0302ni \u25b3 =\ndiag {\n\u03a3ni ,\u03a3ni ,\u03a3 (1) ni , \u00b7 \u00b7 \u00b7 ,\u03a3ni ,\u03a3 (1) ni , \u00b7 \u00b7 \u00b7 ,\u03a3 (\u03bai) ni\n}\nand \u03a3( j)ni \u25b3 = diag\n{\n\u03a3n j1 , \u00b7 \u00b7 \u00b7 ,\u03a3n j\n\u03c0 ( j) i\n}\n. Then, after some\nalgebra the estimate is given by x\u0303i,t\u22121 = Kimt\u2212\u03ba + Lix (e) i,t\u22121 where\nKi \u25b3 =\n( I \u2212 MHNiH\u0302i )\u22121 \u2212 ( I \u2212 NiH\u0302iMH )\u22121 NiH\u0302i,\nLi \u25b3 = (I \u2212 MH)\n( I \u2212 NiH\u0302iMH )\u22121 .\nHence, we obtain the sub-optimal distributed online learning (SDOL) algorithm as\nx\u0303i,t = Aix\u0303i,t\u22121 + Biyi,t + Ciri,t\u22121 \u2212 Diyt\u2212\u03ba, (26)\nwhere ri,t\u22121 is a vector consisting of currently received observation data from the neighboring agents. The combination matrices defined as\n[ Bi Ci ] = \u03a3x,iH\u0304 T i ( H\u0304i\u03a3x,iH\u0304 T i + \u03a3\u0304ni )\u22121 , (27)\nAi \u25b3 =\n( I \u2212 [ Bi Ci ] H\u0304i ) L\u22121i , (28)\nDi \u25b3 = AiKiM, (29)\nwhere\u03a3xi \u25b3 = (I\u2212NiH\u0302i)\u03a3x. In Table 3, we tabulate the detailed description of the SDOL algorithm. Note that SDOL algorithm is based on the aggregation of information. Correspondingly, we can apply the time-windowing approach to the disclosure of local estimates and formulate a suboptimal efficient distributed online learning algorithm (SEDOL).\nIn the following we provide several remarks about the practical implementation of the SDOL algorithm.\nRemark 4.1:\n\u2022 We point out that in the update (26) the matrices Ai, Bi, Ci, and Di are independent from the data and they are time invariant. Hence, they can be pre-computed and installed onto the agents. Then, the SDOL algorithm basically takes the linear average of the previous estimate, the current measurement and received data, and observation data at time t \u2212 \u03ba.\n\u2022 Different from the conventional approaches, the SDOL algorithm requires to memorize previous observations. Note that if q < p/m, storing observation data individually rather than a linear combination, i.e., mt, might be more efficient in terms of memory usage.\n\u2022 The computational complexity of the SDOL algorithm is O(p2 + 2pmq) (or say O(p2) for q \u226a p/m) and the algorithm requires disclosure of a data vector with (m\u22121)\u00d7q dimensions.\n\u2022 Finally, the steady-state MSE of the SDOL algorithm is given by\nMSE = Tr { AiLi\u03a3xi } .\nIn the sequel, we also provide illustrative simulations showing the enhanced tracking performance due to the proposed algorithm over several distributed network scenarios."}, {"heading": "5. Illustrative Examples", "text": "In this section, we examine the performance of the introduced strategies under different scenarios. Consider a network of m = 20 agents where each agent i observes an underlying state x \u2208 Rp through yi,t = hTi x + ni,t. The unknown state x is a realization of standard-normal distribution. The observation noise is a zero-mean white Gaussian random process. For comparison, we choose hi \u2208 Rp randomly from a standard-normal distribution. We define a global MSE performance measure over a network as 1/m\n\u2211m i=1 MSE(i), where MSE(i) represents the MSE of\nthe ith agent. We compare the performance of the introduced algorithms with the diffusion implementation of Kalman updates (D-Kalman) [9], diffusion least mean square (D-LMS) [8], diffusion recursive least squares (D-RLS) [35], and consensus algorithms [36]. We utilize several combination rules. In the uniform combination rule [20], the combination weights given by agent i for neighbor j are chosen as\n\u03bbi, j =\n{\n1/(\u03c0i + 1) if j \u2208 Ni \u222a {i} 0 otherwise\nsince Ni excludes i. Correspondingly, in the relative variance rule [37, 38], \u03bbi, j is given by\n\u03bbi, j =\n   \u03c3\u22122n j \u2211 k\u2208Ni\u222a{i} \u03c3\u22122nk if j \u2208 Ni \u222a {i}\n0 otherwise.\nIn the D-LMS and consensus algorithms, we set \u00b5 = 0.1. In the D-RLS algorithm, we use the Laplacian combination rule [39] for the incremental update and the Metropolis combination rule for the spatial update.\nIn Figs. 5 and 6, we compare the time evolution of the global MSE of all algorithms for spaceinvariant and space-variant noise statistics, respectively for p = 1. We set the standard deviations\nof the noise parameters from a folded standard normal distribution in the space-variant case. The network topology is chosen arbitrarily and we can readily construct a corresponding spanning tree of that network. As an example, we can construct the spanning tree based on the paths from the most connected agent i to the others, i.e., i = arg max j \u03c0 j, and then we eliminate the multipaths [27]. The relative-variance cooperation rule that considers the noise statistics to adjust the combination weights performs better than the uniform combination rule for space-variant noise profiles across the network. In Fig. 7, we compare the performance of the proposed algorithms for relatively large state vector, i.e., p = 10, and space variant noise profiles. In Figs. 5, 6 and 7 we observe that the proposed algorithms achieve superior performance compared to the other algorithms through the optimal weight selection for the MMSE performance. Additionally, the performance of the OEDOL and ODOL algorithms are close to each other even though the OEDOL algorithm operates over the corresponding spanning tree.\nIn order to examine the influence of the network topology on the MMSE performance, we provide Fig. 8. The fully connected network (or the centralized network) achieves the best possible connection among the agents whereas the line network yields the least possible connection. Over a star network, the furthest distance between the agents is 2 and a single agent, i.e., the so-called pseudo-centralized agent, can access to the information of its neighbors directly. Notably, the star and line networks are the spanning tree of the fully connected network. We observe that the MMSE over the star network is close to the MMSE over the fully connected network. Additionally, the MMSE over even the line network converges to the MMSE over the fully connected network asymptotically.\nFor practical implementations, we propose the SDOL algorithm. In Fig. 9, we examine the influence of the time-windowing depth on the global MSE performance of the SDOL algorithm. We use the SDOL algorithms with time-windowing depths \u03ba = 20 and \u03ba = 50. The time evolution of the MSE performance differs from the MMSE behavior because the SDOL algorithms inherently assume that even at initial time, i.e., t = 0, agents have legitimate observations from\nt = \u03ba to t = 0. By excluding this data in time through sliding time-window, the SDOL algorithms eventually reach the best performance at the corresponding time-windowing depths and maintain this performance afterwards.\nFinally, we evaluate the MSE performance of the SDOL algorithm for abruptly changing state. Assume that the state vector changes at time t = 400 abruptly. In particular, we have chosen a new realization from the standard normal distribution. In Fig. 10, we compare the SDOL algorithms with the D-LMS algorithm. Note that we set the step size of the D-LMS algorithm as \u00b5 = 0.025 so that at steady state (if there were no state change) the D-LMS and SDOL with\n\u03ba = 20 algorithms achieve the same MSE performance for fair tracking performance comparison. The sliding time-window provides relative robustness against abrupt state changes due to the exclusion of the effect of the previous observations. Hence, the SDOL algorithm can also provide enhance tracking performance in the practical applications."}, {"heading": "6. Conclusion", "text": "The distributed algorithms have attracted significant attention due to their wide spread applicability to highly complex structures from biological systems to social and economical networks.\nHowever, there are still challenges for disclosure and utilization of information among agents. We introduce the ODOL algorithm achieving the MMSE performance for Gaussian state and noise statistics and the LMMSE performance for arbitrary statistical profiles. The ODOL algorithm has consensus-like iterations for streaming data over an arbitrary network topology via the aggregation of information at each agent rather than propagation of information through the disclosure of local estimates. Importantly, the disclosure of the local estimate is sufficient only over the certain network topologies, e.g., over the introduced tree network involving cell structures. Moreover, the aggregation of information based approach can require excessive communication load. Therefore, in order to reduce the communication load, we exploit the network structures. We introduce the OEDOL algorithm that achieves the MMSE performance through the disclosure of local estimate over tree networks. We also observe that the MSE performance of the ODOL and OEDOL algorithms are close to each other over even a highly connected arbitrary network while the OEDOL provides reduced communication load in general. Finally, we introduce time-windowing approach for practical applications due to the reduced complexity. The sub-optimal approaches possess consensus-like iterations with time-invariant combination weights that should be calculated only once. Additionally, the sub-optimal approaches provide enhanced tracking performance against changing state due to the sliding of time-window."}, {"heading": "7. Acknowledgment", "text": "This work is in part supported by the Outstanding Researcher Programme of Turkish Academy of Sciences and TUBITAK projects, Contract no: 112E161 and 113E517."}], "references": [{"title": "A", "author": ["M.K. Banavar", "J.J. Zhang", "B. Chakraborty", "H. Kwon", "Y. Li", "H. Jiang", "A. Spanias", "C. Tepedelenlioglu", "C. Chakrabarti"], "venue": "P.-Suppappola, An overview of recent advances on distributed and agile sensing algorithms and implementation, Digital Signal Processing 39 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Diffusion strategies for adaptation and learning over networks: An examination of distributed strategies and network behavior", "author": ["A.H. Sayed", "S.-Y. Tu", "J. Chen", "X. Zhao", "Z.J. Towfic"], "venue": "IEEE Signal Processing Magazine 30 (3) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed estimation in diffusion networks using affine least-squares combiners", "author": ["J. Fernandez-Bes", "L.A. Azpicueta-Ruiz", "J. Arenas-Garcia", "M.T.M. Silva"], "venue": "Digital Signal Processing 36 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Decentralized activation in sensor networks-global games and adaptive filtering games", "author": ["V. Krishnamurthy"], "venue": "Digital Signal Processing 21 (5) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Non-Bayesian social learning", "author": ["A. Jadbabaie", "P. Molavi", "A. Sandroni", "A. Tahbaz-Salehi"], "venue": "Games and Economic Behavior 76 (1) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Dynamic games and applications", "author": ["D. Acemoglu", "A. Ozdaglar"], "venue": "Opinion dynamics and learning in social networks 1 (1) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Social and Economic Networks", "author": ["M. Jackson"], "venue": "Princeton University Press, Princeton, N.J.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Diffusion least-mean squares over adaptive networks: Formulation and performance analysis", "author": ["C.G. Lopes", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing 56 (7) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Diffusion strategies for distributed Kalman filtering and smoothing", "author": ["F.S. Cattivelli", "A.H. Sayed"], "venue": "IEEE Transactions on Automatic Control 55 (9) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Online learning of dynamic parameters in social networks", "author": ["S. Shahrampour", "A. Rakhlin", "A. Jadbabaie"], "venue": "in: Neural Information Processing Systems", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed LMS for consensus-based in-network adaptive processing", "author": ["I. Schizas", "G. Mateos", "G. Giannakis"], "venue": "IEEE Transactions on Signal Processing 57 (6) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Compressive diffusion strategies over distributed networks for reduced communication load", "author": ["M.O. Sayin", "S.S. Kozat"], "venue": "IEEE Transactions on Signal Processing 62 (20) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Chemical Kinetics and Dynamics", "author": ["J.I. Steinfeld", "J.S. Francisco", "W.L. Hase"], "venue": "Pearson", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Single bit and reduced dimension diffusion strategies over distributed networks", "author": ["M.O. Sayin", "S.S. Kozat"], "venue": "IEEE Signal Processing Letters 20 (10) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed kalman filtering for sensor networks", "author": ["R. Olfati-Saber"], "venue": "in: Proceedings of 46th IEEE Conference on Decision and Control", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "SOI: Distributed Kalman filtering with low-cost communications using the sign of innovations", "author": ["A. Ribeiro", "G.B. Giannakis", "S.I. Roumeliotis"], "venue": "IEEE Transactions on Signal Processing 54 (12) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Distributed Kalman filtering: a bibliographic review", "author": ["M.S. Mahmoud", "H.M. Khalid"], "venue": "IET Control Theory and Applications 7 (4) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimal decentralized Kalman filter and Lainiotis filter", "author": ["N. Assimakis", "M. Adam", "M. Koziri", "S. Voliotis", "K. Asimakis"], "venue": "Digital Signal Processing 23 (1) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Agreeing to disagree", "author": ["R.J. Aumann"], "venue": "The Annals of Statistics 4 (6) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1976}, {"title": "Convergence in multiagent coordination", "author": ["V.D. Blondel", "J.M. Hendrickx", "A. Olshevsky", "J.N. Tsitsiklis"], "venue": "consensus, and flocking, in: Proceedings of 44th IEEE Conference on Decision and Control (CDC-ECC)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Locally constructed algorithms for distributed computations in ad-hoc networks", "author": ["D.S. Scherber", "H.C. Papadopoulos"], "venue": "in: Proceedings of Information Processing Sensor Networks (IPSN)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Equation of state calculations by fast computing machines", "author": ["N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller"], "venue": "The Journal of Chemical Physics 21 (6) ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1953}, {"title": "Fundamentals of Adaptive Filtering", "author": ["A.H. Sayed"], "venue": "Wiley, New York", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Reaching a consensus", "author": ["M.H. DeGroot"], "venue": "Journal of the American Statistical Association 69 (345) ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1974}, {"title": "Optimal Filtering", "author": ["B.D.O. Anderson", "J.B. Moore"], "venue": "Prentice-Hall, Inc., Englewood Cliffs, NJ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1979}, {"title": "Implementing Discrete Mathematics: Combinatorics and Graph Theory with Mathematica", "author": ["S. Skiena"], "venue": "Addison- Wesley, Reading, MA", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1990}, {"title": "Spanning Trees and Optimization Problems (Discrete Mathematics and Its Applications)", "author": ["B.Y. Wu", "K.-M. Chao"], "venue": "CRC Press, New York", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "A distributed algorithm for minimum-weight spanning trees", "author": ["R.G. Gallager", "P.A. Humblet", "P.M. Spira"], "venue": "ACM Transactions on Programming Languages and Systems 5 (1) ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1983}, {"title": "Distributed Computing: A Locality-Sensitive Approach", "author": ["D. Peleg"], "venue": "SIAM, Philadelphia, PA", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "Unconditional lower bounds on the time-approximation tradeoffs for the distributed minimum spanning tree problem", "author": ["M. Elkin"], "venue": "in: Proceedings of 36th ACM Symposium on Theory of Computing (STOC)", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "Distributed algorithms for constructing approximate minimum spanning trees in wireless sensor networks", "author": ["M. Khan", "G. Pandurangan", "V.S.A. Kumar"], "venue": "IEEE Transactions on Parallel and Distributed Systems 20 (1) ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimal combination rules for adaptation and learning over networks", "author": ["J. Chen", "A.H. Sayed"], "venue": "in: Proceedings of 50th Annual Allerton Conference on Communication, Control, and Computing (Allerton)", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Diffusion recursive least-squares for distributed estimation over adaptive networks", "author": ["F.S. Cattivelli", "C.G. Lopes", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing 56 (5) ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Diffusion strategies outperform consensus strategies for distributed estimation over adaptive networks", "author": ["S.Y. Tu", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing 60 (12) ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Diffusion LMS for distributed estimation over adaptive networks", "author": ["F.S. Cattivelli", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing 58 (3) ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimal combination rules for adaptation and learning over networks", "author": ["S.-Y. Tu", "A.H. Sayed"], "venue": "in: Proceedings of 4th IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Consensus problems in networks of agents with switching topology and time-delays", "author": ["R. Olfati-Saber", "R.M. Murray"], "venue": "IEEE Transactions on Automatic Control 49 (9) ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": ", fast response time, relative to the centralized networks by distributing the processing power over the networks [1, 2, 3].", "startOffset": 114, "endOffset": 123}, {"referenceID": 1, "context": ", fast response time, relative to the centralized networks by distributing the processing power over the networks [1, 2, 3].", "startOffset": 114, "endOffset": 123}, {"referenceID": 2, "context": ", fast response time, relative to the centralized networks by distributing the processing power over the networks [1, 2, 3].", "startOffset": 114, "endOffset": 123}, {"referenceID": 3, "context": "This framework is conveniently used to model highly complex structures from defense applications to social and economical networks [4, 5, 6, 7].", "startOffset": 131, "endOffset": 143}, {"referenceID": 4, "context": "This framework is conveniently used to model highly complex structures from defense applications to social and economical networks [4, 5, 6, 7].", "startOffset": 131, "endOffset": 143}, {"referenceID": 5, "context": "This framework is conveniently used to model highly complex structures from defense applications to social and economical networks [4, 5, 6, 7].", "startOffset": 131, "endOffset": 143}, {"referenceID": 6, "context": "This framework is conveniently used to model highly complex structures from defense applications to social and economical networks [4, 5, 6, 7].", "startOffset": 131, "endOffset": 143}, {"referenceID": 1, "context": ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.", "startOffset": 2, "endOffset": 24}, {"referenceID": 7, "context": ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.", "startOffset": 2, "endOffset": 24}, {"referenceID": 8, "context": ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.", "startOffset": 2, "endOffset": 24}, {"referenceID": 9, "context": ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.", "startOffset": 2, "endOffset": 24}, {"referenceID": 4, "context": ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.", "startOffset": 2, "endOffset": 24}, {"referenceID": 5, "context": ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.", "startOffset": 2, "endOffset": 24}, {"referenceID": 6, "context": ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.", "startOffset": 2, "endOffset": 24}, {"referenceID": 1, "context": ", diffusion or consensus based estimation algorithms [2, 8, 11, 12], due to processing power related practical concerns.", "startOffset": 53, "endOffset": 67}, {"referenceID": 7, "context": ", diffusion or consensus based estimation algorithms [2, 8, 11, 12], due to processing power related practical concerns.", "startOffset": 53, "endOffset": 67}, {"referenceID": 10, "context": ", diffusion or consensus based estimation algorithms [2, 8, 11, 12], due to processing power related practical concerns.", "startOffset": 53, "endOffset": 67}, {"referenceID": 11, "context": ", diffusion or consensus based estimation algorithms [2, 8, 11, 12], due to processing power related practical concerns.", "startOffset": 53, "endOffset": 67}, {"referenceID": 12, "context": "In chemical kinetics, this phenomena is called as the rate determining step (RDS) such that speed of a chain of reactions can be approximately determined by the speed of the slowest reaction [13].", "startOffset": 191, "endOffset": 195}, {"referenceID": 3, "context": "To this end, in the design of distributed networks, we should also consider the trade-off in terms of the communication power and the estimation performance [4, 14, 12].", "startOffset": 157, "endOffset": 168}, {"referenceID": 13, "context": "To this end, in the design of distributed networks, we should also consider the trade-off in terms of the communication power and the estimation performance [4, 14, 12].", "startOffset": 157, "endOffset": 168}, {"referenceID": 11, "context": "To this end, in the design of distributed networks, we should also consider the trade-off in terms of the communication power and the estimation performance [4, 14, 12].", "startOffset": 157, "endOffset": 168}, {"referenceID": 13, "context": "In [14, 12], the authors study this problem by compressing the data before the transmission and extracting the compressed information adaptively.", "startOffset": 3, "endOffset": 11}, {"referenceID": 11, "context": "In [14, 12], the authors study this problem by compressing the data before the transmission and extracting the compressed information adaptively.", "startOffset": 3, "endOffset": 11}, {"referenceID": 14, "context": "Correspondingly, there have been abundant attempts to formulate distributed Kalman filter [15, 16, 17, 18].", "startOffset": 90, "endOffset": 106}, {"referenceID": 15, "context": "Correspondingly, there have been abundant attempts to formulate distributed Kalman filter [15, 16, 17, 18].", "startOffset": 90, "endOffset": 106}, {"referenceID": 16, "context": "Correspondingly, there have been abundant attempts to formulate distributed Kalman filter [15, 16, 17, 18].", "startOffset": 90, "endOffset": 106}, {"referenceID": 17, "context": "Correspondingly, there have been abundant attempts to formulate distributed Kalman filter [15, 16, 17, 18].", "startOffset": 90, "endOffset": 106}, {"referenceID": 18, "context": "As explained in [19], once agent-1 transmits an information to agent-2, this information becomes common knowledge between the agents.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": ", the uniform rule [20], the Laplacian rule [21] or the Metropolis rule [22].", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": ", the uniform rule [20], the Laplacian rule [21] or the Metropolis rule [22].", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": ", the uniform rule [20], the Laplacian rule [21] or the Metropolis rule [22].", "startOffset": 72, "endOffset": 76}, {"referenceID": 1, "context": ", each agent observes diverse signal-to-noise ratios, by ignoring the variation in noise, these rules yield severe decline in the estimation performance [2].", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "In such cases the agents can even perform better without cooperation [2].", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "Notably, the ODOL algorithm utilizes the aggregation of information at each agent via time-stamped information exchange and is different from the conventional algorithms that benefit from the diffusion of local estimates [2, 8, 11].", "startOffset": 221, "endOffset": 231}, {"referenceID": 7, "context": "Notably, the ODOL algorithm utilizes the aggregation of information at each agent via time-stamped information exchange and is different from the conventional algorithms that benefit from the diffusion of local estimates [2, 8, 11].", "startOffset": 221, "endOffset": 231}, {"referenceID": 10, "context": "Notably, the ODOL algorithm utilizes the aggregation of information at each agent via time-stamped information exchange and is different from the conventional algorithms that benefit from the diffusion of local estimates [2, 8, 11].", "startOffset": 221, "endOffset": 231}, {"referenceID": 22, "context": "We assume that the variance of the noise signals are known, which can also be readily estimated from the data [23].", "startOffset": 110, "endOffset": 114}, {"referenceID": 22, "context": "Naturally, the agents can learn the true state, under certain regularity conditions [23], irrespective of cooperation among them, provided that their own measurements are not biased and sufficient to estimate the true state in time.", "startOffset": 84, "endOffset": 88}, {"referenceID": 1, "context": "However, through the cooperation of agents, we can increase the learning rate significantly [2].", "startOffset": 92, "endOffset": 95}, {"referenceID": 11, "context": "Since the communication load is crucial for the applicability of the distributed learning algorithms [12, 14], we also seek to reduce the communication load yet achieve the MMSE performance.", "startOffset": 101, "endOffset": 109}, {"referenceID": 13, "context": "Since the communication load is crucial for the applicability of the distributed learning algorithms [12, 14], we also seek to reduce the communication load yet achieve the MMSE performance.", "startOffset": 101, "endOffset": 109}, {"referenceID": 1, "context": "In the consensus and diffusion implementations, the agents disclose the local estimates, which requires relatively reduced communication load [2, 24, 11, 8].", "startOffset": 142, "endOffset": 156}, {"referenceID": 23, "context": "In the consensus and diffusion implementations, the agents disclose the local estimates, which requires relatively reduced communication load [2, 24, 11, 8].", "startOffset": 142, "endOffset": 156}, {"referenceID": 10, "context": "In the consensus and diffusion implementations, the agents disclose the local estimates, which requires relatively reduced communication load [2, 24, 11, 8].", "startOffset": 142, "endOffset": 156}, {"referenceID": 7, "context": "In the consensus and diffusion implementations, the agents disclose the local estimates, which requires relatively reduced communication load [2, 24, 11, 8].", "startOffset": 142, "endOffset": 156}, {"referenceID": 24, "context": "2If the inverse fails to exist, a pseudo inverse can replace the inverse [25].", "startOffset": 73, "endOffset": 77}, {"referenceID": 25, "context": ", connected and undirected without any cycles [26].", "startOffset": 46, "endOffset": 50}, {"referenceID": 26, "context": "In the literature, there exists numerous distributed algorithms for minimum spanning tree construction [27, 28, 29, 30, 31].", "startOffset": 103, "endOffset": 123}, {"referenceID": 27, "context": "In the literature, there exists numerous distributed algorithms for minimum spanning tree construction [27, 28, 29, 30, 31].", "startOffset": 103, "endOffset": 123}, {"referenceID": 28, "context": "In the literature, there exists numerous distributed algorithms for minimum spanning tree construction [27, 28, 29, 30, 31].", "startOffset": 103, "endOffset": 123}, {"referenceID": 29, "context": "In the literature, there exists numerous distributed algorithms for minimum spanning tree construction [27, 28, 29, 30, 31].", "startOffset": 103, "endOffset": 123}, {"referenceID": 30, "context": "In the literature, there exists numerous distributed algorithms for minimum spanning tree construction [27, 28, 29, 30, 31].", "startOffset": 103, "endOffset": 123}, {"referenceID": 31, "context": "Similarly, in [32, 33, 34], the authors show that the performance of the diffusion based algorithms could approach the performance of a fully connected network under certain regularity conditions.", "startOffset": 14, "endOffset": 26}, {"referenceID": 8, "context": "We compare the performance of the introduced algorithms with the diffusion implementation of Kalman updates (D-Kalman) [9], diffusion least mean square (D-LMS) [8], diffusion recursive least squares (D-RLS) [35], and consensus algorithms [36].", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "We compare the performance of the introduced algorithms with the diffusion implementation of Kalman updates (D-Kalman) [9], diffusion least mean square (D-LMS) [8], diffusion recursive least squares (D-RLS) [35], and consensus algorithms [36].", "startOffset": 160, "endOffset": 163}, {"referenceID": 32, "context": "We compare the performance of the introduced algorithms with the diffusion implementation of Kalman updates (D-Kalman) [9], diffusion least mean square (D-LMS) [8], diffusion recursive least squares (D-RLS) [35], and consensus algorithms [36].", "startOffset": 207, "endOffset": 211}, {"referenceID": 33, "context": "We compare the performance of the introduced algorithms with the diffusion implementation of Kalman updates (D-Kalman) [9], diffusion least mean square (D-LMS) [8], diffusion recursive least squares (D-RLS) [35], and consensus algorithms [36].", "startOffset": 238, "endOffset": 242}, {"referenceID": 19, "context": "In the uniform combination rule [20], the combination weights given by agent i for neighbor j are chosen as \u03bbi, j = { 1/(\u03c0i + 1) if j \u2208 Ni \u222a {i} 0 otherwise since Ni excludes i.", "startOffset": 32, "endOffset": 36}, {"referenceID": 34, "context": "Correspondingly, in the relative variance rule [37, 38], \u03bbi, j is given by", "startOffset": 47, "endOffset": 55}, {"referenceID": 35, "context": "Correspondingly, in the relative variance rule [37, 38], \u03bbi, j is given by", "startOffset": 47, "endOffset": 55}, {"referenceID": 36, "context": "In the D-RLS algorithm, we use the Laplacian combination rule [39] for the incremental update and the Metropolis combination rule for the spatial update.", "startOffset": 62, "endOffset": 66}, {"referenceID": 26, "context": ", i = arg max j \u03c0 j, and then we eliminate the multipaths [27].", "startOffset": 58, "endOffset": 62}], "year": 2017, "abstractText": "We construct optimal estimation algorithms over distributed networks for state estimation in the mean-square error (MSE) sense. Here, we have a distributed collection of agents with processing and cooperation capabilities. These agents continually observe a noisy version of a desired state of the nature through a linear model and seek to learn this state by interacting with each other. Although this problem has attracted significant attention and extensively been studied in several different fields including machine learning theory to signal processing, all the well-known strategies achieve suboptimal learning performance in the MSE sense. To this end, we provide algorithms that achieve distributed minimum MSE (MMSE) performance over an arbitrary network topology based on the aggregation of information at each agent. This approach differs from the diffusion of information across network, i.e., exchange of local estimates per time instance. Importantly, we show that exchange of local estimates is sufficient only over the certain network topologies. By inspecting these network structures, we also propose strategies that achieve the distributed MMSE performance also through the diffusion of information such that we can substantially reduce the communication load while achieving the best possible MSE performance. For practical implementations we provide approaches to reduce the complexity of the algorithms through the time-windowing of the observations. Finally, in the numerical examples, we demonstrate the superior performance of the introduced algorithms in the MSE sense due to optimal estimation.", "creator": "LaTeX with hyperref package"}}}