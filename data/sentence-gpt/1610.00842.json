{"id": "1610.00842", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Oct-2016", "title": "Chinese Event Extraction Using DeepNeural Network with Word Embedding", "abstract": "A lot of prior work on event extraction has exploited a variety of features to represent events. Such methods have several drawbacks: 1) the features are often specific for a particular domain and do not generalize well; 2) the features are derived from various linguistic analyses and are error-prone; and 3) some features may be expensive and require domain expert. In this paper, we develop a Chinese event extraction system that uses word embedding vectors to represent language, and deep neural networks to learn the abstract feature representation in order to greatly reduce the effort of feature engineering for those who have not yet gained access to the full dataset. In our paper, we will present a detailed analysis of the deep neural network from a functional model of the event extraction and the deep neural network from a corpus callosum. In order to illustrate the approach we employ, we first demonstrate how these neural networks could be used in the real world. In an alternative, we will present our first example of a \"C\" model with the following data:\n\nC:C:C\nC:C\nC:C\nC:C\nOur data was extracted by this method, and our results were compared to the rest of the results in this paper.", "histories": [["v1", "Tue, 4 Oct 2016 04:56:23 GMT  (90kb,D)", "http://arxiv.org/abs/1610.00842v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yandi xia", "yang liu"], "accepted": false, "id": "1610.00842"}, "pdf": {"name": "1610.00842.pdf", "metadata": {"source": "CRF", "title": "Chinese Event Extraction Using Deep Neural Network with Word Embedding", "authors": ["Yandi Xia"], "emails": ["yandixia@hlt.utdallas.edu", "yangl@hlt.utdallas.edu"], "sections": [{"heading": "1 Introduction", "text": "Event extraction is a task of information extraction. It is a complicated task including many sub-tasks. In ACE, event extraction includes four sub-tasks (Ahn, 2006; Grishman et al., 2005): event trigger identification, trigger type classification, argument identification, and argument role classification. In this paper, we focus on trigger identification\non Chinese. Event trigger identification is often the first step of an event extraction system. It identifies the words that most explicitly indicate the occurrence of events. For example, in this sentence: Yahoo announced its acquisition of KIMO website. Here \u2018acquisition\u2019 is an event trigger, and it triggers one of the subtypes of BUSINESS event type \u2013 Merge-Org event.\nThere are some trends in the research for event extraction in the past years. First, much prior work has focused on exploiting rich language features like syntax, part-of-speech (POS), parsing structure, named entities, synonym dictionaries, etc. For example, Chen and Ji (2009) and Chen and NG (2012) both used a variety of such features, achieving the stateof-the-art performance for Chinese event extraction. In English corpus, the earliest work on ACE corpus (Ahn, 2006) also designed a lot of features for this task and set the baseline of English event extraction. However, these systems have some common drawbacks: a) those features vary from corpus to corpus, and language to language. One almost needs to setup the whole system for new data. For example, (Chen and Ji, 2009) designed a character based system with character based features for Chinese event extraction, in order to reduce the errors caused by word segmentation. However, such features make no sense for English corpora. On the other hand, in the English systems, features are needed to represent phenomena such as tenses, whereas in Chinese, this is not a problem. Same thing happens if one wants to migrate an event extraction system from news article domain to biomedical domains. Most of the features designed previously are not very useful for the new data. b) those features depend on some fundamenar X iv :1 61 0. 00 84 2v 1 [ cs .C L ] 4\nO ct\n2 01\n6\ntal NLP modules (e.g., POS tagging, parsing, named entity recognition), which are not perfect and introduce a lot of noise in the derived features. This is especially true for Chinese (and languages with low resources), in which many NLP tools do not perform well. c) some of features are expensive to acquire, because expert knowledge or resources are required, for example, synonym dictionaries.\nSecond, due to the lack of labeled data for event extraction, more and more research started focusing on semi-supervised learning (Liao and Grishman, 2010; Ali, 2014; Li et al., 2014), unsupervised learning (Rusu et al., 2014), and distantly supervised learning (Reschke et al., ). These studies show that additional unlabeled data is very useful to acquire more information or understand about language. For example, large unlabeled data can be used to learn the patterns for event extraction (Liao and Grishman, 2010; Huang and Riloff, 2012).\nMotivated by the need to overcome the problem with designing features and the potential benefit of unlabeled data, in this pilot study, we set to answer three questions: (1) can we get around feature engineering in event trigger detection using the deep neural networks and word embedding? (2) is word embedding a better representation for this task than the large set of carefully crafted discrete features? (3) can we effectively leverage unlabeled data for word embedding in the trigger detection task?\nWord embedding has been proved to be very successful combining with deep learning, the increasingly popular learning method. After Collobert et al. (2011) and Socher et al. (2012) brought up a unified deep structure for NLP tasks, much work using this combination has emerged to challenge the traditional feature based methods. Collobert et al. (2011) showed great performance in tasks such as part-ofspeech-tagging, chunking, named entity recognition, and semantic role labeling with one unified deep structure, which is comparable to those feature based methods. Li and Liu (2014)\u2019s work applied word embedding on text normalization task which use the similarity between word vector to represent the sematic relationship between two words. Qi et al.\n(2014) and Zheng et al. (2013) adopted this structure for Chinese NLP tasks, and beat the state-of-the-art performance in word segmentation, part-of-speech-tagging and named entity recognition tasks. In addition, Li et al. (2015) applied word embedding on two large corpus, one is a set of news articles and the other is their corresponding summaries. Then for each token in two word embedding, they design additional features to help estimate their importance for final summary. The experiments showed that the feature based on word embedding are very useful. Inspired by these successful efforts, in this work we design a deep structure event extraction system which takes word embedding representation of Chinese data. We expect that deep learning can learn abstract feature representation and word embedding can effectively represent semantic and syntactic similarity between words, and thus can help identify new event trigger words that are not in the training examples. For example, if beat is a trigger word in the training set, it is hard to use synonym information to determine word attack is a trigger in the test data. However, word embedding may be able to find such semantic similar words, in either a supervised or unsupervised fashion, and improve system\u2019s recall. To our knowledge, there is no prior work that has explored the use of word embedding and deep learning for Chinese event extraction.\nIn this work, we build a deep neural network model which represents Chinese characters with word embedding vectors. To leverage unlabeled data, we generate word embeddings from that and use them for pre-training in DNN. We evaluate our methods on the Chinese ACE corpus. Different from previous work that used ground truth information such as named entities, time and value labels, we use a more realistic setup with such information automatically generated. Our results show that we can achieve better performance using DNN than a feature-based maximum entropy classifier for event trigger detection, and then using unlabeled data for word embedding pretraining has additional benefit.\nThe rest of the paper is structured as follows. Section 2 describes the event trigger identification methods, including the maxi-\nmum entropy classifier that uses a set of rich features, and our DNN and word embedding system. Section 3 shows the experimental results. Conclusions and future work appear in Section 4."}, {"heading": "2 Event Trigger Identification", "text": "The follow introduces the two methods we use for event trigger word identification."}, {"heading": "2.1 Baseline", "text": "Following the works of (Chen and Ji, 2009; Chen and NG, 2012), we build the feature based baseline system. As observed in (Chen and Ji, 2009), there is a considerable portion of segmentation errors because of the low performance of the Chinese word segmentation tool and the ambiguity in human labels. Therefore, we model it as a sequence-labeling task and use the BIO encoding method (each character either begins, is inside, or outside of a trigger word) As for the features, both papers did a lot in feature analysis, therefore we adopt most of their features, listed below:\n\u2022 Lexical features: current character; the characters that surround it; current POS; and the POS that surround it; the combination of current character and its POS.\n\u2022 Syntactic features: depth of the current character in the parse tree; path from the current node to the root; the phrase structure expanded from the parent node; phrase type.\n\u2022 Semantic dictionary: whether it is in the predicate list from (Xue and Palmer, 2009); synonym entry number of the current character.\n\u2022 Nearest entity: the type of the left nearest entity to the current character; the type of the right nearest entity to the current character. Here, distance is measured by text length.\nIn (Chen and Ji, 2009; Chen and NG, 2012), human labeled named entity information was used. In this study, we use a more realistic setup \u2013 we use automatically identified named entities in our features. In our experiments, we will evaluate the effect of using such imperfect features."}, {"heading": "2.2 DNN Model", "text": "We followed the work of (Collobert et al., 2011) and designed a similar DNN architecture, as shown in Figure 1. Considering the word segmentation problem in Chinese, we also choose the window based character stream as input. Here we model the input character as its index in the character dictionary. The first layer is a lookup table whose entries are word embedding vectors. Then we concatenate those selected vectors and pass it to the hidden layers. In the hidden layers we use nonlinear Tanh as the activation function. At the top of the structure, we put softmax to output the probabilities of the character being a part of a trigger. Note in this method, we do not use any linguistically motivated features. The input used in this DNN is just word embedding vectors.\nAll the weights in the DNN including the word embedding vectors are trained automatically in the back-propagation process using the gradient descent algorithm. During testing, the DNN system gives the probabilities of each character as BIO tag, we treat them as emission probabilities, together with the transition probability from baseline CRF system, we use Viterbi decoding process to conduct the final prediction. Li and Liu (2015a) and Li and Liu (2015b) showed that using separated but well trained weight in Viterbi decoding can give improvement in certain conditions. The DNN tool we use is python based software Theano (Bastien et al., 2012)."}, {"heading": "2.3 Using Unlabeled Data", "text": "Unlabeled data contains a lot of abstract syntax and semantic information that can be very useful to NLP tasks. In order to take advantage of unlabeled data (simply Chinese sentences, no event labels), we first use the RNN based word2vec toolkit from to generate the initial word vector dictionary as pre-training. These vectors then will act like part of the DNN weights and change their values in the supervised learning progress via back propagation."}, {"heading": "3 Experiments", "text": "The corpus we use is ACE 2005 Chinese corpus. There are 633 documents in the corpus. We randomly choose 66 documents as the test data, and 567 documents as training data, which is similar to (Chen and Ji, 2009). For performance metric, we use precision, recall, and F1 score. If the offset and word length of the identified character chunk exactly match the gold value, we consider that the corresponding trigger is correctly identified. The unlabeled data we use is from (Graff and Chen, 2005). It contains 2,466,840 Chinese newswire documents, totaling 3.9 GB. We use 100K documents of them.\nTable 1 shows the trigger identification results for different methods. For the baseline system using ME classifier, we show two results. One is using the named entities obtained from the Stanford coreNLP tool, and the other one uses ground truth NER labels. It is clear that ground truth NER information can boost the performance considerably. However, such accurate information is very hard to get in real world cases.\nFor the DNN model, we also report two results, without using the unlabeled data to pretrain the network, vs. the one that takes advantage of large amount of unlabeled data for pretraining. We can see that pretraining improves the system performance significantly. In addition, during the experiments, we noticed that DNN without pretraining converges far more slower than that with pretraining.\nComparing the results using the DNN and the ME classifier, we can see even without using the unlabeled data, the DNN results are better than the feature-based ME classi-\nfier (using automatically generated NE information). This suggests that no feature engineering is required in the DNN model \u2013 it simply uses the character embedding to generate more effective abstract feature representation for this classification task. When unlabeled data is incorporated, the DNN performance is much better, even outperforming using the reference NE information in the ME classifier. In addition, the improved DNN results over the ME method are because of the higher recall, which is consistent with our expectation \u2013 using word embedding vectors can find semantically similar words.\nFigure 2 shows the F1 score when varying the vector size of the character embeddings. This pattern is similar to that for other tasks. When the size is too small, it cannot represent detailed information of the language characteristics; when it is too big, the system has too many parameters and loses the power of the abstract representation."}, {"heading": "4 Conclusions and Future Work", "text": "In this paper, we applied word embeddings to represent Chinese characters. Our result on the event trigger identification task shows that it is a better representation compared to human designed language specific features. We also show that the combination of word embedding and DNN outperform the classifier that relies on a large set of linguistic features, and that this framework can effectively leverage unlabeled data to improve system performance. This is the first study exploring deep learning and word embedding for Chinese event extraction. In our current work, we use a relatively small window of characters as the input of the DNN. For future work, we plan to find a way to model longer context in DNN for event extraction. Furthermore, we will move on CNN and RNN architecture for this task."}, {"heading": "Acknowledgments", "text": ""}], "references": [{"title": "The stages of event extraction", "author": ["David Ahn"], "venue": "In Proceedings of the Workshop on Annotating and Reasoning about Time and Events,", "citeRegEx": "Ahn.,? \\Q2006\\E", "shortCiteRegEx": "Ahn.", "year": 2006}, {"title": "Event extraction for balkan languages", "author": ["H Ali"], "venue": "EACL", "citeRegEx": "Ali.,? \\Q2014\\E", "shortCiteRegEx": "Ali.", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Language specific issue and feature exploration in chinese event extraction", "author": ["Chen", "Ji2009] Zheng Chen", "Heng Ji"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter", "citeRegEx": "Chen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "Joint modeling for chinese event extraction with rich linguistic features", "author": ["Chen", "NG2012] Chen Chen", "V Incent NG"], "venue": "COLING. Citeseer", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Bootstrapped training of event extraction classifiers", "author": ["Huang", "Riloff2012] Ruihong Huang", "Ellen Riloff"], "venue": "In Proceedings of the 13th Conference of the European Chapter of the Association", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Improving text normalization via unsupervised model and discriminative reranking", "author": ["Li", "Liu2014] Chen Li", "Yang Liu"], "venue": "In Proceedings of ACL", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Improving named entity recognition in tweets via detecting non-standard words", "author": ["Li", "Liu2015a] Chen Li", "Yang Liu"], "venue": "In Proceedings of ACL", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "2015b. Joint pos tagging and text normalization for informal text", "author": ["Li", "Liu2015b] Chen Li", "Yang Liu"], "venue": "In Proceedings of IJCAI", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Employing event inference to improve semi-supervised chinese event extraction", "author": ["Li et al.2014] Peifeng Li", "Qiaoming Zhu", "Guodong Zhou"], "venue": "Proceedings of COLING", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Using external resources and joint learning for bigram weighting in ilp-based multidocument summarization", "author": ["Chen Li", "Yang Liu", "Lin Zhao"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Filtered ranking for bootstrapping in event extraction", "author": ["Liao", "Grishman2010] Shasha Liao", "Ralph Grishman"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics,", "citeRegEx": "Liao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liao et al\\.", "year": 2010}, {"title": "Deep learning for character-based information extraction", "author": ["Qi et al.2014] Yanjun Qi", "Sujatha G Das", "Ronan Collobert", "Jason Weston"], "venue": "In Advances in Information Retrieval,", "citeRegEx": "Qi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qi et al\\.", "year": 2014}, {"title": "Unsupervised techniques for extracting and clustering complex events in news", "author": ["Rusu et al.2014] Delia Rusu", "James Hodson", "Anthony Kimball"], "venue": "ACL", "citeRegEx": "Rusu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2014}, {"title": "Deep learning for nlp (without magic)", "author": ["Yoshua Bengio", "Christopher D Manning"], "venue": "In Tutorial Abstracts of ACL", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Adding semantic roles to the chinese treebank", "author": ["Xue", "Palmer2009] Nianwen Xue", "Martha Palmer"], "venue": "Natural Language Engineering,", "citeRegEx": "Xue et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2009}, {"title": "Deep learning for chinese word segmentation and pos tagging", "author": ["Zheng et al.2013] Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu"], "venue": "In EMNLP,", "citeRegEx": "Zheng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "In ACE, event extraction includes four sub-tasks (Ahn, 2006; Grishman et al., 2005): event trigger identification, trigger type classification, argument identification, and argument role classification.", "startOffset": 49, "endOffset": 83}, {"referenceID": 0, "context": "In English corpus, the earliest work on ACE corpus (Ahn, 2006) also designed a lot of features for this task and set the baseline of English event extraction.", "startOffset": 51, "endOffset": 62}, {"referenceID": 1, "context": "(Liao and Grishman, 2010; Ali, 2014; Li et al., 2014), unsupervised learning (Rusu et al.", "startOffset": 0, "endOffset": 53}, {"referenceID": 7, "context": "(Liao and Grishman, 2010; Ali, 2014; Li et al., 2014), unsupervised learning (Rusu et al.", "startOffset": 0, "endOffset": 53}, {"referenceID": 14, "context": ", 2014), unsupervised learning (Rusu et al., 2014), and distantly supervised learning (Reschke et al.", "startOffset": 31, "endOffset": 50}, {"referenceID": 5, "context": "After Collobert et al. (2011) and Socher et al.", "startOffset": 6, "endOffset": 30}, {"referenceID": 5, "context": "After Collobert et al. (2011) and Socher et al. (2012) brought up a unified deep structure for NLP tasks, much work using this combination has", "startOffset": 6, "endOffset": 55}, {"referenceID": 5, "context": "Collobert et al. (2011) showed great performance in tasks such as part-ofspeech-tagging, chunking, named entity recognition, and semantic role labeling with one", "startOffset": 0, "endOffset": 24}, {"referenceID": 13, "context": "Qi et al. (2014) and Zheng et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 13, "context": "Qi et al. (2014) and Zheng et al. (2013) adopted this structure for Chinese NLP tasks, and beat the state-of-the-art performance in word segmen-", "startOffset": 0, "endOffset": 41}, {"referenceID": 7, "context": "In addition, Li et al. (2015) applied word embedding on two large corpus, one is a set of news articles and the other is their corresponding summaries.", "startOffset": 13, "endOffset": 30}, {"referenceID": 5, "context": "We followed the work of (Collobert et al., 2011) and designed a similar DNN architecture, as shown in Figure 1.", "startOffset": 24, "endOffset": 48}, {"referenceID": 2, "context": "The DNN tool we use is python based software Theano (Bastien et al., 2012).", "startOffset": 52, "endOffset": 74}], "year": 2016, "abstractText": "A lot of prior work on event extraction has exploited a variety of features to represent events. Such methods have several drawbacks: 1) the features are often specific for a particular domain and do not generalize well; 2) the features are derived from various linguistic analyses and are error-prone; and 3) some features may be expensive and require domain expert. In this paper, we develop a Chinese event extraction system that uses word embedding vectors to represent language, and deep neural networks to learn the abstract feature representation in order to greatly reduce the effort of feature engineering. In addition, in this framework, we leverage large amount of unlabeled data, which can address the problem of limited labeled corpus for this task. Our experiments show that our proposed method performs better compared to the system using rich language features, and using unlabeled data benefits the word embeddings. This study suggests the potential of DNN and word embedding for the event extraction task.", "creator": "LaTeX with hyperref package"}}}