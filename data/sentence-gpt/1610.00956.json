{"id": "1610.00956", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Oct-2016", "title": "Embracing data abundance: BookTest Dataset for Reading Comprehension", "abstract": "There is a practically unlimited amount of natural language data available. Still, recent work in text comprehension has focused on datasets which are small relative to current computing possibilities. This article is making a case for the community to move to larger data and as a step in that direction it is proposing the BookTest, a new dataset similar to the popular Children's Book Test (CBT), however more than 60 times larger than the earlier CBT, which has been widely used for some time.\n\n\n\nSome major data streams and services exist, which is a great success in developing and distributing children's children's books, and they are in development in an ever-increasing number of countries and countries. However there is no easy way to support this large data pool of data on children's books and use them to guide them on how to use them. There are only several methods to support these efforts; there are only one, but you can download the full paper in PDF format. However there is no comprehensive solution, however, that would make for a more stable and user-friendly use of these services.\nWhile this is not a solution for children's books, there are some practical improvements which can be used in both the production of children's books and as a way to enhance the child's learning and learning. For example, a child can easily read only the first three paragraphs of each chapter of the book.\nThe child's book library can be downloaded from the child's library. This is a very helpful feature in writing a child's books. For example, the child can easily read only two paragraphs of each chapter of the book and can easily read only two paragraphs of each chapter of the book. The child can easily read only two paragraphs of each chapter of the book. The child can easily read only three pages of each chapter of the book and can easily read only two paragraphs of each chapter of the book and can easily read only two paragraphs of each chapter of the book and can easily read only two paragraphs of each chapter of the book and can easily read only two paragraphs of each chapter of the book and can easily read only two paragraphs of each chapter of the book and can easily read only two paragraphs of each chapter of the book and can easily read only two paragraphs of each chapter of the book and can easily read only two paragraphs of each chapter of the book and can easily read only two paragraphs of each chapter of the book and can easily read only two paragraphs of each chapter of the book and can easily read only two paragraphs of each chapter of the", "histories": [["v1", "Tue, 4 Oct 2016 12:48:51 GMT  (238kb,D)", "http://arxiv.org/abs/1610.00956v1", "The first two authors contributed equally to this work. Submitted to EACL 2017. Code and dataset are publicly available"]], "COMMENTS": "The first two authors contributed equally to this work. Submitted to EACL 2017. Code and dataset are publicly available", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["ondrej bajgar", "rudolf kadlec", "jan kleindienst"], "accepted": false, "id": "1610.00956"}, "pdf": {"name": "1610.00956.pdf", "metadata": {"source": "CRF", "title": "Embracing data abundance: BookTest Dataset for Reading Comprehension", "authors": ["Ondrej Bajgar", "Rudolf Kadlec", "Jan Kleindienst"], "emails": ["kadlec@cz.ibm.com", "jankle@cz.ibm.com"], "sections": [{"heading": null, "text": "We show that training on the new data improves the accuracy of our Attention-Sum Reader model on the original CBT test data by a much larger margin than many recent attempts to improve the model architecture. On one version of the dataset our ensemble even exceeds the human baseline provided by Facebook. We then show in our own human study that there is still space for further improvement."}, {"heading": "1 Introduction", "text": "Since humans amass more and more generally available data in the form of unstructured text it would be very useful to teach machines to read and comprehend such data and then use this understanding to answer our questions. A significant amount of research has recently focused on answering one particular kind of questions the answer to which depends on understanding a context document. These are cloze-style questions (Taylor, 1953) which require the reader to fill in a missing word in a sentence. An important advantage of such questions is that they can be generated automatically from a suitable text corpus which allows us to produce a\n\u2217These authors contributed equally to this work.\npractically unlimited amount of them. That opens the task to notoriously data-hungry deep-learning techniques which now seem to outperform all alternative approaches.\nTwo such large-scale datasets have recently been proposed by researchers from Google DeepMind and Facebook AI: the CNN/Daily Mail dataset (Hermann et al., 2015) and the Children\u2019s Book Test (CBT) (Hill et al., 2015) respectively. These have attracted a lot of attention from the research community (Kobayashi et al., 2016; Kadlec et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a; Li et al., 2016) with a new state-of-the-art model coming out every few weeks.\nHowever if our goal is a production-level system actually capable of helping humans, we want the model to use all available resources as efficiently as possible. Given that\n1. there is an almost unlimited amount of data that can be used for generating cloze-style question-answering datasets;\n2. it is widely accepted that more data can significantly improve performance of most models (Banko and Brill, 2001; Halevy et al., 2009);\n3. training a model takes only about two hours on the CBT or about two days on the Daily Mail dataset, the largest of the three sets, therefore these datasets are not allowing us to fully use current computing potential;\nwe believe that if the community is striving to bring the performance as far as possible, it should move its work to larger data.\nThis thinking goes in line with recent developments in the area of language modelling. For a long\nar X\niv :1\n61 0.\n00 95\n6v 1\n[ cs\n.C L\n] 4\nO ct\n2 01\n6\ntime models were being compared on several \u201dstandard\u201d datasets with publications often presenting minuscule improvements in performance. Then the large-scale One Billion Word corpus dataset appeared (Chelba et al., 2014) and it allowed Jozefowicz et al. to train much larger LSTM models (Jozefowicz et al., 2016) that almost halved the state-of-the-art perplexity on this dataset.\nWe think it is time to make a similar step in the area of text comprehension. Hence we are introducing the BookTest, a new dataset very similar to the Children\u2019s Book test but more than 60 times larger1 to enable training larger models even in the domain of text comprehension. Furthermore the methodology used to create our data can later be used to create even larger datasets when the need arises thanks to further technological progress.\nWe show that if we evaluate a model trained on the new dataset on the now standard Children\u2019s Book Test dataset, we see an improvement in accuracy much larger than other research groups achieved by enhancing the model architecture itself (while still using the original CBT training data). By training on the new dataset, we reduce the prediction error by almost one third. On the named-entity version of CBT this brings the ensemble of our models to the level of human baseline as reported by Facebook (Hill et al., 2015). However in the final section we show in our own human study that there is still room for improvement on the CBT beyond the performance of our model."}, {"heading": "2 Task Description", "text": "A natural way of testing a reader\u2019s comprehension of a text is to ask her a question the answer to which can be deduced from the text. Hence the task we are trying to solve consists of answering a clozestyle question, the answer to which depends on the understanding of a context document provided with the question. The model is also provided with a set of possible answers from which the correct one is to be selected2. This can be formalized as follows:\nThe training data consist of tuples (q,d, a, A), where q is a question, d is a document that contains the answer to question q, A is a set of possible answers and a \u2208 A is the ground-truth answer. Both q and d are sequences of words from vocabulary\n1Therefore more than 16 times larger than the Daily Mail dataset.\n2To get open-domain question answering we can provide the whole vocabulary as a candidate list.\nW 3. We also assume that all possible answers are words from the vocabulary, that is A \u2286W . In the CBT and CNN/Daily Mail datasets it is also true that the ground-truth answer a appears in the document. This is exploited by many machine learning models (Hill et al., 2015; Kadlec et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Cui et al., 2016b; Cui et al., 2016a; Li et al., 2016), however some do not explicitly depend on this property (Hermann et al., 2015; Kobayashi et al., 2016; Chen et al., 2016; Weissenborn, 2016)"}, {"heading": "3 Current Landscape", "text": "We will now briefly review what datasets for text comprehension have been published up to date and look at models which have been recently applied to solving the task we have just described."}, {"heading": "3.1 Datasets", "text": "A crucial condition for applying deep-learning techniques is to have a huge amount of data available for training. For question answering this specifically means having a large number of documentquestion-answer triples available. While there is an unlimited amount of text available, coming up with relevant questions and the corresponding answers can be extremely labour-intensive if done by human annotators. There were efforts to provide such human-generated datasets, e.g. Microsoft\u2019s MCTest (Richardson et al., 2013), however their scale is not suitable for deep learning without pretraining on other data (Trischler et al., 2016a) (such as using pre-trained word embedding vectors).\nGoogle DeepMind managed to avoid this scale issue with their way of generating documentquestion-answer triples automatically, closely followed by Facebook with a similar method. Let us now briefly introduce the two resulting datasets whose properties are summarized in Table 1."}, {"heading": "3.1.1 CNN & Daily Mail datasets", "text": "These two datasets (Hermann et al., 2015) exploit a useful feature of online news articles \u2013 many articles include a short summarizing sentence near the top of the page. Since all information in the summary sentence is also presented in the article body, we get a nice cloze-style question about the\n3Note that this vocabulary can be distinct from the model\u2019s own dictionary. For instance the model may replace certain rare words by generic unknown-word tags hence reducing its own dictionary size.\narticle contents by removing a word from the short summary.\nThe dataset\u2019s authors also replaced all named entities in the dataset by anonymous tokens which are further shuffled for each new batch. This forces the model to rely solely on information from the context document, not being able to transfer any meaning of the named entities between documents.\nThis restricts the task to one specific aspect of context-dependent question answering which may be useful however it moves the task further from the real application scenario, where we would like the model to use all information available to answer questions. Furthermore Chen et al. (Chen et al., 2016) have suggested that this can make about 17% of the questions unanswerable even by humans. They also claim that more than a half of the question sentences are mere paraphrases or exact matches of a single sentence from the context document. This raises a question to what extent the dataset can test deeper understanding of the articles."}, {"heading": "3.1.2 Children\u2019s Book Test", "text": "The Children\u2019s Book Test (Hill et al., 2015) uses a different source - books freely available thanks to Project Gutenberg4. Since no summary is available, each example consists of a context document formed from 20 consecutive sentences from the story together with a question formed from the subsequent sentence.\nThe dataset comes in four flavours depending on what type of word is omitted from the question sentence. Based on human evaluation done in (Hill et al., 2015) it seems that named entities (NEs) and common nouns (CNs) are more context dependent than the other two types \u2013 prepositions and verbs. Therefore we (and all of the recent publications)\n4https://www.gutenberg.org/\nfocus only on these two word types."}, {"heading": "3.1.3 Recent additions", "text": "Several new datasets related to the (now almost standard) ones above emerged recently. We will now briefly present them and explain how the dataset we are introducing in this article differs from them.\nThe LAMBADA dataset (Paperno et al., 2016) is designed to measure progress in understanding common-sense questions about short stories that can be easily answered by humans but cannot be answered by current standard machine-learning models (e.g. plain LSTM language models). This dataset is useful for measuring the gap between humans and machine learning algorithms. However, by contrast to our BookTest dataset, it will not allow us to track progress towards the performance of the baseline systems or on examples where machine learning may show super-human performance. Also LAMBADA is just a diagnostic dataset and does not provide ready-to-use questionanswering training data, just a plain-text corpus which may moreover include copyrighted books making its use potentially problematic for some purposes. We are providing ready training data consisting of copyright-free books only.\nThe SQuAD dataset (Rajpurkar et al., 2016) based on Wikipedia and the Who-did-What dataset (Onishi et al., 2016) based on Gigaword news articles are factoid question-answering datasets where a multi-word answer should be extracted from a context document. This is in contrast to the previous datasets, including CNN/DM, CBT, LAMBADA and our new dataset, which require only single-word answers. Both these datasets however provide less than 130,000 training questions, two orders of magnitude less than our dataset does.\nThe Story Cloze Test (Mostafazadeh et al., 2016)\nprovides a crowd-sourced corpus of 49,255 commonsense stories for training and 3,744 testing stories with right and wrong endings. Hence the dataset is again rather small. Similarly to LAMBADA, the Story Cloze Test was designed to be easily answerable by humans.\nIn the WikiReading (Hewlett et al., 2016) dataset the context document is formed from a Wikipedia article and the question-answer pair is taken from the corresponding WikiData page. For each entity (e.g. Hillary Clinton), WikiData contain a number of property-value pairs (e.g. place of birth: Chicago) which form the datasets\u2019s questionanswer pairs. The dataset is certainly relevant to the community, however the questions are of very limited variety with only 20 properties (and hence unique questions) covering 75% of the dataset. Furthermore many of the frequent properties are mentioned at a set spot within the article (e.g. the date of birth is almost always in brackets behind the name of a person) which may make the task easier for machines. We are trying to provide a more varied dataset.\nAlthough there are several datasets related to task we are aiming to solve, they differ sufficiently for our dataset to bring new value to the community. Its biggest advantage is its size which can furthermore be easily upscaled without expensive human annotation. Finally while we are emphasizing the differences, models could certainly benefit from as diverse a collection of datasets as possible."}, {"heading": "3.2 Machine Learning Models", "text": "A first major work applying deep-learning techniques to text comprehension was Hermann et al. (Hermann et al., 2015). This work was followed by the application of Memory Networks to the same task (Hill et al., 2015). Later three models emerged around the same time (Kobayashi et al., 2016; Kadlec et al., 2016; Chen et al., 2016) including our Attention Sum Reader (AS Reader) model (Kadlec et al., 2016). The AS Reader inspired several subsequent models that use it as a sub-component in a diverse ensemble (Trischler et al., 2016b); extend it with a hierarchical structure (Sordoni et al., 2016; Shen et al., 2016; Dhingra et al., 2016); compute attention over the context document for every word in the query (Cui et al., 2016b) or use two-way context-query attention mechanism for every word in the context and the query (Cui et al., 2016a) that is similar in its\nspirit to models recently proposed in different domains, e.g. (dos Santos et al., 2016) in information retrieval. Other neural approaches to text comprehension are explored in (Weissenborn, 2016; Li et al., 2016)."}, {"heading": "3.3 Possible Directions for Improvements", "text": "Accuracy in any machine learning tasks can be enhanced either by improving a machine learning model or by using more in-domain training data. Current state of the art models (Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Cui et al., 2016a) improve over AS Reader\u2019s accuracy on CBT NE and CN datasets by 1-2 percent absolute. This suggests that with current techniques there is only limited room for improvement on the algorithmic side.\nThe other possibility to improve performance is simply to use more training data. The importance of training data was highlighted by the frequently quoted Mercer\u2019s statement that \u201cThere is no data like more data.\u201d5 The observation that having more data is often more important than having better algorithms has been frequently stressed since then (Banko and Brill, 2001; Halevy et al., 2009).\nAs a step in the direction of exploiting the potential of more data in the domain of text comprehension, we created a new dataset called BookTest similar to, but much larger than the widely used CBT and CNN/DM datasets."}, {"heading": "4 BookTest", "text": "Similarly to the CBT, our BookTest dataset6 is derived from books available through project Gutenberg. We used 3555 copyright-free books to extract CN examples and 10507 books for NE examples, for comparison the CBT dataset was extracted from just 108 books.\nWhen creating our dataset we follow the same procedure as was used to create the CBT dataset (Hill et al., 2015). That is, we detect whether each sentence contains either a named entity or a common noun that already appeared in one of the preceding twenty sentences. This word is then replaced by a gap tag (XXXXX) in this sentence which is hence turned into a clozestyle question. The preceding 20 sentences are used as the context document. For common noun\n5Quote attributed to Robert Mercer by Fred Jelinek (Jelinek, 2004)\n6BookTest dataset can be downloaded from https:// ibm.biz/booktest-v1.\nand named entity detection we use the Stanford POS tagger (Toutanova et al., 2003) and Stanford NER (Finkel et al., 2005)7.\nThe training dataset consists of the original CBT NE and CN data extended with new NE and CN examples. The new BookTest dataset hence contains 14, 140, 825 training examples and 7, 917, 523, 807 tokens.\nThe validation dataset consists of 10, 000 NE and 10, 000 CN questions. We have one test set for NEs and one for CNs, each containing 10, 000 examples. The training, validation and test sets were generated from non-overlapping sets of books.\nWhen generating the dataset we removed all editions of books used to create CBT validation and test sets from our training dataset. Therefore the models trained on the BookTest corpus can be evaluated on the original CBT data and they can be compared with recent text-comprehension models utilizing this dataset (Hill et al., 2015; Kadlec et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a)."}, {"heading": "5 Baselines", "text": "We will now use our AS Reader model to evaluate the performance gain from increasing the dataset size."}, {"heading": "5.1 AS Reader", "text": "In (Kadlec et al., 2016) we introduced the AS Reader8, which at the time of publication significantly outperformed all other architectures on the CNN, DM and CBT datasets. This model is built to leverage the fact that the answer is a single word from the context document. Similarly to many other models it uses attention over the document \u2013 intuitively a measure of how relevant each word is to answering the question. However while most previous models used this attention as weights to calculate a blended representation of the answer word, we simply sum the attention across all occurrences of each unique words and then simply select the word with the highest sum as the final answer. While simple, this trick seems both to improve accuracy and to speed-up training. It was adopted by\n7We use version 3.6.0 of both NER and POS taggers. For POS tagging we use the model stanfordpostagger-full-2015-12-09/wsj-0-18-bidirectionaldistsim.tagger and for NER we use stanford-ner-201512-09/english.all.3class.distsim.crf.ser.gz.\n8AS Reader is available at https://github.com/ rkadlec/asreader\nmany subsequent models (Trischler et al., 2016b; Sordoni et al., 2016; Dhingra et al., 2016; Cui et al., 2016b; Cui et al., 2016a; Shen et al., 2016).\nLet us now describe the model in more detail. Figure 1 may help you in understanding the following paragraphs."}, {"heading": "5.1.1 Basic structure", "text": "The words from the document and the question are first converted into vector embeddings using a look-up matrix V. The document is then read by a bidirectional GRU network (Cho et al., 2014). A concatenation of the hidden states of the forward and backward GRUs at each word is then used as a contextual embedding of this word, intuitively representing the context in which the word is appearing. We can also understand it as representing the set of questions to which this word may be an answer.\nSimilarly the question is read by a bidirectional GRU but in this case only the final hidden states are concatenated to form the question embedding.\nThe attention over each word in the context is then calculated as the dot product of its contextual embedding with the question embedding. This attention is then normalized by the softmax function and summed across all occurrences of each answer candidate. The candidate with most accumulated attention is selected as the final answer.\nFor a more detailed description of the model including equations check (Kadlec et al., 2016). More details about the training setup and model hyperparameters can be found in the Appendix."}, {"heading": "5.1.2 Out-of-vocabulary words", "text": "During our past experiments on the CNN, DM and CBT datasets (Kadlec et al., 2016) each unique word from the training, validation and test datasets had its row in the look-up matrix V. However as we radically increased the dataset size, this would result in an extremely large number of model parameters so we decided to limit the vocabulary size to 200, 000 most frequent words. For each example, each unique out-of-vocabulary word is now mapped on one of 1000 anonymous tokens which are randomly initialized and untrained. Fixing the embeddings of these anonymous tags proved to significantly improve the performance."}, {"heading": "5.1.3 Query-initiated encoder", "text": "While mostly using the original AS Reader model, we have also tried introducing a minor tweak in\nsome instances of the model. We tried initializing the context encoder GRU\u2019s hidden state by letting the encoder read the question first before proceeding to read the context document. Intuitively this allows the encoder to know in advance what to look for when reading over the context document.\nIncluding models of this kind in the ensemble helped to improve the performance."}, {"heading": "5.2 Results", "text": "Table 2 shows the accuracy of the AS Reader and other architectures on the CBT validation and test data. The last two rows show the performance of the AS Reader trained on the BookTest dataset; all the other models have been trained on the original CBT training data.\nIf we take the best AS Reader ensemble trained on CBT as a baseline, improving the model architecture as in (Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a), continuing to use the original CBT training data, lead to improvements of 1% and 2.1% absolute on named entities and common nouns respectively. By contrast, inflating the training dataset provided a boost of 7.4\u2212 14.8% while using the same model. The ensemble of our models even exceeded the human baseline provided by Facebook (Hill et al., 2015) on the Common Noun dataset.\nOur model takes approximately two weeks to converge when trained on the BookTest dataset on a single Nvidia Tesla K40 GPU."}, {"heading": "6 Discussion", "text": "Embracing the abundance of data may mean focusing on other aspects of system design than with smaller data. Here are some of the challenges that we need to face in this situation.\nFirstly, since the amount of data is practically unlimited \u2013 we could even generate them on the fly resulting in continuous learning similar to the NeverEnding Language Learning by Carnegie Mellon University (Mitchell et al., 2015) \u2013 it is now the speed of training that determines how much data the model is able to see. Since more training data significantly help the model performance, focusing on speeding up the algorithm may be more important than ever before. This may for instance influence the decision whether to use regularization such as dropout which does seem to somewhat improve the model performance, however usually at a cost of slowing down training.\nThanks to its simplicity, the AS Reader seems to be training fast - for example around seven times faster than the models proposed by Chen et al. (Chen et al., 2016). Hence the AS Reader may be particularly suitable for training on large datasets.\nThe second challenge is how to generalize the performance gains from large data to a specific target domain. While there are huge amounts of natural language data in general, it may not be the case in the domain where we may want to ultimately apply our model.\nHence we are usually not facing a scenario of simply using a larger amount of the same training data, but rather extending training to a related domain of data, hoping that some of what the model\nlearns on the new data will still help it on the original task.\nThis is highlighted by our observations from applying a model trained on the BookTest to Children\u2019s Book Test test data. If we move model training from joint CBT NE+CN training data9 to a subset of the BookTest of the same size (230k examples), we see a drop in accuracy of around 10% on the CBT test datasets.\nHence even though the Children\u2019s Book Test and BookTest datasets are almost as close as two disjoint datasets can get, the transfer is still very imperfect 10. Rightly choosing data to augment the in-domain training data is certainly a problem worth exploring in future work.\nOur results show that given enough data the AS Reader was able to exceed the human performance on CBT CN reported by Facebook. However we hypothesized that the system is still not achieving its full potential so we decided to examine the room for improvement in our own small human study.\n9Note that while here we are using joint CBT NE+CN data to create an equivalent of a 230k subset of our BookTest, for most experiments on the CBT we and some other teams have used NE and CN as two separate training datasests.\n10This also suggests that the increase in accuracy when using more data that are strictly in the same domain as the original training data results in a performance increase even larger that the one we are reporting on CBT. However the scenario of having to look for additional data elsewhere is more realistic, so we are focusing this article in that direction."}, {"heading": "7 Human Study", "text": "After adding more data we have the performance on the CBT validation and test datasets soaring. However is there still potential for much further growth beyond the results we have observed?\nWe decided to explore the remaining space for improvement on the CBT by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the AS Reader ensemble could not answer correctly. These questions were answered by 10 non-native English speakers from our research laboratory, each on a disjoint subset of questions.. Participants had unlimited time to answer the questions and were told that these questions were not correctly answered by a machine, providing additional motivation to prove they are better than computers. The results of the human study are summarized in Table 3. They show that a majority of questions that our system could not answer so far are in fact answerable. This suggests that 1) the original human baselines might have been underestimated, however, it might also be the case that there are some examples that can be answered by machines and not by humans; 2) there is still space for improvement.\nA system that would answer correctly every time when either our ensemble or human answered correctly would achieve accuracy over 92% percent on both validation and test NE datasets and over 96% on both CN datasets. Hence it still makes sense to\nuse CBT dataset to study further improvements of text-comprehension systems."}, {"heading": "8 Conclusion", "text": "Few ways of improving model performance are as solidly established as using more training data. Yet we believe this principle has been somewhat neglected by recent research in text comprehension. While there is a practically unlimited amount of data available in this field, most research was performed on unnecessarily small datasets.\nAs a gentle reminder to the community we have shown that simply infusing a model with more data can yield performance improvements of up to 14.8% where several attempts to improve the model architecture on the same training data have given gains of at most 2.1% compared to our best ensemble result. Yes, experiments on small datasets certainly can bring useful insights. However we believe that the community should also embrace the real-world scenario of data abundance.\nThe BookTest dataset we are proposing gives the reading-comprehension community an opportunity to make a step in that direction."}, {"heading": "Appendix A Training Details", "text": "The training details are similar to those in (Kadlec et al., 2016) however we are including them here for completeness.\nTo train the model we used stochastic gradient descent with the ADAM update rule (Kingma and Ba, 2015) and learning rates of 0.0005, 0.0002 and 0.0001. The best learning rate in our experiments was 0.0005. We minimized negative log-likelihood as the training objective.\nThe initial weights in the word-embedding matrix were drawn randomly uniformly from the interval [\u22120.1, 0.1]. Weights in the GRU networks were initialized by random orthogonal matrices (Saxe et al., 2014) and biases were initialized to zero. We also used a gradient clipping (Pascanu et al., 2012) threshold of 10 and batches of sizes between 32 or 256. Increasing the batch from 32 to 128 seems to significantly improve performance on the large dataset - something we did not observe on the original CBT data. Increasing the batch size much above 128 is currently difficult due to memory constraints of the GPU.\nDuring training we randomly shuffled all examples at the beginning of each epoch. To speed up training, we always pre-fetched 10 batches worth of examples and sorted them according to document length. Hence each batch contained documents of roughly the same length.\nWe also did not use pre-trained word embeddings.\nWe did not perform any text pre-processing since the datasets were already tokenized.\nDuring training we evaluated the model performance every 12 hours and at the end of each epoch and stopped training when the error on the 20k BookTest validation set started increasing.\nWe explored the hyperparameter space by training 67 different models11 The region of the parameter space that we explored together with the param-\n11Some of these models were trained on data other than the BookTest which were however very similar.\neters of the model with best validation accuracy are summarized in Table 4.\nOur model was implemented using Theano (Bastien et al., 2012) and Blocks (van Merrienboer et al., 2015).\nThe ensembles were formed by simply averaging the predictions from the constituent single models. These single models were selected using the following algorithm.\nWe started with the best performing model according to validation performance. Then in each step we tried adding the best performing model that had not been previously tried. We kept it in the ensemble if it did improve its validation performance and discarded it otherwise. This way we gradually tried each model once. We call the resulting model a greedy ensemble. We used the 20, 000 BookTest validation dataset for this procedure.\nThe algorithm was offered 10 models and selected 5 of them for the final ensemble."}], "references": [{"title": "Scaling to Very Very Large Corpora for Natural Language Disambiguation", "author": ["Banko", "Brill2001] Michele Banko", "Eric Brill"], "venue": "ACL \u201901 Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Banko et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2001}, {"title": "Theano: new features and speed improvements", "author": ["Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": "Proceedings of the Annual Con-", "citeRegEx": "Chelba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2014}, {"title": "A Thorough Examination of the CNN / Daily Mail Reading Comprehension Task. In Association for Computational Linguistics (ACL)", "author": ["Chen et al.2016] Danqi Chen", "Jason Bolton", "Christopher D. Manning"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Transla", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "2016a. Attention-over-Attention Neural Networks for Reading Comprehension", "author": ["Cui et al.2016a] Yiming Cui", "Zhipeng Chen", "Si Wei", "Shijin Wang", "Ting Liu", "Guoping Hu"], "venue": null, "citeRegEx": "Cui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Consensus Attention-based Neural Networks for Chinese Reading Comprehension", "author": ["Cui et al.2016b] Yiming Cui", "Ting Liu", "Zhipeng Chen", "Shijin Wang", "Guoping Hu"], "venue": null, "citeRegEx": "Cui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Gated-Attention Readers for Text Comprehension", "author": ["Hanxiao Liu", "William W. Cohen", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Dhingra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Incorporating nonlocal information into information extraction systems by Gibbs sampling", "author": ["Trond Grenager", "Christopher Manning"], "venue": "Proceedings of the 43rd Annual Meeting on Association", "citeRegEx": "Finkel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "The unreasonable effectiveness of data", "author": ["Halevy et al.2009] Alon Halevy", "Peter Norvig", "Fernando Pereira"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "Halevy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Halevy et al\\.", "year": 2009}, {"title": "Teaching machines to read and comprehend", "author": ["Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "WIKI READING : A Novel Large-scale Language Understanding Task over Wikipedia", "author": ["Alexandre Lacoste", "Llion Jones", "Illia Polosukhin", "Andrew Fandrianto", "Jay Han", "Matthew Kelcey", "David Berthelot"], "venue": "Acl", "citeRegEx": "Hewlett et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hewlett et al\\.", "year": 2016}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations. arXiv preprint arXiv:1511.02301", "author": ["Hill et al.2015] Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Some of my Best Friends are Linguists Applying Information Theoretic Methods : Evaluation of Grammar Quality", "author": ["Frederick Jelinek"], "venue": null, "citeRegEx": "Jelinek.,? \\Q2004\\E", "shortCiteRegEx": "Jelinek.", "year": 2004}, {"title": "Exploring the Limits of Language Modeling", "author": ["Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "International Conference on Machine Learning (ICML)", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Neural Text Understanding with Attention Sum Reader", "author": ["Kadlec et al.2016] Rudolf Kadlec", "Martin Schmid", "Ondej Bajgar", "Jan Kleindienst"], "venue": "Proceedings of ACL", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: a Method for Stochastic Optimization", "author": ["Kingma", "Ba2015] Diederik P. Kingma", "Jimmy Lei Ba"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Dynamic Entity Representation with Max-pooling Improves Machine Reading", "author": ["Ran Tian", "Naoaki Okazaki", "Kentaro Inui"], "venue": "Proceedings of the North American Chapter of the Association", "citeRegEx": "Kobayashi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kobayashi et al\\.", "year": 2016}, {"title": "Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering", "author": ["Li et al.2016] Peng Li", "Wei Li", "Zhengyan He", "Xuguang Wang", "Ying Cao", "Jie Zhou", "Wei Xu"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Never-ending learning", "author": ["R. Wang", "D. Wijaya", "A. Gupta", "X. Chen", "A. Saparov", "M. Greaves", "J. Welling."], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15).", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "A Corpus and Evaluation Framework for Deeper Understanding", "author": ["Nathanael Chambers", "Xiaodong He", "Devi Parikh", "Dhruv Batra", "Lucy Vanderwende", "Pushmeet Kohli", "James Allen"], "venue": null, "citeRegEx": "Mostafazadeh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mostafazadeh et al\\.", "year": 2016}, {"title": "Who did What: A Large-Scale Person-Centered Cloze Dataset", "author": ["Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David McAllester"], "venue": null, "citeRegEx": "Onishi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Onishi et al\\.", "year": 2016}, {"title": "The LAMBADA dataset : Word prediction requiring a broad discourse", "author": ["Angeliki Lazaridou", "Quan Ngoc Pham", "Raffaella Bernardi", "Sandro Pezzelle", "Marco Baroni", "Gemma Boleda", "Raquel Fern"], "venue": null, "citeRegEx": "Paperno et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Paperno et al\\.", "year": 2016}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": "Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "author": ["Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "venue": null, "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text", "author": ["Christopher J C Burges", "Erin Renshaw"], "venue": "Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe et al.2014] Andrew M Saxe", "James L Mcclelland", "Surya Ganguli"], "venue": "International Conference on Learning Representations", "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "ReasoNet: Learning to Stop Reading in Machine Comprehension", "author": ["Shen et al.2016] Yelong Shen", "Po-Sen Huang", "Jianfeng Gao", "Weizhu Chen"], "venue": null, "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Iterative Alternating Neural Attention for Machine Reading", "author": ["Phillip Bachman", "Yoshua Bengio"], "venue": null, "citeRegEx": "Sordoni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2016}, {"title": "Cloze procedure: a new tool for measuring readability", "author": ["Wilson L Taylor"], "venue": "Journalism and Mass Communication", "citeRegEx": "Taylor.,? \\Q1953\\E", "shortCiteRegEx": "Taylor.", "year": 1953}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Dan Klein", "Christopher D Manning"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "2016a. A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data", "author": ["Zheng Ye", "Xingdi Yuan", "Jing He", "Phillip Bachman", "Kaheer Suleman"], "venue": "Proceedings of ACL", "citeRegEx": "Trischler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Natural Language Comprehension with the EpiReader", "author": ["Zheng Ye", "Xingdi Yuan", "Kaheer Suleman"], "venue": null, "citeRegEx": "Trischler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Separating Answers from Queries for Neural Reading Comprehension", "author": ["Dirk Weissenborn"], "venue": null, "citeRegEx": "Weissenborn.,? \\Q2016\\E", "shortCiteRegEx": "Weissenborn.", "year": 2016}], "referenceMentions": [{"referenceID": 29, "context": "These are cloze-style questions (Taylor, 1953) which require the reader to fill in a missing word in a sentence.", "startOffset": 32, "endOffset": 46}, {"referenceID": 10, "context": "Two such large-scale datasets have recently been proposed by researchers from Google DeepMind and Facebook AI: the CNN/Daily Mail dataset (Hermann et al., 2015) and the Children\u2019s Book Test (CBT) (Hill et al.", "startOffset": 138, "endOffset": 160}, {"referenceID": 12, "context": ", 2015) and the Children\u2019s Book Test (CBT) (Hill et al., 2015) respectively.", "startOffset": 43, "endOffset": 62}, {"referenceID": 9, "context": "it is widely accepted that more data can significantly improve performance of most models (Banko and Brill, 2001; Halevy et al., 2009);", "startOffset": 90, "endOffset": 134}, {"referenceID": 2, "context": "Then the large-scale One Billion Word corpus dataset appeared (Chelba et al., 2014) and it allowed Jozefowicz et al.", "startOffset": 62, "endOffset": 83}, {"referenceID": 14, "context": "to train much larger LSTM models (Jozefowicz et al., 2016) that almost halved the state-of-the-art perplexity on this dataset.", "startOffset": 33, "endOffset": 58}, {"referenceID": 12, "context": "On the named-entity version of CBT this brings the ensemble of our models to the level of human baseline as reported by Facebook (Hill et al., 2015).", "startOffset": 129, "endOffset": 148}, {"referenceID": 12, "context": "This is exploited by many machine learning models (Hill et al., 2015; Kadlec et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Cui et al., 2016b; Cui et al., 2016a; Li et al., 2016), however some do not explicitly depend on this property (Hermann et al.", "startOffset": 50, "endOffset": 214}, {"referenceID": 15, "context": "This is exploited by many machine learning models (Hill et al., 2015; Kadlec et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Cui et al., 2016b; Cui et al., 2016a; Li et al., 2016), however some do not explicitly depend on this property (Hermann et al.", "startOffset": 50, "endOffset": 214}, {"referenceID": 28, "context": "This is exploited by many machine learning models (Hill et al., 2015; Kadlec et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Cui et al., 2016b; Cui et al., 2016a; Li et al., 2016), however some do not explicitly depend on this property (Hermann et al.", "startOffset": 50, "endOffset": 214}, {"referenceID": 7, "context": "This is exploited by many machine learning models (Hill et al., 2015; Kadlec et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Cui et al., 2016b; Cui et al., 2016a; Li et al., 2016), however some do not explicitly depend on this property (Hermann et al.", "startOffset": 50, "endOffset": 214}, {"referenceID": 18, "context": "This is exploited by many machine learning models (Hill et al., 2015; Kadlec et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Cui et al., 2016b; Cui et al., 2016a; Li et al., 2016), however some do not explicitly depend on this property (Hermann et al.", "startOffset": 50, "endOffset": 214}, {"referenceID": 10, "context": ", 2016), however some do not explicitly depend on this property (Hermann et al., 2015; Kobayashi et al., 2016; Chen et al., 2016; Weissenborn, 2016)", "startOffset": 64, "endOffset": 148}, {"referenceID": 17, "context": ", 2016), however some do not explicitly depend on this property (Hermann et al., 2015; Kobayashi et al., 2016; Chen et al., 2016; Weissenborn, 2016)", "startOffset": 64, "endOffset": 148}, {"referenceID": 3, "context": ", 2016), however some do not explicitly depend on this property (Hermann et al., 2015; Kobayashi et al., 2016; Chen et al., 2016; Weissenborn, 2016)", "startOffset": 64, "endOffset": 148}, {"referenceID": 33, "context": ", 2016), however some do not explicitly depend on this property (Hermann et al., 2015; Kobayashi et al., 2016; Chen et al., 2016; Weissenborn, 2016)", "startOffset": 64, "endOffset": 148}, {"referenceID": 25, "context": "MCTest (Richardson et al., 2013), however their scale is not suitable for deep learning without pretraining on other data (Trischler et al.", "startOffset": 7, "endOffset": 32}, {"referenceID": 10, "context": "These two datasets (Hermann et al., 2015) exploit a useful feature of online news articles \u2013 many articles include a short summarizing sentence near the top of the page.", "startOffset": 19, "endOffset": 41}, {"referenceID": 10, "context": "Statistics were taken from (Hermann et al., 2015) and the statistics provided with the CBT data set.", "startOffset": 27, "endOffset": 49}, {"referenceID": 3, "context": "(Chen et al., 2016) have suggested that this can make about 17% of the questions unanswerable even by humans.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "The Children\u2019s Book Test (Hill et al., 2015) uses a different source - books freely available thanks to Project Gutenberg4.", "startOffset": 25, "endOffset": 44}, {"referenceID": 12, "context": "Based on human evaluation done in (Hill et al., 2015) it seems that named entities (NEs) and common nouns (CNs) are more context dependent than the other two types \u2013 prepositions and verbs.", "startOffset": 34, "endOffset": 53}, {"referenceID": 22, "context": "The LAMBADA dataset (Paperno et al., 2016) is designed to measure progress in understanding common-sense questions about short stories that can be easily answered by humans but cannot be answered by current standard machine-learning models (e.", "startOffset": 20, "endOffset": 42}, {"referenceID": 24, "context": "The SQuAD dataset (Rajpurkar et al., 2016) based on Wikipedia and the Who-did-What dataset (Onishi et al.", "startOffset": 18, "endOffset": 42}, {"referenceID": 21, "context": ", 2016) based on Wikipedia and the Who-did-What dataset (Onishi et al., 2016) based on Gigaword news articles are factoid question-answering datasets where a multi-word answer should be extracted from a context document.", "startOffset": 56, "endOffset": 77}, {"referenceID": 20, "context": "The Story Cloze Test (Mostafazadeh et al., 2016)", "startOffset": 21, "endOffset": 48}, {"referenceID": 11, "context": "In the WikiReading (Hewlett et al., 2016) dataset the context document is formed from a Wikipedia article and the question-answer pair is taken from the corresponding WikiData page.", "startOffset": 19, "endOffset": 41}, {"referenceID": 10, "context": "(Hermann et al., 2015).", "startOffset": 0, "endOffset": 22}, {"referenceID": 12, "context": "This work was followed by the application of Memory Networks to the same task (Hill et al., 2015).", "startOffset": 78, "endOffset": 97}, {"referenceID": 17, "context": "Later three models emerged around the same time (Kobayashi et al., 2016; Kadlec et al., 2016; Chen et al., 2016) including our Attention Sum Reader (AS Reader) model (Kadlec et al.", "startOffset": 48, "endOffset": 112}, {"referenceID": 15, "context": "Later three models emerged around the same time (Kobayashi et al., 2016; Kadlec et al., 2016; Chen et al., 2016) including our Attention Sum Reader (AS Reader) model (Kadlec et al.", "startOffset": 48, "endOffset": 112}, {"referenceID": 3, "context": "Later three models emerged around the same time (Kobayashi et al., 2016; Kadlec et al., 2016; Chen et al., 2016) including our Attention Sum Reader (AS Reader) model (Kadlec et al.", "startOffset": 48, "endOffset": 112}, {"referenceID": 15, "context": ", 2016) including our Attention Sum Reader (AS Reader) model (Kadlec et al., 2016).", "startOffset": 61, "endOffset": 82}, {"referenceID": 28, "context": ", 2016b); extend it with a hierarchical structure (Sordoni et al., 2016; Shen et al., 2016; Dhingra et al., 2016); compute attention over the context document for every word in the query (Cui et al.", "startOffset": 50, "endOffset": 113}, {"referenceID": 27, "context": ", 2016b); extend it with a hierarchical structure (Sordoni et al., 2016; Shen et al., 2016; Dhingra et al., 2016); compute attention over the context document for every word in the query (Cui et al.", "startOffset": 50, "endOffset": 113}, {"referenceID": 7, "context": ", 2016b); extend it with a hierarchical structure (Sordoni et al., 2016; Shen et al., 2016; Dhingra et al., 2016); compute attention over the context document for every word in the query (Cui et al.", "startOffset": 50, "endOffset": 113}, {"referenceID": 33, "context": "Other neural approaches to text comprehension are explored in (Weissenborn, 2016; Li et al., 2016).", "startOffset": 62, "endOffset": 98}, {"referenceID": 18, "context": "Other neural approaches to text comprehension are explored in (Weissenborn, 2016; Li et al., 2016).", "startOffset": 62, "endOffset": 98}, {"referenceID": 28, "context": "Current state of the art models (Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Cui et al., 2016a) improve over AS Reader\u2019s accuracy on CBT NE and CN datasets by 1-2 percent absolute.", "startOffset": 32, "endOffset": 120}, {"referenceID": 7, "context": "Current state of the art models (Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Cui et al., 2016a) improve over AS Reader\u2019s accuracy on CBT NE and CN datasets by 1-2 percent absolute.", "startOffset": 32, "endOffset": 120}, {"referenceID": 9, "context": "then (Banko and Brill, 2001; Halevy et al., 2009).", "startOffset": 5, "endOffset": 49}, {"referenceID": 12, "context": "When creating our dataset we follow the same procedure as was used to create the CBT dataset (Hill et al., 2015).", "startOffset": 93, "endOffset": 112}, {"referenceID": 13, "context": "Quote attributed to Robert Mercer by Fred Jelinek (Jelinek, 2004) BookTest dataset can be downloaded from https:// ibm.", "startOffset": 50, "endOffset": 65}, {"referenceID": 30, "context": "and named entity detection we use the Stanford POS tagger (Toutanova et al., 2003) and Stanford NER (Finkel et al.", "startOffset": 58, "endOffset": 82}, {"referenceID": 8, "context": ", 2003) and Stanford NER (Finkel et al., 2005)7.", "startOffset": 25, "endOffset": 46}, {"referenceID": 12, "context": "Therefore the models trained on the BookTest corpus can be evaluated on the original CBT data and they can be compared with recent text-comprehension models utilizing this dataset (Hill et al., 2015; Kadlec et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a).", "startOffset": 180, "endOffset": 365}, {"referenceID": 15, "context": "Therefore the models trained on the BookTest corpus can be evaluated on the original CBT data and they can be compared with recent text-comprehension models utilizing this dataset (Hill et al., 2015; Kadlec et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a).", "startOffset": 180, "endOffset": 365}, {"referenceID": 3, "context": "Therefore the models trained on the BookTest corpus can be evaluated on the original CBT data and they can be compared with recent text-comprehension models utilizing this dataset (Hill et al., 2015; Kadlec et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a).", "startOffset": 180, "endOffset": 365}, {"referenceID": 28, "context": "Therefore the models trained on the BookTest corpus can be evaluated on the original CBT data and they can be compared with recent text-comprehension models utilizing this dataset (Hill et al., 2015; Kadlec et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a).", "startOffset": 180, "endOffset": 365}, {"referenceID": 7, "context": "Therefore the models trained on the BookTest corpus can be evaluated on the original CBT data and they can be compared with recent text-comprehension models utilizing this dataset (Hill et al., 2015; Kadlec et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a).", "startOffset": 180, "endOffset": 365}, {"referenceID": 33, "context": "Therefore the models trained on the BookTest corpus can be evaluated on the original CBT data and they can be compared with recent text-comprehension models utilizing this dataset (Hill et al., 2015; Kadlec et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a).", "startOffset": 180, "endOffset": 365}, {"referenceID": 15, "context": "In (Kadlec et al., 2016) we introduced the AS Reader8, which at the time of publication significantly outperformed all other architectures on the CNN, DM and CBT datasets.", "startOffset": 3, "endOffset": 24}, {"referenceID": 28, "context": "rkadlec/asreader many subsequent models (Trischler et al., 2016b; Sordoni et al., 2016; Dhingra et al., 2016; Cui et al., 2016b; Cui et al., 2016a; Shen et al., 2016).", "startOffset": 40, "endOffset": 166}, {"referenceID": 7, "context": "rkadlec/asreader many subsequent models (Trischler et al., 2016b; Sordoni et al., 2016; Dhingra et al., 2016; Cui et al., 2016b; Cui et al., 2016a; Shen et al., 2016).", "startOffset": 40, "endOffset": 166}, {"referenceID": 27, "context": "rkadlec/asreader many subsequent models (Trischler et al., 2016b; Sordoni et al., 2016; Dhingra et al., 2016; Cui et al., 2016b; Cui et al., 2016a; Shen et al., 2016).", "startOffset": 40, "endOffset": 166}, {"referenceID": 4, "context": "The document is then read by a bidirectional GRU network (Cho et al., 2014).", "startOffset": 57, "endOffset": 75}, {"referenceID": 15, "context": "For a more detailed description of the model including equations check (Kadlec et al., 2016).", "startOffset": 71, "endOffset": 92}, {"referenceID": 15, "context": "During our past experiments on the CNN, DM and CBT datasets (Kadlec et al., 2016) each unique word from the training, validation and test datasets had its row in the look-up matrix V.", "startOffset": 60, "endOffset": 81}, {"referenceID": 28, "context": "If we take the best AS Reader ensemble trained on CBT as a baseline, improving the model architecture as in (Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a), continuing to use the original CBT training data, lead to improvements of 1% and 2.", "startOffset": 108, "endOffset": 234}, {"referenceID": 7, "context": "If we take the best AS Reader ensemble trained on CBT as a baseline, improving the model architecture as in (Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a), continuing to use the original CBT training data, lead to improvements of 1% and 2.", "startOffset": 108, "endOffset": 234}, {"referenceID": 33, "context": "If we take the best AS Reader ensemble trained on CBT as a baseline, improving the model architecture as in (Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a), continuing to use the original CBT training data, lead to improvements of 1% and 2.", "startOffset": 108, "endOffset": 234}, {"referenceID": 12, "context": "The ensemble of our models even exceeded the human baseline provided by Facebook (Hill et al., 2015) on the Common Noun dataset.", "startOffset": 81, "endOffset": 100}, {"referenceID": 3, "context": "(Chen et al., 2016).", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "Humans (query) (Hill et al., 2015) NA 52.", "startOffset": 15, "endOffset": 34}, {"referenceID": 12, "context": "4 Humans (context+query) (Hill et al., 2015) NA 81.", "startOffset": 25, "endOffset": 44}, {"referenceID": 12, "context": "LSTMs (context+query) (Hill et al., 2015) 51.", "startOffset": 22, "endOffset": 41}, {"referenceID": 12, "context": "0 \uf8fc\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8fe CBT training data Memory Networks (Hill et al., 2015) 70.", "startOffset": 61, "endOffset": 80}, {"referenceID": 7, "context": "GA Reader (ensemble) (Dhingra et al., 2016) 75.", "startOffset": 21, "endOffset": 43}, {"referenceID": 28, "context": "6 IA Reader (ensemble) (Sordoni et al., 2016) 76.", "startOffset": 23, "endOffset": 45}], "year": 2016, "abstractText": "There is a practically unlimited amount of natural language data available. Still, recent work in text comprehension has focused on datasets which are small relative to current computing possibilities. This article is making a case for the community to move to larger data and as a step in that direction it is proposing the BookTest, a new dataset similar to the popular Children\u2019s Book Test (CBT), however more than 60 times larger. We show that training on the new data improves the accuracy of our Attention-Sum Reader model on the original CBT test data by a much larger margin than many recent attempts to improve the model architecture. On one version of the dataset our ensemble even exceeds the human baseline provided by Facebook. We then show in our own human study that there is still space for further improvement.", "creator": "LaTeX with hyperref package"}}}