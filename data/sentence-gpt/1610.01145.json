{"id": "1610.01145", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2016", "title": "Error bounds for approximations with deep ReLU networks", "abstract": "We study how approximation errors of neural networks with ReLU activation functions depend on the depth of the network. We establish rigorous error bounds showing that deep ReLU networks are significantly more expressive than shallow ones as long as approximations of smooth functions are concerned. At the same time, we show that on a set of functions constrained only by their degree of smoothness, a ReLU network architecture cannot in general achieve approximation accuracy with better than a power law dependence on the network size, regardless of its depth. For example, the number of nodes in an extended network is 1,000 nodes (N, 2) or more. The ReLU network architecture can be tested with a set of ReLU networks with a total of 2,000 nodes for an extended network of at least 1,000 nodes (N, 2). We test the performance of algorithms for algorithms for the following subcommands:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 3 Oct 2016 23:08:22 GMT  (134kb,D)", "https://arxiv.org/abs/1610.01145v1", null], ["v2", "Mon, 7 Nov 2016 16:57:35 GMT  (162kb,D)", "http://arxiv.org/abs/1610.01145v2", "16 pages; Theorem 3 added in v2"], ["v3", "Mon, 1 May 2017 14:01:32 GMT  (314kb,D)", "http://arxiv.org/abs/1610.01145v3", "31 pages; major revision in v3; submitted to Neural Networks"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["dmitry yarotsky"], "accepted": false, "id": "1610.01145"}, "pdf": {"name": "1610.01145.pdf", "metadata": {"source": "CRF", "title": "Error bounds for approximations with deep ReLU networks", "authors": ["Dmitry Yarotsky"], "emails": ["d.yarotsky@skoltech.ru"], "sections": [{"heading": "1 Introduction", "text": "Recently, multiple successful applications of deep neural networks to pattern recognition problems (Schmidhuber [2015], LeCun et al. [2015]) have revived active interest in theoretical properties of such networks, in particular their expressive power. It has been argued that deep networks may be more expressive than shallow ones of comparable size (see, e.g., Delalleau and Bengio [2011], Raghu et al. [2016], Montufar et al. [2014], Bianchini and Scarselli [2014], Telgarsky [2015]). In contrast to a shallow network, a deep one can be viewed as a long sequence of non-commutative transformations, which is a natural setting for high expressiveness (cf. the well-known Solovay-Kitaev theorem on fast approximation of arbitrary quantum operations by sequences of non-commutative gates, see Kitaev et al. [2002], Dawson and Nielsen [2006]).\nThere are various ways to characterize expressive power of networks. Delalleau and Bengio 2011 consider sum-product networks and prove for certain classes of polynomials that they are much more easily represented by deep networks than by shallow networks. Montufar\n\u2217Skolkovo Institute of Science and Technology, Skolkovo Innovation Center, Building 3, Moscow 143026 Russia \u2020Institute for Information Transmission Problems, Bolshoy Karetny per. 19, build.1, Moscow 127051, Russia\nar X\niv :1\n61 0.\n01 14\n5v 3\n[ cs\n.L G\n] 1\nM ay\net al. 2014 estimate the number of linear regions in the network\u2019s landscape. Bianchini and Scarselli 2014 give bounds for Betti numbers characterizing topological properties of functions represented by networks. Telgarsky 2015, 2016 provides specific examples of classification problems where deep networks are provably more efficient than shallow ones.\nIn the context of classification problems, a general and standard approach to characterizing expressiveness is based on the notion of the Vapnik-Chervonenkis dimension (Vapnik and Chervonenkis [2015]). There exist several bounds for VC-dimension of deep networks with piece-wise polynomial activation functions that go back to geometric techniques of Goldberg and Jerrum 1995 and earlier results of Warren 1968; see Bartlett et al. [1998], Sakurai [1999] and the book Anthony and Bartlett [2009]. There is a related concept, the fat-shattering dimension, for real-valued approximation problems (Kearns and Schapire [1990], Anthony and Bartlett [2009]).\nA very general approach to expressiveness in the context of approximation is the method of nonlinear widths by DeVore et al. 1989 that concerns approximation of a family of functions under assumption of a continuous dependence of the model on the approximated function.\nIn this paper we examine the problem of shallow-vs-deep expressiveness from the perspective of approximation theory and general spaces of functions having derivatives up to certain order (Sobolev-type spaces). In this framework, the problem of expressiveness is very well studied in the case of shallow networks with a single hidden layer, where it is known, in particular, that to approximate a Cn-function on a d-dimensional set with infinitesimal error one needs a network of size about \u2212d/n, assuming a smooth activation function (see, e.g., Mhaskar [1996], Pinkus [1999] for a number of related rigorous upper and lower bounds and further qualifications of this result). Much less seems to be known about deep networks in this setting, though Mhaskar et al. 2016, 2016 have recently introduced functional spaces constructed using deep dependency graphs and obtained expressiveness bounds for related deep networks.\nWe will focus our attention on networks with the ReLU activation function \u03c3(x) = max(0, x), which, despite its utter simplicity, seems to be the most popular choice in practical applications LeCun et al. [2015]. We will consider L\u221e-error of approximation of functions belonging to the Sobolev spacesWn,\u221e([0, 1]d) (without any assumptions of hierarchical structure). We will often consider families of approximations, as the approximated function runs over the unit ball Fd,n in Wn,\u221e([0, 1]d). In such cases we will distinguish scenarios of fixed and adaptive network architectures. Our goal is to obtain lower and upper bounds on the expressiveness of deep and shallow networks in different scenarios. We measure complexity of networks in a conventional way, by counting the number of their weights and computation units (cf. Anthony and Bartlett [2009]).\nThe main body of the paper consists of Sections 2, 3 and 4. In Section 2 we describe our ReLU network model and show that the ReLU function is replaceable by any other continuous piece-wise linear activation function, up to constant factors in complexity asymptotics (Proposition 1).\nIn Section 3 we establish several upper bounds on the complexity of approximating by\nReLU networks, in particular showing that deep networks are quite efficient for approximating smooth functions. Specifically:\n\u2022 In Subsection 3.1 we show that the function f(x) = x2 can be -approximated by a network of depth and complexity O(ln(1/ )) (Proposition 2). This also leads to similar upper bounds on the depth and complexity that are sufficient to implement an approximate multiplication in a ReLU network (Proposition 3).\n\u2022 In Subsection 3.2 we describe a ReLU network architecture of depth O(ln(1/ )) and complexity O( \u2212d/n ln(1/ )) that is capable of approximating with error any function from Fd,n (Theorem 1).\n\u2022 In Subsection 3.3 we show that, even with fixed-depth networks, one can further decrease the approximation complexity if the network architecture is allowed to depend on the approximated function. Specifically, we prove that one can -approximate a given Lipschitz function on the segment [0, 1] by a depth-6 ReLU network with O( 1\nln(1/ ) )\nconnections and activation units (Theorem 2). This upper bound is of interest since it lies below the lower bound provided by the method of nonlinear widths under assumption of continuous model selection (see Subsection 4.1).\nIn Section 4 we obtain several lower bounds on the complexity of approximation by deep and shallow ReLU networks, using different approaches and assumptions.\n\u2022 In Subsection 4.1 we recall the general lower bound provided by the method of continuous nonlinear widths. This method assumes that parameters of the approximation continuously depend on the approximated function, but does not assume anything about how the approximation depends on its parameters. In this setup, at least \u223c \u2212d/n connections and weights are required for an -approximation on Fd,n (Theorem 3). As already mentioned, for d = n = 1 this lower bound is above the upper bound provided by Theorem 2.\n\u2022 In Subsection 4.2 we consider the setup where the same network architecture is used to approximate all functions in Fd,n, but the weights are not assumed to continuously depend on the function. In this case, application of existing results on VC-dimension of deep piece-wise polynomial networks yields a \u223c d/(2n) lower bound in general and a \u223c \u2212d/n ln\u22122p\u22121(1/ ) lower bound if the network depth grows as O(lnp(1/ )) (Theorem 4).\n\u2022 In Subsection 4.3 we consider an individual approximation, without any assumptions regarding it as an element of a family as in Subsections 4.1 and 4.2. We prove that for any d, n there exists a function inWn,\u221e([0, 1]d) such that its approximation complexity is not o( \u2212d/(9n)) as \u2192 0 (Theorem 5).\n\u2022 In Subsection 4.4 we prove that -approximation of any nonlinear C2-function by a network of fixed depth L requires at least \u223c \u22121/(2(L\u22122)) computation units (Theorem 6). By comparison with Theorem 1, this shows that for sufficiently smooth functions\napproximation by fixed-depth ReLU networks is less efficient than by unbounded-depth networks.\nIn Section 5 we discuss the obtained bounds and summarize their implications, in particular comparing deep vs. shallow networks and fixed vs. adaptive architectures.\nThe arXiv preprint of the first version of the present work appeared almost simultaneously with the work of Liang and Srikant Liang and Srikant [2016] containing results partly overlapping with our results in Subsections 3.1,3.2 and 4.4. Liang and Srikant consider networks equipped with both ReLU and threshold activation functions. They prove a logarithmic upper bound for the complexity of approximating the function f(x) = x2, which is analogous to our Proposition 2. Then, they extend this upper bound to polynomials and smooth functions. In contrast to our treatment of generic smooth functions based on standard Sobolev spaces, they impose more complex assumptions on the function (including, in particular, how many derivatives it has) that depend on the required approximation accuracy . As a consequence, they obtain strong O(lnc(1/ )) complexity bounds rather different from our bound in Theorem 1 (in fact, our lower bound proved in Theorem 5 rules out, in general, such strong upper bounds for functions having only finitely many derivatives). Also, Liang and Srikant prove a lower bound for the complexity of approximating convex functions by shallow networks. Our version of this result, given in Subsection 4.4, is different in that we assume smoothness and nonlinearity instead of global convexity."}, {"heading": "2 The ReLU network model", "text": "Throughout the paper, we consider feedforward neural networks with the ReLU (Rectified Linear Unit) activation function\n\u03c3(x) = max(0, x).\nThe network consists of several input units, one output unit, and a number of \u201chidden\u201d computation units. Each hidden unit performs an operation of the form\ny = \u03c3 ( N\u2211\nk=1\nwkxk + b )\n(1)\nwith some weights (adjustable parameters) (wk) N k=1 and b depending on the unit. The output unit is also a computation unit, but without the nonlinearity, i.e., it computes y = \u2211N k=1 wkxk + b. The units are grouped in layers, and the inputs (xk) N k=1 of a computation unit in a certain layer are outputs of some units belonging to any of the preceding layers (see Fig. 1). Note that we allow connections between units in non-neighboring layers. Occasionally, when this cannot cause confusion, we may denote the network and the function it implements by the same symbol.\nThe depth of the network, the number of units and the total number of weights are standard measures of network complexity (Anthony and Bartlett [2009]). We will use these measures throughout the paper. The number of weights is, clearly, the sum of the total number of connections and the number of computation units. We identify the depth with the number of layers (in particular, the most common type of neural networks \u2013 shallow networks having a single hidden layer \u2013 are depth-3 networks according to this convention).\nWe finish this subsection with a proposition showing that, given our complexity measures, using the ReLU activation function is not much different from using any other piece-wise linear activation function with finitely many breakpoints: one can replace one network by an equivalent one but having another activation function while only increasing the number of units and weights by constant factors. This justifies our restricted attention to the ReLU networks (which could otherwise have been perceived as an excessively particular example of networks).\nProposition 1. Let \u03c1 : R\u2192 R be any continuous piece-wise linear function with M breakpoints, where 1 \u2264M <\u221e.\na) Let \u03be be a network with the activation function \u03c1, having depth L, W weights and U computation units. Then there exists a ReLU network \u03b7 that has depth L, not more than (M + 1)2W weights and not more than (M + 1)U units, and that computes the same function as \u03be.\nb) Conversely, let \u03b7 be a ReLU network of depth L with W weights and U computation units. Let D be a bounded subset of Rn, where n is the input dimension of \u03b7. Then there exists a network with the activation function \u03c1 that has depth L, 4W weights and 2U units, and that computes the same function as \u03b7 on the set D.\nProof. a) Let a1 < . . . < aM be the breakpoints of \u03c1, i.e., the points where its derivative is discontinuous: \u03c1\u2032(ak+) 6= \u03c1\u2032(ak\u2212). We can then express \u03c1 via the ReLU function \u03c3, as a linear combination\n\u03c1(x) = c0\u03c3(a1 \u2212 x) + M\u2211\nm=1\ncm\u03c3(x\u2212 am) + h\nwith appropriately chosen coefficients (cm) M m=0 and h. It follows that computation performed by a single \u03c1-unit,\nx1, . . . , xN 7\u2192 \u03c1 ( N\u2211\nk=1\nwkxk + b ) ,\ncan be equivalently represented by a linear combination of a constant function and computations of M + 1 \u03c3-units,\nx1, . . . , xN 7\u2192    \u03c3 (\u2211N k=1 wkxk + b\u2212 am ) , m = 1, . . . ,M, \u03c3 ( a1 \u2212 b\u2212 \u2211N k=1wkxk), m = 0\n(here m is the index of a \u03c1-unit). We can then replace one-by-one all the \u03c1-units in the network \u03be by \u03c3-units, without changing the output of the network. Obviously, these replacements do not change the network depth. Since each hidden unit gets replaced by M + 1 new units, the number of units in the new network is not greater than M + 1 times their number in the original network. Note also that the number of connections in the network is multiplied, at most, by (M + 1)2. Indeed, each unit replacement entails replacing each of the incoming and outgoing connections of this unit by M + 1 new connections, and each connection is replaced twice: as an incoming and as an outgoing one. These considerations imply the claimed complexity bounds for the resulting \u03c3-network \u03b7.\nb) Let a be any breakpoint of \u03c1, so that \u03c1\u2032(a+) 6= \u03c1\u2032(a\u2212). Let r0 be the distance separating a from the nearest other breakpoint, so that \u03c1 is linear on [a, a + r0] and on [a \u2212 r0, a] (if \u03c1 has only one node, any r0 > 0 will do). Then, for any r > 0, we can express the ReLU function \u03c3 via \u03c1 in the r-neighborhood of 0:\n\u03c3(x) = \u03c1 ( a+ r0 2r x ) \u2212 \u03c1 ( a\u2212 r0 2 + r0 2r x ) \u2212 \u03c1(a) + \u03c1 ( a\u2212 r0 2 ) ( \u03c1\u2032(a+)\u2212 \u03c1\u2032(a\u2212) ) r0 2r , x \u2208 [\u2212r, r].\nIt follows that a computation performed by a single \u03c3-unit,\nx1, . . . , xN 7\u2192 \u03c3 ( N\u2211\nk=1\nwkxk + b ) ,\ncan be equivalently represented by a linear combination of a constant function and two \u03c1-units,\nx1, . . . , xN 7\u2192    \u03c1 ( a+ r0 2r b+ r0 2r \u2211N k=1wkxk ) , \u03c1 ( a\u2212 r0\n2 + r0 2r b+ r0 2r \u2211N k=1wkxk ) ,\nprovided the condition N\u2211\nk=1\nwkxk + b \u2208 [\u2212r, r] (2)\nholds. Since D is a bounded set, we can choose r at each unit of the initial network \u03b7 sufficiently large so as to satisfy condition (2) for all network inputs from D. Then, like in a), we replace each \u03c3-unit with two \u03c1-units, which produces the desired \u03c1-network."}, {"heading": "3 Upper bounds", "text": "Throughout the paper, we will be interested in approximating functions f : [0, 1]d \u2192 R by ReLU networks. Given a function f : [0, 1]d \u2192 R and its approximation f\u0303 , by the approximation error we will always mean the uniform maximum error\n\u2016f \u2212 f\u0303\u2016\u221e = max x\u2208[0,1]d |f(x)\u2212 f\u0303(x)|."}, {"heading": "3.1 Fast deep approximation of squaring and multiplication", "text": "Our first key result shows that ReLU networks with unconstrained depth can very efficiently approximate the function f(x) = x2 (more efficiently than any fixed-depth network, as we will see in Section 4.4). Our construction uses the \u201csawtooth\u201d function that has previously appeared in the paper Telgarsky [2015].\nProposition 2. The function f(x) = x2 on the segment [0, 1] can be approximated with any error > 0 by a ReLU network having the depth and the number of weights and computation units O(ln(1/ )).\nProof. Consider the \u201ctooth\u201d (or \u201cmirror\u201d) function g : [0, 1]\u2192 [0, 1],\ng(x) =\n{ 2x, x < 1\n2 ,\n2(1\u2212 x), x \u2265 1 2 ,\nand the iterated functions gs(x) = g \u25e6 g \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 g\ufe38 \ufe37\ufe37 \ufe38\ns\n(x).\nTelgarsky has shown (see Lemma 2.4 in Telgarsky [2015]) that gs is a \u201csawtooth\u201d function with 2s\u22121 uniformly distributed \u201cteeth\u201d (each application of g doubles the number of teeth):\ngs(x) =\n{ 2s ( x\u2212 2k\n2s\n) , x \u2208 [ 2k 2s , 2k+1 2s ], k = 0, 1, . . . , 2s\u22121 \u2212 1,\n2s ( 2k 2s \u2212 x ) , x \u2208 [ 2k\u22121 2s , 2k 2s ], k = 1, 2, . . . , 2s\u22121,\n(see Fig. 2a). Our key observation now is that the function f(x) = x2 can be approximated by linear combinations of the functions gs. Namely, let fm be the piece-wise linear interpolation of f with 2m + 1 uniformly distributed breakpoints k\n2m , k = 0, . . . , 2m:\nfm ( k 2m ) = ( k 2m )2 , k = 0, . . . , 2m\n(see Fig. 2b). The function fm approximates f with the error m = 2 \u22122m\u22122. Now note that refining the interpolation from fm\u22121 to fm amounts to adjusting it by a function proportional to a sawtooth function:\nfm\u22121(x)\u2212 fm(x) = gm(x)\n22m .\nHence\nfm(x) = x\u2212 m\u2211\ns=1\ngs(x)\n22s .\nSince g can be implemented by a finite ReLU network (as g(x) = 2\u03c3(x)\u22124\u03c3 ( x\u2212 1\n2\n) +2\u03c3(x\u22121))\nand since construction of fm only involves O(m) linear operations and compositions of g, we can implement fm by a ReLU network having depth and the number of weights and computation units all being O(m) (see Fig. 2c). This implies the claim of the proposition.\nSince\nxy = 1\n2 ((x+ y)2 \u2212 x2 \u2212 y2), (3)\nwe can use Proposition 2 to efficiently implement accurate multiplication in a ReLU network. The implementation will depend on the required accuracy and the magnitude of the multiplied quantities.\nProposition 3. Given M > 0 and \u2208 (0, 1), there is a ReLU network \u03b7 with two input units that implements a function \u00d7\u0303 : R2 \u2192 R so that\na) for any inputs x, y, if |x| \u2264M and |y| \u2264M, then |\u00d7\u0303(x, y)\u2212 xy| \u2264 ;\nb) if x = 0 or y = 0, then \u00d7\u0303(x, y) = 0;\nc) the depth and the number of weights and computation units in \u03b7 is not greater than c1 ln(1/ ) + c2 with an absolute constant c1 and a constant c2 = c2(M).\nProof. Let f\u0303sq,\u03b4 be the approximate squaring function from Proposition 2 such that f\u0303sq,\u03b4(0) = 0 and |f\u0303sq,\u03b4(x) \u2212 x2| < \u03b4 for x \u2208 [0, 1]. Assume without loss of generality that M \u2265 1 and set\n\u00d7\u0303(x, y) = M 2\n8\n( f\u0303sq,\u03b4 ( |x+ y| 2M ) \u2212 f\u0303sq,\u03b4 ( |x| 2M ) \u2212 f\u0303sq,\u03b4 ( |y| 2M )) , (4)\nwhere \u03b4 = 8 3M2 . Then property b) is immediate and a) follows easily using expansion (3). To conclude c), observe that computation (4) consists of three instances of f\u0303sq,\u03b4 and finitely many linear and ReLU operations, so, using Proposition 2, we can implement \u00d7\u0303 by a ReLU network such that its depth and the number of computation units and weights are O(ln(1/\u03b4)), i.e. are O(ln(1/ ) + lnM)."}, {"heading": "3.2 Fast deep approximation of general smooth functions", "text": "In order to formulate our general result, Theorem 1, we consider the Sobolev spaces Wn,\u221e([0, 1]d) with n = 1, 2, . . . Recall that Wn,\u221e([0, 1]d) is defined as the space of functions on [0, 1]d lying in L\u221e along with their weak derivatives up to order n. The norm in Wn,\u221e([0, 1]d) can be defined by\n\u2016f\u2016Wn,\u221e([0,1]d) = max n:|n|\u2264n ess sup x\u2208[0,1]d |Dnf(x)|,\nwhere n = (n1, . . . , nd) \u2208 {0, 1, . . .}d, |n| = n1 + . . . + nd, and Dnf is the respective weak derivative. Here and in the sequel we denote vectors by boldface characters. The space Wn,\u221e([0, 1]d) can be equivalently described as consisting of the functions from Cn\u22121([0, 1]d) such that all their derivatives of order n\u2212 1 are Lipschitz continuous.\nThroughout the paper, we denote by Fn,d the unit ball in Wn,\u221e([0, 1]d):\nFn,d = {f \u2208 Wn,\u221e([0, 1]d) : \u2016f\u2016Wn,\u221e([0,1]d) \u2264 1}.\nAlso, it will now be convenient to make a distinction between networks and network architectures : we define the latter as the former with unspecified weights. We say that a network architecture is capable of expressing any function from Fd,n with error meaning that this can be achieved by some weight assignment.\nTheorem 1. For any d, n and \u2208 (0, 1), there is a ReLU network architecture that\n1. is capable of expressing any function from Fd,n with error ;\n2. has the depth at most c(ln(1/ ) + 1) and at most c \u2212d/n(ln(1/ ) + 1) weights and computation units, with some constant c = c(d, n).\nProof. The proof will consist of two steps. We start with approximating f by a sum-product combination f1 of local Taylor polynomials and one-dimensional piecewise-linear functions. After that, we will use results of the previous section to approximate f1 by a neural network.\nLet N be a positive integer. Consider a partition of unity formed by a grid of (N + 1)d\nfunctions \u03c6m on the domain [0, 1] d: \u2211\nm\n\u03c6m(x) \u2261 1, x \u2208 [0, 1]d.\nHere m = (m1, . . . ,md) \u2208 {0, 1, . . . , N}d, and the function \u03c6m is defined as the product\n\u03c6m(x) = d\u220f\nk=1\n\u03c8 ( 3N ( xk \u2212\nmk N\n)) , (5)\nwhere\n\u03c8(x) =    1, |x| < 1, 0, 2 < |x|, 2\u2212 |x|, 1 \u2264 |x| \u2264 2\n(see Fig. 3). Note that \u2016\u03c8\u2016\u221e = 1 and \u2016\u03c6m\u2016\u221e = 1 \u2200m (6)\nand\nsupp\u03c6m \u2282 { x : \u2223\u2223\u2223xk \u2212\nmk N \u2223\u2223\u2223 < 1 N \u2200k } . (7)\nFor any m \u2208 {0, . . . , N}d, consider the degree-(n\u2212 1) Taylor polynomial for the function f at x = m\nN :\nPm(x) = \u2211\nn:|n|<n\nDnf\nn! \u2223\u2223\u2223\u2223 x=m\nN\n( x\u2212 m\nN\n)n , (8)\nwith the usual conventions n! = \u220fd k=1 nk! and (x\u2212 mN )n = \u220fd\nk=1(xk \u2212 mkN )nk . Now define an approximation to f by\nf1 = \u2211\nm\u2208{0,...,N}d \u03c6mPm. (9)\nWe bound the approximation error using the Taylor expansion of f :\n|f(x)\u2212 f1(x)| = \u2223\u2223\u2223 \u2211\nm\n\u03c6m(x)(f(x)\u2212 Pm(x)) \u2223\u2223\u2223\n\u2264 \u2211\nm:|xk\u2212 mk N |< 1 N \u2200k\n|f(x)\u2212 Pm(x)|\n\u2264 2d max m:|xk\u2212 mk N |< 1 N \u2200k |f(x)\u2212 Pm(x)| \u2264 2 ddn\nn! ( 1 N )n max n:|n|=n ess sup x\u2208[0,1]d |Dnf(x)|\n\u2264 2 ddn\nn! ( 1 N )n .\nHere in the second step we used the support property (7) and the bound (6), in the third the observation that any x \u2208 [0, 1]d belongs to the support of at most 2d functions \u03c6m, in the fourth a standard bound for the Taylor remainder, and in the fifth the property \u2016f\u2016Wn,\u221e([0,1]d) \u2264 1.\nIt follows that if we choose\nN = \u2308( n!\n2ddn 2\n)\u22121/n\u2309 (10)\n(where d\u00b7e is the ceiling function), then\n\u2016f \u2212 f1\u2016\u221e \u2264\n2 . (11)\nNote that, by (8) the coefficients of the polynomials Pm are uniformly bounded for all f \u2208 Fd,n:\nPm(x) = \u2211\nn:|n|<n\nam,n\n( x\u2212 m\nN\n)n , |am,n| \u2264 1. (12)\nWe have therefore reduced our task to the following: construct a network architecture capable of approximating with uniform error\n2 any function of the form (9), assuming that\nN is given by (10) and the polynomials Pm are of the form (12). Expand f1 as\nf1(x) = \u2211\nm\u2208{0,...,N}d\n\u2211\nn:|n|<n\nam,n\u03c6m(x) ( x\u2212 m\nN\n)n . (13)\nThe expansion is a linear combination of not more than dn(N + 1)d terms \u03c6m(x)(x\u2212 mN )n. Each of these terms is a product of at most d+ n\u2212 1 piece-wise linear univariate factors: d functions \u03c8(3Nxk \u2212 3mk) (see (5)) and at most n \u2212 1 linear expressions xk \u2212 mkN . We can implement an approximation of this product by a neural network with the help of Proposition 3. Specifically, let \u00d7\u0303 be the approximate multiplication from Proposition 3 for M = d + n\nand some accuracy \u03b4 to be chosen later, and consider the approximation of the product \u03c6m(x)(x\u2212 mN )n obtained by the chained application of \u00d7\u0303:\nf\u0303m,n(x) = \u00d7\u0303 ( \u03c8(3Nx1 \u2212 3m1), \u00d7\u0303 ( \u03c8(3Nx2 \u2212 3m2), . . . , \u00d7\u0303 ( xk \u2212 mkN , . . . ) . . . )) . (14)\nthat Using statement c) of Proposition 3, we see f\u0303m,n can be implemented by a ReLU network with the depth and the number of weights and computation units not larger than (d+ n)c1 ln(1/\u03b4), for some constant c1 = c1(d, n).\nNow we estimate the error of this approximation. Note that we have |\u03c8(3Nxk\u22123mk)| \u2264 1 and |xk \u2212 mkN | \u2264 1 for all k and all x \u2208 [0, 1]d. By statement a) of Proposition 3, if |a| \u2264 1 and |b| \u2264 M , then |\u00d7\u0303(a, b)| \u2264 |b| + \u03b4. Repeatedly applying this observation to all approximate multiplications in (14) while assuming \u03b4 < 1, we see that the arguments of all these multiplications are bounded by our M (equal to d + n) and the statement a) of Proposition 3 holds for each of them. We then have\n\u2223\u2223f\u0303m,n(x)\u2212\u03c6m(x) ( x\u2212 m\nN )n\u2223\u2223 = \u2223\u2223\u00d7\u0303 ( \u03c8(3Nx1 \u2212 3m1), \u00d7\u0303 ( \u03c8(3Nx2 \u2212 3m2), \u00d7\u0303 ( \u03c8(3Nx3 \u2212 3m3), . . . )))\n\u2212 \u03c8(3Nx1 \u2212 3m1)\u03c8(3Nx2 \u2212 3m2)\u03c8(3Nx3 \u2212 3m3) . . . \u2223\u2223\n\u2264 \u2223\u2223\u00d7\u0303 ( \u03c8(3Nx1 \u2212 3m1), \u00d7\u0303 ( \u03c8(3Nx2 \u2212 3m2), \u00d7\u0303 ( \u03c8(3Nx3 \u2212 3m3), . . . )))\n\u2212 \u03c8(3Nx1 \u2212 3m1) \u00b7 \u00d7\u0303 ( \u03c8(3Nx2 \u2212 3m2), \u00d7\u0303 ( \u03c8(3Nx3 \u2212 3m3), . . . ))\u2223\u2223 + |\u03c8(3Nx1 \u2212 3m1)| \u00b7 \u2223\u2223\u00d7\u0303 ( \u03c8(3Nx2 \u2212 3m2), \u00d7\u0303 ( \u03c8(3Nx3 \u2212 3m3), . . . )) \u2212 \u03c8(3Nx2 \u2212 3m2) \u00b7 \u00d7\u0303 ( \u03c8(3Nx3 \u2212 3m3), . . .\n)\u2223\u2223 + . . .\n\u2264(d+ n)\u03b4.\n(15)\nMoreover, by statement b) of Proposition 3,\nf\u0303m,n(x) = \u03c6m(x) ( x\u2212 m\nN\n)n , x /\u2208 supp\u03c6m. (16)\nNow we define the full approximation by\nf\u0303 = \u2211\nm\u2208{0,...,N}d\n\u2211\nn:|n|<n\nam,nf\u0303m,n. (17)\nWe estimate the approximation error of f\u0303 :\n|f\u0303(x)\u2212 f1(x)| = \u2223\u2223\u2223\u2223\n\u2211\nm\u2208{0,...,N}d\n\u2211\nn:|n|<n\nam,n ( f\u0303m,n(x)\u2212 \u03c6m(x) ( x\u2212 m\nN )n) \u2223\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223 \u2211\nm:x\u2208supp\u03c6m\n\u2211\nn:|n|<n\nam,n ( f\u0303m,n(x)\u2212 \u03c6m(x) ( x\u2212 m\nN )n) \u2223\u2223\u2223\u2223\n\u2264 2d max m:x\u2208supp\u03c6m\n\u2211\nn:|n|<n\n\u2223\u2223\u2223f\u0303m,n(x)\u2212 \u03c6m(x) ( x\u2212 m\nN\n)n\u2223\u2223\u2223\n\u2264 2ddn(d+ n)\u03b4,\nwhere in the first step we use expansion (13), in the second the identity (16), in the third the bound |am,n| \u2264 1 and the fact that x \u2208 supp\u03c6m for at most 2d functions \u03c6m, and in the fourth the bound (15). It follows that if we choose\n\u03b4 = 2d+1dn(d+ n) , (18)\nthen \u2016f\u0303 \u2212 f1\u2016\u221e \u2264 2 and hence, by (11),\n\u2016f\u0303 \u2212 f\u2016\u221e \u2264 \u2016f\u0303 \u2212 f1\u2016\u221e + \u2016f1 \u2212 f\u2016\u221e \u2264\n2 + 2 \u2264 .\nOn the other hand, note that by (17), f\u0303 can be implemented by a network consisting of\nparallel subnetworks that compute each of f\u0303m,n; the final output is obtained by weighting the outputs of the subnetworks with the weights am,n. The architecture of the full network does not depend on f ; only the weights am,n do. As already shown, each of these subnetworks has not more than c1 ln(1/\u03b4) layers, weights and computation units, with some constant c1 = c1(d, n). There are not more than d n(N + 1)d such subnetworks. Therefore, the full network for f\u0303 has not more than c1 ln(1/\u03b4) + 1 layers and d n(N + 1)d(c1 ln(1/\u03b4) + 1) weights and computation units. With \u03b4 given by (18) and N given by (10), we obtain the claimed complexity bounds."}, {"heading": "3.3 Faster approximations using adaptive network architectures", "text": "Theorem 1 provides an upper bound for the approximation complexity in the case when the same network architecture is used to approximate all functions in Fd,n. We can consider an alternative, \u201cadaptive architecture\u201d scenario where not only the weights, but also the architecture is adjusted to the approximated function. We expect, of course, that this would decrease the complexity of the resulting architectures, in general (at the price of needing to find the appropriate architecture). In this section we show that we can indeed obtain better upper bounds in this scenario.\nFor simplicity, we will only consider the case d = n = 1. Then, Wn,\u221e([0, 1]d) is the space of Lipschitz functions on the segment [0, 1]. The set F1,1 consists of functions f having both \u2016f\u2016\u221e and the Lipschitz constant bounded by 1. Theorem 1 provides an upper bound O( ln(1/ ) ) for the number of weights and computation units, but in this special case there is in fact a better bound O(1 ) obtained simply by piece-wise interpolation.\nNamely, given f \u2208 F1,1 and > 0, set T = d1 e and let f\u0303 be the piece-wise interpolation of f with T + 1 uniformly spaced breakpoints ( t\nT )Tt=0 (i.e., f\u0303( t T ) = f( t T ), t = 0, . . . , T ). The\nfunction f\u0303 is also Lipschitz with constant 1 and hence \u2016f \u2212 f\u0303\u2016\u221e \u2264 1T \u2264 (since for any x \u2208 [0, 1] we can find t such that |x \u2212 t\nT | \u2264 1 2T and then |f(x) \u2212 f\u0303(x)| \u2264 |f(x) \u2212 f( t T )| +\n|f\u0303( t T )\u2212 f\u0303(x)| \u2264 2 \u00b7 1 2T = 1 T\n). At the same time, the function f\u0303 can be expressed in terms of the ReLU function \u03c3 by\nf\u0303(x) = b+ T\u22121\u2211\nt=0\nwt\u03c3 ( x\u2212 t\nT\n)\nwith some coefficients b and (wt) T\u22121 t=0 . This expression can be viewed as a special case of the depth-3 ReLU network with O(1 ) weights and computation units.\nWe show now how the bound O(1 ) can be improved by using adaptive architectures.\nTheorem 2. For any f \u2208 F1,1 and \u2208 (0, 12), there exists a depth-6 ReLU network \u03b7 (with architecture depending on f) that provides an -approximation of f while having not more than c\nln(1/ ) weights, connections and computation units. Here c is an absolute constant.\nProof. We first explain the idea of the proof. We start with interpolating f by a piece-wise linear function, but not on the length scale \u2013 instead, we do it on a coarser length scale m , with some m = m( ) > 1. We then create a \u201ccache\u201d of auxiliary subnetworks that we use to fill in the details and go down to the scale , in each of the m -subintervals. This allows us to reduce the amount of computations for small because the complexity of the cache only depends on m. The assignment of cached subnetworks to the subintervals is encoded in the network architecture and depends on the function f . We optimize m by balancing the complexity of the cache with that of the initial coarse approximation. This leads to m \u223c ln(1/ ) and hence to the reduction of the total complexity of the network by a factor \u223c ln(1/ ) compared to the simple piece-wise linear approximation on the scale . This construction is inspired by a similar argument used to prove the O(2n/n) upper bound for the complexity of Boolean circuits implementing n-ary functions Shannon [1949].\nThe proof becomes simpler if, in addition to the ReLU function \u03c3, we are allowed to use the activation function\n\u03c1(x) = { x, x \u2208 [0, 1), 0, x /\u2208 [0, 1) (19)\nin our neural network. Since \u03c1 is discontinuous, we cannot just use Proposition 1 to replace \u03c1-units by \u03c3-units. We will first prove the analog of the claimed result for the model including \u03c1-units, and then we will show how to construct a purely ReLU nework.\nLemma 1. For any f \u2208 F1,1 and \u2208 (0, 12), there exists a depth-5 network including \u03c3units and \u03c1-units, that provides an -approximation of f while having not more than c\nln(1/ )\nweights, where c is an absolute constant.\nProof. Given f \u2208 F1,1, we will construct an approximation f\u0303 to f in the form\nf\u0303 = f\u03031 + f\u03032.\nHere, f\u03031 is the piece-wise linear interpolation of f with the breakpoints { tT }Tt=0, for some positive integer T to be chosen later. Since f is Lipschitz with constant 1, f\u03031 is also Lipschitz with constant 1. We will denote by It the intervals between the breakpoints:\nIt = [ t T , t+ 1 T ) , t = 0, . . . , T \u2212 1.\nWe will now construct f\u03032 as an approximation to the difference\nf2 = f \u2212 f\u03031. (20)\nNote that f2 vanishes at the endpoints of the intervals It:\nf2 ( t T ) = 0, t = 0, . . . , T, (21)\nand f2 is Lipschitz with constant 2:\n|f2(x1)\u2212 f2(x2)| \u2264 2|x1 \u2212 x2|, (22)\nsince f and f\u03031 are Lipschitz with constant 1. To define f\u03032, we first construct a set \u0393 of cached functions. Let m be a positive integer to be chosen later. Let \u0393 be the set of piecewise linear functions \u03b3 : [0, 1] \u2192 R with the breakpoints { r\nm }mr=0 and the properties\n\u03b3(0) = \u03b3(1) = 0\nand\n\u03b3 ( r m ) \u2212 \u03b3 (r \u2212 1 m ) \u2208 { \u2212 2 m , 0, 2 m } , r = 1, . . . ,m.\nNote that the size |\u0393| of \u0393 is not larger than 3m. If g : [0, 1] \u2192 R is any Lipschitz function with constant 2 and g(0) = g(1) = 0, then g can be approximated by some \u03b3 \u2208 \u0393 with error not larger than 2 m : namely, take \u03b3( r m\n) = 2 m bg( r m )/ 2 m c.\nMoreover, if f2 is defined by (20), then, using (21), (22), on each interval It the function f2 can be approximated with error not larger than 2 Tm\nby a properly rescaled function \u03b3 \u2208 \u0393. Namely, for each t = 0, . . . , T \u2212 1 we can define the function g by g(y) = Tf2( t+yT ). Then it is Lipschitz with constant 2 and g(0) = g(1) = 0, so we can find \u03b3t \u2208 \u0393 such that\nsup y\u2208[0,1)\n\u2223\u2223\u2223Tf2 (t+ y\nT\n) \u2212 \u03b3t(y) \u2223\u2223\u2223 \u2264 2 m .\nThis can be equivalently written as\nsup x\u2208It\n\u2223\u2223\u2223f2(x)\u2212 1\nT \u03b3t(Tx\u2212 t) \u2223\u2223\u2223 \u2264 2 Tm .\nNote that the obtained assignment t 7\u2192 \u03b3t is not injective, in general (T will be much larger than |\u0393|).\nWe can then define f\u03032 on the whole [0, 1) by\nf\u03032(x) = 1\nT \u03b3t(Tx\u2212 t), x \u2208 It, t = 0, . . . , T \u2212 1. (23)\nThis f\u03032 approximates f2 with error 2 Tm on [0, 1):\nsup x\u2208[0,1)\n|f2(x)\u2212 f\u03032(x)| \u2264 2\nTm , (24)\nand hence, by (20), for the full approximation f\u0303 = f\u03031 + f\u03032 we will also have\nsup x\u2208[0,1) |f(x)\u2212 f\u0303(x)| \u2264 2 Tm . (25)\nNote that the approximation f\u03032 has properties analogous to (21), (22):\nf\u03032 ( t T ) = 0, t = 0, . . . , T, (26)\n|f\u03032(x1)\u2212 f\u03032(x2)| \u2264 2|x1 \u2212 x2|, (27) in particular, f\u03032 is continuous on [0, 1).\nWe will now rewrite f\u03032 in a different form interpretable as a computation by a neural network. Specifically, using our additional activation function \u03c1 given by (19), we can express\nf\u03032 as\nf\u03032(x) = 1\nT\n\u2211\n\u03b3\u2208\u0393\n\u03b3 ( \u2211\nt:\u03b3t=\u03b3\n\u03c1(Tx\u2212 t) ) . (28)\nIndeed, given x \u2208 [0, 1), observe that all the terms in the inner sum vanish except for the one corresponding to the t determined by the condition x \u2208 It. For this particular t we have \u03c1(Tx\u2212 t) = Tx\u2212 t. Since \u03b3(0) = 0, we conclude that (28) agrees with (23).\nLet us also expand \u03b3 \u2208 \u0393 over the basis of shifted ReLU functions:\n\u03b3(x) = m\u22121\u2211\nr=0\nc\u03b3,r\u03c3 ( x\u2212 r\nm\n) , x \u2208 [0, 1].\nSubstituting this expansion in (28), we finally obtain\nf\u03032(x) = 1\nT\n\u2211\n\u03b3\u2208\u0393\nm\u22121\u2211\nr=0\nc\u03b3,r\u03c3 ( \u2211\nt:\u03b3t=\u03b3\n\u03c1(Tx\u2212 t)\u2212 r m\n) . (29)\nNow consider the implementation of f\u0303 by a neural nework. The term f\u03031 can clearly be implemented by a depth-3 ReLU network using O(T ) connections and computation units.\nThe term f\u03032 can be implemented by a depth-5 network with \u03c1- and \u03c3-units as follows (we denote a computation unit by Q with a superscript indexing the layer and a subscript indexing the unit within the layer).\n1. The first layer contains the single input unit Q(1).\n2. The second layer contains T units (Q (2) t ) T t=1 computing Q (2) t = \u03c1(TQ (1) \u2212 t). 3. The third layer contains |\u0393| units (Q(3)\u03b3 )\u03b3\u2208\u0393 computing Q(3)\u03b3 = \u03c3( \u2211\nt:\u03b3t=\u03b3 Q (2) t ). This is\nequivalent to Q (3) \u03b3 = \u2211 t:\u03b3t=\u03b3 Q (2) t , because Q (2) t \u2265 0.\n4. The fourth layer contains m|\u0393| units (Q(4)\u03b3,r) \u03b3\u2208\u0393 r=0,...,m\u22121 computing Q (4) \u03b3,r = \u03c3(Q (3) \u03b3 \u2212 rm). 5. The final layer consists of a single output unit Q(5) = \u2211 \u03b3\u2208\u0393 \u2211m\u22121 r=0 c\u03b3,r T Q (4) \u03b3,r.\nExamining this network, we see that the total number of connections and units in it is O(T + m|\u0393|) and hence is O(T + m3m). This also holds for the full network implementing f\u0303 = f\u03031 + f\u03032, since the term f\u03031 requires even fewer layers, connections and units. The output units of the subnetworks for f\u03031 and f\u03032 can be merged into the output unit for f\u03031 + f\u03032, so the depth of the full network is the maximum of the depths of the networks implementing f\u03031 and f\u03032, i.e., is 5 (see Fig. 4).\nNow, given \u2208 (0, 1 2 ), take m = d1 2 log3(1/ )e and T = d 2m e. Then, by (25), the approximation error maxx\u2208[0,1] |f(x)\u2212 f\u0303(x)| \u2264 2Tm \u2264 , while T + m3m = O( 1 ln(1/ )), which implies the claimed complexity bound.\nWe show now how to modify the constructed network so as to remove \u03c1-units. We only need to modify the f\u03032 part of the network. We will show that for any \u03b4 > 0 we can replace f\u03032 with a function f\u03033,\u03b4 (defined below) that\na) obeys the following analog of approximation bound (24):\nsup x\u2208[0,1]\n|f2(x)\u2212 f\u03033,\u03b4(x)| \u2264 8\u03b4\nT +\n2\nTm , (30)\nb) and is implementable by a depth-6 ReLU network having complexity c(T +m3m) with an absolute constant c independent of \u03b4.\nSince \u03b4 can be taken arbitrarily small, the Theorem then follows by arguing as in Lemma 1, only with f\u03032 replaced by f\u03033,\u03b4.\nAs a first step, we approximate \u03c1 by a continuous piece-wise linear function \u03c1\u03b4, with a small \u03b4 > 0:\n\u03c1(y) =    y, y \u2208 [0, 1\u2212 \u03b4), 1\u2212\u03b4 \u03b4\n(1\u2212 y), y \u2208 [1\u2212 \u03b4, 1), 0, y /\u2208 [0, 1).\nLet f\u03032,\u03b4 be defined as f\u03032 in (29), but with \u03c1 replaced by \u03c1\u03b4:\nf\u03032,\u03b4(x) = 1\nT\n\u2211\n\u03b3\u2208\u0393\nm\u22121\u2211\nr=0\nc\u03b3,r\u03c3 ( \u2211\nt:\u03b3t=\u03b3\n\u03c1\u03b4(Tx\u2212 t)\u2212 r\nm\n) .\nSince \u03c1\u03b4 is a continuous piece-wise linear function with three breakpoints, we can express it via the ReLU function, and hence implement f\u03032,\u03b4 by a purely ReLU network, as in Proposition 1, and the complexity of the implementation does not depend on \u03b4. However, replacing \u03c1 with \u03c1\u03b4 affects the function f\u03032 on the intervals ( t\u2212\u03b4 T , t T ], t = 1, . . . , T , introducing there a large error (of magnitude O( 1 T )). But recall that both f2 and f\u03032 vanish at the points t T , t = 0, . . . , T, by (21), (26). We can then largely remove this newly introduced error by simply suppressing f\u03032,\u03b4 near the points t T\n. Precisely, consider the continuous piece-wise linear function\n\u03c6\u03b4(y) =    0, y /\u2208 [0, 1\u2212 \u03b4), y \u03b4 , y \u2208 [0, \u03b4),\n1, y \u2208 [\u03b4, 1\u2212 2\u03b4), 1\u2212\u03b4\u2212y \u03b4 , y \u2208 [1\u2212 2\u03b4, 1\u2212 \u03b4)\nand the full comb-like filtering function\n\u03a6\u03b4(x) = T\u22121\u2211\nt=0\n\u03c6\u03b4(Tx\u2212 t).\nNote that \u03a6\u03b4 is continuous piece-wise linear with 4T breakpoints, and 0 \u2264 \u03a6\u03b4(x) \u2264 1. We then define our final modification of f\u03032 as\nf\u03033,\u03b4(x) = \u03c3 ( f\u03032,\u03b4(x) + 2\u03a6\u03b4(x)\u2212 1 ) \u2212 \u03c3 ( 2\u03a6\u03b4(x)\u2212 1 ) . (31)\nLemma 2. The function f\u03033,\u03b4 obeys the bound (30).\nProof. Given x \u2208 [0, 1), let t \u2208 {0, . . . , T \u2212 1} and y \u2208 [0, 1) be determined from the representation x = t+y\nT (i.e., y is the relative position of x in the respective interval It).\nConsider several possibilities for y:\n1. y \u2208 [1\u2212 \u03b4, 1]. In this case \u03a6\u03b4(x) = 0. Note that\nsup x\u2208[0,1]\n|f\u03032,\u03b4(x)| \u2264 1, (32)\nbecause, by construction, supx\u2208[0,1] |f\u03032,\u03b4(x)| \u2264 supx\u2208[0,1] |f\u03032(x)|, and supx\u2208[0,1] |f\u03032(x)| \u2264 1 by (26), (27). It follows that both terms in (31) vanish, i.e., f\u03033,\u03b4(x) = 0. But, since f2 is Lipschitz with constant 2 by (22) and f2( t+1 T\n) = 0, we have |f2(x)| \u2264 |f2(x)\u2212f2( t+1T )| \u2264 2|y\u22121| T \u2264 2\u03b4 T . This implies |f2(x)\u2212 f\u03033,\u03b4(x)| \u2264 2\u03b4T .\n2. y \u2208 [\u03b4, 1 \u2212 2\u03b4]. In this case \u03a6\u03b4(x) = 1 and f\u03032,\u03b4(x) = f\u03032(x). Using (32), we find that f\u03033,\u03b4(x) = f\u03032,\u03b4(x) = f\u03032(x). It follows that |f2(x)\u2212 f\u03033,\u03b4(x)| = |f2(x)\u2212 f\u03032(x)| \u2264 2Tm .\n3. y \u2208 [0, \u03b4]\u222a [1\u22122\u03b4, 1\u2212\u03b4]. In this case f\u03032,\u03b4(x) = f\u03032(x). Since \u03c3 is Lipschitz with constant 1, |f\u03033,\u03b4(x)| \u2264 |f\u03032,\u03b4(x)| = |f\u03032(x)|. Both f2 and f\u03032 are Lipschitz with constant 2 (by (22), (27)) and vanish at t\nT and t+1 T (by (21), (26)). It follows that\n|f2(x)\u2212 f\u03033,\u03b4(x)| \u2264 |f2(x)|+ |f\u03032(x)| \u2264 2 { 2|x\u2212 t T |, y \u2208 [0, \u03b4]\n2|x\u2212 t+1 T |, y \u2208 [1\u2212 2\u03b4, 1\u2212 \u03b4] \u2264\n8\u03b4 T .\nIt remains to verify the complexity property b) of the function f\u03033,\u03b4. As already mentioned, f\u03032,\u03b4 can be implemented by a depth-5 purely ReLU network with not more than c(T +m3 m) weights, connections and computation units, where c is an absolute constant independent of \u03b4. The function \u03a6\u03b4 can be implemented by a shallow, depth-3 network with O(T ) units and connection. Then, computation of f\u03033,\u03b4 can be implemented by a network including two subnetworks for computing f\u03032,\u03b4 and \u03a8\u03b4, and an additional layer containing two \u03c3-units as written in (31). We thus obtain 6 layers in the resulting full network and, choosing T and m in the same way as in Lemma 1, obtain the bound c\nln(1/ ) for the number of its connections,\nweights, and computation units."}, {"heading": "4 Lower bounds", "text": ""}, {"heading": "4.1 Continuous nonlinear widths", "text": "The method of continuous nonlinear widths (DeVore et al. [1989]) is a very general approach to the analysis of parameterized nonlinear approximations, based on the assumption of continuous selection of their parameters. We are interested in the following lower bound for the complexity of approximations in Wn,\u221e([0, 1]d).\nTheorem 3 (DeVore et al. [1989], Theorem 4.2). Fix d, n. Let W be a positive integer and \u03b7 : RW \u2192 C([0, 1]d) be any mapping between the space RW and the space C([0, 1]d). Suppose that there is a continuous map M : Fd,n \u2192 RW such that \u2016f \u2212 \u03b7(M(f))\u2016\u221e \u2264 for all f \u2208 Fd,n. Then W \u2265 c \u2212d/n, with some constant c depending only on n.\nWe apply this theorem by taking \u03b7 to be some ReLU network architecture, and RW the corresponding weight space. It follows that if a ReLU network architecture is capable of expressing any function from Fd,n with error , then, under the hypothesis of continuous weight selection, the network must have at least c \u2212d/n weights. The number of connections is then lower bounded by c\n2 \u2212d/n (since the number of weights is not larger than the sum of\nthe number of computation units and the number of connections, and there are at least as many connections as units).\nThe hypothesis of continuous weight selection is crucial in Theorem 3. By examining our proof of the counterpart upper bound O( \u2212d/n ln(1/ )) in Theorem 1, the weights are selected there in a continuous manner, so this upper bound asymptotically lies above c \u2212d/n in agreement with Theorem 3. We remark, however, that the optimal choice of the network weights (minimizing the error) is known to be discontinuous in general, even for shallow networks (Kainen et al. [1999]).\nWe also compare the bounds of Theorems 3 and 2. In the case d = n = 1, Theorem 3 provides a lower bound c for the number of weights and connections. On the other hand, in the adaptive architecture scenario, Theorem 2 provides the upper bound c ln(1/ )\nfor the number of weights, connections and computation units. The fact that this latter bound is asymptotically below the bound of Theorem 3 reflects the extra expressiveness associated with variable network architecture."}, {"heading": "4.2 Bounds based on VC-dimension", "text": "In this section we consider the setup where the same network architecture is used to approximate all functions f \u2208 Fd,n, but the dependence of the weights on f is not assumed to be necessarily continuous. In this setup, some lower bounds on the network complexity can be obtained as a consequence of existing upper bounds on VC-dimension of networks with piece-wise polynomial activation functions and Boolean outputs (Anthony and Bartlett [2009]). In the next theorem, part a) is a more general but weaker bound, while part b) is a stronger bound assuming a constrained growth of the network depth.\nTheorem 4. Fix d, n.\na) For any \u2208 (0, 1), a ReLU network architecture capable of approximating any function f \u2208 Fd,n with error must have at least c \u2212d/(2n) weights, with some constant c = c(d, n) > 0.\nb) Let p \u2265 0, c1 > 0 be some constants. For any \u2208 (0, 12), if a ReLU network architecture of depth L \u2264 c1 lnp(1/ ) is capable of approximating any function f \u2208 Fd,n with error\n, then the network must have at least c2 \u2212d/n ln\u22122p\u22121(1/ ) weights, with some constant\nc2 = c2(d, n, p, c1) > 0. 1\nProof. Recall that given a class H of Boolean functions on [0, 1]d, the VC-dimension of H is defined as the size of the largest shattered subset S \u2282 [0, 1]d, i.e. the largest subset on which H can compute any dichotomy (see, e.g., Anthony and Bartlett [2009], Section 3.3). We are interested in the case when H is the family of functions obtained by applying thresholds 1(x > a) to a ReLU network with fixed architecture but variable weights. In this case Theorem 8.7 of Anthony and Bartlett [2009] implies that\nVCdim(H) \u2264 c3W 2, (33)\nand Theorem 8.8 implies that\nVCdim(H) \u2264 c3L2W lnW, (34)\nwhere W is the number of weights, L is the network depth, and c3 is an absolute constant. Given a positive integer N to be chosen later, choose S as a set of Nd points x1, . . . ,xNd in the cube [0, 1]d such that the distance between any two of them is not less than 1 N\n. Given any assignment of values y1, . . . , yNd \u2208 R, we can construct a smooth function f satisfying f(xm) = ym for all m by setting\nf(x) = Nd\u2211\nm=1\nym\u03c6(N(x\u2212 xm)), (35)\nwith some C\u221e function \u03c6 : Rd \u2192 R such that \u03c6(0) = 1 and \u03c6(x) = 0 if |x| > 1 2 .\nLet us obtain a condition ensuring that such f \u2208 Fd,n. For any multi-index n,\nmax x |Dnf(x)| = N |n|max m |ym|max x |Dn\u03c6(x)|,\nso if max m |ym| \u2264 c4N\u2212n, (36) with the constant c4 = (maxn:|n|\u2264n maxx |Dn\u03c6(x)|)\u22121, then f \u2208 Fd,n. Now set\n= c4 3 N\u2212n. (37)\nSuppose that there is a ReLU network architecture \u03b7 that can approximate, by adjusting its weights, any f \u2208 Fd,n with error less than . Denote by \u03b7(x,w) the output of the network for the input vector x and the vector of weights w.\nConsider any assignment z of Boolean values z1, . . . , zNd \u2208 {0, 1}. Set\nym = zmc4N \u2212n, m = 1, . . . , Nd,\nand let f be given by (35) (see Fig. 5); then (36) holds and hence f \u2208 Fd,n. By assumption, 1The author thanks Matus Telgarsky for suggesting this part of the theorem.\nthere is then a vector of weights, w = wz, such that for all m we have |\u03b7(xm,wz)\u2212 ym| \u2264 , and in particular\n\u03b7(xm,wz) { \u2265 c4N\u2212n \u2212 > c4N\u2212n/2, if zm = 1, \u2264 < c4N\u2212n/2, if zm = 0,\nso the thresholded network \u03b71 = 1(\u03b7 > c4N \u2212n/2) has outputs\n\u03b71(xm,wz) = zm, m = 1, . . . , N d.\nSince the Boolean values zm were arbitrary, we conclude that the subset S is shattered and hence VCdim(\u03b71) \u2265 Nd. Expressing N through with (37), we obtain\nVCdim(\u03b71) \u2265 (3 c4 )\u2212d/n . (38)\nTo establish part a) of the Theorem, we apply bound (33) to the network \u03b71:\nVCdim(\u03b71) \u2264 c3W 2, (39)\nwhere W is the number of weights in \u03b71, which is the same as in \u03b7 if we do not count the threshold parameter. Combining (38) with (39), we obtain the desired lower bound W \u2265 c \u2212d/(2n) with c = (c4/3)d/(2n)c\u22121/23 . To establish part b) of the Theorem, we use bound (34) and the hypothesis L \u2264 c1 ln p(1/ ): VCdim(\u03b71) \u2264 c3c21 ln2p(1/ )W lnW. (40) Combining (38) with (40), we obtain\nW lnW \u2265 1 c3c21 (3 c4 )\u2212d/n ln\u22122p(1/ ). (41)\nTrying a W of the form Wc2 = c2 \u2212d/n ln\u22122p\u22121(1/ ) with a constant c2, we get\nWc2 lnWc2 = c2 \u2212d/n ln\u22122p\u22121(1/ ) (d n ln(1/ ) + ln c2 \u2212 (2p+ 1) ln ln(1/ ) )\n= ( c2 d\nn + o(1)\n) \u2212d/n ln\u22122p(1/ ).\nComparing this with (41), we see that if we choose c2 < (c4/3) d/nn/(dc3c 2 1), then for sufficiently small we have W lnW \u2265 Wc2 lnWc2 and hence W \u2265 Wc2 , as claimed. We can ensure that W \u2265 Wc2 for all \u2208 (0, 12) by further decreasing c2.\nWe remark that the constrained depth hypothesis of part b) is satisfied, with p = 1, by the architecture used for the upper bound in Theorem 1. The bound stated in part b) of Theorem 4 matches the upper bound of Theorem 1 and the lower bound of Theorem 3 up to a power of ln(1/ )."}, {"heading": "4.3 Adaptive network architectures", "text": "Our goal in this section is to obtain a lower bound for the approximation complexity in the scenario where the network architecture may depend on the approximated function. This lower bound is thus a counterpart to the upper bound of Section 3.3.\nTo state this result we define the complexity N (f, ) of approximating the function f with error as the minimal number of hidden computation units in a ReLU network that provides such an approximation.\nTheorem 5. For any d, n, there exists f \u2208 Wn,\u221e([0, 1]d) such that N (f, ) is not o( \u2212d/(9n)) as \u2192 0.\nThe proof relies on the following lemma.\nLemma 3. Fix d, n. For any sufficiently small > 0 there exists f \u2208 Fd,n such that N (f , ) \u2265 c1 \u2212d/(8n), with some constant c1 = c1(d, n) > 0.\nProof. Observe that all the networks with not more than m hidden computation units can be embedded in the single \u201cenveloping\u201d network that has m hidden layers, each consisting of m units, and that includes all the connections between units not in the same layer (see Fig. 6a). The number of weights in this enveloping network is O(m4). On the other hand, Theorem 4a) states that at least c \u2212d/(2n) weights are needed for an architecture capable of -approximating any function in Fd,n. It follows that there is a function f \u2208 Fd,n that cannot be -approximated by networks with fewer than c1 \u2212d/(8n) computation units.\nBefore proceeding to the proof of Theorem 5, note that N (f, ) is a monotone decreasing function of with a few obvious properties:\nN (af, |a| ) = N (f, ), for any a \u2208 R \\ {0} (42)\n(follows by multiplying the weights of the output unit of the approximating network by a constant); N (f \u00b1 g, + \u2016g\u2016\u221e) \u2264 N (f, ) (43) (follows by approximating f \u00b1 g by an approximation of f);\nN (f1 \u00b1 f2, 1 + 2) \u2264 N (f1, 1) +N (f2, 2) (44)\n(follows by combining approximating networks for f1 and f2 as in Fig. 6b).\nProof of Theorem 5. The claim of Theorem 5 is similar to the claim of Lemma 3, but is about a single function f satisfying a slightly weaker complexity bound at multiple values of \u2192 0. We will assume that Theorem 5 is false, i.e.,\nN (f, ) = o( \u2212d/(9n)) (45)\nfor all f \u2208 Wn,\u221e([0, 1]d), and we will reach contradiction by presenting f violating this assumption. Specifically, we construct this f as\nf = \u221e\u2211\nk=1\nakfk, (46)\nwith some ak \u2208 R, fk \u2208 Fd,n, and we will make sure that\nN (f, k) \u2265 \u2212d/(9n)k (47)\nfor a sequence of k \u2192 0. We determine ak, fk, k sequentially. Suppose we have already found {as, fs, s}k\u22121s=1 ; let us describe how we define ak, fk, k.\nFirst, we set\nak = min s=1,...,k\u22121 s 2k\u2212s . (48)\nIn particular, this ensures that ak \u2264 121\u2212k, so that the function f defined by the series (46) will be in Wn,\u221e([0, 1]d), because \u2016fk\u2016Wn,\u221e([0,1]d) \u2264 1.\nNext, using Lemma 3 and Eq. (42), observe that if k is sufficiently small, then we can find fk \u2208 Fd,n such that\nN ( akfk, 3 k ) = N ( fk,\n3 k ak\n) \u2265 c1 (3 k ak )\u2212d/(8n) \u2265 2 \u2212d/(9n)k . (49)\nIn addition, by assumption (45), if k is small enough then\nN ( k\u22121\u2211\ns=1\nasfs, k ) \u2264 \u2212d/(9n)k . (50)\nLet us choose k and fk so that both (49) and (50) hold. Obviously, we can also make sure that k \u2192 0 as k \u2192\u221e.\nLet us check that the above choice of {ak, fk, k}\u221ek=1 ensures that inequality (47) holds for all k:\nN ( \u221e\u2211\ns=1\nasfs, k\n) \u2265 N ( k\u2211\ns=1\nasfs, k + \u2225\u2225\u2225 \u221e\u2211\ns=k+1\nasfs \u2225\u2225\u2225 \u221e )\n\u2265 N ( k\u2211\ns=1\nasfs, k + \u221e\u2211\ns=k+1\nas\n)\n\u2265 N ( k\u2211\ns=1\nasfs, 2 k\n)\n\u2265 N (akfk, 3 k)\u2212N ( k\u22121\u2211\ns=1\nasfs, k\n)\n\u2265 \u2212d/(9n).\nHere in the first step we use inequality (43), in the second the monotonicity of N (f, ), in the third the monotonicity of N (f, ) and the setting (48), in the fourth the inequality (44), and in the fifth the conditions (49) and (50)."}, {"heading": "4.4 Slow approximation of smooth functions by shallow networks", "text": "In this section we show that, in contrast to deep ReLU networks, shallow ReLU networks relatively inefficiently approximate sufficiently smooth (C2) nonlinear functions. We remark\nthat Liang and Srikant 2016 prove a similar result assuming global convexity instead of smoothness and nonlinearity.\nTheorem 6. Let f \u2208 C2([0, 1]d) be a nonlinear function (i.e., not of the form f(x1, . . . , xd) \u2261 a0 + \u2211d k=1 akxk on the whole [0, 1]\nd). Then, for any fixed L, a depth-L ReLU network approximating f with error \u2208 (0, 1) must have at least c \u22121/(2(L\u22122)) weights and computation units, with some constant c = c(f, L) > 0.\nProof. Since f \u2208 C2([0, 1]d and f is nonlinear, we can find x0 \u2208 [0, 1]d and v \u2208 Rd such that x0 + xv \u2208 [0, 1]d for all x \u2208 [\u22121, 1] and the function f1 : x 7\u2192 f(x0 + xv) is strictly convex or concave on [\u22121, 1]. Suppose without loss of generality that it is strictly convex:\nmin x\u2208[\u22121,1]\nf \u2032\u20321 (x) = c1 > 0. (51)\nSuppose that f\u0303 is an -approximation of function f , and let f\u0303 be implemented by a ReLU network \u03b7 of depth L. Let f\u03031 : x 7\u2192 f\u0303(x0 +xv). Then f\u03031 also approximates f1 with error not larger than . Moreover, since f\u03031 is obtained from f\u0303 by a linear substitution x = x0 + xv, f\u03031 can be implemented by a ReLU network \u03b71 of the same depth L and with the number of units and weights not larger than in \u03b7 (we can obtain \u03b71 from \u03b7 by replacing the input layer in \u03b7 with a single unit, accordingly modifying the input connections, and adjusting the weights associated with these connections). It is thus sufficient to establish the claimed bounds for \u03b71.\nBy construction, f\u03031 is a continuous piece-wise linear function of x. Denote by M the number of linear pieces in f\u03031. We will use the following counting lemma. Lemma 4. M \u2264 (2U)L\u22122, where U is the number of computation units in \u03b71.\nProof. This bound, up to minor details, is proved in Lemma 2.1 of Telgarsky [2015]. Precisely, Telgarsky\u2019s lemma states that if a network has a single input, connections only between neighboring layers, at most m units in a layer, and a piece-wise linear activation function with t pieces, then the number of linear pieces in the output of the network is not greater than (tm)L. By examining the proof of the lemma we see that it will remain valid for networks with connections not necessarily between neighboring layers, if we replace m by U in the expression (tm)L. Moreover, we can slightly strengthen the bound by noting that in the present paper the input and output units are counted as separate layers, only units of layers 3 to L have multiple incoming connections, and the activation function is applied only in layers 2 to L \u2212 1. By following Telgarsky\u2019s arguments, this gives the slightly more accurate bound (tU)L\u22122 (i.e., with the power L \u2212 2 instead of L). It remains to note that the ReLU activation function corresponds to t = 2.\nLemma 4 implies that there is an interval [a, b] \u2282 [\u22121, 1] of length not less than 2(2U)\u2212(L\u22122) on which the function f\u03031 is linear. Let g = f1 \u2212 f\u03031. Then, by the approximation accuracy assumption, supx\u2208[a,b] |g(x)| \u2264 , while by (51) and by the linearity of f\u03031\non [a, b], maxx\u2208[a,b] g \u2032\u2032(x) \u2265 c1 > 0. It follows that max(g(a), g(b)) \u2265 g(a+b2 ) + c12 ( b\u2212a2 )2 and hence\n\u2265 1 2\n( max(g(a), g(b))\u2212 g(a+b 2 ) ) \u2265 c1\n4 (b\u2212 a 2 )2 \u2265 c1 4 (2U)\u22122(L\u22122),\nwhich implies the claimed bound U \u2265 1 2 ( 4 c1 )\u22121/(2(L\u22122)). Since there are at least as many weights as computation units in a network, a similar bound holds for the number of weights."}, {"heading": "5 Discussion", "text": "We discuss some implications of the obtained bounds.\nDeep vs. shallow ReLU approximations of smooth functions. Our results clearly show that deep ReLU networks more efficiently express smooth functions than shallow ReLU networks. By Theorem 1, functions from the Sobolev space Wn,\u221e([0, 1]d) can be -approximated by ReLU networks with depth O(ln(1/ )) and the number of computation units O( \u2212d/n ln(1/ )). In contrast, by Theorem 6, a nonlinear function from C2([0, 1]d) cannot be -approximated by a ReLU network of fixed depth L with the number of units less than c \u22121/(2(L\u22122)). In particular, it follows that in terms of the required number of computation units, unbounded-depth approximations of functions from Wn,\u221e([0, 1]d) are asymptotically strictly more efficient than approximations with a fixed depth L at least when\nd n <\n1\n2(L\u2212 2)\n(assuming also n > 2, so that Wn,\u221e([0, 1]d) \u2282 C2([0, 1]d)). The efficiency of depth is even more pronounced for very smooth functions such as polynomials, which can be implemented by deep networks using only O(ln(1/ )) units (cf. Propositions 2 and 3 and the proof of Theorem 1). Liang and Srikant describe in Liang and Srikant [2016] some conditions on the approximated function (resembling conditions of local analyticity) under which complexity of deep -approximation is O(lnc(1/ )) with a constant power c.\nContinuous model selection vs. function-dependent network architectures. When approximating a function by a neural network, one can either view the network architecture as fixed and only tune the weights, or optimize the architecture as well. Moreover, when tuning the weights, one can either require them to continuously depend on the approximated function or not. We naturally expect that more freedom in the choice of the approximation should lead to higher expressiveness.\nOur bounds confirm this expectation to a certain extent. Specifically, the complexity of -approximation of functions from the unit ball F1,1 in W1,\u221e([0, 1]) is lower bounded by c in the scenario with a fixed architecture and continuously selected weights (see Theorem 3). On the other hand, we show in Theorem 2 that this complexity is upper bounded by O( 1\nln(1/ ) )\nif we are allowed to adjust the network architecture. This bound is achieved by finite-depth (depth-6) ReLU networks using the idea of reused subnetworks familiar from the theory of Boolean circuits Shannon [1949].\nIn the case of fixed architecture, we have not established any evidence of complexity improvement for unconstrained weight selection compared to continuous weight selection. We remark however that, already for approximations with depth-3 networks, the optimal weights are known to discontinuously depend, in general, on the approximated function (Kainen et al. [1999]). On the other hand, part b) of Theorem 4 shows that if the network depth scales as O(lnp(1/ )), discontinuous weight selection cannot improve the continuouscase complexity more than by a factor being some power of ln(1/ ).\nUpper vs. lower complexity bounds. We indicate the gaps between respective upper and lower bounds in the three scenarios mentioned above: fixed architectures with continuous selection of weights, fixed architectures with unconstrained selection of weights, or adaptive architectures.\nFor fixed architectures with continuous selection the lower bound c \u2212d/n is provided by Proposition 3, and the upper bound O( \u2212d/n ln(1/ )) by Theorem 1, so these bounds are tight up to a factor O(ln(1/ )).\nIn the case of fixed architecture but unconstrained selection, part b) of Theorem 4 gives a lower bound c \u2212d/n ln\u22122p\u22121(1/ ) under assumption that the depth is constrained by O(lnp(1/ )). This is only different by a factor of O(ln2p+2(1/ )) from the upper bound of Theorem 1. Without this depth constraint we only have the significantly weaker bound c \u2212d/(2n) (part a) of Theorem 4).\nIn the case of adaptive architectures, there is a big gap between our upper and lower bounds. The upper bound O( 1\nln(1/ ) ) is given by Theorem 2 for d = n = 1. The lower\nbound, proved for general d, n in Theorem 5, only states that there are f \u2208 Wn,\u221e([0, 1]d) for which the complexity is not o( \u2212d/(9n)).\nReLU vs. smooth activation functions. A popular general-purpose method of approximation is shallow (depth-3) networks with smooth activation functions (e.g., logistic sigmoid). Upper and lower approximation complexity bounds for these networks (Mhaskar [1996], Maiorov and Meir [2000]) show that complexity scales as \u223c \u2212d/n up to some ln(1/ ) factors. Comparing this with our bounds in Theorems 1,2,4, it appears that deep ReLU networks are roughly (up to ln(1/ ) factors) as expressive as shallow networks with smooth activation functions.\nConclusion. We have established several upper and lower bounds for the expressive power of deep ReLU networks in the context of approximation in Sobolev spaces. We should note, however, that this setting may not quite reflect typical real world applications, which usually possess symmetries and hierarchical and other structural properties substantially narrowing the actually interesting classes of approximated functions (LeCun et al. [2015]). Some recent publications introduce and study expressive power of deep networks in frameworks bridging\nthis gap, in particular, graph-based hierarchical approximations are studied in Mhaskar et al. [2016], Mhaskar and Poggio [2016] and convolutional arithmetic circuits in Cohen et al. [2015]. Theoretical analysis of expressiveness of deep networks taking into account such properties of real data seems to be an important and promising direction of future research."}, {"heading": "Acknowledgments", "text": "The author thanks Matus Telgarsky and the anonymous referees for multiple helpful comments on the preliminary versions of the paper. The research was funded by Skolkovo Institute of Science and Technology."}], "references": [{"title": "Neural network learning: Theoretical foundations", "author": ["Martin Anthony", "Peter L Bartlett"], "venue": "Cambridge university press,", "citeRegEx": "Anthony and Bartlett.,? \\Q2009\\E", "shortCiteRegEx": "Anthony and Bartlett.", "year": 2009}, {"title": "Almost linear VC-dimension bounds for piecewise polynomial networks", "author": ["Peter L Bartlett", "Vitaly Maiorov", "Ron Meir"], "venue": "Neural computation,", "citeRegEx": "Bartlett et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 1998}, {"title": "On the complexity of neural network classifiers: A comparison between shallow and deep architectures", "author": ["Monica Bianchini", "Franco Scarselli"], "venue": "IEEE transactions on neural networks and learning systems,", "citeRegEx": "Bianchini and Scarselli.,? \\Q2014\\E", "shortCiteRegEx": "Bianchini and Scarselli.", "year": 2014}, {"title": "Why deep neural networks", "author": ["Nadav Cohen", "Or Sharir", "Amnon Shashua"], "venue": "arXiv preprint arXiv:1509.05009,", "citeRegEx": "Cohen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2015}, {"title": "The Solovay-Kitaev algorithm", "author": ["CM Dawson", "MA Nielsen"], "venue": "Quantum Information and Computation,", "citeRegEx": "Dawson and Nielsen.,? \\Q2006\\E", "shortCiteRegEx": "Dawson and Nielsen.", "year": 2006}, {"title": "Shallow vs. deep sum-product networks", "author": ["Olivier Delalleau", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Delalleau and Bengio.,? \\Q2011\\E", "shortCiteRegEx": "Delalleau and Bengio.", "year": 2011}, {"title": "Optimal nonlinear approximation", "author": ["Ronald A DeVore", "Ralph Howard", "Charles Micchelli"], "venue": "Manuscripta mathematica,", "citeRegEx": "DeVore et al\\.,? \\Q1989\\E", "shortCiteRegEx": "DeVore et al\\.", "year": 1989}, {"title": "Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers", "author": ["Paul W Goldberg", "Mark R Jerrum"], "venue": "Machine Learning,", "citeRegEx": "Goldberg and Jerrum.,? \\Q1995\\E", "shortCiteRegEx": "Goldberg and Jerrum.", "year": 1995}, {"title": "Approximation by neural networks is not", "author": ["Paul C Kainen", "V\u011bra K\u016frkov\u00e1", "Andrew Vogt"], "venue": "continuous. Neurocomputing,", "citeRegEx": "Kainen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kainen et al\\.", "year": 1999}, {"title": "Efficient distribution-free learning of probabilistic concepts", "author": ["Michael J Kearns", "Robert E Schapire"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Kearns and Schapire.,? \\Q1990\\E", "shortCiteRegEx": "Kearns and Schapire.", "year": 1990}, {"title": "Classical and Quantum Computation", "author": ["A. Yu. Kitaev", "A.H. Shen", "M.N. Vyalyi"], "venue": "American Mathematical Society,", "citeRegEx": "Kitaev et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kitaev et al\\.", "year": 2002}, {"title": "Why deep neural networks", "author": ["Shiyu Liang", "R. Srikant"], "venue": "arXiv preprint arXiv:1610.04161,", "citeRegEx": "Liang and Srikant.,? \\Q2016\\E", "shortCiteRegEx": "Liang and Srikant.", "year": 2016}, {"title": "On the near optimality of the stochastic approximation of smooth functions by neural networks", "author": ["VE Maiorov", "Ron Meir"], "venue": "Advances in Computational Mathematics,", "citeRegEx": "Maiorov and Meir.,? \\Q2000\\E", "shortCiteRegEx": "Maiorov and Meir.", "year": 2000}, {"title": "Neural networks for optimal approximation of smooth and analytic functions", "author": ["HN Mhaskar"], "venue": "Neural Computation,", "citeRegEx": "Mhaskar.,? \\Q1996\\E", "shortCiteRegEx": "Mhaskar.", "year": 1996}, {"title": "Deep vs. shallow networks: An approximation theory perspective", "author": ["Hrushikesh Mhaskar", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1608.03287,", "citeRegEx": "Mhaskar and Poggio.,? \\Q2016\\E", "shortCiteRegEx": "Mhaskar and Poggio.", "year": 2016}, {"title": "Learning real and boolean functions: When is deep better than shallow", "author": ["Hrushikesh Mhaskar", "Qianli Liao", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1603.00988,", "citeRegEx": "Mhaskar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mhaskar et al\\.", "year": 2016}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Approximation theory of the MLP model in neural networks", "author": ["Allan Pinkus"], "venue": "Acta Numerica,", "citeRegEx": "Pinkus.,? \\Q1999\\E", "shortCiteRegEx": "Pinkus.", "year": 1999}, {"title": "On the expressive power of deep neural networks", "author": ["Maithra Raghu", "Ben Poole", "Jon Kleinberg", "Surya Ganguli", "Jascha Sohl-Dickstein"], "venue": "arXiv preprint arXiv:1606.05336,", "citeRegEx": "Raghu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Raghu et al\\.", "year": 2016}, {"title": "Tight Bounds for the VC-Dimension of Piecewise Polynomial Networks", "author": ["Akito Sakurai"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sakurai.,? \\Q1999\\E", "shortCiteRegEx": "Sakurai.", "year": 1999}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "The synthesis of two-terminal switching circuits", "author": ["Claude Shannon"], "venue": "Bell Labs Technical Journal,", "citeRegEx": "Shannon.,? \\Q1949\\E", "shortCiteRegEx": "Shannon.", "year": 1949}, {"title": "Representation benefits of deep feedforward networks", "author": ["Matus Telgarsky"], "venue": "arXiv preprint arXiv:1509.08101,", "citeRegEx": "Telgarsky.,? \\Q2015\\E", "shortCiteRegEx": "Telgarsky.", "year": 2015}, {"title": "Benefits of depth in neural networks", "author": ["Matus Telgarsky"], "venue": "arXiv preprint arXiv:1602.04485,", "citeRegEx": "Telgarsky.,? \\Q2016\\E", "shortCiteRegEx": "Telgarsky.", "year": 2016}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["Vladimir N Vapnik", "A Ya Chervonenkis"], "venue": "In Measures of Complexity,", "citeRegEx": "Vapnik and Chervonenkis.,? \\Q2015\\E", "shortCiteRegEx": "Vapnik and Chervonenkis.", "year": 2015}, {"title": "Lower bounds for approximation by nonlinear manifolds", "author": ["Hugh E Warren"], "venue": "Transactions of the American Mathematical Society,", "citeRegEx": "Warren.,? \\Q1968\\E", "shortCiteRegEx": "Warren.", "year": 1968}], "referenceMentions": [{"referenceID": 14, "context": "Recently, multiple successful applications of deep neural networks to pattern recognition problems (Schmidhuber [2015], LeCun et al.", "startOffset": 100, "endOffset": 119}, {"referenceID": 14, "context": "Recently, multiple successful applications of deep neural networks to pattern recognition problems (Schmidhuber [2015], LeCun et al. [2015]) have revived active interest in theoretical properties of such networks, in particular their expressive power.", "startOffset": 100, "endOffset": 140}, {"referenceID": 3, "context": ", Delalleau and Bengio [2011], Raghu et al.", "startOffset": 2, "endOffset": 30}, {"referenceID": 3, "context": ", Delalleau and Bengio [2011], Raghu et al. [2016], Montufar et al.", "startOffset": 2, "endOffset": 51}, {"referenceID": 3, "context": ", Delalleau and Bengio [2011], Raghu et al. [2016], Montufar et al. [2014], Bianchini and Scarselli [2014], Telgarsky [2015]).", "startOffset": 2, "endOffset": 75}, {"referenceID": 2, "context": "[2014], Bianchini and Scarselli [2014], Telgarsky [2015]).", "startOffset": 8, "endOffset": 39}, {"referenceID": 2, "context": "[2014], Bianchini and Scarselli [2014], Telgarsky [2015]).", "startOffset": 8, "endOffset": 57}, {"referenceID": 2, "context": "[2014], Bianchini and Scarselli [2014], Telgarsky [2015]). In contrast to a shallow network, a deep one can be viewed as a long sequence of non-commutative transformations, which is a natural setting for high expressiveness (cf. the well-known Solovay-Kitaev theorem on fast approximation of arbitrary quantum operations by sequences of non-commutative gates, see Kitaev et al. [2002], Dawson and Nielsen [2006]).", "startOffset": 8, "endOffset": 385}, {"referenceID": 2, "context": "[2014], Bianchini and Scarselli [2014], Telgarsky [2015]). In contrast to a shallow network, a deep one can be viewed as a long sequence of non-commutative transformations, which is a natural setting for high expressiveness (cf. the well-known Solovay-Kitaev theorem on fast approximation of arbitrary quantum operations by sequences of non-commutative gates, see Kitaev et al. [2002], Dawson and Nielsen [2006]).", "startOffset": 8, "endOffset": 412}, {"referenceID": 0, "context": "Bianchini and Scarselli 2014 give bounds for Betti numbers characterizing topological properties of functions represented by networks. Telgarsky 2015, 2016 provides specific examples of classification problems where deep networks are provably more efficient than shallow ones. In the context of classification problems, a general and standard approach to characterizing expressiveness is based on the notion of the Vapnik-Chervonenkis dimension (Vapnik and Chervonenkis [2015]).", "startOffset": 0, "endOffset": 477}, {"referenceID": 0, "context": "There exist several bounds for VC-dimension of deep networks with piece-wise polynomial activation functions that go back to geometric techniques of Goldberg and Jerrum 1995 and earlier results of Warren 1968; see Bartlett et al. [1998], Sakurai [1999] and the book Anthony and Bartlett [2009].", "startOffset": 214, "endOffset": 237}, {"referenceID": 0, "context": "There exist several bounds for VC-dimension of deep networks with piece-wise polynomial activation functions that go back to geometric techniques of Goldberg and Jerrum 1995 and earlier results of Warren 1968; see Bartlett et al. [1998], Sakurai [1999] and the book Anthony and Bartlett [2009].", "startOffset": 214, "endOffset": 253}, {"referenceID": 0, "context": "[1998], Sakurai [1999] and the book Anthony and Bartlett [2009]. There is a related concept, the fat-shattering dimension, for real-valued approximation problems (Kearns and Schapire [1990], Anthony and Bartlett [2009]).", "startOffset": 36, "endOffset": 64}, {"referenceID": 0, "context": "[1998], Sakurai [1999] and the book Anthony and Bartlett [2009]. There is a related concept, the fat-shattering dimension, for real-valued approximation problems (Kearns and Schapire [1990], Anthony and Bartlett [2009]).", "startOffset": 36, "endOffset": 190}, {"referenceID": 0, "context": "[1998], Sakurai [1999] and the book Anthony and Bartlett [2009]. There is a related concept, the fat-shattering dimension, for real-valued approximation problems (Kearns and Schapire [1990], Anthony and Bartlett [2009]).", "startOffset": 36, "endOffset": 219}, {"referenceID": 0, "context": "[1998], Sakurai [1999] and the book Anthony and Bartlett [2009]. There is a related concept, the fat-shattering dimension, for real-valued approximation problems (Kearns and Schapire [1990], Anthony and Bartlett [2009]). A very general approach to expressiveness in the context of approximation is the method of nonlinear widths by DeVore et al. 1989 that concerns approximation of a family of functions under assumption of a continuous dependence of the model on the approximated function. In this paper we examine the problem of shallow-vs-deep expressiveness from the perspective of approximation theory and general spaces of functions having derivatives up to certain order (Sobolev-type spaces). In this framework, the problem of expressiveness is very well studied in the case of shallow networks with a single hidden layer, where it is known, in particular, that to approximate a C-function on a d-dimensional set with infinitesimal error one needs a network of size about \u2212d/n, assuming a smooth activation function (see, e.g., Mhaskar [1996], Pinkus [1999] for a number of related rigorous upper and lower bounds and further qualifications of this result).", "startOffset": 36, "endOffset": 1051}, {"referenceID": 0, "context": "[1998], Sakurai [1999] and the book Anthony and Bartlett [2009]. There is a related concept, the fat-shattering dimension, for real-valued approximation problems (Kearns and Schapire [1990], Anthony and Bartlett [2009]). A very general approach to expressiveness in the context of approximation is the method of nonlinear widths by DeVore et al. 1989 that concerns approximation of a family of functions under assumption of a continuous dependence of the model on the approximated function. In this paper we examine the problem of shallow-vs-deep expressiveness from the perspective of approximation theory and general spaces of functions having derivatives up to certain order (Sobolev-type spaces). In this framework, the problem of expressiveness is very well studied in the case of shallow networks with a single hidden layer, where it is known, in particular, that to approximate a C-function on a d-dimensional set with infinitesimal error one needs a network of size about \u2212d/n, assuming a smooth activation function (see, e.g., Mhaskar [1996], Pinkus [1999] for a number of related rigorous upper and lower bounds and further qualifications of this result).", "startOffset": 36, "endOffset": 1066}, {"referenceID": 0, "context": "[1998], Sakurai [1999] and the book Anthony and Bartlett [2009]. There is a related concept, the fat-shattering dimension, for real-valued approximation problems (Kearns and Schapire [1990], Anthony and Bartlett [2009]). A very general approach to expressiveness in the context of approximation is the method of nonlinear widths by DeVore et al. 1989 that concerns approximation of a family of functions under assumption of a continuous dependence of the model on the approximated function. In this paper we examine the problem of shallow-vs-deep expressiveness from the perspective of approximation theory and general spaces of functions having derivatives up to certain order (Sobolev-type spaces). In this framework, the problem of expressiveness is very well studied in the case of shallow networks with a single hidden layer, where it is known, in particular, that to approximate a C-function on a d-dimensional set with infinitesimal error one needs a network of size about \u2212d/n, assuming a smooth activation function (see, e.g., Mhaskar [1996], Pinkus [1999] for a number of related rigorous upper and lower bounds and further qualifications of this result). Much less seems to be known about deep networks in this setting, though Mhaskar et al. 2016, 2016 have recently introduced functional spaces constructed using deep dependency graphs and obtained expressiveness bounds for related deep networks. We will focus our attention on networks with the ReLU activation function \u03c3(x) = max(0, x), which, despite its utter simplicity, seems to be the most popular choice in practical applications LeCun et al. [2015]. We will consider L\u221e-error of approximation of functions belonging to the Sobolev spacesWn,\u221e([0, 1]) (without any assumptions of hierarchical structure).", "startOffset": 36, "endOffset": 1621}, {"referenceID": 0, "context": "[1998], Sakurai [1999] and the book Anthony and Bartlett [2009]. There is a related concept, the fat-shattering dimension, for real-valued approximation problems (Kearns and Schapire [1990], Anthony and Bartlett [2009]). A very general approach to expressiveness in the context of approximation is the method of nonlinear widths by DeVore et al. 1989 that concerns approximation of a family of functions under assumption of a continuous dependence of the model on the approximated function. In this paper we examine the problem of shallow-vs-deep expressiveness from the perspective of approximation theory and general spaces of functions having derivatives up to certain order (Sobolev-type spaces). In this framework, the problem of expressiveness is very well studied in the case of shallow networks with a single hidden layer, where it is known, in particular, that to approximate a C-function on a d-dimensional set with infinitesimal error one needs a network of size about \u2212d/n, assuming a smooth activation function (see, e.g., Mhaskar [1996], Pinkus [1999] for a number of related rigorous upper and lower bounds and further qualifications of this result). Much less seems to be known about deep networks in this setting, though Mhaskar et al. 2016, 2016 have recently introduced functional spaces constructed using deep dependency graphs and obtained expressiveness bounds for related deep networks. We will focus our attention on networks with the ReLU activation function \u03c3(x) = max(0, x), which, despite its utter simplicity, seems to be the most popular choice in practical applications LeCun et al. [2015]. We will consider L\u221e-error of approximation of functions belonging to the Sobolev spacesWn,\u221e([0, 1]) (without any assumptions of hierarchical structure). We will often consider families of approximations, as the approximated function runs over the unit ball Fd,n in Wn,\u221e([0, 1]). In such cases we will distinguish scenarios of fixed and adaptive network architectures. Our goal is to obtain lower and upper bounds on the expressiveness of deep and shallow networks in different scenarios. We measure complexity of networks in a conventional way, by counting the number of their weights and computation units (cf. Anthony and Bartlett [2009]).", "startOffset": 36, "endOffset": 2262}, {"referenceID": 11, "context": "The arXiv preprint of the first version of the present work appeared almost simultaneously with the work of Liang and Srikant Liang and Srikant [2016] containing results partly overlapping with our results in Subsections 3.", "startOffset": 108, "endOffset": 151}, {"referenceID": 0, "context": "The depth of the network, the number of units and the total number of weights are standard measures of network complexity (Anthony and Bartlett [2009]).", "startOffset": 123, "endOffset": 151}, {"referenceID": 22, "context": "Our construction uses the \u201csawtooth\u201d function that has previously appeared in the paper Telgarsky [2015]. Proposition 2.", "startOffset": 88, "endOffset": 105}, {"referenceID": 21, "context": "This construction is inspired by a similar argument used to prove the O(2/n) upper bound for the complexity of Boolean circuits implementing n-ary functions Shannon [1949]. The proof becomes simpler if, in addition to the ReLU function \u03c3, we are allowed to use the activation function", "startOffset": 157, "endOffset": 172}, {"referenceID": 6, "context": "The method of continuous nonlinear widths (DeVore et al. [1989]) is a very general approach to the analysis of parameterized nonlinear approximations, based on the assumption of continuous selection of their parameters.", "startOffset": 43, "endOffset": 64}, {"referenceID": 6, "context": "Theorem 3 (DeVore et al. [1989], Theorem 4.", "startOffset": 11, "endOffset": 32}, {"referenceID": 6, "context": "Theorem 3 (DeVore et al. [1989], Theorem 4.2). Fix d, n. Let W be a positive integer and \u03b7 : R \u2192 C([0, 1]) be any mapping between the space R and the space C([0, 1]). Suppose that there is a continuous map M : Fd,n \u2192 R such that \u2016f \u2212 \u03b7(M(f))\u2016\u221e \u2264 for all f \u2208 Fd,n. Then W \u2265 c \u2212d/n, with some constant c depending only on n. We apply this theorem by taking \u03b7 to be some ReLU network architecture, and R the corresponding weight space. It follows that if a ReLU network architecture is capable of expressing any function from Fd,n with error , then, under the hypothesis of continuous weight selection, the network must have at least c \u2212d/n weights. The number of connections is then lower bounded by c 2 \u2212d/n (since the number of weights is not larger than the sum of the number of computation units and the number of connections, and there are at least as many connections as units). The hypothesis of continuous weight selection is crucial in Theorem 3. By examining our proof of the counterpart upper bound O( \u2212d/n ln(1/ )) in Theorem 1, the weights are selected there in a continuous manner, so this upper bound asymptotically lies above c \u2212d/n in agreement with Theorem 3. We remark, however, that the optimal choice of the network weights (minimizing the error) is known to be discontinuous in general, even for shallow networks (Kainen et al. [1999]).", "startOffset": 11, "endOffset": 1355}, {"referenceID": 0, "context": "In this setup, some lower bounds on the network complexity can be obtained as a consequence of existing upper bounds on VC-dimension of networks with piece-wise polynomial activation functions and Boolean outputs (Anthony and Bartlett [2009]).", "startOffset": 214, "endOffset": 242}, {"referenceID": 0, "context": ", Anthony and Bartlett [2009], Section 3.", "startOffset": 2, "endOffset": 30}, {"referenceID": 0, "context": ", Anthony and Bartlett [2009], Section 3.3). We are interested in the case when H is the family of functions obtained by applying thresholds 1(x > a) to a ReLU network with fixed architecture but variable weights. In this case Theorem 8.7 of Anthony and Bartlett [2009] implies that VCdim(H) \u2264 c3W , (33)", "startOffset": 2, "endOffset": 270}, {"referenceID": 22, "context": "1 of Telgarsky [2015]. Precisely, Telgarsky\u2019s lemma states that if a network has a single input, connections only between neighboring layers, at most m units in a layer, and a piece-wise linear activation function with t pieces, then the number of linear pieces in the output of the network is not greater than (tm).", "startOffset": 5, "endOffset": 22}, {"referenceID": 11, "context": "Liang and Srikant describe in Liang and Srikant [2016] some conditions on the approximated function (resembling conditions of local analyticity) under which complexity of deep -approximation is O(ln(1/ )) with a constant power c.", "startOffset": 0, "endOffset": 55}, {"referenceID": 20, "context": "This bound is achieved by finite-depth (depth-6) ReLU networks using the idea of reused subnetworks familiar from the theory of Boolean circuits Shannon [1949]. In the case of fixed architecture, we have not established any evidence of complexity improvement for unconstrained weight selection compared to continuous weight selection.", "startOffset": 145, "endOffset": 160}, {"referenceID": 8, "context": "We remark however that, already for approximations with depth-3 networks, the optimal weights are known to discontinuously depend, in general, on the approximated function (Kainen et al. [1999]).", "startOffset": 173, "endOffset": 194}, {"referenceID": 12, "context": "Upper and lower approximation complexity bounds for these networks (Mhaskar [1996], Maiorov and Meir [2000]) show that complexity scales as \u223c \u2212d/n up to some ln(1/ ) factors.", "startOffset": 68, "endOffset": 83}, {"referenceID": 12, "context": "Upper and lower approximation complexity bounds for these networks (Mhaskar [1996], Maiorov and Meir [2000]) show that complexity scales as \u223c \u2212d/n up to some ln(1/ ) factors.", "startOffset": 84, "endOffset": 108}, {"referenceID": 12, "context": "this gap, in particular, graph-based hierarchical approximations are studied in Mhaskar et al. [2016], Mhaskar and Poggio [2016] and convolutional arithmetic circuits in Cohen et al.", "startOffset": 80, "endOffset": 102}, {"referenceID": 12, "context": "this gap, in particular, graph-based hierarchical approximations are studied in Mhaskar et al. [2016], Mhaskar and Poggio [2016] and convolutional arithmetic circuits in Cohen et al.", "startOffset": 80, "endOffset": 129}, {"referenceID": 3, "context": "[2016], Mhaskar and Poggio [2016] and convolutional arithmetic circuits in Cohen et al. [2015]. Theoretical analysis of expressiveness of deep networks taking into account such properties of real data seems to be an important and promising direction of future research.", "startOffset": 75, "endOffset": 95}], "year": 2017, "abstractText": "We study expressive power of shallow and deep neural networks with piece-wise linear activation functions. We establish new rigorous upper and lower bounds for the network complexity in the setting of approximations in Sobolev spaces. In particular, we prove that deep ReLU networks more efficiently approximate smooth functions than shallow networks. In the case of approximations of 1D Lipschitz functions we describe adaptive depth-6 network architectures more efficient than the standard shallow architecture.", "creator": "LaTeX with hyperref package"}}}