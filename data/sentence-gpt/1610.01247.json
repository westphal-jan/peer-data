{"id": "1610.01247", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2016", "title": "ECAT: Event Capture Annotation Tool", "abstract": "This paper introduces the Event Capture Annotation Tool (ECAT), a user-friendly, open-source interface tool for annotating events and their participants in video, capable of extracting the 3D positions and orientations of objects in video captured by Microsoft's Kinect(R) hardware. The modeling language VoxML (Pustejovsky and Krishnaswamy, 2016) underlies ECAT's object, program, and attribute representations, although ECAT uses its own spec for explicit labeling of motion instances. The demonstration will show the tool's workflow and the options available for capturing event-participant relations and browsing visual data. Mapping ECAT's output to VoxML will also be addressed.\n\n\n\nIntroduction\n\nEvent capture refers to an event captured by the computer (or the user). The event event capture provides the control of a set of actions from the user to the participant's input. In some scenarios, an event could be represented as an event object by having the event capture the event object. In most cases, the event object can be represented as an event object by being defined as an event object by being defined as a event object by a specific method. The user can be represented as an event object by using the following syntax:\n\nIn all cases, the user has to specify whether the event object is representing the event object or the object's input. In this case, the user has to specify what type of event the event object is representing.\nAn event object can be represented as a new object or a new event object or an event object by having the event capture the event object. In other cases, the user has to specify what type of event the event object is representing. In this case, the user has to specify what type of event the event object is representing. In this case, the user has to specify whether the event object is representing the event object or the object's input.\nIn some scenarios, the user has to specify what type of event the event object is representing. In this case, the user has to specify what type of event the event object is representing. In this case, the user has to specify whether the event object is representing the event object or the object's input. In other cases, the user has to specify whether the event object is representing the event object or the object's input. In other cases, the user has to specify whether the event object is representing the event object or the object's input. In some situations, the user has to specify which type", "histories": [["v1", "Wed, 5 Oct 2016 01:24:42 GMT  (2904kb,D)", "http://arxiv.org/abs/1610.01247v1", "4 pages, 4 figures, ISA workshop 2015"]], "COMMENTS": "4 pages, 4 figures, ISA workshop 2015", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["tuan do", "nikhil krishnaswamy", "james pustejovsky"], "accepted": false, "id": "1610.01247"}, "pdf": {"name": "1610.01247.pdf", "metadata": {"source": "CRF", "title": "ECAT: Event Capture Annotation Tool", "authors": ["Tuan Do", "Nikhil Krishnaswamy", "James Pustejovsky"], "emails": ["tuandn@brandeis.edu", "nkrishna@brandeis.edu", "jamesp@brandeis.edu"], "sections": [{"heading": null, "text": "Keywords: event capture, event annotation, motion capture"}, {"heading": "1. Introduction", "text": "Much existing work in video annotation has focused on capturing objects from video in a purely two-dimensional format (i.e. tracking pixels) as in (Goldman et al., 2008), among others, or in capturing human body positioning in 3D for pose and gesture recognition (Kipp et al., 2014). We seek to wed these two types of capabilities by extracting the positions and orientations of objects and human body-rigs in video captured by the Microsoft Kinectr. These objects can be annotated as participants in a recorded motion event and this labeled data can then be used to build a corpus of multimodal semantic simulations of these events that can model object-object, object-agent, and agent-agent interactions through the durations of said events. This library of simulated motion events can serve as a novel resource of direct linkages from natural language to event visualization. We rely on the Kinect\u2019s capacity for body recognition and object tracking to produce output in the form of annotated object movement over time, allowing us to create an abstract representation of the denoted event.\nThe Kinect\u2019s depth field stream facilitates improved tracking of human movement, as reflected in the Kinect SDK\u2019s skeleton and face tracking performance (Livingston et al., 2012). The depth field provides a way to apply two-dimensional object tracking methods to a threedimensional environment, which allows us to annotate captured video with a labeled event and its participants with their 3D positions throughout the event\u2019s duration. We can directly map from ECAT\u2019s output into VoxML, which was created specifically for modeling visualizations of objects and events. This mapping allows us to recreate the captured event instance in a simulated environment, and to begin compiling a library of labeled events and their participant objects simulated in 3D space, allowing in turn for the possibility of learning automatic discrimination of events from the motions of their participants.\nECAT is released as open source and it is available at https://github.com/tuandnvn/ecat."}, {"heading": "2. Functionality", "text": "We use Kinect Sensor v2 for Windows which supports resolutions of up to HD 1920px \u00d7 1080px (RGB video) and 512px \u00d7 424px \u00d7 8 meters (depth). The latest SDK also supports 25 joint points of body tracking, and face tracking."}, {"heading": "2.1. Capture and Input", "text": "For ECAT, we created our own capture and compression functionality rather than use the Kinect SDK\u2019s default functionality due to the large size of the resultant raw data files. Kinect capture automatically recognizes human bodies. Other objects may be manually marked by annotators. Once a video is captured and loaded, annotators may play it back and edit it. This may include removing an incorrectly recognized human body rig from the scene or cropping the video clip. The video clip may include frames beyond the interval of the captured event. The default RGB color image and depth stream data are saved as separate video files. Body-tracking data is saved along with a scheme file specifying the name and index of every recognized joint in the body rig, how they are connected1 and how they can be projected onto the RGB video. Additionally, users can import a property scheme file specifying what properties each object type can support, allowing them to modify the set of annotatable fields."}, {"heading": "2.2. User Interface", "text": "Figure 1 shows the ECAT GUI. The various components are enumerated below.\n1. Project management panel. Each project can hold multiple captured sessions.\n2. Video display. For displaying either the color video or grayscale depth field video, and locating objects of interest in the scene\u2014e.g., the table outlined in green in Figure 1.\n3. Object annotation controller. Yellow time scrub bars show when each tracked object appears in the video.\n1A human body rig is always a directed rooted tree whose nodes and edges form roughly the shape of a human stick figure.\nar X\niv :1\n61 0.\n01 24\n7v 1\n[ cs\n.C L\n] 5\nO ct\n2 01\n6\nBlack ticks mark frames where an annotator has drawn a bounding polygon around the object using the object toolbox (item 5). Link to links the selected object to another using a specified spatial configuration. Generate3D generates the selected object\u2019s tracking data using the depth field.\n4. Event annotation controller. Time scrub bars here show the duration of a marked event. Users provide a text description for the event, or use Link to to link the selected event to another captured event as a subevent. ECAT supports marking events that comprise multiple non-contiguous segments. Due to space constraints not all annotated subevents are visible in this screenshot.\n5. Object toolbox. Annotators can manually mark an invideo object with a bounding rectangle or arbitrary polygon. Marked bounds can be moved across frames as the object moves.\n6. Object property panel. Data about a selected object shows here, such as ID and name.\n7. Event property panel. The selected event\u2019s properties, including type and participants, show here, and the event can be linked to a VoxML event type.\nUsers can easily specify objects of interest in the scene, generate 3D tracking data, add or change object properties, and link them to VoxML objects. Events can be annotated with both natural language and a parametrized semantic markup, and linked to VoxML semantic programs."}, {"heading": "2.3. Object Annotation", "text": "ECAT supports two ways of marking objects in a video. One is to import objects that have been automatically tracked using other libraries, such as human body rigs recognized by Kinect SDK. The other is to annotate locations of objects on the RGB video stream. Annotators mark the locations of objects at the beginning and end of an interval, and ECAT provides semi-automatic tracking using the depth field data and the iterative closest point method (Besl and McKay, 1992) to track the object\u2019s three-dimensional location. The output of the tracking algorithm can be either a point cloud or a parametric format if the object\u2019s shape can be approximated as a simple geometry (e.g., an orange or apple could be modeled as a spheroid, the tracking output being just the position of the object\u2019s center, and a radius).\nAn object\u2019s objectType field can be set to either 2D or 3D. Objects must be given an ID, Name and semanticType. We address usage of semanticType in Section 3..\nAnnotators may also mark relations between objects. For example, in Fig. 1, two blocks are on top of the table. Users can link a block object and the table object and specify the relation between the objects as \u201con,\u201d resulting in a predicate On(Block 1, Table) that is interpretable as a VoxML RELATION entity. Annotators could modify the available set or specify a different set of available relation predicates by importing a predicate scheme file. By default, ECAT supports the following binary predicates: On, In, Attach to, Part of ."}, {"heading": "2.4. Event Annotation", "text": "In principle, there are at least two ways to annotate an event associated with a video or video subinterval: (a) IDing an event type from an existing ontology or semantic resource, such as FrameNet (Baker et al., 1998); or (b) describing the event in natural language. We currently use the latter approach for filling an event\u2019s text field, but we are working toward incorporating ontologies with the event tag information, addressed in section 4.. As mentioned in section 2.2., ECAT allows annotation of event-subevent relations. Thus an overarching event may be annotated as put, but it contains the subevents grasp, hold, move, and ungrasp, which may overlap with some subsection of the main event and each other."}, {"heading": "3. Links to VoxML", "text": "Entities modeled in VoxML can be objects, programs, attributes, relations, or functions. The VoxML OBJECT is used for modeling nouns, while PROGRAM is used for modeling events. The semanticType field of an object captured in ECAT, filled in with free NL input, can be linked to objects annotated in VoxML if objects with the specified label exist in the VoxML-based lexicon (the voxicon). An object of semanticType=block can be linked in a 3D scene to a VoxML object denoted by the lexeme block, linking the captured object to all the ontological and semantic data provided by the VoxML markup (e.g. an object marked with semanticType=stack will be assigned, in the ECAT-to-VoxML mapping, all the VoxML knowledge of what a \u201cstack\u201d is). Objects whose objectType=3D can then be placed or moved within such a scene according to the Location and Rotation tags from the video annotation. Thus ECAT annotation can be used to recreate an equivalent scene in a VoxML-based 3D environment. The semanticType field of an annotated event can be attached to the motion of the objects in the scene that correspond to the event\u2019s participants. Thus, using the scene above as an example, the interaction of the body rig object and the block objects can explicitly be marked as a put event, and the same object/agent motions can be recreated in a 3D scene, allowing for the creation of a linked dataset of annotated videos and procedurally generated scenes. This dataset could then be used to train machine-learning algorithms to discriminate motion events based on the motions of an event\u2019s respective participants in 3D space."}, {"heading": "4. Output", "text": "Body rigs are saved as objects with semanticType=body rig. They are ID\u2019ed (id=o1 as seen in Fig. 2) and can be given an alias for the user\u2019s ease (here John). Annotated objects are treated similarly, assigned an ID, a name, and a semantic type. Here o2 is the Shell logo block from Fig. 1. Object locations and relative spatial relations can be annotated by frame. At frame 1, o2 is on the table (o3) while by frame 50, it has been put on the other block (o4), so the corresponding LinkTo tags are On(o2, o3) and On(o2, o4), respectively. By default, ECAT supports the relations On, In, Part of , and Attach To, where an\nobject is in a parent-child relationship with another object, such as when a body rig\u2019s hand is carrying a block. annotations denote events, with participants as referents (refs). In Fig. 3, o1, o2, and o4 are refs, while a1\u2019s event\u2019s semanticType=put, marking the three above objects as the \u201cput\u201d event\u2019s participants. An annotation\u2019s superEvent indicates super/subevent relationships, so that a2, a \u201cgrasp\u201d event, is notated as a subevent of \u201cput\u201d a1. Both objects and annotations can be mapped to VoxML representations, for instance as in Fig. 4 below, which shows a VoxML representation of put, an event annotated in Fig. 3."}, {"heading": "5. Conclusions", "text": "Event and action detection and recognition in video is receiving increasing attention in the scientific community, due to its relevance to a wide variety of applications (Ballan et al., 2011) and there have been calls for annotation infrastructure that includes video (Ide, 2013). We have presented here a tool that provides a user-friendly interface for video annotation that is able to capture a level of detail not provided by most existing video annotation tools, provides links to existing linguistic infrastructures, and is well suited for building a corpus of event-annotated multimodal simulations for use in the study of spatial and motion semantics (Pustejovsky and Moszkowicz, 2011; Pustejovsky, 2013). For future annotation capabilities, we are planning on introducing links to existing semantic lexical resources, such as FrameNet, as well as event ontologies. More significantly, we are extending the ECAT environment to allow for annotation of much longer videos, encompassing multiple event sequences comprising narratives, including simultaneous or overlapping events that do not hold super/subevent relations between them but together make up a larger story (e.g. a man cooking dinner while a woman sets the table). This will entail enriching our specification to enable the markup of discourse connectives, linking the events in the narrative."}, {"heading": "Acknowledgements", "text": "This work is supported by DARPA Contract W911NF-15C-0238 with the US Defense Advanced Research Projects Agency (DARPA) and the Army Research Office (ARO). Approved for Public Release, Distribution Unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. All errors and mistakes are, of course, the responsibilities of the authors."}, {"heading": "6. Bibliographical References", "text": "Baker, C. F., Fillmore, C. J., and Lowe, J. B. (1998). The\nberkeley framenet project. In Proceedings of the 17th international conference on Computational linguisticsVolume 1, pages 86\u201390. Association for Computational Linguistics.\nBallan, L., Bertini, M., Del Bimbo, A., Seidenari, L., and Serra, G. (2011). Event detection and recognition for semantic annotation of video. Multimedia Tools and Applications, 51(1):279\u2013302.\nBesl, P. J. and McKay, N. D. (1992). Method for registration of 3-d shapes. In Robotics-DL tentative, pages 586\u2013 606. International Society for Optics and Photonics.\nGoldman, D. B., Gonterman, C., Curless, B., Salesin, D., and Seitz, S. M. (2008). Video object annotation, navigation, and composition. In Proceedings of the 21st annual ACM symposium on User interface software and technology, pages 3\u201312. ACM.\nIde, N. (2013). An open linguistic infrastructure for annotated corpora. In The People\u0131\u0308\u00bf 12 s Web Meets NLP, pages 265\u2013285. Springer.\nKipp, M., von Hollen, L. F., Hrstka, M. C., and Zamponi, F. (2014). Single-person and multi-party 3d visualizations for nonverbal communication analysis. In Proceedings\nof the Ninth International Conference on Language Resources and Evaluation (LREC), ELDA, Paris. Livingston, M. A., Sebastian, J., Ai, Z., and Decker, J. W. (2012). Performance measurements for the microsoft kinect skeleton. In Virtual Reality Short Papers and Posters (VRW), 2012 IEEE, pages 119\u2013120. IEEE. Pustejovsky, J. and Krishnaswamy, N. (2016). Voxml: A visual object modeling language. Proceedings of LREC. Pustejovsky, J. and Moszkowicz, J. (2011). The qualitative spatial dynamics of motion. The Journal of Spatial Cognition and Computation. Pustejovsky, J. (2013). Dynamic event structure and habitat theory. In Proceedings of the 6th International Conference on Generative Approaches to the Lexicon (GL2013), pages 1\u201310. ACL."}], "references": [{"title": "The berkeley framenet project", "author": ["C.F. Baker", "C.J. Fillmore", "J.B. Lowe"], "venue": "In Proceedings of the 17th international conference on Computational linguisticsVolume", "citeRegEx": "Baker et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Event detection and recognition for semantic annotation of video", "author": ["L. Ballan", "M. Bertini", "A. Del Bimbo", "L. Seidenari", "G. Serra"], "venue": "Multimedia Tools and Applications,", "citeRegEx": "Ballan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ballan et al\\.", "year": 2011}, {"title": "Method for registration of 3-d shapes", "author": ["P.J. Besl", "N.D. McKay"], "venue": "In Robotics-DL tentative,", "citeRegEx": "Besl and McKay,? \\Q1992\\E", "shortCiteRegEx": "Besl and McKay", "year": 1992}, {"title": "Video object annotation, navigation, and composition", "author": ["D.B. Goldman", "C. Gonterman", "B. Curless", "D. Salesin", "S.M. Seitz"], "venue": "In Proceedings of the 21st annual ACM symposium on User interface software and technology,", "citeRegEx": "Goldman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Goldman et al\\.", "year": 2008}, {"title": "An open linguistic infrastructure for annotated corpora", "author": ["N. Ide"], "venue": null, "citeRegEx": "Ide,? \\Q2013\\E", "shortCiteRegEx": "Ide", "year": 2013}, {"title": "Single-person and multi-party 3d visualizations for nonverbal communication analysis", "author": ["M. Springer. Kipp", "L.F. von Hollen", "M.C. Hrstka", "F. Zamponi"], "venue": "Web Meets NLP,", "citeRegEx": "Kipp et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kipp et al\\.", "year": 2014}, {"title": "Performance measurements for the microsoft kinect skeleton", "author": ["M.A. Livingston", "J. Sebastian", "Z. Ai", "J.W. Decker"], "venue": "In Virtual Reality Short Papers and Posters (VRW),", "citeRegEx": "Livingston et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Livingston et al\\.", "year": 2012}, {"title": "Voxml: A visual object modeling language", "author": ["J. Pustejovsky", "N. Krishnaswamy"], "venue": "Proceedings of LREC", "citeRegEx": "Pustejovsky and Krishnaswamy,? \\Q2016\\E", "shortCiteRegEx": "Pustejovsky and Krishnaswamy", "year": 2016}, {"title": "The qualitative spatial dynamics of motion", "author": ["J. Pustejovsky", "J. Moszkowicz"], "venue": "The Journal of Spatial Cognition and Computation", "citeRegEx": "Pustejovsky and Moszkowicz,? \\Q2011\\E", "shortCiteRegEx": "Pustejovsky and Moszkowicz", "year": 2011}, {"title": "Dynamic event structure and habitat theory", "author": ["J. Pustejovsky"], "venue": "In Proceedings of the 6th International Conference on Generative Approaches to the Lexicon", "citeRegEx": "Pustejovsky,? \\Q2013\\E", "shortCiteRegEx": "Pustejovsky", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "The modeling language VoxML (Pustejovsky and Krishnaswamy, 2016) underlies ECAT\u2019s object, program, and attribute representations, although ECAT uses its own spec for explicit labeling of motion instances.", "startOffset": 28, "endOffset": 64}, {"referenceID": 3, "context": "tracking pixels) as in (Goldman et al., 2008), among others, or in capturing human body positioning in 3D for pose and gesture recognition (Kipp et al.", "startOffset": 23, "endOffset": 45}, {"referenceID": 5, "context": ", 2008), among others, or in capturing human body positioning in 3D for pose and gesture recognition (Kipp et al., 2014).", "startOffset": 101, "endOffset": 120}, {"referenceID": 6, "context": "The Kinect\u2019s depth field stream facilitates improved tracking of human movement, as reflected in the Kinect SDK\u2019s skeleton and face tracking performance (Livingston et al., 2012).", "startOffset": 153, "endOffset": 178}, {"referenceID": 2, "context": "Annotators mark the locations of objects at the beginning and end of an interval, and ECAT provides semi-automatic tracking using the depth field data and the iterative closest point method (Besl and McKay, 1992) to track the object\u2019s three-dimensional location.", "startOffset": 190, "endOffset": 212}, {"referenceID": 0, "context": "In principle, there are at least two ways to annotate an event associated with a video or video subinterval: (a) IDing an event type from an existing ontology or semantic resource, such as FrameNet (Baker et al., 1998); or (b) describing the event in natural language.", "startOffset": 198, "endOffset": 218}, {"referenceID": 1, "context": "Event and action detection and recognition in video is receiving increasing attention in the scientific community, due to its relevance to a wide variety of applications (Ballan et al., 2011) and there have been calls for annotation infrastructure that includes video (Ide, 2013).", "startOffset": 170, "endOffset": 191}, {"referenceID": 4, "context": ", 2011) and there have been calls for annotation infrastructure that includes video (Ide, 2013).", "startOffset": 84, "endOffset": 95}, {"referenceID": 8, "context": "We have presented here a tool that provides a user-friendly interface for video annotation that is able to capture a level of detail not provided by most existing video annotation tools, provides links to existing linguistic infrastructures, and is well suited for building a corpus of event-annotated multimodal simulations for use in the study of spatial and motion semantics (Pustejovsky and Moszkowicz, 2011; Pustejovsky, 2013).", "startOffset": 378, "endOffset": 431}, {"referenceID": 9, "context": "We have presented here a tool that provides a user-friendly interface for video annotation that is able to capture a level of detail not provided by most existing video annotation tools, provides links to existing linguistic infrastructures, and is well suited for building a corpus of event-annotated multimodal simulations for use in the study of spatial and motion semantics (Pustejovsky and Moszkowicz, 2011; Pustejovsky, 2013).", "startOffset": 378, "endOffset": 431}], "year": 2016, "abstractText": "This paper introduces the Event Capture Annotation Tool (ECAT), a user-friendly, open-source interface tool for annotating events and their participants in video, capable of extracting the 3D positions and orientations of objects in video captured by Microsoft\u2019s Kinectr hardware. The modeling language VoxML (Pustejovsky and Krishnaswamy, 2016) underlies ECAT\u2019s object, program, and attribute representations, although ECAT uses its own spec for explicit labeling of motion instances. The demonstration will show the tool\u2019s workflow and the options available for capturing event-participant relations and browsing visual data. Mapping ECAT\u2019s output to VoxML will also be addressed.", "creator": "LaTeX with hyperref package"}}}