{"id": "1610.01465", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2016", "title": "Visual Question Answering: Datasets, Algorithms, and Future Challenges", "abstract": "Visual Question Answering (VQA) is a recent problem in computer vision and natural language processing that has garnered a large amount of interest from the deep learning, computer vision, and natural language processing communities. In VQA, an algorithm needs to answer text-based questions about images. Since the release of the first VQA dataset in 2014, several additional datasets have been released and many algorithms have been proposed to improve the human interface of these algorithms.\n\n\n\nThe VQA dataset represents the first time a dataset has been created by a machine, which can look at the object from the left, and the subject line, and compare it to a previous picture. A VQA dataset is the result of a neural network algorithm, which has been applied successfully for this task.\nIn VQA, images were then automatically generated on each image, and the resulting image was then evaluated to see if they were correct. An algorithm then compares images by which the image has a known similarity, and the two or more image points match. In VQA, the images were then compared to images by which the two or more image points match. The algorithm then compares them to the images by which the two or more images match. In the data point, the image is then compared to the image by which the two or more images match. The algorithm then compares the images by which the two or more images match.\nThis approach also applies to image comparisons. Since the data point is a new image, the two or more images match as well. In VQA, the data point is the subject line, and the two or more images match as well.\nThe goal of the VQA dataset is to have an intuitive visual interface. In the VQA dataset, it will be useful to determine when images are correct, which are not. As it is important to know what data points are correct, it can be used to perform the tests in order to evaluate the accuracy and accuracy of an image.\nIn the VQA dataset, there are a couple of algorithms that perform the test. The first is a computer model, which takes a look at the object from the left and the subject line, and compares it to a previous picture. This algorithm also performs the data points that have been previously analyzed, so that the same object was considered correct. The second, the second is a system-independent model, which evaluates all possible representations of all possible representations of all possible representations of all possible representations of", "histories": [["v1", "Wed, 5 Oct 2016 14:58:36 GMT  (3353kb,D)", "http://arxiv.org/abs/1610.01465v1", null], ["v2", "Wed, 26 Oct 2016 01:39:40 GMT  (3353kb,D)", "http://arxiv.org/abs/1610.01465v2", null], ["v3", "Wed, 1 Mar 2017 05:39:21 GMT  (1766kb,D)", "http://arxiv.org/abs/1610.01465v3", null], ["v4", "Thu, 15 Jun 2017 01:52:59 GMT  (8046kb,D)", "http://arxiv.org/abs/1610.01465v4", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL", "authors": ["kushal kafle", "christopher kanan"], "accepted": false, "id": "1610.01465"}, "pdf": {"name": "1610.01465.pdf", "metadata": {"source": "CRF", "title": "Visual Question Answering: Datasets, Algorithms, and Future Challenges", "authors": ["Kushal Kafle", "Christopher Kanan"], "emails": [], "sections": [{"heading": null, "text": "Visual Question Answering (VQA) is a recent problem in computer vision and natural language processing that has garnered a large amount of interest from the deep learning, computer vision, and natural language processing communities. In VQA, an algorithm needs to answer text-based questions about images. Since the release of the first VQA dataset in 2014, several additional datasets have been released and many algorithms have been proposed. In this review, we critically examine the current state of VQA in terms of problem formulation, existing datasets, evaluation metrics, and algorithms. In particular, we discuss the limitations of current datasets with regard to their ability to properly train and assess VQA algorithms. We then exhaustively review existing algorithms for VQA. Finally, we discuss possible future directions for VQA and image understanding research."}, {"heading": "1 Introduction", "text": "Recent advancements in computer vision and deep learning research have enabled enormous progress in many computer vision tasks, such as image classification [1, 2], object detection [3, 4], and activity recognition [5, 6, 7]. Given enough data, deep convolutional neural networks (CNNs) rival the abilities of humans to do image classification [2]. With annotated datasets rapidly increasing in size thanks to crowd-sourcing, similar outcomes can be anticipated for other focused computer vision problems. However, these problems are narrow in scope and do not require holistic understanding of images. As humans, we can identify the objects in an image, understand the spatial positions of these objects, infer their attributes and relationships to each other, and also reason about the purpose of each object given the surrounding context. We can ask arbitrary questions about images and also communicate the information\ngleaned from them. Until recently, developing a computer vision system that can answer arbitrary natural language questions about images has been thought to be an ambitious, but intractable, goal. However, since 2014, there has been enormous progress in developing systems with these abilities. Visual Question Answering (VQA) is a computer vision task where a system is given a text-based question about an image, and it must infer the answer. Questions can be arbitrary and they encompass many sub-problems in computer vision, e.g.,\n\u2022 Object recognition - What is in the image? \u2022 Object detection - Are there any cats in the im-\nage? \u2022 Attribute classification - What color is the cat? \u2022 Scene classification - Is it sunny? \u2022 Counting - How many cats are in the image?\nBeyond these, there are many more complex questions that can be asked, such as questions about the spatial relationships among objects (What is between the cat and the sofa?) and common sense reasoning questions (Why is the the girl crying?). A robust VQA system must be capable of solving a wide range of classical computer vision tasks as well as needing the ability to reason about images.\nThere are many potential applications for VQA. The most immediate is as an aid to blind and visually impaired individuals, enabling them to get information about images both on the web and in the real world. More generally, VQA could be used to improve human-computer interaction as a natural way to query visual content. A VQA system can also be used for image retrieval, without using image metadata or tags. For example, to find all images taken in a rainy setting, we can simply ask \u2018Is it raining?\u2019 to all images in the dataset. Beyond applications, VQA is an important basic research problem. Because a good VQA system must be able to solve many computer vi-\nPage 1\nar X\niv :1\n61 0.\n01 46\n5v 1\n[ cs\n.C V\n] 5\nO ct\n2 01\n6\nsion problems, it can be considered a component of a Turing Test for image understanding [8, 9].\nA Visual Turing Test rigorously evaluates a computer vision system to assess whether it is capable of human-level semantic analysis of images [8, 9]. Passing this test requires a system to be capable of many different visual tasks. VQA can be considered a kind of Visual Turing Test that also requires the ability to understand questions, but not necessarily more sophisticated natural language processing. If an algorithm performs as well as or better than humans on arbitrary questions about images, then arguably much of computer vision would be solved. But, this is only true if the benchmarks and evaluation tools are sufficient to make such bold claims.\nIn this review, we discuss existing datasets and methods for VQA. We place particular emphasis on exploring the suitability of current benchmarks as being sufficient for evaluating whether a system is capable of robust image understanding. In Section 2, we compare VQA with other computer vision tasks, some of which also require the integration of vision and language (e.g., image captioning). Then, in Section 3, we explore currently available datasets for VQA with an emphasis on their strengths and weaknesses. We discuss how biases in some of these datasets severely limit their ability to test algorithms. In Section 4, we discuss the evaluation metrics used for VQA. Then, we review existing algorithms for VQA and analyze their efficacy in Section 5. Finally, we discuss future developments in VQA and open questions."}, {"heading": "2 Vision and Language Tasks", "text": "Related to VQA\nMuch of computer vision involves extracting semantic information from images. Object recognition, activity recognition, and scene classification can all be posed as image classification tasks, with today\u2019s best methods doing this using CNNs trained to classify images into particular semantic categories. The most successful of these is object recognition, where algorithms now rival humans in accuracy [2]. But, object recognition requires only classifying the dominant object in an image without knowledge of its spatial position or its role within the larger scene. Object detection involves the localization of specific semantic concepts (e.g., cars or people) by placing a bounding box around each instance of the object in an image. The best object detection methods all use deep CNNs [11, 4, 3]. Semantic segmentation takes the task of localization a step further\nby classifying each pixel as belonging to a particular semantic class [12, 13] . Instance segmentation further improves upon localization by differentiating between separate instances of the same semantic class [14, 15, 16].\nWhile semantic and instance segmentation are important computer vision problems that generalize object detection and recognition, they are not sufficient for holistic scene understanding. One of the major problems they face is label ambiguity. For example, in Figure 1, the assigned semantic label for the position of the yellow cross can be \u2018bag\u2019, \u2018black,\u2019 or \u2018person.\u2019 The label depends on the task. Moreover, these approaches alone have no understanding of the role of an object within a larger context. In this example, labeling a pixel as \u2018bag\u2019 does not inform us about whether it is being carried by the person, and labeling a pixel as \u2018person\u2019 does not tell us if the person person is sitting, running, or skateboarding. This is in contrast to VQA, where a system is required to answer arbitrary questions about the images, which may require reasoning about the relationships of objects with each other and the overall scene. The question specifies the visual task.\nBesides VQA, there is a significant amount of recent work that combines vision with language. One of the most studied is image captioning [17, 5, 18, 19, 20], in which an algorithm\u2019s goal is to produce a natural language description of a given image. Image captioning is a very broad task that potentially involves describing complex attributes and object relationships to provide a detailed description of an image. However, there are several problems with the visual captioning task. Multiple descriptions could be appropriate for an image, with some being very specific and others generic in nature (see Figure 1).\nEvaluation of captioning is also very challenging. Since human evaluation of the generated caption is not always feasible, several automatic evaluation schemes have been proposed. Most widely used evaluation schemes include BLEU [21], ROUGE [22], METEOR [23] and CIDEr [24]. With exception of CIDEr, which was developed specifically for scoring image descriptions, all caption evaluation metrics were originally developed for machine translation evaluation. Each of these metrics has limitations. BLEU, the most widely used metric, is known to have the same score for large variations in sentence structure with largely varying semantic content [25]. For captions generated in [26], BLEU scores ranked machine generated captions above human captions. However, when human judges were used to judge the same captions, only 23.3% of the judges ranked the captions to be of equal or better quality than hu-\nPage 2\nPage 3\nman captions. While other evaluation metrics, especially CIDEr and METEOR, show more robustness in terms of agreement with human judges, they still often rank automatically generated captions higher than human captions [27].\nA challenge for both automatic and human evaluation of captions is that there are often multiple captions that would be valid for an image, but some captions are much richer and specific. However, captioning systems that produce generic captions that only superficially describe an image\u2019s content are often ranked high by metrics. Generic captions such as \u2018A person is walking down a street\u2019 or \u2018Several cars are parked on the side of the road\u2019 can be applicable to a large number of images are often ranked highly by evaluation schemes and human judges. In fact, a simple system that returns the caption of the training image with the most similar visual features using nearest neighbor yields relatively high scores using automatic evaluation metrics [28].\nDense image captioning (DenseCap) avoids the generic caption problem by annotating an image densely with short visual descriptions pertaining to small, but salient, image regions [29]. For example, a DenseCap system may output \u2018a man wearing black shirt,\u2019 \u2019large green trees,\u2019 and \u2018roof of a building,\u2019 with each description accompanied by a bounding box. A system may generate a large number of these descriptions for rich scenes. Although many of these descriptions are short, it is still difficult to automatically assess their quality. DenseCap can also forego important relationships between the objects in the scene by only producing isolated descriptions for each regions. Captioning is also task agnostic and a system is not required to perform exhaustive image understanding.\nIn captioning, a system is at liberty to arbitrarily choose the level of granularity of its image analysis. This is in contrast to VQA, where the level of granularity is specified by the nature of the question asked. For example, \u2018What season is this?\u2019 will require understanding the entire scene, but \u2018What is the color of dog standing behind the girl with white dress?\u2019 would require attention to specific details of the scene. Moreover, many kinds of questions have specific and unambiguous answers, making VQA far more amenable to automated evaluation metric than captioning. Ambiguity may still exist for some question types (see Section 4), but for many questions the answer produced by a VQA algorithm can be evaluated with one-to-one matching with the ground truth answer."}, {"heading": "3 Datasets for VQA", "text": "Beginning in 2014, five major datasets for VQA have been publicly released. These datasets enable VQA systems to be trained and evaluated. The ideal VQA dataset needs to be sufficiently large to capture the variability within questions, images, and concepts that occur in real world scenarios. It should also have a fair evaluation scheme that is difficult to \u2018game\u2019 and doing well on it indicates that an algorithm can answer questions about images that have definitive answers. If a dataset contains easily exploitable biases in the distribution of the questions or answers, it may be possible for an algorithm to perform well on the dataset without really solving the VQA problem.\nAs of this article, the main datasets for VQA are DAQUAR [30], COCO-QA [31], The VQA Dataset [32], FM-IQA [33], Visual7W [34], and Visual Genome [35]. With exception of DAQUAR, all of the datasets include images from the Microsoft Common Objects in Context (COCO) dataset [10], which consists of 328,000 images, 91 common object categories with over 2 million labeled instances, and an average of 5 captions per image. Visual genome and Visual7W use images from Flickr100M in addition to the COCO images. A portion of The VQA Dataset contains synthetic cartoon imagery. We refer to the portion containing images from COCO as COCO-VQA. Table 1 contains statistics for each of these datasets.\nMost of the datasets have open-ended answers, but The VQA dataset and Visual7W also are evaluated using an explicit multiple choice paradigm. The VQA Dataset offers both multiple-choice (MC) and open-ended (OE) versions. The authors of Visual7W strongly encourage using multiple-choice evaluation for the dataset.\nIn the following subsections, we critically review the available datasets. We describe how the datasets were created and discuss their limitations."}, {"heading": "3.1 DAQUAR", "text": "The DAtaset for QUestion Answering on Real-world images (DAQUAR) [30] was the first major VQA dataset to be released. It is one of the smallest VQA datasets. It consists of 6795 training and 5673 testing QA pairs based on images from the NYUDepthV2 Dataset [36]. The dataset is also available in an even smaller configuration consisting of only 37 object categories, known as DAQUAR-37. DAQUAR-37 consists of only 3825 training QA pairs and 297 testing QA pairs. In [37], additional ground truth answers were collected for DAQUAR to cre-\nPage 4\nT ab\nle 1:\nS ta\nti st\nic s\nfo r\nV Q\nA d\na ta\nse ts\n.\nD A\nQ U\nA R\n[3 0]\nC O\nC O\n-Q A\n[3 1]\nC O\nC O\n-V Q\nA [3\n2 ]\nF M\n-I Q\nA [3\n3 ]\n1 V\nis u\na l7\nW [3\n4 ]\nV is\nu a l\ng en\no m\ne [3\n5 ]\nT ot\nal Im\nag es\n14 49\n12 32\n87 20\n4 7 2 1\n1 2 0 3 6 0\n4 7 3 0 0\n1 0 8 0 0 0\nQ A\nP ai\nrs 12\n46 8\n11 76\n84 61\n41 6 3\n2 5 0 5 6 9\n3 2 7 9 3 9\n1 ,7\n7 3 ,2\n5 8\nD is\nti n\nct A\nn sw\ner s\n96 8\n43 0\n10 5 9 6 9\nN / A\n6 5 1 6 1\n2 0 7 6 7 5\n% co\nve re\nd b y\nto p\n-1 00\n0 10\n0% 10\n0% 82\n.8 %\nN / A\n5 6 .2\n9 %\n6 0 .8 % % co ve re d b y to p -1 0 25 .0 4% 19 .7 1% 51 .1 3 % N / A 1 7 .1 3 % 1 3 .0\n7 %\nH u\nm an\nA cc\nu ra\ncy 50\n.2 N\n/A 83\n.3 N\n/ A\n9 6 .6\nN / A\nL on\nge st\nQ u\nes ti\non 2 5\n24 32\nN / A\n2 4\nT B D L on ge st A n sw er 7 (l is t of 1 w or d s) 1 w or d 17 w o rd s N / A 2 0 w o rd s 2 4\nw o rd s A v ge A n sw er L en gt h 1. 2 w or d s 1. 0 w or d s 1. 1 w o rd s N / A 2 .0 w o rd s 1 .8 w o rd s Im ag e S ou rc e N Y U D v 2 C O C O C O C O + C G I C O C O C O C O C O C O , Y F C C A n n ot at io n M an u al + A u to A u to M an u a l M a n u a l M a n u a l M a n u a l Q u es ti on T y p es 3 4 - - - -\n1 W\ne w er e u n a b le\nto re tr ie v e th\ne E n g li sh\nv er si o n o f th\ne d a ta se t fr o m\np ro v id ed\nd o w n lo a d li n k .\nate an alternative evaluation metric. This variant of DAQUAR is called DAQUAR-consensus, named after the evaluation metric. While DAQUAR was a pioneering dataset for VQA, it is too small to successfully train and evaluate more complex models. Apart from the small size, DAQUAR contains exclusively indoor scenes, which constrains the variety of questions available. The images tend to have significant clutter and in some cases extreme lighting conditions (see Figure 2). This makes many questions difficult to answer, and even humans are only able to achieve 50.2% accuracy on the full dataset."}, {"heading": "3.2 COCO-QA", "text": "In COCO-QA, QA pairs are created for images using an Natural Language Processing (NLP) algorithm that derives them using the captions from the COCO images. For example, using the image caption A boy is playing Frisbee, it is possible to create the question What is the boy playing? with frisbee as the answer. COCO-QA contains 78,736 training and 38,948 testing QA pairs. Most questions ask about the object in the image (69.84%), with the other questions being about color (16.59%), counting (7.47%) and location (6.10%). All of the questions have a single word answer, and there are only 435 unique answers. These constraints on the answers makes evaluation relatively straightforward.\nThe biggest shortcoming of COCO-QA is due to flaws in the NLP algorithm that was used to generate the QA pairs. Longer sentences are broken into smaller chunks for ease of processing, but in many of these cases the algorithm does not cope well with the presence of clauses and grammatical variations in sentence formation. This results in awkwardly phrased questions, with many containing grammatical errors, and others being completely unintelligible (see Figure 2). The other major shortcoming is the limited variety in the kinds of questions asked."}, {"heading": "3.3 The VQA Dataset", "text": "The VQA Dataset [32] consists of both real images from COCO and abstract cartoon images. Most work on this dataset has focused solely on the portion containing real world imagery from COCO, which we refer to as COCO-VQA. We refer to the synthetic portion of the dataset as SYNTH-VQA.\nCOCO-VQA consists of three questions per image, with ten answers per question. Amazon Mechanical Turk (AMT) workers were employed to generate questions for each image by being asked to \u2018Stump a smart robot,\u2019 and a separate pool of workers were\nPage 5\nhired to generate the answers to the questions. Compared to other VQA datasets, COCO-VQA consists of a relatively large number of questions (614,163 total, with 248,349 for training, 121,512 for validation, and 244,302 for testing). Each of the questions is then answered by 10 independent annotators. The multiple answers per question are used in the consensus-based evaluation metric for the dataset, which is discussed in Section 4.\nSYNTH-VQA consists of 50,000 synthetic scenes that depict cartoon images in different simulated scenarios. Scenes are made from over 100 different objects, 30 different animal models, and 20 human cartoon models. The human models are the same as those used in [38], and they contain deformable limbs and eight different facial expressions. The models also span different age, gender, and races to provide variation in appearance. SYNTH-VQA has 150,000 QA pairs with 3 questions per scene and 10 ground truth answers per question. By using synthetic images, it becomes possible to create a more varied and balanced dataset. Natural images datasets tend to have more consistent context and biases, e.g., a street scene is more likely to have picture of a dog than a zebra. Using synthetic images, these biases can be reduced. Yin and Yang [39] is a dataset built on top of SYNTH-VQA that tried to eliminate biases in the answers people have to questions. We further discuss Yin and Yang in Section 6.1.\nBoth SYNTH-VQA and COCO-VQA come both open-ended and multiple-choice formats. The\nmultiple-choice format contains all the same QA pairs, but it also contains 18 different choices that are comprised of\n\u2022 The Correct Answer, which is the most frequent answer given by the ten annotators. \u2022 Plausible Answers, which are three answers collected from annotators without looking at the image. \u2022 Popular Answers, which are the top ten most popular answers in the dataset. \u2022 Random Answers, which are randomly selected correct answers for other questions.\nDue to diversity and size of the dataset, COCOVQA has been widely used to evaluate algorithms. However, there are problems with the dataset. While COCO-VQA has a large variety of questions, many of them can be accurately answered without using the image due to language biases. Relatively simple image-blind algorithms have achieved 49.6% accuracy on COCO-VQA using the question alone [40]. The dataset also contains many subjective, opinionseeking questions that do not have a single objective answer (see Figure 3). Similarly, many questions seek explanations or verbose descriptions. An example of this is given in Figure 3c, which also shows unreliability of human annotators as the most popular answer is \u2018yes\u2019 which is completely wrong for the given question. These complications are reflected by interhuman agreement on this dataset, which is about 83%. Several other practical issues also arise out of the dataset\u2019s biases. For example, \u2018yes/no\u2019 answers\nPage 6\nspan about 38% of all questions, and almost 59% of them are answered with \u2018yes.\u2019 Combined with the evaluation metric used with COCO-VQA (see Section 4), these biases can make it difficult to assess whether an algorithm is truly solving the VQA problem using solely this dataset. We discuss this further in Section 4."}, {"heading": "3.4 FM-IQA", "text": "The Freestyle Multilingual Image Question Answering (FM-IQA) dataset is another dataset based on COCO [33]. It contains human generated answers and questions. The dataset was originally collected in Chinese, but English translations have been made available. Unlike COCO-QA and DAQUAR, this dataset also allowed for answers to be full sentences. This makes automatic evaluation with common metrics intractable. For this reason, the authors suggested using human judges for evaluation, where the judges are tasked with deciding whether or not the answer is provided by a human or not as well as assessing the quality of an answer on a scale of 0\u20132. This approach is impractical for most research groups and makes developing algorithms difficult. This has led to limited use of FM-IQA, with the only exception being the original paper by the creators of the dataset. We further discuss the importance of automatic evaluation metrics in Section 4."}, {"heading": "3.5 Visual Genome", "text": "Visual Genome [35] consists of 108,249 images that occur in both YFCC100M [41] and COCO images. It contains 1.7 million QA pairs for images, with an average of 17 QA pairs per image. As of this article, Visual Genome is the largest VQA dataset. Because it\nwas only recently introduced, no methods have been evaluated on it beyond the baselines established by the authors.\nVisual Genome consists of six types of \u2018W\u2019 questions: What, Where, How, When, Who, and Why. Two distinct modes of data collection were used to make the dataset. In the free-form method, annotators were free to ask any question about an image. However, when asking free-form questions, human annotators tend to ask similar questions about an image\u2019s holistic content, e.g., asking \u2018How many horses are there?\u2019 or \u2018Is it sunny?\u2019 This can promote bias in the kinds of questions asked. The creators of Visual Genome combated this by also prompting workers to ask questions about specific image regions. When using this region-specific method, a worker might be prompted to ask a question about a region of an image containing a fire hydrant. Region-specific question prompting was made possible using Visual Genome\u2019s descriptive bounding-box annotations. An example of region bounding boxes and QA pairs from Visual Genome are shown in Figure 4a.\nVisual Genome has much greater answer diversity compared to other datasets, which is shown in Figure 5. The 1000 answers that occur most frequently in Visual Genome only cover 65% of all answers in the dataset, whereas they cover 82% for COCO-VQA and 100% for DAQUAR and COCOQA. Visual Genome\u2019s long-tailed distribution is also observed in the length of the answers. Only 57% of answers are single words, compared to 88% of answers in COCO-VQA, 100% of answers in COCO-QA, and 90% of answers in DAQUAR. This diversity in answers makes open-ended evaluation significantly more challenging. Moreover, since the categories themselves are required to strictly belong to one of the\nPage 7\nsix \u2018W\u2019 types, the diversity in answer may at times artificially stem simply from variations in phrasing which could be eliminated by prompting the annotators to choose more concise answers. For example, Where are the cars parked? can be answered with \u2018on the street\u2019 or more concisely with \u2018street.\u2019\nVisual Genome has no binary (yes/no) questions. The dataset creators argue that this will encourage using more complex questions. This is in contrast to The VQA Dataset, where \u2018yes\u2019 and \u2018no\u2019 are the more frequent answers in the dataset. We discuss this issue further in Section 6.4."}, {"heading": "3.6 Visual7W", "text": "The Visual7W dataset is a subset of Visual Genome. Visual7W contains 47,300 images from Visual Genome that are also present in COCO. Visual7W is named after the seven categories of questions it contains: What, Where, How, When, Who, Why, and Which. The dataset consists of two distinct types of questions. The \u2018telling\u2019 questions are identical to Visual Genome questions, and the answer is text-based. The \u2018pointing\u2019 questions are the ones that begin with \u2018Which,\u2019 and for these questions the algorithm has to select the correct bounding box among alternatives. An example pointing question is shown in Figure 4b.\nVisual7W uses a multiple-choice answer framework as the standard evaluation, with four possible answers being made available to an algorithm during evaluation. To make the task challenging, the multiplechoices consist of answers that are plausible for the given question. Plausible answers are collected by prompting annotators to answer the question without seeing the image. For pointing questions, the multiple-choice options are four plausible bounding boxes surrounding the likely answer. Like Visual Genome, the dataset does not contain any binary questions."}, {"heading": "3.7 SHAPES", "text": "While the other VQA datasets contain either real or synthetic scenes, the SHAPES dataset [42] consists of shapes of varying arrangements, types, and colors. Questions are about the attributes, relationships, and positions of the shapes. This approach enables the creation of a vast amount of data, free of many of the biases that plague other datasets to varying degrees.\nSHAPES consists of 244 unique questions, with every question asked about each of the 64 images in the dataset. Unlike other datasets, this means it is completely balanced and free of bias. All questions\nare binary, with yes/no answers. Many of the questions require positional reasoning about the layout and properties of the shapes. While, SHAPES cannot substitute for using scenes, the idea behind it is extremely valuable. An algorithm that cannot perform well on SHAPES, but performs well on other VQA datasets may indicate that it is only capable of analyzing images in only a limited manner."}, {"heading": "4 Evaluation Metrics for VQA", "text": "As mentioned earlier, VQA has been posed as either an open-ended task, in which an algorithm generates a string to answer a question, or as a multiple-choice question where it picks among choices. For multiplechoice, simple accuracy is often used to evaluate, with an algorithm getting an answer right if it makes the correct choice. For open-ended VQA, simple accuracy can also be used. In this case, an algorithm\u2019s predicted answer string must exactly match the ground truth answer. However, accuracy can be too stringent because some errors are much worse than others. For example, if the question was \u2018What animals are in the photo?\u2019 and a system outputs \u2018dog\u2019 instead of the correct label \u2018dogs,\u2019 it is penalized just as strongly as it would be if it output \u2018zebra.\u2019 Questions may also have multiple correct answers, e.g., \u2018What is in the\nPage 8\ntree?\u2019 might have \u2018bald eagle\u2019 listed as the correct ground truth answer, so a system that outputs \u2018eagle\u2019 or \u2018bird\u2019 would be penalized just as much as if it had output \u2018yes\u2019 as the answer. Due to these issues, several alternatives to exact accuracy have been proposed for evaluating open-ended VQA algorithms.\nWu-Palmer Similarity (WUPS) [43] was proposed as an alternative to accuracy in [30]. It tries to measure how much a predicted answer differs from the ground truth based on the difference in their semantic meaning. Given a ground truth answer and a predicted answer to a question, WUPS will assign a value between 0 and 1 based on their similarity to each other. It does this by finding the least common subsumer between two semantic senses and assigning scores based on how far back the semantic tree needs to be traversed to find the common subsumer. Using WUPS, semantically similar, but non-identical, words are penalized relatively less. Following our earlier example, \u2018bald eagle\u2019 and \u2018eagle\u2019 have similarity of 0.96, whereas \u2018bald eagle\u2019 and \u2018bird\u2019 have similarity of 0.88. However, WUPS tends to assign relatively high scores to even distant concepts, e.g., \u2018raven\u2019 and \u2018writing desk\u2019 have a WUPS score of 0.4. To remedy this, [30] proposed to threshold WUPS scores, where a score that is below a threshold will be scaled down by a factor. A threshold of 0.9 and scaling factor of 0.1 was suggested by [30]. This modified WUPS metric is the standard measure used for evaluating performance on DAQUAR and COCO-QA, in addition to simple accuracy.\nThere are two major shortcomings to WUPS that make it difficult to use. First, despite using a thresholded version of WUPS, certain pairs of words are lexically very similar but carry vastly different meaning. This is particularly problematic for questions about object attributes, such as color questions. For example, if the correct answer was \u2018white\u2019 and the predicted answer was \u2018black,\u2019 the answer would still receive a WUPS score of 0.91, which seems excessively high. Another major problem with WUPS is that it only works with rigid semantic concepts, which are almost always single words. WUPS cannot be used for phrasal or sentence answers that are occasionally found in The VQA Dataset and in much of Visual7W.\nAn alternative to relying on semantic similarity measures is to have multiple independently collected ground truth answers for each question, which was done for The VQA Dataset [32] and DAQUARconsensus [37]. For DAQUAR-consensus, an average of five human annotated ground truth answers per question were collected. The dataset\u2019s creators proposed two ways to use these answers, which they called average consensus and min consensus. For average consensus, the final score is weighted toward preferring the more popular answer provided by the annotators. For min consensus, the answer needs to agree with at least one annotator.\nFor The VQA Dataset, annotators generated ten answers per question. These are used with a variation of the accuracy metric, which is given by\nAccuracyV QA = min( n\n3 , 1), (1)\nPage 9\nwhere n is the total number of annotators that had the same answer as the algorithm. Using this metric, if the algorithm agrees with three or more annotators then it is awarded a full score for a question. Although this metric helps greatly with the ambiguity problem, substantial problems remain, especially with the COCO-VQA portion of the dataset, which we study further in the next few paragraphs2.\nUsing AccuracyV QA, the inter-human agreement on COCO-VQA is only 83.3%. It is impossible for an algorithm to achieve 100% accuracy. Inter-human agreement is especially poor for \u2018Why\u2019 questions, with over 59% of these questions having less than three annotators giving exactly the same answer. This makes it impossible to get a full score on these questions. Lack of inter-human agreement can also be seen in\n2Note that our analysis for COCO-VQA was only done on the train and validation portions of the dataset, because the test answers are not publicly available.\nsimpler, more straightforward questions (see Figure 7). In this example, if a system predicts any of the 10 answers, it will be awarded a score of at least 1/3. In several cases, the answers provided by annotators consist complete antonyms (e.g., left and right).\nIn many other cases, AccuracyV QA leads to multiple correct answers for a question that are in direct opposition to each other. For example, in COCOVQA more than 13% of the \u2018yes/no\u2019 answers have both \u2018yes\u2019 and \u2018no\u2019 repeated by more than three annotators. Either answering \u2018yes\u2019 or \u2018no\u2019 would receive the highest possible score. Even if eight annotators answered \u2018yes,\u2019 if two answered \u2018no\u2019 then an algorithm would still receive a score of 0.67 for the question. The weight of the majority does not play a role in evaluation.\nThese problems can result in scores being inflated. For example, answering \u2018yes\u2019 to all yes/no questions should ideally have a score of around 50% for those\nPage 10\nquestions. However, using AccuracyV QA, the score is 71%. This is partially due to the dataset being biased, with the majority answer for these questions being \u2018yes\u2019 58% of the time, but a score of 71% seems excessively inflated.\nEvaluating the open-ended responses of VQA systems is made simpler when the answers consist of one word answers. This occurs in 87% of COCO-VQA questions, 100% of COCO-QA questions, and 90% of DAQAUR questions. The possibility of multiple correct answers increases greatly when answers need to be multiple words. This occurs frequently in FMIQA, Visual7W, and Visual Genome, e.g., 27% of Visual7W answers have three or more words. In this scenario, metrics such as AccuracyV QA are unlikely to help score predicted answers to ground truth answers in open-ended VQA.\nThe creators of FM-IQA [33] suggested using human judges to assess multi-word answers, but this presents a number of problems. First, using human judges is an extremely demanding process in terms of time, resources, and expenses. It would make it difficult to iteratively improve a system by measuring how changing the algorithm altered performance. Second, human judges need to be given criteria for judging the quality of an answer. The creators of FM-IQA proposed two metrics for human judges. The first is to determine whether the answer was produced by a human or not, regardless of the answer\u2019s correctness.\nThis metric alone may be a poor indicator of a VQA system\u2019s abilities and could potentially be manipulated. The second metric is to rate an answer on a 3-point scale of totally wrong (0), partially correct (1), and perfectly correct (2).\nAn alternative to using judges for handling multiword answers is to use a multiple-choice paradigm, which is used by part of The VQA Dataset, Visual7W, and Visual Genome. Instead of generating an answer, a system only needs to predict which of the given choices is correct. This greatly simplifies evaluation, but we believe that unless it is used carefully, multiple-choice is ill-suited for VQA because it undermines the effort by allowing a system to peek at the correct answer. We discuss this issue in Section 6.5.\nThe best way to evaluate a VQA system is still an open question. Each of the methods proposed has significant limitations. The method to use depends on how the dataset was constructed, the level of bias within it, and available resources. Considerable work needs to be done to develop better tools for measuring the semantic similarity of answers and for handling multi-word answers."}, {"heading": "5 Algorithms for VQA", "text": "A large number of algorithms for VQA have been proposed. Most of them formulate VQA as a classification problem, treating each answer as a distinct category. To do this, an algorithm needs to extract image and question features. Most algorithms use CNNs that are pre-trained on ImageNet to extract image features, such as VGGNet [1], ResNet [2], and GoogLeNet [56]. There have been a wider range of methods used to extract text features, including bag-of-words (BOW) [32, 44], long short term memory (LSTM) encoders [32, 31, 37, 47, 33], and skipthought vectors [40, 45].\nIn this section, we briefly describe the algorithms used for VQA and organize them into common themes. Results on DAQUAR, COCO-QA, and COCO-VQA for many of the most prominent methods are given in Table 2."}, {"heading": "5.1 Baseline Models", "text": "The simplest baseline algorithms for VQA consist of using linear or multi-layer perceptron (MLP) classifiers applied to a vector of image and text features concatenated to each other [32, 40, 44]. In [44], the authors used a bag-of-words to represent the question and CNN features from GoogLeNet for the vi-\nPage 11\nsual features. They then fed these features into a multi-class logistic regression classifier. Their approach worked remarkably well, surpassing the previous baseline on COCO-VQA from [32], which used a theoretically more powerful model, an LSTM, to represent the question. Similarly, [40] used skip-thought vectors [57] to represent the questions and ResNet152 to represent image features. They found that an MLP model with two hidden layers trained on these features worked remarkably well for all datasets, despite using off-the-shelf features. However, they noted that a linear classifier outperformed the MLP model on the smaller dataset, likely due to the MLP model overfitting.\nSeveral VQA algorithms have used LSTMs to encode questions. In [32], an LSTM encoder acting on a one-hot encoding of the sentence was used to represent question features, and GoogLeNet was used for image features. The dimensionality of the CNN features was reduced to match the dimensionality of the LSTM encoding, and then the Hadamard product of these two vectors was used to fuse them together. The fused vector was used as input to an MLP with two hidden layers. In [37], an LSTM model was fed an embedding of each word sequentially with CNN features concatenated to it. This continued until the end of the question was reached. The subsequent timesteps were used to generate a list of answers. A related approach was used in [31], where an LSTM was fed CNN features during the first and last time-steps, with word features in between. The image features acted as the first and last words in the sentence. The LSTM network was followed by a softmax classifier to predict the answer. A similar approach was used in [33], but the CNN image features were only fed into the LSTM at the end of the question and instead of a classifier, another LSTM was used to generate the answer one word at a time."}, {"heading": "5.2 Bayesian and Question-Aware Models", "text": "VQA requires drawing inferences and modeling relationships between the question and image. Bayesian frameworks have been explored for modeling these relationships. In [30], the first Bayesian framework for VQA was proposed. The authors used semantic segmentation to identify the objects in an image and their positions. Then, a Bayesian algorithm was trained to model the spatial relationships of the objects, which was used to compute each answer\u2019s probability. This was the earliest known algorithm for VQA, but its efficacy is surpassed by simple baseline models. This is partially due to it being dependent\non the results of the semantic segmentation, which was imperfect.\nA very different Bayesian model was proposed in [40]. The model exploited the fact that the type of answer can be predicted using solely the question. For example, \u2018What color is the flower?\u2019 would be assigned as a color question by the model, essentially turning the open-ended problem into a multiplechoice one. To do this, the model used a variant of quadratic discriminant analysis, which modeled the probability of image features given the question features and the answer type. ResNet-152 was used for the image features, and skip-thought vectors were used to represent the question."}, {"heading": "5.3 Attention Based Models", "text": "Numerous VQA systems have incorporated spatial attention to specific CNN features, rather than using global features from the entire image. Some have also incorporated attention into the text representation. The basic idea is that certain visual regions and certain words in a question are more informative than others for answering a given question. For example, for a system answering \u2018What color is the umbrella?\u2019 the image region containing the umbrella is more informative than other image regions. Similarly, \u2018color\u2019 and \u2018umbrella\u2019 are the textual inputs that need to be addressed more directly than the others. Global image features, e.g., the last hidden layer of a CNN, and global text features, e.g., bag-of-words, may not be granular enough to address region specific questions. Similar attention models have shown great success in other vision and NLP tasks, such as captioning [20] and machine translation [58].\nWhile multiple papers have focused on using spatial visual attention for VQA [59, 47, 51, 46, 53, 50, 55], there are significant differences among these methods. To use spatial attentive mechanisms, an algorithm must represent the visual features in different locations, instead of solely at the global level, which is typically produced by the last hidden layer of a pre-trained CNN. The main two ways that have been explored are using region proposals (bounding boxes) and operating on the feature maps produced by the convolutional layers of a CNN.\nThe Focus Regions for VQA [59] and Focused Dynamic Attention (FDA) models [50] both used Edge Boxes [60] to generate bounding box region proposals for images. In [59], a CNN was used to extract features from each of these boxes. The input to their VQA system consisted of these CNN features, question features, and one of the multiple choice answers. Their system was trained to produced a score for each\nPage 13\nmultiple-choice answer, and the highest scoring answer was selected. The score is calculated using a weighted average of scores from each of the regions where the weights are simply learned by passing the dot product of regional CNN feature and question embedding to a fully connected layer.\nIn FDA [50], the authors proposed to only use the region proposals that have the objects mentioned in the question. Their VQA algorithm requires as input a list of bounding boxes with their corresponding object label. During training, the object labels and bounding boxes are obtained from COCO annotations. During test, the labels are obtained by classifying each bounding box using ResNet [2]. Subsequently, word2vec [61] was used to compute the similarity between words in the question and the object labels assigned to each of the bounding boxes. Any box with a score greater than 0.5 is successively fed into an LSTM network. At the last time-step, global CNN features from the entire image are also fed into the network, giving it access to both global and local features. A separate LSTM was also used as the question representation. The output from these two LSTMs are then fed into a fully connected layer that predicts the answer.\nIn contrast to using region proposals, the Stacked Attention Network (SAN) [47] and the Dynamic Memory Network (DMN) [51] models both used visual features from the spatial grid of a CNN\u2019s feature maps (see Figure 8). Both [47] and [51] used the last convolutional layer from VGG-19 with 448 \u00d7 448 images to produce a 14 \u00d7 14 filter response map with 512 dimensional features at each grid location.\nIn SAN [47], an attention layer is specified by a single layer of weights that uses the question and the CNN feature map with a softmax activation function to compute the attention distribution across image locations. This distribution is then applied to the CNN feature map to pool across spatial feature locations using a weighted sum, which generates a global image representation that emphasizes certain spatial regions more than others. This feature vector is then combined with a vector of question features to create a representation that can be used with a softmax layer to predict the answer. They generalized this approach to handle multiple (stacked) attention layers, enabling the system to model complex relationships among multiple objects in an image.\nA similar attentive mechanism was used in the Spatial Memory Network [46] model, where spatial\nPage 14\nattention is produced by estimating the correlation of image patches with individual words in the question. This word-guided attention is used to predict an attention distribution, which is then used to compute the weighted sum of the visual features embedding across image regions. Two different models were then explored. In the one-hop model, the features encoding the entire question are combined with the weighted visual features to predict the answer. In the two-hop model, the combination of the visual and question features is looped back into the attentive mechanism for refining the attention distribution.\nAnother approach that incorporated spatial attention using CNN feature maps is presented in [51]. To do this, they used a modified Dynamic Memory Network (DMN) [62]. A DMN consists of an input module, an episodic memory module, and an answering module. DMNs have been used for text based QA, where each word in a sentence is fed into a recurrent neural network and the output of the network is used to extract \u2018facts.\u2019 Then, the episodic memory module makes multiple passes over a subset of these facts. With each pass, the internal memory representation of the network is updated. An answering module uses the final state of the memory representation and the input question to predict an answer. To use a DMN for VQA, they used visual facts in addition to text. To generate visual facts, the CNN features at each spatial grid location are treated as words in a sentence that are sequentially fed into a recurrent neural network. The episodic memory module then makes passes through both text and visual facts to update its memory. The answering module remains unchanged.\nThe Hierarchical Co-Attention model [53] applies attention to both the image and question to jointly reason about the two different streams of information. The model\u2019s approach to visual attention is similar to the method used in Spatial Memory Network [46]. In addition to visual attention, this method uses a hierarchical encoding of the question, in which the encoding occurs at the word level (using a one-hot encoding), at the phrase level (using bi- or tri-gram window size), and at the question level (using the final time-step of an LSTM network). Using this hierarchical question representation, the authors proposed to use two different attentive mechanisms. The parallel co-attention approach simultaneously attended to both the question and image. The alternative coattention approach alternated between attending to the question or the image. This approach allowed the relevance of words in the question and the relevance of specific image regions to be determined by each other. The answer prediction is made by recursively\ncombining the co-attended features from all three levels of the question hierarchy."}, {"heading": "5.4 Other Noteworthy Models", "text": "In [55], Multimodal Compact Bilinear (MCB) pooling was proposed as a method for combining image and text features in VQA. This idea is to approximate the outer-product between the image and text features, allowing a deeper interaction between the two modalities, compared to other mechanisms, e.g., concatenation or element-wise multiplication. Rather than doing the outer-product explicitly, which would be very high dimensional, MCB does the outer-product in a lower dimensional space. This is then used to predict which spatial features are relevant to the question. In a variation of this model, a soft-attention mechanism, similar to the method in [47], was also used, with the only major change being the use of MCB for combining text and question features instead of element-wise multiplication in [47]. This combination yielded very good results on COCO-VQA, and it was the winner of the 2016 VQA Challenge workshop.\nIn [54], the authors proposed breaking the answering process into abstract sub-tasks by using several complete answering units in a recurrent fashion. Each answering unit on the chain is equipped with an attentive mechanism derived from [47] and a classifier. The authors claimed that the inclusion of multiple recurrent answering units allows inferring the answer from a series of sub-tasks solved by each answering unit. However, they did not perform visualization or ablation studies to show how the answer might get refined in each time-step. This makes it difficult to assess whether progressive refinement is occurring or not, especially since complete image and question information is available to all answering units.\nAnother notable model is provided by [45], which incorporates a Dynamic Parameter Prediction layer into the fully connected layers of a CNN. The parameters of this layer are predicted from the question by using a recurrent neural network. This allows the visual features that the model uses to be specific to the question before the final classification step. This approach can be seen as a kind of implicit attentive mechanism in that it modifies the visual input based on the question.\nIn [52], Multimodal Residual Networks (MRN) were proposed for VQA. Their system is a modification of ResNet [2] to use both visual and question features in the residual mapping. The visual and question embedding are allowed to have their own residual blocks with skip connections. However, after each residual block the visual data is inter-weaved with the\nPage 15\nquestion embedding. The authors explored several alternate arrangement for constructing the residual architecture with multi-modal input and chose the above network based on performance.\nIn [48], the authors explored using external knowledge sources to improve VQA. They were able to show that by doing this they were able to achieve a small increase in performance over a model that did not have access to the external knowledge sources. The external knowledge bases were tailored to general information obtained from DBpedia [63], so it is possible that using a source tailored to VQA could yield greater improvement.\nNeural Module Networks (NMN) are an especially interesting approach to VQA [42, 49]. NMNs treat VQA as a sequence of sub-tasks that are carried out by separate neural sub-networks. Each sub-network performs a single task, e.g., the find[X] module produces a heat map for the presence of certain object. Other modules include describe, measure, and transform. Then, the sentence structure guides the execution sequence for the modules, enabling them to form a complete answering unit. For example, answering \u2018What color is the tie?\u2019 would involve executing the find[tie] module followed by the describe[color] module, which generates the answer."}, {"heading": "6 Discussion", "text": "As shown in Figure 9, there has been rapid improvement in the performance of VQA algorithms, but there is still a significant gap between the best methods and humans. It remains unclear whether the improvements in performance come from the mechanisms incorporated into later systems, e.g., attention, or if it is due to other factors. Moreover, it can be difficult to decouple the contributions of text and image data in isolation. There are also numerous challenges to comparing algorithms due to the variations in how they are evaluated. In this section, we discuss each of these issues."}, {"heading": "6.1 Vision vs. Language in VQA", "text": "VQA consists of two distinct data streams that need to be correctly used to ensure robust performance: images and questions. But, do current systems adequately use both vision and language? Ablation studies [40, 32] have routinely shown that question only models perform drastically better than image only models, especially on open-ended COCO-VQA. On COCO-QA, simple image-blind models that use\nonly the question can achieve 50% accuracy with the gain from using the image being comparatively modest [40]. In [40], it was also shown that for DAQUAR37, using a better language embedding with an imageblind model produced results superior to earlier works employing both images and questions. This is primarily due to two factors. First, the question severely constrains the kinds of answers expected in many cases, essentially turning an open-ended question into a multiple-choice one, e.g., questions about the color of an object will have a color as an answer. Second, the datasets tend to have strong bias. These two factors make language a much stronger prior than the image features alone.\nThe predictive power of language over images have been corroborated by ablation studies. In [64], the authors studied a model that had been trained using both image and question features. They then studied how the predictions of the model differed when it was given only the image or only the question, compared to when it was given both. They found that the image-only model\u2019s predictions differed from the combined model 40% more often than the question only model. They also showed that the way the question is phrased strongly biases the answer. When training a neural network, these regularities will be incorporated into the model. While this produces increased performance on the dataset, it is potentially detrimental to creating a general VQA system.\nTo further study the role of language and images in VQA, we used the model from [44]3. This model was trained on COCO-VQA, and it allows the contribution of the question and image features to be assessed independently by splitting the weights of the softmax output layer into image and question components. We asked simple binary questions with a relatively equal prior for both choices so that the image must be analyzed to answer the question. As seen in Figure 11, performance was poor, especially when considering that the baseline accuracy for yes/no questions for COCO-VQA is about 80%. Similarly, Figure 10 shows the impact on the way a question is phrased can produce different answers. We observed similar results when using the system in [32]. Without safeguards, language bias is prone to cause these issues.\nIn [39], bias in VQA was studied using synthetic cartoon images. They created a dataset with solely binary questions, in which the same question could be asked about two images that were mostly identical, except for a minor change that caused the correct answer to be different. They found that a model trained\n3An online demo is available here: http://visualqa.csail. mit.edu/\nPage 16\nPage 17\non an unbalanced version of this dataset performed 11% worse (absolute difference) on a balanced test dataset compared to a model trained on a balanced version of the dataset."}, {"heading": "6.2 How useful is attention for VQA?", "text": "It is difficult to determine how much attention helps VQA algorithms. The current best VQA model for COCO-VQA does employ spatial visual attention [55], but simple models that do not use attention have been shown to exceed earlier models that used complex attentive mechanisms. In [65], for example, an attention-free model that used multiple global image feature representations (VGG-19, ResNet-101, and ResNet-152), instead of a single CNN, performed very well compared some attentive models. They combined image and question features using both element-wise multiplication and addition, instead of solely concatenating them. Combined with ensembling, this yielded results significantly higher than the complex attention-based models used in [47] and [51]. Similar results have been obtained by other systems that do not employ spatial attention [40, 55, 66].\nIn [67], the authors showed that methods com-\nmonly used to incorporate spatial attention to specific image features do not cause models to attend to the same regions as humans tasked with VQA. They made this observation using both the attentive mechanisms used in [47] and [53]. This may be because the regions the model learns to attend to are discriminative due to biases in the dataset and not due to where the algorithm should attend. For example, when asked a question about whether drapes are in an image, the algorithm may instead look at the bottom of the image for a bed rather than windows because questions about drapes tend to be found in bedrooms. This is an indication that attentive mechanisms may not be correctly deployed due to biases."}, {"heading": "6.3 Bias Impairs Method Evaluation", "text": "Dataset bias significantly impairs the ability to evaluate VQA algorithms. Questions that require the use of the image content are often relatively easy to answer. Many are about the presence of objects or scene attributes. These questions tend to be handled well by CNNs and also have strong language biases. Harder questions, such as those beginning with \u2018Why\u2019 are comparatively rare. This has serious implications\nPage 18\nPage 19\nfor evaluating performance. For COCO-VQA (train and validation partitions), a system that improves accuracy on questions beginning with \u2018Is\u2019 and \u2019Are\u2019 by 15% will increase overall accuracy by 5%. However, the same increase in both \u2018Why\u2019 and \u2018Where\u2019 questions will only increase accuracy by 0.6%. In fact, even if all \u2018Why\u2019 and \u2018Where\u2019 questions are answered correctly, the overall increase in accuracy will only be 4.1%. On the other hand, answering \u2018yes\u2019 to all questions beginning with \u2018Is there\u2019 will yield an accuracy of 85.2% on those questions. These problems could be overcome if each type of question was evaluated in isolation, and then the mean accuracy across question types was used instead of overall accuracy for benchmarking the algorithms. This approach is similar to the mean per-class accuracy metric used for evaluating object classification algorithms, which was adopted due to bias in the amount of test data available for different object categories."}, {"heading": "6.4 Are Binary Questions Sufficient?", "text": "Using binary (yes/no or true/false) questions to evaluate algorithms has attracted significant discussion in the VQA community. The main argument against using binary questions is the lack of complex questions and the relative ease in answering the questions that are typically generated by human annotators. Visual Genome and Visual7W exclude binary questions altogether. The authors argued that this choice would encourage more complex questions from the annotators.\nOn the other hand, binary questions are easy to evaluate and these questions can, in theory, encompass an enormous variety of tasks. The SHAPES dataset [42] uses binary questions exclusively but contains complex questions involving spatial reasoning, counting, and drawing inferences (see Figure 6). Using cartoon images, [39] also showed that these questions can be especially difficult for VQA algorithms when the dataset is balanced. However, there are challenges to creating balanced binary questions for real world imagery. In COCO-VQA, \u2018yes\u2019 is a much more common answer than \u2018no,\u2019 simply because people tend to ask questions biased toward \u2018yes\u2019 as an answer.\nAs long as bias is controlled, we believe yes/no questions will play an important role in future VQA benchmarks, but a VQA system should be capable of more than solely binary questions. A user of a VQA system expects a system to be capable of handing arbitrary open-ended questions. A system that can only handle binary questions has limited utility in real-world applications."}, {"heading": "6.5 Open Ended vs. Multiple Choice", "text": "Because it is challenging to evaluate open-ended multi-word answers, multiple-choice has been proposed as a way to evaluate VQA algorithms. As long as the alternatives are sufficiently difficult, a system could be evaluated in this manner but then be deployed to answer open-ended questions. For these reasons, multiple choice is used to evaluate Visual7W, Visual Genome, and a variant of The VQA Dataset. In this framework, an algorithm has access to a number of possible answers (e.g., 18 for COCO-VQA), along with the question and image. It must then select among possible choices.\nA major problem with multiple-choice evaluation is that the problem can be reduced to determining which of the answers is correct instead of actually answering the question. For example, in [66], they formulated VQA as an answer scoring task, where the system was trained to produce a score based on the image, question, and potential answers. The answers themselves were fed into the system as features. It achieved state-of-the-art results on Visual7W and rivals the best methods on COCO-VQA, with their method performing better than many complex systems that use attention. To a large extent, we believe their system performed well because it learned to better exploit biases in the answers instead of reasoning about images. On Visual7W, they showed that a variant of their system that used solely the answers and was both image- and question-blind rivaled baselines using the question and image.\nWe argue that any VQA system should be able to operate without being given answers as inputs. Multiple-choice can be an important ingredient for evaluating multi-word answers, but it alone is not sufficient. When multiple-choice is used, the choices must be selected carefully to ensure that a question is hard and not deducible from the provided answers alone. A system that is solely capable of operating with answers provided is not really solving VQA, because these are not available when a system is deployed."}, {"heading": "7 Recommendations for Future", "text": "VQA Datasets\nExisting VQA benchmarks are not sufficient to evaluate whether an algorithm has \u2018solved\u2019 VQA. In this section, we discuss future developments in VQA datasets that will make them better benchmarks for the problem.\nFuture datasets need to be larger. While VQA\nPage 20\ndatasets have been growing in size and diversity, algorithms do not have enough data for training and evaluation. We did a small experiment where we trained a simple MLP baseline model for VQA using ResNet152 image features and skip-thought features for the questions, and we assessed performance as a function of the amount of training data available on COCOVQA. The results are shown in Figure 12, where it is clear that the curve has not started to approach an asymptote. This suggests that even on datasets that are biased, increasing the size of the dataset could significantly improve accuracy. However, this does not mean that increasing the size of the dataset is sufficient to turn it into a good benchmark, because humans tend to create questions with strong biases.\nFuture datasets need to be less biased. We have repeatedly discussed the problem of bias in existing VQA datasets in this paper, and pointed out the kinds of problems these biases cause for truly evaluating a VQA algorithm. For real-world open-ended VQA, this will be difficult to achieve without carefully instructing the humans that generate the questions. Bias has long been a problem in images used for computer vision datasets (for a review see [68]), and for VQA this problem is compounded by bias in the questions as well.\nIn addition to being larger and less biased, future datasets need more nuanced analysis for benchmarking. All of the publicly released datasets use evaluation metrics that treat every question with equal weight, but some kinds of questions are far easier, either because of bias or because existing algorithms excel at answering that kind of question, e.g., object recognition questions. Some datasets such as COCO-\nQA have divided VQA questions into distinct categories, e.g., for COCO-QA these are color, counting (number), location, and object. We believe that mean per-question type performance should replace standard accuracy, so each question would not have equal weight in evaluating performance. This would go a long way towards making a VQA algorithm have to perform well at a wide variety of question types to perform well overall, otherwise a system that excelled at answering \u2018Why\u2019 questions but was slightly worse than another model at more common questions would not be fairly evaluated. To do this, each question would need to be assigned a category. We believe this effort would make benchmark results significantly more meaningful. The scores on each question type could also be used to compare algorithms to see which kind of questions they excel at."}, {"heading": "8 Conclusions", "text": "VQA is an important basic research problem in computer vision and natural language processing that requires a system to do much more than task specific algorithms, such as object recognition and object detection. An algorithm that can answer arbitrary questions about images would be a milestone in artificial intelligence. We believe that VQA should be a necessary part of any visual Turing test.\nIn this paper, we critically reviewed existing datasets and algorithms for VQA. We discussed the challenges of evaluating answers generated by algorithms, especially multi-word answers. We described how biases and other problems plague existing datasets. This is a major problem, and the field needs a dataset that evaluates the important characteristics of a VQA algorithm, so that if an algorithm performs well on that dataset then it means it is doing well on VQA in general.\nFuture work on VQA involves the creation of larger and far more varied datasets. Bias in these datasets will be difficult to overcome, but evaluating different kinds of questions individually in a nuanced manner, rather than using naive accuracy alone, will help significantly. Further work will be needed to develop VQA algorithms that can reason about image content, but these algorithms may lead to significant new areas of research."}, {"heading": "Acknowledgments", "text": "We thank Ronald Kemker for helpful comments on an earlier draft of this paper.\nPage 21"}], "references": [{"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "You only look once: Unified, realtime object detection", "author": ["J. Redmon", "S. Divvala", "R. Girshick", "A. Farhadi"], "venue": "CVPR, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "Advances in neural information processing systems, pp. 91\u201399, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Largescale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "CVPR, pp. 1725\u20131732, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "NIPS, pp. 568\u2013576, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual turing test for computer vision systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "Proceedings of the National Academy of Sciences, vol. 112, no. 12, pp. 3618\u2013 3623, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards a visual turing challenge", "author": ["M. Malinowski", "M. Fritz"], "venue": "arXiv preprint arXiv:1410.8027, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for object detection", "author": ["C. Szegedy", "A. Toshev", "D. Erhan"], "venue": "Advances in Neural Information Processing Systems, pp. 2553\u20132561, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR, pp. 3431\u20133440, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "CVPR.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 0}, {"title": "Instance segmentation of indoor scenes using a coverage loss", "author": ["N. Silberman", "D. Sontag", "R. Fergus"], "venue": "ECCV, pp. 616\u2013631, Springer, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Monocular object instance segmentation and depth ordering with CNNs", "author": ["Z. Zhang", "A.G. Schwing", "S. Fidler", "R. Urtasun"], "venue": "CVPR, pp. 2614\u20132622, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Instancelevel segmentation with deep densely connected MRFs", "author": ["Z. Zhang", "S. Fidler", "R. Urtasun"], "venue": "CVPR, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "ICML, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pp. 311\u2013318, Association for Computational Linguistics, 2002.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["C.-Y. Lin"], "venue": "Text summarization branches out: Proceedings of the ACL-04 workshop, vol. 8, Barcelona, Spain, 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments", "author": ["S. Banerjee", "A. Lavie"], "venue": "Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, vol. 29, pp. 65\u201372, 2005. Page 22", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C. Lawrence Zitnick", "D. Parikh"], "venue": "CVPR, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Re-evaluation the role of BLEU in machine translation research", "author": ["C. Callison-Burch", "M. Osborne", "P. Koehn"], "venue": "2006.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J. Platt"], "venue": "CVPR, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic description generation from images: A survey of models, datasets, and evaluation measures", "author": ["R. Bernardi", "R. Cakici", "D. Elliott", "A. Erdem", "E. Erdem", "N. Ikizler-Cinbis", "F. Keller", "A. Muscat", "B. Plank"], "venue": "Journal of Artificial Intelligence Research, vol. 55, pp. 409\u2013 442, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploring nearest neighbor approaches for image captioning", "author": ["J. Devlin", "S. Gupta", "R. Girshick", "M. Mitchell", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1505.04467, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "CVPR, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "NIPS, 2015.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "VQA: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "ICCV, 2015.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Are you talking to a machine? Dataset and methods for multilingual image question answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "NIPS, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual7w: Grounded question answering in images", "author": ["Y. Zhu", "O. Groth", "M. Bernstein", "L. Fei-Fei"], "venue": "CVPR, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual Genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.- J. Li", "D.A. Shamma", "M. Bernstein", "L. Fei- Fei"], "venue": "2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["N. Silberman", "D. Hoiem", "P. Kohli", "R. Fergus"], "venue": "ECCV, 2012.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "ICCV, 2015.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Zeroshot learning via visual abstraction", "author": ["S. Antol", "C.L. Zitnick", "D. Parikh"], "venue": "European Conference on Computer Vision, pp. 401\u2013 416, 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Yin and yang: Balancing and answering binary visual questions", "author": ["P. Zhang", "Y. Goyal", "D. Summers-Stay", "D. Batra", "D. Parikh"], "venue": "CVPR, 2016.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Answer-type prediction for visual question answering", "author": ["K. Kafle", "C. Kanan"], "venue": "CVPR, 2016.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Yfcc100m: The new data in multimedia research", "author": ["B. Thomee", "D.A. Shamma", "G. Friedland", "B. Elizalde", "K. Ni", "D. Poland", "D. Borth", "L.- J. Li"], "venue": "Communications of the ACM, vol. 59, no. 2, pp. 64\u201373, 2016.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural module networks", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "CVPR, 2016.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Verbs semantics and lexical selection", "author": ["Z. Wu", "M. Palmer"], "venue": "Proceedings of the 32nd annual meeting on Association for Computational Linguistics, pp. 133\u2013138, Association for Computational Linguistics, 1994.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1994}, {"title": "Simple baseline for visual question answering", "author": ["B. Zhou", "Y. Tian", "S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "CoRR, vol. abs/1512.02167, 2015.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Image question answering using convolutional neural network with dynamic parameter prediction", "author": ["H. Noh", "P.H. Seo", "B. Han"], "venue": "CVPR, 2016.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["H. Xu", "K. Saenko"], "venue": "arXiv preprint arXiv:1511.05234, 2015.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A.J. Smola"], "venue": "CVPR, 2016. Page 23", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "Ask me anything: Free-form visual question answering based on knowledge from external sources", "author": ["Q. Wu", "P. Wang", "C. Shen", "A. van den Hengel", "A.R. Dick"], "venue": "CVPR, 2016.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to compose neural networks for question answering", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "arXiv preprint arXiv:1601.01705, 2016.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2016}, {"title": "A focused dynamic attention model for visual question answering", "author": ["I. Ilievski", "S. Yan", "J. Feng"], "venue": "arXiv preprint arXiv:1604.01485, 2016.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "arXiv preprint arXiv:1603.01417, 2016.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal residual learning for visual qa", "author": ["J.-H. Kim", "S.-W. Lee", "D.-H. Kwak", "M.-O. Heo", "J. Kim", "J.-W. Ha", "B.-T. Zhang"], "venue": "arXiv preprint arXiv:1606.01455, 2016.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2016}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "CoRR, vol. abs/1606.00061, 2016.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2016}, {"title": "Training recurrent answering units with joint loss minimization for VQA", "author": ["H. Noh", "B. Han"], "venue": "arXiv preprint arXiv:1606.03647, 2016.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["A. Fukui", "D.H. Park", "D. Yang", "A. Rohrbach", "T. Darrell", "M. Rohrbach"], "venue": "CoRR, vol. abs/1606.01847, 2016.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1847}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CVPR, 2015.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "Skipthought vectors", "author": ["R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R.S. Zemel", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "NIPS, 2015.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M.-T. Luong", "H. Pham", "C.D. Manning"], "venue": "arXiv preprint arXiv:1508.04025, 2015.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2015}, {"title": "Where to look: Focus regions for visual question answering", "author": ["K.J. Shih", "S. Singh", "D. Hoiem"], "venue": "CVPR, 2016.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2016}, {"title": "Edge boxes: Locating object proposals from edges", "author": ["C.L. Zitnick", "P. Doll\u00e1r"], "venue": "European Conference on Computer Vision, pp. 391\u2013405, Springer, 2014.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "J. Dean"], "venue": "2013.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2013}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["A. Kumar", "O. Irsoy", "J. Su", "J. Bradbury", "R. English", "B. Pierce", "P. Ondruska", "I. Gulrajani", "R. Socher"], "venue": "ICML, 2016.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2016}, {"title": "DBpedia\u2013a large-scale, multilingual knowledge base extracted from Wikipedia", "author": ["J. Lehmann", "R. Isele", "M. Jakob", "A. Jentzsch", "D. Kontokostas", "P.N. Mendes", "S. Hellmann", "M. Morsey", "P. van Kleef", "S. Auer"], "venue": "Semantic Web, vol. 6, no. 2, pp. 167\u2013195, 2015.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2015}, {"title": "Analyzing the behavior of visual question answering models", "author": ["A. Agrawal", "D. Batra", "D. Parikh"], "venue": "CoRR, vol. abs/1606.07356, 2016.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2016}, {"title": "Dualnet: Domain-invariant network for visual question answering", "author": ["K. Saito", "A. Shin", "Y. Ushiku", "T. Harada"], "venue": "arXiv preprint arXiv:1606.06108, 2016.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2016}, {"title": "Revisiting visual question answering baselines", "author": ["A. Jabri", "A. Joulin", "L. van der Maaten"], "venue": "arXiv preprint arXiv:1606.08390, 2016.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2016}, {"title": "Human attention in visual question answering: Do humans and deep networks look at the same regions", "author": ["A. Das", "H. Agrawal", "C.L. Zitnick", "D. Parikh", "D. Batra"], "venue": "arXiv preprint arXiv:1606.03556, 2016.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2016}, {"title": "Unbiased look at dataset bias", "author": ["A. Torralba", "A. Efros"], "venue": "CVPR, 2011. Page 24", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Recent advancements in computer vision and deep learning research have enabled enormous progress in many computer vision tasks, such as image classification [1, 2], object detection [3, 4], and activity recognition [5, 6, 7].", "startOffset": 157, "endOffset": 163}, {"referenceID": 1, "context": "Recent advancements in computer vision and deep learning research have enabled enormous progress in many computer vision tasks, such as image classification [1, 2], object detection [3, 4], and activity recognition [5, 6, 7].", "startOffset": 157, "endOffset": 163}, {"referenceID": 2, "context": "Recent advancements in computer vision and deep learning research have enabled enormous progress in many computer vision tasks, such as image classification [1, 2], object detection [3, 4], and activity recognition [5, 6, 7].", "startOffset": 182, "endOffset": 188}, {"referenceID": 3, "context": "Recent advancements in computer vision and deep learning research have enabled enormous progress in many computer vision tasks, such as image classification [1, 2], object detection [3, 4], and activity recognition [5, 6, 7].", "startOffset": 182, "endOffset": 188}, {"referenceID": 4, "context": "Recent advancements in computer vision and deep learning research have enabled enormous progress in many computer vision tasks, such as image classification [1, 2], object detection [3, 4], and activity recognition [5, 6, 7].", "startOffset": 215, "endOffset": 224}, {"referenceID": 5, "context": "Recent advancements in computer vision and deep learning research have enabled enormous progress in many computer vision tasks, such as image classification [1, 2], object detection [3, 4], and activity recognition [5, 6, 7].", "startOffset": 215, "endOffset": 224}, {"referenceID": 6, "context": "Recent advancements in computer vision and deep learning research have enabled enormous progress in many computer vision tasks, such as image classification [1, 2], object detection [3, 4], and activity recognition [5, 6, 7].", "startOffset": 215, "endOffset": 224}, {"referenceID": 1, "context": "Given enough data, deep convolutional neural networks (CNNs) rival the abilities of humans to do image classification [2].", "startOffset": 118, "endOffset": 121}, {"referenceID": 7, "context": "sion problems, it can be considered a component of a Turing Test for image understanding [8, 9].", "startOffset": 89, "endOffset": 95}, {"referenceID": 8, "context": "sion problems, it can be considered a component of a Turing Test for image understanding [8, 9].", "startOffset": 89, "endOffset": 95}, {"referenceID": 7, "context": "A Visual Turing Test rigorously evaluates a computer vision system to assess whether it is capable of human-level semantic analysis of images [8, 9].", "startOffset": 142, "endOffset": 148}, {"referenceID": 8, "context": "A Visual Turing Test rigorously evaluates a computer vision system to assess whether it is capable of human-level semantic analysis of images [8, 9].", "startOffset": 142, "endOffset": 148}, {"referenceID": 1, "context": "The most successful of these is object recognition, where algorithms now rival humans in accuracy [2].", "startOffset": 98, "endOffset": 101}, {"referenceID": 10, "context": "The best object detection methods all use deep CNNs [11, 4, 3].", "startOffset": 52, "endOffset": 62}, {"referenceID": 3, "context": "The best object detection methods all use deep CNNs [11, 4, 3].", "startOffset": 52, "endOffset": 62}, {"referenceID": 2, "context": "The best object detection methods all use deep CNNs [11, 4, 3].", "startOffset": 52, "endOffset": 62}, {"referenceID": 11, "context": "Semantic segmentation takes the task of localization a step further by classifying each pixel as belonging to a particular semantic class [12, 13] .", "startOffset": 138, "endOffset": 146}, {"referenceID": 12, "context": "Semantic segmentation takes the task of localization a step further by classifying each pixel as belonging to a particular semantic class [12, 13] .", "startOffset": 138, "endOffset": 146}, {"referenceID": 13, "context": "Instance segmentation further improves upon localization by differentiating between separate instances of the same semantic class [14, 15, 16].", "startOffset": 130, "endOffset": 142}, {"referenceID": 14, "context": "Instance segmentation further improves upon localization by differentiating between separate instances of the same semantic class [14, 15, 16].", "startOffset": 130, "endOffset": 142}, {"referenceID": 15, "context": "Instance segmentation further improves upon localization by differentiating between separate instances of the same semantic class [14, 15, 16].", "startOffset": 130, "endOffset": 142}, {"referenceID": 16, "context": "One of the most studied is image captioning [17, 5, 18, 19, 20], in which an algorithm\u2019s goal is to produce a natural language description of a given image.", "startOffset": 44, "endOffset": 63}, {"referenceID": 4, "context": "One of the most studied is image captioning [17, 5, 18, 19, 20], in which an algorithm\u2019s goal is to produce a natural language description of a given image.", "startOffset": 44, "endOffset": 63}, {"referenceID": 17, "context": "One of the most studied is image captioning [17, 5, 18, 19, 20], in which an algorithm\u2019s goal is to produce a natural language description of a given image.", "startOffset": 44, "endOffset": 63}, {"referenceID": 18, "context": "One of the most studied is image captioning [17, 5, 18, 19, 20], in which an algorithm\u2019s goal is to produce a natural language description of a given image.", "startOffset": 44, "endOffset": 63}, {"referenceID": 19, "context": "One of the most studied is image captioning [17, 5, 18, 19, 20], in which an algorithm\u2019s goal is to produce a natural language description of a given image.", "startOffset": 44, "endOffset": 63}, {"referenceID": 20, "context": "Most widely used evaluation schemes include BLEU [21], ROUGE [22], METEOR [23] and CIDEr [24].", "startOffset": 49, "endOffset": 53}, {"referenceID": 21, "context": "Most widely used evaluation schemes include BLEU [21], ROUGE [22], METEOR [23] and CIDEr [24].", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "Most widely used evaluation schemes include BLEU [21], ROUGE [22], METEOR [23] and CIDEr [24].", "startOffset": 74, "endOffset": 78}, {"referenceID": 23, "context": "Most widely used evaluation schemes include BLEU [21], ROUGE [22], METEOR [23] and CIDEr [24].", "startOffset": 89, "endOffset": 93}, {"referenceID": 24, "context": "BLEU, the most widely used metric, is known to have the same score for large variations in sentence structure with largely varying semantic content [25].", "startOffset": 148, "endOffset": 152}, {"referenceID": 25, "context": "For captions generated in [26], BLEU scores ranked machine generated captions above human captions.", "startOffset": 26, "endOffset": 30}, {"referenceID": 9, "context": "This figure shows a semantic segmentation map from the COCO dataset [10].", "startOffset": 68, "endOffset": 72}, {"referenceID": 26, "context": "While other evaluation metrics, especially CIDEr and METEOR, show more robustness in terms of agreement with human judges, they still often rank automatically generated captions higher than human captions [27].", "startOffset": 205, "endOffset": 209}, {"referenceID": 27, "context": "In fact, a simple system that returns the caption of the training image with the most similar visual features using nearest neighbor yields relatively high scores using automatic evaluation metrics [28].", "startOffset": 198, "endOffset": 202}, {"referenceID": 28, "context": "Dense image captioning (DenseCap) avoids the generic caption problem by annotating an image densely with short visual descriptions pertaining to small, but salient, image regions [29].", "startOffset": 179, "endOffset": 183}, {"referenceID": 29, "context": "As of this article, the main datasets for VQA are DAQUAR [30], COCO-QA [31], The VQA Dataset [32], FM-IQA [33], Visual7W [34], and Visual Genome [35].", "startOffset": 57, "endOffset": 61}, {"referenceID": 30, "context": "As of this article, the main datasets for VQA are DAQUAR [30], COCO-QA [31], The VQA Dataset [32], FM-IQA [33], Visual7W [34], and Visual Genome [35].", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "As of this article, the main datasets for VQA are DAQUAR [30], COCO-QA [31], The VQA Dataset [32], FM-IQA [33], Visual7W [34], and Visual Genome [35].", "startOffset": 93, "endOffset": 97}, {"referenceID": 32, "context": "As of this article, the main datasets for VQA are DAQUAR [30], COCO-QA [31], The VQA Dataset [32], FM-IQA [33], Visual7W [34], and Visual Genome [35].", "startOffset": 106, "endOffset": 110}, {"referenceID": 33, "context": "As of this article, the main datasets for VQA are DAQUAR [30], COCO-QA [31], The VQA Dataset [32], FM-IQA [33], Visual7W [34], and Visual Genome [35].", "startOffset": 121, "endOffset": 125}, {"referenceID": 34, "context": "As of this article, the main datasets for VQA are DAQUAR [30], COCO-QA [31], The VQA Dataset [32], FM-IQA [33], Visual7W [34], and Visual Genome [35].", "startOffset": 145, "endOffset": 149}, {"referenceID": 9, "context": "With exception of DAQUAR, all of the datasets include images from the Microsoft Common Objects in Context (COCO) dataset [10], which consists of 328,000 images, 91 common object categories with over 2 million labeled instances, and an average of 5 captions per image.", "startOffset": 121, "endOffset": 125}, {"referenceID": 29, "context": "The DAtaset for QUestion Answering on Real-world images (DAQUAR) [30] was the first major VQA dataset to be released.", "startOffset": 65, "endOffset": 69}, {"referenceID": 35, "context": "It consists of 6795 training and 5673 testing QA pairs based on images from the NYUDepthV2 Dataset [36].", "startOffset": 99, "endOffset": 103}, {"referenceID": 36, "context": "In [37], additional ground truth answers were collected for DAQUAR to cre-", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "D A Q U A R [3 0 C O C O -Q A [3 1 C O C O -V Q A [3 2 ] F M -I Q A [3 3 ] 1 V is u a l7 W [3 4 ] V is u a l g en o m e [3 5 ]", "startOffset": 50, "endOffset": 56}, {"referenceID": 1, "context": "D A Q U A R [3 0 C O C O -Q A [3 1 C O C O -V Q A [3 2 ] F M -I Q A [3 3 ] 1 V is u a l7 W [3 4 ] V is u a l g en o m e [3 5 ]", "startOffset": 50, "endOffset": 56}, {"referenceID": 2, "context": "D A Q U A R [3 0 C O C O -Q A [3 1 C O C O -V Q A [3 2 ] F M -I Q A [3 3 ] 1 V is u a l7 W [3 4 ] V is u a l g en o m e [3 5 ]", "startOffset": 68, "endOffset": 74}, {"referenceID": 2, "context": "D A Q U A R [3 0 C O C O -Q A [3 1 C O C O -V Q A [3 2 ] F M -I Q A [3 3 ] 1 V is u a l7 W [3 4 ] V is u a l g en o m e [3 5 ]", "startOffset": 68, "endOffset": 74}, {"referenceID": 2, "context": "D A Q U A R [3 0 C O C O -Q A [3 1 C O C O -V Q A [3 2 ] F M -I Q A [3 3 ] 1 V is u a l7 W [3 4 ] V is u a l g en o m e [3 5 ]", "startOffset": 91, "endOffset": 97}, {"referenceID": 3, "context": "D A Q U A R [3 0 C O C O -Q A [3 1 C O C O -V Q A [3 2 ] F M -I Q A [3 3 ] 1 V is u a l7 W [3 4 ] V is u a l g en o m e [3 5 ]", "startOffset": 91, "endOffset": 97}, {"referenceID": 2, "context": "D A Q U A R [3 0 C O C O -Q A [3 1 C O C O -V Q A [3 2 ] F M -I Q A [3 3 ] 1 V is u a l7 W [3 4 ] V is u a l g en o m e [3 5 ]", "startOffset": 120, "endOffset": 126}, {"referenceID": 4, "context": "D A Q U A R [3 0 C O C O -Q A [3 1 C O C O -V Q A [3 2 ] F M -I Q A [3 3 ] 1 V is u a l7 W [3 4 ] V is u a l g en o m e [3 5 ]", "startOffset": 120, "endOffset": 126}, {"referenceID": 31, "context": "The VQA Dataset [32] consists of both real images from COCO and abstract cartoon images.", "startOffset": 16, "endOffset": 20}, {"referenceID": 37, "context": "The human models are the same as those used in [38], and they contain deformable limbs and eight different facial expressions.", "startOffset": 47, "endOffset": 51}, {"referenceID": 38, "context": "Yin and Yang [39] is a dataset built on top of SYNTH-VQA that tried to eliminate biases in the answers people have to questions.", "startOffset": 13, "endOffset": 17}, {"referenceID": 39, "context": "6% accuracy on COCO-VQA using the question alone [40].", "startOffset": 49, "endOffset": 53}, {"referenceID": 32, "context": "The Freestyle Multilingual Image Question Answering (FM-IQA) dataset is another dataset based on COCO [33].", "startOffset": 102, "endOffset": 106}, {"referenceID": 34, "context": "Visual Genome [35] consists of 108,249 images that occur in both YFCC100M [41] and COCO images.", "startOffset": 14, "endOffset": 18}, {"referenceID": 40, "context": "Visual Genome [35] consists of 108,249 images that occur in both YFCC100M [41] and COCO images.", "startOffset": 74, "endOffset": 78}, {"referenceID": 41, "context": "While the other VQA datasets contain either real or synthetic scenes, the SHAPES dataset [42] consists of shapes of varying arrangements, types, and colors.", "startOffset": 89, "endOffset": 93}, {"referenceID": 41, "context": "Questions in the SHAPES dataset [42] include counting (How many triangles are there?), spatial reasoning (Is there a red shape above a circle?), and inference (Is there a blue shape red?)", "startOffset": 32, "endOffset": 36}, {"referenceID": 34, "context": "This figure is taken from [35].", "startOffset": 26, "endOffset": 30}, {"referenceID": 33, "context": "Free form QA: What does the sky look like? Region based QA: What color is the horse? (b) Example of the pointing QA task in Visual7W [34].", "startOffset": 133, "endOffset": 137}, {"referenceID": 42, "context": "Wu-Palmer Similarity (WUPS) [43] was proposed as an alternative to accuracy in [30].", "startOffset": 28, "endOffset": 32}, {"referenceID": 29, "context": "Wu-Palmer Similarity (WUPS) [43] was proposed as an alternative to accuracy in [30].", "startOffset": 79, "endOffset": 83}, {"referenceID": 29, "context": "To remedy this, [30] proposed to threshold WUPS scores, where a score that is below a threshold will be scaled down by a factor.", "startOffset": 16, "endOffset": 20}, {"referenceID": 29, "context": "1 was suggested by [30].", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "An alternative to relying on semantic similarity measures is to have multiple independently collected ground truth answers for each question, which was done for The VQA Dataset [32] and DAQUARconsensus [37].", "startOffset": 177, "endOffset": 181}, {"referenceID": 36, "context": "An alternative to relying on semantic similarity measures is to have multiple independently collected ground truth answers for each question, which was done for The VQA Dataset [32] and DAQUARconsensus [37].", "startOffset": 202, "endOffset": 206}, {"referenceID": 32, "context": "The creators of FM-IQA [33] suggested using human judges to assess multi-word answers, but this presents a number of problems.", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "Most algorithms use CNNs that are pre-trained on ImageNet to extract image features, such as VGGNet [1], ResNet [2], and GoogLeNet [56].", "startOffset": 100, "endOffset": 103}, {"referenceID": 1, "context": "Most algorithms use CNNs that are pre-trained on ImageNet to extract image features, such as VGGNet [1], ResNet [2], and GoogLeNet [56].", "startOffset": 112, "endOffset": 115}, {"referenceID": 55, "context": "Most algorithms use CNNs that are pre-trained on ImageNet to extract image features, such as VGGNet [1], ResNet [2], and GoogLeNet [56].", "startOffset": 131, "endOffset": 135}, {"referenceID": 31, "context": "There have been a wider range of methods used to extract text features, including bag-of-words (BOW) [32, 44], long short term memory (LSTM) encoders [32, 31, 37, 47, 33], and skipthought vectors [40, 45].", "startOffset": 101, "endOffset": 109}, {"referenceID": 43, "context": "There have been a wider range of methods used to extract text features, including bag-of-words (BOW) [32, 44], long short term memory (LSTM) encoders [32, 31, 37, 47, 33], and skipthought vectors [40, 45].", "startOffset": 101, "endOffset": 109}, {"referenceID": 31, "context": "There have been a wider range of methods used to extract text features, including bag-of-words (BOW) [32, 44], long short term memory (LSTM) encoders [32, 31, 37, 47, 33], and skipthought vectors [40, 45].", "startOffset": 150, "endOffset": 170}, {"referenceID": 30, "context": "There have been a wider range of methods used to extract text features, including bag-of-words (BOW) [32, 44], long short term memory (LSTM) encoders [32, 31, 37, 47, 33], and skipthought vectors [40, 45].", "startOffset": 150, "endOffset": 170}, {"referenceID": 36, "context": "There have been a wider range of methods used to extract text features, including bag-of-words (BOW) [32, 44], long short term memory (LSTM) encoders [32, 31, 37, 47, 33], and skipthought vectors [40, 45].", "startOffset": 150, "endOffset": 170}, {"referenceID": 46, "context": "There have been a wider range of methods used to extract text features, including bag-of-words (BOW) [32, 44], long short term memory (LSTM) encoders [32, 31, 37, 47, 33], and skipthought vectors [40, 45].", "startOffset": 150, "endOffset": 170}, {"referenceID": 32, "context": "There have been a wider range of methods used to extract text features, including bag-of-words (BOW) [32, 44], long short term memory (LSTM) encoders [32, 31, 37, 47, 33], and skipthought vectors [40, 45].", "startOffset": 150, "endOffset": 170}, {"referenceID": 39, "context": "There have been a wider range of methods used to extract text features, including bag-of-words (BOW) [32, 44], long short term memory (LSTM) encoders [32, 31, 37, 47, 33], and skipthought vectors [40, 45].", "startOffset": 196, "endOffset": 204}, {"referenceID": 44, "context": "There have been a wider range of methods used to extract text features, including bag-of-words (BOW) [32, 44], long short term memory (LSTM) encoders [32, 31, 37, 47, 33], and skipthought vectors [40, 45].", "startOffset": 196, "endOffset": 204}, {"referenceID": 31, "context": "The simplest baseline algorithms for VQA consist of using linear or multi-layer perceptron (MLP) classifiers applied to a vector of image and text features concatenated to each other [32, 40, 44].", "startOffset": 183, "endOffset": 195}, {"referenceID": 39, "context": "The simplest baseline algorithms for VQA consist of using linear or multi-layer perceptron (MLP) classifiers applied to a vector of image and text features concatenated to each other [32, 40, 44].", "startOffset": 183, "endOffset": 195}, {"referenceID": 43, "context": "The simplest baseline algorithms for VQA consist of using linear or multi-layer perceptron (MLP) classifiers applied to a vector of image and text features concatenated to each other [32, 40, 44].", "startOffset": 183, "endOffset": 195}, {"referenceID": 43, "context": "In [44], the authors used a bag-of-words to represent the question and CNN features from GoogLeNet for the vi-", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "IMAGE-ONLY [40] 6.", "startOffset": 11, "endOffset": 15}, {"referenceID": 39, "context": "59 QUESTION-ONLY [40] 25.", "startOffset": 17, "endOffset": 21}, {"referenceID": 29, "context": "MULTI-WORLD [30] 7.", "startOffset": 12, "endOffset": 16}, {"referenceID": 36, "context": "73 - ASK-NEURON [37] 21.", "startOffset": 16, "endOffset": 20}, {"referenceID": 30, "context": "68 - ENSEMBLE [31] - 36.", "startOffset": 14, "endOffset": 18}, {"referenceID": 31, "context": "84 - LSTM Q+I [32] 54.", "startOffset": 14, "endOffset": 18}, {"referenceID": 43, "context": "17 iBOWIMG [44] 55.", "startOffset": 11, "endOffset": 15}, {"referenceID": 44, "context": "97 DPPNet [45] 28.", "startOffset": 10, "endOffset": 14}, {"referenceID": 45, "context": "69 SMem [46] - 40.", "startOffset": 8, "endOffset": 12}, {"referenceID": 46, "context": "24 SAN [47] 29.", "startOffset": 7, "endOffset": 11}, {"referenceID": 47, "context": "90 AMA [48] 69.", "startOffset": 7, "endOffset": 11}, {"referenceID": 41, "context": "40 NMN [42] 58.", "startOffset": 7, "endOffset": 11}, {"referenceID": 48, "context": "70 D-NMN [49] 59.", "startOffset": 9, "endOffset": 13}, {"referenceID": 49, "context": "40 FDA [50] 59.", "startOffset": 7, "endOffset": 11}, {"referenceID": 39, "context": "18 HYBRID [40] 28.", "startOffset": 10, "endOffset": 14}, {"referenceID": 50, "context": "06 DMN+ [51] 60.", "startOffset": 8, "endOffset": 12}, {"referenceID": 51, "context": "40 MRN [52] 61.", "startOffset": 7, "endOffset": 11}, {"referenceID": 52, "context": "33 HieCoAtten [53] 65.", "startOffset": 14, "endOffset": 18}, {"referenceID": 53, "context": "10 RAU ResNet [54] 63.", "startOffset": 14, "endOffset": 18}, {"referenceID": 54, "context": "30 MCB-ensemble [55] 66.", "startOffset": 16, "endOffset": 20}, {"referenceID": 31, "context": "Their approach worked remarkably well, surpassing the previous baseline on COCO-VQA from [32], which used a theoretically more powerful model, an LSTM, to represent the question.", "startOffset": 89, "endOffset": 93}, {"referenceID": 39, "context": "Similarly, [40] used skip-thought vectors [57] to represent the questions and ResNet152 to represent image features.", "startOffset": 11, "endOffset": 15}, {"referenceID": 56, "context": "Similarly, [40] used skip-thought vectors [57] to represent the questions and ResNet152 to represent image features.", "startOffset": 42, "endOffset": 46}, {"referenceID": 31, "context": "In [32], an LSTM encoder acting on a one-hot encoding of the sentence was used to represent question features, and GoogLeNet was used for image features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 36, "context": "In [37], an LSTM model was fed an embedding of each word sequentially with CNN features concatenated to it.", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "A related approach was used in [31], where an LSTM was fed CNN features during the first and last time-steps, with word features in between.", "startOffset": 31, "endOffset": 35}, {"referenceID": 32, "context": "A similar approach was used in [33], but the CNN image features were only fed into the LSTM at the end of the question and instead of a classifier, another LSTM was used to generate the answer one word at a time.", "startOffset": 31, "endOffset": 35}, {"referenceID": 29, "context": "In [30], the first Bayesian framework for VQA was proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "A very different Bayesian model was proposed in [40].", "startOffset": 48, "endOffset": 52}, {"referenceID": 19, "context": "Similar attention models have shown great success in other vision and NLP tasks, such as captioning [20] and machine translation [58].", "startOffset": 100, "endOffset": 104}, {"referenceID": 57, "context": "Similar attention models have shown great success in other vision and NLP tasks, such as captioning [20] and machine translation [58].", "startOffset": 129, "endOffset": 133}, {"referenceID": 58, "context": "While multiple papers have focused on using spatial visual attention for VQA [59, 47, 51, 46, 53, 50, 55], there are significant differences among these methods.", "startOffset": 77, "endOffset": 105}, {"referenceID": 46, "context": "While multiple papers have focused on using spatial visual attention for VQA [59, 47, 51, 46, 53, 50, 55], there are significant differences among these methods.", "startOffset": 77, "endOffset": 105}, {"referenceID": 50, "context": "While multiple papers have focused on using spatial visual attention for VQA [59, 47, 51, 46, 53, 50, 55], there are significant differences among these methods.", "startOffset": 77, "endOffset": 105}, {"referenceID": 45, "context": "While multiple papers have focused on using spatial visual attention for VQA [59, 47, 51, 46, 53, 50, 55], there are significant differences among these methods.", "startOffset": 77, "endOffset": 105}, {"referenceID": 52, "context": "While multiple papers have focused on using spatial visual attention for VQA [59, 47, 51, 46, 53, 50, 55], there are significant differences among these methods.", "startOffset": 77, "endOffset": 105}, {"referenceID": 49, "context": "While multiple papers have focused on using spatial visual attention for VQA [59, 47, 51, 46, 53, 50, 55], there are significant differences among these methods.", "startOffset": 77, "endOffset": 105}, {"referenceID": 54, "context": "While multiple papers have focused on using spatial visual attention for VQA [59, 47, 51, 46, 53, 50, 55], there are significant differences among these methods.", "startOffset": 77, "endOffset": 105}, {"referenceID": 58, "context": "The Focus Regions for VQA [59] and Focused Dynamic Attention (FDA) models [50] both used Edge Boxes [60] to generate bounding box region proposals for images.", "startOffset": 26, "endOffset": 30}, {"referenceID": 49, "context": "The Focus Regions for VQA [59] and Focused Dynamic Attention (FDA) models [50] both used Edge Boxes [60] to generate bounding box region proposals for images.", "startOffset": 74, "endOffset": 78}, {"referenceID": 59, "context": "The Focus Regions for VQA [59] and Focused Dynamic Attention (FDA) models [50] both used Edge Boxes [60] to generate bounding box region proposals for images.", "startOffset": 100, "endOffset": 104}, {"referenceID": 58, "context": "In [59], a CNN was used to extract features from each of these boxes.", "startOffset": 3, "endOffset": 7}, {"referenceID": 49, "context": "In FDA [50], the authors proposed to only use the region proposals that have the objects mentioned in the question.", "startOffset": 7, "endOffset": 11}, {"referenceID": 1, "context": "During test, the labels are obtained by classifying each bounding box using ResNet [2].", "startOffset": 83, "endOffset": 86}, {"referenceID": 60, "context": "Subsequently, word2vec [61] was used to compute the similarity between words in the question and the object labels assigned to each of the bounding boxes.", "startOffset": 23, "endOffset": 27}, {"referenceID": 46, "context": "In contrast to using region proposals, the Stacked Attention Network (SAN) [47] and the Dynamic Memory Network (DMN) [51] models both used visual features from the spatial grid of a CNN\u2019s feature maps (see Figure 8).", "startOffset": 75, "endOffset": 79}, {"referenceID": 50, "context": "In contrast to using region proposals, the Stacked Attention Network (SAN) [47] and the Dynamic Memory Network (DMN) [51] models both used visual features from the spatial grid of a CNN\u2019s feature maps (see Figure 8).", "startOffset": 117, "endOffset": 121}, {"referenceID": 46, "context": "Both [47] and [51] used the last convolutional layer from VGG-19 with 448 \u00d7 448 images to produce a 14 \u00d7 14 filter response map with 512 dimensional features at each grid location.", "startOffset": 5, "endOffset": 9}, {"referenceID": 50, "context": "Both [47] and [51] used the last convolutional layer from VGG-19 with 448 \u00d7 448 images to produce a 14 \u00d7 14 filter response map with 512 dimensional features at each grid location.", "startOffset": 14, "endOffset": 18}, {"referenceID": 46, "context": "In SAN [47], an attention layer is specified by a single layer of weights that uses the question and the CNN feature map with a softmax activation function to compute the attention distribution across image locations.", "startOffset": 7, "endOffset": 11}, {"referenceID": 45, "context": "A similar attentive mechanism was used in the Spatial Memory Network [46] model, where spatial", "startOffset": 69, "endOffset": 73}, {"referenceID": 50, "context": "Another approach that incorporated spatial attention using CNN feature maps is presented in [51].", "startOffset": 92, "endOffset": 96}, {"referenceID": 61, "context": "To do this, they used a modified Dynamic Memory Network (DMN) [62].", "startOffset": 62, "endOffset": 66}, {"referenceID": 52, "context": "The Hierarchical Co-Attention model [53] applies attention to both the image and question to jointly reason about the two different streams of information.", "startOffset": 36, "endOffset": 40}, {"referenceID": 45, "context": "to the method used in Spatial Memory Network [46].", "startOffset": 45, "endOffset": 49}, {"referenceID": 54, "context": "In [55], Multimodal Compact Bilinear (MCB) pooling was proposed as a method for combining image and text features in VQA.", "startOffset": 3, "endOffset": 7}, {"referenceID": 46, "context": "In a variation of this model, a soft-attention mechanism, similar to the method in [47], was also used, with the only major change being the use of MCB for combining text and question features instead of element-wise multiplication in [47].", "startOffset": 83, "endOffset": 87}, {"referenceID": 46, "context": "In a variation of this model, a soft-attention mechanism, similar to the method in [47], was also used, with the only major change being the use of MCB for combining text and question features instead of element-wise multiplication in [47].", "startOffset": 235, "endOffset": 239}, {"referenceID": 53, "context": "In [54], the authors proposed breaking the answering process into abstract sub-tasks by using several complete answering units in a recurrent fashion.", "startOffset": 3, "endOffset": 7}, {"referenceID": 46, "context": "Each answering unit on the chain is equipped with an attentive mechanism derived from [47] and a classifier.", "startOffset": 86, "endOffset": 90}, {"referenceID": 44, "context": "Another notable model is provided by [45], which incorporates a Dynamic Parameter Prediction layer into the fully connected layers of a CNN.", "startOffset": 37, "endOffset": 41}, {"referenceID": 51, "context": "In [52], Multimodal Residual Networks (MRN) were proposed for VQA.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "Their system is a modification of ResNet [2] to use both visual and question features in the residual mapping.", "startOffset": 41, "endOffset": 44}, {"referenceID": 47, "context": "In [48], the authors explored using external knowledge sources to improve VQA.", "startOffset": 3, "endOffset": 7}, {"referenceID": 62, "context": "The external knowledge bases were tailored to general information obtained from DBpedia [63], so it is possible that using a source tailored to VQA could yield greater improvement.", "startOffset": 88, "endOffset": 92}, {"referenceID": 41, "context": "Neural Module Networks (NMN) are an especially interesting approach to VQA [42, 49].", "startOffset": 75, "endOffset": 83}, {"referenceID": 48, "context": "Neural Module Networks (NMN) are an especially interesting approach to VQA [42, 49].", "startOffset": 75, "endOffset": 83}, {"referenceID": 39, "context": "But, do current systems adequately use both vision and language? Ablation studies [40, 32] have routinely shown that question only models perform drastically better than image only models, especially on open-ended COCO-VQA.", "startOffset": 82, "endOffset": 90}, {"referenceID": 31, "context": "But, do current systems adequately use both vision and language? Ablation studies [40, 32] have routinely shown that question only models perform drastically better than image only models, especially on open-ended COCO-VQA.", "startOffset": 82, "endOffset": 90}, {"referenceID": 39, "context": "On COCO-QA, simple image-blind models that use only the question can achieve 50% accuracy with the gain from using the image being comparatively modest [40].", "startOffset": 152, "endOffset": 156}, {"referenceID": 39, "context": "In [40], it was also shown that for DAQUAR37, using a better language embedding with an imageblind model produced results superior to earlier works employing both images and questions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 63, "context": "In [64], the authors studied a model that had been trained using both image and question features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 43, "context": "To further study the role of language and images in VQA, we used the model from [44].", "startOffset": 80, "endOffset": 84}, {"referenceID": 31, "context": "We observed similar results when using the system in [32].", "startOffset": 53, "endOffset": 57}, {"referenceID": 38, "context": "In [39], bias in VQA was studied using synthetic cartoon images.", "startOffset": 3, "endOffset": 7}, {"referenceID": 43, "context": "The left example uses the system in [44] and the right example uses the system from [40].", "startOffset": 36, "endOffset": 40}, {"referenceID": 39, "context": "The left example uses the system in [44] and the right example uses the system from [40].", "startOffset": 84, "endOffset": 88}, {"referenceID": 54, "context": "The current best VQA model for COCO-VQA does employ spatial visual attention [55], but simple models that do not use attention have been shown to exceed earlier models that used complex attentive mechanisms.", "startOffset": 77, "endOffset": 81}, {"referenceID": 64, "context": "In [65], for example, an attention-free model that used multiple global image feature representations (VGG-19, ResNet-101, and ResNet-152), instead of a single CNN, performed very well compared some attentive models.", "startOffset": 3, "endOffset": 7}, {"referenceID": 46, "context": "Combined with ensembling, this yielded results significantly higher than the complex attention-based models used in [47] and [51].", "startOffset": 116, "endOffset": 120}, {"referenceID": 50, "context": "Combined with ensembling, this yielded results significantly higher than the complex attention-based models used in [47] and [51].", "startOffset": 125, "endOffset": 129}, {"referenceID": 39, "context": "Similar results have been obtained by other systems that do not employ spatial attention [40, 55, 66].", "startOffset": 89, "endOffset": 101}, {"referenceID": 54, "context": "Similar results have been obtained by other systems that do not employ spatial attention [40, 55, 66].", "startOffset": 89, "endOffset": 101}, {"referenceID": 65, "context": "Similar results have been obtained by other systems that do not employ spatial attention [40, 55, 66].", "startOffset": 89, "endOffset": 101}, {"referenceID": 66, "context": "In [67], the authors showed that methods commonly used to incorporate spatial attention to specific image features do not cause models to attend to the same regions as humans tasked with VQA.", "startOffset": 3, "endOffset": 7}, {"referenceID": 46, "context": "They made this observation using both the attentive mechanisms used in [47] and [53].", "startOffset": 71, "endOffset": 75}, {"referenceID": 52, "context": "They made this observation using both the attentive mechanisms used in [47] and [53].", "startOffset": 80, "endOffset": 84}, {"referenceID": 43, "context": "Figure 11: Using the system in [44], the answer score for the question \u2018Are there any people in the picture?\u2019 is roughly the same for \u2018yes\u2019 (8.", "startOffset": 31, "endOffset": 35}, {"referenceID": 41, "context": "The SHAPES dataset [42] uses binary questions exclusively but contains complex questions involving spatial reasoning, counting, and drawing inferences (see Figure 6).", "startOffset": 19, "endOffset": 23}, {"referenceID": 38, "context": "Using cartoon images, [39] also showed that these questions can be especially difficult for VQA algorithms when the dataset is balanced.", "startOffset": 22, "endOffset": 26}, {"referenceID": 65, "context": "For example, in [66], they formulated VQA as an answer scoring task, where the system was trained to produce a score based on the image, question, and potential answers.", "startOffset": 16, "endOffset": 20}, {"referenceID": 67, "context": "Bias has long been a problem in images used for computer vision datasets (for a review see [68]), and for VQA this problem is compounded by bias in the questions as well.", "startOffset": 91, "endOffset": 95}], "year": 2016, "abstractText": "Visual Question Answering (VQA) is a recent problem in computer vision and natural language processing that has garnered a large amount of interest from the deep learning, computer vision, and natural language processing communities. In VQA, an algorithm needs to answer text-based questions about images. Since the release of the first VQA dataset in 2014, several additional datasets have been released and many algorithms have been proposed. In this review, we critically examine the current state of VQA in terms of problem formulation, existing datasets, evaluation metrics, and algorithms. In particular, we discuss the limitations of current datasets with regard to their ability to properly train and assess VQA algorithms. We then exhaustively review existing algorithms for VQA. Finally, we discuss possible future directions for VQA and image understanding research.", "creator": "LaTeX with hyperref package"}}}