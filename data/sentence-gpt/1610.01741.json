{"id": "1610.01741", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "Combining Generative and Discriminative Neural Networks for Sleep Stages Classification", "abstract": "Sleep stages pattern provides important clues in diagnosing the presence of sleep disorder. By analyzing sleep stages pattern and extracting its features from EEG, EOG, and EMG signals, we can classify sleep stages. This study presents a novel classification model for predicting sleep stages with a high accuracy for the diagnosis of sleep disorders. By analyzing sleep stages pattern and extracting its features from EEG, EOG, and EMG signals, we can identify sleep stages with high accuracy for the diagnosis of sleep disorders.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 6 Oct 2016 06:05:16 GMT  (471kb,D)", "http://arxiv.org/abs/1610.01741v1", "Submitted to Computational Intelligence and Neuroscience (Hindawi Publishing). 13 pages"]], "COMMENTS": "Submitted to Computational Intelligence and Neuroscience (Hindawi Publishing). 13 pages", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["endang purnama giri", "mohamad ivan fanany", "aniati murni arymurthy"], "accepted": false, "id": "1610.01741"}, "pdf": {"name": "1610.01741.pdf", "metadata": {"source": "CRF", "title": "Combining Generative and Discriminative Neural Networks for Sleep Stages Classification", "authors": ["Endang Purnama Giri", "Mohamad Ivan Fanany", "Aniati Murni Arymurthy"], "emails": ["epgthebest@gmail.com"], "sections": [{"heading": null, "text": "Sleep stages pattern provides important clues in diagnosing the presence of sleep disorder. By analyzing sleep stages pattern and extracting its features from EEG, EOG, and EMG signals, we can classify sleep stages. This study presents a novel classification model for predicting sleep stages with a high accuracy. The main idea is to combine the generative capability of Deep Belief Network (DBN) with a discriminative ability and sequence pattern recognizing capability of Long Short-term Memory (LSTM). We use DBN that is treated as an automatic higher level features generator. The input to DBN is 28 \u201dhandcrafted\u201d features as used in previous sleep stages studies. We compared our method with other techniques which combined DBN with Hidden Markov Model (HMM).In this study, we exploit the sequence or time series characteristics of sleep dataset. To the best of our knowledge, most of the present sleep analysis from polysomnogram relies only on single instanced label (nonsequence) for classification. In this study, we used two datasets: an open data set that is treated as a benchmark; the other dataset is our sleep stages dataset (available for download) to verify the results further. Our experiments showed that the combination of DBN with LSTM gives better overall accuracy 98.75% (Fscore=0.9875) for benchmark dataset and 98.94% (Fscore=0.9894) for MKG dataset. This result is better than the state of the art of sleep stages classification that was 91.31%i.\nKeywords\u2014 sleep stages classification, long short term memory, deep belief network, deep learning\nIntroduction\nThe increasing life pressures create more stress conditions which are not only felt when the person awake but also when they are sleeping. Decreasing sleep quality has a detrimental effect which leads to lower productivity and a higher increase in sleep disorder or sleep deficiency-related diseases. When a human fall asleep they can go through five sleep transitions or sleep stages that consist of wakefulness (W), sleep stages S1, S2, S3 or Rapid Eye Movement (REM) [8]. By monitoring their proportion and distribution, the sleep stages of a person\u2019s sleep and sleep disorder can be diagnosed [1]. Sleep stages study in [3] uses a feature that was extracted from ECG\n1/13\nar X\niv :1\n61 0.\n01 74\n1v 1\n[ cs\n.L G\n] 6\nO ct\nsignals and respiratory events analysis but most of the sleep stages study still use EEG, EOG, and EMG as a basis object to be extracted and classify the sleep stages.\nDeeper sleep pattern, as shown in Figure 1, is marked by the appearance of a slow wave (delta) on EEG. This condition followed by an increase in the amplitude value. However, when the deepest stages (REM) is reached, the amplitude decreased. A similar event also occurs in EMG signals. For EOG signals, the fluctuation of amplitude occurs when sleep stages getting deeper. The peak of amplitude change took place when the REM reached because, in that condition, the activity of eye movement will happen. Based on this phenomenon it makes sense that sleep stages identification can be done via analyzing the signals of EEG, EMG, and EOG. Nevertheless manually monitoring the three signals is not an easy task and certainly will not be practical. We need an automatic technique to classify sleep stages.\nSome study was conducted to design automatic techniques for sleep stage classification. In [4] various shallow classifiers were evaluated to classify sleep stages based on features that were used in [5]. Lankvist\u2019s study states that 28 features that were extracted from EEG, EMG, and EOG were sufficient enough and can be used to identify the stages of sleep. Two classifiers with the highest accuracy were obtained by Neural Network (70%) and SVM (64%), even though each classifier, in general, showed degradation in accuracy when the amount of data is increased. In our previous study [4], we found that it is potential to obtain performance improvement using the deep neural network techniques such as Deep Belief Network (DBN) and Long-Short Term of Memory (LSTM).\nThe architecture of Deep Learning (DL) technique can be categorized into three different models consist of discriminative, generative, and hybrid models. The discriminative models are an architecture that has direct ability to classify. The example of discriminative architecture, for instance, is CNN, recurrent neural network (RNN). On the other hand, even though not used to classify in a direct way, the generative model is very handy for the classification and prediction task, especially in the stages of data preparation such as initialization process and pre-training task for the training parameter. Some examples of generative models are deep belief networks (DBN) which are formed by stacked Restricted Boltzmann Machines (RBM). For the third model, hybrid model refers to the deep architecture that combines discriminative models and generative models.\nSleep stages classification can be viewed as a time series classification problem. Since the transition between each stage has a majority pattern (for normal people), previous information can be valuable to predict present task. In this study, we designed and\n2/13\ncombined deep learning techniques for sleep stages classification. Our architecture can be categorized into the hybrid model. Deep architectures used in this study are Deep Belief Network (DBN) and Long-Short Term Memory (LSTM). The main idea is to fuse generative ability on DBN to extract multi-level hierarchical features and determine the final label of class prediction using the time series discrimination capability of LSTM. We choose LSTM because the LSTM models have the ability to recognize the patterns from the sequence of events. On the other hand as mention before, sleep stages classification problem can be viewed as time series and sequence classification problem. Our primary goal of this study is to measure and evaluate the performance of deep hybrid architecture and compare to the state of the art performance result of the sleep stages classification problem.\nThe state of the art sleep stages analysis is as follows. In [10], using two-stage classification, i.e., artifact identification and selection and extraction feature, achieved 85.6 % accuracy. Later, in 2015 [6] increased the accuracy level of the state the art to 90% while utilizing EOG channel and using k-nearest neighbor (KNN) as the classifier. Still in 2015, using deep belief net and the combination of multiple classifiers [9] the total accuracy of state the art increased to 91.31% (The detailed accuracy for each class are: Wake 98.49%, S1 80.05%, S2 91.2%, SWS 98.22 % and REM 95.31%). The most difficult class to predict is S1.\nThis paper is organized as follow: Section 1 is an introduction, Section 2 Two deep architectures that use in our model, Section 3 describe data, feature extraction, and data transformation process. Section 4 gives a description of the proposed method, Section 5 presents the result of the experiment, and Section 6 is the conclusion."}, {"heading": "1 Proposed Method", "text": "This section describes our proposed method compared to previous methods. Scheme for each method given by Figure 2."}, {"heading": "1.1 Architecture of proposed method", "text": "As previously mentioned, our goal in this study is to propose and evaluate a new hybrid deep architecture classification model for automatic sleep stages classification problem. To reach this goal we combine excellence generative capability of DBN and smart time sequence discriminative capability of LSTM. The model began with DBN and finalized by LSTM. The 28 data features as an input of DBN will be pretrained using DBN. After pretrained with DBN complete, the output of this stage will become as an input for the LSTM. At the end using LSTM the class label will be predict.\n1. DBN model architecture Input neuron for DBN architecture is 28 neurons. For the pretrained process, we use two layers DBN with some neuron for the visible unit is 200 neurons, and for hidden unit also 200 neurons. Batch training size for each RBM layer and DBN layer is similar in size 1000 data sample. For the output layer of DBN, we use five neurons.\n2. LSTM model architecture Our architecture of LSTM have three stack layers of LSTM. first layer of LSTM have input shape 5 previous data (5 sequence of inputs) with 5 feature for each data. The output for first layer of LSTM is 128 sequence output. The second LSTM layer in our architecture have input shape 5 sequence with size 128 value as a result from first layer LSTM. As the output from second LSTM layer is 64 sequence output. For third layer of LSTM have input shape five sequences with size 32 as a result from second layer of LSTM. As final layer use softmax in order to enable multiclass classification capability. For\n3/13\nloss function we use categorical crossentropy and as optimizer function we use rmsprop. For setting up parameter we use number of epoch 100, and batch size train process is 500 data."}, {"heading": "1.2 Compared Techniques", "text": "To evaluate the classification accuracy obtained by DBN+LSTM, we compared it with some other hybrid learning schemes. In this study, we compare DBN+LSTM with the accuracy and F-score of only DBN, only LSTM, and DBN with Hidden Markov Model. We evaluate HMM to compare our proposed technique to the resulting study in [5]. To obtain optimal value of previous sequence data for LSTM we use three variations of input sequence size 5, 10, and 15."}, {"heading": "2 Experiment Setup", "text": "The experiment divides into two sections. On the first experiment, we use benchmark dataset for the analysis and for the second experiment we use MKG dataset. All of the experiment use Feature extraction module DBN module, and HMM module that implement on Matlab 15.a by [5]. The environment for running this three modules are Windows 8 operating system on a computer with Intel i7-4700HQ 2.6 GHz clock CPU and 4 GB main memory. On the other hand LSTM, module implement on KERAS of python library [2] with the tensor-flow backend. The environment for running LSTM module is Ubuntu 14.04 operating system on a computer with 4xNVidia GTX Titan and 128 GB main memory.\n1. First Experiment: The data is five-night sleep recording benchmark dataset. Use leave one out cross validation we will evaluate each technique in five round. For each round validation one-night sleep data use as test data, and four others used to construct the model. For each 30 seconds before and after switch label class on data that used to build the model removed. The data that used to build the model will divide to the train data and validation data. The proportion of train data and validation data is 5:1. The data that construct the model chosen with consideration the balance of proportion from each class. On this experiment, we will compare four models DBN only, LSTM only, DBN+HMM, and DBN+LSTM. To get an optimal size of the input sequence for LSTM, we try\n4/13"}, {"heading": "3 Result", "text": "This section is divided into three subsections: result from first experiment, result from second experiment, and running time analysis."}, {"heading": "3.1 First Experiment Result", "text": "The model evaluated on this experiment are \u2019only DBN\u2019 (DBN), \u2019DBN+HMM\u2019, \u2019only LSTM\u2019 (LSTM), and \u2019DBN+LSTM\u2019. For the model with LSTM module used three different sizes of sequence input (5, 10, and 15). Figure 3 shows the result of average accuracy for each round (fold) use LSTM model. Evaluation measurement was repeated ten times. The best accuracy for this assessment is 0.73 when the fold four as a data test. On general assessment sequence size of input 15 is better than 5 and 10. We also tried the input sequence size equal to 20, but the accuracy is lower than input sequence 15. From this result, we can conclude that size of input sequence 15 is optimal for LSTM model. From the LSTM model the average accuracy for overall benchmark data\n5/13\n6/13\nset for input sequence size 5, 10, and 15 are 0.599, 0.636, and 0.649 respectively. On the other hand from Figure 4 shows the accuracy level for each fold using DBN+LSTM model. Excellent result obtains from all over fold (higher than 0.98). Use three variants sizes of input sequence on the average we get the best result (0.988) precisely when the input sequence size is 5. Table 1 shows accuracy and F-score from LSTM and DBN+LSTM for each input sequence size.\nBased on the result of our experiment the accuracy level between all model (DBN, LSTM 15Seq, DBN+HMM, and DBN+LSTM 5Seq) we will get the best accuracy is from DBN+LSTM 5Seq (Figure 5). The average of accuracy level for overall data use model DBN+ LSTM is 0.988. On the other hand, the average of accuracy level for overall data from DBN+HMM is only 0.723. When the model only uses DBN, the average of accuracy for overall data is 0.515. This accuracy is the lowest result. From the model only LSTM the accuracy is 0.636 and F-score value 0.655. Order by the performance of accuracy level (Table 2) from the best model are DBN+LSTM, DBN+HMM, LSTM, and for the last is DBN. If we look the performance level based on the value of F-score, the order of level performance will get the same way.\nCompare to the result of the state of the art problem on [9] with value 91.31% the accuracy value from DBN+LSTM get a better result. From it, we can conclude DBN successfully boost the accuracy level for LSTM, because when the LSTM not using DBN, we only get the accuracy value on 0.636 (F-score 0.655). On the other hand, if we use only DBN the accuracy just 0.515. In more detail comparison between our methods to the state of the art, it has similarities and some differences. The similarities are both of methods using DBN for the early step before discriminative task to be performed. On the other hand, the differences are: we have feature extraction stage otherwise in [9] the approach directly process to the raw data, and for the classifier we use single classifier (LSTM) in the final stage otherwise in [9] use multiple classifiers to perform the discriminative process.\nThe hardest sleep stage class to predict - focus on to the top two model (DBN+HMM and DBN+LSTM). From confusion matrix that given in Table 3 and Table 4 show that the hardest class to predict when DBN+HMM use is the S1 class (true prediction only 42.97%). On the other hand use DBN+LSTM model the most challenging class to predict is S2, even though the accuracy is still good enough (97.69%). Use DBN+LSTM the level of success prediction from the biggest level of success are\n7/13\nREM (99.63%), WAKE (98.64%), SWS (98.27%), S1 (98.24%), and S2 (97.69%)."}, {"heading": "3.2 Second Experiment Result", "text": "Based on the result from the first experiment for this article we will focus on to the top two level of accuracy and F-Score (DBN+HMM and DBN+LSTM). The input sequence size for LSTM is 5, the best result on first experiment.\nUse MKG dataset we obtain the result of accuracy for each model as given in Table 5 and Figure 6. From the second when data MKG was used, we obtain almost similarity result. DBN+LSTM get an excellent result of accuracy, the delta between accuracy from DBN+LSTM to the accuracy from DBN+HMM is about 0.36; this value is significant enough. If we look at the variation of accuracy for each fold, the accuracy value from DBN+HMM are between 0.2011 (fold 8) to the biggest value 0.7654 (fold 3) the domain range of accuracy value is quite significant. It is indicated that stability and robustness of adaptability from the model is not too good. On the other hand, the range value of accuracy from DBN+LSTM are in between 0.9859 (fold 3) to the 0.9929 (fold 8). It was interesting phenomena for fold 8 when DBN+HMM model gets the worst value of accuracy vice versa DBN+LSTM model gets the best value of accuracy. Figure 7 shows the plot prediction class (blue) versus plot of actual class labels for each fold. The order for images is, from the top left (fold 1) to the right (fold 2) and to the bottom. Fold 8 is the right picture on the third row. If we look on early time on fold eight it is the most different from others fold, only fold eight at the early time have transition class between 2 \u2019S2\u2019 and 3 \u2019S3\u2019. From this situation, DBN+HMM fail to adapt, but otherwise, DBN+LSTM still obtain a good result. On this case, LSTM intelligently smarter than HMM to utilize sequence in between data.\nFurthermore, in more detail refer to Table 6 and Table 7 class label with the most difficult to predict is S1 for DBN+HMM. On the other hand, the most difficult to predict for DBN+LSTM is REM. On DBN+HMM the most incorrect prediction for class S1 is mapped to the class WAKE (27.587%) vice versa wrong prediction for WAKE(19.512%) on DBN+HMM map to the S1. It shows that DBN+HMM difficult to distinguish between S1 and WAKE. The transition between both of stages (WAKE and S1) frequently occurs at the early time of recording time. We can conclude at the early of sleep time DBN+LSTM smarter than DBN+HMM. Refer to Table 7 level of success prediction from DBN+LSTM for each class is greater than 98%, this achievement is an\n8/13\nexcellent result. Even though not shown in this paper for \u2019only DBN\u2019 model and \u2019only LSTM model\u2019, from the experiment we get the same result with first experiment if we use MKG dataset. On the overall, the rank order of the best performance is DBN+LSTM, DBN+HMM, LSTM, and DBN."}, {"heading": "3.3 Running Time", "text": "How about the running time? From experiment, we measure the biggest computation time is when the task of pretraining DBN run. For about number of train, data is 32,500, and validation data is about 6,500 we need computation time in between 42 minutes until 45 minutes. So when we run for overall benchmark data set the computation time is about almost four hours and for MKG data is about 17 Hours. On the other hand, the computation time for LSTM process is very fast this has happened because of we run this task on tensor flow Python library that supports parallel GPU processing. Use ten different sizes of number data (from 2,000 until 20,000) and three different sizes of input sequence (5, 10, and 15); we perform five-time measurements for each scenario. Figure 8 and Figure 9 show the result measurement on the average. From the chart, we can look that training time for 20,000 data only needs about ten minutes\n10/13\nfor sequence input size 5, 13 minutes for sequence input size 10, and about 14 minutes, for input sequence size 15. Furthermore, for the testing process, we get very fast execution time. Use 20,000 data and input sequence size = 15 we only need about 16 seconds. From the experiment result, the best model is DBN+LSTM with input sequence size = 5 so that we can believe the testing computation from the best model will become not need a lot of resources of computations."}, {"heading": "4 Conclusion", "text": "This paper shows that combined capability of the generative architecture of DBN (through unsupervised pre-training) and superior discriminative power of LSTM for sequence data can attain a high level of classification accuracy on sleep stages classification problem. In this study, we exploit the sequence or time series characteristics of sleep dataset. To the best of our knowledge, most of the present sleep analysis from polysomnogram relies only on single instanced label (nonsequence) for classification. From our experiment, we get better performance of accuracy for our model architecture compared with the state of the art in [9]. The state of the art accuracy for sleep stages classification is 91.31%, whereas our proposed architecture model can get the best accuracy about 98.75% use benchmark dataset and 98.94% use our MKG dataset. Analyzing the prediction mapping of our system, in general, we found that the sleep stage S1 and S2 are the hardest classes to distinguish, due to the similarity of EOG and EMG waveforms characteristics. On the other hand, for the second experiment using MKG dataset, the hardest pair of class to distinguish are between WAKE and S1.\nEven though we obtained good results with high accuracy (better than the state of the art), but our model still needs input in the form of \u201dhandcrafted\u201d feature extraction procedure. On the other hand, the state of the art directly using raw data so that the flexibility of state of the art may be better than our proposed DBN+LSTM model. For our future study, we will test whether it is possible to classify sleep stages from raw data with direct multi-channel observation approach using DBN and LSTM.\n11/13"}, {"heading": "5 Acknowledgment", "text": "This work is supported by Higher Education Center of Excellence Research Grant funded Indonesia Ministry of Research and Higher Education Contract No. 1068/UN2.R12/ HKP.05. 00/2016"}, {"heading": "6 Conflict of Interests", "text": "The authors declare that there is no conflict of interest regarding the publication of this paper."}], "references": [{"title": "Monitoring and staging human sleep", "author": ["M.A. Carskadon", "A. Rechtschaffen"], "venue": "Principles and practice of sleep medicine, Saunders Elsevier, 4:265,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Sleep stage classification with ECG and respiratory effort", "author": ["P. Fonseca", "X. Long", "M. Radha", "R. Haakma", "R.M. Aarts", "J. Rolink"], "venue": "Physiol Meas.10.1088/0967-3334/36/10/2027. Epub 2015 Aug 19.PMID: 26289580, 36, issue10:1\u201315,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Sleep Stages Classification using Shallow Classifier", "author": ["E.P. Giri", "M. Fanany", "A.M. Arymurthy"], "venue": "International Conference on Advanced Computer Science and Information Systems ICACSIS, Proceeding of the IEEE Int Conf on Computer Sciences, pages 297 \u2013 301,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Sleep Stage Classification Using Unsupervised Feature Learning", "author": ["M. Langkvist", "L. Karlsson", "A. Loutfi"], "venue": "Hindawi Publishing Corporation, Advances in Artificial Neural Systems, 2012, Article ID 107046:9 Pages,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatic sleep stage detection and classification: Distinguishing between patients with periodic limb movements, sleep apnea hypopnea syndrome, and healthy controls using electrooculography (eog) signals", "author": ["E. Malaekah", "S. Shahrbabaki", "D. Cvetkovic"], "venue": "Journal of Bioprocessing & Biotechniques, 14:1\u20136,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "A transition-constrained discrete hidden Markov model for automatic sleep staging", "author": ["S.-T. Pan", "C.-E. Kuo", "J.-H. Zeng", "S.-F. Liang"], "venue": "BioMedical Engineering OnLine 2012, pages 1\u201319,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "A manual of standardized terminology, techniques and scoring system for sleep stages of human subjects", "author": ["A. Rechtschaffen", "A. Kales"], "venue": "National Institutes of Health Publications, US Government Printing Office, 204:54,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1968}, {"title": "Automatic sleep stage classification based on sparse deep belief net and combination of multiple classifiers", "author": ["J. Zhang", "Y. Wu", "J. Bai", "F. Chen"], "venue": "Sage Journals, 14:1\u20139,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "and A", "author": ["L. Zoubek", "S. Charbonnier", "S. Lesecq"], "venue": "B. anf Florian Chapotot. A two-steps sleep/wake stages classifier taking into account artefacts in the polysomnographic signals. Proceedings of the 17th World Congress The International Federation of Automatic Control,Seoul, Korea, July 6-11, 2008, 2012, Article ID 107046,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 7, "context": "When a human fall asleep they can go through five sleep transitions or sleep stages that consist of wakefulness (W), sleep stages S1, S2, S3 or Rapid Eye Movement (REM) [8].", "startOffset": 169, "endOffset": 172}, {"referenceID": 0, "context": "By monitoring their proportion and distribution, the sleep stages of a person\u2019s sleep and sleep disorder can be diagnosed [1].", "startOffset": 122, "endOffset": 125}, {"referenceID": 2, "context": "Sleep stages study in [3] uses a feature that was extracted from ECG", "startOffset": 22, "endOffset": 25}, {"referenceID": 6, "context": "Typical polysomnographic recordings for each sleep stages slow-wave sleep (SWS/S3) associated with deep sleep, Each raw signal shows the electroencephalography (EEG), electrooculography (EOG) and electromyography (EMG) (source [7]).", "startOffset": 227, "endOffset": 230}, {"referenceID": 3, "context": "In [4] various shallow classifiers were evaluated to classify sleep stages based on features that were used in [5].", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "In [4] various shallow classifiers were evaluated to classify sleep stages based on features that were used in [5].", "startOffset": 111, "endOffset": 114}, {"referenceID": 3, "context": "In our previous study [4], we found that it is potential to obtain performance improvement using the deep neural network techniques such as Deep Belief Network (DBN) and Long-Short Term of Memory (LSTM).", "startOffset": 22, "endOffset": 25}, {"referenceID": 9, "context": "In [10], using two-stage classification, i.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "Later, in 2015 [6] increased the accuracy level of the state the art to 90% while utilizing EOG channel and using k-nearest neighbor (KNN) as the classifier.", "startOffset": 15, "endOffset": 18}, {"referenceID": 8, "context": "Still in 2015, using deep belief net and the combination of multiple classifiers [9] the total accuracy of state the art increased to 91.", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "We evaluate HMM to compare our proposed technique to the resulting study in [5].", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "a by [5].", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "On the other hand LSTM, module implement on KERAS of python library [2] with the tensor-flow backend.", "startOffset": 68, "endOffset": 71}, {"referenceID": 8, "context": "Compare to the result of the state of the art problem on [9] with value 91.", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "On the other hand, the differences are: we have feature extraction stage otherwise in [9] the approach directly process to the raw data, and for the classifier we use single classifier (LSTM) in the final stage otherwise in [9] use multiple classifiers to perform the discriminative process.", "startOffset": 86, "endOffset": 89}, {"referenceID": 8, "context": "On the other hand, the differences are: we have feature extraction stage otherwise in [9] the approach directly process to the raw data, and for the classifier we use single classifier (LSTM) in the final stage otherwise in [9] use multiple classifiers to perform the discriminative process.", "startOffset": 224, "endOffset": 227}, {"referenceID": 8, "context": "From our experiment, we get better performance of accuracy for our model architecture compared with the state of the art in [9].", "startOffset": 124, "endOffset": 127}], "year": 2016, "abstractText": "Sleep stages pattern provides important clues in diagnosing the presence of sleep disorder. By analyzing sleep stages pattern and extracting its features from EEG, EOG, and EMG signals, we can classify sleep stages. This study presents a novel classification model for predicting sleep stages with a high accuracy. The main idea is to combine the generative capability of Deep Belief Network (DBN) with a discriminative ability and sequence pattern recognizing capability of Long Short-term Memory (LSTM). We use DBN that is treated as an automatic higher level features generator. The input to DBN is 28 \u201dhandcrafted\u201d features as used in previous sleep stages studies. We compared our method with other techniques which combined DBN with Hidden Markov Model (HMM).In this study, we exploit the sequence or time series characteristics of sleep dataset. To the best of our knowledge, most of the present sleep analysis from polysomnogram relies only on single instanced label (nonsequence) for classification. In this study, we used two datasets: an open data set that is treated as a benchmark; the other dataset is our sleep stages dataset (available for download) to verify the results further. Our experiments showed that the combination of DBN with LSTM gives better overall accuracy 98.75% (Fscore=0.9875) for benchmark dataset and 98.94% (Fscore=0.9894) for MKG dataset. This result is better than the state of the art of sleep stages classification that was 91.31%i. Keywords\u2014 sleep stages classification, long short term memory, deep belief network, deep learning", "creator": "LaTeX with hyperref package"}}}