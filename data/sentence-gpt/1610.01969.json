{"id": "1610.01969", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "DeepDGA: Adversarially-Tuned Domain Generation and Detection", "abstract": "Many malware families utilize domain generation algorithms (DGAs) to establish command and control (C&amp;C) connections. While there are many methods to pseudorandomly generate domains, we focus in this paper on detecting (and generating) domains on a per-domain basis which provides a simple and flexible means to detect known DGA families. Recent machine learning approaches to DGA detection have been successful on fairly simplistic DGAs, many of which produce names of fixed length. However, models trained on limited datasets are somewhat blind to new DGA variants. Instead of generating complex names of specific DGA families, we are interested in using the model that incorporates multiple different domains, including domain names and a network that has been observed for over 500 years. We use the model in the current paper. In this paper, we present the model of dynamic domain name detection (DGAs) for DGA domains with only a single domain. The model is a representation of the domain names of an individual entity, an individual domain, and the local domain names of a specific network, with each domain being determined from the domain name to an actual network. We show that domain names from a specific network are distributed among different domains in an interlinked network. We demonstrate that dynamic domain names provide the same information as traditional DGA domains. We provide a model for distributed DNS addresses which we show is robust in this paper. We use the model in the current paper. In a recent paper, we provide a model for the domain name generation and dynamic domain names of a specific network using a network that has been observed for over a 500 years. For example, a user might create an account that has a website with the domain name of an individual user. This network consists of a network containing two domain names of a particular IP address, one domain, and two domains. For each domain, the model is composed of one domain. All domain names and other domain names are expressed as domain names (e.g., name, username, userspace), and each domain is named a domain (e.g., name, username, username). In each domain, the model is composed of one domain. All domain names and other domain names are expressed as domain names (e.g., name, username, username). In each domain, the model is composed of one domain. All domain names and other domain names are expressed as domain names (e.g., name, username, username). In each domain, the model is composed", "histories": [["v1", "Thu, 6 Oct 2016 17:50:27 GMT  (453kb,D)", "http://arxiv.org/abs/1610.01969v1", null]], "reviews": [], "SUBJECTS": "cs.CR cs.AI", "authors": ["hyrum s", "erson", "jonathan woodbridge", "bobby filar"], "accepted": false, "id": "1610.01969"}, "pdf": {"name": "1610.01969.pdf", "metadata": {"source": "META", "title": "DeepDGA: Adversarially-Tuned Domain Generation and Detection", "authors": ["Hyrum S. Anderson", "Jonathan Woodbridge"], "emails": ["hyrum@endgame.com", "jwoodbridge@endgame.com", "bfilar@endgame.com"], "sections": [{"heading": null, "text": "In this paper, we leverage the concept of generative adversarial networks to construct a deep learning based DGA that is designed to intentionally bypass a deep learning based detector. In a series of adversarial rounds, the generator learns to generate domain names that are increasingly more difficult to detect. In turn, a detector model updates its parameters to compensate for the adversarially generated domains. We test the hypothesis of whether adversarially generated domains may be used to augment training sets in order to harden other machine learning models against yet-to-be-observed DGAs. We detail solutions to several challenges in training this character-based generative adversarial network (GAN). In particular, our deep learning architecture begins as a domain name auto-encoder (encoder + decoder) trained on domains in the Alexa one million. Then the encoder and decoder are reassembled competitively in a generative adversarial network (detector + generator), with novel neural architectures and training strategies to improve convergence.\nResults show that domains generated from a GAN to bypass the GAN\u2019s detector also bypass a random forest classifier that leverages hand-crafted features. Conversely, by augmenting the training set with these adversarial examples, the random forest classifier is able to detect with greater efficacy DGA malware families not seen during training.\nI. INTRODUCTION\nLike any defensive technology, machine learning models are subject to false positives and false negatives. An important step in delivering a model as part of a product consists of assessing and (if possible) patching model vulnerabilities (e.g., certain malware families for a malware detector, etc.). While this expert-guided process may always exist, we test a key thesis of [1]: adversarial examples1\u2014artificial samples which a machine learning model misidentifies\u2014can be discovered automatically and used to augment a training dataset to harden\n1In this paper, we employ a significantly broader meaning of adversarial examples as originally used in [2], [1]. In our work, they are not restricted to small perterbations to existing samples, but generally denote artificial samples that \u201cappear realistic\u201d to confound either human or model or both.\n(i.e., make more robust) machine learning models. While this thesis has been confirmed generally [2], [1], we propose a novel framework for generating these adversarial examples using a generative adversarial network for a natural languagebased domain generation algorithm (DGA) detector.\nDGAs are employed by many malware families to make preemptive command-and-control (C&C) countermeasures difficult. Using DGAs, a malware sample may generate hundreds to tens-of-thousands of domain names daily. The domains are generated pseudo-randomly using a seed that is shared by both the malware and the threat actor, so that the threat actor knows a priori the sequence of connection attempts by the malware. This represents an asymmetric attack since the defender must sinkhole, pre-register or blacklist all of the domains to prevent the C&C connection, while the malware need only connect to a single domain that has been registered by the threat actor.\nThere are a variety of strategies to detect domain names that are produced by a DGA. Previous works include using a Hidden Markov Model (HMM) framework to model the generating distribution of several DGA families as well as \u201cnormal\u201c domains [3], and making bulk predictions on large sets of domains using clustering as a filtering technique [4], [5]. Many methods require contextual information separate from the domain name itself [6]. However, in this paper we restrict our focus to machine learning models that distinguish DGA domains from \u201cnormal\u201d domains based solely on the domain name, without contextual information.\nThis paper explores the use of a generative adversarial network (GAN) to pseudo-randomly produce domain names that are difficult for modern DGA classifiers to detect. The proposed technique generates domains on a character-bycharacter basis and greatly exceeds the stealth of typical DGA techniques which use simpler algorithms to draw random characters to compose novel domain names. In contrast to simpler DGAs, we propose a DGA architecture that provides a more direct objective: optimize psuedo-random generation of domain names that are by construction difficult for a DGA classifier to detect. This explicit optimization is done using a DGA language model generator coupled with a DGA detector in a GAN [7]. A similar adversarial approach was previously explored by [8] to generate synthetic samples for malware classification based on the DREBIN Android malware dataset. The authors successfully retrained a binary classifier using adversarially crafted input to harden the original classifier. ar X\niv :1\n61 0.\n01 96\n9v 1\n[ cs\n.C R\n] 6\nO ct\n2 01\n6\nIn turn, we show increased robustness of a DGA detector to never-before-seen DGA families when trained on an augmented dataset that includes adversarial examples.\nContributions of this paper include the following: \u2022 We present the first known use of a deep learning archi-\ntecture to pseudo-randomly generate domain names that are by construction difficult for a classifier to distinguish from real domain names. \u2022 We demonstrate that adversarially-generated domain names discovered for a deep learning model are also adversarial to a totally different model architecture (random forest with human-engineered features). \u2022 We demonstrate experimentally that the same adversarial examples can be used to harden the random forest classifier to never-before-seen DGA families."}, {"heading": "II. BACKGROUND", "text": "We first provide a brief review in this section of common DGA algorithms, machine learning detection approaches, and follow with background on neural network architectures that we employ in our neural language model for domain generation."}, {"heading": "A. Domain Generation Algorithms", "text": "Domain generation algorithms are used by many strains of malware for C&C, including ransomware like cryptolocker [9], [10] and cryptowall [11], banking trojans, such as hesperbot [12], and information stealers, such as ramnit [13]. In part, this paper compares DeepDGA to character-based DGA algorithms reproduced from published literature.\nTraditional DGA techniques vary in complexity from simple approaches that draw characters uniformly at random, to those that attempt to mimic character or word distributions found in real domains. The ramnit DGA, for example, creates domain names using a combination of multiplies, divides and modulos starting from a random seed [13]. On the other hand, suppobox creates domains by concatenating two pseudorandomly chosen English dictionary words [14].\nSome example domains from each of the families we consider in this paper are shown in Table I, which are all character-level DGA algorithms. Other common DGAs, like beebone have a rigid structure, producing domains like ns1.backdates13.biz and ns1.backdates0.biz. The symmi DGA produces nearly-pronounceable domain names like hakueshoubar.ddns.net by drawing a random vowel or a random consonant at each even-numbered index, drawing a random character of the opposite class (vowel/consonant) in the subsequent index location, and appending a second and top-level string like .ddns.net.\nThe unigram distributions for four DGA families and Alexa are shown in Fig. 1. The distributions for cryptolocker and ramnit are both nearly uniform over the same range. This is expected as they are both generated using a series of multiplies, divisions and modulos based on a single seed [13], [10]. On the other hand, suppobox is interesting as it\ngenerates unigrams similar to distributions seen by the Alexa top one million domains. The suppobox DGA constructs domain names by concatenating multiple randomly chosen words from the English dictionary, and thus follows a similar character distribution to the Alexa top one million. In this paper, we demonstrate a character-based generator composed of a deep learning model that also mimics the distribution of Alexa domain names."}, {"heading": "B. DGA detection algorithms", "text": "Previous works in DGA classification approach the problem by either classifying domains in groups to take advantage of bulk statistical properties or common contextual information; or by classifying domains individually with no additional contextual information. We briefly discuss examples of both here, but note that our work falls in the latter category, which may be used in concert with other approaches for DGA detection.\nAuthors in [4], [5] detect DGAs by using both unigram and bigram statistics of domain clusters. A training set is separated into two subsets: those generated by a DGA and those not generated by a DGA. The distributions of both unigrams and bigrams are calculated for both the subsets. Classification occurs in batches. Each batch of unknown domains (DNS responds with NXDOMAIN) is clustered by shared second level domain and domains sharing the same IP address. The unigram and bigram distributions are calculated for each cluster and compared to the two known (labeled) subsets using the Kullback-Leibler (KL) distance. In addition, the authors use the Jaccard distance to compare bigrams between clusters and the known (labeled) sets as well.\nAuthors in [3] apply a similar clustering process to classify domains with unsuccessful DNS resolutions. To train, statistical features are calculated for each subset of labeled DGA generated domains, such as Bobax, Torpig, and Conficker.C. Unknown domains are clustered by statistical characteristics such as length, entropy, and character frequency distribution as well as shared hosts requesting the domain (i.e., cluster two domains together if the same host made a DNS query for both domains). Next, statistical features are calculated for each cluster and compared to the training subsets to classify the clusters as formed by a known DGA. If a cluster is classified as belonging to a known DGA, the host is deemed to be infected.\nOnce a host is deemed to be infected with a DGA-bot, the authors attempt to identify the bot\u2019s active C2 server. This stage of the process uses a Hidden Markov Model trained on each known family of DGA and applied to single domains (i.e., this technique follows the same assumptions as the LSTM technique proposed by this paper). Each domain with a successful DNS request is fed through each HMM. If a domain receives an adequate score (i.e., greater than some threshold \u03b8), the domain is labeled as a DGA. The threshold is learned at training time and set to a maximum false positive rate of 1%. We use this HMM technique as one of our comparisons to previous work.\nAuthors in [15] also present a DGA classifier with the intention of classifying individual domains. This classifier uses two basic linguistic features named meaningful characters ratio and n-gram normality score. The meaningful characters ratio calculates the ratio of characters in a domain that comprise of a meaningful word. For example, endgame has a ratio of 1 as all characters in the domain are covered by the words end and game while game1234 has a ratio of 0.5 as only\nhalf of its characters are covered by the word game. The ngram normality score is calculated by finding n-grams with n \u2208 1, 2, 3 within a domain and calculating their count in the English language. The mean and covariance of these four features are calculated from a benign set (Alexa top 100,000). Unknown domains are then classified by their Mahalanobis distance to the benign set (i.e. a larger distance is indicative of a DGA generated domain). The entire approach is used as a filtering step. Once domains have been classified as a DGA they are fed to a clustering technique (similar to those described above) to further classify the domains.\nIn our experiments, we leverage a random forest classifier trained on features defined in [3], [4], [5], [15]. We do not implement the full system as defined in [3], [4], [5] as it is based on domain clustering and our intent is to classify DGAs on a per-domain basis. The full system in [15] is not evaluated as they use contextual features such as IP addresses. We assume no contextual information in our experiments, but note that adding contextual information may generally improve a model\u2019s ability to detect DGAs."}, {"heading": "C. Adversarial Examples and Generative Adversarial Networks", "text": "Previous work discovered that many machine learning models, including modern neural network architectures, are vulnerable to adversarial examples[2], [1]. Notably, [1] introduced the fast gradient sign method to systematically discover adversarial examples by perturbing a known \u201cgood\u201d sample x by a small amount \u2206x = sign (\u2207xJ (\u03b8,x, y)), where \u03b8 represents the model parameters, and J is the cost incurred for classifying x as class y.\nSeparately, [7] proposed generative adversarial networks as a framework for generating artificial samples that are drawn from the same distribution as the training dataset. Generative adversarial networks incorporate a pair of models\u2014a generator and a discriminator\u2014that compete against each other in a series of adversarial rounds. In the context of our application, the generator learns to create new artificial domain names, and the detector subsequently learns to distinguish the generator\u2019s artificial domains from the true domain data distribution.\nPrevious works apply adversarial examples and GANs to natural images. In this work, we somewhat conflate the use of GANs with the intent of adversarial examples, using a GAN to produce artificial domains and subsequently harden a natural language DGA detector via adversarial training."}, {"heading": "D. Recurrent Neural Network", "text": "In a variety of natural language tasks, recurrent neural networks (RNNs) have been used to capture meaningful temporal relationships among tokens in a sequence [16], [17], [18], [19]. The key benefit of RNNs is that they incorporate contextual (state) information in their mapping from input to output. That is, the output of a single RNN cell is a function of the input layer and previous RNN activations. Due to long chains of operations that are introduced by including self-recurrent connections, the output of a traditional\nRNN may decay exponentially over time (or, more rarely but catastrophically explode) for a given input, leading to the well-known vanishing gradients problem. This makes learning long-term dependencies in an RNN difficult to achieve.\nThe problem of vanishing gradients is a key motivation behind the application of the Long Short-Term Memory (LSTM) cell [20], [21], [22], which consists of a state that can be read, written or reset via a set of programmable gates. In the following we consider a layer of LSTM cells using vector notation (boldface), and denote the time index where necessary with subscript t. Superscripted W and U correspond to particular weight matrices on the input x or emission h, respectively, and superscripted b denotes a particular bias vector.\nLSTM cells\u2019 states c have self-recurrent connections that allow each cell to retain state between time steps:\nct = f \u00b7 ct\u22121 + it \u00b7 gt,\nwhere \u00b7 denotes elementwise (Hadamard) multiplication. However, states may be updated in an additive manner by state updates\ngt = tanh (W gxt + U ght\u22121 + b g)\nvia input gates i, which effectively multiply the state update to each cell by a number that ranges between 0 and 1. Likewise, forget gates f modulate the self-recurrent state connection to each cell\u2019s state by a number between 0 and 1. Thus, if the input gate modulates the state update with 0, and the forget gate modulates the recurrent connection with 1, the cell ignores the input and perfectly retains state. On the other hand, a 1 (input) and a 0 (forget) causes a cell\u2019s state to be overwritten by the input. And in the case of a 0 (input) and 0 (forget), the state is reset to 0. Finally, output gates o modulate the contribution of each cell\u2019s states to the cell\u2019s emission (output) as\nht = ot \u00b7 tanh (ct) ,\nwhich propagate to the input gates of LSTM cells across the layer, as well as to subsequent layers of the network. In particular, the input, forget, and output gates are defined as functions of the input xt at time t and previous LSTM layer emission ht at time t, respectively, as\nit = \u03c3 ( Wixt + U iht\u22121 + b i ) ft = \u03c3 ( Wfxt + U fht\u22121 + b f )\not = \u03c3 (W oxt + U oht\u22121 + b o) .\nThe LSTM cell\u2019s design with multiplicative gates allows a network to store and access state over long sequences, thereby mitigating the vanishing gradients problem. For our use with domain names, the state space is intended to capture combinations of tokens that are important to modeling domain names."}, {"heading": "E. Highway Networks", "text": "Highway networks were recently proposed as a natural extension of gated memory networks like the LSTM unit to feedforward networks [23]. Highway layers allow for training deep networks by adaptively carrying some dimensions of the input directly to the output through the use of gates. Concretely, the output y of a single highway layers is the elementwise convex combination of the raw input x and the transformed input g(Wx + b) with a vector parameter t \u2208 [0, 1]d:\ny = t \u00b7 g(Wx + b) + (1\u2212 t) \u00b7 x, with activation function g. In addition to learning the weights W and bias b, a highway layer also learns the gating parameters t during training."}, {"heading": "III. METHOD", "text": "In this section we describe our DGA neural language architecture and training mechanism. In a first step, we learn to represent valid domain names using an autoencoder architecture, shown in Figure 2(a) and detailed in Section III-A. We then repurpose the encoder (which accepts a domain name and outputs a domain embedding) as a discriminative model, and the decoder (which accepts a domain embedding and outputs a domain name) as a generative model, as show in Figure 2(b) and detailed in Section III-B."}, {"heading": "A. Autoencoder", "text": "An autoencoder is a type of data representation model that consists of an encoder that transforms an input to a (usually) lower-dimensional representation, and a decoder which aims to reproduce the original input from the low-dimensional embedding. The character-level encoder shown in Figure 2(a) is loosely inspired by the neural language framework of [24], and the decoder is loosely a mirror image of the encoder.\nIn what follows, let V denote the set of lowercase valid domain characters. The encoder contains an embedding layer which learns a linear mapping from V 7\u2192 Rd, resulting in a d-dimensional vector for each valid domain character. We use d = 20 < |V| to keep the model size small, and because we don\u2019t need to perfectly reproduce the domain characters. Small convolution filters are applied to the name embeddings, which aim to capture simple character combinations present in valid domains. In our implementation, we utilize 20 filters of length 2 (bigrams) and 10 filters of length 3 (trigrams). The next layer selects important features from the convolutional filters via maxpooling and concatenates them into a compact feature vector. Our max pooling actually consists of a maxover-time pooling (i.e., max over the symbol sequence) for each of the 30 filters\u2014which measures the presence, but not location, of the bigram and trigram features\u2014as well as a traditional max-over-filters pooling\u2014which captures whether a bigram/trigram was discovered at a location, but does not preserve which bigram/trigram. Assembling the output of maxpooling results in a tensor with 32 dimensions for each time step. This is passed through a highway network (found\nto improve performance for character-based neural language modeling in [24]), where weights are shared across time-steps, to the input of an LSTM, which accumulates state over the sequence and returns the final emission from the accumulated state as the domain name embedding.\nThe decoder is loosely the reverse of the encoder process. The domain embedding is repeated over the maximum domain name length (time steps), and the resulting sequence is passed as input to an LSTM layer. The sequence of emissions from the LSTM layer are each passed through a highway network with weights shared over time to the same convolution filters as used in the encoder. This results in a 32-dimensional vector for each element in the sequence. The final step is a timedistributed dense layer that acts as a multinomial regressor with weights shared across time steps. Because of a softmax activation on the dense layer, the output of the decoder represents a multinomial distribution over domain characters for each time step, which can be sampled to produce a new domain name that is causally related to the input domain name."}, {"heading": "B. Generative Adversarial Network", "text": "Generative Adversarial Networks, first introduced in [7] for image classification, train two models: A generative model that seeks to create synthetic data based on samples from the true data distribution with added added noise as an input. A discriminator model receives a sample and must predict whether it is a synthetic or a true data sample. This process continues in the form of an adversarial game where the discriminator trains to predict the most accurate label for a sample and the generator trains to construct samples to confound the discriminator.\nOn its own, the autoencoder described in Section III-A might adequately produce domain names that look as if they might be a valid domain (e.g., in the Alexa top 1M), but are actually pseudo-randomly generated by sampling multinomial distributions at the output. However, the use of the autoencoder as a DGA would require that a list of seed domains be stored for use as inputs to the autoencoder. Instead, with only minor modifications to the structure, we repurpose the autoencoder as a GAN that accepts a random seed (number or numbers) as input, and emits a domain name that appears much like a valid domain name.\nAs shown in Figure 2(b), after the autoencoder has been pretrained on valid domains (e.g., Alexa top 1M), the learned layer weights are frozen. The decoder becomes the key element in a generative model, which merely prepends a dense layer that maps a random input to a domain embedding. Likewise, the encoder becomes a discriminative model, where we simply append a simple logistic regression layer to the domain embedding.\nIn order to reduce the complexity of the learning task of the generator, we restrict the output space of the generator by a predefined box learned offline from training data. We consider two models: a box layer that restricts the output to live in an axis-aligned box defined by embedding vectors of the training data and, a principal axis box layer that defines\na similar but potentially tighter box, with axes aligned to principal dimensions that represent right singular vectors of the training dataset. Like a traditional sigmoid-activated dense layer, this box layer learns a weight matrix W and bias vector b to produce a vector a = \u03c3 (Wx + b). However, rather than passing this result a \u2208 [0, 1]d to the output, we instead use a as a parameterization of a vector that lives within an axis-aligned box, and pass\ny = a \u00b7 vmax + (1\u2212 a)vmin,\nas the output, where vmin and vmax are, respectively, the minimum and maximum corners of a box in a d-dimensional space. These vectors are set a priori simply as the elementwise minimum and maximum over all embedded vectors in the training set produced using the first half (encoder) of the autoencoder. The principal axis box layer is nearly identical, except that vmin and vmax represent the corners of the principal components of the data in the rotated space, and y is subsequently multiplied by the right singular vector matrix V to transform back into the ambient space. We found in preliminary experiments that the effect of both methods are similar, and for simplicity, leverage the simple box layer in experiments. The use of the box layer as the generator allows the generator to learn embeddings within the domain of embedded vectors without requiring it to learn the extent of the domain. The hope is that the generator need only focus on learning the manifold of Alexa-like domains within the predefined box.\nWith the weights to the original autoencoder frozen, we train the generator layer and the logistic regression layer by linking the generator and discriminative model together as a GAN. Then, we roughly follow the GAN training procedure introduced in [7], in which the discriminative and generative model compete in adversarial rounds. In our setup the logistic regression weights are trained to separate valid domain names from names produced by the generative model, then the generative model learns weights for its generator layer by targeting an output of 0 (valid domain) for any random input to the combined GAN.\nIn a slight departure from [7], we regularize the discriminative model by training not only on the most recently-generated samples from the generative model, but a sampled history of domain names from both the current and previous adversarial rounds. This allows the discriminator to \u201cremember\u201d deficiencies in model coverage, and subsequently the generator is forced to learn novel domain embeddings and retreat from a common failure mode of GANs: that without care, samples produced by the generator can collapse to a single point. Subsequent to our experiments, authors in [25] proposed minibatch discrimination as another way to prevent this common failure mode. Our approach relies on the regularized discriminator to indirectly prevent the generator from entering the failure mode, while the minibatch discrimination encourages diversity of generated samples within a minibatch."}, {"heading": "IV. EXPERIMENTAL SETUP", "text": "We implement our architecture in Python using Keras [26]. We train the autoencoder on the Alexa top 1M dataset. Likewise, we train the GAN to distinguish the Alexa top 1M domains from pseudorandomly generated domains generated by the first half (encoder) of the autoencoder. We found that only a few adversarial rounds were required to learn appropriate generator weights when default Keras learning rates (adam optimizer) and batch sizes of 128 were used. For smaller learning rates, more iterations may be required.\nWe train our generator to accept 20 uniformly distributed numbers using numpy.random.rand, which allows for a seed, in order to produce a fake domain name. Similarly, we use numpy.random.multinomial to sample from the output using the common seed.\nAlthough the GAN framework ensures that DeepDGA maximally confuses the detector model, we are interested in its ability to bypass an independent model. So, to measure detectability of the DGA, we measure detection rates using a random forest DGA classifier that uses manually-crafted domain name features defined in [3], [4], [5], [15]. In particular, the manually crafted features of the random forest DGA classifier include the following:\n\u2022 length of domain name, \u2022 entropy of character distribution in domain name, \u2022 vowel to consonant ratio, \u2022 Alexa top 1M n-gram frequency distribution co-\noccurrence count, where n = 3, 4 or 5, \u2022 n-gram normality score, and \u2022 meaningful characters ratio.\nNote that for the n-gram normality score, we use n = 3, n = 4 and n = 5 as three distinct features as opposed to n = 1, n = 2 and n = 3 as in [15] since the larger n-gram size performed better in preliminary experiments."}, {"heading": "V. RESULTS", "text": "The autoencoder was pretrained for 300 epochs, with each epoch using 256K domains randomly sampled from the Alexa Top 1M, and a batch size of 128. Pretraining required roughly 14 hours on a single NVIDIA Titan X GPU. Subsequent to pretraining, in each adversarial round, we generated 12,800 adversarial samples against the detector. Each round required roughly 7 minutes on the GPU. Results from the training are detailed in the following subsections."}, {"heading": "A. Autoencoder results", "text": "Given a domain as input, the autoencoder produces a multinomial distribution over the possible characters (outcomes), from which a domain can be sampled. For example, a few domains sampled from the output of the autoencoder given the input (with TLD removed) are shown in Table II.\nThe autoencoder does not reconstruct perfectly the input, which can be ascribed to the stochastic sampling of the multinomial distributions to choose each domain character via independent draws, and insufficient model capacity (e.g., heavy model bias) to express all character combinations of\ndomains in the Alexa top 1M. However, since we actually don\u2019t want to generate domain names in the Alexa top 1M, but rather names names from the same (or similar) generative distribution, the reconstructions are wholly adequate."}, {"heading": "B. GAN results", "text": "Figure 3 displays Receiver Operating Characteristic (ROC) curves of a random forest classifier after each of four adversarial rounds. The classifier is trained on DeepDGA generated domains as malicious and the Alexa top 10K as benign. Results are based on 10-fold cross-validation. The ROC curves demonstrate that performance of the random forest classifier degrades with the number of adversarial rounds with an apparent asymptote after three rounds. This degradation is due to the GAN\u2019s ability to generate domains that create confusion for the classifier. Training on these domains will allow us to harden the classifier to blind spots to increase performance of a DGA detector. All subsequent experiments in this paper will be based on three adversarial rounds due to decreasing utility after three rounds.\nFigure 4 displays the unigram distribution of domains generated before adversarial rounds, after three adversarial rounds, and those from the Alexa top 1M. Note that the unigram distribution approaches that of the Alexa top 1M after three rounds demonstrating the confusing nature of domains generated by DeepDGA.\nCompared to other character-based DGAs, DeepDGA shows significant improvement in its ability to produce domains that are undetectable by a DGA classifier. Figure 5 shows the area under the ROC curve for the random forest classifier trained to detect each of ten character-based DGAs and DeepDGA vs. the Alexa top 10K individually. In other words, there are eleven different random forest models, each trained specifically to detect a particular DGA. Using the the random forest model, DeepDGA exhibits a false negative rate of roughly 1 in 14, as opposed to the next-best-performaing DGA, pykspa, which exhibits a false negative rate of roughly 1 in 106, a decrease of over 7\u00d7.\nThe detection rate is even more striking when one trains a general DGA detector on the available samples. We create a dataset using the top 10K Alexa domain names and 10K domain names from each of the DGAs in the comparison set, including DeepDGA. We report the average crossvalidation score over 10-folds using a 20% holdout set for each fold. Results in Table III show that under such a scenario, less than 50% of the DeepDGA samples are detected by the classifier (recall), whereas the next best DGAs (simda and kraken_v2) are detected at a rate of 98%. This represents a 25\u00d7 improvement in avoiding detection (1 in 2 vs. 1 in 50).\nA few DeepDGA domains (without TLDs) after adversarial tuning are shown below in Table IV. Domain names are of varying length, exhibit no strong common patterns, and can be exactly reproduced via a random seed."}, {"heading": "C. Hardening a machine learning model", "text": "It has previously been shown that adversarial examples may be shared across different machine learning models [2],\n[1]. We demonstrate that by augmenting a training set with adversarial examples generated by the GAN, a model can be hardened against DGA families not observed in the training set. In particular, we trained the random forest model using a leave-one-family-out strategy in which an entire DGA family is held out for validation, while the random forest model is trained on the remaining nine families. The top Alexa 10K are included in the training set, and the next Alexa 10K are included in the holdout set. This baseline result is compared to a hardened result in which 10K DeepDGA samples are appended to the 9-family plus Alexa training dataset. For each case, we report the true positive rate (TPR) at a fixed false positive rate (FPR) of 1%. This generally required different thresholds for baseline and hardened models.\nTraining the classifier on adversarially crafted samples generally improved the model\u2019s ability to detect families not in the training set. Table V shows that the hardened classifier maintains or increases the effective TPR especially for families with low baseline TPRs, and all families except dircrypt and lockyv2 which exhibit marginally smaller effective TPRs at a 1% FPR."}, {"heading": "VI. DISCUSSION", "text": "We have demonstrated automatically generating artificial domains that are adversarial to a deep learning DGA model. The adversarial examples are shown to be shared between the deep learning detector model\u2014for which they were explicitly optimized to circumvent\u2014as well as a random forest model. This demonstrates in an information security setting a key point in [2], [1]: that adversarial examples may be shared across different models.\nTraining a GAN to generate these examples requires substantial art to prevent common failure modes. We introduced novel history regularization, neural layers (box layer and principal axis box layer), and more common autoencoder pre-training to simplify the learning task of the generator. Subsequent to our experiments, [25] proposed other strategies for training GANs, which we leave to future work.\nFurthermore, we have demonstrated that by augmenting a training set with DeepDGA adversarial examples, a random forest classifier was hardened against DGA families not observed during training. The ability to harden using GANcrafted samples generally increased TPR for a fixed FPR in our experiments. Unlike the perterbation-based adversarial example generation proposed in [1] (i.e., fast gradient sign method), the GAN-crafted samples are meant to match the data actual distribution, so that, without care, FPR may be adversely affected when used for hardening. A qualitative comparison of the adversarial example quality and quantitative comparison of hardening strategies using the more direct fast-gradient sign method for DGA detection is left to future work."}], "references": [{"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "arXiv preprint arXiv:1412.6572, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "arXiv preprint arXiv:1312.6199, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "From throw-away traffic to bots: detecting the rise of DGA-based malware", "author": ["M. Antonakakis", "R. Perdisci", "Y. Nadji", "N. Vasiloglou", "S. Abu-Nimeh", "W. Lee", "D. Dagon"], "venue": "P21st USENIX Security Symposium (USENIX Security 12), pp. 491\u2013506, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Detecting algorithmically generated malicious domain names", "author": ["S. Yadav", "A.K.K. Reddy", "A. Reddy", "S. Ranjan"], "venue": "Proc. 10th ACM SIGCOMM conference on Internet measurement, pp. 48\u201361, ACM, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Detecting algorithmically generated domain-flux attacks with DNS traffic analysis", "author": ["S. Yadav", "A.K.K. Reddy", "A.N. Reddy", "S. Ranjan"], "venue": "Networking, IEEE/ACM Transactions on, vol. 20, no. 5, pp. 1663\u20131677, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Challenges in experimenting with botnet detection systems", "author": ["A.J. Aviv", "A. Haeberlen"], "venue": "CSET, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, pp. 2672\u20132680, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. Papernot", "P. McDaniel", "X. Wu", "S. Jha", "A. Swami"], "venue": "Proceedings of the 37th IEEE Symposium on Security and Privacy, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Cryptolocker victims to get files back for free", "author": ["M. Ward"], "venue": "BBC News, August, vol. 6, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Ransomware: Emergence of the cyberextortion menace", "author": ["N. Hampton", "Z.A. Baig"], "venue": "Australian Information Security Management Conference, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Hesperbot-A new, advanced banking trojan in the wild", "author": ["A. Cherepanov", "R. Lipovsky"], "venue": "2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "End-to-end analysis of a domain generating algorithm malware family.", "author": ["J. Geffner"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Phoenix: DGAbased botnet tracking and intelligence", "author": ["S. Schiavoni", "F. Maggi", "L. Cavallaro", "S. Zanero"], "venue": "Detection of intrusions and malware, and vulnerability assessment, pp. 192\u2013211, Springer, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "An application of recurrent nets to phone probability estimation", "author": ["A.J. Robinson"], "venue": "Neural Networks, IEEE Transactions on, vol. 5, no. 2, pp. 298\u2013305, 1994.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1994}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH, vol. 2, p. 3, 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Sequence transduction with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1211.3711, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Advances in optimizing recurrent networks", "author": ["Y. Bengio", "N. Boulanger-Lewandowski", "R. Pascanu"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 8624\u2013 8628, IEEE, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["F.A. Gers", "J. Schmidhuber", "F. Cummins"], "venue": "Neural computation, vol. 12, no. 10, pp. 2451\u20132471, 2000.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning precise timing with LSTM recurrent networks", "author": ["F.A. Gers", "N.N. Schraudolph", "J. Schmidhuber"], "venue": "J. Machine Learning Research, vol. 3, pp. 115\u2013143, 2003.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "Highway networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1505.00387, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush"], "venue": "arXiv preprint arXiv:1508.06615, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Improved techniques for training gans", "author": ["T. Salimans", "I. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford", "X. Chen"], "venue": "arXiv preprint arXiv:1606.03498, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "While this expert-guided process may always exist, we test a key thesis of [1]: adversarial examples1\u2014artificial samples which a machine learning model misidentifies\u2014can be discovered automatically and used to augment a training dataset to harden", "startOffset": 75, "endOffset": 78}, {"referenceID": 1, "context": "1In this paper, we employ a significantly broader meaning of adversarial examples as originally used in [2], [1].", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "1In this paper, we employ a significantly broader meaning of adversarial examples as originally used in [2], [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "While this thesis has been confirmed generally [2], [1], we propose a novel framework for generating these adversarial examples using a generative adversarial network for a natural languagebased domain generation algorithm (DGA) detector.", "startOffset": 47, "endOffset": 50}, {"referenceID": 0, "context": "While this thesis has been confirmed generally [2], [1], we propose a novel framework for generating these adversarial examples using a generative adversarial network for a natural languagebased domain generation algorithm (DGA) detector.", "startOffset": 52, "endOffset": 55}, {"referenceID": 2, "context": "Previous works include using a Hidden Markov Model (HMM) framework to model the generating distribution of several DGA families as well as \u201cnormal\u201c domains [3], and making bulk predictions on large sets of domains using clustering as a filtering technique [4], [5].", "startOffset": 156, "endOffset": 159}, {"referenceID": 3, "context": "Previous works include using a Hidden Markov Model (HMM) framework to model the generating distribution of several DGA families as well as \u201cnormal\u201c domains [3], and making bulk predictions on large sets of domains using clustering as a filtering technique [4], [5].", "startOffset": 256, "endOffset": 259}, {"referenceID": 4, "context": "Previous works include using a Hidden Markov Model (HMM) framework to model the generating distribution of several DGA families as well as \u201cnormal\u201c domains [3], and making bulk predictions on large sets of domains using clustering as a filtering technique [4], [5].", "startOffset": 261, "endOffset": 264}, {"referenceID": 5, "context": "Many methods require contextual information separate from the domain name itself [6].", "startOffset": 81, "endOffset": 84}, {"referenceID": 6, "context": "This explicit optimization is done using a DGA language model generator coupled with a DGA detector in a GAN [7].", "startOffset": 109, "endOffset": 112}, {"referenceID": 7, "context": "A similar adversarial approach was previously explored by [8] to generate synthetic samples for malware classification based on the DREBIN Android malware dataset.", "startOffset": 58, "endOffset": 61}, {"referenceID": 8, "context": "Domain generation algorithms are used by many strains of malware for C&C, including ransomware like cryptolocker [9], [10] and cryptowall [11], banking trojans, such as hesperbot [12], and information stealers, such as ramnit [13].", "startOffset": 113, "endOffset": 116}, {"referenceID": 9, "context": "Domain generation algorithms are used by many strains of malware for C&C, including ransomware like cryptolocker [9], [10] and cryptowall [11], banking trojans, such as hesperbot [12], and information stealers, such as ramnit [13].", "startOffset": 138, "endOffset": 142}, {"referenceID": 10, "context": "Domain generation algorithms are used by many strains of malware for C&C, including ransomware like cryptolocker [9], [10] and cryptowall [11], banking trojans, such as hesperbot [12], and information stealers, such as ramnit [13].", "startOffset": 179, "endOffset": 183}, {"referenceID": 11, "context": "On the other hand, suppobox creates domains by concatenating two pseudorandomly chosen English dictionary words [14].", "startOffset": 112, "endOffset": 116}, {"referenceID": 3, "context": "Authors in [4], [5] detect DGAs by using both unigram and bigram statistics of domain clusters.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "Authors in [4], [5] detect DGAs by using both unigram and bigram statistics of domain clusters.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "Authors in [3] apply a similar clustering process to classify domains with unsuccessful DNS resolutions.", "startOffset": 11, "endOffset": 14}, {"referenceID": 12, "context": "Authors in [15] also present a DGA classifier with the intention of classifying individual domains.", "startOffset": 11, "endOffset": 15}, {"referenceID": 2, "context": "In our experiments, we leverage a random forest classifier trained on features defined in [3], [4], [5], [15].", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "In our experiments, we leverage a random forest classifier trained on features defined in [3], [4], [5], [15].", "startOffset": 95, "endOffset": 98}, {"referenceID": 4, "context": "In our experiments, we leverage a random forest classifier trained on features defined in [3], [4], [5], [15].", "startOffset": 100, "endOffset": 103}, {"referenceID": 12, "context": "In our experiments, we leverage a random forest classifier trained on features defined in [3], [4], [5], [15].", "startOffset": 105, "endOffset": 109}, {"referenceID": 2, "context": "We do not implement the full system as defined in [3], [4], [5] as it is based on domain clustering and our intent is to classify DGAs on a per-domain basis.", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "We do not implement the full system as defined in [3], [4], [5] as it is based on domain clustering and our intent is to classify DGAs on a per-domain basis.", "startOffset": 55, "endOffset": 58}, {"referenceID": 4, "context": "We do not implement the full system as defined in [3], [4], [5] as it is based on domain clustering and our intent is to classify DGAs on a per-domain basis.", "startOffset": 60, "endOffset": 63}, {"referenceID": 12, "context": "The full system in [15] is not evaluated as they use contextual features such as IP addresses.", "startOffset": 19, "endOffset": 23}, {"referenceID": 1, "context": "Previous work discovered that many machine learning models, including modern neural network architectures, are vulnerable to adversarial examples[2], [1].", "startOffset": 145, "endOffset": 148}, {"referenceID": 0, "context": "Previous work discovered that many machine learning models, including modern neural network architectures, are vulnerable to adversarial examples[2], [1].", "startOffset": 150, "endOffset": 153}, {"referenceID": 0, "context": "Notably, [1] introduced the fast gradient sign method to systematically discover adversarial examples by perturbing a known \u201cgood\u201d sample x by a small amount \u2206x = sign (\u2207xJ (\u03b8,x, y)), where \u03b8 represents the model parameters, and J is the cost incurred for classifying x as class y.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "Separately, [7] proposed generative adversarial networks as a framework for generating artificial samples that are drawn from the same distribution as the training dataset.", "startOffset": 12, "endOffset": 15}, {"referenceID": 13, "context": "In a variety of natural language tasks, recurrent neural networks (RNNs) have been used to capture meaningful temporal relationships among tokens in a sequence [16], [17], [18], [19].", "startOffset": 160, "endOffset": 164}, {"referenceID": 14, "context": "In a variety of natural language tasks, recurrent neural networks (RNNs) have been used to capture meaningful temporal relationships among tokens in a sequence [16], [17], [18], [19].", "startOffset": 166, "endOffset": 170}, {"referenceID": 15, "context": "In a variety of natural language tasks, recurrent neural networks (RNNs) have been used to capture meaningful temporal relationships among tokens in a sequence [16], [17], [18], [19].", "startOffset": 172, "endOffset": 176}, {"referenceID": 16, "context": "In a variety of natural language tasks, recurrent neural networks (RNNs) have been used to capture meaningful temporal relationships among tokens in a sequence [16], [17], [18], [19].", "startOffset": 178, "endOffset": 182}, {"referenceID": 17, "context": "The problem of vanishing gradients is a key motivation behind the application of the Long Short-Term Memory (LSTM) cell [20], [21], [22], which consists of a state that can be read, written or reset via a set of programmable gates.", "startOffset": 120, "endOffset": 124}, {"referenceID": 18, "context": "The problem of vanishing gradients is a key motivation behind the application of the Long Short-Term Memory (LSTM) cell [20], [21], [22], which consists of a state that can be read, written or reset via a set of programmable gates.", "startOffset": 126, "endOffset": 130}, {"referenceID": 19, "context": "The problem of vanishing gradients is a key motivation behind the application of the Long Short-Term Memory (LSTM) cell [20], [21], [22], which consists of a state that can be read, written or reset via a set of programmable gates.", "startOffset": 132, "endOffset": 136}, {"referenceID": 20, "context": "Highway networks were recently proposed as a natural extension of gated memory networks like the LSTM unit to feedforward networks [23].", "startOffset": 131, "endOffset": 135}, {"referenceID": 0, "context": "Concretely, the output y of a single highway layers is the elementwise convex combination of the raw input x and the transformed input g(Wx + b) with a vector parameter t \u2208 [0, 1]:", "startOffset": 173, "endOffset": 179}, {"referenceID": 21, "context": "The character-level encoder shown in Figure 2(a) is loosely inspired by the neural language framework of [24], and the decoder is loosely a mirror image of the encoder.", "startOffset": 105, "endOffset": 109}, {"referenceID": 21, "context": "modeling in [24]), where weights are shared across time-steps, to the input of an LSTM, which accumulates state over the sequence and returns the final emission from the accumulated state as the domain name embedding.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "Generative Adversarial Networks, first introduced in [7] for image classification, train two models: A generative model that seeks to create synthetic data based on samples from the true data distribution with added added noise as an input.", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "However, rather than passing this result a \u2208 [0, 1] to the output, we instead use a as a parameterization of a vector that lives within an axis-aligned box, and pass", "startOffset": 45, "endOffset": 51}, {"referenceID": 6, "context": "Then, we roughly follow the GAN training procedure introduced in [7], in which the discriminative and generative model compete in adversarial rounds.", "startOffset": 65, "endOffset": 68}, {"referenceID": 6, "context": "In a slight departure from [7], we regularize the discriminative model by training not only on the most recently-generated samples from the generative model, but a sampled history of domain names from both the current and previous adversarial rounds.", "startOffset": 27, "endOffset": 30}, {"referenceID": 22, "context": "Subsequent to our experiments, authors in [25] proposed minibatch discrimination as another way to prevent this common failure mode.", "startOffset": 42, "endOffset": 46}, {"referenceID": 2, "context": "So, to measure detectability of the DGA, we measure detection rates using a random forest DGA classifier that uses manually-crafted domain name features defined in [3], [4], [5], [15].", "startOffset": 164, "endOffset": 167}, {"referenceID": 3, "context": "So, to measure detectability of the DGA, we measure detection rates using a random forest DGA classifier that uses manually-crafted domain name features defined in [3], [4], [5], [15].", "startOffset": 169, "endOffset": 172}, {"referenceID": 4, "context": "So, to measure detectability of the DGA, we measure detection rates using a random forest DGA classifier that uses manually-crafted domain name features defined in [3], [4], [5], [15].", "startOffset": 174, "endOffset": 177}, {"referenceID": 12, "context": "So, to measure detectability of the DGA, we measure detection rates using a random forest DGA classifier that uses manually-crafted domain name features defined in [3], [4], [5], [15].", "startOffset": 179, "endOffset": 183}, {"referenceID": 12, "context": "Note that for the n-gram normality score, we use n = 3, n = 4 and n = 5 as three distinct features as opposed to n = 1, n = 2 and n = 3 as in [15] since the larger n-gram size performed better in preliminary experiments.", "startOffset": 142, "endOffset": 146}, {"referenceID": 1, "context": "It has previously been shown that adversarial examples may be shared across different machine learning models [2], Fig.", "startOffset": 110, "endOffset": 113}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "This demonstrates in an information security setting a key point in [2], [1]: that adversarial examples may be shared across different models.", "startOffset": 68, "endOffset": 71}, {"referenceID": 0, "context": "This demonstrates in an information security setting a key point in [2], [1]: that adversarial examples may be shared across different models.", "startOffset": 73, "endOffset": 76}, {"referenceID": 22, "context": "Subsequent to our experiments, [25] proposed other strategies for training GANs, which we leave to future work.", "startOffset": 31, "endOffset": 35}], "year": 2016, "abstractText": "Many malware families utilize domain generation algorithms (DGAs) to establish command and control (C&C) connections. While there are many methods to pseudorandomly generate domains, we focus in this paper on detecting (and generating) domains on a per-domain basis which provides a simple and flexible means to detect known DGA families. Recent machine learning approaches to DGA detection have been successful on fairly simplistic DGAs, many of which produce names of fixed length. However, models trained on limited datasets are somewhat blind to new DGA variants. In this paper, we leverage the concept of generative adversarial networks to construct a deep learning based DGA that is designed to intentionally bypass a deep learning based detector. In a series of adversarial rounds, the generator learns to generate domain names that are increasingly more difficult to detect. In turn, a detector model updates its parameters to compensate for the adversarially generated domains. We test the hypothesis of whether adversarially generated domains may be used to augment training sets in order to harden other machine learning models against yet-to-be-observed DGAs. We detail solutions to several challenges in training this character-based generative adversarial network (GAN). In particular, our deep learning architecture begins as a domain name auto-encoder (encoder + decoder) trained on domains in the Alexa one million. Then the encoder and decoder are reassembled competitively in a generative adversarial network (detector + generator), with novel neural architectures and training strategies to improve convergence. Results show that domains generated from a GAN to bypass the GAN\u2019s detector also bypass a random forest classifier that leverages hand-crafted features. Conversely, by augmenting the training set with these adversarial examples, the random forest classifier is able to detect with greater efficacy DGA malware families not seen during training.", "creator": "LaTeX with hyperref package"}}}