{"id": "1610.02847", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "Situational Awareness by Risk-Conscious Skills", "abstract": "Hierarchical Reinforcement Learning has been previously shown to speed up the convergence rate of RL planning algorithms as well as mitigate feature-based model misspecification (Mankowitz et. al. 2016a,b, Bacon 2015). To do so, it utilizes hierarchical abstractions, also known as skills -- a type of temporally extended action (Sutton et. al. 1999) to plan at a higher level, abstracting away from the lower-level details. We incorporate risk sensitivity, also referred to as Situational Awareness (SA), into hierarchical RL for the first time by defining and learning risk aware skills in a Probabilistic Goal Semi-Markov Decision Process (PG-SMDP). This is achieved using our novel Situational Awareness by Risk-Conscious Skills (SARiCoS) algorithm which comes with a theoretical convergence guarantee. We show in a RoboCup soccer domain that the learned risk aware skills exhibit complex human behaviors such as `time-wasting' in a soccer game. In addition, the learned risk aware skills are able to mitigate reward-based model misspecification.\n\n\n\nThe task of finding the best predictive model for predicting future goals should include a strategy to predict future goals. In other words, in order to learn the optimal algorithm for predicting future goals, participants must either create an alternative strategy in which a strategy for predicting future goals could be developed. In particular, in the first step of the training, participants must provide their own and specific skills for a task that is in the best of their ability to predict future goals, such as making a choice about whether to take actions or not to use a strategy. To do so, participants must have specific skills. We note in the training that this strategy is the most efficient method in the field of optimization. In addition, this approach enables participants to generate and maintain knowledge about a task they cannot yet perform because of the non-linearity of their approach. In order to learn the optimal algorithm for predicting future goals, participants must have specific skills. Participants must have specific skills. We note in the training that this strategy is the most efficient method in the field of optimization. In addition, this approach enables participants to generate and maintain knowledge about a task they cannot yet perform because of the non-linearity of their approach. In particular, in the first step of the training, participants must have specific skills. We note in the training that this strategy is the most efficient method in the field of optimization. In addition, this", "histories": [["v1", "Mon, 10 Oct 2016 11:01:32 GMT  (4088kb,D)", "http://arxiv.org/abs/1610.02847v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["daniel j mankowitz", "aviv tamar", "shie mannor"], "accepted": false, "id": "1610.02847"}, "pdf": {"name": "1610.02847.pdf", "metadata": {"source": "CRF", "title": "Situational Awareness by Risk-Conscious Skills", "authors": ["Daniel J. Mankowitz"], "emails": ["danielm@tx.technion.ac.il", "avivt@berkeley.edu", "shie@ee.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "Hierarchical-Reinforcement Learning (H-RL) is an RL paradigm that utilizes hierarchical abstractions to solve tasks. This enables an agent to abstract away from the lower-level details and focus more on solving the task at hand. Hierarchical abstractions have been utilized to naturally model many real-world problems in machine learning and, more specifically, in RL. This includes high-level controllers in robotics Peters & Schaal (2008); Hagras et al. (2004); da Silva et al. (2012), strategies (such as attack and defend) in soccer Bai et al. (2015) and video games Mann (2015), as well as highlevel sub-tasks in search and rescue missions Liu & Nejat (2015). In RL, hierarchical abstractions are typically referred to as skills, (da Silva et al. (2012)), Temporally Extended Actions (TEAs), options (Sutton et al. (1999)) or macro-actions, (Hauskrecht (1998)). We will use the term skill to refer to hierarchical abstractions from here on in.\nH-RL is important as it utilizes skills to both speed up the convergence rate in RL planning algorithms Mann & Mannor (2013); Precup & Sutton (1997); Mankowitz et al. (2014) as well as mitigating model misspecification. Model misspecification in RL can be sub-divided into (1) feature-based model misspecification - where a limited, sub-optimal feature set is provided (e.g., due to limited memory resources or sub-optimal feature selection) leading to sub-optimal performance; and (2) reward-based model misspecification whereby the reward shaping function is incorrectly designed\nar X\niv :1\n61 0.\n02 84\n7v 1\n[ cs\n.A I]\n(e.g., due to an incorrect understanding of the target problem). Previous work has focused on utilizing skills to mitigate feature-based model misspecification Mankowitz et al. (2014, 2016a,b), but have not attempted to mitigate reward-based model misspecification. Risk sensitivity can be utilized to mitigate this form of misspecification.\nAn important factor missing in H-RL is risk sensitivity. A risk-sensitive H-RL framework would enable us to generate skills with different Risk Attitudes, also known as Situational Awareness (SA) Endsley (1995); Smith & Hancock (1995), which, as we will show in our paper, allows us to mitigate reward-based model misspecification. As seen in Table 1, previous work in H-RL has focused on skill learning Mankowitz et al. (2014, 2016a,b), but has not incorporated risk-sensitivity into the H-RL objective, nor learned risk aware skills to mitigate reward-based model misspecification. From here on in, the terms risk sensitivity, risk attitude and SA will be used interchangeably.\nSituational Awareness (SA): SA can be dependent on both time and space, although the focus of this paper is on time-based SA. We provide both definitions below.\nTime-based SA: Consider a soccer game composed of complicated strategies (skills), such as attack and defend, based on the status of the game. Consider a team losing by one goal to zero with ten minutes remaining. Here, the team needs to play attacking, risky soccer such as making long, risky passes as well as shooting from distance to try and score goals and win the game (Figure 1a). On the other hand, if the team is winning by one goal to zero with ten minutes remaining, the team needs to \u2018waste time\u2019 by maintaining possession and playing risk-averse, defensive football to prevent the opponent from gaining the ball and scoring goals (Figure 1b). In both scenarios the team has the same objective which is to score more goals than their opponent once time runs out (I.e. win the game). Time-based SA enables an agent to act in a risk-aware manner based on the amount of time remaining in the task.\nSpatial SA: As mentioned previously, SA can also be defined in terms of space. Consider an autonomous vehicle (the agent) driving in a narrow/wide lane or on dry/wet roads as shown in Figure 2. The proximity of the agent to the other vehicles in the lane example (Figure 2a), or the distance of the agent to other vehicles as well as puddles in the dry/wet road example (Figure 2b) determines the SA and therefore the risk attitude of the agent.\nOur main idea in this paper, is that a simple way to add risk-sensitivity to H-RL is by maximizing a risk-sensitive objective rather than the regular expected return formulation. One example that we focus on in this work is that of a Probabilistic Goal Markov Decision Process (PG-MDP) Xu & Mannor (2011). Previous works that incorporate risk into RL have mainly been focused on learning a single risk aware policy in a non-hierarchical setting Avila-Godoy & Fern\u00e1ndez-Gaucherand (1998); Tamar et al. (2015a,b) by maximizing the Conditional Value-at-Risk or the Value-at-Risk objectives. We provide a framework that enables an agent, for the first time to solve a task by maximizing a risksensitive objective in hierarchical RL. We define a Probabilistic Goal Semi-Markov Decision Process (PG-SMDP) which naturally models this setting. By solving the PG-SMDP using our novel SARiCoS algorithm, the agent learns Risk-Aware Skills (RASs) that have a particular Risk Attitude/SA. We show that the learned risk-aware skills exhibit complex human behaviours such as time-wasting in a soccer game. We then show in our experiments that these skills can be used to overcome reward-based model misspecification, in contrast to the regular expected return formulation.\nMain Contributions: (1) Extending hierarchical RL to incorporate SA by defining a Probabilistic Goal Semi-Markov Decision Process (PG-SMDP) (2) The development of the Situational Awareness by Risk-Conscious Skills (SARiCoS) algorithm which optimizes a hierarchical risk-aware RL objective and learns Risk-Aware Skills (RASs) that incorporate SA. (3) Theorem 1 which derives a policy gradient update rule for learning Risk Aware Skills and inter-skill policy parameters in a Probabilistic Goal Semi-Markov Decision Process (PG-SMDP). (4) Theorem 2 which proves that SARiCoS converges to a locally optimal solution. (5) Experiments in the RoboCup domain that exhibit an agent\u2019s ability to learn skills possessing SA (e.g., time wasting in a soccer game). In addition, we show the agent utilizing these skills to overcome reward-based model misspecification."}, {"heading": "2 Background", "text": "Semi-Markov Decision Process (SMDP) Sutton et al. (1999) A Semi-Markov Decision Process can be defined by the 5-tuple \u3008X,\u03a3, P,R, \u03b3\u3009, where X is a set of states, \u03a3 is a set of skills, P is a transition probability function and R is a bounded reward function. We assume that the rewards we receive at each timestep are bounded between [0, Rmax]. Therefore R forms a mapping from X \u00d7 \u03a3 to [0, Rmax1\u2212\u03b3 ] and represents the expected discounted sum of rewards that are received from executing skill \u03c3 \u2208 \u03a3 from state x \u2208 X . The discount factor is defined as \u03b3 \u2208 [0, 1]. The inter-skill policy \u00b5 : X \u2192 \u2206\u03a3 maps states to a probability distribution over skills. The goal in an SMDP is to find\nthe optimal inter-skill policy \u00b5\u2217 that maximizes the value function V \u00b5(x) = E [\u2211\u221e\nt=0 \u03b3 tRt|x, \u00b5\n] .\nThis represents the expected return of following the inter-skill policy \u00b5 from state x. The optimal policy \u00b5\u2217 determines the best action to take for a given state and generates the optimal value function V \u00b5 \u2217 (s).\nSkill, Option and Macro-Action Sutton et al. (1999); da Silva et al. (2012): An RL skill, option or macro action \u03c3 is defined as the 3-tuple \u03c3 = \u3008I, \u03c0\u03b8, p(x)\u3009 where I is a set of initiation states from which a skill can be initialized or executed; \u03c0\u03b8 is the intra-skill policy which selects the lower-level (or primitive) actions to perform whilst the skill is executing and is parameterized by \u03b8 \u2208 Rn; The termination probability p(x) which determines the probability of the skill terminating when in state x.\nProbabilistic Goal MDP (PG-MDP) Xu & Mannor (2011): While the standard MDP objective presented above considered the expected reward, in some situations different objectives may be more appropriate. In particular, risk-sensitive criteria that maximize the probabilty of success, and not just the expected outcome, are natural objectives in domains such as finance and operations research, but also in game-playing, such as soccer. The PG-MDP is an extension of the MDP that accounts for such an objective. In a PG-MDP, the goal is to learn a policy \u03c0 that maximizes the probability that some performance threshold will be attained. That is, it aims to maximize:\nP(W\u03c0 \u2265 \u03b2) , (1)\nwhere W\u03c0 is a random variable representing the total reward of the MDP under the policy \u03c0. The parameter \u03b2 \u2208 R is a performance threshold. The PG-MDP formulation is key for our risk shaping method, and will be further discussed when defining the PG-SMDP.\nPolicy Gradient Peters & Schaal (2006): In continuous as well as high-dimensional MDPs, it is computationally inefficient to learn a policy that determines an action to perform for any given state. Policies therefore need to be generalizable, where the policy will choose the same or similar action to perform when in nearby states. In order to achieve this generalization, a policy is parameterized using techniques such as Linear Function Approximation (LFA) (which we use in this work) Sutton\n& Barto (1998). A popular technique to learning the parameters for these parameterized policies is the policy gradient method. Let J\u03c0(\u03b8) denote the expected return of the policy parametrized by \u03b8 as J\u03c0(\u03b8) = \u222b \u03c4 P (\u03c4)R(\u03c4)d\u03c4 where \u03c4 is a trajectory of T timesteps \u3008x1, a1, r1, x2 \u00b7 \u00b7 \u00b7 , xT \u3009; P (\u03c4) is the probability of a trajectory and R(\u03c4) is defined as the total reward of the trajectory. Policy gradient uses sampling to estimate the gradient \u2207\u03b8J\u03c0(\u03b8) and then updates the parameters using a gradient ascent update rule \u03b8t+1 = \u03b8t + \u2207\u03b8J\u03c0(\u03b8) where denotes a positive step size."}, {"heading": "3 Probabilistic Goal SMDP (PG-SMDP)", "text": "In this work we focus on solving problems in which the agent must maximize its probability of success for solving a given task in a limited amount of time. A natural model for such problems is the the PG-MDP framework described above. However, we are interested in complex problems that require some hierarchical reasoning, and therefore propose to extend PG-MDPs to incorporate skills, leading to a PG Semi-MDP (PG-SMDP) model. We now derive an equivalent PG-SMDP with an augmented state space and skill set \u03a3 that can easily be utilized with policy gradient algorithms.\nWe assume that we are given a set of skills \u03a3 = {\u03c3i|i = 1, 2, \u00b7 \u00b7 \u00b7n, \u03c3j = \u3008Ij , \u03c0j , pj(x)\u3009} and inter-skill policy \u00b5(\u03c3|x)\u2192 \u2206\u03a3 which chooses a skill to execute given the current state x \u2208 X . We wish to maximize the probability that the total accumulated reward, \u2211T t=0 rt, attained during the execution of the inter-skill policy \u00b5, passes the pre-defined performance objective threshold \u03b2 \u2208 R within T timesteps. This takes the form of a Probabilistic Goal SMDP (PG-SMDP) (since we are incorporating skills) defined in Equation 2.\nmax \u00b5 P( T\u2211 t=0 rt \u2265 \u03b2|\u00b5) . (2)\nIn order to solve this PG-SMDP using traditional RL techniques, we augment the state space with the total accumulated reward Xu & Mannor (2011) to create an equivalent augmented PG-SMDP. We will show the important developments of this formulation for reader clarity. This will enable us to utilize traditional RL techniques in order to maximize the probability of surpassing the performance threshold \u03b2, given a set of skills \u03a3, within T timesteps. First note that maximizing the probability can be formulated as an expectation as shown in Equation 3.\nmax\u00b5 P (\u2211T t=0 r(xt, \u03c3t) \u2265 \u03b2 \u2223\u2223\u2223\u00b5)\n= max\u00b5 E\u00b5 [ I (\u2211T t=0 r(xt, \u03c3t) \u2265 \u03b2 )]\n(3)\nThis expectation still contains a constraint. We now formulate an equivalent augmented PG-SMDP that removes the \u03b2 constraint and incorporates the constraint into the reward function. Define an augmented state z = {x,w} where x \u2208 X is the original state space and w = \u2211T t=0 r(xt, \u03c3t) is the accumulated reward up until time T . We can then define the transition probabilities in terms of the augmented state z according to Equation 4.\nP (z\u2032|z, \u03c3) = {{x\u2032, w + r(x, \u03c3)}w.p P (x\u2032|x, \u03c3)} . (4)\nThe reward function for this augmented state is then defined according to Equation 5.\nr\u0303t(z, \u03c3) =  0, t < T 0, t = T,w < \u03b2\n1, t = T,w \u2265 \u03b2 (5)\nTogether, the transition probabilities and the reward function forms an equivalent PG-SMDP with an augmented state space z \u2208 Z as shown in Equation 6. This formulation learns an inter-skill policy\n\u00b5 that maximizes the probability that the total accumulated reward will surpass the performance threshold \u03b2 within T timesteps.\nmax \u00b5\nE [ T\u2211 t=0 r\u0303(zt, \u03c3t) ] (6)\nIn the next Section, we show that risk can be incorporated into the PG-SMDP by incorporating a Risk Awareness Parameter (RAP) into the typical definition of a skill to form a Risk Aware Skill (RAS). We derive a policy gradient algorithm to learn both the inter-skill policy and the RAPs such that the agent is able to successfully solve the PG-SMDP."}, {"heading": "4 Risk-Aware Skill", "text": "We modify the typical definition of a skill to include a parameter, called the Risk-Awareness Parameter (RAP) yw \u2208 R. This is the parameter that controls the risk-attitude of the Risk-Aware Skill (RAS). Definition 1. A Risk Aware Skill (RAS) \u03c3 is a temporally extended action that consists of the 4-tuple \u03c3 = \u3008I, \u03c0\u03b8, p(z), yw\u3009, where I are the set of states from where the RAS can be initialized; \u03c0\u03b8 is the parameterized intra skill policy; p(z) is the probability of terminating in state z \u2208 Z; and yw \u2208 R is the Risk-Awareness Parameter (RAP) governed by the Risk-Aware Distribution (RAD) yw \u223c Pw(\u00b7) with parameters w \u2208 Rm.\nIn practice, the RAP can parameterize the intra-skill policy, or act as a meta-parameter for the RAS (E.g. Dribble power in the RoboCup experiment (See Experiments Section))."}, {"heading": "5 SARiCoS Algorithm", "text": "The Situational Awareness by Risk-Conscious Skills (SARiCoS) algorithm learns the parameters of a two-tiered skill selection policy defined as:\n\u00b5\u03b1,\u2126i(\u03c3, y|z) = \u00b5\u03b1(\u03c3|z)\u00b5 \u03c3i \u2126i (y|z) , (7)\nwhere \u00b5\u03b1 : Z \u2192 \u2206\u03a3 is the inter-skill policy, parameterized by \u03b1 \u2208 Rd, that selects which RAS \u03c3 needs to be executed from a set \u03a3 of N RASs, given the current state z \u2208 Z.; \u00b5\u03c3i\u2126i(\u00b7|z) is the RAD for RAS \u03c3i with RAD parameters \u2126i = wi \u2208 Rm. The RAD parameters for all RASs are stored in a vector \u2126 = [\u03c91, \u03c92, \u00b7 \u00b7 \u00b7 , \u03c9N ] \u2208 R|N ||m|\u00d71 for algorithmic purposes. The two-tiered skill selection policy is executed by first sampling a Risk-Aware Skill \u03c3i to execute from \u00b5\u03b1(\u03c3|z). The risk attitude of the skill is then determined by sampling the RAP from the RAD \u00b5\u03c3i\u2126i(y|z). SARiCoS learns (1) the inter-skill policy parameters \u03b1 \u2208 R\nd and (2) the RAD parameters \u2126 to produce Situationally Aware RASs. In order to derive gradient update rules for these parameters in a policy gradient setting, we define the notion of a risk-aware trajectory.\nRisk-Aware Trajectory: In the standard policy gradient framework, we define a typical trajectory as \u03c4 = (zt, \u03c3t, rt, zt+1) T t=0 where T is the length of the trajectory. To incorporate the two-tiered policy into this trajectory, we define a risk-aware trajectory \u03c4r = (zt, \u03c3t, yw\u03c3t rt, zt+1) T t=0 where at each timestep, we draw a RAP corresponding to the RAS \u03c3t that was selected. We can therefore define the probability of a trajectory as P\u03b1,\u2126(\u03c4r) = P(z0) \u220fT\u22121 t=0 P(zt+1|zt, \u03c3t)\u00b5\u03b1,\u2126(\u03c3t, yt|zt), where P (z0) is the initial state distribution; P (zt+1|zt, \u03c3t) is the transition probability of moving from state zt to state zt+1 given that a RAS \u03c3t was executed; and \u00b5\u03b1,\u2126(\u03c3t, yt|zt) is the two-tiered selection policy. Using this notion, it is now possible to derive the gradient update rules for each set of parameters as shown in Theorem 1."}, {"heading": "5.1 Inter-skill policy and RAP Update Rules", "text": "We define the expected reward for following a policy \u00b5\u03b1,\u2126:\nJ(\u00b5\u03b1,\u2126) = \u222b \u03c4 P (\u03c4 |\u03b1,\u2126)R(\u03c4)d\u03c4 . (8)\nLet us group the parameters for the inter-skill policy and the continuous RAD Parameters into a single vector \u03c7 = [\u03b1,\u2126] \u2208 Rd+m\u00b7N . Taking the derivative of this objective and using the well-known likelihood trick Peters & Schaal (2008) yields:\n\u2207\u03c7J(\u00b5\u03c7) = \u222b \u03c4 P (\u03c4 |\u03c7)\u2207\u03b1 logP (\u03c4 |\u03c7)R(\u03c4)d\u03c4 , (9)\nwhere P (\u03c4 |\u03c7) = P (z0) \u220fT k=1 P (zk+1|zk, \u03c3k)P (\u03c3k|zk, \u03c7); zk \u2208 Z is the state at timestep k; \u03c3k is the RAS selected at timestep k and T is the length of the trajectory. Since only P (\u03c3k|zk, \u03c7) is parameterized, the gradient\u2207\u03c7J(\u00b5\u03c7) can be simplified to:\n\u2207\u03c7J(\u00b5\u03c7) = \u222b \u03c4 P (\u03c4 |\u03c7)\u2207\u03c7 logP (\u03c3k|zk, \u03c7)R(\u03c4)d\u03c4 , (10)\nwhere P (\u03c3k|zk, \u03c7) = \u00b5\u03b1(\u03c3t|zt)\u00b5\u03c3t\u2126 (yt|zt). Therefore, substituting the two-tiered policy into Equation 20 and deriving with respect to \u03b1 leads to the gradient update rule:\n\u2207\u03b1J(\u00b5\u03c7) = \u222b \u03c4 P (\u03c4 |\u03c7)\u2207\u03b1 log\u00b5\u03b1(\u03c3t|zt)R(\u03c4)d\u03c4 .\nIf we represent \u00b5\u03b1(\u03c3t|zt) as a Gibb\u2019s distribution which is a common policy choice in many MDPs Sutton & Barto (1998), then we can easily derive the gradient and estimate it by samples using the following gradient update rule:\n\u2207\u03b1J(\u00b5\u03c7) = \u2329 H\u2211 h=0 \u2207\u03b1 log\u00b5\u03b1(\u03c3h|zh) H\u2211 j=0 \u03b3jrj \u232a (11)\nIf we substitute the two-tiered policy into Equation 20 and deriving with respect to \u2126 for the RAD Parameters, then we get the following gradient update rule:\n\u2207\u2126J(\u00b5\u03c7) = \u222b \u03c4 P (\u03c4 |\u03c7)\u2207\u2126 log\u00b5\u03c3t\u2126 (yt|zt)R(\u03c4)d\u03c4 .\nIf we represent \u00b5\u03c3t\u2126 (yt|zt) as any distribution from the natural exponential family, then we can easily derive the gradient and estimate it by samples using the following gradient update rule:\n\u2207\u2126J(\u00b5\u03c7) = \u2329 H\u2211 h=0 \u2207\u2126 log\u00b5\u03c3t\u2126 (yt|zt) H\u2211 j=0 \u03b3jrj \u232a (12)\nThese derivations are summarized in Theorem 1. A full proof can be found in the supplementary material.\nTheorem 1 (Gradient Update Derivation). Suppose that we are maximizing the Policy Gradient (PG) objective J(\u00b5\u03b1,\u2126) = \u222b \u03c4 P\u03b1,\u2126(\u03c4)R(\u03c4)d\u03c4 using risk-aware trajectories, generated by the twotiered skill selection policy \u00b5\u03b1(\u03c3t|xt)\u00b5\u03c3t\u2126 (yt|zt), then the expectation of the gradient update rules for the inter-skill policy parameters \u03b1 \u2208 Rd and the RAD parameters \u2126 \u2208 R|N ||m| are the true gradients and are defined as (1) \u2207\u03b1J(\u00b5\u03b1,\u2126) = \u2329\u2211H h=0\u2207\u03b1 log\u00b5\u03b1(\u03c3h|zh) \u2211H j=0 \u03b3 jrj \u232a and (2)\n\u2207\u2126J(\u00b5\u03b1,\u2126) = \u2329\u2211H h=0\u2207\u2126 log\u00b5 \u03c3t \u2126 (yt|zt) \u2211H j=0 \u03b3 jrj \u232a respectively. H is the trajectory length and < \u00b7 > is an average over trajectories as in standard PG.\nGiven the gradient update rules, we can derive an algorithm for learning both the inter-skill parameters \u03b1 \u2208 Rd and the continuous RAD parameters \u2126 = [\u03c91, \u03c92, \u00b7 \u00b7 \u00b7 , \u03c9N ] \u2208 R|N ||m|\u00d71 for the N RAS. SARiCoS learns these parameters by two timescale stochastic approximation, as shown in Algorithm 1, and converges to a locally optimal solution as is proven in Theorem 2. The convergence proof is based on standard two-timescale stochastic approximation convergence arguments Borkar (1997) and is found in the supplementary material.\nTheorem 2 (SARiCoS Convergence). Suppose we are optimizing the expected return J(\u00b5\u2126,\u03b1) =\u222b R(\u03c4)P (\u03c4)d\u03c4 for any arbitrary SARiCoS policy \u00b5\u2126,\u03b1 where \u2126 \u2208 R|N ||m| and \u03b1 \u2208 Rd are the inter-skill and Risk Aware Distribution parameters respectively. Then, for step sizes sequences {ak}\u221ek=0, {bk}\u221ek=0 that satisfy \u2211 k ak = \u221e, \u2211 k bk = \u221e, \u2211 k a 2 k < \u221e, \u2211 k b 2 k < \u221e and bk > ak, the SARiCoS iterates converge a.s \u03b1k \u2192 \u03b1\u2217,\u2126k \u2192 \u03bb\u0304(\u03b1\u2217) as k \u2192\u221e to the countable set of locally optimal points of J(\u00b5\u2126,\u03b1).\nAlgorithm 1 SARiCoS Algorithm\nRequire: \u03b1 \u2208 Rd. {Inter-skill policy parameterization}, \u2126 \u2208 R|N ||m|\u00d71 {Set of RAD parameters for each skill}\n1: repeat: 2: \u03b1k+1 \u2192 \u03b1k + ak\u2207\u03b1J\u03b1,\u2126 3: \u2126k+1 \u2192 \u2126k + bk\u2207\u2126J\u03b1,\u2126 {stepsize bk > ak} 4: until convergence"}, {"heading": "6 Experiments", "text": "The experiments were performed in the RoboCup 2D soccer simulation domain Akiyama & Nakashima (2014); a well-known benchmark for many AI challenges. In the experiments, we demonstrate the ability of the agent to learn risk-aware skills (such as \u2018time-wasting\u2019 in a soccer game), and therefore exhibit SA, by maximizing the PG-SMDP objective. In the RoboCup domain, we also show the agent\u2019s ability to exit local optima due to reward shaping and therefore overcome reward-based model misspecification.\nRoboCup Offense (RO) Domain: This domain 1 consists of two teams on a soccer field where the striker (the yellow agent) needs to score against a goalkeeper (purple circle) as shown in Figure 3a. The striker has T = 150 timesteps (length of the episode) to try and score a goal. State space - The state space in RO consists of the continuous \u3008x, y\u3009 field locations of the striker, ball, goalposts and goalkeeper as well as the cumulative sum of rewards w. Skills - The Risk-Aware Skill (RAS) set \u03a3 in each of the experiments consists of three RAS: (1) Move to the ball (M), (2) Move to the ball and shoot towards the goal (S) and (3) Move to the ball and dribble in the direction of the goal (D). Each RAS i is parameterized with a Risk Aware Parameter ywi . We focus on learning the dribbling power RAP yw,D that controls how hard the agent kicks the ball when performing the skill Dribble. Data: SARiCoS is trained over 3 independent trials with 20, 000 episodes per trial. Learning Algorithm and features - The learning algorithm for both the inter-skill policy parameters \u03b1 and the RAPs for the RASs is Actor Critic Policy Gradient (AC-PG) 2. The inter-skill policy \u00b5 that chooses which RAS to execute is represented by a Gibb\u2019s distribution with Fourier Features. The Risk Aware Distribution (RAD) is represented as a normal distribution y \u223c N (\u03c6(s)T\u03c9, V ) with a fixed variance V . Here, \u03c6(s) are state dependent features [1, xagent, yagent, w, distGoal] representing the agent\u2019s x, y location, the cumulative reward and the distance of the agent to the goal. Rewards - Engineering of the reward in RL is common practice for the RoboCup domain Hausknecht & Stone (2015); Bai et al. (2015). The rewards for both of the RoboCup scenarios have been engineered based on logical soccer strategies. The striker gets small positive rewards for dribbling outside the box rD,far and shooting when inside or near the box rS,near. Negative rewards come about when the striker dribbles inside the box, rD,near, or shoots from far, rS,far, as the striker has a smaller probability of scoring Yiannakos & Armatas (2006). The striker also gets a small positive reward for moving towards the ball rmove. There is also a game score reward rscore, which is positive if winning and negative if losing or drawing. In the PG-SMDP setting, the rewards r\u0303 = 1 if w >= \u03b2 at the end of each episode, otherwise the reward is 0 at each timestep. In the Expected Return setting (see Reward-Based Model Misspecification), the regular rewards are utilized at each timestep."}, {"heading": "6.1 Situational Awareness by Risk-Conscious Skills", "text": "In this section we show that learning the inter-skill policy and RAD parameters using SARiCoS so as to maximize a PG-SMDP can bring about risk-aware skills that exhibit time-based SA. We provide\n1 https://github.com/mhauskn/HFO\n2AC-PG has lower variance compared to regular PG and the convergence guarantees are trivial extensions of the current proof.\nthe agent with two different soccer situations: (1) The agent is losing the game 0\u2212 1; (2) The agent is winning the game 1 \u2212 0. Similar results are obtained for different scores (e.g., 2 \u2212 0 and 0 \u2212 2 etc. and have therefore been omitted). For all of the scenarios, the performance threshold \u03b2 for the PG-SMDP is set to a constant value (\u03b2 = 1.0) a-priori.\nSA in a Losing Scenario: In a scenario where a team is losing and time is running out, the team needs to play risky, attacking soccer to try and score goals. The agent is placed in a losing scenario where the score is 0\u2212 1 to the opposition with 150 timesteps remaining. Using SARiCoS, the agent learns to perform a fast Dribble by kicking the ball with significant power to make quick progress along the pitch and get in a position to shoot for goal as seen in Figure 3b(i). The average RAP value for the Dribble RAS is approximately 100 (max value 150, min value 0) prompting the agent to kick the ball with significant power and quickly advance up the pitch. The RAP is state dependent, enabling the agent to learn to initially kick the ball with a large amount of power when near the half-way line and then decrease the dribble power when approaching the goal so as to prevent losing possession to the goalkeeper. This is seen from the dribble power color gradient superimposed onto the RO domain in Figure 3c. The color gradient varies from powerful kicks (in red) to soft kicks (in blue). Once the agent is near the goal, it executes the skill Shoot as seen in the figure 3b(i). The average episode length is 70.0\u00b1 1.0 (mean\u00b1std) as seen in Table 2 and the average number of goals scored over 100 evaluation episodes is 74.3\u00b1 6.5. In addition, the keeper captures the ball on average 21\u00b1 5.29 times indicating that the striker is playing risky football with aggressive dribbling and, as a result, scores a high number of goals. In addition, the average reward is consistently higher than the \u03b2 threshold.\nSA in a Winning Scenario: When winning a game with little time remaining, a natural strategy is to hold onto the ball and run out the clock 3 (\u2018time-wasting\u2019) so as to prevent the opposing team from gaining possession and possibly scoring a goal. SARiCoS learns \u2018time-wasting\u2019 since when the agent is winning the game 1\u2212 0, the agent slowly dribbles his way up the pitch, collecting the dribble from far rewards rD,far in the process as seen in Figure 3b(ii). Once the agent crosses the performance threshold, it stands on the ball, and wastes time by executing the M skill, whilst continuing to collect the positive score rewards rscore and the small positive rmove rewards. This strategy causes the agent to take the largest amount of time on average (142.3 \u00b1 1.5 steps) to complete each episode (time wasting) and as a result only scores 1.3\u00b1 0.6 goals. However, the ball is almost never captured by the opponent 7.3\u00b1 0.6 times on average per 100 evaluation episodes. See a Video4 of the agent\u2019s behavior in each of these scenarios."}, {"heading": "6.2 Mitigating Reward-based Model Misspecified", "text": "The learned risk-aware skills can be utilized to overcome reward-based model misspecification. We focus on the losing scenario in RoboCup soccer. We compared SARiCoS to the regular Expected Return (ER) formulation, i.e., an implementation of Actor-Critic Policy Gradient that utilizes regular rewards at each timestep to learn a game-winning policy.\nAs seen in Figure 3d, the ER striker (light blue circle) does not learn to score goals as the algorithm settles quickly on collecting positive dribble from far rewards rD,far and moving to the ball rewards rmove. The ER agent therefore gets stuck in a local optima causing the agent to execute D until it\n3 http://www.collinsdictionary.com/dictionary/american/run-out-the-clock 4 https://youtu.be/xA-8rWJ4a7I\nsettles on the M skill and stands on the ball, receiving small positive rewards. As seen in Table 2, the ER agent only manages to score 1.7\u00b1 1.2 goals on average and has a low average reward \u22120.3\u00b1 0.1, well below the \u03b2 threshold.\nThese rewards are therefore not enough to enable the SARiCoS agent (yellow circle in Figure 3d) to pass its performance threshold \u03b2, especially since, in the losing scenario, the agents are also receiving a negative game score reward rscore at each timestep. This forces the SARiCoS agent to search for additional rewards such as a goal-scoring reward. As seen in Table 2, the SARiCoS agent learns to score goals (74\u00b1 6.5), and achieves average reward well above the \u03b2 performance threshold. As a result it mitigates the reward shaping-based model misspecification."}, {"heading": "7 Discussion", "text": "We have defined a PG-SMDP which provides a natural risk-sensitive objective for learning SA in hierarchical RL. We find it interesting that an agent can learn a complex human behavior by simply maximizing a risk-sensitive objective. To do so, we have introduced Risk-Aware Skills (RASs) \u2014 a type of parameterized option Sutton et al. (1999) with an additional Risk-Aware Parameter (RAP). We have developed the Situational Awareness by Risk-Conscious Skills (SARiCoS) algorithm which learns both the inter-skill policy that chooses RASs to execute, as well as learning the RAPs for each RAS. We have shown that this algorithm converges to a locally optimal solution. We also show that SARiCoS can induce situational awareness (E.g. \u2018time-wasting\u2019) in Risk-Aware Skills in a time dependent RoboCup soccer scenario. In principle, any other risk criteria can be incorporated into this work such as exponential risk, CVaR and VaR Avila-Godoy & Fern\u00e1ndez-Gaucherand (1998); Tamar et al. (2015a,b). Extensions of this work include optimizing a PG-MDP performance threshold \u03b2 for each RAS as well as utilizing SA in lifelong learning problems Thrun & Mitchell (1995); Pickett & Barto (2002); Brunskill & Li (2014). The SARiCoS policy could also be implemented as a Deep Network Mnih et al. (2015), leading to more complex policies on higher dimensional problems."}, {"heading": "Acknowledgements", "text": "The research leading to these results has received funding from the European Research Council under the European Union\u2019s Seventh Framework Program (FP/2007-2013) / ERC Grant Agreement n. 306638."}, {"heading": "A SARiCoS Supplementary Material", "text": "A.1 Full Derivation of Theorem 1\nWe define the expected reward for following a policy \u00b5\u03b1,\u2126 as:\nJ(\u00b5\u03b1,\u2126) = \u222b \u03c4 P (\u03c4 |\u03b1,\u2126)R(\u03c4)d\u03c4 . (13)\nLet us group the parameters for the inter-RAS policy and the continuous RADPs into a single vector \u03c7 = [\u03b1,\u2126] \u2208 Rd+m\u00b7N . Taking the derivative of this objective and using the well-known likelihood trick Peters & Schaal (2008) yields:\n\u2207\u03c7J(\u00b5\u03c7) = \u2207\u03c7 \u222b \u03c4 P (\u03c4 |\u03c7)R(\u03c4)d\u03c4 (14)\n= \u222b \u03c4 \u2207\u03c7P (\u03c4 |\u03c7)R(\u03c4)d\u03c4 (15)\n= \u222b \u03c4 P (\u03c4 |\u03c7)\u2207\u03b1 logP (\u03c4 |\u03c7)R(\u03c4)d\u03c4 , (16)\nwhere P (\u03c4 |\u03c7) = P (z0) \u220fT k=1 P (zk+1|zk, \u03c3k)P (\u03c3k|zk, \u03c7) where zk \u2208 Z is the state at timestep k; \u03c3k is the RAS selected at timestep k and T is the length of the trajectory. Since only P (\u03c3k|zk, \u03c7) is parameterized, the gradient\u2207\u03c7J(\u00b5\u03c7) can be simplified as follows: \u2207\u03c7J(\u00b5\u03c7) = \u222b \u03c4 P (\u03c4 |\u03c7)\u2207\u03c7 logP (\u03c4 |\u03c7)R(\u03c4)d\u03c4 (17)\n= \u222b \u03c4 P (\u03c4 |\u03c7)\u2207\u03c7 log [ P (z0) T\u220f k=1 P (zk+1|zk, \u03c3k)P (\u03c3k|zk, \u03c7) ] R(\u03c4)d\u03c4 (18)\n= \u222b \u03c4 P (\u03c4 |\u03c7)\u2207\u03c7 [ logP (z0) + \u03a3 T k=1 logP (zk+1|zk, \u03c3k) + logP (\u03c3k|zk, \u03c7) ] R(\u03c4)d\u03c4(19)\n= \u222b \u03c4 P (\u03c4 |\u03c7)\u2207\u03c7 logP (\u03c3k|zk, \u03c7)R(\u03c4)d\u03c4 (20)\nwhere P (\u03c3k|zk, \u03c7) = \u00b5\u03b1(\u03c3t|zt)\u00b5\u03c3t\u2126 (yt|zt). Therefore, substituting the two-tiered policy into Equation 20 and deriving with respect to \u03b1 leads to the gradient update rule:\n\u2207\u03b1J(\u00b5\u03c7) = \u222b \u03c4 P (\u03c4 |\u03c7)\u2207\u03b1 log\u00b5\u03b1(\u03c3t|zt)R(\u03c4)d\u03c4 .\nIf we represent \u00b5\u03b1(\u03c3t|zt) as a Gibb\u2019s distribution which is a common policy choice in many MDPs Sutton & Barto (1998), then we can easily estimate the gradient by sampling:\n\u2207\u03b1J(\u00b5\u03c7) = \u2329 H\u2211 h=0 \u2207\u03b1 log\u00b5\u03b1(\u03c3h|zh) H\u2211 j=0 \u03b3jrj \u232a , (21)\nwhere H is the length of a trajectory; and \u2329 \u00b7 \u232a represents an average over trajectories. If we derive\nEquation 20 with respect to \u2126 for the RADPs, then we get the following gradient update rule:\n\u2207\u2126J(\u00b5\u03c7) = \u222b \u03c4 P (\u03c4 |\u03c7)\u2207\u2126 log\u00b5\u03c3t\u2126 (yt|zt)R(\u03c4)d\u03c4 .\nIf we represent \u00b5\u03c3t\u2126 (yt|zt) as any distribution from the natural exponential family, then we can easily estimate the gradient by samples using the following gradient update rule:\n\u2207\u2126J(\u00b5\u03c7) = \u2329 H\u2211 h=0 \u2207\u2126 log\u00b5\u03c3t\u2126 (yt|zt) H\u2211 j=0 \u03b3jrj \u232a . (22)\nThese derivations are summarized in Theorem 1. Theorem 1 (Gradient Update Derivation). Suppose that we are maximizing the Policy Gradient (PG) objective J(\u00b5\u03b1,\u2126) = \u222b \u03c4 P\u03b1,\u2126(\u03c4)R(\u03c4)d\u03c4 using risk-aware trajectories, generated by the twotiered skill selection policy \u00b5\u03b1(\u03c3t|xt)\u00b5\u03c3t\u2126 (yt|zt), then the expectation of the gradient update rules for the inter-skill policy parameters \u03b1 \u2208 Rd and the RAD parameters \u2126 \u2208 R|N ||m| are the true gradients and are defined as (1) \u2207\u03b1J(\u00b5\u03b1,\u2126) = \u2329\u2211H h=0\u2207\u03b1 log\u00b5\u03b1(\u03c3h|zh) \u2211H j=0 \u03b3 jrj \u232a and (2)\n\u2207\u2126J(\u00b5\u03b1,\u2126) = \u2329\u2211H h=0\u2207\u2126 log\u00b5 \u03c3t \u2126 (yt|zt) \u2211H j=0 \u03b3 jrj \u232a respectively. H is the trajectory length and < \u00b7 > is an average over trajectories as in standard PG.\nA.2 Proof of Theorem 2: SARiCoS Convergence\nTheorem 2 (SARiCoS Convergence). Suppose we are optimizing the expected return J(\u00b5\u2126,\u03b1) =\u222b R(\u03c4)P (\u03c4)d\u03c4 for any arbitrary SARiCoS policy \u00b5\u2126,\u03b1 where \u2126 \u2208 R|N ||m| and \u03b1 \u2208 Rd are the inter-skill and Risk Aware Distribution parameters respectively. Then, for step sizes sequences {ak}\u221ek=0, {bk}\u221ek=0 that satisfy \u2211 k ak = \u221e, \u2211 k bk = \u221e, \u2211 k a 2 k < \u221e, \u2211 k b 2 k < \u221e and bk > ak, the SARiCoS iterates converge a.s \u03b1k \u2192 \u03b1\u2217,\u2126k \u2192 \u03bb\u0304(\u03b1\u2217) as k \u2192\u221e to the countable set of locally optimal points of J(\u00b5\u2126,\u03b1).\nThe true gradient of the two-tiered policy \u00b5\u2126,\u03b1 is:\n\u2207\u2126,\u03b1J(\u00b5\u2126,\u03b1) = E[R\u03c4\u2207 logP (\u03c4)]\nwhere R\u03c4 = \u2211h\u22121 t=0 \u03b3\ntrt is the discounted cumulative reward for a trajectory \u03c4 of length h; the term P (\u03c4) = P (x0)\u03a0h\u22121i=0 P (xi+1|xi, \u03c3i)\u00b5\u2126,\u03b1(\u03c3i, yi|xi) is the probability of a trajectory for a given policy \u00b5\u2126,\u03b1(\u03c3i, yi|xi). The estimated gradient is:\n\u2207\u0302J(x) = R\u03c4\u2207 log\u00b5\u2126,\u03b1(\u03c3t, yt|xt) = R\u03c4\u2207 log(\u00b5\u03b1(\u03c3t|xt)\u00b5\u03c3t\u2126 (yt|xt))\nWe need to prove that the parameters \u03b1 \u2208 Rd of the inter-skill policy and the risk-aware parameters \u2126 \u2208 R|N ||m| converge to a locally optimal solution. Here, N is the number of skills and m is the number of risk-aware distribution parameters for each skill. In order to do so, we first derive the gradient with respect to \u03b1 to yield the following recursive update equations:\n\u03b1k+1 = \u0393\u03b1(\u03b1k + ak\u2207\u0302\u03b1J(x)) = \u0393\u03b1(\u03b1k + ak(R\u03c4\u2207\u03b1 log\u00b5\u03b1(\u03c3t|xt))) (1) = \u0393\u03b1(\u03b1k + ak(R\u03c4z \u03b1 k ))\n= \u0393\u03b1(\u03b1k + ak(R\u03c4z \u03b1 k \u2212 E[R\u03c4z\u03b1k ] + E[R\u03c4z\u03b1k ]))\n= \u0393\u03b1(\u03b1k + ak(f(\u03b1(k),\u2126) +Nk+1)) (23)\nwhere (1) z\u03b1k = \u2207\u03b1 log\u00b5\u03b1(\u03c3t|xt) and Nk+1 = R\u03c4z\u03b1k \u2212 E[R\u03c4z\u03b1k ] is a zero-mean martingale difference sequence; f(\u03b1(k),\u2126) = E[R\u03c4z\u03b1k ] and \u0393\u03b1 : Rd \u2192 Rd is a projection operator that projects any \u03b1k to a compact region C = {\u03b1|gi(\u03b1) \u2264 0, i = 1, \u00b7 \u00b7 \u00b7 l} \u2208 Rn where gi(\u00b7), i = 1, \u00b7 \u00b7 \u00b7 l represent\nthe continuously differentiable constraints that project the iterates to a compact region defined by a ball with a smooth boundary. This operator ensures that the iterates remain bounded. It can be seen by inspection that this recursion represents a noisy discretization of the Ordinary Differential Equation (ODE) Borkar (1997):\n\u03b1\u0307 = \u0393\u03b1(E[R\u03c4z\u03b1k ])\nWe also derive the recursive update for the risk-aware parameters \u2126 as follows:\n\u2126k+1 = \u0393\u2126(\u2126k + bk\u2207\u0302\u2126J(x)) = \u0393\u2126(\u2126k + bk(R\u03c4\u2207 log\u00b5\u03c3t\u2126 (yt|xt)) = \u0393\u2126(\u2126k + bk(R\u03c4z \u2126 k ))\n= \u0393\u2126(\u2126k + bk(R\u03c4z \u2126 k \u2212 E[R\u03c4z\u2126k ] + E[R\u03c4z\u2126k ]))\n= \u0393\u2126(\u2126k + bk(g(\u2126(k), \u03b1) +Mk+1)), (24)\nwhere Mk+1 = R\u03c4z\u2126k \u2212 E[R\u03c4z\u2126k ] is a zero mean martingale difference sequence with respect to the \u03c3\u2212fields Ft = \u03c3(\u2126n, \u03b1n, Nn,Mn, n \u2264 t; t \u2265 0); g(\u2126(k), \u03b1) = E[R\u03c4z\u2126k ] and \u0393\u2126 : R|N ||m| \u2192 R|N ||m| is the corresponding projection operator for \u2126 which ensures that these iterates are projected to a compact region W as in the previous iterate update equation. We can thus represent the \u2126 update with the following ODE:\n\u2126\u0307 = \u0393\u2126(E[R\u03c4z\u2126k ]) Define the continuous time projection operators \u0393\u0302\u2126(v) = lim\u03b4\u2192\u221e \u0393\u2126(\u2126+\u03b4v)\u2212\u2126 \u03b4 and \u0393\u0302\u03b1(p) = lim\u03b4\u2192\u221e \u0393\u03b1(\u03b1+\u03b4v)\u2212\u03b1\n\u03b4 that, given directions v and p to modify the paramaters \u2126 and \u03b1 respectively ensures that the iterates are projected into their compact sets C and W respectively. We can thus define the ODEs using this continuous operator as:\n\u2126\u0307 = \u0393\u0302\u2126(E[R\u03c4z\u2126k ]) . = g\u0304(\u2126(k), \u03b1) \u03b1\u0307 = \u0393\u0302\u03b1(E[R\u03c4z\u03b1k ]) . = f\u0304(\u03b1(k),\u2126)\nAssumption (A1): For each \u03b1 \u2208 Rd, the ODE:\n\u2126\u0307(t) = g\u0304(\u2126(k), \u03b1)\nhas a globally asymptotically stable equilibrium \u03bb\u0304(\u03b1) such that \u03bb\u0304 : Rd \u2192 R|N ||m| is Lipschitz. Assumption (A2): The ODE:\n\u03b1\u0307 = f\u0304(\u03b1(k), \u03bb\u0304(\u03b1(k)))\nhas a unique global asymptotically stable equilibrium \u03b1\u2217.\nIn order to prove that these ODEs collectively converge, we need to make the following assumptions.\nAssumption (A3): The functions f, g are Lipschitz continuous functions\nAssumption (A4): supk \u2016\u2126k\u2016, supk \u2016\u03b1k\u2016 <\u221e Assumption (A5): \u2211 n a(n) = \u2211 n b(n) =\u221e, \u2211 n a(n) 2 = \u2211 n b(n)\n2 <\u221e Assumption (A6): For increasing \u03c3-algebras, the martingale sequences \u2211 akNk, \u2211 bkMk <\u221e a.s\nAssumption (A7) For all \u03b1,\u2126, the objective function J(\u00b5\u03b1,\u2126) has bounded second derivatives and the set Z of local optima of J(\u00b5\u03b1,\u2126) are countable.\nGiven the above assumptions, the parameter \u2126k \u2192 \u03bb\u0304(\u03b1\u2217) and \u03b1k \u2192 \u03b1\u2217 as k \u2192\u221e a.s by standard two-timescale stochastic approximation arguments Borkar (1997). That is, the iterates converge to {\u03bb\u0304(\u03b1\u2217), \u03b1\u2217|\u03b1\u2217 \u2208 Z} .\nA.3 SARiCoS Video\nA video is attached with the supplementary material showing an agent (the striker) applying the learned risk-aware skills in a one-on-one scenario with a goalkeeper. The videos exhibit the Situational Awareness (SA) of the agent in both a losing scenario and a winning scenario."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Hierarchical Reinforcement Learning has been previously shown to speed up the convergence rate of RL planning algorithms as well as mitigate feature-based model misspecification Mankowitz et al. (2016a,b); Bacon & Precup (2015). To do so, it utilizes hierarchical abstractions, also known as skills \u2013 a type of temporally extended action Sutton et al. (1999) to plan at a higher level, abstracting away from the lower-level details. We incorporate risk sensitivity, also referred to as Situational Awareness (SA) , into hierarchical RL for the first time by defining and learning risk aware skills in a Probabilistic Goal Semi-Markov Decision Process (PG-SMDP). This is achieved using our novel Situational Awareness by Risk-Conscious Skills (SARiCoS) algorithm which comes with a theoretical convergence guarantee. We show in a RoboCup soccer domain that the learned risk aware skills exhibit complex human behaviors such as \u2018time-wasting\u2019 in a soccer game. In addition, the learned risk aware skills are able to mitigate reward-based model misspecification.", "creator": "LaTeX with hyperref package"}}}