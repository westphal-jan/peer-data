{"id": "1610.03256", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Oct-2016", "title": "GMM-Free Flat Start Sequence-Discriminative DNN Training", "abstract": "Recently, attempts have been made to remove Gaussian mixture models (GMM) from the training process of deep neural network-based hidden Markov models (HMM/DNN). For the GMM-free training of a HMM/DNN hybrid we have to solve two problems, namely the initial alignment of the frame-level state labels and the creation of context-dependent states. Although flat-start training via iteratively realigning and retraining the DNN using a frame-level error function is viable, it is quite cumbersome. Here, we propose to use a sequence-discriminative training criterion for flat start. While sequence-discriminative training is routinely applied only in the final phase of model training, we show that with proper caution it is also suitable for getting an alignment of context-independent DNN models. For the construction of tied states we apply a recently proposed KL-divergence-based state clustering method, hence our whole training process is GMM-free. In the experimental evaluation we found that the sequence-discriminative flat start training method is not only significantly faster than the straightforward approach of iterative retraining and realignment, but the word error rates attained are slightly better as well. In addition to the results, the results obtained from the first step of the training procedure show that a Gaussian state is more effective at associating input and input states in both order to achieve the same result. For this reason, it is possible to have both the same input and input states without interfering in the alignment of the frame-level state labels. However, we can find that this would not be possible without the additional time that the parameters from the model-discriminative flat start training system are used in the final phase of model training. To our surprise, we have found that the model-discriminative flat start training process is faster and faster than the simple model-discriminative flat start training procedure. To further improve our understanding of model-discriminative flat start training, we have implemented a hierarchical model-discriminative approach based on a model-discriminative grid grid. Each set of model-discriminative grid can be found by using a new model-discriminative grid. The grid of the grid is a grid of a set of two (left) and a right, in the order of the minimum size of a grid and the maximum size of a grid. To achieve the same result, we have", "histories": [["v1", "Tue, 11 Oct 2016 09:52:57 GMT  (23kb)", "http://arxiv.org/abs/1610.03256v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["g\\'abor gosztolya", "tam\\'as gr\\'osz", "l\\'aszl\\'o t\\'oth"], "accepted": false, "id": "1610.03256"}, "pdf": {"name": "1610.03256.pdf", "metadata": {"source": "CRF", "title": "GMM-Free Flat Start Sequence-Discriminative DNN Training", "authors": ["G\u00e1bor Gosztolya"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n03 25\n6v 1\n[ cs\n.C L\n] 1\n1 O\nct 2\n01 6"}, {"heading": "1. Introduction", "text": "While deep neural network (DNN) based speech recognizers have recently replaced Gaussian mixture (GMM) based systems as the state-of-the-art in ASR, the training process of HMM/DNN hybrids still relies on the HMM/GMM framework. Conventionally, we start the training of a HMM/DNN by constructing a HMM/GMM system, which is then applied to get an alignment for the frame-level state labels. These labels are then used as the training targets for the DNN. The second task that requires GMMs is the state tying algorithm utilized for the construction of context-dependent (CD) phone models. We proposed a GMM-free solution for state clustering earlier [1], and in this study we will focus on the issue of obtaining the initial state alignment for training the DNN.\nThe most convenient way of training the DNN component of a HMM/DNN hybrid is by applying a frame-level error criterion, which is usually the cross-entropy (CE) function. This solution, however, requires frame-aligned training labels, while the training dataset contains just orthographic transcripts in most cases. Trivially, one may train a HMM/GMM system to get aligned labels, but this is clearly a waste of resources.\nThe procedure for training HMM/GMM systems without alignment information is commonly known as \u2019flat start training\u2019 [2]. This consists of initializing all phone models with the same parameters, which would result in a uniform alignment\nof phone boundaries in the first iteration of Baum-Welch training. It is possible to construct a flat start-like training procedure for CE-trained DNNs as well, by iteratively training and realigning the DNN. For example, Senior et al. randomly initialized their neural network [3], while Zhang et al. trained their first model on equal-sized segments for each state [4]. As these solutions have a slow convergence rate, they require a lot of training-realignment loops.\nAlthough training the DNN at the frame level is straightforward, it is clearly not optimal, as the recognition is performed and evaluated at the sentence level. Within the framework of HMM/GMM systems, several sequence-discriminative training methods have been developed, and these have now been adapted to HMM/DNN hybrids as well [5, 6, 7]. However, most authors apply sequence-discriminative criteria only in the final phase of training, for the refinement of the DNN model. That is, the first step is always CE-based training, either to initialize the DNN (e.g. [8, 9, 10]) or just to provide frame-level state labels (e.g. [5, 6, 11, 12, 13]).\nThe Connectionist Temporal Classification (CTC) approach has recently become very popular for training DNNs without an initial time alignment being available [14]. Rao et al. proposed a flat start training procedure which is built on CTC [15]. However, CTC has several drawbacks compared to MMI. First, it introduces blank labels, which require special care in the later steps (e.g. CD state tying) of the training process. Second, the CTC algorithm is not a sequence-discriminative training method, so for the best performance it has to be combined with techniques like sMBR training [14, 15].\nIn contrast with the previous authors, here we propose a training procedure that applies sequence-discriminative training in the flat-start training phase. This requires several small modifications compared to the standard usage of sequencediscriminative training, which will be discussed in detail. In the experimental part we compare the proposed method with the CE-based iterative retraining-realignment procedure of Zhang et al. [4], and we find that our method is faster and gives slightly lower word error rates. Furthermore, we can combine sequence-discriminative flat start training with the KullbackLeibler divergence-based state clustering method we proposed recently [1]. With this, we eliminate all dependencies from a HMM/GMM system, making the whole training procedure of context-dependent HMM/DNNs GMM-free."}, {"heading": "2. Flat-start training of HMM/DNN", "text": "Conventionally, the training of a HMM/DNN system is initiated by training a HMM/GMM just to get time-aligned training labels. Here, we compare two approaches that seek to eliminate GMMs from this process. As the baseline method, we apply a simple solution that iterates the loop of CE DNN training and\nrealignment. Afterwards, we propose an approach that creates time-aligned transcriptions for the training data by training a DNN with a sequence training criterion. From the wide variety of sequence training methods, we opted for MMI (Maximum Mutual Information) training [5]. Applying sequence training to flat start requires some slight modifications, which we will now discuss."}, {"heading": "2.1. Iterative CE Training and Realignment", "text": "For comparison we will also test what is perhaps the most straightforward solution for flat start DNN training, namely just using the CE training criterion and iterating DNN training and realignment. Here, we used the following algorithm that was based on the description of Zhang et al. [4]:\n1. Train a DNN using uniformly segmented sound files.\n2. Use the current DNN to realign the labels.\n3. Train a randomly initialized DNN using the new alignments.\n4. Repeat steps 2\u20133 several times.\nThe final DNN was utilized to create time-aligned labels for the training set.\nThe main advantage of this method is that it requires only an implementation of CE training for the DNN, and the realignment step can also be readily performed by using standard ASR toolkits. The drawback is that the procedure of retraining and realignment tends to be rather time-consuming, which was also confirmed by our experiments (see Section 6)."}, {"heading": "2.2. Sequence-Discriminative Training Using MMI", "text": "Several sequence-discriminative training criteria have been developed for HMM/GMMs [16] \u2013 and adapted to HMM/DNNs [5, 6, 12, 17] \u2013 from which the maximum mutual information (MMI) criterion is the oldest and simplest. The MMI function measures the mutual information between the distribution of the observation and the phoneme sequence. Denoting the sequence of all observations by Ou = ou1, . . . , ouTu , and the label-sequence for utterance u by Wu, the MMI criterion can be defined by the formula\nFMMI = \u2211\nu\nlog p(Ou|Su) \u03b1p(Wu) \u2211\nW p(Ou|S)\u03b1p(W )\n, (1)\nwhere Su = su1, . . . , suTu is the sequence of states corresponding to Wu, and \u03b1 is the acoustic scaling factor. The sum in the denominator is taken over all phoneme sequences in the decoded speech lattice for u. Differentiating Eq. (1) with respect to the log-likelihood log p(out|r) for state r at time t, we get\n\u2202FMMI\n\u2202 log p(out|r) = \u03b1\u03b4r;sut \u2212\n\u03b1 \u2211\nW :st=r p(Ou|S) \u03b1p(W ) \u2211\nW p(Ou|S)\u03b1p(W )\n(2)\n= \u03b1 ( \u03b4r;sut \u2212 \u03b3 DEN ut (r) ) ,\nwhere \u03b3DENut (r) is the posterior probability of being in state r at time t, computed over the denominator lattices for utterance u using the forward-backward algorithm, and \u03b4r;sut is the Kronecker delta function (the binary frame-level phonetic targets)."}, {"heading": "3. Performing DNN Flat Start with MMI", "text": "Sequence training criteria like the MMI error function are now widely used in DNN training. However, all authors initialize their networks using CE training, and apply the sequencediscriminative criterion only in the final phase of the training procedure, to fine-tune their models [6, 12], which makes it necessary to use some method (HMM/GMM or iterative CE training) to provide frame-level state targets. In contrast with these authors, here we propose to apply MMI training in the flat start phase. In order to be able to perform flat start of randomly initialized DNNs using sequence training, we made some slight changes in the standard MMI process, which we will describe next.\nFirstly, we use the numerator occupancies \u03b3NUMut (r) in Eq. (2) instead of the \u03b4r;sut values. This way we can work with smoother targets instead of the crude binary ones usually employed during DNN training. Another advantage of eliminating the \u03b4r;sut values is that it allows us to skip the preceding (usually GMM-based) label alignment step, responsible for generating the frame-level training targets. We applied the forwardbackward algorithm to obtain the \u03b3NUMut (r) values, which solution has been mentioned in some studies (e.g. [6, 17]), but we only found Zhou et al. [8] actually doing this. However, they pre-trained their DNN with the CE criterion first, while we apply MMI training from the beginning, starting with randomly initialized weights.\nThe second difference is that sequence training is conventionally applied only to refine a fully trained system. Thus, the MMI training criterion is calculated with CD phone models and a word-level language model. This makes the decoding process slow, and hence the numerator and denominator lattices are calculated only once, before starting MMI training. In contrast to this, we execute sequence DNN training using only phone-level transcripts and CI phone models. This allows very fast decoding, so we can recalculate the lattices after each sentence. This difference is crucial for the fast convergence of our procedure. For converting the orthographic transcripts to phone sequences one can follow the strategy of HTK. That is, in the very first step we get the phonetic transcripts from the dictionary, with no silences between the words. Pronunciation alternatives and the optional short pause at word endings can be added later on, performing realignment with a sufficiently well-trained model [2].\nA further difference is that we use no state priors or language model, which makes the \u03b1 scaling factor in Eq. (2) unnecessary as well. Next, to reduce the computational requirements of the algorithm, we estimated \u03b3DENut (r) using just the most probable decoded path instead of summing over all possible paths in the lattice (denoted by \u03b3\u0302DENut (r)).\nWith these modifications, the gradient with respect to the output activations (aut) of the DNN is found using\n\u2202FMMI \u2202aut(s) = \u2211\nr\n\u2202FMMI\n\u2202 log p(out|r)\n\u2202 log p(out|r)\n\u2202aut(s) (3)\n= \u03b3NUMut (s)\u2212 \u03b3\u0302 DEN ut (s),\nwhich can be applied directly for DNN training. A standard technique in DNN training is to separate a hold-out set from the training data (see e.g. [18]). If the error increases on this hold-out set after a training iteration, then the DNN weights are restored from a backup and the training continues with a smaller learning rate. This strategy can be readily adapted to sequence DNN training [5], and we found it to be essential for the stability of our flat-start MMI DNN training method.\nTable 1 summarizes the modifications that we made to make MMI suitable for DNN flat start. Note that steps (1) through (4) seek to simplify the procedure both to speed it up and to make it more robust. Step (2) also helps us to perform sequencediscriminative DNN training before CD state tying, which is essential for applying it in flat start. Step (5), however, is applied in our general DNN training process, but we found it essential to avoid the \u201crunaway silence model\u201d issue which is a common side effect haunting sequence-discriminative DNN training."}, {"heading": "4. KL divergence-based CD state tying", "text": "Having aligned the CI phone models using flat-start training, the next step is the construction of CD models. Currently, the dominant solution for this is the decision tree-based state tying method [19]. This technique pools all context variants of a state, and then builds a decision tree by successively splitting this set into two, according to one of the pre-defined questions. For each step, it fits Gaussians on the distribution of the states, and chooses the question which leads to the highest likelihood gain. However, modeling the distribution of states with a Gaussian function might be suboptimal when we utilize DNNs in the final acoustic model.\nTo this end, we decided to first train an auxiliary neural network on the CI target labels and then perform the CD state tying based on the output of this network. Such a frame-level output can be treated as a discrete probability distribution, and a natural distance function for such distributions is the KullbackLeibler (KL) divergence [20]. Therefore, to control the state tying process, we utilized the KL divergence-based decision criterion introduced by Imseng et al. [21, 22]. We basically replaced the Gaussian-based likelihood function with a KL-divergence based state divergence function; in other respects, the mechanism of the CD state tying process remained the same. With this technique we were not only able to eliminate GMMs from the state tying process, but we also achieved a 4% reduction in WER. For details, see [1]."}, {"heading": "5. Experimental Setup", "text": "Our experimental setup is essentially the same as that of our previous study [1]. We employed a DNN with 5 hidden layers, each containing 1000 rectified neurons [23], while the softmax activation function was applied in the output layer. We used our custom DNN implementation for GPU, which achieved out-\nstanding results on several datasets (e.g. [24, 25, 26, 27, 28]). We used 40 mel filter bank energies as features along with their first and second order derivatives. Decoding and evaluation was performed by a modified version of HTK [2].\nThe 28 hour-long speech corpus of Hungarian broadcast news [29] was collected from eight TV channels. The training set was about 22 hours long, a small part (2 hours) was used for validation purposes, and a 4-hour part was used for testing. We used a trigram language model and a vocabulary of 500k word forms. The order of utterances was randomized at the beginning of training. We configured the state tying algorithms to get roughly 600, 1200, 1800, 2400, 3000 and 3600 tied states.\nWe tested four approaches for flat start training (i.e. to get the frame-level phonetic targets for CD state tying and CE DNN training). Firstly, we applied the standard GMM-based flat-start training to produce initial time-aligned labels. To further improve the segmentation, we trained a shallow CI ANN using the CE criterion and re-aligned the frame labels based on the outputs of this ANN (we will refer to this approach as the \u201cGMM + ANN\u201d method). (In our previous study we found that using a deep network for this re-alignment setup did not give any significant improvement [1].) After the realignment, we applied both the standard GMM-based and our KL-criterion algorithms for state tying. Then KL-based state tying was performed on the output of the CI ANN.\nBesides the standard GMM flat start approach, we evaluated the two algorithms presented in sections 2 and 3 for flat starting with DNNs. In these tests we always used five-hiddenlayer CI DNNs. For the flat-start method with iterative CE training (\u201cIterative CE\u201d) we performed four training-aligning iterations, and KL-based CD state tying was performed using the output and the alignments created by the final DNN. For MMI training (\u201cMMI\u201d) we also commenced with a randomly initialized CI DNN. After applying the discriminative sequence training method proposed in Section 3, the resulting DNN was used to create forced aligned labels and also to provide the input posterior estimates for KL clustering. In the last flat start approach tested, we first applied the sequence-discriminative flat start method (i.e. \u201cMMI\u201d). Then, based on the alignments of this network, we trained another DNN with the CE criterion to supply both the final frame labels and the likelihoods for KLbased CE state tying (\u201cMMI + CE\u201d).\nThe aim of this study was to compare various flat-start strategies. This is why, after obtaining the CD labels, the final DNN models were trained starting from randomly initialized weights and using just the CE criterion. Of course, it might be possible to extend the training with a final refinement step using sequence-discriminative training."}, {"heading": "6. Results and Discussion", "text": "Figure 1 shows the resulting WER scores as a function of the number of CD tied states. As can be seen, the MMI-based flat start strategy gave slightly better results than the iterative method in every case. We also observed that the final CD models which got their training labels from the MMI-trained DNN were more stable with respect to varying the number of CD states. Fine-tuning the labels of the MMI-trained DNN with a CE-trained DNN (\u201cMMI\u201d vs. \u201cMMI+CE\u201d) seems unnecessary, as it was not able to significantly improve the results. This indicates that sequence training yields both fine alignments and good posterior estimates.\nTable 2 summarizes the best WER values on the development set, and the corresponding scores on the test set. The KL clustering method clearly outperformed the GMM-based state tying technique. Comparing the alignment methods, we see that relying on the alignments produced by the HMM/GMM resulted in the lowest accuracy score, in spite of the fine-tuning step using an ANN. With the parameter configurations applied, the iterative CE training method performed slightly worse than the MMI-based strategies. Unfortunately, for the iterative CE method the right number of training-aligning steps is hard to tune. For example, Zhang et al. performed 20 such iterations [4], while we employed only 4 iterations. In this respect, it is more informative to compare the training times, which are shown in the rightmost column of Table 2. (We did not indicate the number of epochs for the \u201cGMM + ANN\u201d method, as the training procedure was radically different there.) For our 28-hour dataset, 48 epochs were required by the four iterations of iterative CE flat-start training, while MMI required only one-fourth of it; and although performing the forwardbackward search adds a slight overhead to the training process, it is clear that MMI was still much faster, even when the final CE re-alignment step was also involved.\nMeasuring the training times in CPU/GPU time gives even larger differences in favor of the MMI method (3 hours vs. 16 hours). The reason is that for iterative CE flat-start training we used a mini-batch of 100 frames (which we found optimal previously [1]), while for MMI whole utterances (usually more than 1000 frames) were used to update the weights, and this allowed better parallelization on the GPU.\nIn our view, two modifications are crucial for the speed and\nstability of the proposed algorithm. The first one is that we use only CI phone models without phone language model, so we can very quickly update the numerator and denominator lattices after the processing of each sentence. This continuous refinement of the frame-level soft targets obviously leads to a faster convergence. The only study we know of, which does not perform the re-alignment of the frame-level targets strictly after a training iteration, is that of Bacchiani et al. [30]. Their study focuses on describing their massively parallelized online neural network optimization system, where a separate thread is responsible for the alignment of the phonetic targets, while DNN training is performed by the client machines. Besides the fact that in their model there is no guarantee for that the alignment of phonetic targets are up-to-date, it is easy to see that their architecture is quite different from a standard DNN training architecture, making their techniques pretty hard to adapt. In contrast, our slight modifications can be applied relatively easily.\nAs regards stability, a known drawback of sequence training methods is that the same process is responsible both for aligning and training the DNN, which often leads to the \u201crun-away silence model\u201d issue [31]. That is, after a few iterations, only one model (usually the silence model) dominates most parts of the utterances, which is even reinforced in the next training step. To detect the occurrence of this phenomenon, we monitored the error rate on a hold-out set during training. If the error increased after an iteration, we restored the weights of the network to their previous values and the learning rate was halved. In our experience, restoring the weights to their previous values and continuing the training using a lower learning rate can successfully handle this issue."}, {"heading": "7. Conclusions", "text": "Here, we showed how to perform flat start with sequencediscriminative DNN training. We applied the standard MMI sequence training method, for which we introduced several minor modifications. Our results showed that, compared to the standard procedure of iterative CE DNN training and re-alignment, not only were we able to reduce the WER scores, but we also achieved a significant reduction in training times. By also utilizing the Kullback-Leibler divergence-based CD state tying method proposed earlier, we made the whole training procedure of context-dependent HMM/DNNs GMM-free."}, {"heading": "8. References", "text": "[1] G. Gosztolya, T. Gro\u0301sz, L. To\u0301th, and D. Imseng, \u201cBuilding\ncontext-dependent DNN acousitc models using Kullback-Leibler divergence-based state tying,\u201d in Proceedings of ICASSP, 2015, pp. 4570\u20134574.\n[2] S. Young, G. Evermann, M. J. F. Gales, T. Hain, D. Kershaw, G. Moore, J. Odell, D. Ollason, D. Povey, V. Valtchev, and P. Woodland, The HTK Book. Cambridge, UK: Cambridge University Engineering Department, 2006.\n[3] A. Senior, G. Heigold, M. Bacchiani, and H. Liao, \u201cGMM-free DNN acoustic model training,\u201d in Proceedings of ICASSP, 2014, pp. 5639\u20135643.\n[4] C. Zhang and P. Woodland, \u201cStandalone training of contextdependent Deep Neural Network acoustic models,\u201d in Proceedings of ICASSP, 2014, pp. 5634\u20135638.\n[5] B. Kingsbury, \u201cLattice-based optimization of sequence classification criteria for neural-network acoustic modeling,\u201d in Proceedings of ICASSP, 2009, pp. 3761\u20133764.\n[6] K. Vesely\u0301, A. Ghoshal, L. Burget, and D. Povey, \u201cSequencediscriminative training of deep neural networks,\u201d in Proceedings of Interspeech, 2013, pp. 2345\u20132349.\n[7] T. Gro\u0301sz, G. Gosztolya, and L. To\u0301th, \u201cA sequence training method for Deep Rectifier Neural Networks in speech recognition.\u201d in Proceedings of SPECOM, Novi Sad, Serbia, Sep 2014, pp. 81\u2013 88.\n[8] P. Zhou, L. Dai, and H. Jiang, \u201cSequence training of multiple Deep Neural Networks for better performance and faster training speed,\u201d in Proceedings of ICASSP, 2014, pp. 5664\u20135668.\n[9] E. McDermott, G. Heigold, P. Moreno, A. Senior, and M. Bacchiani, \u201cAsynchronous stochastic optimization for sequence training of Deep Neural Networks: Towards big data,\u201d in Proceedings of Interspeech, 2014, pp. 1224\u20131228.\n[10] A. Mohamed, D. Yu, and L. Deng, \u201cInvestigation of full-sequence training of Deep Belief Networks for speech recognition,\u201d in Proceedings of Interspeech, 2010, pp. 2846\u20132849.\n[11] G. Saon and H. Soltau, \u201cA comparison of two optimization techniques for sequence discriminative training of Deep Neural Networks,\u201d in Proceedings of ICASSP, 2014, pp. 5604\u20135608.\n[12] S. Wiesler, P. Golik, R. Schu\u0308ter, and H. Ney, \u201cInvestigations on sequence training of neural networks,\u201d in Proceedings of ICASSP, 2015, pp. 4565\u20134569.\n[13] D. Chen, B. Mak, and S. Sivadas, \u201cJoint sequence training of phone and grapheme acoustic model based on multi-task learning Deep Neural Networks,\u201d in Proceedings of Interspeech, 2014, pp. 1083\u20131087.\n[14] A. Graves, A.-R. Mohamed, and G. E. Hinton, \u201cSpeech recognition with Deep Recurrent Neural Networks,\u201d in Proceedings of ICASSP, 2013, pp. 6645\u20136649.\n[15] K. Rao, A. Senior, and H. Sak, \u201cFlat start training of CD-CTCSMBR LSTM RNN acoustic models,\u201d in Proceedings of ICASSP, Shanghai, China, 2016, pp. 5405\u20135409.\n[16] X. He and L. Deng, Discriminative Learning for Speech Recognition. San Rafael, CA, USA: Morgan & Claypool, 2008.\n[17] D. Yu and L. Deng, \u201cChapter 8: Deep neural network sequencediscriminative training,\u201d in Automatic Speech Recognition \u2014 A Deep Learning Approach. Springer, October 2014.\n[18] S. J. Rennie, V. Goel, and S. Thomas, \u201cAnnealed dropout training of deep networks,\u201d in Proceedings of SLT, South Lake Tahoe, NV, USA, 2014, pp. 159\u2013164.\n[19] S. J. Young, J. J. Odell, and P. C. Woodland, \u201cTree-based state tying for high accuracy acoustic modelling,\u201d in Proceedings of HLT, 1994, pp. 307\u2013312.\n[20] S. Kullback and R. Leibler, \u201cOn information and sufficiency,\u201d Ann. Math. Statist., vol. 22, no. 1, pp. 79\u201386, 1951.\n[21] D. Imseng and J. Dines, \u201cDecision tree clustering for KL-HMM,\u201d IDIAP Research Institute, Tech. Rep. Idiap-Com-01-2012, 2012.\n[22] D. Imseng, J. Dines, P. Motlicek, P. Garner, and H. Bourlard, \u201cComparing different acoustic modeling techniques for multilingual boosting,\u201d in Proceedings of Interspeech, 2012.\n[23] X. Glorot, A. Bordes, and Y. Bengio, \u201cDeep sparse rectifier networks,\u201d in Proceedings of AISTATS, 2011, pp. 315\u2013323.\n[24] L. To\u0301th, \u201cConvolutional deep maxout networks for phone recognition,\u201d in Proceedings of Interspeech, 2014, pp. 1078\u20131082.\n[25] T. Gro\u0301sz, R. Busa-Fekete, G. Gosztolya, and L. To\u0301th, \u201cAssessing the degree of nativeness and Parkinson\u2019s condition using Gaussian Processes and Deep Rectifier Neural Networks,\u201d in Proceedings of Interspeech, Sep 2015, pp. 1339\u20131343.\n[26] G. Gosztolya, T. Gro\u0301sz, R. Busa-Fekete, and L. To\u0301th, \u201cDetecting the intensity of cognitive and physical load using AdaBoost and Deep Rectifier Neural Networks,\u201d in Proceedings of Interspeech, Singapore, Sep 2014, pp. 452\u2013456.\n[27] L. To\u0301th, G. Gosztolya, V. Vincze, I. Hoffmann, G. Szatlo\u0301czki, E. Biro\u0301, F. Zsura, M. Pa\u0301ka\u0301ski, and J. Ka\u0301lma\u0301n, \u201cAutomatic detection of mild cognitive impairment from spontaneous speech using ASR,\u201d in Proceedings of Interspeech, Dresden, Germany, Sep 2015, pp. 2694\u20132698.\n[28] Gy. Kova\u0301cs and L. To\u0301th, \u201cJoint optimization of spectro-temporal features and Deep Neural Nets for robust automatic speech recognition,\u201d Acta Cybernetica, vol. 22, no. 1, pp. 117\u2013134, 2015.\n[29] T. Gro\u0301sz and L. To\u0301th, \u201cA comparison of Deep Neural Network training methods for Large Vocabulary Speech Recognition,\u201d in Proceedings of TSD, Pilsen, Czech Republic, 2013, pp. 36\u201343.\n[30] M. Bacchiani, A. Senior, and G. Heigold, \u201cAsynchronous, online, GMM-free training of a context dependent acoustic model for speech recognition,\u201d in Proceedings of Interspeech, Singapore, Singapore, Sep 2014, pp. 1900\u20131904.\n[31] H. Su, G. Li, D. Yu, and F. Seide, \u201cError back propagation for sequence training of context-dependent deep networks for conversational speech transcription,\u201d in Proceedings of ICASSP, 2013, pp. 6664\u20136668."}], "references": [{"title": "Building context-dependent DNN acousitc models using Kullback-Leibler divergence-based state tying", "author": ["G. Gosztolya", "T. Gr\u00f3sz", "L. T\u00f3th", "D. Imseng"], "venue": "Proceedings of ICASSP, 2015, pp. 4570\u20134574.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "GMM-free DNN acoustic model training", "author": ["A. Senior", "G. Heigold", "M. Bacchiani", "H. Liao"], "venue": "Proceedings of ICASSP, 2014, pp. 5639\u20135643.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Standalone training of contextdependent Deep Neural Network acoustic models", "author": ["C. Zhang", "P. Woodland"], "venue": "Proceedings of ICASSP, 2014, pp. 5634\u20135638.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling", "author": ["B. Kingsbury"], "venue": "Proceedings of ICASSP, 2009, pp. 3761\u20133764.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Sequencediscriminative training of deep neural networks", "author": ["K. Vesel\u00fd", "A. Ghoshal", "L. Burget", "D. Povey"], "venue": "Proceedings of Interspeech, 2013, pp. 2345\u20132349.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A sequence training method for Deep Rectifier Neural Networks in speech recognition.", "author": ["T. Gr\u00f3sz", "G. Gosztolya", "L. T\u00f3th"], "venue": "Proceedings of SPECOM, Novi Sad, Serbia,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Sequence training of multiple Deep Neural Networks for better performance and faster training speed", "author": ["P. Zhou", "L. Dai", "H. Jiang"], "venue": "Proceedings of ICASSP, 2014, pp. 5664\u20135668.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Asynchronous stochastic optimization for sequence training of Deep Neural Networks: Towards big data", "author": ["E. McDermott", "G. Heigold", "P. Moreno", "A. Senior", "M. Bacchiani"], "venue": "Proceedings of Interspeech, 2014, pp. 1224\u20131228.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Investigation of full-sequence training of Deep Belief Networks for speech recognition", "author": ["A. Mohamed", "D. Yu", "L. Deng"], "venue": "Proceedings of Interspeech, 2010, pp. 2846\u20132849.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "A comparison of two optimization techniques for sequence discriminative training of Deep Neural Networks", "author": ["G. Saon", "H. Soltau"], "venue": "Proceedings of ICASSP, 2014, pp. 5604\u20135608.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Investigations on sequence training of neural networks", "author": ["S. Wiesler", "P. Golik", "R. Sch\u00fcter", "H. Ney"], "venue": "Proceedings of ICASSP, 2015, pp. 4565\u20134569.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint sequence training of phone and grapheme acoustic model based on multi-task learning Deep Neural Networks", "author": ["D. Chen", "B. Mak", "S. Sivadas"], "venue": "Proceedings of Interspeech, 2014, pp. 1083\u20131087.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech recognition with Deep Recurrent Neural Networks", "author": ["A. Graves", "A.-R. Mohamed", "G.E. Hinton"], "venue": "Proceedings of ICASSP, 2013, pp. 6645\u20136649.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Flat start training of CD-CTC- SMBR LSTM RNN acoustic models", "author": ["K. Rao", "A. Senior", "H. Sak"], "venue": "Proceedings of ICASSP, Shanghai, China, 2016, pp. 5405\u20135409.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminative Learning for Speech Recognition", "author": ["X. He", "L. Deng"], "venue": "San Rafael,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Chapter 8: Deep neural network sequencediscriminative training", "author": ["D. Yu", "L. Deng"], "venue": "Automatic Speech Recognition \u2014 A Deep Learning Approach. Springer, October 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Annealed dropout training of deep networks", "author": ["S.J. Rennie", "V. Goel", "S. Thomas"], "venue": "Proceedings of SLT, South Lake Tahoe, NV, USA, 2014, pp. 159\u2013164.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Tree-based state tying for high accuracy acoustic modelling", "author": ["S.J. Young", "J.J. Odell", "P.C. Woodland"], "venue": "Proceedings of HLT, 1994, pp. 307\u2013312.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "On information and sufficiency", "author": ["S. Kullback", "R. Leibler"], "venue": "Ann. Math. Statist., vol. 22, no. 1, pp. 79\u201386, 1951.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1951}, {"title": "Decision tree clustering for KL-HMM", "author": ["D. Imseng", "J. Dines"], "venue": "IDIAP Research Institute, Tech. Rep. Idiap-Com-01-2012, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Comparing different acoustic modeling techniques for multilingual boosting", "author": ["D. Imseng", "J. Dines", "P. Motlicek", "P. Garner", "H. Bourlard"], "venue": "Proceedings of Interspeech, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep sparse rectifier networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Proceedings of AISTATS, 2011, pp. 315\u2013323.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Convolutional deep maxout networks for phone recognition", "author": ["L. T\u00f3th"], "venue": "Proceedings of Interspeech, 2014, pp. 1078\u20131082.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Assessing the degree of nativeness and Parkinson\u2019s condition using Gaussian Processes and Deep Rectifier Neural Networks", "author": ["T. Gr\u00f3sz", "R. Busa-Fekete", "G. Gosztolya", "L. T\u00f3th"], "venue": "Proceedings of Interspeech, Sep 2015, pp. 1339\u20131343.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Detecting the intensity of cognitive and physical load using AdaBoost and Deep Rectifier Neural Networks", "author": ["G. Gosztolya", "T. Gr\u00f3sz", "R. Busa-Fekete", "L. T\u00f3th"], "venue": "Proceedings of Interspeech, Singapore, Sep 2014, pp. 452\u2013456.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic detection of mild cognitive impairment from spontaneous speech using ASR", "author": ["L. T\u00f3th", "G. Gosztolya", "V. Vincze", "I. Hoffmann", "G. Szatl\u00f3czki", "E. Bir\u00f3", "F. Zsura", "M. P\u00e1k\u00e1ski", "J. K\u00e1lm\u00e1n"], "venue": "Proceedings of Interspeech, Dresden, Germany, Sep 2015, pp. 2694\u20132698.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint optimization of spectro-temporal features and Deep Neural Nets for robust automatic speech recognition", "author": ["Gy. Kov\u00e1cs", "L. T\u00f3th"], "venue": "Acta Cybernetica, vol. 22, no. 1, pp. 117\u2013134, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "A comparison of Deep Neural Network training methods for Large Vocabulary Speech Recognition", "author": ["T. Gr\u00f3sz", "L. T\u00f3th"], "venue": "Proceedings of TSD, Pilsen, Czech Republic, 2013, pp. 36\u201343.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Asynchronous, online, GMM-free training of a context dependent acoustic model for speech recognition", "author": ["M. Bacchiani", "A. Senior", "G. Heigold"], "venue": "Proceedings of Interspeech, Singapore, Singapore, Sep 2014, pp. 1900\u20131904.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Error back propagation for sequence training of context-dependent deep networks for conversational speech transcription", "author": ["H. Su", "G. Li", "D. Yu", "F. Seide"], "venue": "Proceedings of ICASSP, 2013, pp. 6664\u20136668.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "We proposed a GMM-free solution for state clustering earlier [1], and in this study we will focus on the issue of obtaining the initial state alignment for training the DNN.", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "randomly initialized their neural network [3], while Zhang et al.", "startOffset": 42, "endOffset": 45}, {"referenceID": 2, "context": "trained their first model on equal-sized segments for each state [4].", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "Within the framework of HMM/GMM systems, several sequence-discriminative training methods have been developed, and these have now been adapted to HMM/DNN hybrids as well [5, 6, 7].", "startOffset": 170, "endOffset": 179}, {"referenceID": 4, "context": "Within the framework of HMM/GMM systems, several sequence-discriminative training methods have been developed, and these have now been adapted to HMM/DNN hybrids as well [5, 6, 7].", "startOffset": 170, "endOffset": 179}, {"referenceID": 5, "context": "Within the framework of HMM/GMM systems, several sequence-discriminative training methods have been developed, and these have now been adapted to HMM/DNN hybrids as well [5, 6, 7].", "startOffset": 170, "endOffset": 179}, {"referenceID": 6, "context": "[8, 9, 10]) or just to provide frame-level state labels (e.", "startOffset": 0, "endOffset": 10}, {"referenceID": 7, "context": "[8, 9, 10]) or just to provide frame-level state labels (e.", "startOffset": 0, "endOffset": 10}, {"referenceID": 8, "context": "[8, 9, 10]) or just to provide frame-level state labels (e.", "startOffset": 0, "endOffset": 10}, {"referenceID": 3, "context": "[5, 6, 11, 12, 13]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "[5, 6, 11, 12, 13]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "[5, 6, 11, 12, 13]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 10, "context": "[5, 6, 11, 12, 13]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 11, "context": "[5, 6, 11, 12, 13]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 12, "context": "The Connectionist Temporal Classification (CTC) approach has recently become very popular for training DNNs without an initial time alignment being available [14].", "startOffset": 158, "endOffset": 162}, {"referenceID": 13, "context": "proposed a flat start training procedure which is built on CTC [15].", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "Second, the CTC algorithm is not a sequence-discriminative training method, so for the best performance it has to be combined with techniques like sMBR training [14, 15].", "startOffset": 161, "endOffset": 169}, {"referenceID": 13, "context": "Second, the CTC algorithm is not a sequence-discriminative training method, so for the best performance it has to be combined with techniques like sMBR training [14, 15].", "startOffset": 161, "endOffset": 169}, {"referenceID": 2, "context": "[4], and we find that our method is faster and gives slightly lower word error rates.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Furthermore, we can combine sequence-discriminative flat start training with the KullbackLeibler divergence-based state clustering method we proposed recently [1].", "startOffset": 159, "endOffset": 162}, {"referenceID": 3, "context": "From the wide variety of sequence training methods, we opted for MMI (Maximum Mutual Information) training [5].", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "[4]:", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "Several sequence-discriminative training criteria have been developed for HMM/GMMs [16] \u2013 and adapted to HMM/DNNs [5, 6, 12, 17] \u2013 from which the maximum mutual information (MMI) criterion is the oldest and simplest.", "startOffset": 83, "endOffset": 87}, {"referenceID": 3, "context": "Several sequence-discriminative training criteria have been developed for HMM/GMMs [16] \u2013 and adapted to HMM/DNNs [5, 6, 12, 17] \u2013 from which the maximum mutual information (MMI) criterion is the oldest and simplest.", "startOffset": 114, "endOffset": 128}, {"referenceID": 4, "context": "Several sequence-discriminative training criteria have been developed for HMM/GMMs [16] \u2013 and adapted to HMM/DNNs [5, 6, 12, 17] \u2013 from which the maximum mutual information (MMI) criterion is the oldest and simplest.", "startOffset": 114, "endOffset": 128}, {"referenceID": 10, "context": "Several sequence-discriminative training criteria have been developed for HMM/GMMs [16] \u2013 and adapted to HMM/DNNs [5, 6, 12, 17] \u2013 from which the maximum mutual information (MMI) criterion is the oldest and simplest.", "startOffset": 114, "endOffset": 128}, {"referenceID": 15, "context": "Several sequence-discriminative training criteria have been developed for HMM/GMMs [16] \u2013 and adapted to HMM/DNNs [5, 6, 12, 17] \u2013 from which the maximum mutual information (MMI) criterion is the oldest and simplest.", "startOffset": 114, "endOffset": 128}, {"referenceID": 4, "context": "However, all authors initialize their networks using CE training, and apply the sequencediscriminative criterion only in the final phase of the training procedure, to fine-tune their models [6, 12], which makes it necessary to use some method (HMM/GMM or iterative CE training) to provide frame-level state targets.", "startOffset": 190, "endOffset": 197}, {"referenceID": 10, "context": "However, all authors initialize their networks using CE training, and apply the sequencediscriminative criterion only in the final phase of the training procedure, to fine-tune their models [6, 12], which makes it necessary to use some method (HMM/GMM or iterative CE training) to provide frame-level state targets.", "startOffset": 190, "endOffset": 197}, {"referenceID": 4, "context": "[6, 17]), but we only found Zhou et al.", "startOffset": 0, "endOffset": 7}, {"referenceID": 15, "context": "[6, 17]), but we only found Zhou et al.", "startOffset": 0, "endOffset": 7}, {"referenceID": 6, "context": "[8] actually doing this.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[18]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "This strategy can be readily adapted to sequence DNN training [5], and we found it to be essential for the stability of our flat-start MMI DNN training method.", "startOffset": 62, "endOffset": 65}, {"referenceID": 17, "context": "Currently, the dominant solution for this is the decision tree-based state tying method [19].", "startOffset": 88, "endOffset": 92}, {"referenceID": 18, "context": "Such a frame-level output can be treated as a discrete probability distribution, and a natural distance function for such distributions is the KullbackLeibler (KL) divergence [20].", "startOffset": 175, "endOffset": 179}, {"referenceID": 19, "context": "[21, 22].", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "[21, 22].", "startOffset": 0, "endOffset": 8}, {"referenceID": 0, "context": "For details, see [1].", "startOffset": 17, "endOffset": 20}, {"referenceID": 0, "context": "Our experimental setup is essentially the same as that of our previous study [1].", "startOffset": 77, "endOffset": 80}, {"referenceID": 21, "context": "We employed a DNN with 5 hidden layers, each containing 1000 rectified neurons [23], while the softmax activation function was applied in the output layer.", "startOffset": 79, "endOffset": 83}, {"referenceID": 22, "context": "[24, 25, 26, 27, 28]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 23, "context": "[24, 25, 26, 27, 28]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 24, "context": "[24, 25, 26, 27, 28]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 25, "context": "[24, 25, 26, 27, 28]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 26, "context": "[24, 25, 26, 27, 28]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 27, "context": "The 28 hour-long speech corpus of Hungarian broadcast news [29] was collected from eight TV channels.", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "(In our previous study we found that using a deep network for this re-alignment setup did not give any significant improvement [1].", "startOffset": 127, "endOffset": 130}, {"referenceID": 2, "context": "performed 20 such iterations [4], while we employed only 4 iterations.", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "The reason is that for iterative CE flat-start training we used a mini-batch of 100 frames (which we found optimal previously [1]), while for MMI whole utterances (usually more than 1000 frames) were used to update the weights, and this allowed better parallelization on the GPU.", "startOffset": 126, "endOffset": 129}, {"referenceID": 28, "context": "[30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "As regards stability, a known drawback of sequence training methods is that the same process is responsible both for aligning and training the DNN, which often leads to the \u201crun-away silence model\u201d issue [31].", "startOffset": 204, "endOffset": 208}], "year": 2016, "abstractText": "Recently, attempts have been made to remove Gaussian mixture models (GMM) from the training process of deep neural network-based hidden Markov models (HMM/DNN). For the GMM-free training of a HMM/DNN hybrid we have to solve two problems, namely the initial alignment of the frame-level state labels and the creation of context-dependent states. Although flat-start training via iteratively realigning and retraining the DNN using a frame-level error function is viable, it is quite cumbersome. Here, we propose to use a sequencediscriminative training criterion for flat start. While sequencediscriminative training is routinely applied only in the final phase of model training, we show that with proper caution it is also suitable for getting an alignment of context-independent DNN models. For the construction of tied states we apply a recently proposed KL-divergence-based state clustering method, hence our whole training process is GMM-free. In the experimental evaluation we found that the sequence-discriminative flat start training method is not only significantly faster than the straightforward approach of iterative retraining and realignment, but the word error rates attained are slightly better as well.", "creator": "LaTeX with hyperref package"}}}