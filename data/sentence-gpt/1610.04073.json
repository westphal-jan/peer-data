{"id": "1610.04073", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "Improved Knowledge Base Completion by Path-Augmented TransR Model", "abstract": "Knowledge base completion aims to infer new relations from existing information. In this paper, we propose path-augmented TransR (PTransR) model to improve the accuracy of link prediction. In our approach, we base PTransR model on TransR, which is the best one-hop model at present. To achieve the goal of achieving high accuracy, we base PTransR model on PTransR using the PTransR model on T. Heisenberg's theory (1).\n\n\n\n\n\n\nThis theoretical model will provide a comprehensive and useful guide to the understanding of link prediction and the potential use of TransR in predicting future outcomes. Based on this model, the first step is to implement the TransR model on T. Heisenberg's theory to increase PTransR as well as improve the accuracy of the relationship between PTransR and PtransR by using the PTransR model on T. Heisenberg's theory.\n\nIn this new approach, we also use the PTransR model on T. Heisenberg's theory to use the PTransR model on T. Heisenberg's theory to construct an optimal relationship between PTransR and PtransR by using the PTransR model on T. Heisenberg's theory to calculate a path-augmented TransR model using the PTransR model on T. Heisenberg's theory to calculate a path-augmented TransR model using the PTransR model on T. Heisenberg's theory to determine a path-augmented TransR model using the PTransR model on T. Heisenberg's theory to calculate a path-augmented TransR model using the PTransR model on T. Heisenberg's theory to calculate a path-augmented TransR model using the PTransR model on T. Heisenberg's theory to calculate a path-augmented TransR model using the PTransR model on T. Heisenberg's theory to calculate a path-augmented TransR model using the PTransR model on T. Heisenberg's theory to calculate a path-augmented TransR model using the PTransR model on T. Heisenberg's theory to calculate a path-augmented TransR model using the PTransR model on T. Heisenberg's theory to calculate a path-augmented TransR model using the PTransR model on T. Heisenberg's", "histories": [["v1", "Thu, 6 Oct 2016 08:34:15 GMT  (301kb,D)", "http://arxiv.org/abs/1610.04073v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["wenhao huang", "ge li", "zhi jin"], "accepted": false, "id": "1610.04073"}, "pdf": {"name": "1610.04073.pdf", "metadata": {"source": "CRF", "title": "Improved Knowledge Base Completion by Path-Augmented TransR Model", "authors": ["Wenhao Huang", "Ge Li", "Zhi Jin"], "emails": ["wenhao.huang@pku.edu.cn", "lige@sei.pku.edu.cn", "zhijin@sei.pku.edu.cn"], "sections": [{"heading": null, "text": "existing information. In this paper, we propose path-augmented TransR (PTransR) model to improve the accuracy of link prediction. In our approach, we base PTransR model on TransR, which is the best one-hop model at present. Then we regularize TransR with information of relation paths. In our experiment, we evaluate PTransR on the task of entity prediction. Experimental results show that PTransR outperforms previous models.\nKeywords: knowledge base completion, relation path, link prediction"}, {"heading": "1 Introduction", "text": "Large scale knowledge bases such as WordNet [1] and FreeBase [2] are important resources for natural language processing (NLP) applications like web searching [3], automatic question answering systems [4], and even medical informatics [5]. Formally, a knowledge base is a dataset containing triples of two entities and their relation. A triplet (h, r, t), for example, indicates that the head entity h and the tail entity t have a relation r. Despite massive triplets a knowledge base contains, evidence in the literature suggests that existing knowledge bases are far from complete [6,7].\nIn the past decades, researchers have proposed various methods to automatically construct or populate knowledge bases from plain texts [6,8], semistructured data on the Web [9,10], etc. Recently, studies show that embedding the entities and relations of a knowledge base into a continuous vector space is an effective way to integrate the global information in the existing knowledge base and to predict missing triplets without using external resources (i.e., additional text or tables) [11,7,12,13,14].\nBordes et al. [11] propose the TransE approach, which translates entities\u2019 embeddings by that of a relation, to model knowledge bases. That is to say, the relation between two entities can be represented as a vector offset, similar to word analogy tasks for word embeddings [15] and sentence relation classification\nar X\niv :1\n61 0.\n04 07\n3v 1\n[ cs\n.A I]\n6 O\nct 2\n01 6\nby sentence embeddings [16]. For one-to-many, many-to-one, and many-to-many relations, however, such straightforward vector offset does not make much sense. Considering a head entity China and the country-city relation, we can think of multiple plausible tail entities like Beijing, Tianjin, and Shanghai. These entities cannot be captured in the same time by translating the head entity and relation embeddings. Therefore, researchers propose to map entities to a new space where embedding translation is computed, resulting in TransH [7], TransR [12], and other variants. Among the above approaches, TransR achieves the highest performance on established benchmarks.\nOne shortcoming of the above methods is that only the direct relation (i.e., one-hop relation) between two entities is considered. In a knowledge base, some entities and relations only appear a few times; they suffer from the problem of data sparsity during training. Fortunately, the problem can be alleviated by using multi-hop information in a knowledge base. Guu et al. [13] present a random walk approach to sample entities with composited relations. Likewise, Lin et al. [14] propose a path-augmenting approach that uses multi-hop relations between two entities to regularize the direct relation between the same entity pair. Their experiments show the path-augmented TransE model (denoted as PTransE) outperforms the one-hop TransE model.\nIn this paper, we are curious whether we can combine the worlds, i.e., whether the path-augmenting technique is also useful to a better one-hop \u201cbase\u201d model. Therefore, we propose to leverage TransR [12] as our cornerstone, but enhance it with path information as in [14], resulting a new variant, PTransR. We evaluate our model on the FreeBase dataset. Experimental results show that modeling relation paths is beneficial to the base model TransR, and that PTransR also outperforms PTransE in entity prediction. In this way, we achieve the state-ofthe-art link prediction performance in the category that uses only the knowledge base itself (i.e., without additional textual information).\nThe rest of this paper is organized as follows. In Section 2, we describe the base model TransR and then discuss the path-augmented variant PTransR. In Section 3, we compare our PTransR model with other baselines in an entity prediction experiment; we also have in-depth analysis regarding different groups of relations, namely 1-to-1, 1-to-n, n-to-1, and n-to-n relations. In Section 4, we briefly review previous work in information extraction. Finally, we conclude our paper in Section 5."}, {"heading": "2 Our Approach", "text": "In this section, we present our PTransR model in detail. In Subsection 2.1, we introduce the TransE model and explain how TransR overcomes the weakness of TransE. Then, we augment TransR model with path information in Subsection 2.2."}, {"heading": "2.1 Base Model: TransR", "text": "As said in Section 1, embedding entities and their relation into vector spaces can effectively exploit internal structures that a knowledge base contains, and thus is helpful in predicting missing triplets without using additional texts.\nThe first model in such research direction is TransE [11]. It embeds entities and their relation in a same low-dimensional vector space; the two entities\u2019 embeddings are translated by a relation embedding, which can be viewed as an offset vector. In other words, for a triplet (h, r, t), we would like h+r \u2248 t. (Here, bold letters refer to the embeddings of head/tail entities and the relation.) The plausibility of a triplet (h, r, t) is then evaluated by a scoring function\nfr(h, t) , \u2016h + r\u2212 t\u2016 , (1)\nwhere \u2016 \u00b7 \u2016 denotes either `1-norm or `2-norm. fr(h, t) is expected to be small if (h, r, t) is a positve triplet.\nTo further analyze the performance of TransE, Bordes et al. [11] divide relations into four groups, namely 1-to-1, 1-to-n, n-to-1, and n-to-n, according to the mapping properties of a relataion. For example, country-city is a 1-to-n relation, because a country may have multiple cities, but a city belongs to only one country. The weakness of TransE is that entity embeddings on the many side tend to be close to each other, which is the result of expecting fr(h, t) to be small for all positive triplets. Therefore, it is hard for TransE to distinguish among the entities which are on the many side.\nTo solve the above problem, TransR [12] embeds entities and relations into two separate spaces: the entity space and the relation space. It uses relationspecific matrices Mr to map an entity from its own space to the relation space, given by hr = Mrh and tr = Mrt, so that translation can be accomplished by regarding relation embedding as an offset vector, i.e., hr + r \u2248 tr. To achieve this goal, TransR defines the scoring function as\nfr(h, t) , \u2016Mrh + r\u2212Mrt\u201622 . (2)\nTo train the model, we shall generate negative samples and use the hinge loss. The overall cost function of the TransR model is\nLTransR = \u2211\n(h,r,t)\u2208S \u2211 (h\u2032,r,t\u2032)\u2208S\u2032 max { 0, \u03b3 + fr(h, t)\u2212 fr(h\u2032, t\u2032) } , (3)\nwhere negative samples are constructed as S\u2032 = {(h\u2032, r, t)|(h\u2032, r, t) /\u2208 S} \u22c3 {(h, r, t\u2032)|(h, r, t\u2032) /\u2208 S} .\nResults in link prediction show that TransR outperforms other models on established benchmarks, indicating TransR is the best one-hop model at present. However, TransR fails to utilize the rich path information, which will be dealt with in the following subsection."}, {"heading": "2.2 Path-Augmented TransR: PTransR", "text": "Using path information to regularize one-hop models can be beneficial [14,13]. Here, we adopt the path modeling method in PTransE, and extend the TransR model to path-augmented TransR (denoted as PTransR).\nA relation path is a set of relations that connect head entity and tail entity in succession. An n-hop relation path from h to t is defined as p = {r1, r2, \u00b7 \u00b7 \u00b7 , rn}, satisfying h r1\u2212\u2192 e1 r2\u2212\u2192 \u00b7 \u00b7 \u00b7 rn\u2212\u2192 t. If n = 1, then p = r1 is a direct (1-hop) relation. To enhance TransR model with multi-hop information, we follow the treatment in PTransE [14] and represent a relation path as an embedding vector by additive compositional methods. Then such multi-hop information is used to regularize one-hop direct relation between the same entity pair. A reliability score is computed to address the strength of regularization by a particular path. Details are described as follows.\nTo compute the representation of a relation path p that composites primitive relations r1, r2, \u00b7 \u00b7 \u00b7 , rn, i.e., p = r1 \u25e6r2 \u25e6\u00b7 \u00b7 \u00b7\u25e6rn (where \u25e6 denotes the composition operation), we add the embeddings of these primitive relations, given by\np = r1 + r2 + \u00b7 \u00b7 \u00b7+ rn, (4)\nwhere bold letters denote the vector of a relation or a path. The choice of addition as the composition operation is reasonable, because the vector representation of path p should be close to that of direct relation r if it is likely to infer r from p. For example, the representation of path father\u2212\u2212\u2212\u2212\u2192 mother\u2212\u2212\u2212\u2212\u2212\u2192 is expected to be close to that of direct relation grandmother\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192.\nAlthough a knowledge base may contain a variety of relation paths between two entities, not every path is equally useful for inferring direct relations. For example, the relation path John friend\u2212\u2212\u2212\u2212\u2192 Tim gender\u2212\u2212\u2212\u2212\u2192 male gives little contribution to inferring the gender of John. To evaluate the reliability of a path, PTransE uses a path-constraint resource algorithm (PCRA) [14], which is also applied in our approach. This algorithm first assigns a certain amount of resource (i.e., a value of 1) to the head entity h; then each node distributes resource evenly to its direct child nodes (Figure 2). The value of p along an entity pair h, t is denoted as v(p|h, t).\nHowever, v(p|h, t) alone does not embody the relatedness between a relation path p and a direct relation r. To address this problem, a relatedness measure is defined as Pr(r|p) = Pr(r, p)/Pr(p), where Pr(p) is the sum of v(p|h, t) for every training triplet (h, r, t) with p as a relation path from h to t. Pr(r, p) is the sum of v(p|h, t) for every training triplet (h, r, t) with r being a direct relation and p as a relation path. The overall reliability of a path p on a triplet (h, r, t) is given by\nR(p|h, r, t) = Pr(r|p) \u00b7 v(p|h, t). (5)\nPTransR\u2019s scoring function fPTransR(h, r, t) is composed of two scores:\nfPTransR(h, r, t) = E(h, r, t) + E(P|h, r, t). (6)\nE(h, r, t) is the same as the scoring function of TransR (Equation 2) which evaluates the plausibility of (h, r, t) without considering relation paths from h to t. Following PTransE, E(P|h, r, t) is defined as\nE(P|h, r, t) = 1 Z \u2211 p\u2208P E(p|h, r, t), (7)\nE(p|h, r, t) = R(p|h, r, t) \u2016p\u2212 r\u201622 = Pr(r|p)v(p|h, t) \u2016p\u2212 r\u2016 2 2 , (8)\nwhere Z is a normalizing factor for R(p|h, t) and P is the set of all paths from h to t. E(P|h, r, t) evaluates the plausibility of (h, r, t) with the consideration of relation paths from h to t. The overall loss function of PTransR is\nLPTransR = \u2211\n(h,r,t)\u2208S\n[L(h, r, t) + 1\nZ \u2211 p\u2208P L(p|h, r, t)], (9)\nL(h, r, t) = \u2211\n(h\u2032,r,t\u2032)/\u2208S\nmax { 0, \u03b31 + E(h, r, t)\u2212 E(h\u2032, r, t\u2032) } , (10)\nL(p|h, r, t) = \u2211\n(h,r\u2032,t)/\u2208S\nmax { 0, \u03b32 + E(p|h, r, t)\u2212 E(p|h, r\u2032, t) } , (11)\nPTransR learns entity and relation embeddings by minimizing LPTransR."}, {"heading": "2.3 Training Details", "text": "We train PTransR by mainly following PTransE [14]. Initial vectors and matrices. Following TransR, initial vectors and matrices for PTransR are obtained from TransE. The configuration of TransE is: margin \u03b3 = 1, learning rate \u03b1 = 0.01, method = unif, and epoch = 1000. Negative samples. We sample negative triplets by randomly replacing head entity h or tail entity t or relation r. For example, (h, r, t)-derived negative triplets are (h\u2032, r, t), (h, r, t\u2032), and (h, r\u2032, t), where (h, r, t) \u2208 S and (h\u2032, r, t), (h, r\u2032, t), (h, r, t\u2032)/\u2208 S. Vector representation constraints. Following TransR, to regularize the representations, we impose the following constraints on the entity and relation embeddings.\n\u2016 h \u2016=\u2016 t \u2016=\u2016 r \u2016= 1, \u2016Mrh \u2016\u2264 1, \u2016Mrt \u2016\u2264 1. (12)\nPath selection. PTranE restricts the length of path to less than 3. Its results show that 3-hop paths do not make significant improvement, compared to 2-hop paths. For efficiency, we only consider 2-hop relation paths. Inverse relation. As inverse relations sometimes contain useful information, for each training triplet (h, r, t), (t, r\u22121, h) is added to the training set."}, {"heading": "3 Evaluation", "text": "In this section, we present results of our experiment. We first briefly introduce the dataset and the task of entity prediction. Then we show the experimental results and analyze the performance."}, {"heading": "3.1 Dataset", "text": "FB15k is a commonly used dataset in knowledge base completion. Table 1 shows statistics of FB15k. FB15k dataset contains factual information in our world, e.g., location/country/language spoken. As FB15k has various kinds of relation, it is suitable for the evaluation of PTransR. Therefore, we choose FB15k as our experimental dataset."}, {"heading": "3.2 Experimental Settings", "text": "We evaluate PtransR on the task of entity prediction. Entity prediction aims at predicting the missing entity in an incomplete triplet, i.e., predicting h given\nr and t, or predicting t given h and r. Following the settings in TransE, for a triplet (h, r, t), we replace the head entity h with every entity e and compute the score of (e, r, t). Entity candidates are ranked according to their scores. We repeat the same process to predict the tail entity t. Then we use the two metrices in TransE to evaluate the performance: MeanRank (average rank of the expected entity) and Hits@10 (proportion of triplets whose head/tail entity is among top10 in the ranking). However, there could be several entities that are plausible for the same incomplete triplet. The plausible entities which are ranked before h or t may cause underestimation of performance. One solution is to remove other plausible entities in the ranking, which is referred to as a filter. In comparison, the results without removing other plausible entities are referred to as raw. A good model should achieve low MeanRank and high Hits@10.\nTo utilize the inverse relation, instead of only using the score fPTransR(h, r, t), we use the sum of fPTransR(h, r, t) and fPTransR(t, r\n\u22121, h) to rank the candidates, i.e.,\nscore(h, r, t) = fPTransR(h, r, t) + fPTransR(t, r \u22121, h). (13)\nTo accelerate the testing process, we use the reranking method in PTransE. We first rank all candidates according to their scores which are computed by the scoring function of TransR, which means that path information is not considered in the first ranking. Then we rerank the top-500 candidates according to the scores computed by the scoring function mentioned above, namely score(h, r, t).\nThe configurations for experiments are given as follows: learning rate \u03b1 for SGD among {0.01,0.001,0.0001}, dimension of enitity space Rk and relation space Rd between {20,50}, \u03b31 and \u03b32 among {1,2,4}, batch size B among {480,960,4800}. The optimal configuration on valid set is \u03b1 = 0.001, k = d = 50, \u03b31 = \u03b32 = 1, and B = 4800. The training process is limited to less than 500 epochs."}, {"heading": "3.3 Overall Performance", "text": "Table 2 shows the experimental results. By comparing the results of PTransR with the results of previous models, we have the following main observations: (1) PTransR outperforms TransR on every metric to a large margin, which shows that path-augmented model can achieve better results than one-hop base model. (2) PTransR outperforms PTransE in MeanRank and is comparable to PTransE in Hits@10, which shows that path-agumented model with a better one-hop base model can achieve better performance.\nTable 3 presents the performance on the four relation catogories 1-to-1, 1- to-n, n-to-1, and n-to-n, with Hits@10(filter) as the metric. From Table 3, we find that, compared to TransR, PTransR shows consistent imporvement on all four relation categories. Also, compared to PTransE, PTransR performs better on 1-to-1, 1-to-n and n-to-1 relations, especially on 1-to-n and n-to-1."}, {"heading": "3.4 In-Depth Analysis and Discussion", "text": "As pointed out in Section 1, despite the massive train set of FB15k, some relations cannot be properly captured due to the problem of data sparsity. We separate relations into five groups according to their frequency in the train set, as shown in Table 4. MeanRank(raw) of TransR and PTransR is compared in Table 4 and the improvement from TransR to PTransR is presented. First of all, we see PTransR outperforms TransR in all five groups of relations. Second, as relation frequency decreases, the improvement goes up, which means that path information is useful for dealing with the problem of data sparsity."}, {"heading": "4 Related Work", "text": "Relation extraction is an important research topic in NLP. It can be roughly divided into two categories based on the source of information.\nText-based approaches extraction entities and/or relations from plain text. For example, Hearst [17] uses \u201cis a|an\u201d pattern to extract hyponymy relations. Banko et al. [6] proposes to extract open-domain relations from the Web. Fully supervised relation extraction, which classify two marked entities into several predefined relations, have become a hot research arena in the past several years [18,19,8].\nKnowledge base completion/population, on the other hand, does not use additional text. Socher et al. [20] propose a tensor model to predict missing relations in an existing knowledge base, showing neural networks\u2019 ability of entity-relation inference. Then, translating embeddings approaches are proposed for knowledge base completion [11,7,12,14]. Recently, Wang et al. [21] use additional information to improve knowledge base completion by using textual context.\nIn this paper, we focus on pure knowledge base completion, i.e., we do not use additional resources. We combine the state-of-the-art one-hop TransR model [12] and path augmentation method [14], resulting in the new PTransR variant."}, {"heading": "5 Conclusion", "text": "In this paper, we augment one-hop TransR model with path modeling method, resulting in PTransR model. We evaluate PTransR on the task of entity pre-\ndiction and compare the performance of PTransR with that of previous models. Experimental results show that path information is useful in sovling the problem of data sparsity, and that PTransR outperforms previous models, which makes PTransR the state-of-the-art model in the field that populates knowledge base without using additional text."}], "references": [{"title": "WordNet: A lexical database for English", "author": ["G.A. Miller"], "venue": "Communications of the ACM 38(11)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Freebase: A collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning and inferencing in user ontology for personalized semantic web search", "author": ["X. Jiang", "A.H. Tan"], "venue": "Information Sciences 179(16)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Information extraction over structured data: Question answering with freebase", "author": ["X. Yao", "B. Van Durme"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Gene Ontology: Tool for the unification of biology", "author": ["M. Ashburner", "C.A. Ball", "J.A. Blake", "D. Botstein", "H. Butler", "J.M. Cherry", "A.P. Davis", "K. Dolinski", "S.S. Dwight", "Eppig", "J.T"], "venue": "Nature Genetics 25(1)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Open information extraction from the web", "author": ["M. Banko", "M.J. Cafarella", "S. Soderland", "M. Broadhead", "O. Etzioni"], "venue": "Proceedings of the 20th International Joint Conference on Artificial Intelligence.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "Proceedings of the 28th AAAI Conference on Artificial Intelligence.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Y. Xu", "L. Mou", "G. Li", "Y. Chen", "H. Peng", "Z. Jin"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey of Web information extraction systems", "author": ["C.H. Chang", "M. Kayed", "M.R. Girgis", "K.F. Shaalan"], "venue": "IEEE Transactions on Knowledge and Data Engineering 18(10)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Extracting lexical semantic knowledge from Wikipedia and Wiktionary", "author": ["T. Zesch", "C. M\u00fcller", "I. Gurevych"], "venue": "Proceedings of the 6th International Conference on Language Resources and Evaluation.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Y. Lin", "Z. Liu", "M. Sun", "Y. Liu", "X. Zhu"], "venue": "Proceedings of the 29th AAAI Conference on Artificial Intelligence.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Traversing knowledge graphs in vector space", "author": ["K. Guu", "J. Miller", "P. Liang"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, Association for Computational Linguistics", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling relation paths for representation learning of knowledge bases", "author": ["Y. Lin", "Z. Liu", "H. Luan", "M. Sun", "S. Rao", "S. Liu"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W.T. Yih", "G. Zweig"], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Natural language inference by tree-based convolution and heuristic matching", "author": ["L. Mou", "R. Men", "G. Li", "Y. Xu", "L. Zhang", "R. Yan", "Z. Jin"], "venue": "The 54th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic acquisition of hyponyms from large text corpora", "author": ["M.A. Hearst"], "venue": "Proceedings of the 14th conference on Computational linguistics-Volume 2, Association for Computational Linguistics", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1992}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["C.N. dos Santos", "B. Xiang", "B. Zhou"], "venue": "arXiv preprint arXiv:1504.06580", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Improved relation classification by deep recurrent neural networks with data augmentation", "author": ["Y. Xu", "R. Jia", "L. Mou", "G. Li", "Y. Chen", "Y. Lu", "Z. Jin"], "venue": "COLING.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A. Ng"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Text-enhanced representation learning for knowledge graph", "author": ["Z. Wang", "J. Li"], "venue": "International Joint Conference on Artificial Intelligence.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Large scale knowledge bases such as WordNet [1] and FreeBase [2] are important resources for natural language processing (NLP) applications like web searching [3], automatic question answering systems [4], and even medical informatics [5].", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "Large scale knowledge bases such as WordNet [1] and FreeBase [2] are important resources for natural language processing (NLP) applications like web searching [3], automatic question answering systems [4], and even medical informatics [5].", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "Large scale knowledge bases such as WordNet [1] and FreeBase [2] are important resources for natural language processing (NLP) applications like web searching [3], automatic question answering systems [4], and even medical informatics [5].", "startOffset": 159, "endOffset": 162}, {"referenceID": 3, "context": "Large scale knowledge bases such as WordNet [1] and FreeBase [2] are important resources for natural language processing (NLP) applications like web searching [3], automatic question answering systems [4], and even medical informatics [5].", "startOffset": 201, "endOffset": 204}, {"referenceID": 4, "context": "Large scale knowledge bases such as WordNet [1] and FreeBase [2] are important resources for natural language processing (NLP) applications like web searching [3], automatic question answering systems [4], and even medical informatics [5].", "startOffset": 235, "endOffset": 238}, {"referenceID": 5, "context": "Despite massive triplets a knowledge base contains, evidence in the literature suggests that existing knowledge bases are far from complete [6,7].", "startOffset": 140, "endOffset": 145}, {"referenceID": 6, "context": "Despite massive triplets a knowledge base contains, evidence in the literature suggests that existing knowledge bases are far from complete [6,7].", "startOffset": 140, "endOffset": 145}, {"referenceID": 5, "context": "In the past decades, researchers have proposed various methods to automatically construct or populate knowledge bases from plain texts [6,8], semistructured data on the Web [9,10], etc.", "startOffset": 135, "endOffset": 140}, {"referenceID": 7, "context": "In the past decades, researchers have proposed various methods to automatically construct or populate knowledge bases from plain texts [6,8], semistructured data on the Web [9,10], etc.", "startOffset": 135, "endOffset": 140}, {"referenceID": 8, "context": "In the past decades, researchers have proposed various methods to automatically construct or populate knowledge bases from plain texts [6,8], semistructured data on the Web [9,10], etc.", "startOffset": 173, "endOffset": 179}, {"referenceID": 9, "context": "In the past decades, researchers have proposed various methods to automatically construct or populate knowledge bases from plain texts [6,8], semistructured data on the Web [9,10], etc.", "startOffset": 173, "endOffset": 179}, {"referenceID": 10, "context": ", additional text or tables) [11,7,12,13,14].", "startOffset": 29, "endOffset": 44}, {"referenceID": 6, "context": ", additional text or tables) [11,7,12,13,14].", "startOffset": 29, "endOffset": 44}, {"referenceID": 11, "context": ", additional text or tables) [11,7,12,13,14].", "startOffset": 29, "endOffset": 44}, {"referenceID": 12, "context": ", additional text or tables) [11,7,12,13,14].", "startOffset": 29, "endOffset": 44}, {"referenceID": 13, "context": ", additional text or tables) [11,7,12,13,14].", "startOffset": 29, "endOffset": 44}, {"referenceID": 10, "context": "[11] propose the TransE approach, which translates entities\u2019 embeddings by that of a relation, to model knowledge bases.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "That is to say, the relation between two entities can be represented as a vector offset, similar to word analogy tasks for word embeddings [15] and sentence relation classification ar X iv :1 61 0.", "startOffset": 139, "endOffset": 143}, {"referenceID": 15, "context": "by sentence embeddings [16].", "startOffset": 23, "endOffset": 27}, {"referenceID": 6, "context": "Therefore, researchers propose to map entities to a new space where embedding translation is computed, resulting in TransH [7], TransR [12], and other variants.", "startOffset": 123, "endOffset": 126}, {"referenceID": 11, "context": "Therefore, researchers propose to map entities to a new space where embedding translation is computed, resulting in TransH [7], TransR [12], and other variants.", "startOffset": 135, "endOffset": 139}, {"referenceID": 12, "context": "[13] present a random walk approach to sample entities with composited relations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] propose a path-augmenting approach that uses multi-hop relations between two entities to regularize the direct relation between the same entity pair.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Therefore, we propose to leverage TransR [12] as our cornerstone, but enhance it with path information as in [14], resulting a new variant, PTransR.", "startOffset": 41, "endOffset": 45}, {"referenceID": 13, "context": "Therefore, we propose to leverage TransR [12] as our cornerstone, but enhance it with path information as in [14], resulting a new variant, PTransR.", "startOffset": 109, "endOffset": 113}, {"referenceID": 10, "context": "The first model in such research direction is TransE [11].", "startOffset": 53, "endOffset": 57}, {"referenceID": 10, "context": "[11] divide relations into four groups, namely 1-to-1, 1-to-n, n-to-1, and n-to-n, according to the mapping properties of a relataion.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "To solve the above problem, TransR [12] embeds entities and relations into two separate spaces: the entity space and the relation space.", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "Using path information to regularize one-hop models can be beneficial [14,13].", "startOffset": 70, "endOffset": 77}, {"referenceID": 12, "context": "Using path information to regularize one-hop models can be beneficial [14,13].", "startOffset": 70, "endOffset": 77}, {"referenceID": 13, "context": "To enhance TransR model with multi-hop information, we follow the treatment in PTransE [14] and represent a relation path as an embedding vector by additive compositional methods.", "startOffset": 87, "endOffset": 91}, {"referenceID": 13, "context": "To evaluate the reliability of a path, PTransE uses a path-constraint resource algorithm (PCRA) [14], which is also applied in our approach.", "startOffset": 96, "endOffset": 100}, {"referenceID": 13, "context": "We train PTransR by mainly following PTransE [14].", "startOffset": 45, "endOffset": 49}, {"referenceID": 16, "context": "For example, Hearst [17] uses \u201cis a|an\u201d pattern to extract hyponymy relations.", "startOffset": 20, "endOffset": 24}, {"referenceID": 5, "context": "[6] proposes to extract open-domain relations from the Web.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "Fully supervised relation extraction, which classify two marked entities into several predefined relations, have become a hot research arena in the past several years [18,19,8].", "startOffset": 167, "endOffset": 176}, {"referenceID": 18, "context": "Fully supervised relation extraction, which classify two marked entities into several predefined relations, have become a hot research arena in the past several years [18,19,8].", "startOffset": 167, "endOffset": 176}, {"referenceID": 7, "context": "Fully supervised relation extraction, which classify two marked entities into several predefined relations, have become a hot research arena in the past several years [18,19,8].", "startOffset": 167, "endOffset": 176}, {"referenceID": 19, "context": "[20] propose a tensor model to predict missing relations in an existing knowledge base, showing neural networks\u2019 ability of entity-relation inference.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Then, translating embeddings approaches are proposed for knowledge base completion [11,7,12,14].", "startOffset": 83, "endOffset": 95}, {"referenceID": 6, "context": "Then, translating embeddings approaches are proposed for knowledge base completion [11,7,12,14].", "startOffset": 83, "endOffset": 95}, {"referenceID": 11, "context": "Then, translating embeddings approaches are proposed for knowledge base completion [11,7,12,14].", "startOffset": 83, "endOffset": 95}, {"referenceID": 13, "context": "Then, translating embeddings approaches are proposed for knowledge base completion [11,7,12,14].", "startOffset": 83, "endOffset": 95}, {"referenceID": 20, "context": "[21] use additional information to improve knowledge base completion by using textual context.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "We combine the state-of-the-art one-hop TransR model [12] and path augmentation method [14], resulting in the new PTransR variant.", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "We combine the state-of-the-art one-hop TransR model [12] and path augmentation method [14], resulting in the new PTransR variant.", "startOffset": 87, "endOffset": 91}], "year": 2016, "abstractText": "Knowledge base completion aims to infer new relations from existing information. In this paper, we propose path-augmented TransR (PTransR) model to improve the accuracy of link prediction. In our approach, we base PTransR model on TransR, which is the best one-hop model at present. Then we regularize TransR with information of relation paths. In our experiment, we evaluate PTransR on the task of entity prediction. Experimental results show that PTransR outperforms previous models.", "creator": "LaTeX with hyperref package"}}}