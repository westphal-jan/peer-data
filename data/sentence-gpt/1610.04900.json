{"id": "1610.04900", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Oct-2016", "title": "Convergence rate of stochastic k-means", "abstract": "We analyze online and mini-batch k-means variants. Both scale up the widely used Lloyd 's algorithm via stochastic approximation, and have become popular for large-scale clustering and unsupervised feature learning. We show, for the first time, that they have global convergence towards local optima at $O(\\frac{1}{t})$ rate under general conditions. In addition, we show if the dataset is clusterable, with suitable initialization, mini-batch k-means converges to an optimal k-means solution with $O(\\frac{1}{t})$ convergence rate with high probability. The k-means objective is non-convex and non-differentiable: we exploit ideas from non-convex gradient-based optimization by providing a novel characterization of the trajectory of k-means algorithm on its solution space, and circumvent its non-differentiability via geometric insights about k-means update. The algorithm is currently used by computer model and modelers to map and identify clusters for the largest possible number of k-means queries, a model for which one has the right size of a cluster.\n\n\n\n\nTo perform these experiments, we use a non-convex method to compare the relative clustering of a single cluster with its own dataset. We find that the resulting clusters are of an optimal size, with the median cluster size of a cluster size of approximately 7.8.5.\nThe model shows that the k-means algorithm predicts a small, linear clustering that is able to detect and predict clusters on a regular basis. As a result, the k-means optimization of clustering can easily be calculated.\n\nUsing a method called voxel , which is a computational model that predicts clustering between single clusters, we show that the k-means algorithm predicts a very fine clustering procedure that can reliably predict clustering across multiple clusters. Our algorithm is currently very stable, with a very fast, predictable, and straightforward implementation of the non-convex algorithm in the future.\nAs a result, we are able to create a simple model that generates clustering clusters on a consistent basis. The result of the approach is a highly efficient model that can be generated without compromising performance. We show that we can estimate cluster sizes from any given cluster size as well as using the general notion of clustering in general, which is highly efficient.\nThis paper is published in Nature Communications.\n\nImage Source", "histories": [["v1", "Sun, 16 Oct 2016 18:59:59 GMT  (326kb,D)", "http://arxiv.org/abs/1610.04900v1", null], ["v2", "Mon, 7 Nov 2016 18:20:06 GMT  (423kb,D)", "http://arxiv.org/abs/1610.04900v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cheng tang", "claire monteleoni"], "accepted": false, "id": "1610.04900"}, "pdf": {"name": "1610.04900.pdf", "metadata": {"source": "CRF", "title": "Convergence rate of stochastic k-means", "authors": ["Cheng Tang", "Claire Monteleoni"], "emails": ["tangch@gwu.edu", "cmontel@gwu.edu"], "sections": [{"heading": "1 Introduction", "text": "Stochastic k-means, including online [6] and mini-batch k-means [20], has gained increasing attention for large-scale clustering and is included in widely used machine learning packages, such as Sofia-ML [20] and scikit-learn [18]. Figure 1 demonstrates the efficiency of stochastic k-means against batch k-means on the RCV1 dataset [15]. The advantage is clear, and the results raise some natural questions: Can we characterize the convergence rate of stochastic k-means? Why do the algorithms appear to converge to different \u2217Department of Computer Science, George Washington University\nar X\niv :1\n61 0.\n04 90\n0v 1\n[ cs\n.L G\n] 1\n\u201clocal optima\u201d? Why and how does mini-batch size affect the quality of the final solution? Our goal is to address these questions rigorously.\nGiven a discrete dataset of n input data points, denoted by X := {x, x \u2208 Rd}, a common way to cluster X is to first select a set of k centroids, denoted by C = {cr \u2208 Rd, r \u2208 [k]}, and assign each x to a centroid. Data points assigned to the same centroid cr, form a cluster Ar \u2282 X, and \u222ar\u2208[k]Ar = X; we let A := {Ar, r \u2208 [k]} denote the resulting clustering. This is center-based clustering, where each clustering is specified by a tuple (C,A). The k-means cost of (C,A) is defined as: \u03c6X(C,A) := \u2211 r\u2208[k] \u2211 x\u2208Ar \u2016x\u2212cr\u2016\n2. The k-means clustering problem is cast as the optimization problem: minC,A \u03c6X(C,A). This is an NP-hard problem [17].\nHowever, if either C or A is fixed, the problem can be easily solved. Fixing the set of k centroids C, the smallest k-means cost is achieved by choosing the clustering that assigns each point x to it closest center, which we denote by C(x) := mincr\u2208C \u2016x\u2212 cr\u2016. That is,\n\u03c6X(C) := min A \u03c6X(C,A) = \u2211 r\u2208[k] \u2211 C(x)=cr \u2016x\u2212 cr\u20162 (1)\nThis clustering can also be induced by the Voronoi diagram of C, denoted by V (C) := {V (cr), r \u2208 [k]}, where\nV (cr) := {x \u2208 Rd, \u2016x\u2212 cr\u2016 \u2264 \u2016x\u2212 cs\u2016, \u2200s 6= r}\nClustering A induced by V (C) is such that \u2200Ar \u2208 A,Ar = V (cr) \u2229 X. Subsequently, we will use V (C)\u2229X to denote this induced clustering. Likewise, fixing a k-clustering A of X, the smallest k-means cost is achieved by setting the new centers as the mean of each cluster, denoted by m(Ar),\n\u03c6X(A) := min C \u03c6X(C,A) = \u2211 r\u2208[k] \u2211 x\u2208Ar \u2016x\u2212m(Ar)\u20162 (2)\nBatch k-means or Lloyd\u2019s algorithm [16], one of the most popular clustering heuristics [11], essentially proceeds by finding the solution to (1) and (2) alternatively: At t = 0, it initializes the position of k centroids, C0, via a seeding algorithm. \u2200t \u2265 1, it alternates between two steps, Step 1 Fix Ct\u22121, find At such that\nAt = arg min A \u03c6X(C t\u22121, A) = V (Ct\u22121) \u2229X\nStep 2 Fix At, find Ct such that\nCt = arg min C \u03c6X(C,A t) = m(At)\nHowever, Step 1 requires computation of the closest centroid to every point in the dataset. Even with fast implementations such as [8], which reduces the computation for finding the closest centroid of each point, the per-iteration running time is still O(n), making it a computational bottleneck for large datasets.\nTo scale up batch k-means, \u201cstochastic approximation\u201d was proposed [6, 20], which we present as Algorithm 1. 1 The main idea is, at each iteration, the centroids are updated using one (online [6]) or a few (mini-batch [20]) randomly sampled points, denoted by St, instead of the entire dataset X. At every iteration, Algorithm 1 first approximates Step 1 and 2 using a random sample St of constant size, then it computes Ct by interpolating between Ct\u22121 and C\u0302t. Thus, stochastic k-means never directly clusters X but keeps updating its set of centroids using constant sized random samples St, so the per-iteration computation cost is reduced from O(n) in the batch case to O(1).\nNotation: In the paper, superscripts index a particular clustering, e.g., At denotes the clustering at the t-th iteration in Algorithm 1 (or batch kmeans); subscripts index individual clusters or centroids: cr denotes the r-th centroid in C corresponding to the r-th cluster Ar. We use letter n to denote cardinality, n = |X|, nr = |Ar|, etc. conv(X) denotes the convex hull of set X. We let \u03c6(C,A) denote the k-means cost of (C,A); we let \u03c6(C), \u03c6(A) denote the induced (optimal) k-means cost of a fixed C or A; as a shorthand, we often move the superscript (subscript) on the input of \u03c6(\u00b7) to \u03c6, e.g., we use \u03c6t to denote \u03c6(Ct), and \u03c6tr to denote the cost of the r-th cluster at t. We denote the largest k-means cost on X as \u03c6max and the smallest k-means cost as \u03c6opt. Finally, we let \u03c0(\u00b7) to denote a permutation on [k]."}, {"heading": "2 Related work and our contributions", "text": "Our work builds on analysis of batch k-means, which is notoriously difficult [7]. A great breakthrough was made recently by Kumar and Kannan [14], where\n1In Claim 1 of the Appendix, we formally show Algorithm 1 subsumes both online and mini-batch k-means.\nthey showed the k-SVD + constant k-means approximation + k-means update scheme efficiently and correctly clusters most data points on well-clusterable instances, and that the algorithm converges to an approximately optimal solution at geometric rate until reaching a plateau. Subsequent progress has been made on relaxing the assumptions [2] and simplifying the seeding [21]. However, these works only study local convergence of batch k-means in the sense that they require that the initial centroids are already close to the optimal solution.\nAlternating Minimization and EM Temporarily abusing notation, we let X \u2208 Rd\u00d7n be a matrix whose columns represent data points; similarly, let columns of C \u2208 Rd\u00d7k represent centroids, and let columns of A \u2208 Rk\u00d7n indicate cluster membership, i.e., Aij = 1 if and only if xi belongs to cluster j. The k-means problem can be equivalently formulated as\nmin C,A \u2016X \u2212 CA\u20162F subject to Aij \u2208 {0, 1}, \u2016Ai\u20162 = 1\nThis is a non-convex low-rank matrix factorization problem. In particular, with the discrete constraint on A, the k-means problem is close to the sparse coding problem (also known as dictionary learning or basis pursuit), where C corresponds to a set of dictionary items and A the coding matrix, and batch\nk-means can be viewed as a variant of Alternating Minimization (AM) for sparsity-constrained matrix factorization. In statistics, the algorithm is also well known to relate to EM for gaussians mixture models. Both AM and EM are popular heuristics for non-convex unsupervised learning problems, ranging from matrix completion to latent variable models, where growing interest and exciting progress towards understanding their convergence behavior are emerging [12, 10, 1, 3].\nHowever, existing analyses of AM or EM do not apply to k-means. For one thing, the prevalent assumptions for AM-style algorithms is incoherence+sparsity, while EM applies to generative models. Both are different from our deterministic assumption on the geometry of the dataset. For another, the [12, 10, 1, 3] all rely on closed form update expression; usually partial gradients are easily derivable in these problems. In contrast, we cannot obtain a closed form update rule for Step 1 of batch k-means, due to the discrete constraint on A. Instead, we work around this problem using geometric insights about k-means update developed from the clustering literature [14, 21].\nNon-convex stochastic optimization For convex problems, stochastic optimization methods are well studied [19]. Much less can be said about non-convex problems. Our analysis of stochastic k-means is influenced by two recent works in non-convex stochastic optimization [9, 4]. The first [9] studies the convergence of stochastic gradient descent (SGD) for tensor decomposition problems, which amounts to finding a local optimum of a nonconvex objective function with only saddle points and local optima. Inspired by their analysis framework, we divide our analysis of Algorithm 1 into two phases, those of global and local convergence, according to the distance from the current solution to the set of all \u201clocal optima\u201d. At the local convergence phase, since multiple local optima are present, stochastic noise may drive the algorithm\u2019s iterate off the neighborhood of attraction, and the algorithm may fail to converge locally. To deal with this, we adapted techniques that bound martingale large deviation from [4] to our local convergence analysis; the work in [4] studies the convergence of stochastic PCA algorithms, where the objective function is the non-convex Rayleigh quotient.\nOur contributions Our contributions are two-fold. For users of stochastic k-means, Theorem 1 guarantees that it converges to a local optimum with any reasonable seeding (it only requires the seeds be in the convex hull of the\ndataset) and a properly chosen learning rate, with O(1 t ) expected convergence rate, where the convergence is with respect to k-means objective. In contrast to recent batch k-means analysis [14, 2, 21], it establishes a global convergence result for stochastic k-means, since it applies to almost any initialization C0; it also applies to a wide range of datasets, without requiring a strong clusterability assumption.\nTheoretically, we have three major contributions. First, our analysis provides a novel analysis framework for k-means algorithms, by connecting the discrete optimization approach to that of gradient-based continuous optimization. Under this framework, we identify a \u201cLipschitz\u201d condition of a local optimum, under which stochastic k-means converges locally. This approach to establish local convergence is similar in-spirit to the local convergence analysis of [1, 3]. Second, we show this \u201cLipschitz\u201d condition relates to clusterability assumptions of the dataset. Consequently, Theorem 2 shows, just as batch k-means, stochastic k-means can also converge to an optimal k-means solution, under a clusterability assumption similar to [14], with a scalable seeding algorithm developed in [21]. This result extends the batch k-means results on well-clusterable instances [14, 2, 21] to stochastic k-means, and shows the two are equally powerful at finding an optimal k-means solution with strong enough clusterability assumption. Finally, the martingale technique, which we modified from [4], can be applied to future analysis of non-convex stochastic optimization problems.\n3 Revisiting batch k-means, local convergence, and clusterability\nThis section introduces the framework we construct to study k-means updates, under which we characterize the \u201clocal optima\u201d of batch k-means and identify a sufficient condition for a local optimum to be a locally stable attractor2 to the algorithm. Then we show how the clusterability of the dataset in fact determines the strength of a local optimum as an attractor.\n2By \u201clocally stable\u201d, we mean within a radius, batch k-means always converges to this local optimum.\nAlgorithm 1 Stochastic k-means Input: dataset X, number of clusters k, mini-batch size m, learning rate \u03b7tr, r \u2208 [k], convergence_criterion Seeding: Apply seeding algorithm T on X and obtain seeds C0 = {c01, . . . , c0k}; repeat At iteration t (t \u2265 1), obtain sample St \u2282 X of size m uniformly at random with replacement; set count n\u0302tr \u2190 0 and set Str \u2190 \u2205, \u2200r \u2208 [k] for s \u2208 St do Find I(s) s.t. cI(s) = C(s) StI(s) \u2190 StI(s) \u222a s; n\u0302tI(s) \u2190 n\u0302tI(s) + 1\nend for for ct\u22121r \u2208 Ct\u22121 do if n\u0302tr 6= 0 then ctr \u2190 (1\u2212 \u03b7tr)ct\u22121r + \u03b7trc\u0302tr with c\u0302tr := \u2211 s\u2208Str s\nn\u0302tr end if\nend for until convergence_criterion is satisfied\n3.1 Batch k-means as an alternating mapping\nCorresponding to the two steps in an iteration of batch k-means, it alternates between two solution spaces: the continuous space of sets of k centroids, which we denote by {C}, and the finite set of all k-clusterings, which we denote by {A}. Our key observation is that {C} can be partitioned into equivalence classes by the clustering they induce on X, and the algorithm stops if and only if two consecutive iterations stay within the same equivalence class in {C}: for any C, let v(C) denote the clustering induced by its Voronoi diagram, i.e., v is the mapping v(C) := V (C) \u2229X, where V (C) \u2229X is as defined in Section 1. It can be shown that v is a well-defined function if and only if C is not a boundary point.\nDefinition 1 (Boundary points). C is a boundary point if \u2203A \u2208 V (C) \u2229X s.t. for some r \u2208 [k], s 6= r and x \u2208 Ar \u222a As, \u2016x\u2212 cr\u2016 = \u2016x\u2212 cs\u2016.\nWe used \u201cA \u2208 V (C)\u2229X\u201d instead of A = V (C)\u2229X, since V (C)\u2229X may induce more than one clustering if C is a boundary point (see Lemma 2). So we abuse notation V (C)\u2229X to let it be the set of all possible clusterings of C.\nFor now, we ignore boundary points. Then we say C1, C2 are equivalent if they induce the same clustering, i.e., C1 \u223c C2 if v(C1) = v(C2). This construction reveals that {C} can be partitioned into a finite number of equivalence classes; each corresponds to a unique clustering A \u2208 {A}.\nAn iteration of batch k-means can be viewed as applying the composite mapping m\u25e6v : {C} \u2192 {C}, where Step 1 goes from {C} to {A} via mapping v, and Step 2 goes from {A} to {C} via the mean operation m.\n\u201cLocal optima\u201d It is well known that batch k-means stops at t when At+1 = At. Since At+1 = v(Ct) = v \u25e6m(At) and At = v(Ct\u22121), this implies it stops at t if and only if v(Ct) = v(Ct\u22121), or equivalently, v \u25e6m(At) = At. Similarly, we can derive the algorithm stops if and only if m(At+1) = Ct, or m \u25e6 v(Ct) = Ct.\nWe can thus visualize batch k-means as an iterative mapping m \u25e6 v on {C} that jumps from one equivalence class to another until it stays in the same equivalence class in two consecutive iterations, i.e., v(Ct+1) = v(Ct), and stops at some C\u2217 \u2208 {C\u2217} (see Figure 3 in the Appendix). This stopping condition also provides a natural way of defining the \u201clocal optima\u201d of k-means cost, i.e., they are the fixed points of mapping m \u25e6 v and v \u25e6m, which we call stationary solutions.\nDefinition 2 (Stationary solutions). We define C\u2217 \u2208 {C} such that m \u25e6 v(C\u2217) = C\u2217, and call it a stationary point of batch k-means. We let {C\u2217} denote the set of all stationary points. Similarly, we call A\u2217 \u2208 {A} a stationary clustering if v \u25e6m(A\u2217) = A\u2217, and we let {A\u2217} denote the set of all stationary clusterings.\nFinally, to characterize local convergence we need to define a suitable measure of distance on each space.\nDefinition 3 (Centroidal distance). For C \u2032 and C, we define centroidal distance \u2206(C \u2032, C) := min\u03c0:[k]\u2192[k] \u2211 r nr\u2016c\u2032\u03c0(r) \u2212 cr\u20162, where nr = |Ar|.\nDefinition 4 (Clustering distance). For v(C \u2032) and v(C), we define the clustering distance ClustDist(v(C \u2032), v(C)) := maxr |A\u2032 \u03c0(r) 4Ar|\nnr , where A\u2032 := v(C \u2032),\nA = v(C), 4 denotes set difference, and \u03c0 is the permutation attaining \u2206(C \u2032, C).\nBoth distances are asymmetric, non-negative, and evaluates to zero if and only if two sets of centroids (clusterings) coincide. If C\u2217 is a stationary point,\nthen for any solution C, \u2206(C,C\u2217) upper bounds the difference of k-means objective, \u03c6(C)\u2212 \u03c6(C\u2217) (Lemma 18).\nRemark For clarity of presentation, we have ignored the fact that k-means may produce degenerate solutions, where one or more clusters may be empty; similarly, the definitions of stationary solutions and centroidal distance here ignore the possible existence of boundary points. In our analysis, we used more general definitions to handle these issues, whose details are provided in the Appendix.\n3.2 A sufficient condition for the local convergence of batch k-means\nWhen is a stationary point also a locally stable attractor on {C}? We propose to characterize stability as a local Lipschitz condition on mapping v(\u00b7): we require ClustDist(v(C), v(C\u2217)) to be upper bounded by \u2206(C,C\u2217) locally.\nDefinition 5. We call C\u2217 a (b0, \u03b1)-stable stationary point if for any C \u2208 {C} such that \u2206(C,C\u2217) \u2264 b\u2032\u03c6\u2217, b\u2032 \u2264 b0, we have ClustDist(v(C), v(C\u2217)) \u2264 b 5b+4(1+\u03c6(C)/\u03c6\u2217) , with b \u2264 \u03b1b\u2032 for some \u03b1 \u2208 [0, 1).\nGeneralizing combinatorial arguments about batch k-means update in [14, 21], we show that the definition above is indeed a sufficient condition for local convergence of batch k-means.\nLemma 1. Let C\u2217 be a (b0, \u03b1)-stable stationary point. For any C such that \u2206(C,C\u2217) \u2264 b\u2032\u03c6\u2217, b\u2032 \u2264 b0, apply one step of batch k-means update on C results in a new solution C1 such that \u2206(C1, C\u2217) \u2264 \u03b1b\u2032\u03c6\u2217.\nNeighborhood of attraction By Lemma 1, we can view b0 as the radius of the neighborhood of attraction and \u03b1 the strength of the attractor, since it determines the convergence rate. A special case of (b0, \u03b1)-stability is when \u03b1 = 0, which implies v(C) = v(C\u2217) if C is within radius b0 to C\u2217. In this case, batch k-means converges in one iteration. Per our construction in Section 3.1, b0 in this case is the radius of the equivalence class that maps to clustering A\u2217 = v(C\u2217). In general, when \u03b1 > 0, we expect the radius b0 to be much larger.\nOur characterization of local convergence of batch k-means does not depend on a specific clusterability assumption, unlike previous work [14, 2, 21].\nInstead, we will see that clusterability implies local Lipschitzness of mapping v."}, {"heading": "3.3 Local Lipschitzness and clusterability", "text": "As discussed in Section 3.1, boundary points are problematic to the definition of mapping v, but how likely do they arise in practice? We answer this question by revealing the geometric implication of a boundary point, which will lead us to discover the connection between local Lipschitzness of v and clusterability.\nConsider any C \u2208 {C}, and let A\u2032 \u2208 V (C) \u2229X: for a point x \u2208 A\u2032r \u222a A\u2032s, s 6= r, let x\u0304 denote the projection of x onto the line joining cr, cs, we define\n\u2206rs(C) := min x\u2208A\u2032r\u222aA\u2032s\n|\u2016x\u0304\u2212 cr\u2016 \u2212 \u2016x\u0304\u2212 cs\u2016|\nDefinition 6 (\u03b4-margin). For any C, we say V (C) has a \u03b4-margin with respect to X if \u2203A \u2208 V (C) \u2229X such that minr,s 6=r \u2206rs(C) = \u03b4.\nLemma 2. The following are equivalent 1. C is a boundary point 2. V (C) has a zero margin with respect to X 3. |V (C)\u2229X| > 1, i.e., the clustering determined by V (C) is not unique. Thus, a set of k centroids C is a boundary point in space {C} if and only if there is a data point x \u2208 X that sits exactly on the bisector of two centroids in C. We believe a symmetric configuration like this has a low probability to arise in practice, due to random perturbations in the real world, e.g., computational error. With this insight, we define a general dataset to be one that is free of boundary stationary points.\nAssumption A [General dataset] X is a general dataset if \u2200C\u2217 \u2208 {C\u2217}, C\u2217 has \u03b4-margin with \u03b4 > 0.\nNote {C\u2217} is a finite set, since {A\u2217} is finite and we have the relation C\u2217 = m(A\u2217) (Lemma 5). Thus, Assumption A is a mild condition, as it only requires that a finite subset of the continuous space {C} to be free of boundary points, hence the name \u201cgeneral\u201d.\nWe show that for a general dataset, every stationary point is a locally stable attractor (in fact its neighborhood of attraction is exactly its equivalence\nclass induced by v). In other words, mapping v is always locally Lipschitz on a sufficiently small neighborhood of any stationary point. Moreover, on a general dataset, we can lower bound the centroidal distance between two consecutive k-means iteration, provided the algorithm has not converged. Both results, summarized in Lemma 3, are important building blocks for our proof of Theorem 1.\nLemma 3. If X is a general dataset, then \u2203rmin > 0 s.t.\n1. \u2200C\u2217 \u2208 {C\u2217}, C\u2217 is a (rmin, 0)-stable stationary point.\n2. Let m(A\u2032) /\u2208 {C\u2217} for some A\u2032 \u2208 {A} and let A\u2032 \u2208 V (C \u2032) \u2229 X, then \u2206(C \u2032,m(A\u2032)) \u2265 rmin\u03c6(m(A\u2032)).\nIn the lemma, rmin is a lower bound on the radius of attraction for points in {C\u2217}. As discussed below Lemma 1, this radius, although positive, can be very small. The radius of an attractor, as we show next, can be related to the strength of margin \u03b4.\nAssumption B [f(\u03b1)-clusterability] We say a dataset-solution pair (X,C\u2217) is f(\u03b1)-clusterable, if C\u2217 \u2208 {C\u2217} and C\u2217 has \u03b4-margin s.t. \u2200r \u2208 [k], s 6= r,\n\u03b4 \u2265 f(\u03b1) \u221a \u03c6\u2217(\n1\u221a n\u2217r + 1\u221a n\u2217s ) for \u03b1 \u2208 (0, 1)\nwith f(\u03b1) > max{642, 5\u03b1+5 256\u03b1 ,maxr\u2208[k],s 6=r n\u2217r n\u2217s }.\nProposition 1. Suppose (X,C\u2217) satisfies Assumption (B). Then, for any C such that \u2206(C,C\u2217) \u2264 b\u03c6\u2217 for some b \u2264 f(\u03b1) 2\n162 , we have maxr\u2208[k] |Ar4A\u2217r | n\u2217r \u2264 b f(\u03b1)3 .\nThat is, C\u2217 is (f(\u03b1) 2\n162 , \u03b1) -stable.\nf(\u03b1)-clusterability assumption is a simplified version of the proximity assumption in [14]. It essentially requires that \u03b4 = \u2126( \u221a k\u03c3max) for a stationary point C\u2217, where \u03c3max is the maximal standard deviation of an individual cluster. It serves as an example showing how clusterability implies local Lipschitzness of v. Furthermore, Proposition 1 reveals that the larger f(\u03b1) is, the larger the radius of attraction.\n4 Convergence analysis of stochastic k-means This section has three components. We first describe the technique we developed that helps us establish the local convergence of stochastic k-means, which will be used in the proof of both our main theorems. Then, we provide proof outlines of Theorem 1 and Theorem 2, respectively. Throughout our analysis, we consider learning rate of the form:\n\u03b7tr = \u03b7 t =\nc\u2032\nto + t , \u2200r \u2208 [k] (3)"}, {"heading": "4.1 Local convergence in the presence of stochastic noise", "text": "Unlike a convex problem, the difficulty of establishing local convergence in our case is, if the algorithm\u2019s solution is driven off the current neighborhood of attraction by stochastic noise at any iteration, it may be drawn to a different attractor. Fixing a (b0, \u03b1)-stable stationary point C\u2217, suppose the algorithm is within the neighborhood of attraction of C\u2217 at time \u03c4 . The event \u201cthe algorithm\u2019s iterate is within radius b0 to C\u2217 up to t\u2212 1\u201d can be formalized as:\n\u2126t := {\u2206(Ci, C\u2217) \u2264 b0\u03c6\u2217,\u2200\u03c4 \u2264 i < t} (4)\nLetting t\u2192\u221e leads to the following definition:\n\u2126\u221e := {\u2206(Ci, C\u2217) \u2264 b0\u03c6\u2217, \u2200i \u2265 \u03c4} (5)\nClearly, local convergence to C\u2217 implies \u2126\u221e; Lemma 1 also requires \u2126\u221e as a prerequisite. So we need to show Pr(\u2126\u221e) \u2248 1, even with stochastic noise."}, {"heading": "4.1.1 Inequality for a martingale-like process", "text": "We use \u2206t := \u2206(Ct, C\u2217) as a shorthand and we let E\u2126t [\u00b7] denote the expectation conditioning on \u2126t. Let \u2126 represent the sample space starting from \u03c4 , then \u2126t+1 \u2282 \u2126t \u2282 \u2126,\u2200t > \u03c4 . Conditioning on \u2126t, we can apply Lemma 1 to get\n\u2206t \u2264 \u2206t\u22121(1\u2212 \u03b2 to + t\n) + [ c\u2032\nto + t ]2 t1 +\n2c\u2032\nto + t t2 |\u2126t\nwhere with probability 1, \u03b2 \u2265 2, and the stochastic noise terms t1, t2 are of order O(\u03c6t\u22121). Therefore, (\u2206t) is a supermartingale-like process with bounded stochastic noise, conditioning on \u2126t.\nTo exploit this conditional structure, we partition the failure event \u2126\\\u2126\u221e, i.e., the event that the algorithm eventually escapes this neighborhood, as a disjoint union of events \u2126t \\\u2126t+1, and then our task becomes upper bounding Pr(\u2126t \\\u2126t+1) for all t. To achieve this, we first derive an upper bound on the conditional moment generating function E\u2126t [exp\u03bb\u2206t] as a function of b0\u03c6\u2217 and the noise terms, using ideas in [4]. Then applying conditional Markov\u2019s inequality, we get\nPr(\u2126t \\ \u2126t+1) = Pr{\u2206t > b0\u03c6\u2217|\u2126t} \u2264 E\u2126t [exp\u03bb\u2206\nt]\nexp\u03bbb0\u03c6\u2217\nSince the inequality holds for all \u03bb > 0, we can choose \u03bb as a function of ln t, which enables us to bound Pr(\u2126t \\ \u2126t+1) by \u03b4(t+1)2 , for all t \u2265 1, \u03b4 > 0, with sufficiently large c\u2032 and to in (3). This implies\nP (\u2126\u221e) = 1\u2212 \u2211 t\u2265\u03c4 Pr(\u2126t \\ \u2126t+1) \u2265 1\u2212 \u03b4\nEssentially, this is our variant of martingale large deviation bound. Comparing to related work [4, 9], our technique yields a tighter bound on the failure probability than [9], which uses Azuma\u2019s inequality, and is much simpler than [4]; the latter constructs a complex nested sample space and applies Doob\u2019s inequality, whereas ours simply uses Markov\u2019s inequality. In addition, our technique allows us to explore the noise dependence on \u2126t, which leads to a weaker dependence of parameter to on the initial condition bo\u03c6\u2217.\nWe believe this technique can be useful for other non-convex analysis of stochastic methods. We provide one example here. Our current analysis considers the flat learning rate in (3). However, in practice the following adaptive learning rate is commonly used:\n\u03b7tr := n\u0302tr\u2211 i\u2264t n\u0302 i r\n(6)\nWe conjecture that stochastic k-means with the above learning rate also has O(1\nt ) convergence, as supported by our experiments (see Section 5).\nHowever, it is difficult to incorporate (6) into our analysis: n\u0302ir is a random quantity whose probability depends on the clustering configuration v(Ci\u22121). To establish O(1\nt ) convergence, we need to show E\u03b7tr \u2248 \u0398(1t ). Without\nadditional information, this is hopeless, as \u03b7tr depends on information of the entire history of the process. But conditioning on \u2126t, we can show that nir \u2248 n\u2217r, for all r \u2208 [k], i \u2265 \u03c4 . Using this relation, we may approximate E\u03b7tr.\nSince our technique allows this conditional dependence, we may extend our local convergence analysis to incorporate the case where \u03b7tr is adaptive.\nFinally, conditioning on \u2126t, we can combine Lemma 1 with the standard arguments in stochastic gradient descent [19, 1, 3] to obtain the O(1\nt ) local\nconvergence rate (Theorem 3): E\u2126t [\u2206(Ct, C\u2217)] = O(1t )."}, {"heading": "4.1.2 Proof sketch of main theorems", "text": "Equipped with the necessary ingredients, we are ready to explain the analysis that leads to our main theorems.\nTheorem 1 is a global convergence result. To prove it, we divide our analysis of Algorithm 1 into two phases, that of global convergence and local convergence, indicated by the distance from the current solution to stationary points, \u2206(Ct, C\u2217).\nWe define global convergence phase as a time interval of random length \u03c4 such that \u2200t < \u03c4 , \u2200C\u2217 \u2208 {C\u2217}, \u2206(Ct, C\u2217) > 1\n2 rmin\u03c6 \u2217 (rmin as defined in Lemma 3). During this phase, we obtain a lower bound on the expected decrease in k-means objective (Lemma 14):\nE[\u03c6t+1 \u2212 \u03c6t|Ft] \u2264 \u22122\u03b7t+1pt+1min(\u03c6t \u2212 \u03c6\u0303t) + (\u03b7t+1)26\u03c6t\nHere, \u03c6\u0303t := \u2211\nr \u2211 x\u2208v(ctr) \u2016x\u2212m(v(ctr))\u20162 and ptmin := minr,ptr(m)>0 p t r(m), with\nptr(m) = Pr{ct\u22121r is updated at t with sample size m} = 1\u2212 (1\u2212 ntr n\n)m. Thus, the term pt+1min(\u03c6t \u2212 \u03c6\u0303t) lower bounds the drop in k-means objective. For pt+1r (m) > 0, by the discrete nature of cluster assignment, ntr \u2265 1. So pt+1min \u2265 1\u2212 (1\u2212 1n)\nm \u2265 1\u2212 e\u2212mn . On the other hand, \u03c6t \u2212 \u03c6\u0303t = \u2206(Ct,m(v(Ct))) by Lemma 21. Thus, to\nlower bound the decrease by zero, we only need to lower bound \u2206(Ct,m(v(Ct))). The idea is, in case m(v(Ct))) is a non-stationary point, by part 2 of Lemma 3, \u2206(Ct,m(v(Ct))) > 1\n2 rmin\u03c6(m(v(C t))). Otherwise, m(v(Ct))) is a stationary point, and by definition of the global convergence phase, the same lower bound applies, which implies ptmin(\u03c6t\u2212 \u03c6\u0303t) is lower bounded by a positive constant in the global convergence phase. Since we choose \u03b7t := \u0398(1\nt ), the expected per\niteration drop of cost is of order \u2126(1 t ), which forms a divergent series; after a sufficient number of iterations the expected drop can be arbitrarily large. We conclude that \u2206(Ct, C\u2217) cannot be bounded away from zero asymptotically, since the k-means cost of any clustering is positive (Lemma 15). Hence, starting from any initial point C0, the algorithm will always be drawn to a\nstationary point, ending its global convergence phase after a finite number of iterations, i.e., Pr(\u03c4 <\u221e) = 1.\nAt the beginning of the local convergence phase, \u2206(C\u03c4 , C\u2217) \u2264 1 2 rmin\u03c6 \u2217 for some C\u2217 \u2208 {C\u2217}. Again by Lemma 3, the algorithm is within the neighborhood of attraction of C\u2217, and thus we can apply the local convergence result in Theorem 3. Combining both phases leads us to Theorem 1.\nTheorem 1. Suppose X satisfies Assumption (A). Fix any 0 < \u03b4 < 1 e , if we run Algorithm 1 with arbitrary C0 such that C0 \u2282 conv(X), and any mini-batch size m \u2265 1, and choose learning rate \u03b7t = c\u2032\nt+to such that\nc\u2032 > max{ \u03c6max (1\u2212 e\u2212mn )rmin\u03c6opt , 1 (1\u2212 e 4m5n ) }\nto \u2265 768(c\u2032)2(1 + 1\nrmin )2n2 ln2\n1\n\u03b4\nThen there exists events G(A\u2217), parametrized by A\u2217, such that\nPr{\u222aA\u2217\u2208{A\u2217}[k]G(A \u2217)} \u2265 1\u2212 \u03b4\nFor any stationary clustering A\u2217, we have \u2200t \u2265 0,\nE{\u03c6t \u2212 \u03c6\u2217|G(A\u2217)} = O(1 t )\nRemark: \u222aA\u2217\u2208{A\u2217}G(A\u2217) is contained in the event that Algorithm 1 converges to a stationary point. Thus, Theorem 1 implies that, with any reasonable initialization and sufficiently large c\u2032, to, stochastic k-means converges globally almost surely; conditioning on global convergence to a stationary point A\u2217, the convergence rate is O(1\nt ) in expectation. Also note \u03c6max is\nupper bounded, since C0 \u2282 conv(X) implies Ct \u2282 conv(X), \u2200t \u2265 1 (see Claim 2). Finally, note that Theorem 1 establishes global convergence to a local optimum, but it does not guarantee that stochastic k-means converges to the same local optimum as its batch counterpart, even with the same initialization.\nOur next theorem complements Theorem 1 in the sense that it provides local convergence to a global optimum of k-means problem: it shows that if we use Algorithm 2 as our seeding algorithm and the optimal clustering satisfies f(\u03b1)-clusterability, then stochastic k-means converges to a global\nAlgorithm 2 Buckshot seeding [21] {\u03bdi, i \u2208 [mo]} \u2190 sample mo points from X uniformly at random with replacement {S1, . . . , Sk} \u2190run Single-Linkage on {\u03bdi, i \u2208 [mo]} until there are only k connected components left C0 = {\u03bd\u2217r , r \u2208 [k]} \u2190 take the mean of the points in each connected component Sr, r \u2208 [k]\noptimum of k-means objective at rate O(1 t ). Its proof has three parts: First, we show f(\u03b1)-clusterability implies (b0, \u03b1)-stability, as stated in Proposition 1. Second, we show C0 found by Algorithm 2 is within the neighborhood of attraction of the optimal solution with high probability, by adapting Theorem of [21] with the additional assumption that\nf(\u03b1) \u2265 5\n\u221a 1\n2wmin ln(\n2\n\u03bep\u2217min ln\n2k\n\u03be ) (7)\nwhere wmin and p\u2217min are geometric properties of clustering v(C\u2217) (see definitions in Appendix). Finally, combining these with Theorem 3 completes the proof.\nTheorem 2. Suppose (X,C\u2217) satisfies Assumption (B) and f(\u03b1) in addition satisfies (7) for any 0 < \u03b1 < 1, \u03be > 0. Fix \u03b2 \u2265 2, and 0 < \u03b4 < 1\ne . If we\ninitialize Co in Algorithm 1 by Algorithm 2, with mo satisfying log 2k \u03be\np\u2217min < mo <\n\u03be 2 exp{2(f(\u03b1) 4 \u2212 1)2w2min}, and running Algorithm 1 with learning rate of the form \u03b7t = c \u2032\nt+to and mini-batch size m so that\nm > ln(1\u2212\n\u221a \u03b1)\nln(1\u2212 4 5 p\u2217min)\nc\u2032 > \u03b2\n2[1\u2212 \u221a \u03b1\u2212 e\u2212 45mp\u2217min ]\nand to \u2265 867(c\u2032)2n2 ln2 1\n\u03b4\nThen \u2200t \u2265 1, there exists event Gt \u2282 \u2126 s.t.\nPr{Gt} \u2265 (1\u2212 \u03b4)(1\u2212 \u03be) and\nE[\u03c6t|Gt]\u2212 \u03c6\u2217 \u2264 E[\u2206(Ct, C\u2217)|Gt] = O( 1\nt )\nRemark: Theorem 2 in fact applies to any stationary point satisfying f(\u03b1)clusterability, which includes the optimal k-means solution. Interestingly, we cannot provide guarantee for online k-means (m = 1) here. Our intuition is, instead of allowing stochastic k-means to converge to any stationary point as in Theorem 1, it studies convergence to a target stationary point; a larger m provides more stability to the algorithm and prevents it from straying away from the target.\nAlgorithmic parameters: The constant on the bound of convergence rate for both theorems, which we hide in Big-O notation, in fact scales with c\u2032. This suggests one should choose c\u2032 to be neither too large nor too small. The lower bound on c\u2032, in turn, depends on other parameters like m and k (implicitly). For larger m and smaller k, the lower bound on c\u2032 is smaller."}, {"heading": "5 Experiments", "text": "To verify the O(1\nt ) global convergence rate of Theorem 1, we run stochastic\nk-means with varying learning rate, mini-batch size, and k on RCV1 [15]. The\ndataset has manually categorized 804414 newswire stories with 103 topics, where each story is a 47236-dimensional sparse vector; it was used in [20] for empirical evaluation of mini-batch k-means. We experimented with both flat learning rate in (3) and the adaptive learning rate (6), which we refer to as BBS-rate, since it was proposed by authors of [6, 20].\nFigure 2 shows the convergence in k-means cost of stochastic k-means algorithm over 100 iterations for varying m and k; fix each pair (m, k), we initialize Algorithm 1 with a same set of k random centroids and run stochastic k-means with varying learning rate parameters (c\u2032, to), and we average the performance of each learning rate setup over 5 runs to obtain the original convergence plot. Figure 2b is an example of a convergence plot before transformation. The dashed black line in each log-log figure is \u03c6\n0\u2212\u03c6min t\n(\u03c6min is the lowest empirical k-means cost), a function of order \u0398(1t ). To compare the performance of stochastic k-means with this baseline, we first transform the original \u03c6t vs t plot to that of \u03c6t \u2212 \u03c6min vs t. By Theorem 1, E[\u03c6t\u2212\u03c6\u2217|G(A\u2217)] = O(1\nt ), so we expect the slope of the log-log plot of \u03c6t\u2212\u03c6\u2217\nvs t to be at least as large as that of \u0398(1 t ). Although we do not know the exact cost of the stationary point, since the algorithm has reached a stable phase over 100 iterations, as illustrated by Figure 2b, we simply use \u03c6min, as an estimate of \u03c6\u2217. Most log-log convergence graphs fluctuate around a line with a slope at least as steep as that of \u0398(1\nt ), and do not seem to be sensitive\nto the choice of learning rate in our experiment. Note in some plots, such as Figure 2a, the initial phase is flat. This is because we force the plot to start at \u03c60 \u2212 \u03c6min instead of their true intercept on the y-axis. BBS-rate exhibits similar behavior to our flat learning rates."}, {"heading": "6 Discussion", "text": "This work provides the first analysis of the convergence rate of stochastic kmeans, but several questions remain unanswered. First, our analysis applies to the flat learning rate in (3) while adaptive learning rate in (6) is more common in practice. From our experiments, we conjecture that O(1\nt ) convergence can\nalso be attained in the latter case. As discussed in Section 4.1.1, the key question is whether we can show the adaptive rate, a random quantity that depends on all information prior t, is of order \u0398(1\nt ). Second, we provide two\nexamples of assumptions that imply Lipschitzness of v. Can we find other assumptions? In particular, is there a spectrum of assumptions in-between our Assumption (A) and (B) that imply different strength of Lipschitzness? We\nalso believe further study of batch k-means can be made using our framework in Section 3.1. For example, we observed that the radius of the neighborhood of an attractor (stationary point) is related to clusterability. Can we use this to relate the number of local attractors to clusterability of the dataset? In addition, if a stationary point has a large radius of attraction in {C}, then intuitively, two different random initializations will likely fall into this same neighborhood. Does this provide another angle to the clustering stability analysis [22, 5]?"}, {"heading": "7 Appendix A: supplementary materials to Sec-", "text": "tion 3.1\nIn this part of the Appendix, we provide details on the construction of our framework that are not included in Section 3.1 due to space constraints.\nHandling non-degenerate and boundary points One problem with k-means is it may produce degenerate solutions: if the solution Ct has k centroids, it is possible that data points are mapped to only k\u2032 < k centroids. To handle degenerate cases, starting with |C0| = k, we consider an enlarged clustering space {A}[k], which is the union of all k\u2032-clusterings with 1 \u2264 k\u2032 \u2264 k. We use the pre-image v\u22121(A) \u2208 {C} to denote the non-boundary points C such that v(C) = A, i.e., these are the set of non-boundary points in the equivalence class induced by clustering A. To include boundary points as well, we devise the operator Cl(\u00b7) as the \u201cclosure\u201d of an equivalence class v\u22121(A), which includes all boundary points C \u2032 such that A \u2208 V (C \u2032) \u2229X.\nUsing the above two extensions, we give the robust definition of stationary clusterings and stationary points, which we use in our analysis.\nDefinition 7 (Stationary clusterings). We call A\u2217 a stationary clustering of X, if m(A\u2217) \u2208 Cl(v\u22121(A\u2217)). We let {A\u2217}[k] \u2282 {A}[k] denote the set of all stationary clusterings of X with number of clusters k\u2032 \u2208 [k].\nFor each A\u2217, we define a matching centroidal solution C\u2217.\nDefinition 8 (Stationary points). For a stationary clustering A\u2217 with k\u2032 clusters, we define C\u2217 = {c\u2217r , r \u2208 [k\u2032]} to be a stationary point corresponding to A\u2217, so that \u2200A\u2217r \u2208 A\u2217, c\u2217r := m(A\u2217r). We let {C\u2217}[k] denote the corresponding set of all stationary points of X with k\u2032 \u2208 [k].\nWith the robust definitions, Figure 3 provides a visualization of batch k-means walking on {C} (and {A}[k]) as an iterative mapping m \u25e6 v (v \u25e6m, resp.). In {C}, it jumps from one equivalence class to another until it stays in the same equivalence class in two consecutive iterations.\nNow we extend \u2206(\u00b7, \u00b7) to include the degenerate cases. Fix a clustering A with its induced k centroids C := m(A), and another set of k\u2032-centroids C \u2032 (k\u2032 \u2265 k) with its induced clustering A\u2032, if |A\u2032| = |A| = k (this means if k\u2032 > k, then C \u2032 has at least one degenerate centroid), then we can pair the subset of non-degenerate k centroids in C \u2032 with those in C, and ignore the degenerate centroids. Under this condition, we can extend Definition 3 to include degenerate solutions as well, provided C = m(A) for some clustering A, which is always satisfied in our subsequent analysis.\nA sufficient condition for the local convergence of batch k-means We show batch k-means algorithm has geometric convergence in the local neighborhood of a stable stationary point in the solution space.\nProof of Lemma 1. Without loss of generality, we let \u03c0(r) = r, \u2200r \u2208 [k]. Let \u03c1rout := |\u222as 6=r(As\u2229A\u2217r)| n\u2217r , and \u03c1rin := |\u222as6=r(Ar\u2229A\u2217s)| n\u2217r ; let \u03c1max := maxr |Ar4A\u2217r | n\u2217r . Clearly, (\u03c1rout + \u03c1 r in) = |Ar4A\u2217r | n\u2217r \u2264 \u03c1max, by our definition. Now, similar to [21], we can get \u2016m(Ar) \u2212 c\u2217r\u2016 = \u2016 (1\u2212\u03c1rout)n\u2217rm(Ar\u2229A\u2217r)+ \u2211 s 6=r \u2211 x\u2208Ar\u2229A\u2217s x\n(1\u2212\u03c1rout+\u03c1rin)n\u2217r \u2212 c\u2217r\u2016 \u2264 1\u2212\u03c1out 1\u2212\u03c1rout+\u03c1rin \u2016m(Ar \u2229 A\u2217r)\u2212 c\u2217r\u2016+ \u2016 \u2211 s 6=r \u2211 x\u2208Ar\u2229A\u2217s x\u2212c\u2217r\u2016 (1\u2212\u03c1rout+\u03c1rin)n\u2217r And as in [21], we get (1\u2212 \u03c1out)\u2016m(Ar \u2229A\u2217r)\u2212\nc\u2217r\u2016 \u2264 \u221a \u03c1rout\u03c6 \u2217 r\u221a\nn\u2217r . Now we bound the second term: by Cauchy-Schwarz inequality, \u2016 \u2211 s 6=r \u2211 x\u2208Ar\u2229A\u2217s x \u2212 c \u2217 r\u20162 \u2264 ( \u2211 s 6=r \u2211 x\u2208Ar\u2229A\u2217s 1 2)( \u2211 s 6=r \u2211 x\u2208Ar\u2229A\u2217s \u2016x \u2212 c \u2217 r\u20162) = \u03c1rinn \u2217 r \u2211 s 6=r \u2211 x\u2208Ar\u2229A\u2217s \u2016x \u2212 c \u2217 r\u20162. Thus, \u2200r \u2208 [k], \u2016m(Ar) \u2212 c\u2217r\u20162 \u2264 4 \u03c1rout\u03c6 \u2217 r n\u2217r + 4 \u03c1rin \u2211 s 6=r \u2211 x\u2208Ar\u2229A\u2217s \u2016x\u2212c\u2217r\u20162\nn\u2217r , where we use the assumption that \u03c1max < 14 < 1\u2212 1\u221a 2 . Summing over all r, \u2211\nr n \u2217 r\u2016m(Ar)\u2212 c\u2217r\u20162 \u2264 4\u03c1max \u2211 r(\u03c6 \u2217 r + \u2211 s 6=r \u2211\nx\u2208Ar\u2229A\u2217s \u2016x\u2212 c\u2217r\u20162). By Lemma 4, \u2211 r \u2211 s 6=r \u2211 x\u2208Ar\u2229A\u2217s \u2016x \u2212 c \u2217 r\u20162 can be upper bounded by\n\u03c6(C \u2032) + \u2211 r nr\u2016m(Ar) \u2212 c\u2217r\u20162 = \u03c6(C \u2032) + \u2211\nr(1 \u2212 \u03c1rout + \u03c1rin)n\u2217r\u2016m(Ar) \u2212 c\u2217r\u20162 \u2264 \u03c6(C \u2032) + (1 + \u03c1max) \u2211 r n \u2217 r\u2016m(Ar) \u2212 c\u2217r\u20162. Substituting this into the previous\ninequality, we have (1 \u2212 4\u03c1max(1 + \u03c1max)) \u2211 r n \u2217 r\u2016m(Ar) \u2212 c\u2217r\u20162 \u2264 4\u03c1max(\u03c6\u2217 +\n\u03c6(C \u2032)). Thus, \u2211\nr n \u2217 r\u2016m(Ar) \u2212 c\u2217r\u20162 \u2264 \u03c1max 1\u22124\u03c1max(1+\u03c1max) [\u03c6 \u2217 + \u03c6(C \u2032)]. By our assumption, \u03c1max \u2264 b\n5b+4(1+ \u03c6(C) \u03c6\u2217 )\n< 14 , so \u03c1max 1\u22124\u03c1max(1+\u03c1max) \u2264 \u03c1max 1\u22125\u03c1max \u2264 b\n1+ \u03c6(C) \u03c6\u2217 , and \u03c1max\n1\u22124\u03c1max(1+\u03c1max) [\u03c6 \u2217 + \u03c6(C \u2032)] \u2264 b\u03c6\u2217, since \u03c6(C \u2032) \u2264 \u03c6(C) (equality holds if C is a stationary point).\nLemma 4. Fix any target clustering C\u2217, and another clustering C with a matching \u03c0 : [k]\u2192 [k]. Let C \u2032 := {m(Ar), r \u2208 [k]}. Then\u2211\nr \u2211 s6=r \u2211 x\u2208A\u03c0(r)\u2229A\u2217s \u2016x\u2212 c\u2217r\u20162\n\u2264 \u03c6(C \u2032)\u2212 \u2211 r \u03c6(c\u2217r ;A\u03c0(r) \u2229A\u2217r) + \u2211 r nr\u2016m(Ar)\u2212 c\u2217r\u20162\nProof. Without loss of generality, we let \u03c0(r) = r.\n\u03c6(C \u2032)\u2212 \u03c6(C\u2217) = \u2211 r \u2211 x\u2208Ar \u2016x\u2212 c\u2217r\u20162 \u2212 \u2211 r \u2211 x\u2208A\u2217r \u2016x\u2212 c\u2217r\u20162\n+ \u2211 r \u2211 x\u2208Ar \u2016x\u2212m(Ar)\u20162 \u2212 \u2211 r \u2211 x\u2208Ar \u2016x\u2212 c\u2217r\u20162\nSo \u2211\nr \u2211 x\u2208Ar \u2016x\u2212 c \u2217 r\u20162\u2212 \u2211 r \u2211 x\u2208A\u2217r \u2016x\u2212 c \u2217 r\u20162 = \u03c6(C \u2032)\u2212 \u03c6(C\u2217)\u2212 \u2211 r \u2211 x\u2208Ar \u2016x\u2212\nm(Ar)\u20162 + \u2211\nr \u2211 x\u2208Ar \u2016x\u2212 c \u2217 r\u20162 \u2264 \u03c6(C)\u2212 \u03c6(C\u2217) + \u2211 r nr\u2016m(Ar)\u2212 c\u2217r\u20162. Now, we\nclaim \u2211\nr \u2211 x\u2208Ar \u2016x\u2212c \u2217 r\u20162\u2212 \u2211 r \u2211 x\u2208A\u2217r \u2016x\u2212c \u2217 r\u20162 = \u2211 r \u2211 s 6=r \u2211 x\u2208Ar\u2229A\u2217s{\u2016x\u2212c \u2217 r\u20162\u2212\n\u2016x \u2212 c\u2217s\u20162}. This is because we can enumerate x using clustering \u222arAr: for each x \u2208 Ar, either x \u2208 Ar \u2229A\u2217r , then \u2016x\u2212 c\u2217r\u20162\u2212\u2016x\u2212 c\u2217r\u20162 = 0, or x \u2208 Ar \u2229A\u2217s for some s 6= r, which means the difference is \u2016x\u2212c\u2217r\u20162\u2212\u2016x\u2212c\u2217s\u20162 (and this term is positive by optimality of clustering \u222arA\u2217r fixing {c\u2217r}). Thus, \u2211 r \u2211 s 6=r \u2211 x\u2208Ar\u2229A\u2217s \u2016x\u2212 c \u2217 r\u20162 =\u2211\nr \u2211 x\u2208Ar \u2016x \u2212 c \u2217 r\u20162 \u2212 \u2211 r \u2211 x\u2208A\u2217r \u2016x \u2212 c \u2217 r\u20162 + \u2211 r \u2211 s 6=r \u2211 x\u2208Ar\u2229A\u2217s \u2016x \u2212 c \u2217 s\u20162 \u2264\n\u03c6(C \u2032)\u2212 \u03c6(C\u2217) + \u2211 r nr\u2016m(Ar)\u2212 c\u2217r\u20162 + \u2211 r \u2211 s 6=r \u2211 x\u2208Ar\u2229A\u2217s \u2016x\u2212 c \u2217 s\u20162 = \u03c6(C \u2032)\u2212\u2211\nr \u03c6(c \u2217 r;Ar \u2229A\u2217r) + \u2211 r nr\u2016m(Ar)\u2212 c\u2217r\u20162, where the last equality is by observing\nthat \u03c6(C\u2217) = \u2211\nr \u2211 Ar\u2229A\u2217r \u2016x\u2212 c \u2217 r\u20162 + \u2211 r \u2211 s 6=r \u2211 x\u2208Ar\u2229A\u2217s \u2016x\u2212 c \u2217 s\u20162."}, {"heading": "7.1 Local Lipschitzness and clusterability", "text": "Proof of Lemma 2. \u201c1 =\u21d2 2\u201d obviously holds since \u2016x\u2212 cr\u2016 = \u2016x\u2212 cs\u2016 if and only if \u2016x\u0304\u2212cr\u2016\u2212\u2016x\u0304\u2212cs\u2016. \u201c2 =\u21d2 3\u201d: let A \u2208 V (C)\u2229X be the clustering achieving the zero margin, and consider x \u2208 Ar \u222a As s.t. \u2016x\u0304\u2212 cr\u2016 \u2212 \u2016x\u0304\u2212 cs\u2016; without loss of generality, assume x \u2208 Ar according to clustering A, and define A\u2032 to be the same clustering as A for all points in X but x, where it assigns x to As. Then A\u2032 \u2208 V (C) \u2229X and |V (C) \u2229X| \u2265 2 > 1. \u201c3 =\u21d2 1\u201d: Suppose otherwise. Then every point x has a unique center that minimizes its distance to it, which means the clustering determined by V (C) \u2229A is unique. A contradiction.\nLemma 5. If C\u2217 \u2208 {C\u2217}, then C\u2217 = m(A\u2217), where A\u2217 \u2208 {A\u2217} and A\u2217 = v(C\u2217).\nProof. By definition of stationary points, C\u2217 = m \u25e6 v(C\u2217). Let A = v(C\u2217), then m(A) = C\u2217 and v\u25e6m(A) = v(C\u2217) = A. Thus A \u2208 {A\u2217} by definition of a stationary clustering.\nLemma 6. Fix a clustering A = {A1, . . . , Ak}, and let C \u2208 v\u22121(A). Then \u2203\u03b4 > 0 such that the following statement holds:\nFor C \u2032s.t. \u2206(\u00b7, \u00b7) is defined ,\u2206(C \u2032, C) < \u03b4 =\u21d2 C \u2032 \u2208 v\u22121(A) (8)\nProof. Since C is not a boundary point, \u2200x \u2208 Ar, r \u2208 [k],\n\u2016x\u2212 cr\u2016 < \u2016x\u2212 cs\u2016,\u2200s 6= r\nSo we can choose \u03b4 > 0 s.t. \u2200x \u2208 Ar, \u2200r \u2208 [k], s 6= r,\n\u2016x\u2212 cr\u2016 < \u2016x\u2212 cs\u2016 \u2212 2 \u221a \u03b4\nLet \u03c0\u2217 be a permutation such that \u2206(C \u2032, C) is defined. We have \u2200x \u2208 Ar, r \u2208 [k], s 6= r,\n\u2016x\u2212 c\u2032\u03c0\u2217(s)\u2016 \u2212 \u2016x\u2212 c \u2032 \u03c0\u2217(r)\u2016 \u2265 \u2016x\u2212 cs\u2016 \u2212 \u2016c \u2032 \u03c0\u2217(s) \u2212 cs\u2016\n\u2212(\u2016x\u2212 cr\u2016+ \u2016cr \u2212 c\u2032\u03c0\u2217(r)\u2016) > \u2016x\u2212 cs\u2016 \u2212 \u2016x\u2212 cr\u2016 \u2212 2 \u221a \u03b4 \u2265 0\nwhere the second inequality is by the fact that\nmax r \u2016c\u2032\u03c0\u2217(r) \u2212 cr\u2016 2 \u2264 \u2206(C \u2032, C) < \u03b4\nTherefore, V (C \u2032) \u2229X = A, i.e., C \u2032 \u2208 v\u22121(A).\nLemma 7. Suppose \u2200C\u2217 \u2208 {C\u2217}[k], C\u2217 is not a boundary point (i.e., suppose Assumption (A) holds). Let C = m(A\u2032) /\u2208 {C\u2217}[k] for some A\u2032 \u2208 {A} and let C \u2032 \u2208 Cl(v\u22121(A\u2032)), then \u2203\u03b4 > 0 s.t. \u2206(C \u2032, C) \u2265 \u03b4.\nProof. We prove the lemma by contradiction: suppose \u2200\u03b4 > 0, \u2203C \u2032 s.t. C \u2032 \u2208 Cl(v\u22121(A\u2032)) and \u2206(C \u2032, C) < \u03b4. First, we claim that for \u03b4 sufficiently small, C must be a boundary point: suppose otherwise, then by Lemma 6, v(C \u2032) = v(C) = A\u2032, contradicting the fact that C /\u2208 {C\u2217}[k]. Let A \u2208 V (C)\u2229X. Since C is a boundary point, \u2203r, s and x \u2208 Ar \u222aAs s.t.\n\u2016x\u2212 cr\u2016 = \u2016x\u2212 cs\u2016\nNow, we choose \u03b4 > 0 to be sufficiently small so that for any A\u2032 \u2208 V (C \u2032) \u2229 X, clustering A\u2032 only differs from A on the assignment of these points sitting on the bisector. This implies C \u2208 Cl(v\u22121(A\u2032)), which implies C is a boundary stationary point, a contradiction.\nLemma 8. If \u2200C\u2217 \u2208 {C\u2217}[k], C\u2217 is a non-boundary stationary point, that is, C\u2217 := m(A\u2217) \u2208 v\u22121(A\u2217). Then \u2203rmin > 0 such that \u2200C\u2217 \u2208 {C\u2217}[k], C\u2217 is a (rmin, 0)-stable stationary point.\nProof. Fix any k in the range of [k] (we abuse the notation with the same k here). For any C such that \u2206(C,C\u2217) exists (i.e., |C| = k\u2032 \u2265 k = |C\u2217|), we first show \u2203r\u2217 > 0, such that the following statement holds:\n\u2206(C,C\u2217) < r\u2217\u03c6\u2217 =\u21d2 C \u2208 v\u22121(A\u2217)\nSince C\u2217 is a non-boundary point, there is a permutation \u03c0o of [k] such that \u2200x \u2208 Ar,\u2200r \u2208 [k] and \u2200s 6= r,\n\u2016x\u2212 c\u2217\u03c0o(r)\u2016 < \u2016x\u2212 c \u2217 \u03c0o(s) \u2016\nWe choose r\u2217 > 0 so that \u2200x \u2208 Ar,\u2200r \u2208 [k], \u2200s 6= r,\n\u2016x\u2212 c\u2217\u03c0o(r)\u2016 \u2264 \u2016x\u2212 c \u2217 \u03c0o(s) \u2016 \u2212 2\n\u221a r\u2217\u03c6\u2217, \u2200r \u2208 [k], s 6= r\nwith equality holds for at least one triple of (x, r, s). Let \u03c0\u2217 be a permutation satisfying\n\u03c0\u2217 = arg min \u03c0 \u2211 r\u2208[k] n\u2217r\u2016c\u03c0(r) \u2212 c\u2217r\u20162\nLet \u03c0\u2032 := \u03c0\u2217 \u25e6 \u03c0o. We have \u2200(x, r, s) triples,\n\u2016x\u2212 c\u03c0\u2032(s)\u2016 \u2212 \u2016x\u2212 c\u03c0\u2032(r)\u2016 \u2265 \u2016x\u2212 c\u2217\u03c0o(s)\u2016 \u2212 \u2016c \u2217 \u03c0o(s)\n\u2212 c\u03c0\u2032(s)\u2016 \u2212(\u2016x\u2212 c\u2217\u03c0o(r)\u2016+ \u2016c \u2217 \u03c0o(r) \u2212 c\u03c0\u2032(r)\u2016)\n> \u2016x\u2212 c\u2217\u03c0o(s)\u2016 \u2212 \u2016x\u2212 c \u2217 \u03c0o(r) \u2016 \u2212 2\n\u221a r\u2217\u03c6\u2217 \u2265 0\nwhere the second inequality is by the fact that\nmax r \u2016c\u03c0\u2217(r) \u2212 c\u2217r\u20162 \u2264 \u2206(C,C\u2217) < r\u2217\u03c6\u2217\n=\u21d2 max r \u2016c\u03c0\u2217(r) \u2212 c\u2217r\u2016 <\n\u221a r\u2217\u03c6\u2217\nSince \u03c0\u2032 is the composition of two permutations of [k], it is also a permutation of [k], and \u2200r, s 6= r, \u2016x\u2212 c\u03c0\u2032(r)\u2016 < \u2016x\u2212 c\u03c0\u2032(s)\u2016, so C \u2208 v\u22121(A\u2217). Since by our definition, r\u2217 is unique for each C\u2217. Since {C\u2217}[k] is finite, taking the minimum over all such r\u2217, i.e., rmin := minC\u2217\u2208{C\u2217}[k] r \u2217 completes the proof.\nThe following is a restatement of Lemma 3, which is robust to degeneracy and boundary points.\nLemma 9 (Restatement of Lemma 3). If X is a general dataset, then \u2203rmin > 0 s.t.\n1. \u2200C\u2217 \u2208 {C\u2217}[k], C\u2217 is a (rmin, 0)-stable stationary point.\n2. Let m(A\u2032) /\u2208 {C\u2217}[k] for some A\u2032 \u2208 {A} and let C \u2032 \u2208 Cl(v\u22121(A\u2032)), then \u2206(C \u2032,m(A)) \u2265 rmin\u03c6(m(A)).\nProof. By Lemma 8, \u2203r\u2217min > 0 s.t. \u2200C\u2217, C\u2217 is r\u2217min-stable. Furthermore, by Lemma 7, \u2203r\u2032min > 0 s.t. \u2200C\u2217, \u2206(C \u2032,m(A)) \u2265 r\u2032min\u03c6(m(A)). Let rmin := min{r\u2217min, r\u2032min} completes the proof.\nProof of Proposition 1. For all r \u2208 [k],\nn\u2217r\u2016cr \u2212 c\u2217r\u20162 \u2264 \u2206(C,C\u2217) \u2264 b\u03c6\u2217\nso \u2016cr \u2212 c\u2217r\u2016 \u2264 \u221a b\u03c6\u2217\nn\u2217r . Then for all r 6= s,\n\u2016cr \u2212 c\u2217r\u2016+ \u2016cs \u2212 c\u2217s\u2016 \u2264 \u221a b \u221a \u03c6\u2217(\n1\u221a n\u2217r + 1\u221a n\u2217s )\n=\n\u221a b f f \u221a \u03c6\u2217( 1\u221a n\u2217r + 1\u221a n\u2217s ) \u2264 \u221a b f \u2206rs \u2264 1 16 \u2206rs\nwhere the second inequality is by (B), and the last inequality by our assumption on b. Thus, we may apply Lemma 17 to get |Ar4A \u2217 r |\nn\u2217r \u2264 b f3 for all r, proving the first\nstatement. Now by Lemma 18, \u03c6(C) \u2264 (b+ 1)\u03c6\u2217, so\n\u03b1b 5\u03b1b+ 4(1 + \u03c6(C)\u03c6\u2217 ) \u2265 \u03b1b 5\u03b1b+ 4(2 + b)\n\u2265 \u03b1b 5\u03b1f2/162 + 4(2 + f2/162)\n\u2265 b f3(\u03b1)\n\u2265 |Ar4A \u2217 r |\nn\u2217r\nwhere the third inequality holds since f \u2265 max{642, 5\u03b1+5 162\u03b1 } by (B). This proves the second statement since C\u2217 is then ( f 2\n162 , \u03b1)-stable by Definition 5."}, {"heading": "8 Appendix B: Proof of Theorem 3", "text": "Theorem 3. Fix any 0 < \u03b4 \u2264 1e . Suppose C\n\u2217 is (bo, \u03b1)-stable. If we run Algorithm 1 with parameters satisfying\nm > ln(1\u2212\n\u221a \u03b1)\nln(1\u2212 45p \u2217 min)\nc\u2032 > \u03b2\n2[1\u2212 \u221a \u03b1\u2212 (1\u2212 45p \u2217 min) m] with \u03b2 \u2265 2\nto \u2265 768(c\u2032)2(1 + 1\nbo )2n2 ln2\n1\n\u03b4\nThen if at some iteration i, \u2206i \u2264 12bo\u03c6 \u2217, we have \u2200t > i,\nPr(\u2126t) \u2265 1\u2212 \u03b4 and\nEt[\u2206 t] \u2264 ( to + i+ 1\nto + t+ 1 )\u03b2\u2206i\n+ (c\u2032)2B\n\u03b2 \u2212 1 ( to + i+ 2 to + i+ 1 )\u03b2+1\n1\nto + t+ 1\nwhere B := 4(bo + 1)n\u03c6\u2217."}, {"heading": "8.1 Proofs leading to Theorem 3", "text": "In the subsequent analysis, we let\n\u03b2t := 2c\u2032min r ptr(m)(1\u2212\nmaxr p t r(m) mins pts(m) \u221a \u03b1)\nwhere\nptr(m) := Pr{ct\u22121r is updated at t with sample size m} = 1\u2212 (1\u2212 n t\u22121 r\nn )m\nSo, \u03b2t = 2c\u2032(min\nr ptr(m)\u2212\n\u221a \u03b1max\ns pts(m))\nThe noise terms appearing in our analysis are: E[ \u2211 r \u2211 x\u2208At+1r\n\u2016x\u2212 c\u0302t+1r \u20162 + \u03c6t|Ft] (9)\u2211 r\nn\u2217r\u3008ct\u22121r \u2212 c\u2217r , c\u0302tr \u2212 E[c\u0302tr|Ft\u22121]\u3009 (10)\u2211 r n\u2217r\u2016c\u0302tr \u2212 c\u2217r\u20162 (11)\nIn the analysis of this section, we use Et[\u00b7] as a shorthand notation for E[\u00b7|\u2126t], where \u2126t is as defined in the main paper. Let Ft denote the natural filtration of the stochastic process C0, C1, . . . , up to t.\nThe main idea of the proof is to show that with proper choice with the algorithm\u2019s parameters m, c\u2032, and to, the following holds at every step t:\n\u2022 \u03b2t \u2265 2 |\u2126t\n\u2022 Noise terms (10) and (11) are upper bounded by a function of \u03c6\u2217|\u2126t\n\u2022 Pr(\u2126t \\ \u2126t+1) is negligible |\u2126t, \u03b2t \u2265 2, bounded noise\n\u2022 Et[\u2206t|Ft\u22121] \u2264 (1\u2212 \u03b2 t to+t )\u2206t\u22121 + t |\u2126t\nwhere t, the noise term, decreases of order O( 1 t2 ).\nLemma 10. Suppose C\u2217 is (bo, \u03b1)-stable. If\nm > ln(1\u2212\n\u221a \u03b1)\nln(1\u2212 45p \u2217 min)\nand c\u2032 >\n\u03b2\n2[1\u2212 \u221a \u03b1\u2212 (1\u2212 45p \u2217 min) m]\nThen conditioning on \u2126t, we have \u03b2t \u2265 \u03b2.\nProof. Let\u2019s first consider ptr(1) = nt\u22121r n . Conditioning on \u2126t, using the fact that C \u2217 is (bo, \u03b1)-stable, we have\nnt\u22121r n \u2265 p\u2217min(1\u2212maxr |Atr4A\u2217r | n\u2217r )\n\u2265 p\u2217min(1\u2212 \u03b1bo\n5\u03b1bo + 4(1 + \u03c6t \u03c6\u2217 ) ) \u2265 4 5 p\u2217min\nAnd hence,\nmin r ptr(m) \u2265 1\u2212 (1\u2212\n4 5 p\u2217min) m\nNow,\n\u03b2t \u2265 2c\u2032(min r ptr(m)\u2212\n\u221a \u03b1)\n\u2265 2c\u2032(1\u2212 (1\u2212 4 5 p\u2217min) m \u2212 \u221a \u03b1) \u2265 \u03b2\nwhere the last inequality is by our requirement on c\u2032 and the fact that 1 \u2212 (1 \u2212 4 5p \u2217 min) m \u2212 \u221a \u03b1 > 0 by our requirement on m.\nLemma 11. Suppose C\u2217 is (bo, \u03b1)-stable. Then if we apply one step of Algorithm 1, with m, c\u2032 satisfying conditions in Lemma 10, then conditioning on \u2126i,\n\u2206i \u2264 \u2206i\u22121(1\u2212 \u03b2 to + i\n) + [ c\u2032 to + i ]2 \u2211 r n\u2217r\u2016c\u0302ir \u2212 c\u2217r\u20162\n+ 2c\u2032\nto + i \u2211 r n\u2217r\u3008ci\u22121r \u2212 c\u2217r , \u03beir\u3009\nwhere \u03beir := c\u0302ir \u2212 E[c\u0302ir|Fi\u22121]. Proof. Let \u2206ir := n\u2217r\u2016cir \u2212 c\u2217r\u20162, so \u2206i = \u2211 r \u2206 i r, and we use ptr as a shorthand for ptr(m). By the update rule of Algorithm 1,\n\u2206ir = n \u2217 r\u2016(1\u2212 \u03b7i)(ci\u22121r \u2212 c\u2217r) + \u03b7i(c\u0302ir \u2212 c\u2217r)\u20162\n\u2264 n\u2217r{(1\u2212 2\u03b7i)\u2016ci\u22121r \u2212 c\u2217r\u20162 + 2\u03b7i\u3008ci\u22121r \u2212 c\u2217r , c\u0302ir \u2212 c\u2217r\u3009 +(\u03b7i)2[\u2016ci\u22121r \u2212 c\u2217r\u20162 + \u2016c\u0302ir \u2212 c\u2217r\u20162]}\nLet \u03beir = c\u0302ir \u2212 E[c\u0302ir|Fi\u22121], where E[c\u0302ir|Fi\u22121] = (1\u2212 pir)ci\u22121r + pirm(Air). Since\n\u3008ci\u22121r \u2212 c\u2217r , c\u0302ir \u2212 c\u2217r\u3009 = \u3008ci\u22121r \u2212 c\u2217r , E[c\u0302ir|Fi\u22121] + \u03beir \u2212 c\u2217r\u3009 \u2264 (1\u2212 pir)\u2016ci\u22121r \u2212 c\u2217r\u20162\n+pir\u2016m(Air)\u2212 c\u2217r\u2016\u2016ci\u22121r \u2212 c\u2217r\u2016+ \u3008ci\u22121r \u2212 c\u2217r , \u03beir\u3009\nWe have\n\u2206ir \u2264 n\u2217r{\u22122\u03b7i[\u2016ci\u22121r \u2212 c\u2217r\u20162 \u2212 (1\u2212 pir)\u2016ci\u22121r \u2212 c\u2217r\u20162\n\u2212pir\u2016ci\u22121r \u2212 c\u2217r\u2016\u2016m(Air)\u2212 c\u2217r\u2016] + \u2016ci\u22121r \u2212 c\u2217r\u20162\n+2\u03b7i\u3008\u03beir, ci\u22121r \u2212 c\u2217r\u3009+ (\u03b7i)2[\u2016ci\u22121r \u2212 c\u2217r\u20162 + \u2016c\u0302ir \u2212 c\u2217r\u20162]}\n\u2264 n\u2217r{\u2212 2c\u2032\nto + i min r ptr\u2016ci\u22121r \u2212 c\u2217r\u20162\n+ 2c\u2032\nto + i max s pts\u2016ci\u22121r \u2212 c\u2217r\u2016\u2016m(Air)\u2212 c\u2217r\u2016\n+\u2016ci\u22121r \u2212 c\u2217r\u20162 + 2\u03b7i\u3008\u03beir, ci\u22121r \u2212 c\u2217r\u3009 +(\u03b7i)2[\u2016ci\u22121r \u2212 c\u2217r\u20162 + \u2016c\u0302ir \u2212 c\u2217r\u20162]}\nNote \u2211 r n\u2217r\u2016cir \u2212 c\u2217r\u2016\u2016m(Air)\u2212 c\u2217r\u2016\n\u2264 \u221a ( \u2211 r n\u2217r\u2016ci\u22121r \u2212 c\u2217r\u20162)( \u2211 r n\u2217r\u2016m(Air)\u2212 c\u2217r\u20162)\n= \u221a \u2206i\u22121\u2206(m(Ai), C\u2217) \u2264 \u221a \u03b1\u2206i\u22121\nwhere the first inequality is by Cauchy-Schwartz and the last inequality is by applying Lemma 1. Finally, summing over \u2206ir, we get\n\u2206i = \u2211 r \u2206ir \u2264 \u2206i\u22121[1\u2212 2c\u2032 to + i min r ptr(1\u2212 maxs p t s minr ptr \u221a \u03b1)]\n+[ c\u2032 (to + i) ]2 \u2211 r n\u2217r\u2016c\u0302ir \u2212 c\u2217r\u20162\n+ 2c\u2032\n(to + i)pir \u2211 r n\u2217r\u3008ci\u22121r \u2212 c\u2217r , \u03beir\u3009\n\u2264 \u2206i\u22121(1\u2212 \u03b2 to + i\n) + [ c\u2032 to + i ]2 \u2211 r n\u2217r\u2016c\u0302ir \u2212 c\u2217r\u20162\n+ 2c\u2032\nto + i \u2211 r n\u2217r\u3008ci\u22121r \u2212 c\u2217r , \u03beir\u3009\nThe second inequality is by \u03b2t \u2265 \u03b2, as proven in Lemma 10. Lemma 12. Suppose X satisfies (A1), Co \u2208 conv(X), and C\u2217 is (bo, \u03b1)-stable. If we run one step of Algorithm 1, with m, c\u2032 satisfying conditions in Lemma 10, then conditioning on \u2126i, we have, for any \u03bb > 0,\nEi{exp{\u03bb\u2206i}|Fi\u22121} \u2264 exp { \u03bb{(1\u2212 \u03b2\nt0 + i )\u2206i\u22121 +\n(c\u2032)2B (t0 + i)2 + \u03bb(c\u2032)2B2 2(t0 + i)2 } }\nProof. By Lemma 24, we have (10) and (11) are both upper bounded by B. By Lemma 11, we have\nEi{exp(\u03bb\u2206i)|Fi\u22121} \u2264 exp\u03bb[\u2206i\u22121(1\u2212 \u03b2\nto + i ) +\n(c\u2032)2B\n(to + i)2 ]\nEi{exp\u03bb 2c\u2032\nto + i \u2211 r n\u2217r\u3008ci\u22121r \u2212 c\u2217r , \u03beir\u3009|Fi\u22121}\nSince 2\u03bbc\u2032\ni+ t0 \u2211 r n\u2217r\u3008\u03beir, ci\u22121r \u2212 c\u2217r\u3009 \u2264 2\u03bbc\u2032 i+ t0 B\nand Ei{ 2\u03bbc \u2032\ni+t0\n\u2211 r n \u2217 r\u3008\u03beir, ci\u22121r \u2212 c\u2217r\u3009|Fi\u22121} = 0, by Hoeffding\u2019s lemma\nEi\n{ exp{ 2\u03bbc \u2032\ni+ t0 \u2211 r n\u2217r\u3008\u03beir, ci\u22121r \u2212 c\u2217r\u3009|Fi\u22121}\n}\n\u2264 exp{\u03bb 2(c\u2032)2B2\n2(i+ t0)2 }\nCombining this with the previous bound completes the proof.\nLemma 13 (adapted from [4]). For any \u03bb > 0, Ei{e\u03bb\u2206 i\u22121} \u2264 Ei\u22121{e\u03bb\u2206 i\u22121}\nProof. By our partitioning of the sample space, \u2126i\u22121 = \u2126i \u222a (\u2126i\u22121 \\ \u2126i), and for any \u03c9 \u2208 \u2126i and \u03c9\u2032 \u2208 \u2126i\u22121 \\ \u2126i, \u2206i\u22121(\u03c9) \u2264 bo\u03c6\u2217 < \u2206i\u22121(\u03c9\u2032). Taking expectation over \u2126i and \u2126i\u22121, we get Ei{e\u03bb\u2206 i\u22121} \u2264 Ei\u22121{e\u03bb\u2206 i\u22121}.\nProposition 2. Fix any 0 < \u03b4 \u2264 1e . Suppose C \u2217 is (bo, \u03b1)-stable. If \u2206o \u2264 12bo\u03c6 \u2217, and if\nm > ln(1\u2212\n\u221a \u03b1)\nln(1\u2212 45p \u2217 min)\nc\u2032 > \u03b2\n2[1\u2212 \u221a \u03b1\u2212 (1\u2212 45p \u2217 min) m] with \u03b2 \u2265 2\nto \u2265 768(c\u2032)2(1 + 1\nbo )2n2 ln2\n1\n\u03b4\nThen P (\u2126\u221e) \u2264 \u03b4\n(here we used \u22060 instead of \u2206i and treat the starting time, the i-th iteration in Theorem 3 as the zeroth iteration for cleaner presentation).\nProof. By Lemma 12, for any \u03bb > 0,\nEi{e\u03bb\u2206 i} \u2264 Ei{e\u03bb{(1\u2212 \u03b2 to+i )\u2206i\u22121} exp{ \u03bb(c \u2032)2B (to + i)2 + \u03bb2(c\u2032)2B2 2(to + i)2 }\n\u2264 Ei\u22121{e\u03bb (1)\u2206i\u22121} exp{ \u03bb(c\n\u2032)2B (to + i)2 + \u03bb2(c\u2032)2B2 2(to + i)2 }\nwhere \u03bb(1) = \u03bb(1\u2212 \u03b2to+i), and the second inequality is by Lemma 13. Similarly, the following recurrence relation holds for k = 0, . . . , i:\nEi\u2212k{e\u03bb (k)\u2206i\u2212k} \u2264 Ei\u2212(k+1){e\u03bb (k+1)\u2206i\u2212k\u22121}\nexp{ \u03bb (k)(c\u2032)2B\n(to + i\u2212 k)2 +\n(\u03bb(k))2(c\u2032)2B2 2(to + i\u2212 k)2 }\nwhere \u03bb(0) := \u03bb, and for k \u2265 1, \u03bb(k) := \u03a0kt=1(1\u2212 \u03b2 to+(i\u2212t+1))\u03bb (0).\nNote (see, e.g., [4]) \u2200\u03b2 > 0, k \u2265 1,\n\u03bb(k) = \u03a0kt=1(1\u2212 \u03b2 to + (i\u2212 t+ 1) ) \u2264 ( to + i\u2212 k + 1 to + i )\u03b2\nSince the bound is shrinking as \u03b2 increases and \u03b2 \u2265 2,\n\u03bb(k) (t0 + i\u2212 k)2 \u2264 ( to + i\u2212 k + 1 to + i )2\n\u03bb (to + i\u2212 k)2 \u2264 4\u03bb (to + i)2\nRepeatedly applying the relation, we get\nEi{e\u03bb\u2206 i} \u2264 e\u03bb(i)\u22060 exp{ i\u22121\u2211 k=0 ( 4\u03bb(c\u2032)2B (to + i)2 + 4\u03bb2(c\u2032)2B2 2(to + i)2 )}\n\u2264 exp{\u03bb( to + 1 to + i\n)\u03b2\u22060 + [\u03bb(c\u2032)2B + \u03bb2(c\u2032)2B2\n2 ]\n4i\n(to + i)2 }\n\u2264 exp{\u03bb( to + 1 to + i )\u03b2 1 2 bo\u03c6 \u2217 + [\u03bb(c\u2032)2B +\n\u03bb2(c\u2032)2B2\n2 ]\n4i\n(to + i)2 }\nThen we can apply the conditional Markov\u2019s inequality, for any \u03bbi > 0,\nPr(\u03c9 \u2208 \u2126i \\ \u2126i+1) = Pr(\u2206i > bo\u03c6\u2217|\u2126i)\n= Pr(e\u03bbi\u2206 i > e\u03bbibo\u03c6 \u2217 |\u2126i) \u2264 E[e\u03bbi\u2206 i r |\u2126i]\ne\u03bbibo\u03c6\u2217\nCombining this with the upper bound on Eie\u03bbi\u2206 i , we get\nPr(\u03c9 \u2208 \u2126i \\ \u2126i+1)\n\u2264 exp{\u2212\u03bbi{ 1\n2 bo[2\u2212 (\nto + 1 to + i )\u03b2]\n\u2212(B + \u03bbiB 2\n2 )\n4(c\u2032)2i\n(to + i)2 }} \u2264 exp { \u2212\u03bbi{ bo\u03c6 \u2217\n2 \u2212 (B + \u03bbiB\n2\n2 )\n4(c\u2032)2i (to + i)2 } }\nsince i \u2265 1. We choose \u03bbi = 1\u2206 ln (i+1)2 \u03b4 with \u2206 = bo\u03c6\u2217 4 , and show that bo\u03c6\u2217\n2 \u2212 (B + \u03bbiB 2\n2 ) 4(c\u2032)2i (to+i)2 is lower bounded by \u2206.\nCase 1: B > \u03bbiB22 . We get\n1 2 bo\u03c6 \u2217 \u2212 (B + \u03bbiB\n2\n2 )\n4(c\u2032)2i\n(to + i)2 \u2265 \u2206\nsince to \u2265 128(c \u2032)2(bo+1)n bo = 64(c \u2032)2(bo+1)n\u03c6\u2217\n1 2 bo\u03c6\u2217\n= 16(c \u2032)2B\n1 2 bo\u03c6\u2217\n.\nCase 2: B \u2264 \u03bbiB22 . We get\n1 2 bo\u03c6 \u2217 \u2212 (B + \u03bbiB\n2\n2 )\n4(c\u2032)2i\n(to + i)2\n\u2265 2\u2206\u2212 \u03bbiB2 4(c\u2032)2i\n(to + i)2\n= 2\u2206\u2212 1 \u2206 ln (1 + i)2 \u03b4 4(c\u2032)2B2i (to + i)2\n\u2265 2\u2206\u2212 1 \u2206 ln (to + i)\n2\n\u03b4\n4(c\u2032)2B2(to + i)\n(to + i)2\nNow we show 1\n\u2206 ln\n(to + i) 2\n\u03b4\n4(c\u2032)2B2\nto + i \u2264 \u2206\nSince\nto + i \u2265 to \u2265 768(c\u2032)2(1 + 1\nbo )2n2 ln2\n1\n\u03b4\n= 48(c\u2032)2B2\n(12bo\u03c6 \u2217)2\nln2 1\n\u03b4\nln 1\u03b4 \u2265 1, and 16(c\u2032)2B2\n( 1 2 bo\u03c6\u2217)2\n\u2265 13 , we can apply Lemma 25 with b = 2, C := 16(c\u2032)2B2\n( 1 2 bo\u03c6\u2217)2\n,\nt := to + i \u2265 ( 3Cb\u22121 ln 1 \u03b4 )\n2 b\u22121 , and get\n4(c\u2032)2B2\n\u22062 ln\n(to + i) 2\n\u03b4 := 2C ln t+ C ln\n1 \u03b4 < tb\u22121 = to + i\nThat is, 1\u2206 ln (to+i)2 \u03b4 4(c\u2032)2B2 to+i \u2264 \u2206. Thus, for both cases,\n2\u2206\u2212 (B + \u03bbiB 2\n2 )\n4(c\u2032)2i\n(to + i)2 \u2265= \u2206\nand hence,\nPr(\u03c9 \u2208 \u2126i \\ \u2126i+1) \u2264 e\u2212 1 \u2206\n(ln (1+i)2\n\u03b4 )\u2206 =\n\u03b4\n(i+ 1)2\nFinally, we have\nPr(\u222ai\u22651\u2126i \\ \u2126i+1) \u2264 \u221e\u2211 i=1 Pr(\u03c9 \u2208 \u2126i \\ \u2126i+1) \u2264 \u03b4\nProof of Theorem 3. Since the conditions in Proposition 2 holds for any t > i, we apply it and get\nPr(\u2126t) \u2265 1\u2212 Pr(\u222at>i\u2126t \\ \u2126t+1) \u2265 1\u2212 \u03b4\nThis proves the first statement. Taking expectation over \u2126t conditioning on filtration Ft\u22121 with respect to the inequality derived in Lemma 11, we get\nEt[\u2206 t|Ft\u22121] \u2264 \u2206t\u22121(1\u2212\n\u03b2\nto + t ) + [\nc\u2032\nto + t ]2B\nsince (11) is bounded by B by Lemma 24, and since Et{\u03betr|Ft\u22121} = 0, \u2200r \u2208 [k]. Taking total expectation over \u2126t, we get\nEt[\u2206 t] \u2264 Et[\u2206t\u22121](1\u2212\n\u03b2\nto + t ) +\n(c\u2032)2B\n(t+ to)2\n\u2264 Et\u22121[\u2206t\u22121](1\u2212 \u03b2\nto + t ) +\n(c\u2032)2B\n(t+ to)2\nWe can apply Lemma 26 by letting ut \u2190 Et+to [\u2206t+to ] (we temporarily change the notation Et[\u2206t] to Et+to [\u2206t+to ] to match the notation in Lemma 26), to \u2190 to + i, a\u2190 \u03b2, and b\u2190 (c\u2032)2B\nEt[\u2206 t] \u2264 ( to + i+ 1\nto + t+ 1 )\u03b2\u2206i +\n(c\u2032)2B\n\u03b2 \u2212 1 ( to + i+ 2 to + i+ 1 )\u03b2+1\n1\nto + t+ 1"}, {"heading": "9 Proofs of Theorem 1 and Theorem 2", "text": "One subtlety we need to point out before the proofs is that, in Algorithm 1, the learning rate \u03b7tr as well as the update rule:\nctr \u2190 (1\u2212 \u03b7tr)ct\u22121r + \u03b7tr c\u0302tr\nis only defined for a cluster r that is \u201csampled\u201d at the t-th iteration. However, even if the cluster is not \u201csampled\u201d, i.e., ctr = ct\u22121r , the same update rule with c\u0302tr = ct\u22121r and and the same learning rate still holds for this case. So in our analysis, we equivalently treat each cluster r as updated with learning rate \u03b7tr, and differentiates between a sampled and not-sampled cluster only through the definition of c\u0302tr.\nProof leading to Theorem 1 Lemma 14. Suppose \u2200r \u2208 [k], \u03b7tr \u2264 \u03b7tmax w.p. 1. Then, E[\u03c6t+1 \u2212 \u03c6t|Ft] \u2264 \u22122 minr,t;pt+1r >0 \u03b7 t+1 r p t+1 r (\u03c6 t \u2212 \u03c6\u0303t) + (\u03b7t+1max)26\u03c6t, where \u03c6\u0303t := \u2211 r \u2211 x\u2208At+1r \u2016x \u2212 m(At+1r )\u20162.\nProof of Lemma 14. For simplicity, we denote E[\u00b7|Ft] by Et[\u00b7] (the same notation is also used as a shorthand to E[\u00b7|\u2126t] in the proof of Theorem 3; we abuse the notation here).\nEt[\u03c6 t+1] = Et[ k\u2211 r=1 \u2211 x\u2208At+2r \u2016x\u2212 ct+1r \u20162]\n\u2264 Et[ \u2211 r \u2211 x\u2208At+1r \u2016x\u2212 ct+1r \u20162]\n= Et[ \u2211 r \u2211 x\u2208At+1r \u2016x\u2212 (1\u2212 \u03b7t+1r )ctr \u2212 \u03b7t+1r c\u0302t+1r \u20162]\n= Et[ \u2211 r \u2211 x\u2208At+1r (1\u2212 \u03b7t+1r )2\u2016x\u2212 ctr\u20162\n+(\u03b7t+1r ) 2\u2016x\u2212 c\u0302t+1r \u20162 + 2\u03b7t+1r (1\u2212 \u03b7t+1r )\u3008x\u2212 ctr, x\u2212 c\u0302t+1r \u3009]\nwhere the inequality is due to the optimality of clustering At+2 for centroids Ct+1. Since\nEt[c\u0302 t+1 r ] = (1\u2212 pt+1r )ctr + pt+1r m(At+1r )\nwe have\n\u3008x\u2212 ctr, x\u2212 c\u0302t+1r \u3009 = (1\u2212 pt+1r )\u2016x\u2212 ctr\u20162 + pt+1r \u3008x\u2212 ctr, x\u2212m(At+1r )\u3009\nPlug this into the previous inequality, we get\nEt[\u03c6 t+1] \u2264 \u2211 r (1\u2212 2\u03b7t+1r )\u03c6tr + (\u03b7t+1r )2\u03c6tr\n+(\u03b7t+1r ) 2 \u2211\nx\u2208At+1r\n\u2016x\u2212 c\u0302t+1r \u20162\n+2\u03b7t+1r {(1\u2212 pt+1r ) \u2211\nx\u2208At+1r\n\u2016x\u2212 ctr\u20162\n+pt+1r \u2211\nx\u2208At+1r\n\u3008x\u2212 ctr, x\u2212m(At+1r )\u3009}\n= \u03c6t \u2212 2 \u2211 r \u03b7t+1r p t+1 r \u03c6 t r\n+2 \u2211 r \u03b7t+1r p t+1 r \u2211 x\u2208At+1r \u3008x\u2212 ctr, x\u2212m(At+1r )\u3009}\n+(\u03b7t+1r ) 2\u03c6tr + (\u03b7 t+1 r )\n2 \u2211\nx\u2208At+1r\n\u2016x\u2212 c\u0302t+1r \u20162\nNow, \u2211 x\u2208At+1r \u3008x\u2212 ctr, x\u2212m(At+1r )\u3009\n= \u2211\nx\u2208At+1r\n\u3008x\u2212m(At+1r ) +m(At+1r )\u2212 ctr, x\u2212m(At+1r )\u3009\n= \u2211\nx\u2208At+1r\n\u2016x\u2212m(At+1r )\u20162\n+ \u2211\nx\u2208At+1r\n\u3008m(At+1r )\u2212 ctr, x\u2212m(At+1r )\u3009 = \u03c6tr\nsince \u2211\nx\u2208At+1r \u3008m(A t+1 r )\u2212ctr, x\u2212m(At+1r )\u3009 = 0, by property of the mean of a cluster.\nThen\nEt[\u03c6 t+1] \u2264 \u03c6t + \u2211 r 2\u03b7t+1r p t+1 r (\u2212\u03c6tr + \u03c6\u0303tr)\n+(\u03b7t+1r ) 2[\u03c6tr + Et[ \u2211 x\u2208At+1r \u2016x\u2212 c\u0302t+1r \u20162]\nNow a key observation is that pt+1r = 0 if and only if cluster At+1r is empty, i.e., degenerate. Since the degenerate clusters do not contribute to the k-means cost,\nwe have \u2211\nr;pt+1r >0 \u03c6tr = \u03c6\nt, and similarly, \u2211\nr;pt+1r >0 \u03c6\u0303tr = \u03c6\u0303 t. Therefore,\nEt[\u03c6 t+1] \u2264 \u03c6t \u2212 2 min r,t;pt+1r >0 \u03b7t+1r p t+1 r (\u03c6 t \u2212 \u03c6\u0303t)\n+(\u03b7t+1max) 2(Et \u2211 r \u2211 x\u2208At+1r \u2016x\u2212 c\u0302t+1r \u20162 + \u03c6t)\n= \u03c6t \u2212 2 min r,t;pt+1r >0 \u03b7t+1r p t+1 r (\u03c6 t \u2212 \u03c6\u0303t) + (\u03b7t+1max)26\u03c6t\nwhere the last inequality is by Lemma 23.\nLemma 15. Suppose Assumption (A) holds. If we run Algorithm 1 on X with \u03b7t = c \u2032\nto+t , and to > 1, with any initial set of k centroids C0 \u2208 conv(X). Then for\nany \u03b4 > 0, \u2203t s.t. \u2206(Ct, C\u2217) \u2264 \u03b4 with C\u2217 := m(A\u2217) for some A\u2217 \u2208 {A\u2217}[k].\nProof of Lemma 15. First note that since {C\u2217}[k] includes all stationary points with 1 \u2264 k\u2032 \u2264 k non-degenerate centroids, and at any time t, Ct must have kt \u2208 [k] non-degenerate centroids, so there exists C\u2217 \u2208 {C\u2217}kt \u2208 {C\u2217}[k] such that \u2206(Ct, C\u2217) is well defined. For a contradiction, suppose \u2200t \u2265 1, \u2206(Ct, C\u2217) > \u03b4, for all C\u2217 \u2208 {C\u2217}kt . Then\nCase 1: m(At+1) \u2208 {C\u2217}kt Then\n\u2206(Ct,m(At+1)) > \u03b4\nby our assumption.\nCase 2: m(At+1) /\u2208 {C\u2217}kt Since Ct \u2208 Cl(v\u22121(At+1)) by our definition, applying Lemma 3,\n\u2206(Ct,m(At+1)) \u2265 rmin\u03c6(m(At+1))\nSo for both cases, \u2206(Ct,m(At+1)) \u2265 min{\u03b4, rmin\u03c6opt}\nLet denote \u03b4o := min{\u03b4, rmin\u03c6(m(At+1))}, then by Lemma 14,\nE[\u03c6t+1 \u2212 \u03c6t|Ft]\n\u2264 \u2212 2c\u2032minr\u2208[k];pt+1r (m)>0 p t+1 r (m)\nt+ 1 + to \u03c6t(1\u2212 \u03c6\u0303\nt\n\u03c6t )\n+( c\u2032\nt+ 1 + to )26\u03c6max\nNote for pt+1r (m) > 0, by the discrete nature of the dataset, nt+1r n \u2265 1 n , therefore,\nmin r\u2208[k];ptr(m)>0\nptr(m) \u2265 1\u2212 (1\u2212 1 n )m \u2265 1\u2212 e\u2212 m n\nAlso note\n\u03c6t \u2212 \u03c6\u0303t = \u2211 r\u2208[k\u2032] \u2211 x\u2208At+1r \u2016x\u2212 Ct\u20162 \u2212 \u2016x\u2212m(At+1r )\u20162\n= \u2211 r \u2016ctr \u2212m(At+1r )\u20162nt+1r = \u2206(Ct,m(At+1)) \u2265 \u03b4o\nThen \u2200t \u2265 1,\nE[\u03c6t+1]\u2212 E[\u03c6t]\n\u2264 \u22122c \u2032(1\u2212 e\u2212 m n )\nt+ 1 + to \u03b4o +\n6\u03c6max(c \u2032)2\n(t+ 1 + to)2\nSumming up all inequalities,\nE[\u03c6t+1]\u2212 E[\u03c60]\n\u2264 \u22122c\u2032(1\u2212 e\u2212 m n )\u03b4o ln\nt+ to + 1\nto +\n6\u03c6max(c \u2032)2\nto \u2212 1\nSince t is unbounded and ln t+to+1to increases with t while 6\u03c6max(c\u2032)2\nto\u22121 is a constant, \u2203T such that for all t \u2265 T , E\u03c6t \u2212 \u03c60 \u2264 \u2212\u03c60, which means E[\u03c6t] \u2264 0, for all t large enough. This implies the k-means cost of some clusterings is negative, which is impossible. So we have a contradiction.\nProof setup of Theorem 1 The goal of the proof is to show that first, with high probability, the algorithm converges to some stationary clustering, A\u2217 \u2208 {A\u2217}[k]. We call this event G; formally,"}, {"heading": "G := {\u2203T \u2265 1,\u2203A\u2217 \u2208 {A\u2217}[k], s.t. At = A\u2217,\u2200t \u2265 T}", "text": "Second, we want to establish the O(1t ) expected convergence rate of the algorithm to this stationary clustering A\u2217.\nTo prove that the event G has high probability, we first consider random variable \u03c4 :\n\u03c4 := min{t \u2265 0 | min A\u2217\u2208{A\u2217}[k] \u2206(Ct,m(A\u2217)) \u2264 1 2 rmin\u03c6 \u2217}\nThat is, \u03c4 is the first time the algorithm \u201chits\u201d a stationary clustering; \u03c4 is a stopping time since \u2200t \u2265 0, {\u03c4 \u2264 t} is Ft-measurable. By Lemma 15\nPr({\u03c4 <\u221e}) = Pr({\u03c4 \u2208 N}) = Pr(\u222aT\u22650{\u03c4 = T}) = 1 (12)\nFixing \u03c4 , we denote the stationary clustering that the algorithm \u201chits\u201d by\nA\u2217(\u03c4) := arg min A\u2217\u2208{A\u2217}[k] \u2206(C\u03c4 ,m(A\u2217))\nA\u2217(\u03c4) is well defined; the reason is that when \u2206(C\u03c4 ,m(A\u2217)) \u2264 12rmin\u03c6 \u2217, A\u03c4 = A\u2217, so there can be only one minimizer. We will prove a subset Go \u2282 G holds with high probability. To do this, we construct Go as a union of disjoint events determined by the realization of \u03c4 and A\u2217(\u03c4): we define events\nGT (A \u2217) := {\u03c4 = T} \u2229 {A\u2217(\u03c4) = A\u2217} \u2229 {\u2200t \u2265 T,\u2206t \u2264 rmin\u03c6\u2217}\nThen we can represent the event where the algorithm\u2019s iterate converges to a particular stationary clustering A\u2217 as\nG(A\u2217) := \u222aT\u22650GT (A\u2217)\nFinally, we define Go := \u222aA\u2217\u2208{A\u2217}[k]G(A \u2217)\nGo \u2282 G since the event \u2206t \u2264 rmin\u03c6\u2217 implies At = A\u2217.\nProof of Theorem 1. Fix any (T,A\u2217), conditioning on {\u03c4 = T} \u2229 {A\u2217(\u03c4) = A\u2217}, since we have\nc\u2032 > \u03c6max\n(1\u2212 e\u2212 m n )rmin\u03c6opt\nWe can envoke Lemma 16 to get \u2200t < T ,\nE{\u03c6t \u2212 \u03c6(A\u2217)|GT (A\u2217)} = O( 1\nt ) (13)\nNow let\u2019s consider the case t \u2265 T . Since by Lemma 3, A\u2217 is (rmin, 0)-stable, we can apply Theorem 3: in this context, the parameters in the statement of Theorem 3 are bo = rmin, \u03b1 = 0, p\u2217min \u2265 1n . Thus, for any\nm \u2265 1\nc\u2032 > \u03b2\n2(1\u2212 e 4m 5n )\nwith \u03b2 \u2265 2\nand to \u2265 768(c\u2032)2(1 + 1\nrmin )2n2 ln2\n1\n\u03b4\nthe conditions required by Theorem 3 are satisfied. Then by the first statement of Theorem 3,\nPr({\u2200t \u2265 T,\u2206t \u2264 rmin\u03c6\u2217}|{\u03c4 = T} \u2229 {A\u2217(\u03c4) = A\u2217}) = P (\u2126\u221e|{\u03c4 = T} \u2229 {A\u2217(\u03c4) = A\u2217}) \u2265 1\u2212 \u03b4 (14)\nand by the second statement of Theorem 3, \u2200t > T ,\nE{\u03c6t \u2212 \u03c6(A\u2217)|\u2126t, {\u03c4 = T} \u2229 {A\u2217(\u03c4) = A\u2217}}\n\u2264 E{\u2206(Ct, C\u2217)|\u2126t, {\u03c4 = T} \u2229 {A\u2217(\u03c4) = A\u2217}} = O( 1\nt )\nwhere the first inequality is by Lemma 18. Since \u2126\u221e \u2282 \u2126t, \u2200t \u2265 0, this implies\nE{\u2206(Ct, C\u2217)|\u2126\u221e, {\u03c4 = T} \u2229 {A\u2217(\u03c4) = A\u2217}}\n= E{\u2206(Ct, C\u2217)|GT (A\u2217)} = O( 1\nt ) (15)\nFinally, we turn to prove Pr(G) is large. Recall\nPr{G} \u2265 Pr{Go} = Pr{\u222aT\u22650 \u222aA\u2217\u2208{A\u2217}[k] GT (A \u2217)} = \u2211\nT\u22650,A\u2217\u2208{A\u2217}[k]\nPr{GT (A\u2217)}\nwhere the second equality holds because the events GT (A\u2217) are disjoint for different pairs of (T,A\u2217), since the stopping time \u03c4 and the minimizer A\u2217(\u03c4) are unique for each experiment. Since \u2211\nT\u22650,A\u2217\u2208{A\u2217}[k]\nPr{GT (A\u2217)}\n= \u2211 T,A\u2217 Pr{\u2126\u221e|{\u03c4 = T} \u2229 {A\u2217(\u03c4) = A\u2217}}\nPr({\u03c4 = T} \u2229 {A\u2217(\u03c4) = A\u2217}) \u2265 (1\u2212 \u03b4) \u2211 T,A\u2217 Pr({\u03c4 = T} \u2229 {A\u2217(\u03c4) = A\u2217}) = (1\u2212 \u03b4)Pr{\u222aT \u222aA\u2217 {\u03c4 = T} \u2229 {A\u2217(\u03c4) = A\u2217}} = (1\u2212 \u03b4)Pr{\u222aT\u22650{\u03c4 = T}} = 1\u2212 \u03b4\nwhere the inequality is by (14), and the last two equalities are due to the finiteness of {A\u2217}[k] and by (12), respectively. Therefore, Pr{G} \u2265 1\u2212 \u03b4, which completes the proof of the first statement. In addition,\nPr{\u222aA\u2217\u2208{A\u2217}[k]G(A \u2217)}\n= Pr{\u222aT\u22650,A\u2217\u2208{A\u2217}[k]\u2126\u221e \u2229 {\u03c4 = T} \u2229 {A \u2217(\u03c4) = A\u2217}}\n\u2265 1\u2212 \u03b4\nwhich proves the second statement. Finally, combining inequalities (13) and (15), we have \u2200 \u2265 1 and \u2200t \u2265 1,\nE{\u03c6t \u2212 \u03c6(A\u2217)|GT (A\u2217)} = O( 1\nt )\nSince the quantity \u03c6t \u2212 \u03c6(A\u2217\u2217) is independent of T , we reach the conclusion\nE{\u03c6t \u2212 \u03c6(A\u2217)|G(A\u2217)} = O(1 t )\nLemma 16. Suppose the assumptions and settings in Theorem 1 hold, conditioning on any GT (A\u2217), we have \u22001 \u2264 t < T ,\nE{\u03c6t \u2212 \u03c6(A\u2217)|GT (A\u2217)} = O( 1\nt )\nProof. First observe that conditioning on the event GT (A\u2217), \u2206(Ct, C\u2217) > 12rmin\u03c6 \u2217, \u2200t < T . Now we are in a setup similar to that in the proof Lemma 15, and the argument therein will lead us to the conclusion that\n\u03c6t \u2212 \u03c6\u0303t > min{1 2 rmin, rmin}\u03c6\u0303t = 1 2 rmin\u03c6\u0303 t\nProceeding as in Lemma 15, we have conditioning on GT (A\u2217),\nE[\u03c6t|GT (A\u2217)]\n\u2264 E[\u03c6t\u22121|GT (A\u2217)]{1\u2212 2c\u2032minr\u2208[k];ptr(m)>0 p t r(m)\nt+ to\nrmin\u03c6opt 2\u03c6max\n}+ ( c \u2032\nt+ to )26\u03c6max\nsince \u2200t \u2265 1,\n1\u2212 \u03c6\u0303 t \u03c6t \u2265 rmin 2 \u03c6\u0303t \u03c6t \u2265 rmin 2 \u03c6opt \u03c6max\nNow, since we set\nc\u2032 > \u03c6max\n(1\u2212 e\u2212 m n )rmin\u03c6opt\nwe have\n2c\u2032 min r\u2208[k];ptr(m)>0 ptr(m) rmin\u03c6opt 2\u03c6max\n\u2265 2c\u2032(1\u2212 (1\u2212 1 n )m) rmin\u03c6opt 2\u03c6max\n\u2265 2c\u2032(1\u2212 e\u2212 m n ) rmin\u03c6opt 2\u03c6max\n> 2 \u03c6max\n(1\u2212 e\u2212 m n )rmin\u03c6opt (1\u2212 e\u2212 m n ) rmin\u03c6opt 2\u03c6max > 1\nApplying Lemma 26 with\na := 2c\u2032 min r\u2208[k];ptr(m)>0 ptr(m) rmin\u03c6opt 2\u03c6max > 1\nb := 6(c\u2032)2\u03c6max (to + t)2\nWe conclude that \u22001 \u2264 t < T ,\nE[\u03c6t|GT (A\u2217)] \u2264 to + 1\nto + t+ 1 \u03c6o +\nb a\u2212 1 ( to + 2 to + 1 )a+1\n1\nto + t+ 1\nSubtracting \u03c6(A\u2217) from both sides of the equation, we get\nE[\u03c6t \u2212 \u03c6(A\u2217)|GT (A\u2217)] \u2264 to + 1 to + t+ 1 (\u03c6o \u2212 \u03c6(A\u2217))\n+ b a\u2212 1 ( to + 2 to + 1 )a+1\n1\nto + t+ 1 = O(\n1 t )\nProofs leading to Theorem 2 Here, we additionally define two quantities that characterizes C\u2217: Let A\u2217 = v(C\u2217), we use p\u2217min := minr\u2208[k] n\u2217r n to characterize the fraction of the smallest cluster in A\u2217 to the entire dataset. We use wr := \u03c6r\u2217 n\u2217r\nmaxx\u2208A\u2217r \u2016x\u2212c \u2217 r\u20162 to characterize the ratio between average and maximal \u201cspread\u201d of cluster A\u2217r , and we let wmin := minr\u2208[k]wr."}, {"heading": "9.1 Existence of stable stationary point under geometric assumptions on the dataset", "text": "First, we observe that our Assumption (B) implies two lower bounds on \u2016c\u2217r \u2212 c\u2217s\u2016, \u2200r, s 6= r. Let x \u2208 A\u2217r \u2229Ats. Split x into its projection on the line joining c\u2217r and c\u2217s, and its orthogonal component:\nx = 1\n2 (c\u2217r + c \u2217 s) + \u03bb(c \u2217 r \u2212 c\u2217s) + u (16)\nwith u \u22a5 c\u2217r \u2212 c\u2217s. Note \u03bb measures the ratio between departure of the projected point from the mid-point of c\u2217r and c\u2217s and the norm \u2016c\u2217r \u2212 c\u2217s\u2016. By minimality of our definition of margin \u2206rs,\n\u2016x\u0304\u2212 1 2 (c\u2217r + c \u2217 s)\u2016 = \u03bb\u2016c\u2217r \u2212 c\u2217s\u2016 \u2265 1 2 \u2206rs (17)\nIn addition, since c\u2217r is the mean of A\u2217r, we know there exists x \u2208 A\u2217r such that x\u0304 falls outside of the line segment c\u2217r \u2212 c\u2217s (or exactly on c\u2217r in the special case where all points projects on c\u2217r). Similar holds for c\u2217s. Thus,\n\u2016c\u2217r \u2212 c\u2217s\u2016 \u2265 \u2206rs \u2265 f(\u03b1) \u221a \u03c6\u2217(\n1\u221a n\u2217r + 1\u221a n\u2217s ) (18)\nLemma 17 (Theorem 5.4 of [14]). Suppose (X,C\u2217) satisfies (B). If \u2200r \u2208 [k], s 6= r, \u2206tr + \u2206 t s \u2264 \u2206rs16 . Then for any s 6= r, |A \u2217 r \u2229Ats| \u2264 b 2 f(\u03b1) , where b \u2265 maxr,s \u2206tr+\u2206 t s \u2206rs .\nThe proof is almost verbatim of Theorem 5.4 of [14]; we include it here for completeness.\nProof. Since the projection of x on the line joining ctr, cts is closer to s, we have\nx(cts \u2212 ctr) \u2265 1\n2 (cts \u2212 ctr)(cts + ctr)\nSubstituting (16) into the inequality above,\n1 2 (c\u2217r + c \u2217 s)(c t s \u2212 ctr) + \u03bb(c\u2217r \u2212 c\u2217s)(cts \u2212 ctr)\n+u(cts \u2212 ctr) \u2265 1\n2 (cts \u2212 ctr)(cts + ctr) (19)\nSince u \u22a5 c\u2217r \u2212 c\u2217s, let \u2206 = \u2206ts + \u2206tr. We have\nu(cts \u2212 ctr) = u(cts \u2212 c\u2217s \u2212 (ctr \u2212 c\u2217r)) \u2264 \u2016u\u2016\u2206\nRearranging (19), we have\n1 2 (c\u2217r + c \u2217 s \u2212 cts \u2212 ctr)(cts \u2212 ctr)\n+\u03bb(c\u2217r \u2212 c\u2217s)(cts \u2212 ctr) + u(cts \u2212 ctr) \u2265 0\n\u2261 \u2206 2\n2 +\n\u2206 2 \u2016c\u2217r \u2212 c\u2217s\u2016 \u2212 \u03bb\u2016c\u2217r \u2212 c\u2217s\u20162 +\u03bb\u2206\u2016c\u2217r \u2212 c\u2217s\u2016+ \u2016u\u2016\u2206 \u2265 0\nTherefore,\n\u2016x\u2212 c\u2217r\u2016 = \u2016( 1 2 \u2212 \u03bb)(c\u2217s \u2212 c\u2217r) + u\u2016 \u2265 \u2016u\u2016\n\u2265 \u03bb \u2206 \u2016c\u2217r \u2212 c\u2217s\u20162 \u2212 \u2206 2\n\u22121 2 \u2016c\u2217r \u2212 c\u2217s\u2016 \u2212 \u03bb\u2016c\u2217r \u2212 c\u2217s\u2016 \u2265 \u2206rs\u2016c\u2217r \u2212 c\u2217s\u2016 64\u2206\nwhere the last inequality is by our assumption that \u2206 \u2264 \u2206rs16 , and \u03bb \u2265 \u2206rs 2\u2016c\u2217r\u2212c\u2217s\u2016 by (17). By previous inequality and our assumption on f , 3 for all s 6= r\n|A\u2217r \u2229Ats| \u22062rs\u2016c\u2217r \u2212 c\u2217s\u20162\nf\u22062 \u2264 \u2211 x\u2208A\u2217r\u2229Ats \u2016x\u2212 c\u2217r\u20162\nSo |A\u2217r \u2229 Ats| \u2264 \u2211 x\u2208A\u2217r\u2229Ats \u2016x \u2212 c \u2217 r\u20162 f(\u2206tr+\u2206 t s) 2 \u22062rs\u2016c\u2217r\u2212c\u2217s\u20162 \u2264 fb 2\nf2\u03c6\u2217( 1 n\u2217r\n) ( \u2211 A\u2217r\u2229Ats \u2016x \u2212 c \u2217 r\u20162),\nwhere the second inequality is by (18). That is, |A \u2217 r\u2229Ats| n\u2217r \u2264 b2f\u03c6\u2217 \u2211 A\u2217r\u2229Ats \u2016x\u2212 c \u2217 r\u20162. Similarly, for all s 6= r, |A \u2217 s\u2229Atr| n\u2217r \u2264 b2f\u03c6\u2217 \u2211 A\u2217s\u2229Atr \u2016x \u2212 c \u2217 s\u20162 Summing over all s 6= r, |Ar4A\u2217r | n\u2217r = \u03c1out + \u03c1in \u2264 b 2 f\u03c6\u2217\u03c6 \u2217 = b 2 f .\nLemma 18. Fix a stationary point C\u2217 with k centroids, and any other set of k\u2032-centroids, C, with k\u2032 \u2265 k so that C has exactly k non-degenerate centroids. We have\n\u03c6(C)\u2212 \u03c6\u2217 \u2264 min \u03c0 \u2211 r n\u2217r\u2016c\u03c0(r) \u2212 c\u2217r\u20162 = \u2206(C,C\u2217)\nProof. Since degenerate centroids do not contribute to k-means cost, in the following we only consider the sets of non-degenerate centroids {cs, s \u2208 [k]} \u2282 C and {c\u2217r , r \u2208\n3We use f as a shorthand for f(\u03b1) in the subsequent proof.\n[k]} \u2282 C\u2217. We have for any permutation \u03c0,\n\u03c6(C)\u2212 \u03c6\u2217 = \u2211 s \u2211 x\u2208As \u2016x\u2212 cs\u20162 \u2212 \u2211 r \u2211 x\u2208A\u2217r \u2016x\u2212 c\u2217r\u20162\n\u2264 \u2211 r \u2211 x\u2208A\u2217r \u2016x\u2212 c\u03c0(r)\u20162 \u2212 \u2211 r \u2211 x\u2208A\u2217r \u2016x\u2212 c\u2217r\u20162\n= \u2211 r n\u2217r\u2016c\u03c0(r) \u2212 c\u2217r\u20162\nwhere the last inequality is by optimality of clustering assignment based on Voronoi diagram, and the second inequality is by applying the centroidal property in Lemma 21 to each centroid in C\u2217. Since the inequality holds for any \u03c0, it must holds for min\u03c0 \u2211 r n \u2217 r\u2016c\u03c0(r) \u2212 c\u2217r\u20162, which completes the proof.\nProofs regarding seeding guarantee\nLemma 19 (Theorem 4 of [21]). Suppose (X,C\u2217) satisfies (B). If we obtain seeds from Algorithm 2, then\n\u2206(C0, C\u2217) \u2264 1 2\nf(\u03b1)2\n162 \u03c6\u2217\nwith probability at least 1\u2212mo exp(\u22122(f(\u03b1)4 \u2212 1) 2w2min)\u2212 k exp(\u2212mop\u2217min).\nProof. First recall that, as in (18), assumption (B) implies center-separability assumption in Definition 1 of [21], i.e.\n\u2200r \u2208 [k], s 6= r, \u2016c\u2217r \u2212 c\u2217s\u2016 \u2265 f(\u03b1) \u221a \u03c6\u2217(\n1\u221a n\u2217r + 1\u221a n\u2217s )\nwith f(\u03b1) \u2265 maxr\u2208[k],s 6=r n\u2217r n\u2217s . 4 Applying Theorem 4 of [21] with \u00b5r = c\u2217r and \u03bdr = c 0 r, we get \u2200r \u2208 [k], \u2016c0r \u2212 c\u2217r\u2016 \u2264 \u221a f(\u03b1)\n2 \u221a \u03c6\u2217r n\u2217r with probability at least 1 \u2212\nmo exp(\u22122(f(\u03b1)4 \u2212 1) 2w2min) \u2212 k exp(\u2212mop\u2217min). Summing over all r, the previous event implies \u2211\nr n \u2217 r\u2016c0r \u2212 c\u2217r\u20162 \u2264 f(\u03b1) 4 \u03c6 \u2217 \u2264 12 f(\u03b1)2 162 \u03c6\u2217, where the last inequality is by\nthe assumption that f \u2265 642 in (B).\nLemma 20. Assume the conditions Lemma 19 hold. For any \u03be > 0, if in addition,\nf(\u03b1) \u2265 5\n\u221a 1\n2wmin ln(\n2\n\u03bep\u2217min ln\n2k\n\u03be )\n4note: \u201c\u03b1\u201d in [21] is defined as minr\u2208[k],s6=r n\u2217r n\u2217s ,which is not to be confused with our \u201c\u03b1\u201d.\nIf we obtain seeds from Algorithm 2 choosing\nln 2k\u03be p\u2217min < mo < \u03be 2 exp{2(f(\u03b1) 4 \u2212 1)2w2min}\nThen \u2206(C0, C\u2217) \u2264 12 f(\u03b1)2 162 \u03c6\u2217 with probability at least 1\u2212 \u03be.\nProof. By Lemma 19, a sufficient condition for the success probability to be at least 1\u2212 \u03be is:\nmo exp(\u22122( f(\u03b1)\n4 \u2212 1)2w2min) \u2264\n\u03be\n2 and\nk exp(\u2212mop\u2217min) \u2264 \u03be\n2 This translates to requiring\n1\np\u2217min ln\n2k\n\u03be \u2264 mo \u2264\n\u03be 2 exp(2( f(\u03b1) 4 \u2212 1)2w2min)\nNote for this inequality to be possible, we also need 1p\u2217min ln 2k \u03be \u2264 \u03be 2 exp(2( f(\u03b1) 4 \u2212 1)2w2min), imposing a constraint on f(\u03b1). Taking logarithm on both sides and rearrange, we get\n( f(\u03b1) 4 \u2212 1)2 \u2265 1 2wmin ln( 2 \u03bep\u2217min ln 2k \u03be )\nThis satisfied since f(\u03b1) \u2265 5 \u221a\n1 2wmin ln( 2\u03bep\u2217min ln 2k\u03be ).\nProof of Theorem 2. By Proposition 1, (X,C\u2217) satisfying (B) implies C\u2217 is (f(\u03b1) 2\n162 , \u03b1)-stable. Let b0 :=\nf(\u03b1)2\n162 , and we denote event F := {\u2206(C0, Copt) \u2264 12b0\u03c6 \u2217}. Since f(\u03b1) \u2265 5 \u221a\n1 2wmin ln( 2\u03bep\u2217min ln 2k\u03be ), and\nlog 2k \u03be\np\u2217min < mo <\n\u03be 2 exp{2( f(\u03b1) 4 \u2212 1) 2w2min}, we can apply Lemma 20 to get\nPr{F} \u2265 1\u2212 \u03be\nConditioning on F , we can invoke Theorem 3, since (A1) is satisfied implicitly by (B), Co \u2282 conv(X) by the sampling method used in Algorithm 2, and we can guarantee that the setting of our parameters, m, c\u2032, and to, satisfies the condition required in Theorem 3. Let \u2126t be as defined in the main paper, by Theorem 3, \u2200t \u2265 1,\nE{\u2206t|\u2126t, F} = O( 1\nt ) and Pr{\u2126t|F} \u2265 1\u2212 \u03b4\nSo Pr{\u2126t \u2229 F} = Pr{\u2126t|F}Pr{F} \u2265 (1\u2212 \u03b4)(1\u2212 \u03be)\nFinally, using Lemma 18, and letting Gt := \u2126t \u2229 F , we get the desired result."}, {"heading": "10 Appendix D: auxiliary lemmas", "text": "Equivalence of Algorithm 1 to stochastic k-means Here, we formally show that Algorithm 1 with specific instantiation of sample size m and learning rates \u03b7tr is equivalent to online k-means [6] and mini-batch k-means [20].\nClaim 1. In Algorithm 1, if we set a counter for N\u0302 tr := \u2211t i=1 n\u0302 i r and if we set the learning rate \u03b7tr := n\u0302tr N\u0302tr , then provided the same random sampling scheme is used,\n1. When mini-batch size m = 1, the update of Algorithm 1 is equivalent to that described in [Section 3.3, [6]].\n2. When m > 1, the update of Algorithm 1 is equivalent to that described from line 3 to line 14 in [Algorithm 1, [20]] with mini-batch size m.\nProof. For the first claim, we first re-define the variables used in [Section 3.3, [6]]. We substitute index k in [6] with r used in Algorithm 1. For any iteration t, we define the equivalence of definitions: s \u2190 xi, ctr \u2190 wk, n\u0302tr \u2190 \u2206nk, N\u0302 tr \u2190 nk. According to the update rule in [6], \u2206nk = 1 if the sampled point xi is assigned to cluster with center wk. Therefore, the update of the k-th centroid according to online k-means in [6] is:\nwk \u2190 wk + 1\nnk (xi \u2212 wk)1{\u2206nk=1}\nUsing the re-defined variables, at iteration t, this is equivalent to\nctr = c t\u22121 r +\n1\nN\u0302 tr (s\u2212 ct\u22121r )1{n\u0302tr=1}\nNow the update defined by Algorithm 1 with m = 1 and \u03b7tr = n\u0302tr N\u0302tr is:\nctr = c t\u22121 r + \u03b7 t r(c\u0302 t r \u2212 ct\u22121r )1{n\u0302tr 6=0}\n= ct\u22121r + n\u0302tr\nN\u0302 tr (s\u2212 ct\u22121r )1{n\u0302tr=1}\n= ct\u22121r + 1 N\u0302 tr (s\u2212 ct\u22121r )1{n\u0302tr=1}\nsince n\u0302tr can only take value from {0, 1}. This completes the first claim. For the second claim, consider line 4 to line 14 in [Algorithm 1, [20]]. We substitute their index of time i with t in Algorithm 1. We define the equivalence of definitions: m\u2190 b, St \u2190M , s\u2190 x, ct\u22121I(s) \u2190 d[x], c t\u22121 r \u2190 c.\nAt iteration t, we let v[ct\u22121r ]t denote the value of counter v[c] upon completion of the loop from line 9 to line 14 for each center c, then N\u0302 tr \u2190 v[ct\u22121r ]t. Since according to Lemma 22, from line 9 to line 14, the updated centroid ctr after iteration t is\nctr = 1\nv[ct\u22121r ]t \u2211 s\u2208\u222ati=1Sir s = 1 N\u0302 tr \u2211 s\u2208\u222ati=1Sir s\nThis implies\nctr \u2212 ct\u22121r = 1\nN\u0302 tr \u2211 s\u2208\u222ati=1Sir s\u2212 ct\u22121r\n= 1 N\u0302 tr [ \u2211 s\u2208Str s+ \u2211\ns\u2032\u2208\u222at\u22121i=1Sir\ns\u2032]\u2212 ct\u22121r\n= 1 N\u0302 tr [ \u2211 s\u2208Str s+ N\u0302 t\u22121r c t\u22121 r ]\u2212 ct\u22121r\n= \u2212 n\u0302 t r\nN\u0302 tr ct\u22121r +\nn\u0302tr N\u0302 tr\n\u2211 s\u2208Str s\nn\u0302tr = \u2212\u03b7trct\u22121r + \u03b7tr c\u0302tr\nHence, the updates in Algorithm 1 and line 4 to line 14 in [Algorithm 1, [20]] are equivalent.\nLemma 21 (Centroidal property, Lemma 2.1 of [13]). For any point set Y and any point c in Rd,\u2211\nx\u2208Y \u2016x\u2212 c\u20162 = \u2211 x\u2208Y \u2016x\u2212m(Y )\u20162 + |Y |\u2016m(Y )\u2212 c\u20162\nLemma 22. Let wt, gt denote vectors of dimension Rd at time t. If we choose w0 arbitrarily, and for t = 1 . . . T , we repeatdly apply the following update\nwt = (1\u2212 1\nt )wt\u22121 +\n1 t gt\nThen\nwT = 1\nT T\u2211 t=1 gt\nProof. We prove by induction on T . For T = 1, w1 = (1\u2212 1)w0 + g1 = 11 \u22111\nt=1 gt. So the claim holds for T = 1.\nSuppose the claim holds for T , then for T + 1, by the update rule\nwT+1 = (1\u2212 1\nT + 1 )wT +\n1\nT + 1 gT+1\n= (1\u2212 1 T + 1 ) 1 T T\u2211 t=1 gt + 1 T + 1 gT+1\n= T\nT + 1\n1\nT T\u2211 t=1 gt + 1 T + 1 gT+1\n= 1\nT + 1 T+1\u2211 t=1 gt\nSo the claim holds for any T \u2265 1.\nLemma 23. \u2200t \u2265 1, conditioning on Ft, the noise term (9) is upper bounded by B1 := 5\u03c6 t.\nProof. Since \u2016x\u2212 c\u0302t+1r \u20162 \u2264 2\u2016x\u2212 ctr\u20162 + 2\u2016ctr \u2212 c\u0302t+1r \u20162\nWe have\nE[ \u2211 r \u2211 x\u2208At+1r \u2016x\u2212 c\u0302t+1r \u20162 + \u03c6t|Ft]\n\u2264 2 \u2211 r \u2211 x\u2208At+1r \u2016x\u2212 ctr\u20162\n+2 \u2211 r \u2211 x\u2208At+1r E[\u2016ctr \u2212 c\u0302t+1r \u20162|Ft] + \u03c6t\nNow,\nE[\u2016ctr \u2212 c\u0302t+1r \u20162|Ft] \u2264 E \u2211 s\u2208Str \u2016c t r \u2212 s\u20162\n|Str| = \u03c6tr ntr\nwhere Str is the sampled from Atr in Algorithm 1, and the inequality is by convexity of l2-norm. Substituting this into the previous inequality completes the proof.\nLemma 24. Suppose C\u2217 is (bo, \u03b1)-stable. Conditioning on \u2126i, we have, The terms (10), and (11), for t = i, are upper bounded by B := 4(bo + 1)n\u03c6\u2217.\nProof. Conditioning on \u2126i, \u2206i\u22121 \u2264 bo\u03c6\u2217\nBy Lemma 18, we also have\n\u03c6i\u22121 \u2212 \u03c6\u2217 \u2264 \u2206i\u22121 \u2264 bo\u03c6\u2217\nBy Cauchy-Schwarz, \u2211 r n\u2217r\u3008ci\u22121r \u2212 c\u2217r , c\u0302ir \u2212 ci\u22121r \u3009\n\u2264 \u221a\u2211\nr\nn\u2217r\u2016ci\u22121r \u2212 c\u2217r\u20162 \u221a\u2211\nr\nn\u2217r\u2016c\u0302ir \u2212 ci\u22121r \u20162\nNow, since c\u0302ir is the mean of a subset of Air, \u2016c\u0302ir \u2212 ci\u22121r \u20162 \u2264 \u03c6i\u22121r Hence \u2211\nr\nn\u2217r\u2016c\u0302ir \u2212 ci\u22121r \u20162 \u2264 n\u03c6i\u22121\nOn the other hand,\u2211 r n\u2217r\u2016ci\u22121r \u2212 E[c\u0302ir|Fi\u22121]\u20162 = \u2211 r n\u2217r\u2016ci\u22121r \u2212m(Air)\u20162\n\u2264 n \u2211 r \u03c6(ci\u22121r )\u2212 \u03c6(m(Air))\n= n[\u03c6i\u22121 \u2212 \u03c6(m(Ai))] \u2264 n(\u03c6i\u22121 \u2212 \u03c6\u2217)\nNow we first bound (10): \u2211 r n\u2217r\u3008ci\u22121r \u2212 c\u2217r , c\u0302ir \u2212 E[c\u0302ir|Fi\u22121]\u3009\n= \u2211 r n\u2217r\u3008ci\u22121r \u2212 c\u2217r , c\u0302ir \u2212 ci\u22121r \u3009\n+ \u2211 r n\u2217r\u3008ci\u22121r \u2212 c\u2217r , ci\u22121r \u2212 E[c\u0302ir|Fi\u22121]\u3009\n\u2264 \u221a \u2206i\u22121 \u221a n\u03c6i\u22121 + \u221a \u2206i\u22121 \u221a n(\u03c6i\u22121 \u2212 \u03c6\u2217)\n\u2264 \u221a bo\u03c6\u2217 \u221a n(bo + 1)\u03c6\u2217 + \u221a nbo\u03c6 \u2217 \u2264 2(bo + 1) \u221a n\u03c6\u2217\nTo bound (11), \u2211 r n\u2217r\u2016c\u0302ir \u2212 c\u2217r\u20162\n\u2264 2 \u2211 r n\u2217r\u2016c\u0302ir \u2212 ci\u22121r \u20162 + 2 \u2211 r n\u2217r\u2016ci\u22121r \u2212 c\u2217r\u20162\n\u2264 2n\u03c6i\u22121 + 2\u2206i\u22121 \u2264 2n(bo + 1)\u03c6\u2217 + 2bo\u03c6\u2217 \u2264 4n(bo + 1)\u03c6\u2217\nClaim 2. In the context of Algorithm 1, if \u2200ctr \u2208 Ct, ctr \u2208 conv(X), then \u2200ct+1r \u2208 Ct+1, ct+1r \u2208 conv(X).\nProof of Claim. By the update rule in Algorithm 1, ct+1r is a convex combination of ctr and c\u0302t+1r , where c\u0302t+1r is the mean of a subset of X, and hence c\u0302t+1r \u2208 conv(X). Since both ctr and c\u0302t+1r are in conv(X), ct+1r \u2208 conv(X).\nLemma 25 (technical lemma). For any fixed b \u2208 (1, 2]. If C \u2265 b\u221213 , \u03b4 \u2264 1 e , and t \u2265 ( 3Cb\u22121 ln 1 \u03b4 ) 2 b\u22121 , then tb\u22121 \u2212 2C ln t\u2212 C ln 1\u03b4 > 0.\nProof. Let f(t) := tb\u22121 \u2212 2C ln t \u2212 C ln 1\u03b4 . Taking derivative, we get f \u2032(t) = (b\u2212 1)tb\u22122 \u2212 2Ct \u2265 0 when t \u2265 ( 2C b\u22121) 1 b\u22121 . Since ln 1\u03b4 3C b\u22121 \u2265 3C b\u22121 \u2265 1, (ln 1 \u03b4 3C b\u22121) 2 b\u22121 \u2265 ( 2Cb\u22121) 1 b\u22121 , it suffices to show f((ln 1\u03b4 3C b\u22121) 2 b\u22121 ) > 0 for our statement to hold. f((ln 1\u03b4 3C b\u22121) 2 b\u22121 ) = (ln 1\u03b4 3C b\u22121) 2\u22122C ln{(ln 1\u03b4 3C b\u22121) 2 b\u22121 }\u2212C ln 1\u03b4 = (ln 1 \u03b4 ) 2 9C2 (b\u22121)2\u2212 4C b\u22121 ln(ln 1 \u03b4 3C b\u22121)\u2212C ln 1 \u03b4 =\n4C b\u22121 [\n3 2 C\nb\u22121 ln 1 \u03b4 \u2212 ln( 3C b\u22121 ln 1 \u03b4 )] + C ln 1 \u03b4 [ 3C (b\u22121)2 \u2212 1] > 0, where the first term is greater\nthan zero because x\u2212 ln(2x) > 0 for x > 0, and the second term is greater than zero by our assumption on C.\nLemma 26 (Lemma D1 of [4]). Consider a nonnegative sequence (ut : t \u2265 to), such that for some constants a, b > 0 and for all t > to \u2265 0, ut \u2264 (1\u2212 at )ut\u22121 + b t2 . Then, if a > 1,\nut \u2264 ( to + 1\nt+ 1 )auto +\nb\na\u2212 1 (1 +\n1\nto + 1 )a+1\n1\nt+ 1"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>We analyze online [6] and mini-batch [20] k-means variants. Both<lb>scale up the widely used Lloyd\u2019s algorithm via stochastic approximation,<lb>and have become popular for large-scale clustering and unsupervised<lb>feature learning. We show, for the first time, that they have global<lb>convergence towards \u201clocal optima\u201d at rate<lb>O(1t ) under general condi-<lb>tions. In addition, we show if the dataset is clusterable, with suitable<lb>initialization, mini-batch k-means converges to an optimal k-means<lb>solution at rate<lb>O(1t ) with high probability. The k-means objective is<lb>non-convex and non-differentiable: we exploit ideas from non-convex<lb>gradient-based optimization by providing a novel characterization of the<lb>trajectory of k-means algorithm on its solution space, and circumvent<lb>its non-differentiability via geometric insights about k-means update.", "creator": "LaTeX with hyperref package"}}}