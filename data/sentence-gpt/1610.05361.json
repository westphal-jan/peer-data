{"id": "1610.05361", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Oct-2016", "title": "End-to-end attention-based distant speech recognition with Highway LSTM", "abstract": "End-to-end attention-based models have been shown to be competitive alternatives to conventional DNN-HMM models in the Speech Recognition Systems. In this paper, we extend existing end-to-end attention-based models that can be applied for Distant Speech Recognition (DSR) task. Specifically, we propose an end-to-end attention-based speech recognizer with multichannel input that performs sequence prediction directly at the character level of the speech. We describe three approaches that provide an alternative method for detecting a DNN problem for DNN: an interconnected neural network that can learn by transcribing a stream of information about the input. First, we propose a hierarchical DNN system with a hierarchical learning scheme that can be used as a direct model for DNN algorithms that can provide information about the input. Second, we introduce an algorithm that could also be used for recognizing a DNN problem for DNN algorithms. We propose a system that could also be used as a direct model for DNN algorithms that can provide information about the input. Finally, we implement an algorithm that can be used as a direct model for DNN algorithms. Finally, we propose an adaptive system that can learn from input from a DNN input, to identify the inputs and to implement their implementation. Finally, we add a hierarchical DNN system for DNN algorithms that can learn from input in the form of discrete or continuous learning, which provides the information needed to detect the input.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 17 Oct 2016 21:16:56 GMT  (70kb,D)", "http://arxiv.org/abs/1610.05361v1", "8 pages, 2 figures"]], "COMMENTS": "8 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hassan taherian"], "accepted": false, "id": "1610.05361"}, "pdf": {"name": "1610.05361.pdf", "metadata": {"source": "CRF", "title": "END-TO-END ATTENTION-BASED DISTANT SPEECH RECOGNITION WITH HIGHWAY LSTM", "authors": ["Hassan Taherian"], "emails": ["hasan.t@aut.ac.ir"], "sections": [{"heading": null, "text": "Keywords: Distant Speech Recognition, Attention, Highway LSTM"}, {"heading": "1 Introduction", "text": "Recently end-to-end neural network speech recognition systems has shown promising results [1,2,3]. Further improvement were reported by using more advanced models such as Attention-based Recurrent Sequence Generator (ARSG) as part of an end-to-end speech recognition system [4].\nAlthough these new techniques help to decrease the word error rate (WER) on the automatic speech recognition system (ASR), Distant Speech Recognition (DSR) remains a challenging task due to the reverberation and overlapping acoustic signals, even with sophisticated front-end processing techniques such as state-of-the-art beamforming [5].\nIt is reported that using multiple microphones with signal preprocessing techniques like beamforming will improve the performance of the DSR [5]. However the performance of such techniques are suboptimal since they are depended heavily on the microphone array geometry and the location of target source [6]. Other works have shown that Deep Neural Networks with multichannel inputs can be used for learning a suitable representation for distant speech recognition without any front-end preprocessing [7].\nar X\niv :1\n61 0.\n05 36\n1v 1\n[ cs\n.C L\n] 1\nIn this paper, we propose an extension to the current Attention-based Recurrent Sequence Generator (ARSG) that can handle multichannel inputs. We use [4] as the baseline while extend its single channel input to multiple channel. We also integrate the language model in the decoder of ASRG with Weighted Finite State Transducer (WFST) framework as it was purposed in [4]. To avoid slowness in convergence, we use Highway LSTM [8] in our model which allows us to have stacked LSTM layers without having gradient vanishing problem.\nRest of the paper is organized as follows. In Section 2 we briefly discuss related work. In Section 3 we introduce our proposed model and describe integration of language model with Attention-based Recurrent Sequence Generator. Finally, Conclusions are drawn in Section 4."}, {"heading": "2 Related Work", "text": "Kim et al. already proposed an attention-based approach with multichannel input [9]. In their work an attention mechanism is embeded within a Recurrent Neural Network based acoustic model in order to weigh channel inputs for a number of frames. The more reliable channel input will get higher score. By calculating phase difference between microphone pairs, they also incorporated spatial information in their attention mechanism to accelerate learning of auditory attention. They reported that this model achieve comparable performance to beamforming techniques.\nOne advantage of using such model is that no prior knowledge of microphone array geometry is required and real time processing is done much faster than models with front-end preprocessing techniques. However, by using conventional DNN-HMM acoustic model, they did not utilize the full potential of attention mechanism in their proposed model. The feature vectors, weighted by attention mechanism, can help to condition the generation of the next element of the output sequence. In our model, attention mechanism is similar to [10] but it I also connected to the ARGS to produce an end-to-end results."}, {"heading": "3 Proposed Model", "text": "Our model consists of three parts: an encoder, an attention mechanism and a decoder. The model takes multichannel input X and stochastically generates an output sequence (y1, y2, ..., yT ). The input X consists of N channel inputs X={Xch1, Xch2,..., XchN} where each channel Xchi is a sequence of small overlapping window of audio frames Xchi={Xchi1 , Xchi2 ,..., XchiL }. The encoder transforms the input into another representation by using Recurrent Neural Network (RNN). Then the attention mechanism weighs elements of new representation based on their relevance to the output y at each time step. Finally, the decoder which is a RNN in our model, produces an output sequence (y1, y2, ..., yT ) one character at time by using weighted representation produced by attention mechanism and the hidden state of the decoder. In the literature, combination\nof the attention mechanism and the decoder is called Attention Recurrent Sequence Generator (ARSG). ARSG is proposed to address the problem of learning variable-length input and output sequences since in the conventional RNNs, the length of the sequence of hidden state vectors is always equal to the length of the input sequence.\nIn the following subsections we will discuss these three parts in detail."}, {"heading": "3.1 The Encoder", "text": "We concatenate frames of all channels and feed them as an input to the encoder which is a RNN, at each time step t. Another approach to handle multichannel input as Kim et al. proposed in [9], is to feed each channel separately in to the RNN at each time step. Training such RNN in this approach can be very slow for an end-to-end scenario since the attention mechanism should weigh frames of each channel at every time step. Moreover, phase difference calculation between microphone pairs are needed for scoring frames. Because of these computational complexities, we decided to concatenate all channel features and feed them to the network as Liu et al. have done [10] in their model.\nWe used bidirectional highway long short-term memory (BHLSTM) in the encoder. Zhang et al. showed that BHLSTM with dropout achieved state-of-theart performance in the AMI (SDM) task [8]. This model addresses the gradient vanishing problem caused by stacking LSTM layers. Thus this model allows the network to go much deeper [8]. BHLSTM is based on long short-term memory projected (LSTMP) which originally proposed by [11]. The operation of LSTMP network follows the equations\nit = \u03c3(Wxixt + Wmirt\u22121 + Wcict\u22121 + bi) (1)\nft = \u03c3(Wxfxt + Wmfrt\u22121 + Wcf ct\u22121 + bf ) (2)\nct = ft ct\u22121 + it tanh(Wxcxt + Wmcrt\u22121 + bc) (3)\not = \u03c3(Wxoxt + Wmort\u22121 + Wcoct + bo) (4)\nmt = ot tanh(ct) (5)\nrt = Wrmmt (6)\nyt = \u03c3(Wyrrt + by) (7)\niteratively from t = 1 to T , where W term denote weight matrices. Wfc, Wic, Woc are diagonal weight matrices for peephole connections. the b terms denote bias vectors, denotes the element-wise product, i, f , o, c and m are input gate, forget gate, output gate, cell activation and cell output activation vector respectively. xt and yt denotes input and output to the layer at time step t. \u03c3() is sigmoid activation fuction. finally r denotes the recurrent unit activations. a LSTMP unit can be seen as combination of standard LSTM unit with peephole that its cell output activation m is transformed by a linear projection\nlayer. This architecture, shown in the Figure 1, converges faster than standard LSTM and it has less parameters than standard LSTM while keeping the performance [11].\nThe HLSTM RNN is illustrated in Figure 2. It is an extension to the LSTMP model. It has an extra gate, called carry gate, between memory cells of adjacent layers. The carry gate controls how much information can flow between cells in adjacent layers. HLSTM architecture can be orbtained by modification of Eq. (3):\nclt = f l t clt\u22121 + ilt tanh(Wlxcxlt + Wlmcrlt\u22121 + bc) + dlt cl\u22121t (8)\ndlt = \u03c3(W l xdx l t + w l d1 clt\u22121 + wld2 cl\u22121t + bld) (9)\nif the predecessor layer (l \u2212 1) is also an LSTM layer, otherwise:\nclt = f l t clt\u22121 + ilt tanh(Wlxcxlt + Wlmcrlt\u22121 + bc) + dlt xlt (10)\ndlt = \u03c3(W l xdx l t + w l d1 clt\u22121 + bld) (11)\nwhere bld is a bias vector, w l d1 and w l d2 are weight vectors, W l xd is the weight matrix connecting the carry gate to the input of this layer. In this study, we\nalso extend the HLSTM RNNs from unidirection to bidirection. Note that the backward layer follows the same equations used in the forward layer except that t\u2212 1 is replaced by t+ 1 to exploit future frames and the model operates from t = T to 1. The output of the forward and backward layers are concatenated to form the input to the next layer."}, {"heading": "3.2 The Attetion Mechanism", "text": "Attention mechanism is a subnetwork that weighs the encoded inputs h. It selects the temporal locations on input sequence that is useful for updating hidden state of the decoder. Specifically, at each time step i, the attention mechanism computes scalar energy ei,l for each time step of encoded input hl and hidden state of decoder si. The scalar energy will be converted into probability distribu-\ntion, which is called alignment, over time steps using softmax function. Finally, the context vector ci is calculated by linear combination of elements of encoded inputs hl [12].\nei,l = w T tanh(Wsi + Vhl + b) (12) \u03b1i,l = exp(ei,l)\u2211L l=1 exp(ei) (13)\nci = \u2211 l \u03b1i,lhl (14)\nWhere W and V are matrix parameters and w and b are vector parameters. Note that the \u03b1i,l > 0 and \u2211 l \u03b1i = 1. This method is called context-based. Bahdanau et al. showed that using context-based scheme for attention model is prone to error since similar elements of embedded input h is scored equally [4]. To alleviate this problem, Bahdanau et al. suggested to use previous alignment \u03b1i\u22121 by convolving it along time axis for calculation of the scalar energy. Thus by changing Eq. (12) we have:\nF = Q \u2217 \u03b1i\u22121 (15)\nei,l = w T tanh(Wsi + Vhl + Ufl + b) (16)\nWhere U, Q are parameter matrices, * denotes convolution and fl are feature vectors of matrix F. A disadvantage of this model is the complexity of the training procedure. The alignments \u03b1i,l should be calculated for all pairs of input and output position which is O(LT). Chorowski et al. suggested a windowing approach which limits the number of embedded inputs for computation of alignments [13]. This approach reduces the complexity to O(L + T)."}, {"heading": "3.3 The Decoder", "text": "The task of the decoder which is a RNN in ARSG framework, is to produce probability distribution over the next character conditioned on all the characters that already has been seen. This distribution is generated with a MLP and a softmax by the hidden state of the decoder si and the context vector of attention mechanism ci.\nP(yi|X, y<i) = MLP(si, ci) (17)\nThe hidden state of decoder si is a function of previous hidden state si\u22121, the previously emitted character yi\u22121 and context vector of attention mechanism ci\u22121:\nsi = f(si\u22121, yi\u22121, ci\u22121) (18)\nWhere f is a single layer of standard LSTM. The ARSG with encoder can be trained jointly for end-to-end speech recognition. The objective function is to maximize the log probability of each character sequence condition on the previous characters.\nmax\u03b8 \u2211 i log P(yi|X, y\u2217<i; \u03b8) (19)\nWhere \u03b8 is the network parameter and y\u2217 is the ground truth of previous characters. To make it more resilient to the bad predictions during test, Chen et al. suggested to sample from previous predicted characters with the rate of 10% and use them as input for predicting next character instead of ground truth transcript [12].\nChen et al. showed that end-to-end models for speech recognition can achieve good results without using language model but for reducing word error rate (WER), language model is essential [12]. One way for integrating language model with the ARSG framework, as Bahdanau et al. suggested, is to use Weighted Finite State Transducer (WFST) framework [4]. By composition of grammar and lexicon, WFST defines the log probability for a character sequence ( see [14]) . With the combination of WFST and ARGS, we can look for transcripts that maximize:\nL = \u2211 i log P(yi|X, y\u2217<i; \u03b8) + \u03b2logPLM (y) (20)\nin decoding time. Where \u03b2 is tunable parameter. (see [4]). There are also other models for integrating language model to ARGS. Shallow Fusion and Deep Fusion are two of them that are proposed by Gulcehre et al. [15].\nIntegrating Shallow Fusion is similar to the WFST framework but it uses Recurrent Neural Network language model (RNNLM) for producing log probability of sequences. In Deep Fusion, the RNNLM is integrated with the decoder subnetwork in ARSG framework. Specifically, the hidden state of RNNLM is used as input for generating the probability distribution on the next character in Eq. (17). However, in our work we opt to use WFST framework for the sake of simplicity and training tractability of the model."}, {"heading": "4 Conclusion", "text": "In this work, we show the end-to-end models that are based only on neural networks can be applied to the Distant Speech Recognition task with concatenation of multichannel inputs. Also better accuracy is expected with the usage of Highway LSTMs in the encoder network. For the future work, we will\nstudy integration of Recurrent Neural Network Language Model (RNNLM) with Attention-based Recurrent Sequence Generator (ARSG) with Highway LSTM."}], "references": [{"title": "First-Pass Large Vocabulary Continuous Speech Recognition using Bi-Directional Recurrent DNNs", "author": ["A.Y. Hannun", "A.L. Maas", "D. Jurafsky", "A.Y. Ng"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks", "author": ["A. Graves", "S. Fernandez", "F. Gomez", "J. Schmidhuber"], "venue": "Proc. 23rd Int. Conf. Mach. Learn.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Towards End-To-End Speech Recognition with Recurrent Neural Networks, in JMLR", "author": ["A. Graves", "N. Jaitly"], "venue": "Workshop and Conference Proceedings,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "End-To-End Attention-Based Large Vocabulary Speech Recognition, arXiv", "author": ["D. Bahdanau", "J. Chorowski", "D. Serdyuk", "P. Brakel", "Y. Bengio"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Microphone array processing for distant speech recognition: From close-talking microphones to far-field sensors", "author": ["K. Kumatani", "J. McDonough", "B. Raj"], "venue": "IEEE Signal Process. Mag., vol. 29,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Likelihood-maximizing beamforming for robust hands-free speech recognition", "author": ["M.L. Seltzer", "B. Raj", "R.M. Stern"], "venue": "IEEE Trans. Speech Audio Process.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Convolutional neural networks for distant speech recognition", "author": ["P. Swietojanski", "A. Ghoshal", "S. Renals"], "venue": "IEEE Signal Process. Lett., vol. 21,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Highway long short-term memory RNNS for distant speech recognition, in ICASSP", "author": ["Y. Zhang", "G. Chen", "D. Yu", "K. Yaco", "S. Khudanpur", "J. Glass"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Recurrent Models for auditory attention in multi-microphone distance speech recognition", "author": ["S. Kim", "I. Lane"], "venue": "Proc. Annu. Conf. Int. Speech Commun. Assoc. INTERSPEECH,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Using neural network front-ends on far field multiple microphones based speech recognition, in ICASSP", "author": ["Y. Liu", "P. Zhang", "T. Hain"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Long short-term memory based recurrent neural network architectures for large scale acoustic modeling, in Interspeech", "author": ["H. Sak", "A. Senior", "F. Beaufays"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition, in ICASSP", "author": ["W. Chan", "N. Jaitly", "Q. Le", "O. Vinyals"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results", "author": ["J. Chorowski", "D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "Deep Learn. Represent. Learn. Work. NIPS", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Weighted finite-state transducers in speech recognition, Comput", "author": ["M. Mohri", "F. Pereira", "M. Riley"], "venue": "Speech Lang.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Recently end-to-end neural network speech recognition systems has shown promising results [1,2,3].", "startOffset": 90, "endOffset": 97}, {"referenceID": 1, "context": "Recently end-to-end neural network speech recognition systems has shown promising results [1,2,3].", "startOffset": 90, "endOffset": 97}, {"referenceID": 2, "context": "Recently end-to-end neural network speech recognition systems has shown promising results [1,2,3].", "startOffset": 90, "endOffset": 97}, {"referenceID": 3, "context": "Further improvement were reported by using more advanced models such as Attention-based Recurrent Sequence Generator (ARSG) as part of an end-to-end speech recognition system [4].", "startOffset": 175, "endOffset": 178}, {"referenceID": 4, "context": "Although these new techniques help to decrease the word error rate (WER) on the automatic speech recognition system (ASR), Distant Speech Recognition (DSR) remains a challenging task due to the reverberation and overlapping acoustic signals, even with sophisticated front-end processing techniques such as state-of-the-art beamforming [5].", "startOffset": 335, "endOffset": 338}, {"referenceID": 4, "context": "It is reported that using multiple microphones with signal preprocessing techniques like beamforming will improve the performance of the DSR [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 5, "context": "However the performance of such techniques are suboptimal since they are depended heavily on the microphone array geometry and the location of target source [6].", "startOffset": 157, "endOffset": 160}, {"referenceID": 6, "context": "Other works have shown that Deep Neural Networks with multichannel inputs can be used for learning a suitable representation for distant speech recognition without any front-end preprocessing [7].", "startOffset": 192, "endOffset": 195}, {"referenceID": 3, "context": "We use [4] as the baseline while extend its single channel input to multiple channel.", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "We also integrate the language model in the decoder of ASRG with Weighted Finite State Transducer (WFST) framework as it was purposed in [4].", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "To avoid slowness in convergence, we use Highway LSTM [8] in our model which allows us to have stacked LSTM layers without having gradient vanishing problem.", "startOffset": 54, "endOffset": 57}, {"referenceID": 8, "context": "already proposed an attention-based approach with multichannel input [9].", "startOffset": 69, "endOffset": 72}, {"referenceID": 9, "context": "In our model, attention mechanism is similar to [10] but it I also connected to the ARGS to produce an end-to-end results.", "startOffset": 48, "endOffset": 52}, {"referenceID": 8, "context": "proposed in [9], is to feed each channel separately in to the RNN at each time step.", "startOffset": 12, "endOffset": 15}, {"referenceID": 9, "context": "have done [10] in their model.", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "showed that BHLSTM with dropout achieved state-of-theart performance in the AMI (SDM) task [8].", "startOffset": 91, "endOffset": 94}, {"referenceID": 7, "context": "Thus this model allows the network to go much deeper [8].", "startOffset": 53, "endOffset": 56}, {"referenceID": 10, "context": "BHLSTM is based on long short-term memory projected (LSTMP) which originally proposed by [11].", "startOffset": 89, "endOffset": 93}, {"referenceID": 10, "context": "This architecture, shown in the Figure 1, converges faster than standard LSTM and it has less parameters than standard LSTM while keeping the performance [11].", "startOffset": 154, "endOffset": 158}, {"referenceID": 11, "context": "Finally, the context vector ci is calculated by linear combination of elements of encoded inputs hl [12].", "startOffset": 100, "endOffset": 104}, {"referenceID": 3, "context": "showed that using context-based scheme for attention model is prone to error since similar elements of embedded input h is scored equally [4].", "startOffset": 138, "endOffset": 141}, {"referenceID": 12, "context": "suggested a windowing approach which limits the number of embedded inputs for computation of alignments [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 11, "context": "suggested to sample from previous predicted characters with the rate of 10% and use them as input for predicting next character instead of ground truth transcript [12].", "startOffset": 163, "endOffset": 167}, {"referenceID": 11, "context": "showed that end-to-end models for speech recognition can achieve good results without using language model but for reducing word error rate (WER), language model is essential [12].", "startOffset": 175, "endOffset": 179}, {"referenceID": 3, "context": "suggested, is to use Weighted Finite State Transducer (WFST) framework [4].", "startOffset": 71, "endOffset": 74}, {"referenceID": 13, "context": "By composition of grammar and lexicon, WFST defines the log probability for a character sequence ( see [14]) .", "startOffset": 103, "endOffset": 107}, {"referenceID": 3, "context": "(see [4]).", "startOffset": 5, "endOffset": 8}], "year": 2016, "abstractText": "End-to-end attention-based models have been shown to be competitive alternatives to conventional DNN-HMM models in the Speech Recognition Systems. In this paper, we extend existing end-to-end attentionbased models that can be applied for Distant Speech Recognition (DSR) task. Specifically, we propose an end-to-end attention-based speech recognizer with multichannel input that performs sequence prediction directly at the character level. To gain a better performance, we also incorporate Highway long short-term memory (HLSTM) which outperforms previous models on AMI distant speech recognition task.", "creator": "LaTeX with hyperref package"}}}