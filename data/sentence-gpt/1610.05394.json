{"id": "1610.05394", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Sequential Learning without Feedback", "abstract": "In many security and healthcare systems a sequence of features/sensors/tests are used for detection and diagnosis. Each test outputs a prediction of the latent state, and carries with it inherent costs. Our objective is to {\\it learn} strategies for selecting tests to optimize accuracy \\&amp; costs are identified by the test itself. To minimize these costs we will assume an appropriate balance between the available test parameters and the tests with test parameters. For example, the test outputs a 1:1, 2:1, and 3:1.\n\n\nLet's assume that these tests were a set of tests:\nAs we now know we have many types of tests, and we need to add them into our tests:\nSo the test outputs a 1:1, 2:1, and 3:1 test. In this example we are building a test with a test parameter that is the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of a test parameter and the sum of the test parameters in the output of", "histories": [["v1", "Tue, 18 Oct 2016 01:15:57 GMT  (228kb,D)", "http://arxiv.org/abs/1610.05394v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["manjesh hanawal", "csaba szepesvari", "venkatesh saligrama"], "accepted": false, "id": "1610.05394"}, "pdf": {"name": "1610.05394.pdf", "metadata": {"source": "CRF", "title": "Sequential Learning without Feedback\u2217", "authors": ["Manjesh Hanawal", "Csaba Szepesvari", "Venkatesh Saligrama"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Sequential sensor acquisition arises in many security and healthcare diagnostic systems. In these applications we have a diverse collection of sensor-based-tests with differing costs and accuracy. In these applications (see Fig. 1) inexpensive tests are first conducted and based on their outcomes a decision for acquiring more (expensive) tests are made. The goal in these systems is to maximize overall accuracy for an available cost-budget. Generally, the components that can be optimized include sensor classifiers (to improve test accuracy), sensor ordering, and decision strategies for sequential sensor selection. Nevertheless, sensor classifiers and sensor ordering are typically part of the infrastructure and generally harder to control/modify in both security and medical systems. To this end we focus here on the sequential sensor selection problem and use the terms sensor and test interchangeably. The need for systematically learning optimal decision strategies for balancing accuracy & costs arises from the fact that these applications involve legacy systems where sensor/test selection strategies are local; often managed under institutional, rather than national guidelines [?]. While it is possible to learn such decision strategies given sufficient annotated training data, what makes these applications challenging is that it is often difficult to acquire in-situ ground truth labels.\nThese observations leads us to the problem of learning decision strategies for optimal sensor selection in situations where we do not have the benefit of ground-truth annotations, and what we\n\u2217Manjesh Hanawal is with the Department of Industrial Engineering at IIT Bombay. His research was conducted while he was a post-doctoral associate at Boston University. Csaba Szepesvari is with the Department of Computer Science at University of Alberta. Venkatesh Saligrama is with the Department of Electrical and Computer Engineering at Boston University.\nar X\niv :1\n61 0.\n05 39\n4v 1\n[ cs\n.L G\n] 1\n8 O\nct 2\n01 6\nSensor 1 (Cost c1)\nSensor 2 (Cost c2)\nSensor K (Cost cK)\nrefer to as the Unsupervised Sensor Selection (USS) problem. In Sec. 3 we pose our problem as a version of stochastic partial monitoring problem [2] with atypical reward structure, where tests are viewed as actions and sequential observations serves as side information. As is common, we pose the problem in terms of competitive optimality. We consider a competitor who can choose an optimal test with the benefit of hindsight. Our goal is to minimize cummulative regret based on learning the optimal test based on observations in multiple rounds of play. Recall that in a stochastic partial monitoring problem a decision maker needs to choose the action with the lowest expected cost by repeatedly trying the actions and observing some feedback. The decision maker lacks the knowledge of some key information, such as in our case, the misclassification error rates of the classifiers, but had this information been available, the decision maker could calculate the expected costs of all the actions (sensor acquisitions) and could choose the best action (test). The feedback received by the decision maker in a given round depends stochastically on the unknown information and the action chosen. Bandit problems [3] are a special case of partial monitoring, where the key missing information is the expected cost for each action, and the feedback is the noisy version of the expected cost of the action chosen. In the USS problem the learner only observes the outputs of the classifiers, but not the label to be predicted over multiple rounds in a stochastic, stationary environment.\nIn Sec. 3 we first show that, unsurprisingly that no learner can achieve sublinear regret without further assumptions. To this end we propose the notions of weak and strong dominance on tests, which correspond to constraints on joint probability distributions over the latent state and testoutcomes. Strong dominance (SD) is a property arising in many Engineered systems and says that whenever a test is accurate on an example, a later test in the sequence is almost surely accurate on that example. Weak dominance is a relaxed notion that allows for errors in these predictions. We empirically demonstrate that weak dominance appears to hold by evaluating it on several real datasets. We also show that in a sense weak dominance is fundamental, namely, without this condition there exist problem instances that result in linear regret. On the other hand whenever this condition is satisfied there exist polynomial time algorithms that lead to sublinear regret.\nOur proof of sublinear regret in Sec. 5 is based on reducing USS to a version of multi-armed bandit problem (MAB) with side-observation. The latter problem has already been shown to have sub-linear regret in the literature. In our reduction, we identify tests as bandit arms. The payoff of an arm is given by marginal losses relative to the root test, and the side observation structure is\ndefined by the feedback graph induced by the directed graph. We then formally show that there is a one-to-one mapping between algorithms for USS and algorithms for MAB with side-observation. In particular, under weak dominance, the regret bounds for MAB with side-observation then imply corresponding regret bounds for USS."}, {"heading": "1.1 Related Work", "text": "In contrast to our USS setup there exists a wide body of literature dealing with sensor acquisition (see[4, 5, 6]). Like us they also deal with cascade models with costs for features/tests but their method is based on training decision strategies with fully supervised data for prediction-time use. There are also several methods that work in an online bandit setting and train prediction models with feature costs [12] but again they require ground-truth labels as reward-feedback. A somewhat more general version of [12] is developed in [13] where in addition the learner can choose to acquire ground-truth labels for a cost.\nOur paper bears some similarity with the concept of active classification, which deals with learning stopping policies[7, 8] among a given sequence of tests. Like us these works also consider costs for utilizing tests and the goal is to learn when to stop to make decisions. Nevertheless, unlike our setup the loss associated with the decision is observed in their context.\nOur paper is related to the framework of finite partial monitoring problems[2], which deals with how to infer unknown key information and where tests/actions reveal different types of information about the unknown information. In this context [14] consider special cases where payoff/rewards for a subset of actions are observed. This is further studied as a side-observation problem in [15] and as graph-structured feedback [16, 17, 18]). Our work is distinct from these setups because we are unable to observe rewards for our chosen actions or any other actions."}, {"heading": "2 Background", "text": "In this section we will introduce a number of sequential decision making problems, namely stochastic partial monitoring, bandits and bandits with side-observations, which we will build upon later.\nFirst, a few words about our notation: We will use upper case letters to denote random variables. The set of real numbers is denoted by R. For positive integer n, we let [n] = {1, . . . , n}. We let M1(X ) to denote the set of probability distributions over some set X . When X is finite with a cardinality of d .= |X |, M1(X ) can be identified with the d-dimensional probability simplex.\nIn a stochastic partial monitoring problem (SPM) a learner interacts with a stochastic environment in a sequential manner. In round t = 1, 2, . . . the learner chooses an action At from an action set A, and receives a feedback Yt \u2208 Y from a distribution p which depends on the action chosen and also on the environment instance identified with a \u201cparameter\u201d \u03b8 \u2208 \u0398: Yt \u223c p(\u00b7;At, \u03b8). The learner also incurs a reward Rt, which is a function of the action chosen and the unknown parameter \u03b8: Rt = r(At, \u03b8). The reward may or may not be part of the feedback for round t. The learner\u2019s goal is to maximize its total expected reward. The family of distributions (p(\u00b7; a, \u03b8))a,\u03b8 and the family of rewards (r(a, \u03b8))a,\u03b8 and the set of possible parameters \u0398 are known to the learner, who uses this knowledge to judiciously choose its next action to reduce its uncertainty about \u03b8 so that it is able to eventually converge on choosing only an optimal action a\u2217(\u03b8), achieving the best possible reward per round, r\u2217(\u03b8) = maxa\u2208A r(a, \u03b8). The quantification of the learning speed is given by the expected regret Rn = nr\u2217(\u03b8)\u2212 E [ \u2211n t=1Rt], which, for brevity and when it does not cause confusion, we will just call regret. A sublinear expected regret, i.e., Rn/n\u2192 0 as n\u2192\u221e means that the learner in\nthe long run collects almost as much reward on expectation as if the optimal action was known to it. In some cases it is more natural to define the problems in terms of costs as opposed to rewards; in such cases the definition of regret is modified appropriately by flipping the sign of rewards and costs.\nBandit Problems are a special case of SPMs where Y is the set of real numbers, r(a, \u03b8) is the mean of distribution p(\u00b7; a, \u03b8) and thus the learner in every round the learner upon choosing an action At receives the noisy reward Yt \u223c p(\u00b7;At, \u03b8) as feedback. A finite armed bandit with side-observations [15] is also a special case of SPMs, where the learner upon choosing an action a \u2208 A receives noisy reward observations, namely, Yt = (Yt,a)a\u2208N(At), Yt,a \u223c pr(\u00b7; a, \u03b8), E [Yt,a] = r(a, \u03b8), from a neighbor-set N (a) \u2282 A, which is a priori known to the learner. To cast this setting as an SPM we let Y as the set \u222aKi=0Ri and define the family of distributions (p(\u00b7; a, \u03b8))a,\u03b8 such that Yt \u223c p(\u00b7;At, \u03b8). The framework of SPM is quite general and allows for parametric and non-parametric sets \u0398. In what follows we identify \u0398 with set of instances (p(\u00b7; a, \u03b8), r(a, \u03b8))\u03b8\u2208\u0398. In other words we view elements of \u0398 as a pair p, r where p(\u00b7; a) is a probability distribution over Y for each a \u2208 A and r is a map from A to the reals."}, {"heading": "3 Unsupervised Sensor Selection", "text": "Preliminaries and Notation: Proofs for formal statements appears in the Appendix. We will use upper case letters to denote random variables. The set of real numbers is denoted by R. For positive integer n, we let [n] = {1, . . . , n}. We let M1(X ) to denote the set of probability distributions over some set X . When X is finite with a cardinality of d .= |X |, M1(X ) can be identified with the d-dimensional probability simplex.\nWe first consider the unsupervised, stochastic, cascaded sensor selection problem. We cast it as a special case of stochastic partial monitoring problem (SPM), which is described in the appendix. Later we will briefly describe extensions to tree-structures and contextual cases. Formally, a problem instance is specified by a pair \u03b8 = (P, c), where P is a distribution over the K + 1 dimensional hypercube, and c is a K-dimensional, nonnegative valued vector of costs. While c is known to the learner from the start, P is initially unknown. Henceforth we identify problem instance \u03b8 by P . The instance parameters specify the learner-environment interaction as follows: In each round for t = 1, 2, . . . , the environment generates a K + 1-dimensional binary vector Y = (Yt, Y 1t , . . . , Y Kt ) chosen at random from P . Here, Y it is the output of sensor i, while Yt is a (hidden) label to be guessed by the learner. Simultaneously, the learner chooses an index It \u2208 [K] and observes the sensor outputs Y 1t , . . . , Y It t . The sensors are known to be ordered from least accurate to most accurate, i.e., \u03b3k(\u03b8) . = P ( Yt 6= Y kt ) is decreasing with k increasing. Knowing this, the learner\u2019s\nchoice of It also indicates that he/she chooses It to predict the unknown label Yt. Observing sensors is costly: The cost of choosing It is CIt . = c1 + \u00b7 \u00b7 \u00b7 + cIt . The total cost suffered by the learner in round t is thus CIt + I{Yt 6= Y Itt }. The goal of the learner is to compete with the best choice\ngiven the hindsight of the values (\u03b3k)k. The expected regret of learner up to the end of round n is Rn = ( \u2211n t=1 E [ CIt + I{Yt 6= Y Itt } ] )\u2212 nmink(Ck + \u03b3k). Sublinear Regret: The quantification of the learning speed is given by the expected regret Rn = nr \u2217(\u03b8)\u2212 E [ \u2211n\nt=1Rt], which, for brevity and when it does not cause confusion, we will just call regret. A sublinear expected regret, i.e., Rn/n \u2192 0 as n \u2192 \u221e means that the learner in the long run collects almost as much reward on expectation as if the optimal action was known to it.\nFor future reference, we let c(k, \u03b8) = E [ Ck + I{Yt 6= Y kt } ] (= Ck + \u03b3k) and c\u2217(\u03b8) = mink c(k, \u03b8).\nThus, Rn = ( \u2211n\nt=1 E [c(It, \u03b8)])\u2212nc\u2217(\u03b8). In what follows, we shall denote by A\u2217(\u03b8) the set of optimal actions of \u03b8 and we let a\u2217(\u03b8) denote the optimal action that has the smallest index 1. Thus, in particular, a\u2217(\u03b8) = minA\u2217(\u03b8). Next, for future reference note that one can express optimal actions from the viewpoint of marginal costs and marginal error. In particular an action i is optimal if for all j > i the marginal increase in cost, Cj \u2212Ci, is larger than the marginal decrease in error, \u03b3i \u2212 \u03b3j : \u2200 j \u2265 i\nCj \u2212 Ci\ufe38 \ufe37\ufe37 \ufe38 Marginal Cost\n\u2265 E [ I{Yt 6= Y it } \u2212 I{Yt 6= Y j t } ]\n\ufe38 \ufe37\ufe37 \ufe38 Marginal Error = \u03b3i \u2212 \u03b3j . (1)"}, {"heading": "4 When is USS Learnable?", "text": "Let \u0398SA be the set of all stochastic, cascaded sensor acquisition problems. Thus, \u03b8 \u2208 \u0398SA such that if Y \u223c \u03b8 then \u03b3k(\u03b8) := P ( Y 6= Y k ) is a decreasing sequence. Given a subset \u0398 \u2282 \u0398SA, we say that \u0398 is learnable if there exists a learning algorithm A such that for any \u03b8 \u2208 \u0398, the expected regret E [Rn(A, \u03b8)] of algorithm A on instance \u03b8 is sublinear. A subset \u0398 is said to be a maximal learnable problem class if it is learnable and for any \u0398\u2032 \u2282 \u0398SA superset of \u0398, \u0398\u2032 is not learnable. In this section we study two special learnable problem classes, \u0398SD \u2282 \u0398WD, where the regularity properties of the instances in \u0398SD are more intuitive, while \u0398WD can be seen as a maximal extension of \u0398SD.\nLet us start with some definitions. Given an instance \u03b8 \u2208 \u0398SA, we can decompose \u03b8 (or P ) into the joint distribution PS of the sensor outputs S = (Y 1, . . . , Y k) and the conditional distribution of the state of the environment, given the sensor outputs, PY |S . Specifically, letting (Y, S) \u223c P , for s \u2208 {0, 1}K and y \u2208 {0, 1}, PS(s) = P (S = s) and PY |S(y|s) = P (Y = y|S = s). We denote this by P = PS \u2297 PY |S . A learner who observes the output of all sensors for long enough is able to identify PS with arbitrary precision, while PY |S remains hidden from the learner. This leads to the following statement:\nProposition 1. A subset \u0398 \u2282 \u0398SA is learnable if and only if there exists a map a : M1({0, 1}K)\u2192 [K] such that for any \u03b8 \u2208 \u0398 with decomposition P = PS \u2297 PY |S, a(PS) is an optimal action in \u03b8.\nAn action selection map a : M1({0, 1}K) \u2192 [K] is said to be sound for an instance \u03b8 \u2208 \u0398SA with \u03b8 = PS \u2297 PY |S if a(PS) selects an optimal action in \u03b8. With this terminology, the previous proposition says that a set of instances \u0398 is learnable if and only if there exists a sound action selection map for all the instances in \u0398.\nA class of sensor acquisition problems that contains instances that satisfy the so-called strong dominance condition will be shown to be learnable:\n1Note that even if i < j are optimal actions, there can be suboptimal actions in the interval [i, j](= [i, j] \u2229 N) (e.g., \u03b31 = 0.3, C1 = 0, \u03b32 = 0.25, C2 = 0.1, \u03b33 = 0, C3 = 0.3.\nDefinition 1 (Strong Dominance (SD)). An instance \u03b8 \u2208 \u0398SA is said to satisfy the strong dominance property if it holds in the instance that if a sensor predicts correctly then all the sensors in the subsequent stages of the cascade also predict correctly, i.e., for any i \u2208 [K],\nY i = Y \u21d2 Y i+1 = \u00b7 \u00b7 \u00b7 = Y K = Y (2)\nalmost surely (a.s.) where (Y, Y 1, . . . , Y K) \u223c P .\nBefore we develop this concept further we will motivate strong dominance based on experiments on a few real-world datasets. SD property naturally arises in the context of error-correcting codes (see [?, ?]), where one sequentially corrects for errors, and thus whenever a decoder is accurate the following decoders are also accurate. On the other hand for real-datasets SD holds only approximately. Table 4 lists the error probabilities of the classifiers (sensors) for the heart and diabetic datasets from UCI repository. We split features into two sets based on provided costs (cheap tests are based on patient history and costly tests include all the features). We then trained an SVM classifier with 5-fold cross-validation and report scores based on held-out test data. The last column lists the\nprobability that second sensor misclassifies an instance that is correctly classified by the first sensor. SD is the notion that suggests that this probability is zero. We find in these datasets that \u03b412 is small thus justifying our notion. In general we have found this behavior is representative of other cost-associated datasets.\nWe next show that strong dominance conditions ensures learnability. To this end, let \u0398SD = {\u03b8 \u2208 \u0398SA : \u03b8 satisfies the strong dominance condition }.\nTheorem 1. The set \u0398SD is learnable.\nWe start with a proposition that will be useful beyond the proof of this result. In this proposition, \u03b3i = \u03b3i(\u03b8) for \u03b8 \u2208 \u0398SA and (Y, Y 1, . . . , Y K) \u223c \u03b8.\nProposition 2. For any i, j \u2208 [K], \u03b3i \u2212 \u03b3j = P ( Y i 6= Y j ) \u2212 2P ( Y j 6= Y, Y i = Y ) .\nThe proof motivates the definition of weak dominance, a concept that we develop next through a series of smaller propositions. In these propositions, as before (Y, Y 1, . . . , Y K) \u223c P where P \u2208M1({0, 1}K+1), \u03b3i = P ( Y i 6= Y ) , i \u2208 [K], and Ci = c1 + \u00b7 \u00b7 \u00b7+ ci. We start with a corollary of Proposition 2\nCorollary 1. Let i < j. Then 0 \u2264 \u03b3i \u2212 \u03b3j \u2264 P ( Y i 6= Y j ) .\nProposition 3. Let i < j. Assume\nCj \u2212 Ci 6\u2208 [\u03b3i \u2212 \u03b3j ,P ( Y i 6= Y j ) ) . (3)\nThen \u03b3i + Ci \u2264 \u03b3j + Cj if and only if Cj \u2212 Ci \u2265 P ( Y i 6= Y j ) .\nProof. \u21d2: From the premise, it follows that Cj\u2212Ci \u2265 \u03b3i\u2212\u03b3j . Thus, by (3), Cj\u2212Ci \u2265 P ( Y i 6= Y j ) .\n\u21d0: We have Cj \u2212 Ci \u2265 P ( Y i 6= Y j ) \u2265 \u03b3i \u2212 \u03b3j , where the last inequality is by Corollary 1.\nProposition 4. Let j < i. Assume\nCi \u2212 Cj 6\u2208 (\u03b3j \u2212 \u03b3i,P ( Y i 6= Y j ) ] . (4)\nThen, \u03b3i + Ci \u2264 \u03b3j + Cj if and only if Ci \u2212 Cj \u2264 P ( Y i 6= Y j ) . Proof. \u21d2: The condition \u03b3i + Ci \u2264 \u03b3j + Cj implies that \u03b3j \u2212 \u03b3i \u2265 Ci \u2212 Cj . By Corollary 1 we get P ( Y i 6= Y j ) \u2265 Ci \u2212 Cj . \u21d0: Let Ci \u2212 Cj \u2264 P ( Y i 6= Y j ) . Then, by (4), Ci \u2212 Cj \u2264 \u03b3j \u2212 \u03b3i.\nThese results motivate the following definition:\nDefinition 2 (Weak Dominance). An instance \u03b8 \u2208 \u0398SA is said to satisfy the weak dominance property if for i = a\u2217(\u03b8),\n\u2200j > i : Cj \u2212 Ci \u2265 P ( Y i 6= Y j ) . (5)\nWe denote the set of all instances in \u0398SA that satisfies this condition by \u0398WD.\nNote that \u0398SD \u2282 \u0398WD since for any \u03b8 \u2208 \u0398SD, any j > i = a\u2217(\u03b8), on the one hand Cj\u2212Ci \u2265 \u03b3i\u2212\u03b3j , while on the other hand, by the strong dominance property, P ( Y i 6= Y j ) = \u03b3i \u2212 \u03b3j .\nWe now relate weak dominance to the optimality condition described in Eq. (1). Weak dominance can be viewed as a more stringent condition for optimal actions. Namely, for an action to be optimal we also require that the marginal cost be larger than marginal absolute error:\nCj \u2212 Ci\ufe38 \ufe37\ufe37 \ufe38 Marginal Cost \u2265 E [\u2223\u2223\u2223I{Yt 6= Y it } \u2212 I{Yt 6= Y jt }\u2223\u2223\u2223]\ufe38 \ufe37\ufe37 \ufe38\nMarginal Absolute Error\n, \u2200 j \u2265 i . (6)\nThe difference between marginal error in Eq. (1) and marginal absolute error is the presence of the absolute value. We will show later that weak-dominant set is a maximal learnable set, namely, the set cannot be expanded while ensuring learnability.\nWe propose the following action selector awd : M1({0, 1}K)\u2192 [K]:\nDefinition 3. For PS \u2208M1({0, 1}K) let awd(PS) denote the smallest index i \u2208 [K] such that\n\u2200j < i : Ci \u2212 Cj < P ( Y i 6= Y j ) , (7a)\n\u2200j > i : Cj \u2212 Ci \u2265 P ( Y i 6= Y j ) , (7b)\nwhere Ci = c1 + \u00b7 \u00b7 \u00b7+ ci, i \u2208 [K] and (Y 1, . . . , Y K) \u223c PS. (If no such index exists, awd is undefined, i.e., awd is a partial function.)\nProposition 5. For any \u03b8 \u2208 \u0398WD with \u03b8 = PS \u2297 PY |S, awd(PS) is well-defined.\nProposition 6. The map awd is sound over \u0398WD: In particular, for any \u03b8 \u2208 \u0398WD with \u03b8 = PS \u2297 PY |S, awd(PS) = a\u2217(\u03b8).\nCorollary 2. The set \u0398WD is learnable.\nProof. By Proposition 5, awd is well-defined over \u0398WD, while by Proposition 6, awd is sound over \u0398WD. By Proposition 1, \u0398WD is learnable, as witnessed by awd.\nProposition 7. Let \u03b8 \u2208 \u0398SA and \u03b8 = PS \u2297 PY |S be such that awd is defined for PS and awd(PS) = a\u2217(\u03b8). Then \u03b8 \u2208 \u0398WD.\nProof. Immediate from the definitions.\nAn immediate corollary of the previous proposition is as follows:\nCorollary 3. Let \u03b8 \u2208 \u0398SA and \u03b8 = PS \u2297 PY |S. Assume that awd is defined for PS and \u03b8 6\u2208 \u0398WD. Then awd(PS) 6= a\u2217(\u03b8).\nThe next proposition states that awd is essentially the only sound action selector map defined for all instances derived from instances of \u0398WD:\nProposition 8. Take any action selector map a : M1({0, 1}K) \u2192 [K] which is sound over \u0398WD. Then, for any PS such that \u03b8 = PS \u2297 PY |S \u2208 \u0398WD with some PY |S, a(PS) = awd(PS).\nThe next result shows that the set \u0398WD is essentially a maximal learnable set in dom(awd):\nTheorem 2. Let a : M1({0, 1}K) \u2192 [K] be an action selector map such that a is sound over the instances of \u0398WD. Then there is no instance \u03b8 = PS \u2297PY |S \u2208 \u0398SA \\\u0398WD such that PS \u2208 dom(awd), the optimal action of \u03b8 is unique and a(PS) = a\u2217(\u03b8).\nNote that dom(awd) \\ {PS : \u2203PY |S s.t. PS \u2297 PY |S \u2208 \u0398WD} 6= \u2205, i.e., the theorem statement is non-vacuous. In particular, for K = 2, consider (Y, Y 1, Y 2) such that Y and Y 1 are independent and Y 2 = 1\u2212 Y 1, we can see that the resulting instance gives rise to PS which is in the domain of awd for any c \u2208 RK+ (because here \u03b31 = \u03b32 = 1/2, thus \u03b31 \u2212 \u03b32 = 0 while P ( Y 1 6= Y 2 ) = 1).\nProof. Let a as in the theorem statement. By Proposition 8, awd is the unique sound action-selector map over \u0398WD. Thus, for any \u03b8 = PS \u2297 PY |S \u2208 \u0398WD, awd(PS) = a(PS). Hence, the result follows from Corollary 3.\nWhile \u0398WD is learnable, it is not uniformly learnable, i.e., the minimax regret R\u2217n(\u0398WD) = infA sup\u03b8\u2208\u0398WD Rn(A, \u03b8) over \u0398WD grows linearly: Theorem 3. \u0398WD is not uniformly learnable: R\u2217n(\u0398WD) = \u2126(n).\nExtensions: We describe a few extensions here but omit details due to lack of space. Tree-Architectures: We can deal with trees in an analogous fashion. Like for cascades we keep track of disagreements along each path. Under SD these disagreements in turn approximate marginal error. This fact is sufficient to identify the optimal sensor (see Eqn. 1). Similarly our results also generalize to trees under WD condition. Context-Dependent Sensor Selection: Each example has a context x \u2208 Rd. A test is a mapping that takes (partial) context and maps to an output space. We briefly describe how to extend to context-dependent sensor selection. Analogous to the context-independent case we impose context dependent notions for SD and WD, namely, Eq. 2 and Eq. 5 hold conditioned on each x \u2208 X . To handle these cases we let \u03b3i(x) , Pr(Y i 6= Y ) and \u03b3ij(x) , Pr(Y i 6= Y j) denote the conditional error probability for the ith sensor and the conditional marginal error probability between sensor i and j respectively. Then the results can be generalized under a parameterized GLM model for disagreement, namely, the log-odds ratio log \u03b3ij(x)1\u2212\u03b3ij(x) can be written as \u03b8 \u2032 ijx for some unknown \u03b8ij \u2208 Rd."}, {"heading": "5 Regret Equivalence", "text": "In this section we establish that USS with SD property is \u2018regret equivalent\u2019 to an instance of multi-armed-bandit (MAB) with side-information. The corresponding MAB algorithm can then be suitably imported to solve USS efficiently.\nLet PUSS be the set of USSs with action set A = [K]. The corresponding bandit problems will have the same action set, while for action k \u2208 [K] the neighborhood set is N (k) = [k]. Take any instance (P, c) \u2208 PUSS and let (Y, Y 1, . . . , Y K) \u223c P be the unobserved state of environment. We let the reward distribution for arm k in the corresponding bandit problem be a shifted Bernoulli distribution. In particular, the cost of arm k follows the distribution of I{Y k 6=Y 1} \u2212 Ck (we use costs here to avoid flipping signs).\nThe costs for different arms are defined to be independent of each other. Let Pside denote the set of resulting bandit problems and let f : PUSS \u2192 Pside be the map that transforms USS instances to bandit instances by following the transformation that was just described.\nNow let \u03c0 \u2208 \u03a0(Pside) be a policy for Pside. Policy \u03c0 can also be used on any (P, c) instance in PUSS in an obvious way: In particular, given the history of actions and states A1, U1, . . . , At, Ut in \u03b8 = (P, c) where Us = (Ys, Y 1s , . . . , Y Ks ) such that the distribution of Us given that As = a is P marginalized to Ya, the next action to be taken is At+1 \u223c \u03c0(\u00b7|A1, V1, . . . , At, Vt), where Vs = (I{Y 1s 6=Y 1s }\u2212C1, . . . , I{Y 1s 6=Y Ass }\u2212CAs). Let the resulting policy be denoted by \u03c0\n\u2032. The following can be checked by simple direct calculation:\nProposition 9. If \u03b8 \u2208 \u0398SD, then the regret of \u03c0 on f(\u03b8) \u2208 Pside is the same as the regret of \u03c0\u2032 on \u03b8.\nThis implies that R\u2217T (\u0398SD) \u2264 R\u2217T (f(\u0398SD)). Now note that this reasoning can also be repeated in the other \u201cdirection\u201d: For this, first note that the map f has a right inverse g (thus, f \u25e6 g is the identity over Pside) and if \u03c0\u2032 is a policy for PUSS, then \u03c0\u2032 can be \u201cused\u201d on any instance \u03b8 \u2208 Pside via the \u201cinverse\u201d of the above policy-transformation: Given the sequence (A1, V1, . . . , At, Vt) where Vs = (B 1 s + C1, . . . , B K s + Cs) is the vector of costs for round s with Bks being a Bernoulli with parameter \u03b3k, let At+1 \u223c \u03c0\u2032(\u00b7|A1,W1, . . . , At,Wt) where Ws = (B1s , . . . , BAss ). Let the resulting policy be denoted by \u03c0. Then the following holds:\nProposition 10. Let \u03b8 \u2208 f(\u0398SD). Then the regret of policy \u03c0 on \u03b8 \u2208 f(\u0398SD) is the same as the regret of policy \u03c0\u2032 on instance f\u22121(\u03b8).\nHence, R\u2217T (f(\u0398SD)) \u2264 R\u2217T (\u0398SD). In summary, we get the following result:\nCorollary 4. R\u2217T (\u0398SD) = R \u2217 T (f(\u0398SD)).\nLower Bounds: Note that as a consequence of the reduction and the one-to-one correspondence between the two problems, lower bounds for MAB with side-information is a lower bound for USS problem."}, {"heading": "6 Algorithms", "text": "The reduction of the previous section suggests that one can utilize an algorithm developed for stochastic bandits with side-observation to solve a USS satisfying SD property. In this paper we make use of Algorithm 1 of [18] that was proposed for stochastic bandits with Gaussian side observations. As noted in the same paper, the algorithm is also suitable for problems where the payoff distributions\nare sub-Gaussian. As Bernoulli random variables are \u03c32 = 1/4-sub-Gaussian (after centering), the algorithm is also applicable in our case.\nFor the convenience of the reader, we give the algorithm resulting from applying the reduction to Algorithm 1 of [18] in an explicit form. For specifying the algorithm we need some extra notation. Recall that given a USS instance \u03b8 = (P, c), we let \u03b3k = P ( Y 6= Y k ) where (Y, Y 1, . . . , Y K) \u223c P and k \u2208 [K]. Let k\u2217 = arg mink \u03b3k + Ck denote the optimal action and \u2206k(\u03b8) = \u03b3k + Ck \u2212 (\u03b3k\u2217 + Ck\u2217) the sub-optimality gap of arm k. Further, let \u2206\u2217(\u03b8) = min{\u2206k(\u03b8), k 6= k\u2217} denote the smallest positive sub-optimality gap and define \u2206\u2217k(\u03b8) = max{\u2206k(\u03b8),\u2206\u2217(\u03b8)}.\nSince cost vector c is fixed, in the following we use parameter \u03b3 in place of \u03b8 to denote the problem instance. A (fractional) allocation count u \u2208 [0,\u221e)K determines for each action i how many times the action is selected. Thanks to the cascade structure, using an action i implies observing the output of all the sensors with index j less than equal to i. Hence, a sensor j gets observed uj+uj+1 + \u00b7 \u00b7 \u00b7+uK times. We call an allocation count \u201csufficiently informative\u201d if (with some level of confidence) it holds that (i) for each suboptimal choice, the number of observations for the corresponding sensor is sufficiently large to distinguish it from the optimal choice; and (ii) the optimal choice is also distinguishable from the second best choice. We collect these counts into the set C(\u03b3) for a given parameter \u03b3: C(\u03b3) = {u \u2208 [0,\u221e)K : uj + uj+1 + \u00b7 \u00b7 \u00b7+ uK \u2265 2\u03c3 2\n(\u2206\u2217j (\u03b8)) 2 , j \u2208 [K]} (note that \u03c32 = 1/4).\nFurther, let u\u2217(\u03b3) be the allocation count that minimizes the total expected excess cost over the set of sufficiently informative allocation counts: In particular, we let u\u2217(\u03b3) = argminu\u2208C(\u03b3)\u3008u,\u2206(\u03b8)\u3009 with the understanding that for any optimal action k, u\u2217k(\u03b3) = min{uk : u \u2208 C(\u03b3)} (here, \u3008x, y\u3009 = \u2211 i xiyi is the standard inner product of vectors x, y). For an allocation count u \u2208 [0,\u221e)K let m(u) \u2208 NK denote total sensor observations, where mj(u) = \u2211j i=1 ui corresponds to observations of sensor j.\nAlgorithm 1 Algorithm for USS under SD property 1: Play action K and observe Y 1, . . . , Y K . 2: Set \u03b3\u03021i \u2190 I{Y 1 6=Y i} for all i \u2208 [K]. 3: Initialize the exploration count: ne \u2190 0. 4: Initialize the allocation counts: NK(1)\u2190 1.\n5: for t = 2, 3, ... do 6: if N(t\u22121)4\u03b1 log t \u2208 C(\u03b3\u0302\nt\u22121) then 7: Set It \u2190 argmink\u2208[K] c(k, \u03b3\u0302t\u22121). 8: else 9: if NK(t\u2212 1) < \u03b2(ne)/K then\n10: Set It = K. 11: else 12: Set It to some i for which Ni(t\u2212 1) < u\u2217i (\u03b3\u0302t\u22121)4\u03b1 log t. 13: end if 14: Increment exploration count: ne \u2190 ne+ 1. 15: end if 16: Play It and observe Y 1, . . . , Y It . 17: For i \u2208 [It], set\n\u03b3\u0302ti \u2190 (1\u2212 1t )\u03b3\u0302 t\u22121 i + 1 t I{Y 1 6=Y i}.\n18: end for\nThe idea of Algorithm 1 is as follows: The algorithm keeps track of an estimate \u03b3\u0302t := (\u03b3\u0302ti )i\u2208[K] of \u03b3 in each round, which is initialized by pulling arm K as this arm gives information about all the other arms. In each round, the algorithm first checks whether given the current estimate \u03b3\u0302t and the current confidence level (where the confidence level is gradually increased over time), the current allocation count N(t) \u2208 NK is sufficiently informative (cf. line 6). If this holds, the action that is optimal under \u03b3\u0302(t) is chosen (cf. line 7). If the check fails, we need to explore. The idea of the exploration is that it tries to ensure that the \u201coptimal plan\u201d \u2013 assuming \u03b3\u0302 is the \u201ccorrect\u201d parameter \u2013 is followed (line 12). However, this is only reasonable, if all components of \u03b3 are relatively well-estimated. Thus, first the algorithm checks whether any of the components of \u03b3 has a chance of being extremely poorly estimated (line 9). Note that the requirement here is that a significant, but still altogether diminishing fraction of the exploration rounds is spent on estimating each components: In the long run, the fraction of exploration rounds amongst all rounds itself is diminishing; hence the forced exploration of line 10 overall has a small impact on the regret, while it allows to stabilize the algorithm.\nFor \u03b8 \u2208 \u0398SD, let \u03b3(\u03b8) be the error probabilities for the various sensors. The following result follows from Theorem 6 of [18]:\nTheorem 4. Let > 0, \u03b1 > 2 arbitrary and choose any non-decreasing \u03b2(n) that satisfies 0 \u2264 \u03b2(n) \u2264 n/2 and \u03b2(m + n) \u2264 \u03b2(m) + \u03b2(n) for m,n \u2208 N. Then, for any \u03b8 \u2208 \u0398SD, letting \u03b3 = \u03b3(\u03b8)\nthe expected regret of Algorithm 1 after T steps satisfies\nRT (\u03b8) \u2264 ( 2K + 2 + 4K\n\u03b1\u2212 2\n) + 4K T\u2211 s=0 exp (\u22128\u03b2(s) 2 2K ) + 2\u03b2 ( 4\u03b1 log T\n\u2211 i\u2208[K] u\u2217i (\u03b3, ) +K ) + 4\u03b1 log T \u2211 i\u2208[K] u\u2217i (\u03b3, )\u2206i(\u03b8) ,\nwhere u\u2217i (\u03b3, ) = sup{u\u2217i (\u03b3\u2032) : \u2016\u03b3\u2032 \u2212 \u03b3\u2016\u221e \u2264 }.\nFurther specifying \u03b2(n) and using the continuity of u\u2217(\u00b7) at \u03b8, it immediately follows that Algorithm 1 achieves asymptotically optimal performance:\nCorollary 5. Suppose the conditions of Theorem 4 hold. Assume, furthermore, that \u03b2(n) satisfies \u03b2(n) = o(n) and \u2211\u221e s=0 exp ( \u2212\u03b2(s) 2 2K\u03c32 ) <\u221e for any > 0, then for any \u03b8 such that u\u2217(\u03b8) is unique, lim supT\u2192\u221eRT (\u03b8, c)/ log T \u2264 4\u03b1 infu\u2208C\u03b8\u3008u,\u2206(\u03b8)\u3009 .\nNote that any \u03b2(n) = anb with a \u2208 (0, 12 ], b \u2208 (0, 1) satisfies the requirements in Theorem 4 and Corollary 5.\nThe algorithm in 1 only estimates the disagreements P{Y 1 6= Y j} for all j \u2208 [K] which suffices to identify the optimal arm when SD property (see Sec 5) holds. Clearly, one can estimate pairwise disagreements probabilities P{Y i 6= Y j} for i 6= j and use them to order the arms. We next develop a heuristic algorithm that uses this information and works for USS under WD."}, {"heading": "6.1 Algorithm for Weak Dominance", "text": "The reduction scheme described above is optimal under SD property and can fail under the more relaxed WD property. This is because, under SD, marginal error is equal to marginal disagreement (see Prop. 2); it follows for any two sensors i < j, \u03b3i \u2212 \u03b3j = (\u03b3i \u2212 \u03b31)\u2212 (\u03b3j \u2212 \u03b31) = P ( Y i 6= Y 1 ) \u2212\nP ( Y i 6= Y 1 ) ; and so we can compute marginal error between any two sensors by only keeping track of disagreements relative to sensor 1. Under WD, marginal error is a lower bound and keeping track only of disagreements relative to sensor 1 no longer suffices because P ( Y i 6= Y 1 ) \u2212P ( Y i 6= Y 1 ) is no longer\na good estimate for P ( Y i 6= Y j ) . Our key insight is based on the fact that, under WD, for a given set of the disagreement probabilities, for all i 6= j, the set {i \u2208 [K\u22121] : Cj\u2212Ci \u2265 P{Y i 6= Y j} for all j > i} includes the optimal arm. We use this idea in Algorithm 2 to identify the optimal arm when an instance of USS satisfies WD. We will experimentally validate its performance on real datasets in the next section.\nAlgorithm 2 Algorithm for USS with WD property 1: Play action K and observe Y 1, . . . , Y K\n2: Set \u03b3\u03021ij \u2190 I{Y i 6=Y j} for all i, j \u2208 [K] and i < j. 3: ni(1)\u2190 I{i=K} \u2200i \u2208 [K]. 4: for t = 2, 3, ... do 5: U tij = \u03b3\u0302 t\u22121 ij + \u221a 1.5 log(t) nj(t\u22121) \u2200 i, j \u2208 [K] and i < j 6: St = {i \u2208 [K \u2212 1] : Cj \u2212Ci \u2265 U tij \u2200 j > i}\n7: Set It = arg minSt \u222a {K} 8: Play It and observe Y 1, . . . , Y It . 9: for i \u2208 [It] do\n10: ni(t)\u2190 ni(t\u2212 1) + 1 11: \u03b3\u0302tij \u2190 ( 1\u2212 1nj(t) ) \u03b3\u0302t\u22121ij +\n1 nj(t) I{Y j 6=Y i}\u2200 i < j \u2264 It 12: end for 13: end for\nThe algorithm works as follows. At each round, t, based on history, we keep track of estimates, \u03b3\u0302tij , of disagreements between sensor i and sensor j. In the first round, the algorithm plays arm K and initializes its values. In each subsequent round, the algorithm computes the upper confidence value of \u03b3\u0302tij denoted as U t ij (5) for all pairs (i, j) and orders the arms: i is considered better than arm j if Cj \u2212 Ci \u2265 U tij . Specifically, the algorithm plays an arm i that satisfies Cj \u2212 Ci \u2265 U tij for all j > i 6. If no such arm is found, then it plays arm K. nj(t), j \u2208 [K] counts the total number of observation of pairs (Y i, Y j), for all i < j, till round t and uses it to update the estimates \u03b3\u0302tij (11). Regret guarantees analogous to Thm 4 under SD for this new scheme can also be derived but requires additional conditions. Nevertheless, no such guarantee can be provided under WD because it is not uniformly learnable (Thm 3)."}, {"heading": "6.2 Extensions", "text": "We describe briefly a few extensions here, which we will elaborate on in a longer version of the paper. Tree Structures: We can deal with trees, where now the sensors are organized as a tree with root node corresponding to sensor 1 and we can select any path starting from the root node. To see this, note that the marginal error condition of Eqn. 1 still characterizes optimal sensor selection. Under a modified variant of SD, namely, Eq. 2 is in terms of the children of an optimal sensor, it follows that it is again sufficient to keep track of disagreements. In an analogous fashion Eq. ?? can be suitably modified as well. This leads to sublinear regret algorithm for tree-structures. Context-Dependent Sensor Selection: Each example has a context x \u2208 Rd and a test maps (partial) context to the output space. Analogous to the context-independent case we impose context dependent notions for SD and WD, namely, Eq. 2 and Eq. ?? hold conditioned on each x \u2208 X . To handle these cases we let \u03b3i(x) , Pr(Y i 6= Y | x) and \u03b3ij(x) , Pr(Y i 6= Y j | x) denote the corresponding\nFig. 3 depicts regret of USS (Alg. 1). Under SD it is always sublinear regardless of costs/probability. Fig. 4 demonstrates phase-transition effect for USS (Alg. 2). Alg. 1 is not plotted here because it fails in this case. As \u03c1\u2192 1 (from the right) regret-per-round drastically increases thus validating our theory that WD is a maximal learnable set.\ncontextual error and disagreement probabilities. Our sublinear regret guarantees can be generalized for a parameterized GLM model for disagreement, namely, when the log-odds ratio log \u03b3ij(x)1\u2212\u03b3ij(x) can be written as \u03b8\u2032ijx for some unknown \u03b8ij \u2208 Rd."}, {"heading": "7 Experiments", "text": "In this section we evaluate performance of Algorithms 1 and 2 on synthetic and real datasets (PIMA-Diabetes) and Heart Disease (Cleveland). Both of these datasets have accompanying costs for individual features.\nSynthetic: We generate data as follows. The input, Yt, is generated IID Ber(0.7). Outputs for channels 1, 2, 3 have an overall error statistic, \u03b31 = 0.4, \u03b32 = 0.1, \u03b33 = 0.05 respectively. To ensure SD we enforce Defn. 2 during the generation process. To relax SD requirement we introduce errors upto 10% during data generation for sensor outputs 2 and 3 when sensor 1 predicts correctly.\nReal Datasets: We split the features into three \u201csensors\u201d based on their costs. For PIMADiabetes dataset (size=768) the first sensor is associated with patient history/profile with cost ($6), the 2nd sensor in addition utilizes insulin test (cost $ 22) and the 3rd sensor uses all attributes (cost $46). For the Heart dataset (size=297) we use the first 7 attributes that includes cholestrol readings, blood-sugar, and rest-ECG (Cost $27), the 2nd sensor utilizes in addition thalach, exang, oldpeak attributes that cost $300 and the 3rd sensor utilizes more extensive tests at a total cost of $601.\nWe train three linear SVM classifiers with 5-fold cross-validation and have verified that our results match known state-of-art. Note that Table 4 shows that the resulting classifiers/tests on these datasets approximately satisfies SD condition and thus our setup should apply to these datasets. The size of these datasets is relatively small and limits our ability to experiment. To run the online algorithm we therefore generate an instance randomly from the dataset (with replacement) in each round. We repeat the experiments 20 times and averages are shown with 95% confidence bounds.\nTesting Learnability: We experiment with different USS algorithms on the synthetic dataset. Our purpose is twofold: (a) verify sublinear regret reduction scheme (Alg 1) under SD (b) verify that\n0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 time T\n0\n20\n40\n60\n80\n100\n120\n140\n160\ncu m\nul at\niv e\nre gr\net\nDiabetes\nunsupervised (high cost) supervised (high cost)\nFigure 5: Regret Curves on PIMA Diabetes 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 time T\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\ncu m\nul at\niv e\nre gr\net\nHeart\nunsupervised (low cost) supervised (low cost)\nFigure 6: Regret Curves on Heart Disease\nFig. 5 and Fig. 6 depict performance for real-datasets and presents comparisons against supervised (bandit with ground-truth feedback) scenario. Typically (but not in the worst-case (see Thm 5)), under WD, the unsupervised learning rate is within a constant factor of supervised setting.\nWD condition is a maximal learnable set. Fig. 3 depicts the results of Alg. 1 when SD condition is satisfied and shows that we obtain sublinear regret regardless of costs/probabilities. To test WD requirement we parameterize the problem by varying costs. Without loss of generality we fix the cost of sensor 1 to be zero and the total cost of the entire system to be Ctot. We then vary cost of sensors 2 and 3. We test the hypothesis that WD is a maximal learnable set. We enforce Sensor 2 as the optimal sensor and vary the costs so that we continuously pass from the situation where WD holds (\u03c1 \u2265 1) to the case where WD does not hold (\u03c1 < 1). Fig. 4 depicts regret-per-round for Alg. 2 and as we can verify there is indeed a transition at \u03c1 = 1 . Unsupervised vs. Supervised Learning: The real datasets provide an opportunity to understand how different types of information can impact performance. We compare USS algorithm (Alg. 2) against a corresponding bandit algorithm where the learner receives feedback. In particular, for each action in each round, in the bandit setting, the learner knows whether or not the corresponding sensor output is correct. We implement the supervised bandit setting by replacing Step 5 in Alg. 2 with estimated marginal error rates.\nWe scale costs by means of a tuning parameter (since the costs of features are all greater than one) and consider minimizing a combined objective (\u03bb Cost + Error) as stated in Sec. 2. High (low)-values for \u03bb correspond to low (high)-budget constraint. If we set a fixed budget of ($ 50), this corresponds to high-budget (small \u03bb) and low budget (large \u03bb) for PIMA Diabetes (3rd test optimal) and Heart Disease (1st test optimal) respectively. Figs 5 and 6 depicts performance for typical scenarios. We notice that for both high as well as low cost scenarios, while supervised does have lower regret, the USS cummulative regret is also sublinear and within a constant fraction of the supervised case. This is qualititively interesting because these plots demonstrate that (although under WD we do not have uniform learnability), in typical cases, we learn as well as the supervised setting."}, {"heading": "8 Conclusions", "text": "The paper describes a novel approach for unsupervised sensor selection. These types of problems arise in a number of healthcare and security applications. In these applications we have a collection of tests that are typically organized in a hierarchical architecture wherein test results are acquired in a sequential fashion. The goal is to identify the best balance between accuracy & test-costs. The main challenge in these applications is that ground-truth annotated examples are unavailable and it is often difficult to acquire them in-situ. We proposed a novel approach for sensor selection based on a novel notion of weak and strong dominance properties. We showed that weak dominance condition is maximal in that violation of this condition leads to loss of learnability. Our experiments demonstrate that weak dominance does hold in practice for real datasets and typically for these datasets, unsupervised selection can be as effective as a supervised (bandit) setting. Acknowledgements: The authors thank Dr. Joe Wang for helpful discussions and in particular suggesting the concept of strong dominance."}, {"heading": "9 Appendix", "text": ""}, {"heading": "Stochastic Partial Monitoring Problem 3", "text": "In an SPM a learner interacts with a stochastic environment in a sequential manner. In round t = 1, 2, . . . the learner chooses an action At from an action set A, and receives a feedback Yt \u2208 Y from a distribution p which depends on the action chosen and also on the environment instance identified with a \u201cparameter\u201d \u03b8 \u2208 \u0398: Yt \u223c p(\u00b7;At, \u03b8). The learner also incurs a reward Rt, which is a function of the action chosen and the unknown parameter \u03b8: Rt = r(At, \u03b8). The reward may or may not be part of the feedback for round t. The learner\u2019s goal is to maximize its total expected reward. The family of distributions (p(\u00b7; a, \u03b8))a,\u03b8 and the family of rewards (r(a, \u03b8))a,\u03b8 and the set of possible parameters \u0398 are known to the learner, who uses this knowledge to judiciously choose its next action to reduce its uncertainty about \u03b8 so that it is able to eventually converge on choosing only an optimal action a\u2217(\u03b8), achieving the best possible reward per round, r\u2217(\u03b8) = maxa\u2208A r(a, \u03b8). Bandit problems are a special case of SPMs where Y is the set of real numbers, r(a, \u03b8) is the mean of distribution p(\u00b7; a, \u03b8)."}, {"heading": "Proof of Proposition 1", "text": "Proof. \u21d2: Let A be an algorithm that achieves sublinear regret and pick an instance \u03b8 \u2208 \u0398. Let P = PS \u2297 PY |S . The regret Rn(A, \u03b8) of A on instance \u03b8 can be written in the form\nRn(A, \u03b8) = \u2211 k\u2208[K] EPS [Nk(n)] \u2206k(\u03b8) ,\nwhere Nk(n) is the number of times action k is chosen by A during the n rounds while A interacts with \u03b8, \u2206k(\u03b8) = c(k, \u03b8)\u2212 c\u2217(\u03b8) is the immediate regret and EPS [\u00b7] denotes the expectation under the distribution induced by PS . In particular, Nk(n) hides dependence on the iid sequence Y1, . . . , Yn \u223c PS that we are taking the expectation over here. Since the regret is sublinear, for any k suboptimal action, EPS [Nk(n)] = o(n). Define a(PS) = min{k \u2208 [K] ; EPS [Nk(n)] = \u2126(n) }. Then, a is well-defined as the distribution of Nk(n) for any k depends only on PS (and c). Furthermore, a(PS) selects an optimal action. \u21d0: Let a be the map in the statement and let f : N+ \u2192 N+ be such that 1 \u2264 f(n) \u2264 n for any n \u2208 N, f(n)/ log(n)\u2192\u221e as n\u2192\u221e and f(n)/n\u2192 0 as n\u2192\u221e (say, f(n) = d \u221a ne). Consider the algorithm that chooses It = K for the first f(n) steps, after which it estimates P\u0302S by frequency counting and then uses It = a(P\u0302S) in the remaining n \u2212 f(n) trials. Pick any \u03b8 \u2208 \u0398 so that \u03b8 = PS \u2297 PY |S . Note that by Hoeffding\u2019s inequality, supy\u2208{0,1}K |P\u0302S(y)\u2212 PS(y)| \u2264 \u221a K log(4n) 2f(n) holds\nwith probability 1\u2212 1/n. Let n0 be the first index such that for any n \u2265 n0, \u221a K log(4n) 2f(n) \u2264 \u2206 \u2217(\u03b8) . = mink:\u2206k(\u03b8)>0 \u2206k(\u03b8). Such an index n0 exists by our assumptions that f grows faster than n 7\u2192 log(n). For n \u2265 n0, the expected regret of A is at most n\u00d7 1/n+ f(n)(1\u2212 1/n) \u2264 1 + f(n) = o(n)."}, {"heading": "Proof of Corollary 2", "text": "Proof. By Proposition 5, awd is well-defined over \u0398WD, while by Proposition 6, awd is sound over \u0398WD."}, {"heading": "Proof of Proposition 2", "text": "Proof. We construct a map as required by Proposition 1. Take an instance \u03b8 \u2208 \u0398WD and let \u03b8 = PS \u2297 PY |S be its decomposition as defined above. Let \u03b3i = P ( Y i 6= Y ) , (Y, Y 1, . . . , Y K) \u223c \u03b8. For identifying an optimal action in \u03b8, it clearly suffices to know the sign of \u03b3i + Ci \u2212 (\u03b3j + Cj) for all pairs i, j \u2208 [K]2. Since Ci \u2212 Cj is known, it remains to study \u03b3i \u2212 \u03b3j . Without loss of generality (WLOG) let i < j. Then,\n0 \u2264 \u03b3i \u2212 \u03b3j = P ( Y i 6= Y ) \u2212 P ( Y j 6= Y ) =\n(((( (((\n(((P ( Y i 6= Y, Y i = Y j ) + P ( Y i 6= Y, Y i 6= Y j ) \u2212\n\u2212 { ((( (((( (((P ( Y j 6= Y, Y i = Y j ) + P ( Y j 6= Y, Y i 6= Y j )} = P ( Y i 6= Y, Y i 6= Y j ) + P ( Y i = Y, Y i 6= Y j\n) \u2212 { P ( Y j 6= Y, Y i 6= Y j ) + P ( Y i = Y, Y i 6= Y j\n)} (a) = P ( Y j 6= Y i ) \u2212 2P ( Y j 6= Y, Y i = Y ) ,\nwhere in (a) we used that P ( Y j 6= Y, Y i 6= Y j ) = P ( Y j 6= Y, Y i = Y ) and also P ( Y i = Y, Y i 6= Y j ) =\nP ( Y j 6= Y, Y i = Y ) which hold because Y, Y i, Y j only take on two possible values."}, {"heading": "Proof of Proposition 3", "text": "Proof. \u21d2: From the premise, it follows that Cj\u2212Ci \u2265 \u03b3i\u2212\u03b3j . Thus, by (3), Cj\u2212Ci \u2265 P ( Y i 6= Y j ) .\n\u21d0: We have Cj \u2212 Ci \u2265 P ( Y i 6= Y j ) \u2265 \u03b3i \u2212 \u03b3j , where the last inequality is by Corollary 1."}, {"heading": "Proof of proposition 4", "text": "Proof. \u21d2: The condition \u03b3i + Ci \u2264 \u03b3j + Cj implies that \u03b3j \u2212 \u03b3i \u2265 Ci \u2212 Cj . By Corollary 1 we get P ( Y i 6= Y j ) \u2265 Ci \u2212 Cj . \u21d0: Let Ci \u2212 Cj \u2264 P ( Y i 6= Y j ) . Then, by (4), Ci \u2212 Cj \u2264 \u03b3j \u2212 \u03b3i."}, {"heading": "Proof of Theorem ??", "text": "Proof. Let a as in the theorem statement. By Proposition 8, awd is the unique sound action-selector map over \u0398WD. Thus, for any \u03b8 = PS \u2297 PY |S \u2208 \u0398WD, awd(PS) = a(PS). Hence, the result follows from Corollary 3."}, {"heading": "Proof of Proposition 5", "text": "Proof. Let \u03b8 \u2208 \u0398WD, i = a\u2217(\u03b8). Obviously, (7b) holds by the definition of \u0398WD. Thus, the only question is whether (7a) also holds. We prove this by contradiction: Thus, assume that (7a) does not hold, i.e., for some j < i, Ci \u2212 Cj \u2265 P ( Y i 6= Y j ) . Then, by Corollary 1, P ( Y i 6= Y j ) \u2265 \u03b3j \u2212 \u03b3i, hence \u03b3j + Cj \u2264 \u03b3i + Ci, which contradicts the definition of i, thus finishing the proof."}, {"heading": "Proof of Proposition 6", "text": "Proof. Take any \u03b8 \u2208 \u0398WD with \u03b8 = PS \u2297 PY |S , i = awd(PS), j = a\u2217(\u03b8). If i = j, there is nothing to be proven. Hence, first assume that j > i. Then, by (7b), Cj \u2212 Ci \u2265 P ( Y i 6= Y j ) . By Corollary 1,\nP ( Y i 6= Y j ) \u2265 \u03b3i \u2212 \u03b3j . Combining these two inequalities we get that \u03b3i + Ci \u2264 \u03b3j + Cj , which\ncontradicts with the definition of j. Now, assume that j < i. Then, by (5), Ci \u2212 Cj \u2265 P ( Y i 6= Y j ) .\nHowever, by (7a), Ci\u2212Cj < P ( Y i 6= Y j ) , thus j < i cannot hold either and we must have i = j."}, {"heading": "Proof of Proposition 8", "text": "Proof. Pick any \u03b8 = PS \u2297 PY |S \u2208 \u0398WD. If A\u2217(\u03b8) is a singleton, then clearly a(PS) = awd(PS) since both are sound over \u0398WD. Hence, assume that A\u2217(\u03b8) is not a singleton. Let i = a\u2217(\u03b8) = minA\u2217(\u03b8) and let j = minA\u2217(\u03b8) \\ {i}. We argue that PY |S can be changed so that on the new instance i is still an optimal action, while j is not an optimal action, while the new instance \u03b8\u2032 = PS \u2297 P \u2032Y |S is in \u0398WD.\nThe modification is as follows: Consider any y\u2212j .= (y1, . . . , yj\u22121, yj+1, . . . , yK) \u2208 {0, 1}K\u22121. For y, yj \u2208 {0, 1}, define q(y|yj) = PY |S(y|y1, . . . , yj\u22121, yj , yj+1, . . . , yK) and similarly let q\u2032(y|yj) = P \u2032Y |S(y|y\n1, . . . , yj\u22121, yj , yj+1, . . . , yK) Then, we let q\u2032(0|0) = 0 and q\u2032(0|1) = q(0|0) + q(0|1), while we let q\u2032(1|1) = 0 and q\u2032(1|0) = q(1|1) + q(1|0). This makes P \u2032Y |S well-defined (P \u2032 Y |S(\u00b7|y\n1, . . . , yK) is a distribution for any y1, . . . , yK). Further, we claim that the transformation has the property that it\nleaves \u03b3p unchanged for p 6= j, while \u03b3j is guaranteed to decrease. To see why \u03b3p is left unchanged for p 6= j note that \u03b3p = \u2211 yp PY p(y\np)PY |Y p(1\u2212 yp|yp). Clearly, PY p is left unchanged. Introducing y\u2212k to denote a tuple where the kth component is left out, PY |Y p(1\u2212 yp|yp) = \u2211 y\u2212p,\u2212j PY |Y 1,...,Y K (1\u2212 yp|y1, . . . , yj\u22121, 0, yj+1, . . . , yK) + PY |Y 1,...,Y K (1\u2212 yp|y1, . . . , yj\u22121, 1, yj+1, . . . , yK) and by definition,\nPY |Y 1,...,Y K (1\u2212 yp|y1, . . . , yj\u22121, 0, yj+1, . . . , yK) + PY |Y 1,...,Y K (1\u2212 yp|y1, . . . , yj\u22121, 1, yj+1, . . . , yK)\n= P \u2032Y |Y 1,...,Y K (1\u2212 y p|y1, . . . , yj\u22121, 0, yj+1, . . . , yK)\n+ P \u2032Y |Y 1,...,Y K (1\u2212 y p|y1, . . . , yj\u22121, 1, yj+1, . . . , yK) ,\nwhere the equality holds because \u201cq\u2032(y|0) + q\u2032(y|1) = q(y|0) + q(y|1)\u201d. Thus, PY |Y p(1 \u2212 yp|yp) = P \u2032Y |Y p(1\u2212 y\np|yp) as claimed. That \u03b3j is non-increasing follows with an analogue calculation. In fact, this shows that \u03b3j is strictly decreased if for any (y1, . . . , yj\u22121, yj+1, . . . , yK) \u2208 {0, 1}K\u22121, either q(0|0) or q(1|1) was positive. If these are never positive, this means that \u03b3j = 1. But then j cannot be optimal since cj > 0. Since j was optimal, \u03b3j is guaranteed to decrease.\nFinally, it is clear that the new instance is still in \u0398WD since a\u2217(\u03b8) is left unchanged."}, {"heading": "Proof of Theorem 1", "text": "Proof of Theorem 1. We construct a map as required by Proposition 1. Take an instance \u03b8 \u2208 \u0398SD and let \u03b8 = PS \u2297 PY |S be its decomposition as before. Let \u03b3i = P ( Y i 6= Y ) , (Y, Y 1, . . . , Y K) \u223c \u03b8, Ci = c1 + \u00b7 \u00b7 \u00b7 + ci. For identifying an optimal action in \u03b8, it clearly suffices to know the sign of \u03b3i +Ci\u2212 (\u03b3j +Cj) = \u03b3i\u2212 \u03b3j + (Ci\u2212Cj) for all pairs i, j \u2208 [K]2. Without loss of generality (WLOG) let i < j. By Proposition 2, \u03b3i \u2212 \u03b3j = P ( Y i 6= Y j ) \u2212 2P ( Y j 6= Y, Y i = Y ) . Now, since \u03b8 satisfies\nthe strong dominance condition, P ( Y j 6= Y, Y i = Y ) = 0. Thus, \u03b3i \u2212 \u03b3j = P ( Y i 6= Y j ) which is a function of PS only. Since (Ci)i are known, a map as required by Proposition 1 exists."}, {"heading": "Proof of Theorem 3", "text": "Proof. We first consider the case when K = 2 and arbitrarily choose C2\u2212C1 = 1/4. We will consider two instances, \u03b8, \u03b8\u2032 \u2208 \u0398WD such that for instance \u03b8, action k = 1 is optimal with an action gap of c(2, \u03b8)\u2212 c(1, \u03b8) = 1/4 between the cost of the second and the first action, while for instance \u03b8\u2032, k = 2 is the optimal action and the action gap is c(1, \u03b8)\u2212 c(2, \u03b8) = where 0 < < 3/8. Further, the entries in PS(\u03b8) and PS(\u03b8\u2032) differ by at most . From this, a standard reasoning gives that no algorithm can achieve sublinear minimax regret over \u0398WD because any algorithm is only able to identify PS .\nThe constructions of \u03b8 and \u03b8\u2032 are shown in Table 2: The entry in a cell gives the probability of the event as specified by the column and row labels. For example, in instance \u03b8, 3/8 is the probability of Y = Y 1 = Y 2, while the probability of Y 1 = Y 6= Y 2 is 1/8. Note that the cells with zero actually correspond to impossible events, i.e., these cannot be assigned a positive probability. The rationale of a redundant (and hence sparse) table is so that probabilities of certain events of interest, such as Y 1 6= Y 2 are easier to determine based on the table. The reader should also verify that the positive probabilities correspond to events that are possible.\nWe need to verify the following: (i) \u03b8, \u03b8\u2032 \u2208 \u0398WD; (ii) the optimality of the respective actions in the respective instances; (iii) the claim concerning the size of the action gaps; (iv) that PS(\u03b8) and PS(\u03b8\u2032) are close. Details of the calculations to support (i)\u2013(iii) can be found in Table 3. The row marked by (\u2217) supports that the instances are proper USS instances. In the row marked by (\u2217\u2217), there is no requirement for \u03b8\u2032 because in \u03b8\u2032 action two is optimal, and hence there is no action with larger index than the optimal action, hence \u03b8\u2032 \u2208 \u0398WD automatically holds. To verify the closeness of PS(\u03b8) and PS(\u03b8\u2032) we actually would need to first specify PS (the tables do not fully specify these). However, it is clear the only restriction we put on PS is the value of P ( Y 1 6= Y 2\n) (and that of P ( Y 1 = Y 2 ) ) and these values are within an distance of each other. Hence, PS can also be specified to satisfy this. In particular, one possibility for P and PS are given in Table 4."}, {"heading": "Proof of Proposition 9", "text": "Proof. First note that the mapping of the policies is such that number of pull of arm k after n rounds by policy \u03c0 on problem instance f(\u03b8) is the same as the number of pulls of arm k by \u03c0\u2032 on\nproblem instance \u03b8. Recall that mean value of arm k in problem instance \u03b8 is\u03b3k + Ck and that of corresponding arm in problem instance f(\u03b8) is \u03b31 \u2212 (\u03b3i + Ci). We have\nRn(\u03c0 \u2032, \u03b8) = \u2211 k\u2208[K] EPS [Nk(n)] (\u03b3k + Ck \u2212 \u03b3k\u2217 \u2212 Ck\u2217) ,\nand\nRn(\u03c0, f(\u03b8)) = \u2211 k\u2208[K] EPS [Nk(n)] ( max i\u2208[K] {\u03b31 \u2212 \u03b3i \u2212 Ci} \u2212 (\u03b31 \u2212 \u03b3k \u2212 Ck) )\n= \u2211 k\u2208[K] EPS [Nk(n)] ( \u03b3k + Ck \u2212 min i\u2208[K] {\u03b3i + Ci} ) = Rn(\u03c0 \u2032, \u03b8)."}], "references": [{"title": "Multi-stage classifier design", "author": ["K. Trapeznikov", "V. Saligrama", "D.A. Castanon"], "venue": "Machine Learning, vol. 39, pp. 1\u201324, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Partial monitoring \u2013 classification, regret bounds, and algorithms", "author": ["G. Bart\u00f3k", "D. Foster", "D. P\u00e1l", "A. Rakhlin", "C. Szepesv\u00e1ri"], "venue": "Mathematics of Operations Research, vol. 39, pp. 967\u2013997, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W. Thompson"], "venue": "Biometrika, vol. 25, pp. 285\u2013294, 1933.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1933}, {"title": "Supervised sequential classification under budget constraints", "author": ["K. Trapeznikov", "V. Saligrama"], "venue": "AISTATS, 2013, pp. 235\u2013242.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Directed acyclic graph for resource constrained prediction", "author": ["J. Wang", "K. Trapeznikov", "V. Saligrama"], "venue": "Proceeding of Conference on Neural Information Processing Systems, NIPS, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Feature-budgeted random forest", "author": ["F. Nan", "J. Wang", "V. Saligrama"], "venue": "Proceedings of the International Conference on Machine Learning, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning when to stop thinking and do something!", "author": ["B. P\u00f3czos", "Y. Abbasi-Yadkori", "C. Szepesv\u00e1ri", "R. Greiner", "N. Sturtevant"], "venue": "in ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Learning cost-sensitive active classifiers", "author": ["R. Greiner", "A. Grove", "D. Roth"], "venue": "Artificial Intelligence, vol. 139, pp. 137\u2013174, 2002.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning and classifying under hard budgets", "author": ["A. Kapoor", "R. Greiner"], "venue": "ECML, 2005.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Adore: Adaptive object recognition", "author": ["B. Draper", "J. Bins", "K. Baek"], "venue": "International Conference on Vision Systems, 1999, pp. 522\u2013537.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1999}, {"title": "Efficient interpretation policies", "author": ["R. Isukapalli", "R. Greiner"], "venue": "International Joint Conference on Artificial Intelligence, 2001, pp. 1381\u20131387. 16", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Prediction with limited advice and multiarmed bandits with paid observations", "author": ["Y. Seldin", "P. Bartlett", "K. Crammer", "Y. Abbasi-Yadkori"], "venue": "Proceeding of International Conference on Machine Learning, ICML, 2014, pp. 208\u2013287.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Online learning with costly features and labels", "author": ["N. Zolghadr", "G. Bart\u00f3k", "R. Greiner", "A. Gy\u00f6rgy", "C. Szepesv\u00e1ri"], "venue": "NIPS, 2013, pp. 1241\u20131249.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Asymptotically efficient adaptive allocation schemes for controlled i.i.d. processes: Finite parameter space", "author": ["R. Agrawal", "D. Teneketzis", "V. Anantharam"], "venue": "IEEE Transaction on Automatic Control, vol. 34, pp. 258\u2013267, 1989.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1989}, {"title": "From bandits to experts: On the value of side-observations", "author": ["S. Mannor", "O. Shamir"], "venue": "NIPS, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Online learning with feedback graphs:beyond bandits", "author": ["N. Alon", "N. Cesa-Biancbi", "O. Dekel", "T. Koren"], "venue": "Proceeding of Conference on Learning Theory, 2015, pp. 23\u201335.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "From bandits to experts: A tale of domination and independence", "author": ["N. Alon", "N. Cesa-Biancbi", "C. Gentile", "Y. Mansour"], "venue": "Proceeding of Conference on Neural Information Processing Systems, NIPS, 2013, pp. 1610\u20131618.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Online learning with gaussian payoffs and side observations", "author": ["Y. Wu", "A. Gy\u00f6rgy", "C. Szepesv\u00e1ri"], "venue": "NIPS, September 2015, pp. 1360\u20131368.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "A number of different imaging and non-imaging tests are sequentially processed (see [1]).", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "3 we pose our problem as a version of stochastic partial monitoring problem [2] with atypical reward structure, where tests are viewed as actions and sequential observations serves as side information.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "Bandit problems [3] are a special case of partial monitoring, where the key missing information is the expected cost for each action, and the feedback is the noisy version of the expected cost of the action chosen.", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "In contrast to our USS setup there exists a wide body of literature dealing with sensor acquisition (see[4, 5, 6]).", "startOffset": 104, "endOffset": 113}, {"referenceID": 4, "context": "In contrast to our USS setup there exists a wide body of literature dealing with sensor acquisition (see[4, 5, 6]).", "startOffset": 104, "endOffset": 113}, {"referenceID": 5, "context": "In contrast to our USS setup there exists a wide body of literature dealing with sensor acquisition (see[4, 5, 6]).", "startOffset": 104, "endOffset": 113}, {"referenceID": 11, "context": "There are also several methods that work in an online bandit setting and train prediction models with feature costs [12] but again they require ground-truth labels as reward-feedback.", "startOffset": 116, "endOffset": 120}, {"referenceID": 11, "context": "A somewhat more general version of [12] is developed in [13] where in addition the learner can choose to acquire ground-truth labels for a cost.", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "A somewhat more general version of [12] is developed in [13] where in addition the learner can choose to acquire ground-truth labels for a cost.", "startOffset": 56, "endOffset": 60}, {"referenceID": 6, "context": "Our paper bears some similarity with the concept of active classification, which deals with learning stopping policies[7, 8] among a given sequence of tests.", "startOffset": 118, "endOffset": 124}, {"referenceID": 7, "context": "Our paper bears some similarity with the concept of active classification, which deals with learning stopping policies[7, 8] among a given sequence of tests.", "startOffset": 118, "endOffset": 124}, {"referenceID": 1, "context": "Our paper is related to the framework of finite partial monitoring problems[2], which deals with how to infer unknown key information and where tests/actions reveal different types of information about the unknown information.", "startOffset": 75, "endOffset": 78}, {"referenceID": 13, "context": "In this context [14] consider special cases where payoff/rewards for a subset of actions are observed.", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "This is further studied as a side-observation problem in [15] and as graph-structured feedback [16, 17, 18]).", "startOffset": 57, "endOffset": 61}, {"referenceID": 15, "context": "This is further studied as a side-observation problem in [15] and as graph-structured feedback [16, 17, 18]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 16, "context": "This is further studied as a side-observation problem in [15] and as graph-structured feedback [16, 17, 18]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 17, "context": "This is further studied as a side-observation problem in [15] and as graph-structured feedback [16, 17, 18]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 14, "context": "A finite armed bandit with side-observations [15] is also a special case of SPMs, where the learner upon choosing an action a \u2208 A receives noisy reward observations, namely, Yt = (Yt,a)a\u2208N(At), Yt,a \u223c pr(\u00b7; a, \u03b8), E [Yt,a] = r(a, \u03b8), from a neighbor-set N (a) \u2282 A, which is a priori known to the learner.", "startOffset": 45, "endOffset": 49}, {"referenceID": 17, "context": "In this paper we make use of Algorithm 1 of [18] that was proposed for stochastic bandits with Gaussian side observations.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "For the convenience of the reader, we give the algorithm resulting from applying the reduction to Algorithm 1 of [18] in an explicit form.", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "The following result follows from Theorem 6 of [18]:", "startOffset": 47, "endOffset": 51}], "year": 2016, "abstractText": "In many security and healthcare systems a sequence of features/sensors/tests are used for detection and diagnosis. Each test outputs a prediction of the latent state, and carries with it inherent costs. Our objective is to learn strategies for selecting tests to optimize accuracy & costs. Unfortunately it is often impossible to acquire-in-situ ground truth annotations and we are left with the problem of unsupervised sensor selection (USS). We pose USS as a version of stochastic partial monitoring problem with an unusual reward structure (even noisy annotations are unavailable). Unsurprisingly no learner can achieve sublinear regret without further assumptions. To this end we propose the notion of weak-dominance. This is a condition on the joint probability distribution of test outputs and latent state and says that whenever a test is accurate on an example, a later test in the sequence is likely to be accurate as well.We empirically verify that weak dominance holds on real datasets and prove that it is a maximal condition for achieving sublinear regret. We reduce USS to a special case of multi-armed bandit problem with side information and develop polynomial time algorithms that achieve sublinear regret.", "creator": "LaTeX with hyperref package"}}}