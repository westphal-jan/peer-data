{"id": "1610.05710", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Feasibility Based-Large Margin Nearest Neighbor Metric Learning", "abstract": "In the area of data classification, one of the prominent algorithms is the large margin nearest neighbor (LMNN) approach which is a metric learning to enhance the performance of the popular k-nearest neighbor classifier. In principles, LMNN learns a more efficient metric in the input space by using a linear mapping as the outcome of a convex optimization problem. However, one of the greatest weak points of LMNN is the strong reliance of its optimization paradigm on how the neighboring points are chosen. In this paper, it is mathematically proved for the first time that the regular way of choosing the target points can lead to non-feasible optimization conditions regardless of the number of chosen neighboring points. We present a mathematical approach to categorize the target points into feasible and infeasible exemplars, an also we provide a feasibility measure for preference of the target candidates. In our proposed Feasibility Based-LMNN algorithm, we use the above clue to construct the optimization problem based on the most promising general mapping directions in the input space. Our empirical results shows that via using the proposed FB-LMNN approach the optimization problem will converge in a better optimum point, and therefor leads to better classification on the well-known benchmark datasets. The general distribution can be seen as the one that provides the most effective statistical results.\n\n\n\nThe above figure shows the top-to-bottom-right data of the input space. The top-to-bottom-right figure shows the top-to-bottom-right data of the input space. The top-to-bottom-right figure shows the top-to-bottom-right data of the input space. The top-to-bottom-right figure shows the top-to-bottom-right data of the input space. The top-to-bottom-right figure shows the top-to-bottom-right data of the input space. The top-to-bottom-right figure shows the top-to-bottom-right data of the input space. The top-to-bottom-right figure shows the top-to-bottom-right data of the input space. The top-to-bottom-right figure shows the top-to-bottom-right data of the input space. The top-to-bottom-right figure shows the top-to-bottom-right data of the input space. The top-to-bottom-right figure shows the top-to-bottom-right data of the input space. The top-to-bottom-right", "histories": [["v1", "Tue, 18 Oct 2016 17:06:26 GMT  (1717kb,D)", "http://arxiv.org/abs/1610.05710v1", "This is the preprint of a submitted conference paper as provided by the authors"]], "COMMENTS": "This is the preprint of a submitted conference paper as provided by the authors", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["babak hosseini", "barbara hammer"], "accepted": false, "id": "1610.05710"}, "pdf": {"name": "1610.05710.pdf", "metadata": {"source": "META", "title": "Feasibility Based-Large Margin Nearest Neighbor Metric Learning", "authors": ["Babak Hosseini"], "emails": ["bhammer@techfak.uni-bielefeld.de"], "sections": [{"heading": null, "text": "Keywords: Large margin nearest neighbor, Feasibility measure, Convex Optimization, K-nearest neighbor classifie."}, {"heading": "1 Introduction", "text": "Metric learning as widely used approach in the area of discriminative data mining is relted to finding a suitable metric for the given data in order to enhance the processing of specific dataset and mostly to improve the classification accuracy. In basic terms it tries to make similar class data closer and distance of them to other classes farther. One of the well-known methods for metric learning is the large margin nearest neighbor (LMNN) which is fundamentally destined to increase the performance of k-nearest neighbor classifier[1, 2, 3], by transferring the maximum margin concept of SVM classifier to the kNN framework. LMNN\nar X\niv :1\n61 0.\n05 71\n0v 1\n[ cs\n.D S]\n1 8\nwas used in many real problems such as face recognition [4], motion classification [5] and pedestrian identification [6]. Furthermore, this adaptation of metric in this manner also enhances the model interpretability along with increasing the accuracy, and in some domain it is used for easier visualization of data [7, 8].\nThere has been several attempts to improve the performance of the original LMNN approach, such as complexity reduction of the optmization framework [9], eigenvalue based optimization [10], its extension to the multi-taksing problem [11] and a hierarchical prepossessing of data [12] all of which have interesting results. However there are still some open issues in the fundamental parts of this metric learning algorithm which hasn\u2019t been addressed yet.\nTo our knowledge, no one yet studied the idea of selecting the target points in a way rather than the sorted order of distances. One of the weak points of this approach is the fact that it always includes the nearest neighbors based on the current metric even if the size of the neighborhood is increased. And we believe this fact can decrease the algorithm performance in specific situations. Based on that perspective, in this paper we introduced a new insight into the optimization algorithm of LMNN approach and proposed a novel technique in order to enhance the selection process of neighborhood points based on some feasibility criteria. In the next sections we will explain the main concept of LMNN, then we talk about the feasibility measure to that problem and at the end we evaluate our method on real and artificial datasets."}, {"heading": "2 Large Margin Nearest Neighbor Algorithm", "text": "Large margin nearest neighbor algorithm is a metric learning algorithm which is design to enhance the performance of nearest neighbor classifiers such as knearest neighbors (kNN). For set of labeled data vectors ~xi \u2208 Rn, i = 1, . . . ,m LMNN tries to find a quadratic metric which can increase the accuracy of kNN classifier. As the performance of kNN is highly depended on the distance of data points to their nearby neighbors, LMNN learns an optimized metric as\nD(~xi, ~xj) = (L(~xi \u2212 ~xj))2 = (~xi \u2212 ~xj)>L>L(~xi \u2212 ~xj) (1)\nin order to improve the neighborhood structure of data in a robust way. From another aspect of view, matrix L is a linear mapping with the quadratic form of M := L>L. LMNN algorithm\u2019s objective function is formed based on the k-nearest data samples ~xj (targets) to each data points ~xi having the same class labels. These sets of data which we note them as N ki will be encoded in the optimization problem using \u03b7ij = 1 in case ~xj \u2208 N ki . The cost function of LMNN is formed based on minimizing the distance to the nearby same class data samples and pushing away data samples from data point of other classes which are known as imposters:\n(L) := \u2211 ij \u03b7ijD(~xi, ~xj) +m \u2211 ijl \u03b7ij(1\u2212 \u03b4il)\u00b7\n[c+D(~xi, ~xj)\u2212D(~xi, ~xl)]+ (2)\nin which [\u00b7]+ is the Hinge loss function and c > 0 is a margin parameter, so that in the ideal case, all the imposters are pushed behind the margin of nearest\nsame label neighbors. As a notation, we consider the set of imposters points for ~xi with target neighbor ~xj as\nImji = {~xl : D(~xi, ~xl) < c+D(~xi, ~xj), c > 0, ~xj \u2208 N k i }\nThere is another meta parameter in (2) as m which decides between the ratio of pull and push forces in the algorithm. According to [3] the optimization problem is a category of semidefinite programming as:\nmin \u2211 ij D(~xi, ~xj) +m \u2211 ijl \u03b7ij(1\u2212 \u03b4il)\u03beijl where D(~xi, ~xl)\u2212D(~xi, ~xj) \u2265 c\u2212 \u03beijl \u03beijl \u2265 0 M \u2265 0\n(3)\nthe parameter \u03b4il = 1 if i, l have same labels, so the second part of the optimization function will be considered only for ~xl that have different labels than ~xi. Based on the convexity of the problem, it is possible to optimize it with respect to a positive semidefinite matrix M."}, {"heading": "3 Feasibility of target neighborhoods", "text": "As explained in section 2 the first step in the LMNN approach is the selection of target neighbors ~xj for each input ~xi prior to solving the optimization problem (3). And the convexity of this optimization problem promises its convergence to a global minimum point M\u2217 for the objective function; However the optimality of this point is respect to the selected set of all chosen target points N k = {N ki ,\u2200i = 1, ..., N} which is done based on the initial metric (L). Therefore, in many real problems it is possible to find a different set S \u2032 based on which the optimization problem can reach another optimum point M \u2032\nwhich leads to a lower objective value. In other words, the global optimum point to (3) can be a local optimum point for the general optimization problem\nmin M,\u03b7\n\u2211 ij \u03b7ijD(~xi, ~xj) +m \u2211 ijl \u03b7ij(1\u2212 \u03b4il)\u03beijl\ns.t. D(~xi, ~xl)\u2212D(~xi, ~xj) \u2265 c\u2212 \u03beijl \u03beijl \u2265 0 M 0\n(4)\nwhich takes into account also the parameter \u03b7 or in other words, considering different possibilities for the target neighbors N ki for a given k. The optimization problem (4) is considered a NP-hard problem Which is computationally difficult to solve. One way to get close to the global optimum of problem is to use the multiple pass technique suggested by [3], which tries to update the targets neighbors N k based on the learned metric L after the convergence of the optimization problem (3). Then repeat the optimization again based on the updated \u03b7ij and the last value of L, and repeat these iterations until there will be no further change in N k. It is proved in [13] that in each iteration t of multiple pass LMNN, the optimization problem will converge to a better or equal optimal pointLt comparing to Lt. This technique tries to push the algorithm towards a the global optimum direction, however it cannot promise the\nconvergence to the global optimum because it is still depended to the initial and following choices of N k in the optimization process.\nFurthermore, There is another major weak-point in the main principles of LMNN which to our knowledge has not been addressed in the literature by anyone else. Based on the given backgrounds in section 2, the algorithm selects the target neighbors S = {N ki , } in the order of their distance from ~xi. Meaning that the N 1i \u2282 N 2i \u2282 ... \u2282 N ki which we see it as a general restriction in the optimization problem. Relevantly, It can be directly concluded from the (4) that solving the main optimization problem needs having more flexibility in choosing the target neighbors.\nFor example, consider a sample arrangement of data points in a 2-D space as in Fig. 1 in which ~xi is the main data point and { ~xj , j = 1, ..., 3} are the candidate targets and ~x4 is a nearby data from other classes which are called imposters. As you can see, if we choose ~x1 as the target point for ~xi based on their small distance, consequently ~x4 will be selected as the relevant imposter and the algorithm tries to send ~x4 beyond ~xi using a linear mapping L. At the same time due to the first part of the objective function in (3), it tries to bring { ~xj , j = 1, ..., 3} closer to ~xi. However as the triple (i,1,4) are positioned on a straight line, they will still remain on a line after applying any linear transform L, and also the ratio of their distances will be preserved, because L is a non-singular affine transform. Or in mathematical terms, because (~x1 \u2212 ~xi) = \u03b1(~x4 \u2212 ~xi), \u03b1 > 1, under the linear transform L we will have (L~x1 \u2212 L~xi) = \u03b1(L~x4 \u2212 L~xi) or (~x \u2032 1 \u2212 ~x \u2032 i) = \u03b1(~x \u2032 4 \u2212 ~x \u2032\ni) which means the new points are in the same relational positions as before (1.b). Even through out increasing the neighborhood size k and adding more targets like ~x2 and ~x3 to N ki , the optimization problem still tries to bring ~x1 closer to ~xi which inevitably brings the imposter ~x4 closer too, and ending in a weak optimum point in the scope of (4). In more general terms, neighboring points like ~x1 are illegal targets for ~xi due to the infeasibility of the relevant triples, and having such infeasible triples in our optimization framework will mislead the algorithm to local minimums in the scope of the main optimization problem. Although the algorithm might reduce the objective function by bringing same class data closer to each others like Fig. 1.b, but based on kNN classifier principles ~x4 will be wrongly chosen as the same label sample for the test data ~xi.\nIn order to address this problem mathematically we will refer to the relaxation part in (3) which tries to make\nD(~xi, ~xj) + c \u2264 D(~xi, ~xl) (5)\nhowever in cases like the example in Fig. 1 for the triples (i, 1, 4) the inequality (5) is not feasible for any linear mapping L as the solution. Hence, we are interested in defining a criteria in order to mathematically check the feasibility of the above inequality.\nTheorem 1 A triple set of (i, j, l) will be infeasible for the LMNN optimization \u21d0\u21d2 the angle between (~xi \u2212 ~xj) and (~xi \u2212 ~xl) is zero.\nProof 1 Assuming that (~xi \u2212 ~xj) > (~xi \u2212 ~xl) and we want (5) to be feasible. As by the definition, c \u2265 0, so it is required to have D(~xi, ~xj) \u2264 D(~xi, ~xl), which means\n(~xi \u2212 ~xj)TM(~xi \u2212 ~xj)\u2212 (~xi \u2212 ~xl)TM(~xi \u2212 ~xl) \u2264 0\nor Tr{((~xi \u2212 ~xj)(~xi \u2212 ~xj)T \u2212 (~xi \u2212 ~xl)(~xi \u2212 ~xl)T )M} \u2264 0 (6)\nlet\u2019s call it Tr{QM} \u2264 0. Based on the semidefinite structure of the optimization problem, M is a positive semidefinite matrix. Then if matrix Q happens to be positive semidefinite too, the resulting matrix QM will be positive semidefinite as well as having positive eigenvalues. Therefore, Tr{QM} > 0 which means that (5) becomes infeasible.\nFor simplicity we consider Q = ~a~aT \u2212~b~bT , meaning that Q \u2282 span{~a,~b} and follows the fact that Q has at most two non-zero eigenvalues. If we assume ~a,~b linearly independent, then the restriction matrix of Q to U = span{~a,~b} would be [\n~a \u00b7 ~a ~a \u00b7~b \u2212~a \u00b7~b \u2212~b \u00b7 ~a ] and its determinant or the multiplication the eigenvalues would be\n\u2212\u2016~a\u20162\u2016~b\u20162 + (~a \u00b7~b)2 = \u2212\u2016~a\u20162\u2016~b\u20162 sin2 \u03b8\nin which \u03b8 is the angle between the two vectors ~a and ~b. The determinant is always negative, unless \u03b8 = 0, which in that case Q = (\u03b12\u22121)~b~bT as ~a = \u03b1~b, \u03b1 > 1 by the first assumption. As a result Q would be a positive semidefinite matrix with one positive eigenvalue accordingly; Consequently (6) and (5) become infeasible.\nBased on the above demonstrations and the support of theorem 1, it is possible that exist infeasible target set\nFki = {~xj : ~xj \u2208 N ki ,D(~xi, ~xj) > D(~xi, ~xl),\u2200M 0}\nin the k vicinity of ~xi which can influence the optimization problem dramatically. Another important fact is that with normal selection of target neighbors N ki based on their sorted distance to ~xi, infeasible points Fki \u2208 N k \u2032 i for any k \u2032 > k, so increasing the neighborhood size cannot guarantee to eliminate the role of infeasible targets.\nIn the next section we will present an effective approach to benefit from proposed concept of feasibility check in order to enhance the optimization algorithm."}, {"heading": "4 Feasibility Based Large Margin Nearest Neighbor", "text": "Based on the discussion in the previous section, Fk is set of (~xi,~xj) which are infeasible based on the optimization constraint and we see them as the singular conditions for the optimization algorithm. One way to prevent the infeasible target set Fk from influencing the optimization problem (4) is to simply rule them out prior to the start of the optimization algorithm as a pre-processing step. To do so, we check the relevant nearby imposters ~xl for each pair of (~xi,~xj) and in case of existing any infeasible triples in combination with (~xi,~xj) will be eliminated from N ki by making \u03b7ij = 0 in (3) otherwise Wij = 1. Then we search for the next closest neighbor to ~xi in order to substitute ~xj with that neighbor."}, {"heading": "4.1 Feasibility Bounds.", "text": "In principles, that strategy is effective against the infeasible cases like in the imaginary situation in Fig. 1, however in real experiments the chances to observe a definite cases of infeasibility might be small, for example the smallest eigenvalue of Q in theorem 1 can be a small but not equal to zero. Nevertheless we will show the introduced feasibility concept plays a direct rule in the performance of LMNN algorithm and definitely has to be considered in the optimization algorithm. For example consider a more realistic 2-D situation like Fig. 2 in which the ~x4 is an imposter placed between ~xi and ~x1 and so close to their connecting line, however according to theorem 1 the smallest eigenvalue of matrix Q has very small positive value. Hence, it is still feasible to find a linear mapping L to make (5) feasible like in Fig. 2.a. Although this mapping makes the cost function smaller and the hing loss equal to zero for the data triple (i,1,4), but it is so tight due the small eigenvalue of Q in one of the directions. In other words, in order to pull ~x1 toward ~xi and to pull ~xl away, the algorithm is bounded to use a relevant set S of possible metrics with restricted shapes\nSijl = {M : M 0,D(~xi, ~xj) < D(~xi, ~xl)} (7)\nAnd if it tries to find an optimal S\u2217 \u2208 Si,1,4 to apply to other data points in the problem, the probability of its failure can be high Fig. 2.b. In comparison, it we take ~x2 as the target to ~xi, then based on the wide angle between (~x4 \u2212 ~xi) and (~x2 \u2212 ~xi), the eigenvalues of relevant matrix Q will be relatively bigger than the previous case, and easily can be inferred from the Fig. 2 that Si,4,2\nincludes wider range of possible shapes which gives the optimization algorithm more flexibility to find a suitable metric M\u2217 \u2208 Si,2,4 to fit to other data samples as in Fig. 2.c. As a result, for the sake of optimization (4), it would be better to put more efforts on optimizing the situation toward ~x2 than ~x1.\nIn order to mathematically address this problem, we investigate the characteristics of feasible set S to the inequality (6), and more precisely we check the conditions in which the restrictions on the solution set S will be tighter or looser.\nLemma 1 For any Hermitian matrix Q and a positive semidefinite matrix M , \u03bbk(Q)\u03bbmin(M) \u2264 \u03bbk(Tr(QM)). Proof 2 Because M is positive semidefinite, \u03bbk(QM) = \u03bbk(Q \u221a M \u221a M) where\u221a\nM is the square root matrix of M . And since, Q and M are symmetric, \u03bbk(QM) = \u03bbk( \u221a MQ \u221a M). And according to the min-max theorem \u03bbk(QM) is equal to\nmin F\u2282Rn\ndim(F )=k\n( max\nx\u2208F\\{0}\n( \u221a MQ \u221a Mx, x)\n(x, x)\n)\n= min F\u2282Rn\ndim(F )=k\n( max\nx\u2208F\\{0}\n(Q \u221a Mx, \u221a Mx)\n( \u221a Mx, \u221a Mx)\n(Mx, x)\n(x, x)\n) .\nAs we know that \u03bbmin(M) \u2264 \u3008Mx,x\u3009\u3008x,x\u3009 \u2264 \u03bbn(M), then\n\u03bbk(QM) \u2265 \u03bbmin(M) min F\u2282Rn\ndim(F )=k\n( max\nx\u2208F\\{0}\n(Q \u221a Mx, \u221a Mx)\n( \u221a Mx, \u221a Mx)\n)\nand based on the min-max theorem it means \u03bbk(QM) \u2265 \u03bbmin(M)\u03bbk(Q).\nBased on lemma 1 we have\n\u03bbmax(Q)\u03bbmin(M) \u2264 \u03bbmax(QM). (8)\nAlso it is proven in [14] that for our set of {Q,M}\n\u03bbmin(Q)\u03bbmax(M) \u2264 \u03bbmin(QM)\nadding the above to (8) will result in\n\u03bbmin(Q)\u03bbmax(M) + \u03bbmax(Q)\u03bbmin(M) \u2264 Tr(QM)\nAs we want Tr(QM) < 0 to happen, the left part of the above inequality should be negative, meaning that\n|\u03bbmin(Q)| \u03bbmax(Q) > \u03bbmin(M) \u03bbmax(M) (9)\nAs the first clear conclusion, in case of \u03bbmin(Q) = 0 it results in the infeasibility of (9) which agrees with the outcome of theorem 1. The second important point about (9) is the fact that as the absolute value of \u03bbmin(Q) gets smaller, the restriction on the values of \u03bbmin(M) will be tighter which force the value of them to be smaller in size. Also another supporting fact to the above claim is proved in [15] as \u03bbmin(Q) Tr(M) \u2264 Tr(QM) Which imposes the bound on all eigenvalues of M so if \u03bbmin(Q) becomes really small it forces \u2016\u03bb(M)\u20161 to shrink in size. So although given any matrix Q with even really small eigenvalues the size of solution set S will be indefinite, but there would be strict bounds imposed on the shape (or boundaries of) of the solution set spectrahedron. In the following we are going to see how the above findings can improve the solution to the optimization problem (4)"}, {"heading": "4.2 Feasibility Bound Weights in the Optimization.", "text": "Considering the global solution to the LMNN optimization problem, we are looking for a metric transform L\u2217 which can be the global optimum point to (4). So although the first part of the objective is necessary to help the optimization algorithm to converge to an optimum point, at the end, an ideal global optimum with respect to nearest neighbor classification would be a metric L\u2217, that fulfills (5) for all the triples inside set N k. In other words,\nL\u2217 \u2208 \u22c2\ni,j\u2208Nk,\u2200l\n{Sijl}\naccording to (10). As result, if the problem has a linear solution for the kNN classifier, that intersection will not be empty. However in most of real problems, there won\u2019t be an intersection between all the solution sets, but still there is a best solution L\u0302\u2217 such that\nmax L\u0302\u2217 |S|, {S : S = \u22c2 i,j\u2282Nk {Sijl}, S 6= \u2205} (10)\nAgain we point out the important fact that this optimality is with respect to the nearest neighbor criteria used for prediction in a kNN classifier which highly depends on the distance order of nearby data samples than the value of their distances. Concluding based on (10), if the optimization problem (3) focuses more on data triples (i,j,l) with feasibility sets Sijl that have wider restrictions on their eigenvalues, then the chances for learning a metric close to L\u0302\u2217 will be increased. According to (9), a good estimate for the tightness of the feasibility set Sijl would be\n|\u03bbmin(Qijl)| \u03bbmax(Qijl)\n(11)\nAs a result, we want to modify the convex optimization problem (3) with respect to the above measure. Furthermore, as we discuss before, we are also interested to know which neighboring points inside N ki are more important or more promising to be considered as the important targets for ~xi. To do so, for each l \u2208 Imji , we calculate (11) and assign Rij = |\u03bbmin(Qijlo )| \u03bbmax(Qijlo ) in which lo is the\n~xl that results in the smallest (11). In other words, Sijlo = \u22c2 l\u2208Imji\n{Sijl} which means the set of feasible solutions for the couple (i, j), therefore we can take Rij as a measure on how promising ~xj could be as a target for ~xi according to the optimization structure.\nIn order to adapt the optimization with respect to R, a greedy strategy would be to choose only targets neighbors for each ~xi with high Rij values, and ruling out others from N ki by making \u03b7ij = 0 in the optimization problem (3). This will be despite the fact that they might be even the closets samples to ~xi based on the current metric L. However in order not to restrict the flexibility of the optimization algorithm we augment the weight matrix R in the optimization problem as (12). Doing so, the optimization algorithm puts more gains on the optimization cost of target neighbors which have solution sets with wider spectrahedrons.\nmin M\n\u2211 ij \u03b7ijRijD(~xi, ~xj) +m \u2211 ijl \u03b7ijRij(1\u2212 \u03b4il)\u03beijl\ns.t. D(~xi, ~xl)\u2212D(~xi, ~xj) \u2265 c\u2212 \u03beijl \u03beijl \u2265 0 M 0\n(12)\nAnother important fact about the weighted structure of (12) is that, it also considers the number of similar data points with respect to the condition of their solution sets. For example consider a data type in which the majority of the data samples are distributed such that the optimum metric L\u2217 will be the S \u2032 \u2282 S as the intersection of the tightest sets from S. In that situation, the\noptimization algorithm is still able to choose those targets, because the general weight of the subset S\u2032 will becomes bigger through its repeated samples in the optimization framework. Also it is clear that considering the case of having complete infeasible target points will result in Rij = 0 for j \u2208 Fki and practically they will be removed from the optimization framework.\nIn order to implement the FB-LMNN algorithm, first as a pre-processing step the N ki for each data point should be determined based on its k-nearest same-class neighbors using the original metric L (or current metric in the multipass loop). Then for each j \u2208 N ki we find {l : D(~xi, ~xj) + c > D(~xi, ~xl)} for which we calculate (11) and then Rij will be determined. However, in practice we noticed that it is better to normalize Ri,: for each ~xi which improve the scalability of R over all data samples. Having weight matrix R ready, we can start solving 12 as the second part of the Algorithm 1.\nInput: Metric M, Data matrix X \u2208 Rd\u00d7N , k 1 for i = 1, ..., N do 2 calculate N ki ; 3 for j \u2208 N ki do 4 calculate Imji ; 5 Rij = min\nl\n|\u03bbmin(Qijl)| \u03bbmax(Qijl) , l \u2208 Imji 6 end\n7 Ri,: = \u2211 j Ri,j\nmax(Ri,:)\n8 end 9 update Rij in (12);\n10 solve (12) using SD programming ;\nAlgorithm 1: Feasibility Based LMNN"}, {"heading": "5 Experiments", "text": "In this section we investigate the performance of the explained FB-LMNN algorithm. We implement our algorithm on some real data with different structures and also on a synthetic data to better show the effective point of the approach. We use the 10-fold cross validation to split the data into test and train batches. For choosing the parameter k as the number of nearest neighbors we used cross validation and we fix the parameter m = 0 which generally results in good performance for practical applications. The summary of datasets and the chosen meta parameter for the LMNN approach is brought in Table. 1 We compare our FB-LMNN algorithm first with kNN using Euclidean distance, and as the second approach we used the original LMNN method introduced by [3], also we tried the regular LMNN algorithm in a multi-pass fashion as [13] in order to improve its performance. As another rival linear classifier we checked our results also with multiclass SVM algorithm with RBF kernel."}, {"heading": "5.1 Synthetic Data", "text": "To better demonstrate the effectiveness of the approach, we manually generated the so called zebra stripe toy data set which was also tried by the original and multi-pass versions of LMNN algorithm by [3] and [13]. As you can see in Fig. 3 according to the alternate distribution of data classes, the first suggestion to handle the task is to solve it by focusing on local distances between the data. Therefore nearest neighbor classifiers should be and ideal choice based on their principles. However the main difference of our dataset and the ones used in [3] and [13], is the ratio of the local distance to the nearest class data comparing to the distance to the nearest different class, similar to the example data in Fig. 1. based on the original metric, the closets target neighbor for each data sample ~xi belongs to a different row than ~xi row based on its smaller proximity in the horizontal axis. We take this characteristic to a more extreme case in which the distance to a same row target is so bigger than to a target in a different stripe, so the normal optimization problem (3) will be trapped in a local minimum (with respect to the global problem (4)) and even a multi-pass loop of the optimization will be trapped in a global path of optimization which will not get closer to the global optimum point. Fig. 4 shows the result of normal multi-pass LMNN at the convergence point. As you can see algorithm tried to scale and rotate the data in order to reduce the total cost function of optimization but was not able to solve the problem efficiently due to the tight feasibility sets of the nearest neighbor data triples. The accuracy of the multi-pass LMNN was 23% for this artificial data which was almost the same as the accuracy of vanilla kNN classifier. Although by increasing the number of k the good targets (same stripe data) can be selected too, but the wrong targets still will be a part the optimization which prevent the algorithm to move on a optimum path.\nOn the other hand, the proposed FB-LMNN considers the feasibility of triples of the data. As a result, the triple sets in which the data point and the target are located on different rows are either infeasible or so narrow, while the feasible bounds will be wider for same row targets. As a result there will be bigger Rijl weights for optimization the metric respect to latter set of targets which or in general to the vertical axis. Hence optimization problem leads to better optimum points as you see in Fig. 5, which shows that the algorithm focused on the data on the target neighbors on the same stripe even it was farther than the nearest neighbors. At the end it rotated and compressed the data along the stripes which resulted in 76% accuracy. Although this artificial data strongly supports benefit of considering feasibility sets for LMNN, we also provided implementation on real datasets to be confident about the performance of the approach in real situations."}, {"heading": "5.2 Real Data", "text": "The real data sets are all chosen from the UCI repository library [16] and the results are reproducible easily. The selected data sets are are related to different classification problems in different domain and also with different structures. The result of application of kNN, multi-pass LMNN, SVM and the Feasibility based LMNN are presented in Table 2. According to the results, for some datasets such as Car Evaluation and Iris the difference between regular LMNN\nand the FB-LMNN is subtle, however in some other datasets the FB-LMNN has a higher performance than the Multi-pass LMNN showing that considering the form of feasibility sets for N k was effective, or in other words, there was similarity between same class data samples based on that feature. Comparing with SVM classifier, for few of the datasets SVM has a better result than the LMNN approaches which is based on the structure of the data which was more suitable for that category of classifiers."}, {"heading": "6 Conclusion", "text": "In this paper, we addressed an important issue in the Large Margin metric learning algorithm for the first time, which is related to the distance based selection of optimization targets regardless of their importance or influence to the optimization problem. Even though the optimization problem of LMNN converges to a global minimum because of its convexity, that point is optimal with respect to the selected target Neighbors for the algorithm. We presented examples in which the typical way of changing neighborhood distance will not handle this problem. We stated that regardless of the distance of same class data, there might exist closed but infeasible neighboring points which can mislead the optimization path if we take them as target exemplars. We mathematically demonstrated how to detect and measure the infeasibility of the target points and rearrange the optimization problem to consider the above conditions during its process. As a result we proposed the FB-LMNN which is an alternating optimization optimization framework that tries to consider directions in the metric space in\nwhich there is a higher chance for an optimum convergence point. We presented our method with mathematical proves showing that the measure of feasibility of data triples has to be considered in the optimization problem. We illustrated the algorithms performance on artificial data set to show the strong point of the approach ans also evaluated its performance for real problems, which showed the effectiveness of the proposed methodology. Although in some real data sets there is no difference between using the regular LMNN or the feasibility based, we still encourage using this extra measure as a safety approach to prevent from local optimum points. As a future work, the mentioned feasibility measure can be used to partition the data space into the clusters of data which are similar with respect to the direction of the optimal malahobian metric. We strongly believe that this is a promising starting point to have more detail insight into better selection of target points in LMNN as a metric adaptation algorithm which highly depends on the mentioned criteria."}, {"heading": "7 Acknowledgment", "text": "This research was supported by the Cluster of Excellence Cognitive Interaction Technology \u2019CITEC\u2019 (EXC 277) at Bielefeld University, which is funded by the"}, {"heading": "B. Cancer 20 30 256 5 3", "text": "German Research Foundation (DFG)."}], "references": [{"title": "A survey on metric learning for feature vectors and structured data", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "arXiv preprint arXiv:1306.6709, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Metric learning: A survey", "author": ["B. Kulis"], "venue": "Foundations and Trends in Machine Learning, vol. 5, no. 4, pp. 287\u2013364, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "Journal of Machine Learning Research, vol. 10, pp. 207\u2013244, 2009. [Online]. Available: http://doi.acm.org/10.1145/1577069.1577078", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Is that you? metric learning approaches for face identification", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": "2009 IEEE 12th International Conference on Computer Vision. IEEE, 2009, pp. 498\u2013505.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient metric learning for the analysis of motion data", "author": ["B. Hosseini", "B. Hammer"], "venue": "Data Science and Advanced Analytics (DSAA), 2015. 36678 2015. IEEE International Conference on, Oct 2015, pp. 1\u201310.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Pedestrian recognition with a learned metric", "author": ["M. Dikmen", "E. Akbas", "T.S. Huang", "N. Ahuja"], "venue": "Asian conference on Computer vision. Springer, 2010, pp. 501\u2013512. 14  Table 2: Classification accuracy(%) for the selected datasets and the chosen approaches Dataset kNN MP-LMNN FB-LMNN SVM Zebra 21.31 23.51 72.21 50.82 Wine 73.4 96.91 98.77 77.23 Balance 87.42 94.03 96.08 97.5 B. Cancer 94.66 96.68 97.07 78.49 Car Eval. 92.57 98.32 98.4 60.08 Tic-Tac-Toe 87.42 97.66 98.13 85 Hepatitis 84.16 84.46 90 79.11 iris 92.64 93.24 94.12 97.47", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Analysis of flow cytometry data by matrix relevance learning vector quantization", "author": ["M. Biehl", "K. Bunte", "P. Schneider"], "venue": "PLoS One, vol. 8, no. 3, p. e59401, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Classification in high-dimensional spectral data: Accuracy vs. interpretability vs. model size", "author": ["A. Backhaus", "U. Seiffert"], "venue": "Neurocomputing, vol. 131, pp. 15\u201322, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficiently learning a distance metric for large margin nearest neighbor classification.", "author": ["K. Park", "C. Shen", "Z. Hao", "J. Kim"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Distance metric learning with eigenvalue optimization", "author": ["Y. Ying", "P. Li"], "venue": "Journal of Machine Learning Research, vol. 13, no. Jan, pp. 1\u201326, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Large margin multi-task metric learning", "author": ["S. Parameswaran", "K.Q. Weinberger"], "venue": "Advances in neural information processing systems, 2010, pp. 1867\u20131875.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Hierarchical large margin nearest neighbor classification", "author": ["Q. Chen", "S. Sun"], "venue": "Pattern Recognition (ICPR), 2010 20th International Conference on. IEEE, 2010, pp. 906\u2013909.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Convergence of multi-pass large margin nearest neighbor metric learning", "author": ["C. G\u00f6pfert", "B. Paassen", "B. Hammer"], "venue": "International Conference on Artificial Neural Networks. Springer, 2016, pp. 510\u2013517.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Eigenvalue inequalities for matrix product", "author": ["F. Zhang", "Q. Zhang"], "venue": "IEEE Transactions on Automatic Control, vol. 51, no. 9, p. 1506, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Trace bounds on the solution of the algebraic matrix riccati and lyapunov equation", "author": ["S.-D. Wang", "T.-S. Kuo", "C.-F. Hsu"], "venue": "IEEE Transactions on Automatic Control, vol. 31, no. 7, pp. 654\u2013656, 1986.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1986}, {"title": "UCI machine learning repository", "author": ["M. Lichman"], "venue": "2013. [Online]. Available: http://archive.ics.uci.edu/ml 15", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "One of the well-known methods for metric learning is the large margin nearest neighbor (LMNN) which is fundamentally destined to increase the performance of k-nearest neighbor classifier[1, 2, 3], by transferring the maximum margin concept of SVM classifier to the kNN framework.", "startOffset": 186, "endOffset": 195}, {"referenceID": 1, "context": "One of the well-known methods for metric learning is the large margin nearest neighbor (LMNN) which is fundamentally destined to increase the performance of k-nearest neighbor classifier[1, 2, 3], by transferring the maximum margin concept of SVM classifier to the kNN framework.", "startOffset": 186, "endOffset": 195}, {"referenceID": 2, "context": "One of the well-known methods for metric learning is the large margin nearest neighbor (LMNN) which is fundamentally destined to increase the performance of k-nearest neighbor classifier[1, 2, 3], by transferring the maximum margin concept of SVM classifier to the kNN framework.", "startOffset": 186, "endOffset": 195}, {"referenceID": 3, "context": "was used in many real problems such as face recognition [4], motion classification [5] and pedestrian identification [6].", "startOffset": 56, "endOffset": 59}, {"referenceID": 4, "context": "was used in many real problems such as face recognition [4], motion classification [5] and pedestrian identification [6].", "startOffset": 83, "endOffset": 86}, {"referenceID": 5, "context": "was used in many real problems such as face recognition [4], motion classification [5] and pedestrian identification [6].", "startOffset": 117, "endOffset": 120}, {"referenceID": 6, "context": "Furthermore, this adaptation of metric in this manner also enhances the model interpretability along with increasing the accuracy, and in some domain it is used for easier visualization of data [7, 8].", "startOffset": 194, "endOffset": 200}, {"referenceID": 7, "context": "Furthermore, this adaptation of metric in this manner also enhances the model interpretability along with increasing the accuracy, and in some domain it is used for easier visualization of data [7, 8].", "startOffset": 194, "endOffset": 200}, {"referenceID": 8, "context": "There has been several attempts to improve the performance of the original LMNN approach, such as complexity reduction of the optmization framework [9], eigenvalue based optimization [10], its extension to the multi-taksing problem [11] and a hierarchical prepossessing of data [12] all of which have interesting results.", "startOffset": 148, "endOffset": 151}, {"referenceID": 9, "context": "There has been several attempts to improve the performance of the original LMNN approach, such as complexity reduction of the optmization framework [9], eigenvalue based optimization [10], its extension to the multi-taksing problem [11] and a hierarchical prepossessing of data [12] all of which have interesting results.", "startOffset": 183, "endOffset": 187}, {"referenceID": 10, "context": "There has been several attempts to improve the performance of the original LMNN approach, such as complexity reduction of the optmization framework [9], eigenvalue based optimization [10], its extension to the multi-taksing problem [11] and a hierarchical prepossessing of data [12] all of which have interesting results.", "startOffset": 232, "endOffset": 236}, {"referenceID": 11, "context": "There has been several attempts to improve the performance of the original LMNN approach, such as complexity reduction of the optmization framework [9], eigenvalue based optimization [10], its extension to the multi-taksing problem [11] and a hierarchical prepossessing of data [12] all of which have interesting results.", "startOffset": 278, "endOffset": 282}, {"referenceID": 2, "context": "According to [3] the optimization problem is a category of semidefinite programming as:", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "One way to get close to the global optimum of problem is to use the multiple pass technique suggested by [3], which tries to update the targets neighbors N k based on the learned metric L after the convergence of the optimization problem (3).", "startOffset": 105, "endOffset": 108}, {"referenceID": 12, "context": "It is proved in [13] that in each iteration t of multiple pass LMNN, the optimization problem will converge to a better or equal optimal pointL comparing to L.", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "Also it is proven in [14] that for our set of {Q,M} \u03bbmin(Q)\u03bbmax(M) \u2264 \u03bbmin(QM)", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "Also another supporting fact to the above claim is proved in [15] as \u03bbmin(Q) Tr(M) \u2264 Tr(QM) Which imposes the bound on all eigenvalues of M so if \u03bbmin(Q) becomes really small it forces \u2016\u03bb(M)\u20161 to shrink in size.", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "1 We compare our FB-LMNN algorithm first with kNN using Euclidean distance, and as the second approach we used the original LMNN method introduced by [3], also we tried the regular LMNN algorithm in a multi-pass fashion as [13] in order to improve its performance.", "startOffset": 150, "endOffset": 153}, {"referenceID": 12, "context": "1 We compare our FB-LMNN algorithm first with kNN using Euclidean distance, and as the second approach we used the original LMNN method introduced by [3], also we tried the regular LMNN algorithm in a multi-pass fashion as [13] in order to improve its performance.", "startOffset": 223, "endOffset": 227}, {"referenceID": 2, "context": "To better demonstrate the effectiveness of the approach, we manually generated the so called zebra stripe toy data set which was also tried by the original and multi-pass versions of LMNN algorithm by [3] and [13].", "startOffset": 201, "endOffset": 204}, {"referenceID": 12, "context": "To better demonstrate the effectiveness of the approach, we manually generated the so called zebra stripe toy data set which was also tried by the original and multi-pass versions of LMNN algorithm by [3] and [13].", "startOffset": 209, "endOffset": 213}, {"referenceID": 2, "context": "However the main difference of our dataset and the ones used in [3] and [13], is the ratio of the local distance to the nearest class data comparing to the distance to the nearest different class, similar to the example data in Fig.", "startOffset": 64, "endOffset": 67}, {"referenceID": 12, "context": "However the main difference of our dataset and the ones used in [3] and [13], is the ratio of the local distance to the nearest class data comparing to the distance to the nearest different class, similar to the example data in Fig.", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "The real data sets are all chosen from the UCI repository library [16] and the results are reproducible easily.", "startOffset": 66, "endOffset": 70}], "year": 2016, "abstractText": "In the area of data classification, one of the prominent algorithms is the large margin nearest neighbor (LMNN) approach which is a metric learning to enhance the performance of the popular k-nearest neighbor classifier. In principles, LMNN learns a more efficient metric in the input space by using a linear mapping as the outcome of a convex optimization problem. However, one of the greatest weak points of LMNN is the strong reliance of its optimization paradigm on how the neighboring points are chosen. In this paper, it is mathematically proved for the first time that the regular way of choosing the target points can lead to non-feasible optimization conditions regardless of the number of chosen neighboring points. We present a mathematical approach to categorize the target points into feasible and infeasible exemplars, an also we provide a feasibility measure for preference of the target candidates. In our proposed Feasibility Based-LMNN algorithm, we use the above clue to construct the optimization problem based on the most promising general mapping directions in the input space. Our empirical results shows that via using the proposed FB-LMNN approach the optimization problem will converge in a better optimum point, and therefor leads to better classification on the well-known benchmark datasets", "creator": "XeLaTeX"}}}