{"id": "1610.07363", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "Learning Reporting Dynamics during Breaking News for Rumour Detection in Social Media", "abstract": "Breaking news leads to situations of fast-paced reporting in social media, producing all kinds of updates related to news stories, albeit with the caveat that some of those early updates tend to be rumours, i.e., information with an unverified status at the time of posting.\n\n\n\n\nIn terms of the current coverage of the US news cycle, there have been quite a few mentions of the new 'fake news' category, with the occasional 're-emergence' in news that have emerged at the time of writing, a number of them appearing in social media reports. Some are just as likely to appear on various social media sites as they are on Facebook, Twitter, LinkedIn and Instagram, and many others are not on social media at all.\nIt should be noted that the term 'fake news' is typically used to refer to events of the past such as a police attack on a public property, in which a person or person was killed. However, even a number of websites are still using the term as a term that is considered to be an important part of the news cycle, meaning it is not possible to find any links to these reports.\nA number of these social media profiles are now under the purview of some news reporters who are just as likely to appear on social media websites to be involved in a criminal offence as those who have seen them. The situation seems to have been very interesting for many social media users, with many reporting to the BBC that they have seen some \"fake news\" stories as being fabricated and that one person or the other might have made the mistake of making a mistake. While these reports are typically not official news stories, they can be seen in some of these accounts, with other social media accounts claiming to be in possession of any stolen or misreported information. In fact, there are reports that some of these social media profiles have even been used to target individuals with criminal records.\nWe should be clear: when you read reports about the US news cycle, a number of these are in fact stories from news sources. Many of these stories are based on official reporting. However, the number of social media accounts that have been identified as fake news on social media has fluctuated due to the fact that many media accounts are running fake news, and the number of fake news accounts that are being identified by their source has dropped from 2.8 million to 1.7 million. These were very similar stories at the time of writing.\nSome of these reports are not official news reports,", "histories": [["v1", "Mon, 24 Oct 2016 11:25:24 GMT  (154kb,D)", "http://arxiv.org/abs/1610.07363v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.SI", "authors": ["arkaitz zubiaga", "maria liakata", "rob procter"], "accepted": false, "id": "1610.07363"}, "pdf": {"name": "1610.07363.pdf", "metadata": {"source": "CRF", "title": "Learning Reporting Dynamics during Breaking News for Rumour Detection in Social Media", "authors": ["Arkaitz Zubiaga", "Maria Liakata", "Rob Procter"], "emails": ["a.zubiaga@warwick.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "The use of social media to follow news stories has become commonplace in recent years. Well-known platforms such as Twitter are increasingly being used by people to learn about the latest developments (Sankaranarayanan et al., 2009), as well as by journalists for news gathering (Zubiaga et al., 2013). This is possible thanks to the way in which they enable users to post and share updates from anywhere and at any time, hence making it possible to get reports from users on the ground who happen to witness a newsworthy event or from users that, for some reason, appear to have access to exclusive information. However, the speed at which breaking news unfolds on social media during fast-paced events, such as terrorist attacks or riots, inevitably means that much of the information posted in the early stages of news reporting is unverified (Procter et al., 2013a). The presence of such rumours\nar X\niv :1\n61 0.\n07 36\n3v 1\n[ cs\n.C L\n] 2\n4 O\nin the stream of tweets makes it more difficult for users to distinguish verified information from rumours, and coverage of the news becomes more challenging for news practitioners.\nIn this work we set out to develop a rumour detection system that enables flagging of unverified posts, so that one can easily distinguish information that is unsubstantiated. A rumour detection system would ultimately warn users of the unverified status of a post, letting them know that it might later be proven false; this can be useful both to limit the diffusion of information that might turn out subsequently to be false and so reduce the risk of harm to individuals, communities and society (Webb et al., 2016). Research in rumour detection is scarce in the scientific literature, (Zhao et al., 2015) being the only published work to date that addresses this issue. They introduced an approach that looks for \u2019enquiry tweets\u2019, i.e., tweets that query or challenge the credibility of a previous posting to determine whether it is rumourous; a tweet is deemed to be querying if it matches one of a number of manually curated, regular expressions. While this is an ingenious approach, it has some important limitations: it is reliant on there being a human in the loop to regularly revise the list of regular expressions as these may not generalise well to new datasets; it assumes that querying posts will arise, though this may lead to low recall as not all rumourous posts will necessarily provoke queries; and it takes no account of the context surrounding the information, which we believe can be exploited to gain insight into the way that a piece of information emerges. Other work has dealt with \u201crumour detection\u201d with what we argue is a questionable definition and which conflicts with our own (Zubiaga et al., 2016). These studies understand rumours as false pieces of information, and therefore define the rumour detection task as consisting of distinguishing true and false stories. In our study we adhere to the prevailing definition in the scientific literature that understands a rumour as the information that is being circulated while its veracity is yet to be confirmed (Allport and Postman, 1946; DiFonzo and Bordia, 2007). Consequently, we define the goal of the rumour detection task as that of identifying pieces of information that are yet to be verified, distinguishing them from non-rumours. Our work makes the following contributions within the scope of this definition of the rumour detection task:\n\u2022 We describe a novel methodology for the collection and annotation of Twitter datasets containing a diverse range of rumours and non-rumours. Our\nmethodology, developed in close collaboration with journalists, consists in a bottom-up approach that enables going through a timeline of tweets associated with a breaking news story to annotate rumours that were not necessarily known a priori. Previous work has largely focused on top-down approaches that first list a set of rumourous stories known to have been circulating, and then go through the tweets to find them, which does not make possible discovery of new rumours that have not yet been listed. \u2022 To the best of our knowledge, our work is the first to perform the rumour detection task without having to observe querying tweets to identify that\na piece of information is rumourous. Instead, we introduce a sequential ap-\nproach based on Linear-Chain Conditional Random Fields (CRF) to learn the dynamics of information during breaking news, which enables us to classify a piece of information as a rumour or non-rumour by leveraging the context learnt as the event unfolds, and relying only on the content of a tweet to determine if is rumourous. Hence, our approach does not require a tweet to trigger querying posts to determine if it is rumourous. \u2022 We investigate the performance of CRF as a sequential classifier on five Twitter datasets associated with breaking news to detect the tweets that constitute\nrumours. The performance of CRF is compared with its non-sequential equivalent, a Maximum Entropy classifier, as well as the state-of-the-art rumour detection approach by (Zhao et al., 2015) and additional baseline classifiers. Our experiments show substantial improvements with CRF\u2019s use of the sequential dynamics of reporting learnt during an event as context that enriches the content of the tweet itself. These improvements are consistent across the different events in our dataset, as well as over different phases of event reporting, including in the early stages where the sequence to be exploited is more limited."}, {"heading": "2 Background: Definition of Rumour", "text": "Rumours have been studied and analysed from a range of perspectives, and within and across different disciplines (Donovan, 2007). Most of the definitions given in the literature agree with that of major dictionaries such as the Oxford English Dictionary, which defines a rumour as \u201ca currently circulating story or report of uncertain or doubtful truth\u201d, as well as the Merriam-Webster dictionary defining it as \u201cinformation or a story that is passed from person to person but has not been proven to be true\u201d. Irrespective of the underlying story being ultimately proven true or false, or remaining unsubstantiated, a rumour circulates while it is yet to be verified. A number of researchers have extended the definition of rumour. For instance, (DiFonzo and Bordia, 2007) define rumours as \u201cunverified and instrumentally relevant information statements in circulation that arise in contexts of ambiguity, danger or potential threat, and that function to help people make sense and manage risk\u201d. Moreover, (Allport and Postman, 1946) posit that one of the main reasons why rumours circulate is that \u201cthe topic has importance for the individual who hears and spreads the story\u201d. In (Allport and Postman, 1947), the authors also emphasise that \u201cnewsworthy events are likely to breed rumors\u201d and that \u201cthe amount of rumor in circulation will vary with the importance of the subject to the individuals involved times the ambiguity of the evidence pertaining to the topic at issue\u201d.\nConsistent with these definitions, we adhere here to a definition adapted to the context of breaking news, which we introduced in previous work (Zubiaga et al., 2016): a rumour is a \u201ccirculating story of questionable veracity, which is apparently credible but hard to verify, and produces sufficient skepticism and/or anxiety so as to motivate finding out the actual truth\u201d. In the context of journalism, spreading rumours can have harmful consequences for the reputation of a news organisation if they are used in reporting and later proven false, and hence being able with con-\nfidence to quickly assess whether information has not yet been verified as breaking news unfolds is crucial."}, {"heading": "3 Related Work", "text": "Despite the increasing interest in analysing rumours in social media (Procter et al., 2013b; Procter et al., 2013a; Starbird et al., 2014; Zubiaga et al., 2015; Takayasu et al., 2015; Tolosi et al., 2016; Zubiaga et al., 2016) and the building of tools to deal with rumours that had been previously identified (Seo et al., 2012; Takahashi and Igata, 2012), there has been very little work in automatic rumour detection. Some of the work in rumour detection (Qazvinian et al., 2011; Hamidian and Diab, 2015; Hamidian and Diab, 2016) has been limited to finding rumours known a priori. A classifier is fed with a set of predefined rumours (e.g., Obama is muslim), which then classifies new tweets as being related to one of the known rumours or not (e.g., I think Obama is not muslim would be about the rumour, while Obama was talking to a group of Muslims wouldn\u2019t). An approach like this can be useful for long-standing rumours, where one wants to identify relevant tweets to track the rumours that have already been identified; one may also refer to this task as rumour tracking rather than rumour detection. However, this would not work for fast-paced contexts such as breaking news, where new, previously unseen rumours emerge, and one does not know a priori the specific keywords linked to the rumour, which is yet to be detected. To deal with such situations, a classifier would need to learn generalisable patterns that will help identify new rumours during breaking stories.\nTo the best of our knowledge, the only work that has tackled the detection of new rumours is that by (Zhao et al., 2015). Their approach builds on the assumption that rumours will provoke tweets from skeptical users who question or enquire about their veracity; the fact that a piece of information has a number of querying tweets associated with it would then imply that the information is rumourous. The authors created a manually curated list of five regular expressions (e.g., \u201cis (that | this | it) true\u201d), which are used to identify querying tweets. These querying tweets are then clustered by similarity, each cluster being ultimately deemed a candidate rumour. It was not viable for the authors to evaluate by recall, but their best approach achieved 52% and 28% precision for two datasets. While this work builds on a sensible hypothesis and presents a clever approach to tackling the rumour detection task, we foresee three potential limitations: (1) being based on manually curated regular expressions the approach may not generalise well, (2) the hypothesis might not always apply and hence lead to low recall as, for example, certain rumours reported by reputable media are not always questioned by the general public (Zubiaga et al., 2016), and (3) it takes no account of the context that precedes the rumour, which can give additional insights into what is going on and how a piece of information can be rumourous in that context (e.g., the rumour that a gunman is on the loose, when the police has not confirmed it yet, is easier to be deemed a rumour if we put it into the context of the preceding events, such as additional reports that the identity of the gunman is unknown and the\nreasons that motivated the shooting have not been found out). In this work, we introduce a context-aware rumour detection system that uses a sequential classifier to examine the reporting dynamics during breaking news to determine if a new piece of information constitutes a rumour.\nWhile not strictly doing rumour detection, other researchers have worked on related tasks. For instance, there is an increasing body of work (Qazvinian et al., 2011; Liu et al., 2015; Hamidian and Diab, 2015; Hamidian and Diab, 2016; Lukasik et al., 2015; Zeng et al., 2016) looking into stance classification of tweets discussing rumours, categorising tweets as supporting, denying or questioning the rumour. The approach has been to train a classifier from a labelled set of tweets to categorise the stance observed in new tweets discussing rumours; however, these authors do not deal with non-rumours, assuming instead that the input to the classifier is already cleaned up to include only tweets related to rumours. There is also work on veracity classification both in the context of rumours and beyond (Sun et al., 2013; Cai et al., 2014; Liang et al., 2015; Liu et al., 2015; Ma et al., 2015; Wu et al., 2015; Ma et al., 2016; Jin et al., 2016). Work on stance and veracity classification can be seen as complementary to our objectives; one could use the set of rumours detected by a rumour detection system as input to a classifier that determines stance of tweets in those rumours and/or veracity of those rumours. However, this previous step of distinguishing between out rumours and non-rumours is largely unexplored, and most work deals directly with subsequent steps."}, {"heading": "4 Dataset", "text": "One of our main objectives when planning to put together a dataset of rumours and non-rumours was to develop a means to collect a diverse set of stories, which would not necessarily be known a priori and which would include both rumours and non-rumours. We did this by emulating the scenario in which a user is following reports associated with breaking news. Seeing a timeline of tweets about the breaking news, a user would then annotate each of the tweets as being a rumour or a non-rumour. To make sure that our users had the expertise to perform this annotation, we enlisted the help of a team of journalists who are partners of our research project. Our data collection approach differs substantially from that of previous work (Qazvinian et al., 2011; Procter et al., 2013b; Starbird et al., 2014), who first identified the rumours of interest and then collected tweets associated with those by filtering using relevant keywords. By following the latter approach of gathering rumours known a priori, one can, for instance, search for tweets with specific keywords, e.g., for tweets posted during the 2011 England Riots, one can search for \u2018London Eye fire\u2019, to retrieve tweets associated with the rumour that the London Eye had been set on fire. However, this requires the rumour in question to be known a priori, and will fail to identify rumours associated with events for which specific keywords have not been previously defined. We argue that this approach will miss some of the rumours, a problem that we overcome here by having journalists sift through the timeline of tweets to discover rumours. While manual annotation of the whole stream of tweets associated with breaking news is not vi-\nable, we alleviate the task by sampling the tweets that provoked a large number of retweets and hence are likely to be of interest for reporting. This is also consistent with one of the main characteristics of rumours, which tend to generate significant levels of interest.\nOur data collection approach consists of harvesting tweets from the Twitter streaming API relating to newsworthy events that could potentially prompt the initiation and propagation of rumours. Collection through the streaming API was launched straight after the journalists identified a newsworthy event likely to give rise to rumours. As soon as the journalists informed us about a newsworthy event, we set up the data collection process for that event, tracking the main hashtags and keywords pertaining to the event as a whole. Note that while launching the collection slightly after the start of the event means that we may have missed the very early tweets, we kept collecting subsequent retweets of those early tweets, making it much more likely that we would retrieve the most retweeted tweets from the very first minutes. Once we had the collection of tweets for a newsworthy event in place, we sampled the timeline of tweets to enable manual annotation (signaled by highly retweeted tweets associated with newsworthy current events). Afterwards, journalists read through the timeline to mark each of the tweets as being a rumour or not, making sure that the identification of rumours was in line with the established criteria (Zubiaga et al., 2015).\nWe followed the process above for five different newsworthy events, all of which\nattracted substantial interest in the media and were rife with rumours:\n\u2022 Ferguson unrest: citizens of Ferguson in Michigan, USA, protested after the fatal shooting of an 18-year-old African American, Michael Brown, by a white\npolice officer on August 9, 2014.\n\u2022 Ottawa shooting: shootings occurred on Ottawas Parliament Hill in Canada, resulting in the death of a Canadian soldier on October 22, 2014. \u2022 Sydney siege: a gunman held hostage ten customers and eight employees of a Lindt chocolate caf located at Martin Place in Sydney, Australia, on De-\ncember 15, 2014.\n\u2022 Charlie Hebdo shooting: two brothers forced their way into the offices of the French satirical weekly newspaper Charlie Hebdo in Paris, killing 11 people\nand wounding 11 more, on January 7, 2015.\n\u2022 Germanwings plane crash: a passenger plane from Barcelona to Dsseldorf crashed in the French Alps on March 24, 2015, killing all passengers and\ncrew. The plane was ultimately found to have been deliberately crashed by the co-pilot of the plane.\nGiven the large volume tweets in the datasets, we sampled them by picking tweets that provoked a high number of retweets. The retweet threshold was set to 100, selected based on the size of the resulting dataset. For each of these tweets in the sampled subset, we also collect all the tweets that reply to them. While Twitter does not provide an API endpoint to retrieve conversations provoked by tweets, it is possible to collect them by scraping tweets through the web client interface. We developed a script that enabled us to collect and store complete conversations for\nall the rumourous source tweets1. We use the replying tweets for two purposes: (1) for the manual annotation work, where replies to each tweet can provide context for the annotator where needed to decide if a tweet is a rumour, and (2) we use them to reproduce a baseline classifier based on the baseline introduced by (Zhao et al., 2015). However, our approach ignores replying tweets, relying only on the source tweet itself.\nThe sampled subsets of tweets were visualised in a separate timeline per day and sorted by time (see Figure 1). Using these timelines, journalists were asked to identify rumours and non-rumours. Along with each tweet, journalists could optionally click on the bubble next to the tweet to visualise tweets replying to the tweet; the conversation provoked by the tweet could assist them by providing context, albeit the annotation was independent of this context and based on their experience. The fact that this annotation work was performed by journalists was convenient as they had continually tracked the five events while they were unfolding, and so they were knowledgeable about the stories. The annotation, however, was conducted a posteriori, once the reporting about the event had come to an end. This encouraged careful annotation that encompassed a broad set of rumours; the journalists could go through the whole timeline of tweets as we presented them and perform the annotations. The annotation work led to the manual categorisation of each tweet as being a rumour or not. The methodology they followed to perform these annotations in a more manageable and scalable way was to go through the timeline by analysing carefully those tweets that reported new stories that they had not seen before; for those cases, they investigated the story further on social media and the Web when the origin and nature of the story was not known to them. As they progressed in the timeline, new tweets reporting repeated stories where assigned the same annotation as in the previous instance. This made their job easier as they had had only to investigate carefully stories that they had not seen.\nThe annotation of tweets sampled for all five events led to a collection of 5,802 annotated tweets, of which 1,972 were deemed rumours and 3,830 were deemed non-rumours. These annotations are distributed differently across the five events, as shown in Table 1. While slightly over 50% of the tweets were rumours for the Germanwings Crash and the Ottawa Shooting, less than 25% were so for Charlie Hebdo and Ferguson. The Sydney Siege had an intermediate ratio of rumours (42.8%). While the global figures of rumours vary substantially across events, we dug further into these distributions to understand how rumours and non-rumours are distributed during events, e.g., to look at whether rumours occur especially at the beginning of the event, along with the very early reports. To do this, we broke down the timeline of tweets for each event into deciles (10% percentiles) and look at the ratio of rumours in each of these deciles. Figure 2 shows the ratios of rumours for each of the deciles for the five events in our dataset. Contrary to what we initially expected, there is no common pattern across events. One can see events\n1 The conversation collection script is available at https://github.com/azubiaga/phemetwitter-conversation-collection.\nwith uniformly distributed ratios of rumours, such as with the Ottawa Shooting, events where the ratio of rumours fades at least eventually, such as Charlie Hebdo, Germanwings crash and Sydney Siege, or events where the majority of the rumours emerge in later stages of the reporting, such as Ferguson. These varying distributions of rumours across different events makes the rumour detection task even more challenging, as one may not be able to rely on the earliness of a report to determine if it is more likely to be a rumour."}, {"heading": "5 Rumour Detection Task", "text": "We define the rumour detection task as that in which, given a timeline of tweets, the system has to determine which of the tweets are reporting rumours, and hence are spreading information that is yet to be verified. The identification of rumours within a timeline is ultimately meant to warn users that the information has not\nbeen confirmed, and so it may turn out to be false. This can be operationalised by flagging those tweets that are identified as rumours, warning users to think twice before spreading the information. Formally, the task takes an evolving timeline of tweets TL = {t1, ..., t|TL|} as input, and the classifier has to determine whether each of these tweets, ti, is a rumour or a non-rumour by assigning a label from Y = {R,NR}. Hence, we formulate the task as a binary classification problem, whose performance is evaluated by computing the precision, recall and F1 scores for the target category, i.e., rumours."}, {"heading": "6 Learning Sequential Dynamics for Rumour Detection", "text": ""}, {"heading": "6.1 Hypothesis", "text": "We argue that a single headline or tweet may not always be indicative of a piece of information being a rumour. There are, indeed, cases where a single tweet uses hedging words or provides little or no evidence so as to be deemed corroborated information, and hence those cases can be deemed rumours from the tweet alone. This is the case, for instance, of tweets reporting during the Ferguson riots that \u201cthe name of the police officer who fatally shot the kid would be reportedly announced by the police later in the day\u201d. If the tweet itself expresses uncertainty, as is the case here with the use of \u201creportedly\u201d, one can consider that the underlying information is not confirmed. However, reports confidently reporting that \u201cthe kid was involved in a robbery before being shot\u201d may not be as easily identified by an automated classifier from the tweet alone, despite being a rumour. The dearth of sufficient\nevidence as occurs in many tweets encourages us to further leverage context that could help the classifier distinguish rumours and non-rumours.\nOne possibility to extend a tweet with context is to look at how others react to it, as (Zhao et al., 2015) proposed in their work that querying or enquiring tweets provoked by a posting may indicate it is a rumour. However, we believe the public will not always question the veracity of rumours, given that average users may not always notice that a piece of information is not confirmed. This is the case of a number of tweets during the Ferguson riots reporting that \u201cthe kid was shot 10 times by the police\u201d. While this information was not queried by the public, the media treated the information as not being verified; the autopsy later confirmed that he was shot 6 times. Hence, while reactions may be indicative of a piece of information being unverified, we believe that it may lead to low recall, missing other cases that are not rebutted.\nTo better harness the context surrounding a tweet, we believe that the classifier needs to be aware of how the whole event is unfolding, analysing the different announcements that build a story before the current tweet is posted. The tweet that is being classified as rumour or non-rumour should therefore leverage earlier happenings within that event, both rumours and non-rumours, that make up a story in which the current tweet fits. For instance, a tweet reporting the rumour that \u201cthe police officer who shot the kid has left the town\u201d may be easier to classify being aware of all the previous reports related to the police officer and the killing. Based on this, we set forth the hypothesis that aggregation of all the rumourous and non-rumourous reports leading up to the tweet being classified will provide key context to boost performance of the rumour detection system."}, {"heading": "6.2 Classifiers", "text": "In order to test our hypothesis, we use Conditional Random Fields (CRF) as a sequential classifier that enables aggregation of tweets as a chain of reports. We use a Maximum Entropy classifier as the non-sequential equivalent of CRF to test the validity of the hypothesis, and also use additional baseline classifiers for further comparison. Moreover, we also reproduce a baseline based on the approach introduced by (Zhao et al., 2015) to compare the performance of our approach with that of a state-of-the-art approach.\nConditional Random Fields (CRF). We use CRF as a structured classifier to model sequences of tweets as observed in the timelines of Twitter breaking news. With CRF, we can model the timeline as a linear chain or graph that will be treated as a sequence of rumours and non-rumours. In contrast to classifiers traditionally used for this task, which choose a label for each input unit (e.g., a tweet), CRF also consider the neighbours of each unit, learning the probabilities of transitions of label pairs to be followed by each other. The input for CRF is a graph G = (V,E), where in our case each of the vertices V is a tweet, and the edges E are relations of tweets, i.e., a link between a tweet and its preceding tweet in the event. Hence, having a data sequence X as input, CRF outputs a sequence of labels Y (Lafferty et al., 2001), where the output of each element yi will not only depend on its features, but\nalso on the probabilities of other labels surrounding it. The generalisable conditional distribution of CRF is shown in Equation 1 (Sutton and McCallum, 2011)2.\np(y|x) = 1 Z(x) A\u220f a=1 \u03a8a(ya, xa) (1)\nwhere Z(x) is the normalisation constant, and \u03a8a is the set of factors in the graph\nG.\nTherefore, in our specific case of rumour detection, CRF will exploit the sequence of rumours and non-rumours leading up to the current tweet to determine if it is a rumour or not. It is important to note that with CRF the sequence of rumours and non-rumours preceding the tweet being classified will be based on the predictions of the classifier itself, and will not use any ground truth annotations. Errors in early tweets in the sequence may then augment errors in subsequent tweets. For each tweet to be classified, we solely feed the preceding tweets to the classifier to simulate a realistic scenario where subsequent tweets are not yet posted and early decisions need to be made on each tweet.\nMaximum Entropy classifier (MaxEnt). As the non-sequential equivalent of CRF, we use a Maximum Entropy (or logistic regression) classifier, which is also a conditional classifier but which will operate at the tweet level, ignoring the sequence and hence the preceding tweets. This enables us to compare directly the extent to which treating the tweets posted during an event as a sequence instead of having each tweet as a separate unit can boost the performance of the classifier.\nEnquiry-based approach by (Zhao et al., 2015): As a state-of-the-art baseline for rumour detection, and the only approach that so far has tackled rumour detection in social media, we reproduce the approach by Zhao et al., which uses regular expressions to look for enquiry posts. We use the set of replies responding to each tweet to look for enquiry posts. Following the approach described by the authors, we consider that a tweet is a rumour if at least one of the replying tweets matches with one of the regular expressions that the authors curated. The list of regular expressions defined by the authors is shown in Table 2.\nAdditional baselines. We also compare three more non-sequential classifiers3:\nNaive Bayes (NB), Support Vector Machines (SVM), and Random Forests (RF).\nWe perform the experiments in a 5-fold cross-validation setting, having in each case four of the events for training, and the remainder event for testing. This enables us to simulate a realistic scenario where an event is completely unknown to the classifier and it has to identify rumours from the knowledge garnered from events in the training set. For evaluation purposes, we aggregate the output of all five runs as the micro-averaged evaluation across runs.\n2 We use the PyStruct to implement Conditional Random Fields (Mu\u0308ller and Behnke, 2014). 3 We use their implementation in the scikit-learn Python package for Maximum Entropy, Naive Bayes, Support Vector Machines and Random Forests."}, {"heading": "6.3 Features", "text": "We use two types of features with the classifiers: content-based features and social features. We test them separately as well as combined. The features that fall in each of these categories are as follows:"}, {"heading": "6.3.1 Content-based Features", "text": "We use seven different features extracted from the content of the tweets:\n\u2022 Word Vectors: to create vectors representing the words in each tweet, we build word vector representations using Word2Vec (Mikolov et al., 2013). We\ntrain a different Word2Vec model with 300 dimensions for each of the five folds, training the model in each case from the collection of tweets pertaining to the four events in the training set, so that the event (and the vocabulary) in the test set is unknown. As a result, we get five different Word2Vec models, each used in a separate fold. \u2022 Part-of-speech Tags: we build a vector of part-of-speech (POS) tags with each feature in the vector representing the number of occurrences of a certain\nPOS tag in the tweet. We use Twitie (Bontcheva et al., 2013) to parse the tweets for POS tags, an information extraction package that is part of GATE (Cunningham et al., 2011). \u2022 Capital Ratio: the ratio of capital letters among all alphabetic characters in the tweet. Use of capitalisation tends to represent emphasis, among others. \u2022 Word Count: the number of words in the tweet, counted as the number of space-separated tokens. \u2022 Use of Question Mark: a binary feature representing if the tweet has at least a question mark in it. Question marks may be indicative of uncertainty. \u2022 Use of Exclamation Mark: a binary feature representing if the tweet has at least an exclamation mark in it. Exclamation marks may be indicative of\nemphasis or surprise.\n\u2022 Use of Period: a binary feature representing if the tweet has at least a period in it. Punctuation may be indicative of good writing and hence potentially of\nslow reporting."}, {"heading": "6.3.2 Social Features", "text": "We use five social features, all of which can be inferred from the metadata associated with the author of the tweet, and which is embedded as part of a tweet object retrieved from the Twitter API. We define a set of social features that are indicative of a user\u2019s experience and reputation:\n\u2022 Tweet Count: we infer this feature from the number of tweets that a user has posted on Twitter. As numbers can vary substantially across users, we\nnormalise them by rounding up the 10-base logarithm of the tweet count:\ndlog10(statusescount)e. \u2022 Listed Count: this feature is computed by normalising the number of lists\na user belongs to, i.e., the number of times other users decided to add them\nto a list: dlog10(listedcount)e. \u2022 Follow Ratio: in this feature we look at the reputation of a user as reflected\nby their number of followers. However, the number of followers might occasionally be rigged, e.g., by users who simply follow many others to attract more followers. To control for this effect, we define the follow ratio as the logarithmi-\ncally scaled ratio of followers over followees: blog10 (#followers/#following)e. \u2022 Age: we compute the age of a user as the rounded number of years that the\nuser has spent on Twitter, i.e., from the day the account was set up to the day of the current tweet. \u2022 Verified: a binary feature representing if the user has been verified by Twitter or not. Verified users are those whose identity Twitter has validated, and tend\nto be reputable people."}, {"heading": "7 Results", "text": ""}, {"heading": "7.1 Comparison of Classifiers", "text": "Table 3 shows the results for different classifiers using either or both of the contentbased and social features, as well as the results for the state-of-the-art classifier by (Zhao et al., 2015). Performance results of the classifiers using content-based features suggests a remarkable improvement for CRF over the rest of the classifiers, implying that CRF benefits from the use of the sequence of tweets preceding each tweet as context to enrich the input to the classifier. This is especially true when we look at precision, where CRF performs substantially better than the rest. Only the Naive Bayes classifier performs better in terms of recall, however, it performs poorly in terms of precision. As a result, CRF balances precision and recall in a clearly better way, outperforming all the other classifiers in terms of the F1 score.\nResults are not as clear when we look at those using social features. CRF still performs best in terms of precision, but performance drops if we look at the recall. In fact, most of the classifiers perform better than CRF in terms of recall, with SVM as the best performing classifier. Combining both precision and recall in an F1 score shows that SVM is the classifier that best exploits social features. However, performance results using social features are significantly worse than those using content-based features, which suggests that social features alone are not sufficient.\nWhen both content-based features and social features are combined as an input to the classifier, we see that the results resemble that of the use of content-based features alone. CRF outperforms all the rest in terms of precision, while Naive Bayes is good only in terms of recall. As a result, the aggregation of features also leads to CRF being the best classifier in terms of F1 score. In fact, CRF leads to an improvement of 39.9% over the second best classifier in terms of F1, Naive Bayes. If we compare the results of CRF with the use of content-based features alone or combining both types of features, we notice that the improvement comes especially for recall, which is balanced out with a slight drop of precision. As a result, we get an F1 score that is slightly better when using both features together. In fact, all F1 scores for combined features are superior to their counterparts using content-based features alone, among which CRF performs best.\nComparison with respect to the enquiry-based baseline approach introduced by Zhao et al. buttresses our conjecture that a manually curated list of regular expressions may lead to low recall, which is as low as 0.065 in this case. This approach gets a relatively good precision score, which beats all of our baselines, although it performs substantially worse than CRF. However, 59% of false positives as can be inferred from the precision of 0.41 indicates that the regular expressions also match non-rumours. One could also opt to expand the list of regular expressions and/or adapt them to our specific scenario and events; however, this may involve substantial manual work and would not guarantee generalisable performance."}, {"heading": "7.2 Consistency of the Sequential Classifier\u2019s Performance", "text": "Even though CRF as a sequential classifier has proven to perform better than the rest of the non-sequential classifiers overall, we are interested in seeing whether the high performance of CRF is consistent over time. Given that CRF depends on the sequence as context to enhance classification of a tweet, the first few tweets in each event lack the context that later tweets have. To analyse performance over time, we look at the F1 scores for each decile and for each event separately. Figure 3 shows these results, broken down by event and decile; thick, orange bars represent the F1 score of the CRF classifier in each decile, while the thinner, grey bars represent the highest F1 score across all non-sequential classifiers in each decile. We make some interesting observations from these results:\n\u2022 CRF generally outperforms the best of the non-sequential classifiers, also when breaking down the results by decile; for the 50 deciles in our dataset,\nthis is true in 43 of the cases, with only 7 cases where another classifier performs better in a decile. \u2022 CRF does suffer from a cold start problem at the beginning of each event, which is especially noticeable with Charlie Hebdo and Ottawa shootings. In\nthe five events under study, CRF performs better in the second decile than in the first, and it tends to perform better in later deciles than in the first. This may indicate, as we conjectured, that CRF can only make use of a short sequence, providing little context, when classifying the very first tweets."}, {"heading": "8 Discussion", "text": "Our rumour detection experiments on five datasets, each associated with a breaking news story, show that a classifier that sequentially exploits context from earlier tweets achieves significant improvements over standard, non-sequential classifiers. We have proven this in the case of a CRF classifier, which substantially outperforms its non-sequential counterpart, a Maximum Entropy classifier, as well as other nonsequential classifiers. Moreover, our approach also beats the state-of-the-art baseline by (Zhao et al., 2015) that uses regular expressions to classify as rumours the tweets that provoke reactions matching certain patterns. The latter fails to achieve a competitive recall score, which we argue is for two main reasons: (1) rumours will not always ignite enquiring reactions from the general public, and (2) the manually curated regular expressions might be limited, requiring regular updates involving a human in the loop. Our fully automated sequential classifiers can, instead, classify a tweet as a rumour or non-rumour from its own content and context from earlier tweets, without having to wait for any reactions.\nIn contrast with previous work in related tasks dealing with rumours, our work here has covered a wide range of rumours and non-rumours that were not necessarily known a priori. This was possible thanks to having as annotators a team of journalists who had followed the events closely and for the way the annotation work was performed, i.e., showing them a timeline of tweets that enabled them discovering rumours and non-rumours that one may have initially missed. While we are confident that this approach covers a diverse range of rumours and non-rumours, one caveat that it is important to note is that it is restricted to a subset of highly retweeted tweets. Consequently, our experiments have been limited to tweets being\nretweeted at least 100 times. This is consistent with one of the key characteristics of rumours, i.e., that they have to attract a substantial interest to be deemed rumours. While this is sensible for the task of rumour detection and the objectives of our work, it is necessary to wait until a tweet gets retweeted a number of times before it can be considered a candidate for input to the classifier. The development of a classifier that identifies these highly retweeted tweets promptly would enable early detection of rumours by not having to wait for the tweets to reach a certain threshold of retweets."}, {"heading": "9 Conclusion", "text": "We have introduced a novel approach to rumour detection in social media by leveraging the context preceding a tweet with a sequential classifier. Experimenting over five breaking news datasets collected from Twitter and annotated for rumours and non-rumours by journalists, we show that the preceding context exploited as a sequence can substantially boost the rumour detector\u2019s performance. Our approach has also proven to outperform the state-of-the-art rumour detection system introduced by (Zhao et al., 2015) that, instead, relies on find querying posts that match a set of manually curated list of regular expressions. Their approach performs well in terms of precision but fails in terms of recall, suggesting that regular manual input is needed to revise the regular expressions. Our fully automated approach instead achieves superior performance that is better balanced for both precision and recall.\nSocial media and user-generated content (UGC) are increasingly important features in a number of different ways for the work of not only journalists but also government agencies such as the police and civil protection agencies (Procter et al., 2013a). However, their use present major challenges, not least because information posted on social media is not always reliable and its veracity needs to be checked before it can be considered as fit for use in the reporting of news, or decision-making in the case of criminal activity (Procter et al., 2013a) or disaster response (Bazerli et al., 2015). Hence, it is vital that tools be developed that can aid a) the detection of rumours and b) determining their likely veracity. In the Pheme project (Derczynski et al., 2015), we have been developing tools that address the need for the latter (Zubiaga et al., 2016; Lukasik et al., 2016). However, for tools for rumour veracity determination to be effective, they need to be applied in combination with the former and progress so far has been limited. In this paper, we present a novel approach whose performance suggests it has the potential to address the former problem.\nWith this work we also make the annotated datasets publicly available to enable\nand encourage further research in the task4.\n4 https://figshare.com/articles/PHEME_dataset_of_rumours_and_non-rumours/ 4010619"}, {"heading": "10 Acknowledgments", "text": "This work has been supported by the PHEME FP7 project (grant No. 611233). This research utilised Queen Mary\u2019s MidPlus computational facilities, supported by QMUL Research-IT and funded by EPSRC grant EP/K000128/1."}], "references": [{"title": "An analysis of rumor", "author": ["G.W. Allport", "L. Postman"], "venue": "Public Opinion Quarterly,", "citeRegEx": "Allport and Postman,? 1946", "shortCiteRegEx": "Allport and Postman", "year": 1946}, {"title": "The psychology of rumor", "author": ["G.W. Allport", "L. Postman"], "venue": null, "citeRegEx": "Allport and Postman,? \\Q1947\\E", "shortCiteRegEx": "Allport and Postman", "year": 1947}, {"title": "Rumors detection in chinese via crowd responses", "author": ["G. Cai", "H. Wu", "R. Lv"], "venue": null, "citeRegEx": "Cai et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2014}, {"title": "Text processing with gate", "author": ["H. IEEE. Cunningham", "D. Maynard", "K. Bontcheva"], "venue": "International Conference on,", "citeRegEx": "Cunningham et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cunningham et al\\.", "year": 2011}, {"title": "How idle is idle talk? one hundred years of rumor research", "author": ["P. Donovan"], "venue": "Diogenes,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Rumor detection and classification for twitter", "author": ["S. Hamidian", "M.T. Diab"], "venue": null, "citeRegEx": "Hamidian and Diab,? \\Q2015\\E", "shortCiteRegEx": "Hamidian and Diab", "year": 2015}, {"title": "Rumor identification and belief investigation", "author": ["Communication", "M.T. Diab"], "venue": null, "citeRegEx": "Communication et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Communication et al\\.", "year": 2016}, {"title": "News verification by exploiting conflicting", "author": ["Z. Jin", "J. Cao", "Y. Zhang", "J. Luo"], "venue": "Proceedings of NAACL-HLT,", "citeRegEx": "Jin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2016}, {"title": "Rumor identification", "author": ["W. He", "C. Xu", "L. Chen", "J. Zeng"], "venue": "international conference on machine learning, ICML,", "citeRegEx": "G. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "G. et al\\.", "year": 2015}, {"title": "Real-time rumor debunking", "author": ["X. Liu", "A. Nourbakhsh", "Q. Li", "R. Fang", "S. Shah"], "venue": "Social Systems,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Using gaussian processes for rumour stance classification", "author": ["M. Lukasik", "T. Cohn", "K. Bontcheva"], "venue": null, "citeRegEx": "Lukasik et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lukasik et al\\.", "year": 2015}, {"title": "Detecting rumors from microblogs with recurrent neural networks", "author": ["J. Ma", "W. Gao", "P. Mitra", "S. Kwon", "B.J. Jansen", "Wong", "K.-F.", "M. Cha"], "venue": "Proceedings of IJCAI.", "citeRegEx": "Ma et al\\.,? 2016", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "Detect rumors using time series of social context information on microblogging websites", "author": ["J. Ma", "W. Gao", "Z. Wei", "Y. Lu", "Wong", "K.-F."], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 1751\u20131754. ACM.", "citeRegEx": "Ma et al\\.,? 2015", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Pystruct: learning structured prediction in python", "author": ["A.C. M\u00fcller", "S. Behnke"], "venue": "The Journal of Machine Learning Research, 15(1):2055\u20132060.", "citeRegEx": "M\u00fcller and Behnke,? 2014", "shortCiteRegEx": "M\u00fcller and Behnke", "year": 2014}, {"title": "Reading the riots: What were the police doing on twitter", "author": ["R. Procter", "J. Crump", "S. Karstedt", "A. Voss", "M. Cantijoch"], "venue": "Policing and society,", "citeRegEx": "Procter et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Procter et al\\.", "year": 2013}, {"title": "Reading the riots on twitter: methodological innovation for the analysis of big data", "author": ["R. Procter", "F. Vis", "A. Voss"], "venue": "International journal of social research methodology, 16(3):197\u2013214.", "citeRegEx": "Procter et al\\.,? 2013b", "shortCiteRegEx": "Procter et al\\.", "year": 2013}, {"title": "Rumor has it: Identifying misinformation in microblogs", "author": ["V. Qazvinian", "E. Rosengren", "D.R. Radev", "Q. Mei"], "venue": "Proceedings of EMNLP, pages 1589\u20131599.", "citeRegEx": "Qazvinian et al\\.,? 2011", "shortCiteRegEx": "Qazvinian et al\\.", "year": 2011}, {"title": "Twitterstand: news in tweets", "author": ["J. Sankaranarayanan", "H. Samet", "B.E. Teitler", "M.D. Lieberman", "J. Sperling"], "venue": "Proceedings of the 17th ACM SIGSPATIAL international conference on advances in geographic information systems, pages 42\u201351. ACM.", "citeRegEx": "Sankaranarayanan et al\\.,? 2009", "shortCiteRegEx": "Sankaranarayanan et al\\.", "year": 2009}, {"title": "Identifying rumors and their sources in social networks", "author": ["E. Seo", "P. Mohapatra", "T. Abdelzaher"], "venue": "SPIE defense, security, and sensing, pages 83891I\u201383891I. International Society for Optics and Photonics.", "citeRegEx": "Seo et al\\.,? 2012", "shortCiteRegEx": "Seo et al\\.", "year": 2012}, {"title": "Rumors, false flags, and digital vigilantes: Misinformation on twitter after the 2013 boston marathon bombing", "author": ["K. Starbird", "J. Maddock", "M. Orand", "P. Achterman", "R.M. Mason"], "venue": "iConference 2014 Proceedings.", "citeRegEx": "Starbird et al\\.,? 2014", "shortCiteRegEx": "Starbird et al\\.", "year": 2014}, {"title": "Detecting event rumors on sina weibo automatically", "author": ["S. Sun", "H. Liu", "J. He", "X. Du"], "venue": "Asia-Pacific Web Conference, pages 120\u2013131. Springer.", "citeRegEx": "Sun et al\\.,? 2013", "shortCiteRegEx": "Sun et al\\.", "year": 2013}, {"title": "An introduction to conditional random fields", "author": ["C. Sutton", "A. McCallum"], "venue": "Machine Learning, 4(4):267\u2013373.", "citeRegEx": "Sutton and McCallum,? 2011", "shortCiteRegEx": "Sutton and McCallum", "year": 2011}, {"title": "Rumor detection on twitter", "author": ["T. Takahashi", "N. Igata"], "venue": "Soft Computing and Intelligent Systems (SCIS) and 13th International Symposium on Advanced Intelligent Systems (ISIS), 2012 Joint 6th International Conference on, pages 452\u2013457. IEEE.", "citeRegEx": "Takahashi and Igata,? 2012", "shortCiteRegEx": "Takahashi and Igata", "year": 2012}, {"title": "Rumor diffusion and convergence during the 3.11 earthquake: a twitter case study", "author": ["M. Takayasu", "K. Sato", "Y. Sano", "K. Yamada", "W. Miura", "H. Takayasu"], "venue": "PLoS one,", "citeRegEx": "Takayasu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Takayasu et al\\.", "year": 2015}, {"title": "An analysis of event-agnostic features for rumour classification in twitter", "author": ["L. Tolosi", "A. Tagarev", "G. Georgiev"], "venue": "ICWSM Workshop on Social Media in the Newsroom.", "citeRegEx": "Tolosi et al\\.,? 2016", "shortCiteRegEx": "Tolosi et al\\.", "year": 2016}, {"title": "Digital wildfires: Propagation, verification, regulation, and responsible innovation", "author": ["H. Webb", "P. Burnap", "R. Procter", "O. Rana", "B. Stahl", "M. Williams", "W. Housley", "A. Edwards", "M. Jirotka"], "venue": "ACM Transactions on Information Systems, 34(3).", "citeRegEx": "Webb et al\\.,? 2016", "shortCiteRegEx": "Webb et al\\.", "year": 2016}, {"title": "False rumors detection on sina weibo by propagation structures", "author": ["K. Wu", "S. Yang", "K.Q. Zhu"], "venue": "2015 IEEE 31st International Conference on Data Engineering, pages 651\u2013662. IEEE.", "citeRegEx": "Wu et al\\.,? 2015", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": " unconfirmed: Classifying rumor stance in crisis-related social media messages", "author": ["L. Zeng", "K. Starbird", "E.S. Spiro"], "venue": "Tenth International AAAI Conference on Web and Social Media.", "citeRegEx": "Zeng et al\\.,? 2016", "shortCiteRegEx": "Zeng et al\\.", "year": 2016}, {"title": "Enquiring minds: Early detection of rumors in social media from enquiry posts", "author": ["Z. Zhao", "P. Resnick", "Q. Mei"], "venue": "Proceedings of the 24th International Conference on World Wide Web, pages 1395\u20131405. ACM.", "citeRegEx": "Zhao et al\\.,? 2015", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Curating and contextualizing twitter stories to assist with social newsgathering", "author": ["A. Zubiaga", "H. Ji", "K. Knight"], "venue": "Proceedings of the 2013 international conference on Intelligent user interfaces, pages 213\u2013224. ACM.", "citeRegEx": "Zubiaga et al\\.,? 2013", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2013}, {"title": "Crowdsourcing the annotation of rumourous conversations in social media", "author": ["A. Zubiaga", "M. Liakata", "R. Procter", "K. Bontcheva", "P. Tolmie"], "venue": "Proceedings of the 24th International Conference on World Wide Web Companion, pages 347\u2013353. International World Wide Web Conferences Steering Committee.", "citeRegEx": "Zubiaga et al\\.,? 2015", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2015}, {"title": "Analysing how people orient to and spread rumours in social media by looking at conversational threads", "author": ["A. Zubiaga", "M. Liakata", "R. Procter", "G. Wong Sak Hoi", "P. Tolmie"], "venue": "PLoS ONE, 11(3):1\u201329.", "citeRegEx": "Zubiaga et al\\.,? 2016", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "Well-known platforms such as Twitter are increasingly being used by people to learn about the latest developments (Sankaranarayanan et al., 2009), as well as by journalists for news gathering (Zubiaga et al.", "startOffset": 114, "endOffset": 145}, {"referenceID": 30, "context": ", 2009), as well as by journalists for news gathering (Zubiaga et al., 2013).", "startOffset": 54, "endOffset": 76}, {"referenceID": 26, "context": "A rumour detection system would ultimately warn users of the unverified status of a post, letting them know that it might later be proven false; this can be useful both to limit the diffusion of information that might turn out subsequently to be false and so reduce the risk of harm to individuals, communities and society (Webb et al., 2016).", "startOffset": 323, "endOffset": 342}, {"referenceID": 29, "context": "Research in rumour detection is scarce in the scientific literature, (Zhao et al., 2015) being the only published work to date that addresses this issue.", "startOffset": 69, "endOffset": 88}, {"referenceID": 32, "context": "Other work has dealt with \u201crumour detection\u201d with what we argue is a questionable definition and which conflicts with our own (Zubiaga et al., 2016).", "startOffset": 126, "endOffset": 148}, {"referenceID": 0, "context": "In our study we adhere to the prevailing definition in the scientific literature that understands a rumour as the information that is being circulated while its veracity is yet to be confirmed (Allport and Postman, 1946; DiFonzo and Bordia, 2007).", "startOffset": 193, "endOffset": 246}, {"referenceID": 29, "context": "The performance of CRF is compared with its non-sequential equivalent, a Maximum Entropy classifier, as well as the state-of-the-art rumour detection approach by (Zhao et al., 2015) and additional baseline classifiers.", "startOffset": 162, "endOffset": 181}, {"referenceID": 0, "context": "Moreover, (Allport and Postman, 1946) posit that one of the main reasons why rumours circulate is that \u201cthe topic has importance for the individual who hears and spreads the story\u201d.", "startOffset": 10, "endOffset": 37}, {"referenceID": 1, "context": "In (Allport and Postman, 1947), the authors also emphasise that \u201cnewsworthy events are likely to breed rumors\u201d and that \u201cthe amount of rumor in circulation will vary with the importance of the subject to the individuals involved times the ambiguity of the evidence pertaining to the topic at issue\u201d.", "startOffset": 3, "endOffset": 30}, {"referenceID": 32, "context": "Consistent with these definitions, we adhere here to a definition adapted to the context of breaking news, which we introduced in previous work (Zubiaga et al., 2016): a rumour is a \u201ccirculating story of questionable veracity, which is apparently credible but hard to verify, and produces sufficient skepticism and/or anxiety so as to motivate finding out the actual truth\u201d.", "startOffset": 144, "endOffset": 166}, {"referenceID": 16, "context": "Despite the increasing interest in analysing rumours in social media (Procter et al., 2013b; Procter et al., 2013a; Starbird et al., 2014; Zubiaga et al., 2015; Takayasu et al., 2015; Tolosi et al., 2016; Zubiaga et al., 2016) and the building of tools to deal with rumours that had been previously identified (Seo et al.", "startOffset": 69, "endOffset": 226}, {"referenceID": 20, "context": "Despite the increasing interest in analysing rumours in social media (Procter et al., 2013b; Procter et al., 2013a; Starbird et al., 2014; Zubiaga et al., 2015; Takayasu et al., 2015; Tolosi et al., 2016; Zubiaga et al., 2016) and the building of tools to deal with rumours that had been previously identified (Seo et al.", "startOffset": 69, "endOffset": 226}, {"referenceID": 31, "context": "Despite the increasing interest in analysing rumours in social media (Procter et al., 2013b; Procter et al., 2013a; Starbird et al., 2014; Zubiaga et al., 2015; Takayasu et al., 2015; Tolosi et al., 2016; Zubiaga et al., 2016) and the building of tools to deal with rumours that had been previously identified (Seo et al.", "startOffset": 69, "endOffset": 226}, {"referenceID": 24, "context": "Despite the increasing interest in analysing rumours in social media (Procter et al., 2013b; Procter et al., 2013a; Starbird et al., 2014; Zubiaga et al., 2015; Takayasu et al., 2015; Tolosi et al., 2016; Zubiaga et al., 2016) and the building of tools to deal with rumours that had been previously identified (Seo et al.", "startOffset": 69, "endOffset": 226}, {"referenceID": 25, "context": "Despite the increasing interest in analysing rumours in social media (Procter et al., 2013b; Procter et al., 2013a; Starbird et al., 2014; Zubiaga et al., 2015; Takayasu et al., 2015; Tolosi et al., 2016; Zubiaga et al., 2016) and the building of tools to deal with rumours that had been previously identified (Seo et al.", "startOffset": 69, "endOffset": 226}, {"referenceID": 32, "context": "Despite the increasing interest in analysing rumours in social media (Procter et al., 2013b; Procter et al., 2013a; Starbird et al., 2014; Zubiaga et al., 2015; Takayasu et al., 2015; Tolosi et al., 2016; Zubiaga et al., 2016) and the building of tools to deal with rumours that had been previously identified (Seo et al.", "startOffset": 69, "endOffset": 226}, {"referenceID": 19, "context": ", 2016) and the building of tools to deal with rumours that had been previously identified (Seo et al., 2012; Takahashi and Igata, 2012), there has been very little work in automatic rumour detection.", "startOffset": 91, "endOffset": 136}, {"referenceID": 23, "context": ", 2016) and the building of tools to deal with rumours that had been previously identified (Seo et al., 2012; Takahashi and Igata, 2012), there has been very little work in automatic rumour detection.", "startOffset": 91, "endOffset": 136}, {"referenceID": 17, "context": "Some of the work in rumour detection (Qazvinian et al., 2011; Hamidian and Diab, 2015; Hamidian and Diab, 2016) has been limited to finding rumours known a priori.", "startOffset": 37, "endOffset": 111}, {"referenceID": 5, "context": "Some of the work in rumour detection (Qazvinian et al., 2011; Hamidian and Diab, 2015; Hamidian and Diab, 2016) has been limited to finding rumours known a priori.", "startOffset": 37, "endOffset": 111}, {"referenceID": 29, "context": "To the best of our knowledge, the only work that has tackled the detection of new rumours is that by (Zhao et al., 2015).", "startOffset": 101, "endOffset": 120}, {"referenceID": 32, "context": "While this work builds on a sensible hypothesis and presents a clever approach to tackling the rumour detection task, we foresee three potential limitations: (1) being based on manually curated regular expressions the approach may not generalise well, (2) the hypothesis might not always apply and hence lead to low recall as, for example, certain rumours reported by reputable media are not always questioned by the general public (Zubiaga et al., 2016), and (3) it takes no account of the context that precedes the rumour, which can give additional insights into what is going on and how a piece of information can be rumourous in that context (e.", "startOffset": 432, "endOffset": 454}, {"referenceID": 17, "context": "For instance, there is an increasing body of work (Qazvinian et al., 2011; Liu et al., 2015; Hamidian and Diab, 2015; Hamidian and Diab, 2016; Lukasik et al., 2015; Zeng et al., 2016) looking into stance classification of tweets discussing rumours, categorising tweets as supporting, denying or questioning the rumour.", "startOffset": 50, "endOffset": 183}, {"referenceID": 9, "context": "For instance, there is an increasing body of work (Qazvinian et al., 2011; Liu et al., 2015; Hamidian and Diab, 2015; Hamidian and Diab, 2016; Lukasik et al., 2015; Zeng et al., 2016) looking into stance classification of tweets discussing rumours, categorising tweets as supporting, denying or questioning the rumour.", "startOffset": 50, "endOffset": 183}, {"referenceID": 5, "context": "For instance, there is an increasing body of work (Qazvinian et al., 2011; Liu et al., 2015; Hamidian and Diab, 2015; Hamidian and Diab, 2016; Lukasik et al., 2015; Zeng et al., 2016) looking into stance classification of tweets discussing rumours, categorising tweets as supporting, denying or questioning the rumour.", "startOffset": 50, "endOffset": 183}, {"referenceID": 10, "context": "For instance, there is an increasing body of work (Qazvinian et al., 2011; Liu et al., 2015; Hamidian and Diab, 2015; Hamidian and Diab, 2016; Lukasik et al., 2015; Zeng et al., 2016) looking into stance classification of tweets discussing rumours, categorising tweets as supporting, denying or questioning the rumour.", "startOffset": 50, "endOffset": 183}, {"referenceID": 28, "context": "For instance, there is an increasing body of work (Qazvinian et al., 2011; Liu et al., 2015; Hamidian and Diab, 2015; Hamidian and Diab, 2016; Lukasik et al., 2015; Zeng et al., 2016) looking into stance classification of tweets discussing rumours, categorising tweets as supporting, denying or questioning the rumour.", "startOffset": 50, "endOffset": 183}, {"referenceID": 21, "context": "There is also work on veracity classification both in the context of rumours and beyond (Sun et al., 2013; Cai et al., 2014; Liang et al., 2015; Liu et al., 2015; Ma et al., 2015; Wu et al., 2015; Ma et al., 2016; Jin et al., 2016).", "startOffset": 88, "endOffset": 231}, {"referenceID": 2, "context": "There is also work on veracity classification both in the context of rumours and beyond (Sun et al., 2013; Cai et al., 2014; Liang et al., 2015; Liu et al., 2015; Ma et al., 2015; Wu et al., 2015; Ma et al., 2016; Jin et al., 2016).", "startOffset": 88, "endOffset": 231}, {"referenceID": 9, "context": "There is also work on veracity classification both in the context of rumours and beyond (Sun et al., 2013; Cai et al., 2014; Liang et al., 2015; Liu et al., 2015; Ma et al., 2015; Wu et al., 2015; Ma et al., 2016; Jin et al., 2016).", "startOffset": 88, "endOffset": 231}, {"referenceID": 12, "context": "There is also work on veracity classification both in the context of rumours and beyond (Sun et al., 2013; Cai et al., 2014; Liang et al., 2015; Liu et al., 2015; Ma et al., 2015; Wu et al., 2015; Ma et al., 2016; Jin et al., 2016).", "startOffset": 88, "endOffset": 231}, {"referenceID": 27, "context": "There is also work on veracity classification both in the context of rumours and beyond (Sun et al., 2013; Cai et al., 2014; Liang et al., 2015; Liu et al., 2015; Ma et al., 2015; Wu et al., 2015; Ma et al., 2016; Jin et al., 2016).", "startOffset": 88, "endOffset": 231}, {"referenceID": 11, "context": "There is also work on veracity classification both in the context of rumours and beyond (Sun et al., 2013; Cai et al., 2014; Liang et al., 2015; Liu et al., 2015; Ma et al., 2015; Wu et al., 2015; Ma et al., 2016; Jin et al., 2016).", "startOffset": 88, "endOffset": 231}, {"referenceID": 7, "context": "There is also work on veracity classification both in the context of rumours and beyond (Sun et al., 2013; Cai et al., 2014; Liang et al., 2015; Liu et al., 2015; Ma et al., 2015; Wu et al., 2015; Ma et al., 2016; Jin et al., 2016).", "startOffset": 88, "endOffset": 231}, {"referenceID": 17, "context": "Our data collection approach differs substantially from that of previous work (Qazvinian et al., 2011; Procter et al., 2013b; Starbird et al., 2014), who first identified the rumours of interest and then collected tweets associated with those by filtering using relevant keywords.", "startOffset": 78, "endOffset": 148}, {"referenceID": 16, "context": "Our data collection approach differs substantially from that of previous work (Qazvinian et al., 2011; Procter et al., 2013b; Starbird et al., 2014), who first identified the rumours of interest and then collected tweets associated with those by filtering using relevant keywords.", "startOffset": 78, "endOffset": 148}, {"referenceID": 20, "context": "Our data collection approach differs substantially from that of previous work (Qazvinian et al., 2011; Procter et al., 2013b; Starbird et al., 2014), who first identified the rumours of interest and then collected tweets associated with those by filtering using relevant keywords.", "startOffset": 78, "endOffset": 148}, {"referenceID": 31, "context": "Afterwards, journalists read through the timeline to mark each of the tweets as being a rumour or not, making sure that the identification of rumours was in line with the established criteria (Zubiaga et al., 2015).", "startOffset": 192, "endOffset": 214}, {"referenceID": 29, "context": "We use the replying tweets for two purposes: (1) for the manual annotation work, where replies to each tweet can provide context for the annotator where needed to decide if a tweet is a rumour, and (2) we use them to reproduce a baseline classifier based on the baseline introduced by (Zhao et al., 2015).", "startOffset": 285, "endOffset": 304}, {"referenceID": 29, "context": "One possibility to extend a tweet with context is to look at how others react to it, as (Zhao et al., 2015) proposed in their work that querying or enquiring tweets provoked by a posting may indicate it is a rumour.", "startOffset": 88, "endOffset": 107}, {"referenceID": 29, "context": "Moreover, we also reproduce a baseline based on the approach introduced by (Zhao et al., 2015) to compare the performance of our approach with that of a state-of-the-art approach.", "startOffset": 75, "endOffset": 94}, {"referenceID": 22, "context": "The generalisable conditional distribution of CRF is shown in Equation 1 (Sutton and McCallum, 2011).", "startOffset": 73, "endOffset": 100}, {"referenceID": 29, "context": "Enquiry-based approach by (Zhao et al., 2015): As a state-of-the-art baseline for rumour detection, and the only approach that so far has tackled rumour detection in social media, we reproduce the approach by Zhao et al.", "startOffset": 26, "endOffset": 45}, {"referenceID": 14, "context": "2 We use the PyStruct to implement Conditional Random Fields (M\u00fcller and Behnke, 2014).", "startOffset": 61, "endOffset": 86}, {"referenceID": 13, "context": "\u2022 Word Vectors: to create vectors representing the words in each tweet, we build word vector representations using Word2Vec (Mikolov et al., 2013).", "startOffset": 124, "endOffset": 146}, {"referenceID": 3, "context": ", 2013) to parse the tweets for POS tags, an information extraction package that is part of GATE (Cunningham et al., 2011).", "startOffset": 97, "endOffset": 122}, {"referenceID": 29, "context": "Table 3 shows the results for different classifiers using either or both of the contentbased and social features, as well as the results for the state-of-the-art classifier by (Zhao et al., 2015).", "startOffset": 176, "endOffset": 195}, {"referenceID": 29, "context": "(Zhao et al., 2015) 0.", "startOffset": 0, "endOffset": 19}, {"referenceID": 29, "context": "Moreover, our approach also beats the state-of-the-art baseline by (Zhao et al., 2015) that uses regular expressions to classify as rumours the tweets that provoke reactions matching certain patterns.", "startOffset": 67, "endOffset": 86}, {"referenceID": 29, "context": "Our approach has also proven to outperform the state-of-the-art rumour detection system introduced by (Zhao et al., 2015) that, instead, relies on find querying posts that match a set of manually curated list of regular expressions.", "startOffset": 102, "endOffset": 121}, {"referenceID": 32, "context": ", 2015), we have been developing tools that address the need for the latter (Zubiaga et al., 2016; Lukasik et al., 2016).", "startOffset": 76, "endOffset": 120}], "year": 2016, "abstractText": "Breaking news leads to situations of fast-paced reporting in social media, producing all kinds of updates related to news stories, albeit with the caveat that some of those early updates tend to be rumours, i.e., information with an unverified status at the time of posting. Flagging information that is unverified can be helpful to avoid the spread of information that may turn out to be false. Detection of rumours can also feed a rumour tracking system that ultimately determines their veracity. In this paper we introduce a novel approach to rumour detection that learns from the sequential dynamics of reporting during breaking news in social media to detect rumours in new stories. Using Twitter datasets collected during five breaking news stories, we experiment with Conditional Random Fields as a sequential classifier that leverages context learnt during an event for rumour detection, which we compare with the state-of-the-art rumour detection system as well as other baselines. In contrast to existing work, our classifier does not need to observe tweets querying a piece of information to deem it a rumour, but instead we detect rumours from the tweet alone by exploiting context learnt during the event. Our classifier achieves competitive performance, beating the state-of-the-art classifier that relies on querying tweets with improved precision and recall, as well as outperforming our best baseline with nearly 40% improvement in terms of F1 score. The scale and diversity of our experiments reinforces the generalisability of our classifier.", "creator": "LaTeX with hyperref package"}}}