{"id": "1610.08095", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Modeling Ambiguity, Subjectivity, and Diverging Viewpoints in Opinion Question Answering Systems", "abstract": "Product review websites provide an incredible lens into the wide variety of opinions and experiences of different people, and play a critical role in helping users discover products that match their personal needs and preferences. To help address questions that can't easily be answered by reading others' reviews, some review websites also allow users to pose questions to the community via a question-answering (QA) system. As one would expect, just as opinions diverge among different reviewers, answers to such questions may also be subjective, opinionated, and divergent.\n\n\n\n\n\"The question that can't be answered by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews, or by reading others' reviews,", "histories": [["v1", "Tue, 25 Oct 2016 21:08:15 GMT  (242kb,D)", "http://arxiv.org/abs/1610.08095v1", "10 pages, accepted by ICDM'2016"]], "COMMENTS": "10 pages, accepted by ICDM'2016", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["mengting wan", "julian mcauley"], "accepted": false, "id": "1610.08095"}, "pdf": {"name": "1610.08095.pdf", "metadata": {"source": "CRF", "title": "Modeling Ambiguity, Subjectivity, and Diverging Viewpoints in Opinion Question Answering Systems", "authors": ["Mengting Wan", "Julian McAuley"], "emails": ["m5wan@eng.ucsd.edu", "jmcauley@eng.ucsd.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION User-generated reviews are a valuable resource to help people make decisions. Reviews may contain a wide range of both objective and subjective product-related information, including features of the product, evaluations of its positive and negative attributes, and various personal experiences and niche use-cases. Although a key factor in guiding many people\u2019s decisions, it can be time-consuming for a user to digest the content in large volumes of reviews, many of which may not be relevant to their own opinions or interests.\nIn addition to passively searching for information that users are interested in among reviews, a number of ecommerce websites, such as Amazon and ebay, also provide community question answering systems where users can ask and answer specific product-related questions. While such systems allow users to seek targeted information (as opposed for searching for it in reviews), asking the community is still time-consuming in the sense that the user must wait for a response, and even then may have quite different preferences from the user who answers their questions.\nThe above issues motivate us to study systems that help users to automatically navigate large volumes of reviews in\norder to locate relevant and informative opinions, in response to a particular query.\nThis kind of \u2018opinion question answering\u2019 system (opinion QA) is quite different from typical community question answering (cQA) systems. In particular, traditional cQA systems are usually concerned with objective information, such that answers can be generated by constructing and exploring a knowledge-base which is composed of facts.\nHowever, for the opinion QA problem, users often ask for subjective information, such as \u201cIs this a good lens for my Nikon D3300 camera?\u201d Such a seemingly simple question is complex because it depends on (a) objective information (is the lens even compatible?); (b) subjective information (whether it\u2019s \u2018good\u2019 is a matter of opinion); and (c) personalization (which answer is correct for the user asking the question; are they an expert? an amateur? on a budget? etc.). Perhaps not surprisingly, opinion QA systems generate a wide variety of subjective and possibly contradictory answers (see Figure 1, from Amazon).\nIdeally answers to this kind of question should leverage data describing personal opinions and experiences, such as the kind of information available in product reviews. To build systems capable of leveraging such information, a series of methods [1]\u2013[3] for product-related opinion ques-\nar X\niv :1\n61 0.\n08 09\n5v 1\n[ cs\n.I R\n] 2\n5 O\nct 2\n01 6\ntions answering have been developed. These methods can automatically retrieve potential answers from reviews based on different linguistic features. Many of these approaches develop information retrieval systems where traditional text similarity measures are explored and text fragments are filtered based on question types or the attributes of the product that users refer to in their questions [1], [2].\nRecently, a supervised approach, Mixtures of Opinions for Question Answering (MoQA) [3], was developed for opinion QA systems using product reviews. There, product-related questions were categorized into two types as follows: \u2022 Binary questions. A large fraction of questions in real-\nworld opinion QA data are binary questions where answers amount to either \u2018Yes\u2019 or \u2018No\u2019. Such answers can easily be detected (i.e., to build a labeled dataset) in a supervised setting using a binary classifier [4]. When addressing binary questions, we are interested both in mining relevant opinions from reviews, but also providing a yes/no answer directly. \u2022 Open-ended questions. In addition to binary questions, a significant number of product-related questions are open-ended, or compound questions (etc.). It is usually impractical to answer such questions directly with an automated system. Instead, we are more interested in learning a good relevance function which can help us retrieve useful information from reviews, so that the user can be aided in reaching a conclusion themselves. In this paper, we continue to study these two types of questions. Where we extend existing work, and the main contribution of our paper, is to explicitly account for the fact that questions may have multiple, subjective, and possibly contradictory answers.1 We evaluate our system by collecting a new QA dataset from Amazon.com\u2014consisting of 800 thousand questions and 3.1 million answers, which uses all of the available answers for training (in contrast to previous approaches, where each question was associated with only a single answer).\nOur main goals are to show quantitatively that by leveraging multiple answers in a supervised framework we can provide more accurate responses to both subjective and objective questions (where \u2018accurate\u2019 for a subjective question means that we can correctly estimate the distribution of views). Qualitatively, we aim to build systems that are capable of presenting users with a more nuanced selection of supporting evidence, capturing the full spectrum of relevant opinions."}, {"heading": "A. Ambiguity and Subjectivity in Opinion QA Systems", "text": "Addressing this new view of question-answering is challenging, and requires new techniques to be developed in order to make use of multiple, possibly contradictory labels\n1Note that even binary questions may still be subjective, such that both \u2018yes\u2019 and \u2018no\u2019 answers may be possible.\nwithin a supervised framework. We identify two main perspectives from which ambiguity and subjectivity in productrelated opinion QA systems can be studied:\n\u2022 Multiple Answers. We notice that in previous studies, only one ground-truth answer is included for each question. However, in real-world opinion QA systems, multiple answers are often available. We find this to be true both for binary and open-ended questions. When multiple answers are available, they often describe different aspects of the questions or different personal experiences. By including multiple answers at training time, we expect that the relevant reviews retrieved by the system at test time should cover those subjective responses more comprehensively. \u2022 Subjective Reviews. In addition, as indicated in traditional opinion mining studies, reviews as reflections of users\u2019 opinions may be subjective since different reviewers may have different expertise and bias. In some review websites, such as Amazon.com, review rating scores and review helpfulness can be obtained, which could be good features reflecting the subjectivity of the reviews. Intuitively, subjective information may affect the language that users apply to express their opinion so that their reviews should be handled to address questions accordingly. For example, \u2018picky\u2019 reviewers may tend to provide negative responses while \u2018generous\u2019 reviewers may usually provide more favorable information about the product. This motivates us to apply user modeling approaches and incorporate more subjective review-related features into opinion QA systems.\nThe above observations provide us with a strong motivation to study ambiguity and subjectivity from the perspective of multiple answers and subjective reviews in opinion QA systems. We conclude by stating the problem specifically as follows:\nGoal: Given a question related to a product, we would like to determine how relevant each review of that product is to the question with emphasis on modeling ambiguity and subjectivity, where \u2018relevance\u2019 is measured in terms of how helpful the review will be in terms of identifying the proper response (or responses) to the question."}, {"heading": "B. Contributions", "text": "To our knowledge, our study is the first one to systematically model ambiguity and subjectivity in opinion QA systems. We provide a new dataset consisting of 135 thousands product from Amazon, 808 thousand questions, 3 million answers and 11 million reviews.2 By modeling ambiguity in product-related questions, this study not only bridges QA systems and reviews, but also bridges opinion mining and the\n2Data and code are available on the first author\u2019s webpage.\nidea of \u2018learning from crowds.\u2019 For both binary and openended questions, we successfully develop a model to handle multiple (and possibly conflicting) answers and incorporate subjective features, where labels are predicted for binary questions, and a relevance-ranked list of reviews is surfaced to the user. Quantitatively, we show that modeling ambiguity and subjectivity leads to substantial performance gains in terms of the accuracy of our question answering system."}, {"heading": "II. BACKGROUND", "text": "In this study, we build upon the mixture of experts (MoE) framework as used previously by [3]. We enhance this approach by modeling ambiguity and subjectivity from the perspectives of answers and reviews. Before introducing the complete model, we introduce standard relevance measures and the mixture of experts (MoE) framework as background knowledge. The basic notation used throughout this paper is provided in Table I."}, {"heading": "A. Standard Relevance Measures", "text": "We first describe two kinds of similarity measures for relevance ranking in the context of our opinion QA problem as follows.\n1) Okapi BM25: One of the standard relevance ranking measures for information retrieval, Okapi BM25 is a bag-ofwords \u2018tf-idf\u2019-based ranking function that has been successfully applied in a number of problems including QA tasks [5], [6]. Particularly, for a given question q and a review r, the standard BM25 measure is defined as\nbm25 (q, r) = n\u2211 i=1 idf (qi)\u00d7 f(qi, r)\u00d7 (k1 + 1) f(qi, r) + k1 \u00d7 (1\u2212 b+ b\u00d7 |r|avgrl ) , (1)\nwhere qi, i = 1, . . . , n are keywords in q, f(qi, r) denotes the frequency of qi in r, |r| is the length of review r and avgrl is the average review length among all reviews.3 Here idf (qi), the inverse document frequency of qi, is defined as\nidf(qi) = log N \u2212 n(qi) + 0.5 n(qi) + 0.5 , (2)\nwhere N = |R| is the total number of reviews and n(qi) is the number of reviews which contain qi.\n2) Rouge-L: Next we consider another similarity measure, Rouge-L [7], which is a Longest Common Subsequence (LCS) based statistic. For a question q and a review r, if the length of their longest common subsequence is denoted as LCS (q, r), then we have RLCS = LCS (q, r)/|q| and PLCS = LCS (q, r)/|r|. Now Rouge-L is defined as\nFLCS = (1 + \u03b22)RLCSPLCS RLCS + \u03b22PLCS , (3)\nwhere \u03b2 = PLCS/RLCS .\n3In practice we set k1 = 1.5 and b = 0.75."}, {"heading": "B. Mixtures of Experts", "text": "Mixtures of experts (MoE) [8] is a supervised learning approach that smoothly combines the outputs of several \u2018weak\u2019 classifiers in order to generate predictions. Here, this method can be applied for opinion QA systems where each individual review is regarded as a weak classifier that makes a prediction about the response to a query. For each classifier (review), we output a relevance/confidence score (how relevant is this review to the query?), as well as a prediction (e.g. is the response \u2018yes\u2019 based on the evidence in this review?). Then an overall prediction can be obtained for a particular question by combining outputs from all reviews of a product, weighted by their confidence.\n1) MoE for binary questions: For a binary question, each classifier produces a probability associated with a positive label, i.e., a probability that the answer is \u2018yes.\u2019 Suppose for a question q, the associated features (including the text itself, the identity of the querier, etc.) are denoted Xq and the label for this question is denoted as yq (yq \u2208 {0, 1}). Then we have\nP (yq|Xq) = \u2211 r\u2208Rq how relevant is r\ufe37 \ufe38\ufe38 \ufe37 P (r|Xq) \u00d7 prediction from r\ufe37 \ufe38\ufe38 \ufe37 P (yq|r,Xq) , (4)\nwhere r is a review among the set of reviews Rq associated with the question q. In (4), P (r|Xq) measures the confidence of review r\u2019s ability in terms of responding to the question q, and P (yq|r,Xq) is the prediction for q given by review r. These two terms can be modeled as follows:\n(Relevance) P (r|Xq) = exp(vq,r)/ \u2211\nr\u2032\u2208Rq\nexp(vq,r\u2032);\n(Prediction) P (yq = 1|r,Xq) = \u03c3(wq,r), (5)\nwhere \u03c3(x) = 1/(1 + exp(\u2212x)) is the sigmoid function. Here vq,r and wq,r are real-valued (i.e., unnormalized) \u2018relevance\u2019 and \u2018prediction\u2019 scores where multiple question and review related features can be involved.\n2) MoE for open-ended questions: Similarly, for an openended question, we may be interested in whether a \u2018true\u2019 answer aq is preferred over some arbitrary non-answer a\u0304. For this we have a similar MoE structure as follows:\nP (aq > a\u0304|Xq) = \u2211 r\u2208Rq P (r|Xq)P (aq > a\u0304|r,Xq). (6)\nThe relevance term can be kept the same while we have a slightly different prediction term:\nP (aq > a\u0304|r,Xq) = \u03c3(waq>a\u0304,r). (7)\nHere waq>a\u0304,r is a real-valued \u2018prediction\u2019 score where multiple answer and review features can be included."}, {"heading": "C. Relevance and Prediction with Text-Only Features", "text": "As described above, for a binary question, the probability associated with a positive (i.e., \u2018yes\u2019) label P (yq = 1|Xq) (pq in shorthand) can be modeled using an MoE framework\nwhere each review is regarded as a weak classifier. If only one label is included for a question in the training procedure, we can train by maximizing the following log-likelihood:\nL = logP (Y|X ) = \u2211 q [yq log pq + (1\u2212 yq) log(1\u2212 pq)] (8)\nwhere \u0398 includes all parameters and pq is modeled as in (4).\nA number of features can be applied to define the \u2018relevance\u2019 (vq,r) and \u2018prediction\u2019 (wq,r) functions. Previously in [3], only text features were used to define pairwise similarity measures and bilinear models. Starting with the same textonly model, suppose fq and fr are vectors with length N that represent bag-of-words text features for question q and review r. Then we define the \u2018relevance\u2019 function as follows:\nvq,r = pairwise similarities (bm25 etc.)\ufe37 \ufe38\ufe38 \ufe37 \u3008\u03ba, s(q, r)\u3009 + term-to-term similarity\ufe37 \ufe38\ufe38 \ufe37 \u3008\u03b7,fq \u25e6 fr\u3009 , (9)\nwhere x\u25e6y is the Hadamard product. Note that we have two parts in vq,r: (1) a weighted combination of state-of-the-art pairwise similarities; and (2) a parameterized term-to-term similarity. Following [3], we include BM25 [5] and RougeL [7] measures in s(q, r). Recall that the purpose of this function is to learn a set of parameters {\u03ba,\u03b7} that ranks reviews in order of relevance.\nIn addition, we define the following prediction function:\nwq,r = interaction between q. & r. text\ufe37 \ufe38\ufe38 \ufe37 \u3008\u00b5,fq \u25e6 fr\u3009 + prediction from r. text\ufe37 \ufe38\ufe38 \ufe37 \u3008\u03be,fr\u3009 . (10)\nThe idea here is that the first term models the interaction between the question and review text, while the second models only the review (which can capture e.g. sentiment words in the review).\nTraining: Finally, to optimize the parameters \u0398 = {\u03ba,\u03b7,\u00b5,u} from (9) and (10), we apply L-BFGS [9]. To avoid overfitting, this model also includes a simple L2 regularizer on all model parameters."}, {"heading": "III. AMBIGUITY AND SUBJECTIVITY IN BINARY QUESTIONS", "text": "So far, we have followed the basic approach of [3], assuming text-only features, and a single label (answer) associated with each question. But as we find in our data (see Section V), responses in real-world opinion QA systems have significant ambiguity, even for binary questions. However, in previous studies, only a single response was considered to each question. In this section we develop additional machinery allowing us to model ambiguity and subjectivity, and in particular to handle training data with multiple (and possibly contradictory) answers."}, {"heading": "A. Modeling Ambiguity: Learning with Multiple Labels.", "text": "Notice that in the previous log-likelihood exppression (8), only one label can be included for each question. Below are two options to extend this framework to handle multiple labels.\n1) KL-MoE: A straightforward approach is to replace the single label yq in (8) by the the fraction of positive labels rq = n+q /(n + q + n \u2212 q ), where n + q , n \u2212 q are the number of positive and negative (yes/no) answers for question q. If we assume that for a question q, the response provided from the answers given follows Bernoulli(rq) and the response predicted from reviews follows Bernoulli(pq), then the objective function\u2211\nq\n[rq log pq + (1\u2212 rq) log(1\u2212 pq)] (11)\ncan be regarded as the summation of the KL-divergences between answers and predictions for all questions.\n2) EM-MoE: Note that only the ratio of positive and negative labels is included in the previous KL-divergence loss (11), while the real counts of positive and negative labels are discarded. However, this fraction may not be enough to model the strength of the ambiguity (or controversy) in the question. For example, a question with 10 positive and 10 negative labels seems more controversial than a question with 1 positive and 1 negative label. However, their positive/negative ratios rq are the same.\nTo distinguish such cases, instead of applying a fixed ratio rq , we use two sets of parameters, allowing us to incorporate multiple noisy labels at training time, and to update (our noisy estimate of) rq based on multiple labels yq,j and generated predictions pq iteratively using the EM-algorithm.\nSpecifically, for a binary question q, we model its \u2018true\u2019 answer yq as an unknown with probability distribution P (yq = 1|Xq,\u0398), which is assumed to generate the provided (noisy) labels yq,j (j = 1, . . . , nq) independently.\nThen the joint probability of the observed labels is given by\nP (yq,1, . . . , yq,nq |Xq,\u0398) = \u2211\ni\u2208{0,1}\nP (yq,1, . . . , yq,nq |yq = i,Xq,\u0398)P (yq = i|Xq,\u0398)\n= \u2211\ni\u2208{0,1} ( nq\u220f j=1 P (yq,j |yq = i,Xq,\u0398) ) P (yq = i|Xq,\u0398)\n(12) Here we separate the joint probability into two parts: \u2022 P (yq = i|Xq,\u0398) models the estimated distribution of the\n\u2018true\u2019 answer yq from the provided reviews. \u2022 \u220fnq\nj=1 P (yq,j |yq = i,Xq,\u0398) models the probability of a given ground-truth label yq,j as a function of yq . Letting \u03b1q = P (yq,j = 1|yq = 1, Xq,\u0398) and \u03b2q = P (yq,j = 0|yq = 0, Xq,\u0398) for all j \u2208 Sq , then \u03b1q and \u03b2q represent the \u2018sensitivity\u2019 (probability of a positive observation if the true label is positive) and \u2018specificity\u2019 (probability of a negative observation if the label is negative) for question q.\nNote that \u2018positive\u2019 and \u2018negative\u2019 questions may not be symmetric concepts (i.e., different types of questions may be more likely to have yes vs. no answers). Thus we model sensitivity and specificity separately, using features from the question text as prior knowledge. Specifically, we model \u03b1 and \u03b2 as:\n\u03b1q = \u03c3(\u3008\u03b31,fq\u3009); \u03b2q = \u03c3(\u3008\u03b32,fq\u3009). (13)\nThen we have the following joint distributions which are denoted as aq and bq:\naq := nq\u220f j=1 P (yq,j |yq = 1, Xq,\u0398) =\u03b1 n+q q (1\u2212 \u03b1q)n \u2212 q\nbq := nq\u220f j=1 P (yq,j |yq = 0, Xq,\u0398) =(1\u2212 \u03b2q)n + q \u03b2 n\u2212q q .\n(14)\nNow based on (12), (13), and (14), we can consider maximizing following log-likelihood:\nL = logP (Y|X ,\u0398) = \u2211 q logP (yq,j , j = 1, . . . , nq|Xq)\n= \u2211 q log (aqpq + bq(1\u2212 pq)) , (15)\nwhere pq = P (yq = 1|Xq,\u0398) is modeled based on (4), (5), (9) and (10). Here the parameter set is \u0398 = {\u03ba,\u03b7,\u00b5,u,\u03b31,\u03b32}.\nInference: In contrast to MoE and KL-MoE, directly optimizing (15) is non-trivial. However, we can apply the EM Algorithm [10] to optimize it by estimating the label yq and the parameters \u0398 iteratively.\nBy introducing the missing labels {yq}, we have a complete likelihood expression\nLc = \u2211 q (yq log aqpq + (1\u2212 yq) log bq(1\u2212 pq)) . (16)\n\u2022 In the E-step, we assume that parameters \u0398 are given. Then we take the expectation of yq in (16) and we obtain a new objective:\nELc = \u2211 q (tq log aqpq + (1\u2212 tq) log bq(1\u2212 pq)) , (17)\nwhere\ntq = P (yq = 1|yq,1, ..., yq,nq , Xq) = aqpq\naqpq + bq(1\u2212 pq) .\n\u2022 In the M-step, once tq is obtained, similar to MoE and KL-MoE, we can apply L-BFGS [9] to optimize ELc with respect to \u0398.\nThese two procedures are repeated until convergence.\nB. Incorporating Subjective Information\nEM-MoE-S. Subjective information from reviews (and reviewers) can be included to enhance the performance of both our relevance and prediction functions, including features such as review helpfulness, reviewer expertise, rating scores and reviewer biases. We can incorporate these features into our previous expressions for vq,r and wq,r as follows:\nvq,r = pairwise similarities\ufe37 \ufe38\ufe38 \ufe37 \u3008\u03ba, s(q, r)\u3009 + term-to-term similarity\ufe37 \ufe38\ufe38 \ufe37 \u3008\u03b7,fq \u25e6 fr\u3009 + review\u2019s helpfulness\ufe37 \ufe38\ufe38 \ufe37 \u3008g,hr\u3009 + reviewer\u2019s expertise\ufe37\ufe38\ufe38\ufe37 eur\nwq,r =( \u3008\u00b5,fq \u25e6 fr\u3009\ufe38 \ufe37\ufe37 \ufe38 interaction bet. q. &r. text + \u3008\u03be,fr\u3009\ufe38 \ufe37\ufe37 \ufe38 prediction from r. text )\u00d7 (1 + c \u00b7 rtr\ufe38 \ufe37\ufe37 \ufe38 rating score + bur\ufe38\ufe37\ufe37\ufe38 reviewer\u2019s bias ).\n(18) As shown in Figure 1, here rtr is the star rating score and hr = (h (1) r , h (2) r )T represents the helpfulness features of review r where h(1)r , h (2) r are fractions of users who respectively find or do not find the review helpful. eur and bur are parameters that make up a simple user model; the former captures the overall tendency for a user u to write reviews that are likely to be \u2018relevant,\u2019 while the latter captures the tendency of their reviews to support positive responses. Note that both parameters are latent variables that are automatically estimated when we optimize the likelihood expression above."}, {"heading": "IV. MODELING OPEN-ENDED QUESTIONS", "text": "Although our KL-MoE and EM-MoE frameworks can model ambiguity in binary questions, and account for simple features encoding subjectivity, we still need to develop methods to account for ambiguity in open-ended questions. Here we are no longer concerned with divergence between yes/no answers, but rather want to model the idea that there is a pool of answers to each question which should be regarded as more valid than alternatives. As with binary questions, these open-ended questions may be subjective and multiple answers often exist in our data. What is different is that it is difficult for us to automatically judge whether these answers are consistent or not. Thus we aim to generate\ncandidate answers that cover the spectrum of ground-truth answers as much as possible.\nFirst we give some more detail about the basic framework with a single open-ended answer, which we described briefly in Section II-B2. Then we simply extend this framework to include multiple open-ended answers and incorporate subjective information."}, {"heading": "A. Basic Framework: Learning with a Single Answer.", "text": "s-MoE. Our objective for open-ended questions is to maximize the Area Under Curve (AUC), which is defined as\nAUCo = 1 |Q| \u2211 q AUC(q) = 1 |Q| \u2211 q ( 1 |A\u0304q| \u2211 a\u0304\u2208A\u0304q \u03b4(aq > a\u0304)).\n(19) where aq is the ground-truth answer to the question q and A\u0304q is a set of non-answers (randomly sampled from among all answers). In other words, a good system is one that can correctly determine which answer is the real one.4\nIn practice, we maximize a smooth objective to approximate this measure in the form of the log-likelihood:\nL = \u2211 q \u2211 a\u0304\u2208A\u0304q log pq,aq>a\u0304. (20)\nHere pq,aq>a\u0304 = P (aq > a\u0304|Xq) is as defined in (6). The \u2018relevance\u2019 term in pq,aq>a\u0304 is the same as for binary questions while the \u2018prediction\u2019 term is defined as\npq,aq>a\u0304|r = \u03c3(waq>a\u0304|r). (21)\nAs before, waq>a\u0304|r can be modeled in terms of answer and review text. Letting faq and fa\u0304 denote the text features of the answer aq and the non-answer a\u0304 respectively. Then we have\nwaq>a\u0304 = waq,r \u2212 wa\u0304,r = interaction between ans. difference & review text\ufe37 \ufe38\ufe38 \ufe37\u2329 \u00b5, (faq \u2212 fa\u0304) \u25e6 fr \u232a . (22)\nfaq \u2212 fa\u0304 represents the difference between the answers aq and a\u0304, so that (22) models which of the answers aq or a\u0304 is more supported by review r.\nB. Incorporating Subjective Information with Multiple Answers.\nm-MoE. The previous AUC measure can be straightforwardly extended to be compatible with multiple answers. If multiple answers exist for a question q, then our target is to maximize the following AUC measure:\nAUCo = 1 |Q| \u2211 q ( 1 |Aq||A\u0304q| \u2211 a\u2208Aq \u2211 a\u0304\u2208A\u0304q \u03b4(a > a\u0304)). (23)\n4Note that in practice, at test time, one would not have a selection of candidate answers to choose from; the purpose of the model in this case is simply to identify which reviews are relevant (by using the answers at training time), rather than to answer the question directly.\nwhere Aq denotes the set of answers to question q and A\u0304q is defined as before.\nSimilarly, we maximize the following log-likelihood loss function to approximately optimize the AUC:\nL = \u2211 q 1 |Aq| \u2211 a\u2208Aq \u2211 a\u0304\u2208A\u0304q log pq,a>a\u0304. (24)\nC. Incorporating Additional Information from Reviews\n(m-MoE-S.) Similar to binary questions, we can incorporate more subjective features into vq,r and wa>a\u0304,r. Basically, vq,r can be kept the same as in (18). For waq>a\u0304,r, we have\nwaq>a\u0304,r = which answer the review favors\ufe37 \ufe38\ufe38 \ufe37\u2329 \u00b5, (faq \u2212 fa\u0304) \u25e6 fr \u232a\ufe38 \ufe37\ufe37 \ufe38 interaction b.w. ans. difference &r. text \u00d7 how supportive based on the review\ufe37 \ufe38\ufe38 \ufe37 (1 + c \u00b7 rtr\ufe38 \ufe37\ufe37 \ufe38 rating score + bur\ufe38\ufe37\ufe37\ufe38 reviewer\u2019s bias ) . (25) The left part of this formula is the same as in (22) which models which of the answers the review favors. The right part of this formula is an amplifier which models how supportive the review r is based on its subjective information."}, {"heading": "V. DATASET AND EXPLORATORY ANALYSIS", "text": "In [3], the authors collected Q/A data from Amazon.com, including a single answer (the top-voted) for each question. We collected all the related urls in this dataset and further crawled all available answers to each question (duplicates were discarded, as were questions that have been removed from Amazon since the original dataset was collected). For each product we also have its related reviews. Ultimately we obtained around 808 thousand questions with 3 million answers on 135 thousand products in 8 large categories. For these products, we have 11 million reviews in total. Detailed information is shown in Table II.\nIn practice, we split review paragraphs into sentences, such that each sentence is treated as a single \u2018expert\u2019 in our MoE framework. We used the Stanford CoreNLP [11] library to split reviews into sentences, handle word tokenization, etc."}, {"heading": "A. Obtaining Ground-Truth labels for Binary Questions", "text": "In the dataset from [3], one thousand questions have been manually labeled as \u2018binary\u2019 or \u2018open-ended.\u2019 For\nbinary questions, a positive or negative label is provided for each answer. We used these labels as seeds to train simple classifiers to identify binary questions with positive and negative answers.\nAs in [3], we applied an approach developed by Google [12] to determine whether a question is binary, using a series of simple grammatical rules. Among our labeled data, this approach achieved 97% precision and 82% recall in this manually labeled dataset.5\nFollowing this, we developed a simple logistic regression model to label observed answers to these binary questions. The features we applied are the frequency of each unigram plus whether the first word is \u2018yes\u2019 or whether it is \u2018no\u2019 (as is often the case in practice). Notice that since we want to study ambiguity that arises due to the question itself rather than due to any error in our machine labels, we need to ensure that the binary labels obtained from this logistic model are as accurate as possible. Thus again we sacrifice some recall and keep only those answers about which the regressor is most confident (here we kept the top 50% of most confident predictions). This gave us zero error on the held-out manually labeled data from [3]. Ultimately we obtained 88,559 questions with 197,210 highly confident binary labels of which around 65% are positive. In our experiments, two thirds of these questions and associated labels are involved in training and the rest are used for evaluation."}, {"heading": "B. Exploratory Analysis", "text": "Having constructed classifiers to label our training data with high confidence, we next want to determine whether there really are conflicts between multiple answers for binary questions. The distribution of ambiguous (i.e., both yes and no answers) versus non-ambiguous binary questions is shown in Figure 2a. From this figure, we notice that we do\n5Note that we are happy to sacrifice some recall for the sake of precision, as low recall simply means discarding some instances from our dataset, as opposed to training on incorrectly labeled instances.\nhave a portion of binary questions that can be confidently classified as \u2018ambiguous.\u2019 A real-world example of such a question is shown in Figure 1. One might expect that the answer to such a question would be an unambiguous \u2018yes\u2019 or \u2018no,\u2019 since it is a (seemingly objective) question about compatibility. However the answers prove to be inconsistent since different users focus on different aspects of the product. Thus even seemingly objective questions question can prove to be \u2018ambiguous,\u2019 demonstrating the need for a model that handles such conflicting evidence. Ideally a system to address such a query would retrieve relevant reviews covering a variety of angles and in this case provide an overall neutral prediction.\nUltimately, around 14% of the questions in our dataset are ambiguous (i.e., multiple binary labels are inconsistent). Distributions of ambiguous/non-ambiguous questions are plotted in Figure 2a. Even though we filtered our dataset to include only answers with high-confidence labels (i.e., clear \u2018yes\u2019 vs. \u2018no\u2019 answers), there is still a significant number of questions with conflicting labels, which indicates that modeling ambiguity is necessary for opinion QA systems."}, {"heading": "VI. EXPERIMENTS", "text": "We evaluate our proposed methods for binary questions and open-ended questions on a large dataset composed of questions, answers and reviews from Amazon. For binary questions, we evaluate the model\u2019s ability to make proper predictions. For open-ended questions, we evaluate the model\u2019s ability to distinguish \u2018true\u2019 answers from alternatives. Since our main goal is to address ambiguity and subjectivity, we focus on evaluating our model\u2019s ability to exploit multiple labels/answers, and the effect of features derived from subjective information."}, {"heading": "A. Binary Questions", "text": "1) Evaluation Methodology: For yes/no questions, our target is to evaluate whether our model can predict their \u2018true\u2019 labels correctly. Since multiple labels are collected for a single question, and since we are comparing against methods capable of predicting only a single label, it is difficult to evaluate which system\u2019s predictions are most \u2018correct\u2019 in the event of a conflict. Thus for evaluation we build two test sets consisting of decreasingly ambiguous questions. Our hope then is that by modeling ambiguity and personalization during training, our system will be more reliable even for unambiguous questions. We build two evaluation sets as follows: \u2022 Silver Standard Ground-truth. Here we simply regard\nthe majority vote among ambiguous answers as the \u2018true\u2019 label (questions with ties are discarded). \u2022 Gold Standard Ground-truth. More aggressively, here we ignore all questions with conflicting labels. For the remaining questions, we have consistent labels that we regard as ground-truth.\nNotice that all the questions and labels (ambiguous or otherwise) in the training set are involved in the training procedure of KL-MoE, EM-MoE and EM-MoE-S. We only attempt to resolve ambiguity when building our test set for evaluation.\nNaturally, it is not possible to address all questions using the content of product reviews. Thus we are more interested in the probability that the model will rank a random positive instance higher than a random negative one. We adopt the standard AUC measure, which for binary predictions is defined as:\nAUC b = \u222b \u221e \u2212\u221e TPR(t) d(FPR(t)), (26)\nwhere t is a threshold between 0 and 1. Suppose the label for question q is yq and the predicted probability of being positive from a particular model is p\u0302q . Then we have\nTPR(t) = \u2211 q 1p\u0302q\u2265t,yq=1\u2211\nq 1yq=1 ; FPR(t) =\n\u2211 q 1p\u0302q\u2265t,yq=0\n1yq=0 .\nNote that this is different from the AUC in equation (23), which is in the context of open-ended questions. Note that a na\u0131\u0308ve classifier (random predictions, random confidence ranks) has an AUC of 0.5.\n2) Baselines: We compare the performance of the following methods: \u2022 MoE. This is a state-of-the-art method for opinion QA\nfrom [3]. This is the model described in Section II-B. Here only a single label (the top-voted) is used for training, and text features from the reviews are included. \u2022 KL-MoE. This is a straightforward approach to include multiple labels by replacing a single label yq by the ratio of positive vs. negative answers (rq = n+q /(n + q + n \u2212 q ) in\n(8) (see Sec. III-A). \u2022 EM-MoE. To include all the labels instead of just a\nratio, we use an EM-like approach to update our estimates of noisy labels and parameters iteratively. Here question text features are used as prior knowledge to model the \u2018sensitivity\u2019 and \u2018specificity\u2019 regarding a question. \u2022 EM-MoE-S. Note that the above models only make use of features from reviews, and are designed to measure the performance improvements that can be achieved by harnessing multiple labels. For our final method, we include other subjective information into our model, such as user bieses, rating features, etc. (see Sec. III-B).\nUltimately the above baselines are intended to demonstrate: (a) the performance of the existing state-of-the-art (MoE); (b) the improvement from leveraging conflicting labels during training (KL-MoE and EM-MoE); and (c) the improvement from incorporating additional subjective information in the data (EM-MoE-S).\n3) Results and Discussion: Results of the above methods in terms of the AUC are shown in Table III. We notice that while KL-MoE is not able to improve upon MoE for\nall categories, EM-MoE and EM-MoE-S yield consistent improvements in all cases.6 This improvement is relatively large for some large categories, such as Cell Phones & Accessories and Electronics. Incorporating subjective features (EM-MoE-S) seems to help most for large categories, indicating that it is useful when enough training data is available to make the additional parameters affordable.\nWhen modeling ambiguity in opinion QA systems, a possible reason for the failure of KL-MoE is that the ratio rq involved in the objective function may not be a representative label for training. If the observed positive label ratio rq does not properly reflect the \u2018true\u2019 distribution, it could adversely affect the optimization procedure. In our EM-like frameworks, i.e., EM-MoE and EM-MoE-S, this ratio is replaced by a posterior probability, tq , which is updated iteratively. These EM-like frameworks are relatively more robust to data with multiple noisy labels compared with KL-MoE.\nEM-MoE-S includes subjective information related to reviews and reviewers. Due to the number of parameters involved, modeling reviewer expertise and bias is only useful for users who write several reviews, which is indeed a small fraction of reviewers. Thus in the larger categories these terms appear more useful, once we have enough observations to successfully model them.\nNote that the AUC represents the ranking performance on all questions. Generally, this value is relatively low in our experiments. This is presumably due to the simple fact that many questions cannot be answered based on the evidence in reviews. Since all of the methods being compared output confidence scores, we are interested in whether competing systems are correct in those instances where they have high confidence. If Q denotes the set of all the questions and Qa denotes the set of questions associated with the first largest (1\u2212a)|Q| values of |p\u0302q\u22120.5| (i.e., the most confident about either a yes or a no answer), then we have the following measure for a given confidence threshold 0 \u2264 a \u2264 1:\naccuracy@a = 1 |Qa| \u2211 q\u2208Qa (1p\u0302q\u22650.5,yq=1 + 1p\u0302q<0.5,yq=0). (27)\nRecall that the AUC measures the model\u2019s ability to rank questions appropriately based on the ground-truth positive and negative labels. In contrast, the accuracy@a instead measures the model\u2019s ability to correctly predict labels of those questions with highly confident output ranks. We plot this accuracy score as a function of a for the smallest category (Automotive) and the largest category (Electronics) in Figure 3. We notice that the improvement from modeling ambiguity (MoE vs. others) is relatively consistent for all confidence levels. However, modeling subjective information only seems to improve the performance on the most highly\n6Improvements in accuracy over MoE are statistically significant at the 1% level or better.\nconfident instances. For a small category like Automotive, since there is too little data to model those inactive reviewers, the EM-MoE-S model performs poorly on lowconfidence instances."}, {"heading": "B. Open-ended Questions", "text": "After our previous procedure to distinguish binary vs. open-ended questions, we are left with a total of 698,618 open-ended questions (85% of all the questions) in our dataset. We plot the distribution of the number of answers provided to each open-ended question in Figure 2b and find that the majority of these questions have more than one answer provided.\n1) Evaluation Methodology: Our goal here is to explore whether using multiple answers at training time can provide us with more accurate results, in terms of the AUC of (23). In practice, for each answer a, we randomly sample one alternative non-answer a\u0304 from the pool of all answers.\nSuppose the output probability that answer a to question q is preferred over a non-answer a\u0304 is p\u0302q,a>a\u0304. Then the AUC measure is defined as\nAUC o = 1 |Q| \u2211 q 1 |Aq| \u2211 a\u2208Aq 1(p\u0302q,a>a\u0304 > 0.5). (28)\nNote that although different answers are involved in the training procedures for different models, this evaluation measure is calculated in the same format for the same test data.\n2) Baselines: We compare the performance of the following methods: \u2022 s-MoE. This is the method from [3]. Here only the top-\nvoted answer is included for training. \u2022 m-MoE. We include all answers for each question in\nthis method and optimize the objective function in (24). Thus we evaluate whether training with multiple answers improves performance. \u2022 m-MoE-S. Similarly, we add additional subjective information to our model in order to evaluate the contribution of subjective features.\nAgain our evaluation is intended to compare (a) the performance of the existing state-of-the-art (s-MoE); (b) the improvement when training with multiple answers (m-MoE); and (c) the impact of including subjective features in the model (m-MoE-S).\n3) Results and Discussion: Results from s-MoE, mMoE and m-MoE-S are included in Table IV. We find that including multiple answers in our training procedure helps us to obtain slightly better results, while incorporating subjective information was not effective here. A possible reason could be that open-ended questions may not be as polarized as binary questions so that subjective information may not be as good an indicator as compared to the content of the review itself."}, {"heading": "VII. RELATED WORK", "text": "There are several previous studies considering the problem of opinion question answering [1]\u2013[3], [13]\u2013[17], where questions are subjective and traditional QA approaches may not be as effective as they have been for factual questions. Yu and Hatzivassiloglou [18] first proposed a series approaches\nto separate opinions and facts and identify the polarities of opinion sentences. Ku et al. [19] applied a two-layer framework to classify questions and estimated question types and polarities to filter irrelevant sentences. Li et al. [16] proposed a graph-based approach that regarded sentences as nodes and weighted edges by sentences similarity; by constructing such a graph, they could apply an \u2018Opinion PageRank\u2019 model and an \u2018Opinion HITS\u2019 model to explore different relations. Particularly for product-related opinion QA, i.e., addressing product-related questions with reviews, an aspect-based approach was proposed where aspect-rating data were applied [1]. In Yu et al. [2], a new model was developed to generate appropriate answers for opinion questions by exploiting the hierarchical organization of consumer reviews. Most recently, a supervised learning approach, MoQA, was proposed for the product-related opinion QA problem, where a mixture of experts model was applied and each review was regarded as an expert [3].\nOpinion mining is a broad topic where customer reviews are a powerful resource to explore. A number of opinion mining studies focus on opinion summarization [20], and opinion retrieval and search in review text [21]. In addition, review text can be used to improve recommender systems by modeling different aspects related to customers\u2019 opinions [22], [23]. Subjective features and user modeling approaches were frequently applied in these studies, though they were not considered for the opinion QA problem.\nThe major technique of modeling ambiguity with multiple labels in this study is inspired by approaches for resolving noisy labels in crowdsourcing tasks [24]. Notice that the main target of crowdsourcing is to resolve conflicts from annotators and obtain the actual label instead of directly providing accurate predictions from data, which is different from the setting of answering subjective questions as in our opinion QA problem. In essence, our study can be regarded as a combination of question answering, opinion mining and the idea of learning from crowds."}, {"heading": "VIII. CONCLUSION AND FUTURE DIRECTIONS", "text": "In this study, we systematically developed a series of methods to model ambiguity and subjectivity in productrelated opinion question answering systems. We proposed an EM-like mixture-of-experts framework for binary questions which can successfully incorporate multiple noisy labels and subjective information. Results indicate that this kind of framework consistently outperforms traditional frameworks that train using only a single label. For open-ended questions, we similarly found that including multiple answers during training improves the ability of the model to identify correct answers at test time.\nAcknowledgments. This work is supported by NSF-IIS-1636879, and donations from Adobe, Symantec, and NVIDIA."}], "references": [{"title": "AQA: aspect-based opinion question answering", "author": ["S. Moghaddam", "M. Ester"], "venue": "ICDMW, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Answering opinion questions on products by exploiting hierarchical organization of consumer reviews", "author": ["J. Yu", "Z.-J. Zha", "T.-S. Chua"], "venue": "EMNLP-CoNLL, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Addressing complex and subjective product-related queries with customer reviews", "author": ["J. McAuley", "A. Yang"], "venue": "WWW, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Summarization of yes/no questions using a feature function model", "author": ["J. He", "D. Dai"], "venue": "JMLR, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "A probabilistic model of information retrieval: development and comparative experiments", "author": ["K. Jones", "S. Walker", "S. Robertson"], "venue": "Information Processing & Management, 2000.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["C.-Y. Lin"], "venue": "ACL Workshop on Text Summarization Branches Out, 2004.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Hierarchical mixtures of experts and the em algorithm", "author": ["M.I. Jordan", "R.A. Jacobs"], "venue": "Neural computation, 1994.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1994}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the royal statistical society, 1977.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1977}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "ACL System Demo, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Summarization of yes/no questions using a feature function model", "author": ["J. He", "D. Dai"], "venue": "ACML, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Going beyond traditional QA systems: challenges and keys in opinion question answering", "author": ["A. Balahur", "E. Boldrini", "A. Montoyo", "P. Mart\u0131\u0301nez-Barco"], "venue": "COLING, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Mart\u0131\u0301nez-Barco, \u201cOpinion question answering: Towards a unified approach.", "author": ["A. Balahur", "E. Boldrini", "A. Montoyo"], "venue": "in ECAI,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Opinion and generic question answering systems: a performance analysis", "author": ["A. Balahur", "E. Boldrini", "A. Montoyo", "P. Martinez- Barco"], "venue": "ACL-IJCNLP, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Answering opinion questions with random walks on graphs", "author": ["F. Li", "Y. Tang", "M. Huang", "X. Zhu"], "venue": "ACL-IJCNLP, 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-perspective question answering using the OpQA corpus", "author": ["V. Stoyanov", "C. Cardie", "J. Wiebe"], "venue": "HLT/EMNLP, 2005.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences", "author": ["H. Yu", "V. Hatzivassiloglou"], "venue": "EMNLP, 2003.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Question analysis and answer passage retrieval for opinion question answering systems.", "author": ["L.-W. Ku", "Y.-T. Liang", "H.-H. Chen"], "venue": "ROCLING,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "SIGKDD, 2004.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Opinion searching in multiproduct reviews", "author": ["J. Liu", "G. Wu", "J. Yao"], "venue": "CIT, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Hidden factors and hidden topics: understanding rating dimensions with review text", "author": ["J. McAuley", "J. Leskovec"], "venue": "RecSys, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Latent aspect rating analysis on review text data: a rating regression approach", "author": ["H. Wang", "Y. Lu", "C. Zhai"], "venue": "SIGKDD, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning from crowds", "author": ["V. Raykar", "S. Yu", "L. Zhao", "G.H. Valadez", "C. Florin", "L. Bogoni", "L. Moy"], "venue": "JMLR, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "To build systems capable of leveraging such information, a series of methods [1]\u2013[3] for product-related opinion quesar X iv :1 61 0.", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": "To build systems capable of leveraging such information, a series of methods [1]\u2013[3] for product-related opinion quesar X iv :1 61 0.", "startOffset": 81, "endOffset": 84}, {"referenceID": 0, "context": "Many of these approaches develop information retrieval systems where traditional text similarity measures are explored and text fragments are filtered based on question types or the attributes of the product that users refer to in their questions [1], [2].", "startOffset": 247, "endOffset": 250}, {"referenceID": 1, "context": "Many of these approaches develop information retrieval systems where traditional text similarity measures are explored and text fragments are filtered based on question types or the attributes of the product that users refer to in their questions [1], [2].", "startOffset": 252, "endOffset": 255}, {"referenceID": 2, "context": "Recently, a supervised approach, Mixtures of Opinions for Question Answering (MoQA) [3], was developed for opinion QA systems using product reviews.", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": ", to build a labeled dataset) in a supervised setting using a binary classifier [4].", "startOffset": 80, "endOffset": 83}, {"referenceID": 2, "context": "In this study, we build upon the mixture of experts (MoE) framework as used previously by [3].", "startOffset": 90, "endOffset": 93}, {"referenceID": 4, "context": "1) Okapi BM25: One of the standard relevance ranking measures for information retrieval, Okapi BM25 is a bag-ofwords \u2018tf-idf\u2019-based ranking function that has been successfully applied in a number of problems including QA tasks [5], [6].", "startOffset": 232, "endOffset": 235}, {"referenceID": 5, "context": "2) Rouge-L: Next we consider another similarity measure, Rouge-L [7], which is a Longest Common Subsequence (LCS) based statistic.", "startOffset": 65, "endOffset": 68}, {"referenceID": 6, "context": "Mixtures of experts (MoE) [8] is a supervised learning approach that smoothly combines the outputs of several \u2018weak\u2019 classifiers in order to generate predictions.", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "Previously in [3], only text features were used to define pairwise similarity measures and bilinear models.", "startOffset": 14, "endOffset": 17}, {"referenceID": 2, "context": "Following [3], we include BM25 [5] and RougeL [7] measures in s(q, r).", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "Following [3], we include BM25 [5] and RougeL [7] measures in s(q, r).", "startOffset": 46, "endOffset": 49}, {"referenceID": 2, "context": "So far, we have followed the basic approach of [3], assuming text-only features, and a single label (answer) associated with each question.", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "However, we can apply the EM Algorithm [10] to optimize it by estimating the label yq and the parameters \u0398 iteratively.", "startOffset": 39, "endOffset": 43}, {"referenceID": 2, "context": "In [3], the authors collected Q/A data from Amazon.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "We used the Stanford CoreNLP [11] library to split reviews into sentences, handle word tokenization, etc.", "startOffset": 29, "endOffset": 33}, {"referenceID": 2, "context": "In the dataset from [3], one thousand questions have been manually labeled as \u2018binary\u2019 or \u2018open-ended.", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": "As in [3], we applied an approach developed by Google [12] to determine whether a question is binary, using a series of simple grammatical rules.", "startOffset": 6, "endOffset": 9}, {"referenceID": 9, "context": "As in [3], we applied an approach developed by Google [12] to determine whether a question is binary, using a series of simple grammatical rules.", "startOffset": 54, "endOffset": 58}, {"referenceID": 2, "context": "This gave us zero error on the held-out manually labeled data from [3].", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "This is a state-of-the-art method for opinion QA from [3].", "startOffset": 54, "endOffset": 57}, {"referenceID": 2, "context": "This is the method from [3].", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "There are several previous studies considering the problem of opinion question answering [1]\u2013[3], [13]\u2013[17], where questions are subjective and traditional QA approaches may not be as effective as they have been for factual questions.", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "There are several previous studies considering the problem of opinion question answering [1]\u2013[3], [13]\u2013[17], where questions are subjective and traditional QA approaches may not be as effective as they have been for factual questions.", "startOffset": 93, "endOffset": 96}, {"referenceID": 10, "context": "There are several previous studies considering the problem of opinion question answering [1]\u2013[3], [13]\u2013[17], where questions are subjective and traditional QA approaches may not be as effective as they have been for factual questions.", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "There are several previous studies considering the problem of opinion question answering [1]\u2013[3], [13]\u2013[17], where questions are subjective and traditional QA approaches may not be as effective as they have been for factual questions.", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "Yu and Hatzivassiloglou [18] first proposed a series approaches", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "[19] applied a two-layer framework to classify questions and estimated question types and polarities to filter irrelevant sentences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] proposed a graph-based approach that regarded sentences as nodes and weighted edges by sentences similarity; by constructing such a graph, they could apply an \u2018Opinion PageRank\u2019 model and an \u2018Opinion HITS\u2019 model to explore different relations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": ", addressing product-related questions with reviews, an aspect-based approach was proposed where aspect-rating data were applied [1].", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "[2], a new model was developed to generate appropriate answers for opinion questions by exploiting the hierarchical organization of consumer reviews.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Most recently, a supervised learning approach, MoQA, was proposed for the product-related opinion QA problem, where a mixture of experts model was applied and each review was regarded as an expert [3].", "startOffset": 197, "endOffset": 200}, {"referenceID": 17, "context": "A number of opinion mining studies focus on opinion summarization [20], and opinion retrieval and search in review text [21].", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "A number of opinion mining studies focus on opinion summarization [20], and opinion retrieval and search in review text [21].", "startOffset": 120, "endOffset": 124}, {"referenceID": 19, "context": "In addition, review text can be used to improve recommender systems by modeling different aspects related to customers\u2019 opinions [22], [23].", "startOffset": 129, "endOffset": 133}, {"referenceID": 20, "context": "In addition, review text can be used to improve recommender systems by modeling different aspects related to customers\u2019 opinions [22], [23].", "startOffset": 135, "endOffset": 139}, {"referenceID": 21, "context": "The major technique of modeling ambiguity with multiple labels in this study is inspired by approaches for resolving noisy labels in crowdsourcing tasks [24].", "startOffset": 153, "endOffset": 157}], "year": 2016, "abstractText": "Product review websites provide an incredible lens into the wide variety of opinions and experiences of different people, and play a critical role in helping users discover products that match their personal needs and preferences. To help address questions that can\u2019t easily be answered by reading others\u2019 reviews, some review websites also allow users to pose questions to the community via a question-answering (QA) system. As one would expect, just as opinions diverge among different reviewers, answers to such questions may also be subjective, opinionated, and divergent. This means that answering such questions automatically is quite different from traditional QA tasks, where it is assumed that a single \u2018correct\u2019 answer is available. While recent work introduced the idea of question-answering using product reviews, it did not account for two aspects that we consider in this paper: (1) Questions have multiple, often divergent, answers, and this full spectrum of answers should somehow be used to train the system; and (2) What makes a \u2018good\u2019 answer depends on the asker and the answerer, and these factors should be incorporated in order for the system to be more personalized. Here we build a new QA dataset with 800 thousand questions\u2014and over 3.1 million answers\u2014and show that explicitly accounting for personalization and ambiguity leads both to quantitatively better answers, but also a more nuanced view of the range of supporting, but subjective, opinions.", "creator": "LaTeX with hyperref package"}}}