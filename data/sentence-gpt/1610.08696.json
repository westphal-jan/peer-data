{"id": "1610.08696", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Oct-2016", "title": "Learning Bound for Parameter Transfer Learning", "abstract": "We consider a transfer-learning problem by using the parameter transfer approach, where a suitable parameter of feature mapping is learned through one task and applied to another objective task. Then, we introduce the notion of the local stability of parametric feature mapping and parameter transfer learnability, and thereby derive a learning bound for parameter transfer algorithms. As an application of parameter transfer learning, we discuss the performance of sparse coding in self-taught learning. A further application of parameter transfer learning is available in S3 and S5.\n\n\n\nIn this section, we talk about the properties of parameter transfer learning. For further details, see Methods for defining parameter transfer learning and how to obtain parameter transfer learning. We discuss parameter transfer learning and the method in S3.\n\nIn conclusion, parametric feature mapping is useful in self-taught learning and will provide the basis for building more sophisticated methods for embedding and performing parameter transfer learning.\n\nIntroduction\nParameter Transfer Learning uses parametric feature mapping to specify the behavior of features in a self-taught learning program. For more information about parametric feature mapping, see Figure 6.\nIn conclusion, parametric feature mapping is a powerful and highly efficient and efficient method for determining the behavior of features. It can be learned from its general properties, such as the parameter size and the general behavior of the target variable.\nIn principle, parametric feature mapping can be used to specify the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior of features by specifying the behavior", "histories": [["v1", "Thu, 27 Oct 2016 10:50:55 GMT  (38kb)", "http://arxiv.org/abs/1610.08696v1", "This paper was accepted at NIPS 2016 as a poster presentation"], ["v2", "Wed, 9 Nov 2016 01:08:40 GMT  (37kb)", "http://arxiv.org/abs/1610.08696v2", "This paper was accepted at NIPS 2016 as a poster presentation"], ["v3", "Wed, 18 Jan 2017 04:41:17 GMT  (37kb)", "http://arxiv.org/abs/1610.08696v3", "This paper was accepted at NIPS 2016 as a poster presentation"]], "COMMENTS": "This paper was accepted at NIPS 2016 as a poster presentation", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["wataru kumagai"], "accepted": true, "id": "1610.08696"}, "pdf": {"name": "1610.08696.pdf", "metadata": {"source": "CRF", "title": "Learning Bound for Parameter Transfer Learning", "authors": ["Wataru Kumagai"], "emails": ["kumagai@kanagawa-u.ac.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n08 69\n6v 1\n[ st\nat .M\nL ]\n2 7\nO ct"}, {"heading": "1 Introduction", "text": "In traditional machine learning, it is assumed that data are identically drawn from a single distribution. However, this assumption does not always hold in real-world applications. Therefore, it would be significant to develop methods capable of incorporating samples drawn from different distributions. In this case, transfer learning provides a general way to accommodate these situations. In transfer learning, besides the availability of relatively few samples related with an objective task, abundant samples in other domains that are not necessarily drawn from an identical distribution, are available. Then, transfer learning aims at extracting some useful knowledge from data in other domains and applying the knowledge to improve the performance of the objective task. In accordance with the kind of knowledge that is transferred, approaches to solving transfer-learning problems can be classified into cases such as instance transfer, feature representation transfer, and parameter transfer (Pan and Yang (2010)). In this paper, we consider the parameter transfer approach, where some kind of parametric model is supposed and the transferred knowledge is encoded into parameters. Since the parameter transfer approach typically requires many samples to accurately learn a suitable parameter, unsupervised methods are often utilized for the learning process. In particular, transfer learning from unlabeled data for predictive tasks is known as self-taught learning (Raina et al. (2007)), where a joint generative model is not assumed to underlie unlabeled samples even though the unlabeled samples should be indicative of a structure that would subsequently be helpful in predicting tasks. In recent years, self-taught learning has been intensively studied, encouraged by the development of strong unsupervised methods. Furthermore, sparsity-based methods such as sparse coding or sparse neural networks have often been used in empirical studies of self-taught learning.\nAlthough many algorithms based on the parameter transfer approach have empirically demonstrated impressive performance in self-taught learning, some fundamental problems remain. First, the theoretical aspects of the parameter transfer approach have not been studied, and in particular, no learning bound was obtained. Second, although it is believed that a large amount of unlabeled data help to improve the performance of the objective task in self-taught learning, it has not been sufficiently clarified how many samples are required. Third, although sparsity-based methods are typically employed in self-taught learning, it is unknown how the sparsity works to guarantee the performance of self-taught learning.\nThe aim of the research presented in this paper is to shed light on the above problems. We first consider a general model of parametric feature mapping in the parameter transfer approach. Then, we newly formulate the local stability of parametric feature mapping and the parameter transfer learnability for this mapping, and provide a theoretical learning bound for parameter transfer learning algorithms based on the notions. Next, we consider the stability of sparse coding. Then we discuss the parameter transfer learnability by dictionary learning under the sparse model. Applying the learning bound for parameter transfer learning algorithms, we provide a learning bound of the sparse coding algorithm in self-taught learning.\nThis paper is organized as follows. In the remainder of this section, we refer to some related studies. In Section 2, we formulate the stability and the parameter transfer learnability of the parametric feature mapping. Then, we present a learning bound for parameter transfer learning. In Section 3, we show the stability of the sparse coding under perturbation of the dictionaries. Then, by imposing sparsity assumptions on samples and by considering dictionary learning, we derive the parameter transfer learnability for sparse coding. In particular, a learning bound is obtained for sparse coding in the setting of self-taught learning. In Section 4, we conclude the paper."}, {"heading": "1.1 Related Work", "text": "Approaches to transfer learning can be classified into some cases based on the kind of knowledge being transferred (Pan and Yang (2010)). In this paper, we consider the parameter transfer approach. This approach can be applied to various notable algorithms such as sparse coding, multiple kernel learning, and deep learning since the dictionary, weights on kernels, and weights on the neural network are regarded as parameters, respectively. Then, those parameters are typically trained or tuned on samples that are not necessarily drawn from a target region. In the parameter transfer setting, a number of samples in the source region are often needed to accurately estimate the parameter to be transferred. Thus, it is desirable to be able to use unlabeled samples in the source region.\nSelf-taught learning corresponds to the case where only unlabeled samples are given in the source region while labeled samples are available in the target domain. In this sense, self-taught learning is compatible with the parameter transfer approach. Actually, in Raina et al. (2007) where self-taught learning was first introduced, the sparse coding-based method is employed and the parameter transfer approach is already used regarding the dictionary learnt from images as the parameter to be transferred. Although self-taught learning has been studied in various contexts (Dai et al. (2008); Lee et al. (2009); Wang et al. (2013); Zhu et al. (2013)), its theoretical aspects have not been sufficiently analyzed. One of the main results in this paper is to provide a first theoretical learning bound in self-taught learning with the parameter transfer approach. We note that our setting differs from the environment-based setting (Baxter (2000), Maurer (2009)), where a distribution on distributions on labeled samples, known as an environment, is assumed. In our formulation, the existence of the environment is not assumed and labeled data in the source region are not required.\nSelf-taught learning algorithms are often based on sparse coding. In the seminal paper by Raina et al. (2007), they already proposed an algorithm that learns a dictionary in the source region and transfers it to the target region. They also showed the effectiveness of the sparse coding-based method. Moreover, since remarkable progress has been made in unsupervised learning based on sparse neural networks (Coates et al. (2011), Le (2013)), unlabeled samples of the source domain in self-taught learning are often preprocessed by sparsity-based methods. Recently, a sparse coding-based generalization bound was studied (Mehta and Gray (2013); Maurer et al. (2012)) and the analysis in Section 3.1 is based on (Mehta and Gray (2013))."}, {"heading": "2 Learning Bound for Parameter Transfer Learning", "text": ""}, {"heading": "2.1 Problem Setting of Parameter Transfer Learning", "text": "We formulate parameter transfer learning in this subsection. We first briefly introduce notations and terminology in transfer learning (Pan and Yang (2010)). Let X and Y be a sample space and a label space, respectively. We refer to a pair of Z := X \u00d7 Y and a joint distribution P (x, y) on Z as a region. Then, a domain comprises a pair consisting of a sample space X and a marginal probability of P (x) on X and a task consists of a pair containing a label set Y and a conditional distribution P (y|x). In addition, let H = {h : X \u2192 Y} be a hypothesis space and \u2113 : Z \u00d7 Z \u2192 R\u22650\nrepresent a loss function. Then, the expected risk and the empirical risk are defined by R(h) := Ez=(x,y)\u223cP [\u2113(z, h(x))] and R\u0302n(h) := 1n \u2211n j=1 \u2113(yj, h(xj)), respectively. In the setting of transfer learning, besides samples from a region of interest known as a target region, it is assumed that samples from another region known as a source region are also available. We distinguish between the target and source regions by adding a subscript T or S to each notation introduced above, (e.g. PT , RS ). Then, the homogeneous setting (i.e., XS = XT ) is not assumed in general, and thus, the heterogeneous setting (i.e., XS 6= XT ) can be treated. We note that self-taught learning, which is treated in Section 3, corresponds to the case when the label space YS in the source region is the set of a single element.\nWe consider the parameter transfer approach, where the knowledge to be transferred is encoded into a parameter. The parameter transfer approach aims to learn a hypothesis with low expected risk for the target task by obtaining some knowledge about an effective parameter in the source region and transfer it to the target region. In this paper, we suppose that there are parametric models on both the source and target regions and that their parameter spaces are partly shared. Then, our strategy is to learn an effective parameter in the source region and then transfer a part of the parameter to the target region. We describe the formulation in the following. In the target region, we assume that YT \u2282 R and there is a parametric feature mapping \u03c8\u03b8 : XT \u2192 Rm on the target domain such that each hypothesis hT ,\u03b8,w : XT \u2192 YT is represented by\nhT ,\u03b8,w(x) := \u3008w, \u03c8\u03b8(x)\u3009 (1) with parameters \u03b8 \u2208 \u0398 and w \u2208 WT , where \u0398 is a subset of a normed space with a norm \u2016 \u00b7 \u2016 and WT is a subset of Rm. Then the hypothesis set in the target region is parameterized as\nHT = {hT ,\u03b8,w|\u03b8 \u2208 \u0398,w \u2208 WT }.\nIn the following, we simply denote RT (hT ,\u03b8,w) and R\u0302T (hT ,\u03b8,w) by RT (\u03b8,w) and R\u0302T (\u03b8,w), respectively. In the source region, we suppose that there exists some kind of parametric model such as a sample distribution PS,\u03b8,w or a hypothesis hS,\u03b8,w with parameters \u03b8 \u2208 \u0398 and w \u2208 WS , and a part \u0398 of the parameter space is shared with the target region. Then, let \u03b8\u2217S \u2208 \u0398 and w\u2217S \u2208 WS be parameters that are supposed to be effective in the source region (e.g., the true parameter of the sample distribution, the parameter of the optimal hypothesis with respect to the expected risk RS); however, explicit assumptions are not imposed on the parameters. Then, the parameter transfer algorithm treated in this paper is described as follows. Let N - and n-samples be available in the source and target regions, respectively. First, a parameter transfer algorithm outputs the estimator \u03b8\u0302N \u2208 \u0398 of \u03b8\u2217S by using N -samples. Next, for the parameter\nw \u2217 T := argmin w\u2208WT RT (\u03b8\u2217S ,w)\nin the target region, the algorithm outputs its estimator\nw\u0302N,n := argmin w\u2208WT R\u0302T ,n(\u03b8\u0302N ,w) + \u03c1r(w)\nby using n-samples, where r(w) is a 1-strongly convex function with respect to \u2016 \u00b7 \u20162 and \u03c1 > 0. If the source region relates to the target region in some sense, the effective parameter \u03b8\u2217S in the source region is expected to also be useful for the target task. In the next subsection, we regard RT (\u03b8\u2217S ,w\u2217T ) as the baseline of predictive performance and derive a learning bound."}, {"heading": "2.2 Learning Bound Based on Stability and Learnability", "text": "We newly introduce the local stability and the parameter transfer learnability as below. These notions are essential to derive a learning bound in Theorem 1.\nDefinition 1 (Local Stability). A parametric feature mapping \u03c8\u03b8 is said to be locally stable if there exist \u01eb\u03b8 : X \u2192 R>0 for each \u03b8 \u2208 \u0398 and L\u03c8 > 0 such that for \u03b8\u2032 \u2208 \u0398\n\u2016\u03b8 \u2212 \u03b8\u2032\u2016 \u2264 \u01eb\u03b8(x) \u21d2 \u2016\u03c8\u03b8(x)\u2212 \u03c8\u03b8\u2032(x)\u20162 \u2264 L\u03c8\u2016\u03b8 \u2212 \u03b8\u2032\u2016.\nWe term \u01eb\u03b8(x) the permissible radius of perturbation for \u03b8 at x. For samples Xn = {x1, . . .xn}, we denote as \u01eb\u03b8(Xn) := minj\u2208[n] \u01eb\u03b8(xj), where [n] := {1, . . . , n} for a positive integer n. Next, we formulate the parameter transfer learnability based on the local stability.\nDefinition 2 (Parameter Transfer Learnability). Suppose that N -samples in the source domain and n-samples Xn in the target domain are available. Let a parametric feature mapping {\u03c8\u03b8}\u03b8\u2208\u0398 be locally stable. For \u03b4\u0304 \u2208 [0, 1), {\u03c8\u03b8}\u03b8\u2208\u0398 is said to be parameter transfer learnable with probability 1 \u2212 \u03b4\u0304 if there exists an algorithm that depends only on N -samples in the source domain such that, the output \u03b8\u0302N of the algorithm satisfies\nPr [ \u2016\u03b8\u0302N \u2212 \u03b8\u2217S\u2016 \u2264 \u01eb\u03b8\u2217S (X n) ] \u2265 1\u2212 \u03b4\u0304.\nIn the following, we assume that parametric feature mapping is bounded as \u2016\u03c8\u03b8(x)\u20162 \u2264 R\u03c8 for arbitrary x \u2208 X and \u03b8 \u2208 \u0398 and linear predictors are also bounded as \u2016w\u20162 \u2264 RW for any w \u2208 W . In addition, we suppose that a loss function \u2113(\u00b7, \u00b7) is L\u2113-Lipschitz and convex with respect to the second variable. We denote as Rr := supw\u2208W |r(w)|. Then, the following learning bound is obtained, where the strong convexity of the regularization term \u03c1r(w) is essential.\nTheorem 1 (Learning Bound). Suppose that the parametric feature mapping \u03c8\u03b8 is locally stable and an estimator \u03b8\u0302N learned in the source region satisfies the parameter transfer learnability with\nprobability 1\u2212 \u03b4\u0304. When \u03c1 = L\u2113R\u03c8 \u221a\n8(32+log(2/\u03b4)) Rrn , the following inequality holds with probability\n1\u2212 (\u03b4 + 2\u03b4\u0304):\nRT ( \u03b8\u0302N , w\u0302N,n ) \u2212RT (\u03b8\u2217S ,w\u2217T )\n\u2264 L\u2113R\u03c8 ( RW \u221a 2 log(2/\u03b4) + 2 \u221a 2Rr(32 + log(2/\u03b4)) ) 1\u221a n + L\u2113L\u03c8R\u03c8 \u2225\u2225\u2225\u03b8\u0302N \u2212 \u03b8\u2217S \u2225\u2225\u2225\n+L\u2113 \u221a L\u03c8RWR\u03c8\n( Rr\n2(32 + log(2/\u03b4))\n) 1 4\nn 1 4 \u221a\u2225\u2225\u2225\u03b8\u0302N \u2212 \u03b8\u2217S \u2225\u2225\u2225. (2)\nIf the estimation error \u2016\u03b8\u0302N \u2212 \u03b8\u2217S\u2016 can be evaluated in terms of the number N of samples, Theorem 1 clarifies which term is dominant, and in particular, the number of samples required in the source domain such that this number is sufficiently large compared to the samples in the target domain."}, {"heading": "2.3 Proof of Learning Bound", "text": "We prove Theorem 1 in this subsection. In this proof, we omit the subscript T for simplicity. In addition, we denote \u03b8\u2217S simply by \u03b8 \u2217. We set as\nw\u0302 \u2217 n := argmin\nw\u2208W\n1\nn\nn\u2211\nj=1\n\u2113(yj , \u3008w, \u03c8\u03b8\u2217(xj)\u3009) + \u03c1r(w).\nThen, we have\nRT ( \u03b8\u0302N , w\u0302N,n ) \u2212RT (\u03b8\u2217,w\u2217)\n= E(x,y)\u223cP [ \u2113(y, \u3008w\u0302N,n, \u03c8\u03b8\u0302N (x)\u3009) ] \u2212 E(x,y)\u223cP [\u2113(y, \u3008w\u0302N,n, \u03c8\u03b8\u2217(x)\u3009)]\n+E(x,y)\u223cP [\u2113(y, \u3008w\u0302N,n, \u03c8\u03b8\u2217(x)\u3009)] \u2212 E(x,y)\u223cP [\u2113(y, \u3008w\u0302\u2217n, \u03c8\u03b8\u2217(x)\u3009)] (3) +E(x,y)\u223cP [\u2113(y, \u3008w\u0302\u2217n, \u03c8\u03b8\u2217(x)\u3009)] \u2212 E(x,y)\u223cP [\u2113(y, \u3008w\u2217, \u03c8\u03b8\u2217(x)\u3009)] .\nIn the following, we bound three parts of (3). First, we have the following inequality with probability 1\u2212 (\u03b4/2 + \u03b4\u0304):\nE(x,y)\u223cP [ \u2113(y, \u3008w\u0302N,n, \u03c8\u03b8\u0302N (x)\u3009) ] \u2212 E(x,y)\u223cP [\u2113(y, \u3008w\u0302N,n, \u03c8\u03b8\u2217(x)\u3009)]\n\u2264 L\u2113RWE(x,y)\u223cP [\u2225\u2225\u2225\u03c8\n\u03b8\u0302N (x) \u2212 \u03c8\u03b8\u2217(x)\n\u2225\u2225\u2225 ]\n\u2264 L\u2113RW 1\nn\nn\u2211\nj=1\n\u2225\u2225\u2225\u03c8 \u03b8\u0302N (xj)\u2212 \u03c8\u03b8\u2217(xj) \u2225\u2225\u2225+ L\u2113RWR\u03c8\n\u221a 2 log(2/\u03b4)\nn\n\u2264 L\u2113L\u03c8RW \u2225\u2225\u2225\u03b8\u0302N \u2212 \u03b8\u2217 \u2225\u2225\u2225+ L\u2113RWR\u03c8 \u221a 2 log(2/\u03b4)\nn ,\nwhere we used Hoeffding\u2019s inequality as the third inequality, and the local stability and parameter transfer learnability in the last inequality. Second, we have the following inequality with probability 1\u2212 \u03b4\u0304:\nE(x,y)\u223cP [\u2113(y, \u3008w\u0302N,n, \u03c8\u03b8\u2217(x)\u3009)] \u2212 E(x,y)\u223cP [\u2113(y, \u3008w\u0302\u2217n, \u03c8\u03b8\u2217(x)\u3009)] \u2264 L\u2113E(x,y)\u223cP [|\u3008w\u0302N,n, \u03c8\u03b8\u2217(x)\u3009 \u2212 \u3008w\u0302\u2217n, \u03c8\u03b8\u2217(x)\u3009|] \u2264 L\u2113R\u03c8 \u2016w\u0302N,n \u2212 w\u0302\u2217n\u20162\n\u2264 L\u2113R\u03c8\n\u221a 2L\u2113L\u03c8RW\n\u03c1\n\u2225\u2225\u2225\u03b8\u0302N \u2212 \u03b8\u2217 \u2225\u2225\u2225, (4)\nwhere the last inequality is derived by the strong convexity of the regularizer \u03c1r(w) in the Appendix. Third, the following holds by Theorem 1 of Sridharan et al. (2009) with probability 1\u2212 \u03b4/2:\nE(x,y)\u223cP [\u2113(y, \u3008w\u0302\u2217n, \u03c8\u03b8\u2217(x)\u3009)] \u2212 E(x,y)\u223cP [\u2113(y, \u3008w\u2217, \u03c8\u03b8\u2217(x)\u3009)] = E(x,y)\u223cP [\u2113(y, \u3008w\u0302\u2217n, \u03c8\u03b8\u2217(x)\u3009) + \u03c1r(w\u0302\u2217n)]\n\u2212E(x,y)\u223cP [\u2113(y, \u3008w\u2217, \u03c8\u03b8\u2217(x)\u3009) + \u03c1r(w\u2217)] + \u03c1(r(w\u2217)\u2212 r(w\u0302\u2217n))\n\u2264 ( 8L2\u2113R 2 \u03c8(32 + log(2/\u03b4))\n\u03c1n\n) + \u03c1Rr.\nThus, when \u03c1 = L\u2113R\u03c8 \u221a\n8(32+log(2/\u03b4)) Rrn , we have (2) with probability 1\u2212 (\u03b4 + 2\u03b4\u0304)."}, {"heading": "3 Stability and Learnability in Sparse Coding", "text": "In this section, we consider the sparse coding in self-taught learning, where the source region essentially consists of the sample space XS without the label space YS . We assume that the sample spaces in both regions are Rd. Then, the sparse coding method treated here consists of a two-stage procedure, where a dictionary is learnt on the source region, and then a sparse coding with the learnt dictionary is used for a predictive task in the target region.\nFirst, we show that sparse coding satisfies the local stability in Section 3.1 and next explain that appropriate dictionary learning algorithms satisfy the parameter transfer learnability in Section 3.4. As a consequence of Theorem 1, we obtain the learning bound of self-taught learning algorithms based on sparse coding. We note that the results in this section are useful independent of transfer learning.\nWe here summarize the notations used in this section. Let \u2016 \u00b7 \u2016p be the p-norm on Rd. We define as supp(a) := {i \u2208 [m]|ai 6= 0} for a \u2208 Rm. We denote the number of elements of a set S by |S|. When a vector a satisfies \u2016a\u20160 = |supp(a)| \u2264 k, a is said to be k-sparse. We denote the ball with radius R centered at 0 by BRd(R) := {x \u2208 Rd|\u2016x\u20162 \u2264 R}. We set as D := {D = [d1, . . . ,dm] \u2208 BRd(1)\nm|\u2016dj\u20162 = 1 (i = 1, . . . ,m)} and each D \u2208 D a dictionary with size m. Definition 3 (Induced matrix norm). For an arbitrary matrix E = [e1, . . . , em] \u2208 Rd\u00d7m, 1) the induced matrix norm is defined by \u2016E\u20161,2 := maxi\u2208[m] \u2016ei\u20162.\nWe adopt \u2016 \u00b7 \u20161,2 to measure the difference of dictionaries since it is typically used in the framework of dictionary learning. We note that \u2016D\u2212 D\u0303\u20161,2 \u2264 2 holds for arbitrary dictionaries D, D\u0303 \u2208 D."}, {"heading": "3.1 Local Stability of Sparse Representation", "text": "We show the local stability of sparse representation under a sparse model. A sparse representation with dictionary parameter D of a sample x \u2208 Rd is expressed as follows:\n\u03d5D(x) := argmin z\u2208Rm\n1 2 \u2016x\u2212Dz\u201622 + \u03bb\u2016z\u20161,\n1) In general, the (p, q)-induced norm for p, q \u2265 1 is defined by \u2016E\u2016p,q := supv\u2208Rm,\u2016v\u2016p=1 \u2016Ev\u2016q . Then, \u2016 \u00b7 \u20161,2 in this general definition coincides with that in Definition 3 by Lemma 17 of Vainsencher et al. (2011).\nwhere \u03bb > 0 is a regularization parameter. This situation corresponds to the case where \u03b8 = D and \u03c8\u03b8 = \u03d5D in the setting of Section 2.1. We prepare some notions to the stability of the sparse representation. The following margin and incoherence were introduced by Mehta and Gray (2013).\nDefinition 4 (k-margin). Given a dictionary D = [d1, . . . ,dm] \u2208 D and a point x \u2208 Rd, the k-margin of D on x is\nMk(D,x) := max I\u2282[m],|I|=m\u2212k min j\u2208I {\u03bb\u2212 |\u3008dj ,x\u2212D\u03d5D(x)\u3009|} .\nDefinition 5 (\u00b5-incoherence). A dictionary matrix D = [d1, . . . ,dm] \u2208 D is termed \u00b5-incoherent if \u3008di,dj\u3009 \u2264 \u00b5/ \u221a d for all i 6= j.\nThen, the following theorem is obtained.\nTheorem 2 (Sparse Coding Stability). Let D \u2208 D be \u00b5-incoherent and \u03bb \u2264 1. When\n\u2016D\u2212 D\u0303\u20161,2 \u2264 \u01ebk,D(x) := Mk,D(x)2\u03bb\n64max{1, \u2016x\u2016}4 , (5)\nthe following stability bound holds:\n\u2016\u03d5D(x) \u2212 \u03d5D\u0303(x)\u20162 \u2264 4\u2016x\u20162\n\u221a k\n(1\u2212 \u00b5k/ \u221a d)\u03bb \u2016D\u2212 D\u0303\u20161,2.\nFrom Theorem 2, \u01ebk,D(x) becomes the permissible radius of perturbation in Definition 1.\nHere, we refer to the relation with the sparse coding stability (Theorem 4) of Mehta and Gray (2013), who measured the difference of dictionaries by \u2016 \u00b7 \u20162,2 instead of \u2016 \u00b7 \u20161,2 and the permissible radius of perturbation is given by Mk,D(x)2\u03bb except for a constant factor. Applying the simple inequality \u2016E\u20162,2 \u2264 \u221a m\u2016E\u20161,2 for E \u2208 Rd\u00d7m, we can obtain a variant of the sparse coding stability with the norm \u2016 \u00b7 \u20161,2. However, then the dictionary size m affects the permissible radius of perturbation and the stability bound of the sparse coding stability. On the other hand, the factor of m does not appear in Theorem 2, and thus, the result is effective even for a large m. In addition, whereas \u2016x\u2016 \u2264 1 is assumed in Mehta and Gray (2013), Theorem 2 does not assume that \u2016x\u2016 \u2264 1 and clarifies the dependency for the norm \u2016x\u2016. In existing studies related to sparse coding, the sparse representation \u03d5D(x) is modified as \u03d5D(x)\u2297 x (Mairal et al. (2009)) or \u03d5D(x) \u2297 (x \u2212 D\u03d5D(x)) (Raina et al. (2007)) where \u2297 is the tensor product. By the stability of sparse representation (Theorem 2), it can be shown that such modified representations also have local stability."}, {"heading": "3.2 Sparse Modeling and Margin Bound", "text": "In this subsection, we assume a sparse structure for samples x \u2208 Rd and specify a lower bound for the k-margin used in (5). The result obtained in this section plays an essential role to show the parameter transfer learnability in Section 3.4. Assumption 1 (Model). There exists a dictionary matrix D\u2217 such that every sample x is independently generated by a representation a and noise \u03be as\nx = D\u2217a+ \u03be.\nMoreover, we impose the following three assumptions on the above model.\nAssumption 2 (Dictionary). The dictionary matrix D\u2217 = [d1, . . . ,dm] \u2208 D is \u00b5-incoherent. Assumption 3 (Representation). The representation a is a random variable that is k-sparse (i.e., \u2016a\u20160 \u2264 k) and the non-zero entries are lower bounded by C > 0 (i.e., ai 6= 0 satisfy |ai| \u2265 C). Assumption 4 (Noise). The noise \u03be is independent across coordinates and sub-Gaussian with parameter \u03c3/ \u221a d on each component.\nWe note that the assumptions do not require the representation a or noise \u03be to be identically distributed while those components are independent. This is essential because samples in the source and target domains cannot be assumed to be identically distributed in transfer learning.\nTheorem 3 (Margin Bound). Let 0 < t < 1. We set as\n\u03b4t,\u03bb := 2\u03c3\n(1\u2212 t) \u221a d\u03bb exp\n( \u2212 (1\u2212 t) 2d\u03bb2\n8\u03c32\n) +\n2\u03c3m\u221a d\u03bb exp\n( \u2212d\u03bb 2\n8\u03c32\n)\n+ 4\u03c3k\nC \u221a d(1 \u2212 \u00b5k/ \u221a d) exp\n( \u2212C 2d(1 \u2212 \u00b5k/ \u221a d)\n8\u03c32\n) +\n8\u03c3(d\u2212 k)\u221a d\u03bb exp\n( \u2212 d\u03bb 2\n32\u03c32\n) . (6)\nWe suppose that d \u2265 {(\n1 + 6(1\u2212t)\n) \u00b5k }2\nand \u03bb = d\u2212\u03c4 for arbitrary 1/4 \u2264 \u03c4 \u2264 1/2. Under Assumptions 1-4, the following inequality holds with probability 1\u2212 \u03b4t,\u03bb at least:\nMk,D\u2217(x) \u2265 t\u03bb. (7)\nWe refer to the regularization parameter \u03bb. An appropriate reflection of the sparsity of samples requires the regularization parameter \u03bb to be set suitably. According to Theorem 4 of Zhao and Yu (2006)2), when samples follow the sparse model as in Assumptions 1-4 and \u03bb \u223c= d\u2212\u03c4 for 1/4 \u2264 \u03c4 \u2264 1/2, the representation \u03d5D(x) reconstructs the true sparse representation a of sample x with a small error. In particular, when \u03c4 = 1/4 (i.e., \u03bb \u223c= d\u22121/4) in Theorem 3, the failure probability \u03b4t,\u03bb \u223c= e\u2212 \u221a d on the margin is guaranteed to become sub-exponentially small with respect to dimension d and is negligible for the high-dimensional case. On the other hand, the typical choice \u03c4 = 1/2 (i.e., \u03bb \u223c= d\u22121/2) does not provide a useful result because \u03b4t,\u03bb is not small at all."}, {"heading": "3.3 Proof of Margin Bound", "text": "We provide a sketch of the proof of Theorem 3. We denote the first term, the second term, and the sum of the third and fourth terms of (6) by \u03b41, \u03b42 and \u03b43, respectively. From Assumptions 1 and 3, a sample is represented as x = D\u2217a + \u03be and \u2016a\u20160 \u2264 k. Without the loss of generality, we assume that the first m\u2212 k components of a are 0 and the last k components are not 0. Since Mk,D\u2217(x) \u2265 min\n1\u2264j\u2264m\u2212k \u03bb\u2212 \u3008dj ,x\u2212D\u2217\u03d5D(x)\u3009 = min 1\u2264j\u2264m\u2212k \u03bb\u2212 \u3008dj , \u03be\u3009 \u2212 \u3008D\u2217\u22a4dj , a\u2212 \u03d5D(x)\u3009,\nit suffices to show that the following holds for an arbitrary 1 \u2264 j \u2264 m\u2212 k to prove Theorem 3: Pr[\u3008dj , \u03be\u3009+ \u3008D\u2217\u22a4dj , a\u2212 \u03d5D(x)\u3009 > (1\u2212 t)\u03bb] \u2264 \u03b4t,\u03bb. (8)\nThen, (8) follows from the following inequalities:\nPr [ \u3008dj , \u03be\u3009 >\n1\u2212 t 2 \u03bb\n] \u2264 \u03b41, (9)\nPr [ \u3008D\u2217\u22a4dj , a\u2212 \u03d5D(x)\u3009 >\n1\u2212 t 2 \u03bb\n] \u2264 \u03b42 + \u03b43. (10)\nThe inequality (9) holds since \u2016dj\u2016 = 1 by the definition and Assumption 4. Thus, all we have to do is to show (10). We have\n\u3008D\u2217\u22a4dj , a\u2212 \u03d5D(x)\u3009 = \u3008[\u3008d1,dj\u3009, . . . , \u3008dm,dj\u3009]\u22a4, a\u2212 \u03d5D(x)\u3009 = \u3008(1supp(a\u2212\u03d5D(x)) \u25e6 [\u3008d1,dj\u3009, . . . , \u3008dm,dj\u3009])\u22a4, a\u2212 \u03d5D(x)\u3009 \u2264 \u20161supp(a\u2212\u03d5D(x)) \u25e6 [\u3008d1,dj\u3009, . . . , \u3008dm,dj\u3009]\u20162\u2016a\u2212 \u03d5D(x)\u20162,(11)\nwhere u \u25e6 v is the Hadamard product (i.e., component-wise product) between u and v, and 1A for a set A \u2282 [m] is a vector whose i-th component is 1 if i \u2208 A and 0 otherwise. Applying Theorem 4 of Zhao and Yu (2006) and using the condition for \u03bb, the following holds with probability 1\u2212 \u03b43:\nsupp(a) = supp(\u03d5D(x)). (12)\n2)Theorem 4 of Zhao and Yu (2006) is stated for Gaussian noise. However, it can be easily generalized to sub-Gaussian noise as in Assumption 4. Our setting corresponds to the case in which c1 = 1/2, c2 = 1, c3 = (log \u03ba + log log d)/ log d for some \u03ba > 1 (i.e., ed\nc3 \u223c= d\u03ba) and c4 = c in Theorem 4 of Zhao and Yu (2006). Note that our regularization parameter \u03bb corresponds to \u03bbd/d in (Zhao and Yu (2006)).\nMoreover, under (12), the following holds with probability 1 \u2212 \u03b42 by modifying Corollary 1 of Negahban et al. (2009) and using the condition for \u03bb:\n\u2016a\u2212 \u03d5D(x)\u20162 \u2264 6 \u221a k\u03bb\n1\u2212 \u00b5k\u221a d\n. (13)\nThus, if both of (12) and (13) hold, the right-hand side of (11) is bounded as follows:\n\u20161supp(a\u2212\u03d5D(x)) \u25e6 [\u3008d1,dj\u3009, . . . , \u3008dm,dj\u3009]\u20162\u2016a\u2212 \u03d5D(x)\u20162\n\u2264 \u221a |supp(a\u2212 \u03d5D(x))|\n\u00b5\u221a d\n6 \u221a k\u03bb\n1\u2212 \u00b5k\u221a d\n= 6\u00b5k\u221a d\u2212 \u00b5k \u03bb \u2264 1\u2212 t 2 \u03bb,\nwhere we used Assumption 2 in the first inequality, (12) and Assumption 3 in the equality, and the condition for d in the last inequality. From the above discussion, the left-hand side of (10) is bounded by the sum of the probability \u03b43 that (12) does not hold and the probability \u03b42 that (12) holds but (13) does not hold."}, {"heading": "3.4 Transfer Learnability for Dictionary Learning", "text": "When the true dictionary D\u2217 exists as in Assumption 1, we show that the output D\u0302N of a suitable dictionary learning algorithm from N -unlabeled samples satisfies the parameter transfer learnability for the sparse coding \u03d5D. Then, Theorem 1 guarantees the learning bound in self-taught learning since the discussion in this section does not assume the label space in the source region. This situation corresponds to the case where \u03b8\u2217S = D\n\u2217, \u03b8\u0302N = D\u0302N and \u2016 \u00b7 \u2016 = \u2016 \u00b7 \u20161,2 in Section 2.1. We show that an appropriate dictionary learning algorithm satisfies the parameter transfer learnability for the sparse coding \u03d5D by focusing on the permissible radius of perturbation in (5) under some assumptions. When Assumptions 1-4 hold and \u03bb = d\u2212\u03c4 for 1/4 \u2264 \u03c4 \u2264 1/2, the margin bound (7) for x \u2208 X holds with probability 1\u2212 \u03b4t,\u03bb, and thus, we have\n\u01ebk,D\u2217(x) \u2265 t2\u03bb3\n64max{1, \u2016x\u2016}4 = \u0398(d \u22123\u03c4 ).\nThus, if a dictionary learning algorithm outputs the estimator D\u0302N such that \u2016D\u0302N \u2212D\u2217\u20161,2 \u2264 O(d\u22123\u03c4 ) (14) with probability 1\u2212 \u03b4N , the estimator D\u0302N of D\u2217 satisfies the parameter transfer learnability for the sparse coding \u03d5D with probability \u03b4\u0304 = \u03b4N + n\u03b4t,\u03bb. Then, by the local stability of the sparse representation and the parameter transfer learnability of such a dictionary learning, Theorem 1 guarantees that sparse coding in self-taught learning satisfies the learning bound in (2).\nWe note that Theorem 1 can apply to any dictionary learning algorithm as long as (14) is satisfied. For example, Arora et al. (2015) show that, when k = O( \u221a d/ log d), m = O(d), Assumptions 1-4\nand some additional conditions are assumed, their dictionary learning algorithm outputs D\u0302N which satisfies \u2016D\u0302N \u2212D\u2217\u20161,2 = O(d\u2212M ) with probability 1\u2212 d\u2212M \u2032 for arbitrarily large M,M \u2032 as long as N is sufficiently large."}, {"heading": "4 Conclusion", "text": "We derived a learning bound (Theorem 1) for a parameter transfer learning problem based on the local stability and parameter transfer learnability, which are newly introduced in this paper. Then, applying it to a sparse coding-based algorithm under a sparse model (Assumptions 1-4), we obtained the first theoretical guarantee of a learning bound in self-taught learning. Although we only consider sparse coding, the framework of parameter transfer learning includes other promising algorithms such as multiple kernel learning and deep neural networks, and thus, our results are expected to be effective to analyze the theoretical performance of these algorithms. Finally, we note that our learning bound can be applied to different settings from self-taught learning because Theorem 1 includes the case in which labeled samples are available in the source region."}, {"heading": "A Appendix: Lemma for Proof of Theorem 1", "text": "In this subsection, we omit the subscript T for simplicity. In addition, we denote \u03b8\u2217S by \u03b8\u2217 simply. We recall\nw\u0302N,n := argmin w\u2208WT\n1\nn\nn\u2211\nj=1\n\u2113(yj, \u3008w, \u03c8\u03b8\u0302N (x)\u3009) + \u03c1r(w),\nw\u0302 \u2217 n := argmin\nw\u2208W\n1\nn\nn\u2211\nj=1\n\u2113(yj, \u3008w, \u03c8\u03b8\u2217(xj)\u3009) + \u03c1r(w).\nThe inequality (4) is obtained by the following lemma.\nLemma 1. The following holds with probability 1\u2212 \u03b4\u0304:\n\u2016w\u0302N,n \u2212 w\u0302\u2217n\u20162 \u2264 \u221a\n2RWL\u2113L\u03c8 \u03c1\n\u2225\u2225\u2225\u03b8\u0302N \u2212 \u03b8\u2217 \u2225\u2225\u2225. (15)\n[Proof] Let us define as\nf\u0302N,n(w) := 1\nn\nn\u2211\nj=1\n\u2113(yj, \u3008w, \u03c8\u03b8\u0302N (x)\u3009) + \u03c1r(w),\nf\u0302\u2217n(w) := 1\nn\nn\u2211\nj=1\n\u2113(yj, \u3008w, \u03c8\u03b8\u2217(xj)\u3009) + \u03c1r(w).\nIf\nf\u0302\u2217n(w\u0302 \u2217 n) \u2264 f\u0302N,n(w\u0302N,n),\nwe have the following with probability 1\u2212 \u03b4\u0304: f\u0302N,n(w\u0302 \u2217 n)\u2212 f\u0302N,n(w\u0302N,n) \u2264 f\u0302N,n(w\u0302\u2217n)\u2212 f\u0302\u2217n(w\u0302\u2217n) + f\u0302\u2217n(w\u0302\u2217n)\u2212 f\u0302N,n(w\u0302N,n)\n\u2264 f\u0302N,n(w\u0302\u2217n)\u2212 f\u0302\u2217n(w\u0302\u2217n)\n= 1\nn\nn\u2211\nj=1\n\u2113(yj , \u3008w\u0302\u2217n, \u03c8\u03b8\u0302N (xj)\u3009)\u2212 1\nn\nn\u2211\nj=1\n\u2113(yj , \u3008w\u0302\u2217n, \u03c8\u03b8\u2217(xj)\u3009)\n\u2264 1 n\nn\u2211\nj=1\nL\u2113 \u2223\u2223\u2223\u3008w\u0302\u2217n, \u03c8\u03b8\u0302N (xj)\u3009 \u2212 \u3008w\u0302 \u2217 n, \u03c8\u03b8\u2217(xj)\u3009 \u2223\u2223\u2223\n\u2264 1 n\nn\u2211\nj=1\nL\u2113RW \u2225\u2225\u2225\u03c8\n\u03b8\u0302N (xj)\u2212 \u03c8\u03b8\u2217(xj)\n\u2225\u2225\u2225\n\u2264 1 n\nn\u2211\nj=1\nL\u2113RWL\u03c8 \u2225\u2225\u2225\u03b8\u0302N \u2212 \u03b8\u2217 \u2225\u2225\u2225\n= L\u2113RWL\u03c8 \u2225\u2225\u2225\u03b8\u0302N \u2212 \u03b8\u2217 \u2225\u2225\u2225 .\nSince f\u0302N,n is \u03c1-strongly convex and w\u0302N,n is its miniizer,\nf\u0302N,n(w\u0302 \u2217 n)\u2212 f\u0302N,n(w\u0302N,n) \u2265\n\u03c1 2 \u2016w\u0302\u2217n \u2212 w\u0302N,n\u201622.\nThus, we obtain (15).\nSimilarly, if\nf\u0302\u2217n(w\u0302 \u2217 n) \u2265 f\u0302N,n(w\u0302N,n),\nwe have the following with probability 1\u2212 \u03b4\u0304: f\u0302\u2217n(w\u0302N,n)\u2212 f\u0302\u2217n(w\u0302\u2217n) \u2264 f\u0302\u2217n(w\u0302N,n)\u2212 f\u0302N,n(w\u0302N,n) + f\u0302N,n(w\u0302N,n)\u2212 f\u0302\u2217n(w\u0302\u2217n)\n\u2264 f\u0302\u2217n(w\u0302N,n)\u2212 f\u0302N,n(w\u0302N,n)\n= 1\nn\nn\u2211\nj=1\n\u2113(yj , \u3008w\u0302N,n, \u03c8\u03b8\u2217(xj)\u3009)\u2212 1\nn\nn\u2211\nj=1\n\u2113(yj, \u3008w\u0302N,n, \u03c8\u03b8\u0302N (xj)\u3009)\n\u2264 1 n\nn\u2211\nj=1\nL\u2113 \u2223\u2223\u2223\u3008w\u0302N,n, \u03c8\u03b8\u2217(xj)\u3009 \u2212 \u3008w\u0302N,n, \u03c8\u03b8\u0302N (xj)\u3009 \u2223\u2223\u2223\n\u2264 1 n\nn\u2211\nj=1\nL\u2113RW \u2225\u2225\u2225\u03c8\u03b8\u2217(xj)\u2212 \u03c8\u03b8\u0302N (xj) \u2225\u2225\u2225\n\u2264 1 n\nn\u2211\nj=1\nL\u2113RWL\u03c8 \u2225\u2225\u2225\u03b8\u0302N \u2212 \u03b8\u2217 \u2225\u2225\u2225\n= L\u2113RWL\u03c8 \u2225\u2225\u2225\u03b8\u0302N \u2212 \u03b8\u2217 \u2225\u2225\u2225 .\nSince f\u0302\u2217n is \u03c1-strongly convex and w\u0302 \u2217 n is its minimizer,\nf\u0302\u2217n(w\u0302N,n)\u2212 f\u0302\u2217n(w\u0302\u2217n) \u2265 \u03c1 2 \u2016w\u0302N,n \u2212 w\u0302\u2217n\u201622.\nThus, we obtain (15)."}, {"heading": "B Appendix: Proof of Sparse Coding Stability", "text": "The proof of Theorem 2 is almost the same as that of Theorem 1 in Mehta and Gray (2012). However, since a part of the proof can not applied to our setting, we provide the full proof of Theorem 2 in this section. Lemma 2. Let a \u2208 Rm and E \u2208 Rd\u00d7m. Then, \u2016Ea\u20162 \u2264 \u2016E\u20161,2\u2016a\u20161.\n[Proof]\n\u2016Ea\u20162 = \u2016 m\u2211\ni=1\naiei\u20162 \u2264 m\u2211\ni=1\n|ai|\u2016ei\u20162 \u2264 \u2016E\u20161,2 m\u2211\ni=1\n|ai| = \u2016E\u20161,2\u2016a\u20161.\nLemma 3. The sparse representation \u03d5D(x) satisfies \u2016\u03d5D(x)\u20161 \u2264 \u2016x\u201622/\u03bb.\n[Proof] \u03bb \u2016\u03d5D(x)\u20161 \u2264 \u2016x\u2212D\u03d5D(x)\u201622 + \u03bb \u2016\u03d5D(x)\u20161\n= min z\u2208Rm \u2016x\u2212Dz\u201622 + \u03bb \u2016z\u20161 \u2264 \u2016x\u201622.\nLet a\u2217 and a\u0303\u2217 respectively denote the solutions to the LASSO problems for the dictionary D and D\u0303:\na \u2217 = argmin\nz\u2208Rm\n1 2 \u2016x\u2212Dz\u201622 + \u03bb\u2016z\u20161,\na\u0303 \u2217 = argmin\nz\u2208Rm\n1 2 \u2016x\u2212 D\u0303z\u201622 + \u03bb\u2016z\u20161.\nLet vD and vD\u0303 be the optimal values of the LASSO problems for the dictionary D and D\u0303:\nvD = min z\u2208Rm\n1 2 \u2016x\u2212Dz\u201622 + \u03bb\u2016z\u20161 = 1 2 \u2016x\u2212Da\u2217\u201622 + \u03bb\u2016a\u2217\u20161,\nv D\u0303 = min z\u2208Rm\n1 2 \u2016x\u2212 D\u0303z\u201622 + \u03bb\u2016z\u20161 = 1 2 \u2016x\u2212 D\u0303a\u0303\u2217\u201622 + \u03bb\u2016a\u0303\u2217\u20161.\nLemma 4 (Optimal Value Stability).\n|vD \u2212 vD\u0303| \u2264 1\n2 (2\u2016x\u20162 + 1) \u2016x\u201622 \u2016D\u2212 D\u0303\u20161,2 \u03bb .\n[Proof]\nv D\u0303 \u2264 1 2 \u2016x\u2212 D\u0303a\u2217\u201622 + \u03bb\u2016a\u2217\u20161\n= 1\n2 \u2016x\u2212Da\u2217 + (D\u2212 D\u0303)a\u2217\u201622 + \u03bb\u2016a\u2217\u20161\n\u2264 1 2 (\u2016x\u2212Da\u2217\u201622 + 2\u2016x\u2212Da\u2217\u20162\u2016(D\u2212 D\u0303)a\u2217\u20162 + \u2016(D\u2212 D\u0303)a\u2217\u201622) + \u03bb\u2016a\u2217\u20161\n\u2264 1 2 \u2016x\u2212Da\u2217\u201622 + \u03bb\u2016a\u2217\u20161 + \u2016x\u20162\n( \u2016x\u201622\u2016D\u2212 D\u0303\u20161,2\n\u03bb\n) + 1\n2\n( \u2016x\u201622\u2016D\u2212 D\u0303\u20161,2\n\u03bb\n)2\n\u2264 vD + ( \u2016x\u20162 + 1\n2 ) \u2016x\u201622 \u03bb \u2016D\u2212 D\u0303\u20161,2,\nwhere we used\n\u2016x\u2212Da\u2217\u20162 = \u221a \u2016x\u2212Da\u2217\u20162 \u2264 \u221a \u2016x\u2212Da\u2217\u20162 + \u03bb\u2016a\u2217\u20161 \u2264 \u221a \u2016x\u201622 = \u2016x\u20162.\nLemma 5 (Stability of Norm of Reconstructor). If \u2016D\u2212 D\u0303\u20161,2 \u2264 \u03bb, then \u2223\u2223\u2223\u2016Da\u2217\u201622 \u2212 \u2016D\u0303a\u0303\u2217\u201622 \u2223\u2223\u2223 \u2264 (2\u2016x\u20162 + 1) \u2016x\u201622 \u2016D\u2212 D\u0303\u20161,2\n\u03bb .\nThe proof of Lemma 5 is the same as that of Lemma 11 in Mehta and Gray (2012).\nLemma 6. If \u2016D\u2212 D\u0303\u20161,2 \u2264 \u03bb, then \u2223\u2223\u2016Da\u2217\u201622 \u2212 \u2016Da\u0303\u2217\u201622 \u2223\u2223 \u2264 ( 3\u2016x\u201622 + 6\u2016x\u20162 + 1 ) \u2016x\u201622\n\u2016D\u2212 D\u0303\u20161,2 \u03bb .\n[Proof] First, note that\n\u2016(D\u0303\u2212D)a\u0303\u2217\u20162 \u2264 \u2016(D\u0303\u2212D)\u20161,2\u2016a\u0303\u2217\u20162 \u2264 \u2016x\u201622 \u2016D\u2212 D\u0303\u20161,2\n\u03bb\nand\n\u2016Da\u0303\u2217\u20162 \u2264 \u2016(D\u2212 D\u0303)a\u0303\u2217\u20162 + \u2016D\u0303a\u0303\u2217 \u2212 x\u20162 + \u2016x\u20162\n\u2264 \u2016x\u201622 \u2016D\u2212 D\u0303\u20161,2\n\u03bb + 2\u2016x\u20162\n\u2264 (\u2016x\u20162 + 2)\u2016x\u20162, where we used Lemma 3. Then, we have\n\u2223\u2223\u2223\u2016Da\u0303\u2217\u201622 \u2212 \u2016D\u0303a\u0303\u2217\u201622 \u2223\u2223\u2223\n\u2264 2 \u2223\u2223\u2223\u3008Da\u0303\u2217, (D\u0303\u2212D)a\u0303\u2217\u3009 \u2223\u2223\u2223+ \u2016(D\u0303\u2212D)a\u0303\u2217\u201622 \u2264 2\u2016Da\u0303\u2217\u20162\u2016(D\u0303\u2212D)a\u0303\u2217\u20162 + \u2016(D\u0303\u2212D)a\u0303\u2217\u201622 \u2264 2(\u2016x\u20162 + 2)\u2016x\u20162 ( \u2016x\u201622\u2016D\u2212 D\u0303\u20161,2\n\u03bb\n) + ( \u2016x\u201622\u2016D\u2212 D\u0303\u20161,2\n\u03bb\n)2\n\u2264 (3\u2016x\u20162 + 4)\u2016x\u20162 ( \u2016x\u201622\u2016D\u2212 D\u0303\u20161,2\n\u03bb\n) .\nCombining this fact with Lemma 5, we have\u2223\u2223\u2016Da\u2217\u201622 \u2212 \u2016Da\u0303\u2217\u201622 \u2223\u2223\n\u2264 \u2223\u2223\u2223\u2016Da\u2217\u201622 \u2212 \u2016D\u0303a\u0303\u2217\u201622 \u2223\u2223\u2223+ \u2223\u2223\u2223\u2016D\u0303a\u0303\u2217\u201622 \u2212 \u2016Da\u0303\u2217\u201622 \u2223\u2223\u2223 \u2264 (2\u2016x\u20162 + 1) ( \u2016x\u201622\u2016D\u2212 D\u0303\u20161,2\n\u03bb\n) + (3\u2016x\u20162 + 4)\u2016x\u20162 ( \u2016x\u201622\u2016D\u2212 D\u0303\u20161,2\n\u03bb\n)\n= ( 3\u2016x\u201622 + 6\u2016x\u20162 + 1 ) \u2016x\u201622 \u2016D\u2212 D\u0303\u20161,2 \u03bb .\nLemma 7 (Reconstructor Stability). If \u2016D\u2212 D\u0303\u20161,2 \u2264 \u03bb, then\n\u2016Da\u2217 \u2212Da\u0303\u2217\u201622 \u2264 2 ( 3\u2016x\u201622 + 10\u2016x\u20162 + 3 ) \u2016x\u201622 \u2016D\u2212 D\u0303\u20161,2 \u03bb .\n[Proof] We set as a\u0304\u2217 := 12 (a \u2217 + a\u0303\u2217). From the optimality of a\u2217, it follows that vD(a\u2217) \u2264 vD(a\u0304\u2217), that is, 1\n2 \u2016x\u2212Da\u2217\u201622 + \u03bb\u2016a\u2217\u20161 \u2264\n1 2 \u2016x\u2212Da\u0304\u2217\u201622 + \u03bb\u2016a\u0304\u2217\u20161. (16)\nWe denote as \u01eb := \u2016D\u2212 D\u0303\u20161,2, cx := (2\u2016x\u20162 + 1) \u2016x\u201622 and c\u2032x := ( 3\u2016x\u201622 + 6\u2016x\u20162 + 1 ) \u2016x\u201622.\nBy the convexity of the l1-norm, the RHS of (16) obeys:\n1\n2\n\u2225\u2225\u2225\u2225x\u2212D ( a \u2217 + a\u0303\u2217\n2\n)\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb \u2225\u2225\u2225\u2225 a \u2217 + a\u0303\u2217\n2 \u2225\u2225\u2225\u2225 1\n\u2264 1 2 \u2225\u2225\u2225\u2225x\u2212 1 2 (Da\u2217 +Da\u0303\u2217) \u2225\u2225\u2225\u2225 2\n2\n+ \u03bb\n2 \u2016a\u2217\u20161 +\n\u03bb 2 \u2016a\u0303\u2217\u20161\n= 1\n2\n( \u2016x\u201622 \u2212 2 \u2329 x, 1\n2 (Da\u2217 +Da\u0303\u2217)\n\u232a + 1\n4 \u2016Da\u2217 +Da\u0303\u2217\u201622\n) + \u03bb\n2 \u2016\u03b1\u20161 +\n\u03bb 2 \u2016\u03b1\u0303\u20161\n= 1\n2 \u2016x\u201622 \u2212\n1 2 \u3008x,Da\u2217\u3009 \u2212 1 2 \u3008x,Da\u0303\u2217\u3009+ 1 8 (\u2016Da\u2217\u201622 + \u2016Da\u0303\u2217\u201622 + 2\u3008Da\u2217,Da\u0303\u2217\u3009)\n+ \u03bb\n2 \u2016\u03b1\u20161 +\n\u03bb 2 \u2016\u03b1\u0303\u20161\n\u2264 1 2 \u2016x\u201622 \u2212 1 2 \u3008x,Da\u2217\u3009 \u2212 1 2 \u3008x,Da\u0303\u2217\u3009+ 1 4 \u2016Da\u2217\u201622 + 1 4 \u3008Da\u2217,Da\u0303\u2217\u3009\n+ \u03bb\n2 \u2016\u03b1\u20161 +\n\u03bb 2 \u2016\u03b1\u0303\u20161 + c\u2032x 8 \u01eb \u03bb\n\u2264 1 2 \u2016x\u201622 \u2212 1 2 \u3008x,Da\u2217\u3009 \u2212 1 2 \u3008x,Da\u0303\u2217\u3009+ 1 4 \u2016Da\u2217\u201622 + 1 4 \u3008Da\u2217,Da\u0303\u2217\u3009\n+ 1 2 \u3008x\u2212Da\u2217,Da\u2217\u3009+ 1 2 \u3008x\u2212 D\u0303a\u0303\u2217, D\u0303a\u0303\u2217\u3009+ c\n\u2032 x\n8\n\u01eb\n\u03bb\n\u2264 1 2 \u2016x\u201622 \u2212 1 2 \u3008x,Da\u2217\u3009 \u2212 1 2 \u3008x,Da\u0303\u2217\u3009+ 1 4 \u2016Da\u2217\u201622 + 1 4 \u3008Da\u2217,Da\u0303\u2217\u3009\n+ 1 2 \u3008x,Da\u2217\u3009 \u2212 1 2 \u2016Da\u2217\u201622 + 1 2 \u3008x, D\u0303a\u0303\u2217\u3009 \u2212 1 2 \u2016Da\u2217\u201622 + ( c\u2032x 8 + cx 2 ) \u01eb \u03bb\n= 1\n2 \u2016x\u201622 \u2212\n3 4 \u2016Da\u2217\u201622 + 1 4 \u3008Da\u2217,Da\u0303\u2217\u3009+ ( c\u2032 x + 4cx 8 ) \u01eb \u03bb .\nNow, taking the (expanded) LHS of (16) and the newly derived upper bound of the RHS of (16) yields the inequality:\n1 2 \u2016x\u20161 \u2212 \u3008x,Da\u2217\u3009+ 1 2 \u2016Da\u2217\u201622 + \u03bb\u2016a\u2217\u20161\n\u2264 1 2 \u2016x\u201622 \u2212 3 4 \u2016Da\u2217\u201622 + 1 4 \u3008Da\u2217,Da\u0303\u2217\u3009+ ( c\u2032 x + 4cx 8 ) \u01eb \u03bb .\nReplacing \u03bb\u2016a\u2217\u20161 with \u3008x\u2212Da\u2217,Da\u2217\u3009 yields:\n\u2212\u3008x,Da\u2217\u3009+ 1 2 \u2016Da\u2217\u201622 + \u3008x\u2212Da\u2217,Da\u2217\u3009\n\u2264 \u22123 4 \u2016Da\u2217\u201622 + 1 4 \u3008Da\u2217,Da\u0303\u2217\u3009+ ( c\u2032 x + 4cx 8 ) \u01eb \u03bb .\nHence,\n\u2016Da\u2217\u201622 \u2264 \u3008Da\u2217,Da\u0303\u2217\u3009+ c\u2032x + 4cx\n2\n\u01eb \u03bb .\nNow, note that\n\u2016Da\u2217 \u2212Da\u0303\u2217\u201622 = \u2016Da\u2217\u201622 + \u2016Da\u0303\u2217\u201622 \u2212 2\u3008Da\u2217,Da\u0303\u2217\u3009 \u2264 \u2016Da\u2217\u201622 + ( \u2016Da\u2217\u201622 + c\u2032x \u01eb\n\u03bb\n) + ( \u22122\u2016Da\u2217\u201622 + (c\u2032x + 4cx) \u01eb\n\u03bb\n)\n\u2264 2(c\u2032 x + 2cx)\n\u01eb \u03bb .\nLemma 8. [Preservation of Sparsity] If\nMk(D,x) \u2265 ( 1 +\n\u2016x\u20162 \u03bb\n) \u2016x\u20162\u2016D\u2212 D\u0303\u20161,2 +\n\u221a\n2 (3\u2016x\u201622 + 10\u2016x\u20162 + 3) \u2016x\u201622 \u2016D\u2212 D\u0303\u20161,2\n\u03bb , (17)\nthen\n\u2016\u03d5D(x)\u2212 \u03d5D\u0303(x)\u20160 \u2264 k. (18)\n[Proof] In this proof, we denote\u03d5D(x) and \u03d5D\u0303(x) by a \u2217 = [a\u22171, . . . , a \u2217 m] \u22a4 and a\u0303\u2217 = [a\u0303\u22171, . . . , a\u0303 \u2217 m] \u22a4, respectively. When D\u0303 = D, Lemma 8 obviously holds. In the following, we assume D\u0303 6= D. Since Mk(D,x) > 0 from (17), there is a I \u2282 [m] with |I| = m\u2212 k such that for all i \u2208 I:\n0 < Mk(D,x) \u2264 \u03bb\u2212 |\u3008dj ,x\u2212D\u03d5D(x)\u3009|. (19) To obtain (18), it is enough to show that a\u2217i = 0 and a\u0303 \u2217 i = 0 for all i \u2208 I. First, we show a\u2217i = 0 for all i \u2208 I. From the optimality conditions for the LASSO (Asif and Romberg (2010), conditions L1 and L2), we have\n\u3008dj ,x\u2212Da\u2217\u3009 = sign(a\u2217j )\u03bb if a\u2217j 6= 0, |\u3008dj ,x\u2212Da\u2217\u3009| < \u03bb otherwise.\nNote that the above optimality conditions imply that if a\u2217j 6= 0 then |\u3008dj ,x\u2212Da\u2217\u3009| = \u03bb. (20) Combining (20) with (19), it holds that a\u2217i = 0 for all i \u2208 I. Next, we show a\u0303\u2217i = 0 for all i \u2208 I. To do so, it is sufficient to show that\n|\u3008d\u0303i,x\u2212 D\u0303a\u0303\u2217\u3009| < \u03bb (21) for all i \u2208 I. Note that\n|\u3008d\u0303i,x\u2212 D\u0303a\u0303\u2217\u3009| = |\u3008di + d\u0303i \u2212 di,x\u2212 D\u0303a\u0303\u2217\u3009| \u2264 |\u3008di,x\u2212 D\u0303a\u0303\u2217\u3009|+ \u2016d\u0303i \u2212 di\u20162\u2016x\u2212 D\u0303a\u0303\u2217\u20162 \u2264 |\u3008di,x\u2212 D\u0303a\u0303\u2217\u3009|+ \u2016D\u0303\u2212D\u20161,2\u2016x\u20162\nand\n|\u3008di,x\u2212 D\u0303a\u0303\u2217\u3009| = |\u3008di,x\u2212 (D+ D\u0303\u2212D)a\u0303\u2217\u3009| \u2264 |\u3008di,x\u2212Da\u0303\u2217\u3009|+ |\u3008di, (D\u0303\u2212D)a\u0303\u2217\u3009| \u2264 |\u3008di,x\u2212Da\u0303\u2217\u3009|+ \u2016D\u0303\u2212D\u20161,2\u2016a\u0303\u2217\u20161.\nHence,\n|\u3008d\u0303i,x\u2212 D\u0303a\u0303\u2217\u3009| \u2264 |\u3008di,x\u2212Da\u0303\u2217\u3009|+ ( 1 +\n\u2016x\u20162 \u03bb\n) \u2016x\u20162\u2016D\u2212 D\u0303\u20161,2.\nNow,\n|\u3008di,x\u2212Da\u0303\u2217\u3009| = |\u3008di,x\u2212Da\u2217 +Da\u2217 \u2212Da\u0303\u2217\u3009| \u2264 |\u3008di,x\u2212Da\u2217\u3009|+ |\u3008di,Da\u2217 \u2212Da\u0303\u2217\u3009| \u2264 \u03bb\u2212Mk(D,x) + \u2016Da\u2217 \u2212Da\u0303\u2217\u20162\n\u2264 \u03bb\u2212Mk(D,x) +\n\u221a\n2 (3\u2016x\u201622 + 10\u2016x\u20162 + 3) \u2016x\u201622 \u2016D\u2212 D\u0303\u20161,2\n\u03bb , (22)\nwhere (22) is due to Lemma 7. Then, (21) is obtained by (17).\n[Proof of Theorem 2]\nFollowing by the notations of Mehta and Gray (2012), we denote \u03d5D(x) and \u03d5D\u0303(x) by z\u2217 and t\u2217, respectively. From (23) of Mehta and Gray (2012), we have\n(z\u2217 \u2212 t\u2217)\u22a4D\u22a4D(z\u2217 \u2212 t\u2217) \u2264 (z\u2217 \u2212 t\u2217)\u22a4 ( (D\u0303\u22a4D\u0303\u2212D\u22a4D)t\u2217 + 2(D\u2212 D\u0303)\u22a4x )\n= (z\u2217 \u2212 t\u2217)\u22a4(D\u0303\u22a4D\u0303\u2212D\u22a4D)t\u2217 + 2(z\u2217 \u2212 t\u2217)\u22a4(D\u2212 D\u0303)\u22a4x. (23)\nWe evaluate the second term in (23) 3). We have the following by the definition of z\u2217:\n\u2016x\u2212 D\u0303t\u2217\u201622 + \u03bb\u2016t\u2217\u20161 \u2265 \u2016x\u2212 D\u0303z\u2217\u201622 + \u03bb\u2016z\u2217\u20161, and thus,\n2(z\u2217 \u2212 t\u2217)\u22a4D\u0303\u22a4x \u2265 z\u22a4\u2217 D\u0303\u22a4D\u0303z\u2217 \u2212 t\u22a4\u2217 D\u0303\u22a4D\u0303t\u2217 + \u03bb(\u2016z\u2217\u20161 \u2212 \u2016t\u2217\u20161). Similarly, we have\n2(t\u2217 \u2212 z\u2217)\u22a4D\u22a4x \u2265 t\u22a4\u2217 D\u22a4Dt\u2217 \u2212 z\u22a4\u2217 D\u22a4Dz\u2217 + \u03bb(\u2016t\u2217\u20161 \u2212 \u2016z\u2217\u20161). Summing up the above inequalities and multiplying \u22121, we obtain\n2(z\u2217 \u2212 t\u2217)\u22a4(D\u2212 D\u0303)\u22a4x \u2264 \u2212z\u22a4\u2217 D\u0303\u22a4D\u0303z\u2217 + t\u22a4\u2217 D\u0303\u22a4D\u0303t\u2217 \u2212 t\u22a4\u2217 D\u22a4Dt\u2217 + z\u22a4\u2217 D\u22a4Dz\u2217 = \u2212z\u22a4\u2217 (D\u0303\u22a4D\u0303\u2212D\u22a4D)z\u2217 + t\u22a4\u2217 (D\u0303\u22a4D\u0303\u2212D\u22a4D)t\u2217 = (z\u2217 \u2212 t\u2217)\u22a4(D\u22a4D\u2212 D\u0303\u22a4D\u0303)z\u2217 \u2212 (z\u2217 \u2212 t\u2217)\u22a4(D\u0303\u22a4D\u0303\u2212D\u22a4D)t\u2217 (24)\nWhen E := D\u2212 D\u0303, from (23) and (24), (z\u2217 \u2212 t\u2217)\u22a4D\u22a4D(z\u2217 \u2212 t\u2217)\n\u2264 (z\u2217 \u2212 t\u2217)\u22a4(D\u22a4D\u2212 D\u0303\u22a4D\u0303)z\u2217 \u2264 |(z\u2217 \u2212 t\u2217)\u22a4(E\u22a4D\u0303+ D\u0303\u22a4E+E\u22a4E)z\u2217| \u2264 |(z\u2217 \u2212 t\u2217)\u22a4E\u22a4D\u0303z\u2217|+ |(z\u2217 \u2212 t\u2217)\u22a4D\u0303\u22a4Ez\u2217|+ |(z\u2217 \u2212 t\u2217)\u22a4E\u22a4Ez\u2217| \u2264 \u2016E(z\u2217 \u2212 t\u2217)\u20162\u2016D\u0303z\u2217\u20162 + \u2016D\u0303(z\u2217 \u2212 t\u2217)\u20162\u2016Ez\u2217\u20162 + \u2016E(z\u2217 \u2212 t\u2217)\u20162\u2016Ez\u2217\u20162 \u2264 (\u2016E\u20161,2\u2016D\u0303\u20161,2\u2016z\u2217\u20161 + \u2016D\u0303\u20161,2\u2016E\u20161,2\u2016z\u2217\u20161 + \u2016E\u20161,2\u2016E\u20161,2\u2016z\u2217\u20161)\u2016z\u2217 \u2212 t\u2217\u20161 \u2264 ( \u2016x\u201622\u2016E\u20161,2\n\u03bb + \u2016x\u201622\u2016E\u20161,2 \u03bb + \u2016x\u201622\u2016E\u201621,2 \u03bb\n) \u221a k\u2016z\u2217 \u2212 t\u2217\u20162\n\u2264 ( 4\u2016x\u201622 \u03bb ) \u2016E\u20161,2 \u221a k\u2016z\u2217 \u2212 t\u2217\u20162, (25)\n3)The following bound in Mehta and Gray (2012) is not used in this paper:\n2(z\u2217 \u2212 t\u2217)\u22a4(D\u2212 D\u0303)\u22a4x \u2264 2\u2016D \u2212 D\u0303\u20161,2 \u221a k\u2016z\u2217 \u2212 t\u2217\u20162\u2016x\u20162.\nwhere we used \u2016E\u20161,2 \u2264 2 in the last inequality. We note that the assumption (17) of Lemma 8 follows from (5). Then, since \u2016z\u2217 \u2212 t\u2217\u20160 \u2264 k from Lemma 8, we have the following lower bound of (23) from the \u00b5-incoherence of D:\n(z\u2217 \u2212 t\u2217)\u22a4D\u22a4D(z\u2217 \u2212 t\u2217) \u2265 (1\u2212 \u00b5k/ \u221a d)\u2016z\u2217 \u2212 t\u2217\u201622. (26)\nBy (25) and (26), we obtain\n\u2016z\u2217 \u2212 t\u2217\u20162 \u2264 4\u2016x\u201622\n\u221a k\n(1\u2212 \u00b5k/ \u221a d)\u03bb \u2016D\u2212 D\u0303\u20161,2."}, {"heading": "C Appendix: Proof of Margin Bound", "text": "In this proof, we set as\n\u03b41 := 2\u03c3\n(1\u2212 t) \u221a d\u03bb exp\n( \u2212 (1\u2212 t) 2d\u03bb2\n8\u03c32\n) ,\n\u03b42 := 2\u03c3m\u221a d\u03bb exp\n( \u2212d\u03bb 2\n8\u03c32\n) ,\n\u03b4\u20323 := 4\u03c3k\nC \u221a d(1 \u2212 \u00b5k/ \u221a d) exp\n( \u2212C 2d(1 \u2212 \u00b5k/ \u221a d)\n8\u03c32\n)\n\u03b4\u2032\u20323 := 8\u03c3(d\u2212 k)\nd\u03bb exp\n( \u2212d 2\u03bb2\n32\u03c32\n) ,\n\u03b43 := \u03b4 \u2032 3 + \u03b4 \u2032\u2032 3 .\nThen, \u03b4t,\u03bb = \u03b41 + \u03b42 + \u03b43.\nThe column vectors for a \u00b5-incoherent dictionary are in general position. Thus, a solution of LASSO for a \u00b5-incoherent dictionary is unique due to Lemma 3 in Tibshirani et al. (2013).\nLemma 9. When a dictionary D is \u00b5-incoherent, then the following bound holds for an arbitrary k-sparse vector b:\nb \u22a4 D \u22a4 Db \u2265 ( 1\u2212 \u00b5k\u221a\nd\n) \u2016b\u201622.\nRemark 1. We mention the relation with the k-incoherence of a dictionary, which is the assumption of the sparse coding stability in Mehta and Gray (2013). For k \u2208 [m] and D \u2208 D, the k-incoherence sk(D) is defined as\nsk(D) := (min{\u03c2k(D\u039b)|\u039b \u2282 [m], |\u039b| = k})2, where \u03c2k(D\u039b) is the k-th singular value of D\u039b = [di1 , . . . ,dik ] for \u039b = {i1, . . . , ik}. From Lemma 9, when a dictionary D is \u00b5-incoherent, the k-incoherence of D satisfies\nsk(D) \u2265 1\u2212 \u00b5k\u221a d .\nThus, a \u00b5-incoherent dictionary has positive k-incoherence when d > (\u00b5k)2. On the other hand, when k \u2265 2, if a dictionary D has positive k-incoherence sk(D), there is \u00b5 > 0 such that the dictionary is \u00b5-incoherent.\nThe following notions are introduced in Zhao and Yu (2006). Let a be a k-sparse vector. Without loss of generality, we assume that a = [a1, . . . , ak, 0, . . . , 0]\u22a4. Then, we denote as D(1) = [d1, . . . ,dk] and D(2) = [dk+1, . . . ,dm]. Then, we define as Cij := 1dD(i) \u22a4 D(j) for i, j \u2208 {1, 2}. When a dictionary D is \u00b5-incoherent and \u00b5k2/d < 1, C11 is positive definite due to Lemma 9 and especially invertible.\nDefinition 6 (Strong Irrepresentation Condition). There exists a positive vector \u03b7 such that\n|C21C\u2212111 sign(a)| \u2264 1\u2212 \u03b7, where sign(a) maps positive entry of a to 1, negative entry to \u22121 and 0 to 0, 1 is the (d \u2212 k) \u00d7 1 vector of 1\u2019s and the inequality holds element-wise.\nThen, the following lemma is derived by modifying the proof of Lemma 2 of Zhao and Yu (2006).\nLemma 10 (Strong Irrepresentation Condition). When a dictionary D is \u00b5-incoherent and d > \u00b5(2k \u2212 1) holds, the strong irrepresentation condition holds with \u03b7 = (1\u2212 \u00b5(2k \u2212 1)/d)1. Lemma 11. Under Assuptions 1-4, when D is \u00b5-incoherent and d > \u00b5(2k\u22121), the following holds:\nPr [|supp(a\u2212 \u03d5D(x))| = k] \u2265 1\u2212 \u03b43.\n[Proof] The following inequality obviously holds:\nPr [|supp(a\u2212 \u03d5D(x))| = k] \u2265 Pr [sign(a) = sign(\u03d5D(x))] .\nDue to Lemma 10 and Proofs of Theorems 3 and 4 in Zhao and Yu (2006), there exist sub-Gaussian random variables {zi}ki=1 and {\u03b6i}d\u2212ki=1 such that their variances are bounded as E[z2i ] \u2264 \u03c32/d(1\u2212 \u00b5k/ \u221a d) \u2264 \u03c32/d(1\u2212 \u00b5k/ \u221a d) and E[\u03b62i ] \u2264 \u03c32/d2 and\nPr [sign(a) = sign(\u03d5D(x))]\n\u2265 1\u2212 k\u2211\ni=1\nPr [ |zi| \u2265 \u221a d ( |ai| \u2212\n\u221a k\u03bb\n2(1\u2212 \u00b5k/ \u221a d)d\n)] \u2212 d\u2212k\u2211\ni=1\nPr [ |\u03b6i| \u2265\n(1\u2212 \u00b5(2k \u2212 1)/d)\u03bb 2 \u221a d\n] .\nWhen \u03bb \u2264 (1 \u2212 \u00b5k/ \u221a d)Cd/ \u221a k, the inequality |ai| \u2212\n\u221a k\u03bb\n2(1\u2212\u00b5k/ \u221a d)d \u2265 C/2 holds since |ai| \u2265 C. Then, since 1\u2212 \u00b5(2k \u2212 1)/d \u2265 1/2 holds, we obtain\nPr [ |zi| \u2265 \u221a d ( |ai| \u2212\n\u221a k\u03bb\n2(1\u2212 \u00b5k/ \u221a d)d\n)] \u2264 Pr [ |zi| \u2265 C \u221a d\n2\n] \u2264 \u03b4\u20323,\nPr [ |\u03b6i| \u2265\n(1\u2212 \u00b5(2k \u2212 1)/d)\u03bb 2 \u221a d\n] \u2264 Pr [ |\u03b6i| \u2265 \u03bb4\u221ad ] \u2264 \u03b4\u2032\u20323 ,\nwhere we used that zi and \u03b6i are sub-Gaussian. Thus the proof is completed.\nLemma 12. Let D be a dictionary. When \u03be satisfies Assumption 4, the following holds:\nPr[\u03bb \u2265 2\u2016D\u22a4\u03be\u2016\u221e] \u2264 1\u2212 \u03b42,\n[Proof] Let \u03be be a 1-dimensional sub-Gaussian with parameter \u03c3/ \u221a d. Then, it holds that for t > 0\nPr [|\u03be| > \u03bb] \u2264 \u03c3\u221a d\u03bb exp\n( \u2212d\u03bb 2\n2\u03c32\n) . (27)\nNote that \u3008dj , \u03be\u3009 is sub-Gaussian with parameter \u03c3/ \u221a d because \u2016dj\u20162 = 1 for every j \u2208 [m] and\ncomponents of \u03be are independent and sub-Gaussian with parameter \u03c3/ \u221a d. Thus,\nPr[\u03bb < 2\u2016D\u22a4\u03be\u2016\u221e] = Pr [ \u222amj=1{\u03bb < 2|\u3008dj , \u03be\u3009|} ] \u2264 m\u2211\nj=1\nPr[\u03bb < 2|\u3008dj, \u03be\u3009|] \u2264 \u03b42,\nwhere we used (27) in the last inequality.\nLemma 13. Under Assuptions 1-4, then\nPr [ \u2016a\u2212 \u03d5D(x)\u20162 \u2264\n3 \u221a k\n(1\u2212 \u00b5k/ \u221a d) \u03bb\n] \u2265 1\u2212 \u03b42 \u2212 \u03b43.\n[Proof] By Assumption 1, x = Da + \u03be. We denote \u03d5D(x) by a\u2217 and a \u2212 a\u2217 by \u2206. We have the following inequality by the definition of a\u2217:\n1 2 \u2016x\u2212Da\u2217\u201622 + \u03bb\u2016a\u2217\u20161 \u2264 1 2 \u2016x\u2212Da\u201622 + \u03bb\u2016a\u20161.\nSubstituting x = Da+ \u03be, we have\n1 2 \u2016D\u2206\u201622 \u2264 \u2212\u3008D\u22a4\u03be,\u2206\u3009+ \u03bb(\u2016a\u20161 \u2212 \u2016a\u2217\u20161)\n\u2264 \u2016D\u22a4\u03be\u2016\u221e\u2016\u2206\u20161 + \u03bb(\u2016a\u20161 \u2212 \u2016a\u2217\u20161). (28)\nLet \u2206k be the vector whose i-th component equals that of \u2206 if i is in the support of a and equals 0 otherwise. In addition, let \u2206\u22a5k = \u2206\u2212\u2206k. Using \u2206 = \u2206k +\u2206\u22a5k , we have\n\u2016a\u2217\u2016 = \u2016a+\u2206\u22a5k +\u2206k\u20161 \u2265 \u2016a\u20161 + \u2016\u2206\u22a5k \u20161 \u2212 \u2016\u2206k\u20161\nSubstituting the above inequality into (28), we have\n1 2 \u2016D\u2206\u201622 \u2264 \u2016D\u22a4\u03be\u2016\u221e\u2016\u2206\u20161 + \u03bb(\u2016\u2206k\u20161 \u2212 \u2016\u2206\u22a5k \u20161)\nThe inequality \u03bb \u2265 2\u2016D\u22a4\u03be\u2016\u221e holds with with probability 1 \u2212 \u03b42 due to Lemma 12, and then, the following inequality holds:\n0 \u2264 1 2 \u2016D\u2206\u201622 \u2264 1 2 \u03bb(\u2016\u2206k\u20161 + \u2016\u2206\u22a5k \u20161) + \u03bb(\u2016\u2206k\u20161 \u2212 \u2016\u2206\u22a5k \u20161).\nThus, \u2016\u2206\u22a5k \u20161 \u2264 3\u2016\u2206k\u20161 and 1\n2 \u2016D\u2206\u201622 \u2264\n3 2 \u03bb\u2016\u2206k\u20161 \u2212 1 2 \u03bb\u2016\u2206\u22a5k \u20161 \u2264 3 2 \u03bb\u2016\u2206k\u20161 \u2264 3 2 \u03bb \u221a k\u2016\u2206k\u20162.\nThus, we have\n\u2016D\u2206\u201622 \u2264 3\u03bb \u221a k\u2016\u2206k\u20162 \u2264 3\u03bb \u221a k\u2016\u2206\u20162.\nHere, \u2016supp(\u2206)\u20160 = k with probability 1\u2212 \u03b43 due to Lemma 11 and the following inequality holds by the \u00b5-incoherence of the dictionary D:\n(1 \u2212 \u00b5k/ \u221a d)\u2016\u2206\u201622 \u2264 \u2016D\u2206\u201622,\nand thus,\n\u2016\u2206\u20162 \u2264 3\u03bb\n\u221a k\n(1\u2212 \u00b5k/ \u221a d) .\n[Proof of Theorem 3] From Assumption 1, an arbitrary sample x is represented as x = D\u2217a + \u03be. Then,\n\u3008dj ,x\u2212D\u2217\u03d5D(x)\u3009 = \u3008dj , \u03be +D\u2217(a\u2212 \u03d5D(x))\u3009 = \u3008dj , \u03be\u3009+ \u3008D\u2217\u22a4dj , a\u2212 \u03d5D(x)\u3009.\nThen, we evaluate the probability that the first and second terms is bounded above by 1\u2212t2 \u03bb. We evaluate the probability for the first term. Since \u2016dj\u2016 = 1 by the definition and \u03be is drawn from a sub-Gaussian distribution with parameter \u03c32/ \u221a d, we have\nPr [ \u3008dj , \u03be\u3009 \u2264\n1\u2212 t 2 \u03bb\n] \u2264 1\u2212 \u03b41.\nWith probability 1\u2212 \u03b42 \u2212 \u03b43, the second term is evaluated as follows:\n\u3008D\u2217\u22a4dj , a\u2212 \u03d5D(x)\u3009 = \u3008[\u3008d1,dj\u3009, . . . , \u3008dm,dj\u3009]\u22a4, a\u2212 \u03d5D(x)\u3009 = \u3008(1supp(a\u2212\u03d5D(x)) \u25e6 [\u3008d1,dj\u3009, . . . , \u3008dm,dj\u3009])\u22a4, a\u2212 \u03d5D(x)\u3009 \u2264 \u2016(1supp(a\u2212\u03d5D(x)) \u25e6 [\u3008d1,dj\u3009, . . . , \u3008dm,dj\u3009])\u22a4\u20162\u2016a\u2212 \u03d5D(x)\u20162 \u2264 \u00b5\u221a\nd\n\u221a |supp(a\u2212 \u03d5D(x))|\u2016a\u2212 \u03d5D(x)\u20162\n\u2264 3\u00b5k (1 \u2212 \u00b5k/ \u221a d) \u221a d \u03bb (29) \u2264 1\u2212 t 2 \u03bb, (30)\nwhere we used Lemmas 11 and 13 in (29) and d \u2265 (\n6\u00b5k\n(1\u2212\u00b5k/ \u221a d)(1\u2212t)\n)2 in (30). Thus, with probabil-\nity 1\u2212 (\u03b41 + \u03b42 + \u03b43) = 1\u2212 \u03b4t,\u03bb, Mk,D\u2217(x) \u2265 \u03bb\u2212 \u3008dj ,x\u2212D\u2217\u03d5D(x)\u3009 \u2265 t\u03bb.\nWe note that d \u2265 {(\n1 + 6(1\u2212t)\n) \u00b5k }2 is enough to satisfy the condition d \u2265 (\n6\u00b5k\n(1\u2212\u00b5k/ \u221a d)(1\u2212t)\n)2 .\nThus, the proof of Theorem 3 is completed."}], "references": [{"title": "Simple, efficient, and neural algorithms for sparse coding,", "author": ["S. Arora", "R. Ge", "T. Ma", "A. Moitra"], "venue": "arXiv preprint arXiv:1503.00778", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "On the lasso and dantzig selector equivalence,", "author": ["M.S. Asif", "J. Romberg"], "venue": "Information Sciences and Systems (CISS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "A model of inductive bias learning,", "author": ["J. Baxter"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "An analysis of single-layer networks in unsupervised feature learning,", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "in International conference on artificial intelligence and statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Self-taught clustering,", "author": ["W. Dai", "Q. Yang", "G.-R. Xue", "Y. Yu"], "venue": "Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Building high-level features using large scale unsupervised learning,", "author": ["V. Q"], "venue": "in Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Exponential Family Sparse Coding with Application to Self-taught Learning,", "author": ["H. Lee", "R. Raina", "A. Teichman", "A.Y. Ng"], "venue": "in IJCAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Supervised dictionary learning,\u201d in Advances in neural information processing", "author": ["J. Mairal", "J. Ponce", "G. Sapiro", "A. Zisserman", "F.R. Bach"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Transfer bounds for linear feature learning,", "author": ["A. Maurer"], "venue": "Machine learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Sparse coding for multitask and transfer learning,", "author": ["A. Maurer", "M. Pontil", "B. Romera-Paredes"], "venue": "arXiv preprint arXiv:1209.0738", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "A unified framework for highdimensional analysis of M -estimators with decomposable regularizers,", "author": ["S. Negahban", "B. Yu", "M.J. Wainwright", "P.K. Ravikumar"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "A survey on transfer learning,", "author": ["S.J. Pan", "Q. Yang"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Self-taught learning: transfer learning from unlabeled data,", "author": ["R. Raina", "A. Battle", "H. Lee", "B. Packer", "A.Y. Ng"], "venue": "Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Fast rates for regularized objectives,", "author": ["K. Sridharan", "S. Shalev-Shwartz", "N. Srebro"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "The lasso problem and uniqueness,", "author": ["R.J. Tibshirani"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "The sample complexity of dictionary learning,", "author": ["D. Vainsencher", "S. Mannor", "A.M. Bruckstein"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Robust and discriminative self-taught learning,", "author": ["H. Wang", "F. Nie", "H. Huang"], "venue": "Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "On model selection consistency of Lasso,", "author": ["P. Zhao", "B. Yu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "References [1] S.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] W.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Q.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] H.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] P.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "We consider a transfer-learning problem by using the parameter transfer approach, where a suitable parameter of feature mapping is learned through one task and applied to another objective task. Then, we introduce the notion of the local stability of parametric feature mapping and parameter transfer learnability, and thereby derive a learning bound for parameter transfer algorithms. As an application of parameter transfer learning, we discuss the performance of sparse coding in selftaught learning. Although self-taught learning algorithms with plentiful unlabeled data often show excellent empirical performance, their theoretical analysis has not been studied. In this paper, we also provide the first theoretical learning bound for self-taught learning.", "creator": "LaTeX with hyperref package"}}}