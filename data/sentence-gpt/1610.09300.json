{"id": "1610.09300", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2016", "title": "Globally Optimal Training of Generalized Polynomial Neural Networks with Nonlinear Spectral Methods", "abstract": "The optimization problem behind neural networks is highly non-convex. Training with stochastic gradient descent and variants requires careful parameter tuning and provides no guarantee to achieve the global optimum. In contrast we show under quite weak assumptions on the data that a particular class of feedforward neural networks can be trained globally optimal with a linear convergence rate with our nonlinear spectral method. Up to our knowledge this is the first practically feasible method which achieves such a guarantee. While the method can in principle be applied to deep networks, we restrict ourselves for simplicity in this paper to one and two hidden layer networks. Our experiments confirm that these models are rich enough to achieve good performance on a series of real-world datasets. The data are still quite coarse in our experiments, and we therefore recommend that they be considered in addition to the standard procedure for deep learning on a single dataset. The first two datasets on an over-representative dataset will provide a clear understanding of how well they perform and how well the training can be achieved. We also recommend that we train a few basic neural networks to improve on the overall training performance of this deep network using the Deep Learning Toolkit. In a special note, our training model supports deep learning as a very deep neural network that can be trained from deep learning using multiple neural networks. We show that our training model can also be used for training on any network, such as those with recurrent neural networks. The basic approach used to train neural networks is to train the neural networks directly from the network and then train the neural networks directly from the network. This is achieved by a combination of training with the deep learning algorithm, an ensemble-based network, a stochastic gradient descent approach, and a gradient descent approach.\n\n\n\nA first look at the results on the deep learning training model and our previous work is available in my previous paper, \"Lifting Deep Learning and Neural Networks, in General Model Models for Deep Learning, with Different Model Models for Deep Learning, for Deep Learning, for Deep Learning, for Deep Learning.\" The paper discusses the process described in the paper's supplementary material. I want to thank David E. Waugh, Irs. Dene, David J. Irs. Nilsen, Jonathan S. Wassermann, and Jonathan S. Wassermann.", "histories": [["v1", "Fri, 28 Oct 2016 16:28:23 GMT  (310kb,D)", "http://arxiv.org/abs/1610.09300v1", "Long version of NIPS 2016 paper"]], "COMMENTS": "Long version of NIPS 2016 paper", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["antoine gautier", "quynh n nguyen", "matthias hein 0001"], "accepted": true, "id": "1610.09300"}, "pdf": {"name": "1610.09300.pdf", "metadata": {"source": "CRF", "title": "Globally Optimal Training of Generalized Polynomial Neural Networks with Nonlinear Spectral Methods", "authors": ["A. Gautier", "Q. Nguyen"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Deep learning [13, 16] is currently the state of the art machine learning technique in many application areas such as computer vision or natural language processing. While the theoretical foundations of neural networks have been explored in depth see e.g. [1], the understanding of the success of training deep neural networks is a currently very active research area [5, 6, 9]. On the other hand the parameter search for stochastic gradient descent and variants such as Adagrad and Adam can be quite tedious and there is no guarantee that one converges to the global optimum. In particular, the problem is even for a single hidden layer in general NP hard, see [17] and references therein. This implies that to achieve global optimality efficiently one has to impose certain conditions on the problem. A recent line of research has directly tackled the optimization problem of neural networks and provided either certain guarantees [2, 15] in terms of the global optimum or proved directly convergence to the global optimum [8, 11]. The latter two papers are up to our knowledge the first results which provide a globally optimal algorithm for training neural networks. While providing a lot of interesting insights on the relationship of structured matrix factorization and training of neural networks, Haeffele and Vidal admit themselves in their paper [8] that their results are \u201cchallenging to apply in practice\u201d. In the work of Janzamin et al. [11] they use a tensor approach and propose a globally optimal algorithm for a feedforward neural network with one hidden layer and squared loss. However, their approach requires the computation of the score function tensor which uses the density of the data-generating measure. However, the data generating measure is unknown and also difficult to estimate for high-dimensional feature spaces. Moreover, one has to check certain non-degeneracy conditions of the tensor decomposition to get the global optimality guarantee.\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 0.\n09 30\n0v 1\n[ cs\n.L G\n] 2\n8 O\nct 2\nIn contrast our nonlinear spectral method just requires that the data is nonnegative which is true for all sorts of count data such as images, word frequencies etc. The condition which guarantees global optimality just depends on the parameters of the architecture of the network and boils down to the computation of the spectral radius of a small nonnegative matrix. The condition can be checked without running the algorithm. Moreover, the nonlinear spectral method has a linear convergence rate and thus the globally optimal training of the network is very fast. The two main changes compared to the standard setting are that we require nonnegativity on the weights of the network and we have to minimize a modified objective function which is the sum of loss and the negative total sum of the outputs. While this model is non-standard, we show in some first experimental results that the resulting classifier is still expressive enough to create complex decision boundaries. As well, we achieve competitive performance on some UCI datasets. As the nonlinear spectral method requires some non-standard techniques, we use the main part of the paper to develop the key steps necessary for the proof. However, some proofs of the intermediate results are moved to the supplementary material."}, {"heading": "2 Main result", "text": "0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nIn this section we present the algorithm together with the main theorem providing the convergence guarantee. We limit the presentation to one hidden layer networks to improve the readability of the paper. Our approach can be generalized to feedforward networks of arbitrary depth. In particular, we present in Section 4.1 results for two hidden layers. We consider in this paper multi-class classification where d is the dimension of the feature space and K is the number of classes. We use the negative cross-entropy loss defined for label y \u2208 [K] := {1, . . . ,K} and classifier f : Rd \u2192 RK as\n( )\nThe function class we are using is a feedforward neural network with one hidden layer with n1 hidden units. As activation functions we use real powers of the form of a generalized polyomial, that is for \u03b1 \u2208 Rn1 with \u03b1i \u2265 1, i \u2208 [K], we define:\nfr(x) = fr(w, u)(x) = n1\u2211 l=1 wrl ( d\u2211 m=1 ulmxm )\u03b1l , (1)\nwhere R+ = {x \u2208 R |x \u2265 0} and w \u2208 RK\u00d7n1+ , u \u2208 R n1\u00d7d + are the parameters of the network which we optimize. The function class in (1) can be seen as a generalized polynomial in the sense that the powers do not have to be integers. Polynomial neural networks have been recently analyzed in [15]. Please note that a ReLU activation function makes no sense in our setting as we require the data as well as the weights to be nonnegative. Even though nonnegativity of the weights is a strong constraint, one can model quite complex decision boundaries (see Figure 1, where we show the outcome of our method for a toy dataset in R2). In order to simplify the notation we use w = (w1, . . . , wK) for the K output units wi \u2208 Rn1+ , i = 1, . . . ,K. All output units and the hidden layer are normalized. We optimize over the set\nS+ = { (w, u) \u2208 RK\u00d7n1+ \u00d7 R n1\u00d7d + \u2223\u2223 \u2016u\u2016pu = \u03c1u, \u2016wi\u2016pw = \u03c1w, \u2200i = 1, . . . ,K}. We also introduce S++ where one replaces R+ with R++ = {t \u2208 R | t > 0}. The final optimization problem we are going to solve is given as\nmax (w,u)\u2208S+ \u03a6(w, u) with (2)\n\u03a6(w, u) = 1 n n\u2211 i=1 [ \u2212 L ( yi, f(w, u)(xi) ) + K\u2211 r=1 fr(w, u)(xi) ] + ( K\u2211 r=1 n1\u2211 l=1 wr,l + n1\u2211 l=1 d\u2211 m=1 ulm ) ,\nwhere (xi, yi) \u2208 Rd+ \u00d7 [K], i = 1, . . . , n is the training data. Note that this is a maximization problem and thus we use minus the loss in the objective so that we are effectively minimizing the loss. The reason to write this as a maximization problem is that our nonlinear spectral method is inspired by the theory of (sub)-homogeneous nonlinear eigenproblems on convex cones [14] which has its origin in the Perron-Frobenius theory for nonnegative matrices. In fact our work is motivated by the closely related Perron-Frobenius theory for multihomogeneous problems developed in [7]. This is also the reason why we have nonnegative weights, as we work on the positive orthant which is a convex cone. Note that > 0 in the objective can be chosen arbitrarily small and is added out of technical reasons. In order to state our main theorem we need some additional notation. For p \u2208 (1,\u221e), we let p\u2032 = p/(p\u2212 1) be the H\u00f6lder conjugate of p, and \u03c8p(x) = sign(x)|x|p\u22121. We apply \u03c8p to scalars and vectors in which case the function is applied componentwise. For a square matrix A we denote its spectral radius by \u03c1(A). Finally, we write \u2207wi\u03a6(w, u) (resp. \u2207u\u03a6(w, u)) to denote the gradient of \u03a6 with respect to wi (resp. u) at (w, u). The mapping\nG\u03a6(w, u) = ( \u03c1w\u03c8p\u2032w(\u2207w1\u03a6(w, u)) \u2016\u03c8p\u2032w(\u2207w1\u03a6(w, u))\u2016pw , . . . , \u03c1w\u03c8p\u2032w(\u2207wK\u03a6(w, u)) \u2016\u03c8p\u2032w(\u2207wK\u03a6(w, u))\u2016pw , \u03c1u\u03c8p\u2032u(\u2207u\u03a6(w, u)) \u2016\u03c8p\u2032u(\u2207u\u03a6(w, u))\u2016pu ) ,\n(3) defines a sequence converging to the global optimum of (2). Indeed, we prove: Theorem 1. Let {xi, yi}ni=1 \u2282 Rd+ \u00d7 [K], pw, pu \u2208 (1,\u221e), \u03c1w, \u03c1u > 0, n1 \u2208 N and \u03b1 \u2208 Rn1 with \u03b1i \u2265 1 for every i \u2208 [n1]. Define \u03c1x, \u03be1, \u03be2 > 0 as \u03c1x = maxi\u2208[n] \u2016xi\u20161, \u03be1 = \u03c1w \u2211n1 l=1(\u03c1u\u03c1x)\u03b1l , \u03be2 = \u03c1w \u2211n1 l=1 \u03b1l(\u03c1u\u03c1x)\u03b1l and let A \u2208 R (K+1)\u00d7(K+1) ++ be defined as\nAl,m = 4(p\u2032w \u2212 1)\u03be1, Al,K+1 = 2(p\u2032w \u2212 1)(2\u03be2 + \u2016\u03b1\u2016\u221e), AK+1,m = 2(p\u2032u \u2212 1)(2\u03be1 + 1), AK+1,K+1 = 2(p\u2032u \u2212 1)(2\u03be2 + \u2016\u03b1\u2016\u221e \u2212 1),\n\u2200m, l \u2208 [K].\nIf the spectral radius \u03c1(A) of A satisfies \u03c1(A) < 1, then (2) has a unique global maximizer (w\u2217, u\u2217) \u2208 S++. Moreover, for every (w0, u0) \u2208 S++, there exists R > 0 such that\nlim k\u2192\u221e (wk, uk) = (w\u2217, u\u2217) and \u2016(wk, uk)\u2212 (w\u2217, u\u2217)\u2016\u221e \u2264 R\u03c1(A)k \u2200k \u2208 N,\nwhere (wk+1, uk+1) = G\u03a6(wk, uk) for every k \u2208 N.\nNote that one can check for a given model (number of hidden units n1, choice of \u03b1, pw, pu, \u03c1u, \u03c1w) easily if the convergence guarantee to the global optimum holds by computing the spectral radius of a square matrix of size K + 1. As our bounds for the matrix A are very conservative, the \u201ceffective\u201d spectral radius is typically much smaller, so that we have very fast convergence in only a few iterations, see Section 5 for a discussion. Up to our knowledge this is the first practically feasible algorithm to achieve global optimality for a non-trivial neural network model. Additionally, compared to stochastic gradient descent, there is no free parameter in the algorithm. Thus no careful tuning of the learning rate is required. The reader might wonder why we add the second term in the objective, where we sum over all outputs. The reason is that we need that the gradient of G\u03a6 is strictly positive in S+, this is why we also have to add the third term for arbitrarily small > 0. In Section 5 we show that this model achieves competitive results on a few UCI datasets.\nChoice of \u03b1: It turns out that in order to get a non-trivial classifier one has to choose \u03b11, . . . , \u03b1n1 \u2265 1 so that \u03b1i 6= \u03b1j for every i, j \u2208 [n1] with i 6= j. The reason for this lies in certain invariance properties of the network. Suppose that we use a permutation invariant componentwise activation function \u03c3, that is \u03c3(Px) = P\u03c3(x) for any permutation matrix P and suppose that A,B are globally optimal weight matrices for a one hidden layer architecture, then for any permutation matrix P ,\nA\u03c3(Bx) = APTP\u03c3(Bx) = APT\u03c3(PBx), which implies that A\u2032 = APT and B\u2032 = PB yield the same function and thus are also globally optimal. In our setting we know that the global optimum is unique and thus it has to hold that, A = APT and B = PB for all permutation matrices P . This implies that both A and B have rank one and thus lead to trivial classifiers. This is the reason why one has to use different \u03b1 for every unit.\nDependence of \u03c1(A) on the model parameters: Let Q, Q\u0303 \u2208 Rm\u00d7m+ and assume 0 \u2264 Qi,j \u2264 Q\u0303i,j for every i, j \u2208 [m], then \u03c1(Q) \u2264 \u03c1(Q\u0303), see Corollary 3.30 [3]. It follows that \u03c1(A) in Theorem 1 is increasing w.r.t. \u03c1u, \u03c1w, \u03c1x and the number of hidden units n1. Moreover, \u03c1(A) is decreasing w.r.t. pu, pw and in particular, we note that for any fixed architecture (n1, \u03b1, \u03c1u, \u03c1w) it is always possible to find pu, pw large enough so that \u03c1(A) < 1. Indeed, we know from the Collatz-Wielandt formula (Theorem 8.1.26 in [10]) that \u03c1(A) = \u03c1(AT ) \u2264 maxi\u2208[K+1](AT v)i/vi for any v \u2208 RK+1++ . We use this to derive lower bounds on pu, pw that ensure \u03c1(A) < 1. Let v = (pw \u2212 1, . . . , pw \u2212 1, pu \u2212 1), then (AT v)i < vi for every i \u2208 [K + 1] guarantees \u03c1(A) < 1 and is equivalent to\npw > 4(K + 1)\u03be1 + 3 and pu > 2(K + 1)(\u2016\u03b1\u2016\u221e + 2\u03be2)\u2212 1, (4)\nwhere \u03be1, \u03be2 are defined as in Theorem 1. However, we think that our current bounds are sub-optimal so that this choice is quite conservative. Finally, we note that the constant R in Theorem 1 can be explicitly computed when running the algorithm (see Theorem 3).\nProof Strategy: The following main part of the paper is devoted to the proof of the algorithm. For that we need some further notation. We introduce the sets\nV+ = RK\u00d7n1+ \u00d7 R n1\u00d7d + , V++ = R K\u00d7n1 ++ \u00d7 R n1\u00d7d ++ B+ = { (w, u) \u2208 V+ \u2223\u2223 \u2016u\u2016pu \u2264 \u03c1u, \u2016wi\u2016pw \u2264 \u03c1w, \u2200i = 1, . . . ,K},\nand similarly we define B++ replacing V+ by V++ in the definition. The high-level idea of the proof is that we first show that the global maximum of our optimization problem in (2) is attained in the \u201cinterior\u201d of S+, that is S++. Moreover, we prove that any critical point of (2) in S++ is a fixed point of the mapping G\u03a6. Then we proceed to show that there exists a unique fixed point of G\u03a6 in S++ and thus there is a unique critical point of (2) in S++. As the global maximizer of (2) exists and is attained in the interior, this fixed point has to be the global maximizer. Finally, the proof of the fact that G\u03a6 has a unique fixed point follows by noting that G\u03a6 maps B++ into B++ and the fact that B++ is a complete metric space with respect to the Thompson metric. We provide a characterization of the Lipschitz constant of G\u03a6 and in turn derive conditions under which G\u03a6 is a contraction. Finally, the application of the Banach fixed point theorem yields the uniqueness of the fixed point of G\u03a6 and the linear convergence rate to the global optimum of (2). In Section 4 we show the application of the established framework for our neural networks."}, {"heading": "3 From the optimization problem to fixed point theory", "text": "Lemma 1. Let \u03a6 : V \u2192 R be differentiable. If \u2207\u03a6(w, u) \u2208 V++ for every (w, u) \u2208 S+, then the global maximum of \u03a6 on S+ is attained in S++.\nProof. First note that as \u03a6 is a continuous function on the compact set S+ the global minimum and maximum are attained. A boundary point (wb, ub) of S+ is characterized by the fact that at least one of the variables (wb, ub) has a zero component. Suppose w.l.o.g. that the subset J \u2282 [n1] of components of wb1 \u2208 R n1 + are zero, that is wb1,J = 0. The normal vector of the pw-sphere at wb1 is given by \u03bd = \u03c8pw(wb1). The set of tangent directions is thus given by T = {v \u2208 Rn1 | \u3008\u03bd, v\u3009 = 0}. Note that if (wb, ub) is a local maximum, then\u2329\n\u2207wb1\u03a6(w b, ub), t \u232a \u2264 0, \u2200t \u2208 T+ = {v \u2208 Rn1+ | \u3008\u03bd, v\u3009 = 0}, (5)\nwhere T+ is the set of \u201cpositive\u201d tangent directions, that are pointing inside the set {w1 \u2208 Rn1+ | \u2016w1\u2016pw = \u03c1w}. Otherwise there would exist a direction of ascent which leads to a feasible point. Now note that \u03bd has non-negative components as wb1 \u2208 R n1 + . Thus\nT+ = {v \u2208 Rn1+ | vi = 0 if i /\u2208 J}.\nHowever, by assumption \u2207wb1\u03a6(w b, ub) is a vector with strictly positive components and thus (5) can never be fulfilled as T+ contains only vectors with non-negative components and at least one of the components is strictly positive as J 6= [n1]. Finally, as the global maximum is attained in S+ and no local maximum exists at the boundary, the global maximum has to be attained in S++.\nWe now identify critical points of the objective \u03a6 in S++ with fixed points of G\u03a6 in S++. Lemma 2. Let \u03a6 : V \u2192 R be differentiable. If \u2207\u03a6(w, u) \u2208 V++ for all (w, u) \u2208 S++, then (w\u2217, u\u2217) is a critical point of \u03a6 in S++ if and only if it is a fixed point of G\u03a6.\nProof. The Lagrangian of \u03a6 constrained to the unit sphere S is given by\nL(w, u, \u03bb) = \u03a6(w, u)\u2212 \u03bbK+1(\u2016u\u2016pu \u2212 \u03c1u)\u2212 K\u2211 j=1 \u03bbi(\u2016wj\u2016pw \u2212 \u03c1w).\nA necessary and sufficient condition [4] for (w, u) \u2208 S++ being a critical point of \u03a6 is the existence of \u03bbi with\n\u2207wj\u03a6(w, u) = \u03bbj\u03c8pw(wj) \u2200j \u2208 [K] and \u2207u\u03a6(w, u) = \u03bbK+1\u03c8pu(u). (6)\nNote that as wj \u2208 Rn1++ and the gradients are strictly positive in S++ the \u03bbi, i = 1, . . . ,K+ 1 have to be strictly positive. Noting that \u03c8p\u2032(\u03c8p(x)) = x, we get\n\u03c8p\u2032w ( \u2207wj\u03a6(w, u) ) = \u03bbp \u2032 w\u22121 j wj \u2200j \u2208 [K] and \u03c8p\u2032u ( \u2207u\u03a6(w, u) ) = \u03bbp \u2032 u\u22121 K+1 u. (7)\nIn particular, (w\u2217, u\u2217) is a critical point of \u03a6 in S++ if and only if it satisfies (7). Finally, note that( \u03c1w\u03c8p\u2032w(\u2207w1\u03a6(w, u))\n\u2016\u03c8p\u2032w ( \u2207w1\u03a6(w, u) ) \u2016pw , . . . , \u03c1w\u03c8p\u2032w\n( \u2207wK\u03a6(w, u) ) \u2016\u03c8p\u2032w ( \u2207wK\u03a6(w, u) ) \u2016pw , \u03c1u\u03c8p\u2032u ( \u2207u\u03a6(w, u) ) \u2016\u03c8p\u2032u ( \u2207u\u03a6(w, u) ) \u2016pu ) \u2208 S++,\nas the gradient is strictly positive on S++ and thus G\u03a6 : S++ \u2192 S++ defined in (3) is well-defined and if (w\u2217, u\u2217) is a critical point, then by (7) it holds G\u03a6(w\u2217, u\u2217) = (w\u2217, u\u2217). On the other hand if G\u03a6(w\u2217, u\u2217) = (w\u2217, u\u2217), then\n\u03c1w \u03c8p\u2032w\n( \u2207wj\u03a6(w\u2217, u\u2217) ) \u2016\u03c8p\u2032w ( \u2207wj\u03a6(w\u2217, u\u2217) ) \u2016pw = w\u2217j , j = 1, . . . ,K, \u03c1u \u03c8p\u2032u ( \u2207u\u03a6(w\u2217, u\u2217) ) \u2016\u03c8p\u2032u ( \u2207u\u03a6(w\u2217, u\u2217) ) \u2016pu = u\u2217\nand thus there exists \u03bbj = \u2016\u03c8p\u2032w (\u2207wj\u03a6(w \u2217,u\u2217))\u2016pw \u03c1w , j \u2208 [K] and \u03bbK+1 = \u2016\u03c8p\u2032u (\u2207u\u03a6(w \u2217,u\u2217))\u2016pu \u03c1u such that (7) holds and thus (w\u2217, u\u2217) is a critical point of \u03a6 in S++.\nOur goal is to apply the Banach fixed point theorem to G\u03a6 : B++ \u2192 S++ \u2282 B++. We recall this theorem for the convenience of the reader. Theorem 2 (Banach fixed point theorem e.g. [12]). Let (X, d) be a complete metric space with a mapping T : X \u2192 X such that d(T (x), T (y)) \u2264 q d(x, y) for q \u2208 [0, 1) and all x, y \u2208 X. Then T has a unique fixed-point x\u2217 in X, that is T (x\u2217) = x\u2217 and the sequence defined as xn+1 = T (xn) with x0 \u2208 X converges limn\u2192\u221e xn = x\u2217 with linear convergence rate\nd(xn, x\u2217) \u2264 q n\n1\u2212 q d(x 1, x0).\nSo, we need to endow B++ with a metric \u00b5 so that (B++, \u00b5) is a complete metric space. A popular metric for the study of nonlinear eigenvalue problems on the positive orthant is the so-called Thompson metric d : Rm++ \u00d7 Rm++ \u2192 R+ [18] defined as\nd(z, z\u0303) = \u2016 ln(z)\u2212 ln(z\u0303)\u2016\u221e where ln(z) = ( ln(z1), . . . , ln(zm) ) .\nUsing the known facts that (Rn++, d) is a complete metric space and its topology coincides with the norm topology (see e.g. Corollary 2.5.6 and Proposition 2.5.2 [14]), we prove:\nLemma 3. For p \u2208 (1,\u221e) and \u03c1 > 0, ({z \u2208 Rn++ | \u2016z\u2016p \u2264 \u03c1}, d) is a complete metric space.\nProof. Let (zk)k \u2282 {z \u2208 Rn++ | \u2016z\u2016p \u2264 \u03c1} be a Cauchy sequence w.r.t. to the metric d. We know from Proposition 2.5.2 in [14] that (Rn++, d) is a complete metric space and thus there exists z\u2217 \u2208 Rn++ such that zk converge to z\u2217 w.r.t. d. Corollary 2.5.6 in [14] implies that the topology of (Rn++, d) coincide with the norm topology implying that limk\u2192\u221e zk = z\u2217 w.r.t. the norm topology. Finally, since z 7\u2192 \u2016z\u2016p is a continuous function, we get \u2016z\u2217\u2016p = limk\u2192\u221e \u2016zk\u2016p \u2264 \u03c1, i.e. z\u2217 \u2208 {z \u2208 Rn++ | \u2016z\u2016p \u2264 1} which proves our claim.\nNow, the idea is to see B++ as a product of such metric spaces. For i = 1, . . . ,K, let Bi++ = {wi \u2208 R n1 ++ | \u2016wi\u2016pw \u2264 \u03c1w} and di(wi, w\u0303i) = \u03b3i\u2016 ln(wi) \u2212 ln(w\u0303i)\u2016\u221e for some constant \u03b3i > 0. Furthermore, let BK+1++ = {u \u2208 R n1\u00d7d ++ | \u2016u\u2016pu \u2264 \u03c1u} and dK+1(u, u\u0303) = \u03b3K+1\u2016 ln(u)\u2212 ln(u\u0303)\u2016\u221e. Then (Bi++, di) is a complete metric space for every i \u2208 [K + 1] and B++ = B1++ \u00d7 . . .\u00d7BK++ \u00d7BK+1++ . It follows that (B++, \u00b5) is a complete metric space with \u00b5 : B++ \u00d7B++ \u2192 R+ defined as\n\u00b5 ( (w, u), (w\u0303, u\u0303) ) = K\u2211 i=1 \u03b3i\u2016 ln(wi)\u2212 ln(w\u0303i)\u2016\u221e + \u03b3K+1\u2016 ln(u)\u2212 ln(u\u0303)\u2016\u221e.\nThe motivation for introducing the weights \u03b31, . . . , \u03b3K+1 > 0 is given by the next theorem. We provide a characterization of the Lipschitz constant of a mapping F : B++ \u2192 B++ with respect to \u00b5. Moreover, this Lipschitz constant can be minimized by a smart choice of \u03b3. For i \u2208 [K], a, j \u2208 [n1], b \u2208 [d], we write Fwi,j and Fuab to denote the components of F such that F = (Fw1,1 , . . . , Fw1,n1 , Fw2,1 , . . . , FwK,n1 , Fu11 , . . . , Fun1d).\nLemma 4. Suppose that F \u2208 C1(B++, V++) and A \u2208 R(K+1)\u00d7(K+1)+ satisfies\u2329 |\u2207wkFwi,j (w, u)|, wk \u232a \u2264 Ai,k Fwi,j (w, u), \u2329 |\u2207uFwi,j (w, u)|, u \u232a \u2264 Ai,K+1 Fwi,j (w, u)\nand \u3008|\u2207wkFuab(w, u)|, wk\u3009 \u2264 AK+1,k Fuab(w, u), \u3008|\u2207uFuab(w, u)|, u\u3009 \u2264 AK+1,K+1 Fuab(w, u)\nfor all i, k \u2208 [K], a, j \u2208 [n1], b \u2208 [d] and (w, u) \u2208 B++. Then, for every (w, u), (w\u0303, u\u0303) \u2208 B++ it holds\n\u00b5 ( F (w, u), F (w\u0303, u\u0303) ) \u2264 U \u00b5 ( (w, u), (w\u0303, u\u0303) ) with U = max\nk\u2208[K+1] (AT \u03b3)k \u03b3k .\nProof. Let i \u2208 [K] and j \u2208 [n1]. First, we show that for (w, u), (w\u0303, u\u0303) \u2208 B++, one has\u2223\u2223\u2223 ln (Fwi,j (w, u))\u2212 ln (Fwi,j (w\u0303, u\u0303))\u2223\u2223\u2223 \u2264 K\u2211 s=1 Ai,s\u2016 ln(ws)\u2212 ln(w\u0303s)\u2016\u221e+Ai,K+1\u2016 ln(u)\u2212 ln(u\u0303)\u2016\u221e.\nLet (w, u), (w\u0303, u\u0303) \u2208 B++. If (w, u) = (w\u0303, u\u0303), there is nothing to prove. So, suppose that (w, u) 6= (w\u0303, u\u0303). Let B\u0302 = {ln(v) | v \u2208 B++} \u2282 V and g : B\u0302 \u2192 R with\ng(w, u) = ln ( Fwi,j ( exp(w, u) )) where exp(w, u) = ( ew1,1 , . . . , ewK,n1 , eu11 , . . . , eun1d ) .\nNote that g is continuously differentiable because it is the composition of continuously differentiable mappings. Set (v, x) = ( ln(w), ln(u) ) and (y, z) = ( ln(w\u0303), ln(u\u0303) ) . By the\nmean value theorem, there exists t \u2208 (0, 1), such that (w\u0302, u\u0302) = t(v, x) + (1\u2212 t)(y, z) satisfies g(v, x)\u2212 g(y, z) = \u3008\u2207g(w\u0302, u\u0302), (v, x)\u2212 (y, z)\u3009 .\nLet (w, u) = exp(w\u0302, u\u0302). It follows that ln ( Fwi,j (w, u) ) \u2212 ln ( Fwi,j (w\u0303, u\u0303) ) = \u3008\u2207g(w\u0302, u\u0302), (v, x)\u2212 (y, z)\u3009\n= \u3008 ( \u2207Fwi,j (w, u) ) \u25e6 (w, u), (v, x)\u2212 (y, z) \u232a Fwi,j (w, u)\nwhere \u25e6 denotes the Hadamard product. Note that by convexity of the exponential and \u2016 \u00b7 \u2016p we have (w, u) \u2208 B++ as\n\u2016w\u2016pw = \u2016etv+(1\u2212t)y\u2016pw \u2264 \u2016tev + (1\u2212 t)ey\u2016pw \u2264 t\u2016ev\u2016pw + (1\u2212 t)\u2016ey\u2016pw = t\u2016w\u2016pw + (1\u2212 t)\u2016w\u0303\u2016pw \u2264 \u03c1w,\nwhere we have used in the second step convexity of the exponential and that the p-norms are order preserving, \u2016x\u2016p \u2264 \u2016y\u2016p if xi \u2264 zi for all i. A similar argument shows \u2016u\u2016pu \u2264 \u03c1u. Hence\u2223\u2223 ln (Fwi,j (w, u))\u2212 ln (Fwi,j (w\u0303, u\u0303))\u2223\u2223 \u2264\nK\u2211 k=1 \u2223\u2223\u3008(\u2207wkFwi,j (w, u)) \u25e6 wk, vk \u2212 yk\u232a\u2223\u2223 Fwi,j (w, u) + |\u3008 ( \u2207uFwi,j (w, u) ) \u25e6 u, x\u2212 z \u232a | Fwi,j (w, u)\n\u2264 K\u2211 k=1\n\u2016 ( \u2207wkFwi,j (w, u) ) \u25e6 wk\u20161\nFwi,j (w, u) \u2016vk \u2212 yk\u2016\u221e +\n\u2016 ( \u2207uFwi,j (w, u) ) \u25e6 u\u20161\nFwi,j (w, u) \u2016x\u2212 z\u2016\u221e\n= K\u2211 k=1 \u2329\u2223\u2223\u2207wkFwi,j (w, u)\u2223\u2223, wk\u232a Fwi,j (w, u) \u2016 ln(wk)\u2212 ln(w\u0303k)\u2016\u221e + \u2329\u2223\u2223\u2207uFwi,j (w, u)\u2223\u2223, u\u232a Fwi,j (w, u) \u2016 ln(u)\u2212 ln(u\u0303)\u2016\u221e\n\u2264 K\u2211 k=1 Ai,k\u2016 ln(wk)\u2212 ln(w\u0303k)\u2016\u221e +Ai,K+1\u2016 ln(u)\u2212 ln(u\u0303)\u2016\u221e.\nIn particular, taking the maximum over j \u2208 [n1] shows that for every i \u2208 [K] we have \u2016 ln ( Fwi(w, u) ) \u2212ln ( Fwi(w\u0303, u\u0303) ) \u2016\u221e \u2264 K\u2211 s=1 Ai,s\u2016 ln(ws)\u2212ln(w\u0303s)\u2016\u221e+Ai,K+1\u2016 ln(u)\u2212ln(u\u0303)\u2016\u221e.\nA similar argument shows that \u2016 ln ( Fu(w, u) ) \u2212 ln ( Fu(w\u0303, u\u0303) ) \u2016\u221e is upper bounded by\nK\u2211 s=1 AK+1,s\u2016 ln(ws)\u2212 ln(w\u0303s)\u2016\u221e +AK+1,K+1\u2016 ln(u)\u2212 ln(u\u0303)\u2016\u221e.\nSo, we finally get\n\u00b5 ( F (w, u), F (w\u0303, u\u0303) ) = K\u2211 i=1 \u03b3i\u2016 ln ( Fwi(w, u) ) \u2212 ln ( Fwi(w\u0303, u\u0303) ) \u2016\u221e\n+ \u03b3K+1\u2016 ln ( Fu(w, u) ) \u2212 ln ( Fu(w\u0303, u\u0303) ) \u2016\u221e\n\u2264 K\u2211 s=1 (AT \u03b3)s\u2016 ln(ws)\u2212 ln(w\u0303s)\u2016\u221e + (AT \u03b3)K+1\u2016 ln(u)\u2212 ln(u\u0303)\u2016\u221e\n\u2264 U\u00b5 ( (w, u), (w\u0303, u\u0303) ) .\nNote that, from the Collatz-Wielandt ratio for nonnegative matrices, we know that the constant U in Lemma 4 is lower bounded by the spectral radius \u03c1(A) of A. Indeed, by Theorem 8.1.31 in [10], we know that if AT has a positive eigenvector \u03b3 \u2208 RK+1++ , then\nmax i\u2208[K+1] (AT \u03b3)i \u03b3i = \u03c1(A) = min \u03b3\u0303\u2208RK+1++ max i\u2208[K+1] (AT \u03b3\u0303)i \u03b3\u0303i . (8)\nTherefore, in order to obtain the minimal Lipschitz constant U in Lemma 4, we choose the weights of the metric \u00b5 to be the components of \u03b3. A combination of Theorem 2, Lemma 4 and this observation implies the following result. Theorem 3. Let \u03a6 \u2208 C1(V,R) \u2229 C2(B++,R) with \u2207\u03a6(S+) \u2282 V++. Let G\u03a6 : B++ \u2192 B++ be defined as in (3). Suppose that there exists a matrix A \u2208 R(K+1)\u00d7(K+1)+ such that G\u03a6 and A satisfies the assumptions of Lemma 4 and AT has a positive eigenvector \u03b3 \u2208 RK+1++ . If\n\u03c1(A) < 1, then \u03a6 has a unique critical point (w\u2217, u\u2217) in S++ which is the global maximum of the optimization problem (2). Moreover, the sequence ( (wk, uk) ) k defined for any (w0, u0) \u2208 S++ as (wk+1, uk+1) = G\u03a6(wk, uk), k \u2208 N, satisfies limk\u2192\u221e(wk, uk) = (w\u2217, u\u2217) and\n\u2016(wk, uk)\u2212 (w\u2217, u\u2217)\u2016\u221e \u2264 \u03c1(A)k (\n\u00b5 ( (w1, u1), (w0, u0) )( 1\u2212 \u03c1(A) ) min {\u03b3K+1 \u03c1u ,mint\u2208[K] \u03b3t\u03c1w }) \u2200k \u2208 N,\nwhere the weights in the definition of \u00b5 are the entries of \u03b3.\nProof. As \u03b3 is a positive eigenvector of AT , by (8), we know that AT \u03b3 = \u03c1(A)\u03b3. It follows from Lemma 4 that \u00b5(G\u03a6(w, u), G\u03a6(w\u0303, u\u0303) ) < \u03c1(A)\u00b5 ( (w, u), (w\u0303, u\u0303) ) for every (w, u), (w\u0303, u\u0303) \u2208 B++, i.e. G\u03a6 is a strict contraction on the complete metric space (B++, \u00b5) and by the Banach fixed point theorem 2 we know that G\u03a6 has a unique fixed point (w\u2217, u\u2217) in S++. From Lemma 1 we know that \u03a6 attains its global maximum in S++ and, by Lemma 2, this maximum is a fixed point of G\u03a6. Hence, (w\u2217, u\u2217) is the unique global maximum of \u03a6 in S++. Finally, Theorem 2 implies that\n\u00b5 ( (wk, uk), (w\u2217, u\u2217) ) \u2264 \u03c1(A) k 1\u2212 \u03c1(A)\u00b5 ( (w1, u1), (w0, u0) ) \u2200k \u2208 N.\nThe mean value theorem implies that for every r \u2208 R, we have\n|es \u2212 et| \u2264 |s\u2212 t| max \u03be\u2208(\u2212\u221e,r] e\u03be = er|s\u2212 t| \u2200s, t \u2208 (\u2212\u221e, r].\nIn particular, we have\nln(wka,b), ln(w\u2217a,b) \u2208 (\u2212\u221e, ln(\u03c1w)] and ln(ukst), ln(u\u2217st) \u2208 (\u2212\u221e, ln(\u03c1u)].\nIt follows that\n\u00b5 ( (wk, uk), (w\u2217, u\u2217) ) = K\u2211 t=1 \u03b3t\u2016 ln(wkt )\u2212 ln(w\u2217t )\u2016\u221e + \u03b3K+1\u2016 ln(uk)\u2212 ln(u\u2217)\u2016\u221e\n\u2265 K\u2211 t=1 \u03b3t \u03c1w \u2016wkt \u2212 w\u2217t \u2016\u221e + \u03b3K+1 \u03c1u \u2016uk \u2212 u\u2217\u2016\u221e\n\u2265 max {\nmax t\u2208[K] \u03b3t \u03c1w \u2016wkt \u2212 w\u2217t \u2016\u221e, \u03b3K+1 \u03c1u \u2016uk \u2212 u\u2217\u2016\u221e } \u2265 min\n{\u03b3K+1 \u03c1u , min t\u2208[K] \u03b3t \u03c1w } \u2016(wk, uk)\u2212 (w\u2217, u\u2217)\u2016\u221e\nand thus \u2016(wk, uk)\u2212 (w\u2217, u\u2217)\u2016\u221e \u2264 \u00b5 ( (wk, uk), (w\u2217, u\u2217) ) min {\u03b3K+1 \u03c1u ,mint\u2208[K] \u03b3t\u03c1w }\n\u2264 \u03c1(A)k (\n\u00b5 ( (w1, u1), (w0, u0) ) (1\u2212 \u03c1(A)) min {\u03b3K+1 \u03c1u ,mint\u2208[K] \u03b3t\u03c1w })."}, {"heading": "4 Application to Neural Networks", "text": "In the previous sections we have outlined the proof of our main result for a general objective function satisfying certain properties. The purpose of this section is to prove that the properties hold for our optimization problem for neural networks. We recall our objective function from (2)\n\u03a6(w, u) = 1 n n\u2211 i=1 [ \u2212 L ( yi, f(w, u)(xi) ) + K\u2211 r=1 fr(w, u)(xi) ] + ( K\u2211 r=1 n1\u2211 l=1 wr,l + n1\u2211 l=1 d\u2211 m=1 ulm )\nand the function class we are considering from (1)\nfr(x) = fr(w, u)(x) = n1\u2211 l=1 wr,l ( d\u2211 m=1 ulmxm )\u03b1l ,\nThe arbitrarily small in the objective is needed to make the gradient strictly positive on the boundary of V+. We note that the assumption \u03b1i \u2265 1 for every i \u2208 [n1] is crucial in the following lemma in order to guarantee that \u2207\u03a6 is well defined on S+. Lemma 5. Let \u03a6 be defined as in (2), then \u2207\u03a6(w, u) is strictly positive for any (w, u) \u2208 S+.\nProof. One can compute\n\u2202\u03a6(w, u) \u2202wa,b = 1 n n\u2211 i=1 ( \u03b4yia \u2212 efa(x i)\u2211K j=1 e fj(xi) + 1 )( d\u2211 m=1 ubmx i m )\u03b1b + ,\nwhere \u03b4ij = {\n1 if i = j, 0 else . denotes the Kronecker delta, and\n\u2202\u03a6(w, u) \u2202uab = 1 n n\u2211 i=1 K\u2211 r=1 ( \u03b4yir \u2212 efr(x i)\u2211K j=1 e fj(xi) + 1 )( wr,a\u03b1a ( d\u2211 m=1 uamx i m )\u03b1a\u22121 xib ) + .\nNote that \u03b4yia \u2212\nefa(xi)\u2211K j=1 e fj(x) + 1 > 0,\nand thus using (w, u) \u2208 S+, we deduce that even without the additional , the gradient would be strictly positive on S++ assuming that there exists k \u2208 [n] such that xk 6= 0. However, at the boundary it can happen that the gradient is zero and thus an arbitrarily small is sufficent to guarantee that the gradient is strictly positive on S++.\nNext, we derive the matrix A \u2208 R(K+1)\u00d7(K+1) in order to apply Theorem 3 to G\u03a6 with \u03a6 defined in (2). As discussed in its proof, the matrix A given in the following theorem has a smaller spectral radius than that of Theorem 1. To express this matrix, we consider \u03a8\u03b1p,q : R n1 ++ \u00d7 R++ \u2192 R++ defined for p, q \u2208 (1,\u221e) and \u03b1 \u2208 R n1 ++ as\n\u03a8\u03b1p,q(\u03b4, t) = ([\u2211\nl\u2208J\n(\u03b4l t\u03b1l) p q q\u2212\u03b1p ]1\u2212\u03b1pq + max j\u2208Jc (\u03b4j t\u03b1j )p )1/p , (9)\nwhere J = {l \u2208 [n1] | \u03b1lp \u2264 q}, Jc = {l \u2208 [n1] | \u03b1lp > q} and \u03b1 = minl\u2208J \u03b1l. Theorem 4. Let \u03a6 be defined as above and G\u03a6 be as in (3). Set Cw = \u03c1w \u03a8\u03b1p\u2032w,pu(1, \u03c1u\u03c1x), Cu = \u03c1w \u03a8\u03b1p\u2032w,pu(\u03b1, \u03c1u\u03c1x) and \u03c1x = maxi\u2208[n] \u2016x i\u2016p\u2032u . Then A and G \u03a6 satisfy all assumptions of Lemma 4 with\nA = 2 diag ( p\u2032w \u2212 1, . . . , p\u2032w \u2212 1, p\u2032u \u2212 1 )(Qw,w Qw,u Qu,w Qu,u ) where Qw,w \u2208 RK\u00d7K++ , Qw,u \u2208 RK\u00d71++ , Qu,w \u2208 R1\u00d7K++ and Qu,u \u2208 R++ are defined as\nQw,w = 2Cw11T , Qw,u = (2Cu + \u2016\u03b1\u2016\u221e)1, Qu,w = (2Cw + 1)1T , Qu,u = (2Cu + \u2016\u03b1\u2016\u221e \u2212 1).\nIn the supplementary material, we prove that \u03a8\u03b1p,q(\u03b4, t) \u2264 \u2211n1 l=1 \u03b4lt\n\u03b1l which yields the weaker bounds \u03be1, \u03be2 given in Theorem 1. In particular, this observation combined with Theorems 3 and 4 implies Theorem 1.\nProof. We omit in the following the summation intervals as this is clear from the definition of the variables e.g. if w \u2208 RK\u00d7n1 then \u2211 a,b wa,b = \u2211K a=1 \u2211n1 b=1 wa,b. We split the proof in three steps so that we can reuse some of them in the proof of the similar theorem for two hidden layers.\nProposition 1. Suppose there exist Mi,j , Ci > 0, i, j \u2208 {w, u} such that for every x \u2208 {x1, . . . , xn} and r, s \u2208 [K], we have\n\u2211 t ws,t \u2202fr(x) \u2202ws,t \u2264 Cw, \u2211 a,b uab \u2202fr(x) \u2202uab\n\u2264 Cu,\u2211 t ws,t \u22022fr(x) \u2202ws,t\u2202wa,b \u2264Mw,w \u2202fr(x) \u2202wa,b , \u2211 a,b uab \u22022fr(x) \u2202uab\u2202ws,t \u2264Mw,u \u2202f(x) \u2202ws,t ,\n\u2211 t ws,t \u22022fr(x) \u2202ws,t\u2202uab \u2264Mu,w \u2202fr(x) \u2202uab , \u2211 a,b uab \u22022fr(x) \u2202uab\u2202ust \u2264Mu,u \u2202f(x) \u2202ust .\nThen A and F = G\u03a6 satisfy all assumptions of Lemma 4 for\nA = 2 diag ( p\u2032w \u2212 1, . . . , p\u2032w \u2212 1, p\u2032u \u2212 1 )(Qw,w Qw,u Qu,w Qu,u )\nwhere Qw,w \u2208 RK\u00d7K++ , Qw,u \u2208 RK\u00d71++ , Qu,w \u2208 R1\u00d7K++ , Qu,u \u2208 R++ are constant matrices given by\nQw,w = (2Cw +Mw,w)11T , Qw,u = (2Cu +Mw,u)1, Qu,w = (2Cw +Mu,w)1T , Qu,u = (2Cu +Mu,u).\nProof. In order to save space we make the proof w.r.t. to abstract variables g, h, that is g, h \u2208 {w1, . . . , wK , u}. First of all, note that we have\n\u2202G\u03a6ha \u2202gs = \u2202 \u2202gs \u03c1h\n( \u2202\u03a6 \u2202ha )p\u2032h\u22121 \u2016\u03c8p\u2032\nh\n( \u2207h\u03a6 ) \u2016ph = \u2202 \u2202gs \u03c1h\n( \u2202\u03a6 \u2202ha )p\u2032h\u22121 \u2016\u2207h\u03a6\u2016 p\u2032 h \u22121\np\u2032 h\n= (p\u2032h \u2212 1)\u03c1h\n[( \u2202\u03a6 \u2202ha )p\u2032h\u22122 \u22022\u03a6 \u2202gs\u2202ha\n\u2016\u2207h\u03a6\u2016 p\u2032 h \u22121\np\u2032 h\n\u2212\n( \u2202\u03a6 \u2202ha )p\u2032h\u22121 \u2016\u2207h\u03a6\u2016\u22121p\u2032\nh\n\u2211 c ( \u2202\u03a6 \u2202hc )p\u2032h\u22121 \u22022\u03a6 \u2202gs\u2202hc\n\u2016\u2207h\u03a6\u2016 2p\u2032 h \u22122\np\u2032 h\n]\n= [\n\u22022\u03a6 \u2202gs\u2202ha \u2202\u03a6 \u2202ha \u2212\n\u2211 c ( \u2202\u03a6 \u2202hc )p\u2032h\u22121 \u22022\u03a6 \u2202gs\u2202hc\n\u2016\u2207h\u03a6\u2016 p\u2032 h\np\u2032 h\n] (p\u2032h \u2212 1)G\u03a6ha ,\nwhere \u03c1h = \u03c1w if h \u2208 {w1, . . . , wK} and \u03c1 = \u03c1u if h = u. Thus,\n\u2211 s gs \u2223\u2223\u2223\u2202G\u03a6ha \u2202gs\n\u2223\u2223\u2223 = \u2211\ns\ngs \u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u22022\u03a6 \u2202gs\u2202ha \u2202\u03a6 \u2202ha \u2212 \u2211 c ( \u2202\u03a6 \u2202hc )p\u2032h\u22121 \u22022\u03a6 \u2202gs\u2202hc \u2016\u2207h\u03a6\u2016 p\u2032 h\np\u2032 h\n\u2223\u2223\u2223\u2223\u2223\u2223\u2223 (p\u2032h \u2212 1)G\u03a6ha .\nNow, suppose that there exists Rh,g > 0 such that\n\u2211 s gs \u2223\u2223\u2223\u2223 \u22022\u03a6\u2202gs\u2202ha \u2223\u2223\u2223\u2223 \u2264 Rh,g \u2202\u03a6\u2202ha , (10)\nthen we get\nAh,g = (p\u2032h \u2212 1) max a \u2211 s gs \u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u22022\u03a6 \u2202gs\u2202ha \u2202\u03a6 \u2202ha \u2212 \u2211 c ( \u2202\u03a6 \u2202hc )p\u2032h\u22121 \u22022\u03a6 \u2202gs\u2202hc \u2016\u2207h\u03a6\u2016 p\u2032 h\np\u2032 h \u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2264 (p\u2032 \u2212 1) max\na \u2211 s gs \u2223\u2223\u2223\u2223\u2223 \u22022\u03a6 \u2202gs\u2202ha \u2202\u03a6 \u2202ha \u2223\u2223\u2223\u2223\u2223+ (p\u2032h \u2212 1)\u2211 s gs \u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2211 c ( \u2202\u03a6 \u2202hc )p\u2032h\u22121 \u22022\u03a6 \u2202gs\u2202hc \u2016\u2207h\u03a6\u2016 p\u2032 h\np\u2032 h \u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2264 (p\u2032h \u2212 1) max\na \u2211 s gs \u2223\u2223\u2223\u2223\u2223 \u22022\u03a6 \u2202gs\u2202ha \u2202\u03a6 \u2202ha \u2223\u2223\u2223\u2223\u2223+ (p\u2032h \u2212 1) \u2211 c ( \u2202\u03a6 \u2202hc )p\u2032h\u22121\u2211 s gs \u2223\u2223\u2223 \u22022\u03a6\u2202gs\u2202hc \u2223\u2223\u2223 \u2016\u2207h\u03a6\u2016 p\u2032 h\np\u2032 h\n\u2264 (p\u2032h \u2212 1)Rh,g + (p\u2032h \u2212 1)Rh,g\n\u2211 c ( \u2202\u03a6 \u2202hc )p\u2032h \u2016\u2207h\u03a6\u2016 p\u2032 h\np\u2032 h\n= 2(p\u2032h \u2212 1)Rh,g.\nIt follows that, if we define Ah,g = 2(p\u2032h \u2212 1)Rh,g for all g, h \u2208 {w1, . . . , wK , u}, then A and G\u03a6 satisfy all assumptions of Lemma 4. In order to conclude the proof, we show that Rh,g = 2Cg +Mh,g. We compute the derivatives of the cross-entropy loss\n\u2212 \u2202L \u2202fr(x) = \u03b4yr \u2212 efr(x)\u2211K j=1 e fj(x) ,\n\u2212 \u2202 2L\n\u2202fq(x)\u2202fr(x) = \u2212\u03b4qr efr(x)\u2211K j=1 e fj(x) + e fr(x)efq(x)(\u2211K j=1 e fj(x) )2 ,\nWe get for the derivatives of the objective with respect to the abstract variables g, h\n\u2202\u03a6 \u2202gs = 1 n n\u2211 i=1 K\u2211 r=1 ( \u2212 \u2202L \u2202fr \u2223\u2223\u2223 f(xi) + 1 )\u2202fr \u2202gs \u2223\u2223\u2223 xi + (11)\nand\n\u2202\u03a6 \u2202gs\u2202ha = 1 n n\u2211 i=1 K\u2211 q,r=1 ( \u2212 \u2202 2L \u2202fq\u2202fr \u2223\u2223\u2223 f(xi) ) \u2202fr \u2202ha \u2223\u2223\u2223 xi \u2202fq \u2202gs \u2223\u2223\u2223 xi + 1 n n\u2211 i=1 K\u2211 r=1 ( \u2212 \u2202L \u2202fr \u2223\u2223\u2223 f(xi) + 1 ) \u22022fr \u2202gs\u2202ha \u2223\u2223\u2223 xi\nWe then can upper bound\n\u2211 s gs \u2223\u2223\u2223\u2223\u2223 K\u2211 q,r=1 ( \u2212 \u2202 2L \u2202fq\u2202fr \u2223\u2223\u2223 f(xi) ) \u2202fr \u2202ha \u2223\u2223\u2223 xi \u2202fq \u2202gs \u2223\u2223\u2223 xi \u2223\u2223\u2223\u2223\u2223 \u2264\nK\u2211 q,r=1 \u2223\u2223\u2223\u2223( \u22022L\u2202fq\u2202fr \u2223\u2223\u2223 f(xi) )\u2223\u2223\u2223\u2223 \u2202fr\u2202ha \u2223\u2223\u2223 xi \u2211 s gs \u2202fq \u2202gs \u2223\u2223\u2223 xi\n\u2264 max l=1,...,K \u2211 s gs \u2202fl \u2202gs \u2223\u2223\u2223 xi K\u2211 r=1 efr(x i)\u2211K j=1 e fj(xi) [\u2211 q 6=r efq(x i)\u2211K j=1 e fj(x) + ( 1\u2212 e fr(xi)\u2211K j=1 e fj(xi) )] \u2202fr \u2202ha \u2223\u2223\u2223 xi\n=2 max l=1,...,K \u2211 s gs \u2202fl \u2202gs \u2223\u2223\u2223 xi K\u2211 r=1 efr(x i)\u2211K j=1 e fj(xi) ( 1\u2212 e fr(xi)\u2211K j=1 e fj(xi) ) \u2202fr \u2202ha \u2223\u2223\u2223 xi\n\u22642 max l=1,...,K \u2211 s gs \u2202fl \u2202gs \u2223\u2223\u2223 xi K\u2211 r=1 ( 1\u2212 e fr(xi)\u2211K j=1 e fj(xi) + \u03b4yir ) \u2202fr \u2202ha \u2223\u2223\u2223 xi\nwhere we have used the non-negativity of the derivatives of the function class in the first step and \u2211\nq 6=r efq(x)\u2211K j=1 e fj(x) = 1\u2212 e fr(x)\u2211K j=1 e fj(x) .\nNow, note that\u2211 s gs 1 n n\u2211 i=1 K\u2211 r=1 ( \u2212 \u2202L \u2202fr \u2223\u2223\u2223 f(xi) + 1 ) \u22022fr \u2202ha\u2202gs \u2223\u2223\u2223 xi \u2264Mh,g 1 n n\u2211 i=1 K\u2211 r=1 ( \u2212 \u2202L \u2202fr \u2223\u2223\u2223 f(xi) + 1 ) \u2202fr \u2202ha \u2223\u2223\u2223 xi\n\u2264Mh,g \u2202\u03a6 \u2202ha .\nThis shows that\u2211 s gs \u2223\u2223\u2223\u2223 \u22022\u03a6\u2202gs\u2202ha \u2223\u2223\u2223\u2223\n\u22642 max k,l \u2211 s gs \u2202fl \u2202gs \u2223\u2223\u2223 xk 1 n n\u2211 i=1 K\u2211 r=1 ( 1\u2212 e fr(xi)\u2211K j=1 e fj(xi) + \u03b4yir ) \u2202fr \u2202ha \u2223\u2223\u2223 xi +Mh,g \u2202\u03a6 \u2202ha\n\u2264 [ 2 max\nk,l \u2211 s gs \u2202fl \u2202gs \u2223\u2223\u2223 xk +Mh,g ] \u2202\u03a6 \u2202ha \u2264 (2Cg +Mh,g) \u2202\u03a6 \u2202ha .\nThus, we get Rh,g \u2264 2Cg +Mh,g.\nThe following lemma is useful for the estimation of the bounds Cw, Cu in Proposition 1. Lemma 6. Let \u03b1 \u2208 Rr++, p, pu \u2208 [1,\u221e) and \u03a8\u03b1p,pu : R r ++ \u00d7 R++ \u2192 R++ defined as in (9). Then, for x \u2208 Rs+ and u \u2208 Rr\u00d7s++ satisfying \u2016u\u2016pu \u2264 \u03c1u, we have[\u2211 l \u03b4pl (\u2211 m ulmxm )p\u03b1l]1/p \u2264 \u03a8\u03b1p,pu(\u03b4, \u03c1u\u2016x\u2016p\u2032u).\nMoreover, if (\u03b4, t) \u2264 (\u03b4\u0303, t\u0303) then \u03a8\u03b1p,pu(\u03b4, t) \u2264 \u03a8 \u03b1 p,pu(\u03b4\u0303, t\u0303) \u2264 \u2211r l=1 \u03b4\u0303lt\u0303 \u03b1l .\nProof. Let J = {l \u2208 [r] | \u03b1lp < pu} and Jc = {l \u2208 [r] | \u03b1lp \u2265 pu}, we start with some observations. On the one hand, as \u2016u\u2016pu \u2264 \u03c1u, we have\u2211\nl\u2208Jc\n(\u2211 m u pu lm\n\u03c1puu )p\u03b1l/pu \u2264 \u2211 l\u2208Jc \u2211 m u pu lm \u03c1puu \u2264 \u2016u\u2016pupu \u03c1puu \u2264 1.\nOn the other hand, if \u03b1 = minl\u2208J \u03b1l, then p = pu\u03b1p > 1, p \u2032 = pupu\u2212\u03b1p , and\u2211\nl\u2208J\n(\u2211 m u pu lm\n\u03c1puu )p(p\u03b1l/pu) = \u2211 l\u2208J (\u2211 m u pu lm \u03c1puu )\u03b1l \u03b1 \u2264 \u2211 l\u2208J (\u2211 m u pu lm \u03c1puu ) \u2264 1.\nIt follows that\u2211 l \u03b4pl (\u2211 m ulmxm )p\u03b1l \u2264 \u2211 l (\u2211 m upulm ) p\u03b1l pu \u03b4pl \u2016x\u2016 p\u03b1l p\u2032u\n= \u2211 l\u2208J (\u2211 m u pu lm \u03c1puu )p\u03b1l/pu \u03b4pl (\u03c1u\u2016x\u2016p\u2032u) p\u03b1l + \u2211 l\u2208Jc (\u2211 m u pu lm \u03c1puu )p\u03b1l/pu \u03b4pl (\u03c1u\u2016x\u2016p\u2032u) p\u03b1l\n\u2264 \u2211 l\u2208J (\u2211 m u pu lm \u03c1puu )p\u03b1l/pu \u03b4pl (\u03c1u\u2016x\u2016p\u2032u) p\u03b1l + max j\u2208Jc \u03b4pj (\u03c1u\u2016x\u2016p\u2032u) p\u03b1j \u2211 l\u2208Jc (\u2211 m u pu lm \u03c1puu )p\u03b1l/pu \u2264 [\u2211 l\u2208J (\u2211 m u pu lm \u03c1puu )p(p\u03b1l/pu)]1/p[\u2211 l\u2208J (\u03b4pl (\u03c1u\u2016x\u2016p\u2032u) p\u03b1l)p \u2032 ]1/p\u2032 + max j\u2208Jc \u03b4pj (\u03c1u\u2016x\u2016p\u2032u) p\u03b1j\n\u2264 [\u2211 l\u2208J (\u03b4pl (\u03c1u\u2016x\u2016p\u2032u) p\u03b1l)p \u2032 ]1/p\u2032 + max j\u2208Jc \u03b4pj (\u03c1u\u2016x\u2016p\u2032u) p\u03b1j = [ \u03a8\u03b1p,pu(\u03b4, \u03c1u\u2016x\u2016p\u2032u) ]p\nIn particular, we see that\n\u03a8\u03b1p,pu(\u03b4, t) = ([\u2211\nl\u2208J\n(\u03b4l t\u03b1l)p p \u2032 ]1/p\u2032\n+ max j\u2208Jc\n(\u03b4j t\u03b1j )p )1/p .\nFinally, let (\u03b4, t) \u2264 (\u03b4\u0303, t\u0303). By monotonicity of \u2016\u00b7\u2016p and \u2016\u00b7\u2016p, we have \u03a8\u03b1p,pu(\u03b4, t) \u2264 \u03a8 \u03b1 p,pu(\u03b4\u0303, t\u0303). Now, using \u2016 \u00b7 \u2016p \u2264 \u2016 \u00b7 \u20161 and \u2016 \u00b7 \u2016p p\u2032 \u2264 \u2016 \u00b7 \u20161, we get\n\u03a8\u03b1p,pu(\u03b4, t) = (([\u2211\nl\u2208J\n(\u03b4l t\u03b1l)p p \u2032 ]1/(p p\u2032))p\n+ max j\u2208Jc\n(\u03b4j t\u03b1j )p )1/p\n\u2264 [\u2211 l\u2208J (\u03b4l t\u03b1l)p p \u2032 ]1/(p p\u2032) + max j\u2208Jc \u03b4j t \u03b1j \u2264 \u2211 l\u2208J \u03b4l t \u03b1l + max j\u2208Jc \u03b4j t \u03b1j\n\u2264 \u2211 l\u2208[r] \u03b4l t \u03b1l .\nFinally, using the Lemma above, we explicit the bounds of Proposition 1.\nLemma 7. Let f be defined as in (1) and let \u03c1x = maxi=1\u2208[n] \u2016xi\u2016p\u2032u , then the bounds in Proposition 1 are given by Cw = \u03c1w\u03a8\u03b1p\u2032w,pu(1, \u03c1u\u03c1x), Cu = \u03c1w\u03a8 \u03b1 p\u2032w,pu\n(\u03b1, \u03c1u\u03c1x), Mw,w = 0, Mw,u = \u2016\u03b1\u2016\u221e, Mu,w = 1 and Mu,u = \u2016\u03b1\u2016\u221e \u2212 1.\nProof. Let x \u2208 {x1, . . . , xn} and, for (w, u) \u2208 B++, we have\n\u2202fr(x) \u2202wa,b = \u03b4ra (\u2211 m ubmxm )\u03b1b ,\n\u2202fr(x) \u2202uab = wr,a\u03b1a (\u2211 m uamxm )\u03b1a\u22121 xb,\n\u22022fr(x) \u2202ws,t\u2202wa,b = 0,\n\u22022fr(x) \u2202ws,t\u2202uab = \u03b4rs\u03b4at\u03b1a (\u2211 m uamxm )\u03b1a\u22121 xb,\n\u22022fr(x) \u2202ust\u2202uab = \u03b4aswr,a\u03b1a(\u03b1a \u2212 1) (\u2211 m uamxm )\u03b1a\u22122 xbxt.\nWith Lemma 6, we have\n\u2211 b wa,b \u2202fr(x) \u2202wa,b = \u03b4ra \u2211 b wa,b (\u2211 m ubmxm )\u03b1b \u2264 \u03b4ra\u2016wa\u2016pw(\u2211 b (\u2211 m ubmxm )\u03b1bp\u2032w)1/p\u2032w\n\u2264 \u03b4ra\u2016wa\u2016pw\u03a8\u03b1p\u2032w,pu(1, \u03c1u\u2016x\u2016p\u2032u) \u2264 \u03c1w\u03a8 \u03b1 p\u2032w,pu (1, \u03c1u\u03c1x) = Cw,\nand \u2211 a,b uab \u2202fr(x) \u2202uab \u2264 \u2211 a wr,a\u03b1a (\u2211 m uamxm )\u03b1a \u2264 \u2016wr\u2016pw\u03a8\u03b1p\u2032w,pu(\u03b1, \u03c1u\u2016x\u2016p\u2032u)\n\u2264 \u03c1w\u03a8\u03b1p\u2032w,pu(\u03b1, \u03c1u\u03c1x) = Cu.\nFurthermore,\u2211 t ws,t \u22022fr(x) \u2202ws,t\u2202wa,b = 0 =\u21d2 Mw,w = 0\n\u2211 a,b uab \u22022fr(x) \u2202uab\u2202ws,t = \u03b4rs\u03b1t (\u2211 m utmxm )\u03b1t = \u03b1t \u2202f(x) \u2202ws,t \u2264 \u2016\u03b1\u2016\u221e \u2202f(x) \u2202ws,t\n=\u21d2 Mw,u = \u2016\u03b1\u2016\u221e\u2211 t ws,t \u22022fr(x) \u2202ws,t\u2202uab = \u03b4rsws,a\u03b1a (\u2211 m uamxm )\u03b1a\u22121 xb = \u03b4sr \u2202fr(x) \u2202uab \u2264 \u2202fr(x) \u2202uab\n=\u21d2 Mu,w = 1\u2211 a,b uab \u22022fr(x) \u2202ust\u2202uab = wr,s\u03b1s(\u03b1s \u2212 1) (\u2211 m usmxm )\u03b1s\u22121 xt = (\u03b1s \u2212 1) \u2202fr(x) \u2202ust\n\u2264 \u2016\u03b1\u2212 1\u2016\u221e \u2202fr(x) \u2202ust\n=\u21d2 Mu,u = \u2016\u03b1\u2212 1\u2016\u221e.\nFinally, as \u03b1i \u2265 1 for every i \u2208 [n1], we have \u2016\u03b1\u2212 1\u2016\u221e = \u2016\u03b1\u2016\u221e \u2212 1.\nCombining Lemma 7 and Proposition 1 concludes the proof. Finally, we note that the upper bound on \u03a8\u03b1p,q proved in Lemma 6 implies that the matrix A in Theorem 4 is componentwise smaller or equal than that of Theorem 1 and thus has a smaller spectral radius by Corollary 3.30 [3]."}, {"heading": "4.1 Neural networks with two hidden layers", "text": "We show how to extend our framework for neural networks with 2 hidden layers. In future work we will consider the general case. We briefly explain the major changes. Let n1, n2 \u2208 N and \u03b1 \u2208 Rn1++, \u03b2 \u2208 R n2 ++ with \u03b1i, \u03b2j \u2265 1 for all i \u2208 [n1], j \u2208 [n2], our function class is:\nfr(x) = fr(w, v, u)(x) = n2\u2211 l=1 wr,l ( n1\u2211 m=1 vlm ( d\u2211 s=1 umsxs )\u03b1m)\u03b2l and the optimization problem becomes\nmax (w,v,u)\u2208S+ \u03a6(w, v, u) where V+ = RK\u00d7n2+ \u00d7 R n2\u00d7n1 + \u00d7 R n1\u00d7d + , (12)\nS+ = {(w1, . . . , wK , v, u) \u2208 V+ | \u2016wi\u2016pw = \u03c1w, \u2016v\u2016pv = \u03c1v, \u2016u\u2016pu = \u03c1u} and\n\u03a6(w, v, u) = 1 n n\u2211 i=1 [ \u2212L ( yi, f(xi) ) + K\u2211 r=1 fr(xi) ] + ( K\u2211 r=1 n2\u2211 l=1 wr,l+ n2\u2211 l=1 n1\u2211 m=1 vlm+ n1\u2211 m=1 d\u2211 s=1 ums ) .\nThe map G\u03a6 : S++ \u2192 S++ = {z \u2208 S+ | z > 0}, G\u03a6 = (G\u03a6w1 , . . . , G \u03a6 wK , G \u03a6 v , G \u03a6 u ), becomes\nG\u03a6wi(w, v, u) = \u03c1w \u03c8p\u2032w(\u2207wi\u03a6(w, u))\n\u2016\u03c8p\u2032w(\u2207wi\u03a6(w, v, u))\u2016pw \u2200i \u2208 [K] (13)\nand\nG\u03a6v (w, v, u) = \u03c1v \u03c8p\u2032v (\u2207v\u03a6(w, v, u)) \u2016\u03c8p\u2032v (\u2207v\u03a6(w, v, u))\u2016pv , G\u03a6u (w, v, u) = \u03c1u \u03c8p\u2032u(\u2207u\u03a6(w, v, u)) \u2016\u03c8p\u2032u(\u2207u\u03a6(w, v, u))\u2016pu .\nWe have the following equivalent of Theorem 1 for 2 hidden layers. Theorem 5. Let {xi, yi}ni=1 \u2282 Rd+\u00d7 [K], pw, pv, pu \u2208 (1,\u221e), \u03c1w, \u03c1v, \u03c1u > 0, n1, n2 \u2208 N and \u03b1 \u2208 Rn1++, \u03b2 \u2208 R n2 ++ with \u03b1i, \u03b2j \u2265 1 for all i \u2208 [n1], j \u2208 [n2]. Let \u03c1x = maxi\u2208[n] \u2016xi\u2016p\u2032u ,\n\u03b8 = \u03c1v\u03a8\u03b1p\u2032v,pu(1, \u03c1u\u03c1x), Cw = \u03c1w\u03a8 \u03b2 p\u2032w,pv (1, \u03b8), Cv = \u03c1w\u03a8\u03b2p\u2032w,pv (\u03b2, \u03b8), Cu = \u2016\u03b1\u2016\u221eCv,\nand define A \u2208 R(K+2)\u00d7(K+2)++ as\nAm,l = 4(p\u2032w \u2212 1)Cw, Am,K+1 = 2(p\u2032w \u2212 1)(2Cv + \u2016\u03b2\u2016\u221e) Am,K+2 = 2(p\u2032w \u2212 1) ( 2Cu + \u2016\u03b1\u2016\u221e\u2016\u03b2\u2016\u221e ) , AK+1,l = 2(p\u2032v \u2212 1) ( 2Cw + 1 ) AK+1,K+1 = 2(p\u2032v \u2212 1) ( 2Cv + \u2016\u03b2\u2016\u221e \u2212 1 ) , AK+1,K+2 = 2(p\u2032v \u2212 1) ( 2Cu + \u2016\u03b1\u2016\u221e\u2016\u03b2\u2016\u221e\n) AK+2,l = 2(p\u2032u \u2212 1)(2Cw + 1), AK+2,K+1 = 2(p\u2032u \u2212 1)(2Cv + \u2016\u03b2\u2016\u221e),\nAK+2,K+2 = 2(p\u2032u \u2212 1)(2Cu + \u2016\u03b1\u2016\u221e\u2016\u03b2\u2016\u221e \u2212 1) \u2200m, l \u2208 [K].\nIf \u03c1(A) < 1, then (12) has a unique global maximizer (w\u2217, v\u2217, u\u2217) \u2208 S++. Moreover, for every (w0, v0, u0) \u2208 S++, there exists R > 0 such that\nlim k\u2192\u221e (wk, vk, uk) = (w\u2217, v\u2217, u\u2217) and \u2016(wk, vk, uk)\u2212(w\u2217, v\u2217, u\u2217)\u2016\u221e \u2264 R\u03c1(A)k \u2200k \u2208 N\nwhere (wk+1, vk+1, uk+1) = G\u03a6(wk, vk, uk) for every k \u2208 N and G\u03a6 is defined as in (13).\nProof. The proof of Theorem 3, can be extended by considering the weighted Thompson metric \u00b5 : V++ \u00d7 V++ \u2192 R+ defined as \u00b5 ( (w, v, u), (w\u0303, v\u0303, u\u0303) ) =\nK\u2211 i=1 \u03b3i\u2016 ln(wi)\u2212ln(w\u0303i)\u2016\u221e+\u03b3K+1\u2016 ln(v)\u2212ln(v\u0303)\u2016\u221e+\u03b3K+2\u2016 ln(u)\u2212ln(u\u0303)\u2016.\nIn particular, when \u03c1(A) < 1, the convergence rate becomes \u2016(wk, vk, uk)\u2212(w\u2217, v\u2217, u\u2217)\u2016\u221e \u2264 \u03c1(A)k ( \u00b5 ( (w1, v1, u1), (w0, v0, u0) )( 1\u2212 \u03c1(A) ) min {\u03b3K+2 \u03c1u , \u03b3K+1\u03c1v ,mint\u2208[K] \u03b3t \u03c1w }) \u2200k \u2208 N, where the weights in the definition of \u00b5 are the components of the positive eigenvector of AT . As AT \u2208 R(K+2)\u00d7(K+2)++ , this vector always exists by the Perron-Frobenius theorem (see for instance Theorem 8.4.4 in [10]). Proposition 1 generalizes straightforwardly. So, we need to compute the corresponding quantities Cg,Mg,h > 0 for g, h \u2208 {w1, . . . , wK , u, v}. To shorten notations, we write\n(ux)\u03b1 = ( (ux)\u03b111 , . . . , (ux) \u03b1n1 n1 ) and (v(ux)\u03b1)\u03b2 = ( (v(ux)\u03b1)\u03b211 , . . . , (v(ux)\u03b1) \u03b2n2 n2 ) ,\nwhere\n(ux)i = d\u2211 t=1 uitxt and (v(ux)\u03b1)j = n1\u2211 s=1 vjs(ux)\u03b1ss \u2200i \u2208 [n1], j \u2208 [n2].\nAgain, we omit the summation intervals in the following. First, we compute \u2202fr(x) \u2202wa,b = \u03b4ra(v(ux)\u03b1)\u03b2bb\n\u2202fr(x) \u2202vab = wr,a\u03b2a(v(ux)\u03b1)\u03b2a\u22121a (ux) \u03b1b b\n\u2202fr(x) \u2202uab = \u2211 j wr,j\u03b2j(v(ux)\u03b1) \u03b2j\u22121 j vja\u03b1a(ux) \u03b1a\u22121 a xb\nHence\u2211 b wa,b \u2202fr(x) \u2202wa,b = \u03b4ra \u2211 b wr,b(v(ux)\u03b1)\u03b2bb = \u03b4rafr(x)\n\u2211 a,b vab \u2202fr(x) \u2202vab = \u2211 a wr,a\u03b2a(v(ux)\u03b1)\u03b2a\u22121a \u2211 b vab(ux)\u03b1bb = \u2211 a \u03b2awr,a(v(ux)\u03b1)\u03b2aa\n\u2211 a,b uab \u2202fr(x) \u2202uab = \u2211 j wr,j\u03b2j(v(ux)\u03b1) \u03b2j\u22121 j \u2211 a vja\u03b1a(ux)\u03b1a\u22121a \u2211 b uabxb\n= \u2211 j \u03b2jwr,j(v(ux)\u03b1) \u03b2j\u22121 j \u2211 a \u03b1avja(ux)\u03b1aa .\nIt follows with Lemma 6 that, with \u03b8 = \u03c1v \u03a8\u03b1p\u2032v,pu(1, \u03c1u\u03c1x), we have\u2211 b wa,b \u2202fr(x) \u2202wa,b = \u03b4ra \u2211 b wa,b(v(ux)\u03b1)\u03b2bb \u2264 \u2016wa\u2016pw (\u2211 b (v(ux)\u03b1)\u03b2bp \u2032 w b )1/p\u2032w \u2264 \u2016wa\u2016pw\u03a8 \u03b2 p\u2032w,pv (1, \u03c1v\u2016(ux)\u03b1\u2016p\u2032v ) \u2264 \u2016wa\u2016pw\u03a8 \u03b2 p\u2032w,pv ( 1, \u03c1v \u03a8\u03b1p\u2032v,pu(1, \u03c1u\u2016x\u2016p\u2032u)\n) \u2264 \u03c1w\u03a8\u03b2p\u2032w,pv ( 1, \u03b8 )\n= Cw\u2211 a,b vab \u2202fr(x) \u2202vab = \u2211 a \u03b2awr,a(v(ux)\u03b1)\u03b2aa \u2264 \u2016wr\u2016pw (\u2211 a [ \u03b2a(v(ux)\u03b1)\u03b2aa ]p\u2032w)1/p\u2032w \u2264 \u2016wr\u2016pw\u03a8 \u03b2 p\u2032w,pv (\u03b2, \u03c1v\u2016(ux)\u03b1\u2016p\u2032v ) \u2264 \u03c1w\u03a8 \u03b2 p\u2032w,pv ( \u03b2, \u03b8 )\n= Cv\u2211 a,b uab \u2202fr(x) \u2202uab = \u2211 j \u03b2jwr,j(v(ux)\u03b1) \u03b2j\u22121 j \u2211 a \u03b1avja(ux)\u03b1aa \u2264 \u2016\u03b1\u2016\u221e \u2211 j \u03b2jwr,j(v(ux)\u03b1) \u03b2j j\n\u2264 \u2016\u03b1\u2016\u221eCv = Cu. Now, for the bound involving the second derivatives, we have\n\u22022fr(x) \u2202ws,t\u2202wa,b = 0 =\u21d2 Mw,w = 0,\nand \u22022fr(x) \u2202vst\u2202vab = \u03b4sawr,a\u03b2a(\u03b2a \u2212 1)(v(ux)\u03b1)\u03b2a\u22122a (ux) \u03b1b b (ux) \u03b1t t so that\u2211 s,t vst \u22022fr(x) \u2202vst\u2202vab = wr,a\u03b2a(\u03b2a \u2212 1)(v(ux)\u03b1)\u03b2a\u22122a (ux) \u03b1b b \u2211 t vat(ux)\u03b1tt\n= wr,a(\u03b2a \u2212 1)\u03b2a(v(ux)\u03b1)\u03b2a\u22121a (ux) \u03b1b b\n= (\u03b2a \u2212 1) \u2202fr(x) \u2202vab \u2264 \u2016\u03b2 \u2212 1\u2016\u221e \u2202fr(x) \u2202vab\n=\u21d2 Mv,v = \u2016\u03b2 \u2212 1\u2016\u221e.\nMoreover, we have \u22022fr(x) \u2202ust\u2202uab = \u2211 j wr,j\u03b2jvja\u03b1a(ux)\u03b1a\u22121a xb(\u03b2j \u2212 1)(v(ux)\u03b1) \u03b2j\u22122 j vjs\u03b1s(ux) \u03b1s\u22121 s xt\n+ \u03b4as \u2211 j wr,j\u03b2j(v(ux)\u03b1) \u03b2j\u22121 j vja\u03b1axb(\u03b1a \u2212 1)(ux) \u03b1a\u22122 a xt,\nthus\u2211 s,t ust \u22022fr(x) \u2202ust\u2202uab = \u2211 j wr,j\u03b2jvja\u03b1a(ux)\u03b1a\u22121a xb(\u03b2j \u2212 1)(v(ux)\u03b1) \u03b2j\u22122 j \u2211 s vjs\u03b1s(ux)\u03b1s\u22121s \u2211 t ustxt\n+ \u2211 j wr,j\u03b2j(v(ux)\u03b1) \u03b2j\u22121 j vja\u03b1axb(\u03b1a \u2212 1)(ux) \u03b1a\u22122 a \u2211 t uatxt\n= \u2211 j wr,j\u03b2jvja\u03b1a(ux)\u03b1a\u22121a xb(\u03b2j \u2212 1)(v(ux)\u03b1) \u03b2j\u22122 j \u2211 s vjs\u03b1s(ux)\u03b1ss\n+ \u2211 j wr,j\u03b2j(v(ux)\u03b1) \u03b2j\u22121 j vja\u03b1axb(\u03b1a \u2212 1)(ux) \u03b1a\u22121 a\n\u2264 \u2016\u03b1\u2016\u221e \u2211 j wr,j\u03b2jvja\u03b1a(ux)\u03b1a\u22121a xb(\u03b2j \u2212 1)(v(ux)\u03b1) \u03b2j\u22122 j \u2211 s vjs(ux)\u03b1ss\n+ \u2016\u03b1\u2212 1\u2016\u221e \u2211 j wr,j\u03b2j(v(ux)\u03b1) \u03b2j\u22121 j vja\u03b1axb(ux) \u03b1a\u22121 a\n\u2264 (\u2016\u03b1\u2016\u221e\u2016\u03b2 \u2212 1\u2016\u221e + \u2016\u03b1\u2212 1\u2016\u221e) \u2202fr(x) \u2202uab\n=\u21d2 Mu,u = \u2016\u03b1\u2016\u221e\u2016\u03b2 \u2212 1\u2016\u221e + \u2016\u03b1\u2212 1\u2016\u221e.\nAs \u03b1i, \u03b2j \u2265 1 for every i \u2208 [n1], j \u2208 [n2] by assumption, we have \u2016\u03b1\u2212 1\u2016\u221e = \u2016\u03b1\u2016\u221e \u2212 1 and \u2016\u03b2 \u2212 1\u2016\u221e = \u2016\u03b2\u2016\u221e \u2212 1 so that Mu,u = \u2016\u03b1\u2016\u221e\u2016\u03b2\u2016\u221e \u2212 1 and Mv,v = \u2016\u03b2\u2016\u221e \u2212 1. Now, we look at the mixed second derivatives. We have\n\u22022fr(x) \u2202vst\u2202wa,b = \u03b4ra\u03b4bs\u03b2s(v(ux)\u03b1)\u03b2s\u22121s (ux) \u03b1t t .\nIt follows that\u2211 b wa,b \u22022fr(x) \u2202vst\u2202wa,b = \u03b4rawa,s\u03b2s(v(ux)\u03b1)\u03b2s\u22121s (ux) \u03b1t t \u2264 wr,s\u03b2s(v(ux)\u03b1)\u03b2s\u22121s (ux) \u03b1t t = \u2202fr(x) \u2202vst\n=\u21d2 Mv,w = 1, and \u2211\ns,t\nvst \u22022fr(x) \u2202vst\u2202wa,b = \u03b4ra\u03b2b(v(ux)\u03b1)\u03b2b\u22121b \u2211 t vbt(ux)\u03b1tt = \u03b2b \u2202fr(x) \u2202wa,b \u2264 \u2016\u03b2\u2016\u221e \u2202fr(x) \u2202wa,b\n=\u21d2 Mw,v = \u2016\u03b2\u2016\u221e. Furthermore, it holds\n\u22022fr(x) \u2202ust\u2202wab = \u03b4ra\u03b2b(v(ux)\u03b1)\u03b2b\u22121b vbs\u03b1s(ux) \u03b1s\u22121 s xt,\nso that \u2211 b wa,b \u22022fr(x) \u2202wab\u2202ust = \u03b4ra \u2211 b wa,b\u03b2b(v(ux)\u03b1)\u03b2b\u22121b vbs\u03b1s(ux) \u03b1s\u22121 s xt\n\u2264 \u2211 b wr,b\u03b2b(v(ux)\u03b1)\u03b2b\u22121b vbs\u03b1s(ux) \u03b1s\u22121 s xt = \u2202fr(x) \u2202ust\n=\u21d2 Mu,w = 1, and \u2211\ns,t\nust \u22022fr(x) \u2202wab\u2202ust = \u03b4ra\u03b2b(v(ux)\u03b1)\u03b2b\u22121b \u2211 s vbs\u03b1s(ux)\u03b1s\u22121s \u2211 t ustxt\n= \u03b4ra\u03b2b(v(ux)\u03b1)\u03b2b\u22121b \u2211 s vbs\u03b1s(ux)\u03b1ss\n\u2264 \u2016\u03b1\u2016\u221e\u03b4ra\u03b2b(v(ux)\u03b1)\u03b2bb \u2264 \u2016\u03b1\u2016\u221e\u2016\u03b2\u2016\u221e \u2202fr(x) \u2202wa,b\n=\u21d2 Mw,u = \u2016\u03b1\u2016\u221e\u2016\u03b2\u2016\u221e. Finally, we have\n\u22022fr(x) \u2202ust\u2202vab = wr,a\u03b2a(ux)\u03b1bb (\u03b2a \u2212 1)(v(ux) \u03b1)\u03b2a\u22122a vas\u03b1s(ux)\u03b1s\u22121s xt\n+ \u03b4sbwr,a\u03b2a(v(ux)\u03b1)\u03b2a\u22121a \u03b1b(ux) \u03b1b\u22121 b xt.\nHence,\u2211 a,b vab \u22022fr(x) \u2202ust\u2202vab = \u2211 a wr,a\u03b2a(\u03b2a \u2212 1)(v(ux)\u03b1)\u03b2a\u22122a vas\u03b1s(ux)\u03b1s\u22121s xt \u2211 b vab(ux)\u03b1bb\n+ \u2211 a wr,a\u03b2a(v(ux)\u03b1)\u03b2a\u22121a vas\u03b1s(ux)\u03b1s\u22121s xt\n= \u2211 a wr,a\u03b2a(\u03b2a \u2212 1)(v(ux)\u03b1)\u03b2a\u22121a vas\u03b1s(ux)\u03b1s\u22121s xt + \u2202fr(x) \u2202ust\n\u2264 (\u2016\u03b2 \u2212 1\u2016\u221e + 1) \u2202fr(x) \u2202ust\n=\u21d2 Mu,v = \u2016\u03b2 \u2212 1\u2016\u221e + 1 = \u2016\u03b2\u2016\u221e,\nand \u2211 s,t ust \u22022fr(x) \u2202ust\u2202vab = wr,a\u03b2a(ux)\u03b1bb (\u03b2a \u2212 1)(v(ux) \u03b1)\u03b2a\u22122a \u2211 s vas\u03b1s(ux)\u03b1s\u22121s \u2211 t ustxt\n+ wr,a\u03b2a(v(ux)\u03b1)\u03b2a\u22121a \u03b1b(ux) \u03b1b\u22121 b \u2211 t ubtxt\n= wr,a\u03b2a(ux)\u03b1bb (\u03b2a \u2212 1)(v(ux) \u03b1)\u03b2a\u22122a \u2211 s vas\u03b1s(ux)\u03b1ss\n+ wr,a\u03b2a(v(ux)\u03b1)\u03b2a\u22121a \u03b1b(ux) \u03b1b b\n\u2264 \u2016\u03b1\u2016\u221ewr,a\u03b2a(ux)\u03b1bb (\u03b2a \u2212 1)(v(ux) \u03b1)\u03b2a\u22122a \u2211 s vas(ux)\u03b1ss\n+ \u2016\u03b1\u2016\u221ewr,a\u03b2a(v(ux)\u03b1)\u03b2a\u22121a (ux) \u03b1b b\n= \u2016\u03b1\u2016\u221ewr,a\u03b2a(ux)\u03b1bb (\u03b2a \u2212 1)(v(ux) \u03b1)\u03b2a\u22121a + \u2016\u03b1\u2016\u221e \u2202fr(x) \u2202vab\n\u2264 \u2016\u03b1\u2016\u221e(\u2016\u03b2 \u2212 1\u2016\u221e + 1) \u2202fr(x) \u2202vab\n=\u21d2 Mv,u = \u2016\u03b1\u2016\u221e(\u2016\u03b2 \u2212 1\u2016\u221e + 1) = \u2016\u03b1\u2016\u221e\u2016\u03b2\u2016\u221e.\nAs for the case with one hidden layer, for any fixed architecture \u03c1w, \u03c1v, \u03c1u > 0, n1, n2 \u2208 N and \u03b1 \u2208 Rn1++, \u03b2 \u2208 R n2 ++ with \u03b1i, \u03b2j \u2265 1 for all i \u2208 [n1], j \u2208 [n2], it is possible to derive lower bounds on pw, pv, pu that guarantee \u03c1(A) < 1 in Theorem 5. Indeed, it holds\nCw \u2264 \u03b61 = \u03c1w n2\u2211 j=1 [ \u03c1v n1\u2211 l=1 (\u03c1u\u03c1\u0303x)\u03b1l ]\u03b2j and Cv \u2264 \u03b62 = \u03c1w n2\u2211 j=1 \u03b2j [ \u03c1v n1\u2211 l=1 (\u03c1u\u03c1\u0303x)\u03b1l ]\u03b2j ,\nwith \u03c1\u0303x = maxi\u2208[n] \u2016xi\u20161. Hence, the two hidden layers equivalent of (4) becomes pw > 4(K+ 2)\u03b61 + 5, pv > 2(K+ 2) [ 2\u03b62 +\u2016\u03b2\u2016\u221e ] \u22121, pu > 2(K+ 2)\u2016\u03b1\u2016\u221e(2\u03b62 +\u2016\u03b2\u2016\u221e)\u22121.\n(14)"}, {"heading": "5 Experiments", "text": "10 0 10 1 10 2\nepochs\n10 -14\n10 -12\n10 -10\n10 -8\n(p \u2217 \u2212 f )/ |p\n\u2217 |\nNLSM1 SGD 100 SGD 10 SGD 1 SGD 0.1 SGD 0.01\n10 0\n10 1\n10 2\n10 3\nepochs\n0\n20\n40\n60\n80\n100\nte s t\ne rr\no r\nNLSM1 SGD 100 SGD 10 SGD 1 SGD 0.1 SGD 0.01\nFigure 2: Training score (left) w.r.t. the optimal score p\u2217 and test error (right) of NLSM1 and Batch-SGD with different step-sizes.\nTable 1: Test accuracy on UCI datasets\nDataset NLSM1 NLSM2 ReLU1 ReLU2 SVM\nCancer 96.4 96.4 95.7 93.6 95.7 Iris 90.0 96.7 100 93.3 100 Banknote 97.1 96.4 100 97.8 100 Blood 76.0 76.7 76.0 76.0 77.3 Haberman 75.4 75.4 70.5 72.1 72.1 Seeds 88.1 90.5 90.5 92.9 95.2 Pima 79.2 80.5 76.6 79.2 79.9\nThe shown experiments should be seen as a proof of concept. We do not have yet a good understanding of how one should pick the parameters of our model to achieve good performance. However, the other papers which have up to now discussed global optimality for neural networks [11, 8] have not included any results on real datasets. Thus, up to our knowledge, we show for the first time a globally optimal algorithm for neural networks that leads to non-trivial classification results. We test our methods on several low dimensional UCI datasets and denote our algorithms as NLSM1 (one hidden layer) and NLSM2 (two hidden layers). We choose the parameters of our model out of 100 randomly generated combinations of (n1, \u03b1, \u03c1w, \u03c1u) \u2208 [2, 20]\u00d7 [1, 4]\u00d7 (0, 1]2\nNonlinear Spectral Method for 1 hidden layer Input: Model n1 \u2208 N, pw, pu \u2208 (1,\u221e), \u03c1w, \u03c1u > 0, \u03b11, . . . , \u03b1n1 \u2265 1, > 0 so that the matrix A of Theorem 1 satisfies \u03c1(A) < 1. Accuracy \u03c4 > 0 and (w0, u0) \u2208 S++. 1 Let (w1, u1) = G\u03a6(w0, u0) and compute R as in Theorem 3 2 Repeat 3 (wk+1, uk+1) = G\u03a6(wk, uk) 4 k \u2190 k + 1 5 Until k \u2265 ln ( \u03c4/R ) / ln ( \u03c1(A)\n) Output: (wk, uk) fulfills \u2016(wk, uk)\u2212 (w\u2217, u\u2217)\u2016\u221e < \u03c4 . With G\u03a6 defined as in (3). The method for two hidden layers is similar: consider G\u03a6\nas in (13) instead of (3) and assume that the model satisfies Theorem 5.\n(respectively (n1, n2, \u03b1, \u03b2, \u03c1w, \u03c1v, \u03c1u) \u2208 [2, 10]2 \u00d7 [1, 4]2 \u00d7 (0, 1]2) and pick the best one based on 5-fold cross-validation error. We use Equation (4) (resp. Equation (14)) to choose pu, pw (resp. pu, pv, pw) so that every generated model satisfies the conditions of Theorem 1 (resp. Theorem 5), i.e. \u03c1(A) < 1. Thus, global optimality is guaranteed in all our experiments. For comparison, we use the nonlinear RBF-kernel SVM and implement two versions of the Rectified-Linear Unit network - one for one hidden layer networks (ReLU1) and one for two hidden layers networks (ReLU2). To train ReLU, we use a stochastic gradient descent method which minimizes the sum of logistic loss and L2 regularization term over weight matrices to avoid over-fitting. All parameters of each method are jointly cross validated. More precisely, for ReLU the number of hidden units takes values from 2 to 20, the step-sizes and regularizers are taken in {10\u22126, 10\u22125, . . . , 102} and {0, 10\u22124, 10\u22123, . . . , 104} respectively. For SVM, the hyperparameter C and the kernel parameter \u03b3 of the radius basis function K(xi, xj) = exp(\u2212\u03b3\u2016xi \u2212 xj\u20162) are taken from {2\u22125, 2\u22124 . . . , 220} and {2\u221215, 2\u221214 . . . , 23} respectively. Note that ReLUs allow negative weights while our models do not. The results presented in Table 1 show that overall our nonlinear spectral methods achieve slightly worse performance than kernel SVM while being competitive/slightly better than ReLU networks. Notably in case of Cancer, Haberman and Pima, NLSM2 outperforms all the other models. For Iris and Banknote, we note that without any constraints ReLU1 can easily find an architecture which achieves zero test error while this is difficult for our models as we impose constraints on the architecture in order to prove global optimality. We compare our algorithms with Batch-SGD in order to optimize (2) with batch-size being 5% of the training data while the step-size is fixed and selected between 10\u22122 and 102. At each iteration of our spectral method and each epoch of Batch-SGD, we compute the objective and test error of each method and show the results in Figure 2. One can see that our method is much faster than SGDs, and has a linear convergence rate. We noted in our experiments that as \u03b1 is large and our data lies between [0, 1], all units in the network tend to have small values that make the whole objective function relatively small. Thus, a relatively large change in (w, u) might cause only small changes in the objective function but performance may vary significantly as the distance is large in the parameter space. In other words, a small change in the objective may have been caused by a large change in the parameter space, and thus, largely influences the performance - which explains the behavior of SGDs in Figure 2. The magnitude of the entries of the matrix A in Theorems 1 and 5 grows with the number of hidden units and thus the spectral radius \u03c1(A) also increases with this number. As we expect that the number of required hidden units grows with the dimension of the datasets we have limited ourselves in the experiments to low-dimensional datasets. However, these bounds are likely not to be tight, so that there might be room for improvement in terms of dependency on the number of hidden units."}, {"heading": "Acknowledgment", "text": "The authors acknowledge support by the ERC starting grant NOLEPRO 307793."}], "references": [{"title": "Neural Network Learning: Theoretical Foundations", "author": ["M. Anthony", "P. Bartlett"], "venue": "Cambridge University Press, New York", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Provable bounds for learning some deep representations", "author": ["S. Arora", "A. Bhaskara", "R. Ge", "T. Ma"], "venue": "ICML", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonnegative Matrices in the Mathematical Sciences", "author": ["A. Berman", "R.J. Plemmons"], "venue": "SIAM, Philadelphia", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1994}, {"title": "Nonlinear Programming", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific, Belmont, Mass.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "The loss surfaces of multilayer networks", "author": ["A. Choromanska", "M. Hena", "M. Mathieu", "G.B. Arous", "Y. LeCun"], "venue": "AISTATS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "and Y", "author": ["A Daniely", "R. Frostigy"], "venue": "Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "The Perron-Frobenius Theorem for Multi-Homogeneous Maps", "author": ["A. Gautier", "F. Tudisco", "M. Hein"], "venue": "preparation", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Global optimality in tensor factorization, deep learning, and beyond", "author": ["B.D. Haeffele", "Rene Vidal"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Train faster", "author": ["M. Hardt", "B. Recht", "Y. Singer"], "venue": "generalize better: Stability of stochastic gradient descent. In ICML", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Matrix Analysis", "author": ["R.A. Horn", "C.R. Johnson"], "venue": "Cambridge University Press, New York, second edition", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "and A", "author": ["M. Janzamin", "H. Sedghi"], "venue": "Anandkumar. Beating the perils of non-convexity:guaranteed training of neural networks using tensor methods", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "An Introduction to Metric Spaces and Fixed Point Theory", "author": ["W.A. Kirk", "M.A. Khamsi"], "venue": "John Wiley, New York", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonlinear Perron-Frobenius theory", "author": ["B. Lemmens", "R.D. Nussbaum"], "venue": "Cambridge University Press, New York, general edition", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "On the computational efficiency of training neural networks", "author": ["R. Livni", "S. Shalev-Shwartz", "O. Shamir"], "venue": "NIPS, pages 855\u2013863", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Learning in Neural Networks: An Overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks, 61:85\u2013117", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Training a single sigmoidal neuron is hard", "author": ["J. Sima"], "venue": "Neural Computation, 14:2709\u20132728", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "On certain contraction mappings in a partially ordered vector space", "author": ["A.C. Thompson"], "venue": "Proceedings of the American Mathematical Society, 14:438\u2013443", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1963}], "referenceMentions": [{"referenceID": 12, "context": "1 Introduction Deep learning [13, 16] is currently the state of the art machine learning technique in many application areas such as computer vision or natural language processing.", "startOffset": 29, "endOffset": 37}, {"referenceID": 15, "context": "1 Introduction Deep learning [13, 16] is currently the state of the art machine learning technique in many application areas such as computer vision or natural language processing.", "startOffset": 29, "endOffset": 37}, {"referenceID": 0, "context": "[1], the understanding of the success of training deep neural networks is a currently very active research area [5, 6, 9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[1], the understanding of the success of training deep neural networks is a currently very active research area [5, 6, 9].", "startOffset": 112, "endOffset": 121}, {"referenceID": 5, "context": "[1], the understanding of the success of training deep neural networks is a currently very active research area [5, 6, 9].", "startOffset": 112, "endOffset": 121}, {"referenceID": 8, "context": "[1], the understanding of the success of training deep neural networks is a currently very active research area [5, 6, 9].", "startOffset": 112, "endOffset": 121}, {"referenceID": 16, "context": "In particular, the problem is even for a single hidden layer in general NP hard, see [17] and references therein.", "startOffset": 85, "endOffset": 89}, {"referenceID": 1, "context": "A recent line of research has directly tackled the optimization problem of neural networks and provided either certain guarantees [2, 15] in terms of the global optimum or proved directly convergence to the global optimum [8, 11].", "startOffset": 130, "endOffset": 137}, {"referenceID": 14, "context": "A recent line of research has directly tackled the optimization problem of neural networks and provided either certain guarantees [2, 15] in terms of the global optimum or proved directly convergence to the global optimum [8, 11].", "startOffset": 130, "endOffset": 137}, {"referenceID": 7, "context": "A recent line of research has directly tackled the optimization problem of neural networks and provided either certain guarantees [2, 15] in terms of the global optimum or proved directly convergence to the global optimum [8, 11].", "startOffset": 222, "endOffset": 229}, {"referenceID": 10, "context": "A recent line of research has directly tackled the optimization problem of neural networks and provided either certain guarantees [2, 15] in terms of the global optimum or proved directly convergence to the global optimum [8, 11].", "startOffset": 222, "endOffset": 229}, {"referenceID": 7, "context": "While providing a lot of interesting insights on the relationship of structured matrix factorization and training of neural networks, Haeffele and Vidal admit themselves in their paper [8] that their results are \u201cchallenging to apply in practice\u201d.", "startOffset": 185, "endOffset": 188}, {"referenceID": 10, "context": "[11] they use a tensor approach and propose a globally optimal algorithm for a feedforward neural network with one hidden layer and squared loss.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Polynomial neural networks have been recently analyzed in [15].", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "The reason to write this as a maximization problem is that our nonlinear spectral method is inspired by the theory of (sub)-homogeneous nonlinear eigenproblems on convex cones [14] which has its origin in the Perron-Frobenius theory for nonnegative matrices.", "startOffset": 176, "endOffset": 180}, {"referenceID": 6, "context": "In fact our work is motivated by the closely related Perron-Frobenius theory for multihomogeneous problems developed in [7].", "startOffset": 120, "endOffset": 123}, {"referenceID": 2, "context": "30 [3].", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "26 in [10]) that \u03c1(A) = \u03c1(A ) \u2264 maxi\u2208[K+1](A v)i/vi for any v \u2208 RK+1 ++ .", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "A necessary and sufficient condition [4] for (w, u) \u2208 S++ being a critical point of \u03a6 is the existence of \u03bbi with \u2207wj\u03a6(w, u) = \u03bbj\u03c8pw(wj) \u2200j \u2208 [K] and \u2207u\u03a6(w, u) = \u03bbK+1\u03c8pu(u).", "startOffset": 37, "endOffset": 40}, {"referenceID": 11, "context": "[12]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "A popular metric for the study of nonlinear eigenvalue problems on the positive orthant is the so-called Thompson metric d : R++ \u00d7 R++ \u2192 R+ [18] defined as d(z, z\u0303) = \u2016 ln(z)\u2212 ln(z\u0303)\u2016\u221e where ln(z) = ( ln(z1), .", "startOffset": 140, "endOffset": 144}, {"referenceID": 13, "context": "2 [14]), we prove:", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "2 in [14] that (R++, d) is a complete metric space and thus there exists z\u2217 \u2208 R++ such that z converge to z\u2217 w.", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "6 in [14] implies that the topology of (R++, d) coincide with the norm topology implying that limk\u2192\u221e z = z\u2217 w.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "31 in [10], we know that if A has a positive eigenvector \u03b3 \u2208 RK+1 ++ , then max i\u2208[K+1] (A \u03b3)i \u03b3i = \u03c1(A) = min \u03b3\u0303\u2208RK+1 ++ max i\u2208[K+1] (A \u03b3\u0303)i \u03b3\u0303i .", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "30 [3].", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "4 in [10]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "However, the other papers which have up to now discussed global optimality for neural networks [11, 8] have not included any results on real datasets.", "startOffset": 95, "endOffset": 102}, {"referenceID": 7, "context": "However, the other papers which have up to now discussed global optimality for neural networks [11, 8] have not included any results on real datasets.", "startOffset": 95, "endOffset": 102}, {"referenceID": 1, "context": "We choose the parameters of our model out of 100 randomly generated combinations of (n1, \u03b1, \u03c1w, \u03c1u) \u2208 [2, 20]\u00d7 [1, 4]\u00d7 (0, 1]2 18", "startOffset": 102, "endOffset": 109}, {"referenceID": 0, "context": "We choose the parameters of our model out of 100 randomly generated combinations of (n1, \u03b1, \u03c1w, \u03c1u) \u2208 [2, 20]\u00d7 [1, 4]\u00d7 (0, 1]2 18", "startOffset": 111, "endOffset": 117}, {"referenceID": 3, "context": "We choose the parameters of our model out of 100 randomly generated combinations of (n1, \u03b1, \u03c1w, \u03c1u) \u2208 [2, 20]\u00d7 [1, 4]\u00d7 (0, 1]2 18", "startOffset": 111, "endOffset": 117}, {"referenceID": 1, "context": "(respectively (n1, n2, \u03b1, \u03b2, \u03c1w, \u03c1v, \u03c1u) \u2208 [2, 10]2 \u00d7 [1, 4]2 \u00d7 (0, 1]2) and pick the best one based on 5-fold cross-validation error.", "startOffset": 43, "endOffset": 50}, {"referenceID": 9, "context": "(respectively (n1, n2, \u03b1, \u03b2, \u03c1w, \u03c1v, \u03c1u) \u2208 [2, 10]2 \u00d7 [1, 4]2 \u00d7 (0, 1]2) and pick the best one based on 5-fold cross-validation error.", "startOffset": 43, "endOffset": 50}, {"referenceID": 0, "context": "(respectively (n1, n2, \u03b1, \u03b2, \u03c1w, \u03c1v, \u03c1u) \u2208 [2, 10]2 \u00d7 [1, 4]2 \u00d7 (0, 1]2) and pick the best one based on 5-fold cross-validation error.", "startOffset": 54, "endOffset": 60}, {"referenceID": 3, "context": "(respectively (n1, n2, \u03b1, \u03b2, \u03c1w, \u03c1v, \u03c1u) \u2208 [2, 10]2 \u00d7 [1, 4]2 \u00d7 (0, 1]2) and pick the best one based on 5-fold cross-validation error.", "startOffset": 54, "endOffset": 60}, {"referenceID": 0, "context": "We noted in our experiments that as \u03b1 is large and our data lies between [0, 1], all units in the network tend to have small values that make the whole objective function relatively small.", "startOffset": 73, "endOffset": 79}], "year": 2016, "abstractText": "The optimization problem behind neural networks is highly non-convex. Training with stochastic gradient descent and variants requires careful parameter tuning and provides no guarantee to achieve the global optimum. In contrast we show under quite weak assumptions on the data that a particular class of feedforward neural networks can be trained globally optimal with a linear convergence rate with our nonlinear spectral method. Up to our knowledge this is the first practically feasible method which achieves such a guarantee. While the method can in principle be applied to deep networks, we restrict ourselves for simplicity in this paper to one and two hidden layer networks. Our experiments confirm that these models are rich enough to achieve good performance on a series of real-world datasets.", "creator": "LaTeX with hyperref package"}}}