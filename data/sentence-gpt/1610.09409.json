{"id": "1610.09409", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2016", "title": "Probabilistic Model Checking for Complex Cognitive Tasks -- A case study in human-robot interaction", "abstract": "This paper proposes to use probabilistic model checking to synthesize optimal robot policies in multi-tasking autonomous systems that are subject to human-robot interaction. Given the convincing empirical evidence that human behavior can be related to reinforcement models, we take as input a well-studied Q-table model of the human behavior for flexible scenarios. We first describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary but fixed scenario. We describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary but fixed scenario. We describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary but fixed scenario. We describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary but fixed scenario. We describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary but fixed scenario. We describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary but fixed scenario. We describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary but fixed scenario. We describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary but fixed scenario. We describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary and fixed scenario. We describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary but fixed scenario. We describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary but fixed scenario. We describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary and fixed scenario. We describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary and fixed scenario. We describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary and fixed scenario. We describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary and fixed scenario. We describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary and fixed scenario. We describe an automated procedure to distill a Mark", "histories": [["v1", "Fri, 28 Oct 2016 21:37:14 GMT  (2429kb,D)", "http://arxiv.org/abs/1610.09409v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.RO", "authors": ["sebastian junges", "nils jansen", "joost-pieter katoen", "ufuk topcu"], "accepted": false, "id": "1610.09409"}, "pdf": {"name": "1610.09409.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Model Checking for Complex Cognitive Tasks", "authors": ["Sebastian Junges", "Nils Jansen", "Joost-Pieter Katoen", "Ufuk Topcu"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Verification and design for autonomous systems that work with humans account for the human\u2019s capabilities, preferences and limitations by embedding behavioural models of humans. With increasing capabilities to monitor humans in dynamic, possibly mixed-reality environments, datadriven modeling enables to encode such data into behavioural models. Moreover, reinforcement learning [1] (RL) sufficiently describes quantitative aspects of human behaviour when solving complicated tasks in realistic environments [2,3]. Basically, RL comprises algorithms addressing the optimal control problem through learning, i.e., an agent\u2014the human\u2014learns how to solve a task based on repeated interaction with an environment. So\u2013called Q-tables store quantitative information about possible choices of the human. We consider this information the data describing human behaviour.\nConsider the visio-motor setting from [4] in Fig. 1. A human walks down a sidewalk and shall attend to three modular tasks: avoiding obstacles (purple), approaching targets (blue), or following the walkway (grey). RL\nar X\niv :1\n61 0.\n09 40\n9v 1\n[ cs\n.A I]\n2 8\nO ct\n484 Biol Cybern (2013) 107:477\u2013490 where |Sd | is the dimensionality of the distance state space. See Fig. 3 for a depiction of this discretization of the state space. Heading angles are \u221250 \u2264 \u03b8 \u2264 50 degrees, with linear spacing. The parametrization used for the distance to the walkway is: \u2308|Sd |(0.5 + 0.5(1 \u2212 (2\u22120.5\u2217|\u03c1)|)))sign(\u03c1))\u2309 where \u03c1 is the signed distance from the center line of the walkway. Given these representations, data can be collected for a given walk on the sidewalk for each of the three modules. For the current study, this results in a total of 121 states for each of the three component tasks giving a total of 363 states. Note that the full joint state space would give a total of more than 1.7 million states. Accordingly, the Q-functions have 605 state\u2013action pairs, resulting in a total of 1815 state\u2013 action pairs for the four component tasks, compared with over 8 million state\u2013action pairs for the joint Q-function. Accordingly, we are not aware of a current IRL method capable of inferring the full joint reward function.\nFirst, it is necessary to determine how expressive this model is, i.e., what range of different behaviors can be obtained. Different values of reward for the modules lead to radically different behaviors. For example, Fig. 2 shows the different avatar trajectories for three different reward sets. In the topmost traces, the avatar is rewarded for all three component tasks, such that obstacles are avoided, targets are approached, and the path is maintained on the walkway. In the second case, the avatar is only rewarded for approaching targets, resulting in trajectories that leave the walkway and also walk through obstacles. Finally, in the third case,\nthe avatar is only rewarded for avoiding obstacles, and he accordingly wanders off to the corners of the defined area. Note that although the trajectories in this latter case show a high degree of variability, they are all obtained by a single set of task weights. This demonstrates the importance of a computational model of visuomotor behavior that does not average over individual trajectories but accommodates a probabilistic relationship between behavioral goals and observed behavior. 6 Experimental results\nThe first and essential question to ask is: how well can the algorithm recover known rewards? To answer this question, we choose reward sets and have the avatar learn MDP modules. The important assumption here is that the learning avatar is allowed to have the state space of the avatar generating the data. This assumption may seem quite restrictive, but it is the common assumption in IRL (including Ng and Russell 2000; Ramachandran and Amir 2007; Lopes et al. 2009). Furthermore, for basic ambulatory behaviors, the agents in question operate under very similar constraints owing to common physical environments and movement systems. The experimental protocol for each individual episode has multiple steps:\n1. Chosen rewards are used by the avatar to learn module MDP policies using Eqs. 2 and 3. 2. Runs using the learned policy are made to generate trajectories for the data sequences.\nFig. 2 Comparison of trajectories of the agent with different sets of task weights. a Target approach, obstacle avoidance, and walkway navigation are weighted as (0.5, 0.3, 0.2). b With only incentive to pick up targets, the avatar wonders off the walkway and hits obstacles. When targets are valuable, a large number of targets (pink) are collected. c With reward only being given for avoiding obstacles, there is neither an incentive to stay on the walkway nor to pick up targets (color figure online)\n123\nFig. 1. Visio-motor setting with different tasks while walking down a sidewalk shown in a simulation environment. The state space is divided into the modular tasks (i) approach targets (purple), (ii) avoid obstacles (blue), and (iii) follow walkway (grey). This picture is taken from [4] with permission from Ballard.\ngenerates Q-tables for the individual tasks; each table quantifies available choices that ultimately lead to completing the task. To build an accurate and general model of observed human behaviour for different tasks, inverse reinforcement learning (IRL) [5] assigns weights describing preferences over these tasks. It might, e. g., be more important to avoid an obstacle than to approach a target. In particular, IRL connects data sets about human behaviour\u2014either observed over time or obtained by RL\u2014to a general model describing how a human typically behaves in presence of different tasks. A large class of human behavioural models is covered by a set of Q\u2013tables together with weights obtained by the methods in [4]. The given tables describe behaviour for generic scenarios in the sense that they take into account distances to features such as obstacles or targets instead of their concrete position.\nThis paper proposes probabilistic model checking [6] to analyse human behaviour models described by weighted Q\u2013tables3 For an arbitrary concrete scenario, a Markov decision process (MDP) [8] is generated automatically, see Fig. 2. Intuitively, this MDP describes all possible human behaviours for this concrete scenario based on the behavioural model. The distinctive issue is that\u2014in contrast to existing models\u2014 under-specification of the human behaviour is included. We assess the performance of the human for the scenario as well as properties of the human model itself by employing MDP model checking as supported by PRISM [9], StORM, and iscasMc [10]. We then assess robot behaviour in the presence of possibly disturbing humans. This is done by combining the human model with a robot-MDP model. The joint human-robot interaction model is a stochastic two-player game (SG) [11]. We synthesise optimal policies for the robot under the human behaviour using SG model-checking with PRISM-Games [12].\n3 A high-level conceptual view in the form of an extended abstract is given in [7]. All technical content, implementation details, experiments and lessons learned are novel.\nhuman behaviour (set of Q\u2013tables)\nweights\nRL\nIRL\ninput data from [4]\nobservations of human behaviour\nMDP\nFig. 3. Graphical representation of our gridworlds areas Gh, Gr \u2713 loc. Locations are given by\nloc = {(x, y) | x 2 [0,Gridx] y 2 [0,Gridy]}\nfor Gridx,Gridy 2 N and the features are a set of tuples Feattp \u2713 {tp}\u21e5loc. A feature f = (tpf , `f ) 2 Feat consists of a type and a(feature-)location. [NJ] What is tpf? [NJ] Perhaps Loc for the set of locations and type is a set Tp := {Obst, Litt, Wpt} which can be defined before the definition. Example 1. Consider the example depicted in Fig. 3, which we use as a running example. The environment depicted is formally given as Env = {loc, Feat}\n[NJ] Envshould be a tuple as defined, right? Goal areas missing.\nwith\nloc = {(x, y) | x 2 [0, 4] y 2 [0, 5]}, and\nFeat = {fi = (Wpt, (2, i)) | i 2 {0 . . . 5} } [ {f6 = (Obst, (1, 1)), f7 = (Obst, (3, 3))} [ {f8 = (Litt, (1, 3)), f9 = (Litt, (4, 3))}\nHuman. The human is represented by its position which is a tuple of a location and orientation posh = (`h,\u21b5h). An orientation has 8 possible directions, i.e. \u21b5h 2 Orient = {i \u00b7 14\u21e1 | i 2 [0, 7]}. As an auxiliary we define for each direction an associated direction vector Dir : Orient ! { 1, 0, 1}2 \\ {(0, 0)}, which we depict in Fig. 4(a). Human movements Mh = {LEFT, STRAIGHT, RIGHT} have associated changes in angle of = 14\u21e1, 0, or 14\u21e1. We depict the movement options in Fig. 4(b).\n[NJ] Do we really need the Orient-definition, or would the direction suffice?\n7\nscenario\nstochastic game\n0.1 1 2 0\n0.2\n0.4\n0.6\n0.8\n1\nTemperature\nP ro\nb\n20 40 60 0\n0.2\n0.4\n0.6\n0.8\n1\nSteps\nP ro\nb\nPmin Pmax Pmin30 Pmax30\nPmin/0.05 Pmax/0.05 Pmin/0.5 Pmax/0.5\nDetailed analysis is possible by considering the schedulers \u2013 showing where underspecification is having a large e\u21b5ect, and counterexamples \u2013 uncovering the most relevant subset of the reachable state-space.\nWe also analyse the stochastic game, for which we use Prism-Games v2.0beta3. Notice that only the explicit engine is supported. The state space is a rough grid-size times robot directions (4) times turn-flag (2) larger.\n5 Conclusion and discussion We have successfully translated a cognitive model into a formal setting and used it to compute control plans for robots moving in the presence of humans handling complex tasks. In the paper, we discussed the model as well as several (open) challenges we stumbled upon.\n5.1 Discussion of the model The FOLLOW objective in [] has been given by a line, from which the human should not diverge to far. Current experiments (on which our data is based) use a waypoint description, where a set of waypoints is given which one should pass. The latter is said to be more flexible, especially if it isn\u2019t a straight line. The following a line approach, however, has a notion of progress as any diverging of the line gives the human a negative reward. Such a notion is lacking in the waypoint formulation that does not penalise walking in circles, as it only gives positive reward to actually visiting the waypoints. As the Q-tables also do not take into account the border they are not avoiding deadlock actively. Besides, the Q-tables contain some unexpected outliers, which in particular configurations lead to unexpected behaviour.\n16\nhuman performance robot\nstrategy\nrobot behaviour\nmodelchecking\nmodelchecking\nWe stress that our approach is applicable to any human behaviour models described by weighted Q\u2013tables. The approach is evaluated on an existing model by Rothkopf et al. for visio-motor ta ks [13,4,14]. The main bottleneck is to handle the textual MDP description of the human behaviour of over 80,000 lines of PRISM-code. We thoroughly analysed MDPs of 106 states induced by a 20\u00d720 grid scenario. The human-interaction model for a 8\u00d78 grid scenario constitute a SG with 1.6\u00b7107 states, its generation\u2014in absence of a symbolic e gine [15,16] for SGs\u2014takes over twenty hours; analysing maxmi re chability pr babili ies akes three hours. The SGs have a noticeably more complex structure than benchmarks in [17] and offer new challenges to probabilistic verification."}, {"heading": "2 Preliminaries", "text": "In this section, we give a short introduction to models, specifications, and our notations; for details we refer to [18, Ch. 10].\nDefinition 1 (Probabilistic models). A stochastic game (SG) is a tuple M = (S, sI ,Act ,P) with a finite set S of states such that S = S\u25e6]S2, an initial state sI \u2208 S, a finite set Act of actions, and a transition function P : S \u00d7Act \u00d7 S \u2192 [0, 1] and \u2211s\u2032\u2208S P(s, \u03b1, s\u2032) \u2208 {0, 1} \u2200s \u2208 S, a \u2208 Act. \u2013 M is a Markov decision process (MDP) if S\u25e6 = \u2205 or S2 = \u2205. \u2013 MDP M is a Markov chain (MC) if |Act(s)| = 1 for all s \u2208 S.\nWe refer to MDPs by M. SGs are two-player stochastic games with players \u25e6 and 2 having states in S\u25e6 and S2, respectively. Players nondeterministically choose an action at each state; successors are determined probabilistically according to transition probabilities. MDPs and MCs are one- and zero-player stochastic games, respectively. As MCs have one action at each state, we omit this action and write P(s, s\u2032). For analysis, w.l.o.g. we assume that in each state there is at least one action available.\nProbabilistic models are extended with rewards (or costs) by adding a reward function rew: S \u2192 R+ which assigns rewards to states of the model. Intuitively, the reward rew(s) is earned upon leaving the state s. Nondeterministic choices of actions in SGs and MDPs are resolved schedulers ; here it suffices to consider memoryless deterministic schedulers [19]. Resolving all nondeterminism in SGs or MDPs yields induced Markov chains; note that for SG we need individual schedulers for each player.\nAs specifications we consider reachability and expected reward properties. A reachability property asserts that a set T \u2286 S of target states is to be reached from the initial state with probability at most \u03bb \u2208 [0, 1], denoted P\u2264\u03bb(\u2666T ). Analogously, an expected reward property bounds the expected reward of reaching T by a threshold \u03ba \u2208 R, denoted E\u2264\u03ba(\u2666T ).\nFor MDPs and SGs, properties have to hold for all possible schedulers on the corresponding induced MCs. Verification can be performed by computing maximal (or minimal) probabilities or expected rewards to reach target states using standard techniques, such as linear programming, value iteration, or policy iteration [8]."}, {"heading": "3 Description of the cognitive model", "text": "This section describes the scenario of the cognitive model used as case study. It also define a formalisation that can be applied to similar case studies. This provides the basis to obtain the underlying representation as an MDP and\u2014incorporating robot movement\u2014an SG.\nGeneral scenario. We consider a scenario involving a human agent going over a sidewalk encountering objects like obstacles and litter, see Fig. 1. The human is given three modular objectives: while FOLLOW a sidewalk (represented as line) to get to the other side, she should AVOID walking into obstacles and aim to COLLECT litter. The line to follow is abstracted as a set of waypoints. Waypoints and objects such as obstacles and litter are called features of the scenario. As obstacles can be run over, litter can be collected and waypoints can be visited, these features are either being present or disappeared. For analysis purposes, we mark specific regions of\nthe environment as goal-areas. We assume that all features are initially present and that disappeared features remain absent. A toy-scenario is given in Fig. 3(b). The general scenario is abstracted to a discrete state model to allow to characterise human behaviour in the model. It consists of two parts: a (mostly) static environment and the dynamic human. The behaviour is determined by the movement-values assigned to each movement. These values are then translated into a probability distribution over the movements4."}, {"heading": "3.1 Formal model", "text": "Let us describe how the human interacts with its environment. The environment consists of a two-dimensional grid and its features, where features have a type in Tp = {Obst,Litt,Wpt}. Definition 2 (Environment). An environment Env = (Loc,Feat) consists of a finite set of locations Loc with\nLoc = {(x, y) | x \u2208 [0,Gridx] y \u2208 [0,Gridy]} for Gridx,Gridy \u2208 N,\nand a set of features Feat \u2286 Tp \u00d7 Loc. A feature f = (tpf , `f ) \u2208 Feat consists of a type and a (feature-)location.\nFeatures are partitioned according to their type: Feat = FeatObst\u222aFeatLitt\u222a FeatWpt, such that Feattp \u2286 {tp} \u00d7 Loc. Example 1. Consider our running example in Fig. 3(b). The environment is given as Env = (Loc,Feat) with Loc = {(x, y) | x \u2208 [0, 4] y \u2208 [0, 4]}, and\nFeat = {fi = (Wpt, (2, i)) | i \u2208 {1 . . . 4} } 4 We use the term movement to avoid confusion with actions in MDPs.\n\u222a {f5 = (Obst, (3, 3)), f6 = (Obst, (0, 2)), f7 = (Obst, (0, 4))} \u222a {f8 = (Litt, (1, 3)), f9 = (Litt, (4, 3))}\nHuman. The human h is represented by its position posh = (`h, \u03b1h) which is a pair of a location `h and an orientation \u03b1h. An orientation has 8 possible directions, i.e. \u03b1h \u2208 Orient = {i \u00b7 14\u03c0 | i \u2208 [0, 7]}. We assume that the human starts in position inith.\nHuman movement. For each direction, let the associated direction vector Dir : Orient \u2192 {\u22121, 0, 1}2 \\ {(0, 0)}, see Fig. 4(a). Human movements Mh = {LEFT,STRAIGHT,RIGHT}, see Fig. 4(b), have associated changes in angle of \u03b2 = \u221214\u03c0, 0, or 14\u03c0. To be precise, the post-position postm(posh) for human position posh = (`h, \u03b1h) and movement m with \u03b2m, is given by:\npostm(posh) = (`h + Dir(\u03b1h + \u03b2m), (\u03b1h + \u03b2m) mod 2\u03c0).\nA movement m is valid in posh if postm(posh) is a well-defined position, that is, the location of the post-position is on the grid.\nSituations. The status of a scenario is called a situation:\nDefinition 3 (Situation). Let Env = (Loc,Feat) be an environment. A situation s is a pair of a human location and the present features (posh,PFeat) \u2208 Loc\u00d7 2Feat. As we see in Sect. 3.2, the situations define the state space of our model. If the post-position of a movement coincides with a feature, then the feature is marked disappeared. Formally, for a valid movement m in position posh, the present features PFeat are updated as follows:\nfum(s) = PFeat \\ {(tp, postm(posh))} \u2286 Feat.\nThe effect of a movement of the human is as follows.\nDefinition 4 (Human movement effect). The effect of a human movement m in a situation s = (posh,PFeat) is the situation\neffm(s) = (postm(posh), fum(s)).\nDistance and angle. Let f be a feature and posh = (`h, \u03b1h) the position of the human h. The distance between h and f is given by the Euclidean norm, i.e. dh(f) = \u2016`h \u2212 `\u20162, and \u03b3h(f) denotes the signed angle between the human orientation and `h \u2212 `.\nExample 2. In the situation s = (posh,PFeat) depicted in Fig. 3(c), effLEFT(s) = (((1, 2), 3 4\u03c0),PFeat\\{f2}). The distance of the human h to f9\nat (4, 3) is dh(f9) = \u2016(4, 3)\u2212 (2, 1)\u20162 = 2 \u221a 2, the angle is \u03b3h(f9) = \u221214\u03c0.\nMovement-valuation. Let us consider how to obtain movement values when combining objectives. These values are a kind of importance factor of the movement in a particular situation. The steps are outlined in Fig. 5. Fig. 5(a) outlines how to obtain values for the possible movements for each objective. These values are not always unique. Fig. 5(b) describes how to combine the movement values for different objectives, and to translate it into distributions. Details of the various steps are given below.\nRelevant features. Recall that we consider three objectives, i.e. Objectives = {FOLLOW,AVOID,COLLECT}. For each objective o, we have exactly one\ncorresponding feature-type f(o); Waypoints, obstacles and litter correspond to FOLLOW, AVOID and COLLECT, respectively. As the behaviour of the human is independent of disappeared features, we call the set of present features the relevant features RelFeato(s) = {(f(o), `) \u2208 PFeat} w.r.t. objective o in situation s = ((`h, \u03b1h),PFeat).\nClosest relevant features. We adopt the assumption from [4] that for each objective o, only the closest feature of type f(o) is relevant for the behaviour with respect to o.\nRemark 1. While this assumption is strong, it reduces the number of hidden parameters a learning method has to estimate. Our method can be easily adapted to models where other/multiple features per type are relevant.\nLet Close(s, o) be closest relevant features for objective o in situation s:\nClose(s, o) = {f \u2208 RelFeato(s) | \u2200f \u2032 \u2208 RelFeato(s). dh(f) \u2264 dh(f \u2032)}.\nExample 3. For the situation s depicted in Fig. 3(b), Close(s,AVOID) = {f5, f6} and Close(s,FOLLOW) = {f1}.\nWhile in the physical reality two objects are almost never equally far away, in the grid abstraction, this happens frequently. Thus, the set of closest relevant features is not necessarily a singleton. The human behaviour is underspecified in this case, and any of the features in the set might be the actually relevant feature.\nWhile we support under-specification, it can be valuable to collapse this, and create the unique closest relevant features. We do so by selecting the feature from the closest relevant features with (i) the smallest absolute angle, and in case of a tie (ii) left of the human over right-of-the human. This assumption allows us to treat larger benchmarks at the cost of a less precise model.\nExample 4. Breaking the tie means that in the situation s depicted in Fig. 3(b), Close(s, AVOID) = { f5 }, as |\u03b3h(f5)| \u2248 26\u25e6 < |\u03b3h(f6)| \u2248 63\u25e6.\nMovement-values. We assume that we have Q-tables as in [4]. In particular, for each movement m and each objective o, a Q-table Qmo : R\u00d7 R+ \u2192 R maps the angle \u03b3h(f) and distance dh(f) between a human and a close relevant feature f to an objective-movement-value. Partial tables are given in Tab. 1. Alike to [4], we translate the set of closest relevant features\ninto a set of objective-movement-vectors V o(s) by a lookup in the Q-table, and store the feature for later use.\nV o(s) ={(f, [ql, qs, qr]) | f \u2208 Close(s, o), ql = QLEFTo (\u03b3h(f), dh(f)), qs = Q STRAIGHT o (\u03b3h(f), dh(f)), qr = Q RIGHT o (\u03b3h(f), dh(f)) }\nThe vector entries collect movement-values with respect to a fixed feature.\nExample 5. For s as depicted in Fig. 3(b), Close(s,AVOID) = {f5, f6}, we obtain by lookup in Tab. 1 \u2013 using dh(fi) = \u221a 5 \u2208 [2, 3) for i \u2208 {5, 6}, \u03b3h(f5) \u2248 26\u25e6 \u2208 [15, 45), and \u03b3h(f6) \u2248 \u221263\u25e6 \u2208 [\u221290,\u221245) \u2013 the following result: V AVOID(s) = {(f5, [0,\u22120.67,\u22120.32]), (f6, [\u22120.01, 0, 0])}.\nCombining movement values. We assume that have an objective-weight vector w = [wAVOID, wCOLLECT, wFOLLOW] \u2208 Distr(Objectives) over the objectives, e.g. obtained by IRL as in [4]. The objective-movement-vectors are translated into movement-vectors by calculating a weighted sum over all combinations: We first scale all V o(s) with wo, that is, the weightedobjective-movement values wV o(s) = {(f, wo \u00b7 x)|(f,x) \u2208 V o(s)}. Then, we take the sum of the weighted vectors and construct the union of all involved features. Formally, for any movement-vector and position of the human the corresponding set of movement-values, a pair consisting of the movement-vector and a set of the features involved:\nV (s) = {({f, f \u2032, f \u2032\u2032},x+x\u2032+x\u2032\u2032)|(f,x, f \u2032,x\u2032, f \u2032\u2032,x\u2032\u2032) \u2208 \u00d7 o\u2208Objectives wV o(s)}\nwhere the sum is the component-wise sum.\nExample 6. Similar to Ex. 5, we obtain V COLLECT(s) = {(f8, [2.5, 2.4, 2])}, V FOLLOW(s) = {(f1, [0, 1.14, 0])}. The V (s) can be computed using the objective-weights (provided by e.g. IRL) w = [0.414, 0.215, 0.369]. We get\nV (s) = {({f6, f2, f9}, 0.414 \u00b7 [.., .., ..] + 0.215 \u00b7 [.., .., ..] + 0.369 \u00b7 [.., .., ..]), ({f7, f2, f9}, 0.414 \u00b7 [.., .., ..] + 0.215 \u00b7 [.., .., ..] + 0.369 \u00b7 [.., .., ..])}\n= {({f6, f2, f9}, [0.53, 0.66, 0.3]), ({f7, f2, f9}, [0.53, 0.93, 0.42]}\nNotice that at the grid borders, only some movements are possible. Thus, we need to rule out such movements. In the course of this paper, we do this by resetting those movement-values to \u2212\u221e. If no movement is possible, we remove the vector from the movement values.\nDistribution. The last step (cf. Fig. 5(b)) is to transfer movement values into a distribution. Under the working hypothesis that the valuation reflects the likelihood that a human makes a specific move, we can translate this into a distribution over the movements \u2013 provided any movement is possible.\nThe stochastic behaviour of the human is obtained by translating any vector x 6= \u2212\u221e for (F,x) \u2208 V (s) for some situation s to a distribution over movements, by means of a softmax -function [1]: Rn\u221e \u2192 [0, 1]n \u2013 which attributes most, but not all probability to the maximum, hence the name. Using e\u2212\u221e := 0, the distribution is defined as:\nsoftmax\u03c4 (x)i = exi/\u03c4\u2211 i\u2264|x| e xi/\u03c4 .\nFor any invalid movement, the denominator is thus unaffected as the term is zero, the numerator is zero, effectively ruling out the transition. The parameter \u03c4 is called the temperature. Towards a zero temperature, the function is as a (hard) maximum, while with a high temperature it yields an almost uniform distribution."}, {"heading": "3.2 MDP model of the human behaviour", "text": "Given the formal description above, we are now ready to construct the MDP for the human behaviour. The state space is given by the set of situations. The initial state is given by the start-location of the human and the assumption that initially all features are present. As stated before we have two possible sources of non-determinism: (1) Under-specification of the model: the closest relevant feature is not unique. (2) Insufficient confidence in some entries in the Q-table, e.g. if the amount of data does\nnot allow us to draw conclusions. Here we only consider the first source, the latter is an extension with some more non-determinism.\nNon-determinism due to under-specification. Resolving the non-singleton movement-vectors is modelled to be an action of the environment.\nDefinition 5. The MDP M = (S, sI ,Act ,P) reflecting the human behaviour starting in inith on a environment Env = (Loc,Feat) using temperature \u03c4 is given by\n\u2013 S = {(posh, P ) | posh = (`h, \u03b1h) \u2208 Loc\u00d7 Orient, P \u2286 Feat} \u2013 sI = (inith,Feat) \u2013 Act = Feat3\n\u2013 P(s, a) = { {eff(Mh)i(s) 7\u2192 softmax\u03c4 (x)i | i \u2208 {1, 2, 3}} (a,x) \u2208 V (s) 0 otherwise.\nRewards for the human performance. In order to evaluate the performance with respect to the objectives, we define transition-reward mappings rewo for each o \u2208 Objectives,\n\u2200s, s\u2032 \u2208 S \u2200a \u2208 Act , rewo(s, a, s\u2032) = {\n1 if RelFeato(s) 6= RelFeato(s\u2032) 0 otherwise.\nTo define a combined reward, we want to avoid obstacles and collect litter, which means to penalise visiting some fields while rewarding others; this calls for the use of both positive and negative rewards, which is not possible in PRISM. With the help of the goal-states, we can give rewards upon entering them and considering the performance afterwards5."}, {"heading": "3.3 SG model of human-robot interaction", "text": "In order to obtain a model which models human behaviour and allows us to synthesise a plan for the robot, we have to consider a unified model. As we want to choose actions for the robot, the robot is naturally modelled as a (potentially non-probabilistic) MDP. Notice that the nondeterminism of the robot is controllable, whereas the non-determinism of the human model is uncontrollable. This naturally leads to a stochastic two-player game. As a design choice, we let robot and human move in turns (typically alternating). While this abstraction is not inherently different from synchronous movements, this means that we can use single actions to determine the movements. 5 In combination with the robot, this prevents distinguishing robot and human\nperformance: here encoding the reward in the state space would be our last resort.\nThe robot model. We can support any MDP on the same grid given as a PRISM-module. Within the scope of the paper, we considered a simple robot either turning 90-degrees (left or right) in place, or moving forward. Synchronisation with the environment is via shared variables.\nConsidering the robot movement. We both (1) considered a scenario where the human behaviour is not influenced by the robot and (2) the robot as an obstacle-feature. (1) yields a controller for the robot which is not intrusive, whereas (2) means that the human tries to avoid the robot and the controller takes this into account. For the latter, we had no suitable data and we assumed that the human treats the robot as if the robot were static. However, the methodology presented here can be used as is if suitable Q-tables for dynamic obstacles are available."}, {"heading": "4 Experiments", "text": "We developed a prototypical tool-chain to show the feasibility of our approach outlined in Fig. 2. The tool realizes Fig. 5 for any scenario and exploits the PRISM-Games v2.0beta3 [12]. We first discuss how to create the model and then present some evaluation results obtained under Debian 8 on a HP BL685C G7, 48 cores, 2.0GHz each, and 192GB of RAM.\nState and transition encoding. We encode the SG for the joint human and (optional) robot behaviour in the PRISM-language. The model has a module for the human and one for the robot. A global flag indicates whether the human (or robot) moves next. The robot (human) has a precondition that the robot (human) may move and an update to let the human (robot) move next. A module for the human consists of 3 integers to represent her location and orientation, and a boolean bf for each feature f , with bf true iff f is present. The location of the (static) features are constants. Though the set of reachable scenarios is exponential in the number of features, the encoding is cubic as the behaviour is based on the nearest present features only; see also App. A.\nReward encoding. The PRISM language does not support state-actiontarget rewards as used in Sect. 3.3. We support two options for encoding the objective-reward: (1) Rescale the rewards to state-action rewards (preserving for expected reward measures [8]). As the rescaling depends on the probabilities, we de-facto have to scale rewards for each command which may reach a feature separately. Commands can only be named in the scenario without a robot; the combination of named commands and global variables (the status of the features) is not supported. (2) Introduce a first-time flag for any location with features indicating whether it is visited\nfor the first time. This increases the number of states by |{` | (tp, `) \u2208 Feat}| \u00b7 |Orient|. The number of commands does not increase, the BDD size is hardly affected. The rewards are attached to the states: the encoding is as large as the number of locations with features.\nOptimisations. We investigated various performance improvements. Two notable effective insights are: (1) (Only) the Q-table for obstacle avoidance shows equal values for the far-away bins\u2014indicating that human behaviour does not consider far-away obstacles. It is thus not necessary to distinguish which obstacle is nearest once they all induce the same lookup-value. This is especially relevant on large and sparse scenarios. (2) As every human location occurs on a large number of commands, naming expressions (called formulas in the PRISM-language) reduces the overhead of specifying the location repeatedly. Using short non-descriptive variable names reduces the encoding further. Together, they reduce the parsing time by 40%.\nModel construction and parsing. As most case studies can be succinctly described in the PRISM-language, parsing usually is not an issue. Here it is. Our models are up to 100,000 lines of PRISM code. App. B lists details about the model sizes and their building times. Parsing takes a significant amount of time. The hybrid PRISM engine yields good overall performance: model construction is much faster than for explicit state spaces, and value iteration with many small probabilities (< 0.01) takes many iterations to converge, which is slow on a purely BDD-based engine.\nEvaluating the human. For the MDP model w/o robot, we compute some minimum and maximum probabilities; their difference indicates the relevance of the underspecification. Fig. 6(a) gives some verification results for a 20x20 grid with 2 landmarks, 2 obstacles, and 7 waypoints. The module contains 84,000 commands. Fig. 6(b) plots the min/max probability to reach the goal area when the human is only told to follow the waypoints against the temperature (controlling the variability in the softmax function). It shows that with low variability most humans indeed reach the other side without leaving the grid. This quickly drops with higher variability (where any features are mostly ignored). Fig. 6(c) indicates a similar behaviour for step-bounded reachability for different number of steps (x-axis) and temperatures 0.05 and 0.5. Most humans need > 30 steps to reach the goal, indicating that based on the given data, they very unlikely walk in straight lines. The gap due to underspecification is significant as long as the variability is not too high. With low variability, most humans arrive within 60 steps. Detailed analyses considering the obtained schedulers show where underspecification has the largest effect.\nHuman-robot interaction. The state space is about a factor robot locations times robot directions (4) times turn-flag (2) larger. The number of choices is roughly twice the number of states. We built a 8\u00d78 grid with 2/2/3 features; construction of the 1.6\u00b7107 states took 20 hours. (Using the explicit engine, as PRISM-Games has no symbolic engine.) Model checking of a min/max reachability probability took 3 hours. Constructing a 11\u00d711 grid with 2/2/3 features with 7.4\u00b7107 SG-states took 198 hours. States are slowly created compared to typical benchmarks due to the large number of commands, rendering a thorough analysis impossible. We made simplifications: (1) resolve the underspecification to have one player making trivial choices, or (2) consider a coalition where the underspecification is controllable. Both observations allow us to do a (suboptimal) analysis on the (isomorphic) single-player SG, i.e. an MDP. While this does not reduce the size of the model, it allows for using symbolic engines. We give figures for the coalition approach here; removing underspecification was slightly faster. The aforementioned 11\u00d711 grid-model takes 191, 290 MTBDD nodes (13, 725 terminal) in the symbolic engine. Model construction in 19 seconds. Checking unbounded reachability takes about 500 iterations and four hours. As we are interested in the robot reaching its goal, finitehorizon plans are relevant. The first 100 iterations are done within 40 minutes and yield an almost identical scheduler."}, {"heading": "5 Conclusion and discussion", "text": "We have successfully translated a cognitive model into a formal setting and used it to compute control plans for robots moving in the presence of humans handling complex tasks. In the paper, we discussed the model as well as several (open) challenges we stumbled upon.\nLessons learned: the model. Based on the model-checking results, we obtained five lessons about the weighted Q-table model. (1) Fig. 6(c)\nshows that humans most likely walk in wavy lines. This is due to the lack of a notion of progress in visiting waypoints\u2014it does not penalise walking in circles, as only positive reward is earned on visiting waypoints. Following a line and giving a penalty for any diverging move (as in [4]) would provide a notion of progress. (2) As the Q-tables do not take into account the border of the environment, they are not avoiding a deadlock (or unspecified behaviour): The probability for leaving the grid is substantial in many cases. (3) For the discrete model, there is a potentially huge difference in behaviour based on how the underspecification is resolved; any analysis on the learned model has to take this underspecification into account. (4) Modelling variability over human behaviour by a single softmax and using a memoryless model are rough estimates. Therefore it is quite likely that a human behaves inconsistent (in the SG) to statements in e.g. [14]. (5) Finally, the Q-tables contain some unexpected outliers which in some configurations lead to unexpected behaviour.\nLessons learned: the method. The description for the human behaviour including variability can be translated into a formal model; allowing a variety of properties to be easily analysed\u2014in particular, it allows for analysing under-specifications and ill-defined data. The generalisation to a robot planning scenario (and to a SG) is straightforward, and enables to compute plans fulfilling specific properties. Probabilistic verification of this model raised six challenges: (1) The large number of different probabilities in learned data blows up the encoding (and the BDD), it also prevents successful application of typical reduction techniques such as bisimulation. It would be interesting to regularise the model on the learning side, or use techniques like -bisimulation [20]. (2) The softmax function serves the only purpose of introducing variability; its nice features w.r.t. differentiability are not relevant here. Sensitivity analysis over Q-table values would be of interest but current parameter synthesis techniques based on rational functions [21,22] cannot cope with exponentials. (3) Despite the lacking of typical symmetries in the scenarios, its encoding is significantly smaller than enumerating all states, as (here) the behavior only depends on the nearest obstacles: even for three features of every kind, reduction of a factor over 50 results. (4) While the approach yields promising results for the MDP scenario, the SG suffers from a state space explosion. Although the turn-based game is highly regular, this cannot be exploited by the lack of a symbolic engine to solve SGs. (5) The explicit engine suffers from the loss of information in the PRISM-encoding: formulas grouping states and making explicit that in a set of states only one or two commands are relevant are lost. 6) Finally, the scheduler is always computed for the\nfull state space, whereas based on reachability of many states, we are only interested in a fragment of the full state space; for states with a low reachability probability, we can take any action.\nFuture work. For future work, we would also like to investigate automatic abstraction techniques as in [23] and restrict exploration of the model as in [24], as well as sensitivity analysis and/or model repair, potentially based on techniques presented in [22] or those in [25].\nAcknowledgement. We thank Mary Hayhoe and Matthew Tong for providing and\nexplaining the data."}, {"heading": "A The size of the encoding", "text": "We argue that the encoding is cubic. For the description of the transition probabilities, it suffices to only consider under which circumstances a feature is not in Close(s, o): That is exactly if there is another feature f \u2032 of type f(o) s.t. dh(f \u2032) < dh(f). For any action [fObst, fLitt, fWpt] \u2208 FeatObst \u00d7 FeatLitt \u00d7 FeatWpt and each human position, we have as a precondition \u2227 o\u2208Objectives \u2227 f\u2208Feat{\u00acbf |f \u2208 Feato \u2227 dh(fo) < f}. That\nmeans that for each human position we have \u2264 \u220fo\u2208Objectives |Featf(o)| commands.\nIf avoiding the robot is to be treated as a separate objective, than the transition encoding for any command in the human module has to be given separately for every possible location of the robot."}, {"heading": "B Experimental Performance", "text": "Table 2(a) gives the grid size, the number of obstacles, landmarks, and waypoints, the number of commands, the number of states, choices and branches of the underlying model using \u03c4 = 0.075, the number of decision diagram nodes (with the number of terminal nodes in parentheses) as well as the time to parse and the time to build the model, either explicitly or via the symbolic engine. A \u2212 indicates that the figure could not be obtained within 20 hours. The high number of terminal mtbdd nodes is a major challenge for the dd-based engines, whereas the explicit engine cannot cope with the large number of commands.\nIn Table 2(b) we give the same information for the models including a robot; notice that here, the symbolic numbers where obtained by transforming the model into an MDP. Please notice that the decision diagrams are comparatively small: This is due to the regularity of the alternating movements.\nFor the (8,8) SG models, the model checking time to obtain a scheduler maximising the probability to reach the other side of the grid without crashing, taking into account any scheduler resolving the underspecification is roughly 1300, 3000, 5400 and 10700 seconds, respectively.\nThese files are also available on GitHub: https://github.com/moves-rwth/ human_factor_models."}], "references": [{"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Adaptive critics and the basal ganglia", "author": ["A.G. Barto"], "venue": "Models of Information Processing in the Basal Ganglia. MIT Press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "The computational neurobiology of learning and reward", "author": ["N.D. Daw", "K. Doya"], "venue": "Current Opinion in Neurobiology 16(2)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Modular inverse reinforcement learning for visuomotor behavior", "author": ["C.A. Rothkopf", "D.H. Ballard"], "venue": "Biological Cybernetics 107(4)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Algorithms for inverse reinforcement learning", "author": ["A.Y. Ng", "S.J. Russell"], "venue": "Proc. of ICML, Morgan Kaufmann", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "The probabilistic model checking landscape", "author": ["J.P. Katoen"], "venue": "Proc. of LICS.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Probabilistic verification for cognitive models", "author": ["S. Junges", "N. Jansen", "J.P. Katoen", "U. Topcu"], "venue": "Proc. of Cross-Disciplinary Challenges for Autonomous Systems (CDCAS). AAAI Fall Symposium", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M.L. Puterman"], "venue": "John Wiley and Sons", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1994}, {"title": "Prism 4.0: Verification of probabilistic real-time systems", "author": ["M. Kwiatkowska", "G. Norman", "D. Parker"], "venue": "Proc. of CAV. Volume 6806 of LNCS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "iscasMc: A web-based probabilistic model checker", "author": ["E.M. Hahn", "Y. Li", "S. Schewe", "A. Turrini", "L. Zhang"], "venue": "Proc. of FM. Volume 8442 of LNCS, Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "The complexity of stochastic games", "author": ["A. Condon"], "venue": "Inf. Comput. 96(2)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1992}, {"title": "PRISM-Games 2.0: A tool for multiobjective strategy synthesis for stochastic games", "author": ["M. Kwiatkowska", "D. Parker", "C. Wiltsche"], "venue": "Proc. of TACAS. Volume 9636 of LNCS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Credit assignment in multiple goal embodied visuomotor behavior", "author": ["C.A. Rothkopf", "D.H. Ballard"], "venue": "Frontiers in Psychology 1", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Control of gaze while walking: task structure, reward, and uncertainty", "author": ["M.H. Tong", "O. Zohar", "M.M. Hayhoe"], "venue": "Journal of Vision", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Implementation of Symbolic Model Checking for Probabilistic Systems", "author": ["D. Parker"], "venue": "PhD thesis, University of Birmingham", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Symbolic model checking for probabilistic processes", "author": ["C. Baier", "E.M. Clarke", "V. Hartonas-Garmhausen", "M.Z. Kwiatkowska", "M. Ryan"], "venue": "Proc. of ICALP. Volume 1256 of LNCS", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "PRISM-games: A model checker for stochastic multi-player games", "author": ["T. Chen", "V. Forejt", "M. Kwiatkowska", "D. Parker", "A. Simaitis"], "venue": "Proc. of TACAS. Volume 7795 of LNCS, Springer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Principles of Model Checking", "author": ["C. Baier", "J.P. Katoen"], "venue": "The MIT Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Automatic verification of probabilistic concurrent finite-state programs", "author": ["M.Y. Vardi"], "venue": "Proc. of FOCS, IEEE CS", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1985}, {"title": "Computing behavioral distances, compositionally", "author": ["G. Bacci", "G. Bacci", "K.G. Larsen", "R. Mardare"], "venue": "Proc. of MFCS. Volume 8087 of LNCS, Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic reachability for parametric Markov models", "author": ["E.M. Hahn", "H. Hermanns", "L. Zhang"], "venue": "Software Tools for Technology Transfer 13(1)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Prophesy: A probabilistic parameter synthesis tool", "author": ["C. Dehnert", "S. Junges", "N. Jansen", "F. Corzilius", "M. Volk", "H. Bruintjes", "J.P. Katoen", "E. Abraham"], "venue": "Proc. of CAV. Volume 9206", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic model checking modulo theories", "author": ["B. Wachter", "L. Zhang", "H. Hermanns"], "venue": "Proc. of QEST, IEEE CS", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Verification of Markov decision processes using learning algorithms", "author": ["T. Br\u00e1zdil", "K. Chatterjee", "M. Chmelik", "V. Forejt", "J. Kret\u0301\u0131nsk\u00fd", "M.Z. Kwiatkowska", "D. Parker", "M. Ujma"], "venue": "Proc. of ATVA. Volume 8837 of LNCS, Springer", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Moreover, reinforcement learning [1] (RL) sufficiently describes quantitative aspects of human behaviour when solving complicated tasks in realistic environments [2,3].", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": "Moreover, reinforcement learning [1] (RL) sufficiently describes quantitative aspects of human behaviour when solving complicated tasks in realistic environments [2,3].", "startOffset": 162, "endOffset": 167}, {"referenceID": 2, "context": "Moreover, reinforcement learning [1] (RL) sufficiently describes quantitative aspects of human behaviour when solving complicated tasks in realistic environments [2,3].", "startOffset": 162, "endOffset": 167}, {"referenceID": 3, "context": "Consider the visio-motor setting from [4] in Fig.", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "This picture is taken from [4] with permission from Ballard.", "startOffset": 27, "endOffset": 30}, {"referenceID": 4, "context": "To build an accurate and general model of observed human behaviour for different tasks, inverse reinforcement learning (IRL) [5] assigns weights describing preferences over these tasks.", "startOffset": 125, "endOffset": 128}, {"referenceID": 3, "context": "A large class of human behavioural models is covered by a set of Q\u2013tables together with weights obtained by the methods in [4].", "startOffset": 123, "endOffset": 126}, {"referenceID": 5, "context": "This paper proposes probabilistic model checking [6] to analyse human behaviour models described by weighted Q\u2013tables3 For an arbitrary concrete scenario, a Markov decision process (MDP) [8] is generated automatically, see Fig.", "startOffset": 49, "endOffset": 52}, {"referenceID": 7, "context": "This paper proposes probabilistic model checking [6] to analyse human behaviour models described by weighted Q\u2013tables3 For an arbitrary concrete scenario, a Markov decision process (MDP) [8] is generated automatically, see Fig.", "startOffset": 187, "endOffset": 190}, {"referenceID": 8, "context": "We assess the performance of the human for the scenario as well as properties of the human model itself by employing MDP model checking as supported by PRISM [9], StORM, and iscasMc [10].", "startOffset": 158, "endOffset": 161}, {"referenceID": 9, "context": "We assess the performance of the human for the scenario as well as properties of the human model itself by employing MDP model checking as supported by PRISM [9], StORM, and iscasMc [10].", "startOffset": 182, "endOffset": 186}, {"referenceID": 10, "context": "The joint human-robot interaction model is a stochastic two-player game (SG) [11].", "startOffset": 77, "endOffset": 81}, {"referenceID": 11, "context": "We synthesise optimal policies for the robot under the human behaviour using SG model-checking with PRISM-Games [12].", "startOffset": 112, "endOffset": 116}, {"referenceID": 6, "context": "3 A high-level conceptual view in the form of an extended abstract is given in [7].", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "input data from [4] observations of human behaviour", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "with loc = {(x, y) | x 2 [0, 4] y 2 [0, 5]}, and", "startOffset": 25, "endOffset": 31}, {"referenceID": 4, "context": "with loc = {(x, y) | x 2 [0, 4] y 2 [0, 5]}, and", "startOffset": 36, "endOffset": 42}, {"referenceID": 6, "context": "\u21b5h 2 Orient = {i \u00b7 14\u21e1 | i 2 [0, 7]}.", "startOffset": 29, "endOffset": 35}, {"referenceID": 12, "context": "for visio-motor ta ks [13,4,14].", "startOffset": 22, "endOffset": 31}, {"referenceID": 3, "context": "for visio-motor ta ks [13,4,14].", "startOffset": 22, "endOffset": 31}, {"referenceID": 13, "context": "for visio-motor ta ks [13,4,14].", "startOffset": 22, "endOffset": 31}, {"referenceID": 14, "context": "6\u00b7107 states, its generation\u2014in absence of a symbolic e gine [15,16] for SGs\u2014takes over twenty hours; analysing maxmi re chability pr babili ies akes three hours.", "startOffset": 61, "endOffset": 68}, {"referenceID": 15, "context": "6\u00b7107 states, its generation\u2014in absence of a symbolic e gine [15,16] for SGs\u2014takes over twenty hours; analysing maxmi re chability pr babili ies akes three hours.", "startOffset": 61, "endOffset": 68}, {"referenceID": 16, "context": "The SGs have a noticeably more complex structure than benchmarks in [17] and offer new challenges to probabilistic verification.", "startOffset": 68, "endOffset": 72}, {"referenceID": 0, "context": "A stochastic game (SG) is a tuple M = (S, sI ,Act ,P) with a finite set S of states such that S = S\u25e6]S2, an initial state sI \u2208 S, a finite set Act of actions, and a transition function P : S \u00d7Act \u00d7 S \u2192 [0, 1] and \u2211s\u2032\u2208S P(s, \u03b1, s\u2032) \u2208 {0, 1} \u2200s \u2208 S, a \u2208 Act.", "startOffset": 202, "endOffset": 208}, {"referenceID": 18, "context": "Nondeterministic choices of actions in SGs and MDPs are resolved schedulers ; here it suffices to consider memoryless deterministic schedulers [19].", "startOffset": 143, "endOffset": 147}, {"referenceID": 0, "context": "A reachability property asserts that a set T \u2286 S of target states is to be reached from the initial state with probability at most \u03bb \u2208 [0, 1], denoted P\u2264\u03bb(\u2666T ).", "startOffset": 135, "endOffset": 141}, {"referenceID": 7, "context": "Verification can be performed by computing maximal (or minimal) probabilities or expected rewards to reach target states using standard techniques, such as linear programming, value iteration, or policy iteration [8].", "startOffset": 213, "endOffset": 216}, {"referenceID": 3, "context": "(a) From [4] (with permission) Wpt Litt Obst inith Goal", "startOffset": 9, "endOffset": 12}, {"referenceID": 3, "context": "The environment is given as Env = (Loc,Feat) with Loc = {(x, y) | x \u2208 [0, 4] y \u2208 [0, 4]}, and", "startOffset": 70, "endOffset": 76}, {"referenceID": 3, "context": "The environment is given as Env = (Loc,Feat) with Loc = {(x, y) | x \u2208 [0, 4] y \u2208 [0, 4]}, and", "startOffset": 81, "endOffset": 87}, {"referenceID": 6, "context": "\u03b1h \u2208 Orient = {i \u00b7 1 4\u03c0 | i \u2208 [0, 7]}.", "startOffset": 30, "endOffset": 36}, {"referenceID": 3, "context": "We adopt the assumption from [4] that for each objective o, only the closest feature of type f(o) is relevant for the behaviour with respect to o.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "We assume that we have Q-tables as in [4].", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "Alike to [4], we translate the set of closest relevant features", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": "[0, 1] (1, 2] (2, 3] .", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[0, 1] (1, 2] (2, 3] .", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[0, 1] (1, 2] (2, 3] .", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "obtained by IRL as in [4].", "startOffset": 22, "endOffset": 25}, {"referenceID": 0, "context": "The stochastic behaviour of the human is obtained by translating any vector x 6= \u2212\u221e for (F,x) \u2208 V (s) for some situation s to a distribution over movements, by means of a softmax -function [1]: R\u221e \u2192 [0, 1]n \u2013 which attributes most, but not all probability to the maximum, hence the name.", "startOffset": 189, "endOffset": 192}, {"referenceID": 0, "context": "The stochastic behaviour of the human is obtained by translating any vector x 6= \u2212\u221e for (F,x) \u2208 V (s) for some situation s to a distribution over movements, by means of a softmax -function [1]: R\u221e \u2192 [0, 1]n \u2013 which attributes most, but not all probability to the maximum, hence the name.", "startOffset": 199, "endOffset": 205}, {"referenceID": 11, "context": "0beta3 [12].", "startOffset": 7, "endOffset": 11}, {"referenceID": 7, "context": "We support two options for encoding the objective-reward: (1) Rescale the rewards to state-action rewards (preserving for expected reward measures [8]).", "startOffset": 147, "endOffset": 150}, {"referenceID": 3, "context": "Following a line and giving a penalty for any diverging move (as in [4]) would provide a notion of progress.", "startOffset": 68, "endOffset": 71}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "It would be interesting to regularise the model on the learning side, or use techniques like -bisimulation [20].", "startOffset": 107, "endOffset": 111}, {"referenceID": 20, "context": "Sensitivity analysis over Q-table values would be of interest but current parameter synthesis techniques based on rational functions [21,22] cannot cope with exponentials.", "startOffset": 133, "endOffset": 140}, {"referenceID": 21, "context": "Sensitivity analysis over Q-table values would be of interest but current parameter synthesis techniques based on rational functions [21,22] cannot cope with exponentials.", "startOffset": 133, "endOffset": 140}, {"referenceID": 22, "context": "For future work, we would also like to investigate automatic abstraction techniques as in [23] and restrict exploration of the model as in [24], as well as sensitivity analysis and/or model repair, potentially based on techniques presented in [22] or those in [25].", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "For future work, we would also like to investigate automatic abstraction techniques as in [23] and restrict exploration of the model as in [24], as well as sensitivity analysis and/or model repair, potentially based on techniques presented in [22] or those in [25].", "startOffset": 139, "endOffset": 143}, {"referenceID": 21, "context": "For future work, we would also like to investigate automatic abstraction techniques as in [23] and restrict exploration of the model as in [24], as well as sensitivity analysis and/or model repair, potentially based on techniques presented in [22] or those in [25].", "startOffset": 243, "endOffset": 247}], "year": 2016, "abstractText": "This paper proposes to use probabilistic model checking to synthesize optimal robot policies in multi-tasking autonomous systems that are subject to human-robot interaction. Given the convincing empirical evidence that human behavior can be related to reinforcement models, we take as input a well-studied Q-table model of the human behavior for flexible scenarios. We first describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary but fixed scenario. The distinctive issue is that \u2013 in contrast to existing models \u2013 under-specification of the human behavior is included. Probabilistic model checking is used to predict the human\u2019s behavior. Finally, the MDP model is extended with a robot model. Optimal robot policies are synthesized by analyzing the resulting two-player stochastic game. Experimental results with a prototypical implementation using PRISM show promising results.", "creator": "LaTeX with hyperref package"}}}