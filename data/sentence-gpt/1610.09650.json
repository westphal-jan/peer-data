{"id": "1610.09650", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2016", "title": "Deep Model Compression: Distilling Knowledge from Noisy Teachers", "abstract": "The remarkable successes of deep learning models across various applications have resulted in the design of deeper networks that can solve complex problems. However, the increasing depth of such models also results in a higher storage and runtime complexity, which restricts the deployability of such very deep models on mobile and portable devices, which have limited storage and battery capacity. While many methods have been proposed for deep model compression in recent years, almost all of them have focused on reducing storage complexity. In this work, we extend the teacher-student framework for deep model compression, since it has the potential to address runtime and train time complexity too. We propose a simple methodology to include a noise-based regularizer while training the student from the teacher, which provides a healthy improvement in the performance of the student network. Our experiments on the CIFAR-10, SVHN and MNIST datasets show promising improvement, with the best performance on the CIFAR-10 dataset. We also conduct a comprehensive empirical evaluation of the proposed method under related settings on the CIFAR-10 dataset to show the promise of the proposed approach. In particular, we test the model and model to see if its accuracy is adequate.\n\n\n\n\nThe main features in our experiments were the high-performance, efficient performance, and low-impact performance of the data in the data set. This method is supported by the M-VM (VM-8, TVM-11, TVM-12). The M-VM also enables the implementation of a system-wide, non-interruptive and persistent state. The M-VM also provides support for the use of several methods to control the performance of the data set using a new framework for the system, which is available in the MVM. The M-VM also allows for a number of different operations, but the first is to ensure that the data is at least consistent on a specific type of model. For example, the model contains a fixed type of data which can be partitioned into multiple layers by using the DSP and M-VM. As the data is partitioned, the model can be reused to modify all the data from the data. The system-wide model can also include the implementation of a system-wide, non-interruptive and persistent state, which enables the implementation of a system-wide, non-interruptive and persistent state.\nIn this study, we tested how to build the model with a single approach:\nThe model is modular, with a limited number of implementations. The", "histories": [["v1", "Sun, 30 Oct 2016 13:54:39 GMT  (259kb,D)", "http://arxiv.org/abs/1610.09650v1", "Submitted to WACV,2017"], ["v2", "Wed, 2 Nov 2016 16:32:23 GMT  (259kb,D)", "http://arxiv.org/abs/1610.09650v2", "9 pages, 3 figures"]], "COMMENTS": "Submitted to WACV,2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bharat bhusan sau", "vineeth n balasubramanian"], "accepted": false, "id": "1610.09650"}, "pdf": {"name": "1610.09650.pdf", "metadata": {"source": "CRF", "title": "Deep Model Compression: Distilling Knowledge from Noisy Teachers", "authors": ["Bharat Bhusan Sau", "Vineeth N. Balasubramanian"], "emails": ["cs14mtech11002@iith.ac.in", "vineethnb@iith.ac.in"], "sections": [{"heading": "1. Introduction", "text": "Since the remarkable success in object recognition by the famous work of Krizhevsky et al. [16] in 2012, deep learning has become very popular and replaced classical computer vision in a wide variety of real-world applications. We have observed performances on challenging real-world datasets that were once considered unachievable. With the introduction of networks like GoogLeNet [31], VGGNet [27] and ResNets [11] in recent times, networks are becoming deeper and deeper. Impressive progress has been made to train very deep networks with the invention of newer methods (as in [11]), as well as the relatively easier availability of computational resources today.\nAt the same time, consumer devices are also getting\nsmaller and smaller in terms of mobility and portability, resulting in a widening chasm between deeper networks and their deployability on small mobile devices. Consumer devices have limited memory and computational capability to store or run such deep models (for e.g. the VGGNet network [27] requires 540 MB of storage, which is not suitable for a mobile device). Further, as described with relevant statistics by Han et al. in [10], running large networks on mobile devices increases memory access, which in turn consumes considerable battery power. This has resulted in a need for deep model compression methods.\nVarious deep model compression methods have been developed recently in the past 2-3 years to deal with these issues. These methods (described in Section 2) can be broadly categorized into parameter sharing methods, network pruning methods, \u2018dark knowledge\u2019 or teacher-student methods, and matrix decomposition methods. Although the performance of the compressed models have been good, most of these methods have focused on reducing only the storage complexity of the deep models. The compressed models have to be decompressed at runtime; the aforementioned issues of deployability on mobile devices thus continue to remain.\nOne approach among existing methods, which is not explored well enough, which can address this gap is the teacher-student approach ([1][3][12]) to deep model compression. This is a rather simple approach where a shallow student network is trained from a deep teacher network. However, considering that the student has to be a shallow network in order to minimize storage and runtime complexity, it becomes very challenging for the shallow student to achieve accuracy comparable to the deep teacher network. In this work, we propose a method based on the teacherstudent framework to improve the performance of the student network, while remaining shallow. The impact of our contribution is directly relevant to the limitations discussed so far with existing deep model compression methods.\nThe key contributions of this work are as follows: (i) We focus our efforts on reducing storage as well as runtime complexity by building on the idea of teacher-student learning algorithm (considering a pretrained teacher such\nar X\niv :1\n61 0.\n09 65\n0v 1\n[ cs\n.L G\n] 3\n0 O\nct 2\n01 6\nas AlexNet or VGGNet, this will also reduce training time complexity); (ii) While teacher-student learning algorithm is a promising approach for compressing deep models (in terms of storage, runtime and training time complexities), distilling knowledge from a single teacher can be limiting. We propose a simple noise-based regularization methodology to simulate learning from multiple teachers for better deep model compression; and (iii) We validate our methodology on three benchmark datasets: MNIST, SVHN and CIFAR-10 to highlight the usefulness of the proposed method. We also perform a comprehensive empirical analysis of the proposed method on CIFAR-10 to help identify success and failure cases which could help in adoption of the method to real-world applications.\nWe now present the related earlier work that is relevant to this work."}, {"heading": "2. Background and Related Work", "text": "Model compression of deep networks has attracted attention very recently, and existing efforts can be broadly categorized into 4 kinds, each of which is described below.\nParameter sharing methods: Chen et al. [4] proposed a HashedNets model which used a low-cost hash function to group weights into hash buckets to share parameters where each hash bucket denotes a single parameter. Gong et al. [7] used k-means clustering to quantize the weights in fully connected layers and achieved upto 24x compression rate for their CNN network with only 1% loss on accuracy on the ImageNet challenge. Soulie et al. [28] used a regularization technique instead to coarsely quantize the weights of fully connected layers.\nNetwork pruning methods: Han et al. [10] proposed to reduce the number of parameters by pruning weights which are below a threshold after the network is trained. They extended this work by using Huffman encoding [9] to reduce the number of parameters further. Leroux et al. [19] aimed at reducing computations by ignoring the convolutional filters which produce least activational strength. Srinivas and Babu [29] explored the redundancy among neurons, and proposed a data-free pruning methodology to remove redundant neurons.\n\u2018Dark knowledge\u2019 methods: The key idea of this group of methods, which is the focus of this work, is adopting a teacher-student strategy, where a large deep network trained for a given task teaches shallower student networks on the same task. The teacher-student training method was first proposed by Bucilu et al. [3] where they created synthetic data by labeling unlabeled data with a teacher model. This synthetically labeled data, which contains the knowledge captured by the teacher model, is then used to\ntrain a smaller student model. Ba and Caruana [1] proposed to train the student model by mimicking the logit values of the teacher model. This approach forms the baseline for this work, and we describe this method in more detail in Section 3.2. Romero et al. [24] extended this work by using intermediate hidden layer outputs as target values for training a student model. Hinton et al. [12] generalized this method by introducing a temperature variable in the softmax function. They showed that the softened outputs at higher temperatures convey important information. They termed this information which is expressed by the relative scores (probabilities) of the output classes as \u2018dark knowledge\u2019.\nMatrix decomposition methods: Denil et al. [5], Sainath et al. [25] and Nakkiran et al. [21] all adopted low-rank decomposition to compress the weights in different layers. Novikov et al. [23] converted the dense weight matrices of the fully connected layers to the Tensor Train format, such that the number of parameters is reduced by huge factor while preserving the expressive power of the layer. Other methods such as Denton et al. [6] that use matrix factorization attempt to speed up operations, for instance in the convolutional layer, but do not provide a holistic solution to the issues of complexity mentioned so far.\nCompression of deep models is ideally important from all three perspectives: storage, train time and runtime complexities. Both parameter sharing and matrix decomposition methods concentrate on only the storage complexity of the deep models, but they fail to improve on runtime (or train time) complexity. Network pruning methods have shown very good performance on the storage complexity task, but are not aimed at reducing the runtime (or train time) complexity too. Teacher-student methods show promise in this direction of achieving compression across all complexities, but there has not been much follow-up work since it was proposed. This observation, supported by the potential of such methods in addressing the aforementioned issues, motivates our proposed work."}, {"heading": "3. Proposed Methodology", "text": "Since our method is built on the teacher-student, viz. the \u2018dark knowledge\u2019 framework for deep model compression, we begin with a brief overview of teacher-student learning methods in Section 3.1, describe one such method in Section 3.2 (which we use as a baseline for comparison), and then provide our methodology in Section 3.3. We then show in Section 3.4 on how the proposed method is equivalent to noise-based regularization on the teacher."}, {"heading": "3.1. Teacher-Student Learning", "text": "In teacher-student frameworks in deep learning, the teacher is a pretrained deep model, which is used to train\nanother (typically shallow) model, called the student. There has been limited earlier work so far in this context, as described in Section 2. However, there are significant advantages of using a teacher-student framework beyond just model compression, as observed by Hinton et al. [12]: \u2022 The \u2018dark knowledge\u2019 present in the teacher outputs\nworks as a powerful target-cum-regularizer for the student model, as it provides soft targets that share helpful information. \u2022 Convergence is typically faster than using only orig-\ninal 0/1 hard labels, due to the soft targets, that help training. \u2022 A small amount of training data is generally sufficient\nto train the student network.\nThese advantages motivate us to extend this idea using noisy teachers. We now describe one of the teacher-student methods, which is used as a baseline for our experiments."}, {"heading": "3.2. Student Learning using Logit Regression", "text": "Ba and Caruana [1] proposed a method to train the student directly on the log probability values z, also called logits, which is the output of the layer before softmax activation. The student network is trained in a regression setting using the logits, with its training data given by: {(x(1), z(1)), ...., (x(i), z(i)), ...., (x(n), z(n))}. The L2 loss function minimized during training is given by:\nL(x, z, \u03b8) = 1\n2T\n\u2211\ni\n\u2016g(x(i); \u03b8)\u2212 z(i)\u201622 (1)\nwhere: \u2022 T is the mini-batch size \u2022 x(i) is ith training sample in the mini-batch \u2022 z(i) is the corresponding logit output of the pretrained\nteacher for x(i)\n\u2022 \u03b8 is the set of student model parameters \u2022 g(x(i); \u03b8) is the student model\u2019s logit output for x(i)\nWe build on this approach to develop our idea of noisy teachers."}, {"heading": "3.3. \u2018Noisy Teachers\u2019: Student Learning using Logit Perturbation", "text": "The performance of shallow models in teacher-student framework settings was greatly improved by the methods proposed by Hinton et al. in [12], as well as Ba and Caruana in [1]. In this work, we explore this line of work further by asking the question: what if a student learns from multiple teachers? Analogous to human learning environments where a student can hone his understanding of a subject by learning the same topic from multiple experts of the field, we hypothesize that the performance of the student will improve under such a setting. However, instead of directly learning from multiple teachers, we propose a methodology to simulate the effect of multiple teachers, by injecting noise and perturbing the logit outputs of a teacher. The perturbed outputs, not only, simulate a multiple-teacher setting, but also results in noise in the loss layer, thus producing the effect of a regularizer. This new noisy teacher thus acts as a target-cum-regularizer and helps the student to learn better and produce results closer to the teacher network. We now describe the formulation of this method, which we also call Logit Perturbation.\nLet \u03be be a vector of Gaussian noise with mean \u00b5 = 0 and standard deviation \u03c3. Dimension of \u03be is equal to the number of classes/logits in the teacher network. If z(i) is the logit layer output of the teacher model for x(i), then we modify z(i) as follows:\nz\u2032(i) = (1 + \u03be).z(i) (2)\nwhere 1 is a vector of ones and i \u2208 Rn where n is the number of classes. The loss function in Eqn 1 then becomes:\nL(x, z\u2032, \u03b8) = 1\n2T\n\u2211\ni\n\u2016g(x(i); \u03b8)\u2212 z\u2032(i)\u201622 (3)\nThe value of \u03c3 determines the amount of perturbation on the teacher\u2019s original logit values z(i). Higher the value of \u03c3, higher is the perturbation. However, this perturbation need not be imposed on all samples. We instead select samples from the mini-batch with some fixed probability \u03b1, and the logit values of the selected samples are then perturbed using Eqn 2.\nGiven a student model with initial parameters (weights) \u03b80, we then find its final parameters \u03b8 using Stochastic Gradient Descent (SGD), where in the (t + 1)th iteration, \u03b8 is updated as follows:\n\u03b8t+1 = \u03b8t \u2212 \u03b3t. \u2211\n(x,y)\u2208Dt \u2207\u03b8t [L(x, z\u2032, \u03b8)] (4)\nHere, T = |Dt| is the size of the mini-batch randomly drawn from the training dataset D, \u03b3t is the learning rate, L(x, z\u2032, \u03b8) is as in Eqn 3, and \u2207\u03b8t [L(x, z\u2032, \u03b8)] is computed using gradient backpropagation.\nAlgorithm 1 Training Student with Logit Perturbation 1: Input: Training Data D=(x, z), probability \u03b1, std \u03c3\n2: Initialization: \u03b80 = Model Parameters of student model 3: for each mini-batch Dt = {xt, zt} do 4: Generate \u03be, \u03be \u223c N (0, \u03c32I) 5: Select samples from xt with probability \u03b1\n6: Perturb corresponding logit values in zt using Eqn (2) 7: Calculate L2 loss using Eqn (3) 8: Update model parameters \u03b8t using Eqn (4)\n9: end for 10: Output: \u03b8 = Trained Student Model Parameters\nIn summary, some samples are selected with probability \u03b1 from the mini-batch. The target logit values of the selected samples are perturbed using Eqn 2. The student network is then trained with the loss function in Eqn 3. A high-level flowchart of our method is shown in Figure 1. Algorithm 1 describes the pseudocode."}, {"heading": "3.4. Equivalence to Noise-Based Regularization", "text": "It has been long established that noisy data in training helps to regularize a model (one of the earliest works showing this was [26]). Bishop showed in [2] that adding an L2 regularization term in the loss function is equivalent to adding Gaussian noise in the input data. The regularized loss function is given as:\nL(x\u2032, \u03b8, z) = L(x, \u03b8, z) +R(\u03b8) (5)\nwhere x\u2032 corresponds to x with Gaussian noise, L(x\u2032, \u03b8, z) is the L2 loss equivalent to noisy input data, L(x, \u03b8, z) is the L2 loss for original input data, and R(\u03b8) is the L2 regularizer. In our method, we perturb the target output z with noise, instead of the input data x. It is now trivial to show that perturbing the target output z, the logit values of the teacher, is equivalent to adding a noise-based regularization term to the loss function. From Eqn 2, we get:\nz\u2032(i) = (1 + \u03be).z(i) = z(i) + \u03be.z(i)\nHence, we can rewrite the L2 loss in Eqn 3, as:\nL(x, \u03b8, z\u2032) = \u2016(z(i) \u2212 g(x(i), \u03b8))\u2212 \u03be.z(i)\u201622 = \u2016z(i) \u2212 g(xi, \u03b8)\u201622 + \u2016\u03be.z(i)\u201622\n+ 2\u2016z(i) \u2212 g(x(i), \u03b8)\u20162 \u2217 \u2016\u03be.z\u20162 = L(x, \u03b8, z) + ER\nwhere ER = \u2016\u03be.z\u201622 + 2\u2016z(i) \u2212 g(x(i), \u03b8)\u20162\u2016\u03be.z\u20162 is the new regularizer that is based on the noise \u03be. Thus, perturbing logits in the teacher network is equivalent to adding\na noise-based regularizer to the loss function. (In Section 5, we show how this regularizer compares against using a standard L2 regularizer on the student network.) We now present our experimental results."}, {"heading": "4. Experimental Results", "text": "We evaluated our method on three benchmark datasets: MNIST [18] and SVHN [22] for digit recognition, and CIFAR-10 [15] for natural image recognition - each of which is described below. Stochastic Gradient Descent (SGD) is used to train all the networks used with a minibatch size of 64. ADAM [14], which combines the ideas of momentum and adaptive learning rates, is used to adjust the learning rate in each iteration of SGD. Convergence was decided by testing on a hold-out validation set. For experiments in this section, we compare the performance of our method against the baseline performance of the teacherstudent method as proposed by Ba and Caruana [1]. We also conducted preliminary experiments on comparing the proposed method against the work of Hinton et al. [12]; however, we found it non-trivial to identify the temperature at which their method\u2019s performance is maximized. In fact, we found it to give worse performance than [1] in our studies, and hence used [1] as the baseline comparison. Our results on all these datasets was promising, with the best improvement on the CIFAR-10 dataset. We further analyze the proposed method under various scenarios in Section 5."}, {"heading": "4.1. MNIST", "text": "MNIST [18] is a popular dataset for handwritten digit recognition with 10 classes (0-9). The training set contains 50000 images and validation set contains 10000 images. All samples are 28\u00d728 grayscale images. No pre-processing is done on the training data, keeping in line with earlier work on this dataset.\nTeacher Network: We use a modified network of LeNet [17] as the teacher network on this dataset. LeNet has two convolutional layers and a fully connected layer followed by a 10-way classifier. We abbreviate the configuration of the teacher network as: [C5(S1P0)@20-MP2(S2)][C5(S1P0)@50-MP2(S2)]- FC500- FC10, where: \u2022 C = Convolution Layer, with the following number in-\ndicating the size of the network, i.e. C5 indicates a convolutional layer with a 5\u00d7 5 kernel \u2022 S = Stride, i.e. S1 indicates a stride of 1 \u2022 P = Padding, i.e. P0 indicates a padding of zero \u2022 @ = Number of kernels in Convolution Layer, i.e. @20\nindicates 20 kernels in that layer \u2022 MP = Max Pooling Layer, with the following number\nindicating the subsampling window, i.e. MP2 indicates max pooling of 2\u00d7 2 windows \u2022 FC = Fully Connected Layer, with the following num-\nber indicating the number of nodes in that layer\nWe use the above abbreviation for the sake of brevity in the remainder of this paper.\nStudent Network: We use a shallow network of two fully connected layers only with 800 neurons in each layer, as described in Hinton et al. [12], as our student network. The architecture can be encoded as: FC800-FC800-FC10.\nResults: The teacher model achieved 68 test errors (out of 10000 test samples, error rate = 0.0068). The student model achieved 97 test errors (error rate = 0.0097) with the baseline teacher-student method (logit regression method described in Section 3.2). As the difference of performance between teacher and student is not very high, we chose the sample selection probability \u03b1 = 0.15, i.e., around 15% samples of each mini-batch are selected for logit perturbation. The perturbation was done at various noise levels (Gaussian with \u00b5 = 0, different \u03c3s) as shown in Table 1. This noise is added directly to the unnormalized logits in all our experiments in this work. We see that there is consistent improvement of performance of the student while applying perturbation to logits."}, {"heading": "4.2. SVHN", "text": "The Street View House Numbers (SVHN) [22] is a realworld image dataset containing cropped digits in house numbers from Google Street View images. It contains a much higher number of training and testing samples than MNIST. It has 73257 training samples, 26032 test samples, along with 531131 additional samples which can be used for training, each of which is an RGB image of size 32 \u00d7 32. As in earlier work [8] that have used this dataset, we selected 400 samples per category from the training set and 200 samples per category from the additional set to constitute a validation set of 6000 samples (which is used to decide if the training has converged sufficiently). The remaining 598388 samples are combined to form the training set. We also preprocessed all the data using local contrast normalization as in [13].\nTeacher Network: We used the Network-in-Network [20] model as our teacher network for this dataset. It has several convolutional layers, and its uniqueness lies in replacing the fully connected layer with a global average\npooling layer (denoted by AP, where the number following AP denotes the subsampling window size). The full configuration of the network can be written as: [C5(S1P2)@192][C1(S1P0)@160]- [C1(S1P0)@96-MP3(S2)]- D0.5[C5(S1P2)@192]- [C1(S1P0)@192]- [C1(S1P0)@192AP3(S2)]- D0.5- [C3(S1P1)@192]- [C1(S1P0)@192][C1(S1P0)@10]- AP8(S1). D0.5 denotes DropOut in between the corresponding layers with a probability of 0.5.\nStudent Network: We used a version of LeNet, as our student network. The network can be written as: [C5(S1P2)@32-MP3(S2)]- [C5(S1P2)@64-MP3(S2)]FC1024-FC10. We decided to have two convolutional layers in our student architecture based on the result shown by Urban et al. [32]. They showed that in order to mimic a large convnet and achieve significant performance, the student layer must have some convolutional layers.\nResults: The teacher model gave 3.82% error on the SVHN test set. The baseline student model (trained, as before, using the logit regression method described in Section 3.2) gave a 4.6% error rate. Once again, as the difference between student\u2019s and teacher\u2019s performance is not very high, we selected \u03b1 = 0.15. Using the proposed logit perturbation methodology, our student model achieved a 4.45% error rate within the range of standard deviations considered for the noise. Detailed results are shown in Table 2. We also observe that higher noise levels result in deterioration of performance on this dataset, indicating the importance of setting the values of \u03b1 and \u03c3 appropriately."}, {"heading": "4.3. CIFAR-10", "text": "CIFAR-10 [15] is a popular dataset for small-scale image recognition. The dataset contains 10 classes of natural images with a total of 50000 training and 10000 testing samples, each of which is a 32\u00d732 RGB image. We preprocessed the data by subtracting per-pixel mean and also took mirror image of the samples for data augmentation during training (thus increasing the training set to 100000 images).\nTeacher Network: We used the same teacher network (the Network-in-Network model) as used for the SVHN dataset (Section 4.2).\nStudent Network: The student network we used here\nis also a modified version of the LeNet architecture, with two convolutional layers and one fully connected layer. The architecture of our student network for this dataset can be given as: [C5(S1P2)@64-MP2(S2)]- [C5(S1P2)@128MP2(S2)]-FC1024-FC10.\nResults: Trained on 100000 data samples, the teacher network obtained 8.4% error rate. The baseline student model (trained using the logit regression method described in Section 3.2) provided a best performance of a 21.94% error rate. The results of the proposed method are described in Table 3. We observe that the proposed method gives a healthy improvement over the baseline method. Considering the large difference between teacher and student networks\u2019 performances, we chose \u03b1 = 0.5 for this dataset. This indicates that a good amount of perturbation helps the student network achieve better performance in this case.\nThe aforementioned results corroborate the promise of the proposed methodology. We present a detailed analysis of the method across different settings, including runtime complexities, in the subsequent section."}, {"heading": "5. Discussions and Analysis", "text": "In this section, we perform a comprehensive analysis of the performance of the proposed method under different conditions: varying noise in the teacher, comparison of a noisy teacher vs a student regularized directly with noise, the equivalence of this noisy teacher framework to learning from multiple teachers, as well as the comparison of the proposed noisy teacher approach to other regularization techniques. Our experimental results in this section are all carried out on the CIFAR-10 dataset."}, {"heading": "5.1. Varying Noise in the Teacher", "text": "In Section 4.3, we selected samples from a mini-batch with a fixed probability \u03b1 = 0.5 and varied the \u03c3 to experiment on different noise levels. In this section, we fix \u03c3 (standard deviation of the Gaussian noise), and vary the probability \u03b1 of the number of logit values on which noise is added. The results are shown in Table 4. For this test, we kept \u03c3 = 0.6 constant for all the experiments. The teacher and student networks are the same as described in Section\n4.3. From the table, we see that higher values of \u03b1 helped the student to achieve better performance. The performance reaches the best at \u03b1 = 0.8, i.e, when around 80% samples of mini-batch are selected for perturbation.\nOur empirical studies revealed that the value of \u03b1 for which a student network may give best result depends on the performance gap between the baseline student and teacher network. If this gap is large, a higher amount of noise is required to help the student train better and achieve its best possible performance. If this gap is small, as in the case of MNIST and SVHN (Sections 4.1 and 4.2), a lower number of noisy logits is more helpful."}, {"heading": "5.2. Noise in Teacher vs Noise in Student", "text": "In the experiments in Section 4.3, we saw that noise in the teacher, i.e., perturbing the logit outputs of the teacher network, has a positive regularization effect on the student, and helps it to achieve higher performance than the baseline (only logit regression). In this section, we ask the question if such an improvement in performance can be obtained\nby using the baseline method directly and instead regularizing (using weight decay) the student network during its training. This can be considered as directly adding noise to the student while training. The key difference between these two settings is that teacher network is already trained whereas student network is being trained. Hence, the logit output from a noisy teacher for a sample remains the same throughout the training phase whereas in case of a noisy student, this keeps changing. In this experiment, we modified the logit output from student in the same way we did for the teacher logit outputs. We used the same teacher and student network as in Section 4.3. The results are shown in Figure 2. The figure shows that a noisy teacher is more helpful than a noisy student. We believe this could be attributed to the understanding that learning from a noisy teacher with random noise is equivalent to learning from multiple similar teachers with minor variations though. Drawing a parallel to human learning environments, it would only be expected that students learn well under such conditions."}, {"heading": "5.3. Comparison with DropOut", "text": "While adding noise to the student is one form of regularization (as discussed in Section 3.4), we also compared the performance of the proposed method against another often used regularization method: DropOut [30]. We used the same teacher and student network described in Section 4.3, with one DropOut layer added after the fully connected layer of the student network. We studied the performance of the student under different DropOut ratios (i.e. varying probabilities of dropping nodes). The results are summarized in Table 5. We notice that DropOut does not perform as well as the proposed noisy teacher regularizer. The best improvement over the baseline that DropOut achieved was a decrease in error rate by 1.53%, while the proposed method performed significantly better (as in Table 3) with a best decrease in error rate of 3.26%."}, {"heading": "5.4. More Results", "text": "In this section, we describe other experiments that we carried out to study related issues: (i) the impact of random\nnoise level in each iteration of training; and (ii) learning from multiple teachers. Effect of Random Noise Levels: As described in Section 3, we used a Gaussian noise to perturb the logit output of the teacher in this work. In all previous experiments, we used a fixed \u03c3 (standard deviation, or noise level, as we called it) in all iterations. So, the total perturbation is, in a sense, controlled. To study the impact of more uncontrolled noise across mini-batches of training, we investigate the effect of using a random value of \u03c3 in each mini-batch iteration, which causes the perturbation to vary across iterations. We select a value of \u03c3 uniformly in [0.01, 1], and accordingly generate Gaussian noise to perturb the teacher outputs. The teacher and student network remain the same as in Section 4.3. We observed an improvement (decrease) in error rate of 2.86% over the baseline reported in Section 4.3. While this is lesser than the best performance we got using a fixed \u03c3 (a best decrease in error rate of 3.26% when \u03c3 = 0.9 in Table 3), the healthy improvement in performance still points to the conclusion that even when there is limited budget/bandwidth for experimenting with various noise levels, using random noise levels can be beneficial in training the student network. Learning from Multiple Teachers: In Section 3, we described the proposed logit perturbation method to be analogous to learning from multiple teachers, where the noise dictates the amount of diversity between the teachers. In this section, we extend this perspective to this work by experimenting with two teachers on the same student to study their effect on the student\u2019s performance. The two teachers we use are: (i) Network-inNetwork model (Teacher1), as described in Section 4.3 and (ii) A modified version of Alexnet [16](Teacher2). The configuration of Teacher2 is: [C5(S1P2)@96MP3(S2)]- [C5(S1P2)@256-MP3(S2)]- [C3(S1P1)@384][C3(S1P1)@384]- [C3(S1P1)@256-MP3(S2)]- FC2048D0.5- FC2048- D0.5- FC10. The student network(modified LeNet) is the same as described in Section 4.3.\nThe baseline error rate that we obtained in our experiments of Teacher1 was 8.4%, and that of Teacher2 was 13.99%. To let the student learn from two teachers via the baseline logit regression method (We did not use noisy teachers here, since the objective is to study learning from multiple teachers as is; we used the baseline algorithm as in Section 3.2), we took the geometric mean of the logits from the two teacher networks and used that as the target logit value for the student. We obtained a best performance (error rate) of 20.44% for the student trained in this manner, whereas the same same student trained using only Teacher1 gave 21.94% error rate (as in Section 4.3), and using only Teacher2 gave 22.62%. This supports our hypothesis that learning from multiple teachers (of which noisy teachers are a special case) results in a positive effect on the student."}, {"heading": "5.5. Runtime Compression of Shallow Students", "text": "Considering that the key application of the proposed idea in deep learning applications is in model compression, we study the issue of compression in this section. Recent methods, especially in the last 2 years, have done very well in reducing storage complexity (e.g. [9]). However, the compressed networks, in most such cases, need to be decompressed before deployment in real-time, thus resulting in a higher runtime complexity. Evidently, using shallow student networks has the potential to compress deep networks from both perspectives of storage and runtime complexity. The reduced storage complexity of shallow student networks is evident due to the reduced depth (similarly, reduced train-time complexity is also evident given a pre-trained teacher). In particular, we study the runtime complexity of shallow student networks in this section.\nWe build three shallow networks of depth 41, which require significantly lower computations than standard deep networks used today, for the CIFAR-10 dataset. We study the performance of these shallow networks using the logit regression method (standard teacher) and proposed logit perturbation method (noisy teacher) along with their runtime compression performance.\nTeacher Network: We use the same teacher as described in Section 4.3, which has an error rate of 8.4% (or accuracy of 91.6%) on CIFAR-10. This network requires 223 million multiplications in each forward pass (for convenience of analysis, without loss in generality, we consider multiplications as our measure of computations).\nStudent1: Its architecture is: [C5(S1P2)@64MP2(S2)]- [C5(S1P2)@112-MP2(S2)]- [C3(S1P1)@128MP2(S2)]- FC1024- FC10. Total computations required\n1We used student networks of depth 3 until now, and use student networks of depth 4 in this section to highlight the issue of compression.\nfor this student network is 61 million/forward pass. The computations in each layers are: 5-46-8-2 million/forward pass. Hence, runtime compression obtained w.r.t the teacher = 223/61 = 3.66.\nStudent2: The architecture of this student is: [C5(S1P2)@32-MP2(S2)]- [C5(S1P2)@32-MP2(S2)][C3(S1P1)@64-MP2(S2)]- FC1024- FC10. Total computations required for this network are 11.2 million/forward pass. Layerwise computations are: 2.5-6.5-1.2-1 million/forward pass. Hence, its runtime compression = 223/11.2 = 19.91.\nStudent3: In this student, we reduced the number of filters in the first layer drastically. The architecture is: [C5(S1P2)@16-MP2(S2)]- [C5(S1P2)@32-MP2(S2)][C3(S1P1)@64-MP2(S2)]- FC1024- FC10. Hence, the number of computations in each layer are: 1.25-3.25-1.2- 1 million/forward pass, with the total computations hence reduced to 6.7 million/forward pass. The run-time compression of this network w.r.t. teacher = 223/6.7 = 33.28.\nWe compared the performance obtained from our method and the baseline (logit regression) method for these student networks on CIFAR-10. For all the student networks trained using the proposed method, \u03b1 was set to 0.5, and the noise levels were varied between 0.1 and 1.0, and the best result was chosen. The results are shown in Figure 3 and Table 6. Evidently, as the amount of computations decrease, the performance also starts to decrease. Yet in all cases, the proposed method provides superior performance to the baseline logit regression method. While shallow networks expectedly do not perform as well as the original deep teacher network, we highlight that the use of noisy teachers while training shallow student networks provide a significant boost to performance, while maintaining high levels of compression both in terms of storage and runtime."}, {"heading": "6. Conclusions", "text": "In this work, we presented a method based on teacherstudent learning framework for deep model compression, considering both storage and runtime complexities. Our noise-based regularization method helped the shallow student model to do significantly better than the baseline teacher-student algorithm. The proposed method can be viewed also as simulating learning from multiple teachers, thus helping student models to get closer to the teacher\u2019s performance."}], "references": [{"title": "Do deep nets really need to be deep? In Advances in neural information processing", "author": ["J. Ba", "R. Caruana"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Training with noise is equivalent to tikhonov regularization", "author": ["C.M. Bishop"], "venue": "Neural computation, 7(1):108\u2013116,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Model compression", "author": ["C. Bucilu", "R. Caruana", "A. Niculescu-Mizil"], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535\u2013541. ACM,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "CoRR, abs/1504.04788,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "N. de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["E.L. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems, pages 1269\u20131277,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Y. Gong", "L. Liu", "M. Yang", "L. Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A.C. Courville", "Y. Bengio"], "venue": "ICML (3), 28:1319\u20131327,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "CoRR, abs/1510.00149, 2,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Advances in Neural Information Processing Systems, pages 1135\u20131143,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["K. Jarrett", "K. Kavukcuoglu", "Y. Lecun"], "venue": "IEEE 12th International Conference on Computer Vision,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["B.B. Le Cun", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Advances in neural information processing systems. Citeseer,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1990}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "Lazy evaluation of convolutional filters", "author": ["S. Leroux", "S. Bohez", "C. De Boom", "E. De Coninck", "T. Verbelen", "B. Vankeirsbilck", "P. Simoens", "B. Dhoedt"], "venue": "arXiv preprint arXiv:1605.08543,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "arXiv preprint arXiv:1312.4400,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Compressing deep neural networks using a rank-constrained topology", "author": ["P. Nakkiran", "R. Alvarez", "R. Prabhavalkar", "C. Parada"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Tensorizing neural networks", "author": ["A. Novikov", "D. Podoprikhin", "A. Osokin", "D.P. Vetrov"], "venue": "Advances in Neural Information Processing Systems, pages 442\u2013450,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Fitnets: Hints for thin deep nets", "author": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.6550,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["T.N. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6655\u20136659. IEEE,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Creating artificial neural networks that generalize", "author": ["J. Sietsma", "R.J. Dow"], "venue": "Neural networks, 4(1):67\u201379,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1991}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Compression of deep neural networks on the fly", "author": ["G. Souli\u00e9", "V. Gripon", "M. Robert"], "venue": "arXiv preprint arXiv:1509.08745,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Data-free parameter pruning for deep neural networks", "author": ["S. Srinivas", "R.V. Babu"], "venue": "arXiv preprint arXiv:1507.06149,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20139,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Do deep convolutional nets really need to be deep (or even convolutional)", "author": ["G. Urban", "K.J. Geras", "S.E. Kahou", "O. Aslan", "S. Wang", "R. Caruana", "A. Mohamed", "M. Philipose", "M. Richardson"], "venue": "arXiv preprint arXiv:1603.05691,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "[16] in 2012, deep learning has become very popular and replaced classical computer vision in a wide variety of real-world applications.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "With the introduction of networks like GoogLeNet [31], VGGNet [27] and ResNets [11] in recent times, networks are becoming deeper and deeper.", "startOffset": 49, "endOffset": 53}, {"referenceID": 26, "context": "With the introduction of networks like GoogLeNet [31], VGGNet [27] and ResNets [11] in recent times, networks are becoming deeper and deeper.", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "With the introduction of networks like GoogLeNet [31], VGGNet [27] and ResNets [11] in recent times, networks are becoming deeper and deeper.", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "Impressive progress has been made to train very deep networks with the invention of newer methods (as in [11]), as well as the relatively easier availability of computational resources today.", "startOffset": 105, "endOffset": 109}, {"referenceID": 26, "context": "the VGGNet network [27] requires 540 MB of storage, which is not suitable for a mobile device).", "startOffset": 19, "endOffset": 23}, {"referenceID": 9, "context": "in [10], running large networks on mobile devices increases memory access, which in turn consumes considerable battery power.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "One approach among existing methods, which is not explored well enough, which can address this gap is the teacher-student approach ([1][3][12]) to deep model compression.", "startOffset": 132, "endOffset": 135}, {"referenceID": 2, "context": "One approach among existing methods, which is not explored well enough, which can address this gap is the teacher-student approach ([1][3][12]) to deep model compression.", "startOffset": 135, "endOffset": 138}, {"referenceID": 11, "context": "One approach among existing methods, which is not explored well enough, which can address this gap is the teacher-student approach ([1][3][12]) to deep model compression.", "startOffset": 138, "endOffset": 142}, {"referenceID": 3, "context": "[4] proposed a HashedNets model which used a low-cost hash function to group weights into hash buckets to share parameters where each hash bucket denotes a single parameter.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] used k-means clustering to quantize the weights in fully connected layers and achieved upto 24x compression rate for their CNN network with only 1% loss on accuracy on the ImageNet challenge.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "[28] used a regularization technique instead to coarsely quantize the weights of fully connected layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] proposed to reduce the number of parameters by pruning weights which are below a threshold after the network is trained.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "They extended this work by using Huffman encoding [9] to reduce the number of parameters further.", "startOffset": 50, "endOffset": 53}, {"referenceID": 18, "context": "[19] aimed at reducing computations by ignoring the convolutional filters which produce least activational strength.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Srinivas and Babu [29] explored the redundancy among neurons, and proposed a data-free pruning methodology to remove redundant neurons.", "startOffset": 18, "endOffset": 22}, {"referenceID": 2, "context": "[3] where they created synthetic data by labeling unlabeled data with a teacher model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Ba and Caruana [1] proposed to train the student model by mimicking the logit values of the teacher model.", "startOffset": 15, "endOffset": 18}, {"referenceID": 23, "context": "[24] extended this work by using intermediate hidden layer outputs as target values for training a student model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] generalized this method by introducing a temperature variable in the softmax function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5], Sainath et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "[25] and Nakkiran et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] all adopted low-rank decomposition to compress the weights in different layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] converted the dense weight matrices of the fully connected layers to the Tensor Train format, such that the number of parameters is reduced by huge factor while preserving the expressive power of the layer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] that use matrix factorization attempt to speed up operations, for instance in the convolutional layer, but do not provide a holistic solution to the issues of complexity mentioned so far.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12]: \u2022 The \u2018dark knowledge\u2019 present in the teacher outputs works as a powerful target-cum-regularizer for the student model, as it provides soft targets that share helpful information.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Ba and Caruana [1] proposed a method to train the student directly on the log probability values z, also called logits, which is the output of the layer before softmax activation.", "startOffset": 15, "endOffset": 18}, {"referenceID": 11, "context": "in [12], as well as Ba and Caruana in [1].", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "in [12], as well as Ba and Caruana in [1].", "startOffset": 38, "endOffset": 41}, {"referenceID": 25, "context": "It has been long established that noisy data in training helps to regularize a model (one of the earliest works showing this was [26]).", "startOffset": 129, "endOffset": 133}, {"referenceID": 1, "context": "Bishop showed in [2] that adding an L2 regularization term in the loss function is equivalent to adding Gaussian noise in the input data.", "startOffset": 17, "endOffset": 20}, {"referenceID": 17, "context": "We evaluated our method on three benchmark datasets: MNIST [18] and SVHN [22] for digit recognition, and CIFAR-10 [15] for natural image recognition - each of which is described below.", "startOffset": 59, "endOffset": 63}, {"referenceID": 21, "context": "We evaluated our method on three benchmark datasets: MNIST [18] and SVHN [22] for digit recognition, and CIFAR-10 [15] for natural image recognition - each of which is described below.", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": "We evaluated our method on three benchmark datasets: MNIST [18] and SVHN [22] for digit recognition, and CIFAR-10 [15] for natural image recognition - each of which is described below.", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "ADAM [14], which combines the ideas of momentum and adaptive learning rates, is used to adjust the learning rate in each iteration of SGD.", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "For experiments in this section, we compare the performance of our method against the baseline performance of the teacherstudent method as proposed by Ba and Caruana [1].", "startOffset": 166, "endOffset": 169}, {"referenceID": 11, "context": "[12]; however, we found it non-trivial to identify the temperature at which their method\u2019s performance is maximized.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "In fact, we found it to give worse performance than [1] in our studies, and hence used [1] as the baseline comparison.", "startOffset": 52, "endOffset": 55}, {"referenceID": 0, "context": "In fact, we found it to give worse performance than [1] in our studies, and hence used [1] as the baseline comparison.", "startOffset": 87, "endOffset": 90}, {"referenceID": 17, "context": "MNIST [18] is a popular dataset for handwritten digit recognition with 10 classes (0-9).", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": "Teacher Network: We use a modified network of LeNet [17] as the teacher network on this dataset.", "startOffset": 52, "endOffset": 56}, {"referenceID": 11, "context": "[12], as our student network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "The Street View House Numbers (SVHN) [22] is a realworld image dataset containing cropped digits in house numbers from Google Street View images.", "startOffset": 37, "endOffset": 41}, {"referenceID": 7, "context": "As in earlier work [8] that have used this dataset, we selected 400 samples per category from the training set and 200 samples per category from the additional set to constitute a validation set of 6000 samples (which is used to decide if the training has converged sufficiently).", "startOffset": 19, "endOffset": 22}, {"referenceID": 12, "context": "We also preprocessed all the data using local contrast normalization as in [13].", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "Teacher Network: We used the Network-in-Network [20] model as our teacher network for this dataset.", "startOffset": 48, "endOffset": 52}, {"referenceID": 31, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "CIFAR-10 [15] is a popular dataset for small-scale image recognition.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "4), we also compared the performance of the proposed method against another often used regularization method: DropOut [30].", "startOffset": 118, "endOffset": 122}, {"referenceID": 15, "context": "3 and (ii) A modified version of Alexnet [16](Teacher2).", "startOffset": 41, "endOffset": 45}, {"referenceID": 8, "context": "[9]).", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "The remarkable successes of deep learning models across various applications have resulted in the design of deeper networks that can solve complex problems. However, the increasing depth of such models also results in a higher storage and runtime complexity, which restricts the deployability of such very deep models on mobile and portable devices, which have limited storage and battery capacity. While many methods have been proposed for deep model compression in recent years, almost all of them have focused on reducing storage complexity. In this work, we extend the teacher-student framework for deep model compression, since it has the potential to address runtime and train time complexity too. We propose a simple methodology to include a noise-based regularizer while training the student from the teacher, which provides a healthy improvement in the performance of the student network. Our experiments on the CIFAR-10, SVHN and MNIST datasets show promising improvement, with the best performance on the CIFAR-10 dataset. We also conduct a comprehensive empirical evaluation of the proposed method under related settings on the CIFAR-10 dataset to show the promise of the proposed approach.", "creator": "LaTeX with hyperref package"}}}