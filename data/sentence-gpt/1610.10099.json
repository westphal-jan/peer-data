{"id": "1610.10099", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Neural Machine Translation in Linear Time", "abstract": "We present a neural architecture for sequence processing. The ByteNet is a stack of two dilated convolutional neural networks, one to encode the source sequence and one to decode the target sequence, where the target network unfolds dynamically to generate variable length outputs. The ByteNet has two core properties: it runs in time that is linear in the length of the sequences and it preserves the sequences' temporal resolution. This algorithm performs a full run at its current state at the state of the memory pool, with the initial state remaining. A second core property is the entropy of the sample. The ByteNet can be configured as a bit of memory, if it is smaller. The ByteNet allows us to implement the byteNet's protocol for each of the input inputs to store values for each input. When the byteNet is compressed we can process the data as long as the value of the input is the same value, or in the case of a binary file that contains the specified number of digits, the output output should be the same value. The ByteNet can be set to the output value of the input and then a bit of memory will be stored as a bit of memory. This ensures that all input values can be stored as long as one can be stored in the same size.\n\n\nThe program takes a short amount of time to initialize the machine. The ByteNet then generates a new program, and can then run in time to generate the output output. When a program is run in time it will start running. The output output can be read only on a single page and the output will also be read and read on that same page. As the program runs, the machine is run using a random number generator to determine which data the output will contain. This process is complete by adding a new data set. It then prints a data set with a read/write interval. It then prints the input and then outputs the output to the machine, the output to the machine.\nIn the program, the machine is run running in time. Once it reaches its last step, it will begin executing the new program. After running the program and running in time, the machine will start running again. As the machine reaches its last step, it will stop executing the new program. The machine is run executing the new program and its output will be read only on a single page. The machine can then run again, and it will continue executing the new program. When a process is run in time, the machine will start running again. When a process is", "histories": [["v1", "Mon, 31 Oct 2016 19:56:39 GMT  (5893kb,D)", "http://arxiv.org/abs/1610.10099v1", "11 pages"], ["v2", "Wed, 15 Mar 2017 18:09:51 GMT  (12174kb,D)", "http://arxiv.org/abs/1610.10099v2", "9 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["nal kalchbrenner", "lasse espeholt", "karen simonyan", "aaron van den oord", "alex graves", "koray kavukcuoglu"], "accepted": false, "id": "1610.10099"}, "pdf": {"name": "1610.10099.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Translation in Linear Time", "authors": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "A\u00e4ron van den Oord", "Alex Graves", "Koray Kavukcuoglu"], "emails": ["nalk@google.com", "lespeholt@google.com", "simonyan@google.com", "avdnoord@google.com", "gravesa@google.com", "korayk@google.com"], "sections": [{"heading": "1 Introduction", "text": "In neural language modelling, a neural network estimates a distribution over sequences of words or characters that belong to a given language (Bengio et al., 2003). In neural machine translation, the network estimates a distribution over sequences in the target language conditioned on a given sequence in the source language. In the latter case the network can be thought of as composed of two sub-networks, a source network that processes the source sequence into a representation and a target network that uses the representation of the source to generate the target sequence (Kalchbrenner and Blunsom, 2013)\nRecurrent neural networks (RNN) are powerful sequence models (Hochreiter and Schmidhuber, 1997) and are widely used in language modelling (Mikolov et al., 2010), yet they have a potential drawback. RNNs have an inherently serial structure that prevents them from being run in parallel along the sequence length. Forward and backward signals in a RNN also need to traverse the full distance of the serial path to reach from one point to another in the sequence. The larger the distance the harder it is to learn dependencies between the points (Hochreiter et al., 2001).\nA number of neural architectures have been proposed for modelling translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Kalchbrenner et al., 2016a; Kaiser and Bengio, 2016). These networks either have running time that is super linear in the length of the source and target sequences, or they process the source sequence into a constant size representation, burdening the model with a memorization step. Both of these drawbacks grow more severe as the length of the sequences increases.\nWe present a neural translation model, the ByteNet, and a neural language model, the ByteNet Decoder, that aim at addressing these drawbacks. The ByteNet uses convolutional neural networks with dilation for both the source network and the target network. The ByteNet connects the source and target networks via stacking and unfolds the target network dynamically to generate variable length output sequences. We view the ByteNet as an instance of a wider family of sequence-mapping architectures that stack the sub-networks and use dynamic unfolding. The sub-networks themselves may be convolutional or recurrent.\nar X\niv :1\n61 0.\n10 09\n9v 1\n[ cs\n.C L\n] 3\n1 O\nct 2\n01 6\nThe ByteNet with recurrent sub-networks may be viewed as a strict generalization of the RNN Enc-Dec network (Sutskever et al., 2014; Cho et al., 2014) (Sect. 4). The ByteNet Decoder has the same architecture as the target network in the ByteNet. In contrast to neural language models based on RNNs (Mikolov et al., 2010) or on feed-forward networks (Bengio et al., 2003; Arisoy et al., 2012), the ByteNet Decoder is based on a novel convolutional structure designed to capture a very long range of past inputs.\nThe ByteNet has a number of beneficial computational and learning properties. From a computational perspective, the network has a running time that is linear in the length of the source and target sequences (up to a constant c \u2248 log d where d is the size of the desired dependency field). The computation in the source network during training and decoding and in the target network during training can also be run efficiently in parallel along the strings \u2013 by definition this is not possible for a target network during decoding (Sect. 2). From a learning perspective, the representation of the source string in the ByteNet is resolution preserving ; the representation sidesteps the need for memorization and allows for maximal bandwidth between the source and target networks. In addition, the distance traversed by forward and backward signals between any input and output tokens in the networks corresponds to the fixed depth of the networks and is largely independent of the distance between the tokens. Dependencies over large distances are connected by short paths and can be learnt more easily.\nWe deploy ByteNets on raw sequences of characters. We evaluate the ByteNet Decoder on the Hutter Prize Wikipedia task; the model achieves 1.33 bits/character showing that the convolutional language model is able to outperform the previous best results obtained with recurrent neural networks. Furthermore, we evaluate the ByteNet on raw character-level machine translation on the English-German WMT benchmark. The ByteNet achieves a score of 18.9 and 21.7 BLEU points on, respectively, the 2014 and the 2015 test sets; these results approach the best results obtained with other neural translation models that have quadratic running time (Chung et al., 2016b; Wu et al., 2016a). We use gradient-based visualization (Simonyan et al., 2013) to reveal the latent structure that arises between the source and target sequences in the ByteNet. We find the structure to mirror the expected word alignments between the source and target sequences."}, {"heading": "2 Neural Translation Model", "text": "Given a string s from a source language, a neural translation model estimates a distribution p(t|s) over strings t of a target language. The distribution indicates the probability of a\nstring t being a translation of s. A product of conditionals over the tokens in the target t = t0, ..., tN leads to a tractable formulation of the distribution:\np(t|s) = N\u220f i=0 p(ti|t<i, s) (1)\nEach conditional factor expresses complex and long-range dependencies among the source and target tokens. The strings are usually sentences of the respective languages; the tokens are words or, as in the present case, characters. The network that models p(t|s) is composed of two sub-networks, a source network that processes the source string into a representation and a target network that uses the source representation to generate the target string (Kalchbrenner and Blunsom, 2013). The target network functions as a language model for the target language.\nA neural translation model has some basic properties. The target network is autoregressive in the target tokens and the network is sensitive to the ordering of the tokens in the source and target strings. It is also useful for the model to be able to assign a non-zero probability to any string in the target language and retain an open vocabulary."}, {"heading": "2.1 Desiderata", "text": "Beyond these basic properties the definition of a neural translation model does not determine a unique neural architecture, so we aim at identifying some desiderata. (i) The running time of the network should be linear in the length of the source and target strings. This is more pressing the longer the strings or when using characters as tokens. The use of operations that run in parallel along the sequence length can also be beneficial for reducing computation time. (ii) The size of the source representation should be linear in the length of the source string, i.e. it should be resolution preserving, and not have constant size. This is to avoid burdening the model with an additional memorization step before translation. In more general terms, the size of a representation should be proportional to the amount of information it represents or predicts. A related desideratum concerns the path traversed by forward and backward signals in the network between a (source or target) input token and a predicted output token. Shorter paths whose length is decoupled from the sequence distance between the two tokens have the potential to better propagate the signals (Hochreiter et al., 2001) and to let the network learn long-range dependencies more easily."}, {"heading": "3 ByteNet", "text": "We aim at building neural language and translation models that capture the desiderata set out in Sect. 2.1. The proposed ByteNet architecture is composed of a target network that is stacked on a source network and generates variable-length outputs via dynamic unfolding. The target network, referred to as the ByteNet Decoder, is a language model that is formed of one-dimensional convolutional layers that use dilation (Sect. 3.3) and are masked (Sect. 3.2). The source network processes the source string into a representation and is formed of one-dimensional convolutional layers that use dilation but are not masked. Figure 1 depicts the two networks and their combination in the ByteNet."}, {"heading": "3.1 Dynamic Unfolding", "text": "To accommodate source and target sequences of different lengths, the ByteNet uses dynamic unfolding. The source network builds a representation that has the same width as the source\nsequence. At each step the target network takes as input the corresponding column of the source representation until the target network produces the end-of-sequence symbol. The source representation is zero-padded on the fly: if the target network produces symbols beyond the length of the source sequence, the corresponding conditioning column is set to zero. In the latter case the predictions of the target network are conditioned on source and target representations from previous steps. Figure 2 represents the dynamic unfolding process."}, {"heading": "3.2 Masked One-dimensional Convolutions", "text": "Given a target string t = t0, ..., tn the target network embeds each of the first n tokens t0, ..., tn\u22121 via a look-up table (the n tokens t1, ..., tn serve as targets for the predictions). The resulting embeddings are concatenated into a tensor of size 1\u00d7 n\u00d7 2d where d is the number of inner channels in the network. The target network applies masked one-dimensional convolutions (van den Oord et al., 2016b) to the embedding tensor that have a masked kernel of size k. The masking ensures that information from future tokens does not affect the prediction of the current token. The operation can be implemented either by zeroing out some of the weights on a wider kernel of size 2k \u2212 1 or by padding the output map."}, {"heading": "3.3 Dilation", "text": "The masked convolutions use dilation to increase the receptive field of the target network (Chen et al., 2014; Yu and Koltun, 2015). Dilation makes the receptive field grow exponentially in terms of the depth of the networks, as opposed to linearly. We use a dilation scheme whereby the dilation rates are doubled every layer up to a maximum rate r (for our experiments r = 16). The scheme is repeated multiple times in the network always starting from a dilation rate of 1 (van den Oord et al., 2016a; Kalchbrenner et al., 2016b)."}, {"heading": "3.4 Residual Blocks", "text": "Each layer is wrapped in a residual block that contains additional convolutional layers with filters of size 1 (He et al., 2015). We adopt two variants of the residual blocks, one with ReLUs, which is used in the machine translation experiments, and one with Multiplicative Units (Kalchbrenner et al., 2016b), which is used in the language modelling experiments. Figure 3 diagrams the two variants of the blocks."}, {"heading": "3.5 Sub-Batch Normalization", "text": "We introduce a modification to Batch Normalization (BN) (Ioffe and Szegedy, 2015) in order to make it applicable to target networks and decoders. Standard BN computes the mean and variance of the activations of a given convolutional layer along the batch, height, and width dimensions. In a decoder, the standard BN operation at training time would average\nactivations along all the tokens in the input target sequence, and the BN output for each target token would incorporate the information about the tokens that follow it. This breaks the conditioning structure of Eq. 1, since the succeeding tokens are yet to be predicted.\nTo circumvent this issue, we present Sub-Batch Normalization (SubBN). It is a variant of BN, where a batch of training samples is split into two parts: the main batch and the auxiliary batch. For each layer, the mean and variance of its activations are computed over the auxiliary batch, but are used for the batch normalization of the main batch. At the same time, the loss is computed only on the predictions of the main batch, ignoring the predictions from the auxiliary batch.\n3.6 Bag of Character n-Grams\nThe tokens that we adopt correspond to characters in the input sequences. An efficient way to increase the capacity of the models is to use input embeddings not just for single tokens, but also for n-grams of adjacent tokens. At each position we sum the embeddings of the respective n-grams for 1 \u2264 n \u2264 5 component-wise into a single vector. Although the portion of seen n-grams decreases as the value of n increases \u2013 a cutoff threshold is chosen for each n \u2013 all characters (n-grams for n = 1) are seen during training. This fallback structure provided by the bag of character n-grams guarantees that at any position the input given to the network is always well defined. The length of the sequences corresponds to the number of characters and does not change when using bags of n-grams."}, {"heading": "4 Model Comparison", "text": "In this section we analyze the properties of various previously and currently introduced neural translation models. For the sake of a more complete analysis, we also consider two recurrent variants in the ByteNet family of architectures, which we do not evaluate in the experiments."}, {"heading": "4.1 Recurrent ByteNets", "text": "The ByteNet is composed of two stacked source and target networks where the top network dynamically adapts to the output length. This way of combining source and target networks is not tied to the networks being strictly convolutional. We may consider two variants of the ByteNet that use recurrent networks for one or both of the sub-networks (see Figure 4). The first variant replaces the convolutional target network with a recurrent one that is similarly stacked and dynamically unfolded. The second variant replaces the convolutional source network with a recurrent network, namely a bidirectional RNN. The target RNN is placed on top of the bidirectional source RNN. We can see that the RNN Enc-Dec network (Sutskever et al., 2014; Cho et al., 2014) is a Recurrent ByteNet where all connections between source and target \u2013 except for the first one that connects s0 and t0 \u2013 have been severed. The Recurrent ByteNet is thus a generalization of the RNN Enc-Dec and, modulo the type of sequential architecture, so is the ByteNet."}, {"heading": "4.2 Comparison of Properties", "text": "In our comparison we consider the following neural translation models: the Recurrent Continuous Translation Model (RCTM) 1 and 2 (Kalchbrenner and Blunsom, 2013); the RNN Enc-Dec (Sutskever et al., 2014; Cho et al., 2014); the RNN Enc-Dec Att with the attentional pooling mechanism (Bahdanau et al., 2014) of which there are a few variations (Luong et al., 2015; Chung et al., 2016a); the Grid LSTM translation model (Kalchbrenner et al., 2016a) that uses a multi-dimensional architecture; the Extended Neural GPU model (Kaiser and Bengio, 2016) that has a convolutional RNN architecture; the ByteNet and the two Recurrent ByteNet variants.\nThe two grounds of comparison are the desiderata (i) and (ii) set out in Sect 2.1. We separate the computation time desideratum (i) into three columns. The first column indicates the time complexity of the network as a function of the length of the sequences and is denoted by Time. The other two columns NetS and NetT indicate, respectively, whether the source and the target network uses a convolutional structure (CNN) or a recurrent one (RNN); a CNN structure has the advantage that it can be run in parallel along the length of the sequence. We also break the learning desideratum (ii) into three columns. The first is denoted by RP and indicates whether the source representation in the network is resolution preserving. The second PathS column corresponds to the length in layer steps of the shortest path between a source token and any output target token. Similarly, the third PathT column corresponds to the length of the shortest path between an input target token and any output target token. Shorter paths lead to better forward and backward signal propagation.\nTable 1 summarizes the properties of the models. The ByteNet, the Recurrent ByteNets and the RNN Enc-Dec are the only networks that have linear running time (up to the constant c). The RNN Enc-Dec, however, does not preserve the source sequence resolution, a feature that aggravates learning for long sequences such as those in character-level machine translation (Luong and Manning, 2016). The RCTM 2, the RNN Enc-Dec Att, the Grid LSTM and the Extended Neural GPU do preserve the resolution, but at a cost of a quadratic running time. The ByteNet stands out also for its Path properties. The dilated structure of the convolutions connects any two source or target tokens in the sequences by way of a small number of network layers corresponding to the depth of the source or target networks. For character sequences where learning long-range dependencies is important, paths that are sub-linear in the distance are advantageous."}, {"heading": "5 Character Prediction", "text": "We first evaluate the ByteNet Decoder separately on a character-level language modelling benchmark. We use the Hutter Prize version of the Wikipedia dataset and follow the standard split where the first 90 million bytes are used for training, the next 5 million bytes are used for validation and the last 5 million bytes are used for testing (Chung et al., 2015). The total number of characters in the vocabulary is 205.\nThe ByteNet Decoder that we use for the result has 25 residual blocks split into five sets of five blocks each; for the five blocks in each set the dilation rates are, respectively, 1,2,4,8 and 16. The masked kernel has size 3. This gives a receptive field of 315 characters. The number of hidden units d is 892. For this task we use residual multiplicative blocks and Sub-BN (Fig. 3 Right); we do not use bags of character n-grams for the inputs. For the optimization we use Adam (Kingma and Ba, 2014) with a learning rate of 10\u22122 and a weight decay term of 10\u22125. We do not reduce the learning rate during training. At each step we sample a batch of sequences of 515 characters each, use the first 315 characters as context and predict only the latter 200 characters.\nTable 2 lists recent results of various neural sequence models on the Wikipedia dataset. All the results except for the ByteNet result are obtained using some variant of the LSTM recurrent neural network (Hochreiter and Schmidhuber, 1997). The ByteNet Decoder achieves 1.33 bits/character on the test set."}, {"heading": "6 Character-Level Machine Translation", "text": "We evaluate the full ByteNet on the WMT English to German translation task. We use NewsTest 2013 for development and NewsTest 2014 and 2015 for testing. The English and German strings are encoded as sequences of characters; no explicit segmentation into words or morphemes is applied to the strings. The outputs of the network are strings of characters in the target language. There are about 140 characters in each of the languages.\nThe ByteNet used in the experiments has 15 residual blocks in the source network and 15 residual blocks in the target network. As in the ByteNet Decoder, the residual blocks are arranged in sets of five with corresponding dilation rates of 1,2,4,8 and 16. For this task we use residual blocks with ReLUs and Sub-BN (Fig. 3 Left). The number of hidden\nunits d is 892. The size of the kernel in the source network is 1\u00d7 5, whereas the size of the masked kernel in the target network is 1\u00d7 3. We use bags of character n-grams as additional embeddings at the source and target inputs: for n > 2 we prune all n-grams that occur less than 500 times. For the optimization we use Adam with a learning rate of 0.003.\nEach sentence is padded with special characters to the nearest greater multiple of 25. Each pair of sentences is mapped to a bucket based on the pair of padded lengths for efficient batching during training. We find that Sub-BN learns bucket-specific statistics that cannot easily be merged across buckets. We circumvent this issue by simply searching over possible target intervals as a first step during decoding with a beam search; each hypothesis uses Sub-BN statistics that are specific to a target length interval. The hypotheses are ranked according to the average likelihood of each character.\nTable 3 contains the results of the experiments. We note that the lengths of the translations generated by the ByteNet are especially close to the lengths of the reference translations and do not tend to be too short; the brevity penalty in the BLEU scores is 0.995 and 1.0 for the two test sets, respectively. We also note that the ByteNet architecture seems particularly apt for machine translation. The correlation coefficient between the lengths of sentences from different languages is often very high (Fig. 5), an aspect that is compatible with the resolution preserving property of the architecture.\nTable 4 contains some of the unaltered generated translations from the ByteNet that highlight reordering and other phenomena such as transliteration. The character-level aspect of the model makes post-processing unnecessary in principle. We further visualize the sensitivity of the ByteNet\u2019s predictions to specific source and target inputs. Figure 6 represents a heatmap of the magnitude of the gradients of source and target inputs with respect to the generated outputs. For visual clarity, we sum the gradients for all the characters that make up each\nword and normalize the values along each column. In contrast with the attentional pooling mechanism (Bahdanau et al., 2014), this general technique allows us to inspect not just dependencies of the outputs on the source inputs, but also dependencies of the outputs on previous target inputs, or on any other neural network layers."}, {"heading": "7 Conclusion", "text": "We have introduced the ByteNet, a neural translation model that has linear running time, decouples translation from memorization and has short signal propagation paths for tokens in sequences. We have shown that the ByteNet Decoder is a state-of-the-art character-level language model based on a convolutional neural network that significantly outperforms recurrent language models. We have also shown that the ByteNet generalizes the RNN EncDec architecture and achieves promising results for raw character-level machine translation while maintaining linear running time complexity. We have revealed the latent structure learnt by the ByteNet and found it to mirror the expected alignment between the tokens in the sentences."}], "references": [{"title": "Deep neural network language models", "author": ["Ebru Arisoy", "Tara N. Sainath", "Brian Kingsbury", "Bhuvana Ramabhadran"], "venue": "In Proceedings of the NAACL-HLT 2012 Workshop. Association for Computational Linguistics,", "citeRegEx": "Arisoy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arisoy et al\\.", "year": 2012}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["Liang-Chieh Chen", "George Papandreou", "Iasonas Kokkinos", "Kevin Murphy", "Alan L. Yuille"], "venue": "CoRR, abs/1412.7062,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "CoRR, abs/1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "Caglar G\u00fcl\u00e7ehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1502.02367,", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.01704,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Eu-bridge mt: Combined machine translation", "author": ["Markus Freitag", "Stephan Peitz", "Joern Wuebker", "Hermann Ney", "Matthias Huck", "Rico Sennrich", "Nadir Durrani", "Maria Nadejde", "Philip Williams", "Philipp Koehn", "Teresa Herrmann", "Eunah Cho", "Alex Waibel"], "venue": "In ACL 2014 Ninth Workshop on Statistical Machine Translation,", "citeRegEx": "Freitag et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Freitag et al\\.", "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi"], "venue": null, "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In ICML,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Can active memory replace attention", "author": ["Lukasz Kaiser", "Samy Bengio"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Kaiser and Bengio.,? \\Q2016\\E", "shortCiteRegEx": "Kaiser and Bengio.", "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kalchbrenner and Blunsom.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D. Manning"], "venue": null, "citeRegEx": "Luong and Manning.,? \\Q2016\\E", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "INTERSPEECH", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Recurrent memory array structures", "author": ["Kamil Rocki"], "venue": "arXiv preprint arXiv:1607.03085,", "citeRegEx": "Rocki.,? \\Q2016\\E", "shortCiteRegEx": "Rocki.", "year": 2016}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Wavenet: A generative model for raw audio", "author": ["Aaron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu"], "venue": "CoRR, abs/1609.03499,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Pixel recurrent neural networks", "author": ["A\u00e4ron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "venue": "In ICML,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Edinburgh\u2019s syntax-based systems at WMT 2015", "author": ["Philip Williams", "Rico Sennrich", "Maria Nadejde", "Matthias Huck", "Philipp Koehn"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Williams et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2015}, {"title": "On multiplicative integration with recurrent neural networks", "author": ["Yuhuai Wu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1606.06630,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["Fisher Yu", "Vladlen Koltun"], "venue": "CoRR, abs/1511.07122,", "citeRegEx": "Yu and Koltun.,? \\Q2015\\E", "shortCiteRegEx": "Yu and Koltun.", "year": 2015}, {"title": "Deep recurrent models with fast-forward connections for neural machine translation", "author": ["Jie Zhou", "Ying Cao", "Xuguang Wang", "Peng Li", "Wei Xu"], "venue": "arXiv preprint arXiv:1606.04199,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "In neural language modelling, a neural network estimates a distribution over sequences of words or characters that belong to a given language (Bengio et al., 2003).", "startOffset": 142, "endOffset": 163}, {"referenceID": 15, "context": "In the latter case the network can be thought of as composed of two sub-networks, a source network that processes the source sequence into a representation and a target network that uses the representation of the source to generate the target sequence (Kalchbrenner and Blunsom, 2013) Recurrent neural networks (RNN) are powerful sequence models (Hochreiter and Schmidhuber, 1997) and are widely used in language modelling (Mikolov et al.", "startOffset": 252, "endOffset": 284}, {"referenceID": 11, "context": "In the latter case the network can be thought of as composed of two sub-networks, a source network that processes the source sequence into a representation and a target network that uses the representation of the source to generate the target sequence (Kalchbrenner and Blunsom, 2013) Recurrent neural networks (RNN) are powerful sequence models (Hochreiter and Schmidhuber, 1997) and are widely used in language modelling (Mikolov et al.", "startOffset": 346, "endOffset": 380}, {"referenceID": 20, "context": "In the latter case the network can be thought of as composed of two sub-networks, a source network that processes the source sequence into a representation and a target network that uses the representation of the source to generate the target sequence (Kalchbrenner and Blunsom, 2013) Recurrent neural networks (RNN) are powerful sequence models (Hochreiter and Schmidhuber, 1997) and are widely used in language modelling (Mikolov et al., 2010), yet they have a potential drawback.", "startOffset": 423, "endOffset": 445}, {"referenceID": 12, "context": "The larger the distance the harder it is to learn dependencies between the points (Hochreiter et al., 2001).", "startOffset": 82, "endOffset": 107}, {"referenceID": 15, "context": "A number of neural architectures have been proposed for modelling translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Kalchbrenner et al., 2016a; Kaiser and Bengio, 2016).", "startOffset": 78, "endOffset": 228}, {"referenceID": 23, "context": "A number of neural architectures have been proposed for modelling translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Kalchbrenner et al., 2016a; Kaiser and Bengio, 2016).", "startOffset": 78, "endOffset": 228}, {"referenceID": 4, "context": "A number of neural architectures have been proposed for modelling translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Kalchbrenner et al., 2016a; Kaiser and Bengio, 2016).", "startOffset": 78, "endOffset": 228}, {"referenceID": 1, "context": "A number of neural architectures have been proposed for modelling translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Kalchbrenner et al., 2016a; Kaiser and Bengio, 2016).", "startOffset": 78, "endOffset": 228}, {"referenceID": 14, "context": "A number of neural architectures have been proposed for modelling translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Kalchbrenner et al., 2016a; Kaiser and Bengio, 2016).", "startOffset": 78, "endOffset": 228}, {"referenceID": 23, "context": "The ByteNet with recurrent sub-networks may be viewed as a strict generalization of the RNN Enc-Dec network (Sutskever et al., 2014; Cho et al., 2014) (Sect.", "startOffset": 108, "endOffset": 150}, {"referenceID": 4, "context": "The ByteNet with recurrent sub-networks may be viewed as a strict generalization of the RNN Enc-Dec network (Sutskever et al., 2014; Cho et al., 2014) (Sect.", "startOffset": 108, "endOffset": 150}, {"referenceID": 20, "context": "In contrast to neural language models based on RNNs (Mikolov et al., 2010) or on feed-forward networks (Bengio et al.", "startOffset": 52, "endOffset": 74}, {"referenceID": 2, "context": ", 2010) or on feed-forward networks (Bengio et al., 2003; Arisoy et al., 2012), the ByteNet Decoder is based on a novel convolutional structure designed to capture a very long range of past inputs.", "startOffset": 36, "endOffset": 78}, {"referenceID": 0, "context": ", 2010) or on feed-forward networks (Bengio et al., 2003; Arisoy et al., 2012), the ByteNet Decoder is based on a novel convolutional structure designed to capture a very long range of past inputs.", "startOffset": 36, "endOffset": 78}, {"referenceID": 22, "context": "We use gradient-based visualization (Simonyan et al., 2013) to reveal the latent structure that arises between the source and target sequences in the ByteNet.", "startOffset": 36, "endOffset": 59}, {"referenceID": 15, "context": "The network that models p(t|s) is composed of two sub-networks, a source network that processes the source string into a representation and a target network that uses the source representation to generate the target string (Kalchbrenner and Blunsom, 2013).", "startOffset": 223, "endOffset": 255}, {"referenceID": 12, "context": "Shorter paths whose length is decoupled from the sequence distance between the two tokens have the potential to better propagate the signals (Hochreiter et al., 2001) and to let the network learn long-range dependencies more easily.", "startOffset": 141, "endOffset": 166}, {"referenceID": 10, "context": "Figure 3: Left: Residual block with ReLUs (He et al., 2015) adapted for decoders.", "startOffset": 42, "endOffset": 59}, {"referenceID": 3, "context": "The masked convolutions use dilation to increase the receptive field of the target network (Chen et al., 2014; Yu and Koltun, 2015).", "startOffset": 91, "endOffset": 131}, {"referenceID": 28, "context": "The masked convolutions use dilation to increase the receptive field of the target network (Chen et al., 2014; Yu and Koltun, 2015).", "startOffset": 91, "endOffset": 131}, {"referenceID": 10, "context": "Each layer is wrapped in a residual block that contains additional convolutional layers with filters of size 1 (He et al., 2015).", "startOffset": 111, "endOffset": 128}, {"referenceID": 13, "context": "We introduce a modification to Batch Normalization (BN) (Ioffe and Szegedy, 2015) in order to make it applicable to target networks and decoders.", "startOffset": 56, "endOffset": 81}, {"referenceID": 23, "context": "We can see that the RNN Enc-Dec network (Sutskever et al., 2014; Cho et al., 2014) is a Recurrent ByteNet where all connections between source and target \u2013 except for the first one that connects s0 and t0 \u2013 have been severed.", "startOffset": 40, "endOffset": 82}, {"referenceID": 4, "context": "We can see that the RNN Enc-Dec network (Sutskever et al., 2014; Cho et al., 2014) is a Recurrent ByteNet where all connections between source and target \u2013 except for the first one that connects s0 and t0 \u2013 have been severed.", "startOffset": 40, "endOffset": 82}, {"referenceID": 15, "context": "In our comparison we consider the following neural translation models: the Recurrent Continuous Translation Model (RCTM) 1 and 2 (Kalchbrenner and Blunsom, 2013); the RNN Enc-Dec (Sutskever et al.", "startOffset": 129, "endOffset": 161}, {"referenceID": 23, "context": "In our comparison we consider the following neural translation models: the Recurrent Continuous Translation Model (RCTM) 1 and 2 (Kalchbrenner and Blunsom, 2013); the RNN Enc-Dec (Sutskever et al., 2014; Cho et al., 2014); the RNN Enc-Dec Att with the attentional pooling mechanism (Bahdanau et al.", "startOffset": 179, "endOffset": 221}, {"referenceID": 4, "context": "In our comparison we consider the following neural translation models: the Recurrent Continuous Translation Model (RCTM) 1 and 2 (Kalchbrenner and Blunsom, 2013); the RNN Enc-Dec (Sutskever et al., 2014; Cho et al., 2014); the RNN Enc-Dec Att with the attentional pooling mechanism (Bahdanau et al.", "startOffset": 179, "endOffset": 221}, {"referenceID": 1, "context": ", 2014); the RNN Enc-Dec Att with the attentional pooling mechanism (Bahdanau et al., 2014) of which there are a few variations (Luong et al.", "startOffset": 68, "endOffset": 91}, {"referenceID": 19, "context": ", 2014) of which there are a few variations (Luong et al., 2015; Chung et al., 2016a); the Grid LSTM translation model (Kalchbrenner et al.", "startOffset": 44, "endOffset": 85}, {"referenceID": 14, "context": ", 2016a) that uses a multi-dimensional architecture; the Extended Neural GPU model (Kaiser and Bengio, 2016) that has a convolutional RNN architecture; the ByteNet and the two Recurrent ByteNet variants.", "startOffset": 83, "endOffset": 108}, {"referenceID": 18, "context": "The RNN Enc-Dec, however, does not preserve the source sequence resolution, a feature that aggravates learning for long sequences such as those in character-level machine translation (Luong and Manning, 2016).", "startOffset": 183, "endOffset": 208}, {"referenceID": 5, "context": "We use the Hutter Prize version of the Wikipedia dataset and follow the standard split where the first 90 million bytes are used for training, the next 5 million bytes are used for validation and the last 5 million bytes are used for testing (Chung et al., 2015).", "startOffset": 242, "endOffset": 262}, {"referenceID": 9, "context": "Model Test Stacked LSTM (Graves, 2013) 1.", "startOffset": 24, "endOffset": 38}, {"referenceID": 5, "context": "67 GF-LSTM (Chung et al., 2015) 1.", "startOffset": 11, "endOffset": 31}, {"referenceID": 21, "context": "42 Recurrent Memory Array Structures (Rocki, 2016) 1.", "startOffset": 37, "endOffset": 50}, {"referenceID": 29, "context": "9 RNN Enc-Dec Att + deep (Zhou et al., 2016) 20.", "startOffset": 25, "endOffset": 44}, {"referenceID": 8, "context": "Result (1) is from (Freitag et al., 2014), result (2) is from (Williams et al.", "startOffset": 19, "endOffset": 41}, {"referenceID": 26, "context": ", 2014), result (2) is from (Williams et al., 2015), results (3) are from (Luong et al.", "startOffset": 28, "endOffset": 51}, {"referenceID": 19, "context": ", 2015), results (3) are from (Luong et al., 2015) and results (4) are from (Chung et al.", "startOffset": 30, "endOffset": 50}, {"referenceID": 17, "context": "For the optimization we use Adam (Kingma and Ba, 2014) with a learning rate of 10\u22122 and a weight decay term of 10\u22125.", "startOffset": 33, "endOffset": 54}, {"referenceID": 11, "context": "All the results except for the ByteNet result are obtained using some variant of the LSTM recurrent neural network (Hochreiter and Schmidhuber, 1997).", "startOffset": 115, "endOffset": 149}, {"referenceID": 1, "context": "In contrast with the attentional pooling mechanism (Bahdanau et al., 2014), this general technique allows us to inspect not just dependencies of the outputs on the source inputs, but also dependencies of the outputs on previous target inputs, or on any other neural network layers.", "startOffset": 51, "endOffset": 74}], "year": 2016, "abstractText": "We present a neural architecture for sequence processing. The ByteNet is a stack of two dilated convolutional neural networks, one to encode the source sequence and one to decode the target sequence, where the target network unfolds dynamically to generate variable length outputs. The ByteNet has two core properties: it runs in time that is linear in the length of the sequences and it preserves the sequences\u2019 temporal resolution. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent neural networks. The ByteNet also achieves a performance on raw character-level machine translation that approaches that of the best neural translation models that run in quadratic time. The implicit structure learnt by the ByteNet mirrors the expected alignments between the sequences.", "creator": "LaTeX with hyperref package"}}}