{"id": "1611.00027", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2015", "title": "CBAS: context based arabic stemmer", "abstract": "Arabic morphology encapsulates many valuable features such as word root. Arabic roots are being utilized for many tasks; the process of extracting a word root is referred to as stemming. Stemming is an essential part of most Natural Language Processing tasks, especially for derivative languages such as Arabic.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 28 Oct 2015 10:10:51 GMT  (241kb)", "http://arxiv.org/abs/1611.00027v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["mahmoud el-defrawy", "yasser el-sonbaty", "nahla a belal"], "accepted": false, "id": "1611.00027"}, "pdf": {"name": "1611.00027.pdf", "metadata": {"source": "CRF", "title": "CBAS: CONTEXT BASED ARABIC STEMMER", "authors": ["Mahmoud El-Defrawy", "Yasser El-Sonbaty", "Nahla A. Belal"], "emails": [], "sections": [{"heading": null, "text": "DOI : 10.5121/ijnlc.2015.4301 1\nArabic morphology encapsulates many valuable features such as word\u2019s root. Arabic roots are being utilized for many tasks; the process of extracting a word\u2019s root is referred to as stemming. Stemming is an essential part of most Natural Language Processing tasks, especially for derivative languages such as Arabic. However, stemming is faced with the problem of ambiguity, where two or more roots could be extracted from the same word. On the other hand, distributional semantics is a powerful co-occurrence model. It captures the meaning of a word based on its context. In this paper, a distributional semantics model utilizing Smoothed Pointwise Mutual Information (SPMI) is constructed to investigate its effectiveness on the stemming analysis task. It showed an accuracy of 81.5%, with a at least 9.4% improvement over other stemmers.\nKEYWORDS\nNatural Language Processing, Computational Linguistics, Text Analysis, Stemming\n1.INTRODUCTION\nNatural Languages (NLs) are the communication channels between humans. It allows conveying information, exchanging knowledge, and sharing ideas. For many years scientists studied Natural Languages and developed theories and rules that govern the use of Natural Languages, such as Grammar and Morphology. Natural Language Processing (NLP) is the intersection between linguistics, and Computational Science (CS) [1]. NLP allows utilizing linguistics to use Natural Languages as a way of communication with computational devices[1].\nThe association curve between linguistics and computational sciences has evolved over time. Machine Translation (MT) was one of the first NLP tasks in the 1950s; it began as translation from Russian to English [2]. The progress of MT was limited due to the complexity of linguistics rules, and low computation power at the time[1]. However, Chomsky\u2019s theory[3] of natural language\u2019s grammar formed the basis for the formation of Backus-Naur Form (BNF). BNF[4] notations are commonly used to represent Context Free Grammar (CFG). CFGs are used to systematically describe, and validate artificial languages, such as programming languages. Using CFGs to describe some aspects of Naturals Languages requires a non-trivial set of rules, which results in some ambiguity due to the unexpected rules\u2019 interactions.\nThe introduction of statistical methods gave some insights for reducing NLP ambiguity[1]. For example, the Probabilistic CFGs extends the traditional CGFs by deducing linguistic rules and assigning weights[5]. Rules and weights are statistically deduced from large annotated corpus. The noticeable improvement in MT sparks the research in NLP [1].\nNLP tends to work with large sets of data from various languages. It raises the need for defining a concise representation for the data while preserving as many of its features as possible. Concise representation is required mostly for any NLP task (word, sentence, or document levels). Stemming is a primary NLP task, and it contributes in many other NLP tasks [6]. Stemming is reducing a word to its basic form[7], while preserving its main characteristics. Many languages define linguistic rules for stemming but not with the same degree[8].\nDerivative languages are highly systematic, and highly supportive for stemming analysis. Most of the derivative languages share the property that complex forms are derived from basic ones. Arabic is one of the derivative languages that linguistically supports stemming. The Arabic language is a widely used language[9] and it exists in different formats. For example, Arabic words can be given in the format of separate text or it could be extracted from images[10]. Arabic language defines an accurate set of rules known as morphological rules, or morphology. Morphology accurately describes the formulation of an Arabic word from its basic form. The basic forms are commonly called roots. However, stemming is faced with ambiguity, as most of the NLP tasks.\nVarious techniques were used to resolve ambiguity. Among which is semantic analysis, that is to capture the intended meaning of a word [11]. Semantic analysis is a very powerful tool to tackle the ambiguity problem but it is very challenging to model. Distributional Semantic (DS)[11] is a type of semantic analysis based on co-occurrence analysis. It represents a word\u2019s meaning by its context (surrounding words) distribution as shown in Table 1. For example, the first row of Table 1 shows that the words \u0627\u062c\u0639\u0626\u0628)means \"Wonders( and (means \u201cCountries\u201d) \u0644\u0648\u062f appeared in the context of the word (means \u201cSystems\u201d) \u0645\u0638\u0646 with frequencies 0 and 5, respectively. Different measures can be computed such as Pointwise Mutual Information (PMI), Positive PMI (PPMI), and Smoothed PMI (SPMI). They measure the correlation between a word, and its context[12,13] . This paper introduces a context based Arabic stemmer for extracting a word\u2019s root. The proposed stemmer (CBAS) explores all possible roots then selects the appropriate root using Distributional Semantics (DS). The DS utilization impact is viewed as a series of comparisons with other stemmers using a manually annotated set of articles. The paper is organized as follows; section 2 is an introduction to Arabic morphology. Section 3 explores related work, and techniques used for constructing stemmers. The description of the proposed stemmer (CBAS) is introduced in section 4. In section 5, a detailed analysis, and evaluation of the proposed stemmer (CBAS) is presented. Finally, a conclusion is presented in section 6."}, {"heading": "2. BACKGROUND", "text": "Regardless of the approach used for developing morphological analyzers, a basic understanding of morphological rules is needed to set expectations, evaluate the results, and design improvements. This section introduces Arabic morphology, and common challenges. Morphology is the study of word formulation. Arabic morphology is based on the derivation principle, whereas words are acquired from roots. Roots are usually three, four, or five characters.\nRoots are the seeds for Arabic words generation. A new word is acquired by modifying its root. For example the word \u0628\u062a\u0627\u0643 (k\u0101tb, means \u201cWriter\u201d) is derived from the root \u0628 \u062a \u0643 (k\u0101f t\u0101\u02be b\u0101\u02be , means \u201cWrote\u201d) by adding \u0627 (\u02belf) in the middle.\nNot every addition is considered to be valid. Arabic language introduces a set of templates to define valid combinations and additions. Templates are referred to as patterns. It is an ordered sequence of letters. Since patterns work with all roots, a set of letters generically represent roots letters and its order while augmented letters are represented by themselves in correct positions. For example the pattern \u0644\u0639\u0627\u0641 (f\u0101\u02bfl, means \u201cActor\u201d) was used to derive the previous word \u0628\u062a\u0627\u0643 (k\u0101tb, means\u201dWriter\u201d) by substituting \u0641 (f\u0101\u02be) with \u0643 (k\u0101f), \u0639 (\u02bfyn) with \u062a (t\u0101\u02bf), and \u0644 (l\u0101m) with \u0628 (b\u0101\u02be),respectively. As noted, roots are commonly written as separated characters to indicate possible insertions[14].\nAugmented letters are reflected on the pattern, and finally on the word itself. However, an augmented unit (one, or more augmented letters) which is added in the front, or at the end of a word, is called a prefix, or a suffix addition. In most cases, prefixes and suffixes are not part of a word\u2019s meaning, rather they are additional features[14]. For example \u0628\u062a\u0627\u0643\u0644\u0627 (\u02belk\u0101tb, means\u201dThe writer\u201d) by adding \u0644\u0627 (\u02bel, means\u201dThe\u201d) in front of the word. This point of view would substantially reduce the number of enumerated patterns.\nAs described above, the root-pattern system is simple, elegant, and straightforward. However, the system is faced with morphological challenges, namely vocalization, mutation, and the absence of diacritics (annotation above, or below a word\u2019s letter that captures morphological, and grammatical additional features). For example, a letter can change its form due to grammatical or phonological rules. Another challenge of the root pattern system is stopwords, such as connection words that do not obey derivational rules.\nThe process of deriving back a word to its root (stemming) looks like a straightforward operation, by simply aligning a word to its pattern, and collecting the letters corresponding to \u0641 (f\u0101\u02be), \u0639 (\u02bfyn), and \u0644 (l\u0101m) letters. However, due to the challenges described above, a word may be derived back to multiple roots."}, {"heading": "3. RELATED WORK", "text": "Information Retrieval (IR) is one of the early tasks that utilized stemming analysis[7]. But, Stemming analysis is not limited to IR. Stemming analysis has improved many tasks, such as Machine Translation (MT)[15], Sentiment Analysis [16], and many more tasks. This section views common stemming analysis algorithms for different Natural Languages (NLs).\nThe nature of a language has a great impact on the development of related stemming algorithms. For example, the nature of the English language makes English stemmers concerned with removing word\u2019s suffixes only, while removing prefixes may imply a different meaning, such as sufficient and insufficient. Various stemmers were developed for English[8,17]. However, the underlying nature of the language limited its extension for other languages, among which is the Arabic[18] and Urdu[19] languages. However, stemming is not effective in the same degree for all languages[20,21]. Arabic is a morphological rich language which has enriched the Natural Language Processing (NLP)[22-24]. Arabic roots have rich linguistic features; they are semantically representative, derivable, and finite in numbers. Stemmers were developed over the years to take advantages of such features. This section introduces common Arabic stemmers, and their root extraction process which\nencapsulates semantic decisions. Finally, it introduces semantic analysis techniques independently from stemming analysis.\nKhoja stemmer[18] is one of the early and most powerful approaches developed for Arabic stemming[7]. Khoja simulates the linguistic process as much as possible. It removes prefixes, and suffixes from a word often after the normalization process, then matches the resulting word to a pattern, and finally extracts the root. The extracted root gets validated against a list of correct Arabic roots to ensure linguistic correctness. Khoja stemmer[18] resolves ambiguity by defining a set of linguistic paths, or decisions based on various features such as words first character, prefixes, or suffixes length. Additionally, decisions are implicitly ordered whereas the result would be the first correct root. For example, Khoja stemmer handles roots with duplicate letters first.\nRoot extraction is a highly complex process due to the existence of overlapping rules, which requires more information. A new type of stemming analysis introduced is light stemming. Light stemming is another way of acquiring reduced representation of Arabic words. Light stemming is not as complex as root extraction. It removes prefixes and suffixes only from a word. For example, the word \u0646\u0648\u0628\u062a\u0627\u0643\u0644\u0627 (\u02belk\u0101tb\u016bn, means \u201cThe writers\u201d) would be stemmed to the word \u0628\u062a\u0627\u0643 (k\u0101tb, means \u201cWriter\u201d) instead of \u0628 \u062a \u0643 (k\u0101f t\u0101\u02be b\u0101\u02be, means \u201cWriting\u201d). Light stemming is widely used for Information Retrieval (IR)[25]. Light stemming has shown competitive results in IR against root extraction based stemmers [6 ,26]. Light stemmers are relatively faster and efficient, which preserves more specific features of the word, for example, \u0628\u062a\u0627\u0643 (k\u0101tb, means \u201cWriter\u201d) is more related to the word \u0646\u0648\u0628\u062a\u0627\u0643\u0644\u0627 (\u02belk\u0101tb\u016bn, means \u201cThe writers) than \u0628 \u062a \u0643 (k\u0101f t\u0101\u02be b\u0101\u02be, means \u201cWriting\u201d). But, the number of words in Arabic without prefixes and suffixes is far more than the roots listed in Arabic dictionaries[14]. However, there is no explicit evidence that lightly stemmed words are more efficient than roots[26].\nISRI [27] is another linguistic based Arabic stemmer that roughly uses the same sequence used by Khojas stemmer [18]. But, the main difference is that ISRI does not linguistically validate the extracted root, no dictionary is used, and organizes the defined pattern set as sub-groups whereas each sub-group has common features. Besides, changes to the normalization process, prefixes, and suffixes handling. The main goal of ISRI is to get the minimum representation of a given word. Due to various changes, and prioritization, ISRI resolves ambiguity differently, by changing the order of applying morphological rules. But, ISRI still makes static decisions. Most likely, ISRI would be used for information retrieval rather than linguistic based tasks.\nTashaphyne [28] is another Arabic stemmer. It mainly supports light stemming (removing prefixes and suffixes). It follows the same approach used by Khoja [18], and ISRI [27] stemmers. It can be used for root extraction as well.\nDarwish[29] utilizes the existing word-root pairs used to construct the Finite State Transducer (FST) like stemmers. It uses a learning based technique to not only infer Arabic patterns, but also to rank extracted roots. This methodology enumerates possible roots like FSTs [30] approaches, but additionally gives a preference to the extracted roots. It is another way to handle ambiguity other than static rules. However, part of the inferred patterns would be inadequate due to cases such as vocalization, and mutation. And, the ranking of roots is based on inferred patterns frequency, neglecting the words features. Later, this approach has been modified to handle light stemming (prefixes and suffixes removal) [7]. Arabic grammar has high influence on morphological analysis[14]. ElixirFM [31] employs syntactic features to enhance morphological results. It takes advantage from Prague Arabic Dependency Treebank (PADT)[32] to acquire syntactic features and other morphological features\nfrom BuckWalter [33] stem dictionary. ElixirFM[31] defines a set of morphological rules to extract possible stems while ranking is used to disambiguate the extracted roots, or stems using the underlying data.\nMADAMIRA[15] morphological analyzer consists of two tools MADA [34], and AMIRA [35]. MADAMIRA [15] takes the advantage of large annotated corpus by using machine learning techniques such as Support Vector Machine[35]. It combines several tools such as word segmentation, Part of Speech Tagging (POST), and light stemming. It is different from the previous approaches, since it does not explicitly define morphological rules.\nStemmers are part of many NLP tasks. For example, in sentiment analysis they employ the use of classifiers such as Naive Bayes or Support Vector Machines (SVMs) to perform classification on sets of test samples given a tagged training set and rich feature sets for various tasks[16,36,37], question and answer systems [38,39], and many more. However, stemming is not limited to NLP. It has been used for Information Retrieval (IR), and most of the stemmers are evaluated indirectly using IR benchmarks [7]. Many IR experiments [6,26,27] showed that Arabic roots had improved the Arabic IR."}, {"heading": "4. PROPOSED STEMMER (CBAS)", "text": "The proposed stemmer, Context-Based Arabic Stemmer (CBAS), utilizes distributional similarity of a word\u2019s context to gain additional information about its semantic. This information assists with the selection of the correct root by excluding semantically irrelevant candidate roots within the context.\nThis section introduces the main phases of the proposed stemmer (CBAS), context matrix construction, roots generation, and root selection, whereas each phase consists of a set of steps. The proposed algorithm is shown in Fig. 1 and Fig. 2\nFig.1: Context Matrix Construction Algorithm\nFig.2: Stemmer\u2019s Algorithm"}, {"heading": "4.1. Data Resources", "text": "Predefined linguistic data is an essential part of the proposed stemmer (CBAS). This section introduces the data defined by CBAS. The Arabic word consists of three parts prefix, infix, and suffix[14]. Prefixes and suffixes are a set of features that can be added to a word, such as the definite article \u0644\u0627 (\u02bel, means\u201dThe\u201d) or connected pronouns \u0645\u06be (hm, means \u201cThem\u201d). Prefixes and suffixes lists contain individual and compound letters that could appear in the front or the end of the Arabic word. There is also a list of Arabic patterns which is used to extract possible Arabic roots. The final list is the Roots\u2019 dictionary which has been extracted from the Khoja stemmer [18] to validate the extracted root. The prefixes, suffixes, patterns, and dictionary lists are being used for the roots\u2019 generation phase.\nPrevious lists are commonly defined for Arabic stemmers. However, CBAS uses a raw data set which consists of a set of Articles that have been extracted from Omani newspapers[40]. The dataset contains 20291 articles from various topics, for example, culture and sport[40], which represents a wide range of the Arabic language current usage. The raw dataset plays a central role in selecting semantically correct roots, which differs from other used stemmers which do not commonly employ context in their algorithms."}, {"heading": "4.2. Context Matrix Construction", "text": "Context Matrix is a powerful and flexible tool to acquire some semantic properties [11]. It defines a window of n words, where the target word is at position i, and the rest of the surrounding words are its context [11]. The window slides over the corpus associating the target word with its context distribution as show in Table 1. Various measures can be computed from the context matrix, and employed in different tasks.\n4.3. Root Generation\nIt is automation for root extraction. However, unlike the manual process, it extracts all possible roots. It consists of three major sub processes, word segmentation, pattern matching, and root validation. \u2013 Word segmentation breaks a word into all possible three parts, prefix, suffix, and infix, using the predefined prefixes, and suffixes lists. \u2013 Pattern matching matches the infix obtained in word segmentation with one or more patterns with respect to its length. For each matched pattern, roots characters are collected, and passed to dictionary validation. It also handles weak letters, stopwords, and some other linguistic cases. \u2013 Dictionary validation ensures that the extracted roots are linguistically correct. It validates extracted roots against a list of correct Arabic roots.\nDictionary is not sufficient to generate only one correct root, due to various linguistic cases, roots\u2019 generation has the potential of extracting one, or more correct roots."}, {"heading": "4.4. Roots\u2019 Selection", "text": "This section will utilize the context matrix to select an appropriate root from two or more candidate roots. Pointwise Mutual Information (PMI) [12,13] measures the correlation between two or more words. Since some words can produce two, or more root candidates. The proposed algorithm (CBAS) uses a variation of PMI, Smoothed PMI (SPMI) [12] to handle sparse matrices. As shown in Table 2, SPMI achieved the highest accuracy of 81.5% when compared to PMI and PPMI, where the achieved accuracy was 78.84% and 79.49%, respectively. This is due to the fact that SPMI overcomes the tendency towards rare co-occurrence events, which is a side effect of PMI [12].\nSPMI is utilized to measure the correlation between the generated roots, and its previous context. To take advantage of the underlying matrix, set words are derived for each candidate root, in addition to the root itself. For each derived word, the average SPMI is computed with the previous word (as context). The root with the highest average correlation to its context is then selected."}, {"heading": "5. RESULTS AND EVALUATION", "text": "IR is the common methodology for evaluating a new stemmer because of the lack of stemmed benchmarks[21]. This section introduces the validation dataset, evaluation measures, and finally the experimental results."}, {"heading": "5.1. Validation Dataset", "text": "Direct evaluation is important to show the stemming accuracy, and potential improvements. A manually annotated dataset has been provided to measure the stemmer accuracy, and compare with other stemmers. The dataset is part of the Intentional Corpus of Arabic (ICA) [41].Various Arabic resources have contributed in collecting the ICA such as newspapers, books, and magazines. It has been constructed to provide an appropriate representation to Arabic language in Modern Standard Arabic (MSA) [41]. The dataset consists of 10302 tokens associated with various features. There exist 3629 unique word-root pairs, while other words do not have roots associated due to the existence of stopwords, and non-Arabic words. The dataset contains 8941 words after stopwords removal. This is shown in Fig 3.\nFig.3: Validation Dataset"}, {"heading": "5.2. Evaluation Criteria", "text": "Stemming is beneficial for many tasks, where every task uses the roots in a different way. For example, IR uses roots as a cluster representative to group related words, while sentiment analysis is more concerned with the linguistic accuracy of a root. A set of metrics were used to measure\ndifferent usages, and compare them with other stemmers.\nStemming accuracy is one of the basic measures for the effectiveness of the stemmer. It is defined as the ratio between the number of correctly stemmed words, and the number of the words in the complete dataset.\nCollecting related words under the same group is important for tasks such as IR. There are two variations of grouping related words. First, words can be grouped correctly under a semantically correct root; this is referred to it as classification. While the second is to group related words together, not necessarily under a correct root, and this is referred to as clustering. Standard metrics for classification, and clustering are: accuracy, precision, recall, and F1 measure, and are defined as follows [42, 43]:\n\u2211 =\n\u222a\n\u2229 =\nn\ni ii\nii\nYX\nYX\nn accuracy\n1 ||\n||1\n\u2211 =\n\u2229 =\nn\ni i\nii\nY\nYX\nn precision\n1 ||\n||1\n\u2211 =\n\u2229 =\nn\ni i\nii\nX\nYX\nn recall\n1 ||\n||1\n\u2211 =\n+\n\u2229 =\nn\ni ii\nii\nYX\nYX\nn measureF\n1\n1 ||||\n||1\nWhere\nn is the number of extracted roots. X is the set of extracted root. Xi is an individual extracted root.\nAnd\nY is the set of extracted root. Yi is an individual valid root."}, {"heading": "5.3. Results", "text": "The complete 8941 words were used to test the proposed stemmer (CBAS), with a window size n=3, then the set was reduced to a set of unique word-root pairs to be compared with other stemmers. Table 2 shows the comparison between the proposed stemmer (CBAS) and other stemmers. It shows that the proposed stemmer (CBAS) achieved an accuracy of 81.5% with an improvement of 9.4%, 67.3%, and 51.2% over Khoja, ISRI, and Tashphanye stemmers, respectively. Accuracy enhancement is due to exploring various possibilities of roots. Such exploration would not be possible without distributional semantics, which provides a dynamic and robust way for selecting an appropriate root.\nTable 3 and Table 4; show the performance of the proposed stemmer (CBAS) when using it as a grouping mechanism. Table 3 clarifies that the proposed stemmer (CBAS) has a higher potential to linguistically group Arabic words than other stemmers. CBAS outperformed other stemmers in the classification task, with an accuracy of 65.45%. While Table 4 shows that the proposed stemmer (CBAS) has potential improvements in non-linguistic based tasks, achieving an accuracy of 73.83% in clustering.\nBy comparing linguistic (classification), and non-linguistic (clustering) grouping measures, there is an increase in all corresponding measures. This is due to that some clusters were correctly formulated irrespective to the clusters seeds. Classification and clustering measures show the superiority of the CBAS over other stemmers. This indicates the beneficial features of the CBAS for the IR task."}, {"heading": "6. CONCLUSION", "text": "Many stemmers were developed to gain the rich linguistic features provided by the roots. Most of the stemmers made explicit decisions, statistical-based or linguistic-based, to select only one root. Other stemmers used ranking to express their selection preference rather than selecting a single root. However, at the very end, a single root would be chosen. Static decisions are very appropriate for common and frequent cases. However, adding other features such as syntactic and manual annotations would also be valuable. The introduced stemmer employs distributional similarity to handle incorrect roots selection, which is a side effect of root generation phase. The existence of robust filtering mechanisms, such distributional analysis, allows exploring various roots. Distributional analysis has several advantages. It can be computed for any corpus and any language, and it is relatively fast and inexpensive to construct compared to manually annotated corpus. Distributional semantics covers many relations between words, and it is robust against any preferences, or missing information. It is also very adaptive to context changes, which makes it suitable for many topics. However, distributional analysis is not as accurate as manually annotated data; hence, the word generation process was added to the roots selection phase to tolerate possible errors. The previous techniques were compared to the proposed stemmer (CBAS) results. CBAS shows an accuracy of 81.5% with an improvement of 9.4%, 67.3%, and 51.2% over Khoja, ISRI, and Tashphanye stemmers, respectively. CBAS also shows an improvement in classification and clustering, with an accuracy of 65.45% and 73.83%, respectively. Results indicate that the proposed stemmer (CBAS) enhances stemming and other related tasks. CBAS represents a methodology for capturing a word\u2019s context and makes decisions based on it. CBAS could change its behaviour based on the underlying data which could be specialized in a sub domain of the Arabic language. The statistical model used by CBAS is relatively simple. It incorporates important information (context) of a word which would be a complex process to include in a rule based stemmer. The statistical model reduces linguistic complexity of representing various linguistic cases. It also prevents unexpected interactions and prioritization schemes for ordering the rules."}], "references": [{"title": "Natural language processing: an introduction", "author": ["P.M. Nadkarni", "L. Ohno-Machado", "W.W. Chapman"], "venue": "Journal of the American Medical Informatics Association, vol. 18, pp. 544-551, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "The first public demonstration of machine translation: the Georgetown-IBM system, 7th January 1954", "author": ["J. Hutchins"], "venue": "noviembre de, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Three models for the description of language", "author": ["N. Chomsky"], "venue": "Information Theory, IRE Transactions on, vol. 2, pp. 113-124, 1956.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1956}, {"title": "R. Sethi, and J. D. Ullman", "author": ["A. Aho"], "venue": "Compilers: Principles, Techniques, and Tools, 1988.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1988}, {"title": "Accurate unlexicalized parsing", "author": ["D. Klein", "C.D. Manning"], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, 2003, pp. 423-430.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "On Arabic search: improving the retrieval effectiveness via a light stemming approach", "author": ["M. Aljlayl", "O. Frieder"], "venue": "Proceedings of the eleventh international conference on Information and knowledge management, 2002, pp. 340-347.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Arabic morphological analysis techniques: A comprehensive survey", "author": ["I.A. Al\u2010Sughaiyer", "I.A. Al\u2010Kharashi"], "venue": "Journal of the American Society for Information Science and Technology, vol. 55, pp. 189-213, 2004.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Snowball: A language for stemming algorithms", "author": ["M.F. Porter"], "venue": "ed, 2001.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Empirical studies in strategies for Arabic retrieval", "author": ["J. Xu", "A. Fraser", "R. Weischedel"], "venue": "Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, 2002, pp. 269-274.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Extraction of Arabic Words form Complex Color Images", "author": ["R. Fathalla", "Y. El Sonbaty", "M.A. Ismail"], "venue": "9th IEEE International Conference on Document Analysis and Recognition (ICDAR 2007), Brazil, pp. 1223-1227.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Utilizing semantic composition in distributional semantic models for word sense discrimination and word sense disambiguation", "author": ["C. Akkaya", "J. Wiebe", "R. Mihalcea"], "venue": "Semantic Computing (ICSC), 2012 IEEE Sixth International Conference on, 2012, pp. 45-51.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Normalized (pointwise) mutual information in collocation extraction", "author": ["G. Bouma"], "venue": "Proceedings of GSCL, pp. 31-40, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "A reference grammar of modern standard Arabic: Cambridge university", "author": ["K.C. Ryding"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Madamira: A fast, comprehensive tool for morphological analysis and disambiguation of arabic,\" in Proceedings of the Language Resources and Evaluation", "author": ["A. Pasha", "M. Al-Badrashiny", "M. Diab", "A. El Kholy", "R. Eskander", "N. Habash"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Exploring the Effects of Word Roots for Arabic Sentiment Analysis", "author": ["S.M. Oraby", "Y. El-Sonbaty", "M.A. El-Nasr"], "venue": "International Joint Conference on Natural Language Processing, Nagoya, Japan, 2013, pp. 471-479.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Development of a stemming algorithm: MIT Information", "author": ["J.B. Lovins"], "venue": "Processing Group, Electronic Systems Laboratory,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1968}, {"title": "Stemming arabic text", "author": ["S. Khoja", "R. Garside"], "venue": "Lancaster, UK, Computing Department, Lancaster University, 1999.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "An unsupervised approach to develop stemmer", "author": ["M.S. Husain"], "venue": "International Journal on Natural Language Computing, vol. 1, pp. 15-23, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "How effective is suffixing", "author": ["D. Harman"], "venue": "JASIS, vol. 42, pp. 7-15, 1991.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1991}, {"title": "Overview of stemming algorithms", "author": ["I. Smirnov"], "venue": "Mechanical Translation, vol. 52, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Arabic named entity recognition using optimized feature sets", "author": ["Y. Benajiba", "M. Diab", "P. Rosso"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2008, pp. 284-293.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "CLIR Experiments at Maryland for TREC-2002: Evidence combination for Arabic-English retrieval", "author": ["K. Darwish", "D.W. Oard"], "venue": "DTIC Document2003.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Arabic information retrieval at UMass in TREC-10", "author": ["L.S. Larkey", "M.E. Connell"], "venue": "DTIC Document2006.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Light stemming for Arabic information retrieval", "author": ["L.S. Larkey", "L. Ballesteros", "M.E. Connell"], "venue": "Arabic computational morphology, ed: Springer, 2007, pp. 221-243. International Journal on Natural Language Computing (IJNLC) Vol. 4, No.3,June 2015 12", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Improving stemming for Arabic information retrieval: light stemming and co-occurrence analysis", "author": ["L.S. Larkey", "L. Ballesteros", "M.E. Connell"], "venue": "Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, 2002, pp. 275-282.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2002}, {"title": "Arabic stemming without a root dictionary", "author": ["K. Taghva", "R. Elkhoury", "J. Coombs"], "venue": "null, 2005, pp. 152-157.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Tashaphyne, Arabic light stemmer/segment", "author": ["T. Zerrouki"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Building a shallow Arabic morphological analyzer in one day", "author": ["K. Darwish"], "venue": "Proceedings of the ACL-02 workshop on Computational approaches to semitic languages, 2002, pp. 1-8.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "Arabic morphological analysis on the Internet", "author": ["K.R. Beesley"], "venue": "Proceedings of the 6th International Conference and Exhibition on Multi-lingual Computing, 1998.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1998}, {"title": "Elixirfm: implementation of functional arabic morphology", "author": ["O. Smr\u017e"], "venue": "Proceedings of the 2007 Workshop on Computational Approaches to Semitic Languages: Common Issues and Resources, 2007, pp. 1-8.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Buckwalter {Arabic} Morphological Analyzer Version 1.0", "author": ["T. Buckwalter"], "venue": "2002.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "MADA+ TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization", "author": ["N. Habash", "O. Rambow", "R. Roth"], "venue": "Proceedings of the 2nd International Conference on Arabic Language Resources and Tools (MEDAR), Cairo, Egypt, 2009, pp. 102-109.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Automated methods for processing arabic text: From tokenization to base phrase chunking", "author": ["M. Diab", "K. Hacioglu", "D. Jurafsky"], "venue": "Arabic Computational Morphology: Knowledge-based and Empirical Methods. Kluwer/Springer, 2007.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "A feature selection algorithm with redundancy reduction for text classification", "author": ["S.N. Saleh", "Y. El-Sonbaty"], "venue": "Computer and information sciences, 2007. iscis 2007. 22nd international symposium on, 2007, pp. 1-6.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "Finding Opinion Strength Using Rule-Based Parsing for Arabic Sentiment Analysis", "author": ["S. Oraby", "Y. El-Sonbaty", "M.A. El-Nasr"], "venue": "Advances in Soft Computing and Its Applications, ed: Springer, 2013, pp. 509-520.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "ALQASIM: Arabic language question answer selection in machines", "author": ["A.M. Ezzeldin", "M.H. Kholief", "Y. El-Sonbaty"], "venue": "Information Access Evaluation. Multilinguality, Multimodality, and Visualization, ed: Springer, 2013, pp. 100-103.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploring the Effects of Root Expansion, Sentence Splitting and Ontology on Arabic Answer Selection", "author": ["A.M. Ezzeldin", "Y. El-Sonbaty", "M.H. Kholief"], "venue": "Natural Language Processing and Cognitive Science: Proceedings 2014, p. 273, 2015.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Evaluation of Topic Identification Methods on Arabic Corpora", "author": ["M. Abbas", "K. Sma\u00efli", "D. Berkani"], "venue": "JDIM, vol. 9, pp. 185-192, 2011.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Building an International Corpus of Arabic (ICA): progress of compilation stage", "author": ["S. Alansary", "M. Nagi", "N. Adly"], "venue": "7th international conference on language engineering, Cairo, Egypt, 2007, pp. 5-6.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2007}, {"title": "Discriminative methods for multi-labeled classification", "author": ["S. Godbole", "S. Sarawagi"], "venue": "Advances in Knowledge Discovery and Data Mining, ed: Springer, 2004, pp. 22-30.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Natural Language Processing (NLP) is the intersection between linguistics, and Computational Science (CS) [1].", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "NLP allows utilizing linguistics to use Natural Languages as a way of communication with computational devices[1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "Machine Translation (MT) was one of the first NLP tasks in the 1950s; it began as translation from Russian to English [2].", "startOffset": 118, "endOffset": 121}, {"referenceID": 0, "context": "The progress of MT was limited due to the complexity of linguistics rules, and low computation power at the time[1].", "startOffset": 112, "endOffset": 115}, {"referenceID": 2, "context": "However, Chomsky\u2019s theory[3] of natural language\u2019s grammar formed the basis for the formation of Backus-Naur Form (BNF).", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": "BNF[4] notations are commonly used to represent Context Free Grammar (CFG).", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "The introduction of statistical methods gave some insights for reducing NLP ambiguity[1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "For example, the Probabilistic CFGs extends the traditional CGFs by deducing linguistic rules and assigning weights[5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 0, "context": "The noticeable improvement in MT sparks the research in NLP [1].", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "Stemming is a primary NLP task, and it contributes in many other NLP tasks [6].", "startOffset": 75, "endOffset": 78}, {"referenceID": 6, "context": "Stemming is reducing a word to its basic form[7], while preserving its main characteristics.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "Many languages define linguistic rules for stemming but not with the same degree[8].", "startOffset": 80, "endOffset": 83}, {"referenceID": 8, "context": "The Arabic language is a widely used language[9] and it exists in different formats.", "startOffset": 45, "endOffset": 48}, {"referenceID": 9, "context": "For example, Arabic words can be given in the format of separate text or it could be extracted from images[10].", "startOffset": 106, "endOffset": 110}, {"referenceID": 10, "context": "Among which is semantic analysis, that is to capture the intended meaning of a word [11].", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "Distributional Semantic (DS)[11] is a type of semantic analysis based on co-occurrence analysis.", "startOffset": 28, "endOffset": 32}, {"referenceID": 11, "context": "They measure the correlation between a word, and its context[12,13] .", "startOffset": 60, "endOffset": 67}, {"referenceID": 12, "context": "As noted, roots are commonly written as separated characters to indicate possible insertions[14].", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "In most cases, prefixes and suffixes are not part of a word\u2019s meaning, rather they are additional features[14].", "startOffset": 106, "endOffset": 110}, {"referenceID": 6, "context": "Information Retrieval (IR) is one of the early tasks that utilized stemming analysis[7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 13, "context": "Stemming analysis has improved many tasks, such as Machine Translation (MT)[15], Sentiment Analysis [16], and many more tasks.", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": "Stemming analysis has improved many tasks, such as Machine Translation (MT)[15], Sentiment Analysis [16], and many more tasks.", "startOffset": 100, "endOffset": 104}, {"referenceID": 7, "context": "Various stemmers were developed for English[8,17].", "startOffset": 43, "endOffset": 49}, {"referenceID": 15, "context": "Various stemmers were developed for English[8,17].", "startOffset": 43, "endOffset": 49}, {"referenceID": 16, "context": "However, the underlying nature of the language limited its extension for other languages, among which is the Arabic[18] and Urdu[19] languages.", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "However, the underlying nature of the language limited its extension for other languages, among which is the Arabic[18] and Urdu[19] languages.", "startOffset": 128, "endOffset": 132}, {"referenceID": 18, "context": "However, stemming is not effective in the same degree for all languages[20,21].", "startOffset": 71, "endOffset": 78}, {"referenceID": 19, "context": "However, stemming is not effective in the same degree for all languages[20,21].", "startOffset": 71, "endOffset": 78}, {"referenceID": 20, "context": "Arabic is a morphological rich language which has enriched the Natural Language Processing (NLP)[22-24].", "startOffset": 96, "endOffset": 103}, {"referenceID": 21, "context": "Arabic is a morphological rich language which has enriched the Natural Language Processing (NLP)[22-24].", "startOffset": 96, "endOffset": 103}, {"referenceID": 22, "context": "Arabic is a morphological rich language which has enriched the Natural Language Processing (NLP)[22-24].", "startOffset": 96, "endOffset": 103}, {"referenceID": 16, "context": "Khoja stemmer[18] is one of the early and most powerful approaches developed for Arabic stemming[7].", "startOffset": 13, "endOffset": 17}, {"referenceID": 6, "context": "Khoja stemmer[18] is one of the early and most powerful approaches developed for Arabic stemming[7].", "startOffset": 96, "endOffset": 99}, {"referenceID": 16, "context": "Khoja stemmer[18] resolves ambiguity by defining a set of linguistic paths, or decisions based on various features such as words first character, prefixes, or suffixes length.", "startOffset": 13, "endOffset": 17}, {"referenceID": 23, "context": "Light stemming is widely used for Information Retrieval (IR)[25].", "startOffset": 60, "endOffset": 64}, {"referenceID": 5, "context": "Light stemming has shown competitive results in IR against root extraction based stemmers [6 ,26].", "startOffset": 90, "endOffset": 97}, {"referenceID": 24, "context": "Light stemming has shown competitive results in IR against root extraction based stemmers [6 ,26].", "startOffset": 90, "endOffset": 97}, {"referenceID": 12, "context": "But, the number of words in Arabic without prefixes and suffixes is far more than the roots listed in Arabic dictionaries[14].", "startOffset": 121, "endOffset": 125}, {"referenceID": 24, "context": "However, there is no explicit evidence that lightly stemmed words are more efficient than roots[26].", "startOffset": 95, "endOffset": 99}, {"referenceID": 25, "context": "ISRI [27] is another linguistic based Arabic stemmer that roughly uses the same sequence used by Khojas stemmer [18].", "startOffset": 5, "endOffset": 9}, {"referenceID": 16, "context": "ISRI [27] is another linguistic based Arabic stemmer that roughly uses the same sequence used by Khojas stemmer [18].", "startOffset": 112, "endOffset": 116}, {"referenceID": 26, "context": "Tashaphyne [28] is another Arabic stemmer.", "startOffset": 11, "endOffset": 15}, {"referenceID": 16, "context": "It follows the same approach used by Khoja [18], and ISRI [27] stemmers.", "startOffset": 43, "endOffset": 47}, {"referenceID": 25, "context": "It follows the same approach used by Khoja [18], and ISRI [27] stemmers.", "startOffset": 58, "endOffset": 62}, {"referenceID": 27, "context": "Darwish[29] utilizes the existing word-root pairs used to construct the Finite State Transducer (FST) like stemmers.", "startOffset": 7, "endOffset": 11}, {"referenceID": 28, "context": "This methodology enumerates possible roots like FSTs [30] approaches, but additionally gives a preference to the extracted roots.", "startOffset": 53, "endOffset": 57}, {"referenceID": 6, "context": "Later, this approach has been modified to handle light stemming (prefixes and suffixes removal) [7].", "startOffset": 96, "endOffset": 99}, {"referenceID": 12, "context": "Arabic grammar has high influence on morphological analysis[14].", "startOffset": 59, "endOffset": 63}, {"referenceID": 29, "context": "ElixirFM [31] employs syntactic features to enhance morphological results.", "startOffset": 9, "endOffset": 13}, {"referenceID": 30, "context": "5 from BuckWalter [33] stem dictionary.", "startOffset": 18, "endOffset": 22}, {"referenceID": 29, "context": "ElixirFM[31] defines a set of morphological rules to extract possible stems while ranking is used to disambiguate the extracted roots, or stems using the underlying data.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "MADAMIRA[15] morphological analyzer consists of two tools MADA [34], and AMIRA [35].", "startOffset": 8, "endOffset": 12}, {"referenceID": 31, "context": "MADAMIRA[15] morphological analyzer consists of two tools MADA [34], and AMIRA [35].", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "MADAMIRA[15] morphological analyzer consists of two tools MADA [34], and AMIRA [35].", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "MADAMIRA [15] takes the advantage of large annotated corpus by using machine learning techniques such as Support Vector Machine[35].", "startOffset": 9, "endOffset": 13}, {"referenceID": 32, "context": "MADAMIRA [15] takes the advantage of large annotated corpus by using machine learning techniques such as Support Vector Machine[35].", "startOffset": 127, "endOffset": 131}, {"referenceID": 14, "context": "For example, in sentiment analysis they employ the use of classifiers such as Naive Bayes or Support Vector Machines (SVMs) to perform classification on sets of test samples given a tagged training set and rich feature sets for various tasks[16,36,37], question and answer systems [38,39], and many more.", "startOffset": 241, "endOffset": 251}, {"referenceID": 33, "context": "For example, in sentiment analysis they employ the use of classifiers such as Naive Bayes or Support Vector Machines (SVMs) to perform classification on sets of test samples given a tagged training set and rich feature sets for various tasks[16,36,37], question and answer systems [38,39], and many more.", "startOffset": 241, "endOffset": 251}, {"referenceID": 34, "context": "For example, in sentiment analysis they employ the use of classifiers such as Naive Bayes or Support Vector Machines (SVMs) to perform classification on sets of test samples given a tagged training set and rich feature sets for various tasks[16,36,37], question and answer systems [38,39], and many more.", "startOffset": 241, "endOffset": 251}, {"referenceID": 35, "context": "For example, in sentiment analysis they employ the use of classifiers such as Naive Bayes or Support Vector Machines (SVMs) to perform classification on sets of test samples given a tagged training set and rich feature sets for various tasks[16,36,37], question and answer systems [38,39], and many more.", "startOffset": 281, "endOffset": 288}, {"referenceID": 36, "context": "For example, in sentiment analysis they employ the use of classifiers such as Naive Bayes or Support Vector Machines (SVMs) to perform classification on sets of test samples given a tagged training set and rich feature sets for various tasks[16,36,37], question and answer systems [38,39], and many more.", "startOffset": 281, "endOffset": 288}, {"referenceID": 6, "context": "It has been used for Information Retrieval (IR), and most of the stemmers are evaluated indirectly using IR benchmarks [7].", "startOffset": 119, "endOffset": 122}, {"referenceID": 5, "context": "Many IR experiments [6,26,27] showed that Arabic roots had improved the Arabic IR.", "startOffset": 20, "endOffset": 29}, {"referenceID": 24, "context": "Many IR experiments [6,26,27] showed that Arabic roots had improved the Arabic IR.", "startOffset": 20, "endOffset": 29}, {"referenceID": 25, "context": "Many IR experiments [6,26,27] showed that Arabic roots had improved the Arabic IR.", "startOffset": 20, "endOffset": 29}, {"referenceID": 12, "context": "The Arabic word consists of three parts prefix, infix, and suffix[14].", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "The final list is the Roots\u2019 dictionary which has been extracted from the Khoja stemmer [18] to validate the extracted root.", "startOffset": 88, "endOffset": 92}, {"referenceID": 37, "context": "However, CBAS uses a raw data set which consists of a set of Articles that have been extracted from Omani newspapers[40].", "startOffset": 116, "endOffset": 120}, {"referenceID": 37, "context": "The dataset contains 20291 articles from various topics, for example, culture and sport[40], which represents a wide range of the Arabic language current usage.", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "Context Matrix is a powerful and flexible tool to acquire some semantic properties [11].", "startOffset": 83, "endOffset": 87}, {"referenceID": 10, "context": "It defines a window of n words, where the target word is at position i, and the rest of the surrounding words are its context [11].", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": "Pointwise Mutual Information (PMI) [12,13] measures the correlation between two or more words.", "startOffset": 35, "endOffset": 42}, {"referenceID": 19, "context": "IR is the common methodology for evaluating a new stemmer because of the lack of stemmed benchmarks[21].", "startOffset": 99, "endOffset": 103}, {"referenceID": 38, "context": "The dataset is part of the Intentional Corpus of Arabic (ICA) [41].", "startOffset": 62, "endOffset": 66}, {"referenceID": 38, "context": "It has been constructed to provide an appropriate representation to Arabic language in Modern Standard Arabic (MSA) [41].", "startOffset": 116, "endOffset": 120}, {"referenceID": 39, "context": "Standard metrics for classification, and clustering are: accuracy, precision, recall, and F1 measure, and are defined as follows [42, 43]:", "startOffset": 129, "endOffset": 137}], "year": 2015, "abstractText": "Arabic morphology encapsulates many valuable features such as word\u2019s root. Arabic roots are being utilized for many tasks; the process of extracting a word\u2019s root is referred to as stemming. Stemming is an essential part of most Natural Language Processing tasks, especially for derivative languages such as Arabic. However, stemming is faced with the problem of ambiguity, where two or more roots could be extracted from the same word. On the other hand, distributional semantics is a powerful co-occurrence model. It captures the meaning of a word based on its context. In this paper, a distributional semantics model utilizing Smoothed Pointwise Mutual Information (SPMI) is constructed to investigate its effectiveness on the stemming analysis task. It showed an accuracy of 81.5%, with a at least 9.4% improvement over other stemmers.", "creator": "PScript5.dll Version 5.2.2"}}}