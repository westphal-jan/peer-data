{"id": "1611.00138", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2016", "title": "MusicMood: Predicting the mood of music from song lyrics using machine learning", "abstract": "Sentiment prediction of contemporary music can have a wide-range of applications in modern society, for instance, selecting music for public institutions such as hospitals or restaurants to potentially improve the emotional well-being of personnel, patients, and customers, respectively. In this project, music recommendation system built upon on a naive Bayes classifier, trained to predict the sentiment of songs based on song lyrics alone. The experimental results show that music corresponding to a happy mood can be detected with high precision based on text features obtained from song lyrics. These predictive predictions also provide an understanding of the relationship between music's affective qualities and social attitudes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 1 Nov 2016 06:05:49 GMT  (2201kb,D)", "http://arxiv.org/abs/1611.00138v1", "9 pages, 5 figures"]], "COMMENTS": "9 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR", "authors": ["sebastian raschka"], "accepted": false, "id": "1611.00138"}, "pdf": {"name": "1611.00138.pdf", "metadata": {"source": "CRF", "title": "MusicMood: Predicting the mood of music from song lyrics using machine learning", "authors": ["Sebastian Raschka"], "emails": ["mail@sebastianraschka.com"], "sections": [{"heading": "1 Introduction", "text": "With the rapid growth of digital music libraries as well as advancements in technology, music classification and recommendation has gained increased popularity in the music industry and among listeners. Many applications using machine learning algorithms have been developed to categorize music by instruments [10, 18] artist similarity [14, 23], emotion [16, 13, 27], or genre [25, 15]. Psychological studies have shown that listening to music is one of the most popular activities in leisure time and that it has an enhancing effect on the social cohesion, emotional state, and mood of the listeners [22, 28]. The increasing number of song lyrics that are freely available on the Internet allow the effective training of machine learning algorithms to perform mood prediction and filtering for music that can be associated with positive or negative emotions. The aim of this project was to build a recommendation system that is able to predict whether a song is happy or sad, which can be applied to song databases in order to select music by sentiment in different social contexts (Figure 1). The main contributions of this project are as follows:\n1. Creation of a new dataset that can provide the basis of future studies on music and mood.\n2. A naive Bayes classification model for mood prediction of music based on lyrics analysis.\n3. An online web application to perform music mood pre-diction given artist name and song title.\nSection 2 provides a formal statement of the problem and related work. Section 3 summarizes the preprocessing and data mining steps that were conducted in this project. The experimental setup and results we obtained are presented and discussed in section 4, and the conclusions and future directions are provided in section 5.\nThe primary goal of this project was to build a classification model to filter for happy music with high precision. A naive Bayes model was chosen for the lyric classification since naive Bayes classifiers are known to perform well given small sample sizes [6] and are successfully being used for similar binary text classification tasks such as e-mail spam detection [21]. Furthermore, empirical studies\nar X\niv :1\n61 1.\n00 13\n8v 1\n[ cs\n.L G\n] 1\nN ov\nhave shown that the performance of naive Bayes classifiers for text categorization is comparable to Support Vector machines [9, 7], while being computationally more efficient for batch and on-line learning.\nThe availability of open-source music datasets for research is either limited to audio feature datasets or requires manual retrieval from on-line music platforms of Creative Commons-licensed music or public domain recordings. A widely used dataset for music information retrieval (MIR) research is the freely-available Million Song Dataset [3] that contains audio features and metadata of a million music tracks. The musiXmatch [4] dataset provides lyrics in a bag of words [8] format for 77% of the songs in the Million Song Dataset after application of a stemming algorithm.\nWhile ground truth genre labels can usually be determined unambiguously through rational analysis, labeling of music by mood is a more challenging task. The perception of mood and the association of mood with different types of music are obviously subjective works. Applications of crowdsourcing approaches to collect mood ratings in Arousal-Valence (A-V) space have been designed [24], and other music mood datasets are available [21] as well; however, datasets that are providing ground truth mood labels for music are typically covering very vast and diverse sets of mood labels, which cannot be transferred to a binary categorization into happy and sad in an unambiguous manner."}, {"heading": "2 Methods", "text": ""}, {"heading": "2.1 Data Acquisition", "text": "A random subsample of 10,000 songs was downloaded from the Million Song Dataset [3] in HDF5 format. Using the provided song title and artist information from these HDF5 files, custom code was written to download the corresponding lyrics from LyricWikia [2]. Songs for which lyrics were not available \u2014 songs that are either instrumental or not deposited in the LyricWikia database \u2014 were removed from the dataset. The choice of acquiring the lyrics in an unprocessed format over the musiXmatch dataset was necessary for comparing different feature extraction and preprocessing steps. Custom code based on the Python NLTK library [5] was written to identify non-English lyrics and remove these songs from the dataset using majority support based on the counts of English words vs. non-English words in the lyrics. After applying those filtering rules, the remaining dataset of 2,773 songs was randomly partitioned into a training dataset (1,000 songs) and a validation dataset (200 songs). Music labels were automatically collected from user-provided content on the music database Last.fm [1]. However, due to the nonexistence of mood-related tags for a majority of songs in the filtered dataset, the two mood labels (happy and sad) were manually assigned based on human interpretation of the lyrics and listening tests. Happy music was defined as music that could be\nassociated with upbeat sounds and positive themes. Sad music was defined as music that the author related to a negative, dark, or violent theme."}, {"heading": "2.2 Feature Extraction", "text": "Prior to the tokenization of the lyrics, a bag of words model [8] (a fixed-size multiset where the order of words has no significance) was used to transform the lyrics into feature vectors. Further processing of the feature vectors include the choice of different n-gram sequences (n \u2208 {1, 2, 3}), stop word removal based on a stop word list from the Python NLTK library [5], and usage of the Porter stemming algorithm [20] for suffix stripping. Also, different representations of the word count in the feature vectors for each song text were used, such as binarization, term frequency (tf ) computation, and term frequency-inverse document frequency (tf-idf ) computation.\nThe term frequency-inverse document frequency was calculated based on the normalized term frequency tf-idf(t, d), which is computed as the number of occurrences of a term t in a song text d divided by the total number of lyrics that contain term t\ntf-idf(t, d) = tf(t, d)\u00d7 idf(t). (1)\nLet tf-idf(t, d) be the normalized term frequency and idf(t) be the inverse document frequency\nidf(t) = log 1 + nd\n1 + df(d, t) + 1,\nwhere nd is the total number of lyrics and df(d, t) the number of lyrics that contain the term t."}, {"heading": "2.3 Model Selection", "text": "Model performances using different combinations of the feature, as mentioned earlier, preprocessing techniques including hyperparameter optimization of the naive Bayes models were evaluated using grid search and 10-fold cross-validation on the 1000-song training set to optimize the F1-score. Defining the mood label happy as the positive class, the F1-score was computed as the harmonic mean of precision and recall\nF1 = 2\u00d7 precision\u00d7 recall precision + recall , (2)\nwhere\nprecision = TP\nTP + FP (3)\nand\nrecall = TP\nTP + FN . (4)\n(TP = number of true positives, FP = number of false negatives, and FN = number of false negatives.)\nGiven the general notation of the posterior probability for naive Bayes classification\nP (\u03c9j |xi) = P (xi|\u03c9j)\u00d7 P (\u03c9j)\nP (xi) , (5)\nthe objective function in the naive Bayes model is to maximize the posterior probability given the training data where P (xi|\u03c9j) is the class-conditional probability of observing feature xi belonging to class \u03c9j :\npredicted class label\u2190 argmax j=1,...,m P (\u03c9j |xi). (6)\nThe class-conditional probabilities of the multi-variate Bernoulli naive Bayes model that is trained based on the binarized feature vectors are defined as\nP (x|\u03c9j) = m\u220f i=1 P (xi|\u03c9j)b \u00d7 (1\u2212 P (xi|\u03c9j))1\u2212b. (7)\nLet P\u0302 (xi|\u03c9j) be the maximum-likelihood estimate that a particular word (or token) xi occurs in class wj\nP\u0302 (x|\u03c9j) = dfxi,y + \u03b1\ndfy + \u03b1n , (8)\nwhere dfxi,y is the number of lyrics in the training dataset that contain the feature xi and belong to class wj . And dfy is the is the number of lyrics in the training dataset that contain the feature xi and belong to class \u03c9y. Lastly, dfy is the number of lyrics in the training dataset that belong to class wj , \u03b1 is the additive smoothing parameter [17], and n is the number of elements in the feature vector.\nAdditionally, a multinomial naive Bayes model was evaluated based on the term frequencies or tf-idf, where the class- conditional probabilities are calculated as follows\nP\u0302 (xi|\u03c9j) = \u2211\ntf(xid \u2208 \u03c9j) + \u03b1\u2211 Nd\u2208\u03c9j + \u03b1n , (9)\nwhere \u2211 Nd\u2208\u03c9j is the sum of all term frequencies in the training dataset that belong to \u03c9j .\nFor both the multi-variate Bernoulli and the multinomial naive Bayes model the class-conditional probability of encountering the song text x can be calculated as the product of the likelihoods of the individual terms under the naive assumption of conditional independence between features\nP (x|\u03c9j) = P (x1|\u03c9j)\u00d7 P (x2|\u03c9j)\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 P (xn|\u03c9j). (10)"}, {"heading": "2.4 Software", "text": "The Python libraries NumPy [26] and scikit-learn [19] were used for model training and model evaluation; the libraries seaborn [29] and matplotlib [12] were used for visualization. All data, code for model training and evaluation, and the final web app have been made available at https://github.com/rasbt/musicmood."}, {"heading": "2.5 Experimental Setup", "text": "After manual assignment of the mood labels and random sampling, the training dataset consisted of happy (44.6%) and sad (55.4%) songs; the number of happy and sad songs in the validation dataset was equal (Table 1). The model selection was performed via grid search and 10-fold cross- validation on the 1000-song training dataset to optimize the performance measured via F1-score. The final model was trained on the entire training dataset, the performance was evaluated on the 200-song validation dataset by measuring the receiver operating characteristic area under the curve (ROC auc), accuracy, precision, recall, and F1-score.\nFor initial model selection, grid search was performed on three separate naive Bayes models to select the best performing combination of feature extraction and selection approaches and parameters for each model. These three models were Multi-variate Bernoulli Bayes with binary word counts as feature vectors, multinomial Bayes with term frequency features, and multinomial naive Bayes with tf-idf features. After the three models had been individually optimized via grid search, the performance of the best performing model, from each of the three categories, was evaluated via ROC auc. The best performing model was then chosen for a more thorough optimization via grid search.\nTable 1: Mood label distribution in the training and validation datasets.\nMood Training Validation Total\nhappy 446 95 541 sad 554 95 649\nFigure 2: Wordcloud visualizations of the most frequent words of the happy songs (A) and sad songs (B) in the training dataset. The size of the words is proportional to the frequency across lyrics.\nDuring the grid search optimization, the following settings and parameters were optimized: n-gram range for tokenization, stop words removal, Porter stemming, the maximum number of features in the vocabulary (based on the k most frequent tokens), a cut-off for minimum term frequency, and the \u03b1 smoothing parameter."}, {"heading": "3 Results", "text": "The wordcloud visualizations of the most frequent words in the training dataset show an overlap between the most frequent words (love, know, come) between the happy and sad songs (Figure 2). Grouping the songs by release year shows that the random 1000-song sub sample from the Million Song Dataset is bias towards more recent releases (Figure 3 A); interestingly, the fraction of sad songs increases over time Figure (Figure 3 B).\nAfter grid search for three separate naive Bayes classification models yielded an almost equal performance as shown in Figure 4A. The best performing model was a multinomial naive Bayes classifier (average ROC auc 0.75) with a 1-gram tf-idf feature representation after applying Porter stemming for suffix stripping and additional stop word removal. Further evaluation showed that tuning of the \u03b1 smoothing parameter, minimum term frequency cut-off value, and maximum size of the vocabulary had little effect on the performance of the chosen classification model Figure\n4C-E; the attempt to increase the the n-gram range had a visibly negative effect on the classification performance 4F.\nAfter model selection, the final classifier was trained on the complete training dataset and the performance was evaluated based on the validation dataset. The mood classifier achieved a precision performance of 99.60% on the training set and a precision of 88.89% on the 200-sample validation set, suggesting that it may suffer from overfitting."}, {"heading": "4 Discussion", "text": "The exploratory data analysis of the training corpus showed that the fraction of sad songs increases over the years (Figure 3. However, it has to be considered that distribution of songs per year is heavily biased towards more recent releases and older music is underrepresented in the training sample. The apparent trend is still interesting and suggests that modern society could be exposed to a larger amount of sad songs than previous generations, which makes a music recommendation system, which can be used as a mood filter, particularly interesting. All three of the different naive Bayes models that were optimized via grid search showed a better performance when stop words were removed from the lyrics (Figure 4). However, the higher ROC auc score of the model that was trained on tf-idf feature vectors suggests that the lyrics corpus still contained several non-relevant words that were common among both happy and sad songs as it can be seen in the wordclouds (Figure ??). As expected, the multinomial naive Bayes models showed a better performance than the Bernoulli naive Bayes models which used only binary feature vectors as input. Although the mood classifier has a high precision for both the training (99.60%) and validation (88.89%) dataset, the results indicate that the cross-validation approach for model selection did not completely overcome the problem of over-fitting, which might be partially due to the large number of settings and parameters that were evaluated dur- ing grid search and the relatively small size of the training dataset. Also, the low recall rate might also be due to the equal class distribution of happy and sad songs in the validation dataset since the prior probabilities of the naive Bayes model were estimated from the training dataset which contained a larger fraction of sad songs. However, the high precision of the classifier is still satisfactory given the proposed goal of confidently removing sad songs from an extensive music library before performing the genre classification. Based on the promising results, future directions include the re-evaluation of the model using a larger training dataset and mood labels selected by majority support based on labels provided by different individuals."}, {"heading": "5 Conclusion", "text": "The results have shown that a naive Bayes model applied to mood classification based lyrics can predict the positive class (happy) with high precision, which can be useful to filter a large music library for happy music with a low false positive rate. A music library filtered in this manner could further be used as input for genre classification to filter music according to different tastes. Planned future work will include extensions to the mood classification web application to incorporate more\nlyrics to evaluate if the predictive performance of the classifier can be improved given a larger dataset. The extensions will include feedback about the prediction. In one extension, online learning will be implemented to update the hypothesis incrementally."}], "references": [{"title": "NLTK: the natural language toolkit", "author": ["Steven Bird"], "venue": "In Proceedings of the COLING/ACL on Interactive presentation sessions,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "On the optimality of the simple Bayesian classifier under zero-one loss", "author": ["Pedro Domingos", "Michael Pazzani"], "venue": "Machine learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Twitter sentiment classification using distant supervision", "author": ["Alec Go", "Richa Bhayani", "Lei Huang"], "venue": "CS224N Project Report,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Comparing SVM and Naive Bayes classifiers for text categorization with Wikitology as knowledge enrichment", "author": ["Sundus Hassan", "Muhammad Rafi", "Muhammad Shahid Shaikh"], "venue": "In Multitopic Conference (INMIC),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Towards Instrument Segmentation for Music Content Description a Critical Review of Instrument Classification Techniques", "author": ["Perfecto Herrera", "X. Amatriain", "E. Batlle", "Xavier Serra"], "venue": "International Conference on Music Information Retrieval,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Genre classification for million song dataset using confidence-based classifiers combination", "author": ["Yajie Hu", "Mitsunori Ogihara"], "venue": "In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Matplotlib: A 2D graphics environment", "author": ["J D Hunter"], "venue": "Computing In Science & Engineering,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Automatic mood classification for music", "author": ["Pieter Kanters"], "venue": "PhD thesis,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Music artist style identification by semi-supervised learning from both lyrics and content", "author": ["Tao Li", "Mitsunori Ogihara"], "venue": "In International Multimedia Conference: Proceedings of the 12 th annual ACM international conference on Multimedia,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "A comparative study on content-based music genre classification", "author": ["Tao Li", "Mitsunori Ogihara", "Qi Li"], "venue": "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Automatic mood detection and tracking of music audio signals", "author": ["Lie Lu", "Dan Liu", "Hong-Jiang Zhang"], "venue": "IEEE Transactions on audio, speech, and language processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Introduction to information retrieval, volume 1", "author": ["Christopher D Manning", "Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "A Study of Musical Instrument Classification Using Gaussian Mixture Models and Support Vector Machines", "author": ["Janet Marques", "Pedro J. Moreno"], "venue": "COMPAQ CORPORATION, CAMBRIDGE RESEARCH LABORATORY,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "An algorithm for suffix stripping", "author": ["Martin F Porter"], "venue": "Program: electronic library and information systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1980}, {"title": "A Bayesian approach to filtering junk e-mail", "author": ["Mehran Sahami", "Susan Dumais", "David Heckerman", "Eric Horvitz"], "venue": "In Learning for Text Categorization: Papers from the 1998 workshop,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "The psychological functions of music listening", "author": ["Thomas Sch\u00e4fer", "Peter Sedlmeier", "Christine St\u00e4dtler", "David Huron"], "venue": "Frontiers in psychology,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Mining microblogs to infer music artist similarity and cultural listening patterns", "author": ["Markus Schedl", "David Hauger"], "venue": "In Proceedings of the 21st international conference companion on World Wide Web,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Modeling Musical Emotion Dynamics with Conditional Random Fields", "author": ["Erik M Schmidt", "Youngmoo E Kim"], "venue": "In ISMIR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Musical genre classification of audio signals", "author": ["George Tzanetakis", "Perry Cook"], "venue": "Speech and Audio Processing, IEEE transactions on,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "The numpy array: a structure for efficient numerical computation", "author": ["Stefan Van Der Walt", "S Chris Colbert", "Gael Varoquaux"], "venue": "Computing in Science & Engineering,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Automatic Mood Classification Using TF* IDF Based on Lyrics", "author": ["Menno Van Zaanen", "Pieter Kanters"], "venue": "In ISMIR,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Emotion induction through music: A review of the musical mood induction procedure", "author": ["Daniel V\u00e4stfj\u00e4ll"], "venue": "Musicae Scientiae,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}], "referenceMentions": [{"referenceID": 4, "context": "Many applications using machine learning algorithms have been developed to categorize music by instruments [10, 18] artist similarity [14, 23], emotion [16, 13, 27], or genre [25, 15].", "startOffset": 107, "endOffset": 115}, {"referenceID": 12, "context": "Many applications using machine learning algorithms have been developed to categorize music by instruments [10, 18] artist similarity [14, 23], emotion [16, 13, 27], or genre [25, 15].", "startOffset": 107, "endOffset": 115}, {"referenceID": 8, "context": "Many applications using machine learning algorithms have been developed to categorize music by instruments [10, 18] artist similarity [14, 23], emotion [16, 13, 27], or genre [25, 15].", "startOffset": 134, "endOffset": 142}, {"referenceID": 16, "context": "Many applications using machine learning algorithms have been developed to categorize music by instruments [10, 18] artist similarity [14, 23], emotion [16, 13, 27], or genre [25, 15].", "startOffset": 134, "endOffset": 142}, {"referenceID": 10, "context": "Many applications using machine learning algorithms have been developed to categorize music by instruments [10, 18] artist similarity [14, 23], emotion [16, 13, 27], or genre [25, 15].", "startOffset": 152, "endOffset": 164}, {"referenceID": 7, "context": "Many applications using machine learning algorithms have been developed to categorize music by instruments [10, 18] artist similarity [14, 23], emotion [16, 13, 27], or genre [25, 15].", "startOffset": 152, "endOffset": 164}, {"referenceID": 20, "context": "Many applications using machine learning algorithms have been developed to categorize music by instruments [10, 18] artist similarity [14, 23], emotion [16, 13, 27], or genre [25, 15].", "startOffset": 152, "endOffset": 164}, {"referenceID": 18, "context": "Many applications using machine learning algorithms have been developed to categorize music by instruments [10, 18] artist similarity [14, 23], emotion [16, 13, 27], or genre [25, 15].", "startOffset": 175, "endOffset": 183}, {"referenceID": 9, "context": "Many applications using machine learning algorithms have been developed to categorize music by instruments [10, 18] artist similarity [14, 23], emotion [16, 13, 27], or genre [25, 15].", "startOffset": 175, "endOffset": 183}, {"referenceID": 15, "context": "Psychological studies have shown that listening to music is one of the most popular activities in leisure time and that it has an enhancing effect on the social cohesion, emotional state, and mood of the listeners [22, 28].", "startOffset": 214, "endOffset": 222}, {"referenceID": 21, "context": "Psychological studies have shown that listening to music is one of the most popular activities in leisure time and that it has an enhancing effect on the social cohesion, emotional state, and mood of the listeners [22, 28].", "startOffset": 214, "endOffset": 222}, {"referenceID": 1, "context": "A naive Bayes model was chosen for the lyric classification since naive Bayes classifiers are known to perform well given small sample sizes [6] and are successfully being used for similar binary text classification tasks such as e-mail spam detection [21].", "startOffset": 141, "endOffset": 144}, {"referenceID": 14, "context": "A naive Bayes model was chosen for the lyric classification since naive Bayes classifiers are known to perform well given small sample sizes [6] and are successfully being used for similar binary text classification tasks such as e-mail spam detection [21].", "startOffset": 252, "endOffset": 256}, {"referenceID": 5, "context": "A subset of the Million Song Dataset [11] is divided into a training and a validation dataset.", "startOffset": 37, "endOffset": 41}, {"referenceID": 3, "context": "have shown that the performance of naive Bayes classifiers for text categorization is comparable to Support Vector machines [9, 7], while being computationally more efficient for batch and on-line learning.", "startOffset": 124, "endOffset": 130}, {"referenceID": 2, "context": "have shown that the performance of naive Bayes classifiers for text categorization is comparable to Support Vector machines [9, 7], while being computationally more efficient for batch and on-line learning.", "startOffset": 124, "endOffset": 130}, {"referenceID": 17, "context": "Applications of crowdsourcing approaches to collect mood ratings in Arousal-Valence (A-V) space have been designed [24], and other music mood datasets are available [21] as well; however, datasets that are providing ground truth mood labels for music are typically covering very vast and diverse sets of mood labels, which cannot be transferred to a binary categorization into happy and sad in an unambiguous manner.", "startOffset": 115, "endOffset": 119}, {"referenceID": 14, "context": "Applications of crowdsourcing approaches to collect mood ratings in Arousal-Valence (A-V) space have been designed [24], and other music mood datasets are available [21] as well; however, datasets that are providing ground truth mood labels for music are typically covering very vast and diverse sets of mood labels, which cannot be transferred to a binary categorization into happy and sad in an unambiguous manner.", "startOffset": 165, "endOffset": 169}, {"referenceID": 0, "context": "Custom code based on the Python NLTK library [5] was written to identify non-English lyrics and remove these songs from the dataset using majority support based on the counts of English words vs.", "startOffset": 45, "endOffset": 48}, {"referenceID": 0, "context": "Further processing of the feature vectors include the choice of different n-gram sequences (n \u2208 {1, 2, 3}), stop word removal based on a stop word list from the Python NLTK library [5], and usage of the Porter stemming algorithm [20] for suffix stripping.", "startOffset": 181, "endOffset": 184}, {"referenceID": 13, "context": "Further processing of the feature vectors include the choice of different n-gram sequences (n \u2208 {1, 2, 3}), stop word removal based on a stop word list from the Python NLTK library [5], and usage of the Porter stemming algorithm [20] for suffix stripping.", "startOffset": 229, "endOffset": 233}, {"referenceID": 11, "context": "Lastly, dfy is the number of lyrics in the training dataset that belong to class wj , \u03b1 is the additive smoothing parameter [17], and n is the number of elements in the feature vector.", "startOffset": 124, "endOffset": 128}, {"referenceID": 19, "context": "4 Software The Python libraries NumPy [26] and scikit-learn [19] were used for model training and model evaluation; the libraries seaborn [29] and matplotlib [12] were used for visualization.", "startOffset": 38, "endOffset": 42}, {"referenceID": 6, "context": "4 Software The Python libraries NumPy [26] and scikit-learn [19] were used for model training and model evaluation; the libraries seaborn [29] and matplotlib [12] were used for visualization.", "startOffset": 158, "endOffset": 162}], "year": 2016, "abstractText": "Sentiment prediction of contemporary music can have a wide-range of applications in modern society, for instance, selecting music for public institutions such as hospitals or restaurants to potentially improve the emotional well-being of personnel, patients, and customers, respectively. In this project, music recommendation system built upon on a naive Bayes classifier, trained to predict the sentiment of songs based on song lyrics alone. The experimental results show that music corresponding to a happy mood can be detected with high precision based on text features obtained from song lyrics.", "creator": "LaTeX with hyperref package"}}}