{"id": "1611.00183", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2016", "title": "Local Subspace-Based Outlier Detection using Global Neighbourhoods", "abstract": "Outlier detection in high-dimensional data is a challenging yet important task, as it has applications in, e.g., fraud detection and quality control.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 1 Nov 2016 11:22:26 GMT  (1090kb,D)", "http://arxiv.org/abs/1611.00183v1", "Short version accepted at IEEE BigData 2016"]], "COMMENTS": "Short version accepted at IEEE BigData 2016", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["bas van stein", "matthijs van leeuwen", "thomas b\\\"ack"], "accepted": false, "id": "1611.00183"}, "pdf": {"name": "1611.00183.pdf", "metadata": {"source": "CRF", "title": "Local Subspace-Based Outlier Detection using Global Neighbourhoods", "authors": ["Bas van Stein", "Matthijs van Leeuwen"], "emails": ["b.van.stein@liacs.leidenuniv.nl", "m.van.leeuwen@liacs.leidenuniv.nl", "t.h.w.baeck@liacs.leidenuniv.nl"], "sections": [{"heading": null, "text": "We therefore introduce GLOSS, an algorithm that performs local subspace outlier detection using global neighbourhoods. Experiments on synthetic data demonstrate that GLOSS more accurately detects local outliers in mixed data than its competitors. Moreover, experiments on real-world data show that our approach identifies relevant outliers overlooked by existing methods, confirming that one should keep an eye on the global perspective even when doing local outlier detection.\nI. INTRODUCTION\nOutlier detection [1] is an important task that has applications in many domains. In fraud detection, for example, a bank could be interested in detecting fraudulent transactions; in network intrusion detection, it could be of interest to automatically detect suspicious network events; in a manufacturing plant, identifying raw materials or products with strongly deviating properties could be useful as part of quality control. In each of these applications, the data is high-dimensional and each data point is a potential outlier.\nMany techniques for outlier detection have been proposed and studied. Many traditional outlier detection methods [2] are parametric and thus make strong assumptions about the data. Moreover, data points are always considered as a whole and relative to all other data points, which strongly limits the accuracy of these methods on high-dimensional data. Outlier detection in complex, high-dimensional data is an inherently hard problem, as data points tend to have similar distances due to the infamous \u2018curse of dimensionality\u2019. To address both this problem and the limitations of (global) outlier detection, local outlier detection methods [3]\u2013[5] have been proposed over the past few decades. These methods are distance- or density-based, and assign outlier scores based on the distance of a data point to its closest neighbours relative to the local density of its neighbourhood. To further improve on this, local subspace outlier detection methods [6]\u2013[8] have been introduced. They search for local outliers within so-called subspaces, i.e., subsets of the complete set of features. This results in each outlier being reported\ntogether with a corresponding subspace in which it is far away from its neighbours. Existing local outlier detection approaches, however, are bound to overlook outliers when the data is a mixture of high-dimensional data points drawn from different data distributions. That is, as we will show, a local neighbourhood found within a given subspace may very well include data points from different components of the mixture, which might result in clear outliers hiding in the crowd of a different component. This is especially relevant when the individual components of the mixture are unknown and hence the dataset has to be analysed as a whole.\nWe encountered this exact situation in an ongoing collaboration with the BMW Group, where our aim is to identify steel coils strongly deviating in terms of their material properties. The data is very high-dimensional, as it contains hundreds of measurements per coil, but is also known to be a mixture of samples from different distributions: the steel coils have different grades and come from different suppliers. Unfortunately, part of this information is not available in the data and we therefore had to analyse the complete, mixed data. However, what is a \u2018normal\u2019 measurement for one type of coil can be a clear deviation for another type of coil; therefore, existing outlier detection methods did not perform well. Subsection VI-D will show examples of relevant outliers detected by our approach that were not found by existing methods.\nFigure 1 illustrates the problem that we consider on a synthetic dataset. The data consists of three normally distributed clusters in six dimensions; the generative process (and experiments on generated data) will be described in detail in Subsection VI-A. When considering all data points, the data point depicted by the red star is not a local outlier in any of the subspaces, neither in the global nor in any of the two-dimensional subspaces (only three shown). However, when only considering the data point\u2019s neighbours in the global space, here depicted with yellow diamonds, we can observe that the red star is a clear outlier in the 2D subspace shown in the top right plot. it happens to be close to data points from other components, but is far away from data points from the component it belongs to and is therefore an outlier. As we will show in Subsection VI-A, existing algorithms are unable to detect such outliers, especially in high-dimensional data, whereas our method can.\nApproach and contributions Our first contribution is the\nar X\niv :1\n61 1.\n00 18\n3v 1\n[ cs\n.A I]\n1 N\nov 2\n01 6\nformalisation of the Local Subpace Outlier in Global Neighbourhood problem. That is, we propose to combine local subspace outlier detection with neighbourhoods selected in the global data space. The purpose of using global neighbourhoods is to assess the degree of outlierness of a given data point relative to other data points belonging to the same mixture component, avoiding the possibility that outliers can hide among members of other components of the mixture distribution. Following this, our second contribution is the introduction of the GLOSS algorithm, which combines our ideas on outlier detection using global neighbourhoods with techniques from LoOP [5] and HiCS [6].\nGiven a dataset, it computes the probability that a data point is an outlier according to the problem definition. Moreover, it does so for all feature subspaces deemed relevant and hence also provides information about the subspace(s) in which a data point is considered to be an outlier.\nFinally, the third contribution of this paper is an extensive set of experiments on both synthetic and real-world data, in which we evaluate GLOSS and compare its performance to its state-of-the-art competitors. The experiments demonstrate that the use of global neighbourhoods enable the discovery of outliers that would otherwise be left undetected, without sacrificing detection accuracy on \u2018regular\u2019 outliers. Moreover, global neighbourhoods give GLOSS an edge in terms of computational efficiency. Finally, GLOSS identifies relevant outliers on real-world manufacturing data from the BMW Group that are not marked as such by existing methods. This confirms that outliers can indeed be hidden in mixture distributions in real-world applications and that taking this into account results in better outlier detection."}, {"heading": "II. RELATED WORK", "text": "Although most previous work on outlier detection has been done in statistics, there are also clustering-based [9], nearest neighbour-based [10], classification-based [11] and spectral-based [12] outlier detection algorithms. Statistical approaches can be categorised as: distribution-based [2], where a standard distribution is used to fit the data; distancebased [13], where the distance to neighbouring points are used to classify outliers versus non-outliers; and densitybased, where the density of a group of points is estimated to determine an outlier score. While classification, clusteringand distribution-based algorithms aim to find global outliers by comparing each data point to (a representation of) the complete dataset, distance- and density-based algorithms detect local outliers. We next describe the methods most relevant to our paper:\nLocal Outlier Factor (LOF) [3] was the first algorithm to introduce the concept of local density to identify outliers. The authors also claim that they are the first to use a (continuous) \u2018outlier factor\u2019 rather than a Boolean outlier class. The LOF algorithm uses a user-defined parameter, MinPts, that determines the local neighbourhood used for computing the outlier factor for each data point. The outcome of the algorithm strongly depends on this setting. One of the disadvantages of the LOF algorithm is that it is hard to tune the MinPts parameter. Quite some modifications and/or enhancements of LOF, such as the Incremental Local Outlier Factor (ILOF) [14] algorithm, have been proposed. ILOF is a modification of LOF that can handle large data streams and compute local outlier factors on-the-fly. It also updates the profiles of already calculated data points since the profiles may change over time. Local Correlation Integral (LOCI) [4] detects outliers and groups of outliers (small clusters) using the multigranularity deviation factor (MDEF). If a point differs more than three standard deviations from the local average MDEF, it is labelled as outlier. This method uses two neighbourhood definitions: one neighbourhood to use for the average granularity (density) and one neighbourhood for the local granularity of a given point. The setting of these \u03b1 and r determines the complexity and accuracy of the algorithm. Typically \u03b1 is set to 0.5 and r is set in such way that it always covers at least 20 neighbours. Local Outlier Probabilities (LoOP) [5] is also similar to LOF but does not provide an outlier factor. Instead, it provides the probability of a point being an outlier using the probabilistic set distance of a point to its k nearest neighbours. Given this distance and the distances of its neighbours, a Probabilistic Local Outlier Factor (PLOF) is computed and normalised. We will build\nupon LoOP in this paper. Subspace Outlier Detection (SOD) [7] is an algorithm that\nsearches for outliers in meaningful subspaces of the data space or even in arbitrarily-oriented subspaces [8]. Other work in the area of spatial data uses special spatial attributes to define neighbourhood and usually one other attribute to find outliers that deviate in this attribute given its spatial neighbours [15], [16].1 Outlier Ranking (OutRank) [18] determines the degree of outlierness of points using subspace analysis. For the analysis of subspaces it uses clustering methods and subspace similarity measurements. High Contrast Subspaces(HiCS) [6] is a state-of-the-art algorithm that searches for high contrast subspaces in which to perform local outlier detection. It uses LOF as the local outlier detection method for each such subspace, but other algorithms could also be used. Runtime is exponential in the number of dimensions, but this can be reduced by limiting the maximum number of subspaces. We will use an adaptation of HiCS for subspace search.\nOther recent work such as [19] combines density clustering with local and global outlier detection. We will empirically compare to LOF, HiCS, and two variants of LoOP in Section VI, as these are well-studied and representative of the state-of-the-art in local (subspace) outlier detection."}, {"heading": "III. THE PROBLEM", "text": "Many outlier detection (and data mining) algorithms assume\u2014either implicitly or explicitly\u2014that the data is an i.i.d. sample from some underlying distribution. That is, assumed is a dataset D1 drawn from some fixed distribution q1, denoted D1 \u223c q1. Given this, global outliers can be found by approximating q1 from the data, estimating P (d | q1) for all d \u2208 D1, and ranking all data points according to the resulting probabilities or scores.\nIn practice, however, many datasets are mixture distributions of multiple components. Consider for example a dataset D2 consisting of a mixture of two components C1 and C2, drawn from two different distributions, i.e., D2 = C1 \u222a C2, C1 \u223c q1, and C2 \u223c q2. Globally scoring and ranking outliers now becomes a very challenging task, as identifying the underlying distributions is a hard problem and different components may have different characteristics (such as overall density, attribute-value marginals, etc.).\nLocal outlier detection algorithms address this problem by considering distances or densities locally in the dataset, i.e., within the neighbourhood of each individual data point. Although this approach generally works well, it has the disadvantage that it breaks down on high-dimensional datasets, for which all distances become similar; no data points are much further apart than others.\n1More details and a comparison of these algorithms can be found in [17].\nThis problem can be addressed by using a local subspace outlier detection algorithm such as HiCS [6]. That is, given a dataset D consisting of data points over a feature space F , these methods search for local outliers within feature subspaces F \u2282 F . Each reported outlier is associated with a subspace F , explaining in which features the data point is different from its neighbours.\nHowever, as argued in the Introduction, this approach suffers from a severe limitation: existing approaches do not take into account that datasets may be mixtures of multiple components. That is, when searching for local outliers within a feature space F , the density is locally estimated using a neighbourhood determined using the dataset projected onto the feature subspace only. Unfortunately, as we will see next, this may have very undesirable side-effects.\nThat is, consider again our mixture dataset D2. Suppose that a data point o \u2208 C1, i.e., drawn from q1, is a clear outlier in a (small) subspace F , but its values for F are very normal for data points drawn from q2. Then outlier o may go completely undetected by using existing algorithms:\n1) First, because the data is high-dimensional, global outlier detection methods do not consider o to be far away from other data points in C1 (o is only different in the feature set F ); 2) Second, local outlier detection suffers from the same problem when considering all features; 3) Finally, local subspace outlier detection will not find the outlier either: the neighbourhood of o based on D2 projected onto F consists of members of component C2. Although o does not belong to that component, it is in fact very close to those \u2018neighbours\u2019 and is therefore not considered an outlier!\nSummarising, existing methods cannot detect outliers that 1) are confined to a feature subspace but 2) can only be observed within the global neighbourhood of the outlier, i.e., when the outlier is compared to data points belonging to the same component. This leads to the following definition.\nProblem 1 (Subspace Outlier in Global Neighbourhood): Given a dataset D over features F and neighbourhood size k, we define the probability p that a data point d \u2208 D is a subspace outlier in global neighbourhood w.r.t. F \u2286 F as\npF,k(d) = P (\u03c0F (d) | \u03c0F (NNk(d))),\nwhere \u03c0F (X) denotes X projected onto F and NNk(d) denotes d\u2019s global k-neighbourhood, i.e., the k data points closest to d in D (over all features F).\nThat is, it is our aim to estimate the probability that a data point is an outlier within a feature subspace, but relative to its neighbours in the complete, global feature space. In the following two sections we will introduce the concepts and theory needed to accomplish this. Note that we will often drop k from pF,k as this is usually a constant.\nBefore that, however, it is important to observe that we use the global feature space only to determine a reference\ncollection, after which any subspace can be considered for the actual estimation of the outlier probabilities. Although the absolute distances between the data points in F will be small when the data is high-dimensional, a ranking of data points based on distances from a given d is likely to result in neighbourhoods that primarily consist of data points belonging to the same component as d. That is, we implicitly assume that the components of the mixture are\u2014to a large extent\u2014separable in the global feature space, but this seems very reasonable for the setting that we consider."}, {"heading": "IV. PRELIMINARIES", "text": "In this section we briefly describe LoOP [5] and HiCS [6], as we will build upon both techniques for our own algorithm, which we will introduce in the next section. The main reason for choosing LoOP is that it closely resembles the well-known LOF procedure but normalises the outlier factors to probabilities, making interpretation much easier. Further, we use an adapted version of the HiCS algorithm to search for relevant subspaces when there is no set of candidate subspaces known in advance.\nLoOP [5] Given neighbourhood size k and data point d, LoOP computes the probability that d is an outlier. This probability is derived from a so-called standard distance from d to reference points S:\n\u03c3(d, S) =\n\u221a\u2211 s\u2208S dist(d, s) 2\n|S| , (1)\nwhere dist(x, y) is the distance between x and y given by a distance metric (e.g., Euclidean or Manhattan distance).\nThen, the probabilistic set distance of a point d to reference points S with \u2018significance\u2019 \u03bb (usually 3, corresponding to 98% confidence) is defined as\npdist(\u03bb, d, S) = \u03bb \u2217 \u03c3(d, S). (2)\nFrom the following step onward nearest neighbours are used as reference sets. That is, given neighbourhood size k and significance \u03bb, define the Probabilistic Local Outlier Factor (PLOF) of data point d as\nPLOF\u03bb,k(d) = pdist(\u03bb, d,NNk(d))\nIEs\u2208NNk(d)[pdist(\u03bb, s,NNk(s))] \u22121. (3)\nFinally, this is used to define Local Outlier Probabilities. Definition 1 (Local Outlier Probability (LoOP)): Given the previous, the probability that a data point d \u2208 D is a local outlier is defined as:\nLoOP\u03bb,k(d) = max\n{ 0, erf ( PLOF\u03bb,k(d)\nnPLOF \u00b7 \u221a 2 )} where nPLOF = \u03bb \u00b7 Stddev(PLOF ), i.e., the standard deviation of PLOF values assuming a mean of 0, and erf is the standard Gauss error function.\nHiCS [6] HiCS is an algorithm that performs an Apriorilike, bottom-up search for subspaces manifesting a high\ncontrast, i.e., subspaces in which the features have high conditional dependences. For a given candidate subspace it randomly selects data slices so that a statistical test can be used to assess whether the features in the subspace are conditionally dependent. To make this procedure robust, this is repeated a number of times (Monte Carlo sampling) and the resulting p-values are averaged. Although the method was originally evaluated using both the Kolmogorov-Smirnov test and Welch\u2019s t-test, we here choose the former as this does not require any (parametric) assumptions about the data. Parameters are the number of Monte Carlo samples M (= 50, default value), test statistic size \u03b1 (= 0.1), and candidate_cutoff (= 400), which limits the number of subspace candidates considered."}, {"heading": "V. THE GLOSS ALGORITHM", "text": "We introduce GLOSS, for Global\u2013Local Outliers in SubSpaces, an algorithm for finding local, density-based subpace outliers in global neighbourhoods, as defined in Problem 1. On a high level, GLOSS, shown in Algorithm 1, employs the following procedure. First, if no subspaces are given a subspace search method is used to find suitable subspaces (Line 1). Then, the global k-neighbourhood is computed for each data point in the data (2\u20133). After that, for each data point an outlier probability is computed for each considered subspace, relative to its global neighbourhood (4\u20139). Finally, these outlier probabilities are returned as result (10).\nAs the algorithm computes an outlier probability for each combination of data point and subspace, the probabilities need to be aggregated in order to rank the data points according to outlierness. As we are interested in strong outliers in any subspace, we will use the maximum outlier probability found for a data point, i.e., p(d) = maxF\u2208F (pF (d)). Using the average, for example, would give very low outlier probabilities for data points that [only] strongly deviate in a small subspace.\nMore in detail, GLOSS builds upon both LoOP and HiCS by integrating both algorithms and adapting them to the global neighbourhood setting that we consider in this paper. The details of outlier detection and subspace search will be described in the next two subsections."}, {"heading": "A. Global Local Outlier Probabilities", "text": "First, we introduce the extended standard distance, inspired by LoOP, which incorporates 1) a feature subspace F and 2) a global neighbourhood relation G:\n\u03c3(dF , Gd) =\n\u221a\u2211 s\u2208Gd dist(dF , sF ) 2\n|Gd| , (4)\nwhere dF and sF are shortcuts for \u03c0F (d) and \u03c0F (s) respectively, and Gd is the global neighbourhood defined as Gd = NNk(d).\nThen, using probabilistic set distance as defined in the previous section together with the extended standard distance, we define the Probabilistic Global Local Outlier Factor PGLOF as:\nPGLOF\u03bb,Gd(dF ) = pdist(\u03bb, dF , Gd)\nIEs\u2208Gd [pdist(\u03bb, s,Gs)] \u2212 1 (5)\nFinally, a subspace outlier probability pF,k(d) is computed for each data point and subspace according to Definition 1, but using PGLOF instead of PLOF ; see Line 9 of Algorithm 1. That is, with the global neighbourhood projected onto the features in the selected subspace.\nAlgorithm 1 GLOSS Given: Dataset D, neighbourhood size k, optional: sub-\nspaces F 1: F = SubspaceSearch(D) # Only if F not given 2: for all d \u2208 D do: 3: Gd = NNk(d) 4: for all d \u2208 D do 5: for all F \u2208 F do 6: \u03c3(dF , Gd) = \u221a\u2211 s\u2208Gd dist(dF ,sF )2\n|Gd|\n7: pdist(\u03bb, dF , Gd) = \u03bb \u00b7 \u03c3(dF , Gd) 8: PGLOF\u03bb,Gd(dF ) =\npdist(\u03bb,dF ,Gd) IEs\u2208Gd [pdist(\u03bb,s,Gs)] \u2212 1 9: pF,k(d) = max { 0, erf ( PGLOF\u03bb,Gd (dF )\nnPGLOF \u00b7 \u221a 2 )} 10: return p"}, {"heading": "B. Subspace Search", "text": "GLOSS can either perform subspace search or use a given set of relevant subspaces. In the latter case, the subspace search (Line 1 in Algorithm 1) is skipped. By parametrising this, we allow background knowledge to be used to reduce the number of subspaces whenever possible, hence avoiding an exponential search for subspaces and thus reducing runtime. In the manufacturing case study that we will present in Subsection VI-D, for example, there is a natural collection of subspaces that can be exploited.\nWhen subspace search is enabled, the search procedure of HiCS is used. However, instead of testing each feature of a candidate subspace against the remaining subspace features, GLOSS tests each candidate subspace feature against the remainder of the entire feature space, emphasizing the relation between local and global spaces. As such, the algorithm searches for subspaces that exhibit high contrast relative to the global feature space. Because subspace search is adapted from HiCS, the parameters and their default values are the same as those described in Section IV."}, {"heading": "VI. EXPERIMENTS", "text": "We evaluate GLOSS on 1) synthetic data, 2) benchmark data with implanted outliers, 3) benchmark data with the\nminority class as outlier class, and 4) a real-world dataset provided by an industrial partner. The source code and experimental setup can also be found on the Github repository2.\nThe second and third experiment are available in the preprint version of this paper on arXiv3. In the first experiment, in Subsections VI-A, we simulate an (unbalanced) Boolean classification task where the class labels are 1) outlier and 2) not an outlier. This is a very common approach in outlier detection, because objective evaluation is very hard otherwise. Performance is quantified by 1) Area Under the Curve (AUC) of the ROC curve and 2) runtime.\nWe compare GLOSS to LoOP, LOF, HiCS, and LoOP local, a variant of LoOP that detects outliers in each 2D subspace and then assigns the maximum probability over all subspaces to the data point. For all algorithms the neighbourhood size k is set to 20, which is considered to be sufficiently large; the distance metric is set to Euclidean. For both HiCS and GLOSS, the parameters are set to their defaults and the maximum number of subspaces considered is also set to the default: 100."}, {"heading": "A. Synthetic Data", "text": "Setup We first devise a generative model to generate data with known outliers that satisfy the assumptions of our problem statement: the data is a mixture of samples from different distributions, and outliers have values sampled from another distribution for some random subspace. More formally, the generative process generates a dataset D with features F and clusters C, where each cluster c \u2208 C is assigned a random center \u00b5c and variance \u03c32c. Each data point d \u2208 D is assigned to one of the clusters uniformly at random, denoted C(d), and then sampled from a normal distribution with specified center and variance:\n\u2200d \u2208 D : d\u2190 N (\u00b5C(d), \u03c32C(d)).\nAfter generating the mixed dataset, outliers O are introduced by changing a random subset of the features for some of the data points. Given a data point o, a random F \u2282 F and a randomly chosen cluster r 6= C(o), o is marked as outlier and o projected onto F is changed as follows:\noF \u2190 N (\u00b5r, \u03c32r)F .\nExperiments are performed on synthetic datasets with 1000 data points, of which 50 are marked as outliers. The number of dimensions d is set to 10, 20, 50, 100, 200 or 400; the number of clusters tested are 2, 3 and 5; and \u00b5 is per dimension randomly drawn from [0, 2], [0, 3], [0, 5] or [0, 10] (\u03c32c is fixed to 1). This results in 18 parameter settings per dimensionality.\nResults Figure 2 shows ROC curves for all algorithms per dimensionality, using 3 clusters and \u00b5 drawn from [0, 3].\n2GLOSS GitHub repository: https://github.com/Basvanstein/Gloss 3For all experiments see the preprint on arXiv: [cs.LG]\nTable I presents the obtained AUC scores and runtimes averaged over all 18 runs per dimensionality. It can be observed from Table I that the purely local subspace analysis done by Local LoOP completely fails to identify the \u2018hidden\u2019 outliers, whereas HiCS and the global outlier detection methods fail when the number of dimensions increases. GLOSS, on the other hand, is able to detect most outliers even when the dimensionality increases all the way up to 400. From the ROC curves in Figure 2 it can be observed that GLOSS tends to find many more outliers at very low false positive rates, while other algorithms only manage to catch up once the false positives rate increases substantially. From the results per individual parameter setting (not included here but available on GitHub), we can see that a higher \u00b5 makes it easier for all algorithms to detect the outliers. This makes sense, since the clusters become more separated and therefore the impact of the local deviation on the global space will be higher. The number of clusters in the data does not seem to be of substantial importance."}, {"heading": "B. Benchmark Data with Implanted Outliers", "text": "Setup We next compare GLOSS to its competitors using a large set of well-known benchmark data from the UCI machine learning repository [20]: Ann Thyroid, Arrhythmia, Glass, Diabetes, Ionosphere, Pen Digits 16, Segments, Ailerons, Pol, Waveform 5000, Mfeat Fourier and Optdigits.\nPrevious papers usually considered the minority class as \u2018outlier class\u2019 for purposes of evaluation, but this clearly would not demonstrate the strengths of our approach: we assume the data to be a mixture of components (i.e., classes), and we search for outliers within those classes. We therefore use the UCI datasets as examples of realistic data and implant artificial outliers. That is, we pick a random sample of 10% of the data points and transform each such data point to an outlier by replacing a randomly picked subspace with the values of a data point from a different class (the size of each subspace was chosen uniformly from [2,max(2, 0.1 \u2217 d)]). Note that the datasets most likely already contain \u2018natural\u2019 outliers, which makes the task at hand even more difficult.\nResults Figure 3 shows the effect of the neighbourhood size on the performance of GLOSS. It can be seen that the method is very robust with respect to this setting. As k = 20 is considered to be a \u201cgood\u201d choice in literature for LoOP and\nrelated local outlier detection methods, we chose this as the default for all other experiments and methods.\nTable III presents average AUC scores and running times over ten runs per dataset, together with basic dataset properties, for all competing methods. GLOSS clearly outperforms its competitors for most datasets when it comes to AUC and is about as fast as HiCS. From this we can conclude that it is beneficial to use GLOSS when the data consists of multiple components and outliers may be hidden as a result of that."}, {"heading": "C. Benchmark Data with Minority Class as Outliers", "text": "Setup We do not expect using the minority class of a dataset as outlier class to demonstrate the strengths of our approach. Nevertheless, we do not want our improved algorithm to perform worse on the regular local outlier detection task either. Hence, we also compare GLOSS to its competitors using the same benchmark datasets but with outliers defined by the more usual procedure of using the minority class as \u2018outlier class\u2019. Apart from that, we use the same setup and parameters as in Section VI-B. Results Table IV presents the average AUC scores obtained over ten runs per dataset. The results show that GLOSS performs pretty much on par with the state-of-theart, demonstrating that our proposed method is capable of detecting \u2018regular\u2019 outliers as well as the ones that GLOSS identifies but other methods miss (see previous subsections)."}, {"heading": "D. Case Study: Outlier Detection for Quality Control", "text": "The last series of experiments of this section are performed on a proprietary dataset made available by the BMW Group at plant Regensburg. This dataset was one the motivations for this work: the data is high-dimensional and a mixture of different, unknown components. Moreover, it is essential for BMW to be able to identify any outliers in the data, as this directly influences their car manufacturing process.\nThe data concerns steel coils, which is the raw material used as input at the stamping plant (also called \u2018press shop\u2019).\nBefore entering the stamping process, each coil\u2014of 2\u20133 km long\u2014is unrolled and cut into shorter pieces. During this process, a large number of measurements is made. We aim to use these measurements to detect steel coils that strongly deviate from a typical coil in some specific region. A complicating factor is that the data contains measurements for different types of steel from different suppliers, but this important information is not available in the data. Hence, we are dealing with mixed data and we are thus facing exactly the problem formalised as Problem 1, for which we proposed GLOSS as solution.\nSetup The dataset, containing all measurements done from December 2014 to December 2015, consists of 2204 data points and has 1200 dimensions, grouped into 100 12- dimensional subspaces using the spatial aspects of the data.\nEach data point represents a coil having 100 segments (in length) and 3 tracks (in width). The most important measurements [21], and the ones we use, are Impoc, quantifying magnetic properties of the steel, and Oil levels, quantifying the amount of oil on the coil. Each subspace consists of 3 Impoc and 9 Oil level values averaged over a segment of size 2% of the length of the coil; the 100 subspaces are consecutive, overlapping segments covering the entire coil.\nWe compare GLOSS to LoOP using all global features and to Local LoOP ran on each of the 100 individual segments/subspaces. Other algorithms are not included in the evaluation because of the high dimensionality of the data; runtimes would be unreasonably long.\nResults As expected, LoOP is unable to detect local outliers: it does not take advantage of the spatial information and cannot deal with the very large number (1200) of dimensions. The results obtained by GLOSS and our Local LoOP variant are generally similar, but are substantially\u2014 and importantly\u2014different for some of the steel coils, as we will show in detail shortly. Moreover, Local LoOP is slower than GLOSS, since the neighbourhood of a coil needs to be computed for each individual subspace, whereas GLOSS only needs to compute a global neighbourhood once.\nWe now zoom in on the 512 coils recorded in March 2015, a representative month. By focusing on data from a specific month, we simulate the setting in which the stamping plant operator will inspect the results in the future; GLOSS is currently being implemented in the production environment at BMW. Given that deviations in the steel coils directly\ninfluence the manufacturing process, this is expected to improve the stability of the process and the quality of the products.\nWhen comparing the outlier rankings obtained with GLOSS and Local LoOP for this particular month, we observe that many top outliers appear in high positions in both rankings. However, 1) some coils are ranked very differently by the two approaches and 2) GLOSS ranks some\ncoils as outliers that Local LoOP does not. Two such coils are depicted in Figure 4 and 5, showing both the outlier probabilities computed by both methods, and the Impoc and Oil level measurements. While GLOSS ranks this coil 5th and 14th respectively, Local LoOP ranks them 103th and 79th. Clearly an operator would inspect this coil, labelled B1, if GLOSS were used to rank the coils, but not if Local LoOP would have been used. We asked a domain expert to inspect the measurements and outlier probabilities of this coil and others. He reported back to us that the probabilities computed using GLOSS more accurately reflect the extend to which the coils are outliers.\nNext, to further validate the rankings provided by our method, a domain expert of BMW was shown two top-10 outlier coil rankings, one obtained by GLOSS and one by Local LoOP (without duplicates; a coil was left out from a ranking if it was ranked higher by the other method). Of course, the test was blind, i.e., the domain expert did not know which method generated which ranking. For each coil in either top-10, the domain expert was shown the plots as in Figures 4 and 5, but only with the outlier probabilities for the corresponding method. Given the two rankings and plots, the domain expert was asked to rank the 20 (unique) coils according to the perceived degree of outlierness from the domain perspective. Table V shows the labels for the coils in the top-10 rankings of Local LoOP and GLOSS, plus the ranking given by the domain expert (using these labels). It is striking that the top four coils selected by the domain expert were all selected by GLOSS, with the top ranked coil being the same coil as the top ranked coil identified by GLOSS. This confirms that our proposed algorithm is capable of detecting and ranking important outliers that existing algorithms overlook.\nFor the application at our industrial partner, deviations in the measurements often indicate problems with the material and these may cause problems during the manufacturing process. Per year, over 100 000 coils are processed at this plant, making it infeasible for operators to inspect every single coil. Thus, GLOSS will help to narrow this down by providing outlier rankings and probabilities."}, {"heading": "VII. CONCLUSIONS", "text": "Motivated by a real-world problem from the automotive industry, we introduced the generic Local Subpace Outlier in Global Neighbourhood problem, and GLOSS, an algorithm that addresses this problem. To enable accurate local subspace outlier detection in high-dimensional data that is a mixture of components, GLOSS uses neighbourhoods selected in the global data space. The experiments show that GLOSS outperforms state-of-the-art algorithms in finding local subspace outliers. Moreover, the experiments show that not only local subspace outliers can be found by GLOSS, but GLOSS performs on par with the state-of-the-art on the regular outlier detection task. The case study on highdimensional measurement data from steel coils demonstrates that GLOSS is capable at finding relevant local subspace outliers that would otherwise remain undetected, confirming that one should keep an eye on the global perspective even when performing local outlier detection."}], "references": [{"title": "A survey of outlier detection methodologies", "author": ["V.J. Hodge", "J. Austin"], "venue": "Artificial Intelligence Review, vol. 22, no. 2, pp. 85\u2013126, 2004.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Outliers in statistical data", "author": ["V. Bamnett", "T. Lewis"], "venue": "Journal of the Royal Statistical Society. Series A (General), vol. 141, no. 4, 1994.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "LOF: Identifying Density-Based Local Outliers", "author": ["M.M. Breunig", "H.-P. Kriegel", "R.T. Ng", "J. Sander"], "venue": "ACM SIGMOD Record, vol. 29, no. 2, pp. 93\u2013104, jun 2000. [Online]. Available: http://portal.acm.org/citation.cfm?doid= 335191.335388", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Loci: Fast outlier detection using the local correlation integral", "author": ["S. Papadimitriou", "H. Kitagawa", "P.B. Gibbons", "C. Faloutsos"], "venue": "Data Engineering, 2003. Proceedings. 19th International Conference on, pp. 315\u2013326, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "LoOP: local outlier probabilities", "author": ["H.-P. Kriegel", "P. Kr\u00f6ger", "E. Schubert", "A. Zimek"], "venue": "Proceedings of the 18th ACM conference on Information and knowledge management, pp. 1649\u20131652, 2009. [Online]. Available: http://doi.acm.org/10.1145/1645953.1646195", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Hics: high contrast subspaces for density-based outlier ranking", "author": ["F. Keller", "E. M\u00fcller", "K. B\u00f6hm"], "venue": "Data Engineering (ICDE), 2012 IEEE 28th International Conference on. IEEE, 2012, pp. 1037\u20131048.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Outlier detection in axis-parallel subspaces of high dimensional data", "author": ["H.-P. Kriegel", "P. Kr\u00f6ger", "E. Schubert", "A. Zimek"], "venue": "Advances in Knowledge Discovery and Data Mining. Springer, 2009, pp. 831\u2013838.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Outlier detection in arbitrarily oriented subspaces", "author": ["H. Kriegel", "P. Kroger", "E. Schubert", "A. Zimek"], "venue": "Data Mining (ICDM), 2012 IEEE 12th International Conference on. IEEE, 2012, pp. 379\u2013388.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Outlier detection using clustering methods: a data cleaning application", "author": ["A. Loureiro", "L. Torgo", "C. Soares"], "venue": "Proceedings of KDNet Symposium on Knowledge-based Systems for the Public Sector. Bonn, Germany, 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Classification based outlier detection techniques", "author": ["S. Upadhyaya", "K. Singh"], "venue": "International Journal of Computer Trends and Technology, vol. 3, no. 2, pp. 294\u2013298, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Outlier detection for stationary time series", "author": ["K. Choy"], "venue": "Journal of Statistical Planning and Inference, vol. 99, no. 2, pp. 111\u2013127, 2001.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Distance-based outliers: algorithms and applications", "author": ["E.M. Knorr", "R.T. Ng", "V. Tucakov"], "venue": "The VLDB Journal\u2013 The International Journal on Very Large Data Bases, vol. 8, no. 3-4, pp. 237\u2013253, 2000.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2000}, {"title": "Incremental Local Outlier Detection for Data Streams", "author": ["D. Pokrajac", "A. Lazarevic", "L.J. Latecki"], "venue": "Computational Intelligence and Data Mining, 2007. CIDM 2007. IEEE Symposium on, no. February, pp. 504\u2013515, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Gls-sod: a generalized local statistical approach for spatial outlier detection", "author": ["F. Chen", "C.-T. Lu", "A.P. Boedihardjo"], "venue": "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2010, pp. 1069\u20131078.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Spatial outlier detection: Random walk based approaches", "author": ["X. Liu", "C.-T. Lu", "F. Chen"], "venue": "Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems. ACM, 2010, pp. 370\u2013379.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Local outlier detection reconsidered: a generalized view on locality with applications to spatial, video, and network outlier detection", "author": ["E. Schubert", "A. Zimek", "H.-P. Kriegel"], "venue": "Data Mining and Knowledge Discovery, vol. 28, no. 1, pp. 190\u2013237, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Outlier ranking via subspace analysis in multiple views of the data", "author": ["E. Muller", "I. Assent", "P. Iglesias", "Y. Mulle", "K. Bohm"], "venue": "Data Mining (ICDM), 2012 IEEE 12th International Conference on. IEEE, 2012, pp. 529\u2013538.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Hierarchical density estimates for data clustering, visualization, and outlier detection", "author": ["R.J.G.B. Campello", "D. Moulavi", "A. Zimek", "J. Sander"], "venue": "ACM Trans. Knowl. Discov. Data, vol. 10, no. 1, pp. 5:1\u20135:51, Jul. 2015. [Online]. Available: http://doi.acm.org/10.1145/2733381", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "UCI machine learning repository", "author": ["K. Bache", "M. Lichman"], "venue": "http://archive.ics.uci.edu/ml, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Stamping plant 4.0\u2013basics for the application of data mining methods in manufacturing car body parts", "author": ["S. Purr", "J. Meinhardt", "A. Lipp", "A. Werner", "M. Ostermair", "B. Gl\u00fcck"], "venue": "Key Engineering Materials, vol. 639. Trans Tech Publ, 2015, pp. 21\u201330.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Outlier detection [1] is an important task that has applications in many domains.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "Many traditional outlier detection methods [2]", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "To address both this problem and the limitations of (global) outlier detection, local outlier detection methods [3]\u2013[5] have been proposed over the past few decades.", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "To address both this problem and the limitations of (global) outlier detection, local outlier detection methods [3]\u2013[5] have been proposed over the past few decades.", "startOffset": 116, "endOffset": 119}, {"referenceID": 5, "context": "To further improve on this, local subspace outlier detection methods [6]\u2013[8] have been introduced.", "startOffset": 69, "endOffset": 72}, {"referenceID": 7, "context": "To further improve on this, local subspace outlier detection methods [6]\u2013[8] have been introduced.", "startOffset": 73, "endOffset": 76}, {"referenceID": 4, "context": "Following this, our second contribution is the introduction of the GLOSS algorithm, which combines our ideas on outlier detection using global neighbourhoods with techniques from LoOP [5] and HiCS [6].", "startOffset": 184, "endOffset": 187}, {"referenceID": 5, "context": "Following this, our second contribution is the introduction of the GLOSS algorithm, which combines our ideas on outlier detection using global neighbourhoods with techniques from LoOP [5] and HiCS [6].", "startOffset": 197, "endOffset": 200}, {"referenceID": 8, "context": "Although most previous work on outlier detection has been done in statistics, there are also clustering-based [9], nearest neighbour-based [10], classification-based [11] and spectral-based [12] outlier detection algorithms.", "startOffset": 110, "endOffset": 113}, {"referenceID": 9, "context": "Although most previous work on outlier detection has been done in statistics, there are also clustering-based [9], nearest neighbour-based [10], classification-based [11] and spectral-based [12] outlier detection algorithms.", "startOffset": 166, "endOffset": 170}, {"referenceID": 10, "context": "Although most previous work on outlier detection has been done in statistics, there are also clustering-based [9], nearest neighbour-based [10], classification-based [11] and spectral-based [12] outlier detection algorithms.", "startOffset": 190, "endOffset": 194}, {"referenceID": 1, "context": "Statistical approaches can be categorised as: distribution-based [2], where a standard distribution is used to fit the data; distancebased [13], where the distance to neighbouring points are used to classify outliers versus non-outliers; and densitybased, where the density of a group of points is estimated to determine an outlier score.", "startOffset": 65, "endOffset": 68}, {"referenceID": 11, "context": "Statistical approaches can be categorised as: distribution-based [2], where a standard distribution is used to fit the data; distancebased [13], where the distance to neighbouring points are used to classify outliers versus non-outliers; and densitybased, where the density of a group of points is estimated to determine an outlier score.", "startOffset": 139, "endOffset": 143}, {"referenceID": 2, "context": "Local Outlier Factor (LOF) [3] was the first algorithm to introduce the concept of local density to identify outliers.", "startOffset": 27, "endOffset": 30}, {"referenceID": 12, "context": "Quite some modifications and/or enhancements of LOF, such as the Incremental Local Outlier Factor (ILOF) [14] algorithm, have been proposed.", "startOffset": 105, "endOffset": 109}, {"referenceID": 3, "context": "Local Correlation Integral (LOCI) [4] detects outliers and groups of outliers (small clusters) using the multigranularity deviation factor (MDEF).", "startOffset": 34, "endOffset": 37}, {"referenceID": 4, "context": "Local Outlier Probabilities (LoOP) [5] is also similar to LOF but does not provide an outlier factor.", "startOffset": 35, "endOffset": 38}, {"referenceID": 6, "context": "Subspace Outlier Detection (SOD) [7] is an algorithm that", "startOffset": 33, "endOffset": 36}, {"referenceID": 7, "context": "searches for outliers in meaningful subspaces of the data space or even in arbitrarily-oriented subspaces [8].", "startOffset": 106, "endOffset": 109}, {"referenceID": 13, "context": "Other work in the area of spatial data uses special spatial attributes to define neighbourhood and usually one other attribute to find outliers that deviate in this attribute given its spatial neighbours [15], [16].", "startOffset": 204, "endOffset": 208}, {"referenceID": 14, "context": "Other work in the area of spatial data uses special spatial attributes to define neighbourhood and usually one other attribute to find outliers that deviate in this attribute given its spatial neighbours [15], [16].", "startOffset": 210, "endOffset": 214}, {"referenceID": 16, "context": "1 Outlier Ranking (OutRank) [18] determines the degree of outlierness of points using subspace analysis.", "startOffset": 28, "endOffset": 32}, {"referenceID": 5, "context": "High Contrast Subspaces(HiCS) [6] is a state-of-the-art algorithm that searches for high contrast subspaces in which to perform local outlier detection.", "startOffset": 30, "endOffset": 33}, {"referenceID": 17, "context": "Other recent work such as [19] combines density clustering with local and global outlier detection.", "startOffset": 26, "endOffset": 30}, {"referenceID": 15, "context": "1More details and a comparison of these algorithms can be found in [17].", "startOffset": 67, "endOffset": 71}, {"referenceID": 5, "context": "outlier detection algorithm such as HiCS [6].", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "In this section we briefly describe LoOP [5] and HiCS [6], as we will build upon both techniques for our own algorithm, which we will introduce in the next section.", "startOffset": 41, "endOffset": 44}, {"referenceID": 5, "context": "In this section we briefly describe LoOP [5] and HiCS [6], as we will build upon both techniques for our own algorithm, which we will introduce in the next section.", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "LoOP [5] Given neighbourhood size k and data point d, LoOP computes the probability that d is an outlier.", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "HiCS [6] HiCS is an algorithm that performs an Apriorilike, bottom-up search for subspaces manifesting a high contrast, i.", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "dimension randomly drawn from [0, 2], [0, 3], [0, 5] or [0, 10] (\u03c3c is fixed to 1).", "startOffset": 30, "endOffset": 36}, {"referenceID": 2, "context": "dimension randomly drawn from [0, 2], [0, 3], [0, 5] or [0, 10] (\u03c3c is fixed to 1).", "startOffset": 38, "endOffset": 44}, {"referenceID": 4, "context": "dimension randomly drawn from [0, 2], [0, 3], [0, 5] or [0, 10] (\u03c3c is fixed to 1).", "startOffset": 46, "endOffset": 52}, {"referenceID": 2, "context": "Results Figure 2 shows ROC curves for all algorithms per dimensionality, using 3 clusters and \u03bc drawn from [0, 3].", "startOffset": 107, "endOffset": 113}, {"referenceID": 18, "context": "Setup We next compare GLOSS to its competitors using a large set of well-known benchmark data from the UCI machine learning repository [20]: Ann Thyroid, Arrhythmia, Glass, Diabetes, Ionosphere, Pen Digits 16, Segments, Ailerons, Pol, Waveform 5000, Mfeat Fourier and Optdigits.", "startOffset": 135, "endOffset": 139}, {"referenceID": 2, "context": "ROC curves for each algorithm and per dimensionality, with \u03bc randomly drawn from [0, 3] and 2 clusters per dataset.", "startOffset": 81, "endOffset": 87}, {"referenceID": 19, "context": "The most important measurements [21], and the ones we use, are Impoc, quantifying magnetic properties of the steel, and Oil levels, quantifying the amount of oil on the coil.", "startOffset": 32, "endOffset": 36}], "year": 2016, "abstractText": "Outlier detection in high-dimensional data is a challenging yet important task, as it has applications in, e.g., fraud detection and quality control. State-of-the-art densitybased algorithms perform well because they 1) take the local neighbourhoods of data points into account and 2) consider feature subspaces. In highly complex and high-dimensional data, however, existing methods are likely to overlook important outliers because they do not explicitly take into account that the data is often a mixture distribution of multiple components. We therefore introduce GLOSS, an algorithm that performs local subspace outlier detection using global neighbourhoods. Experiments on synthetic data demonstrate that GLOSS more accurately detects local outliers in mixed data than its competitors. Moreover, experiments on real-world data show that our approach identifies relevant outliers overlooked by existing methods, confirming that one should keep an eye on the global perspective even when doing local outlier detection.", "creator": "TeX"}}}