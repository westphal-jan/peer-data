{"id": "1611.00301", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2016", "title": "Recurrent Neural Radio Anomaly Detection", "abstract": "We introduce a powerful recurrent neural network based method for novelty detection to the application of detecting radio anomalies. This approach holds promise in significantly increasing the ability of naive anomaly detection to detect small anomalies in highly complex complexity multi-user radio bands. We demonstrate the efficacy of this approach on a number of common real over the air radio communications bands of interest and quantify detection performance in terms of probability of detection an false alarm rates across a range of interference to band power ratios and compare to baseline methods. The effectiveness of this approach in real time is limited in small sample sizes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 1 Nov 2016 17:17:26 GMT  (4437kb,D)", "http://arxiv.org/abs/1611.00301v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["timothy j o'shea", "t charles clancy", "robert w mcgwier"], "accepted": false, "id": "1611.00301"}, "pdf": {"name": "1611.00301.pdf", "metadata": {"source": "CRF", "title": "Recurrent Neural Radio Anomaly Detection", "authors": ["Timothy J. O\u2019Shea", "T. Charles Clancy", "Robert W. McGwier"], "emails": ["oshea@vt.edu", "tcc@vt.edu", "rwmcgwi@vt.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nAnomaly detection is an important time-series function which is widely used in network security monitoring, medical sensor monitoring, financial change modeling, and any number of other applications. There is a significant body of existing work on the subject in theory and applications [4] [13], we focus here primarily on reconstruction based novelty detection. In radio, applications of anomaly detection have been discussed [7] but not widely used outside of a few small niche applications.\nRadio anomaly detection has been leveraged somewhat in wireless sensor networks such as in [11] [8] [3] [9], but most of these applications focus on detecting changes in sensor data (temperature, pressure, etc), or expert features rather than on anomalies occurring in the high rate raw physical layer radio signal itself. We are unaware of any currently widely used or investigated applications of naive anomaly detection within the raw unmodified radio physical layer rather than on an expert feature such as a detection statistic. The focus on raw RF rather than a specialized statistic is based on the hope that such a technique may be able to generalize well to numerous types of signals, rather than relying on specific analysis to help a single scenario.\nThere are a number of driving motivations for such a capability; for instance for spectrum access enforcement by regulatory bodies in which the appearance of new radio users of any kind of emission on unauthorized bands or with unauthorized equipment or techniques presents an enforcement event which should be rapidly addressed. In commercial and defense communications applications radio anomaly detection also presents the opportunity to rapidly recognize interfering emitters, malfunctioning equipment, or malicious users within their licensed bands and take action. Each of these applications currently requires expensive high maintenance expert systems which often perform a series of steps of energy detection, localization, classification, and comparison to baseline databases, and alerting with large amounts of specialization and tuning\nto the band and signals of interest and taking significant computational horsepower and implementation expense to deploy. By shifting these systems to more generally applicable naive methods using neural networks which can be highly optimized on concurrent architectures such as graphics processing units (GPUs) in a very general way, we offer the potential to make such systems much easier to realize, adapt to new domains, and run leveraging economies of scale on computing power and model primitive optimization."}, {"heading": "II. APPROACH", "text": "Recently, the use of recurrent neural networks to form a predictive reconstruction as part of a novelty detector has been proposed [17] and demonstrated to function quite well on several time series datasets including electrocardiograms, physical telemetry signals, and power consumption metrics. In this work a long short term memory [2] (LSTM) based recurrent network is used to train a time series predictor on a training set which is then used to compute an expected error distribution vs the real signal which is well characterized for non-anomalous behavior. The sequence learning capacity of LSTM has in some cases been shown to exceed that of Hidden Markov model [10] and Kalman based linear predictors, because it is able to take into account a much more complex nonlinear representation of state (does not make the Markov assumption), short and long term transition dependencies, and complex nonlinear output mapping dependencies than either of these prior models are capable of. An overview of this system level model is shown in figure 1.\nIn this case we consider a sampled radio data time series X = {x1, x2, ..., xM}, our training support, where each point is a complex base-band sample in R2. We train a predictor f with learned parameters \u03b8 such that for any start offset k, input samples count N , and prediction length `, we form a sequence regression problem shown in equation 1.\n{x\u0302k+N , ..., x\u0302k+N+`\u22121} = f ({xk, ..., xk+N\u22121, \u03b8NN ) (1)\nThis function f can take the form of any number of different prediction methods, but in this case we evaluate a recurrent neural network (RNN) based predictor leveraging LSTM layers.\nA predictor error vector can then be obtained on the training set by computing the difference ei = xi\u2212 x\u0302i for each predicted value given the known (non-causal) actual values of x over [k + N, k + N + ` \u2212 1]. This error vector is used to\nar X\niv :1\n61 1.\n00 30\n1v 1\n[ cs\n.L G\n] 1\nN ov\n2 01\n6\nestimate of distribution {e0, ..., e`\u22121} \u223c Pe(e(`), \u03b8E) through some form of parametric or non-parametric density estimation. In this case, we model using a parametric multivariate Gaussian distribution.\nAfter fitting both the predictive model parameters \u03b8NN and the error distribution parameters \u03b8E on the dataset, we can now use this model to perform novelty detection by observing regions within the signal x(n) where predictor error deviates significantly from the expected error distribution De , p(e(`), \u03b8E). That is to say, we wish to compute p(xi \u223c De).\nThis can be done by thresholding the log-likelihood of ei in De at some level to make a decision, or by combining V sequential samples of the likelihood and thresholding the aggregate statistic. Our expression used looks roughly like that given below in equation 2.\n\u03c4 H1 \u2277 H0 10 log10 V\u220f v=0 p(e(`)v ) (2)\nHere H1 is the hypothesis that the current values of xi, ..., xi+N for each of the V observations are drawn from a distribution matching that of our training set, representing \u2019normal\u2019 behavior, while H0 is the hypothesis that the current values of xi, ..., xi+N are not drawn from the distribution of the training set, representing an anomaly or novelty being present. Our threshold \u03c4 can be fit using an F\u03b2 or typical constant false alarm rate (CFAR) sorts of analysis [1] if a decent dataset of \u201danomalous\u201d behavior is known, or a false alarm rate on \u201dnormal\u201d behavior is used as the metric."}, {"heading": "III. WIDE-BAND RADIO COMMUNICATIONS TIME-SERIES", "text": "Time-series in the radio domain are quite complex models, especially when considering wideband aggregates of numerous channels on separate frequencies, each with their own temporal and spectral channel access scheme, and each carrying whitened randomized data-bits which typically do not repeat aside from reference tones such as preamble, low-entropyheaders and pilots. A time sequence prediction model for such an aggregate signal must then be able to account for short-time expected symbol transitions and pulse shaping of a each carrier and its channel variations as well as the symbols forming higher level traffic and application sequences representing behavior of users. At both layers we must be able to model the sum of all users and emitters combined into a single shared medium on one or more channels. Differing levels of predictor complexity and predictive capacity define how many of these effects of \u2019normal\u2019 behavior are modeled, defining the scale, complexity, or distance from the norm of the anomaly which can be detected using that model.\nIn figure 2 we show spectrogram plots demonstrating a variety of radio time-series complexities of real world signals which we further consider in thi paper. Here we see examples ranging from the most static, continuously modulated analog FM broadcast carriers, through relatively well structured on a macro-scale but extremely complexly coded and changing on a micro-scale, cellular band carriers of both GSM and LTE with its rapidly changing resource block (RBs) allocations in OFDMA time-frequency slots, and finally to the most chaotic ISM band environment comprised by CSMA/CD WiFi/IEEE802.11 bursts occurring at random times, and frequency hopped BlueTooth/IEEE802.15.1 bursts occurring at random times and frequencies among other emitters.\nEach of these bands presents different complexities which a temporal sequence prediction model needs to capture to accurately form a predictive model for signal behavior. We will consider each of these examples while evaluating our anomaly detection performance and train using these real recorded over the air RF datasets including harsh urban channel fading conditions. Recordings are conducted with an Ettus Research B200-mini [6] which uses the Analog Devices AD9361 RFIC front-end and stored to disk for analysis using GNU Radio [5]."}, {"heading": "IV. PREDICTOR MODELS", "text": "Here we describe each model f ({xk, ..., xk+N\u22121, \u03b8NN ) which we use to predict our next samples of the time-series sequence. For fairness of comparison we normalize each predictor to 32 samples of input and 4 predicted output samples. We include a several baseline models as well as a number of models modeled after state of the art time series learning neural network capabilities."}, {"heading": "A. Kalman Sequence Predictor", "text": "We use a 3rd order Unscented Kalman Filter/Predictor similar to that described in [12]. This is implemented using the FilterPy module [18] and forms our performance benchmark for this paper. This implements a traditional Kalman Novelty Detector as one might do without a learned predictive model. In this case the adaptive filter is tuned online while running and the error distribution is characterized on this."}, {"heading": "B. DNN Sequence Predictor", "text": "In our Dense Neural Network (DNN) model (shown in figure 3), we train a naive fully-connected network as a neural network baseline with a high number of free parameters and heavy dropout allowing it to learn a completely unconstrained mapping between input and output samples. This will allow us to compare other specialized/constrained architectures such as convolutional and recurrent varieties for model fitting appropriateness."}, {"heading": "C. Raw LSTM Sequence Predictor", "text": "In the LSTM based sequence predictor model (Figure 4), we implement a 2-layer LSTM followed by 2 fully connected layers culminating in a linear activation for regression of complex continuous valued sample output values. We regularize between each layer with dropout of 0.5 and using proper LSTM weight and activation dropout as described in [14] and implemented in Keras [15]."}, {"heading": "D. DCNN1 Sequence Predictor", "text": "In the Dilated Convolutional Neural Network 1 (DCNN1) (Figure 5), we introduce a simple dilated convolution layer on the front end of a simple fully connected neural network to allow for learning of convolutional features at a stride of 2."}, {"heading": "E. DCNN2 Sequence Predictor", "text": "We model our Dilated Convolutional Neural Network 2 (DCNN2) architecture on a vastly simplified version of Google\u2019s WaveNet architecture [19] which has demonstrated a strong ability to learn raw time series representations on acoustic voice data. Here we use two levels of dilated convolutions where each is a residual block [16] containing identical layers with hyperbolic tan (TanH) and sigmoid activations merged multiplicatively, followed by a 1x1 convolutional layer for dimensionality reduction."}, {"heading": "V. MODEL OPTIMIZATION", "text": "Before evaluating detection performance, we simply attempt to minimize the mean squared error of the predictor\nfunction to select our network parameters \u03b8NN and our architecture and hyper-parameters for the predictor. From initial experimentation, optimal network architectures seem to vary slightly from dataset to dataset, but for now we seek to use a single set of network parameters for all datasets.\nA much more extensive hyper-parameter search is really desired here to find best suited network structures. We hope in future work to do this more extensively, within the scope of this work we only try a handful of architectures derived from proven architectures in prior work on similar tasks."}, {"heading": "VI. PERFORMANCE EVALUATION", "text": "To evaluate performance, we introduce a number of different classes of synthetic anomalies into each recorded sampled RF dataset and measure detection and false alarm rates using various methods of novelty detection. Our anomaly types considered here span the range of time-frequency support from an instantaneous wide-band pulse, to a narrow-band tone at a single frequency, but are each normalized by total power of the same time support over the window [ts, te). The anomaly classes considered are:\n\u2022 Pulsed Complex Sinusoid: expressed as n(t) = exp(j2\u03c0tFc/Fs) for t \u2208 [ts, te) where Fc \u223c Uniform(\u2212Fs/2, Fs/2).\n\u2022 Short-time Broadband Bursts (Sinc pulse): expressed as n(t) = sinc(2\u03c0(t \u2212 (ts + te)/2)Fc/Fs) for t \u2208 [ts, te).\n\u2022 Brief Periods of Signal Non-Linear Compression: approximated as n(t) = 13x(t)\u2212 3x3(t) for t \u2208 [ts, te).\n\u2022 Pulsed QPSK Signals: where symrate \u223c Uniform(Fs/250, Fs/2), Fc \u223c Uniform(\u2212(Fs \u2212 symrate/2)/2, (Fs \u2212 symrate/2)/2), and a rootraised cosine pulse shaping filter of \u03b1 = 0.3 and N = 11 is applied at the baudrate.\n\u2022 Pulsed Chirp Events: n(t) = exp(j2\u03c0tFc/Fs) for t \u2208 [ts, te) where Fc varies linearly in time from Fc1 \u223c Uniform(\u2212Fs/2, Fs/2) to Fc2 \u223c Uniform(\u2212Fs/2, Fs/2)\nWe characterize each of these anomalies by its interferenceto-band-power ratio (IBR) in dB. We refer to this in some\ninstances as signal-to-noise ratio (SNR) for convenience but the signal of interest here is the anomaly and the \u201dnoise\u201d in this case is the power of the non-anomalous band including all signals therein.\nInspecting figure 7 we can see that performance does vary based on the anomaly type. For instance, performance on nonlinear compression, chirp detection, tone detection, and QPSK burst detection, all appear to be quite a bit stronger in the LLR detection metric than the wideband pulsed noise which is very short in time and results in an anomaly spike which is likewise extremely short in time. We include several runs of bands below in figures 8, 9, and 10 for visual inspection.\nIn figure 11 we show the performance of the tone detector across 100,000 samples of FM recording inserting 50 random\ntone events of length 250 samples. As the IBR approaches -5dB, we have nearly perfect Pd/Pfa performance, while in figure 12, we see for the wideband pulse tone, which has very large instantaneous peak power but a very narrow time-support, the IBR does not have nearly as significant an impact on Pd/Pfa performance at these IBR levels. In this case, our probability of detection represents the probability of detecting all anomalies present in the time range, while our probability of false alarm represents the probability of a false alarm being triggered in any 250-sample window.\nWe can repeat these experiments across a range of interference-to-band power ratios to observe the efficacy in a range of different modulation and multi-access schemes. Results for this are shown below in figure 13. Here we can see that for all channel types are relatively effective once we approach 0-5dB IBR. The most difficult here is the ISM band, where our predictive model is used to seeing bursty CSMA/CD kinds of traffic from WiFi and blue-tooth frequency hopped\nbursts across the band. In this case our anomaly detection ability is the most challenged of all the other bands.\nRepeating this experiment with chirp interference instead of pulse interference, we show performance in figure 14. Again we see excellent performance above 0-5dB in most cases, although the ISM band continues to be the most difficult.\nTo summarize these performance behaviors into a more concise performance number, we fix a constant false alarm rate for comparison of detection performance. In figure 15 we show how detection performance varies across a range of constant false alarm rates for the LTE band using the LSTM model.\nBy repeating this for all models on all band-types, we can then pick a constant false alarm rate to compare performance\nacross our different models. Doing this allows us to compare model performance in different types of emitter and channel environments.\nLooking at these results, we see that in most cases the neural network based predictors outperform the Kalman based predictors slightly. In the case of cellular networks, both GSM and LTE where a much more regular and structured temporal pattern on each carrier exists, we see slightly larger\nimprovements in performance, likely due to having better learned a temporal predictive model suited to this behavior."}, {"heading": "VII. CONCLUSION", "text": "In this paper we have shown how the neural network reconstruction-based anomaly detector can be used on several real wideband over the air radio bands of interest to detect anomalies occurring within band. The results have shown\nthat especially in structured radio signal environments where temporal sequence model prediction performs best, we obtain our best performance advantage over Kalman novelty detector methods.\nWe believe this is an important result that shows viability of this form of spectrum change monitoring and provides some starting points for improvements on more traditional methods for time series change detection. We have evaluated several neural predictor models and have shown that both the LSTM model and potentially the DCNN model are viable at low SNR levels, while for an analog modulation (FM Broadcast), there was less difference between the performance of the detectors with these candidate networks.\nIn future work we hope to perform much more extensive architecture and hyper-parameter searches, evaluate longer runs, larger datasets and additional types of anomalies and mixtures of anomalies. We would like to evaluate hybrid architectures such as the LSTM with convolutional features on the front end, including both the use of dilated convolutional layers and residual units combining a number of the promising techniques which have largely been evaluated separately here.\nIn the area of spectrum sensing for communications system failure, interference, security, or monitoring, we hope that this method helps imagine a promising path forward towards general learning of non-signal and non-band specific methods which can be used rapidly on a wide range of systems and deployment models without needing specialized expert prior knowledge of the system of interest."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank the Bradley Department of Electrical and Computer Engineering at the Virginia Polytechnic Institute and State University, the Hume Center, and DARPA all for their generous support in this work.\nThis research was developed with funding from the Defense Advanced Research Projects Agency\u2019s (DARPA) MTO Office under grant HR0011-16-1-0002. The views, opinions, and/or findings expressed are those of the author and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government."}], "references": [{"title": "Constant false alarm rate processing in search radars(receiver output noise control)", "author": ["V.G. Hansen"], "venue": "Radar- Present and future, pp. 325\u2013332, 1973.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1973}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u2013 1780, 1997.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Intrusion detection in wireless ad-hoc networks", "author": ["Y. Zhang", "W. Lee"], "venue": "Proceedings of the 6th annual international conference on Mobile computing and networking, ACM, 2000, pp. 275\u2013283.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Novelty detection in learning systems", "author": ["S. Marsland"], "venue": "Neural computing surveys, vol. 3, no. 2, pp. 157\u2013195, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Gnu radio: tools for exploring the radio frequency spectrum", "author": ["E. Blossom"], "venue": "Linux journal, vol. 2004, no. 122, p. 4, 2004.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Usrp users and developers guide", "author": ["M. Ettus"], "venue": "Ettus Research LLC, 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Cognitive radio architecture", "author": ["III J. Mitola"], "venue": "Cooperation in Wireless Networks: Principles and Applications, Springer, 2006, pp. 243\u2013311.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "An overview of anomaly detection techniques: existing solutions and latest technological trends", "author": ["A. Patcha", "J.-M. Park"], "venue": "Computer networks, vol. 51, no. 12, pp. 3448\u20133470, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Anomaly detection in wireless sensor networks", "author": ["S. Rajasegarar", "C. Leckie", "M. Palaniswami"], "venue": "IEEE Wireless Communications, vol. 15, no. 4, pp. 34\u201340, 2008.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["A. Graves", "M. Liwicki", "S. Fern\u00e1ndez", "R. Bertolami", "H. Bunke", "J. Schmidhuber"], "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 31, no. 5, pp. 855\u2013868, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Anomaly detection in wireless sensor networks: a survey", "author": ["M. Xie", "S. Han", "B. Tian", "S. Parvin"], "venue": "Journal of Network and Computer Applications, vol. 34, no. 4, pp. 1302\u20131325, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Second-order kalman filters using multi-complex step derivatives", "author": ["V. Vittaldev", "R.P. Russell", "N. Arora", "D. Gaylor"], "venue": "American Astronomial Society, vol. 204, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "A review of novelty detection", "author": ["M.A. Pimentel", "D.A. Clifton", "L. Clifton", "L. Tarassenko"], "venue": "Signal Processing, vol. 99, pp. 215\u2013249, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short term memory networks for anomaly detection in time series", "author": ["P. Malhotra", "L. Vig", "G. Shroff", "P. Agarwal"], "venue": "Proceedings, Presses universitaires de Louvain, 2015, p. 89.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Kalman and bayesian filters in python, [Online]. Available: https://github.com/rlabbe/ Kalman- and- Bayesian- Filters - in- Python/ (visited on 10/28/2016)", "author": ["R.R. Labbe"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Wavenet: a generative model for raw audio", "author": ["A. v. d. Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1609.03499, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "There is a significant body of existing work on the subject in theory and applications [4] [13], we focus here primarily on reconstruction based novelty detection.", "startOffset": 87, "endOffset": 90}, {"referenceID": 12, "context": "There is a significant body of existing work on the subject in theory and applications [4] [13], we focus here primarily on reconstruction based novelty detection.", "startOffset": 91, "endOffset": 95}, {"referenceID": 6, "context": "In radio, applications of anomaly detection have been discussed [7] but not widely used outside of a few small niche applications.", "startOffset": 64, "endOffset": 67}, {"referenceID": 10, "context": "Radio anomaly detection has been leveraged somewhat in wireless sensor networks such as in [11] [8] [3] [9], but most of these applications focus on detecting changes in sensor data (temperature, pressure, etc), or expert features rather than on anomalies occurring in the high rate raw physical layer radio signal itself.", "startOffset": 91, "endOffset": 95}, {"referenceID": 7, "context": "Radio anomaly detection has been leveraged somewhat in wireless sensor networks such as in [11] [8] [3] [9], but most of these applications focus on detecting changes in sensor data (temperature, pressure, etc), or expert features rather than on anomalies occurring in the high rate raw physical layer radio signal itself.", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "Radio anomaly detection has been leveraged somewhat in wireless sensor networks such as in [11] [8] [3] [9], but most of these applications focus on detecting changes in sensor data (temperature, pressure, etc), or expert features rather than on anomalies occurring in the high rate raw physical layer radio signal itself.", "startOffset": 100, "endOffset": 103}, {"referenceID": 8, "context": "Radio anomaly detection has been leveraged somewhat in wireless sensor networks such as in [11] [8] [3] [9], but most of these applications focus on detecting changes in sensor data (temperature, pressure, etc), or expert features rather than on anomalies occurring in the high rate raw physical layer radio signal itself.", "startOffset": 104, "endOffset": 107}, {"referenceID": 15, "context": "Recently, the use of recurrent neural networks to form a predictive reconstruction as part of a novelty detector has been proposed [17] and demonstrated to function quite well on several time series datasets including electrocardiograms, physical telemetry signals, and power consumption metrics.", "startOffset": 131, "endOffset": 135}, {"referenceID": 1, "context": "In this work a long short term memory [2] (LSTM) based recurrent network is used to train a time series predictor on a training set which is then used to compute an expected error distribution vs the real signal which is well characterized for non-anomalous behavior.", "startOffset": 38, "endOffset": 41}, {"referenceID": 9, "context": "The sequence learning capacity of LSTM has in some cases been shown to exceed that of Hidden Markov model [10] and Kalman based linear predictors, because it is able to take into account a much more complex nonlinear representation of state (does not make the Markov assumption), short and long term transition dependencies, and complex nonlinear output mapping dependencies than either of these prior models are capable of.", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "Our threshold \u03c4 can be fit using an F\u03b2 or typical constant false alarm rate (CFAR) sorts of analysis [1] if a decent dataset of \u201danomalous\u201d behavior is known, or a false alarm rate on \u201dnormal\u201d behavior is used as the metric.", "startOffset": 101, "endOffset": 104}, {"referenceID": 5, "context": "Recordings are conducted with an Ettus Research B200-mini [6] which uses the Analog Devices AD9361 RFIC front-end and stored to disk for analysis using GNU Radio [5].", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "Recordings are conducted with an Ettus Research B200-mini [6] which uses the Analog Devices AD9361 RFIC front-end and stored to disk for analysis using GNU Radio [5].", "startOffset": 162, "endOffset": 165}, {"referenceID": 11, "context": "We use a 3rd order Unscented Kalman Filter/Predictor similar to that described in [12].", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "This is implemented using the FilterPy module [18] and forms our performance benchmark for this paper.", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "5 and using proper LSTM weight and activation dropout as described in [14] and implemented in Keras [15].", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "We model our Dilated Convolutional Neural Network 2 (DCNN2) architecture on a vastly simplified version of Google\u2019s WaveNet architecture [19] which has demonstrated a strong ability to learn raw time series representations on acoustic voice data.", "startOffset": 137, "endOffset": 141}, {"referenceID": 14, "context": "Here we use two levels of dilated convolutions where each is a residual block [16] containing identical layers with hyperbolic tan (TanH) and sigmoid activations merged multiplicatively, followed by a 1x1 convolutional layer for dimensionality reduction.", "startOffset": 78, "endOffset": 82}], "year": 2016, "abstractText": "We introduce a powerful recurrent neural network based method for novelty detection to the application of detecting radio anomalies. This approach holds promise in significantly increasing the ability of naive anomaly detection to detect small anomalies in highly complex complexity multi-user radio bands. We demonstrate the efficacy of this approach on a number of common real over the air radio communications bands of interest and quantify detection performance in terms of probability of detection an false alarm rates across a range of interference to band power ratios and compare to baseline methods.", "creator": "LaTeX with hyperref package"}}}