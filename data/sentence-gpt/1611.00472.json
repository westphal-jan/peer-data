{"id": "1611.00472", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Towards Sub-Word Level Compositions for Sentiment Analysis of Hindi-English Code Mixed Text", "abstract": "Sentiment analysis (SA) using code-mixed data from social media has several applications in opinion mining ranging from customer satisfaction to social campaign analysis in multilingual societies. Advances in this area are impeded by the lack of a suitable annotated dataset. We introduce a Hindi-English (Hi-En) code-mixed dataset for sentiment analysis and perform empirical analysis comparing the suitability and performance of various state-of-the-art SA methods in social media platforms. Data from social media datasets also supports the following hypotheses:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 2 Nov 2016 05:23:53 GMT  (505kb,D)", "http://arxiv.org/abs/1611.00472v1", "Accepted paper at COLING 2016"]], "COMMENTS": "Accepted paper at COLING 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ameya prabhu", "aditya joshi", "manish shrivastava", "vasudeva varma"], "accepted": false, "id": "1611.00472"}, "pdf": {"name": "1611.00472.pdf", "metadata": {"source": "CRF", "title": "Towards Sub-Word Level Compositions for Sentiment Analysis of Hindi-English Code Mixed Text", "authors": ["Ameya Prabhu", "Manish Shrivastava", "Vasudeva Varma"], "emails": ["@research.iiit.ac.in", "@.iiit.ac.in"], "sections": [{"heading": null, "text": "Sentiment analysis (SA) using code-mixed data from social media has several applications in opinion mining ranging from customer satisfaction to social campaign analysis in multilingual societies. Advances in this area are impeded by the lack of a suitable annotated dataset. We introduce a Hindi-English (Hi-En) code-mixed dataset for sentiment analysis and perform empirical analysis comparing the suitability and performance of various state-of-the-art SA methods in social media.\nIn this paper, we introduce learning sub-word level representations in LSTM (Subword-LSTM) architecture instead of character-level or word-level representations. This linguistic prior in our architecture enables us to learn the information about sentiment value of important morphemes. This also seems to work well in highly noisy text containing misspellings as shown in our experiments which is demonstrated in morpheme-level feature maps learned by our model. Also, we hypothesize that encoding this linguistic prior in the Subword-LSTM architecture leads to the superior performance. Our system attains accuracy 4-5% greater than traditional approaches on our dataset, and also outperforms the available system for sentiment analysis in Hi-En code-mixed text by 18%."}, {"heading": "1 Introduction", "text": "Code Mixing is a natural phenomenon of embedding linguistic units such as phrases, words or morphemes of one language into an utterance of another (Muysken, 2000; Duran, 1994; Gysels, 1992). Code-mixing is widely observed in multilingual societies like India, which has 22 official languages most popular of which are Hindi and English. With over 375 million Indian population online, usage of Hindi has been steadily increasing on the internet.\nThis opens up tremendous potential for research in sentiment and opinion analysis community for studying trends, reviews, events, human behaviour as well as linguistic analysis. Most of the current research works have involved sentiment polarity detection (Feldman, 2013; Liu, 2012; Pang and Lee, 2008) where the aim is to identify whether a given sentence or document is (usually) positive, negative or neutral. Due to availability of large-scale monolingual corpora, resources and widespread use of the language, English has attracted the most attention.\nSeminal work in sentiment analysis of Hindi text was done by Joshi et al. (2010) in which the authors built three step fallback model based on classification, machine translation and sentiment lexicons. They also observed that their system performed best with unigram features without stemming. Bakliwal et al. (2012) generated a sentiment lexicon for Hindi and validated the results on translated form of Amazon Product Dataset Blitzer et al. (2007). Das and Bandyopadhyay (2010) created Hindi SentiWordNet, a sentiment lexicon for Hindi.\n\u2217* indicates these authors contributed equally to this work. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/\nar X\niv :1\n61 1.\n00 47\n2v 1\n[ cs\n.C L\n] 2\nN ov\n2 01\n6\nSentiment Analysis in Code-mixed languages has recently started gaining interest owing to the rising amount of non-English speaking users. Sharma et al. (2015) segregated Hindi and English words and calculated final sentiment score by lexicon lookup in respective sentient dictionaries.\nHindi-English (Hi-En) code mixing allows ease-of-communication among speakers by providing a much wider variety of phrases and expressions. A common form of code mixing is called as romanization 1, which refers to the conversion of writing from a different writing system to the Roman script. But this freedom makes the task for developing NLP tools more difficult, highlighted by (Chittaranjan et al., 2014; Vyas et al., 2014; Barman et al., 2014). Initiatives have been taken by shared tasks (Sequiera et al., 2015; Solorio et al., 2014), however they do not cover the requirements for a sentiment analysis system.\nDeep learning based approaches (Zhang and LeCun, 2015; Socher et al., 2013) have been demonstrated to solve various NLP tasks. We believe these can provide solution to code-mixed and romanized text from various demographics in India, as similar trends are followed in many other Indian languages too. dos Santos and Zadrozny (2014) demonstrated applicability of character models for NLP tasks like POS tagging and Named Entity Recognition (dos Santos and Guimara\u0303es, 2015). LSTMs have been observed to outperform baselines for language modelling (Kim et al., 2015) and classification (Zhou et al., 2015). In a recent work, (Bojanowski et al., 2016) proposed a skip-gram based model in which each word is represented as a bag of character n-grams. The method produced improved results for languages with large vocabularies and rare words.\nThe romanized code mixed data on social media presents additional inherent challenges such as contractions like \u201dbetween\u201d\u2192 \u201dbtwn\u201d, non-standard spellings such as \u201dcooolll\u201d or \u201dbhut bdiya\u201d and nongrammatical constructions like \u201dsir hlp plzz naa\u201d. Hindi is phonetically typed while English (Roman script) doesn\u2019t preserve phonetics in text. Thus, along with diverse sentence construction, words in Hindi can have diverse variations when written online, which leads to large amount of tokens, as illustrated in Table 2. Meanwhile there is a lack of a suitable dataset.\nOur contributions in this paper are (i) Creation, annotation and analysis of a Hi-En code-mixed dataset for the sentiment analysis, (ii) Sub-word level representations that lead to better performance of LSTM networks compared to Character level LSTMs (iii) Experimental evaluation for suitability and evaluation of performance of various state-of-the-art techniques for the SA task, (iv) A preliminary investigation of embedding linguistic priors might be encoded for SA task by char-RNN architecture and the relation of architecture with linguistic priors, leading to the superior performance on this task. Our paper is divided into the following sections: We begin with an introduction to Code Mixing and romanization in Section 1. We mention the issues with code-mixed data in context of Sentiment Analysis and provides an overview of existing solutions. We then discusses the process of creation of the dataset and its features in Section 2. In Section 3, we introduce Sub-word level representation and explains how they are able to model morphemes along with propagating meaningful information, thus capturing sentiment in a sentence. Then in Section 4, we explain our experimental setup, describe the performance of proposed system and compare it with baselines and other methods, proceeded by a discussion on our results.\n1https://en.wikipedia.org/wiki/Romanization"}, {"heading": "2 Dataset", "text": "We collected user comments from public Facebook pages popular in India. We chose pages of Salman Khan, a popular Indian actor with massive fan following, and Narendra Modi, the current Prime Minister of India. The pages have 31 million and 34 million facebook user likes respectively. These pages attract large variety of users from all across India and contain lot of comments to the original posts in codemixed representations in varied sentiment polarities. We manually pre-processed the collected data to remove the comments that were not written in roman script, were longer than 50 words, or were complete English sentences. We also removed the comments that contained more than one sentence, as each sentence might have different sentiment polarity. Then, we proceeded to manual annotation of our dataset. The comments were annotated by two annotators in a 3-level polarity scale - positive, negative or neutral. Only the comments with same polarity marked by both the annotators are considered for the experiments. They agreed on the polarity of 3879 of 4981 (77%) sentences. The Cohen\u2019s Kappa coefficient (Cohen, 1960) was found to be 0.64. We studied the reasons for misalignment and found that causes typically were due to difference in perception of sentiments by individuals, different interpretations by them and sarcastic nature of some comments which is common in social media data. The dataset contains 15% negative, 50% neutral and 35% positive comments owing to the nature of conversations in the selected pages.\nThe dataset exhibits some of the major issues while dealing with code-mixed data like short sentences with unclear grammatical structure. Further, romanization of Hindi presents an additional set of complexities due to loss of phonetics and free ordering in sentence constructions as shown in Table 1. This leads to a number of variations of how words can be written. Table 2 contains some of the words with multiple spelling variations in our dataset, which is one of the major challenges to tackle in Hi-En code-mixed data.\nPopular related datasets are listed in Table 3. STS, SemEval, IMDB etc. have been explored for SA tasks but they contain text in English. The dataset used by Vyas et al. (2014) contains Hi-En Code Mixed text but doesn\u2019t contain sentiment polarity. We constructed a code mixed dataset with sentiment polarity annotations, and the size is comparable with several datasets. Table 4 shows some examples of sentences from our dataset. Here, we have phrases in Hindi (source language) written in English (target) language.\nOur dataset and code is freely available for download 2 to encourage further exploration in this domain.\n2https://github.com/DrImpossible/Sub-word-LSTM"}, {"heading": "3 Learning Compositionality", "text": "Our target is to perform sentiment analysis on the above presented dataset. Most commonly used statistical approaches learn word-level feature representations. We start our exploration for suitable algorithms from models having word-based representations."}, {"heading": "3.1 Word-level models", "text": "Word2Vec(Mikolov et al., 2013) and Word-level RNNs (Word-RNNs) (thang Luong et al., 2013) have substantially contributed to development of new representations and their applications in NLP such as in Summarization (Cao et al., 2015) and Machine Translation (Cho et al., 2014). They are theoretically sound since language consists of inherently arbitrary mappings between ideas and words. Eg: The words person(English) and insaan(Hindi) do not share any priors in their construction and neither do their constructions have any relationship with the semantic concept of a person. Hence, popular approaches consider lexical units to be independent entities. However, operating on the lexical domain draws criticism since the finite vocabulary assumption; which states that models assume language has finite vocabulary but in contrast, people actively learn & understand new words all the time.\nExcitingly, our dataset seems suited to validate some of these assumptions. In our dataset, vocabulary sizes are greater than the size of the dataset as shown in Table 3. Studies on similar datasets have shown strong correlation between number of comments and size of vocabulary (Saif et al., 2013). This rules out methods like Word2Vec, N-grams or Word-RNNs which inherently assume a small vocabulary in comparison to the data size. The finite vocabulary generally used to be a good approximation for English, but is no longer valid in our scenario. Due to the high sparsity of words themselves, it is not possible to learn useful word representations. This opens avenues to learn non-lexical representations, the most widely studied being character-level representations, which is discussed in the next section."}, {"heading": "3.2 Character-level models", "text": "Character-level RNNs (Char-RNNs) have recently become popular, contributing to various tasks like (Kim et al., 2015). They do not have the limitation of vocabulary, hence can freely learn to generate new words. This freedom, in fact, is an issue: Language is composed of lexical units made by combining letters in some specific combinations, i.e. most of the combinations of letters do not make sense. The complexity arises because the mappings between meaning and its construction from characters is arbitrary. Character models may be apriori inappropriate models of language as characters individually do not usually provide semantic information. For example, while \u201c King\u2212Man+Women = Queen\u201d is semantically interpretable by a human, \u201cCat\u2212 C +B = Bat\u201d lacks any linguistic basis.\nBut, groups of characters may serve semantic functions. This is illustrated by Un+Holy = Unholy or Cat + s = Cats which is semantically interpretable by a human. Since sub-word level representations can generate meaningful lexical representations and individually carry semantic weight, we believe that sub-word level representations consisting composition of characters might allow generation of new lexical structures and serve as better linguistic units than characters."}, {"heading": "3.3 Sub-word level representations", "text": "Lexicon based approaches for the SA task (Taboada et al., 2011; Sharma et al., 2015) perform a dictionary look up to obtain an individual score for words in a given sentence and combine these scores to get the sentiment polarity of a sentence. We however want to use intermediate sub-word feature representations learned by the filters during convolution operation. Unlike traditional approaches that add sentiment scores of individual words, we propagate relevant information with LSTM and compute final sentiment of the sentence as illustrated in Figure 1. Hypothesis: We propose that incorporating sub-word level representations into the design of our models should result in better performance. This would also serve as a test scenario for the broader hypothesis proposed by Dyer et. al. in his impressive ICLR keynote 3 - Incorporating linguistic priors in network architectures lead to better performance of models.\n3Available at: http://videolectures.net/iclr2016 dyer model architecture/\nMethodology: We propose a method of generating sub-word level representations through 1-D convolutions on character inputs for a given sentence. Formally, let C be the set of characters and T be an set of input sentences. The sentence s \u2208 T is made up of a sequence of characters [c1, ...., cl] where l is length of the input.\nHence, the representation of the input s is given by the matrixQ \u2208 Rd\u00d7l where d is the dimensionality of character embedding that corresponding to [c1, ...., cl]. We perform convolution of Q with a filter H \u2208 Rd\u00d7m of length m after which we add a bias and apply a non-linearity to obtain a feature map f \u2208 Rl\u2212m+1. Thus we can get sub-word level (morpheme-like) feature map. Specifically, the ith element of f is given by:\nf [i] = g((Q[:, i : i+m\u2212 1] \u2217H) + b) (1)\nwhere Q[:, i : i+m\u2212 1] is the matrix of (i)th to (i+m\u2212 1)th character embedding and g corresponds to ReLU non-linearity. Finally, we pool the maximal responses from p feature representations corresponding to selecting subword representations as:\nyi = max(f [p \u2217 (i : i+ p\u2212 1)]) (2)\nNext, we need to model the relationships between these features yi[:] in order to find the overall sentiment of the sentence. This is achieved by LSTM(Graves, 2013) which is suited to learning to propagate and \u2019remember\u2019 useful information, finally arriving at a sentiment vector representation from the inputs. We provide ft as an input to the memory cell at time t. We then compute values of It - the input gate, C\u0303t - the candidate value for the state of the memory cell at time t and ft - the activation of the forget gate, which can be used to compute the information stored in memory cell at time t. With the new state of memory cell Ct, we can compute the output feature representation by:\nOt = \u03c3(Wyt + Uh(t\u2212 1) + V (Ct + b) (3) ht = Ottanh(Ct) (4)\nwhere W ,U and V are weight matrices and bi are biases. After l steps, hl represents the relevant information retained from the history. That is then passed to a fully connected layer which calculates the final sentiment polarity as illustrated in the Figure 1.\nFigure 2 gives schematic overview of the architecture. We perform extensive experiments to qualitatively and quantitatively validate the above claims as explained in the next section."}, {"heading": "4 Experiments", "text": "We perform extensive evaluation of various approaches, starting with a suitability study for the nature of approaches that would be able to generalize to this data. We compare our approaches with the stateof-the-art methods which are feasible to generalize on code-mixed data and (Sharma et al., 2015), the current state-of-the-art in Hi-En code-mixed SA task."}, {"heading": "4.1 Method Suitability", "text": "Following approaches have been used for performing SA tasks in English but do not suit mix code setting:\n\u2022 Approaches involving NLP tools: RNTN (Socher et al., 2013) etc which involve generation of parse trees which are not available for code mixed text;\n\u2022 Word Embedding Based Approaches: Word2Vec, Word-RNN may not provide reliable embedding in situations with small amount of highly sparse dataset.\n\u2022 Surface Feature engineering based approaches: Hashtags, User Mentions, Emoticons etc. may not exist in the data.\nFigure 2: Schematic overview of the architecture.\nFigure 3: Training accuracy and loss variation."}, {"heading": "4.2 Experimental Setup", "text": "Our dataset is divided into 3 splits- Training, validation and testing. We first divide the data into randomized 80-20 train test split, then further randomly divide the training data into 80-20 split to get the final training, validation and testing data.\nAs the problem is relatively new, we compare state of the art sentiment analysis techniques (Wang and Manning, 2012; Pang and Lee, 2008) which are generalizable to our dataset. We also compare the results with system proposed by Sharma et al. (2015) on our dataset. As their system is not available publicly, we implemented it using language identification and transliteration using the tools provided by Bhat et al. (2015) for Hi-En Code Mixed data. The polarity of thus obtained tokens is computed from SentiWordNet (Esuli and Sebastiani, 2006) and Hindi SentiWordNet (Das and Bandyopadhyay, 2010) to obtain the polarity of words, which are then voted to get final polarity of the sentence.\nThe architecture of the proposed system (Subword-LSTM) is described in Figure 2. We compare it with a character-level LSTM (Char-LSTM) following the same architecture without the convolutional and maxpooling layers. We use Adamax (Kingma and Ba, 2014) (a variant of Adam based on infinity norm) optimizer to train this setup in an end-to-end fashion using batch size of 128. We use very simplistic architectures because of the constraint on the size of the dataset. As the datasets in this domain expand, we would like to scale up our approach to bigger architectures. The stability of training using this architecture is illustrated in Figure 3."}, {"heading": "4.3 Observations", "text": "In the comparative study performed on our dataset, we observe that Multinomial Naive Bayes performs better than SVM(Pang and Lee, 2008) for snippets providing additional validation to this hypothesis given by Wang and Manning (2012).\nWe also observe that unigrams perform better than bigrams and Bag of words performs better than tf-idf in contrast to trends in English, as the approaches inducing more sparsity would yield to poorer results because our dataset is inherently very sparse. The lexicon lookup approach (Sharma et al., 2015) didn\u2019t perform well owing to the heavily misspelt words in the text, which led to incorrect transliterations as shown in Table 6."}, {"heading": "4.4 Validation of proposed hypothesis", "text": "We obtain preliminary validation for our hypothesis that incorporating sub-word level features instead of characters would lead to better performance. Our Subword-LSTM system provides an F-score of 0.658 for our dataset, which is significantly better than Char-LSTM which provides F-score of 0.511.\nSince we do not have any other dataset in Hi-En code-mixed setting of comparable to other settings, we performed cross-validation of our hypothesis on SemEval\u201913 Twitter Sentiment Analysis dataset. We took the raw tweets character-by-character as an input for our model from the training set of 7800 tweets and test on the SemEval\u201913 development set provided containing 1368 tweets. The results are summarized in Table 5. In all the cases, the text was converted to lowercase and tokenized. No extra features or heuristics were used."}, {"heading": "4.5 Visualizing character responses", "text": "Visualizations in Figure 4 shows how the proposed model is learning to identify sentiment lexicons. We see that different filters generally tend to learn mappings from different parts, interestingly showing shifting trends to the right which maybe due to LSTM picking their feature representation in future time steps. The words sections that convey sentiment polarity information are captured despite misspelling in example (i) and (ii). In example (iii), starting and ending phrases show high response which correspond to the sentiment conveying words (party and gift). The severe morpheme stretching in example (iv) also affects the sentiment polarity."}, {"heading": "5 Conclusion", "text": "We introduce Sub-Word Long Short Term Memory model to learn sentiments in a noisy Hindi-English Code Mixed dataset. We discuss that due to the unavailability of NLP tools for Hi-En Code Mixed text and noisy nature of such data, several popular methods for Sentiment Analysis are not applicable. The solutions that involve unsupervised word representations would again fail due to sparsity in the dataset. Sub-Word LSTM interprets sentiment based on morpheme-like structures and the results thus produced are significantly better than baselines.\nFurther work should explore the effect of scaling of RNN and working with larger datasets on the results. In the new system, we would like to explore more deep neural network architectures that are able to capture sentiment in Code Mixed and other varieties of noisy data from the social web."}], "references": [{"title": "Hindi subjective lexicon: A lexical resource for hindi polarity classification", "author": ["Akshat Bakliwal", "Piyush Arora", "Vasudeva Varma."], "venue": "Proceedings of International Conference on Language Resources and Evaluation.", "citeRegEx": "Bakliwal et al\\.,? 2012", "shortCiteRegEx": "Bakliwal et al\\.", "year": 2012}, {"title": "Code-mixing: A challenge for language identification in the language of social media", "author": ["Utsab Barman", "Amitava Das", "Joachim Wagner", "Jennifer Foster."], "venue": "In Proceedings of the First Workshop on Computational Approaches to Code-Switching.", "citeRegEx": "Barman et al\\.,? 2014", "shortCiteRegEx": "Barman et al\\.", "year": 2014}, {"title": "Iiit-h system submission for fire2014 shared task on transliterated search", "author": ["Irshad Ahmad Bhat", "Vandan Mujadia", "Aniruddha Tammewar", "Riyaz Ahmad Bhat", "Manish Shrivastava."], "venue": "Proceedings of the Forum for Information Retrieval Evaluation, FIRE \u201914, pages 48\u201353, New York, NY, USA. ACM.", "citeRegEx": "Bhat et al\\.,? 2015", "shortCiteRegEx": "Bhat et al\\.", "year": 2015}, {"title": "Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification", "author": ["John Blitzer", "Mark Dredze", "Fernando Pereira."], "venue": "In ACL, pages 187\u2013205.", "citeRegEx": "Blitzer et al\\.,? 2007", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Enriching word vectors with subword information", "author": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."], "venue": "CoRR, abs/1607.04606.", "citeRegEx": "Bojanowski et al\\.,? 2016", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "Ranking with recursive neural networks and its application to multi-document summarization", "author": ["Ziqiang Cao", "Furu Wei", "Li Dong", "Sujian Li", "Ming Zhou."], "venue": "AAAI, pages 2153\u20132159.", "citeRegEx": "Cao et al\\.,? 2015", "shortCiteRegEx": "Cao et al\\.", "year": 2015}, {"title": "Word-level language identification using crf: Code-switching shared task report of msr india system", "author": ["Gokul Chittaranjan", "Yogarshi Vyas", "Kalika Bali", "Monojit Choudhury."], "venue": "Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 73\u201379.", "citeRegEx": "Chittaranjan et al\\.,? 2014", "shortCiteRegEx": "Chittaranjan et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1406.1078.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A Coefficient of Agreement for Nominal Scales", "author": ["Jacob Cohen."], "venue": "Educational and Psychological Measurement, 20(1):37\u201346, April.", "citeRegEx": "Cohen.,? 1960", "shortCiteRegEx": "Cohen.", "year": 1960}, {"title": "Sentiwordnet for indian languages", "author": ["Amitava Das", "Sivaji Bandyopadhyay."], "venue": "Proceedings of the Eighth Workshop on Asian Language Resouces.", "citeRegEx": "Das and Bandyopadhyay.,? 2010", "shortCiteRegEx": "Das and Bandyopadhyay.", "year": 2010}, {"title": "Boosting named entity recognition with neural character embeddings. CoRR, abs/1505.05008", "author": ["C\u0131\u0301cero Nogueira dos Santos", "Victor Guimar\u00e3es"], "venue": null, "citeRegEx": "Santos and Guimar\u00e3es.,? \\Q2015\\E", "shortCiteRegEx": "Santos and Guimar\u00e3es.", "year": 2015}, {"title": "Learning character-level representations for part-ofspeech tagging", "author": ["C\u0131\u0301cero Nogueira dos Santos", "Bianca Zadrozny"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Santos and Zadrozny.,? \\Q2014\\E", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "Toward a better understanding of code switching and interlanguage in bilinguality: Implications for bilingual instruction", "author": ["Luisia Duran."], "venue": "Journal of Educational Issues of Language Minority Students, 14:69\u201387.", "citeRegEx": "Duran.,? 1994", "shortCiteRegEx": "Duran.", "year": 1994}, {"title": "Sentiwordnet: A publicly available lexical resource for opinion mining", "author": ["Andrea Esuli", "Fabrizio Sebastiani."], "venue": "In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06, pages 417\u2013422.", "citeRegEx": "Esuli and Sebastiani.,? 2006", "shortCiteRegEx": "Esuli and Sebastiani.", "year": 2006}, {"title": "Techniques and applications for sentiment analysis", "author": ["Ronen Feldman."], "venue": "Commun. ACM, 56(4):82\u201389, April.", "citeRegEx": "Feldman.,? 2013", "shortCiteRegEx": "Feldman.", "year": 2013}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "CoRR, abs/1308.0850.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "French in urban lubumbashi swahili: Codeswitching, borrowing, or both", "author": ["Marjolein Gysels."], "venue": "Journal of Multilingual and Multicultural Development, 13:41\u201355.", "citeRegEx": "Gysels.,? 1992", "shortCiteRegEx": "Gysels.", "year": 1992}, {"title": "A fall-back strategy for sentiment analysis in hindi: a case study", "author": ["Aditya Joshi", "Balamurali R.", "Bhattacharyya Pushpak."], "venue": "Proceedings of the 8th ICON, Stroudsburg, PA. Association for Computational Linguistics.", "citeRegEx": "Joshi et al\\.,? 2010", "shortCiteRegEx": "Joshi et al\\.", "year": 2010}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."], "venue": "CoRR, abs/1508.06615.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR, abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Sentiment Analysis and Opinion Mining", "author": ["Bing Liu."], "venue": "Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers.", "citeRegEx": "Liu.,? 2012", "shortCiteRegEx": "Liu.", "year": 2012}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "HLT-NAACL, pages 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Bilingual Speech: A Typology of Code-mixing", "author": ["Pieter Muysken."], "venue": "Cambridge University Press.", "citeRegEx": "Muysken.,? 2000", "shortCiteRegEx": "Muysken.", "year": 2000}, {"title": "Opinion mining and sentiment analysis", "author": ["Bo Pang", "Lillian Lee."], "venue": "Found. Trends Inf. Retr., 2(1-2):1\u2013135, January.", "citeRegEx": "Pang and Lee.,? 2008", "shortCiteRegEx": "Pang and Lee.", "year": 2008}, {"title": "Evaluation datasets for twitter sentiment analysis: A survey and a new dataset, the sts-gold", "author": ["Hassan Saif", "Miriam Fernandez", "Yulan He", "Harith Alani."], "venue": "1st Interantional Workshop on Emotion and Sentiment in Social and Expressive Media: Approaches and Perspectives from AI (ESSEM 2013).", "citeRegEx": "Saif et al\\.,? 2013", "shortCiteRegEx": "Saif et al\\.", "year": 2013}, {"title": "Overview of FIRE-2015 shared task on mixed script information retrieval", "author": ["Royal Sequiera", "Monojit Choudhury", "Parth Gupta", "Paolo Rosso", "Shubham Kumar", "Somnath Banerjee", "Sudip Kumar Naskar", "Sivaji Bandyopadhyay", "Gokul Chittaranjan", "Amitava Das", "Kunal Chakma."], "venue": "Prasenjit Majumder, Mandar Mitra, Madhulika Agrawal, and Parth Mehta, editors, Post Proceedings of the Workshops at the 7th Forum for Information Retrieval Evaluation, Gandhinagar, India, December 4-6, 2015., volume 1587 of CEUR Workshop Proceedings,", "citeRegEx": "Sequiera et al\\.,? 2015", "shortCiteRegEx": "Sequiera et al\\.", "year": 2015}, {"title": "Text normalization of code mix and sentiment analysis", "author": ["Shashank Sharma", "PYKL Srinivas", "Rakesh Chandra Balabantaray."], "venue": "2015 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2015, Kochi, India, August 10-13, 2015, pages 1468\u20131473.", "citeRegEx": "Sharma et al\\.,? 2015", "shortCiteRegEx": "Sharma et al\\.", "year": 2015}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Stroudsburg, PA, October. Association for Computational Linguistics.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Overview for the first shared task on language identification in code-switched data", "author": ["Thamar Solorio", "Elizabeth Blair", "Suraj Maharjan", "Steven Bethard", "Mona Diab", "Mahmoud Gohneim", "Abdelati Hawwari", "Fahad AlGhamdi", "Julia Hirschberg", "Alison Chang", "Pascale Fung."], "venue": "Proceedings of The First Workshop on Computational Approaches to Code Switching, held in conjunction with EMNLP 2014., pages 62\u201372, Doha, Qatar. ACL.", "citeRegEx": "Solorio et al\\.,? 2014", "shortCiteRegEx": "Solorio et al\\.", "year": 2014}, {"title": "Lexicon-based methods for sentiment analysis", "author": ["Maite Taboada", "Julian Brooke", "Milan Tofiloski", "Kimberly Voll", "Manfred Stede."], "venue": "Comput. Linguist., 37(2):267\u2013307, June.", "citeRegEx": "Taboada et al\\.,? 2011", "shortCiteRegEx": "Taboada et al\\.", "year": 2011}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh thang Luong", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics, pages 104\u2013113.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Pos tagging of english-hindi code-mixed social media content", "author": ["Yogarshi Vyas", "Spandana Gella", "Jatin", "Kalika Bali", "Monojit Choudhury."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 974\u2013979.", "citeRegEx": "Vyas et al\\.,? 2014", "shortCiteRegEx": "Vyas et al\\.", "year": 2014}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Sida Wang", "Christopher D. Manning."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL \u201912, pages 90\u201394, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Wang and Manning.,? 2012", "shortCiteRegEx": "Wang and Manning.", "year": 2012}, {"title": "Text understanding from scratch", "author": ["Xiang Zhang", "Yann LeCun."], "venue": "CoRR, abs/1502.01710.", "citeRegEx": "Zhang and LeCun.,? 2015", "shortCiteRegEx": "Zhang and LeCun.", "year": 2015}, {"title": "A C-LSTM neural network for text classification", "author": ["Chunting Zhou", "Chonglin Sun", "Zhiyuan Liu", "Francis C.M. Lau."], "venue": "CoRR, abs/1511.08630.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 22, "context": "Code Mixing is a natural phenomenon of embedding linguistic units such as phrases, words or morphemes of one language into an utterance of another (Muysken, 2000; Duran, 1994; Gysels, 1992).", "startOffset": 147, "endOffset": 189}, {"referenceID": 12, "context": "Code Mixing is a natural phenomenon of embedding linguistic units such as phrases, words or morphemes of one language into an utterance of another (Muysken, 2000; Duran, 1994; Gysels, 1992).", "startOffset": 147, "endOffset": 189}, {"referenceID": 16, "context": "Code Mixing is a natural phenomenon of embedding linguistic units such as phrases, words or morphemes of one language into an utterance of another (Muysken, 2000; Duran, 1994; Gysels, 1992).", "startOffset": 147, "endOffset": 189}, {"referenceID": 14, "context": "Most of the current research works have involved sentiment polarity detection (Feldman, 2013; Liu, 2012; Pang and Lee, 2008) where the aim is to identify whether a given sentence or document is (usually) positive, negative or neutral.", "startOffset": 78, "endOffset": 124}, {"referenceID": 20, "context": "Most of the current research works have involved sentiment polarity detection (Feldman, 2013; Liu, 2012; Pang and Lee, 2008) where the aim is to identify whether a given sentence or document is (usually) positive, negative or neutral.", "startOffset": 78, "endOffset": 124}, {"referenceID": 23, "context": "Most of the current research works have involved sentiment polarity detection (Feldman, 2013; Liu, 2012; Pang and Lee, 2008) where the aim is to identify whether a given sentence or document is (usually) positive, negative or neutral.", "startOffset": 78, "endOffset": 124}, {"referenceID": 9, "context": "Code Mixing is a natural phenomenon of embedding linguistic units such as phrases, words or morphemes of one language into an utterance of another (Muysken, 2000; Duran, 1994; Gysels, 1992). Code-mixing is widely observed in multilingual societies like India, which has 22 official languages most popular of which are Hindi and English. With over 375 million Indian population online, usage of Hindi has been steadily increasing on the internet. This opens up tremendous potential for research in sentiment and opinion analysis community for studying trends, reviews, events, human behaviour as well as linguistic analysis. Most of the current research works have involved sentiment polarity detection (Feldman, 2013; Liu, 2012; Pang and Lee, 2008) where the aim is to identify whether a given sentence or document is (usually) positive, negative or neutral. Due to availability of large-scale monolingual corpora, resources and widespread use of the language, English has attracted the most attention. Seminal work in sentiment analysis of Hindi text was done by Joshi et al. (2010) in which the authors built three step fallback model based on classification, machine translation and sentiment lexicons.", "startOffset": 163, "endOffset": 1084}, {"referenceID": 0, "context": "Bakliwal et al. (2012) generated a sentiment lexicon for Hindi and validated the results on translated form of Amazon Product Dataset Blitzer et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "Bakliwal et al. (2012) generated a sentiment lexicon for Hindi and validated the results on translated form of Amazon Product Dataset Blitzer et al. (2007). Das and Bandyopadhyay (2010) created Hindi SentiWordNet, a sentiment lexicon for Hindi.", "startOffset": 0, "endOffset": 156}, {"referenceID": 0, "context": "Bakliwal et al. (2012) generated a sentiment lexicon for Hindi and validated the results on translated form of Amazon Product Dataset Blitzer et al. (2007). Das and Bandyopadhyay (2010) created Hindi SentiWordNet, a sentiment lexicon for Hindi.", "startOffset": 0, "endOffset": 186}, {"referenceID": 6, "context": "But this freedom makes the task for developing NLP tools more difficult, highlighted by (Chittaranjan et al., 2014; Vyas et al., 2014; Barman et al., 2014).", "startOffset": 88, "endOffset": 155}, {"referenceID": 31, "context": "But this freedom makes the task for developing NLP tools more difficult, highlighted by (Chittaranjan et al., 2014; Vyas et al., 2014; Barman et al., 2014).", "startOffset": 88, "endOffset": 155}, {"referenceID": 1, "context": "But this freedom makes the task for developing NLP tools more difficult, highlighted by (Chittaranjan et al., 2014; Vyas et al., 2014; Barman et al., 2014).", "startOffset": 88, "endOffset": 155}, {"referenceID": 25, "context": "Initiatives have been taken by shared tasks (Sequiera et al., 2015; Solorio et al., 2014), however they do not cover the requirements for a sentiment analysis system.", "startOffset": 44, "endOffset": 89}, {"referenceID": 28, "context": "Initiatives have been taken by shared tasks (Sequiera et al., 2015; Solorio et al., 2014), however they do not cover the requirements for a sentiment analysis system.", "startOffset": 44, "endOffset": 89}, {"referenceID": 33, "context": "Deep learning based approaches (Zhang and LeCun, 2015; Socher et al., 2013) have been demonstrated to solve various NLP tasks.", "startOffset": 31, "endOffset": 75}, {"referenceID": 27, "context": "Deep learning based approaches (Zhang and LeCun, 2015; Socher et al., 2013) have been demonstrated to solve various NLP tasks.", "startOffset": 31, "endOffset": 75}, {"referenceID": 18, "context": "LSTMs have been observed to outperform baselines for language modelling (Kim et al., 2015) and classification (Zhou et al.", "startOffset": 72, "endOffset": 90}, {"referenceID": 34, "context": ", 2015) and classification (Zhou et al., 2015).", "startOffset": 27, "endOffset": 46}, {"referenceID": 4, "context": "In a recent work, (Bojanowski et al., 2016) proposed a skip-gram based model in which each word is represented as a bag of character n-grams.", "startOffset": 18, "endOffset": 43}, {"referenceID": 19, "context": "Sharma et al. (2015) segregated Hindi and English words and calculated final sentiment score by lexicon lookup in respective sentient dictionaries.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": ", 2014; Barman et al., 2014). Initiatives have been taken by shared tasks (Sequiera et al., 2015; Solorio et al., 2014), however they do not cover the requirements for a sentiment analysis system. Deep learning based approaches (Zhang and LeCun, 2015; Socher et al., 2013) have been demonstrated to solve various NLP tasks. We believe these can provide solution to code-mixed and romanized text from various demographics in India, as similar trends are followed in many other Indian languages too. dos Santos and Zadrozny (2014) demonstrated applicability of character models for NLP tasks like POS tagging and Named Entity Recognition (dos Santos and Guimar\u00e3es, 2015).", "startOffset": 8, "endOffset": 529}, {"referenceID": 8, "context": "The Cohen\u2019s Kappa coefficient (Cohen, 1960) was found to be 0.", "startOffset": 30, "endOffset": 43}, {"referenceID": 31, "context": "Dataset Size # Vocab Social CM Sentiment STS-Test 498 2375 3 3 OMD 3238 6211 3 3 SemEval\u201913 13975 35709 3 3 IMDB 50000 5000 3 (Vyas et al., 2014) 381 - 3 3 Ours 3879 7549 3 3 3", "startOffset": 126, "endOffset": 145}, {"referenceID": 31, "context": "The dataset used by Vyas et al. (2014) contains Hi-En Code Mixed text but doesn\u2019t contain sentiment polarity.", "startOffset": 20, "endOffset": 39}, {"referenceID": 21, "context": "Word2Vec(Mikolov et al., 2013) and Word-level RNNs (Word-RNNs) (thang Luong et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 5, "context": ", 2013) have substantially contributed to development of new representations and their applications in NLP such as in Summarization (Cao et al., 2015) and Machine Translation (Cho et al.", "startOffset": 132, "endOffset": 150}, {"referenceID": 7, "context": ", 2015) and Machine Translation (Cho et al., 2014).", "startOffset": 32, "endOffset": 50}, {"referenceID": 24, "context": "Studies on similar datasets have shown strong correlation between number of comments and size of vocabulary (Saif et al., 2013).", "startOffset": 108, "endOffset": 127}, {"referenceID": 18, "context": "Character-level RNNs (Char-RNNs) have recently become popular, contributing to various tasks like (Kim et al., 2015).", "startOffset": 98, "endOffset": 116}, {"referenceID": 29, "context": "Lexicon based approaches for the SA task (Taboada et al., 2011; Sharma et al., 2015) perform a dictionary look up to obtain an individual score for words in a given sentence and combine these scores to get the sentiment polarity of a sentence.", "startOffset": 41, "endOffset": 84}, {"referenceID": 26, "context": "Lexicon based approaches for the SA task (Taboada et al., 2011; Sharma et al., 2015) perform a dictionary look up to obtain an individual score for words in a given sentence and combine these scores to get the sentiment polarity of a sentence.", "startOffset": 41, "endOffset": 84}, {"referenceID": 15, "context": "This is achieved by LSTM(Graves, 2013) which is suited to learning to propagate and \u2019remember\u2019 useful information, finally arriving at a sentiment vector representation from the inputs.", "startOffset": 24, "endOffset": 38}, {"referenceID": 26, "context": "We compare our approaches with the stateof-the-art methods which are feasible to generalize on code-mixed data and (Sharma et al., 2015), the current state-of-the-art in Hi-En code-mixed SA task.", "startOffset": 115, "endOffset": 136}, {"referenceID": 27, "context": "\u2022 Approaches involving NLP tools: RNTN (Socher et al., 2013) etc which involve generation of parse trees which are not available for code mixed text; \u2022 Word Embedding Based Approaches: Word2Vec, Word-RNN may not provide reliable embedding in situations with small amount of highly sparse dataset.", "startOffset": 39, "endOffset": 60}, {"referenceID": 32, "context": "As the problem is relatively new, we compare state of the art sentiment analysis techniques (Wang and Manning, 2012; Pang and Lee, 2008) which are generalizable to our dataset.", "startOffset": 92, "endOffset": 136}, {"referenceID": 23, "context": "As the problem is relatively new, we compare state of the art sentiment analysis techniques (Wang and Manning, 2012; Pang and Lee, 2008) which are generalizable to our dataset.", "startOffset": 92, "endOffset": 136}, {"referenceID": 13, "context": "The polarity of thus obtained tokens is computed from SentiWordNet (Esuli and Sebastiani, 2006) and Hindi SentiWordNet (Das and Bandyopadhyay, 2010) to obtain the polarity of words, which are then voted to get final polarity of the sentence.", "startOffset": 67, "endOffset": 95}, {"referenceID": 9, "context": "The polarity of thus obtained tokens is computed from SentiWordNet (Esuli and Sebastiani, 2006) and Hindi SentiWordNet (Das and Bandyopadhyay, 2010) to obtain the polarity of words, which are then voted to get final polarity of the sentence.", "startOffset": 119, "endOffset": 148}, {"referenceID": 19, "context": "We use Adamax (Kingma and Ba, 2014) (a variant of Adam based on infinity norm) optimizer to train this setup in an end-to-end fashion using batch size of 128.", "startOffset": 14, "endOffset": 35}, {"referenceID": 19, "context": "As the problem is relatively new, we compare state of the art sentiment analysis techniques (Wang and Manning, 2012; Pang and Lee, 2008) which are generalizable to our dataset. We also compare the results with system proposed by Sharma et al. (2015) on our dataset.", "startOffset": 117, "endOffset": 250}, {"referenceID": 2, "context": "As their system is not available publicly, we implemented it using language identification and transliteration using the tools provided by Bhat et al. (2015) for Hi-En Code Mixed data.", "startOffset": 139, "endOffset": 158}, {"referenceID": 32, "context": "Method Reported In Our dataset SemEval\u2019 13 Accuracy F1-Score Accuracy F1-Score NBSVM (Unigram) (Wang and Manning, 2012) 59.", "startOffset": 95, "endOffset": 119}, {"referenceID": 32, "context": "5369 NBSVM (Uni+Bigram) (Wang and Manning, 2012) 62.", "startOffset": 24, "endOffset": 48}, {"referenceID": 32, "context": "5566 MNB (Unigram) (Wang and Manning, 2012) 66.", "startOffset": 19, "endOffset": 43}, {"referenceID": 32, "context": "4689 MNB (Uni+Bigram) (Wang and Manning, 2012) 66.", "startOffset": 22, "endOffset": 46}, {"referenceID": 32, "context": "469 MNB (Tf-Idf) (Wang and Manning, 2012) 63.", "startOffset": 17, "endOffset": 41}, {"referenceID": 23, "context": "4196 SVM (Unigram) (Pang and Lee, 2008) 57.", "startOffset": 19, "endOffset": 39}, {"referenceID": 23, "context": "5232 SVM (Uni+Bigram) (Pang and Lee, 2008) 52.", "startOffset": 22, "endOffset": 42}, {"referenceID": 26, "context": "3773 Lexicon Lookup (Sharma et al., 2015) 51.", "startOffset": 20, "endOffset": 41}, {"referenceID": 23, "context": "In the comparative study performed on our dataset, we observe that Multinomial Naive Bayes performs better than SVM(Pang and Lee, 2008) for snippets providing additional validation to this hypothesis given by Wang and Manning (2012).", "startOffset": 115, "endOffset": 135}, {"referenceID": 26, "context": "The lexicon lookup approach (Sharma et al., 2015) didn\u2019t perform well owing to the heavily misspelt words in the text, which led to incorrect transliterations as shown in Table 6.", "startOffset": 28, "endOffset": 49}, {"referenceID": 23, "context": "In the comparative study performed on our dataset, we observe that Multinomial Naive Bayes performs better than SVM(Pang and Lee, 2008) for snippets providing additional validation to this hypothesis given by Wang and Manning (2012). We also observe that unigrams perform better than bigrams and Bag of words performs better than tf-idf in contrast to trends in English, as the approaches inducing more sparsity would yield to poorer results because our dataset is inherently very sparse.", "startOffset": 116, "endOffset": 233}], "year": 2016, "abstractText": "Sentiment analysis (SA) using code-mixed data from social media has several applications in opinion mining ranging from customer satisfaction to social campaign analysis in multilingual societies. Advances in this area are impeded by the lack of a suitable annotated dataset. We introduce a Hindi-English (Hi-En) code-mixed dataset for sentiment analysis and perform empirical analysis comparing the suitability and performance of various state-of-the-art SA methods in social media. In this paper, we introduce learning sub-word level representations in LSTM (Subword-LSTM) architecture instead of character-level or word-level representations. This linguistic prior in our architecture enables us to learn the information about sentiment value of important morphemes. This also seems to work well in highly noisy text containing misspellings as shown in our experiments which is demonstrated in morpheme-level feature maps learned by our model. Also, we hypothesize that encoding this linguistic prior in the Subword-LSTM architecture leads to the superior performance. Our system attains accuracy 4-5% greater than traditional approaches on our dataset, and also outperforms the available system for sentiment analysis in Hi-En code-mixed text by 18%.", "creator": "TeX"}}}