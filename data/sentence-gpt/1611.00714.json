{"id": "1611.00714", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Scalable Semi-Supervised Learning over Networks using Nonsmooth Convex Optimization", "abstract": "We propose a scalable method for semi-supervised (transductive) learning from massive network-structured datasets. Our approach to semi-supervised learning is based on representing the underlying hypothesis as a graph signal with small total variation. Requiring a small total variation of the graph signal representing the underlying hypothesis corresponds to the central smoothness assumption that forms the basis for semi-supervised learning, i.e., input points forming clusters have similar output values or labels. We formulate the learning problem as a nonsmooth convex optimization problem which we solve by appealing to Nesterovs optimal first-order method for nonsmooth optimization. We also provide a message passing formulation of the learning method which allows for a highly scalable implementation in big data frameworks.\n\n\n\n\nThe current training algorithm contains an average of one to four instructions per training session, and each instruction has two training time zones (0 and 10), each training time zone represents the number of training time zones per training session. We perform the best of both training time zones (4 vs 10) and training time zones (7 vs 10) and also use the \"supervised\" learning strategy to define the learning process. We also apply the algorithm to all training sessions, and then include training time zones in each training session.\n\nPrerequisites\n\nTo fully complete training tasks, participants in the training session are required to enter the training space. The number of training time zones that are required to train and then pass training time zones must be at least one of these. The first training session is a standard training session in which an average of a single training session must be performed using a supervised training time zone. This process is similar to that of the previous training session:\nTraining time zones\nThis training time zone is one time zone in which participants are required to pass training time zones and then pass training time zones, where each training time zone represents the minimum of time zones that must be performed in each training session. The maximum of training time zones must be used to determine the minimum of time zones that must be conducted.\nIn the training session, the average of the training time zones is represented by the total of the training time zones that are required for a given task and a subset of training time zones, for a given training session. Each training time zone represents the average of the training time zones that are required for a given task. As a result, the training time zone represents a small, continuous, and unbalanced distribution that determines the maximum of training time", "histories": [["v1", "Wed, 2 Nov 2016 18:27:53 GMT  (1572kb)", "http://arxiv.org/abs/1611.00714v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["alexander jung", "alfred o hero iii", "alexandru mara", "sabeur aridhi"], "accepted": false, "id": "1611.00714"}, "pdf": {"name": "1611.00714.pdf", "metadata": {"source": "CRF", "title": "Scalable Semi-Supervised Learning over Networks using Nonsmooth Convex Optimization", "authors": ["Alexander Jung", "Alfred O. Hero III", "Alexandru Mara", "Sabeur Aridhi"], "emails": ["hero@eecs.umich.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n00 71\n4v 1\n[ cs\n.L G\n] 2\nN ov\n2 01\n6\nI. INTRODUCTION\nModern technological systems generate (heterogeneous) data at unprecedented scale, i.e., \u201cBig Data\u201d [9], [11], [16], [23]. While lacking a precise formal definition, Big Data problems typically share four main characteristics: (i) large data volume, (ii) high speed of data generation, (iii) data is heterogeneous, i.e., partially labeled or unlabeled, mixture of audio, video and text data and (iv) data is noisy, i.e., there are statistical variations due to missing labels, labeling errors, or poor data curation [23]. Moreover, in a wide range of big data applications, e.g., social networks, sensor networks, communication networks, and biological networks an intrinsic graph (or network) structure is present. This graph structure reflects either the physical properties of a system (e.g., public transportation networks) or statistical dependencies (e.g., probabilistic graphical models for bioinformatics). Quite often, these two notions of graph structure coincide: in a wireless sensor network, the graph modeling the communication links between nodes and the graph formed by statistical dependencies between sensor measurements resemble each other since both graphs are induced by the nodes mutual proximity [17], [21], [24].\nOn the algorithmic side, having a graph model for the observed datapoints faciliates scalable distributed data processing, in the form of message passing on the graph. On a higher-level, graph models are suitable to deal with data of diverse nature, since they only require a weak notion of similarity between datapoints. Moreover, graph models allow to capitalize on massive amounts of unlabeled data via semi-supervised learning. In particular, semi-supervised learning exploits the information contained in large amounts of unlabeled datapoints by considering their similarities to a small number of labeled datapoints.\nIn this paper, we consider the problem of semi-supervised learning using a graph model for the raw data. The observed data consists of a small number of labeled datapoints and a huge amount of unlabeled datapoints. We tackle this learning problem by casting the dataset as a graph signal. In this graph signal model, the different dimensions of the data are identified as variables and the observed values of these variables are called signals. These signals are represented by nodes of a (empirical) graph whose edges represent pairwise dependencies between signals. Imposition of such graph signal structure on the data is\nThis work was partially supported by the Vienna Science and Technology Fund (WWTF) under grant ICT15-119, Army Research Office grants W911NF-15-1-0479 and W911NF-15-1-0241 and US Department of Energy grant DE-NA0002534.\nanalogous to making the smoothness assumption of semi-supervised learning [7]: signals that are connected by an edge in the graph have similar labels. In other words, the graph signal is expected to reflect the underlying graph structure in the sense that the labels of signals on closely connected nodes have high mutual correlation and thus these signals form close-knit clusters or communities [12]. In order to quantify the smoothness assumption underlying semi-supervised learning, one can use different measures to incorporate the topological dependency structure of graphs signals. For example, one can project the signals onto the column space of the graph Laplacian matrix, using the squared norm of the projected signals, i.e., the graph Laplacian form, as a measure of smoothness. This is the basis for many well-known label propagation methods [7].\nIn contrast, the approach proposed in this paper is based on using (graph) total variation [22], which provides a more natural match between smoothness and the community structure of the data, i.e., input or feature signal nodes forming a community or cluster should yield similar output values or labels."}, {"heading": "Contributions and Outline", "text": "In Section II, we formulate semi-supervised learning using a graph model for the observed data as a convex optimization problem. By adapting Nesterov\u2019s method for nonsmooth convex optimization, which is reviewed in Section III, we propose an efficient learning algorithm in Section IV. We then present a message passing formulation of our learning algorithm in Section V, which only requires local information updating. We also discuss how to implement the message passing formulation for graphs of massive size."}, {"heading": "Notation", "text": "Matrices are denoted by boldcase uppercase letters (e.g. A) and column vectors are denoted by boldface lowercase letters (e.g. x). The ith entry of the vector x is denoted by xi. and the entry in the ith row and jth column of matrix A is Ai,. For vectors x,y \u2208 RN and matrices X,Y \u2208 RN\u00d7N , we define the inner products \u3008x,y\u30092 := \u2211 i xiyi and \u3008X,Y\u3009F := \u2211 i,j Xi,jYi,j with induced norms \u2016x\u20162 := \u221a\n\u3008x,x\u30092 and \u2016X\u2016F := \u221a\n\u3008X,X\u3009F. For a generic Hilbert space H, we denote its inner product by \u3008\u00b7, \u00b7\u3009H. Given a linear operator B mapping the Hilbert space H1 into the Hilbert space H2, we denote its adjoint by B\u2217 and by \u2016B\u2016op := sup\u2016x\u2016H1\u22641 \u2016Bx\u2016H2 its operator norm. The operator norm of a matrix A \u2208 R\nM\u00d7N , interpreted as a mapping from Hilbert space RM to RN , reduces to the spectral norm \u2016A\u20162 := supx\u2208RN\\{0} \u2016Ax\u20162/\u2016x\u20162. The ith column of the identity matrix I is denoted by ei. Given a closed convex subset C \u2286 H of a Hilbert space, we denote by \u03c0C(x) = arg min\nz\u2208C \u2016z \u2212 x\u2016H the orthogonal projection on C. For a diagonal matrix\nD \u2208 RN\u00d7N with non-negative main diagonal entries di,i, we denote by D1/2 the diagonal matrix with main diagonal entries \u221a di,i."}, {"heading": "II. PROBLEM FORMULATION", "text": "We consider a heterogeneous dataset D={zi}Ni=1\u2286Z consisting of N datapoints zi\u2208Z , which might be of significantly different nature, e.g., z1 \u2208 Rd, z2 is a continuous-time signal (i.e., z2 : R\u2192R) and z3 might represent the bag-of-words histogram of a text document. Thus, we assume the input space Z rich enough to accomodate for strongly heterogeneous data. Associated with the dataset D is an undirected empirical graph G = (V, E ,W) with node set V = {1, . . . , N}, edge set E \u2286 V \u00d7 V and symmetric weight matrix W \u2208 RN\u00d7N . The nodes represent the datapoints, i.e., node i corresponds to the datapoint zi. An undirected edge (i, j) \u2208 E encodes some notion of (physical or statistical) similarity from datapoint zi to datapoint zj . Moreover, the presence of an edge (i, j) \u2208 E between nodes i, j \u2208 V is indicated by a nonzero entry Wi,j = Wj,i of the weight matrix W. Given an edge (i, j) \u2208 E , the nonzero value Wi,j > 0 represents the strength of the connection from node i to node j. We assume the empirical graph to be simple, i.e., it contains no self-loops (Wi,i =0 for all i\u2208V). The neighborhood N (i) and degree di of node i \u2208 V is defined, respectively, as\nN (i) := {j \u2208 V : (i, j)\u2208E} (1)\nand di := \u2211\nj\u2208N (i)\nWi,j. (2)\nAn key parameter for the characterization of a graph is the maximum node degree [20]\ndmax := max i\u2208V di. (3)\nWithin supervised machine learning, we assign to each datapoint zi \u2208 D an output value or label xi \u2208 R.1 We emphasize that the label xi of node i \u2208 V can take on binary values (i.e., xi\u2208{0, 1}), multi-level discrete values (i.e., xi \u2208{1, . . . , K}, with K being the number of classes or clusters), or continuous values in R. We can represent the entire labeling of the empirical graph conveniently by a vector x\u2208RN whose ith entry is the label xi of node i \u2208 V . For a small subset S of datapoints zi we are provided with initial labels yi. With slight abuse of notation, we refer by S also to the subset of nodes i \u2208 V representing the datapoints zi for which initial labels yi are available. We refer to the set S \u2286 V as the sampling set, where typically M := |S| \u226a N .\nIn order to learn the entire labeling x from the initial labels {yi}i\u2208S , we invoke the basic smoothness assumption for semi-supervised learning [7]: If two points z1, z2 are close, with respect to a given topology on the input space Z , then so should be the corresponding labels x1, x2, with respect to some distance measure on the label space R. For quantifying the smoothness of a labeling, we appeal to the discrete calculus for graph signals, which rests on the concept of a gradient for graph signals [7, Sec. 13.2]. In order to draw on discrete calculus for quantifying smoothness of a labeling, we interpret the labels xi, for i \u2208 V , as the values of a graph signal, i.e., a mapping x[\u00b7] : V \u2192 R which maps node i\u2208V to graph signal value x[i]=xi. Using this interpretation, we measure the smoothness of the labels via the (local) gradient \u2207ix at node i\u2208V , given as [22] (\n\u2207ix )\nj :=\n\u221a\nWi,j(xj\u2212xi). (4)\nThe norm \u2016\u2207ix\u20162= \u221a\u2211\nj\u2208V Wi,j(xj\u2212xi)2 provides a measure for the local variation of the graph signal x at node i\u2208V . The (global) smoothness of the labels xi is then quantified by the total variation [22]:\n\u2016x\u2016TV := \u2211\ni\u2208V\n\u2016\u2207ix\u20162= \u2211\ni\u2208V\n\u221a \u2211\nj\u2208V\nWi,j(xj\u2212xi)2. (5)\nNote that the total variation (5) is a seminorm, being equal to 0 for labelings that are constant over connected graph components.\nThe basic idea of semi-supervised learning is to find a labeling x of the datapoints zi by balancing the empirical error\nErr[x] :=\n\u221a\n(1/2|S|) \u2211\ni\u2208S\n(xi\u2212yi)2, (6)\nwhich represents the deviation of the learned labels xi from the initial labels yi, with the smoothness \u2016x\u2016TV. If we fix a maximum level \u03b5 > 0 tolerated for the empirical error Err[x], we can formulate semi-supervised learning as the optimization problem\nx\u0302\u2208arg min x\u2208Q \u2016x\u2016TV\nwith Q :={x \u2208 RN : Err[x]\u2264\u03b5}. (7) 1We highlight that the term \u201clabel\u201d is typically reserved for discrete-valued or categorial output variables xi [3]. Since we can always represent the values of categorial output variables by real numbers, we will formulate our learning method for real-valued ouput variables xi \u2208 R. Our learning method Alg. 3, which is based on using the squared error loss to quantify the empirical error, can also be used for classification by suitably quantizing the predicted ouput values. Extensions to other loss functions, more suitable to characterize the empirical error for discrete-valued or categorial labels, will be a focus of future work.\nSince the objective function in (7) is the seminorm \u2016z\u2016TV, which is a convex function and also the constraint set Q is a convex set,2 problem (7) is a convex optimization problem. As the notation in (7) suggests, and which can be verified by simple examples, there typically exist several solutions for this optimization problem. However, the methods we consider for solving (7) in the following do not require uniqueness of the solution, i.e., they work even if there are multiple optimal labelings x\u0302.\nFor completeness, we also mention an alternative convex formulation of the recovery problem (7), based on using a penalty term for the total variation instead of constraining the empirical error:\nx\u0302 \u2208 arg min x\u2208RN Err[x] + \u03bb\u2016x\u2016TV. (8)\nThe regularization parameter \u03bb>0 trades off small empirical risk Err[x\u0302] against small total variation \u2016x\u0302\u2016TV of the learned labeling x\u0302.\nThe convex optimization problems (7) and (8) are related by convex duality [2], [5]: For each choice for \u03b5 there is a choice for \u03bb (and vice-versa) such that the solutions of (7) and (8) coincide. However, the relation between \u03b5 and \u03bb for this equivalence to hold is non-trivial and determining the corresponding \u03bb for a given \u03b5 is as challenging as solving the problem (7) itself [1].\nFrom a practical viewpoint, an advantage of the formulations (7) is that the parameter \u03b5 may be interpreted as a noise level, which can be estimated or adjusted more easily than the parameter \u03bb of the learning problem (8). For the rest of the paper, we will focus on the learning problem (7).\nFinally, for a dataset D whose empirical graph G is composed of several (weakly connected) components [20], the learning problem (7) decompose into independent subproblems, i.e., one learning problem of the form (7) for each of the components. Therefore, we will henceforth, without loss of generality, consider datasets whose empirical graph G is (weakly) connected."}, {"heading": "III. OPTIMAL NONSMOOTH CONVEX OPTIMIZATION", "text": "We will now briefly review a recently proposed method [19] for solving nonsmooth convex optimization problems, i.e., optimization problems with a non-differentiable objective function, such as (7). This method exploits a particular structure, which is present in the problems (7). In particular, this optimization method is based on (i) approximating a nonsmooth objective function by a smooth proxy and (ii) then applying an optimal first order (gradient based) method for minimizing this proxy.\nConsider a structured convex optimization problem of the generic form\nx\u0302\u2208arg min x\u2208Q1 f(x) := f\u0302(x)+max u\u2208Q2 \u3008u,Bx\u3009H2\u2212g\u0302(u) \ufe38 \ufe37\ufe37 \ufe38\n:=h0(x)\n. (9)\nHere, B : H1 \u2192 H2 is a linear operator from a finite dimensional Hilbert space H1 to another finite dimensional Hilbert space H2, both defined over the real numbers. The set Q1 \u2286 H1 is required to be a closed convex set and the set Q2 \u2282 H2 is a bounded, closed convex set. The functions f\u0302 and g\u0302 in (9) are required to be continuous and convex on Q1 and Q2, respectively. Moreover, the function f\u0302 is assumed differentiable with gradient \u2207f\u0302 being Lipschitz-continuous with constant L \u2265 0, i.e.,\n\u2016\u2207f\u0302(y)\u2212\u2207f\u0302(x)\u2016H1 \u2264 L\u2016y\u2212x\u2016H1 . (10)"}, {"heading": "Smooth Approximation of Nonsmooth Objective", "text": "In order to solve the nonsmooth problem (9), we approximate the non-differentiable component h0(x) by the smooth function\nh\u00b5(x) :=max u\u2208Q2\n\u3008u,Bx\u3009H2\u2212g\u0302(u)\u2212(\u00b5/2)\u2016u\u20162H2 (11)\n2The seminorm \u2016x\u2016TV is convex since it is homogeneous (\u2016\u03b1x\u2016TV= |\u03b1|\u2016x\u2016TV for \u03b1 \u2208 R) and satisfies the triangle inequality (\u2016x+y\u2016TV\u2264 \u2016x\u2016TV+\u2016y\u2016TV). These two properties imply convexity [5, Section 3.1.5].\nwith the smoothing parameter \u00b5 > 0, yielding\nf\u00b5(x) := f\u0302(x)+max u\u2208Q2\n\u3008u,Bx\u3009H2\u2212g\u0302(u)\u2212(\u00b5/2)\u2016u\u20162H2. (12)\nThe objective function f(x) of the original problem (9) is obtained formally from (12) for the choice \u00b5 = 0, i.e., f(x) = f0(x). Since the function g(u)=\u2016u\u20162H2 is strongly convex, the optimization problem (11) has a unique optimal point\nu\u00b5(x)=argmax u\u2208Q2\n\u3008u,Bx\u3009H2\u2212g\u0302(u)\u2212(\u00b5/2)\u2016u\u20162H2. (13)\nAccording to [19, Theorem 1], the function h\u00b5(x) (cf. (11)) is differentiable with gradient\n\u2207h\u00b5(x) = B\u2217u\u00b5(x), which can be shown to be Lipschitz continuous with constant (1/\u00b5)\u2016B\u20162op. Since the gradient \u2207f\u0302(x) of f\u0302(x) is assumed Lipschitz continuous with constant L (cf. (10)), the function f\u00b5(x) (cf. (12)) has gradient\n\u2207f\u00b5(x) = \u2207f\u0302(x) +B\u2217u\u00b5(x) (14) which is Lipschitz continuous with constant\nL\u00b5 := L+ (1/\u00b5)\u2016B\u20162op. (15) Furthermore, by evaluating [19, Eq. (2.7)], we have\nf\u00b5(x) \u2264 f0(x) = f(x) \u2264 f\u00b5(x) + (\u00b5/2)max u\u2208Q2 \u2016u\u20162H2 , (16)\nwhich verifies that f\u00b5(x) is a uniform smooth approximation of the objective function f(x) in (9). By replacing the objective f(x) in (9) with its smooth approximation f\u00b5(x), we obtain the smooth optimization problem x\u0302\u00b5 \u2208 arg min\nx\u2208Q1\u2286H1\nf\u00b5(x). (17)\nThe original nonsmooth problem (9) is obtained formally from the smooth approximation (17) for the particular choice \u00b5 = 0. For nonzero \u00b5 > 0, the solutions x\u0302 of (9) will be different from the solutions x\u0302\u00b5 of (17) in general. However, for sufficiently small \u00b5 any solution x\u0302\u00b5 of (17) will be also an approximate solution to (9). We can relate the optimal values f(x\u0302) and f\u00b5(x\u0302\u00b5) of the original problem (9) and its smooth approximation (17), respectively, with the help of (16). Indeed, by inserting the optimal points x\u0302\u00b5 and x\u0302 into the corresponding objective functions in (16), we obtain\nf\u00b5(x\u0302\u00b5)\u2264f(x\u0302) \u2264 f\u00b5(x\u0302\u00b5)+(\u00b5/2)max u\u2208Q2 \u2016u\u20162H2. (18)\nThus, the optimal value f\u00b5(x\u0302\u00b5) of the smoothed problem (12) provides an estimate for the optimal value f(x\u0302) of the original problem (9)."}, {"heading": "Optimal Gradient Method for Smooth Minimization", "text": "For solving the smooth optimization problem (17), being a proxy for the original nonsmooth problem (9), we apply an optimal first-order method [1], [19]. This method achieves the optimal worst-case rate of convergence among all gradient based methods [18], [19]. We summarize this method for solving (17) in Alg. 1, which requires as input the smoothing parameter \u00b5 > 0, an initial guess x0 and a valid Lipschitz constant L\u0302 for the gradient (14), i.e., satisfying L\u0302 \u2265 L\u00b5 = L + (1/\u00b5)\u2016B\u20162op. For a particular stopping criterion of Alg. 1, one can monitor the relative decrease in the objective function f\u00b5(x\u0302k) [1, Sec. 3.5.]. Another option, which is used in our numerical experiments (cf. Section VI), is to run Alg. 1 for a fixed number of iterations.\nAlgorithm 1 Nesterov\u2019s algorithm for solving (17)\nInput: smoothing parameter \u00b5, initial guess x0, Lipschitz constant L\u0302 \u2265 L\u00b5=L+(1/\u00b5)\u2016B\u20162op (cf. (15)) Initialize: iteration counter k :=0 1: repeat 2: gk :=\u2207f\u00b5(xk)=\u2207f\u0302(x)+B\u2217u\u00b5(x) with u\u00b5(x) given by (13) 3: x\u0302k :=arg min\nx\u2208Q1\n(L\u0302/2)\u2016x\u2212xk\u20162H1+\u3008gk,x\u2212xk\u3009H1\n4: zk :=arg min x\u2208Q1\n(L\u0302/2)\u2016x\u2212x0\u20162H1+ \u2211k l=0 l+1 2 \u3008gl,x\u2212xl\u3009H1\n5: xk+1 := 2\nk+3 zk+\n(\n1\u2212 2 k+3\n)\nx\u0302k\n6: k :=k+1 7: until stopping criterion is satisfied\nOutput: x\u0302k\nThe steps 2 and 3 of Alg. 1 amount to computing the projected gradient descent step for the smooth optimization problem (17). However, what sets Alg. 1 apart from standard gradient descent methods is step 4. This step uses all previous gradient information in order to compute a projected minimizer zk of an increasingly more accurate approximation of the objective function f\u00b5(x). The new iterate xk+1 is then obtained in step 5 as a convex combination of the projected gradient descent step x\u0302k and the minimizer zk of the approximation to the objective function.\nThe output x\u0302k of Alg. 1 satisfies [19, Theorem 2]\nf\u00b5(x\u0302k)\u2212 f\u00b5(x\u0302\u00b5) \u2264 2L\u0302\u2016x\u0302\u00b5 \u2212 x0\u201622 (k + 1)(k + 2)\n(19)\nfor any optimal point x\u0302\u00b5 of (17). The convergence rate predicted by (19), i.e., the error f\u00b5(x\u0302k) \u2212 f\u00b5(x\u0302\u00b5) decaying proportional to 1/k2 with the iteration counter k, is optimal among all gradient-based minimization methods for the class of continuously differentiable functions with Lipschitz continuous gradient [18, Theorem 2.1.7].\nThe characterization (19) can be used to bound the number of iterations needed to run Alg. 1 such that it delivers an approximate solution x\u0302k \u2208 Q1 for the nonsmooth problem (9) with prescribed accuracy \u03b4, i.e., the output x\u0302k satisfies f(x\u0302k)\u2212 f(x\u0302) \u2264 \u03b4. Lemma III.1. Let x\u0302 \u2208 RN and x\u0302\u00b5 \u2208 RN be optimal points of the original problem (9) and its smoothed proxy (17), respectively. Denote D := max\nu\u2208Q2 \u2016u\u20162H2 and assume Alg. 1 is used with Lipschitz constant\nL\u0302=L+(1/\u00b5)\u2016B\u20162op. Then, the output x\u0302k after k iterations of Alg. 1 satisfies f(x\u0302k)\u2212 f(x\u0302) \u2264 f\u00b5(x\u0302k)\u2212 f\u00b5(x\u0302\u00b5) + (\u00b5/2)D. (20)\nFor the choice \u00b5 = \u03b4/D, Alg. 1 delivers a solution x\u0302k for the non-smooth problem (9) with accuracy \u03b4, i.e.,\nf(x\u0302k)\u2212f(x\u0302) \u2264 \u03b4 for all k\u2265k\u03b4 (21) k\u03b4 :=(2/\u03b4)\u2016x\u0302\u00b5\u2212x0\u20162 \u221a L\u03b4+D\u2016B\u20162op.\nProof: By combining (16) (for the choice x = x\u0302k) with (18), we have\nf(x\u0302k)\u2212 f(x\u0302) \u2264 f\u00b5(x\u0302k)\u2212 f\u00b5(x\u0302\u00b5) + (\u00b5/2)D. (22)\nChoosing \u00b5=\u03b4/D and using (19) for the particular choice L\u0302=L+(1/\u00b5)\u2016B\u20162op we obtain f(x\u0302k)\u2212f(x\u0302) \u2264 (2/\u03b4)\u2016x\u0302\u00b5\u2212x0\u201622(L\u03b4+D\u2016B\u20162op)(1/k2)+\u03b4/2, (23) which implies (21).\nAccording to Lemma III.1 we need k \u221d 1/\u03b4 iterations of Alg. 1 for solving the nonsmooth optimization problem (9) with accuracy \u03b4 (cf. [19, Theorem 3]). This iteration complexity is essentially optimal for any first-order (sub-)gradient method solving problems of the form (9) [14].\nThe lower bound (21) on the iteration complexity of Alg. 1 depends on both the desired accuracy \u03b4 (which is enforced by choosing the smoothing parameter as \u00b5 = \u03b4/D) and the choice for the inital guess via \u2016x\u0302\u00b5 \u2212 x0\u20162. As discussed in [1], an effective approach to speed up the convergence of Alg. 1 is to run it repeatedly with increasing accuracy (corresponding to decreasing values of the smoothing parameter \u00b5) and using the output of Alg. 1 in a particular run as initial guess for the next run. Since the inital guesses used for Alg. 1 in a new run becomes more accurate, it is possible to use a smaller value for the smooting parameter \u00b5, which effects an increased accuracy of the ouput of Alg. 1 according to (21). However, a simpler option is to adapt the smoothing parameter directly \u201con-the-fly\u201d within the iterations of Alg. 1. This results in Alg. 2 being an accelerated version of Alg. 1.\nAlgorithm 2 Accelerated Nesterov for solving (17)\nInput: initial smoothing parameter \u00b50, decreasing factor \u03ba, initial guess x0 Initialize: iteration counter k = 0 1: repeat 2: \u00b5 :=\u00b50\u03ba k\n3: L\u0302 :=L+(1/\u00b5)\u2016B\u20162op 4: gk :=\u2207f\u00b5(xk)=\u2207f\u0302(x)+B\u2217u\u00b5(x) with u\u00b5(x) given by (13) 5: x\u0302k :=arg min\nx\u2208Q1\n(L\u0302/2)\u2016x\u2212xk\u20162H1+\u3008gk,x\u2212xk\u3009H1\n6: zk :=arg min x\u2208Q1\n(L\u0302/2)\u2016x\u2212x0\u20162H1+ \u2211k l=0 l+1 2 \u3008gl,x\u2212xl\u3009H1\n7: xk+1 := 2\nk+3 zk+\n(\n1\u2212 2 k+3\n)\nx\u0302k\n8: k :=k+1 9: until stopping criterion is satisfied\nOutput: x\u0302k"}, {"heading": "IV. EFFICIENT LEARNING OF GRAPH SIGNALS", "text": "We will now show that the semi-supervised learning problem (7) can be rephrased in the generic form (9). This will then allow us to apply Alg. 1 for semi-supervised learning from big data, i.e., from highdimensional heterogeneous data, over networks. To this end, we need to introduce the graph gradient operator \u2207G as a mapping from the Hilbert space RN endowed with inner product \u3008a,b\u30092=aTb into the Hilbert space RN\u00d7N endowed with inner product \u3008A,B\u3009F=Tr{ABT} [13], [15]. In particular, the gradient operator \u2207G maps a graph signal x \u2208 RN to the matrix\n\u2207Gx := ( \u22071x, . . . ,\u2207Nx )T \u2208 RN\u00d7N . (24) The ith row of the matrix \u2207Gx is given by the local gradient \u2207ix of the graph signal x at node i \u2208 V (cf. (4)). Let us also highlight the close relation between the gradient operator \u2207G : RN \u2192 RN\u00d7N and the\nnormalized graph Laplacian matrix L [8], defined element-wise as\n( L )\ni,j :=\n \n\n1 if i = j and di 6= 0, \u22121/ \u221a didj if (i, j) \u2208 E ,\n0 otherwise,\n(25)\nwith di being the degree of node i \u2208 V (cf. (2)). If we define the diagonal matrix D with diagonal elements di,i = di, we have for any graph signal x (cf. [22, Eq. (6)]) the identity\n\u2016\u2207Gx\u20162F = xTD1/2LD1/2x. (26) We then define the divergence operator divG : RN\u00d7N \u2192 RN as the negative adjoint of the gradient operator \u2207G : RN \u2192 RN\u00d7N (cf. [7, Chapter 13]) , i.e., divG :=\u2212\u2207\u2217G . (27)\nA straightforward calculation (cf. [7, Proposition 13.4]) reveals that the operator divG maps a matrix P \u2208 R N\u00d7N to the vector divG P \u2208 RN with entries\n(divG P)i = \u2211\nj\u2208V\n\u221a Wi,jPi,j \u2212 \u221a Wj,iPj,i\n(1) =\n\u2211\nj\u2208N (i)\n\u221a Wi,jPi,j \u2212 \u221a Wj,iPj,i. (28)\nWe highlight the fact that both, the gradient \u2207G : RN \u2192 RN\u00d7N as well as the divergence operator divG depend on the graph structure due to the presence of the weights Wi,j in (4) and (28). Moreover, the above definitions for the gradient and divergence operator over complex networks are straightforward generalizations of the well-known gradient and divergence operator for grid graphs representing 2D-images [6].\nUsing the identity \u2016\u2207ix\u20162 = max \u2016pi\u20162\u22641 \u3008pi,\u2207ix\u30092, we can represent the total variation (5) as\n\u2016x\u2016TV = \u2211\ni\u2208V\nmax \u2016pi\u20162\u22641 \u3008pi,\u2207ix\u30092 = max P\u2208P \u3008P,\u2207Gx\u3009F (29)\nwith the closed convex set\nP := {P = (p1, . . . ,pN)T \u2208 RN\u00d7N : (30) \u2016pi\u20162 \u2264 1 for every i = 1, . . . , N}.\nUsing (29), the learning problem (7) can be written as\nmin x\u2208Q f0(x) with f0(x) := max P\u2208P\n\u3008P,\u2207Gx\u3009F (31)\nwith constraint set Q={x \u2208 RN : Err[x] \u2264 \u03b5} (cf. (6) and (7)). The optimization problem (31) is exactly of the form (9) with the linear operator B = \u2207G , the functions f\u0302(x) \u2261 0 and g\u0302(u) \u2261 0, and the sets Q1 = Q and Q2 = P . The smoothed version (cf. (12)) of the problem (7) is then obtained as\nmin x\u2208Q f\u00b5(x) with (32)\nf\u00b5(x) := max P\u2208P\n( \u3008P,\u2207Gx\u3009F\u2212(\u00b5/2)\u2016P\u20162F ) .\nIn order to apply Alg. 1 to the smoothed version (32) of the learning problem (7), we have to determine the gradient \u2207f\u00b5(x) and a corresponding valid Lipschitz constant L\u0302 \u2265 L\u00b5 (cf. (15)). The gradient \u2207f\u00b5(x)\nis obtained by specializing (14) for the objective in (32), yielding\n\u2207f\u00b5(x) = \u2212 divG P\u00b5(x) (33) with\nP\u00b5(x) = argmax P\u2208P\n( \u3008P,\u2207Gx\u3009F \u2212 (\u00b5/2)\u2016P\u20162F ) . (34)\nBy the KKT conditions for constrained convex optimization problems [5], [15],\nP\u00b5(x) = (q1, . . . ,qN) T (35)\nwith qi = 1\nmax{\u00b5, \u2016\u2207ix\u20162} \u2207ix\nA particular Lipschitz constant for the gradient \u2207f\u00b5(x) is obtained, by specializing (15) to B = \u2207G , as\nL\u00b5 = (1/\u00b5)\u2016\u2207G\u20162op (27) = (1/\u00b5)\u2016 divG \u20162op. (36)\nHowever, since evaluating the exact operator norm of the gradient (or divergence) operator is difficult for an arbitrary large-scale graph,3 we will rely on a simple upper bound.\nLemma IV.1. Let G = (V, E ,W) be a weighted undirected graph and let \u2207G denote the corresponding gradient operator (24). The norm of the gradient operator satisfies\n\u2016\u2207G\u2016op \u2264 \u221a 2dmax (37)\nwith the maximum node degree dmax (cf. (3)).\nProof: Due to (26), we have \u2016\u2207G\u20162op = \u2016D1/2LD1/2\u20162, (38)\nwhich, since obviously \u2016D\u20162 \u2264 dmax, implies \u2016\u2207G\u20162op \u2264 dmax\u2016L\u20162. (39)\nThe bound (37) follows from the well-known upper bound \u2016L\u20162 \u2264 2 for the maximum eigenvalue (which is equal to the spectral norm) of the normalized Laplacian matrix L (cf. [8, Lemma 1.7])\nAccording to Lemma IV.1, the gradient \u2207f\u00b5(x) (cf. (33)) of f\u00b5(x) is Lipschitz with constant L\u0302=2dmax/\u00b5, (40)\nwhich we can use as input to Alg. 1. In order to apply Alg. 1 to the smoothed learning problem (32), we now present closed-form expressions for the updates in step 3 and 4 of Alg. 1 for Q1 = Q (cf. (7)). Lemma IV.2. Consider the convex set Q = {x : Err[x] \u2264 \u03b5} and let y denote any labeling which is consistent with the initial labels yi, i.e., ( y )\ni = yi for all i \u2208 S. Then, using the shorthand D(S) :=\u2211\ni\u2208S eie T i , the solution x\u0302k of the optimization problem\nx\u0302k=arg min x\u2208Q\n(L\u0302/2)\u2016x\u2212xk\u201622+gTk (x\u2212xk) (41)\n3In many big data applications it is not possible to have a complete description of the graph, e.g. in form of an edge list, available. Instead, one typically has only knowledge about some basic parameters, e.g., the maximum node degree dmax (cf. (3)).\nis given by\nx\u0302k= ( I+\u03bb\u03b5D(S) )\u22121 (q+\u03bb\u03b5D(S)y) (42) = (I\u2212D(S))q+ { D(S) [ y+(\u03b5/r)(q\u2212y) ] if r>\u03b5\nD(S)q otherwise with\nq :=xk\u2212(1/L\u0302)gk, r :=Err[q\u2212y] (43) , and \u03bb\u03b5 :=max{0, (r/\u03b5)\u22121}.\nIn a similar manner, the solution zk of the optimization problem\nzk=arg min x\u2208Q\n(L\u0302/2)\u2016x\u2212x0\u201622+(1/2) k\u2211\nl=0\n(l+1)gTl (x\u2212xl) (44)\nis given by\nzk= ( I+ \u03bb\u0303\u03b5D(S) )\u22121 (q\u0303+\u03bb\u0303\u03b5D(S)y) (45) =(I\u2212D(S))q\u0303+ { D(S) [ y+(\u03b5/r)(q\u0303\u2212y) ] if r>\u03b5\nD(S)q\u0303 otherwise with\nq\u0303 :=x0\u2212(1/2L\u0302) k\u2211\nl=0\n(l+1)gl, r\u0303 :=Err[q\u0303\u2212y] (46)\n, and \u03bb\u0303\u03b5 = max{0, (r\u0303/\u03b5)\u22121}. Proof: see Appendix A.\nThe closed-form expressions (42) and (45) are suitable modifications of those presented in [1, Sec. 3] to our setting of semi-supervised learning over complex networks. We are now in the position to specialize Alg. 1 to the smoothed learning problem (32) by using the closed-form expressions (42) and (45) for step 3 and 4 of Alg. 1. This results in Alg. 3 for semi-supervised learning from big data over networks.\nThe steps 2 and 3 of Alg. 3 amount to computing the gradient gk = \u2207f\u00b5(xk) of the objective function f\u00b5(x) in the learning problem (32). The steps 4-6 of Alg. 3 implement a projected gradient descent step, while steps 7-9 amount to computing the minimizer zk of the approximation in step 4 of Alg. 1.\nCombining (19) with (40), yields the following characterization of the convergence rate of Alg. 3:\nf\u00b5(x\u0302k)\u2212 f\u00b5(x\u0302\u00b5) \u2264 4dmax\u2016x\u0302\u00b5 \u2212 x0\u201622 \u00b5(k + 1)(k + 2)\n(47)\nfor any solution x\u0302\u00b5 of (32). The bound (47) suggests that the convergence is faster for graphs which are more sparse, i.e., have a smaller maximum node degree dmax. As for Alg. 1, the convergence speed of Alg. 3 depends on the accuracy of the inital guess as well as on the smoothing parameter \u00b5. The accelerated version of Alg. 3 is then obtained from Alg. 2, yielding Alg. 4."}, {"heading": "V. MESSAGE PASSING FORMULATIONS", "text": "In order to make semi-supervised learning via the optimization problem (32) feasible for massive (internetscale) datasets, we will now discuss a message passing formulation of Alg. 3, which is summarized as Alg. 5 (being an adaption of [13, Alg. 2] to undirected graphs): The steps 2-6 of Alg. 5 amount to computing the (scaled) gradient \u2207f\u00b5(x) (cf. (33)) of the objective function f\u00b5(x) for the problem (32) in a distributed manner. The quantities Pi,j are the entries of the matrix P\u00b5(x) (cf. (34)). The steps 11-15 and 17-21 of\nAlgorithm 3 Semi-Supervised Learning via Nesterov\u2019s Method\nInput: dataset D with empirical graph G, subset S = {i1, . . . , iM} of datapoints with initial labels {yj}j\u2208S , error level \u03b5, smoothing parameter \u00b5, initial guess for the labeling x0 \u2208 RN\nInitialize: k :=0, Lipschitz constant L\u0302 :=(2/\u00b5)dmax, q\u03030 :=x0, \u03b1 :=1/2 1: repeat 2: \u2200i \u2208 V : pi := 1max{\u00b5,\u2016\u2207ixk\u20162}\u2207ixk 3: gk :=\u2212 divG(P) with P=(p1, . . . ,pN )T 4: qk :=xk\u2212(1/L\u0302)gk 5: r :=Err[qk] (cf. (6))\n6: \u2200i \u2208 V : x\u0302k,i := {\nyi + (\u03b5/r)(qk,i \u2212 yi) if i \u2208 S and r > \u03b5 qk,i otherwise\n7: q\u0303k := q\u0303k\u2212(\u03b1/L\u0302)gk 8: r\u0303 := Err[q\u0303k] 9: \u2200i \u2208 V : zk,i := {\nyi+(\u03b5/r\u0303)(q\u0303k,i\u2212yi) if i \u2208 S and r\u0303 > \u03b5 q\u0303k,i otherwise\n10: xk+1 := 2 k+3 zk+(1\u2212 2k+3)x\u0302k\n11: k := k+1 12: \u03b1 := \u03b1 + 1/2 13: until stopping criterion is satisfied Output: learned labeling x\u0302k for all datapoints\nAlgorithm 4 Semi-Supervised Learning via Accelerated Nesterov\nInput: dataset D with empirical graph G, subset S = {i1, . . . , iM} of datapoints with initial labels {yj}j\u2208S , error level \u03b5, initial smoothing parameter \u00b50, decreasing factor \u03ba, initial guess for the labeling x0 \u2208 RN\nInitialize: k :=0, q\u03030 :=x0, \u03b1 :=1/2 1: repeat 2: \u00b5 :=\u00b50\u03ba k\n3: L\u0302 :=(2/\u00b5)dmax 4: \u2200i \u2208 V : pi := 1max{\u00b5,\u2016\u2207ixk\u20162}\u2207ixk 5: gk :=\u2212 divG(P) with P=(p1, . . . ,pN )T 6: qk :=xk\u2212(1/L\u0302)gk 7: r :=Err[qk] (cf. (6)) 8: \u2200i \u2208 V : x\u0302k,i := {\nyi + (\u03b5/r)(qk,i \u2212 yi) if i \u2208 S and r > \u03b5 qk,i otherwise\n9: q\u0303k := q\u0303k\u2212(\u03b1/L\u0302)gk 10: r\u0303 := Err[q\u0303k] 11: \u2200i \u2208 V : zk,i := {\nyi+(\u03b5/r\u0303)(q\u0303k,i\u2212yi) if i \u2208 S and r\u0303 > \u03b5 q\u0303k,i otherwise\n12: xk+1 := 2 k+3 zk+(1\u2212 2k+3)x\u0302k\n13: k := k+1 14: \u03b1 := \u03b1 + 1/2 15: until stopping criterion is satisfied Output: learned labeling x\u0302k for all datapoints\nAlg. 5 implement a finite number K of iterations of the average consensus algorithm, using MetropolisHastings weights [26] , for (approximately) computing the sums (1/N) \u2211\nj\u2208V bj = (1/N) \u2211\nj\u2208S(yj \u2212 qj)2 and (1/N) \u2211\nj\u2208V b\u0303j = (1/N) \u2211\nj\u2208S(yj \u2212 q\u0303j)2, respectively. In particular, for sufficiently large K, the results ri and r\u0303i in step 16 and 22 of Alg. 5 satisfy ri \u2248 r and r\u0303i \u2248 r\u0303 for every node i \u2208 V (cf. step 6,7 of Alg. 3). The choice for the number K of avarage consensus iterations can be guided by a wide range of results characterzing the convergence rate of average conensus [4], [10], [26]. As a rule of thumb, K should be significantly larger than the diameter d(G) of the underlying graph G [10, Thm. 4.3.]. We highlight that Alg. 5 requires each node i \u2208 V to have access to local information only. In particular, to implement Alg. 5 on a given node i \u2208 V , the measurement yj , the value xj , the matrix entries Pi,j and the edge weights Wi,j are required only for node i itself and its neighborhood N (i).\nWe implemented Alg. 5 using the big data framework AKKA [25], which is a toolkit for building distributed and resilient message-driven applications. The AKKA implementation was run on a computing cluster composed of nine virtual machines (one master and eight slave workers) obtained via the cloud computing service Amazon EC2. Each virtual machine has been configured with a 64-bit CPU, 3.75 GB of main memory, and 8 GB of local disk storage. In Figure 1 we sketch the basic architecture of the AKKA implementation of our graph learning methods. First, we partition the graph in a simple uniform manner, i.e., node i \u2208 {1, . . . , N} is assigend to partition (i mod 8)+1. After partitioning G, the master machine assigns the obtained partitions to the eight workers and manages the execution of the message passing algorithm between the workers. There are two alternating phases in the execution of the AKKA implementation: the master phase, where the states of the worker machines are synchronized and the worker phase. Two types of operations are executed in the worker phase:\n\u2022 intra-block operations: each worker performs local computations within its associated partition, and \u2022 inter-block operations: workers exchange messages across their partitions.\nWe then compared the runtime of the AKKA implementation to the centralized implementation in MATLAB used in [13]. The results indicate a runtime reduction by almost a factor 10 which is reasonable since we are using a cluster of nine machines."}, {"heading": "VI. NUMERICAL EXPERIMENTS", "text": "We assess the performance of (the accelerated version of) the proposed learning algorithm Alg. 4 empirically by applying it to an synthetic dataset with empirical graph G = (V, E ,W), depicted in Fig. 2, whose nodes are made up of 2 disjoint clusters Cc of same size |Cc|=100 giving a total graph size of\nAlgorithm 5 Distributed Semi-Supervised Learning via Nesterov\u2019s Method\nInput: sampling set S = {i1, . . . , iM}, samples {yj}j\u2208S , error level \u03b5, edge weights {Wi,j}i,j\u2208V , smoothing parameter \u00b5, initial guess {x0,i}i\u2208V , number K of average consensus iterations\nInitialize: L\u0302 :=(2/\u00b5)dmax, \u2200i \u2208 V : xi=x0,i, \u03b1 :=1/2, \u03c4 :=2/3, {ui,j :=1/(max{di, dj}+1)}j\u2208N (i) 1: repeat 2: \u2200i \u2208 V : broadcast xi to neighbors N (i)/ collect {xj}j\u2208N (i) from neighbors N (i) 3: \u2200i \u2208 V : update \u03b3i = \u221a\u2211\nj\u2208N (i)(xj \u2212 xi)2Wi,j 4: \u2200i \u2208 V : for neighbor j \u2208 N (i) update Pi,j = 1max{\u00b5,\u03b3i}(xj \u2212 xi) \u221a Wi,j 5: \u2200i \u2208 V : broadcast Pi,j to neighbors N (i)/ collect {Pj,i}j\u2208N (i) from neighbors N (i) 6: \u2200i \u2208 V : update gi = (1/L\u0302) (\u2211\nj\u2208N (i)\n\u221a Wj,iPj,i \u2212 \u2211\nj\u2208N (i)\n\u221a Wi,jPi,j )\n7: \u2200i \u2208 V : update qi = xi \u2212 gi 8: \u2200i \u2208 V : update gi = gi \u2212 \u03b1gi 9: \u2200i \u2208 V : update \u03b1 = \u03b1 + 1/2\n10: \u2200i \u2208 V : update q\u0303i = x0,i\u2212gi 11: \u2200i \u2208 V : update bi = {\n(yi \u2212 qi)2 for i \u2208 S 0 else\n12: for l = 1:K do 13: \u2200i \u2208 V : broadcast bi to neighbors N (i)/ collect {bj}j\u2208N (i) from N (i) 14: \u2200i \u2208 V : update bi = ( 1\u2212\u2211j\u2208N (i) ui,j ) bi + \u2211\nj\u2208N (i) ui,jbj 15: end for 16: \u2200i \u2208 V : update ri = \u221a Nbi 17: \u2200i \u2208 V : update b\u0303i = {\n(yi \u2212 q\u0303i)2 for i \u2208 S 0 else\n18: for l = 1:K do 19: \u2200i \u2208 V : broadcast b\u0303i to neighbors N (i)/collect {b\u0303j}j\u2208N (i) from N (i) 20: \u2200i \u2208 V : update b\u0303i = ( 1\u2212\u2211j\u2208N (i) ui,j ) b\u0303i + \u2211\nj\u2208N (i) ui,j b\u0303j 21: end for 22: \u2200i \u2208 V : update r\u0303i = \u221a Nb\u0303i 23: \u2200i \u2208 V : update x\u0302i = {\nyi + (\u03b5/r)(qi \u2212 yi) if i \u2208 S and r > \u03b5 qi otherwise\n24: \u2200i \u2208 V : update zi = {\nyi + (\u03b5/r\u0303)(q\u0303i \u2212 yi) if i \u2208 S and r\u0303 > \u03b5 q\u0303i otherwise\n25: \u2200i \u2208 V : update xi = \u03c4zi + (1\u2212 \u03c4)x\u0302i 26: \u2200i \u2208 V : update \u03c4 = ( (1/\u03c4) + (1/2) )\u22121\n27: until stopping criterion is satisfied Output: x\u0302i\nN=2 \u00b7100 nodes. The clusters are connected through few \u201cgate\u201d nodes. The maximum node degree of G is dmax=8. Given the empirical graph G, we generated a labeling x(g) by labeling the nodes for each cluster Cc by a random number tc \u223c N (0, 1), i.e., x(g)i = tc for all nodes i \u2208 Cc in the cluster Cc. For each cluster Cc we assume that we are provided initial labels yi = xi for 100 randomly choosen nodes i \u2208 Cc, giving rise to an overall sampling set S with M=2 \u00b7 100 nodes. We run Alg. 4 with initial smoothing parameter \u00b5=1, decreasing factor \u03ba=(2 \u00b7 10\u22125)1/2000, error bound \u03b5 :=\u2016x(g)\u20162/105 and a fixed number of 2000 iterations, to obtain the learned labels x\u0302i for every node i \u2208 V . In Fig. 3, we show the learned labeling x\u0302i output by Alg. 4. We also show the learned labeling x\u0302LPi obtained using the well-known label progagation (LP)\n.\nalgorithm [7, Alg. 11.1] which is run for the same number of iterations. From Fig. 3, it is evident that Alg.\n.\n4 yields better learning accuracy compared to plain LP, which is also reflected in the empirical normalized MSEs NMSEnest \u2248 2.1\u00d7 10\u22124 and NMSELP \u2248 2.4\u00d7 10\u22123 obtained by averaging \u2016x\u0302\u2212x(g)\u201622/\u2016x(g)\u201622 and \u2016x\u0302LP\u2212x(g)\u201622/\u2016x(g)\u201622 over 100 independent Monte Carlo runs. We have also depicted the dependence of the NMSE of Alg. 4 and LP on the iteration number k in Fig. 4, which shows that after some inital phase, which comprises \u2248 100 iterations, the NMSE obtained by Alg. 4 converges quickly to its stationary value.\nRemarkably, According to Fig. 4, the simple LP method provides smaller NMSE for the first few iterations. However, the comparison of the convergence speed of Alg. 4 and LP should be interpreted carefully, since the optimization problem underlying LP is based on the smooth Laplacian quadratic form [7, Sec. 11.3.], whereas Alg. 4 amounts to solving the non-smooth problem (7).\n."}, {"heading": "VII. CONCLUSIONS", "text": "The problem of semi-supervised learning from massive datastes over networks has been formulated as a nonsmooth convex optimization problem based on penalizing the total variation of the labeling. We applied the smoothing technique of Nesterov to this optimization problem for obtaining an efficient learning algorithm which can capitalize on huge amounts of unlabeled data using only few labeled datapoints. Moreover, we proposed an implementation of the learning method as message passing over the underlying data graph. This message passing algorithm can be easily implemented in a big data platform such as AKKA to allow for scalable learning algorithms. Future work includes the extension of the optimization framework to accomodate loss functions, different from the mean squared error, that better characterize the training error for discrete valued or categorial labels."}, {"heading": "APPENDIX A PROOF OF LEMMA IV.2", "text": "We only detail the derivation of (42), since the derviation of (45) is very similar. Our argument closely follows the derivations used in [1, Sec. 3]. Consider the constrained convex optimization problem (41), which we repeat here for convenience:\nx\u0302k = arg min x\u2208Q\n(L\u0302/2)\u2016x\u2212 xk\u201622+gTk (x\u2212xk) (48)\nwith constraint set Q := {x : Err[x] \u2264 \u03b5} = {x : ( Err[x] )2 \u2264 \u03b52}. The Lagrangian associated with (48) is\nL(x, \u03bb)=(L\u0302/2)\u2016x\u2212xk\u201622+gTk (x\u2212xk) +\u03bb( ( Err[x] )2\u2212\u03b52), (49)\nand the corresponding KKT conditions for x\u0302k and \u03bb\u03b5 to be primal and dual optimal read [5, Section 5.5.3]\nL\u0302(x\u0302k\u2212xk)+gk+(\u03bb\u01eb/|S|)D(S)(x\u0302k\u2212y)=0, (50) \u03bb\u03b5(Err[x\u0302k]\u2212 \u03b5) = 0, (51)\nErr[x\u0302k] \u2264 \u03b5, (52) \u03bb\u03b5 \u2265 0, (53)\nwith the diagonal matrix D(S) = \u2211i\u2208S eieTi . From condition (50), we obtain\nx\u0302k= ( I+\u03bb\u0303\u03b5D(S) )\u22121 (xk\u2212(1/L\u0302)gk+\u03bb\u0303\u03b5D(S)y). (54)\nwith \u03bb\u0303\u03b5 := \u03bb\u01eb\nL\u0302|S| (55)\nUsing the elementary identity (I+ aD(S))\u22121 = I\u2212 a\n1 + a D(S) (56)\nwhich is valid for any a \u2265 0, we can develop (54) further to\nx\u0302k = ( I\u2212 \u03bb\u0303\u03b5 1 + \u03bb\u0303\u03b5 D(S) ) (xk \u2212 (1/L\u0302)gk + \u03bb\u0303\u03b5D(S)y). (57)\nInserting (57) into (52) yields\nErr[x\u0302k] (57) = Err[ ( I\u2212 \u03bb\u0303\u03b5\n1 + \u03bb\u0303\u03b5 D(S)\n) (xk \u2212 (1/L\u0302)gk\n+ \u03bb\u0303\u03b5D(S)y)] (6) = 1\n(1 + \u03bb\u0303\u03b5)2 Err[xk \u2212 (1/L\u0302)gk \u2212 y)]\n(52) \u2264 \u03b5. (58)\nFrom (58), we have \u03bb\u0303\u03b5 \u2265 (1/\u03b5)Err[xk\u2212(1/L\u0302)gk\u2212y)]\u22121. (59)\nThus if (1/\u03b5)Err[xk\u2212(1/L\u0302)gk\u2212y)]>1, then (59) implies \u03bb\u03b5 > 0, which, via (51), requires the inequality (58) to become an equality, i.e.,\n\u03bb\u0303\u03b5=(1/\u03b5)Err[xk\u2212(1/L\u0302)gk\u2212y)]\u22121 (60) This equality holds also if Err[xk\u2212(1/L\u0302)gk\u2212y)]/\u03b5 = 1. For Err[xk\u2212(1/L\u0302)gk\u2212y)]/\u03b5 < 1, complementary slackness (51) requires \u03bb\u0303\u03b5 = 0. Thus the optimal dual variable \u03bb\u0303\u03b5 is fully determined by the quantity Err[xk \u2212 (1/L\u0302)gk \u2212 y)] via\n\u03bb\u0303\u03b5 = max{0, (1/\u03b5)Err[xk \u2212 (1/L\u0302)gk \u2212 y)]\u22121}. (61)"}], "references": [{"title": "NESTA: a fast and accurate first-order method for sparse recovery", "author": ["S. Becker", "J. Bobin", "E.J. Cand\u00e8s"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Fastest mixing markov chain on a graph", "author": ["S. Boyd", "P. Diaconis", "L. Xiao"], "venue": "SIAM Review,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "An algorithm for total variation minimization and applications", "author": ["A. Chambolle"], "venue": "Journal of Mathematical imaging and vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien", "editors"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Big Data over Networks", "author": ["S. Cui", "A. Hero", "Z.-Q. Luo", "J. Moura", "editors"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "What do we know about the metropolis algorithm", "author": ["P. Diaconis"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "High-dimensional data analysis: The curses and blessings of dimensionality", "author": ["D.L. Donoho"], "venue": "In Amer. Math. Soc. Lecture:\u201cMath challenges of the 21st century\u201d,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Community detection", "author": ["S. Fortunato"], "venue": "in graphs. arXiv,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Efficient graph signal recovery over big networks", "author": ["G. Hannak", "P. Berger", "G. Matz", "A. Jung"], "venue": "In Proc. Asilomar Conf. Signals, Sstems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "First-order methods for nonsmooth convex large-scale optimization, I: General purpose methods", "author": ["A. Juditsky", "A. Nemirovski"], "venue": "Optimization for Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Scalable graph signal recovery for big data over networks", "author": ["A. Jung", "P. Berger", "G. Hannak", "G. Matz"], "venue": "In 2016 IEEE 17th International Workshop on Signal Processing Advances in Wireless Communications (SPAWC),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Big data: The next frontier for innovation, competition, and productivity", "author": ["J. Mayika", "B. Brown", "J. Bughin", "R. Dobbs", "C. Roxburgh", "A.H. Byers"], "venue": "McKinsey Global Institute,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Distributed decorrelation in sensor networks with application to distributed particle filtering", "author": ["M. Moldaschl", "W.N. Gansterer", "O. Hlinka", "F. Meyer", "F. Hlawatsch"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Introductory lectures on convex optimization, volume 87 of Applied Optimization", "author": ["Y. Nesterov"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Smooth minimization of non-smooth functions", "author": ["Y. Nesterov"], "venue": "Math. Program.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Networks: An Introduction", "author": ["M. Newman"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Innovations diffusion: A spatial sampling scheme for distributed estimation and detection", "author": ["Z. Quan", "W.J. Kaiser", "A.H. Sayed"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "author": ["D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Hadoop: The Definitive Guide", "author": ["T. White"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Distributed covariance estimation in Gaussian graphical models", "author": ["A. Wiesel", "A.O. Hero"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Akka Concurrency", "author": ["D. Wyatt"], "venue": "Artima Incorporation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Distributed average consensus with least-mean-square deviation", "author": ["L. Xiao", "S. Boyd", "S.-J. Kim"], "venue": "Journal of Parallel and Distributed Computing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}], "referenceMentions": [{"referenceID": 6, "context": ", \u201cBig Data\u201d [9], [11], [16], [23].", "startOffset": 13, "endOffset": 16}, {"referenceID": 8, "context": ", \u201cBig Data\u201d [9], [11], [16], [23].", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": ", \u201cBig Data\u201d [9], [11], [16], [23].", "startOffset": 24, "endOffset": 28}, {"referenceID": 20, "context": ", \u201cBig Data\u201d [9], [11], [16], [23].", "startOffset": 30, "endOffset": 34}, {"referenceID": 20, "context": ", there are statistical variations due to missing labels, labeling errors, or poor data curation [23].", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "Quite often, these two notions of graph structure coincide: in a wireless sensor network, the graph modeling the communication links between nodes and the graph formed by statistical dependencies between sensor measurements resemble each other since both graphs are induced by the nodes mutual proximity [17], [21], [24].", "startOffset": 304, "endOffset": 308}, {"referenceID": 18, "context": "Quite often, these two notions of graph structure coincide: in a wireless sensor network, the graph modeling the communication links between nodes and the graph formed by statistical dependencies between sensor measurements resemble each other since both graphs are induced by the nodes mutual proximity [17], [21], [24].", "startOffset": 310, "endOffset": 314}, {"referenceID": 21, "context": "Quite often, these two notions of graph structure coincide: in a wireless sensor network, the graph modeling the communication links between nodes and the graph formed by statistical dependencies between sensor measurements resemble each other since both graphs are induced by the nodes mutual proximity [17], [21], [24].", "startOffset": 316, "endOffset": 320}, {"referenceID": 5, "context": "analogous to making the smoothness assumption of semi-supervised learning [7]: signals that are connected by an edge in the graph have similar labels.", "startOffset": 74, "endOffset": 77}, {"referenceID": 9, "context": "In other words, the graph signal is expected to reflect the underlying graph structure in the sense that the labels of signals on closely connected nodes have high mutual correlation and thus these signals form close-knit clusters or communities [12].", "startOffset": 246, "endOffset": 250}, {"referenceID": 5, "context": "This is the basis for many well-known label propagation methods [7].", "startOffset": 64, "endOffset": 67}, {"referenceID": 19, "context": "In contrast, the approach proposed in this paper is based on using (graph) total variation [22], which provides a more natural match between smoothness and the community structure of the data, i.", "startOffset": 91, "endOffset": 95}, {"referenceID": 17, "context": "An key parameter for the characterization of a graph is the maximum node degree [20]", "startOffset": 80, "endOffset": 84}, {"referenceID": 5, "context": "In order to learn the entire labeling x from the initial labels {yi}i\u2208S , we invoke the basic smoothness assumption for semi-supervised learning [7]: If two points z1, z2 are close, with respect to a given topology on the input space Z , then so should be the corresponding labels x1, x2, with respect to some distance measure on the label space R.", "startOffset": 145, "endOffset": 148}, {"referenceID": 19, "context": "Using this interpretation, we measure the smoothness of the labels via the (local) gradient \u2207ix at node i\u2208V , given as [22] ( \u2207ix )", "startOffset": 119, "endOffset": 123}, {"referenceID": 19, "context": "The (global) smoothness of the labels xi is then quantified by the total variation [22]: \u2016x\u2016TV := \u2211", "startOffset": 83, "endOffset": 87}, {"referenceID": 1, "context": "We highlight that the term \u201clabel\u201d is typically reserved for discrete-valued or categorial output variables xi [3].", "startOffset": 111, "endOffset": 114}, {"referenceID": 3, "context": "The convex optimization problems (7) and (8) are related by convex duality [2], [5]: For each choice for \u03b5 there is a choice for \u03bb (and vice-versa) such that the solutions of (7) and (8) coincide.", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "However, the relation between \u03b5 and \u03bb for this equivalence to hold is non-trivial and determining the corresponding \u03bb for a given \u03b5 is as challenging as solving the problem (7) itself [1].", "startOffset": 184, "endOffset": 187}, {"referenceID": 17, "context": "Finally, for a dataset D whose empirical graph G is composed of several (weakly connected) components [20], the learning problem (7) decompose into independent subproblems, i.", "startOffset": 102, "endOffset": 106}, {"referenceID": 16, "context": "OPTIMAL NONSMOOTH CONVEX OPTIMIZATION We will now briefly review a recently proposed method [19] for solving nonsmooth convex optimization problems, i.", "startOffset": 92, "endOffset": 96}, {"referenceID": 0, "context": "Optimal Gradient Method for Smooth Minimization For solving the smooth optimization problem (17), being a proxy for the original nonsmooth problem (9), we apply an optimal first-order method [1], [19].", "startOffset": 191, "endOffset": 194}, {"referenceID": 16, "context": "Optimal Gradient Method for Smooth Minimization For solving the smooth optimization problem (17), being a proxy for the original nonsmooth problem (9), we apply an optimal first-order method [1], [19].", "startOffset": 196, "endOffset": 200}, {"referenceID": 15, "context": "This method achieves the optimal worst-case rate of convergence among all gradient based methods [18], [19].", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "This method achieves the optimal worst-case rate of convergence among all gradient based methods [18], [19].", "startOffset": 103, "endOffset": 107}, {"referenceID": 11, "context": "This iteration complexity is essentially optimal for any first-order (sub-)gradient method solving problems of the form (9) [14].", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "As discussed in [1], an effective approach to speed up the convergence of Alg.", "startOffset": 16, "endOffset": 19}, {"referenceID": 10, "context": "To this end, we need to introduce the graph gradient operator \u2207G as a mapping from the Hilbert space R endowed with inner product \u3008a,b\u30092=ab into the Hilbert space R endowed with inner product \u3008A,B\u3009F=Tr{AB} [13], [15].", "startOffset": 206, "endOffset": 210}, {"referenceID": 12, "context": "To this end, we need to introduce the graph gradient operator \u2207G as a mapping from the Hilbert space R endowed with inner product \u3008a,b\u30092=ab into the Hilbert space R endowed with inner product \u3008A,B\u3009F=Tr{AB} [13], [15].", "startOffset": 212, "endOffset": 216}, {"referenceID": 4, "context": "Moreover, the above definitions for the gradient and divergence operator over complex networks are straightforward generalizations of the well-known gradient and divergence operator for grid graphs representing 2D-images [6].", "startOffset": 221, "endOffset": 224}, {"referenceID": 3, "context": "By the KKT conditions for constrained convex optimization problems [5], [15],", "startOffset": 67, "endOffset": 70}, {"referenceID": 12, "context": "By the KKT conditions for constrained convex optimization problems [5], [15],", "startOffset": 72, "endOffset": 76}, {"referenceID": 23, "context": "5 implement a finite number K of iterations of the average consensus algorithm, using MetropolisHastings weights [26] , for (approximately) computing the sums (1/N) \u2211 j\u2208V bj = (1/N) \u2211 j\u2208S(yj \u2212 qj) and (1/N) \u2211 j\u2208V b\u0303j = (1/N) \u2211 j\u2208S(yj \u2212 q\u0303j), respectively.", "startOffset": 113, "endOffset": 117}, {"referenceID": 2, "context": "The choice for the number K of avarage consensus iterations can be guided by a wide range of results characterzing the convergence rate of average conensus [4], [10], [26].", "startOffset": 156, "endOffset": 159}, {"referenceID": 7, "context": "The choice for the number K of avarage consensus iterations can be guided by a wide range of results characterzing the convergence rate of average conensus [4], [10], [26].", "startOffset": 161, "endOffset": 165}, {"referenceID": 23, "context": "The choice for the number K of avarage consensus iterations can be guided by a wide range of results characterzing the convergence rate of average conensus [4], [10], [26].", "startOffset": 167, "endOffset": 171}, {"referenceID": 22, "context": "5 using the big data framework AKKA [25], which is a toolkit for building distributed and resilient message-driven applications.", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "We then compared the runtime of the AKKA implementation to the centralized implementation in MATLAB used in [13].", "startOffset": 108, "endOffset": 112}], "year": 2016, "abstractText": "We propose a scalable method for semi-supervised (transductive) learning from massive network-structured datasets. Our approach to semi-supervised learning is based on representing the underlying hypothesis as a graph signal with small total variation. Requiring a small total variation of the graph signal representing the underlying hypothesis corresponds to the central smoothness assumption that forms the basis for semi-supervised learning, i.e., input points forming clusters have similar output values or labels. We formulate the learning problem as a nonsmooth convex optimization problem which we solve by appealing to Nesterov\u2019s optimal first-order method for nonsmooth optimization. We also provide a message passing formulation of the learning method which allows for a highly scalable implementation in big data frameworks.", "creator": "LaTeX with hyperref package"}}}