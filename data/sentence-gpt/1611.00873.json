{"id": "1611.00873", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2016", "title": "Extracting Actionability from Machine Learning Models by Sub-optimal Deterministic Planning", "abstract": "A main focus of machine learning research has been improving the generalization accuracy and efficiency of prediction models. Many models such as SVM, random forest, and deep neural nets have been proposed and achieved great success. However, what emerges as missing in many applications is actionability, i.e., a real-time, machine learning solution. Moreover, in a more theoretical sense, it is often thought that the results can be manipulated by real-time models to improve predictive accuracy. But if there is a real-time, real-time approach to how the predictions of the observed model are made, the model must be based on real-time modeling models to improve prediction accuracy. For example, a model based on a real-time prediction is a real-time model based on a real-time prediction, based on a real-time prediction. To achieve this, the model must be based on the real-time data, which must be used to model the prediction of the predicted model. To achieve this, the model must have the data and the predictions of the predicted model. In the current approach, the model is based on real-time model models. However, even if the model has not yet been validated, the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of", "histories": [["v1", "Thu, 3 Nov 2016 03:53:41 GMT  (287kb)", "http://arxiv.org/abs/1611.00873v1", "16 pages, 4 figures"]], "COMMENTS": "16 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["qiang lyu", "yixin chen", "zhaorong li", "zhicheng cui", "ling chen", "xing zhang", "haihua shen"], "accepted": false, "id": "1611.00873"}, "pdf": {"name": "1611.00873.pdf", "metadata": {"source": "CRF", "title": "Extracting Actionability from Machine Learning Models by Sub-optimal Deterministic Planning", "authors": ["Qiang Lyu", "Yixin Chen", "Zhaorong Li", "Zhicheng Cui", "Ling Chen", "Xing Zhang", "Haihua Shen"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n00 87\n3v 1\n[ cs\n.A I]\n3 N\nov 2\n01 6\nIn this paper, we propose a novel approach that achieves actionability by combining learning with planning, two core areas of AI. In particular, we propose a framework to extract actionable knowledge from random forest, one of the most widely used and best off-the-shelf classifiers. We formulate the actionability problem to a sub-optimal action planning (SOAP) problem, which is to find a plan to alter certain features of a given input so that the random forest would yield a desirable output, while minimizing the total costs of actions. Technically, the SOAP problem is formulated in the SAS+ planning formalism, and solved using a Max-SAT based approach. Our experimental results demonstrate the effectiveness and efficiency of the proposed approach on a personal credit dataset and other benchmarks. Our work represents a new application of automated planning on an emerging and challenging machine learning paradigm. Keywords: actionable knowledge extraction, machine learning, planning, random forest, weighted partial Max-SAT"}, {"heading": "1. Introduction", "text": "Research on machine learning has achieved great success on enhancing the models\u2019 accuracy and efficiency. Successful models such as support vector machines (SVMs), random forests, and deep neural nets have been applied to vast industrial applications Mitchell (1999). However, in many applications, users may need not only a prediction model, but also suggestions on courses of actions to achieve desirable goals. For practitioners, a complex model such as a random forest is often not very useful even if its accuracy is high because of its lack of actionability. Given a learning model, extraction of actionable knowledge entails finding a set of actions to change the input features of a given instance so that it achieves a desired output from the learning model. We elaborate this problem using one example. Example 1. In a credit card company, a key task is to decide on promotion strategies to maximize the long-term profit. The customer relationship management (CRM) department collects data about\nc\u00a9 2016 Q. Lyu, Y. Chen, Z. Li, Z. Cui, L. Chen, X. Zhang & H. Shen.\ncustomers, such as customer education, age, card type, the channel of initiating the card, the number and effect of different kinds of promotions, the number and time of phone contacts, etc.\nFor data scientists, they need to build models to predict the profit brought by customers. In a real case, a company builds a random forest involving 35 customer features. The model predicts the profit (with probability) for each customer. In addition, a more important task is to extract actionable knowledge to revert \u201cnegative profit\u201d customers and retain \u201cpositive profit\u201d customers. In general, it is much cheaper to maintain existing \u201cpositive profit\u201dcustomers than to revert \u201cnegative profit\u201d ones. It is especially valuable to retain high profit, large, enterprise-level customers.\nThere are certain actions that the company can take, such as making phone contacts and sending promotional coupons. Each action can change the value of one or multiple attributes of a customer. Obviously, such actions incur costs for the company. For instance, there are 7 different kinds of promotions and each promotion associates with two features, the number and the accumulation effect of sending this kind of promotion. When performing an action of \u201csending promotion amt N\u201d, it will change features \u201cnbr promotion amt N\u201d and \u201cs amt N\u201d, the number and the accumulation effect of sending the sales promotion, respectively. For a customer with \u201cnegative profit\u201d, the goal is to extract a sequence of actions that change the customer profile so that the model gives a \u201cpositive profit\u201d prediction while minimizing the total action costs. For a customer with \u201cpositive profit\u201d, the goal is to find actions so that the customer has a \u201cpositive profit\u201d prediction with a higher prediction probability.\nResearch on extracting actionability from machine learning models is still limited. There are a few existing works. Statisticians have adopted stochastic models to find specific rules of the response behavior of customer DeSarbo and Ramaswamy (1994); Levin and Zahavi (1996). There have also been efforts on the development of ranking mechanisms with business interests Hilderman and Hamilton (2000); Cao et al. (2007a) and pruning and summarizing learnt rules by considering similarity Liu and Hsu (1996); Liu et al. (1999); Cao et al. (2007b, 2010). However, such approaches are not suitable for the problems studied in this paper due to two major drawbacks. First, they can not provide customized actionable knowledge for each individual since the rules or rankings are derived from the entire population of training data. Second, they did not consider the action costs while building the rules or rankings. For example, a low income housewife may be more sensitive to sales promotion driven by consumption target, while a social housewife may be more interested in promotions related to social networks. Thus, these rule-based and ranking algorithms cannot tackle these problems very well since they are not personalized for each customer.\nAnother related work is extracting actionable knowledge from decision tree and additive tree models by bounded tree search and integer linear programming Yang et al. (2003, 2007); Cui et al. (2015). Yang\u2019s work focuses on finding optimal strategies by using a greedy strategy to search on one or multiple decision trees Yang et al. (2003, 2007). Cui et al. use an integer linear programming (ILP) method to find actions changing sample membership on an ensemble of trees Cui et al. (2015). A limitation of these works is that the actions are assumed to change only one attribute each time. As we discussed above, actions like \u201csending promotion amt N\u201d may change multiple features, such as \u201cnbr promotion amt N\u201d and \u201cs amt N\u201d. Moreover, Yang\u2019s greedy method is fast but cannot give optimal solution Yang et al. (2003), and Cui\u2019s optimization method is optimal but very slow Cui et al. (2015).\nIn order to address these challenges, we propose a novel approach to extract actionable knowledge from random forests, one of the most popular learning models. Our approach leverages planning, one of the core and extensively researched areas of AI. We first rigorously formulate the\nknowledge extracting problem to a sub-optimal actionable planning (SOAP) problem which is defined as finding a sequence of actions transferring a given input to a desirable goal while minimizing the total action costs. Then, our approach consists of two phases. In the offline preprocessing phase, we use an anytime state-space search on an action graph to find a preferred goal for each instance in the training dataset and store the results in a database. In the online phase, for any given input, we translate the SOAP problem into a SAS+ planning problem. The SAS+ planning problem is solved by an efficient MaxSAT-based approach capable of optimizing plan metrics.\nWe perform empirical studies to evaluate our approach. We use a real-world credit card company dataset obtained through an industrial research collaboration. We also evaluate some other standard benchmark datasets. We compare the quality and efficiency of our method to several other stateof-the-art methods. The experimental results show that our method achieves a near-optimal quality and real-time online search as compared to other existing methods."}, {"heading": "2. Preliminaries", "text": ""}, {"heading": "2.1. Random forest", "text": "Random forest is a popular model for classification, one of the main tasks of learning. The reasons why we choose Random forest are: 1) In addition to superior classification/regression performance, Random forest enjoys many appealing properties many other models lack Friedman et al. (2001), including the support for multi-class classification and natural handling of missing values and data of mixed types. 2) Often referred to as one of the best off-the-shelf classifier Friedman et al. (2001), Random forest has been widely deployed in many industrial products such as Kinect Shotton et al. (2013) and face detection in camera Viola and Jones (2004), and is the popular method for some competitions such as web search ranking Mohan et al. (2011).\nConsider a dataset {X,Y }, where X = {x1, \u00b7 \u00b7 \u00b7 , xN} is the set of training samples and Y = {y1, \u00b7 \u00b7 \u00b7 , yN} is the set of classification labels. Each vector xi = (xi1, \u00b7 \u00b7 \u00b7 , x i M ) consists of M attributes, where each attribute xj can be either categorical or numerical and has a finite or infinite domain Dom(xj). Note that we use x = (x1, \u00b7 \u00b7 \u00b7 , xM ) to represent xi = (xi1, \u00b7 \u00b7 \u00b7 , x i M ) when there is no confusion. All labels yi have the same finite categorical domain Dom(Y ). A random forest contains D decision trees where each decision tree d takes an input x and outputs a label y \u2208 Dom(Y ), denoted as od(x) = y. For any label c \u2208 Dom(Y ), the probability of output c is\np ( y = c | x ) =\n\u2211D d=1 wdI(od(x) = c)\n\u2211D d=1 wd\n, (1)\nwhere wd \u2208 R are weights of decision trees, I(od(x) = c) is an indicator function which evaluates to 1 if od(x) = c and 0 otherwise. The overall output predicted label is\nH(x) = argmax c\u2208Dom(Y ) p(y = c|x). (2)\nA random forest is generated as follows Breiman (2001). For d = 1, \u00b7 \u00b7 \u00b7 ,D,\n1. Sample nk (0 < nk < N ) instances from the dataset with replacement.\n2. Train an un-pruned decision tree on the nk sampled instances. At each node, choose the split point from a number of randomly selected features rather than all features."}, {"heading": "2.2. SAS+ formalism", "text": "In classical planning, there are two popular formalisms, STRIPS and PDDL Fox and Long (2003). In recent years, another indirect formalism, SAS+, has attracted increasing uses due to its many favorable features, such as compact encoding with multi-valued variables, natural support for invariants, associated domain transition graphs (DTGs) and causal graphs (CGs) which capture vital structural information Ba\u0308ckstro\u0308m and Nebel (1995); Jonsson and Ba\u0308ckstro\u0308m (1998); Helmert (2006).\nIn SAS+ formalism, a planning problem is defined over a set of multi-valued state variables X = {x1, \u00b7 \u00b7 \u00b7 , x|X |}. Each variable x \u2208 X has a finite domain Dom(x). A state s is a full assignment of all the variables. If a variable x is assigned to f \u2208 Dom(x) at a state s, we denote it as s(x) = f . We use S to represent the set of all states.\nDefinition 1 (Transition) Given a multi-valued state variable x \u2208 X with a domain Dom(x), a transition is defined as a tuple T = (x, f, g), where f, g \u2208 Dom(x), written as \u03b4xf\u2192g. A transition \u03b4xf\u2192g is applicable to a state s if and only if s(x) = f . We use \u2295 to represent applying a transition to a state. Let s\u2032 = s\u2295 \u03b4xf\u2192g be the state after applying the transition to s, we have s\n\u2032(x) = g. We also simplify the notation \u03b4xf\u2192g as \u03b4 x or \u03b4 when there is no confusion.\nA transition \u03b4xf\u2192g is a regular transition if f 6= g or a prevailing transition if f = g. In addition, \u03b4x\u2217\u2192g denotes a mechanical transition, which can be applied to any state s and changes the value of x to g.\nFor a variable x, we denote the set of all transitions that affect x as T (x), i.e., T (x) = {\u03b4xf\u2192g}\u222a {\u03b4x\u2217\u2192g} for all f, g \u2208 Dom(x). We also denote the set of all transitions as T , i.e., T = \u22c3 x\u2208X T (x).\nDefinition 2 (Transition mutex) For two different transitions \u03b4xf\u2192g and \u03b4 x f \u2032\u2192g\u2032 , if at least one of them is a mechanical transition and g = g\u2032, they are compatible; otherwise, they are mutually exclusive (mutex).\nDefinition 3 (Action) An action a is a set of transitions {\u03b41, \u00b7 \u00b7 \u00b7 , \u03b4|a|}, where there do not exist two transitions \u03b4i, \u03b4j \u2208 a that are mutually exclusive. An action a is applicable to a state s if and only if all transitions in a are applicable to s. Each action has a cost \u03c0(a) > 0.\nDefinition 4 (SAS+ planning) A SAS+ planning problem is a tuple \u03a0sas = (X ,O, sI , SG) defined as follows\n\u2022 X = {x1, \u00b7 \u00b7 \u00b7 , x|X |} is a set of state variables.\n\u2022 O is a set of actions.\n\u2022 sI \u2208 S is the initial state.\n\u2022 SG is a set of goal conditions, where each goal condition sG \u2208 SG is a partial assignment of some state variables. A state s is a goal state if there exists sG \u2208 SG such that s agrees with every variable assignment in sG.\nNote that we made a slight generalization of original SAS+ planning, in which SG includes only one goal condition. For a state s with an applicable action a, we use s\u2032 = s \u2295 a to denote the resulting state after applying all the transitions in a to s (in an arbitrary order since they are mutex free).\nDefinition 5 (Action mutex) Two different actions a1 and a2 are mutually exclusive if and only if at least one of the following conditions is satisfied:\n\u2022 There exists a non-prevailing transition \u03b4 such that \u03b4 \u2208 a1 and \u03b4 \u2208 a2.\n\u2022 There exist two transitions \u03b41 \u2208 a1 and \u03b42 \u2208 a2 such that \u03b41 and \u03b42 are mutually exclusive.\nA set of actions P is applicable to s if each action a \u2208 P is applicable to s and no two actions in P are mutex. We denote the resulting state after applying a set of actions P to s as s\u2032 = s\u2295 P .\nDefinition 6 (Solution plan) For a SAS+ problem \u03a0sas = (X ,O, sI , SG), a solution plan is a sequence P = (P1, \u00b7 \u00b7 \u00b7 , PL), where each Pt, t = 1, \u00b7 \u00b7 \u00b7 , L is a set of actions, and there exists sG \u2208 SG, sG \u2286 sI \u2295 P1 \u2295 P2 \u2295 \u00b7 \u00b7 \u00b7PL.\nNote that in a solution plan, multiple non-mutex actions can be applied at the same time step. s \u2295 Pt means applying all actions in Pt in any order to state s. In this work, we want to find a solution plan that minimizes a quality metric, the total action cost \u2211\nPt\u2208P\n\u2211\na\u2208Pt \u03c0(a)."}, {"heading": "3. Sub-Optimal Actionable Plan (SOAP) Problem", "text": "We first give an intuitive description of the SOAP problem. Given a random forest and an input x, the SOAP problem is to find a sequence of actions that, when applied to x, changes it to a new instance which has a desirable output label from the random forest. Since each action incurs a cost, it also needs to minimize the total action costs. In general, the actions and their costs are determined by domain experts. For example, analysts in a credit card company can decide which actions they can perform and how much each action costs.\nThere are two kinds of features, soft attributes which can be changed with reasonable costs and hard attributes which cannot be changed with a reasonable cost, such as gender Yang et al. (2003). We only consider actions that change soft attributes.\nDefinition 7 (SOAP problem) A SOAP problem is a tuple \u03a0soap = (H, xI , c, O), where H is a random forest, xI is a given input, c \u2208 Dom(Y ) is a class label, and O is a set of actions. The goal is to find a sequence of actions A = (a1, \u00b7 \u00b7 \u00b7 , an), ai \u2208 O, to solve:\nmin A\u2286O\nF (A) = \u2211\nai\u2208A\n\u03c0(ai), (3)\nsubject to: p(y = c|x\u0303) \u2265 z, (4)\nwhere \u03c0(a) > 0 is the cost of action a, 0 < z \u2264 1 is a constant, p(y = c|x\u0303) is the output of H as defined in (1), and x\u0303 = xI \u2295 a1 \u2295 a2 \u2295 \u00b7 \u00b7 \u00b7 \u2295 an is the new instance after applying the actions in A to xI .\nExample 2. A random forest H with two trees and three features is shown in Figure 1. x1 is a hard attribute, x2 and x3 are soft attributes. Given H and an input x = (male, 2, 500), the output from H is 0. The goal is to change x to a new instance that has an output of 1 from H . For example, two actions changing x2 from 2 to 5 and x3 from 500 to 1500 is a plan and the new instance is (male, 5, 1500)."}, {"heading": "4. A Planning Approach to SOAP", "text": "The SOAP problem is proven to be an NP-hard problem, even when an action can change only one feature Cui et al. (2015). Therefore, we cannot expect any efficient algorithm for optimally solving it. We propose a planning-based approach to solve the SOAP problem. Our approach consists of an offline preprocessing phase that only needs to be run once for a given random forest, and an online phase that is used to solve each SOAP problem instance."}, {"heading": "4.1. Action graph and preferred goals", "text": "Since there are typically prohibitively high number of possible instances in the feature space, it is too expensive and unnecessary to explore the entire space. We reason that the training dataset for building the random forest gives a representative distribution of the instances. Therefore, in the offline preprocessing, we form an action graph and identify a preferred goal state for each training sample.\nDefinition 8 (Feature partitions) Given a random forest H , we split the domain of each feature xi (i = 1, \u00b7 \u00b7 \u00b7 ,M ) into a number of partitions according to the following rules.\n1. xi is split into n partitions if xi is categorical and has n categories.\n2. xi is split into n+1 partitions if xi is numerical and has n branching nodes in all the decision trees in H . Suppose the branching nodes are (b1, \u00b7 \u00b7 \u00b7 , bn), the partitions are {(\u2212\u221e, b1), [b1, b2), \u00b7 \u00b7 \u00b7 , [bn,+\u221e)}.\nIn Example 2, x1 is splited into {male, female}, x2 and x3 are splited into {(\u2212\u221e, 5), [5,+\u221e)} and {(\u2212\u221e, 1000), [1000, 1500), [1500,+\u221e)}, respectively.\nDefinition 9 (State transformation) For a given instance x = (x1, \u00b7 \u00b7 \u00b7 , xM ), let ni be the number of partitions and pi the partition index for feature xi, we transform it to a SAS+ state s(x) = (z1, \u00b7 \u00b7 \u00b7 , zM ), where |Dom(zi)| = ni and s(zi) = pi, i = 1, \u00b7 \u00b7 \u00b7 ,M .\nFor simplicity, we use s to represent s(x) when there is no confusion. Note that if two instances x1 and x2 transform to the same state s, then they have the same output from the random forest since they fall within the same partition for every feature. In that case, we can use p(y = c|s) in place of p(y = c|x1) and p(y = c|x2).\nAlgorithm 1 Heuristic search (Input: G = (F , E), sI ) 1: Nes \u2190 0, s\u2217 \u2190 NULL, g\u2217 \u2190 \u221e 2: MinHeap.push(sI ), ClosedList \u2190 {} 3: while MinHeap is not empty do 4: s \u2190 MinHeap.pop() 5: if p(y = c|s) \u2265 z and g(s) < g\u2217 then 6: Nes \u2190 |ClosedList|, s\u2217 \u2190 s, g\u2217 \u2190 g(s) 7: end if 8: if |ClosedList| \u2212Nes > \u2206 then return s\u2217\n9: if x /\u2208 ClosedList and p(y = c|s) < z then 10: ClosedList=ClosedList \u222a{s} 11: for each (s, s\u2032) \u2208 E do 12: MinHeap.push(s\u2032) 13: end for 14: end if 15: end while 16: return s\u2217\nGiven the states, we can define SAS+ transitions and actions according to Definitions 1 and 3. For Example 2, x = (x1, x2, x3) can be transformed to state s = (x1, x2, x3), Dom(x1) = {0, 1},Dom(x2) = {0, 1},Dom(x3) = {0, 1, 2}. For an input x = (male, 2, 500), the corresponding state is s = (0, 0, 0). The action a changing x2 from 2 to 5 can be represented as \u03b4 x2 0\u21921. Thus, the resulting state of applying a is s\u2295 a = (0, 1, 0).\nDefinition 10 (Action graph) Given a SOAP problem \u03a0soap = (H, xI , c, O), the action graph is a graph G = (F , E) where F is the set of transformed states and an edge (si\u22121, si)\u2208E if and only if there is an action a \u2208 O such that si\u22121 \u2295 a = si. The weight for this edge is w(si\u22121, si) = \u03c0(a).\nThe SOAP problem in Definition 7 is equivalent to finding the shortest path on the state space graph G = (F , E) from a given state sI to a goal state. A node s is a goal state if p(y = c|s) \u2265 z. Given the training data {X,Y }, we use a heuristic search to find a preferred goal state for each x \u2208 X that p(y = c|x) < z. For each of such x, we find a path in the action graph from s(x) to a state s\u2217 such that p(y = c|s\u2217) \u2265 z while minimizing the cost of the path.\nAlgorithm 1 shows the heuristic search. The search uses a standard evaluation function f(s) = g(s)+h(s). g(s) is the cost of the path leading up to s. Let the path be s0 = sI , s1, \u00b7 \u00b7 \u00b7 , sm = s, and si\u22121 \u2295 ai = si for i = 1, \u00b7 \u00b7 \u00b7 ,m, we have g(s) = \u2211m i=1 \u03c0(ai). We define the heuristic function as h(s) = \u03b1(z \u2212 p(y = c|s)) if p(y = c|s) < z, otherwise h(s) = 0 . For any state s = sI \u2295 a1 \u2295 a2 \u2295 \u00b7 \u00b7 \u00b7 \u2295 am satisfying p(y = c|s) < z, f(s) = g(s) + h(s) = \u2211m i=1 \u03c0(ai) + \u03b1(z \u2212 p(y = c|s)). Since the goal is to achieve p(y = c|s) \u2265 z, h(s) measures how far s is from the goal. \u03b1 is a controlling parameter. In our experiments, \u03b1 is set to the mean of all the action costs.\nAlgorithm 1 maintains two data structures, a min heap and a closed list, and performs the following main steps:\n1. Initialize Nes, s\u2217, and g\u2217 where Nes represent the number of expanded statess\u2217 is the best goal state ever found, and g\u2217 records the cost of the path leading up to s\u2217. Add the initial state sI to the min heap (Lines 1-2).\n2. Pop the state s from the heap with the smallest f(s) (Line 4).\n3. If p(y = c|s) \u2265 z and g(s) < g\u2217, update g\u2217, Nes, and the best goal state s\u2217 (Lines 5-6). 4. If the termination condition (|ClosedList| \u2212Nes > \u2206) is met, stop the search and return s\u2217\n(Line 8).\n5. Add s to the closed list and for each edge (s, s\u2032)\u2208E, add s\u2032 to the min heap if s is not in the closed list and not a goal state (Lines 10-12).\n6. Repeat from Step 2.\nThe closed list is implemented as a set with highly efficient hashing-based duplicate detection. The search terminates when the search has not found a better plan for a long time (|ClosedList| \u2212 Nes > \u2206). We set a large value (\u2206 = 107) in our experiments. Note that Algorithm 1 does not have to search all states since it will stop the search once a state s satisfies the termination condition (Line 8).\nBy the end of the offline phase, for each x \u2208 X and the corresponding state s(x), we find a preferred goal state s\u2217(x). For an input x = (male, 2, 500) in Example 2, the corresponding initial state is s = (0, 0, 0). An optimal solution is P = (a1, a2, a3) where a1 = \u03b4 x2 0\u21921, a2 = \u03b4 x3 0\u21921, a3 = \u03b4 x3 1\u21922, and the preferred goal state is s = (0, 1, 2)."}, {"heading": "4.2. Online SAS+ planning", "text": "Once the offline phase is done, the results can be used to repeatedly solve SOAP instances. We now describe how to handle a new instance xI and find the actionable plan.\nIn online SAS+ planning, we will find a number of closest states of s(xI) and use the combination of their goals to construct the goal s\u2217(xI). This is inspired by the idea of similarity-based learning methods such as k-nearest-neighbor (kNN). We first define the similarity between two states.\nDefinition 11 (Feature similarity) Given two states s(x1, \u00b7 \u00b7 \u00b7 , xM ) and s\u2032(x\u20321, \u00b7 \u00b7 \u00b7 , x \u2032 M ), the similarity of the i-th feature variable is defined as:\n\u2022 if the i-th feature is categorical, \u03bei(s, s\u2032) = 1 if xi = x\u2032i, otherwise \u03bei(s, s \u2032) = 0.\n\u2022 if the i-th feature is numerical, \u03bei(s, s\u2032) = 1\u2212 |pi\u2212p\n\u2032\ni| ni\u22121 , where pi and p\u2032i are the partition index of features xi and x\u2032i, and ni is the number of partitions of the i-th feature.\nNote that \u03bei(s, s\u2032) \u2208 [0, 1]. \u03bei(s, s\u2032) = 1 means they are in the same partition, while \u03bei(s, s\u2032) = 0 means they are totally different.\nDefinition 12 (State similarity) The similarity between two states s(x1, \u00b7 \u00b7 \u00b7 , xM ) and s\u2032(x\u20321, \u00b7 \u00b7 \u00b7 , x \u2032 M ) is 0 if there exists i \u2208 [1,M ], xi is a hard attribute and xi and x\u2032i are not in the same partition. Otherwise, the similarity is\nsim(s, s\u2032) =\n\u2211M i=1 \u03c6i\u03bei(s, s \u2032) \u2211M\ni=1 \u03c6i , (5)\nwhere \u03c6i is the feature weight in the random forest.\nNote that sim(s, s\u2032) \u2208 [0, 1]. A larger sim(s, s\u2032) means higher similarity. Given two vectors x = (male, 2, 500) and x\u2032 = (male, 6, 800) in Example 2, the corresponding states are s = (0, 0, 0) and s\u2032 = (0, 1, 0). Their feature similarities are \u03be0(s, s\u2032) = 1, \u03be1(s, s\u2032) = 0, and \u03be2(s, s\u2032) = 1. Suppose \u03c6i = 1/3, then sim(s, s\u2032) = 2/3.\nGiven two vectors x = (male, 2, 500) and x\u2032 = (female, 2, 500), the corresponding states are s = (0, 0, 0) and s\u2032 = (1, 0, 0). Since x1 is a hard attribute and x1, x\u20321 are not in the same parition, sim(s, s\u2032) = 0. SAS+ formulation. Given a SOAP problem \u03a0soap = (H, xI , c, O), we define a SAS+ problem \u03a0sas = (X ,O, sI , SG) as follows:\n\u2022 X = {x1, \u00b7 \u00b7 \u00b7 , xM} is a set of state variables. Each variable xi has a finite domain Dom(xi) = ni where ni is the number of partitions of the i-th feature of x.\n\u2022 O is a set of SAS+ actions directly mapped from O in \u03a0soap.\n\u2022 sI is transformed from xI according to Definition 9.\n\u2022 Let (s1, \u00b7 \u00b7 \u00b7 , sK) be the K nearest neighbors of sI ranked by sim(s, sj), and their corresponding preferred goal states be (s\u22171, \u00b7 \u00b7 \u00b7 , s \u2217 K), the goal in SAS+ is SG = {s \u2217 1, \u00b7 \u00b7 \u00b7 , s \u2217 K}.\nK > 0 is a user-defined integer.\nIn example 2, if we preprocessed three initial states s1 = (0, 0, 0), s2 = (0, 1, 0), s3 = (0, 1, 1), then three preferred goal states s\u22171 = (0, 1, 2), s \u2217 2 = (0, 1, 2), and s \u2217 3 = (0, 1, 2) will be found in the offline phase. In the online phase, given a new input xI = (male, 2, 1200), the corresponding state is sI = (0, 0, 1). Suppose \u03c6i = 1/3, then sim(sI , s1) = 5/6, sim(sI , s2) = 1/2, and sim(sI , s3) = 2/3. If K = 2, the 2 nearest neighbors of sI are s1 and s3, and the goal of the SAS+ problem is SG = {s\u22171, s \u2217 3}.\nIn the online phase, for a given xI , we solve a SAS+ instance defined above. In addition to classical SAS+ planning, we also want to minimize the total action costs. Since some existing classical planners do not perform well in optimizing the plan quality, we employ a SAT-based method.\nOur method follows the bounded SAT solving strategy, originally proposed in SATPlan Kautz and Selman (1992) and Graphplan Blum and Furst (1997). It starts from a lower bound of makespan (L=1), encodes the SAS+ problem as a weighted partial Max-SAT (WPMax-SAT) instance Lu et al. (2014), and either proves it unsatisfiable or finds a plan while trying to minimize total action costs at the same time.\nFor a SAS+ problem \u03a0sas = (X ,O, sI , SG), given a makespan L, we define a WPMax-SAT problem \u03a8 with the following variable set U and clause set C . The variable set includes three types of variables:\n\u2022 Transition variables: U\u03b4,t, \u2200\u03b4 \u2208 T and t \u2208 [1, L].\n\u2022 Action variables: Ua,t, \u2200a \u2208 O and t \u2208 [1, L].\n\u2022 Goal variables: Us\u2217 , \u2200s\u2217 \u2208 SG.\nEach variable in U represents the assignment of a transition or an action at time t, or a goal condition s\u2217.\nThe clause set C has two types of clauses: soft clauses and hard clauses. The soft clause set Cs is constructed as: Cs = {\u00acUa,t|\u2200a \u2208 O and t \u2208 [1, L]}. For each clause c = \u00acUa,t \u2208 Cs, its weight is defined as w(c) = \u03c0(a). For each clause in the hard clause set Ch, its weight is \u2211\nc\u2208Cs w(c) so that it must be true. Ch has the following hard clauses:\n\u2022 Initial state: \u2200x, sI(x) = f , \u2228\n\u2200\u03b4x f\u2192g \u2208T (x) U\u03b4xf\u2192g,1.\n\u2022 Goal state: \u2228 \u2200s\u2217\u2208SG Us\u2217 . It means at leat one goal condition s\u2217 must be true.\n\u2022 Goal condition: \u2200s\u2217 \u2208 SG, \u2200x, s\u2217(x) = g, Us\u2217 \u2192 \u2228\n\u2200\u03b4x f\u2192g \u2208T (x) U\u03b4xf\u2192g,L. If Us\u2217 is true, then\nfor each assignment s\u2217(x) = g, at least one transition changing variable x to value g must be true at time L.\n\u2022 Progression: \u2200\u03b4xf\u2192g \u2208 T (x) and t \u2208 [1, L\u2212 1], U\u03b4xf\u2192g,t \u2192 \u2228 \u2200\u03b4x g\u2192h \u2208T (x) U\u03b4xg\u2192h,t+1.\n\u2022 Regression: \u2200\u03b4xf\u2192g \u2208 T (x) and t \u2208 [2, L], U\u03b4xf\u2192g,t \u2192 \u2228 \u2200\u03b4x h\u2192f \u2208T (x) U\u03b4xh\u2192f ,t+1.\n\u2022 Mutually exclusive transitions: for each mutually exclusive transitions pair (\u03b41, \u03b42), t \u2208 [1, L], U\u03b41,t \u2228 U\u03b42,t.\n\u2022 Mutually exclusive actions: for each mutually exclusive actions pair (a1, a2), t \u2208 [1, L], Ua1,t \u2228 Ua2,t.\n\u2022 Composition of actions: \u2200a \u2208 O and t \u2208 [1, L \u2212 1], Ua,t \u2192 \u2227 \u2200\u03b4\u2208M(a) U\u03b4,t.\n\u2022 Action existence: for each non-prevailing transition \u03b4 \u2208 T , U\u03b4,t \u2192 \u2228 \u2200a,\u03b4\u2208M(a) Ua,t.\nThere are three main differences between our approach and a related work, SASE encoding Huang et al. (2010, 2012). First, our encoding transforms the SAS+ problem to a WPMax-SAT problem aiming at finding a plan with minimal total action costs while SASE transforms it to a SAT problem which only tries to find a satisfiable plan. Second, besides transition and action variables, our encoding has extra goal variables since the goal definition of our SAS+ problem is a combination of several goal states while in SASE it is a partial assignment of some variables. Third, the goal clauses of our encoding contain two kinds of clauses while SASE has only one since the goal definition of ours is more complicated than SASE.\nWe can solve the above encoding using any of the MaxSAT solvers, which are extensively studied. Using soft clauses to optimize the plan in our WPMax-SAT encoding is similar to Balyo\u2019s work Balyo et al. (2014) which uses a MAXSAT based approach for plan optimization (removing redundant actions)."}, {"heading": "5. Experimental Results", "text": "To test the proposed approach (denoted as \u201cPlanning\u201d), in the offline preprocess, \u2206 in Algorithm 1 is set to 107. In the online search, we set neighborhood size K = 3 and use WPM-2014-in 1 to\n1. http://www.maxsat.udl.cat/\nsolve the encoded WPMax-SAT instances. For comparison, we also implement three solvers: 1) An iterative greedy algorithm, denoted as \u201cGreedy\u201d which chooses one action in each iteration that increases p(y = c|s) while minimizes the total action costs. It keeps iterating until there is no more variables to change. 2) A sub-optimal state space method denoted as \u201cNS\u201d Lu et al. (2016). 3) An integer linear programming (ILP) method Cui et al. (2015), one of the state-of-the-art algorithms for solving the SOAP problem. ILP gives exact optimal solutions.\nWe test these algorithms on a real-world credit card company dataset (\u201cCredit\u201d) and other nine benchmark datasets from the UCI repository2 and the LibSVM website3 used in ILP\u2019s original experiments Cui et al. (2015). Information of the datasets is listed in Table 1. N, D, and C are the number of instances, features, and classes, respectively. A random forest is built on the training set using the Random Trees library in OpenCV 2.4.9. GNU C++ 4.8.4 and Python 2.7 run-time systems are used.\nIn the offline preprocess, we generate all possible initial states and use Algorithm 1 to find a preferred goal state for each initial state. For each dataset, we generate problems with the same parameter settings as in ILP experiments. Specifically, we use a weighted Euclidean distance as the action cost function. For action a which changes state s = (x1, \u00b7 \u00b7 \u00b7 , xM ) to s\u2032 = (x\u20321, \u00b7 \u00b7 \u00b7 , x \u2032 M ), the cost is\n\u03c0(a) =\nM \u2211\nj=1\n\u03b2j(xj \u2212 x \u2032 j) 2, (6)\nwhere \u03b2j is the cost weight on variable j, randomly generated in [1, 100]. Since the offline preprocess works are totally independent, we can parallelly solve them in a large number of workstation nodes. We run the offline preprocess parallelly on a workstation with 125 computational nodes. Each node has a 2.50GHz processor with 8 cores and 64GB memory. For each instance, the time limit is set to 1800 seconds. If the preprocess search does not finish in 1800 seconds, we record the best solution found in terms of net profit and the total search time (1800 seconds).\nWe show the average preprocessing time (T) on each dataset in seconds and the total number of possible initial states (#S) in Table 1. \u2211\nT shows how many days it costs to finish all preprocess works by parallelly solving in 1000 cores. We can see that even though the total number of prepro-\n2. https://archive.ics.uci.edu/ml/datasets.html 3. http://www.csie.ntu.edu.tw/\u223ccjlin/libsvmtools/datasets/\ncessed states are very large, the total preprocess time can be extensively reduced to an acceptable range by parallelly solving.\nIn the offline preprocess, the percentage of actual preprocessed states out of all possible initial states in the transformed state space is a key feature of determing the online search quality. For each preprocessing percentage r \u2208 (0, 100] , we randomly sample r \u2217 #S instances from all possible initial states and use Algorithm 1 to find preferred goals. Then, in the online search, we randomly sample 100 instances from the test set and generate 100 problems based on these preferred goals. We report the online search time in seconds and total action costs of the solutions, averaged over 100 runs. From Figure 2(a), we can see that the total offline preprocessing time linearly increases with the percentage. The average total action costs almost linearly decrease with the percentage. Actually, considering the almost unlimited offline preprocessing time, we can always increase the preprocessing percentage and eventually reach 100%.\nTable 2 shows a comprehensive comparison in terms of the average search time, the solution quality measured by the total action costs, the action number of solutions, and the memory usage under the preprocessing percentage 100%. We report the search time (T) in seconds, total action costs of the solutions (Cost), action number of solutions (L), and the memory usage (GB), averaged over 100 runs.\nFrom Table 2, we can see that even though our method spends quite a lot of time in the offline processing, its online search is very fast. Since our method finds near optimal plans for all training samples, its solution quality is much better than Greedy while spending almost the same search time. Comparing against NP, our method is much faster in online search and maintains better solution qualities in a1a and ionosphere scale and equal solution qualities in other 8 datasets. Comparing against ILP, our method is much faster in online search with the cost of losing optimality. Typically a trained random forest model will be used for long time. Since our offline preprocessing only needs to be run once, its cost is well amortized over large number of repeated uses of the online search. In short, our planning approach gives a good quality-efficiency tradeoff: it achieves a near-optimal quality using search time close to greedy search. Note that since we need to store all preprocessed states and their preferred goal states in the online phase, the memory usage of our method is much larger than greedy and NS approaches."}, {"heading": "6. Conclusions", "text": "We have studied the problem of extracting actionable knowledge from random forest, one of the most widely used and best off-the-shelf classifiers. We have formulated the sub-optimal actionable plan (SOAP) problem, which aims to find an action sequence that can change an input instance\u2019s prediction label to a desired one with the minimum total action costs. We have then proposed a SAS+ planning approach to solve the SOAP problem. In an offline phase, we construct an action graph and identify a preferred goal for each input instance in the training dataset. In the online planning phase, for each given input, we formulate the SOAP problem as a SAS+ planning instance based on a nearest neighborhood search on the preferred goals, encode the SAS+ problem to a WPMax-SAT instance, and solve it by calling a WPMax-SAT solver.\nOur approach is heuristic and suboptimal, but we have leveraged SAS+ planning and carefully engineered the system so that it gives good performance. Empirical results on a credit card company dateset and other nine benchmarks have shown that our algorithm achieves a near-optimal solution quality and is ultra-efficient, representing a much better quality-efficiency tradeoff than some other methods.\nWith the great advancements in data science, an ultimate goal of extracting patterns from data is to facilitate decision making. We envision that machine learning models will be part of larger AI systems that make rational decisions. The support for actionability by these models will be crucial. Our work represents a novel and deep integration of machine learning and planning, two core areas of AI. We believe that such integration will have broad impacts in the future.\nNote that the proposed action extraction algorithm can be easily expanded to other additive tree models (ATMs) Lu et al. (2016), such as adaboost Freund and Schapire (1997), gradient boosting trees Friedman. Thus, the proposed action extraction algorithm has very wide applications.\nIn our SOAP formulation, we only consider actions having deterministic effects. However, in many realistic applications, we may have to tackle some nondeterministic actions. For instance, push a promotional coupon may only have a certain probability to increase the accumulation effect since people do not always accept the coupon. We will consider to add nondeterministic actions to our model in the near future."}, {"heading": "Acknowledgments", "text": "This work has been supported in part by National Natural Science Foundation of China (Nos. 61502412, 61033009, and 61175057), Natural Science Foundation of the Jiangsu Province (No. BK20150459), Natural Science Foundation of the Jiangsu Higher Education Institutions (No. 15KJB520036), National Science Foundation, United States (IIS-0534699, IIS-0713109, CNS-1017701), and a Microsoft Research New Faculty Fellowship."}], "references": [{"title": "Complexity results for sas+ planning", "author": ["C. B\u00e4ckstr\u00f6m", "B. Nebel"], "venue": "Computational Intelligence,", "citeRegEx": "B\u00e4ckstr\u00f6m and Nebel.,? \\Q1995\\E", "shortCiteRegEx": "B\u00e4ckstr\u00f6m and Nebel.", "year": 1995}, {"title": "On different strategies for eliminating redundant actions from plans", "author": ["Tom\u00e1\u0161 Balyo", "Luk\u00e1\u0161 Chrpa", "Asma Kilani"], "venue": "In Seventh Annual Symposium on Combinatorial Search,", "citeRegEx": "Balyo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Balyo et al\\.", "year": 2014}, {"title": "Fast planning through planning graph analysis", "author": ["A. Blum", "M.L. Furst"], "venue": "Artificial Intelligence,", "citeRegEx": "Blum and Furst.,? \\Q1997\\E", "shortCiteRegEx": "Blum and Furst.", "year": 1997}, {"title": "Knowledge actionability: satisfying technical and business interestingness", "author": ["L. Cao", "D. Luo", "C. Zhang"], "venue": "International Journal of Business Intelligence and Data Mining,", "citeRegEx": "Cao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2007}, {"title": "Domain-driven, actionable knowledge discovery", "author": ["L. Cao", "C. Zhang", "D. Taniar", "E. Dubossarsky", "W. Graco", "Q. Yang", "D. Bell", "M. Vlachos", "B. Taneri", "E. Keogh"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "Cao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2007}, {"title": "Flexible frameworks for actionable knowledge discovery", "author": ["L. Cao", "Y. Zhao", "H. Zhang", "D. Luo", "C. Zhang", "E.K. Park"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Cao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2010}, {"title": "Optimal action extraction for random forests and boosted trees", "author": ["Z. Cui", "W. Chen", "Y. He", "Y. Chen"], "venue": "In Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Cui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2015}, {"title": "Crisp: customer response based iterative segmentation procedures for response modeling in direct marketing", "author": ["W.S. DeSarbo", "V. Ramaswamy"], "venue": "Journal of Direct Marketing,", "citeRegEx": "DeSarbo and Ramaswamy.,? \\Q1994\\E", "shortCiteRegEx": "DeSarbo and Ramaswamy.", "year": 1994}, {"title": "PDDL2.1: An extension to PDDL for expressing temporal planning domains", "author": ["M. Fox", "D. Long"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Fox and Long.,? \\Q2003\\E", "shortCiteRegEx": "Fox and Long.", "year": 2003}, {"title": "A decision-theoretic generalization of online learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "The elements of statistical learning, volume", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": null, "citeRegEx": "Friedman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2001}, {"title": "The fast downward planning system", "author": ["M. Helmert"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Helmert.,? \\Q2006\\E", "shortCiteRegEx": "Helmert.", "year": 2006}, {"title": "Applying objective interestingness measures in data mining systems", "author": ["R.J. Hilderman", "H.J. Hamilton"], "venue": "In Proc. Principles of Data Mining and Knowledge Discovery,", "citeRegEx": "Hilderman and Hamilton.,? \\Q2000\\E", "shortCiteRegEx": "Hilderman and Hamilton.", "year": 2000}, {"title": "A novel transition based encoding scheme for planning as satisfiability", "author": ["R. Huang", "Y. Chen", "W. Zhang"], "venue": "In Proc. AAAI Conference on Artificial Intelligence,", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "SAS+ planning as satisfiability", "author": ["R. Huang", "Y. Chen", "W. Zhang"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "State-variable planning under structural restrictions: Algorithms and complexity", "author": ["P. Jonsson", "C. B\u00e4ckstr\u00f6m"], "venue": "Artificial Intelligence,", "citeRegEx": "Jonsson and B\u00e4ckstr\u00f6m.,? \\Q1998\\E", "shortCiteRegEx": "Jonsson and B\u00e4ckstr\u00f6m.", "year": 1998}, {"title": "Planning as satisfiability", "author": ["H. Kautz", "B. Selman"], "venue": "In Proc. European Conference on Artificial Intelligence,", "citeRegEx": "Kautz and Selman.,? \\Q1992\\E", "shortCiteRegEx": "Kautz and Selman.", "year": 1992}, {"title": "Segmentation analysis with managerial judgment", "author": ["N. Levin", "J. Zahavi"], "venue": "Journal of Direct Marketing,", "citeRegEx": "Levin and Zahavi.,? \\Q1996\\E", "shortCiteRegEx": "Levin and Zahavi.", "year": 1996}, {"title": "Post-analysis of learned rules", "author": ["B. Liu", "W. Hsu"], "venue": "In Proc. AAAI Conference on Artificial Intelligence,", "citeRegEx": "Liu and Hsu.,? \\Q1996\\E", "shortCiteRegEx": "Liu and Hsu.", "year": 1996}, {"title": "Pruning and summarizing the discovered associations", "author": ["B. Liu", "W. Hsu", "Y. Ma"], "venue": "In Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Liu et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Liu et al\\.", "year": 1999}, {"title": "A SAT-based approach to cost-sensitive temporally expressive planning", "author": ["Q. Lu", "R. Huang", "Y. Chen", "Y. Xu", "W. Zhang", "G. Chen"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Lu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2014}, {"title": "Extracting optimal actionable plans from additive tree models", "author": ["Q. Lu", "Z. Cui", "Y. Chen", "X. Chen"], "venue": "Frontiers of Computer Science,", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Machine learning and data mining", "author": ["T.M. Mitchell"], "venue": "Communications of the ACM,", "citeRegEx": "Mitchell.,? \\Q1999\\E", "shortCiteRegEx": "Mitchell.", "year": 1999}, {"title": "Web-search ranking with initialized gradient boosted regression trees", "author": ["A. Mohan", "Z. Chen", "K.Q. Weinberger"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Mohan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mohan et al\\.", "year": 2011}, {"title": "Real-time human pose recognition in parts from single depth images", "author": ["J. Shotton", "T. Sharp", "A. Kipman", "A. Fitzgibbon", "M. Finocchio", "A. Blake", "M. Cook", "R. Moore"], "venue": "Communications of the ACM,", "citeRegEx": "Shotton et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shotton et al\\.", "year": 2013}, {"title": "Robust real-time face detection", "author": ["P. Viola", "M.J. Jones"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Viola and Jones.,? \\Q2004\\E", "shortCiteRegEx": "Viola and Jones.", "year": 2004}, {"title": "Postprocessing decision trees to extract actionable knowledge", "author": ["Q. Yang", "J. Yin", "C. Ling", "T. Chen"], "venue": "In Proc. IEEE International Conference on Data Mining,", "citeRegEx": "Yang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2003}, {"title": "Extracting actionable knowledge from decision trees", "author": ["Q. Yang", "J. Yin", "C. Ling", "R. Pan"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Yang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 22, "context": "Successful models such as support vector machines (SVMs), random forests, and deep neural nets have been applied to vast industrial applications Mitchell (1999). However, in many applications, users may need not only a prediction model, but also suggestions on courses of actions to achieve desirable goals.", "startOffset": 145, "endOffset": 161}, {"referenceID": 3, "context": "Statisticians have adopted stochastic models to find specific rules of the response behavior of customer DeSarbo and Ramaswamy (1994); Levin and Zahavi (1996).", "startOffset": 105, "endOffset": 134}, {"referenceID": 3, "context": "Statisticians have adopted stochastic models to find specific rules of the response behavior of customer DeSarbo and Ramaswamy (1994); Levin and Zahavi (1996). There have also been efforts on the development of ranking mechanisms with business interests Hilderman and Hamilton (2000); Cao et al.", "startOffset": 105, "endOffset": 159}, {"referenceID": 3, "context": "Statisticians have adopted stochastic models to find specific rules of the response behavior of customer DeSarbo and Ramaswamy (1994); Levin and Zahavi (1996). There have also been efforts on the development of ranking mechanisms with business interests Hilderman and Hamilton (2000); Cao et al.", "startOffset": 105, "endOffset": 284}, {"referenceID": 3, "context": "There have also been efforts on the development of ranking mechanisms with business interests Hilderman and Hamilton (2000); Cao et al. (2007a) and pruning and summarizing learnt rules by considering similarity Liu and Hsu (1996); Liu et al.", "startOffset": 125, "endOffset": 144}, {"referenceID": 3, "context": "There have also been efforts on the development of ranking mechanisms with business interests Hilderman and Hamilton (2000); Cao et al. (2007a) and pruning and summarizing learnt rules by considering similarity Liu and Hsu (1996); Liu et al.", "startOffset": 125, "endOffset": 230}, {"referenceID": 3, "context": "There have also been efforts on the development of ranking mechanisms with business interests Hilderman and Hamilton (2000); Cao et al. (2007a) and pruning and summarizing learnt rules by considering similarity Liu and Hsu (1996); Liu et al. (1999); Cao et al.", "startOffset": 125, "endOffset": 249}, {"referenceID": 3, "context": "There have also been efforts on the development of ranking mechanisms with business interests Hilderman and Hamilton (2000); Cao et al. (2007a) and pruning and summarizing learnt rules by considering similarity Liu and Hsu (1996); Liu et al. (1999); Cao et al. (2007b, 2010). However, such approaches are not suitable for the problems studied in this paper due to two major drawbacks. First, they can not provide customized actionable knowledge for each individual since the rules or rankings are derived from the entire population of training data. Second, they did not consider the action costs while building the rules or rankings. For example, a low income housewife may be more sensitive to sales promotion driven by consumption target, while a social housewife may be more interested in promotions related to social networks. Thus, these rule-based and ranking algorithms cannot tackle these problems very well since they are not personalized for each customer. Another related work is extracting actionable knowledge from decision tree and additive tree models by bounded tree search and integer linear programming Yang et al. (2003, 2007); Cui et al. (2015). Yang\u2019s work focuses on finding optimal strategies by using a greedy strategy to search on one or multiple decision trees Yang et al.", "startOffset": 125, "endOffset": 1166}, {"referenceID": 3, "context": "There have also been efforts on the development of ranking mechanisms with business interests Hilderman and Hamilton (2000); Cao et al. (2007a) and pruning and summarizing learnt rules by considering similarity Liu and Hsu (1996); Liu et al. (1999); Cao et al. (2007b, 2010). However, such approaches are not suitable for the problems studied in this paper due to two major drawbacks. First, they can not provide customized actionable knowledge for each individual since the rules or rankings are derived from the entire population of training data. Second, they did not consider the action costs while building the rules or rankings. For example, a low income housewife may be more sensitive to sales promotion driven by consumption target, while a social housewife may be more interested in promotions related to social networks. Thus, these rule-based and ranking algorithms cannot tackle these problems very well since they are not personalized for each customer. Another related work is extracting actionable knowledge from decision tree and additive tree models by bounded tree search and integer linear programming Yang et al. (2003, 2007); Cui et al. (2015). Yang\u2019s work focuses on finding optimal strategies by using a greedy strategy to search on one or multiple decision trees Yang et al. (2003, 2007). Cui et al. use an integer linear programming (ILP) method to find actions changing sample membership on an ensemble of trees Cui et al. (2015). A limitation of these works is that the actions are assumed to change only one attribute each time.", "startOffset": 125, "endOffset": 1457}, {"referenceID": 3, "context": "There have also been efforts on the development of ranking mechanisms with business interests Hilderman and Hamilton (2000); Cao et al. (2007a) and pruning and summarizing learnt rules by considering similarity Liu and Hsu (1996); Liu et al. (1999); Cao et al. (2007b, 2010). However, such approaches are not suitable for the problems studied in this paper due to two major drawbacks. First, they can not provide customized actionable knowledge for each individual since the rules or rankings are derived from the entire population of training data. Second, they did not consider the action costs while building the rules or rankings. For example, a low income housewife may be more sensitive to sales promotion driven by consumption target, while a social housewife may be more interested in promotions related to social networks. Thus, these rule-based and ranking algorithms cannot tackle these problems very well since they are not personalized for each customer. Another related work is extracting actionable knowledge from decision tree and additive tree models by bounded tree search and integer linear programming Yang et al. (2003, 2007); Cui et al. (2015). Yang\u2019s work focuses on finding optimal strategies by using a greedy strategy to search on one or multiple decision trees Yang et al. (2003, 2007). Cui et al. use an integer linear programming (ILP) method to find actions changing sample membership on an ensemble of trees Cui et al. (2015). A limitation of these works is that the actions are assumed to change only one attribute each time. As we discussed above, actions like \u201csending promotion amt N\u201d may change multiple features, such as \u201cnbr promotion amt N\u201d and \u201cs amt N\u201d. Moreover, Yang\u2019s greedy method is fast but cannot give optimal solution Yang et al. (2003), and Cui\u2019s optimization method is optimal but very slow Cui et al.", "startOffset": 125, "endOffset": 1786}, {"referenceID": 3, "context": "There have also been efforts on the development of ranking mechanisms with business interests Hilderman and Hamilton (2000); Cao et al. (2007a) and pruning and summarizing learnt rules by considering similarity Liu and Hsu (1996); Liu et al. (1999); Cao et al. (2007b, 2010). However, such approaches are not suitable for the problems studied in this paper due to two major drawbacks. First, they can not provide customized actionable knowledge for each individual since the rules or rankings are derived from the entire population of training data. Second, they did not consider the action costs while building the rules or rankings. For example, a low income housewife may be more sensitive to sales promotion driven by consumption target, while a social housewife may be more interested in promotions related to social networks. Thus, these rule-based and ranking algorithms cannot tackle these problems very well since they are not personalized for each customer. Another related work is extracting actionable knowledge from decision tree and additive tree models by bounded tree search and integer linear programming Yang et al. (2003, 2007); Cui et al. (2015). Yang\u2019s work focuses on finding optimal strategies by using a greedy strategy to search on one or multiple decision trees Yang et al. (2003, 2007). Cui et al. use an integer linear programming (ILP) method to find actions changing sample membership on an ensemble of trees Cui et al. (2015). A limitation of these works is that the actions are assumed to change only one attribute each time. As we discussed above, actions like \u201csending promotion amt N\u201d may change multiple features, such as \u201cnbr promotion amt N\u201d and \u201cs amt N\u201d. Moreover, Yang\u2019s greedy method is fast but cannot give optimal solution Yang et al. (2003), and Cui\u2019s optimization method is optimal but very slow Cui et al. (2015). In order to address these challenges, we propose a novel approach to extract actionable knowledge from random forests, one of the most popular learning models.", "startOffset": 125, "endOffset": 1860}, {"referenceID": 10, "context": "The reasons why we choose Random forest are: 1) In addition to superior classification/regression performance, Random forest enjoys many appealing properties many other models lack Friedman et al. (2001), including the support for multi-class classification and natural handling of missing values and data of mixed types.", "startOffset": 181, "endOffset": 204}, {"referenceID": 10, "context": "The reasons why we choose Random forest are: 1) In addition to superior classification/regression performance, Random forest enjoys many appealing properties many other models lack Friedman et al. (2001), including the support for multi-class classification and natural handling of missing values and data of mixed types. 2) Often referred to as one of the best off-the-shelf classifier Friedman et al. (2001), Random forest has been widely deployed in many industrial products such as Kinect Shotton et al.", "startOffset": 181, "endOffset": 410}, {"referenceID": 10, "context": "The reasons why we choose Random forest are: 1) In addition to superior classification/regression performance, Random forest enjoys many appealing properties many other models lack Friedman et al. (2001), including the support for multi-class classification and natural handling of missing values and data of mixed types. 2) Often referred to as one of the best off-the-shelf classifier Friedman et al. (2001), Random forest has been widely deployed in many industrial products such as Kinect Shotton et al. (2013) and face detection in camera Viola and Jones (2004), and is the popular method for some competitions such as web search ranking Mohan et al.", "startOffset": 181, "endOffset": 515}, {"referenceID": 10, "context": "The reasons why we choose Random forest are: 1) In addition to superior classification/regression performance, Random forest enjoys many appealing properties many other models lack Friedman et al. (2001), including the support for multi-class classification and natural handling of missing values and data of mixed types. 2) Often referred to as one of the best off-the-shelf classifier Friedman et al. (2001), Random forest has been widely deployed in many industrial products such as Kinect Shotton et al. (2013) and face detection in camera Viola and Jones (2004), and is the popular method for some competitions such as web search ranking Mohan et al.", "startOffset": 181, "endOffset": 567}, {"referenceID": 10, "context": "The reasons why we choose Random forest are: 1) In addition to superior classification/regression performance, Random forest enjoys many appealing properties many other models lack Friedman et al. (2001), including the support for multi-class classification and natural handling of missing values and data of mixed types. 2) Often referred to as one of the best off-the-shelf classifier Friedman et al. (2001), Random forest has been widely deployed in many industrial products such as Kinect Shotton et al. (2013) and face detection in camera Viola and Jones (2004), and is the popular method for some competitions such as web search ranking Mohan et al. (2011). Consider a dataset {X,Y }, where X = {x1, \u00b7 \u00b7 \u00b7 , x} is the set of training samples and Y = {y1, \u00b7 \u00b7 \u00b7 , y} is the set of classification labels.", "startOffset": 181, "endOffset": 663}, {"referenceID": 7, "context": "SAS+ formalism In classical planning, there are two popular formalisms, STRIPS and PDDL Fox and Long (2003). In recent years, another indirect formalism, SAS+, has attracted increasing uses due to its many favorable features, such as compact encoding with multi-valued variables, natural support for invariants, associated domain transition graphs (DTGs) and causal graphs (CGs) which capture vital structural information B\u00e4ckstr\u00f6m and Nebel (1995); Jonsson and B\u00e4ckstr\u00f6m (1998); Helmert (2006).", "startOffset": 88, "endOffset": 108}, {"referenceID": 0, "context": "In recent years, another indirect formalism, SAS+, has attracted increasing uses due to its many favorable features, such as compact encoding with multi-valued variables, natural support for invariants, associated domain transition graphs (DTGs) and causal graphs (CGs) which capture vital structural information B\u00e4ckstr\u00f6m and Nebel (1995); Jonsson and B\u00e4ckstr\u00f6m (1998); Helmert (2006).", "startOffset": 313, "endOffset": 340}, {"referenceID": 0, "context": "In recent years, another indirect formalism, SAS+, has attracted increasing uses due to its many favorable features, such as compact encoding with multi-valued variables, natural support for invariants, associated domain transition graphs (DTGs) and causal graphs (CGs) which capture vital structural information B\u00e4ckstr\u00f6m and Nebel (1995); Jonsson and B\u00e4ckstr\u00f6m (1998); Helmert (2006).", "startOffset": 313, "endOffset": 370}, {"referenceID": 0, "context": "In recent years, another indirect formalism, SAS+, has attracted increasing uses due to its many favorable features, such as compact encoding with multi-valued variables, natural support for invariants, associated domain transition graphs (DTGs) and causal graphs (CGs) which capture vital structural information B\u00e4ckstr\u00f6m and Nebel (1995); Jonsson and B\u00e4ckstr\u00f6m (1998); Helmert (2006). In SAS+ formalism, a planning problem is defined over a set of multi-valued state variables X = {x1, \u00b7 \u00b7 \u00b7 , x|X |}.", "startOffset": 313, "endOffset": 386}, {"referenceID": 26, "context": "There are two kinds of features, soft attributes which can be changed with reasonable costs and hard attributes which cannot be changed with a reasonable cost, such as gender Yang et al. (2003). We only consider actions that change soft attributes.", "startOffset": 175, "endOffset": 194}, {"referenceID": 6, "context": "A Planning Approach to SOAP The SOAP problem is proven to be an NP-hard problem, even when an action can change only one feature Cui et al. (2015). Therefore, we cannot expect any efficient algorithm for optimally solving it.", "startOffset": 129, "endOffset": 147}, {"referenceID": 15, "context": "Our method follows the bounded SAT solving strategy, originally proposed in SATPlan Kautz and Selman (1992) and Graphplan Blum and Furst (1997).", "startOffset": 84, "endOffset": 108}, {"referenceID": 2, "context": "Our method follows the bounded SAT solving strategy, originally proposed in SATPlan Kautz and Selman (1992) and Graphplan Blum and Furst (1997). It starts from a lower bound of makespan (L=1), encodes the SAS+ problem as a weighted partial Max-SAT (WPMax-SAT) instance Lu et al.", "startOffset": 122, "endOffset": 144}, {"referenceID": 2, "context": "Our method follows the bounded SAT solving strategy, originally proposed in SATPlan Kautz and Selman (1992) and Graphplan Blum and Furst (1997). It starts from a lower bound of makespan (L=1), encodes the SAS+ problem as a weighted partial Max-SAT (WPMax-SAT) instance Lu et al. (2014), and either proves it unsatisfiable or finds a plan while trying to minimize total action costs at the same time.", "startOffset": 122, "endOffset": 286}, {"referenceID": 1, "context": "Using soft clauses to optimize the plan in our WPMax-SAT encoding is similar to Balyo\u2019s work Balyo et al. (2014) which uses a MAXSAT based approach for plan optimization (removing redundant actions).", "startOffset": 93, "endOffset": 113}, {"referenceID": 19, "context": "2) A sub-optimal state space method denoted as \u201cNS\u201d Lu et al. (2016). 3) An integer linear programming (ILP) method Cui et al.", "startOffset": 52, "endOffset": 69}, {"referenceID": 6, "context": "3) An integer linear programming (ILP) method Cui et al. (2015), one of the state-of-the-art algorithms for solving the SOAP problem.", "startOffset": 46, "endOffset": 64}, {"referenceID": 6, "context": "3) An integer linear programming (ILP) method Cui et al. (2015), one of the state-of-the-art algorithms for solving the SOAP problem. ILP gives exact optimal solutions. We test these algorithms on a real-world credit card company dataset (\u201cCredit\u201d) and other nine benchmark datasets from the UCI repository2 and the LibSVM website3 used in ILP\u2019s original experiments Cui et al. (2015). Information of the datasets is listed in Table 1.", "startOffset": 46, "endOffset": 385}, {"referenceID": 19, "context": "Note that the proposed action extraction algorithm can be easily expanded to other additive tree models (ATMs) Lu et al. (2016), such as adaboost Freund and Schapire (1997), gradient boosting trees Friedman.", "startOffset": 111, "endOffset": 128}, {"referenceID": 9, "context": "(2016), such as adaboost Freund and Schapire (1997), gradient boosting trees Friedman.", "startOffset": 25, "endOffset": 52}], "year": 2016, "abstractText": "A main focus of machine learning research has been improving the generalization accuracy and efficiency of prediction models. Many models such as SVM, random forest, and deep neural nets have been proposed and achieved great success. However, what emerges as missing in many applications is actionability, i.e., the ability to turn prediction results into actions. For example, in applications such as customer relationship management, clinical prediction, and advertisement, the users need not only accurate prediction, but also actionable instructions which can transfer an input to a desirable goal (e.g., higher profit repays, lower morbidity rates, higher ads hit rates). Existing effort in deriving such actionable knowledge is few and limited to simple action models which restricted to only change one attribute for each action. The dilemma is that in many real applications those action models are often more complex and harder to extract an optimal solution. In this paper, we propose a novel approach that achieves actionability by combining learning with planning, two core areas of AI. In particular, we propose a framework to extract actionable knowledge from random forest, one of the most widely used and best off-the-shelf classifiers. We formulate the actionability problem to a sub-optimal action planning (SOAP) problem, which is to find a plan to alter certain features of a given input so that the random forest would yield a desirable output, while minimizing the total costs of actions. Technically, the SOAP problem is formulated in the SAS+ planning formalism, and solved using a Max-SAT based approach. Our experimental results demonstrate the effectiveness and efficiency of the proposed approach on a personal credit dataset and other benchmarks. Our work represents a new application of automated planning on an emerging and challenging machine learning paradigm.", "creator": "LaTeX with hyperref package"}}}