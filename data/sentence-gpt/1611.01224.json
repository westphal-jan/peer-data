{"id": "1611.01224", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2016", "title": "Sample Efficient Actor-Critic with Experience Replay", "abstract": "This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method. The paper describes the experimental data set from a group of researchers at UCLA, the National Research Council of the United States, and the National Science Foundation for research on the potential of these new techniques.", "histories": [["v1", "Thu, 3 Nov 2016 23:21:32 GMT  (1409kb,D)", "http://arxiv.org/abs/1611.01224v1", "20 pages. Prepared for ICLR 2017"], ["v2", "Mon, 10 Jul 2017 14:38:10 GMT  (2708kb,D)", "http://arxiv.org/abs/1611.01224v2", "20 pages. Prepared for ICLR 2017"]], "COMMENTS": "20 pages. Prepared for ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ziyu wang", "victor bapst", "nicolas heess", "volodymyr mnih", "remi munos", "koray kavukcuoglu", "nando de freitas"], "accepted": true, "id": "1611.01224"}, "pdf": {"name": "1611.01224.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ziyu Wang", "Victor Bapst"], "emails": ["ziyu@google.com", "vbapst@google.com", "heess@google.com", "vmnih@google.com", "Munos@google.com", "korayk@google.com", "nandodefreitas@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Realistic simulated environments, where agents can be trained to learn a large repertoire of cognitive skills, are at the core of recent breakthroughs in AI (Bellemare et al., 2013; Mnih et al., 2015; Schulman et al., 2015a; Narasimhan et al., 2015; Mnih et al., 2016; Brockman et al., 2016; Oh et al., 2016). With richer realistic environments, the capabilities of our agents have increased and improved. Unfortunately, these advances have been accompanied by a substantial increase in the cost of simulation. In particular, every time an agent acts upon the environment, an expensive simulation step is conducted. Thus to reduce the cost of simulation, we need to reduce the number of simulation steps (i.e. samples of the environment). This need for sample efficiency is even more compelling when agents are deployed in the real world.\nExperience replay (Lin, 1992) has gained popularity in deep Q-learning (Mnih et al., 2015; Schaul et al., 2016; Wang et al., 2016; Narasimhan et al., 2015), where it is often motivated as a technique for reducing sample correlation. Replay is actually a valuable tool for improving sample efficiency and, as we will see in our experiments, state-of-the-art deep Q-learning methods (Schaul et al., 2016; Wang et al., 2016) have been up to this point the most sample efficient techniques on Atari by a significant margin. However, we need to do better than deep Q-learning, because it has two important limitations. First, the deterministic nature of the optimal policy limits its use in adversarial domains. Second, finding the greedy action with respect to the Q function is costly for large action spaces.\nPolicy gradient methods have been at the heart of significant advances in AI and robotics (Silver et al., 2014; Lillicrap et al., 2015; Silver et al., 2016; Levine et al., 2015; Mnih et al., 2016; Schulman et al., 2015a; Heess\nar X\niv :1\n61 1.\n01 22\n4v 1\n[ cs\n.L G\n] 3\net al., 2015). Many of these methods are restricted to continuous domains or to very specific tasks such as playing Go. The existing variants applicable to both continuous and discrete domains, such as the on-policy asynchronous advantage actor critic (A3C) of Mnih et al. (2016), are sample inefficient.\nThe design of stable, sample efficient actor critic methods that apply to both continuous and discrete action spaces has been a long-standing hurdle of reinforcement learning (RL). We believe this paper is the first to address this challenge successfully at scale. More specifically, we introduce an actor critic with experience replay (ACER) that nearly matches the state-of-the-art performance of deep Q-networks with prioritized replay on Atari, and substantially outperforms A3C in terms of sample efficiency on both Atari and continuous control domains.\nACER capitalizes on recent advances in deep neural networks, variance reduction techniques, the off-policy Retrace algorithm (Munos et al., 2016) and parallel training of RL agents (Mnih et al., 2016). Yet, crucially, its success hinges on innovations advanced in this paper: truncated importance sampling with bias correction, stochastic dueling network architectures, and efficient trust region policy optimization.\nOn the theoretical front, the paper proves that the Retrace operator can be rewritten from our proposed truncated importance sampling with bias correction technique."}, {"heading": "2 BACKGROUND AND PROBLEM SETUP", "text": "Consider an agent interacting with its environment over discrete time steps. At time step t, the agent observes the nx-dimensional state vector xt \u2208 X \u2286 Rnx , chooses an action at according to a policy \u03c0(a|xt) and observes a reward signal rt \u2208 R produced by the environment. We will consider discrete actions at \u2208 {1, 2, . . . , Na} in Sections 3 and 4, and continuous actions at \u2208 A \u2286 Rna in Section 5. The goal of the agent is to maximize the discounted return Rt = \u2211 i\u22650 \u03b3\nirt+i in expectation. The discount factor \u03b3 \u2208 [0, 1) trades-off the importance of immediate and future rewards. For an agent following policy \u03c0, we use the standard definitions of the state-action and state only value functions:\nQ\u03c0(xt, at) = Ext+1:\u221e,at+1:\u221e [Rt|xt, at] and V \u03c0(xt) = Eat [Q\u03c0(xt, at)|xt] . Here, the expectations are with respect to the observed environment states xt and the actions generated by the policy \u03c0, where xt+1:\u221e denotes a state trajectory starting at time t+ 1.\nWe also need to define the advantage function A\u03c0(xt, at) = Q\u03c0(xt, at)\u2212 V \u03c0(xt), which provides a relative measure of value of each action since Eat [A\u03c0(xt, at)] = 0.\nThe parameters \u03b8 of the differentiable policy \u03c0\u03b8(at|xt) can be updated using the discounted approximation to the policy gradient (Sutton et al., 2000), which borrowing notation from Schulman et al. (2015b), is defined as:\ng = Ex0:\u221e,a0:\u221e \u2211 t\u22650 A\u03c0(xt, at)\u2207\u03b8 log \u03c0\u03b8(at|xt)  . (1) Following Proposition 1 of Schulman et al. (2015b), we can replace A\u03c0(xt, at) in the above expression with the state-action value Q\u03c0(xt, at), the discounted return Rt, or the temporal difference residual rt + \u03b3V\n\u03c0(xt+1)\u2212 V \u03c0(xt), without introducing bias. These choices will however have different variance. Moreover, in practice we will approximate these quantities with neural networks thus introducing additional approximation errors and biases. Typically, the policy gradient estimator using Rt will have higher variance and lower bias whereas the estimators using function approximation will have higher bias and lower variance. Combining Rt with the current value function approximation to minimize bias while maintaining bounded variance is one of the central design principles behind ACER.\nTo trade-off bias and variance, the asynchronous advantage actor critic (A3C) of Mnih et al. (2016) uses a single trajectory sample to obtain the following gradient approximation:\ng\u0302a3c = \u2211 t\u22650 (( k\u22121\u2211 i=0 \u03b3irt+i ) + \u03b3kV \u03c0\u03b8v (xt+k)\u2212 V \u03c0\u03b8v (xt) ) \u2207\u03b8 log \u03c0\u03b8(at|xt). (2)\nA3C combines both k-step returns and function approximation to trade-off variance and bias. We may think of V \u03c0\u03b8v (xt) as a policy gradient baseline used to reduce variance.\nIn the following section, we will introduce the discrete-action version of ACER. ACER may be understood as the off-policy counterpart of the A3C method of Mnih et al. (2016). As such, ACER builds on all the engineering innovations of A3C, including efficient parallel CPU computation. ACER uses a single deep neural network to estimate the policy \u03c0\u03b8(at|xt) and the value function V \u03c0\u03b8v (xt). (For clarity and generality, we are using two different symbols to denote the parameters of the policy and value function, \u03b8 and \u03b8v, but most of these parameters are shared in the single neural network.) Our neural networks, though building on the networks used in A3C, will introduce several modifications and new modules."}, {"heading": "3 DISCRETE ACTOR CRITIC WITH EXPERIENCE REPLAY", "text": "Off-policy learning with experience replay may appear to be an obvious strategy for improving the sample efficiency of actor-critics. However, controlling the variance and stability of off-policy estimators is notoriously hard. Importance sampling is one of the most popular approaches for off-policy learning (Meuleau et al., 2000; Jie & Abbeel, 2010; Levine & Koltun, 2013). In our context, it proceeds as follows. Suppose we retrieve a trajectory {x0, a0, r0, \u00b5(\u00b7|x0), \u00b7 \u00b7 \u00b7 , xk, ak, rk, \u00b5(\u00b7|xk)}, where the actions have been sampled according to the behavior policy \u00b5, from our memory of experiences. Then, the importance weighted policy gradient is given by:\ng\u0302imp = ( k\u220f t=0 \u03c1t ) k\u2211 t=0 ( k\u2211 i=0 \u03b3irt+i ) \u2207\u03b8 log \u03c0\u03b8(at|xt), (3)\nwhere \u03c1t = \u03c0(at|xt) \u00b5(at|xt) denotes the importance weight. This estimator is unbiased, but it suffers from very high variance as it involves a product of many potentially unbounded importance weights. To prevent the product of importance weights from exploding, Wawrzyn\u0301ski (2009) truncates this product. Truncated importance sampling over entire trajectories, although bounded in variance, could suffer from significant bias.\nRecently, Degris et al. (2012) attacked this problem by using marginal value functions over the limiting distribution of the process to yield the following approximation of the gradient:\ngmarg = Ext\u223c\u03b2,at\u223c\u00b5 [\u03c1t\u2207\u03b8 log \u03c0\u03b8(at|xt)Q\u03c0(xt, at)] , (4) where Ext\u223c\u03b2,at\u223c\u00b5[\u00b7] is the expectation with respect to the limiting distribution \u03b2(x) = limt\u2192\u221e P (xt = x|x0, \u00b5) with behavior policy \u00b5. To keep the notation succinct, we will replace Ext\u223c\u03b2,at\u223c\u00b5[\u00b7] with Extat [\u00b7] and ensure we remind readers of this when necessary.\nTwo important facts about equation (4) must be highlighted. First, note that it depends on Q\u03c0 and not on Q\u00b5, consequently we must be able to estimate Q\u03c0. Second, we no longer have a product of importance weights, but instead only need to estimate the marginal importance weight \u03c1t. Importance sampling in this lower dimensional space (over marginals as opposed to trajectories) is expected to exhibit lower variance.\nDegris et al. (2012) estimate Q\u03c0 in equation (4) using lambda returns: R\u03bbt = rt + (1 \u2212 \u03bb)\u03b3V (xt+1) + \u03bb\u03b3\u03c1t+1R \u03bb t+1. This estimator requires that we know how to choose \u03bb ahead of time to trade off bias and variance. Moreover, when using small values of \u03bb to reduce variance, occasional large importance weights can still cause instability.\nIn the following subsection, we adopt the Retrace algorithm of Munos et al. (2016) to estimate Q\u03c0. Subsequently, we propose an importance weight truncation technique to improve the stability of the off-policy actor critic of Degris et al. (2012), and introduce a computationally efficient trust region scheme for policy optimization. The formulation of ACER for continuous action spaces will require further innovations that are advanced in Section 5."}, {"heading": "3.1 MULTI-STEP ESTIMATION OF THE STATE-ACTION VALUE FUNCTION", "text": "In this paper, we estimate Q\u03c0(xt, at) using Retrace (Munos et al., 2016). (We also experimented with the related tree backup method of Precup et al. (2000) but found Retrace to perform better in practice.) Given a trajectory generated under the behavior policy \u00b5, the Retrace estimator can be expressed recursively as follows1:\nQret(xt, at) = rt + \u03b3\u03c1\u0304t+1[Q ret(xt+1, at+1)\u2212Q(xt+1, at+1)] + \u03b3V (xt+1), (5)\nwhere \u03c1\u0304t is the truncated importance weight, \u03c1\u0304t = min {c, \u03c1t} with \u03c1t = \u03c0(at|xt)\u00b5(at|xt) , Q is the current value estimate of Q\u03c0, and V (x) = Ea\u223c\u03c0Q(x, a). Retrace is an off-policy, return-based algorithm which has low variance and is proven to converge (in the tabular case) to the value function of the target policy for any behavior policy, see Munos et al. (2016).\nThe recursive Retrace equation depends on the estimate Q. To compute it, in discrete action spaces, we adopt a convolutional neural network with \u201ctwo heads\u201d that outputs the estimate Q\u03b8v (xt, at), as well as the policy \u03c0\u03b8(at|xt). This neural representation is the same as in (Mnih et al., 2016), with the exception that we output the vector Q\u03b8v (xt, at) instead of the scalar V\u03b8v (xt). The estimate V\u03b8v (xt) can be easily derived by taking the expectation of Q\u03b8v under \u03c0\u03b8.\nTo approximate the policy gradient gmarg, ACER uses Qret to estimate Q\u03c0. As Retrace uses multi-step returns, it can significantly reduce bias in the estimation of the policy gradient 2.\nTo learn the critic Q\u03b8v (xt, at), we again use Q ret(xt, at) as a target and update its parameters \u03b8v with the following standard gradient:\n(Qret(xt, at)\u2212Q\u03b8v (xt, at))\u2207\u03b8vQ\u03b8v (xt, at)). (6)\nBecause Retrace is return-based, it also enables faster learning of the critic. Thus the purpose of the multi-step estimator Qret in our setting is twofold: to reduce bias in the policy gradient, and to enable faster learning of the critic, hence further reducing bias."}, {"heading": "3.2 IMPORTANCE WEIGHT TRUNCATION WITH BIAS CORRECTION", "text": "The marginal importance weights in Equation (4) can become large, thus causing instability. To safe-guard against high variance, we propose to truncate the importance weights and introduce a correction term via the following decomposition of gmarg:\ngmarg =Extat [\u03c1t\u2207\u03b8log \u03c0\u03b8(at|xt)Q\u03c0(xt, at)]\n=Ext [ Eat[\u03c1\u0304t\u2207\u03b8log \u03c0\u03b8(at|xt)Q\u03c0(xt, at)] + E\na\u223c\u03c0 ([ \u03c1t(a)\u2212 c \u03c1t(a) ] + \u2207\u03b8log \u03c0\u03b8(a|xt)Q\u03c0(xt, a) )] , (7)\n1For ease of presentation, we consider only \u03bb = 1 for Retrace. 2An alternative to Retrace here is Q(\u03bb) with off-policy corrections (Harutyunyan et al., 2016) which we discuss in\nmore detail in Appendix B.\nwhere \u03c1\u0304t = min {c, \u03c1t} with \u03c1t = \u03c0(at|xt)\u00b5(at|xt) as before. We have also introduced the notation \u03c1t(a) = \u03c0(a|xt) \u00b5(a|xt) , and [x]+ = x if x > 0 and it is zero otherwise. We remind readers that the above expectations are with respect to the limiting state distribution under the behavior policy: xt \u223c \u03b2 and at \u223c \u00b5. The clipping of the importance weight in the first term of equation (7) ensures that the variance of the gradient estimate is bounded. The correction term (second term in equation (7)) ensures that our estimate is unbiased. Note that the correction term is only active for actions such that \u03c1t(a) > c. In particular, if we choose a large value for c, the correction term only comes into effect when the variance of the original off-policy estimator of equation (4) is very high. When this happens, our decomposition has the nice property that the truncated weight in the first term is at most c while the correction weight [ \u03c1t(a)\u2212c \u03c1t(a) ] + in the second term is at most 1.\nWe model Q\u03c0(xt, a) in the correction term with our neural network approximation Q\u03b8v (xt, at). This modification results in what we call the truncation with bias correction trick, in this case applied to the function \u2207\u03b8 log \u03c0\u03b8(at|xt)Q\u03c0(xt, at):\ng\u0302marg =Ext [ Eat [ \u03c1\u0304t\u2207\u03b8log \u03c0\u03b8(at|xt)Qret(xt, at) ] + E a\u223c\u03c0 ([ \u03c1t(a)\u2212 c \u03c1t(a) ] + \u2207\u03b8log \u03c0\u03b8(a|xt)Q\u03b8v (xt, a) )] . (8)\nEquation (8) involves an expectation over the stationary distribution of the Markov process. We can however approximate it by sampling trajectories {x0, a0, r0, \u00b5(\u00b7|x0), \u00b7 \u00b7 \u00b7 , xk, ak, rk, \u00b5(\u00b7|xk)} generated from the behavior policy \u00b5. Here the terms \u00b5(\u00b7|xt) are the policy vectors. Given these trajectories, we can compute the off-policy ACER gradient:\ng\u0302acer = \u03c1\u0304t\u2207\u03b8 log \u03c0\u03b8(at|xt)[Qret(xt, at)\u2212 V\u03b8v (xt)]\n+ E a\u223c\u03c0 ([ \u03c1t(a)\u2212 c \u03c1t(a) ] + \u2207\u03b8 log \u03c0\u03b8(a|xt)[Q\u03b8v (xt, a)\u2212 V\u03b8v (xt)] ) . (9)\nIn the above expression, we have subtracted the classical baseline V\u03b8v (xt) to reduce variance."}, {"heading": "3.3 EFFICIENT TRUST REGION POLICY OPTIMIZATION", "text": "The policy updates of actor-critic methods do often exhibit high variance. Hence, to ensure stability, we must limit the per-step changes to the policy. Simply using smaller learning rates is insufficient as they cannot guard against the occasional large updates while maintaining a desired learning speed. Trust Region Policy Optimization (TRPO) (Schulman et al., 2015a) provides a more adequate solution.\nSchulman et al. (2015a) approximately limit the difference between the updated policy and the current policy to ensure safety. Despite the effectiveness of their TRPO method, it requires repeated computation of Fisher-vector products for each update. This can prove to be prohibitively expensive in large domains.\nIn this section we introduce a new trust region policy optimization method that scales well to large problems. Instead of constraining the updated policy to be close to the current policy (as in TRPO), we propose to maintain an average policy network that represents a running average of past policies and forces the updated policy to not deviate far from this average.\nWe decompose our policy network in two parts: a distribution f , and a deep neural network that generates the statistics \u03c6\u03b8(x) of this distribution. That is, given f , the policy is completely characterized by the network \u03c6\u03b8: \u03c0(\u00b7|x) = f(\u00b7|\u03c6\u03b8(x)). For example, in the discrete domain, we choose f to be the categorical distribution with a probability vector \u03c6\u03b8(x) as its statistics. The probability vector is of course parameterised by \u03b8.\nWe denote the average policy network as \u03c6\u03b8a and update its parameters \u03b8a \u201csoftly\u201d after each update to the policy parameter \u03b8: \u03b8a \u2190 \u03b1\u03b8a + (1\u2212 \u03b1)\u03b8.\nConsider, for example, the ACER policy gradient as defined in Equation (9), but with respect to \u03c6:\ng\u0302acert = \u03c1\u0304t\u2207\u03c6\u03b8(xt) log f(at|\u03c6\u03b8(x))[Qret(xt, at)\u2212 V\u03b8v (xt)]\n+ E a\u223c\u03c0 ([ \u03c1t(a)\u2212 c \u03c1t(a) ] + \u2207\u03c6\u03b8(xt) log f(at|\u03c6\u03b8(x))[Q\u03b8v (xt, a)\u2212 V\u03b8v (xt)] ) . (10)\nGiven the averaged policy network, our proposed trust region update involves two stages. In the first stage, we solve the following optimization problem with a linearized KL divergence constraint:\nminimize z\n1 2 \u2016g\u0302acert \u2212 z\u201622\nsubject to \u2207\u03c6\u03b8(xt)DKL [f(\u00b7|\u03c6\u03b8a(xt))\u2016f(\u00b7|\u03c6\u03b8(xt))] T z \u2264 \u03b4\n(11)\nSince the constraint is linear, the overall optimization problem reduces to a simple quadratic programming problem, the solution of which can be easily derived in closed form using the KKT conditions. Letting k = \u2207\u03c6\u03b8(xt)DKL [f(\u00b7|\u03c6\u03b8a(xt)\u2016f(\u00b7|\u03c6\u03b8(xt)], the solution is:\nz\u2217 = g\u0302acert \u2212max {\n0, kT g\u0302acert \u2212 \u03b4 \u2016k\u201622\n} k (12)\nThis transformation of the gradient has a very natural form. If the constraint is satisfied, there is no change to the gradient with respect to \u03c6\u03b8(xt). Otherwise, the update is scaled down in the direction of k, thus effectively lowering rate of change between the activations of the current policy and the average policy network.\nIn the second stage, we take advantage of back-propagation. Specifically, the updated gradient with respect to \u03c6\u03b8, that is z\u2217, is back-propagated through the network to compute the derivatives with respect to the parameters. The parameter updates for the policy network follow from the chain rule: \u2202\u03c6\u03b8(x)\u2202\u03b8 z \u2217.\nThe trust region step is carried out in the space of the statistics of the distribution f , and not in the space of the policy parameters. This is done deliberately so as to avoid an additional back-propagation step through the policy network.\nWe would like to remark that the algorithm advanced in this section can be thought of as a general strategy for modifying the backward messages in back-propagation so as to stabilize the activations.\nInstead of a trust region update, one could alternatively add an appropriately scaled KL cost to the objective function as proposed by Heess et al. (2015). This approach, however, is less robust to the choice of hyperparameters in our experience.\nThe ACER algorithm results from a combination of the above ideas, with the precise pseudo-code appearing in Appendix A. A master algorithm (Algorithm 1) calls ACER on-policy to perform updates and propose trajectories. It then calls ACER off-policy component to conduct several replay steps. When on-policy, ACER effectively becomes a modified version of A3C where Q instead of V baselines are employed and trust region optimization is used."}, {"heading": "4 RESULTS ON ATARI", "text": "We use the Arcade Learning Environment of Bellemare et al. (2013) to conduct an extensive evaluation. We deploy one single algorithm and network architecture, with fixed hyper-parameters, to learn to play 57 Atari games given only raw pixel observations and game rewards. This task is highly demanding because of the diversity of games, and high-dimensional pixel-level observations.\nOur experimental setup uses 16 actor-learner threads running on a single machine with no GPUs. We adopt the same input pre-processing and network architecture as Mnih et al. (2015). Specifically, the network consists\nof a convolutional layer with 32 8\u00d7 8 filters with stride 4 followed by another convolutional layer with 64 4\u00d7 4 filters with stride 2, followed by a final convolutional layer with 64 3\u00d7 3 filters with stride 1, followed by a fully-connected layer of size 512. Each of the hidden layers is followed by a rectifier nonlinearity. The network outputs a softmax policy and Q values.\nWhen using replay, we add to each thread a replay memory that is up to 50 000 frames in size. The total amount of memory used across all threads is thus similar in size to that of DQN (Mnih et al., 2015). For all Atari experiments, we use a single learning rate adopted from an earlier implementation of A3C without further tuning. We do not anneal the learning rates over the course of training as in Mnih et al. (2016). We otherwise adopt the same optimization procedure as in Mnih et al. (2016). Specifically, we adopt entropy regularization with weight 0.001, discount the rewards with \u03b3 = 0.99, and perform updates every 20 steps (k = 20 in the notation of Section 2). In all our experiments with experience replay, we use importance weight truncation with c = 10. We consider training ACER both with and without trust region updating as described in Section 3.3. When trust region updating is used, we use \u03b4 = 1 and \u03b1 = 0.99 for all experiments.\nTo compare different agents, we adopt as our metric the median of the human normalized score over all 57 games. The normalization is calculated such that, for each game, human scores and random scores are evaluated to 1, and 0 respectively. The normalized score for a given game at time t is computed as the average normalized score over the past 1 million consecutive frames encountered until time t. For each agent, we plot its cumulative maximum median score over time. The result is summarized in Figure 1.\nThe four colors in Figure 1 correspond to four replay ratios (0, 1, 4 and 8) with a ratio of 4 meaning that we use the off-policy component of ACER 4 times after using the on-policy component (A3C). That is, a replay ratio of 0 means that we are using A3C. The solid and dashed lines represent ACER with and without trust region updating respectively. The gray and black curves are the original DQN (Mnih et al., 2015) and Prioritized Replay agent of Schaul et al. (2016) agents respectively.\nAs shown on the left panel of Figure 1, replay significantly increases data efficiency. We observe that when using the trust region optimizer, the average reward as a function of the number of environmental steps increases with the ratio of replay. This increase has diminishing returns, but with enough replay, ACER can\nmatch the performance of the best DQN agents. Moreover, it is clear that the off-policy actor critics (ACER) are much more sample efficient than their on-policy counterpart (A3C).\nThe right panel of Figure 1 shows that ACER agents perform similarly to A3C when measured by wall clock time. Thus, in this case, it is possible to achieve better data-efficiency without necessarily compromising on computation time. In particular, ACER with a replay ratio of 4 is an appealing alternative to either the prioritized DQN agent or A3C."}, {"heading": "5 CONTINUOUS ACTOR CRITIC WITH EXPERIENCE REPLAY", "text": "To extend ACER to continuous action spaces we must overcome some important challenges. Most notably, Retrace requires estimates of both Q and V , but we can no longer easily integrate over Q to derive V . A solution to this problem, as well as modifications necessary for trust region updating, follow in this section."}, {"heading": "5.1 POLICY EVALUATION", "text": "Retrace provides a target for learning Q\u03b8v , but not for learning V\u03b8v . We could use importance sampling to compute V\u03b8v given Q\u03b8v , but this estimator has high variance.\nWe propose a new architecture which we call Stochastic Dueling Networks (SDNs), inspired by the Dueling networks of Wang et al. (2016), which is designed to estimate both V \u03c0 and Q\u03c0 off-policy while maintaining consistency between the two estimates. At each time step, an SDN outputs a stochastic estimate Q\u0303\u03b8v of Q \u03c0 and a deterministic estimate V\u03b8v of V \u03c0 , such that\nQ\u0303\u03b8v (xt, at) \u223c V\u03b8v (xt) +A\u03b8v (xt, at)\u2212 1\nn n\u2211 i=1 A\u03b8v (xt, ui), and ui \u223c \u03c0\u03b8(\u00b7|xt) (13)\nwhere n is a parameter. The two estimates are consistent in the sense that Ea\u223c\u03c0(\u00b7|xt) [ Eu1:n\u223c\u03c0(\u00b7|xt) ( Q\u0303\u03b8v (xt, a) )] = V\u03b8v (xt). Furthermore, we can learn about V \u03c0 by learning Q\u0303\u03b8v .\nTo see this, assume we have learned Q\u03c0 perfectly such that Eu1:n\u223c\u03c0(\u00b7|xt) ( Q\u0303\u03b8v (xt, at) ) = Q\u03c0(xt, at), then\nV\u03b8v (xt) = Ea\u223c\u03c0(\u00b7|xt) [ Eu1:n\u223c\u03c0(\u00b7|xt) ( Q\u0303\u03b8v (xt, a) )] = Ea\u223c\u03c0(\u00b7|xt) [Q\u03c0(xt, a)] = V \u03c0(xt). Therefore, a target\non Q\u0303\u03b8v (xt, at) also provides an error signal for updating V\u03b8v .\nIn addition to SDNs, however, we also construct the following novel target for estimating V \u03c0:\nV target(xt) = min { 1, \u03c0(at|xt) \u00b5(at|xt) }( Qret(xt, at)\u2212Q\u03b8v (xt, at) ) + V\u03b8v (xt). (14)\nThe above target is also derived via the truncation and bias correction trick; for more details, see Appendix D.\nFinally, when estimating Qret in continuous domains, we implement a slightly different formulation of the truncated importance weights \u03c1\u0304t = min { 1, ( \u03c0(at|xt) \u00b5(at|xt) ) 1 d } , where d is the dimensionality of the action space.\nAlthough not essential, we have found this formulation to lead to faster learning."}, {"heading": "5.2 TRUST REGION UPDATING", "text": "To adopt the trust region updating scheme (Section 3.3) in the continuous control domain, one simply has to change the distribution f and the gradient g\u0302acert to adjust to continuous action spaces.\nFor the distribution f , we choose Gaussian distributions with fixed diagonal covariance and mean \u03c6\u03b8(x).\nTo derive g\u0302acert in continuous action spaces, consider the ACER policy gradient for the stochastic dueling network, but with respect to \u03c6:\ngacert = Ext [ Eat [ \u03c1\u0304t\u2207\u03c6\u03b8(xt) log f(at|\u03c6\u03b8(xt))(Qopc(xt, at)\u2212 V\u03b8v (xt)) ] + E a\u223c\u03c0 ([ \u03c1t(a)\u2212 c \u03c1t(a) ] + (Q\u0303\u03b8v (xt, a)\u2212 V\u03b8v (xt))\u2207\u03c6\u03b8(xt) log f(a|\u03c6\u03b8(xt)) )] . (15)\nIn the above definition, we are using Qopc instead of Qret. See Appendix B for definition and discussion. Given an observation xt, we can sample a\u2032t \u223c \u03c0\u03b8(\u00b7|xt) to obtain the following Monte Carlo approximation\ng\u0302acert = \u03c1\u0304t\u2207\u03c6\u03b8(xt) log f(at|\u03c6\u03b8(xt))(Qopc(xt, at)\u2212 V\u03b8v (xt))\n+\n[ \u03c1t(a \u2032 t)\u2212 c\n\u03c1t(a\u2032t) ] + (Q\u0303\u03b8v (xt, a \u2032 t)\u2212 V\u03b8v (xt))\u2207\u03c6\u03b8(xt) log f(a\u2032t|\u03c6\u03b8(xt)). (16)\nGiven f and g\u0302acert , we apply the same steps as detailed in Section 3.3 to complete the update.\nThe precise pseudo-code of ACER algorithm for continuous spaces results is presented in Appendix A."}, {"heading": "6 RESULTS ON MUJOCO", "text": "We evaluate our algorithms on 6 continuous control tasks, all of which are simulated using the MuJoCo physics engine (Todorov et al., 2012). For descriptions of the tasks, please refer to Appendix E.1. Briefly, the tasks with action dimensionality in brackets are: cartpole (1D), reacher (3D), cheetah (6D), fish (5D), walker (6D) and humanoid (21D). These tasks are illustrated in Figure 2.\nTo benchmark ACER for continuous control, we compare it to its on-policy counterpart both with and without trust region updating. We refer to these two baselines as A3C and Trust-A3C. Additionally, we also compare to a baseline with replay where we truncate the importance weights over trajectories as in (Wawrzyn\u0301ski, 2009). For a detailed description of this baseline, please refer to Appendix E. Again, we run this baseline both with and without trust region updating, and refer to these choices as Trust-TIS and TIS respectively. Last but not least, we refer to our proposed approach with SDN and trust region updating as simply ACER. All five setups are implemented in the asynchronous A3C framework.\nAll the aforementioned setups share the same network architecture that computes the policy and state values. We maintain an additional small network that computes the stochastic A values in the case of ACER. We use n = 5 (using the notation in Equation (13)) in all SDNs. Instead of mixing on-policy and replay learning as done in the Atari domain, ACER for continuous actions is entirely off-policy, with experiences generated from the simulator (4 times on average). When using replay, we add to each thread a replay memory that is 5, 000 frames in size and perform updates every 50 steps (k = 50 in the notation of Section 2). The rate of the soft updating (\u03b1 as in Section 3.3) is set to 0.995 in all setups involving trust region updating. The truncation threshold c is set to 5 for ACER.\nWe use diagonal Gaussian policies with fixed diagonal covariances where the diagonal standard deviation is set to 0.3. For all setups, we sample the learning rates log-uniformly in the range [10\u22124, 10\u22123.3]. For setups involving trust region updating, we also sample \u03b4 uniformly in the range [0.1, 2]. With all setups, we use 30 sampled hyper-parameter settings.\n0 50 100 150 200 250\nMillion Steps\n0\n5\n10\n15\n20\n25 Walker2d (9-DoF/6-dim. Actions)\n0 20 40 60 80 100 120 140 160\nMillion Steps\n\u221212\n\u221211\n\u221210\n\u22129\n\u22128\n\u22127\nFish (13-DoF/5-dim. Actions)\n0 10 20 30 40 50 60\nMillion Steps\n\u2212100\n0\n100\n200\n300\n400\n500 Cartpole (2-DoF/1-dim. Actions)\nThe empirical results for all continuous control tasks are shown Figure 2, where we show the mean and standard deviation of the best 5 out of 30 hyper-parameter settings over which we searched 3. For sensitivity analyses with respect to the hyper-parameters, please refer to Figures 3 and 4 in the Appendix.\nIn continuous control, ACER outperforms the A3C and truncated importance sampling baselines by a very significant margin.\nHere, we also find that the proposed trust region optimization method can result in huge improvements to the baselines. The high-dimensional continuous action policies are much harder to optimize than the small discrete action policies in Atari, and hence we observe much higher gains for trust region optimization in the continuous control domains.\nIn spite of the improvements brought in by trust region optimization, ACER still outperforms all other methods, specially in higher dimensions. Replay with the Retrace operator contributes positively to the performance of ACER, while the truncation trick with bias correction prevents unstable behavior.\n3 For videos of the policies learned with ACER, please see: https://www.youtube.com/watch?v= NmbeQYoVv5g&list=PLkmHIkhlFjiTlvwxEnsJMs3v7seR5HSP-."}, {"heading": "7 THEORETICAL ANALYSIS", "text": "Retrace is a very recent development in reinforcement learning. In fact, this work is the first to consider Retrace in the policy gradients setting. For this reason, and given the core role that Retrace plays in ACER, it is valuable to shed more light on this technique. In this section, we will prove that Retrace can be interpreted as an application of the importance weight truncation and bias correction trick advanced in this paper.\nConsider the following equation:\nQ\u03c0(xt, at) = Ext+1at+1 [rt + \u03b3\u03c1t+1Q\u03c0(xt+1, at+1)] . (17) If we apply the weight truncation and bias correction trick to the above equation we obtain\nQ\u03c0(xt, at) = Ext+1at+1 [ rt + \u03b3\u03c1\u0304t+1Q\n\u03c0(xt+1, at+1) + \u03b3 E a\u223c\u03c0 ([ \u03c1t+1(a)\u2212 c \u03c1t+1(a) ] + Q\u03c0(xt+1, a) )] . (18)\nBy recursively expanding Q\u03c0 as in Equation (18), we can represent Q\u03c0(x, a) as:\nQ\u03c0(x, a) = E\u00b5 \u2211 t\u22650 \u03b3t ( t\u220f i=1 \u03c1\u0304i )( rt + \u03b3 E b\u223c\u03c0 ([ \u03c1t+1(b)\u2212 c \u03c1t+1(b) ] + Q\u03c0(xt+1, b) )) . (19) The expectation E\u00b5 is taken over trajectories starting from x with actions generated with respect to \u00b5. When Q\u03c0 is not available, we can replace it with our current estimate Q to get a return-based esitmate of Q\u03c0 . This operation also defines an operator:\nBQ(x, a) = E\u00b5 \u2211 t\u22650 \u03b3t ( t\u220f i=1 \u03c1\u0304i )( rt + \u03b3 E b\u223c\u03c0 ([ \u03c1t+1(b)\u2212 c \u03c1t+1(b) ] + Q(xt+1, b) )) . (20) In the following proposition, we show that B is a contraction operator with a unique fixed point Q\u03c0 and that it is equivalent to the Retrace operator. Proposition 1. The operator B is a contraction operator such that \u2016BQ\u2212Q\u03c0\u2016\u221e \u2264 \u03b3\u2016Q\u2212Q\u03c0\u2016\u221e and B is equivalent to Retrace.\nThe above proposition not only shows an alternative way of arriving at the same operator, but also provides a different proof of contraction for Retrace. Please refer to Appendix C for the regularization conditions and proof of the above proposition.\nFinally, B, and therefore Retrace, generalizes both the Bellman operator T \u03c0 and importance sampling. Specifically, when c = 0, B = T \u03c0 and when c =\u221e, B recovers importance sampling; see Appendix C."}, {"heading": "8 CONCLUDING REMARKS", "text": "We have introduced a stable off-policy actor critic that scales to both continuous and discrete action spaces. This approach integrates several recent advances in RL in a principle manner. In addition, it integrates three innovations advanced in this paper: truncated importance sampling with bias correction, stochastic dueling networks and an efficient trust region policy optimization method.\nWe showed that the method not only matches the performance of the best known methods on Atari, but that it also outperforms popular techniques on several continuous control problems.\nThe efficient trust region optimization method advanced in this paper performs remarkably well in continuous domains. It could prove very useful in other deep learning domains, where it is hard to stabilize the training process."}, {"heading": "ACKNOWLEDGMENTS", "text": "We are very thankful to Marc Bellemare, Jascha Sohl-Dickstein, and Se\u0301bastien Racaniere for proof-reading and valuable suggestions."}, {"heading": "A ACER PSEUDO-CODE FOR DISCRETE ACTIONS", "text": "Algorithm 1 ACER for discrete actions (master algorithm) // Assume global shared parameter vectors \u03b8 and \u03b8v . // Assume ratio of replay r. repeat\nCall ACER on-policy, Algorithm 2. n\u2190 Possion(r) for i \u2208 {1, \u00b7 \u00b7 \u00b7 , n} do\nCall ACER off-policy, Algorithm 2. end for\nuntil Max iteration or time reached.\nAlgorithm 2 ACER for discrete actions Reset gradients d\u03b8 \u2190 0 and d\u03b8v \u2190 0. Initialize parameters \u03b8\u2032 \u2190 \u03b8 and \u03b8\u2032v \u2190 \u03b8v . if not On-Policy then\nSample the trajectory {x0, a0, r0, \u00b5(\u00b7|x0), \u00b7 \u00b7 \u00b7 , xk, ak, rk, \u00b5(\u00b7|xk)} from the replay memory. else\nGet state x0 end if for i \u2208 {0, \u00b7 \u00b7 \u00b7 , k} do\nCompute f(\u00b7|\u03c6\u03b8\u2032(xi)), Q\u03b8\u2032v (xi, \u00b7) and f(\u00b7|\u03c6\u03b8a(xi)). if On-Policy then\nPerform ai according to f(\u00b7|\u03c6\u03b8\u2032(xi)) Receive reward ri and new state xi+1 \u00b5(\u00b7|xi)\u2190 f(\u00b7|\u03c6\u03b8\u2032(xi))\nend if \u03c1\u0304i \u2190 min { 1,\nf(ai|\u03c6\u03b8\u2032 (xi)) \u00b5(ai|xi)\n} .\nend for\nQret \u2190 { 0 for terminal xk\u2211 aQ\u03b8\u2032v (xk, a)f(a|\u03c6\u03b8\u2032(xk)) otherwise for i \u2208 {k \u2212 1, \u00b7 \u00b7 \u00b7 , 0} do Qret \u2190 ri + \u03b3Qret Vi \u2190 \u2211 aQ\u03b8\u2032v (xi, a)f(a|\u03c6\u03b8\u2032(xi))\nComputing quantities needed for trust region updating:\ng \u2190 min {c, \u03c1i(ai)}\u2207\u03c6\u03b8\u2032 (xi) log f(ai|\u03c6\u03b8\u2032(xi))(Q ret \u2212 Vi)\n+ \u2211 a [ 1\u2212 c \u03c1i(a) ] + f(a|\u03c6\u03b8\u2032(xi))\u2207\u03c6\u03b8\u2032 (xi) log f(a|\u03c6\u03b8\u2032(xi))(Q\u03b8\u2032v (xi, ai)\u2212 Vi)\nk \u2190 \u2207\u03c6\u03b8\u2032 (xi)DKL [f(\u00b7|\u03c6\u03b8a(xi)\u2016f(\u00b7|\u03c6\u03b8\u2032(xi)]\nAccumulate gradients wrt \u03b8\u2032: d\u03b8\u2032 \u2190 d\u03b8\u2032 + \u2202\u03c6\u03b8\u2032 (xi) \u2202\u03b8\u2032\n( g \u2212max { 0, k\nT g\u2212\u03b4 \u2016k\u201622\n} k )\nAccumulate gradients wrt \u03b8\u2032v: d\u03b8v \u2190 d\u03b8v +\u2207\u03b8\u2032v (Q ret \u2212Q\u03b8\u2032v (xi, a)) 2 Update Retrace target: Qret \u2190 \u03c1\u0304i ( Qret \u2212Q\u03b8\u2032v (xi, ai) ) + Vi\nend for Perform asynchronous update of \u03b8 using d\u03b8 and of \u03b8v using d\u03b8v . Updating the average policy network: \u03b8a \u2190 \u03b1\u03b8a + (1\u2212 \u03b1)\u03b8\nAlgorithm 3 ACER for Continuous Actions Reset gradients d\u03b8 \u2190 0 and d\u03b8v \u2190 0. Initialize parameters \u03b8\u2032 \u2190 \u03b8 and \u03b8\u2032v \u2190 \u03b8v . Sample the trajectory {x0, a0, r0, \u00b5(\u00b7|x0), \u00b7 \u00b7 \u00b7 , xk, ak, rk, \u00b5(\u00b7|xk)} from the replay memory. for i \u2208 {0, \u00b7 \u00b7 \u00b7 , k} do\nCompute f(\u00b7|\u03c6\u03b8\u2032(xi)), V\u03b8\u2032v (xi), Q\u0303\u03b8\u2032v (xi, ai), and f(\u00b7|\u03c6\u03b8a(xi)). Sample a\u2032i \u223c f(\u00b7|\u03c6\u03b8\u2032(xi)) \u03c1i \u2190 f(ai|\u03c6\u03b8\u2032 (xi))\u00b5(ai|xi) and \u03c1 \u2032 i \u2190 f(a\u2032i|\u03c6\u03b8\u2032 (xi)) \u00b5(a\u2032i|xi)\nci \u2190 min { 1, (\u03c1i) 1 d } .\nend for\nQret \u2190 { 0 for terminal xk V\u03b8\u2032v (xk) otherwise Qopc \u2190 Qret for i \u2208 {k \u2212 1, \u00b7 \u00b7 \u00b7 , 0} do Qret \u2190 ri + \u03b3Qret Qopc \u2190 ri + \u03b3Qopc Computing quantities needed for trust region updating:\ng \u2190 min {c, \u03c1i}\u2207\u03c6\u03b8\u2032 (xi) log f(ai|\u03c6\u03b8\u2032(xi)) ( Qopc(xi, ai)\u2212 V\u03b8\u2032v (xi) ) + [ 1\u2212 c\n\u03c1\u2032i ] + (Q\u0303\u03b8\u2032v (xi, a \u2032 i)\u2212 V\u03b8\u2032v (xi))\u2207\u03c6\u03b8\u2032 (xi) log f(a \u2032 i|\u03c6\u03b8\u2032(xi))\nk \u2190 \u2207\u03c6\u03b8\u2032 (xi)DKL [f(\u00b7|\u03c6\u03b8a(xi)\u2016f(\u00b7|\u03c6\u03b8\u2032(xi)]\nAccumulate gradients wrt \u03b8\u2032: d\u03b8\u2032 \u2190 d\u03b8\u2032 + \u2202\u03c6\u03b8\u2032 (xi) \u2202\u03b8\u2032\n( g \u2212max { 0, k\nT g\u2212\u03b4 \u2016k\u201622\n} k )\nAccumulate gradients wrt \u03b8\u2032v: d\u03b8v \u2190 d\u03b8v + (Qret \u2212 Q\u0303\u03b8\u2032v (xi, ai))\u2207\u03b8\u2032v Q\u0303\u03b8\u2032v (xi, ai) d\u03b8v \u2190 d\u03b8v + min {1, \u03c1i} ( Qret(xt, ai)\u2212 Q\u0303\u03b8\u2032v (xt, ai) ) \u2207\u03b8\u2032vV\u03b8\u2032v (xi)\nUpdate Retrace target: Qret \u2190 ci ( Qret \u2212Q\u03b8\u2032v (xi, ai) ) + V\u03b8\u2032v (xi)\nUpdate Retrace target: Qopc \u2190 ( Qopc \u2212Q\u03b8\u2032v (xi, ai) ) + V\u03b8\u2032v (xi)\nend for Perform asynchronous update of \u03b8 using d\u03b8 and of \u03b8v using d\u03b8v . Updating the average policy network: \u03b8a \u2190 \u03b1\u03b8a + (1\u2212 \u03b1)\u03b8\nB Q(\u03bb) WITH OFF-POLICY CORRECTIONS\nGiven a trajectory generated under the behavior policy \u00b5, theQ(\u03bb) with off-policy corrections estimator (Harutyunyan et al., 2016) can be expressed recursively as follows:\nQopc(xt, at) = rt + \u03b3[Q opc(xt+1, at+1)\u2212Q(xt+1, at+1)] + \u03b3V (xt+1). (21)\nNotice that Qopc(xt, at) is the same as Retrace with the exception that the truncated importance ratio is replaced with 1.\nBecause of the lack of the truncated importance ratio, the operator defined by Qopc is only a contraction if the target and behaviour policies are close to each other (Harutyunyan et al., 2016). Q(\u03bb) with off-policy corrections is therefore less stable compared to Retrace and unsafe for policy evaluation.\nQopc, however, could better utilize the returns as the traces are not cut by the truncated importance weights. As a result, Qopc could be used efficiently to estimate Q\u03c0 in policy gradient (e.g. in Equation (16)). In our continuous control experiments, we have found that Qopc leads to faster learning."}, {"heading": "C RETRACE AS TRUNCATED IMPORTANCE SAMPLING WITH BIAS CORRECTION", "text": "For the purpose of proving proposition 1, we assume our environment to be a Markov Decision Process (X ,A, \u03b3, P, r). We restrict X to be a finite state space. For notational simplicity, we also restrict A to be a finite action space. P : X ,A \u2192 X defines the state transition probabilities and r : X ,A \u2192 [\u2212RMAX, RMAX] defines a reward function. Finally, \u03b3 \u2208 [0, 1) is the discount factor.\nProof of proposition 1. First we show that B is a contraction operator.\n|BQ(x, a)\u2212Q\u03c0(x, a)|\n= \u2223\u2223\u2223\u2223\u2223\u2223E\u00b5 \u2211 t\u22650 \u03b3t ( t\u220f i=1 \u03c1\u0304i )( \u03b3 E b\u223c\u03c0 ([ \u03c1t+1(b)\u2212 c \u03c1t+1(b) ] + (Q(xt+1, b)\u2212Q\u03c0(xt+1, b)) ))\u2223\u2223\u2223\u2223\u2223\u2223\n\u2264 E\u00b5 \u2211 t\u22650 \u03b3t ( t\u220f i=1 \u03c1\u0304i )[ \u03b3 E b\u223c\u03c0 ([ \u03c1t+1(b)\u2212 c \u03c1t+1(b) ] + |Q(xt+1, b)\u2212Q\u03c0(xt+1, b)| )]\n\u2264 E\u00b5 \u2211 t\u22650 \u03b3t ( t\u220f i=1 \u03c1\u0304i )( \u03b3(1\u2212 P\u0304t+1) sup b |Q(xt+1, b)\u2212Q\u03c0(xt+1, b)| ) (22) Where P\u0304t+1 = 1\u2212 E\nb\u223c\u03c0\n[[ \u03c1t+1(b)\u2212c \u03c1t+1(b) ] + ] = E b\u223c\u00b5 [\u03c1\u0304t+1(b)]. The last inequality in the above equation is due to\nHo\u0308lder\u2019s inequality.\n(22) \u2264 sup x,b |Q(x, b)\u2212Q\u03c0(x, b)|E\u00b5 \u2211 t\u22650 \u03b3t ( t\u220f i=1 \u03c1\u0304i )( \u03b3(1\u2212 P\u0304t+1) ) = sup\nx,b |Q(x, b)\u2212Q\u03c0(x, b)|E\u00b5 \u03b3\u2211 t\u22650 \u03b3t ( t\u220f i=1 \u03c1\u0304i ) \u2212 \u2211 t\u22650 \u03b3t ( t\u220f i=1 \u03c1\u0304i )( \u03b3P\u0304t+1 ) = sup\nx,b |Q(x, b)\u2212Q\u03c0(x, b)|E\u00b5 \u03b3\u2211 t\u22650 \u03b3t ( t\u220f i=1 \u03c1\u0304i ) \u2212 \u2211 t\u22650 \u03b3t+1 ( t+1\u220f i=1 \u03c1\u0304i ) = sup\nx,b |Q(x, b)\u2212Q\u03c0(x, b)| (\u03b3C \u2212 (C \u2212 1))\nwhere C = \u2211 t\u22650 \u03b3 t (\u220ft i=1 \u03c1\u0304i ) . Since C \u2265 \u22110t=0 \u03b3t (\u220fti=1 \u03c1\u0304i) = 1, we have that \u03b3C \u2212 (C \u2212 1) \u2264 \u03b3. Therefore, we have shown that B is a contraction operator. Now we show that B is the same as Retrace. By apply the trunction and bias correction trick, we have\nE b\u223c\u03c0 [Q(xt+1, b)] = E b\u223c\u00b5 [\u03c1\u0304t+1(b)Q(xt+1, b)] + E b\u223c\u03c0 ([ \u03c1t+1(b)\u2212 c \u03c1t+1(b) ] + Q(xt+1, b) ) . (23)\nBy adding and subtracting the two sides of Equation (23) inside the summand of Equation (20), we have BQ(x, a) = E\u00b5 [\u2211 t\u22650 \u03b3t ( t\u220f i=1 \u03c1\u0304i )[ rt + \u03b3 E b\u223c\u03c0 ([ \u03c1t+1(b)\u2212 c \u03c1t+1(b) ] + Q(xt+1, b) ) +\u03b3 E b\u223c\u03c0 [Q(xt+1, b)]\n\u2212\u03b3 E b\u223c\u00b5 [\u03c1\u0304t+1(b)Q(xt+1, b)]\u2212 \u03b3 E b\u223c\u03c0 ([ \u03c1t+1(b)\u2212 c \u03c1t+1(b) ] + Q(xt+1, b) )]]\n= E\u00b5 \u2211 t\u22650 \u03b3t ( t\u220f i=1 \u03c1\u0304i )( rt + \u03b3 E b\u223c\u03c0 [Q(xt+1, b)]\u2212 \u03b3 E b\u223c\u00b5 [\u03c1\u0304t+1(b)Q(xt+1, b)] ) = E\u00b5\n\u2211 t\u22650 \u03b3t ( t\u220f i=1 \u03c1\u0304i )( rt + \u03b3 E b\u223c\u03c0 [Q(xt+1, b)]\u2212 \u03b3\u03c1\u0304t+1Q(xt+1, at+1) ) = E\u00b5\n\u2211 t\u22650 \u03b3t ( t\u220f i=1 \u03c1\u0304i )( rt + \u03b3 E b\u223c\u03c0 [Q(xt+1, b)]\u2212Q(xt, at) )+Q(x, a) = RQ(x, a)\nIn the remainder of this appendix, we show that B generalizes both the Bellman operator and importance sampling. First, we reproduce the definition of B:\nBQ(x, a) = E\u00b5 \u2211 t\u22650 \u03b3t ( t\u220f i=1 \u03c1\u0304i )( rt + \u03b3 E b\u223c\u03c0 ([ \u03c1t+1(b)\u2212 c \u03c1t+1(b) ] + Q(xt+1, b) )) . When c = 0, we have that \u03c1\u0304i = 0 \u2200i. Therefore only the first summand of the sum remains:\nBQ(x, a) = E\u00b5 [ rt + \u03b3 E\nb\u223c\u03c0 (Q(xt+1, b))\n] .\nIn this case B = T . When c =\u221e, the compensation term disappears and \u03c1\u0304i = \u03c1i \u2200i:\nBQ(x, a) = E\u00b5 \u2211 t\u22650 \u03b3t ( t\u220f i=1 \u03c1i )( rt + \u03b3 E b\u223c\u03c0 (0\u00d7Q(xt+1, b)) ) = E\u00b5 \u2211 t\u22650 \u03b3t ( t\u220f i=1 \u03c1i ) rt  . In this case B is the same operator defined by importance sampling.\nD DERIVATION OF V target\nBy using the truncation and bias correction trick, we can derive the following:\nV \u03c0(xt) = E a\u223c\u00b5\n[ min { 1, \u03c0(a|xt) \u00b5(a|xt) } Q\u03c0(xt, a) ] + E a\u223c\u03c0 ([ \u03c1t(a)\u2212 1 \u03c1t(a) ] + Q\u03c0(xt+1, a) ) .\nWe, however, cannot use the above equation as a target as we do not have access to Q\u03c0 . To derive a target, we can take a Monte Carlo approximation of the first expectation in the RHS of the above equation and replace the first occurrence of Q\u03c0 with Qret and the second with our current neural net approximation Q\u03b8v (xt, \u00b7):\nV targetpre (xt) := min { 1, \u03c0(at|xt) \u00b5(at|xt) } Qret(xt, at) + E a\u223c\u03c0 ([ \u03c1t(a)\u2212 1 \u03c1t(a) ] + Q\u03b8v (xt, a) ) . (24)\nThrough the truncation and bias correction trick again, we have the following identity:\nE a\u223c\u03c0 [Q\u03b8v (xt, a)] = E a\u223c\u00b5\n[ min { 1, \u03c0(a|xt) \u00b5(a|xt) } Q\u03b8v (xt, a) ] + E a\u223c\u03c0 ([ \u03c1t(a)\u2212 1 \u03c1t(a) ] + Q\u03b8v (xt, a) ) . (25)\nAdding and subtracting both sides of Equation (25) to the RHS of (24) while taking a Monte Carlo approximation, we arrive at V target(xt):\nV target(xt) := min { 1, \u03c0(at|xt) \u00b5(at|xt) }( Qret(xt, at)\u2212Q\u03b8v (xt, at) ) + V\u03b8v (xt)."}, {"heading": "E CONTINUOUS CONTROL EXPERIMENTS", "text": "E.1 DESCRIPTION OF THE CONTINUOUS CONTROL PROBLEMS\nOur continuous control tasks were simulated using the MuJoCo physics engine (Todorov et al. (2012)). For all experiments we considered an episodic setup with an episode length of T = 500 steps and a discount factor of 0.99.\nCartpole swingup This is an instance of the classic cart-pole swing-up task. It consists of a pole attached to a cart running on a finite track. The agent is required to balance the pole near the center of the track by applying a force to the cart only. An episode starts with the pole at a random angle and zero velocity. A reward zero is given except when the pole is approximately upright (within \u00b15 deg) and the track approximately in the center of the track (\u00b10.05) for a track length of 2.4. The observations include position and velocity of the cart, angle and angular velocity of the pole. a sine/cosine of the angle, the position of the tip of the pole, and Cartesian velocities of the pole. The dimension of the action space is 1.\nReacher3 The agent needs to control a planar 3-link robotic arm in order to minimize the distance between the end effector of the arm and a target. Both arm and target position are chosen randomly at the beginning of each episode. The reward is zero except when the tip of the arm is within 0.05 of the target, where it is one. The 8-dimensional observation consists of the angles and angular velocity of all joints as well as the displacement between target and the end effector of the arm. The 3-dimensional action are the torques applied to the joints.\nCheetah The Half-Cheetah (Wawrzyn\u0301ski (2009); Heess et al. (2015)) is a planar locomotion task where the agent is required to control a 9-DoF cheetah-like body (in the vertical plane) to move in the direction of the x-axis as quickly as possible. The reward is given by the velocity along the x-axis and a control cost: r = vx + 0.1\u2016a\u20162. The observation vector consists of the z-position of the torso and its x, z velocities as well as the joint angles and angular velocities. The action dimension is 6.\nFish The goal of this task is to control a 13-DoF fish-like body to swim to a random target in 3D space. The reward is given by the distance between the head of the fish and the target, a small penalty for the body not being upright, and a control cost. At the beginning of an episode the fish is initialized facing in a random direction relative to the target. The 24-dimensional observation is given by the displacement between the fish and the target projected onto the torso coordinate frame, the joint angles and velocities, the cosine of the angle between the z-axis of the torso and the world z-axis, and the velocities of the torso in the torso coordinate frame. The 5-dimensional actions control the position of the side fins and the tail.\nWalker The 9-DoF planar walker is inspired by (Schulman et al. (2015a)) and is required to move forward along the x-axis as quickly as possible without falling. The reward consists of the x-velocity of the torso, a\nquadratic control cost, and terms that penalize deviations of the torso from the preferred height and orientation (i.e. terms that encourage the walker to stay standing and upright). The 24-dimensional observation includes the torso height, velocities of all DoFs, as well as sines and cosines of all body orientations in the x-z plane. The 6-dimensional action controls the torques applied at the joints. Episodes are terminated early with a negative reward when the torso exceeds upper and lower limits on its height and orientation.\nHumanoid The humanoid is a 27 degrees-of-freedom body with 21 actuators (21 action dimensions). It is initialized lying on the ground in a random configuration and the task requires it to achieve a standing position. The reward function penalizes deviations from the height of the head when standing, and includes additional terms that encourage upright standing, as well as a quadratic action penalty. The 94 dimensional observation contains information about joint angles and velocities and several derived features reflecting the body\u2019s pose.\nE.2 UPDATE EQUATIONS OF THE BASELINE TIS\nThe baseline TIS follows the following update equations,\nupdates to the policy: min { 5, ( k\u22121\u220f i=0 \u03c1t+i )}[ k\u22121\u2211 i=0 \u03b3irt+i + \u03b3 kV\u03b8v (xk+t)\u2212 V\u03b8v (xt) ] \u2207\u03b8 log \u03c0\u03b8(at|xt),\nupdates to the value: min { 5, ( k\u22121\u220f i=0 \u03c1t+i )}[ k\u22121\u2211 i=0 \u03b3irt+i + \u03b3 kV\u03b8v (xk+t)\u2212 V\u03b8v (xt) ] \u2207\u03b8vV\u03b8v (xt).\nThe baseline Trust-TIS is appropriately modified according to the trust region update described in Section 3.3.\nE.3 SENSITIVITY ANALYSIS\nIn this section, we assess the sensitivity of ACER to hyper-parameters. In Figures 3 and 4, we show, for each game, the final performance of our ACER agent versus the choice of learning rates, and the trust region constraint \u03b4 respectively.\nNote, as we are doing random hyper-parameter search, each learning rate is associated with a random \u03b4 and vice versa. It is therefore difficult to tease out the effect of either hyper-parameter independently. We observe, however, that ACER is not very sensitive to the hyper-parameters overall. In addition, smaller \u03b4\u2019s do not seem to adversely affect the final performance while larger \u03b4\u2019s do in domains of higher action dimensionality. Similarly, smaller learning rates perform well while bigger learning rates tend to hurt final performance in domains of higher action dimensionality."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>This paper presents an actor-critic deep reinforcement learning agent with experience replay<lb>that is stable, sample efficient, and performs remarkably well on challenging environments,<lb>including the discrete 57-game Atari domain and several continuous control problems.<lb>To achieve this, the paper introduces several innovations, including truncated importance<lb>sampling with bias correction, stochastic dueling network architectures, and a new trust<lb>region policy optimization method.", "creator": "LaTeX with hyperref package"}}}