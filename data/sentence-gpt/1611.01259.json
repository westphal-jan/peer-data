{"id": "1611.01259", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Generalized Topic Modeling", "abstract": "Recently there has been significant activity in developing algorithms with provable guarantees for topic modeling. In standard topic models, a topic (such as sports, business, or politics) is viewed as a probability distribution $\\vec a_i$ over words, and a document is generated by first selecting a mixture $\\vec w$ over topics, and then generating words i.i.x$ and a document with the same type of type of guarantee in the same condition. For example, the following example is a simple example of the topic model and a function of two variables. A function of two variables is a function of the sum of two pairs of variables in one variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable. The same function of two variables is a function of a variable in another variable and two pairs of variables in another variable and two pairs of variables in another variable. In the example, a function of two variables is a function of a variable in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable. One parameter of the sum of two pairs in the following example (i.e. sum of two pairs in one variable and two pairs in another variable) is a function of a variable in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable and two pairs of variables in another variable", "histories": [["v1", "Fri, 4 Nov 2016 03:45:03 GMT  (1119kb,D)", "http://arxiv.org/abs/1611.01259v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.DS cs.IR", "authors": ["avrim blum", "nika haghtalab"], "accepted": false, "id": "1611.01259"}, "pdf": {"name": "1611.01259.pdf", "metadata": {"source": "CRF", "title": "Generalized Topic Modeling", "authors": ["Avrim Blum", "Nika Haghtalab"], "emails": ["avrim@cs.cmu.edu", "nhaghtal@cs.cmu.edu"], "sections": [{"heading": null, "text": "In this work we consider a broad generalization of this framework in which words are no longer assumed to be drawn i.i.d. and instead a topic is a complex distribution over sequences of paragraphs. Since one could not hope to even represent such a distribution in general (even if paragraphs are given using some natural feature representation), we aim instead to directly learn a document classifier. That is, we aim to learn a predictor that given a new document, accurately predicts its topic mixture, without learning the distributions explicitly. We present several natural conditions under which one can do this efficiently and discuss issues such as noise tolerance and sample complexity in this model. More generally, our model can be viewed as a generalization of the multi-view or co-training setting in machine learning.\n\u2217Supported in part by National Science Foundation grants CCF-1525971 and CCF-1535967. \u2020Supported in part by National Science Foundation grant CCF-1525971 and by a Microsoft Research Graduate Fellowship and\nan IBM Ph.D Fellowship.\nar X\niv :1\n61 1.\n01 25\n9v 1\n[ cs\n.L G\n] 4\nN ov\n2 01"}, {"heading": "1 Introduction", "text": "Topic modeling is an area with significant recent work in the intersection of algorithms and machine learning (Arora et al., 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures.\nAlgorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999).\nAs a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution. Better would be a model in which sentences are drawn i.i.d. from a distribution over sentences (this would at least produce grammatical objects and allow for meaningful correlation among related words within a topic, like shooting a free throw or kicking a field goal). Even better would be paragraphs drawn i.i.d. from a distribution over paragraphs (this would at least produce coherent paragraphs). Or, even better, how about a model in which paragraphs are drawn non-independently, so that the second paragraph in a document can depend on what the first paragraph was saying, though presumably with some amount of additional entropy as well? This is the type of model we study here.\nNote that an immediate problem with considering such a model is that now the task of learning an explicit distribution (over sentences or paragraphs) is hopeless. While a distribution over words can be reasonably viewed as a probability vector, one could not hope to learn or even represent an explicit distribution over sentences or paragraphs. Indeed, except in cases of plagiarism, one would not expect to see the same paragraph twice in the entire corpus. Moreover, this is likely to be true even if we assume paragraphs have some natural feature-vector representation. Instead, we bypass this issue by aiming to directly learn a predictor for documents\u2014that is, a function that given a document, predicts its mixture over topics\u2014without explicitly learning topic distributions. Another way to think of this is that our goal is not to learn a model that could be used to write a new document, but instead just a model that could be used to classify a document written by others. This is much as in standard supervised learning where algorithms such as SVMs learn a decision boundary (such as a linear separator) for making predictions on the labels of examples without explicitly learning the distributions D+ and D\u2212 over positive and negative examples respectively. However, our setting is unsupervised (we are not given labeled data containing the correct classifications of the documents in the training set) and furthermore, rather than each data item belonging to one of the k classes (topics), each data item belongs to a mixture of the k topics. Our goal is given a new data item to output what that mixture is.\nWe begin by describing our high level theoretical formulation. This formulation can be viewed as a generalization both of standard topic modeling and of a setting known as multi-view learning or co-training Blum and Mitchell (1998); Dasgupta et al. (2002); Chapelle et al. (2010); Balcan et al. (2004); Sun (2013). We then describe several natural assumptions under which we can indeed efficiently solve the problem, learning accurate topic mixture predictors."}, {"heading": "2 Preliminaries", "text": "We assume that paragraphs are described by n real-valued features and so can be viewed as points x in an instance space X \u2286 Rn. We assume that each document consists of at least two paragraphs and denote it\nby (x1,x2). Furthermore, we consider k topics and partial membership functions f1, . . . , fk : X \u2192 [0, 1], such that fi(x) determines the degree to which paragraph x belongs to topic i, and, \u2211k i=1 fi(x) = 1. For any vector of probabilities w \u2208 Rk \u2014 which we sometimes refer to as mixture weights \u2014 we define Xw = {x \u2208 Rn | \u2200i, fi(x) = wi} to be the set of all paragraphs with partial membership values w. We assume that both paragraphs of a document have the same partial membership values, that is (x1,x2) \u2208 \u22c3 w Xw \u00d7Xw, although we also allow some noise later on. To better relate to the literature on multi-view learning, we will also refer to topics as \u201cclasses\u201d and refer to paragraphs as \u201cviews\u201d of the document.\nMuch like the standard topic models, we consider an unlabeled sample set that is generated by a twostep process. First, we consider a distribution P over vectors of mixture weights and draw w according to P . Then we consider distribution Dw over the set Xw \u00d7 Xw and draw a document (x1,x2) according to Dw. We consider two settings. In the first setting, which is addressed in Section 3, the learner receives the instance (x1,x2). In the second setting, the learner receives samples (x\u03021, x\u03022) that have been perturbed by some noise. We discuss two noise models in Sections 4 and 5.2. In both cases, the goal of the learner is to recover the partial membership functions fi.\nMore specifically, in this work we consider partial membership functions of the form fi(x) = f(vi \u00b7x), where v1, . . . ,vk \u2208 Rn are linearly independent and f : R \u2192 [0, 1] is a monotonic function. For the majority of this work, we consider f to be the identity function, so that fi(x) = vi \u00b7 x. Define ai \u2208 span{v1, . . . ,vk} such that vi\u00b7ai = 1 and vj \u00b7ai = 0 for all j 6= i. That is, ai can be viewed as the projection of a paragraph that is purely of topic i onto the span of v1, . . . ,vk. Define \u2206 = CH({a1, . . . ,ak}) to be the convex hull of a1, . . . ,ak.\nThroughout this work, we use \u2016 \u00b7 \u20162 to denote the spectral norm of a matrix or the L2 norm of a vector. When it is clear from the context, we simply use \u2016 \u00b7 \u2016 to denote these quantities. We denote by Br(x) the ball of radius r around x. For any matrix M , we use M+ to denote the pseudoinverse of M .\nGeneralization of Standard Topic Modeling Let us briefly discuss how the above model is a generalization of the standard topic modeling framework. In the standard framework, a topic is modeled as a probability distribution over n words, expressed as a vector ai \u2208 [0, 1]n, where aij is the probability of word j in topic i. A document is generated by first selecting a mixture w \u2208 [0, 1]k over k topics, and then choosing words i.i.d. from the associated mixture distribution\u2211k\ni=1wiai. The document vector x\u0302 is then the vector of word counts, normalized by dividing by the number of words in the document so that the L1 norm of x\u0302 is 1.\nAs a thought experiment, consider infinitely long documents. In the standard framework, all infinitely long documents of a mixture weight w have the same representation x = \u2211k i=1wiai. This representation implies x\u00b7vi = wi for all i \u2208 [k], where V = (v1, . . . ,vk) is the pseudo-inverse of matrixA = (a1, . . . ,ak). Thus, by partitioning the document into two halves (views) x1 and x2, our noise-free model with fi(x) = vi \u00b7 x generalizes the standard topic model for long documents. However, our model is substantially more general: features within a view can be arbitrarily correlated, the views themselves can be correlated with each other, and even in the zero-noise case, documents of the same mixture can look very different so long as they have the same projection to the span of the a1, . . . ,ak.\nFor a shorter document x\u0302, each feature x\u0302i is drawn according to a distribution with mean xi, where x = \u2211k i=1wiai. Therefore, x\u0302 can be thought of as a noisy measurement of x. The fewer the words in a document, the larger is the noise in x\u0302. Existing work in topic modeling, such as Arora et al. (2012b); Anandkumar et al. (2014), provide elegant procedures for handling large noise that is caused by drawing only 2 or 3 words according to the distribution induced by x. As we show in Section 4, our method can also tolerate large amounts of noise under some conditions. While our method cannot deal with documents that are only 2- or 3-words long, the benefit is a model that is much more general in many other respects."}, {"heading": "3 An Easier Case with Simplifying Assumptions", "text": "We make two main simplifying assumptions in this section, both of which will be relaxed in Section 4: 1) The documents are not noisy, i.e., x1 \u00b7 vi = x2 \u00b7 vi; 2) There is non-negligible probability density on instances that belong purely to one class. In this section we demonstrate ideas and techniques, which we will develop further in the next section, to learn the topic vectors from a corpus of unlabeled documents.\nThe Setting: We make the following assumptions. The documents are not noisy, that is for any document (x1,x2) and for all i \u2208 [k], x1 \u00b7 vi = x2 \u00b7 vi. Regarding distribution P , we assume that a non-negligible probability density is assigned to pure samples for each class. More formally, for some \u03be > 0, for all i \u2208 [k], Prw\u223cP [w = ei] \u2265 \u03be. Regarding distribution Dw, we allow the two paragraphs in a document, i.e., the two views (x1,x2) drawn from Dw, to be correlated as long as for any subspace Z \u2282 null{v1 . . . ,vk} of dimension strictly less than n\u2212 k, Pr(x1,x2)\u223cDw [(x1 \u2212 x2) 6\u2208 Z] \u2265 \u03b6 for some non-negligible \u03b6. One way to view this in the context of topic modeling is that if, say, \u201csports\u201d is a topic, then it should not be the case that the second paragraph always talks about the exact same sport as the first paragraph; else \u201csports\u201d would really be a union of several separate but closely-related topics. Thus, while we do not require independence we do require some non-correlation between the paragraphs.\nAlgorithm and Analysis: The main idea behind our approach is to use the consistency of the two views of the samples to first recover the subspace spanned by v1, . . . ,vk (Phase 1). Once this subspace is recovered, we show that a projection of a sample on this space corresponds to the convex combination of class vectors using the appropriate mixture weight that was used for that sample. Therefore, we find vectors a1, . . . ,ak that purely belong to each class by taking the extreme points of the projected samples (Phase 2). The class vectors v1, . . . ,vk are the unique vectors (up to permutations) that classify a1, . . . ,ak as pure samples. Phase 2 is similar to that of Arora et al. (2012b). Algorithm 1 formalizes the details of this approach.\nAlgorithm 1 ALGORITHM FOR GENERALIZED TOPIC MODELS \u2014 NO NOISE Input: A sample set S = {(x1i ,x2i ) | i \u2208 [m]} such that for each i, first a vector w is drawn from P and then (x1i ,x 2 i ) is drawn from Dw. Phase 1: 1. Let X1 and X2 be matrices where the ith column is x1i and x 2 i , respectively.\n2. Let P be the projection matrix on the last k left singular vectors of (X1 \u2212X2). Phase 2:\n1. Let S = {Pxji | i \u2208 [m], j \u2208 {1, 2}}. 2. Let A be a matrix whose columns are the extreme points of the convex hull of S . (This can be found\nusing farthest traversal or linear programming.) Output: Return columns of A+ as v1, . . . ,vk.\nIn Phase 1 for recovering span{v1, . . . ,vk}, note that for any sample (x1,x2) drawn fromDw, we have that vi \u00b7 x1 = vi \u00b7 x2 = wi. Therefore, regardless of what w was used to produce the sample, we have that vi \u00b7 (x1 \u2212 x2) = 0 for all i \u2208 [k]. That is, v1, . . . ,vk are in the null-space of all such (x1 \u2212 x2). So, if samples (x1i \u2212 x2i ) span a n\u2212 k dimensional subspace, then span{v1, . . . ,vk} can be recovered by taking null{(x1 \u2212 x2) | (x1,x2) \u2208 Xw \u00d7 Xw, \u2200w \u2208 Rk}. Using singular value decomposition, this null space is spanned by the last k singular vectors of X1 \u2212X2, where X1 and X2 are matrices with columns x1i and x2i , respectively.\nThis is where the assumptions on Dw come into play. By assumption, for any strict subspace Z of span{(x1 \u2212 x2) | (x1,x2) \u2208 Xw \u00d7 Xw, \u2200w \u2208 Rk}, Dw has non-negligible probability on instances (x1\u2212x2) /\u2208 Z. Therefore, after seeing sufficiently many samples we can recover the space of all (x1\u2212x2). The next lemma, whose proof appears in Appendix A.1, formalizes this discussion.\nLemma 3.1. Let Z = span{(x1i \u2212 x2i ) | i \u2208 [m]}. Then, m = O(n\u2212k\u03b6 log( 1 \u03b4 )) is sufficient such that with probability 1\u2212 \u03b4, rank(Z) = n\u2212 k.\nUsing Lemma 3.1, Phase 1 of Algorithm 1 recovers span{v1, . . . ,vk}. Next, we show that pure samples are the extreme points of the convex hull of all samples when projected on the subspace span{v1, . . . ,vk}. Figure 1 demonstrates the relation between the class vectors, vi, projection of samples, and the projection of pure samples ai. The next lemma, whose proof appears in Appendix A.2, formalizes this claim.\nLemma 3.2. For any x, let x represent the projection of x on span{v1, . . . ,vk}. Then, x = \u2211 i\u2208[k](vi \u00b7 x)ai.\nWith \u2211\ni\u2208[k](vi\u00b7x)ai representing the projection of x on span{v1, . . . ,vk}, it is clear that the extreme points of the set of all projected instances that belong to Xw for all w are a1, . . . ,ak. Since in a large enough sample set, with high probability for all i \u2208 [k], there is a pure sample of type i, taking the extreme points of the set of projected samples is also a1, . . . ,ak. The following lemma, whose proof appears in Appendix A.3, formalizes this discussion.\nLemma 3.3. Let m = c(1\u03be log( k \u03b4 )) for a large enough constant c > 0. Let P be the projection matrix for span{v1, . . . ,vk} and S = {Pxji | i \u2208 [m], j \u2208 {1, 2}} be the set of projected samples. With probability 1\u2212 \u03b4, {a1, . . . ,ak} is the set of extreme points of CH(S ).\nTherefore, a1, . . . ,ak can be learned by taking the extreme points of the convex hull of all samples projected on span({v1, . . . ,vk}). Furthermore, V = A+ is unique, therefore v1, . . . ,vk can be easily found by taking the pseudoinverse of matrix A. Together with Lemma 3.1 and 3.3 this proves the next theorem regarding learning class vectors in the absence of noise.\nTheorem 3.4 (No Noise). There is a polynomial time algorithm for which m = O ( n\u2212k \u03b6 ln( 1 \u03b4 ) + 1 \u03be ln( k \u03b4 ) ) is sufficient to recover vi exactly for all i \u2208 [k], with probability 1\u2212 \u03b4."}, {"heading": "4 Relaxing the Assumptions", "text": "In this section, we relax the two main simplifying assumptions from Section 3. We relax the assumption on non-noisy documents and allow a large fraction of the documents to not satisfy vi \u00b7 x1 = vi \u00b7 x2. In the standard topic model, this corresponds to having a large fraction of short documents. Furthermore, we relax the assumption on the existence of pure documents to an assumption on the existence of \u201calmostpure\u201d documents. We further develop the approach discussed in the previous section and introduce efficient algorithms that approximately recover the topic vectors in this setting. The Setting: We assume that any sampled document has a non-negligible probability of being non-noisy and with the remaining probability, the two views of the document are perturbed by additive Gaussian noise, independently. More formally, for a given sample (x1,x2), with probability p0 > 0 the algorithm receives (x1,x2) and with the remaining probability 1\u2212 p0, the algorithm receives (x\u03021, x\u03022), such that x\u0302j = xj + ej , where ej \u223c N (0, \u03c32In).\nWe assume that for each topic the probability that a document is mostly about that topic is non-negligible. More formally, for any topic i \u2208 [k], Prw\u223cP [\u2016ei \u2212w\u20161 \u2264 \u2016] \u2265 g( ), where g is a polynomial function of its input. A stronger form of this assumption, better known as the dominant admixture assumption, assumes that every document is mostly about one topic and has been empirically shown to hold on several real world\ndata sets (Bansal et al., 2014). Furthermore, in the Latent Dirichlet Allocation model, Prw\u223cP [maxi\u2208[k]wi \u2265 1\u2212 ] \u2265 O( 2) for typical values of the concentration parameter.\nWe also make mild assumptions on the distribution over instances. We assume that the covariance of the distribution over (x1i \u2212 x2i )(x1i \u2212 x2i )> is significantly larger than the noise covariance \u03c32. That is, for some \u03b40 > 0, the least significant non-zero eigen value of E(x1i ,x2i )[(x 1 i \u2212 x2i )(x1i \u2212 x2i )>], equivalently its (n\u2212 k)th eigen value, is greater than 6\u03c32 + \u03b40. At a high level, these assumptions are necessary, because if \u2016x1i \u2212 x2i \u2016 is too small compared to \u2016x1i \u2016 and \u2016x2i \u2016, then even a small amount of noise affects the structure present in x1i \u2212 x2i completely. Moreover, we assume that the L2 norm of each view of a sample is bounded by some M > 0. We also assume that for all i \u2208 [k], \u2016ai\u2016 \u2264 \u03b1 for some \u03b1 > 0. At a high level, \u2016ai\u2016s are inversely proportional to the non-zero singular values of V = (v1, . . . ,vk). Therefore, \u2016ai\u2016 \u2264 \u03b1 implies that the k topic vectors are sufficiently different.\nAlgorithm and Results: Our approach follows the general theme of the previous section: First, recover span{v1, . . . ,vk} and then recover a1, . . . ,ak by taking the extreme points of the projected samples. In this case, in the first phase we recover span{v1, . . . ,vk} approximately, by finding a projection matrix P\u0302 such that \u2016P \u2212 P\u0302\u2016 \u2264 for an arbitrarily small , where P is the projection matrix on span{v1, . . . ,vk}. At this point in the algorithm, the projection of samples on P\u0302 can include points that are arbitrarily far from \u2206. This is due to the fact that the noisy samples are perturbed by N (0, \u03c32In), so, for large values of \u03c3 some noisy samples map to points that are quite far from \u2206. Therefore, we have to detect and remove these samples before continuing to the second phase. For this purpose, we show that the low density regions of the projected samples can safely be removed such that the convex hull of the remaining points is close to \u2206. In the second phase, we consider projections of each sample using P\u0302 . To approximately recover a1, . . . ,ak, we recover samples, x, that are far from the convex hull of the remaining points, when x and a ball of points close to it are removed. We then show that such points are close to one of the pure class vectors, ai. Algorithm 2 and the details of the above approach and its performance are as follows.\nAlgorithm 2 ALGORITHM FOR GENERALIZED TOPIC MODELS \u2014 WITH NOISE Input: A sample set {(x\u03021i , x\u03022i ) | i \u2208 [m]} such that for each i, first a vector w is drawn from P , then (x1i ,x2i ) is drawn from Dw, then with probability p0, x\u0302ji = x j i , else with probability 1\u2212 p0, x\u0302 j i = x j i +N (0, \u03c32In) for i \u2208 [m] and j \u2208 {1, 2}. Phase 1:\n1. Take m1 = \u2126 ( n\u2212k \u03b6 ln( 1 \u03b4 ) + n\u03c34r2M2\n\u03b420 2 ln(\n1 \u03b4 ) +\nn\u03c32M4r2\n\u03b420 2 polylog(\nnrM \u03b4 ) +\nM4\n\u03b420 ln(n\u03b4 )\n) samples.\n2. Let X\u03021 and X\u03022 be matrices where the ith column is x\u03021i and x\u0302 2 i , respectively.\n3. Let P\u0302 be the projection matrix on the last k left singular vectors of X\u03021 \u2212 X\u03022. Denoising Phase:\n4. Let \u2032 = 8r and \u03b3 = g ( \u2032 8k\u03b1 ) .\n5. Take m2 = \u2126 (\nk p0\u03b3 ln 1\u03b4\n) fresh samples1 and let S\u0302 = { P\u0302 x\u03021i | \u2200i \u2208 [m2] } .\n6. Remove x\u0302 from S\u0302 , for which there are less than p0\u03b3m2/2 points within distance of \u2032\n2 in S\u0302 . Phase 2:\n6. For all x\u0302 in S\u0302 , if dist(x ,CH(S\u0302 \\B6r \u2032(x\u0302)) \u2265 2 \u2032 add x\u0302 to C. 7. Cluster C using single linkage with threshold 16r \u2032. Assign any point from cluster i as a\u0302i.\nOutput: Return a\u03021, . . . , a\u0302k.\n1For the denoising step, we use a fresh set of samples that were not used for learning the projection matrix. This guarantees that the noise distribution in the projected samples remain a Gaussian.\nTheorem 4.1. Consider any , \u03b4 > 0 such that \u2264 O ( r\u03c3 \u221a k )\n, where r is a parameter that depends on the geometry of the simplex a1, . . . ,ak and will be defined later. There is an efficient algorithm for which an unlabeled sample set of size\nm = O ( n\u2212 k \u03b6 ln( 1 \u03b4 ) + n\u03c34r2M2\n\u03b420 2\nln( 1\n\u03b4 ) +\nn\u03c32M4r2\n\u03b420 2\npolylog( nrM\n\u03b4 ) +\nM4\n\u03b420 ln(\nn \u03b4 ) + k ln(1/\u03b4)\np0 g( /(kr\u03b1)) ) is sufficient to recover a\u0302i such that \u2016a\u0302i \u2212 ai\u20162 \u2264 for all i \u2208 [k], with probability 1\u2212 \u03b4.\nThe proof of Theorem 4.1 involves the next three lemmas on the performance of the phases of the above algorithm. We formally state these two lemmas here, but defer their proofs to Sections 4.1, 4.2 and 4.3.\nLemma 4.2 (Phase 1). For any \u03c3 > 0 and > 0, an unlabeled sample set of size\nm = O ( n\u2212 k \u03b6 ln( 1 \u03b4 ) + n\u03c34\n\u03b420 2\nln( 1\n\u03b4 ) +\nn\u03c32M2\n\u03b420 2\npolylog( n\n\u03b4 ) +\nM4\n\u03b420 ln(\nn \u03b4 )\n) .\nis sufficient, such that with probability 1 \u2212 \u03b4, Phase 1 of Algorithm 2 returns a projection matrix P\u0302 , such that \u2016P \u2212 P\u0302\u20162 \u2264 .\nLemma 4.3 (Denoising). Let \u2032 \u2264 13\u03c3 \u221a k, \u2016P \u2212 P\u0302\u2016 \u2264 \u2032/8M , and \u03b3 = g\n( \u2032\n8k\u03b1\n) . An unlabeled sample\nsize of m = O (\nk p0\u03b3 ln(1\u03b4 ) ) is sufficient such that for S\u0302 defined in Step 6 of Algorithm 2 the following holds\nwith probability 1\u2212 \u03b4: For any x \u2208 S\u0302 , dist(x,\u2206) \u2264 \u2032, and, for all i \u2208 [k], there exists a\u0302i \u2208 S\u0302 such that \u2016a\u0302i \u2212 ai\u2016 \u2264 \u2032.\nLemma 4.4 (Phase 2). Let S\u0302 be a set of points for which the conclusion of Lemma 4.3 holds with the value of \u2032 = /8r. Then, Phase 2 of Algorithm 2 returns a\u03021, . . . , a\u0302k such that for all i \u2208 [k], \u2016ai \u2212 a\u0302i\u2016 \u2264 .\nWe now prove our main Theorem 4.1 by directly leveraging the three lemmas we just stated.\nProof of Theorem 4.1. By Lemma 4.2, sample set of size m1 is sufficient such that Phase 1 of Algorithm 2 leads to \u2016P \u2212 P\u0302\u2016 \u2264 32Mr , with probability 1 \u2212 \u03b4/2. Let\n\u2032 = 8r and take a fresh sample of size m2. By Lemma 4.3, with probability 1 \u2212 \u03b4/2, for any x \u2208 S\u0302 , dist(x,\u2206) \u2264 \u2032, and, for all i \u2208 [k], there exists a\u0302i \u2208 S\u0302 such that \u2016a\u0302i\u2212 ai\u2016 \u2264 \u2032. Finally, applying Lemma 4.4 we have that Phase 2 of Algorithm 2 returns a\u0302i, such that for all i \u2208 [k], \u2016ai \u2212 a\u0302i\u2016 \u2264 .\nTheorem 4.1 discusses the approximation of ai for all i \u2208 [k]. It is not hard to see that such an approximation also translates to the approximation of class vectors, vi for all i \u2208 [k]. That is, using the properties of perturbation of pseudoinverse matrices (see Proposition B.5) one can show that \u2016A\u0302+\u2212 V \u2016 \u2264 O(\u2016A\u0302\u2212A\u2016). Therefore, V\u0302 = A\u0302+ is a good approximation for V ."}, {"heading": "4.1 Proof of Lemma 4.2 \u2014 Phase 1", "text": "For j \u2208 {1, 2}, let Xj and X\u0302j be n \u00d7 m matrices with the ith column being xji and x\u0302 j i , respectively. As we demonstrated in Lemma 3.1, with high probability rank(X1 \u2212 X2) = n \u2212 k. Note that the nullspace of columns of X1 \u2212 X2 is spanned by the left singular vectors of X1 \u2212 X2 that correspond to its k zero singular values. Similarly, consider the space spanned by the k least left singular vectors of X\u03021 \u2212 X\u03022. We show that the nullspace of columns of X1 \u2212X2 can be approximated within any desirable accuracy by the space spanned by the k least left singular vectors of X\u03021\u2212 X\u03022, given a sufficiently large number of samples.\nLet D = X1 \u2212 X2 and D\u0302 = X\u03021 \u2212 X\u03022. For ease of exposition, assume that all samples are perturbed by Gaussian noise N (0, \u03c32In).2 Since each view of a sample is perturbed by an independent draw from a Gaussian noise distribution, we can view D\u0302 = D + E, where each column of E is drawn i.i.d from distribution N (0, 2\u03c32In). Then, 1mD\u0302D\u0302 > = 1mDD > + 1mDE > + 1mED > + 1mEE\n>. As a thought experiment, consider this equation in expectation. Since E[ 1mEE\n>] = 2\u03c32In is the covariance matrix of the noise and E[DE> + ED>] = 0, we have\n1 m E [ D\u0302D\u0302> ] \u2212 2\u03c32In = 1 m E [ DD> ] . (1)\nMoreover, the eigen vectors and their order are the same in 1mE[D\u0302D\u0302 >] and 1mE[D\u0302D\u0302 >]\u2212 2\u03c32In. Therefore, one can recover the nullspace of 1mE[DD >] by taking the space of the least k eigen vectors of 1mE[D\u0302D\u0302 >]. Next, we show how to recover the nullspace using D\u0302D\u0302>, rather than E[D\u0302D\u0302>]. Assume that the following properties hold:\n1. Equation 1 holds not only in expectation, but also with high probability. That is, with high probability, \u2016 1mD\u0302D\u0302 > \u2212 2\u03c32In \u2212 1mDD >\u20162 \u2264 . 2. With high probability \u03bbn\u2212k( 1mD\u0302D\u0302 >) > 4\u03c32 + \u03b40/2, where \u03bbi(\u00b7) denotes the ith most significant\neigen value. Let D = U\u03a3V > and D\u0302 = U\u0302 \u03a3\u0302V\u0302 > be SVD representations. We have that 1mD\u0302D\u0302 > \u2212 2\u03c32In = U\u0302( 1m \u03a3\u0302 2 \u2212 2\u03c32In)U\u0302 >. By property 2, \u03bbn\u2212k( 1m \u03a3\u0302\n2) > 4\u03c32 + \u03b40/2. That is, the eigen vectors and their order are the same in 1mD\u0302D\u0302 > \u2212 2\u03c32In and 1mD\u0302D\u0302 >. As a result the projection matrix, P\u0302 , on the least k eigen vectors of 1 mD\u0302D\u0302 >, is the same as the projection matrix, Q, on the least k eigen vectors of 1mD\u0302D\u0302 > \u2212 2\u03c32In.\nRecall that P\u0302 and P andQ are the projection matrices on the least significant k eigen vectors of 1mD\u0302D\u0302 >,\n1 mDD >, and 1mD\u0302D\u0302 > \u2212 2\u03c32I , respectively. As we discussed, P\u0302 = Q. Now, using the Davis and Kahan (1970) or Wedin (1972) sin \u03b8 theorem (see Proposition B.1) from matrix perturbation theory, we have,\n\u2016P \u2212 P\u0302\u20162 = \u2016P \u2212Q\u2016 \u2264 \u2016 1mD\u0302D\u0302 > \u2212 2\u03c32In \u2212 1mDD >\u20162\u2223\u2223\u2223\u03bbn\u2212k( 1mD\u0302D\u0302>)\u2212 2\u03c32 \u2212 \u03bbn\u2212k+1( 1mDD>)\u2223\u2223\u2223 \u2264 2 \u03b40\nwhere we use Properties 1 and 2 and the fact that \u03bbn\u2212k+1( 1mDD >) = 0, in the last transition."}, {"heading": "4.1.1 Concentration", "text": "It remains to prove Properties 1 and 2. We briefly describe our approach for obtaining concentration results and prove that when the number of samples m is large enough, with high probability \u2016 1mD\u0302D\u0302\n> \u2212 2\u03c32In \u2212 1 mDD >\u20162 \u2264 and \u03bbn\u2212k( 1mD\u0302D\u0302 >) > 4\u03c32 + \u03b40/2.\nLet us first describe 1mD\u0302D\u0302 > \u2212 2\u03c32In \u2212 1mDD > in terms of the error matrices. We have\n1\nm D\u0302D\u0302> \u2212 2\u03c32In \u2212\n1\nm DD> =\n( 1\nm EE> \u2212 2\u03c32In\n) + ( 1\nm DE> +\n1\nm ED>\n) . (2)\nIt suffices to show that for large enough m > m ,\u03b4, Pr[\u2016 1mEE > \u2212 2\u03c32In\u20162 \u2265 ] \u2264 \u03b4 and Pr[\u2016 1mDE > + 1 mED >\u20162 \u2265 ] \u2264 \u03b4. In the former, note that 1mEE > is the sample covariance of the Gaussian noise matrix and 2\u03c32In is the true covariance matrix of the noise distribution. The next claim is a direct consequence of the convergence properties of sample covariance of the Gaussian distribution (see Proposition B.2).\n2The assumption that with a non-negligible probability a sample is non-noisy is not needed for the analysis and correctness of Phase 1 of Algorithm 2. This assumption only comes into play in the denoising phase.\nClaim 4.5. For m > n\u03c3 4 2 log(1\u03b4 ), with probability 1\u2212 \u03b4, \u2016 1 mEE > \u2212 2\u03c32In\u20162 \u2264 . 3\nWe use the Matrix Bernstein inequality (Tropp, 2015), described in Appendix B, to demonstrate the concentration of \u2016 1mDE > + 1mED >\u20162. The proof of the next Claim is relegated to Appendix C.1.\nClaim 4.6. m = O(n\u03c3 2M2 2 polylog n \u03b4 ) is sufficient so that with probability 1\u2212\u03b4, \u2225\u2225 1 mDE > + 1mED >\u2225\u2225 2 \u2264 ,\nNext, we prove that \u03bbn\u2212k( 1mD\u0302D\u0302 >) > 4\u03c32 + \u03b40/2. Since for any two matrices, the difference in \u03bbn\u2212k\ncan be bounded by the spectral norm of their difference (see Proposition B.4), using Equation 2, we have\u2223\u2223\u2223\u2223\u03bbn\u2212k ( 1mD\u0302D\u0302> ) \u2212 \u03bbn\u2212k ( 1 m DD> )\u2223\u2223\u2223\u2223 \u2264 \u2225\u2225\u2225\u22252\u03c32I + ( 1mEE> \u2212 2\u03c32In ) \u2212 ( 1 m DE> + 1 m ED> )\u2225\u2225\u2225\u2225 \u2264 2\u03c32 + \u03b404 , where in the last transition we use Claims 4.5 and 4.6 with the value of \u03b40/8 to bound the last two terms by a total of \u03b40/4. Since \u03bbn\u2212k(E[ 1mDD >]) \u2265 6\u03c32 + \u03b40, it is sufficient to show that |\u03bbn\u2212k(E[ 1mDD >]) \u2212 \u03bbn\u2212k([ 1 mDD >])| \u2264 \u03b40/4. Similarly as before, this is bounded by \u2016 1mDD > \u2212 E[ 1mDD\n>]\u2016. We use the Matrix Bernstein inequality (Proposition B.3) to prove this concentration result. The rigorous proof of this claim appears in Appendix C.2,\nClaim 4.7. m = O ( M4\n\u03b420 log n\u03b4\n) is sufficient so that with probability 1\u2212\u03b4, \u2225\u2225 1 mDD > \u2212 E [ 1 mDD >]\u2225\u2225 2 \u2264 \u03b404 .\nThis completes the analysis of Phase 1 of our algorithm and the proof of Lemma 4.2 follows directly from the above analysis and the application of Claims 4.5 and 4.6 with the error of \u03b40, and Claim 4.7."}, {"heading": "4.2 Proof of Lemma 4.3 \u2014 Denoising Step", "text": "Having approximately recovered a projection matrix P\u0302 for span{v1, . . . ,vk}, we can now use this subspace to partially denoise the samples while approximately preserving \u2206 = CH({a1, . . . ,ak}). At a high level, when considering the projection of samples on P\u0302 , one can show that 1) the regions around ai have sufficiently high density, and, 2) the regions that are far from \u2206 have low density.\nWe claim that if x\u0302 \u2208 S\u0302 is non-noisy and corresponds almost purely to one class then S\u0302 also includes a non-negligible number of points within O( \u2032) distance of x\u0302 . This is due to the fact that a non-negligible number of points (about p0\u03b3m points) correspond to non-noisy and almost-pure samples that using P would get projected to points within a distance of O( \u2032) of each other. Furthermore, the inaccuracy in P\u0302 can only perturb the projections up to O( \u2032) distance. So, the projections of all non-noisy samples that are purely of class i fall within O( \u2032) of ai. The following lemma, whose proof appears in Appendix D.1, formalizes this claim.\nIn the following lemmas, let D denote the flattened distribution of the first paragraphs. That is, the distribution over x\u03021 where we first take w \u223c P , then take (x1,x2) \u223c Dw, and finally take x\u03021.\nClaim 4.8. For all i \u2208 [k], Prx\u223cD [ P\u0302x \u2208 B \u2032/4(ai) ] \u2265 p0\u03b3.\nOn the other hand, any projected point that is far from the convex hull of a1, . . . ,ak has to be noisy, and as a result, has been generated by a Gaussian distribution with variance \u03c32. For a choice of \u2032 that is small with respect to \u03c3, such points do not concentrate well within any ball of radius \u2032. In the next lemma we show that the regions that are far from the convex hull have low density.\nClaim 4.9. For any z such that dist(z,\u2206) \u2265 \u2032, we have Prx\u223cD [ P\u0302x \u2208 B \u2032/2(z) ] \u2264 p0\u03b34 .\n3At first sight, the dependence of this sample complexity on \u03c3 might appear unintuitive. But, note that even without seeing any samples we can approximate the noise covariance within 2\u03c32In. Therefore, if = 2\u03c32 our work is done.\nProof. We first show that B \u2032/2(z) does not include any non-noisy points. Take any non-noisy sample x. Note that Px = \u2211k i=1wiai, where wi are the mixture weights corresponding to point x. We have,\u2225\u2225\u2225z\u2212 P\u0302x\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225z\u2212 k\u2211 i=1 wiai + (P \u2212 P\u0302 )x \u2225\u2225\u2225\u2225\u2225 \u2265 \u2225\u2225\u2225\u2225\u2225z\u2212 k\u2211 i=1 wiai \u2225\u2225\u2225\u2225\u2225\u2212 \u2016P \u2212 P\u0302\u2016\u2016x\u2016 \u2265 \u2032/2\nTherefore, B \u2032/2(z) only contains noisy points. Since noisy points are perturbed by a spherical Gaussian, the projection of these points on any k-dimensional subspace can be thought of points generated from a k-dimensional Gaussian distributions with variance \u03c32\nand potentially different centers. One can show that the densest ball of any radius is at the center of a Gaussian. Here, we prove a slightly weaker claim. Consider one such Gaussian distribution, N (0, \u03c32Ik). Note that the pdf of the Gaussian distribution decreases as we get farther from its center. By a coupling between the density of the points, B \u2032/2(0) has higher density than any B \u2032/2(c) with \u2016c\u20162 > \u2032. Therefore,\nsup c Pr x\u223cN (0,\u03c32Ik) [x \u2208 B \u2032/2(c)] \u2264 Pr x\u223cN (0,\u03c32Ik) [x \u2208 B3 \u2032/2(0)].\nSo, over D this value will be maximized if the Gaussians had the same center (see Figure 2). Moreover, in N (0, \u03c32Ik), Pr[\u2016x\u20162 \u2264 \u03c3 \u221a k(1\u2212 t)] \u2264 exp(\u2212kt2/16). Since 3 \u2032/2 \u2264 \u03c3 \u221a k/2 \u2264 \u03c3 \u221a k(1\u2212 \u221a 16 k ln 4 p0\u03b3 )\nwe have Pr x\u0302\u223cD [x \u2208 B \u2032/2(c)] \u2264 Pr x\u223cN (0,\u03c32Ik) [\u2016x\u20162 \u2264 3 \u2032/2] \u2264 p0\u03b3 4 .\nThe next claim shows that in a large sample set, the fraction of samples that fall within any of the described regions in Claims 4.8 and 4.9 is close to the density of that region. The proof of this claim follows from VC dimension of the set of balls.\nClaim 4.10. Let D be any distribution over Rk and x1, . . . ,xm be m points drawn i.i.d from D. Then m = O(k\u03b3 ln 1 \u03b4 ) is sufficient so that with probability 1 \u2212 \u03b4, for any ball B \u2286 R\nk such that Prx\u223cD[x \u2208 B] \u2265 2\u03b3, |{xi | xi \u2208 B}| > \u03b3m and for any ball B \u2286 Rk such that Prx\u223cD[x \u2208 B] \u2264 \u03b3/2, |{xi | xi \u2208 B}| < \u03b3m.\nTherefore, upon seeing \u2126( kp0\u03b3 ln 1 \u03b4 ) samples, with probability 1 \u2212 \u03b4, for all i \u2208 [k] there are more than p0\u03b3m/2 projected points within distance \u2032/4 of ai (by Claims 4.8 and 4.10), and, no point that is \u2032 far from \u2206 has more than p0\u03b3m/2 points in its \u2032/2-neighborhood (by Claims 4.9 and 4.10). Phase 2 of Algorithm 2 leverages these properties of the set of projected points for denoising the samples while preserving \u2206: Remove any point from S\u0302 that has fewer than p0\u03b3m/2 neighbors within distance \u2032/2.\nWe conclude the proof of Lemma 4.3 by noting that the remaining points in S\u0302 are all within distance \u2032 of \u2206. Furthermore, any point in B \u2032/4(ai) has more than p0\u03b3m/2 points within distance of \u2032/2. Therefore, such points remain in S\u0302 and any one of them can serve as a\u0302i for which \u2016ai \u2212 a\u0302i\u2016 \u2264 \u2032/4."}, {"heading": "4.3 Proof of Lemma 4.4 \u2014 Phase 2", "text": "At a high level, we consider two balls around each projected sample point x\u0302 \u2208 S\u0302 with appropriate choice of radii r1 < r2 (see Figure 3a). Consider the set of projections S\u0302 when points in Br2(x) are removed from it. For points that are far from all ai, this set still includes points that are close to ai for all topics i \u2208 [k]. So,\nthe convex hull of S\u0302 \\ Br2(x) is close to \u2206, and in particular, intersects Br1(x). On the other hand, for x that is close to ai, S\u0302 \\ Br2(x) does not include an extreme point of \u2206 or points close to it. So, the convex hull of S\u0302 \\Br2(x) is considerably smaller than \u2206, and in particular, does not intersect Br1(x).\nThe geometry of the simplex and the angles between a1, . . . ,ak play an important role in choosing the appropriate r1 and r2. Note that when the samples are perturbed by noise, a1, . . . ,ak can only be approximately recovered if they are sufficiently far apart and the angles of the simplex at each ai is far from being flat. That is, we assume that for all i 6= j, \u2016ai \u2212 aj\u2016 \u2265 3 . Furthermore, define r \u2265 1 to be the smallest value such that the distance between ai and CH(\u2206\\Br (ai)) is at least . Note that such a value of r always exists and depends entirely on the angles of the simplex defined by the class vectors. Therefore, the number of samples needed for our method depends on the value of r. The smaller the value of r, the larger is the separation between the topic vectors and the easier it is to identify them. See Figure 3b for a demonstration of this concept.\nClaim 4.11. Let \u2032 = /8r. Let S\u0302 be the set of denoised projections, as in step 6 of Algorithm 2. For any x\u0302 \u2208 S\u0302 such that for all i, \u2016x\u0302\u2212 ai\u2016 > 8r \u2032, dist(x\u0302,CH(S\u0302 \\B6r \u2032(x\u0302))) \u2264 2 \u2032. Furthermore, for all i \u2208 [k] there exists a\u0302i \u2208 S\u0302 such that \u2016a\u0302i \u2212 ai\u2016 < \u2032 and dist(a\u0302i,CH(S\u0302 \\B6r \u2032(a\u0302i))) > 2 \u2032.\nProof. Recall that by Lemma 4.3, for any x\u0302 \u2208 S\u0302 there exists x \u2208 \u2206 such that \u2016x\u0302 \u2212 x\u2016 \u2264 \u2032 and for all i \u2208 [k], there exists a\u0302i \u2208 S\u0302 such that \u2016a\u0302i \u2212 ai\u2016 \u2264 \u2032. For the first part, let x = \u2211 i \u03b1iai \u2208 \u2206 be the corresponding point to x\u0302, where \u03b1i\u2019s are the coefficients of the convex combination. Furthermore, let x\u2032 = \u2211 i \u03b1ia\u0302i. We have,\n\u2016x\u2032 \u2212 x\u0302\u2016 \u2264 \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 \u03b1ia\u0302i \u2212 k\u2211 i=1 \u03b1iai + x\u2212 x\u0302 \u2225\u2225\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225maxi\u2208[k] (a\u0302i \u2212 ai) \u2225\u2225\u2225\u2225+ \u2016x\u2212 x\u0302\u2016 \u2264 2 \u2032. The first claim follows from the fact that \u2016x\u0302 \u2212 ai\u2016 > 8r \u2032 and as a result x\u2032 \u2208 CH(S\u0302 \\ B6r \u2032(x\u0302)). Next, note that B4r \u2032(ai) \u2286 B5r \u2032(a\u0302i). So, by the fact that \u2016ai \u2212 a\u0302i\u2016 \u2264 \u2032,\ndist (a\u0302i,CH(\u2206 \\B5r \u2032(a\u0302i))) \u2265 dist (ai,CH(\u2206 \\B4r \u2032(ai)))\u2212 \u2032 \u2265 3 \u2032.\nFurthermore, we argue that if there is x\u0302 \u2208 CH(S\u0302 \\ B5r \u2032(a\u0302i)) then there exists x \u2208 CH(\u2206 \\ B4r \u2032(a\u0302i)), such that \u2016x \u2212 x\u0302\u2016 \u2264 \u2032. The proof of this claim is relegated to Appendix E.1. Using this claim, we have dist ( a\u0302i,CH(S\u0302 \\B6r \u2032(a\u0302i)) ) \u2265 2 \u2032.\nGiven the above structure, it is clear that set of points in C are all within of one of the ai\u2019s. So, we can cluster C using single linkage with threshold to recover ai up to accuracy ."}, {"heading": "5 Additional Results, Extensions, and Open Problems", "text": ""}, {"heading": "5.1 Sample Complexity Lower bound", "text": "As we observed the number of samples required by our method is poly(n). However, as the number of classes can be much smaller than the number of features, one might hope to recover v1, . . . ,vk, with a number of samples that is polynomial in k rather than n. Here, we show that in the general case \u2126(n) samples are needed to learn v1, . . . ,vk regardless of the value of k.\nFor ease of exposition, let k = 1 and note that in this case every sample should be purely of one type. Assume that the class vector, v, is promised to be in the set C = {vj | vj` = 1/ \u221a 2, if ` = 2j \u2212 1 or 2j, else vj` = 0}. Consider instances (x 1 j ,x 2 j ) such that the ` th coordinate of x1j is x 1 j` = \u22121/ \u221a 2 if\n` = 2j\u2212 1 and 1/ \u221a 2 otherwise, and x2j` = \u22121/ \u221a 2 if ` = 2j and 1/ \u221a 2 otherwise. For a given (x1j ,x 2 j ), we have that vj \u00b7 x1j = vj \u00b7 x2j = 0. On the other hand, for all ` 6= j, v` \u00b7 x1j = v` \u00b7 x2j = 1. Therefore, sample (x1j ,x 2 j ) is consistent with v = v\n` for any ` 6= j, but not with v = vj . That is, each instance (x1j ,x2j ) renders only one candidate of C invalid. Even after observing at most n2 \u2212 2 samples of this types, at least 2 possible choices for v remain. So, \u2126(n) samples are indeed needed to find the appropriate v. The next theorem, whose proof appears in Appendix F generalizes this construction and result to the case of any k. Theorem 5.1. For any k \u2264 n, any algorithm that for all i \u2208 [k] learns v\u2032i such that \u2016vi \u2212 v\u2032i\u20162 \u2264 1/ \u221a\n2, requires \u2126(n) samples.\nNote that in the above construction samples have large components in the irrelevant features. It would be interesting to see if this lower bound can be circumvented using additional natural assumptions in this model, such as assuming that the samples have length poly(k)."}, {"heading": "5.2 Alternative Noise Models", "text": "Consider the problem of recovering v1, . . . ,vk in the presence of agnostic noise, where for an fraction of the samples (x1,x2), x1 and x2 correspond to different mixture weights. Furthermore, assume that the distribution over the instance space is rich enough such that any subspace other than span{v1, . . . ,vk} is inconsistent with a set of instances of non-negligible density.4 Since the VC dimension of the set of k dimensional subspaces in Rn is min{k, n \u2212 k}, from the information theoretic point of view, one can recover span{v1, . . . ,vk} as it is the only subspace that is inconsistent with less than O( ) fraction of O\u0303( k\n2 ) samples. Furthermore, we can detect and remove any noisy sample, for which the two views of the sample are not consistent with span{v1, . . . ,vk}. And finally, we can recover a1, . . . ,ak using phase 2 of Algorithm 1.\nIn the above discussion, it is clear that once we have recovered span{v1, . . . ,vk}, denoising and finding the extreme points of the projections can be done in polynomial time. For the problem of recovering a kdimensional nullspace, Hardt and Moitra (2013) introduced an efficient algorithm that tolerates agnostic noise up to = O(k/n). Furthermore, they provide an evidence that this result might be tight. It would be interesting to see whether additional structure present in our model, such as the fact that samples are convex combination of classes, can allow us to efficiently recover the nullspace in presence of more noise.\nAnother interesting open problem is whether it is possible to handle the case of p0 = 0. That is, when every document is affected by Gaussian noise N (0, \u03c32In), for \u03c3 . A simpler form of this problem is\n4This assumption is similar to the richness assumption made in the standard case, where we assume that there is enough \u201centropy\u201d between the two views of the samples such that even in the non-noisy case the subspace can be uniquely determined by taking the nullspace of X1 \u2212X2.\nas follows. Consider a distribution induced by first drawing x \u223c D, where D is an arbitrary and unknown distribution over \u2206 = CH({a1, . . . ,ak}), and taking x\u0302 = x+N (0, \u03c32In). Can we learn ai\u2019s within error of using polynomially many samples? Note that when D is only supported on the corners of \u2206, this problem reduces to learning mixture of Gaussians, for which there is a wealth of literature on estimating Gaussian means and mixture weights (Dasgupta et al., 2002; Kalai et al., 2012; Moitra and Valiant, 2010). It would be interesting to see under what regimes ai (and not necessarily the mixture weights) can be learned when D is an arbitrary distribution over \u2206."}, {"heading": "5.3 General function f(\u00b7)", "text": "Consider the general model described in Section 2, where fi(x) = f(vi \u00b7x) for an unknown strictly increasing function f : R+ \u2192 [0, 1] such that f(0) = 0. We describe how variations of the techniques discussed up to now can extend to this more general setting.\nFor ease of exposition, consider the non-noisy case. Since f is a strictly increasing function, f(vi \u00b7x1) = f(vi \u00b7x2) if and only if vi \u00b7x1 = vi \u00b7x2. Therefore, we can recover span(v1, . . . ,vk) by the same approach as in Phase 1 of Algorithm 1. Although, by definition of pseudoinverse matrices, the projection of x is still represented by x = \u2211 i(vi \u00b7x)ai, this is not necessarily a convex combination of ai\u2019s anymore. This is due to the fact that vi \u00b7 x can add up to values larger than 1 depending on x. However, x is still a non-negative combination of ai\u2019s. Moreover, ai\u2019s are linearly independent, so ai can not be expressed by a nontrivial nonnegative combination of other samples. Therefore, for all i, ai/\u2016ai\u2016 can be recovered by taking the extreme rays of the convex cone of the projected samples. So, we can recover v1, . . . ,vk, by taking the psuedoinverse of ai/\u2016ai\u2016 and re-normalizing the outcome such that \u2016vi\u20162 = 1. When samples are perturbed by noise, a similar argument that also takes into account the smoothness of f proves similar results.\nIt would be interesting to see whether a more general class of similarity functions, such as kernels, can be also learned in this context."}, {"heading": "A Omitted Proof from Section 3 \u2014 No Noise", "text": "A.1 Proof of Lemma 3.1\nFor all j \u2264 n \u2212 k, let Zj = {(x1i \u2212 x2i ) | i \u2264 j \u03b6 ln n \u03b4 }. We prove by induction that for all j, rank(Zj) < j with probability at most j \u03b4n . For j = 0, the claim trivially holds. Now assume that the induction hypothesis holds for some j. Furthermore, assume that rank(Zj) \u2265 j. Then, rank(Zj+1) < j+1 only if the additional 1\u03b6 ln n \u03b4 samples inZj+1 all belong to span(Zj). Since, the space of such samples has rank< n\u2212k, this happens with probability at most\n(1\u2212\u03b6) 1 \u03b6 ln n\n\u03b4 \u2264 \u03b4n . Together with the induction hypothesis that rank(Zj) \u2265 j with probability at most j \u03b4 n , we\nhave that rank(Zj+1) < j + 1 with probability at most (j+1)\u03b4 n . Therefore rank(Z) = rank(Zn\u2212k) = n\u2212 k with probability at least 1\u2212 \u03b4.\nA.2 Proof of Lemma 3.2 First note that V is a the pseudo-inverse ofA, so their span is equal. Hence, \u2211 i\u2208[k](vi\u00b7x)ai \u2208 span{v1, . . . ,vk}.\nIt remains to show that ( x\u2212 \u2211 i\u2208[k](vi \u00b7 x)ai ) \u2208 null{v1, . . . ,vk}. We do so by showing that this vector is orthogonal to vj for all j. We have( x\u2212\nk\u2211 i=1 (vi \u00b7 x)ai\n) \u00b7 vj = x \u00b7 vj \u2212\nk\u2211 i=1 (vi \u00b7 x)(ai \u00b7 vj)\n= x \u00b7 vj \u2212 \u2211 i 6=j (vi \u00b7 x)(ai \u00b7 vj)\u2212 (vj \u00b7 x)(aj \u00b7 vj)\n= x \u00b7 vj \u2212 x \u00b7 vj = 0.\nWhere, the second equality follows from the fact whenA = V +, for all i, ai \u00b7vi = 1 and aj \u00b7vi = for j 6= i. Therefore, \u2211 i\u2208[k](vi \u00b7 x)ai is the projection of x on span{v1, . . . ,vk}.\nA.3 Proof of Lemma 3.3 Assume that S included samples that are purely of type i, for all i \u2208 [k]. That is, for all i \u2208 [k] there is j \u2264 m, such that vi \u00b7 x1j = vi \u00b7 x2j = 1 and vi\u2032 \u00b7 x1j = vi\u2032 \u00b7 x2j = 0 for i\u2032 6= i. By Lemma 3.2, the set of projected vectors form the set { \u2211k i=1(vi \u00b7 xj)ai | j \u2208 [m]}. Note that \u2211k i=1(vi \u00b7 xj)ai is in the simplex with vertices a1, . . . ,ak. Moreover, for each i, there exists a pure sample in S of type i. Therefore, CH{ \u2211k i=1(vi \u00b7 xj)ai | j \u2208 [m]} is the simplex on linearly independent vertices a1, . . . ,ak. As a result, a1, . . . ,ak are the extreme points of it. It remains to prove that with probability 1 \u2212 \u03b4, the sample set has a document of purely type j, for all j \u2208 [k]. By the assumption on the probability distribution P , with probability at most (1\u2212 \u03be)m, there is no document of type purely j. Using the union bound, we get the final result."}, {"heading": "B Technical Spectral Lemmas", "text": "Proposition B.1 (Davis and Kahan (1970) sin \u03b8 theorem). . Let B, B\u0302 \u2208 Rp\u00d7p be symmetric, with eigen values \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbp and \u03bb\u03021 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bb\u0302p, respectively. Fix 1 \u2264 r \u2264 s \u2264 p and let V = (vr, . . . ,vs) and V\u0302 = (v\u0302r, . . . , v\u0302s) be the orthonormal eigenvectors corresponding to \u03bbr, . . . , \u03bbs and \u03bb\u0302r, . . . , \u03bb\u0302s. Let \u03b4 = inf{|\u03bb\u0302\u2212 \u03bb| : \u03bb \u2208 [\u03bbs, \u03bbr], \u03bb\u0302 \u2208 (\u2212\u221e, \u03bb\u0302s\u22121] \u222a [\u03bb\u0302r+1,\u221e)} > 0. Then ,\n\u2016 sin \u0398(V, V\u0302 )\u20162 \u2264 \u2016B\u0302 \u2212B\u20162\n\u03b4 .\nwhere sin \u0398(V, V\u0302 ) = PV \u2212 PV\u0302 , where PV and PV\u0302 are the projection matrices for V and V\u0302 .\nProposition B.2 (Corollary 5.50 (Vershynin, 2010)). Consider a Gaussian distribution in Rn with covariance matrix \u03a3. Let A \u2208 Rn\u00d7m be a matrix whose rows are drawn i.i.d from this distribution, and let \u03a3m = 1mAA\n>. For every \u2208 (0, 1), and t, if m \u2265 cn(t/ )2 for some constant c, then with probability at least 1\u2212 2 exp(\u2212t2n), \u2016\u03a3m \u2212 \u03a3\u20162 \u2264 \u2016\u03a3\u20162\nProposition B.3 (Matrix Bernstein (Tropp, 2015)). Let S1, . . . , Sn be independent, centered random matrices with common dimension d1\u00d7 d2, and assume that each one is uniformly bounded. That is, ESi = 0 and\n\u2016Si\u20162 \u2264 L for all i \u2208 [n]. Let Z = \u2211n i=1 Si, and let v(Z) denote the matrix variance:\nv(Z) = max {\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 E[SiS>i ] \u2225\u2225\u2225\u2225\u2225 , \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 E[S>i Si] \u2225\u2225\u2225\u2225\u2225 } .\nThen,\nP[\u2016Z\u2016 \u2265 t] \u2264 (d1 + d2) exp (\n\u2212t2/2 v(Z) + Lt/3\n) .\nProposition B.4 (Theorem 4.10 of Stewart and Sun (1990)). Let A\u0302 = A + E and let \u03bb1, . . . , \u03bbn and \u03bb\u20321, . . . , \u03bb \u2032 n be the eigen values of A and A+ E. Then, max{|\u03bb\u2032i \u2212 \u03bbi|} \u2264 \u2016E\u20162.\nProposition B.5 (Theorem 3.3 of Stewart (1977)). For any A and B = A+ E, \u2016B+ \u2212A+\u2016 \u2264 max 3 { \u2016A+\u20162, \u2016B+\u20162 } \u2016E\u2016,\nwhere \u2016 \u00b7 \u2016 is an arbitrary norm."}, {"heading": "C Omitted Proof from Section 4.1 \u2014 Phase 1", "text": "C.1 Proof of Claim 4.6\nLet ei and di be the ith row of E and D. Then ED> = \u2211m i=1 eid > i and DE > = \u2211m i=1 die > i . Let Si =\n1 m\n[ 0 eid > i\ndie > i 0\n] . Then, \u2016 1mDE > + 1mED >\u20162 \u2264 2\u2016 \u2211m i=1 Si\u20162. We will use matrix Bernstein to show\nthat \u2211\ni\u2208[m] Si is small with high probability. First note that the distribution of ei is a Gaussian centered at 0, therefore, E[Si] = 0. Furthermore, for each i, with probability 1 \u2212 \u03b4, \u2016ei\u20162 \u2264 \u03c3 \u221a n log 1\u03b4 . So, with probability 1 \u2212 \u03b4, for all samples i \u2208 [m],\n\u2016ei\u20162 \u2264 \u03c3 \u221a n log m\u03b4 . Moreover, by assumption \u2016di\u2016 = \u2016x 1 i \u2212 x2i \u2016 \u2264 2M . Therefore, with probability 1\u2212 \u03b4, L = max\ni \u2016Si\u20162 =\n1\nm max i \u2016ei\u2016\u2016di\u2016 \u2264\n2 m \u03c3 \u221a nM polylog n \u03b4 .\nNote that, \u2225\u2225E[SiS>i ]\u2225\u2225 = 1m2 \u2225\u2225E[(eid>i )2]\u2225\u2225 \u2264 L2. Since Si is Hermitian, the matrix covariance defined\nby Matrix Bernstein inequality is\nv(Z) = max {\u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 E[SiS>i ] \u2225\u2225\u2225\u2225\u2225 , \u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 E[S>i Si] \u2225\u2225\u2225\u2225\u2225 } = \u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 E[SiS>i ] \u2225\u2225\u2225\u2225\u2225 \u2264 mL2. If \u2264 v(Z)/L and m \u2208 \u2126(n\u03c32M2\n2 polylog n \u03b4 ) or \u2265 v(Z)/L and m \u2208 \u2126(\n\u221a n\u03c3M polylog\nn \u03b4 ), using\nMatrix Bernstein inequality (Proposition B.3), we have\nPr [\u2225\u2225\u2225\u2225 1mDE> + 1mED> \u2225\u2225\u2225\u2225 \u2265 ] = Pr [\u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 Si \u2225\u2225\u2225\u2225\u2225 \u2265 2 ] \u2264 \u03b4.\nC.2 Proof of Claim 4.7 Let di be the ith row D. Then DD> = \u2211m i=1 did > i . Let Si = 1 mdid > i \u2212 1mE[did > i ]. Then, \u2016 1mDD\n> \u2212 E [ 1 mDD >] \u20162 = \u2016\u2211mi=1 Si\u20162. Since, di = x1i \u2212 x2i and \u2016xji\u2016 \u2264 M , we have that for any i, \u2016did>i \u2212 E[did>i ]\u2016 \u2264 4M2. Then,\nL = max i \u2016Si\u20162 =\n1\nm max i \u2016did>i \u2212 E[did>i ]\u20162 \u2264\n4\nm M2,\nand \u2016E[SiS>i ] \u2264 L2. Note that Si is Hermitian, so, the matrix covariance is\nv(Z) = max {\u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 E[SiS>i ] \u2225\u2225\u2225\u2225\u2225 , \u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 E[S>i Si] \u2225\u2225\u2225\u2225\u2225 } = \u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 E[SiS>i ] \u2225\u2225\u2225\u2225\u2225 \u2264 mL2. If \u03b40 \u2264 4M2 and m \u2208 \u2126(M 4\n\u03b420 log n\u03b4 ) or \u03b40 \u2265 4M 2 and m \u2208 \u2126(M2\u03b40 log n \u03b4 ), then by Matrix Bernstein\ninequality (Proposition B.3), we have\nPr [\u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 Si \u2225\u2225\u2225\u2225\u2225 \u2265 \u03b402 ] \u2264 \u03b4."}, {"heading": "D Omitted Proof from Section 4.2 \u2014 Denoising", "text": "D.1 Proof of Claim 4.8 Recall that for any i \u2208 [k], with probability \u03b3 = g( \u2032/(8k\u03b1)) a nearly pure weight vector w is generated from P , such that \u2016w \u2212 ei\u2016 \u2264 \u2032/(8k\u03b1). And independently, with probability p0 the point is not noisy. Therefore, there is p0\u03b3 density on non-noisy points that are almost purely of class i. Note that for such points, x,\n\u2016Px\u2212 ai\u2016 = \u2225\u2225\u2225\u2225\u2225\u2225 k\u2211 j=1 wjaj \u2212 ai \u2225\u2225\u2225\u2225\u2225\u2225 \u2264 k( \u2032/(8k\u03b1))(\u03b1) \u2264 \u2032 8 .\nSince \u2016P \u2212 P\u0302\u2016 \u2264 \u2032/8M , we have\n\u2016ai \u2212 P\u0302x\u2016 = \u2016ai \u2212 Px\u2016+ \u2016Px\u2212 P\u0302x\u2016 \u2264 \u2032 8 + \u2032 8 \u2264 \u2032 4\nThe claim follows immediately."}, {"heading": "E Omitted Proof from Section 4.3 \u2014 Phase 2", "text": "E.1 Omitted proof from Claim 4.11\nHere, we prove that x\u0302 \u2208 CH(S\u0302 \\Bd+ \u2032(a\u0302i)) then there exists x \u2208 CH(\u2206\\Bd(a\u0302i)), such that \u2016x\u2212 x\u0302\u2016 \u2264 \u2032. Let x = \u2211 i \u03b1iz\u0302i be the convex combination of z\u03021, . . . , z\u0302` \u2208 S\u0302 \\ Bd+ \u2032(a\u0302i). By Claim 4.3, there are z1, . . . , z` \u2208 \u2206, such that \u2016zi \u2212 z\u0302i\u2016 \u2264 \u2032 for all i \u2208 [k]. Furthermore, by the proximity of zi to z\u0302i we have that zi 6\u2208 Bd(a\u0302i). Therefore, z1, . . . , z` \u2208 \u2206 \\Bd(a\u0302i). Then, x = \u2211 i \u03b1izi is also within distance \u2032."}, {"heading": "F Proof of Theorem 5.1 \u2014 Lower Bound", "text": "For ease of exposition assume that n is a multiple of k. Furthermore, in this proof we adopt the notion (xi,x \u2032 i) to represent the two views of the i\nth sample. For any vector u \u2208 Rn and i \u2208 [k], we use (u)i to denote the ith nk -dimensional block of u, i.e., coordinates u(i\u22121)nk+1, . . . , uink .\nConsider the nk -dimensional vector uj , such that uj` = 1 if ` = 2j \u2212 1 or 2j, and uj` = 0, otherwise. And consider nk -dimensional vectors zj and z \u2032 j , such that zj` = \u22121 if ` = 2j\u22121 and zj` = 1 otherwise, and z\u2032j` = \u22121 if ` = 2j and z\u2032j` = 1 otherwise. Consider a setting where vi is restricted to the set of candidate Ci = {vji | (v j i )i = uj/ \u221a 2 and (vji )i\u2032 = 0 for i \u2032 6= i}. In other words, the `th coordinate of vji is 1/ \u221a 2 if ` = (i \u2212 1)nk + 2j \u2212 1 or (i \u2212 1) n k + 2j, else 0. Furthermore, consider instances (x j i ,x \u2032j i ) such that\n(xji )i = zj/ \u221a 2 and (x\u2032ji )i = z \u2032 j/ \u221a 2 and for all i\u2032 6= i, (xji )i\u2032 = (x \u2032j i )i\u2032 = 0. In other words,\nxji = 1\u221a 2 (0, . . . , 0, 1, . . . , 1,\n(i\u22121)n k +2j\u22121,(i\u22121)n k +2j\ufe37 \ufe38\ufe38 \ufe37\n1,\u22121 , 1, . . . , 1, 0, . . . , 0),\nx\u2032ji = 1\u221a 2 (0, . . . , 0, 1, . . . , 1,\u22121, 1 , 1, . . . , 1, 0, . . . , 0),\nvji = 1\u221a 2 (0, . . . , 0, 0, . . . , 0, 1, 1 , 0, . . . , 0,\ufe38 \ufe37\ufe37 \ufe38 ith block 0, . . . , 0).\nFirst note that, for any i, i\u2032 \u2208 [k] and any j, j\u2032 \u2208 [ n2k ], v j i \u00b7 x\nj\u2032 i\u2032 = v j i \u00b7 x \u2032j\u2032 i\u2032 . That is, the two views of all\ninstances are consistent with each other with respect to all candidate vectors. Furthermore, for any i and i\u2032 such that i 6= i\u2032, for all j, j\u2032, vji \u00b7 x j\u2032 i\u2032 = 0. Therefore, for any observed sample (x j i ,x \u2032j i ), the sample should be purely of type i. For a given i, consider all the samples (xji ,x \u2032j i ) that are observed by the algorithm. Note that v j i \u00b7 x j i = vji \u00b7 x \u2032j i = 0. And for all j \u2032 6= j, vj \u2032 i \u00b7 x j i = v j\u2032 i \u00b7 x \u2032j i = 1. Therefore, observing (x j i ,x \u2032j i ) only rules out v j i as a candidate, while this sample is consistent with candidates vj \u2032\ni for j \u2032 6= j. Therefore, even after observing\n\u2264 n2k \u2212 2 samples of this types, at least 2 possible choices for vi remain valid. Moreover, the distance between any two vji ,v j\u2032 i \u2208 Ci is \u221a\n2. Therefore, n2k \u2212 1 samples are needed to learn vi to an accuracy better than \u221a\n2/2. Note that consistency of the data with vi\u2032 is not affected by the samples of type x j i that are observed by the algorithms when i\u2032 6= i. So, \u2126(knk ) = \u2126(n) samples are required to approximate all vi\u2019s to an accuracy better than \u221a 2/2."}], "references": [{"title": "Tensor decompositions for learning latent variable models", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky"], "venue": "Journal of Machine Learning Research, 15(1):2773\u20132832.", "citeRegEx": "Anandkumar et al\\.,? 2014", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "A spectral algorithm for latent dirichlet allocation", "author": ["A. Anandkumar", "Liu", "Y.-k.", "D.J. Hsu", "D.P. Foster", "S.M. Kakade"], "venue": "Advances in Neural Information Processing Systems, pages 917\u2013925.", "citeRegEx": "Anandkumar et al\\.,? 2012", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["S. Arora", "R. Ge", "Y. Halpern", "D.M. Mimno", "A. Moitra", "D. Sontag", "Y. Wu", "M. Zhu"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML), pages 280\u2013288.", "citeRegEx": "Arora et al\\.,? 2013", "shortCiteRegEx": "Arora et al\\.", "year": 2013}, {"title": "Computing a nonnegative matrix factorization\u2013 provably", "author": ["S. Arora", "R. Ge", "R. Kannan", "A. Moitra"], "venue": "Proceedings of the 44th Annual ACM Symposium on Theory of Computing (STOC), pages 145\u2013162. ACM.", "citeRegEx": "Arora et al\\.,? 2012a", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Learning topic models \u2013 going beyond svd", "author": ["S. Arora", "R. Ge", "A. Moitra"], "venue": "Proceedings of the 53rd Symposium on Foundations of Computer Science (FOCS), pages 1\u201310.", "citeRegEx": "Arora et al\\.,? 2012b", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Co-training and expansion: Towards bridging theory and practice", "author": ["Balcan", "M.-F.", "A. Blum", "K. Yang"], "venue": "Advances in Neural Information Processing Systems, pages 89\u201396.", "citeRegEx": "Balcan et al\\.,? 2004", "shortCiteRegEx": "Balcan et al\\.", "year": 2004}, {"title": "A provable svd-based algorithm for learning topics in dominant admixture corpus", "author": ["T. Bansal", "C. Bhattacharyya", "R. Kannan"], "venue": "Advances in Neural Information Processing Systems, pages 1997\u20132005.", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of machine Learning research, 3(Jan):993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "Proceedings of the 11th Conference on Computational Learning Theory (COLT), pages 92\u2013100. ACM.", "citeRegEx": "Blum and Mitchell,? 1998", "shortCiteRegEx": "Blum and Mitchell", "year": 1998}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Schlkopf", "A. Zien"], "venue": "The MIT Press, 1st edition.", "citeRegEx": "Chapelle et al\\.,? 2010", "shortCiteRegEx": "Chapelle et al\\.", "year": 2010}, {"title": "Pac generalization bounds for co-training", "author": ["S. Dasgupta", "M.L. Littman", "D. McAllester"], "venue": "Advances in Neural Information Processing Systems, 1:375\u2013382.", "citeRegEx": "Dasgupta et al\\.,? 2002", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2002}, {"title": "The rotation of eigenvectors by a perturbation", "author": ["C. Davis", "W.M. Kahan"], "venue": "iii. SIAM Journal on Computing, 7(1):1\u201346.", "citeRegEx": "Davis and Kahan,? 1970", "shortCiteRegEx": "Davis and Kahan", "year": 1970}, {"title": "Algorithms and hardness for robust subspace recovery", "author": ["M. Hardt", "A. Moitra"], "venue": "Proceedings of the 26th Conference on Computational Learning Theory (COLT), pages 354\u2013375.", "citeRegEx": "Hardt and Moitra,? 2013", "shortCiteRegEx": "Hardt and Moitra", "year": 2013}, {"title": "Probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence, pages 289\u2013296. Morgan Kaufmann Publishers Inc.", "citeRegEx": "Hofmann,? 1999", "shortCiteRegEx": "Hofmann", "year": 1999}, {"title": "Disentangling gaussians", "author": ["A.T. Kalai", "A. Moitra", "G. Valiant"], "venue": "Communications of the ACM, 55(2):113\u2013120.", "citeRegEx": "Kalai et al\\.,? 2012", "shortCiteRegEx": "Kalai et al\\.", "year": 2012}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "author": ["A. Moitra", "G. Valiant"], "venue": "Proceedings of the 53rd Symposium on Foundations of Computer Science (FOCS), pages 93\u2013102. IEEE.", "citeRegEx": "Moitra and Valiant,? 2010", "shortCiteRegEx": "Moitra and Valiant", "year": 2010}, {"title": "Latent semantic indexing: A probabilistic analysis", "author": ["C.H. Papadimitriou", "H. Tamaki", "P. Raghavan", "S. Vempala"], "venue": "Proceedings of the seventeenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems, pages 159\u2013168. ACM.", "citeRegEx": "Papadimitriou et al\\.,? 1998", "shortCiteRegEx": "Papadimitriou et al\\.", "year": 1998}, {"title": "Matrix perturbation theory (computer science and scientific computing)", "author": ["G. Stewart", "Sun", "J.-G"], "venue": null, "citeRegEx": "Stewart et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Stewart et al\\.", "year": 1990}, {"title": "On the perturbation of pseudo-inverses, projections and linear least squares problems", "author": ["G.W. Stewart"], "venue": "SIAM Review, 19(4):634\u2013662.", "citeRegEx": "Stewart,? 1977", "shortCiteRegEx": "Stewart", "year": 1977}, {"title": "A survey of multi-view machine learning", "author": ["S. Sun"], "venue": "Neural computing and applications, 23:2031\u2013 2038.", "citeRegEx": "Sun,? 2013", "shortCiteRegEx": "Sun", "year": 2013}, {"title": "An introduction to matrix concentration inequalities", "author": ["J.A. Tropp"], "venue": "arXiv preprint arXiv:1501.01571.", "citeRegEx": "Tropp,? 2015", "shortCiteRegEx": "Tropp", "year": 2015}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": "arXiv preprint arXiv:1011.3027.", "citeRegEx": "Vershynin,? 2010", "shortCiteRegEx": "Vershynin", "year": 2010}, {"title": "Perturbation bounds in connection with singular value decomposition", "author": ["Wedin", "P.-\u00c5."], "venue": "BIT Numerical Mathematics, 12(1):99\u2013111.", "citeRegEx": "Wedin and P..\u00c5.,? 1972", "shortCiteRegEx": "Wedin and P..\u00c5.", "year": 1972}, {"title": "Rp\u00d7p be symmetric, with eigen values", "author": [], "venue": "B Technical Spectral Lemmas Proposition", "citeRegEx": "B and \u2208,? \\Q1970\\E", "shortCiteRegEx": "B and \u2208", "year": 1970}, {"title": "PV\u0302 , where PV and PV\u0302 are the projection matrices for V and V\u0302", "author": ["V\u0302 ) = PV"], "venue": "(Vershynin,", "citeRegEx": "\u2212,? \\Q2010\\E", "shortCiteRegEx": "\u2212", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "Topic modeling is an area with significant recent work in the intersection of algorithms and machine learning (Arora et al., 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014).", "startOffset": 110, "endOffset": 191}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al.", "startOffset": 17, "endOffset": 892}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al.", "startOffset": 17, "endOffset": 918}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al.", "startOffset": 17, "endOffset": 947}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999).", "startOffset": 17, "endOffset": 1071}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999). As a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution.", "startOffset": 17, "endOffset": 1087}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999). As a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution. Better would be a model in which sentences are drawn i.i.d. from a distribution over sentences (this would at least produce grammatical objects and allow for meaningful correlation among related words within a topic, like shooting a free throw or kicking a field goal). Even better would be paragraphs drawn i.i.d. from a distribution over paragraphs (this would at least produce coherent paragraphs). Or, even better, how about a model in which paragraphs are drawn non-independently, so that the second paragraph in a document can depend on what the first paragraph was saying, though presumably with some amount of additional entropy as well? This is the type of model we study here. Note that an immediate problem with considering such a model is that now the task of learning an explicit distribution (over sentences or paragraphs) is hopeless. While a distribution over words can be reasonably viewed as a probability vector, one could not hope to learn or even represent an explicit distribution over sentences or paragraphs. Indeed, except in cases of plagiarism, one would not expect to see the same paragraph twice in the entire corpus. Moreover, this is likely to be true even if we assume paragraphs have some natural feature-vector representation. Instead, we bypass this issue by aiming to directly learn a predictor for documents\u2014that is, a function that given a document, predicts its mixture over topics\u2014without explicitly learning topic distributions. Another way to think of this is that our goal is not to learn a model that could be used to write a new document, but instead just a model that could be used to classify a document written by others. This is much as in standard supervised learning where algorithms such as SVMs learn a decision boundary (such as a linear separator) for making predictions on the labels of examples without explicitly learning the distributions D+ and D\u2212 over positive and negative examples respectively. However, our setting is unsupervised (we are not given labeled data containing the correct classifications of the documents in the training set) and furthermore, rather than each data item belonging to one of the k classes (topics), each data item belongs to a mixture of the k topics. Our goal is given a new data item to output what that mixture is. We begin by describing our high level theoretical formulation. This formulation can be viewed as a generalization both of standard topic modeling and of a setting known as multi-view learning or co-training Blum and Mitchell (1998); Dasgupta et al.", "startOffset": 17, "endOffset": 3847}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999). As a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution. Better would be a model in which sentences are drawn i.i.d. from a distribution over sentences (this would at least produce grammatical objects and allow for meaningful correlation among related words within a topic, like shooting a free throw or kicking a field goal). Even better would be paragraphs drawn i.i.d. from a distribution over paragraphs (this would at least produce coherent paragraphs). Or, even better, how about a model in which paragraphs are drawn non-independently, so that the second paragraph in a document can depend on what the first paragraph was saying, though presumably with some amount of additional entropy as well? This is the type of model we study here. Note that an immediate problem with considering such a model is that now the task of learning an explicit distribution (over sentences or paragraphs) is hopeless. While a distribution over words can be reasonably viewed as a probability vector, one could not hope to learn or even represent an explicit distribution over sentences or paragraphs. Indeed, except in cases of plagiarism, one would not expect to see the same paragraph twice in the entire corpus. Moreover, this is likely to be true even if we assume paragraphs have some natural feature-vector representation. Instead, we bypass this issue by aiming to directly learn a predictor for documents\u2014that is, a function that given a document, predicts its mixture over topics\u2014without explicitly learning topic distributions. Another way to think of this is that our goal is not to learn a model that could be used to write a new document, but instead just a model that could be used to classify a document written by others. This is much as in standard supervised learning where algorithms such as SVMs learn a decision boundary (such as a linear separator) for making predictions on the labels of examples without explicitly learning the distributions D+ and D\u2212 over positive and negative examples respectively. However, our setting is unsupervised (we are not given labeled data containing the correct classifications of the documents in the training set) and furthermore, rather than each data item belonging to one of the k classes (topics), each data item belongs to a mixture of the k topics. Our goal is given a new data item to output what that mixture is. We begin by describing our high level theoretical formulation. This formulation can be viewed as a generalization both of standard topic modeling and of a setting known as multi-view learning or co-training Blum and Mitchell (1998); Dasgupta et al. (2002); Chapelle et al.", "startOffset": 17, "endOffset": 3871}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999). As a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution. Better would be a model in which sentences are drawn i.i.d. from a distribution over sentences (this would at least produce grammatical objects and allow for meaningful correlation among related words within a topic, like shooting a free throw or kicking a field goal). Even better would be paragraphs drawn i.i.d. from a distribution over paragraphs (this would at least produce coherent paragraphs). Or, even better, how about a model in which paragraphs are drawn non-independently, so that the second paragraph in a document can depend on what the first paragraph was saying, though presumably with some amount of additional entropy as well? This is the type of model we study here. Note that an immediate problem with considering such a model is that now the task of learning an explicit distribution (over sentences or paragraphs) is hopeless. While a distribution over words can be reasonably viewed as a probability vector, one could not hope to learn or even represent an explicit distribution over sentences or paragraphs. Indeed, except in cases of plagiarism, one would not expect to see the same paragraph twice in the entire corpus. Moreover, this is likely to be true even if we assume paragraphs have some natural feature-vector representation. Instead, we bypass this issue by aiming to directly learn a predictor for documents\u2014that is, a function that given a document, predicts its mixture over topics\u2014without explicitly learning topic distributions. Another way to think of this is that our goal is not to learn a model that could be used to write a new document, but instead just a model that could be used to classify a document written by others. This is much as in standard supervised learning where algorithms such as SVMs learn a decision boundary (such as a linear separator) for making predictions on the labels of examples without explicitly learning the distributions D+ and D\u2212 over positive and negative examples respectively. However, our setting is unsupervised (we are not given labeled data containing the correct classifications of the documents in the training set) and furthermore, rather than each data item belonging to one of the k classes (topics), each data item belongs to a mixture of the k topics. Our goal is given a new data item to output what that mixture is. We begin by describing our high level theoretical formulation. This formulation can be viewed as a generalization both of standard topic modeling and of a setting known as multi-view learning or co-training Blum and Mitchell (1998); Dasgupta et al. (2002); Chapelle et al. (2010); Balcan et al.", "startOffset": 17, "endOffset": 3895}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999). As a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution. Better would be a model in which sentences are drawn i.i.d. from a distribution over sentences (this would at least produce grammatical objects and allow for meaningful correlation among related words within a topic, like shooting a free throw or kicking a field goal). Even better would be paragraphs drawn i.i.d. from a distribution over paragraphs (this would at least produce coherent paragraphs). Or, even better, how about a model in which paragraphs are drawn non-independently, so that the second paragraph in a document can depend on what the first paragraph was saying, though presumably with some amount of additional entropy as well? This is the type of model we study here. Note that an immediate problem with considering such a model is that now the task of learning an explicit distribution (over sentences or paragraphs) is hopeless. While a distribution over words can be reasonably viewed as a probability vector, one could not hope to learn or even represent an explicit distribution over sentences or paragraphs. Indeed, except in cases of plagiarism, one would not expect to see the same paragraph twice in the entire corpus. Moreover, this is likely to be true even if we assume paragraphs have some natural feature-vector representation. Instead, we bypass this issue by aiming to directly learn a predictor for documents\u2014that is, a function that given a document, predicts its mixture over topics\u2014without explicitly learning topic distributions. Another way to think of this is that our goal is not to learn a model that could be used to write a new document, but instead just a model that could be used to classify a document written by others. This is much as in standard supervised learning where algorithms such as SVMs learn a decision boundary (such as a linear separator) for making predictions on the labels of examples without explicitly learning the distributions D+ and D\u2212 over positive and negative examples respectively. However, our setting is unsupervised (we are not given labeled data containing the correct classifications of the documents in the training set) and furthermore, rather than each data item belonging to one of the k classes (topics), each data item belongs to a mixture of the k topics. Our goal is given a new data item to output what that mixture is. We begin by describing our high level theoretical formulation. This formulation can be viewed as a generalization both of standard topic modeling and of a setting known as multi-view learning or co-training Blum and Mitchell (1998); Dasgupta et al. (2002); Chapelle et al. (2010); Balcan et al. (2004); Sun (2013).", "startOffset": 17, "endOffset": 3917}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999). As a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution. Better would be a model in which sentences are drawn i.i.d. from a distribution over sentences (this would at least produce grammatical objects and allow for meaningful correlation among related words within a topic, like shooting a free throw or kicking a field goal). Even better would be paragraphs drawn i.i.d. from a distribution over paragraphs (this would at least produce coherent paragraphs). Or, even better, how about a model in which paragraphs are drawn non-independently, so that the second paragraph in a document can depend on what the first paragraph was saying, though presumably with some amount of additional entropy as well? This is the type of model we study here. Note that an immediate problem with considering such a model is that now the task of learning an explicit distribution (over sentences or paragraphs) is hopeless. While a distribution over words can be reasonably viewed as a probability vector, one could not hope to learn or even represent an explicit distribution over sentences or paragraphs. Indeed, except in cases of plagiarism, one would not expect to see the same paragraph twice in the entire corpus. Moreover, this is likely to be true even if we assume paragraphs have some natural feature-vector representation. Instead, we bypass this issue by aiming to directly learn a predictor for documents\u2014that is, a function that given a document, predicts its mixture over topics\u2014without explicitly learning topic distributions. Another way to think of this is that our goal is not to learn a model that could be used to write a new document, but instead just a model that could be used to classify a document written by others. This is much as in standard supervised learning where algorithms such as SVMs learn a decision boundary (such as a linear separator) for making predictions on the labels of examples without explicitly learning the distributions D+ and D\u2212 over positive and negative examples respectively. However, our setting is unsupervised (we are not given labeled data containing the correct classifications of the documents in the training set) and furthermore, rather than each data item belonging to one of the k classes (topics), each data item belongs to a mixture of the k topics. Our goal is given a new data item to output what that mixture is. We begin by describing our high level theoretical formulation. This formulation can be viewed as a generalization both of standard topic modeling and of a setting known as multi-view learning or co-training Blum and Mitchell (1998); Dasgupta et al. (2002); Chapelle et al. (2010); Balcan et al. (2004); Sun (2013). We then describe several natural assumptions under which we can indeed efficiently solve the problem, learning accurate topic mixture predictors.", "startOffset": 17, "endOffset": 3929}, {"referenceID": 0, "context": "Existing work in topic modeling, such as Arora et al. (2012b); Anandkumar et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 0, "context": "(2012b); Anandkumar et al. (2014), provide elegant procedures for handling large noise that is caused by drawing only 2 or 3 words according to the distribution induced by x.", "startOffset": 9, "endOffset": 34}, {"referenceID": 2, "context": "Phase 2 is similar to that of Arora et al. (2012b). Algorithm 1 formalizes the details of this approach.", "startOffset": 30, "endOffset": 51}, {"referenceID": 6, "context": "data sets (Bansal et al., 2014).", "startOffset": 10, "endOffset": 31}, {"referenceID": 11, "context": "Now, using the Davis and Kahan (1970) or Wedin (1972) sin \u03b8 theorem (see Proposition B.", "startOffset": 15, "endOffset": 38}, {"referenceID": 11, "context": "Now, using the Davis and Kahan (1970) or Wedin (1972) sin \u03b8 theorem (see Proposition B.", "startOffset": 15, "endOffset": 54}, {"referenceID": 20, "context": "We use the Matrix Bernstein inequality (Tropp, 2015), described in Appendix B, to demonstrate the concentration of \u2016 1 mDE > + 1 mED \u20162.", "startOffset": 39, "endOffset": 52}, {"referenceID": 12, "context": "For the problem of recovering a kdimensional nullspace, Hardt and Moitra (2013) introduced an efficient algorithm that tolerates agnostic noise up to = O(k/n).", "startOffset": 56, "endOffset": 80}, {"referenceID": 10, "context": "Can we learn ai\u2019s within error of using polynomially many samples? Note that when D is only supported on the corners of \u2206, this problem reduces to learning mixture of Gaussians, for which there is a wealth of literature on estimating Gaussian means and mixture weights (Dasgupta et al., 2002; Kalai et al., 2012; Moitra and Valiant, 2010).", "startOffset": 269, "endOffset": 338}, {"referenceID": 14, "context": "Can we learn ai\u2019s within error of using polynomially many samples? Note that when D is only supported on the corners of \u2206, this problem reduces to learning mixture of Gaussians, for which there is a wealth of literature on estimating Gaussian means and mixture weights (Dasgupta et al., 2002; Kalai et al., 2012; Moitra and Valiant, 2010).", "startOffset": 269, "endOffset": 338}, {"referenceID": 15, "context": "Can we learn ai\u2019s within error of using polynomially many samples? Note that when D is only supported on the corners of \u2206, this problem reduces to learning mixture of Gaussians, for which there is a wealth of literature on estimating Gaussian means and mixture weights (Dasgupta et al., 2002; Kalai et al., 2012; Moitra and Valiant, 2010).", "startOffset": 269, "endOffset": 338}], "year": 2016, "abstractText": "Recently there has been significant activity in developing algorithms with provable guarantees for topic modeling. In standard topic models, a topic (such as sports, business, or politics) is viewed as a probability distribution ai over words, and a document is generated by first selecting a mixture w over topics, and then generating words i.i.d. from the associated mixture Aw. Given a large collection of such documents, the goal is to recover the topic vectors and then to correctly classify new documents according to their topic mixture. In this work we consider a broad generalization of this framework in which words are no longer assumed to be drawn i.i.d. and instead a topic is a complex distribution over sequences of paragraphs. Since one could not hope to even represent such a distribution in general (even if paragraphs are given using some natural feature representation), we aim instead to directly learn a document classifier. That is, we aim to learn a predictor that given a new document, accurately predicts its topic mixture, without learning the distributions explicitly. We present several natural conditions under which one can do this efficiently and discuss issues such as noise tolerance and sample complexity in this model. More generally, our model can be viewed as a generalization of the multi-view or co-training setting in machine learning. \u2217Supported in part by National Science Foundation grants CCF-1525971 and CCF-1535967. \u2020Supported in part by National Science Foundation grant CCF-1525971 and by a Microsoft Research Graduate Fellowship and an IBM Ph.D Fellowship. ar X iv :1 61 1. 01 25 9v 1 [ cs .L G ] 4 N ov 2 01 6", "creator": "LaTeX with hyperref package"}}}