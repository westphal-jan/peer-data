{"id": "1611.01587", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2016", "title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search. Instead, it uses a single feed-forward pass for its model to work seamlessly with the rest of the network.\n\n\n\nThe new approach to hierarchical learning is not easy. As I said earlier, the notion of multiple classes is a bit of a novelty. What makes a cluster, with three classes, quite special? Many of these classes are hierarchical:\nWe have a set of features:\nA general language model: hierarchical learning\nLanguages are a single set of features:\nA hierarchical learning algorithm:\nWe are a set of features:\nWe have a model of a clustering model:\nThe model of a cluster is a single model:\nOur model of a cluster is a single model:\nIt is a single model:\nWe have a model of a cluster:\nOur model of a cluster is a single model:\nTo help illustrate these features, I'm using a series of examples of hierarchical learning. One has a few examples. The example we use for this tutorial is the example we show by showing a model of a cluster called VBK, which is a model for clustering. In each instance, VBK consists of two groups of objects that are connected in a single network: a cluster of two VBKs and two VBKs that are linked to each other.\nEach group consists of two VBKs. In each instance, VBKs are related to each other. This group is related to each other. This group is related to each other. This group is related to each other. This group is related to each other. This group", "histories": [["v1", "Sat, 5 Nov 2016 01:59:29 GMT  (729kb,D)", "http://arxiv.org/abs/1611.01587v1", "Under review as a conference paper at ICLR 2017"], ["v2", "Fri, 11 Nov 2016 01:16:07 GMT  (731kb,D)", "http://arxiv.org/abs/1611.01587v2", "Under review as a conference paper at ICLR 2017 (Expanded Appendix for further explanation and analysis)"], ["v3", "Sat, 19 Nov 2016 00:20:12 GMT  (731kb,D)", "http://arxiv.org/abs/1611.01587v3", "Under review as a conference paper at ICLR 2017 (Expanded Appendix for further explanation and analysis; Expanded ablation analysis in Section 6.3)"], ["v4", "Sun, 16 Apr 2017 22:38:21 GMT  (73kb,D)", "http://arxiv.org/abs/1611.01587v4", "Expanded Appendix for further explanation and analysis; Reported additional results"], ["v5", "Mon, 24 Jul 2017 14:41:16 GMT  (79kb,D)", "http://arxiv.org/abs/1611.01587v5", "Accepted as a full paper at the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017)"]], "COMMENTS": "Under review as a conference paper at ICLR 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["kazuma hashimoto", "caiming xiong", "yoshimasa tsuruoka", "richard socher"], "accepted": true, "id": "1611.01587"}, "pdf": {"name": "1611.01587.pdf", "metadata": {"source": "CRF", "title": "A JOINT MANY-TASK MODEL: GROWING A NEURAL NETWORK FOR MULTIPLE NLP TASKS", "authors": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher"], "emails": ["tsuruoka}@logos.t.u-tokyo.ac.jp", "rsocher}@salesforce.com"], "sections": [{"heading": null, "text": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task\u2019s loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search."}, {"heading": "1 INTRODUCTION", "text": "The potential for leveraging multiple levels of representation has been demonstrated in a variety of ways in the field of Natural Language Processing (NLP). For example, Part-Of-Speech (POS) tags are used to train syntactic parsers. The parsers are used to improve higher-level tasks, such as natural language inference (Chen et al., 2016), relation classification (Socher et al., 2012), sentiment analysis (Socher et al., 2013; Tai et al., 2015), or machine translation (Eriguchi et al., 2016). However, higher level tasks are not usually able to improve lower level tasks, often because systems are pipelines and not trained end-to-end.\nIn deep learning, unsupervised word vectors are useful representations and often used to initialize recurrent neural networks for subsequent tasks (Pennington et al., 2014). However, not being jointly trained, deep NLP models have yet shown benefits from predicting many (> 4) increasingly complex linguistic tasks each at a successively deeper layer. Instead, existing models are often designed to predict different tasks either entirely separately or at the same depth (Collobert et al., 2011), ignoring linguistic hierarchies.\nWe introduce a Joint Many-Task (JMT) model, outlined in Fig. 1, which predicts increasingly complex NLP tasks at successively deeper layers. Unlike traditional NLP pipeline systems, our single JMT model can be trained end-to-end for POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment. We propose an adaptive training and regularization strategy to grow this model in its depth. With the help of this strategy we avoid catastrophic interference between tasks, and instead show that both lower and higher level tasks benefit from the joint training. Our model is influenced by the observation of S\u00f8gaard & Goldberg (2016) who showed that predicting two different tasks is more accurate when performed in different layers than in the same layer (Collobert et al., 2011).\n\u2217Work was done while the first author was an intern at Salesforce Research. \u2020Corresponding author.\nar X\niv :1\n61 1.\n01 58\n7v 1\n[ cs\n.C L\n] 5\nN ov\n2 01\n6"}, {"heading": "2 THE JOINT MANY-TASK MODEL", "text": "In this section, we assume that the model is trained and describe its inference procedure. We begin at the lowest level and work our way to higher layers and more complex tasks."}, {"heading": "2.1 WORD REPRESENTATIONS", "text": "For each wordwt in the input sentence s of length L, we construct a representation by concatenating a word and a character embedding.\nWord embeddings: We use Skip-gram (Mikolov et al., 2013) to train a word embedding matrix, which will be shared across all of the tasks. The words which are not included in the vocabulary are mapped to a special UNK token.\nCharacter n-gram embeddings: Character n-gram embeddings are learned using the same skipgram objective function as the word vectors. We construct the vocabulary of the character n-grams in the training data and assign an embedding for each character n-gram. The final character embedding is the average of the unique character n-gram embeddings of a word wt.1 For example, the character n-grams (n = 1, 2, 3) of the word \u201cCat\u201d are {C, a, t, #BEGIN#C, Ca, at, t#END#, #BEGIN#Ca, Cat, at#END#}, where \u201c#BEGIN#\u201d and \u201c#END#\u201d represent the beginning and the end of each word, respectively. The use of the character n-gram embeddings efficiently provides morphological features and information about unknown words. The training procedure for character n-grams is described in Section 3.1. Each word is subsequently represented as xt, the concatenation of its corresponding word and character vectors."}, {"heading": "2.2 WORD-LEVEL TASK: POS TAGGING", "text": "The first layer of the model is a bi-directional LSTM (Graves & Schmidhuber, 2005; Hochreiter & Schmidhuber, 1997) whose hidden states are used to predict POS tags. We use the following Long Short-Term Memory (LSTM) units for the forward direction:\nit = \u03c3 (Wigt + bi) , ft = \u03c3 (Wfgt + bf ) , ot = \u03c3 (Wogt + bo) , ut = tanh (Wugt + bu) , ct = it ut + ft ct\u22121, ht = ot tanh (ct) , (1)\n1Wieting et al. (2016) used a nonlinearity, but we have observed that the simple averaging also works well.\nwhere we define the input gt as gt = [ \u2212\u2192 h t\u22121;xt], i.e. the concatenation of the previous hidden state and the word representation of wt. The backward pass is expanded in the same way, but a different set of weights are used.\nFor predicting the POS tag of wt, we use the concatenation of the forward and backward states in a one-layer bi-LSTM layer corresponding to the t-th word: ht = [ \u2212\u2192 h t; \u2190\u2212 h t]. Then each ht (1 \u2264 t \u2264 L) is fed into a standard softmax classifier with a single ReLU layer which outputs the probability vector y(1) for each of the POS tags."}, {"heading": "2.3 WORD-LEVEL TASK: CHUNKING", "text": "Chunking is also a word-level classification task which assigns a chunking tag (B-NP, I-VP, etc.) for each word. The tag specifies the region of major phrases (or chunks) in the sentence.\nChunking is performed in the second bi-LSTM layer on top of the POS layer. When stacking the bi-LSTM layers, we use Eq. (1) with input g(2)t = [h (2) t\u22121;h (1) t ;xt; y (pos) t ], where h (1) t is the hidden state of the first (POS) layer. We define the weighted label embedding y(pos)t as follows:\ny (pos) t = C\u2211 j=1 p(y (1) t = j|h (1) t )`(j), (2)\nwhere C is the number of the POS tags, p(y(1)t = j|h (1) t ) is the probability value that the j-th POS tag is assigned to wt, and `(j) is the corresponding label embedding. The probability values are automatically predicted by the POS layer working like a built-in POS tagger, and thus no gold POS tags are needed. This output embedding can be regarded as a similar feature to the K-best POS tag feature which has been shown to be effective in syntactic tasks (Andor et al., 2016; Alberti et al., 2015). For predicting the chunking tags, we employ the same strategy as POS tagging by using the concatenated bi-directional hidden states h(2)t = [ \u2212\u2192 h (2) t ; \u2190\u2212 h (2) t ] in the chunking layer. We also use a single ReLU hidden layer before the classifier."}, {"heading": "2.4 SYNTACTIC TASK: DEPENDENCY PARSING", "text": "Dependency parsing identifies syntactic relationships (such as an adjective modifying a noun) between pairs of words in a sentence. We use the third bi-LSTM layer on top of the POS and chunking layers to classify relationships between all pairs of words. The input vector for the LSTM includes hidden states, word representations, and the label embeddings for the two previous tasks: g (3) t = [h (3) t\u22121;h (2) t ;xt; (y (pos) t + y (chk) t )], where we computed the chunking vector in a similar fashion as the POS vector in Eq. (2). The POS and chunking tags are commonly used to improve dependency parsing (Attardi & DellOrletta, 2008).\nLike a sequential labeling task, we simply predict the parent node (head) for each word in the sentence. Then a dependency label is predicted for each of the child-parent node pairs. To predict the parent node of the t-th word wt, we define a matching function between wt and the candidates of the parent node as m (t, j) = h(3)t T Wdh (3) j , where Wd is a parameter matrix. For the root, we\ndefine h(3)L+1 = r as a parameterized vector. To compute the probability that wj (or the root node) is the parent of wt, the scores are normalized:\np(j|h(3)t ) = exp (m (t, j))\u2211L+1\nk=1,k 6=t exp (m (t, k)) , (3)\nwhere L is the sentence length.\nNext, the dependency labels are predicted using [h(3)t ;h (3) j ] as input to a standard softmax classifier with a single ReLU layer. At test time, we greedily select the parent node and the dependency label for each word in the sentence.2 At training time, we use the gold child-parent pairs to train the label predictor."}, {"heading": "2.5 SEMANTIC TASK: SEMANTIC RELATEDNESS", "text": "The next two tasks model the semantic relationships between two input sentences. The first task measures the semantic relatedness between two sentences. The output is a real-valued relatedness score for the input sentence pair. The second task is a textual entailment task, which requires one to determine whether a premise sentence entails a hypothesis sentence. There are typically three classes: entailment, contradiction, and neutral.\nThe two semantic tasks are closely related to each other. If the semantic relatedness between two sentences is very low, they are unlikely to entail each other. Based on this intuition and to make use of the information from lower layers, we use the fourth and fifth bi-LSTM layer for the relatedness and entailment task, respectively.\n2This method currently assumes that each word has only one parent node, but it can be expanded to handle multiple parent nodes, which leads to cyclic graphs.\nNow it is required to obtain the sentence-level representation rather than the word-level representation h(4)t used in the first three tasks. We compute the sentence-level representation h (4) s as the element-wise maximum values across all of the word-level representations in the fourth layer:\nh(4)s = max ( h (4) 1 , h (4) 2 , . . . , h (4) L ) . (4)\nTo model the semantic relatedness between s and s\u2032, we follow Tai et al. (2015). The feature vector for representing the semantic relatedness is computed as follows:\nd1(s, s \u2032) = [\u2223\u2223\u2223h(4)s \u2212 h(4)s\u2032 \u2223\u2223\u2223 ;h(4)s h(4)s\u2032 ] , (5) where\n\u2223\u2223\u2223h(4)s \u2212 h(4)s\u2032 \u2223\u2223\u2223 is the absolute values of the element-wise subtraction, and h(4)s h(4)s\u2032 is the element-wise multiplication. Both of them can be regarded as two different similarity metrics of the two vectors. Then d1(s, s\u2032) is fed into a softmax classifier with a single Maxout hidden layer (Goodfellow et al., 2013) to output a relatedness score (from 1 to 5 in our case) for the sentence pair."}, {"heading": "2.6 SEMANTIC TASK: TEXTUAL ENTAILMENT", "text": "For entailment classification between two sentences, we also use the max-pooling technique as in the semantic relatedness task. To classify the premise-hypothesis pair into one of the three classes, we compute the feature vector d2(s, s\u2032) as in Eq. (5) except that we do not use the absolute values of the element-wise subtraction, because we need to identify which is the premise (or hypothesis). Then d2(s, s\u2032) is fed into a standard softmax classifier.\nTo make use of the output from the relatedness layer directly, we use the label embeddings for the relatedness task. More concretely, we compute the class label embeddings for the semantic relatedness task similar to Eq. (2). The final feature vectors that are concatenated and fed into the entailment classifier are the weighted relatedness label embedding and the feature vector d2(s, s\u2032).3 We use three Maxout hidden layers before the classifier."}, {"heading": "3 TRAINING THE JMT MODEL", "text": "The model is trained jointly over all datasets. During each epoch, the optimization iterates over each full training dataset in the same order as the corresponding tasks described in the modeling section."}, {"heading": "3.1 PRE-TRAINING WORD REPRESENTATIONS", "text": "We pre-train word embeddings using the Skip-gram model with negative sampling (Mikolov et al., 2013). We also pre-train the character n-gram embeddings using Skip-gram. The only difference is that each input word embedding in the Skip-gram model is replaced with its corresponding average embedding of the character n-gram embeddings described in Section 2.1. These embeddings are fine-tuned during the training of our JMT model. We denote the embedding parameters as \u03b8e."}, {"heading": "3.2 TRAINING THE POS LAYER", "text": "Let \u03b8POS = (WPOS, bPOS, \u03b8e) denote the set of model parameters associated with the POS layer, where WPOS is the set of the weight matrices in the first bi-LSTM and the classifier, and bPOS is the set of the bias vectors. The objective function to optimize \u03b8POS is defined as follows:\n\u2212 \u2211 s \u2211 t log p ( y (1) t = \u03b1|h (1) t ) + \u03bb\u2016WPOS\u20162 + \u03b4\u2016\u03b8e \u2212 \u03b8\u2032e\u20162, (6)\nwhere p(y(1)t = \u03b1wt |h (1) t ) is the probability value that the correct label \u03b1 is assigned to wt in the sentence s, \u03bb\u2016WPOS\u20162 is the L2-norm regularization term, and \u03bb is a hyperparameter. 3This modification does not affect the LSTM transitions, and thus it is still possible to add other singlesentence-level tasks on top of our model.\nWe call the second regularization term \u03b4\u2016\u03b8e\u2212 \u03b8\u2032e\u20162 a successive regularization term. The successive regularization is based on the idea that we do not want the model to forget the information learned for the other tasks. In the case of POS tagging, the regularization is applied to \u03b8e, and \u03b8\u2032e is the embedding parameter after training the final task in the top-most layer at the previous training epoch. \u03b4 is a hyperparameter."}, {"heading": "3.3 TRAINING THE CHUNKING LAYER", "text": "The objective function is defined as follows: \u2212 \u2211 s \u2211 t log p(y (2) t = \u03b1|h (2) t ) + \u03bb\u2016Wchunk\u20162 + \u03b4\u2016\u03b8POS \u2212 \u03b8\u2032POS\u20162, (7)\nwhich is similar to that of POS tagging, and \u03b8chunk is (Wchunk, bchunk, EPOS, \u03b8e), where Wchunk and bchunk are the weight and bias parameters including those in \u03b8POS, and EPOS is the set of the POS label embeddings. \u03b8\u2032POS is the one after training the POS layer at the current training epoch."}, {"heading": "3.4 TRAINING THE DEPENDENCY LAYER", "text": "The objective function is defined as follows: \u2212 \u2211 s \u2211 t log p(\u03b1|h(3)t )p(\u03b2|h (3) t , h (3) \u03b1 ) + \u03bb(\u2016Wdep\u20162 + \u2016Wd\u20162) + \u03b4\u2016\u03b8chunk \u2212 \u03b8\u2032chunk\u20162, (8)\nwhere p(\u03b1|h(3)t ) is the probability value assigned to the correct parent node \u03b1 for wt, and p(\u03b2|h(3)t , h (3) \u03b1 ) is the probability value assigned to the correct dependency label \u03b2 for the childparent pair (wt, \u03b1). \u03b8dep is defined as (Wdep, bdep,Wd, r, EPOS, Echunk, \u03b8e), where Wdep and bdep are the weight and bias parameters including those in \u03b8chunk, and Echunk is the set of the chunking label embeddings."}, {"heading": "3.5 TRAINING THE RELATEDNESS LAYER", "text": "Following Tai et al. (2015), the objective function is defined as follows:\u2211 (s,s\u2032) KL ( p\u0302(s, s\u2032) \u2225\u2225\u2225p(h(4)s , h(4)s\u2032 ))+ \u03bb\u2016Wrel\u20162 + \u03b4\u2016\u03b8dep \u2212 \u03b8\u2032dep\u20162 (9) where p\u0302(s, s\u2032) is the gold distribution over the defined relatedness scores, p(h(4)s , h (4) s\u2032 ) is the pre-\ndicted distribution given the the sentence representations, and KL ( p\u0302(s, s\u2032) \u2225\u2225\u2225p(h(4)s , h(4)s\u2032 )) is the KL-divergence between the two distributions. \u03b8rel is defined as (Wrel, brel, EPOS, Echunk, \u03b8e)."}, {"heading": "3.6 TRAINING THE ENTAILMENT LAYER", "text": "The objective function is defined as follows: \u2212 \u2211 (s,s\u2032) log p(y (5) (s,s\u2032) = \u03b1|h (5) s , h (5) s\u2032 ) + \u03bb\u2016Went\u2016 2 + \u03b4\u2016\u03b8rel \u2212 \u03b8\u2032rel\u20162, (10)\nwhere p(y(5)(s,s\u2032) = \u03b1|h (5) s , h (5) s\u2032 ) is the probability value that the correct label \u03b1 is assigned to the premise-hypothesis pair (s, s\u2032). \u03b8ent is defined as (Went, bent, EPOS, Echunk, Erel, \u03b8e), where Erel is the set of the relatedness label embeddings."}, {"heading": "4 RELATED WORK", "text": "Many deep learning approaches have proven to be effective in a variety of NLP tasks and are becoming more and more complex. They are typically designed to handle single tasks, or some of them are designed as general-purpose models (Kumar et al., 2016; Sutskever et al., 2014) but applied to different tasks independently.\nFor handling multiple NLP tasks, multi-task learning models with deep neural networks have been proposed (Collobert et al., 2011; Luong et al., 2016), and more recently S\u00f8gaard & Goldberg (2016) have suggested that using different layers for different tasks is more effective than using the same layer in jointly learning closely-related tasks, such as POS tagging and chunking. However, the number of tasks was limited or they have very similar task settings like word-level tagging, and it was not clear how lower-level tasks could be also improved by combining higher-level tasks.\nIn the field of computer vision, some transfer and multi-task learning approaches have also been proposed (Li & Hoiem, 2016; Misra et al., 2016). For example, Misra et al. (2016) proposed a multi-task learning model to handle different tasks. However, they assume that each data sample has annotations for the different tasks, and do not explicitly consider task hierarchies.\nRecently, Rusu et al. (2016) have proposed a progressive neural network model to handle multiple reinforcement learning tasks, such as Atari games. Like our JMT model, their model is also successively trained according to different tasks using different layers called columns in their paper. In their model, once the first task is completed, the model parameters for the first task are fixed, and then the second task is handled by adding new model parameters. Therefore, accuracy of the previously trained tasks is never improved. In NLP tasks, multi-task learning has the potential to improve not only higher-level tasks, but also lower-level tasks. Rather than fixing the pre-trained model parameters, our successive regularization allows our model to continuously train the lower-level tasks without significant accuracy drops."}, {"heading": "5 EXPERIMENTAL SETTINGS", "text": ""}, {"heading": "5.1 DATASETS", "text": "POS tagging: To train the POS tagging layer, we used the Wall Street Journal (WSJ) portion of Penn Treebank, and followed the standard split for the training (Section 0-18), development (Section 19- 21), and test (Section 22-24) sets. The evaluation metric is the word-level accuracy.\nChunking: For chunking, we also used the WSJ corpus, and followed the standard split for the training (Section 15-18) and test (Section 20) sets as in the CoNLL 2000 shared task. We used Section 19 as the development set, following S\u00f8gaard & Goldberg (2016), and employed the IOBES tagging scheme. The evaluation metric is the F1 score defined in the shared task.\nDependency parsing: We also used the WSJ corpus for dependency parsing, and followed the standard split for the training (Section 2-21), development (Section 22), and test (Section 23) sets. We converted the treebank data to Stanford style dependencies using the version 3.3.0 of the Stanford converter. The evaluation metrics are the Unlabeled Attachment Score (UAS) and the Labeled Attachment Score (LAS), and punctuations are excluded for the evaluation.\nSemantic relatedness: For the semantic relatedness task, we used the SICK dataset (Marelli et al., 2014), and followed the standard split for the training (SICK train.txt), development (SICK trial.txt), and test (SICK test annotated.txt) sets. The evaluation metric is the Mean Squared Error (MSE) between the gold and predicted scores.\nTextual entailment: For textual entailment, we also used the SICK dataset and exactly the same data split as the semantic relatedness dataset. The evaluation metric is the accuracy."}, {"heading": "5.2 TRAINING DETAILS", "text": "Pre-training embeddings: We used the word2vec toolkit to pre-train the word embeddings. We created our training corpus by selecting lowercased English Wikipedia text and obtained 100- dimensional Skip-gram word embeddings trained with the context window size 1, the negative sampling method (15 negative samples), and the sub-sampling method (10\u22125 of the sub-sampling coefficient).4 We also pre-trained the character n-gram embeddings using the same parameter settings with the case-sensitive Wikipedia text. We trained the character n-gram embeddings for n = 1, 2, 3, 4 in the pre-training step.\n4It is empirically known that such a small window size in leads to better results on syntactic tasks than large window sizes. Moreover, we have found that such word embeddings work well even on the semantic tasks.\nEmbedding initialization: We used the pre-trained word embeddings to initialize the word embeddings, and the word vocabulary was built based on the training data of the five tasks. All words in the training data were included in the word vocabulary, and we employed the word-dropout method (Kiperwasser & Goldberg, 2016) to train the word embedding for the unknown words. We also built the character n-gram vocabulary for n = 2, 3, 4, following Wieting et al. (2016), and the character n-gram embeddings were initialized with the pre-trained embeddings. All of the label embeddings were initialized with uniform random values in [\u2212 \u221a 6/(dim+ C), \u221a 6/(dim+ C)], where dim = 100 is the dimensionality of the label embeddings and C is the number of labels.\nWeight initialization: The dimensionality of the hidden layers in the bi-LSTMs was set to 100. We initialized all of the softmax parameters and bias vectors, except for the forget biases in the LSTMs, with zeros, and the weight matrix Wd and the root node vector r for dependency parsing were also initialized with zeros. All of the forget biases were initialized with ones. The other weight matrices were initialized with uniform random values in [\u2212 \u221a 6/(row + col), \u221a 6/(row + col)], where row and col are the number of rows and columns of the matrices, respectively.\nOptimization: At each epoch, we trained our model in the order of POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment. We used mini-batch stochastic gradient decent to train our model. The mini-batch size was set to 25 for POS tagging, chunking, and the SICK tasks, and 15 for dependency parsing. We used a gradient clipping strategy with growing clipping values for the different tasks; concretely, we employed the simple function: min(3.0, depth), where depth is the number of bi-LSTM layers involved in each task, and 3.0 is the maximum value. The learning rate at the k-th epoch was set to \u03b51.0+\u03c1(k\u22121) , where \u03b5 is the initial learning rate, and \u03c1 is the hyperparameter to decrease the learning rate. We set \u03b5 to 1.0 and \u03c1 to 0.3. At each epoch, the same learning rate was shared across all of the tasks.\nRegularization: We set the regularization coefficient to 10\u22126 for the LSTM weight matrices, 10\u22125 for the weight matrices in the classifiers, and 10\u22123 for the successive regularization term excluding the classifier parameters of the lower-level tasks, respectively. The successive regularization coefficient for the classifier parameters was set to 10\u22122. We also used dropout (Hinton et al., 2012). The dropout rate was set to 0.2 for the vertical connections in the multi-layer bi-LSTMs (Pham et al., 2014), the word representations and the label embeddings of the entailment layer, and the classifier of the POS tagging, chunking, dependency parsing, and entailment. A different dropout rate of 0.4 was used for the word representations and the label embeddings of the POS, chunking, and dependency layers, and the classifier of the relatedness layer."}, {"heading": "6 RESULTS AND DISCUSSION", "text": ""}, {"heading": "6.1 SUMMARY OF MULTI-TASK RESULTS", "text": "Table 1 shows our results of the test sets on the five different tasks.5 The column \u201cSingle\u201d shows the results of handling each task separately using single-layer bi-LSTMs, and the column \u201cJMTall\u201d shows the results of our JMT model. The single task settings only use the annotations of their own tasks. For example, when treating dependency parsing as a single task, the POS and chunking tags are not used. We can see that all results of the five different tasks are improved in our JMT model, which shows that our JMT model can handle the five different tasks in a single model. Our JMT model allows us to access arbitrary information learned from the different tasks. If we want to use the model just as a POS tagger, we can use the output from the first bi-LSTM layer. The output can be the weighted POS label embeddings as well as the discrete POS tags.\nTable 1 also shows the results of three subsets of the different tasks. For example, in the case of \u201cJMTABC\u201d, only the first three layers of the bi-LSTMs are used to handle the three tasks. In the case of \u201cJMTDE\u201d, only the top two layers are used just as a two-layer bi-LSTM by omitting all information from the first three layers. The results of the closely-related tasks show that our JMT model improves not only the high-level tasks, but also the low-level tasks.\n5The development and test sentences of the chunking dataset are included in the dependency parsing dataset, although our model does not explicitly use the chunking annotations of the development and test data. In such cases, we show the results in parentheses.\nTable 2: POS tagging results.\nMethod MSE JMTall 0.233 JMTDE 0.238 Zhou et al. (2016) 0.243 Tai et al. (2015) 0.253\nTable 5: Semantic relatedness results."}, {"heading": "6.2 COMPARISON WITH PUBLISHED RESULTS", "text": "POS tagging: Table 2 shows the results of POS tagging, and our JMT model achieves the score close to the state-of-the-art results. The best result to date has been achieved by Ling et al. (2015), which uses character-based LSTMs. Incorporating the character-based encoders into our JMT model would be an interesting direction, but we have shown that the simple pre-trained character n-gram embeddings lead to the promising result.\nChunking: Table 3 shows the results of chunking, and our JMT model achieves the state-of-the-art result. S\u00f8gaard & Goldberg (2016) proposed to jointly learn POS tagging and chunking in different layers, but they only showed improvement for chunking. By contrast, our results show that the low-level tasks are also improved by the joint learning.\nDependency parsing: Table 4 shows the results of dependency parsing by using only the WSJ corpus in terms of the dependency annotations, and our JMT model achieves the state-of-the-art result.6 It is notable that our simple greedy dependency parser outperforms the previous state-ofthe-art result which is based on beam search with global information. The result suggests that the bi-LSTMs efficiently capture global information necessary for dependency parsing. Moreover, our single task result already achieves high accuracy without the POS and chunking information.\nSemantic relatedness: Table 5 shows the results of the semantic relatedness task, and our JMT model achieves the state-of-the-art result. The result of \u201cJMTDE\u201d is already better than the previous state-of-the-art results. Both of Zhou et al. (2016) and Tai et al. (2015) explicitly used syntactic tree structures, and Zhou et al. (2016) relied on attention mechanisms. However, our method uses the simple max-pooling strategy, which suggests that it is worth investigating such simple methods before developing complex methods for simple tasks. Currently, our JMT model does not explicitly use the learned dependency structures, and thus the explicit use of the output from the dependency layer should be an interesting direction of future work.\n6Choe & Charniak (2016) employed the tri-training technique to expand the training data with automatically-generated 400,000 trees in addition to the WSJ corpus, and reported 95.9 UAS and 94.1 LAS.\nTextual entailment: Table 6 shows the results of textual entailment, and our JMT model achieves the state-of-the-art result.7 The previous state-of-the-art result in Yin et al. (2016) relied on attention mechanisms and dataset-specific data pre-processing and features. Again, our simple max-pooling strategy achieves the state-of-the-art result boosted by the joint training. These results show the importance of jointly handling related tasks."}, {"heading": "6.3 ANALYSIS ON MULTI-TASK LEARNING ARCHITECTURES", "text": "Here, we first investigate the effects of using deeper layers for the five different single tasks. We then show the effectiveness of our proposed model and training strategy: the successive regularization, the shortcut connections of the word representations, the embeddings of the output labels, the character n-gram embeddings, and the use of the different layers for the different tasks. All of the results shown in this section are the development set results.\nDepth: The single task settings shown in Table 1 are obtained by using single layer bi-LSTMs, but in our JMT model, the higher-level tasks use successively deeper layers. To investigate the gap between the different number of the layers for each task, we also show the results of using multilayer bi-LSTMs for the single task settings, in the column of \u201cSingle+\u201d in Table 7. More concretely, we use the same number of the layers with our JMT model; for example, three layers are used for dependency parsing, and five layers are used for textual entailment. As shown in these results, deeper layers do not always lead to better results, and the joint learning is more important than making the models complex for only single tasks.\nSuccessive regularization In Table 7, the column of \u201cw/o SR\u201d shows the results of omitting the successive regularization terms described in Section 3. We can see that the accuracy of chunking is improved by the successive regularization, while other results are not affected so much. The chunking dataset used here is relatively small compared with other low-level tasks, POS tagging and dependency parsing. Thus, these results suggest that the successive regularization is effective when dataset sizes are imbalanced.\nShortcut connections: Our JMT model feeds the word representations into all of the bi-LSTM layers, which is called the shortcut connection. Table 7 shows the results of \u201cJMTall\u201d with and without the shortcut connections. The results without the shortcut connections are shown in the column of \u201cw/o SC\u201d. These results clearly show that the importance of the shortcut connections in our JMT model, and in particular, the semantic tasks in the higher layers strongly rely on the shortcut connections. That is, simply stacking the LSTM layers is not sufficient to handle a variety of NLP tasks in a single model. In Appendix A, we show how the shared word representations change according to each task (or layer).\nOutput label embeddings: Table 7 also shows the results without using the output labels of the POS, chunking, and relatedness layers, in the column of \u201cw/o LE\u201d. These results show that the explicit use of the output information from the classifiers of the lower layers is important in our JMT model. The results in the last column of \u201cw/o SC&LE\u201d are the ones without both of the shortcut connections and the label embeddings.\n7The result of \u201cJMTall\u201d is slightly worse than that of \u201cJMTDE\u201d, but the difference is not significant because the training data is small.\nCharacter n-gram embeddings: Table 8 shows the results for the three single tasks, POS tagging, chunking, and dependency parsing, with and without the pre-trained character n-gram embeddings. The column of \u201cW&C\u201d corresponds to using both of the word and character n-gram embeddings, and that of \u201cOnly W\u201d corresponds to using only the word embeddings. These results clearly show that jointly using the pre-trained word and character n-gram embeddings is helpful in improving the results. The pre-training of the character n-gram embeddings is also effective; for example, without the pre-training, the POS accuracy drops from 97.52% to 97.38% and the chunking accuracy drops from 95.65% to 95.14%, but they are still better than those of using word2vec embeddings alone.\nDifferent layers for different tasks: Table 9 shows the results for the three tasks of our \u201cJMTABC\u201d setting and that of not using the shortcut connections and the label embeddings as in Table 7. In addition, in the column of \u201cAll-3\u201d, we show the results of using the highest (i.e., the third) layer for all of the three tasks without any shortcut connections and label embeddings, and thus the two settings \u201cw/o SC&LE\u201d and \u201cAll-3\u201d require exactly the same number of the model parameters. The results show that using the same layers for the three different tasks hampers the effectiveness of our JMT model, and the design of the model is much more important than the number of the model parameters."}, {"heading": "7 CONCLUSION", "text": "We presented a joint many-task model to handle a variety of NLP tasks with growing depth of layers in a single end-to-end deep model. Our model is successively trained by considering linguistic hierarchies, directly connecting word representations to all layers, explicitly using predictions in lower tasks, and applying successive regularization. In our experiments on five different types of NLP tasks, our single model achieves the state-of-the-art results on chunking, dependency parsing, semantic relatedness, and textual entailment."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank the Salesforce Research team members for their fruitful comments and discussions."}, {"heading": "A HOW DO SHARED EMBEDDINGS CHANGE", "text": "In our JMT model, the word and character n-gram embedding matrices are shared across all of the five different tasks. To better qualitatively explain the importance of the shortcut connections shown in Table 7, we inspected how the shared embeddings change when fed into the different biLSTM layers. More concretely, we checked closest neighbors in terms of the cosine similarity for the word representations before and after fed into the forward LSTM layers. In particular, we used the corresponding part of Wu in Eq. (1) to perform linear transformation of the input embeddings, because ut directly affects the hidden states of the LSTMs. Thus, this is a context-independent analysis.\nTable 10 shows the examples of the word \u201cstanding\u201d. The row of \u201cEmbedding\u201d shows the cases of using the shared embeddings, and the others show the results of using the linear-transformed embeddings. In the column of \u201cOnly word\u201d, the results of using only the word embeddings are shown. The closest neighbors in the case of \u201cEmbedding\u201d capture the semantic similarity, but after fed into the POS layer, the semantic similarity is almost washed out. This is not surprising because it is sufficient to cluster the words of the same POS tags: here, NN, VBG, etc. In the chunking layer, the similarity in terms of verbs is captured, and this is because it is sufficient to identify the coarse chunking tags: here, VP. In the dependency layer, the closest neighbors are adverbs, gerunds of verbs, and nouns, and all of them can be child nodes of verbs in dependency trees. However, this information is not sufficient in further classifying the dependency labels. Then we can see that in the column of \u201cWord and char\u201d, jointly using the character n-gram embeddings adds the morphological information, and as shown in Table 8, the LAS score is substantially improved.\nIn the case of semantic tasks, the projected embeddings capture not only syntactic, but also semantic similarities. These results show that different tasks need different aspects of the word similarities, and our JMT model efficiently transforms the shared embeddings for the different tasks by the simple linear transformation. Therefore, without the shortcut connections, the information about the word representations are fed into the semantic tasks after transformed in the lower layers where the semantic similarities are not always important. Indeed, the results of the semantic tasks are very poor without the shortcut connections."}], "references": [{"title": "Improved Transition-Based Parsing and Tagging with Neural Networks", "author": ["Chris Alberti", "David Weiss", "Greg Coppola", "Slav Petrov"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Alberti et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alberti et al\\.", "year": 2015}, {"title": "Globally Normalized Transition-Based Neural Networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Andor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Chunking and Dependency Parsing", "author": ["Giuseppe Attardi", "Felice DellOrletta"], "venue": "In Proceedings of LREC 2008 Workshop on Partial Parsing,", "citeRegEx": "Attardi and DellOrletta.,? \\Q2008\\E", "shortCiteRegEx": "Attardi and DellOrletta.", "year": 2008}, {"title": "Top Accuracy and Fast Dependency Parsing is not a Contradiction", "author": ["Bernd Bohnet"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics,", "citeRegEx": "Bohnet.,? \\Q2010\\E", "shortCiteRegEx": "Bohnet.", "year": 2010}, {"title": "Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference", "author": ["Qian Chen", "Xiaodan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Parsing as Language Modeling", "author": ["Do Kook Choe", "Eugene Charniak"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Choe and Charniak.,? \\Q2016\\E", "shortCiteRegEx": "Choe and Charniak.", "year": 2016}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["Ronan Collobert", "Jason Weston", "Leon Bottou", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "TransitionBased Dependency Parsing with Stack Long Short-Term Memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Tree-to-Sequence Attentional Neural Machine Translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Eriguchi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "Maxout Networks", "author": ["Ian J. Goodfellow", "David Warde-Farley", "Mehdi Mirza", "Aaron Courville", "Yoshua Bengio"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures", "author": ["Alex Graves", "Jurgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves and Schmidhuber.,? \\Q2005\\E", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "CoRR, abs/1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Easy-First Dependency Parsing with Hierarchical Tree LSTMs", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Kiperwasser and Goldberg.,? \\Q2016\\E", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Chunking with Support Vector Machines", "author": ["Taku Kudo", "Yuji Matsumoto"], "venue": "In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Kudo and Matsumoto.,? \\Q2001\\E", "shortCiteRegEx": "Kudo and Matsumoto.", "year": 2001}, {"title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Victor Zhong", "Romain Paulus", "Richard Socher"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Kumar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "Illinois-LH: A Denotational and Distributional Approach to Semantics", "author": ["Alice Lai", "Julia Hockenmaier"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Lai and Hockenmaier.,? \\Q2014\\E", "shortCiteRegEx": "Lai and Hockenmaier.", "year": 2014}, {"title": "Learning without Forgetting", "author": ["Zhizhong Li", "Derek Hoiem"], "venue": "CoRR, abs/1606.09282,", "citeRegEx": "Li and Hoiem.,? \\Q2016\\E", "shortCiteRegEx": "Li and Hoiem.", "year": 2016}, {"title": "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation", "author": ["Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Multi-task Sequence to Sequence Learning", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Lukasz Kaiser"], "venue": "In Proceedings of the 4th International Conference on Learning Representations,", "citeRegEx": "Luong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNsCRF", "author": ["Xuezhe Ma", "Eduard Hovy"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Ma and Hovy.,? \\Q2016\\E", "shortCiteRegEx": "Ma and Hovy.", "year": 2016}, {"title": "SemEval-2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Cross-stitch Networks for Multi-task Learning", "author": ["Ishan Misra", "Abhinav Shrivastava", "Abhinav Gupta", "Martial Hebert"], "venue": null, "citeRegEx": "Misra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Misra et al\\.", "year": 2016}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Dropout improves Recurrent Neural Networks for Handwriting Recognition", "author": ["Vu Pham", "Theodore Bluche", "Christopher Kermorvant", "Jerome Louradour"], "venue": "CoRR, abs/1312.4569,", "citeRegEx": "Pham et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "Semantic Compositionality through Recursive Matrix-Vector Spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Semi-supervised condensed nearest neighbor for part-of-speech tagging", "author": ["Anders S\u00f8gaard"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "S\u00f8gaard.,? \\Q2011\\E", "shortCiteRegEx": "S\u00f8gaard.", "year": 2011}, {"title": "Deep multi-task learning with low level tasks supervised at lower layers", "author": ["Anders S\u00f8gaard", "Yoav Goldberg"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),", "citeRegEx": "S\u00f8gaard and Goldberg.,? \\Q2016\\E", "shortCiteRegEx": "S\u00f8gaard and Goldberg.", "year": 2016}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data", "author": ["Jun Suzuki", "Hideki Isozaki"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Suzuki and Isozaki.,? \\Q2008\\E", "shortCiteRegEx": "Suzuki and Isozaki.", "year": 2008}, {"title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Feature-Rich Partof-Speech Tagging with a Cyclic Dependency Network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Learning with Lookahead: Can HistoryBased Models Rival Globally Optimized Models", "author": ["Yoshimasa Tsuruoka", "Yusuke Miyao", "Jun\u2019ichi Kazama"], "venue": "In Proceedings of the Fifteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Tsuruoka et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tsuruoka et al\\.", "year": 2011}, {"title": "Structured Training for Neural Network Transition-Based Parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "CHARAGRAM: Embedding Words and Sentences via Character n-grams", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Wieting et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2016}, {"title": "ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs", "author": ["Wenpeng Yin", "Hinrich Schtze", "Bing Xiang", "Bowen Zhou"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "Modelling Sentence Pairs with Tree-structured Attentive Encoder", "author": ["Yao Zhou", "Cong Liu", "Yan Pan"], "venue": "In Proceedings of the 26th International Conference on Computational Linguistics,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "The parsers are used to improve higher-level tasks, such as natural language inference (Chen et al., 2016), relation classification (Socher et al.", "startOffset": 87, "endOffset": 106}, {"referenceID": 26, "context": ", 2016), relation classification (Socher et al., 2012), sentiment analysis (Socher et al.", "startOffset": 33, "endOffset": 54}, {"referenceID": 27, "context": ", 2012), sentiment analysis (Socher et al., 2013; Tai et al., 2015), or machine translation (Eriguchi et al.", "startOffset": 28, "endOffset": 67}, {"referenceID": 32, "context": ", 2012), sentiment analysis (Socher et al., 2013; Tai et al., 2015), or machine translation (Eriguchi et al.", "startOffset": 28, "endOffset": 67}, {"referenceID": 8, "context": ", 2015), or machine translation (Eriguchi et al., 2016).", "startOffset": 32, "endOffset": 55}, {"referenceID": 24, "context": "In deep learning, unsupervised word vectors are useful representations and often used to initialize recurrent neural networks for subsequent tasks (Pennington et al., 2014).", "startOffset": 147, "endOffset": 172}, {"referenceID": 6, "context": "Instead, existing models are often designed to predict different tasks either entirely separately or at the same depth (Collobert et al., 2011), ignoring linguistic hierarchies.", "startOffset": 119, "endOffset": 143}, {"referenceID": 6, "context": "Our model is influenced by the observation of S\u00f8gaard & Goldberg (2016) who showed that predicting two different tasks is more accurate when performed in different layers than in the same layer (Collobert et al., 2011).", "startOffset": 194, "endOffset": 218}, {"referenceID": 4, "context": "The parsers are used to improve higher-level tasks, such as natural language inference (Chen et al., 2016), relation classification (Socher et al., 2012), sentiment analysis (Socher et al., 2013; Tai et al., 2015), or machine translation (Eriguchi et al., 2016). However, higher level tasks are not usually able to improve lower level tasks, often because systems are pipelines and not trained end-to-end. In deep learning, unsupervised word vectors are useful representations and often used to initialize recurrent neural networks for subsequent tasks (Pennington et al., 2014). However, not being jointly trained, deep NLP models have yet shown benefits from predicting many (> 4) increasingly complex linguistic tasks each at a successively deeper layer. Instead, existing models are often designed to predict different tasks either entirely separately or at the same depth (Collobert et al., 2011), ignoring linguistic hierarchies. We introduce a Joint Many-Task (JMT) model, outlined in Fig. 1, which predicts increasingly complex NLP tasks at successively deeper layers. Unlike traditional NLP pipeline systems, our single JMT model can be trained end-to-end for POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment. We propose an adaptive training and regularization strategy to grow this model in its depth. With the help of this strategy we avoid catastrophic interference between tasks, and instead show that both lower and higher level tasks benefit from the joint training. Our model is influenced by the observation of S\u00f8gaard & Goldberg (2016) who showed that predicting two different tasks is more accurate when performed in different layers than in the same layer (Collobert et al.", "startOffset": 88, "endOffset": 1593}, {"referenceID": 22, "context": "Word embeddings: We use Skip-gram (Mikolov et al., 2013) to train a word embedding matrix, which will be shared across all of the tasks.", "startOffset": 34, "endOffset": 56}, {"referenceID": 1, "context": "This output embedding can be regarded as a similar feature to the K-best POS tag feature which has been shown to be effective in syntactic tasks (Andor et al., 2016; Alberti et al., 2015).", "startOffset": 145, "endOffset": 187}, {"referenceID": 0, "context": "This output embedding can be regarded as a similar feature to the K-best POS tag feature which has been shown to be effective in syntactic tasks (Andor et al., 2016; Alberti et al., 2015).", "startOffset": 145, "endOffset": 187}, {"referenceID": 32, "context": "To model the semantic relatedness between s and s\u2032, we follow Tai et al. (2015). The feature vector for representing the semantic relatedness is computed as follows:", "startOffset": 62, "endOffset": 80}, {"referenceID": 9, "context": "Then d1(s, s\u2032) is fed into a softmax classifier with a single Maxout hidden layer (Goodfellow et al., 2013) to output a relatedness score (from 1 to 5 in our case) for the sentence pair.", "startOffset": 82, "endOffset": 107}, {"referenceID": 22, "context": "We pre-train word embeddings using the Skip-gram model with negative sampling (Mikolov et al., 2013).", "startOffset": 78, "endOffset": 100}, {"referenceID": 32, "context": "Following Tai et al. (2015), the objective function is defined as follows: \u2211", "startOffset": 10, "endOffset": 28}, {"referenceID": 15, "context": "They are typically designed to handle single tasks, or some of them are designed as general-purpose models (Kumar et al., 2016; Sutskever et al., 2014) but applied to different tasks independently.", "startOffset": 107, "endOffset": 151}, {"referenceID": 30, "context": "They are typically designed to handle single tasks, or some of them are designed as general-purpose models (Kumar et al., 2016; Sutskever et al., 2014) but applied to different tasks independently.", "startOffset": 107, "endOffset": 151}, {"referenceID": 6, "context": "For handling multiple NLP tasks, multi-task learning models with deep neural networks have been proposed (Collobert et al., 2011; Luong et al., 2016), and more recently S\u00f8gaard & Goldberg (2016) have suggested that using different layers for different tasks is more effective than using the same layer in jointly learning closely-related tasks, such as POS tagging and chunking.", "startOffset": 105, "endOffset": 149}, {"referenceID": 19, "context": "For handling multiple NLP tasks, multi-task learning models with deep neural networks have been proposed (Collobert et al., 2011; Luong et al., 2016), and more recently S\u00f8gaard & Goldberg (2016) have suggested that using different layers for different tasks is more effective than using the same layer in jointly learning closely-related tasks, such as POS tagging and chunking.", "startOffset": 105, "endOffset": 149}, {"referenceID": 23, "context": "In the field of computer vision, some transfer and multi-task learning approaches have also been proposed (Li & Hoiem, 2016; Misra et al., 2016).", "startOffset": 106, "endOffset": 144}, {"referenceID": 6, "context": "For handling multiple NLP tasks, multi-task learning models with deep neural networks have been proposed (Collobert et al., 2011; Luong et al., 2016), and more recently S\u00f8gaard & Goldberg (2016) have suggested that using different layers for different tasks is more effective than using the same layer in jointly learning closely-related tasks, such as POS tagging and chunking.", "startOffset": 106, "endOffset": 195}, {"referenceID": 6, "context": "For handling multiple NLP tasks, multi-task learning models with deep neural networks have been proposed (Collobert et al., 2011; Luong et al., 2016), and more recently S\u00f8gaard & Goldberg (2016) have suggested that using different layers for different tasks is more effective than using the same layer in jointly learning closely-related tasks, such as POS tagging and chunking. However, the number of tasks was limited or they have very similar task settings like word-level tagging, and it was not clear how lower-level tasks could be also improved by combining higher-level tasks. In the field of computer vision, some transfer and multi-task learning approaches have also been proposed (Li & Hoiem, 2016; Misra et al., 2016). For example, Misra et al. (2016) proposed a multi-task learning model to handle different tasks.", "startOffset": 106, "endOffset": 763}, {"referenceID": 6, "context": "For handling multiple NLP tasks, multi-task learning models with deep neural networks have been proposed (Collobert et al., 2011; Luong et al., 2016), and more recently S\u00f8gaard & Goldberg (2016) have suggested that using different layers for different tasks is more effective than using the same layer in jointly learning closely-related tasks, such as POS tagging and chunking. However, the number of tasks was limited or they have very similar task settings like word-level tagging, and it was not clear how lower-level tasks could be also improved by combining higher-level tasks. In the field of computer vision, some transfer and multi-task learning approaches have also been proposed (Li & Hoiem, 2016; Misra et al., 2016). For example, Misra et al. (2016) proposed a multi-task learning model to handle different tasks. However, they assume that each data sample has annotations for the different tasks, and do not explicitly consider task hierarchies. Recently, Rusu et al. (2016) have proposed a progressive neural network model to handle multiple reinforcement learning tasks, such as Atari games.", "startOffset": 106, "endOffset": 989}, {"referenceID": 21, "context": "Semantic relatedness: For the semantic relatedness task, we used the SICK dataset (Marelli et al., 2014), and followed the standard split for the training (SICK train.", "startOffset": 82, "endOffset": 104}, {"referenceID": 27, "context": "We used Section 19 as the development set, following S\u00f8gaard & Goldberg (2016), and employed the IOBES tagging scheme.", "startOffset": 53, "endOffset": 79}, {"referenceID": 11, "context": "We also used dropout (Hinton et al., 2012).", "startOffset": 21, "endOffset": 42}, {"referenceID": 25, "context": "2 for the vertical connections in the multi-layer bi-LSTMs (Pham et al., 2014), the word representations and the label embeddings of the entailment layer, and the classifier of the POS tagging, chunking, dependency parsing, and entailment.", "startOffset": 59, "endOffset": 78}, {"referenceID": 34, "context": "We also built the character n-gram vocabulary for n = 2, 3, 4, following Wieting et al. (2016), and the character n-gram embeddings were initialized with the pre-trained embeddings.", "startOffset": 73, "endOffset": 95}, {"referenceID": 16, "context": "55 Ling et al. (2015) 97.", "startOffset": 3, "endOffset": 22}, {"referenceID": 14, "context": "78 Kumar et al. (2016) 97.", "startOffset": 3, "endOffset": 23}, {"referenceID": 14, "context": "78 Kumar et al. (2016) 97.56 Ma & Hovy (2016) 97.", "startOffset": 3, "endOffset": 46}, {"referenceID": 14, "context": "78 Kumar et al. (2016) 97.56 Ma & Hovy (2016) 97.55 S\u00f8gaard (2011) 97.", "startOffset": 3, "endOffset": 67}, {"referenceID": 6, "context": "50 Collobert et al. (2011) 97.", "startOffset": 3, "endOffset": 27}, {"referenceID": 6, "context": "50 Collobert et al. (2011) 97.29 Tsuruoka et al. (2011) 97.", "startOffset": 3, "endOffset": 56}, {"referenceID": 6, "context": "50 Collobert et al. (2011) 97.29 Tsuruoka et al. (2011) 97.28 Toutanova et al. (2003) 97.", "startOffset": 3, "endOffset": 86}, {"referenceID": 27, "context": "77 S\u00f8gaard & Goldberg (2016) 95.", "startOffset": 3, "endOffset": 29}, {"referenceID": 27, "context": "77 S\u00f8gaard & Goldberg (2016) 95.56 Suzuki & Isozaki (2008) 95.", "startOffset": 3, "endOffset": 59}, {"referenceID": 6, "context": "15 Collobert et al. (2011) 94.", "startOffset": 3, "endOffset": 27}, {"referenceID": 6, "context": "15 Collobert et al. (2011) 94.32 Kudo & Matsumoto (2001) 93.", "startOffset": 3, "endOffset": 57}, {"referenceID": 6, "context": "15 Collobert et al. (2011) 94.32 Kudo & Matsumoto (2001) 93.91 Tsuruoka et al. (2011) 93.", "startOffset": 3, "endOffset": 86}, {"referenceID": 0, "context": "42 Andor et al. (2016) 94.", "startOffset": 3, "endOffset": 23}, {"referenceID": 0, "context": "79 Alberti et al. (2015) 94.", "startOffset": 3, "endOffset": 25}, {"referenceID": 0, "context": "79 Alberti et al. (2015) 94.23 92.36 Weiss et al. (2015) 93.", "startOffset": 3, "endOffset": 57}, {"referenceID": 0, "context": "79 Alberti et al. (2015) 94.23 92.36 Weiss et al. (2015) 93.99 92.05 Dyer et al. (2015) 93.", "startOffset": 3, "endOffset": 88}, {"referenceID": 0, "context": "79 Alberti et al. (2015) 94.23 92.36 Weiss et al. (2015) 93.99 92.05 Dyer et al. (2015) 93.10 90.90 Bohnet (2010) 92.", "startOffset": 3, "endOffset": 114}, {"referenceID": 37, "context": "238 Zhou et al. (2016) 0.", "startOffset": 4, "endOffset": 23}, {"referenceID": 32, "context": "243 Tai et al. (2015) 0.", "startOffset": 4, "endOffset": 22}, {"referenceID": 37, "context": "8 Yin et al. (2016) 86.", "startOffset": 2, "endOffset": 20}, {"referenceID": 37, "context": "8 Yin et al. (2016) 86.2 Lai & Hockenmaier (2014) 84.", "startOffset": 2, "endOffset": 50}, {"referenceID": 18, "context": "The best result to date has been achieved by Ling et al. (2015), which uses character-based LSTMs.", "startOffset": 45, "endOffset": 64}, {"referenceID": 18, "context": "The best result to date has been achieved by Ling et al. (2015), which uses character-based LSTMs. Incorporating the character-based encoders into our JMT model would be an interesting direction, but we have shown that the simple pre-trained character n-gram embeddings lead to the promising result. Chunking: Table 3 shows the results of chunking, and our JMT model achieves the state-of-the-art result. S\u00f8gaard & Goldberg (2016) proposed to jointly learn POS tagging and chunking in different layers, but they only showed improvement for chunking.", "startOffset": 45, "endOffset": 431}, {"referenceID": 18, "context": "The best result to date has been achieved by Ling et al. (2015), which uses character-based LSTMs. Incorporating the character-based encoders into our JMT model would be an interesting direction, but we have shown that the simple pre-trained character n-gram embeddings lead to the promising result. Chunking: Table 3 shows the results of chunking, and our JMT model achieves the state-of-the-art result. S\u00f8gaard & Goldberg (2016) proposed to jointly learn POS tagging and chunking in different layers, but they only showed improvement for chunking. By contrast, our results show that the low-level tasks are also improved by the joint learning. Dependency parsing: Table 4 shows the results of dependency parsing by using only the WSJ corpus in terms of the dependency annotations, and our JMT model achieves the state-of-the-art result.6 It is notable that our simple greedy dependency parser outperforms the previous state-ofthe-art result which is based on beam search with global information. The result suggests that the bi-LSTMs efficiently capture global information necessary for dependency parsing. Moreover, our single task result already achieves high accuracy without the POS and chunking information. Semantic relatedness: Table 5 shows the results of the semantic relatedness task, and our JMT model achieves the state-of-the-art result. The result of \u201cJMTDE\u201d is already better than the previous state-of-the-art results. Both of Zhou et al. (2016) and Tai et al.", "startOffset": 45, "endOffset": 1464}, {"referenceID": 18, "context": "The best result to date has been achieved by Ling et al. (2015), which uses character-based LSTMs. Incorporating the character-based encoders into our JMT model would be an interesting direction, but we have shown that the simple pre-trained character n-gram embeddings lead to the promising result. Chunking: Table 3 shows the results of chunking, and our JMT model achieves the state-of-the-art result. S\u00f8gaard & Goldberg (2016) proposed to jointly learn POS tagging and chunking in different layers, but they only showed improvement for chunking. By contrast, our results show that the low-level tasks are also improved by the joint learning. Dependency parsing: Table 4 shows the results of dependency parsing by using only the WSJ corpus in terms of the dependency annotations, and our JMT model achieves the state-of-the-art result.6 It is notable that our simple greedy dependency parser outperforms the previous state-ofthe-art result which is based on beam search with global information. The result suggests that the bi-LSTMs efficiently capture global information necessary for dependency parsing. Moreover, our single task result already achieves high accuracy without the POS and chunking information. Semantic relatedness: Table 5 shows the results of the semantic relatedness task, and our JMT model achieves the state-of-the-art result. The result of \u201cJMTDE\u201d is already better than the previous state-of-the-art results. Both of Zhou et al. (2016) and Tai et al. (2015) explicitly used syntactic tree structures, and Zhou et al.", "startOffset": 45, "endOffset": 1486}, {"referenceID": 18, "context": "The best result to date has been achieved by Ling et al. (2015), which uses character-based LSTMs. Incorporating the character-based encoders into our JMT model would be an interesting direction, but we have shown that the simple pre-trained character n-gram embeddings lead to the promising result. Chunking: Table 3 shows the results of chunking, and our JMT model achieves the state-of-the-art result. S\u00f8gaard & Goldberg (2016) proposed to jointly learn POS tagging and chunking in different layers, but they only showed improvement for chunking. By contrast, our results show that the low-level tasks are also improved by the joint learning. Dependency parsing: Table 4 shows the results of dependency parsing by using only the WSJ corpus in terms of the dependency annotations, and our JMT model achieves the state-of-the-art result.6 It is notable that our simple greedy dependency parser outperforms the previous state-ofthe-art result which is based on beam search with global information. The result suggests that the bi-LSTMs efficiently capture global information necessary for dependency parsing. Moreover, our single task result already achieves high accuracy without the POS and chunking information. Semantic relatedness: Table 5 shows the results of the semantic relatedness task, and our JMT model achieves the state-of-the-art result. The result of \u201cJMTDE\u201d is already better than the previous state-of-the-art results. Both of Zhou et al. (2016) and Tai et al. (2015) explicitly used syntactic tree structures, and Zhou et al. (2016) relied on attention mechanisms.", "startOffset": 45, "endOffset": 1552}, {"referenceID": 37, "context": "7 The previous state-of-the-art result in Yin et al. (2016) relied on attention mechanisms and dataset-specific data pre-processing and features.", "startOffset": 42, "endOffset": 60}], "year": 2017, "abstractText": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task\u2019s loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.", "creator": "LaTeX with hyperref package"}}}