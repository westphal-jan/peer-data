{"id": "1611.01787", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2016", "title": "Learning to superoptimize programs", "abstract": "Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers to perform large-scale transformations. As a result, many optimization programs can be developed to solve this problem by using a particular set of parameters that can be transformed. These parameters can be passed to a code-optimizer or code-optimizer, with the resulting effect: it is possible to transform a program using a type system in order to produce a \"function-complete\": a program is able to do one or more optimizations that are consistent with the underlying code-optimization technique.\n\n\nThe most common form of optimization is a pattern-based transformation.\nA pattern-based transformation involves the following code-optimizer:\npublic String[] type = new String[] { public String value = String[] { public int value = String[] { return true } } public int value = String[] { } } }\nIn case you've seen this, the code is not exactly as straightforward as expected. In fact, it is often a great deal easier to write a program that is a simple statement than some programs that use a specific pattern-based transformation.\nWith this approach, code-optimizer code can be rewritten in a more efficient way with more control. This is one reason that compilers often create complex types such as the form and the output code of a particular program.\nThis is the common form of optimization where the goal is to minimize the overhead that comes with performing an efficient program. Because the code-optimizer is a very complicated, highly efficient method, it will be easier to use more flexibility as the result of the higher cost of code-optimization.\nIf you already have this guide, you may be familiar with the concept of a compiler program which, like compiler programs, takes a variety of steps to generate a program with the same information as the compiler code. For example, in a compiler program, you must build the compiler with the same compiler code as the compiler code.\nThe purpose of a compiler program is to generate a program with the same information as the compiler code. It is the same for the program with the same information as the compiler code", "histories": [["v1", "Sun, 6 Nov 2016 14:35:38 GMT  (549kb,D)", "http://arxiv.org/abs/1611.01787v1", "Submitted to ICLR 2017"], ["v2", "Thu, 23 Feb 2017 14:59:49 GMT  (792kb,D)", "http://arxiv.org/abs/1611.01787v2", "Accepted to ICLR 2017"], ["v3", "Wed, 28 Jun 2017 15:04:46 GMT  (792kb,D)", "http://arxiv.org/abs/1611.01787v3", "Accepted to ICLR 2017"]], "COMMENTS": "Submitted to ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rudy bunel", "alban desmaison", "m pawan kumar", "philip h s torr", "pushmeet kohli"], "accepted": true, "id": "1611.01787"}, "pdf": {"name": "1611.01787.pdf", "metadata": {"source": "CRF", "title": "Learning to superoptimize programs", "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar"], "emails": ["rudy@robots.ox.ac.uk,", "alban@robots.ox.ac.uk,", "pawan@robots.ox.ac.uk,", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Considering the importance of computing to human society, it is not surprising that a very large body of research has gone into the study of the syntax and semantics of programs and programming languages. Code super-optimization is an extremely important problem in this context. Given a program or a snippet of source-code, super-optimization is the task of transforming it to a version that has the same input-output behaviour but can be executed on a target compute architecture more efficiently. In some sense, it is the natural analogue of the paraphrase problem in natural language processing where we want to change syntax without changing semantics. Decades of research has been done on the problem of code optimization resulting in the development of sophisticated rule-based transformation strategies that are used in compilers to allow them to perform code optimization. While modern compilers implement a large set of rewrite rules and are able to achieve impressive speed-ups, they fail to offer any guarantee of optimality, thus leaving room for further improvement. An alternative approach is to search over the space of all possible programs that are equivalent to the compiler output, and select the one that is the most efficient. If the search is carried out in a brute-force manner, we are guaranteed to achieve super-optimization. However, this approach quickly\nar X\niv :1\n61 1.\n01 78\n7v 1\n[ cs\n.L G\n] 6\nN ov\nbecomes computationally infeasible as the number of instructions and the length of the program grows. In order to efficiently perform super-optimization, recent approaches have started to use a stochastic search procedure, inspired by Markov Chain Monte Carlo (MCMC) sampling (Schkufza et al., 2013). Briefly, the search starts at an initial program, such as the compiler output. It iteratively suggests modifications to the program, where the probability of a modification is encoded in a proposal distribution. The modification is either accepted or rejected with a probability that is dependent on the improvement achieved. Under certain conditions on the proposal distribution, the above procedure can be shown, in the limit, to sample from a distribution over programs, where the probability of a program is related to its quality. In other words, the more efficient a program, the more times it is encountered, thereby enabling super-optimization. Using this approach, high-quality implementations of real programs such as the Montgomery multiplication kernel from the OpenSSL library were discovered. These implementations outperformed the output of the gcc compiler and even expert-handwritten assembly code. One of the main factors that governs the efficiency of the above stochastic search is the choice of the proposal distribution. Surprisingly, the state of the art method, Stoke (Schkufza et al., 2013), employs a proposal distribution that is neither learnt from past behaviour nor does it depend on the syntax or semantics of the program under consideration. We argue that this choice fails to fully exploit the power of stochastic search. For example, consider the case where we are interested in performing bitwise operations, as indicated by the compiler output. In this case, it is more likely that the optimal program will contain bitshifts than floating point opcodes. Yet, Stoke will assign an equal probability of use to both types of opcodes. In order to alleviate the aforementioned deficiency of Stoke, we build a reinforcement learning framework to estimate the proposal distribution for optimizing the source code under consideration. The score of the distribution is measured as the expected quality of the program obtained via stochastic search. Using training data, which consists of a set of input programs, the parameters are learnt via the REINFORCE algorithm (Williams, 1992). We demonstrate the efficacy of our approach on two datasets. The first is composed of programs from \u201cHacker\u2019s Delight\u201d (Warren, 2002). Due to the limited diversity of the training samples, we show that it is possible to learn a prior distribution (unconditioned on the input program) that outperforms the state of the art. The second dataset contains automatically generated programs that introduce diversity in the training samples. We show that, in this more challenging setting, we can learn a conditional distribution given the initial program that significantly outperforms Stoke."}, {"heading": "2 Related Works", "text": "Super-optimization The earliest approaches for super-optimization relied on brute-force search. By sequentially enumerating all programs in increasing length orders (Granlund & Kenner, 1992; Massalin, 1987), the shortest program meeting the specification is guaranteed to be found. As expected, this approach scales poorly to longer programs or to large instruction sets. The longest reported synthesized program was 12 instructions long, on a restricted instruction set (Massalin, 1987). Trading off completeness for efficiency, stochastic methods (Schkufza et al., 2013) reduced the number of programs to test by guiding the exploration of the space, using the observed quality of programs encountered as hints. However, the reliance of stochastic methods on a generic unspecific exploratory policy made the optimization blind to the problem at hand. We propose to tackle this problem by learning the proposal distribution.\nNeural Computing Similar work was done in the restricted case of finding efficient implementation of computation of value of degree k polynomials (Zaremba et al., 2014). Programs were generated from a grammar, using a learnt policy to prioritise exploration. This particular approach of guided search looks promising to us, and is in spirit similar to our proposal, although applied on a very restricted case.\nAnother approach to guide the exploration of the space of programs was to make use of the gradients of differentiable relaxation of programs. Bunel et al. (2016) attempted this by simulating program execution using Recurrent Neural Networks. However, this provided no guarantee that the network parameters were going to correspond to real programs. Additionally, this method only had the possibility of performing local, greedy moves, limiting the scope of possible transformations. On the contrary, our proposed approach operates directly on actual programs and is capable of accepting short-term detrimental moves.\nLearning to optimize Outside of program optimization, applying learning algorithms to improve optimization procedures, either in terms of results achieved or runtime, is a well studied subject. Doppa et al. (2014) proposed imitation learning based methods to deal with structured output spaces, in a \u201cLearning to search\u201d framework. While this is similar in spirit to stochastic search, our setting differs in the crucial aspect of having a valid cost function instead of searching for one. More relevant is the recent literature on learning to optimize. Li & Malik (2016) and Andrychowicz et al. (2016) learn how to improve on first-order gradient descent algorithms, making use of neural networks. Our work is similar, as we aim to improve the optimization process. However, as opposed to the gradient descent that they learn on a continuous unconstrained space, our initial algorithm is an MCMC sampler on a discrete domain. Similarly, training a proposal distribution parameterized by a Neural Network was also proposed by Paige & Wood (2016) to accelerate inference in graphical models. Similar approaches were successfully employed in computer vision problems where data driven proposals allowed to make inference feasible (Jampani et al., 2015; Kulkarni et al., 2015; Zhu et al., 2000)."}, {"heading": "3 Learning Stochastic Super-optimization", "text": ""}, {"heading": "3.1 Stochastic search as a program optimization procedure", "text": "Stoke (Schkufza et al., 2013) performs black-box optimization of a cost function on the space of programs, represented as a series of instructions. Each instruction is composed of an opcode, specifying what to execute, and some operands, specifying the corresponding registers. Each given input program T defines a cost function. For a candidate program R called rewrite, the goal is to optimize the following cost function:\ncost (R, T ) = \u03c9e \u00d7 eq(R, T ) + \u03c9p \u00d7 perf(R) (1)\nThe term eq(R; T ) measures how well the outputs of the rewrite match the outputs of the reference program. This can be obtained either exacly by running a symbolic validator or approximately by running test cases. The term perf(R) is a measure of the efficiency of the program. In this paper, we consider runtime to be the measure of this efficiency. It can be approximated by the sum of the latency of all the instructions in the program. Alternatively, runtime of the program on some test cases can be used. To find the optimum of this cost function, Stoke runs an MCMC sampler using the Metropolis (Metropolis et al., 1953) algorithm. This allows us to sample from the probability distribution induced by the cost function:\np(R; T ) = 1 Z exp(\u2212cost (R, T ))). (2)\nThe sampling is done by proposing random moves from a different proposal distribution:\nR\u2032 \u223c q( \u00b7 |R). (3)\nThe cost of the new modified program is evaluated and an acceptance criterion is computed. This acceptance criterion\n\u03b1(R, T ) = min ( 1, p(R \u2032; T )\np(R; T )\n) , (4)\nis then used as the parameter of a Bernoulli distribution from which an accept/reject decision is sampled. If the move is accepted, the state of the optimizer is updated to R\u2032. Otherwise, it remains in R. While the above procedure is only guaranteed to sample from the distribution p( \u00b7 ; T ) in the limit if the proposal distribution q is symmetric (q(R\u2032|R) = q(R|R\u2032) for all R,R\u2032), it still allows us to perform efficient hill-climbing for non-symmetric proposal distributions. Moves leading to an improvement are always going to be accepted, while detrimental moves can still be accepted in order to avoid getting stuck in local minima."}, {"heading": "3.2 Learning to search", "text": "We now describe our approach to improve stochastic search by learning the proposal distribution. We begin our description by defining the learning objective (section 3.2.1), followed by a parameterization of the proposal distribution (section 3.2.2), and finally the reinforcement learning framework to estimate the parameters of the proposal distribution (section 3.2.3)."}, {"heading": "3.2.1 Objective function", "text": "Our goal is to optimize the cost function defined in equation (1). Given a fixed computational budget of T iterations to perform program super-optimization, we want to make moves that lead us to the lowest possible cost. As different programs have different runtimes and therefore different associated costs, we need to perform normalization. As normalized loss function, we use the ratio between the best rewrite found and the cost of the initial unoptimized program R0. Formally, the loss for a set of rewrites {Rt}t=0..T is defined as follows:\nr({Rt}t=0..T ) = (\nmint=0..T cost (Rt, T ) cost (R0, T )\n) . (5)\nRecall that our goal is to learn a proposal distribution. Given that our optimization procedure is stochastic, we will need to consider the expected cost as our loss. This expected loss is a function of the parameters \u03b8 of our parametric proposal distribution q\u03b8:\nL(\u03b8) = E{Rt}\u223cq\u03b8 [r({Rt}t=0..T )] . (6)"}, {"heading": "3.2.2 Parameterization of the Move Proposal Distribution", "text": "The proposal distribution (3) originally used in (Schkufza et al., 2013) takes the form of a hierarchical model. The type of the move is initially sampled from a probability distribution. Additional samples are drawn to specify, for example, the affected location in the programs ,the new operands or opcode to use. Which of these probability distribution get sampled depends on the type of move that was first sampled. The detailed structure of the proposal probability distribution can be found in Appendix A. Stoke uses uniform distributions for each of the elementary probability distributions the model samples from. This corresponds to a specific instantiation of the general stochastic search paradigm. In this work, we propose to learn those probability distributions so as to maximize the probability of reaching the best programs. Our chosen parameterization of q is to keep the hierarchical structure of the original work of Schkufza et al. (2013), as detailed in Appendix A, and parameterize all the elementary probability distributions (over the positions in the programs, the instructions to propose or the arguments) independently. The set \u03b8 of parameters for q\u03b8 will thus contain a set of parameters for each elementary probability distributions. A fixed proposal distribution is kept through the optimization of a given program. The stochastic computation graph corresponding to a run of the Metropolis algorithm is given in Figure 1. We have assumed the operation of evaluating the cost of a program to be a deterministic function, as we will not model the randomness of measuring performance."}, {"heading": "3.2.3 Learning the Proposal Distribution", "text": "In order to learn the proposal distribution, we will use stochastic gradient descent on our loss function (6). We obtain the first order derivatives with regards to our proposal distribution parameters using the REINFORCE (Williams, 1992) estimator, also known as the likelihood ratio estimator (Glynn, 1990) or the score function estimator (Fu, 2006). This estimator relies on a rewriting of the gradient of the expectation. For an expectation with regards to a probability distribution x \u223c f\u03b8, the REINFORCE estimator is:\n\u2207\u03b8 \u2211 x f(x; \u03b8)r(x) = \u2211 x r(x)\u2207\u03b8f(x; \u03b8) = \u2211 x f(x; \u03b8)r(x)\u2207\u03b8 log(f(x; \u03b8)), (7)\nand provides an unbiased estimate of the gradient.\nA helpful way to derive the gradients is to consider the execution traces of the search procedure under the formalism of stochastic computation graphs (Schulman et al., 2015). We introduce one \u201ccost node\u201d in the computation graphs at the end of each iteration of the sampler. The associated cost corresponds to the normalized difference between the best rewrite so far and the current rewrite after this step:\nct = min ( 0, (\ncost (Rt, T )\u2212mini=0..t\u22121 cost (Ri, T ) cost (R0, T )\n)) . (8)\nThe sum of all the cost nodes corresponds to the sum of all the improvements made when a new lowest cost was achieved. It can be shown that up to a constant term, this is equivalent to our objective function (5). As opposed to considering only a final cost node at the end of the T iterations, this has the advantage that moves which were not responsible for the improvements would not get assigned any credit. For each round of MCMC, the gradient with regards to the proposal distribution is computed using the REINFORCE estimator which is equal to\n\u2207\u0302\u03b8,iL(\u03b8) = (\u2207\u03b8 log q\u03b8(Ri|Ri\u22121)) \u2211 t>i ct. (9)\nAs our proposal distribution remains fixed for the duration of a program optimization, these gradients needs to be summed over all the iterations to obtain the total contribution to the proposal distribution. Once this gradient is estimated, it becomes possible to run standard back-propagation with regards to the features on which the proposal distribution is based on, so as to learn the appropriate feature representation."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Setup", "text": "Implementation Our system is built on top of the Stoke super-optimizer from Schkufza et al. (2013). We instrumented the implementation of the Metropolis algorithm to allow sampling from parameterized proposal distribution and to keep track of the traces through the stochastic graph. Using those traces, we can compute the estimator of our gradients, implemented using the Torch framework (Collobert et al., 2011).\nDatasets We validate the feasibility of our learning approach on two experiments. The first is based on the Hacker\u2019s delight (Warren, 2002) corpus, a collection of twenty five bitmanipulation programs, used as benchmark in program synthesis (Gulwani et al., 2011; Jha et al., 2010; Schkufza et al., 2013). Those are short programs, all performing similar types of tasks. Some examples include identifying whether an integer is a power of two from its binary representation, counting the number of bits turned on in a register or computing the maximum of two integers. An exhaustive description of the tasks is given in Appendix B. Our second corpus of programs is automatically generated and is more diverse.\nModels The models that we are learning are a set of simple elementary probabilities for the categorical distribution over the instructions and over the type of moves to perform. We learn the parameters of each separate distribution jointly, using a Softmax transformation to enforce that they are proper probability distributions. For the types of move where opcodes are chosen from a specific subset, the probabilities of each instruction are appropriately renormalized. For each experiment, we train two types of models. Our first model, henceforth denoted the bias, is not conditioned on any property of the programs to optimize. By learning this simple proposal distribution, it is only possible to capture a bias in the dataset. This can be understood as an optimal proposal distribution that Stoke should default to. The second model is a probability distribution conditioned on the input program to optimize. For each input program, we generate a Bag-of-Words representation based on the opcodes of\nthe program. This is then embedded through a three hidden layer Multi Layer Perceptron, with ReLU activation unit. The proposal distribution over the instructions and over the type of moves are each the result of passing the outputs of this embedding through a linear transformation followed by a Softmax. All of our models are trained using the Adam (Kingma & Ba, 2015) optimizer, with its default hyper-parameters \u03b21 = 0.9, \u03b22 = 0.999, = 10\u22128. We use minibatches of size 32. The bias is trained with a learning rate of 10, while the Multi layer perceptron uses a learning rate of 0.1. Those learning rates are divided by the size of the minibatches. For each estimate of the gradient, we draw 100 samples for our estimator."}, {"heading": "4.2 Existing Programs", "text": "In order to have a larger corpus than the twenty-five programs initially present in \u201cHacker\u2019s Delight\u201d, we generate various starting points for each optimization. This is accomplished by running Stoke with a cost function where \u03c9p = 0 in (1), and keeping only the correct programs. Duplicate programs are filtered out. This allows us to create a larger dataset from which to learn. Examples of these programs at different level of optimization can be found in Appendix C. We divide this augmented Hacker\u2019s Delight dataset into two sets. All the programs corresponding to even-numbered tasks are assigned to the first set, which we use for training. The programs corresponding to odd-numbered tasks are kept for separate evaluation, so as to evaluate the generalisation of our learnt proposal distribution. The optimization process is visible in Figure 2, which shows a clear decrease of the training loss and testing loss for both models. While simply using stochastic super-optimization allows to discover programs 40% more efficient on average, using a tuned proposal distribution yield even larger improvements, bringing the improvements up to 60%, as can be seen in Table1. Due to the similarity between the different tasks, conditioning on the program features does not bring any significant improvements.\nIn addition, to clearly demonstrate the practical consequences of our learning, we present in Figure 3 a superposition of score traces, sampled from the optimization of a program of the test set. Figure 3a corresponds to our initialisation, an uniform distribution as was used in the work of Schkufza et al. (2013). Figure 3b corresponds to our optimized version. It can be observed that, while the uniform proposal distribution was successfully decreasing the cost of the program, our learnt proposal distribution manages to achieve lower scores in a more robust manner and in less iterations."}, {"heading": "4.3 Automatically Generated Programs", "text": "While the previous experiments shows promising results on a set of programs of interest, the limited diversity of programs might have made the task too simple, as evidenced by the good performance of a blind model. Indeed, despite the data augmentation, only 25 different tasks were present, all variations of the same programs task having the same optimum. To evaluate our performance on a more challenging problem, we automatically synthesize a larger dataset of programs. Our methods to do so consists in running Stoke repeatedly with a constant cost function, for a large number of iterations. This leads to a fully random walk as every proposed programs will have the same cost, leading to a 50% chance of acceptance. We generate 600 of these programs, 300 that we use as a training set for the optimizer to learn over and 300 that we keep as a test set. The performance achieved on this more complex dataset is shown in Figure 4 and Table 2."}, {"heading": "5 Conclusion", "text": "Within this paper, we have formulated the problem of optimizing the performance of a stochastic super-optimizer as a Machine Learning problem. We demonstrated that learning the proposal distribution of a MCMC sampler was feasible and lead to faster and higher quality improvements. It is interesting to compare our method to the synthesis-style approaches that have been appearing recently in the Deep Learning community (Graves et al., 2014) that aim at learning algorithms directly using differentiable representations of programs. We find that the stochastic search-based approach yields a significant advantage compared to those types of approaches, as the resulting program can be run independently from the Neural Network that was used to discover them. Several improvements are possible to the presented methods. In mature domains such as Computer Vision, the representations of objects of interests have been widely studied and as a result are successful at capturing the information of each sample. In the domains of programs, obtaining informative representations remains a challenge. Our proposed approach ignores part of the structure of the program, notably temporal. Significant performant boosts could be achieved with richer models, provided the associated scalability challenges were addressed. Finally, gathering a larger dataset of frequently used programs so as to measure more accurately the practical performance of those methods seems the evident next step for the task of program synthesis."}, {"heading": "A Structure of the proposal distribution", "text": "The sampling process of a move is a hierarchy of sampling step. The easiest way to represent it is as a generative model for the program transformations. Depending on what type of move is sampled, different series of sampling steps have to be performed. For a given move, all the probabilities are sampled independently so the probability of proposing the move is the product of the probability of picking each of the sampling steps. The generative model is defined in Figure 5. It is going to be parameterized by the the parameters of each specific probability distribution it samples from. The default Stoke version uses uniform probabilities over all of those elementary distributions."}, {"heading": "B Hacker\u2019s Delight Tasks", "text": "The 25 tasks of the Hacker\u2019s delight Warren (2002) datasets are the following:\n1. Turn off the right-most one bit 2. Test whether an unsigned integer is of the form 2(n\u2212 1) 3. Isolate the right-most one bit 4. Form a mask that identifies right-most one bit and trailing zeros 5. Right propagate right-most one bit 6. Turn on the right-most zero bit in a word 7. Isolate the right-most zero bit 8. Form a mask that identifies trailing zeros 9. Absolute value function\n10. Test if the number of leading zeros of two words are the same 11. Test if the number of leading zeros of a word is strictly less than of another work 12. Test if the number of leading zeros of a word is less than of another work 13. Sign Function 14. Floor of average of two integers without overflowing 15. Ceil of average of two integers without overflowing 16. Compute max of two integers 17. Turn off the right-most contiguous string of one bits 18. Determine if an integer is a power of two 19. Exchanging two fields of the same integer according to some input 20. Next higher unsigned number with same number of one bits 21. Cycling through 3 values 22. Compute parity 23. Counting number of bits 24. Round up to next highest power of two 25. Compute higher order half of product of x and y\nReference implementation of those programs were obtained from the examples directory of the stoke repository (Churchll et al., 2016)."}, {"heading": "C Examples of Hacker\u2019s delight optimisation", "text": "The first task of the Hacker\u2019s Delight corpus consists in turning off the right-most one bit of a register. When compiling the code in Listing 6a, llvm generates the code shown in Listing 6b. A typical example of an equivalent version of the same program obtained by the dataaugmentation procedure is shown in Listing 6c. Listing 6d contains the optimal version of this program. Note that such optimization are already feasible using the stoke system of Schkufza et al. (2013)."}], "references": [{"title": "Learning to learn by gradient descent by gradient descent", "author": ["Marcin Andrychowicz", "Misha Denil", "Sergio Gomez", "Matthew W Hoffman", "David Pfau", "Tom Schaul", "Nando de Freitas"], "venue": null, "citeRegEx": "Andrychowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz et al\\.", "year": 2016}, {"title": "Adaptive neural compilation", "author": ["Rudy Bunel", "Alban Desmaison", "Pushmeet Kohli", "Philip HS Torr", "M Pawan Kumar"], "venue": "In NIPS", "citeRegEx": "Bunel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bunel et al\\.", "year": 2016}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In NIPS,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Hc-search: A learning framework for search-based structured prediction", "author": ["Janardhan Rao Doppa", "Alan Fern", "Prasad Tadepalli"], "venue": "JAIR,", "citeRegEx": "Doppa et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Doppa et al\\.", "year": 2014}, {"title": "Gradient estimation", "author": ["Michael C. Fu"], "venue": "Handbooks in Operations Research and Management Science", "citeRegEx": "Fu.,? \\Q2006\\E", "shortCiteRegEx": "Fu.", "year": 2006}, {"title": "Likelihood ratio gradient estimation for stochastic systems", "author": ["Peter W Glynn"], "venue": "Communications of the ACM,", "citeRegEx": "Glynn.,? \\Q1990\\E", "shortCiteRegEx": "Glynn.", "year": 1990}, {"title": "Eliminating branches using a superoptimizer and the GNU C compiler", "author": ["Torbj\u00f6rn Granlund", "Richard Kenner"], "venue": "ACM SIGPLAN Notices,", "citeRegEx": "Granlund and Kenner.,? \\Q1992\\E", "shortCiteRegEx": "Granlund and Kenner.", "year": 1992}, {"title": "Synthesis of loop-free programs", "author": ["Sumit Gulwani", "Susmit Jha", "Ashish Tiwari", "Ramarathnam Venkatesan"], "venue": "In PLDI,", "citeRegEx": "Gulwani et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gulwani et al\\.", "year": 2011}, {"title": "The informed sampler: A discriminative approach to bayesian inference in generative computer vision models", "author": ["Varun Jampani", "Sebastian Nowozin", "Matthew Loper", "Peter V Gehler"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "Jampani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jampani et al\\.", "year": 2015}, {"title": "Oracle-guided component-based program synthesis", "author": ["Susmit Jha", "Sumit Gulwani", "Sanjit A Seshia", "Ashish Tiwari"], "venue": "In International Conference on Software Engineering,", "citeRegEx": "Jha et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jha et al\\.", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Picture: A probabilistic programming language for scene perception", "author": ["Tejas D Kulkarni", "Pushmeet Kohli", "Joshua B Tenenbaum", "Vikash Mansinghka"], "venue": "In CVPR,", "citeRegEx": "Kulkarni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "Learning to optimize", "author": ["Ke Li", "Jitendra Malik"], "venue": null, "citeRegEx": "Li and Malik.,? \\Q2016\\E", "shortCiteRegEx": "Li and Malik.", "year": 2016}, {"title": "Superoptimizer: A look at the smallest program", "author": ["Henry Massalin"], "venue": "In ACM SIGPLAN Notices,", "citeRegEx": "Massalin.,? \\Q1987\\E", "shortCiteRegEx": "Massalin.", "year": 1987}, {"title": "Equation of state calculations by fast computing machines", "author": ["Nicholas Metropolis", "Arianna W Rosenbluth", "Marshall N Rosenbluth", "Augusta H Teller", "Edward Teller"], "venue": "The journal of chemical physics,", "citeRegEx": "Metropolis et al\\.,? \\Q1953\\E", "shortCiteRegEx": "Metropolis et al\\.", "year": 1953}, {"title": "Inference networks for sequential Monte Carlo in graphical models", "author": ["Brookes Paige", "Frank Wood"], "venue": "In ICML,", "citeRegEx": "Paige and Wood.,? \\Q2016\\E", "shortCiteRegEx": "Paige and Wood.", "year": 2016}, {"title": "Gradient estimation using stochastic computation graphs", "author": ["John Schulman", "Nicolas Heess", "Theophane Weber", "Pieter Abbeel"], "venue": "In NIPS,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Learning to discover efficient mathematical identities", "author": ["Wojciech Zaremba", "Karol Kurach", "Rob Fergus"], "venue": "In NIPS", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Integrating bottom-up/top-down for object recognition by data driven markov chain monte carlo", "author": ["Song-Chun Zhu", "Rong Zhang", "Zhuowen Tu"], "venue": "In CVPR,", "citeRegEx": "Zhu et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2000}, {"title": "Delight Tasks The 25 tasks of the Hacker\u2019s delight Warren (2002) datasets", "author": ["B Hacker\u2019s"], "venue": null, "citeRegEx": "Hacker.s,? \\Q2002\\E", "shortCiteRegEx": "Hacker.s", "year": 2002}], "referenceMentions": [{"referenceID": 17, "context": "Using training data, which consists of a set of input programs, the parameters are learnt via the REINFORCE algorithm (Williams, 1992).", "startOffset": 118, "endOffset": 134}, {"referenceID": 13, "context": "By sequentially enumerating all programs in increasing length orders (Granlund & Kenner, 1992; Massalin, 1987), the shortest program meeting the specification is guaranteed to be found.", "startOffset": 69, "endOffset": 110}, {"referenceID": 13, "context": "The longest reported synthesized program was 12 instructions long, on a restricted instruction set (Massalin, 1987).", "startOffset": 99, "endOffset": 115}, {"referenceID": 18, "context": "Neural Computing Similar work was done in the restricted case of finding efficient implementation of computation of value of degree k polynomials (Zaremba et al., 2014).", "startOffset": 146, "endOffset": 168}, {"referenceID": 1, "context": "Bunel et al. (2016) attempted this by simulating program execution using Recurrent Neural Networks.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "Similar approaches were successfully employed in computer vision problems where data driven proposals allowed to make inference feasible (Jampani et al., 2015; Kulkarni et al., 2015; Zhu et al., 2000).", "startOffset": 137, "endOffset": 200}, {"referenceID": 11, "context": "Similar approaches were successfully employed in computer vision problems where data driven proposals allowed to make inference feasible (Jampani et al., 2015; Kulkarni et al., 2015; Zhu et al., 2000).", "startOffset": 137, "endOffset": 200}, {"referenceID": 19, "context": "Similar approaches were successfully employed in computer vision problems where data driven proposals allowed to make inference feasible (Jampani et al., 2015; Kulkarni et al., 2015; Zhu et al., 2000).", "startOffset": 137, "endOffset": 200}, {"referenceID": 2, "context": "Doppa et al. (2014) proposed imitation learning based methods to deal with structured output spaces, in a \u201cLearning to search\u201d framework.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Doppa et al. (2014) proposed imitation learning based methods to deal with structured output spaces, in a \u201cLearning to search\u201d framework. While this is similar in spirit to stochastic search, our setting differs in the crucial aspect of having a valid cost function instead of searching for one. More relevant is the recent literature on learning to optimize. Li & Malik (2016) and Andrychowicz et al.", "startOffset": 0, "endOffset": 378}, {"referenceID": 0, "context": "Li & Malik (2016) and Andrychowicz et al. (2016) learn how to improve on first-order gradient descent algorithms, making use of neural networks.", "startOffset": 22, "endOffset": 49}, {"referenceID": 0, "context": "Li & Malik (2016) and Andrychowicz et al. (2016) learn how to improve on first-order gradient descent algorithms, making use of neural networks. Our work is similar, as we aim to improve the optimization process. However, as opposed to the gradient descent that they learn on a continuous unconstrained space, our initial algorithm is an MCMC sampler on a discrete domain. Similarly, training a proposal distribution parameterized by a Neural Network was also proposed by Paige & Wood (2016) to accelerate inference in graphical models.", "startOffset": 22, "endOffset": 492}, {"referenceID": 14, "context": "To find the optimum of this cost function, Stoke runs an MCMC sampler using the Metropolis (Metropolis et al., 1953) algorithm.", "startOffset": 91, "endOffset": 116}, {"referenceID": 17, "context": "We obtain the first order derivatives with regards to our proposal distribution parameters using the REINFORCE (Williams, 1992) estimator, also known as the likelihood ratio estimator (Glynn, 1990) or the score function estimator (Fu, 2006).", "startOffset": 111, "endOffset": 127}, {"referenceID": 5, "context": "We obtain the first order derivatives with regards to our proposal distribution parameters using the REINFORCE (Williams, 1992) estimator, also known as the likelihood ratio estimator (Glynn, 1990) or the score function estimator (Fu, 2006).", "startOffset": 184, "endOffset": 197}, {"referenceID": 4, "context": "We obtain the first order derivatives with regards to our proposal distribution parameters using the REINFORCE (Williams, 1992) estimator, also known as the likelihood ratio estimator (Glynn, 1990) or the score function estimator (Fu, 2006).", "startOffset": 230, "endOffset": 240}, {"referenceID": 16, "context": "A helpful way to derive the gradients is to consider the execution traces of the search procedure under the formalism of stochastic computation graphs (Schulman et al., 2015).", "startOffset": 151, "endOffset": 174}, {"referenceID": 2, "context": "Using those traces, we can compute the estimator of our gradients, implemented using the Torch framework (Collobert et al., 2011).", "startOffset": 105, "endOffset": 129}, {"referenceID": 7, "context": "The first is based on the Hacker\u2019s delight (Warren, 2002) corpus, a collection of twenty five bitmanipulation programs, used as benchmark in program synthesis (Gulwani et al., 2011; Jha et al., 2010; Schkufza et al., 2013).", "startOffset": 159, "endOffset": 222}, {"referenceID": 9, "context": "The first is based on the Hacker\u2019s delight (Warren, 2002) corpus, a collection of twenty five bitmanipulation programs, used as benchmark in program synthesis (Gulwani et al., 2011; Jha et al., 2010; Schkufza et al., 2013).", "startOffset": 159, "endOffset": 222}], "year": 2016, "abstractText": "Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (\u201cHacker\u2019s Delight\u201d) programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.", "creator": "LaTeX with hyperref package"}}}