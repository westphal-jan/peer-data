{"id": "1611.02360", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "Cruciform: Solving Crosswords with Natural Language Processing", "abstract": "Crossword puzzles are popular word games that require not only a large vocabulary, but also a broad knowledge of topics. Answering each clue is a natural language task on its own as many clues contain nuances, puns, or counter-intuitive word definitions. Additionally, it can be extremely difficult to ascertain definitive answers without the constraints of the crossword grid itself. This task is challenging for both humans and computers. We describe here a new crossword solving system, Cruciform. We employ a group of natural language components, each of which returns a list of candidate words with scores when given a clue. These lists are used in conjunction with the fill intersections in the puzzle grid to formulate a constraint satisfaction problem, in a manner similar to the one used in the Dr. Fill system. We describe the results of several of our experiments with the system.\n\n\nIn the first task, we defined what the solution was for the puzzle grid to be solved, and the solution to solve the problem. This system required an approach that allowed the search process to complete its task at a speed that can be easily demonstrated on the computer. As the search for solutions for certain puzzles progressed, there was no time required to solve each puzzle. There was no requirement for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle to be solved for each puzzle", "histories": [["v1", "Tue, 8 Nov 2016 01:47:41 GMT  (847kb,D)", "http://arxiv.org/abs/1611.02360v1", null], ["v2", "Wed, 23 Nov 2016 16:14:23 GMT  (0kb,I)", "http://arxiv.org/abs/1611.02360v2", "based on feedback, we have determined that the paper needs more work"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dragomir radev", "rui zhang", "steve wilson", "derek van assche", "henrique spyra gubert", "alisa krivokapic", "meixing dong", "chongruo wu", "spruce bondera", "luke brandl", "jeremy dohmann"], "accepted": false, "id": "1611.02360"}, "pdf": {"name": "1611.02360.pdf", "metadata": {"source": "CRF", "title": "Cruciform: Solving Crosswords with Natural Language Processing", "authors": ["Dragomir Radev", "Rui Zhang", "Steve Wilson", "Derek Van Assche", "Henrique Spyra Gubert", "Alisa Krivokapic", "MeiXing Dong", "Chongruo Wu", "Spruce Bondera", "Luke Brandl", "Jeremy Dohmann"], "emails": ["(radev@umich.edu,", "ryanzh@umich.edu,", "steverw@umich.edu,", "dvanassc@umich.edu", "hs2807@columbia.edu,", "ak3533@columbia.edu,", "meixingd@umich.edu,", "chongruo@umich.edu,", "spruceb@umich.edu,", "brandl@umich.edu,", "dohmann@college.harvard.edu)"], "sections": [{"heading": "1 Introduction", "text": "Crossword puzzles are popular word games that require not only a large vocabulary, but also a broad knowledge of topics. Answering each clue is a natural language task on its own as many clues contain nuances, puns, or counter-intuitive word definitions. Additionally, it can be extremely difficult to find definitive answers without the constraints of the crossword grid itself. Completing some of the more difficult crossword puzzles can be a formidable task for many humans. Working in a natural language domain makes the task similarly challenging for computers.\nOne early system designed to solve crossword puzzles, Proverb [6], uses a variety of modules that targeted different types of clues and combined them into a probabilistic list in order to place them into the puzzle. Later, Dr. Fill [4] treats the crossword problem as a probabilistic constraint satisfaction problem and uses various lexical features to rank the candidate answers. IBM\u2019s Watson [3] uses an enormous amount of computation power and engineering talent to successfully take on another difficult natural language game: Jeopardy!.\nCompared with other Question Answering systems, crossword solving systems are generally more complex, because of the following properties of crossword puzzles: [7]:\n\u2022 Clues are mostly formulated in a non-interrogative form.\n\u2022 Clues can be voluntarily ambiguous and misleading.\n\u2022 Topics of the questions are not limited to factoids.\n\u2022 There is a unique and precise answer which is a word or compound word, while in QA, the answer is a sequence of words in order to be recognizable by humans.\nar X\niv :1\n61 1.\n02 36\n0v 1\n[ cs\n.C L\n] 8\nN ov\n2 01\nThere are, however, some aspects which make crossword puzzles more approachable. Crosswords have a bias towards shorter and more common words, as well as words with specific letters and specific bigrams. Another advantage is that the length of the answer is known.\nWe propose a new crossword solving system, Cruciform, which has a general structure similar to that of Proverb and Dr. Fill. We employ a group of natural language components that each return a list of candidate words with likelihood scores when given a clue. The candidate lists for a clue are merged to form a single list; this is done for every clue. Then, these lists are used in conjunction with the fill intersections in the puzzle grid to formulate a constraint satisfaction problem (CSP).\nWe performed all of our experiments on two months (June 2015 and July 2015) of New York Times puzzles to ensure reliable results. We show that filling in blanks using a vocabulary of answers and bigram reranking of generated solutions improves the baseline performance. Additionally, we propose a set of metrics to analyze each subcomponent of the overall crossword puzzle solver."}, {"heading": "2 New York Times Crossword Puzzles", "text": "The New York Times (NYT) releases a new crossword puzzle daily1. Puzzles are submitted by puzzle composers, or cruciverbalists, to the editor of the daily puzzle. The puzzles progressively increase in difficulty throughout the week so Monday puzzles are the easiest and Saturday puzzles are the hardest, where each has a grid size of 15x15. The Sunday puzzle is roughly of the same difficulty as the Thursday puzzle, but is larger, typically with a size of 21x21. Each fill, or answer, is at least three letters long and all squares appear in exactly two words. Typically, the maximum word count for a Monday-Thursday puzzle is 78 words, 72 for a Friday or Saturday puzzle, and 140 words for a Sunday puzzle.\nThe NYT puzzles follow many additional set conventions. When a clue is an abbreviation or is tagged with with \u201cabbr.\u201d, the fill is also an abbreviation. If a clue is phrased as a question and ends in a question mark, then the answer is a play on words. Answers that are in non-English languages will have a clue that is either tagged with the language (ex. \u201cFr.\u201d for French) or has a word from that language (e.g. \u201cMonth for Marcel\u201d for \u201cmai\u201d). Clues and fills always match in part of speech, tense, number, and degree. The fill can consist of multiple words and will never appear in the clue corresponding to it. The first letter of a clue is always capitalized; this is sometimes used to introduce an ambiguity since there could be a significant difference between a word and the proper noun version of that word.\nSome puzzles are rebus puzzles, where some squares have to be filled with symbols, multiple letters, or a word. For instance, a rebus puzzle by Jeff Chen required the letters \u201cpH\u201d to be written together in single squares in fills such as \u201ct/r/i/u/m/pH\u201d and \u201cs/o/pH/o/c/l/e/s.\u201d These puzzles are often unique and require special rules to solve, so they are excluded from our analysis.\nBeyond these constraints, there are also a number of more qualitative elements that make up a quality crossword puzzle. Proper names that lack specific distinction (e.g. \u201cgirl\u2019s name,\u201d \u201cboy\u2019s name\u201d) are generally avoided. Letter intersections where both words are unlikely to be solved by most readers are also discouraged."}, {"heading": "2.1 Example", "text": "An example puzzle by Peter Gordon (edited by Will Shortz) is shown in Figure 1. A single word can have clues that reference many different senses and usages of a word. Even for one word sense, different clues can tie that sense to vastly different contexts. For example, \u201dear\u201d can be clued as \u201cMusician\u2019s asset\u201d, \u201cVan Gogh had one later in life\u201d, or \u201cCorn unit.\u201d\nSample clues for \u201cTREE\u201d, \u201cNOAH\u201d, \u201cASTRO\u201d, and \u201cEAR\u201d. TREE\n1http://www.nytimes.com/crosswords/\n\u2022 It has bark but no bite\n\u2022 Its bark is silent\n\u2022 Branch location\n\u2022 Fort locale\n\u2022 Type of house\n\u2022 House for kids\n\u2022 Lineage display\n\u2022 Every family has one\n\u2022 Family chart\n\u2022 Cobbler\u2019s need\n\u2022 Shoe stretcher\n\u2022 It leaves in the spring\n\u2022 Where to get dates\n\u2022 Site of many a cat rescue\n\u2022 Dendrophobe\u2019s fear\n\u2022 Forbidden fruit source\n\u2022 Ring holder\n\u2022 Leaves home\nNOAH\n\u2022 Flood survivor\n\u2022 Ararat lander\n\u2022 Rider of the lost ark\n\u2022 Biblical helmsman\n\u2022 Guy who believed in \u201ctake two\u201d\n\u2022 Noted couples protector\n\u2022 Early matchmaker\n\u2022 Captain of a famous cruise for couples\n\u2022 Life preserver\n\u2022 Person known for double takes\n\u2022 First name in lexicography\n\u2022 One of the Websters\n\u2022 Actor Wyle\nASTRO\n\u2022 Houston Colt 45 today\n\u2022 Enron Field player, once\n\u2022 Houston player\n\u2022 Texas leaguer\n\u2022 Nolan Ryan, notably\n\u2022 2005 world series participant\n\u2022 Player under a dome\n\u2022 Pirate battler, at times\n\u2022 Cartoon dog\n\u2022 Space age hound\n\u2022 Jetson canine\n\u2022 Animated pooch\n\u2022 Elroy\u2019s pet\n\u2022 Bygone Chevrolet van\nEAR\n\u2022 The sense organ for hearing and equilibrium\n\u2022 Musician\u2019s asset\n\u2022 Teacup handle\n\u2022 Corn serving\n\u2022 Spike\n\u2022 Elephant\u2019s floppy feature\n\u2022 Van Gogh had one later in life\n\u2022 Q-Tip target\n\u2022 Hole in the head?"}, {"heading": "3 Related Work", "text": ""}, {"heading": "3.1 The Dr. Fill System", "text": "The Dr. Fill system [4] starts with producing candidate answers for each clue. Candidate answers are extracted from various resources such as crossword databases, dictionaries, WordNet, and Wikipedia:\n\u2022 A crossword database from various publishers.\n\u2022 A small, basic dictionary containing common words.\n\u2022 An extensive dictionary with manually generated merit scores, where words with a higher score are considered good fill for crosswords. For example, \u201cBUZZ LIGHTYEAR\u201d is considered excellent fill. It is a lively word, uses uncommon letters, and has positive connotations.\nPowered by TCPDF (www.tcpdf.org)\n\u2022 Word roots and synonyms from WordNet.\n\u2022 Information like page titles Wikipedia.\nEach word of the appropriate length is scored by the following five criteria: 1) A match for the clue 2) Part of speech analysis 3) Crossword Merit 4) Abbreviation 5) Fill-in-the-blank. The final score is a weighted sum of the five criteria, where weights are tuned on a development set of puzzles.\nDr. Fill is evaluated (only) on the seven puzzles from the 2010 American Crossword Puzzle Tournament (ACPT) using ACPT\u2019s scoring rules to calculate a performance metric. However, it is not completely fair to use the ACPT score to compare their system with the other human solvers because the rules give bonus points for each minute of time remaining if a competitor finishes early. Since the rules were set for human solvers, we can expect that an automated system would finish solving puzzles much faster than humans and gain many extra points this way.\nSince quantitative results on other puzzles are not provided and the system is not publicly available, we are unable to compare directly against the Dr. Fill system."}, {"heading": "3.1.1 CSP Searching Heuristics", "text": "After generating the candidate answers, Dr. Fill employs the following heuristic to solve the underlying CSP.\n\u2022 Value Selection: Prefer choices not only work well for the word slot in question, but also minimally increase the cost of the crossing clues. It is important to note that if Dr.Fill simply chooses the best candidate to fill, the performance drops significantly.\n\u2022 Variable Selection: Pick the variable where the difference between the best candidate and the second best candidate is maximized.\n\u2022 Limited Discrepancy Search (LDS): The search maintains a list of discarded value choices. Each element of the list is a pair of (v, x) indicating the value v should not be proposed for x. Discarded elements are not considered in the Value and Variable Selection heuristics and remain discarded in the rest of the search. For additional details, see [4].\nThe overall CSP algorithm in Dr. Fill is outlined in Algorithm 1: Given a crossword puzzle C, a fixed discrepancy limit n, a partial solution S, the best solution so far B, and previously pitched assignments P , solve(C, S, B, n, P ) gives the best solution extending S with at most n discrepancies.\nAlgorithm 1 Dr. Fill CSP Algorithm procedure SOLVE(C, S,B, n, P )\nif cost(S) \u2265 cost(B) then return B if S assigns a value to every variable then return S v \u2190 a variable unassigned by S d\u2190 an element of v\u2019s domain such that (v, d) /\u2208 P S\u2032\u2190 S \u222a (v = d) C \u2032\u2190 propagate(C,S\u2032) if propagation succeeded then\nB \u2190 solve(C \u2032, S\u2032, B, n, P ) if |P | < n then\nB \u2190 solve(C, S, B, n, P \u222a (v, d))"}, {"heading": "3.2 Proverb", "text": "Proverb [5, 6] uses thirty expert modules. Given a clue, each module produces a list of candidate answers of any length and numerical scores representing the confidence level for each word. The system also contains several modules to deal with uncommon answers that may not appear in the crossword database such as multi-word answers . Our approach is similar in that we also use a combination of expert modules."}, {"heading": "3.2.1 Merging Lists and Grid Filling", "text": "The merging process is controlled by three parameters: scale(m), length-scale(m) and spread(m), where m refers to an expert module. For each module the weight of each candidate is adjusted by raising its power to spread(m) and then regularized, and the confidence of each module is multiplied by scale(m) and length-scale(m)targetlength. The merger then combines all modules\u2019 candidates by summing together their probabilities weighted by the adjusted confidence and normalizing the sum to one. These parameters are optimized by hill-climbing with the objective function as the average log probability assigned to the correct targets."}, {"heading": "3.3 WebCrow and SACRY", "text": "Webcrow [2] draws candidate words from web documents and previously solved crosswords. The candidate words are given confidence scores by various modules and combined into a solution using a CSP version of weighted A* search."}, {"heading": "3.3.1 Candidate Answers Generation", "text": "WebCrow system draws its candidates primarily from web documents and previously solved crosswords. Using a novel Web Search Module (WSM), the system extracts potential words from web documents and ranks them using various filters. After parsing the document and identifying an unweighted list of candidate words, they are passed through both a statistical filter and a morphological filter. The statistical filter determines a score based on the distance between the candidate words and the search query. The morphological filter is composed of two parts, one of which attempts to determine the morphological class of each word in the document, while the other component identifies the most likely morphological classes of the clue\u2019s answer. The words are also ranked by a fairly standard crossword database module, a hard coded rule based module, a dictionary, and other similar modules. These scores are then merged additively by word and weighted by the confidence of each module. The grid is then filled by a CSP version of WA*.\nThe WebCrow system is evaluated on Italian crossword puzzles and not on English language puzzles. The structure and methodology for designing crossword puzzles varies a lot from one language to another. For instance, Italian puzzles can have letters that don\u2019t participate in crosses and include two letter fills that aren\u2019t actual words. Without running Webcrow on English crossword puzzles, we cannot determine how well the system would generalize to these puzzles or make a direct comparison with our system."}, {"heading": "3.4 SACRY", "text": "SACRY is another crossword solving system introduced in [1] that introduces a new reranking module into the WebCrow system."}, {"heading": "3.4.1 Reranking", "text": "SACRY attempts to relate various clues to each other using kernels in hopes that some higher level features of the clues can help to relate similar clues to a similar group of answers. Each answer is modeled by the set of clues associated with it, and the paper describes a several features which summarize this information. One of these features takes basic statistics the feature values of all such clues as determined by the reranking modules, as well as the frequency of the answer in the crossword database. The occurrences of each answer instance are modeled by a vector the size of the clue list with the values corresponding to the position in each clue\u2019s candidate list or zero if the word is not present. Additionally, the system attempts to determine the similarity between answer candidates and input clues using features based on word embeddings. These features are determined by (i) the similarity between the pair of clues, (ii) the target clue and the candidate answer and (iii) the candidate clue and the candidate answer. The end result is a reranked and aggregated list which can be input into WebCrow\u2019s CSP solver after a logistic regression step."}, {"heading": "4 Methodology used in the Cruciform system", "text": "Given a crossword puzzle to solve, our system first uses different components (Section 4.1) to generate candidate answers for each clue. After merging the answers from different components, we compile a list of candidate answers along with confidence scores. These answers are then used to formulate a CSP solved by the algorithm similar to Dr.Fill (Section 4.2.2). Specifically, the CSP algorithm uses a variable selection heuristic by choosing the variable with the maximum confidence score difference between the best candidate and the second best candidate and then choosing that variable\u2019s best candidate following the value selection heuristic. To introduce some variations, the CSP algorithm also searches with non-best values with a predefined depth limit. The CSP search procedure generates a number of solutions (typically around 20) for the whole puzzle. We then refine each solution with post-processing steps including filling in the blanks and bigram probability reranking (Section 4.3)."}, {"heading": "4.1 Component Mechanism", "text": "Since many clues follow a predetermined pattern, it can be advantageous to prepare specialized programs that are crafted for the purpose of providing answers to certain types of clues. By setting up the system in a modular fashion, more components can easily be added at any time, which can continually increase the overall performance of the system. The task of each component is to recognize a specific clue type, understand the clue in some way, and provide an (optionally) ranked list of candidate answers that are the correct length to be placed into the puzzle.\nThe input to each component consists of a list of all clues in a puzzle along with the clue ID (e.g., 1A, 2D, 3A. . . ) and the length of the correct answer, which is inferred from the puzzle grid layout. The output of each component is a list of candidate answers for each clue, designated with the corresponding clue ID as well as a score indicating the quality of the answer produced. The clue types that the system handles along with the approaches used by each component operates are described below in detail."}, {"heading": "4.1.1 Lucene Components", "text": "The Lucene components are used as the baseline components that can handle any clue pattern in a generic way, accomplished by providing an interface to the Lucene indexing system.2 Clues are converted into queries that are used to search through datasets of known clues as well as Wikipedia titles. The top results from each data source are compiled, and the entries of the wrong length are removed from the list of candidate answers. Lucene automatically generates a score for each result, which is returned along with the list of candidate answers. The three components we consider are CWG, OTSYS, and WIKI which search through the text of the CWG, OTSYS, and Wikipedia datasets detailed in Section 5."}, {"heading": "4.1.2 Specialized NLP Components", "text": "We initially incorporated components that focused on very specific tasks such as identifying missing last names (ex. Clue: Titanic actor Billy (4) Fill: ZANE), missing first names (ex. Clue: \u201cWrecking Ball\u201d singer Cyrus (5) Fill: MILEY), world capitals, acronyms, and other tasks. Unfortunately, these components caused substantial time increases for modest performance improvements. For example, adding components focused on first names, last names, state capitals, and country capitals added less than 1% to the average accuracy for June 2015 puzzles, at the cost of a time increase from 82.4 seconds to 494 seconds per puzzle. As such, we decided to exclude them from our final system.\nDifferent NLP components with examples:\n\u2022 Missing last name: Titanic actor Billy (4) ZANE, James who wrote \u201dA Death in the Family\u201d (4) AGEE\n\u2022 Missing first name: \u201cWrecking Ball\u201d singer Cyrus (5) MILEY, Stuntman Knievel (4) EVEL 2https://lucene.apache.org/core/\n\u2022 Roman years: Year John Dryden died (4) MDCC, Early third-century year (4) CCIV\n\u2022 Common foreign words: \u201dThank you,\u201d in Hawaii (6) MAHALO, When the day\u2019s done, to Denis (4) NUIT\n\u2022 Prefix: Puncture preceder (3) ACU, Intro to physics? (4) META\n\u2022 Suffix: Suffix with miss and dismiss (3) IVE, Fox tail? (4) TROT\n\u2022 Direction: New Orleans-to-Detroit dir. (3) NNE\n\u2022 World capitals: Samoa\u2019s capital (4) APIA\n\u2022 Brands: Hyundai and Kia (5) AUTOS, Honda model (6) ACCORD\n\u2022 Competitors: Marriott alternative (4) OMNI, Reebok alternative (4) NIKE\n\u2022 Acronyms: Canoodling in a restaurant, e.g. (abbr.) (3) PDA, Chemists\u2019 org (3) AIC\n\u2022 Hypernyms: Platinum, for example (5) METAL, Cronus and Hyperion (6) TITANS\n\u2022 Synonyms: Overshoot, say (4) MISS, Gait (4) PACE\n\u2022 Roles: 1963 Elizabeth Taylor role (9) CLEOPATRA, Tom Cruise\u2019s \u2019Risky Business\u2019 co-star (15) REBECCADEMORNAY\n\u2022 Cities: Largest city in Nebraska (5) OMAHA\n\u2022 Works of art: \u201dOwner of a Lonely Heart\u201d band (3) YES, Tom Cruise film set in Chicago (13) RISKYBUSINESS\n\u2022 Partners and counterparts: Gentleman\u2019s partner (4) LADY, Bull\u2019s counterpart (4) BEAR\n\u2022 Dictionary definitions: Coffee dispenser (3) URN"}, {"heading": "4.2 Solution Generation", "text": ""}, {"heading": "4.2.1 CSP", "text": "We formulate this task as a Constraint Satisfaction Problem (CSP). A CSP instance can formally be defined with the triple (X,D,C) [8]. Here, X is the set of variables {X1, X2, ..., Xn}, D is the set of domains for each variable, {D1, D2, ..., Dn}, where each Di = {v1, v2, ..., vn} specifies allowable values for the corresponding variable Xi, and C is the set of constraints between variables. Each constraint is a tuple (scope, rel), where scope contains a list of the variables affected by the constraints, and rel describes the restrictions enforced by the constraint.\nIn the case of a crossword puzzle, let X be the set of entries in the puzzle that must be filled, where each entry Xi has exactly one clue and one correct answer. Both the length of the correct answer, `(Xi), and the constraints, C, can be inferred from examining the puzzle grid layout. Each Ci defines an intersection between two answers (scope) in the puzzle, one in the \u201dacross\u201d direction and the other in the \u201d down\u201d direction, where rel states that either 1) the letter in the square of the intersection must be the same, or 2) at least one of the two variables currently has a \u201dblank\u201d square in the location of the intersection. Finally, D is generated by aggregating the output of all components and removing any values from Di with length not equal to `(Xi) for a given i. Figure 2 illustrates such an example."}, {"heading": "4.2.2 Filling in the Grid", "text": "Initially, all entries are set to \u201c-\u201d, which indicates missing values. This means our CSP is technically solved from the beginning. Instead of looking for a solution that just fulfills the constraints, we search for one that maximizes the likelihood of the words. Every candidate word has some score reflecting how likely it is for the given clue. These are provided by the NLP components.\nFor the algorithm to choose the next variable, we employ the following variable selection heuristic: choose the variable that has the largest difference between the confidence scores of the best and second best candidates. This difference indicates how confident we are in filling that variable. For our value selection heuristic, we choose the candidate with the highest score. However, the best candidate is not always the correct answer and always choosing the candidate with the highest score can lead to poor solutions. To work around this problem, we also implemented Limited Discrepancy Search, where \u201cdiscrepancy\u201d is defined as the number of times that the heuristic is violated. We use LDS with a value selection heuristic where non-best candidates are also chosen. This search procedure introduces variations of solutions.\nAfter CSP, we have a list of possible solutions to the crossword puzzle, each of which has a score by summing up the candidate scores for each filled word."}, {"heading": "4.3 Post-processing", "text": "We employ the following two post-processing steps."}, {"heading": "4.3.1 Filling in the blanks", "text": "In many cases, single letters are missing from the solution. For each solution, we perform the following Algorithm 2. Here, the function SORT(Entries) will sort the list of entries with missing letters by the number of letters missing. Entries with fewer missing letters will be filled first. The function MATCH(e) returns a word that matches the length and the existing letters of entry, or None if none are found.\nAlgorithm 2 Fill in the blank procedure FILLBLANK\nEntries\u2190 a list of incomplete entries SortedEntries\u2190 SORT(Entries) for e in SortedEnries do\nw \u2190MATCH(e) if w is not None then\ne\u2190 w"}, {"heading": "4.3.2 Bigram Probability Reranking", "text": "Lastly, we perform bigram probability reranking over the possible solutions. We first build a bigram language model based on the clue data set described in Section 5.1. Then, we calculate the bigram probability of each solution after filling in the blank. The score of the solution is then rescaled by the bigram probability. We pick the solution with the highest score."}, {"heading": "5 Datasets", "text": ""}, {"heading": "5.1 Crossword Clues", "text": "We use puzzles from Crossword Giant (CWG),3 an online crossword resource with clues and answers, and a dataset of clues collected by Matthew Ginsberg (OTSYS).4 CWG and OTSYS were downloaded around May 2013 and March 2014, respectively.\nStatistics including the number of words and clues for each data set are summarized in Table 1. The word with the most clues is \u201cEAR\u201d with 1,808 different accompanying clues, followed by \u201cSEA\u201d (1,707), \u201cTREE\u201d (1,578), \u201cERA\u201d (1,578), and \u201cARIA\u201d (1,428)."}, {"heading": "5.2 Wikipedia", "text": "We collect the title and the first sentence from Wikipedia pages from 2013."}, {"heading": "5.3 Dataset Coverage", "text": "Our main training dataset used by the Lucene components consists of the clues and answers in CWG and OTSYS. We examine the overlap of clues and answers in the database and NYT clues and answers. If a clue shows up in a puzzle being evaluated and is also in the database, then it is included in the average percentage of clues in the database; this is also done for answers. If a puzzle clue and its corresponding answer both are in the database, then it is included in the average percentage of clue/answer pairs in the database. The percentage of clues and answers from the puzzles of the specified month that are found in the database tells us how much of the puzzle we have seen before.\nIn Table 2, we randomly pick five months of New York Time Puzzles (April 2008, May 2009, June 2015, July 2015, January 2016) and calculate overlaps for each month. As we can see, the percentage of clues previously seen decreases over time while the percentage of answers previously seen stays about the same. This implies that although our database contains many clues and fills, there are always new clues being\n3http://crosswordgiant.com 4http://www.otsys.com/clue/\nwritten for the same words. If our system can do well on newer puzzles, then the system is generalizable to unseen clues.\nFor June 2015 and July 2015, we also zoom in by grouping puzzles by weekdays, as summarized in Table 3. We see that the more difficult and the larger puzzles (e.g. Friday, Saturday, and Sunday puzzles) also tend to have more unseen clues and answers than the easier puzzles from other days of the week. Being able to do well on these puzzles requires generalization as well."}, {"heading": "6 Experimental Results", "text": "Our test set is composed of the 61 crossword puzzles from June and July of 2015. In this section, we describe the baseline system based on Dr.Fill [4], and show that our post-processing (fill in blank and bigram reranking) and additional component based on Wikipedia consistently improve the accuracy of the solver."}, {"heading": "6.1 Baseline", "text": "We consider the following setting as our baseline, which matches the system of Dr.Fill [4] as much as possible. We use two Lucene components based on our crossword datasets, CWG Lucene Component and OTSYS Lucene Component, to retrieve candidate answers. We replicate Dr.Fill\u2019s CSP algorithm with LDS depth limit of 4."}, {"heading": "6.2 System Evaluation", "text": "Finally, to evaluate the performance of the whole system, we use the percentage of correct squares filled by the system compared with golden standards."}, {"heading": "6.2.1 Post-processing", "text": "We evaluate our post-processing steps by measuring the performance of our baseline system with and without the two post-processing steps. The results are in Table 4. Our baseline system uses only the CWG and OTSYS Lucene components. Fill and Rerank refer to the Fill-in-the-Blank and Bigram Reranking steps respectively.\nWe see that using the Fill-in-the-Blank step consistently improves over no post-processing for all days. Bigram Rerank also achieves improvement on average, and it can help Friday and Saturday puzzles by a large margin."}, {"heading": "6.2.2 Additional Lucene Component", "text": "We add the use of the Wikipedia Lucene component (WIKI) to the baseline system that uses only the CWG and OTSYS Lucene components. As we can see from the results in Table 4, the Wikipedia Lucene component give large improvements for the Wednesday, Saturday and Sunday puzzles. However, it decreases\nthe performance for Thursday and Friday puzzles. The best accuracy for both June 2015 and July 2015 is achieved when we combine two post-processing steps with WIKI."}, {"heading": "7 Analysis and Discussion", "text": "Evaluating the output of a crossword puzzle solver is straightforward: we count how many words or squares are filled correctly as given by the solution. However, crossword puzzle solvers often involve multiple stages and a single final accuracy cannot quantify the contribution of each stage. Therefore, intermediate evaluation metrics are crucial to understand different pieces of the system and point out meaningful directions for future improvements. In this section, we propose several evaluation metrics for individual components, for CSP algorithms, and for the whole system."}, {"heading": "7.1 Individual Component Evaluation", "text": "All components are run and evaluated individually using the the following metrics.\n\u2022 Mean Reciprocal Answer Rank (MRAR).\nMRAR = 1\n|C \u2032| |C\u2032|\u2211 i\u2208C\u2032 1 ri\nwhere C \u2032 is set of queries for which the component gives the correct answers, and ri is the rank of the i-th answer.\n\u2022 Average Precision (AP).\nAP = 1\nN N\u2211 i { 1 if ai \u2208 Ci 0 otherwise\nFor i-th clue, ai is the correct answer, and Ci is the list of candidate answers produced by the component.\n\u2022 Attempt Ratio (AR)\nAR = 1\nN N\u2211 i { 1 if answered 0 otherwise\nA good component should be able to achieve a relatively high MRAR and AP. A high MRAR implies that the component gives helpful results to the solver and will not mislead it by giving low scores to valuable answers. A high precision means that there is a good chance the solver actually has the correct answer somewhere in its candidate lists, which is a requirement in order for it to produce a correct solution. Attempted Ratio (AR) quantifies the coverage ability of the component.\nTable 5 lists evaluation results for three Lucene according to the above-mentioned three metrics. The evaluation is obtained from NYT puzzles from June 2015. As we can see, Lucene components achieve high attempt ratios around 97%, indicating that it attempts to answer most clues. Lucene CWG and OTSYS have high MRAR and AP, while Lucene Wiki has much lower numbers."}, {"heading": "7.2 CSP Evaluation", "text": "To examine the CSP system portion specifically, we compute the following performance statistics:\n\u2022 FromComponents - This is the percentage of clues where the correct word is present in at least one of the component candidate lists.\n\u2022 AverageRank - This is the average rank of the correct word in the final candidate list for CSP if it was produced for a clue.\n\u2022 IntoCSP - This is the percentage of correct words presented by the components that are placed into the final candidate list for CSP.\n\u2022 IntoSolution- This is the percentage of correct words in the candidate list for CSP that are placed into the solution by CSP.\nHere, FromComponents provides an overview of the quality of candidate answers generated by different components. AverageRank and IntoCSP quantify the input quality of CSP, while IntoSolution evaluates the quality of CSP algorithm itself. A good CSP algorithm will pick out correct answers from the input candidate lists, resulting in a small gap between IntoCSP and IntoSolution.\nTable 6 summarizes our evaluation of the CSP algorithm\u2019s performance on the NYT June 2015 and July 2015 puzzles. For Monday, Tuesday, and Wednesday puzzles, the CSP algorithm demonstrates strong performance even though the Average Rank is around 10. However, as the difficulty of puzzles increases for the following weekdays, the Average Rank surges to 25, which lowers the CSP algorithm\u2019s performance."}, {"heading": "8 Web Application", "text": "We have constructed a web interface for our puzzle solver.5 As shown in Figure 3, one can select a puzzle to be solved from all NYT puzzles between 1999 and 2015. One can also choose which candidate generation components will be used by the solver. When a puzzle is loaded, both the empty and correctly filled grids are displayed, along with all of the clues. The empty grid is filled with the generated solution after the puzzle has been solved. Evaluation results such as square and word accuracy are also displayed. Figure 4 fives an output example for our interface.\n5http://clair.si.umich.edu/crossword2015/"}, {"heading": "9 Conclusion", "text": "In this paper, we propose Cruciform, a new crossword solving system. We employ a group of natural language components that each return a list of candidate words with likelihood scores when given a clue. Then, these lists are used in conjunction with the fill intersections in the puzzle grid to formulate a constraint satisfaction problem (CSP). Additional stages, such as fill-in-the-blanks using bigram probabilities, round out the components of the system."}, {"heading": "Acknowledgments", "text": "We thank Di Chen, Yanni Gu, Malcolm MacLachlan, Benjamin Englard, Cody Hansen, Anthony Vito, Seunbum Park, Jonathan Juett, Yue Xu, Jonathan Kummerfeld for helpful discussions and feedback."}], "references": [{"title": "Sacry: Syntax-based automatic crossword puzzle resolution system", "author": ["Gianni Barlacchi", "Massimo Nicosia", "Alessandro Moschitti"], "venue": "ACL-IJCNLP", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Webcrow: A web-based system for crossword solving", "author": ["Marco Ernandes", "Giovanni Angelini", "Marco Gori"], "venue": "In AAAI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Ibm\u2019s watson/deepqa", "author": ["David A Ferrucci"], "venue": "In ACM SIGARCH Computer Architecture News,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Dr. fill: Crosswords and an implemented solver for singly weighted csps", "author": ["Matthew L Ginsberg"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Proverb: The probabilistic cruciverbalist", "author": ["Greg A Keim", "Noam M Shazeer", "Michael L Littman", "Sushant Agarwal", "Catherine M Cheves", "Joseph Fitzgerald", "Jason Grosland", "Fan Jiang", "Shannon Pollard", "Karl Weinmeister"], "venue": "New York Times (NYT),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "A probabilistic approach to solving crossword puzzles", "author": ["Michael L Littman", "Greg A Keim", "Noam Shazeer"], "venue": "Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "AI* IA 2005: Advances in Artificial Intelligence: 9th Congress of the Italian Association for Artificial Intelligence Milan", "author": ["Sara Manzoni"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Artificial intelligence: a modern approach, volume 2. Prentice hall", "author": ["Stuart Jonathan Russell", "Peter Norvig", "John F Canny", "Jitendra M Malik", "Douglas D Edwards"], "venue": "Upper Saddle River,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}], "referenceMentions": [{"referenceID": 3, "context": "Fill system[4].", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "One early system designed to solve crossword puzzles, Proverb [6], uses a variety of modules that targeted different types of clues and combined them into a probabilistic list in order to place them into the puzzle.", "startOffset": 62, "endOffset": 65}, {"referenceID": 3, "context": "Fill [4] treats the crossword problem as a probabilistic constraint satisfaction problem and uses various lexical features to rank the candidate answers.", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "IBM\u2019s Watson [3] uses an enormous amount of computation power and engineering talent to successfully take on another difficult natural language game: Jeopardy!.", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "Compared with other Question Answering systems, crossword solving systems are generally more complex, because of the following properties of crossword puzzles: [7]:", "startOffset": 160, "endOffset": 163}, {"referenceID": 3, "context": "Fill system [4] starts with producing candidate answers for each clue.", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "For additional details, see [4].", "startOffset": 28, "endOffset": 31}, {"referenceID": 4, "context": "Proverb [5, 6] uses thirty expert modules.", "startOffset": 8, "endOffset": 14}, {"referenceID": 5, "context": "Proverb [5, 6] uses thirty expert modules.", "startOffset": 8, "endOffset": 14}, {"referenceID": 1, "context": "Webcrow [2] draws candidate words from web documents and previously solved crosswords.", "startOffset": 8, "endOffset": 11}, {"referenceID": 0, "context": "SACRY is another crossword solving system introduced in [1] that introduces a new reranking module into the WebCrow system.", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "A CSP instance can formally be defined with the triple (X,D,C) [8].", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "Fill [4], and show that our post-processing (fill in blank and bigram reranking) and additional component based on Wikipedia consistently improve the accuracy of the solver.", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "Fill [4] as much as possible.", "startOffset": 5, "endOffset": 8}], "year": 2016, "abstractText": "Crossword puzzles are popular word games that require not only a large vocabulary, but also a broad knowledge of topics. Answering each clue is a natural language task on its own as many clues contain nuances, puns, or counter-intuitive word definitions. Additionally, it can be extremely difficult to ascertain definitive answers without the constraints of the crossword grid itself. This task is challenging for both humans and computers. We describe here a new crossword solving system, Cruciform. We employ a group of natural language components, each of which returns a list of candidate words with scores when given a clue. These lists are used in conjunction with the fill intersections in the puzzle grid to formulate a constraint satisfaction problem, in a manner similar to the one used in the Dr. Fill system[4]. We describe the results of several of our experiments with the system.", "creator": "LaTeX with hyperref package"}}}