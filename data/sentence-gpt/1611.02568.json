{"id": "1611.02568", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "PixelSNE: Visualizing Fast with Just Enough Precision via Pixel-Aligned Stochastic Neighbor Embedding", "abstract": "Embedding and visualizing large-scale high-dimensional data in a two-dimensional space is an important problem since such visualization can reveal deep insights out of complex data. Most of the existing embedding approaches, however, run on an excessively high precision, ignoring the fact that at the end, embedding outputs are converted into coarse-grained discrete pixel coordinates in a screen space. Motivated by such an observation and directly considering pixel coordinates in an embedding optimization process, we accelerate Barnes-Hut tree-based t-distributed stochastic neighbor embedding (BH-SNE), known as a state-of-the-art 2D embedding method, and propose a novel method called PixelSNE, a highly-efficient, screen resolution-driven 2D embedding method with a linear computational complexity in terms of the number of data items of each layer in a row.\n\n\n\n\nA high-level implementation of this technique is now supported by OpenCL, open source, and the OpenCL OpenCL OpenCL API for high resolution data, as well as the OpenCL OpenCL OpenCL API for high resolution data, as well as the OpenCL OpenCL OpenCL API for high resolution data, as well as the OpenCL OpenCL OpenCL API for high resolution data, as well as the OpenCL OpenCL OpenCL API for high resolution data, as well as the OpenCL OpenCL OpenCL API for high resolution data, as well as the OpenCL OpenCL OpenCL API for high resolution data, as well as the OpenCL OpenCL OpenCL API for high resolution data, as well as the OpenCL OpenCL OpenCL API for high resolution data, as well as the OpenCL OpenCL OpenCL API for high resolution data, as well as the OpenCL OpenCL OpenCL API for high resolution data, as well as the OpenCL OpenCL OpenCL API for high resolution data, as well as the OpenCL OpenCL OpenCL API for high resolution data, as well as the OpenCL OpenCL OpenCL API for high resolution data, as well as the OpenCL OpenCL OpenCL API for high resolution data, as well as the OpenCL OpenCL OpenCL API for high resolution data, as well as the OpenCL OpenCL OpenCL API for high resolution data, as well as the OpenCL OpenCL OpenCL API for high resolution data, as well as the OpenCL OpenCL OpenCL API for high resolution data, as well as the Open", "histories": [["v1", "Tue, 8 Nov 2016 15:50:27 GMT  (6982kb,D)", "http://arxiv.org/abs/1611.02568v1", null], ["v2", "Thu, 10 Nov 2016 03:28:39 GMT  (6982kb,D)", "http://arxiv.org/abs/1611.02568v2", null], ["v3", "Fri, 3 Mar 2017 06:28:34 GMT  (4339kb,D)", "http://arxiv.org/abs/1611.02568v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["minjeong kim", "minsuk choi", "sunwoong lee", "jian tang", "haesun park", "jaegul choo"], "accepted": false, "id": "1611.02568"}, "pdf": {"name": "1611.02568.pdf", "metadata": {"source": "CRF", "title": "PixelSNE: Visualizing Fast with Just Enough Precision via Pixel-Aligned Stochastic Neighbor Embedding", "authors": ["Minjeong Kim", "Minsuk Choi", "Sunwoong Lee", "Jian Tang", "Haesun Park", "Jaegul Choo"], "emails": ["minjeong1642@gmail.com", "mchoi@korea.ac.kr", "solomoj94@gmail.com", "jiant@umich.edu", "hpark@cc.gatech.edu", "jchoo@korea.ac.kr", "permissions@acm.org."], "sections": null, "references": [{"title": "A hierarchical o (n log n) force-calculation", "author": ["Josh Barnes", "Piet Hut"], "venue": "algorithm. nature,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1986}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "Neural Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Adaptive mesh refinement for hyperbolic partial differential equations", "author": ["Marsha J Berger", "Joseph Oliger"], "venue": "Journal of Computational Physics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1984}, {"title": "GTM: The generative topographic mapping", "author": ["Christopher M Bishop", "Markus Svens\u00e9n", "Christopher KI Williams"], "venue": "Neural computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "The elastic embedding algorithm for dimensionality reduction", "author": ["Miguel A Carreira-Perpin\u00e1n"], "venue": "In Proc. the International Conference on Machine Learning (ICML),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Customizing computational methods for visual analytics with big data", "author": ["Jaegul Choo", "Haesun Park"], "venue": "IEEE Computer Graphics and Applications (CG&A),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Linear basis-function t-sne for fast nonlinear dimensionality reduction", "author": ["A. Gisbrecht", "B. Mokbel", "B. Hammer"], "venue": "Proc. the International Joint Conference on Neural Networks (IJCNN), pages 1\u20138", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(5786):504\u2013507", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Stochastic neighbor embedding", "author": ["Geoffrey E Hinton", "Sam T Roweis"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Principal component analysis", "author": ["Ian T. Jolliffe"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Self-organizing maps", "author": ["T. Kohonen"], "venue": "Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis", "author": ["J. Kruskal"], "venue": "Psychometrika, 29:1\u201327", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1964}, {"title": "Nonmetric multidimensional scaling: A numerical method", "author": ["J. Kruskal"], "venue": "Psychometrika, 29:115\u2013129", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1964}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, 401:788\u2013791", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1999}, {"title": "Nonlinear dimensionality reduction", "author": ["John A Lee", "Michel Verleysen"], "venue": "Springer Science & Business Media,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Learning a parametric embedding by preserving local structure", "author": ["Laurens Maaten"], "venue": "In Proc. the International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "A theory for multiresolution signal decomposition: the wavelet representation", "author": ["S.G. Mallat"], "venue": "IEEE  Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 11(7):674\u2013693", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1989}, {"title": "L", "author": ["N. Pezzotti", "B. Lelieveldt"], "venue": "van der Maaten, T. Hollt, E. Eisemann, and A. Vilanova. Approximated and user steerable tsne for progressive visual analytics. IEEE Transactions on Visualization and Computer Graphics (TVCG), PP(99):1\u20131", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Think globally", "author": ["L.K. Saul", "S.T. Roweis"], "venue": "fit locally: Unsupervised learning of low dimensional manifolds. Journal of Machine Learning Research (JMLR), 4:119\u2013155", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Visualizing large-scale and high-dimensional data", "author": ["Jian Tang", "Jingzhou Liu", "Ming Zhang", "Qiaozhu Mei"], "venue": "In Proc. the International Conference on World Wide Web (WWW),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "A Global Geometric Framework for Nonlinear Dimensionality Reduction", "author": ["Joshua B. Tenenbaum", "Vin de Silva", "John C. Langford"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2000}, {"title": "Visualizing data using t-SNE", "author": ["L. van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Stochastic triplet embedding", "author": ["L. van der Maaten", "K. Weinberger"], "venue": "In IEEE International Workshop on Machine Learning for Signal Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Accelerating t-sne using tree-based algorithms", "author": ["Laurens Van Der Maaten"], "venue": "Journal of machine learning research (JMLR),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Visualizing non-metric similarities in multiple maps", "author": ["Laurens van der Maaten", "Geoffrey Hinton"], "venue": "Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Dimensionality reduction: A comparative review", "author": ["LJP Van der Maaten", "EO Postma", "HJ Van den Herik"], "venue": "Technical Report TiCC TR 2009-005,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Information retrieval perspective to nonlinear dimensionality reduction for data visualization", "author": ["Jarkko Venna", "Jaakko Peltonen", "Kristian Nybo", "Helena Aidos", "Samuel Kaski"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Unsupervised learning of image manifolds by semidefinite programming", "author": ["Kilian Weinberger", "Lawrence Saul"], "venue": "International Journal of Computer Vision,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Data structures and algorithms for nearest neighbor search in general metric spaces", "author": ["Peter N Yianilos"], "venue": "In Proc. the ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1993}], "referenceMentions": [{"referenceID": 9, "context": ", principal component analysis [10] and multidimensional scaling [12, 13]) and recent manifold learning methods (e.", "startOffset": 31, "endOffset": 35}, {"referenceID": 11, "context": ", principal component analysis [10] and multidimensional scaling [12, 13]) and recent manifold learning methods (e.", "startOffset": 65, "endOffset": 73}, {"referenceID": 12, "context": ", principal component analysis [10] and multidimensional scaling [12, 13]) and recent manifold learning methods (e.", "startOffset": 65, "endOffset": 73}, {"referenceID": 20, "context": ", isometric feature mapping [22], locally linear embedding [20], and Laplacian Eigenmaps [2]).", "startOffset": 28, "endOffset": 32}, {"referenceID": 18, "context": ", isometric feature mapping [22], locally linear embedding [20], and Laplacian Eigenmaps [2]).", "startOffset": 59, "endOffset": 63}, {"referenceID": 1, "context": ", isometric feature mapping [22], locally linear embedding [20], and Laplacian Eigenmaps [2]).", "startOffset": 89, "endOffset": 92}, {"referenceID": 21, "context": "These methods, however, do not properly handle the significant information loss due to reducing high dimensionality down to two or three, and in response, an advanced dimension reduction technique called t-distributed stochastic neighbor embedding (t-SNE) [23] has been proposed, showing its outstanding advantages in generating 2D scatterplots.", "startOffset": 256, "endOffset": 260}, {"referenceID": 23, "context": ", BarnesHut SNE (BH-SNE) [25] with the complexity of O (n logn), it still takes much time to apply them to large-scale data.", "startOffset": 25, "endOffset": 29}, {"referenceID": 5, "context": "Moreover, when making sense of a 2D scatterplot, human perception does not often require the results with a high precision [6].", "startOffset": 123, "endOffset": 126}, {"referenceID": 25, "context": "Dimension reduction or low-dimensional embedding of highdimensional data [27] has long been an active research area.", "startOffset": 73, "endOffset": 77}, {"referenceID": 9, "context": "Widely-used dimension reduction methods used for visualization application include principal component analysis (PCA) [10], multidimensional scaling [12, 13], Sammon mapping [19], generative topographic mapping [4], and selforganizing map [11].", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "Widely-used dimension reduction methods used for visualization application include principal component analysis (PCA) [10], multidimensional scaling [12, 13], Sammon mapping [19], generative topographic mapping [4], and selforganizing map [11].", "startOffset": 149, "endOffset": 157}, {"referenceID": 12, "context": "Widely-used dimension reduction methods used for visualization application include principal component analysis (PCA) [10], multidimensional scaling [12, 13], Sammon mapping [19], generative topographic mapping [4], and selforganizing map [11].", "startOffset": 149, "endOffset": 157}, {"referenceID": 3, "context": "Widely-used dimension reduction methods used for visualization application include principal component analysis (PCA) [10], multidimensional scaling [12, 13], Sammon mapping [19], generative topographic mapping [4], and selforganizing map [11].", "startOffset": 211, "endOffset": 214}, {"referenceID": 10, "context": "Widely-used dimension reduction methods used for visualization application include principal component analysis (PCA) [10], multidimensional scaling [12, 13], Sammon mapping [19], generative topographic mapping [4], and selforganizing map [11].", "startOffset": 239, "endOffset": 243}, {"referenceID": 14, "context": "While these traditional methods generally focus on preserving global relationships rather than local ones, a class of nonlinear, local dimension reduction techniques called manifold learning [15] has been actively studied, trying to recover an intrinsic curvilinear manifold out of given high-dimensional data.", "startOffset": 191, "endOffset": 195}, {"referenceID": 20, "context": "Representative methods are isometric feature mapping [22], locally linear embedding [20], Laplacian Eigenmaps [2], maximum variance unfolding [29], and autoencoder [8].", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "Representative methods are isometric feature mapping [22], locally linear embedding [20], Laplacian Eigenmaps [2], maximum variance unfolding [29], and autoencoder [8].", "startOffset": 84, "endOffset": 88}, {"referenceID": 1, "context": "Representative methods are isometric feature mapping [22], locally linear embedding [20], Laplacian Eigenmaps [2], maximum variance unfolding [29], and autoencoder [8].", "startOffset": 110, "endOffset": 113}, {"referenceID": 27, "context": "Representative methods are isometric feature mapping [22], locally linear embedding [20], Laplacian Eigenmaps [2], maximum variance unfolding [29], and autoencoder [8].", "startOffset": 142, "endOffset": 146}, {"referenceID": 7, "context": "Representative methods are isometric feature mapping [22], locally linear embedding [20], Laplacian Eigenmaps [2], maximum variance unfolding [29], and autoencoder [8].", "startOffset": 164, "endOffset": 167}, {"referenceID": 21, "context": "Specifically focusing on visualization applications, a recent method, t-distributed stochastic neighbor embedding [23], which is built upon stochastic neighbor embedding [9], has shown its superiority in generating the 2D scatterplots that can reveal meaningful insights about data such as clusters and outliers.", "startOffset": 114, "endOffset": 118}, {"referenceID": 8, "context": "Specifically focusing on visualization applications, a recent method, t-distributed stochastic neighbor embedding [23], which is built upon stochastic neighbor embedding [9], has shown its superiority in generating the 2D scatterplots that can reveal meaningful insights about data such as clusters and outliers.", "startOffset": 170, "endOffset": 173}, {"referenceID": 15, "context": "For example, a neural network has been integrated with t-SNE to learn the parametric representation of 2D embedding [16].", "startOffset": 116, "endOffset": 120}, {"referenceID": 24, "context": "Rather than the Euclidean distance or its derived similarity information, other information types such as non-metric similarities [26] and relative ordering information about pairwise distances in the form of similarity triplets [24] have been considered as the target information to preserve.", "startOffset": 130, "endOffset": 134}, {"referenceID": 22, "context": "Rather than the Euclidean distance or its derived similarity information, other information types such as non-metric similarities [26] and relative ordering information about pairwise distances in the form of similarity triplets [24] have been considered as the target information to preserve.", "startOffset": 229, "endOffset": 233}, {"referenceID": 4, "context": "Additionally, various other optimization criteria and their optimization approaches, such as elastic embedding [5] and NeRV [28], have been proposed.", "startOffset": 111, "endOffset": 114}, {"referenceID": 26, "context": "Additionally, various other optimization criteria and their optimization approaches, such as elastic embedding [5] and NeRV [28], have been proposed.", "startOffset": 124, "endOffset": 128}, {"referenceID": 23, "context": "An accelerated t-SNE based on the approximation using the BarnesHut tree algorithm has been proposed [25].", "startOffset": 101, "endOffset": 105}, {"referenceID": 6, "context": "proposed a linear approximation of t-SNE [7].", "startOffset": 41, "endOffset": 44}, {"referenceID": 17, "context": "More recently, an approximate, but user-steerable t-SNE, which provides interactions with which a user can control the degree of approximation on user-specified areas, has also been studied [18].", "startOffset": 190, "endOffset": 194}, {"referenceID": 19, "context": "In addition, a scalable 2D embedding technique called LargeVis [21] significantly reduced the computing times with a linear time complexity in terms of the number of data items.", "startOffset": 63, "endOffset": 67}, {"referenceID": 23, "context": "and t-SNE iteratively performs gradient-decent update on Y where the gradient with respect to yi is computed [25] as", "startOffset": 109, "endOffset": 113}, {"referenceID": 28, "context": "The first one called the vantage-point tree [30] approximately computes DX and then P as sparse matrices by ignoring small pairwise distances as zeros (Fig.", "startOffset": 44, "endOffset": 48}, {"referenceID": 0, "context": "1(b)), BH-SNE adopts Barnes-Hut algorithm [1] to compute Frep in Eq.", "startOffset": 42, "endOffset": 45}, {"referenceID": 19, "context": "To accelerate the process of constructing the matrix P , we adopt a recently proposed, highly efficient algorithm of constructing the approximate k-nearest neighbor graph (KNNG) [21].", "startOffset": 178, "endOffset": 182}, {"referenceID": 13, "context": "In addition, we extracted topic labels out of ten topics computed by nonnegative matrix factorization [14] as a topic modeling method.", "startOffset": 102, "endOffset": 106}, {"referenceID": 19, "context": "For PixelSNE, we prepared for its two different versions depending on the algorithm used in constructing P : (1) the vantage-point tree (PixelSNE-VP) used originally in BH-SNE and (2) the random-projection tree (PixelSNERP) used in LargeVis [21] (Section 4.", "startOffset": 241, "endOffset": 245}, {"referenceID": 5, "context": "As pointed out previously [6], such types of algorithms have connections to well-known existing literature in other fields, such as wavelet transform [17] and adaptive mesh refinement [3], which opens up new research directions", "startOffset": 26, "endOffset": 29}, {"referenceID": 16, "context": "As pointed out previously [6], such types of algorithms have connections to well-known existing literature in other fields, such as wavelet transform [17] and adaptive mesh refinement [3], which opens up new research directions", "startOffset": 150, "endOffset": 154}, {"referenceID": 2, "context": "As pointed out previously [6], such types of algorithms have connections to well-known existing literature in other fields, such as wavelet transform [17] and adaptive mesh refinement [3], which opens up new research directions", "startOffset": 184, "endOffset": 187}, {"referenceID": 19, "context": "Additionally, we plan to apply our framework to other advanced algorithms such as LargeVis [21].", "startOffset": 91, "endOffset": 95}], "year": 2016, "abstractText": "Embedding and visualizing large-scale high-dimensional data in a two-dimensional space is an important problem since such visualization can reveal deep insights out of complex data. Most of the existing embedding approaches, however, run on an excessively high precision, ignoring the fact that at the end, embedding outputs are converted into coarsegrained discrete pixel coordinates in a screen space. Motivated by such an observation and directly considering pixel coordinates in an embedding optimization process, we accelerate Barnes-Hut tree-based t-distributed stochastic neighbor embedding (BH-SNE), known as a state-of-the-art 2D embedding method, and propose a novel method called PixelSNE, a highly-efficient, screen resolution-driven 2D embedding method with a linear computational complexity in terms of the number of data items. Our experimental results show the significantly fast running time of PixelSNE by a large margin against BH-SNE, while maintaining the minimal degradation in the embedding quality. Finally, the source code of our method is publicly available at https: //github.com/awesome-davian/sasne.", "creator": "LaTeX with hyperref package"}}}