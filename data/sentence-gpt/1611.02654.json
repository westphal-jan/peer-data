{"id": "1611.02654", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "Sentence Ordering using Recurrent Neural Networks", "abstract": "Modeling the structure of coherent texts is a task of great importance in NLP. The task of organizing a given set of sentences into a coherent order has been commonly used to build and evaluate models that understand such structure. In this work we propose an end-to-end neural approach based on the recently proposed set to sequence mapping framework to address the sentence ordering problem.\n\n\n\n\nA\nAn example of a neural model of an argument based on a set of sentences.\nTo solve this problem, we first define a set of words for each type. Each word must have a single sentence. In this case, all of the sentences are stored in a list of words. Each word must have two meanings. The sentence must have a few distinct words. The sentence can only have a single sentence.\nThe final word can only have two meanings.\nThe next step is to describe a simple representation of two words, with a special meaning, and then describe the sentence in the same way. It can be implemented in a given word.\nThe final word can only have one meaning.\nThe final word can only have two meanings.\nAs you might think, this is a very big work for us. Here we see a very simple representation of the sentence. This works quite well in a number of ways. One of the most important functions of this functional model is that it has the ability to distinguish between a word and a sentence. We can easily distinguish between a sentence and a sentence without specifying the sentence's meaning.\nIn this case, we can identify three meanings with each one. These are:\na sentence to describe a sentence. A sentence to describe a sentence. A sentence to describe a sentence. A sentence to describe a sentence. A sentence to describe a sentence. A sentence to describe a sentence. A sentence to describe a sentence. A sentence to describe a sentence. A sentence to describe a sentence. A sentence to describe a sentence. A sentence to describe a sentence. A sentence to describe a sentence.\nThe final word can only have two meanings.\nNow, we need to define the sentences to each sentence.\nThe key is the word order:\nA word order:\nA sentence to describe a sentence. A sentence to describe a sentence. A sentence to describe a sentence. A sentence to describe a sentence.\nThe final word can only have two meanings.\nThe final word can only have two meanings.\nThe final word can only have two meanings.\nThe final word can only", "histories": [["v1", "Tue, 8 Nov 2016 19:04:09 GMT  (1389kb,D)", "http://arxiv.org/abs/1611.02654v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["lajanugen logeswaran", "honglak lee", "dragomir radev"], "accepted": false, "id": "1611.02654"}, "pdf": {"name": "1611.02654.pdf", "metadata": {"source": "CRF", "title": "RECURRENT NEURAL NETWORKS", "authors": ["Lajanugen Logeswaran", "Honglak Lee", "Dragomir Radev"], "emails": ["llajan@umich.edu", "honglak@umich.edu", "radev@umich.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Modeling the structure of coherent texts is one of the central problems in NLP. A well written piece of text has a particular high level logical and topical structure to it. The actual word and sentence choices as well as their transitions come together to convey the purpose of the text. Our overarching goal is to build models that can learn such structure by learning to arrange a given set of sentences to make coherent text.\nThe sentence ordering task finds several applications. Multi-document Summarization (MDS) and retrieval based question answering involve extracting information from multiple source documents and organizing the content into a coherent summary. Since the relative ordering about sentences that come from different sources can be unclear, being able to automatically evaluate a particular order and/or finding the optimal order is essential. Barzilay and Elhadad (2002) discuss the importance of an explicit ordering component in MDS systems. Their experiments show that finding an acceptable ordering can enhance user comprehension.\nModels that learn to order text fragments can also be used as models of coherence. Automated essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2010) is an application that can benefit from such a coherence model. Coherence is one of the key elements on which student essays are evaluated in standardized writing tests such as GRE (ETS). Apart from its importance and applications, our motivation to address this problem also stems from its stimulating nature. It can be considered as a jigsaw puzzle of sorts in the language domain.\nOur approach to the problem of modeling coherence is driven by recent successes in 1) capturing semantics using distributed representations and 2) using RNNs for sequence modeling tasks.\nSuccess in unsupervised approaches for learning embeddings for textual entities from large text corpora altered the way NLP problems are studied today. These embeddings have been shown to capture syntactic and semantic information as well as higher level analogical structure. These methods have been adopted to learn vector representations of sentences, paragraphs and entire\nar X\niv :1\n61 1.\n02 65\n4v 1\n[ cs\n.C L\n] 8\nN ov\n2 01\n6\ndocuments. Embedding based approaches allow models to be trained end-to-end from scratch with no handcrafting.\nRecurrent Neural Networks (RNNs) have become the de facto approach to sequence learning and mapping problems in recent times. The Sequence to sequence mapping framework (Sutskever et al., 2014), as well as several of its variants have fuelled RNN based approaches to a wide variety of problems including language modeling, language generation, machine translation, question answering and many others.\nVinyals et al. (2015a) recently showed that the order in which tokens of the input sequence are fed to seq2seq models has a significant impact on the performance of the model. In particular, for problems such as sorting which involve a source set (as opposed to a sequence), the optimal order to feed the tokens is not clear. They introduce an attention mechanism over the input tokens which allows the model to learn a soft input order. This is called the read, process and write (or set to sequence) framework. The read block maps the input tokens to a fixed length vector representation. The process block is an RNN encoder which, at each time step, attends to the input token embeddings and computes an attention readout, appending it to the current hidden state. The write block is an RNN which produces the target sequence conditioned on the representation produced by the process block.\nIn this work we propose an RNN based approach to the sentence ordering problem which exploits the set to sequence framework. A word level RNN encoder produces sentence embeddings. A sentence level set encoder RNN iteratively attends to these embeddings (process block above) and constructs a representation of the context. Initialized with this representation, a sentence level pointer network RNN points to the next sentence candidates.\nThe most widely studied task relevant to sentence ordering and coherence modeling in the literature is the order discrimination task. Given a document and a permuted version of it, the task involves identifying the more coherent ordering of the two. Our proposed model achieves state of the art performance on two benchmark datasets for this task, outperforming several classical approaches and more recent data-driven approaches.\nAddressing the more challenging task of ordering a given collection of sentences, we consider the novel and interesting task of ordering sentences from abstracts of conference papers and research grants. Our model strongly outperforms previous work on this task. We visualize the learned sentence representations and show that our model captures high level discourse structure. We provide visualizations that aid understanding what information in the sentences the model uses to identify the next sentence. We also study the quality of the sentence representations learned by the model by training the model on a large text corpus and show that these embeddings are comparable to recent unsupervised methods in capturing semantics.\nIn summary our key contributions are as follows,\n\u2022 We propose an end to end trainable model based on the set to sequence framework to address the challenging problem of organizing a given collection of sentences in a coherent order.\n\u2022 We consider the novel task of understanding structure in abstract paragraphs and demonstrate state of the art results in order discrimination and sentence ordering tasks.\n\u2022 We demonstrate that the proposed model is capable of learning semantic representations of sentences that are comparable to recently proposed methods for learning such representations."}, {"heading": "2 RELATED WORK", "text": "Coherence modeling and sentence ordering The coherence modeling and sentence ordering tasks have been approached by closely related techniques. Most approaches propose a measure of coherence and formulate the ordering problem as finding an order with maximal coherence. Recurring themes from prior work include linguistic features, centering theory, local and global coherence.\nLocal coherence has been modeled by considering properties of a local window of sentences such as sentence similarity and sentence transition structure. Foltz et al. (1998) represent words using vectors of co-occurent counts and sentences as a mean of these word vectors. Sentence similarity is defined as the cosine distance between sentence vectors and text coherence is modeled as a normalized sum of similarity scores of adjacent sentences. Lapata (2003) represents sentences by vectors of\nlinguistic features and learn the transition probabilities from one set of features to another in adjacent sentences. A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns.\nGlobal models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text. Louis and Nenkova (2012) attempt to capture the intentional structure in documents using syntax as a proxy for the communicative goal of a sentence. Syntax features such as parse tree production rules and constituency tags at a particular tree depth were used.\nUnlike previous approaches, we do not employ any handcrafted features and adopt an embedding based approach. Local coherence is taken into account by having a next sentence prediction component in the model and global dependencies are naturally captured by an RNN. We demonstrate that our model is able to capture both logical and topical structure by evaluating its performance on different types of data.\nData-driven approaches Neural approaches have gained attention more recently. Li and Hovy (2014) model sentences as embeddings derived from recurrent/recursive neural nets and train a feedforward neural network that takes an input window of sentence embeddings and outputs a probability which represents the coherence of the sentence window. Coherence evaluation is performed by sliding the window over the text and aggregating the score. Li and Jurafsky (2016) study the same model in a larger scale task and also consider a sequence to sequence approach where the model is trained to generate the next sentence given the current sentence and vice versa. Chen et al. (2016) also propose a sentence embedding based approach where they model the probability that one sentence should come before another and define coherence based on the likelihood of the relative order of every pair of sentences. We believe these models are limited by the fact that they are local in nature and our experiments show that exploiting larger contexts can be very beneficial.\nHierarchical RNNs for document modeling Word level and sentence level RNNs have been used in a hierarchical fashion for modeling documents in prior work. Li et al. (2015b) proposed a hierarchical document autoencoder which has potential to be used in generation and summarization applications. More relevant to our work is a similar model (but without an encoder) considered by Lin et al. (2015). A sentence level RNN predicts the bag of words in the next sentence given the previous sentences and a word level RNN predicts the word sequence conditioned on the sentence level RNN hidden state. The model has a structure similar to the content model of Barzilay and Lee (2004) with RNNs playing the roles of the HMM and the bigram language model. Our model has a hierarchical nature in that a sentence level RNN operates over words of a sentence and a document level RNN operates over sentence embeddings.\nCombinatorial optimization with RNNs Vinyals et al. (2015a) equip sequence to sequence models with the capability to handle input and output sets, and discuss experiments on sorting, language modeling and parsing. Their goal is to show that input and output orderings can matter in these tasks, which is demonstrated using several small scale experiments. Our work exploits this framework to address the challenging problem of modeling logical and hierarchical structure in text. Vinyals et al. (2015b) proposed pointer-networks, aimed at combinatorial optimization problems where the output dictionary size depends on the number of input elements. We use a pointer-network that points to each of the next sentence candidates as the decoder."}, {"heading": "3 APPROACH", "text": "Our proposed model is inspired by the way a human would solve this task. First, the model attempts to read the sentences to capture the semantics of the sentences as well as the general context of the paragraph. Given this knowledge, the model attempts to pick the sentences one by one sequentially till exhaustion.\nOur model is based on the read, process and write framework proposed by Vinyals et al. (2015a) briefly discussed in section 1. We use the encoder-decoder terminology that is more common in the literature in the following discussion.\nThe model is comprised of a sentence encoder RNN, an encoder RNN and a decoder RNN (figure 1). An RNN sentence encoder takes as input the words of a sentence s sequentially and computes an embedding representation of the sentence (Figure 1a). Henceforth, we shall use s to refer to a sentence or its embedding interchangeably. The embeddings {s1, s2, ..., sn} of a given set of n sentences constitute the sentence memory, available to be accessed by subsequent components.\nThe encoder is identical to the originally proposed process block and is defined by equations 1-5 (See Figure 1b). Following the regular LSTM hidden state (ht\u22121enc , c t\u22121 enc ) update, the hidden state is concatenated with an attention readout vector statt, and this concatenated vector is treated as the hidden state for the next time step (Equation 5). Attention probabilities are computed by composing the hidden state with embeddings of the candidate sentences through a scoring function f and taking the softmax (Equations 2, 3). This process is iterated for a number of times, called the number of read cycles. As described in Vinyals et al. (2015a) the encoder has the desirable property of being invariant to the order in which the sentence embeddings reside in the memory. The LSTM used here does not take any inputs (input is clamped to zero).\nh\u0303tenc, c t enc = LSTM(h t\u22121 enc , c t\u22121 enc ) (1)\net,ienc = f(si, h\u0303 t enc); i \u2208 {1, ..., n} (2)\natenc = Softmax(e t enc) (3)\nstatt = n\u2211 i=1 at,iencsi (4)\nhtenc = [h\u0303 t enc s t att] (5)\nThe decoder is a pointer network that takes a similar form with a few differences (equations 6-8, Figure 1c). The LSTM takes the embedding of the previous sentence also as input: At training time the correct order of sentences (so1 , so2 , ..., son) = (x\n1, x2, ..., xn) is known (o represents the correct order) and xt\u22121 is used as the input. At test time the predicted assignment x\u0302t\u22121 is used instead. This makes concatenating the attention readout to the hidden state somewhat redundant (verified empirically), and hence it is omitted. The attention computation is identical to that of the encoder.\nThe initial state of the decoder LSTM is initialized with the final hidden state of the encoder as in sequence to sequence models.1 x0 is a vector of zeros. Figure 1 illustrates the single time-step computation in the encoder and decoder.\nhtdec, c t dec = LSTM(h t\u22121 dec , c t\u22121 dec , x t\u22121) (6)\net,idec = f(si, h t dec); i \u2208 {1, ..., n} (7) atdec = Softmax(e t dec) (8)\nThe attention probability at,idec is interpreted as the probability for si being the correct sentence choice at position t, conditioned on the previous sentence assignments p(St = si|S1, ..., St\u22121)."}, {"heading": "3.1 SCORING FUNCTION", "text": "We consider two choices for the scoring functions f in our experiments. The first one is a single hidden layer feed-forward net that takes s, h as inputs and outputs a score f(s, h) = W \u2032tanh(W [s h] + b) + b\u2032 where W, b,W \u2032, b\u2032 are learnable parameters. This scoring function takes a discriminative approach in classifying the next sentence. Note that the structure of this scoring function is similar to the window network in Li and Hovy (2014). While they used a local window of sentences to capture context, this scoring function exploits the RNN hidden state to score sentence candidates.\nWe also consider a bilinear scoring function f(s, h) = sT (Wh + b). Compared to the previous scoring function, this takes a generative approach of trying to regress the next sentence given the current hidden state (Wh+ b) and enforcing that it be most similar to the correct next sentence. We observed that this scoring function led to learning better sentence representations (section 4.4)."}, {"heading": "3.2 TRAINING OBJECTIVE", "text": "The model is trained with the maximum likelihood objective:\nmax \u2211 x\u2208D |x|\u2211 t=1 log p(xt|x1, ..., xt\u22121) (9)\nwhere D denotes the training set and each training instance is given by an ordered document of sentences x = (x1, ..., x|x|). We also considered an alternative structured margin loss which imposes less penalty for assigning high scores to sentence candidates that are close to the correct sentence in the source document instead of uniformly penalizing all incorrect sentence candidates. However, the softmax output with cross entropy loss consistently performed better."}, {"heading": "3.3 COHERENCE MODELING", "text": "We define the coherence score of an arbitrary partial/complete assignment (sp1 , ..., spk) to the first k sentence positions as\nk\u2211 i=1 log p(Si = spi |S1 = sp1 , ..., Si\u22121 = spi\u22121) (10)\nwhere S1, .., Sk are random variables representing the sentence assignment to positions 1 through k. The conditional probabilities are derived from the network. This is our measure of comparing the coherence of different renderings of a document. It is also used as a heuristic during decoding."}, {"heading": "4 EXPERIMENTAL RESULTS", "text": ""}, {"heading": "4.1 MODEL TRAINING", "text": "For both the tasks discussed in the next sections we train the model with the same objective (equation 9) on the training data relevant to the task. Models are trained end-to-end. Model parameters. We use pre-trained 300 dimensional GloVe word embeddings (Pennington et al., 2014). All LSTMs use a hidden layer size of 1000 and the MLP in section 3.1 has a hidden layer size of 500. The number of read cycles in the encoder is set to 10. The same model architecture is used across all experiments.\n1A subtle difference is that the final hidden state of the encoder hNenc has more dimensions than h0dec and only the first part of the vector is copied (The attention readout is ignored for this time step).\nPreprocessing. The nltk sentence tokenizer was used for word tokenization. The GloVe vocabulary was used as the reference vocabulary. Any word not in the vocabulary is checked for a case insensitive match. If a token is hyphenated, we check if the constituent words are in the vocabulary. In the AAN abstracts data (section 4.3.1), some words tend to have a hyphen in the middle because of word hyphenation across lines in the original document. Hence we also check if stripping hyphens produces a vocabulary word. If all checks fail, and a token appears in the training set above a certain frequency, it is added to the vocabulary. Learning . We used a batch size of 10 and the Adam optimizer (Kingma and Ba, 2014) with a base learning rate of 5e-4 for all experiments. Early stopping is used for regularization."}, {"heading": "4.2 ORDER DISCRIMINATION", "text": "Finding the optimal ordering is a difficult problem when a large number of sentences are required to be rearranged or when there is inherent ambiguity in the ordering of the sentences. For this reason, the ordering problem is commonly formulated as the following binary classification task. Given a reference paragraph and a permuted version of it, the more coherently organized one needs to be identified (Barzilay and Lapata, 2008)."}, {"heading": "4.2.1 DATA", "text": "We consider data from two different domains that have been widely used for this task in previous work since Barzilay and Lee (2004); Barzilay and Lapata (2008). The ACCIDENTS data (aka AIRPLANE data) is a set of aviation accident reports from the National Transportation Safety Board\u2019s database. The EARTHQUAKES data comprises newspaper articles from the North American News Text Corpus. In each of the above datasets the training and test sets include 100 articles as well as approximately 20 permutations of each article. Further statistics about the data are shown in table 1."}, {"heading": "4.2.2 RESULTS", "text": "Table 2 compares the performance of our model against prior approaches. We compare results against traditional approaches in the literature as well as some recent data-driven approaches (See section 2 for more details). The entity grid model provides a strong baseline on the ACCIDENTS dataset, only outperformed by our model and Li and Jurafsky (2016) . On the EARTHQUAKE data the window approach of Li and Hovy (2014) and Li and Jurafsky (2016) perform strongly. Our approach outperforms prior models on both datasets, achieving near perfect performance on the Earthquakes dataset.\nWhile these datasets have been widely used in the literature, they are quite formulaic in nature and are no longer challenging. We hence turn to the more challenging task of ordering a given collection of sentences to make a coherent document."}, {"heading": "4.3 SENTENCE ORDERING", "text": "In this task we directly address the ordering problem. We do not assume the availability of a set of candidate orderings to choose from and instead attempt to find a good ordering from all possible permutations of the sentences.\nThe difficulty of the ordering problem depends on the nature of the text as well as the length of paragraphs considered. Evaluation on text from arbitrary text sources makes it difficult to interpret the results, since it may not be clear whether to attribute the observed performance to a deficient model or ambiguity in next sentence choices due to many plausible orderings.\nText summaries are a suitable source of data for this task. They often exhibit a clear flow of ideas and have minimal redundancy. We specifically look at abstracts of conference papers and NSF research proposals. This data has several favorable properties. Abstracts usually have a particular high level format - They start out with a brief introduction, a description of the problem addressed and proposed approach and conclude with performance remarks. This would allow us to identify if the model is capable of capturing high level logical structure. Second, abstracts have an average length of about 10, making the ordering task more accessible. Furthermore, this also gives us a significant amount of data to train and test our models."}, {"heading": "4.3.1 DATA", "text": "NIPS Abstracts. We consider abstracts from NIPS papers in the past 10 years. We parsed 3280 abstracts from paper pdfs and obtained 3259 abstracts after omitting erroneous extracts. The dataset was split into years 2005-2013 for training and years 2014, 2015 respectively for validation and testing2.\nACL Abstracts. A second source of abstracts we consider are papers from the ACL Anthology Network (AAN) corpus (Radev et al., 2009) of ACL papers. At the time of retrieval, the corpus had publications up to year 2013. We extracted abstracts from the text parses using simple keyword matching for the strings \u2018Abstract\u2019 and \u2018Introduction\u2019. Our extraction is successful for 12,157 articles. Most of the failures occur for older papers due to improper formatting and OCR issues. We use all extracts of papers published up to year 2010 for training, year 2011 for validation and years 2012-2013 for testing. We additionally merge words hyphenated at the edges of paragraph boundaries.\nNSF Abstracts. We also evaluate our model on the NSF Research Award Abstracts dataset (Lichman, 2013). This dataset comprises abstracts from a diverse set of scientific areas in contrast to the previous two sources of data and the abstracts are also lengthier, making this dataset more challenging. Years 1990-1999 were used for training, 2000 for validation and 2001-2003 for testing. We capped the parses of the abstracts to a maximum length of 40 sentences. Unsuccessful parses and parses of excessive length were discarded. Further details about the datasets are provided in table 1."}, {"heading": "4.3.2 METRICS", "text": "We use the following metrics to evaluate performance on this task. Accuracy measures how often the absolute position of a sentence was correctly predicted. Being a too stringent measure, it penalizes correctly predicted subsequences that are shifted. Another metric widely used in the literature is Kendall\u2019s tau (\u03c4 ), computed as 1\u2212 2\u00d7 (number of inversions)/ ( n 2 ) , where the number of inversions is the number of pairs in the predicted sequence with incorrect relative order and n is the length of the sequence. Lapata (2006) discusses that this metric reliably correlates with human judgements.\n2Experimentation with a random split yielded similar performance. We adopt this split so that future work can easily perform comparisons with our results."}, {"heading": "4.3.3 BASELINES", "text": "Entity Grid. Our first baseline is the Entity Grid model of Barzilay and Lapata (2008). We use the Stanford parser Klein and Manning (2003) to get constituency trees for all sentences in our datasets. We derive entity grid representations for the parsed sentences using the Brown Coherence Toolkit.3 A ranking SVM is trained to score correct orderings higher than incorrect orderings as in the original work. We used 20 permutations per document as training data. Since the entity grid representation only provides a means of feature extraction we evaluate the model in the ordering setting as follows. We choose 1000 random permutations for each document, one of them being the correct order, and pick the order with maximum coherence. We experimented with transitions of length at most 3 in the entity-grid.\nSequence to sequence. The second baseline we consider is a sequence to sequence model which is trained to predict the next sentence given the current sentence. Li and Jurafsky (2016) consider similar methods and our model is same as the uni-directional model in their work. These methods were shown to yield sentence embeddings that have competitive performance in several semantic tasks in Kiros et al. (2015).\nWindow Network. We consider the window approach of Li and Hovy (2014) and Li and Jurafsky (2016) which demonstrated strong performance in the order discrimination task as our third baseline. We adopt the same coherence score interpretation considered by the authors in the above work. In both the above models we consider a special embedding vector which is padded at the beginning of a paragraph and learned during training. This vector allows us to identify the initial few sentences during greedy decoding.\nRNN Decoder. Another baseline we consider is our proposed model without the encoder. The decoder hidden state is initialized with zeros. We observed that using a special start symbol as for the other baselines helped obtain better performance with this model. However, a start symbol did not help when the model is equipped with an encoder as the hidden state initialization alone was good enough.\nWe do not place emphasis on the particular search algorithm in this work and thus use beam search using the coherence score heuristic for all models. A beam size of 100 was used. During decoding, sentence candidates that have been already chosen are pruned from the beam. All RNNs use a hidden layer size of 1000. For the window network we used a window size of 3 and a hidden layer size of 2000. We initialize all models with pre-trained GloVe word embeddings."}, {"heading": "4.3.4 RESULTS", "text": "We assess the performance of our model against baseline methods in table 3. The window network performs strongly compared to the other baselines, outperforming the decoder only version of our model in the NIPS and ACL abstracts data. These results show that local context alone conveys significant information about the next sentence. Our model does better by a significant margin by exploiting global context, which shows how global context is important to be successful in this task.\nWhile the Entity-Grid model has been fairly successful for the order discrimination task in the past we observe that it fails to discriminate between a large number of candidates. One reason could be that the feature representation is fairly less sensitive to local changes in sentence order (such as\n3https://bitbucket.org/melsner/browncoherence/overview\nswapping adjacent sentences). We did not use coreference resolution for computing the entity-grids due to the computational overhead. This could potentially improve results by a few percentage points. The computational expense of obtaining parse trees and constructing grids on a large amount of data prohibited us from experimenting with this model on the NSF abstracts data.\nThe sequence to sequence model falls short of the window network in performance. Interestingly, Li and Jurafsky (2016) observe that the seq2seq model outperforms the window network in an order discrimination task on wikipedia data. However, the wikipedia data considered in their work has an order of magnitude more data that the datasets considered here, and that could have potentially helped the generative model. These models are also expensive during inference since they involve computing and sampling from word distributions.\nWe observe that our model performs poorly when not equipped with the encoder. However, when the sentence encoder is initialized with a good set of parameters (such as pre-trained skip-thought parameters (Kiros et al., 2015)) this compensates for the lack of an encoder and leads to better performance. This shows that the encoder helps learn good representations and contributes significantly to performance. Another observation we made was that a randomly initialized sentence encoder with smaller hidden layer size performed better than one initialized with pre-trained skip-thought parameters in all the experiments.\nIn Figure 3 we attempt to visualize the sentence representations learned by the sentence encoder in our model. The figure shows 2-dimensional t-SNE embeddings of test set sentences from each of the datasets color coded by their positions in the source abstract. This shows that the model learns high-level structure in the documents, generalizing well to unseen documents. The structure is less apparent in the NSF data which we presume is because of the data diversity and longer documents. While approaches based on the content model of Barzilay and Lee (2004) attempt to explicitly capture topics by discovering clusters in sentences, we observe that the neural approach implicitly discovers such structure."}, {"heading": "4.4 LEARNED SENTENCE REPRESENTATIONS", "text": "One of the original motivations for this work is the question whether we can learn high quality sentence representations by learning to model text coherence. To address this question we trained our model on a large dataset of paragraphs. We chose the BookCorpus dataset (Kiros et al., 2015) for this purpose. We trained the model with two key changes from the models trained on the abstracts data - 1) In addition to the sentences in the paragraph being considered, we added more contrastive sentences from other paragraphs as well. 2) We use the bilinear scoring function. These techniques helped obtain better representations when training on large amounts of data.\nTo evaluate the quality of the sentence embeddings derived from the model, we use the evaluation pipeline of Kiros et al. (2015) for tasks that involve understanding sentence semantics. These evaluations are performed by training a classifier on top of the embeddings derived from the model so that the performance is indicative of the quality of sentence representations. We consider the semantic relatedness and paraphrase detection tasks. Our results are presented in tables 4a, 4b. Results for only uni-directional versions of different models are discussed here for a reasonable comparison.\nSkip-thought vectors are learned by predicting both the previous and next sentences given the current sentence. Following suit, we train two models - one predicting the correct order in the forward direction and another in the backward direction. Note that the sentence level RNN is still unidirectional in both cases. The numbers shown for the ordering model were obtained by concatenating the representations obtained from the two models.\nConcatenating the above representation with the bag of words representation (using the fine-tuned word embeddings) of the sentence further improves performance4. We believe the reason to be that the ordering model can choose to pay less attention to specific lexical information and instead focus on the high level document structure. Hence the two representations can be seen as capturing complementary semantics. Adding the skip-thought embedding features as well improves performance further.\nOur model has several key advantages over the skip-thought model. The skip-thought model has a word-level reconstruction objective and requires training with large softmax output layers. This limits the size of the vocabulary and makes training very time consuming (they use a vocabulary size of 20k and report 2 weeks of training). Our model achieves comparable performance and does not have such a word reconstruction component. We are able to train with a large vocabulary of 400k words and the above results were obtained with a training time of 2 days.\nA conceptual issue surrounding word-level reconstruction is that it forces the model to predict both the meaning and syntax of the target sentence. This makes learning difficult since there are numerous ways of expressing the same idea in syntax. In our model we instead let the model discover features from a sentence which are both predictive (of the next sentence) and predictable (from the previous sentences) and interpret these set of features as a meaning representation. We believe this is an important distinction and hope to study these models further in the context of learning syntax independent semantic representations of sentences."}, {"heading": "5 CONCLUSION", "text": "In this work we considered the challenging problem of coherently organizing a given set of sentences. Our RNN based model performs strongly compared to baseline methods as well as prior work on sentence ordering and order discrimination tasks. We further demonstrated that the model captures high level document structure and learns useful sentence representations when trained on large amounts of data. Our approach to the ordering problem deviates from most prior work that use handcrafted features. However, exploiting linguistic features for next sentence classification can potentially further improve performance on the task. Entity distribution patterns can provide useful features about named entities that are treated as out of vocabulary words. The ordering problem can be further studied at higher level discourse units such as paragraphs, sections and chapters.\n4We used the same hyperparameters that were used for the abstracts data to train our model. The skip-bow and uni-skip embeddings have dimensionality 640, 2400 respectively. Representations from the ordering model have dimensionality 2000, and adding BoW features gives 2600 dimensional embeddings."}, {"heading": "A WORD INFLUENCE", "text": "We attempt to understand what text level clues the model captures to perform the ordering task. Some techniques for visualizing neural network models in the context of text applications are discussed in Li et al. (2015a). Drawing inspiration from this work, we use gradients of prediction decisions with respect to the words of the correct sentence as a proxy for the salience of each word.\nFor each time step during decoding we do the following. Assume the sentence assignments for all previous time steps have been correct. let h be the current hidden state in this setting and s = (w1, ..., wn) be the correct next sentence candidate, the wi being its words. The score for this sentence is defined as e = f(s, h) (See equation 7). The importance of word wi in predicting s as the correct next sentence is interpreted as \u2016 \u2202e\u2202wi \u2016. We assume h to be fixed and only backpropagate gradients through the sentence encoder.\nTable 5 shows visualizations of a few selected abstracts. Words expressed in darker shades correspond to higher gradient norms. In the first example the model seems to be using the word clues \u2018first\u2019, \u2018second\u2019 and \u2018third\u2019. A similar observation was made by Chen et al. (2016) in their experiments. In the second example we observe that the model has paid attention to phrases such as \u2018We present\u2019, \u2018We argue\u2019 which are typical of abstract texts. The model has also focused on the word \u2018representation\u2019\n(a) \u03c4 scores of order predictions on paragraphs of a given length. (b) Accuracy of predicting the correct sentence at a given position.\nFigure 3: Performance with respect to paragraph length and sentence position - NIPS abstracts test data.\nappearing in the first two sentences. Similarly in the third example, the words \u2018salient\u2019 and \u2018dates\u2019 have been attended to. In the last example, the words \u2018token\u2019, \u2018tokens\u2019, \u2018tokenization\u2019 have received attention. We believe that these observations link to ideas from centering theory which state that entity distributions in coherent discourses adhere to certain patterns. The model has implicitly learned learned these patterns with no syntax annotations or handcrafted features."}, {"heading": "B PERFORMANCE ANALYSIS", "text": "Figure 3a shows the average \u03c4 for the models on the NIPS abstracts test set for a given paragraph length. The performance of local approaches dies down fairly quickly as we can expect and face difficulties handling lengthy paragraphs. Our model attempts to maintain consistent performance with increasing paragraph size with a more gradual decline in performance.\nFigure 3b compares the average prediction accuracy for a given sentence position in the test set. It is interesting to observe that all models fair well in predicting the first sentence. The greedy decoding procedure also contributes to the decline in performance as we move right. Our model remains more robust compared to the other two methods.\nAnother trend to be observed is that as the context size increases (2 for next sentence generation, 3 for window network, complete sentential history for our model) the performance decline is more gradual."}, {"heading": "C MODEL DETAILS", "text": "The LSTM update in equation 1 of the paper\nht, ct = LSTM(ht\u22121, ct\u22121) (11)\nis as follows.\nit = \u03c3(Wiht\u22121 + bi) (12) ft = \u03c3(Wfht\u22121 + bf ) (13) ot = \u03c3(Woht\u22121 + bo) (14) c\u0302t = tanh(Wcht\u22121 + bc) (15) ct = ft ct\u22121 + it c\u0302t (16) ht = ot tanh(ct) (17)\nwhere W{i,f,o,c}, b{i,f,o,c} are learnable parameters.\nThe LSTM update in equation 6\nht, ct = LSTM(ht\u22121, ct\u22121, xt\u22121) (18)\nis given by the following.\nit = \u03c3(Whiht\u22121 +Wxixt\u22121 + bi) (19) ft = \u03c3(Whfht\u22121 +Wxfxt\u22121 + bf ) (20) ot = \u03c3(Whoht\u22121 +Wxoxt\u22121 + bo) (21) c\u0302t = tanh(Whcht\u22121 +Wxcxt\u22121 + bc) (22) ct = ft ct\u22121 + it c\u0302t (23) ht = ot tanh(ct) (24)\nwhere W{hi,hf,ho,hc},W{xi,xf,xo,xc}, b{i,f,o,c} are learnable parameters."}], "references": [{"title": "Inferring strategies for sentence ordering in multidocument news summarization", "author": ["R. Barzilay", "N. Elhadad"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Barzilay and Elhadad.,? \\Q2002\\E", "shortCiteRegEx": "Barzilay and Elhadad.", "year": 2002}, {"title": "Modeling local coherence: An entity-based approach", "author": ["R. Barzilay", "M. Lapata"], "venue": "Computational Linguistics,", "citeRegEx": "Barzilay and Lapata.,? \\Q2008\\E", "shortCiteRegEx": "Barzilay and Lapata.", "year": 2008}, {"title": "Catching the drift: Probabilistic content models, with applications to generation and summarization", "author": ["R. Barzilay", "L. Lee"], "venue": "arXiv preprint cs/0405039,", "citeRegEx": "Barzilay and Lee.,? \\Q2004\\E", "shortCiteRegEx": "Barzilay and Lee.", "year": 2004}, {"title": "Using entity-based features to model coherence in student essays", "author": ["J. Burstein", "J. Tetreault", "S. Andreyev"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Burstein et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Burstein et al\\.", "year": 2010}, {"title": "Neural sentence ordering", "author": ["X. Chen", "X. Qiu", "X. Huang"], "venue": "arXiv preprint arXiv:1607.06952,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "A unified local and global model for discourse coherence", "author": ["M. Elsner", "J.L. Austerweil", "E. Charniak"], "venue": "In HLT-NAACL,", "citeRegEx": "Elsner et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Elsner et al\\.", "year": 2007}, {"title": "The measurement of textual coherence with latent semantic analysis", "author": ["P.W. Foltz", "W. Kintsch", "T.K. Landauer"], "venue": "Discourse processes,", "citeRegEx": "Foltz et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Foltz et al\\.", "year": 1998}, {"title": "Centering: A framework for modeling the local coherence of discourse", "author": ["B.J. Grosz", "S. Weinstein", "A.K. Joshi"], "venue": "Computational linguistics,", "citeRegEx": "Grosz et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Grosz et al\\.", "year": 1995}, {"title": "Discriminative improvements to distributional sentence similarity", "author": ["Y. Ji", "J. Eisenstein"], "venue": "In EMNLP,", "citeRegEx": "Ji and Eisenstein.,? \\Q2013\\E", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R.R. Salakhutdinov", "R. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Accurate unlexicalized parsing", "author": ["D. Klein", "C.D. Manning"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume", "citeRegEx": "Klein and Manning.,? \\Q2003\\E", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Probabilistic text structuring: Experiments with sentence ordering", "author": ["M. Lapata"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume", "citeRegEx": "Lapata.,? \\Q2003\\E", "shortCiteRegEx": "Lapata.", "year": 2003}, {"title": "Automatic evaluation of information ordering: Kendall\u2019s tau", "author": ["M. Lapata"], "venue": "Computational Linguistics,", "citeRegEx": "Lapata.,? \\Q2006\\E", "shortCiteRegEx": "Lapata.", "year": 2006}, {"title": "A model of coherence based on distributed sentence representation", "author": ["J. Li", "E.H. Hovy"], "venue": "In EMNLP, pages 2039\u20132048,", "citeRegEx": "Li and Hovy.,? \\Q2014\\E", "shortCiteRegEx": "Li and Hovy.", "year": 2014}, {"title": "Neural net models for open-domain discourse coherence", "author": ["J. Li", "D. Jurafsky"], "venue": "arXiv preprint arXiv:1606.01545,", "citeRegEx": "Li and Jurafsky.,? \\Q2016\\E", "shortCiteRegEx": "Li and Jurafsky.", "year": 2016}, {"title": "Visualizing and understanding neural models in nlp", "author": ["J. Li", "X. Chen", "E. Hovy", "D. Jurafsky"], "venue": "arXiv preprint arXiv:1506.01066,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["J. Li", "M.-T. Luong", "D. Jurafsky"], "venue": "arXiv preprint arXiv:1506.01057,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["R. Lin", "S. Liu", "M. Yang", "M. Li", "M. Zhou", "S. Li"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "A coherence model based on syntactic patterns", "author": ["A. Louis", "A. Nenkova"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Louis and Nenkova.,? \\Q2012\\E", "shortCiteRegEx": "Louis and Nenkova.", "year": 2012}, {"title": "Re-examining machine translation metrics for paraphrase identification", "author": ["N. Madnani", "J. Tetreault", "M. Chodorow"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Madnani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Madnani et al\\.", "year": 2012}, {"title": "Evaluation of text coherence for electronic essay scoring systems", "author": ["E. Miltsakaki", "K. Kukich"], "venue": "Natural Language Engineering,", "citeRegEx": "Miltsakaki and Kukich.,? \\Q2004\\E", "shortCiteRegEx": "Miltsakaki and Kukich.", "year": 2004}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A Bibliometric and Network Analysis of the field of Computational Linguistics", "author": ["D.R. Radev", "M.T. Joseph", "B. Gibson", "P. Muthukrishnan"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "Radev et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Radev et al\\.", "year": 2009}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["R. Socher", "E.H. Huang", "J. Pennin", "C.D. Manning", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Discourse generation using utility-trained coherence models", "author": ["R. Soricut", "D. Marcu"], "venue": "In Proceedings of the COLING/ACL on Main conference poster sessions,", "citeRegEx": "Soricut and Marcu.,? \\Q2006\\E", "shortCiteRegEx": "Soricut and Marcu.", "year": 2006}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["K.S. Tai", "R. Socher", "C.D. Manning"], "venue": "arXiv preprint arXiv:1503.00075,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Order matters: Sequence to sequence for sets", "author": ["O. Vinyals", "S. Bengio", "M. Kudlur"], "venue": "arXiv preprint arXiv:1511.06391,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Drawing inspiration from this work, we use gradients of prediction decisions", "author": ["Li"], "venue": null, "citeRegEx": "Li,? \\Q2015\\E", "shortCiteRegEx": "Li", "year": 2015}, {"title": "We assume h to be fixed and only backpropagate gradients through the sentence encoder. Table 5 shows visualizations of a few selected abstracts. Words expressed in darker shades correspond to higher gradient norms. In the first example the model seems to be using the word clues \u2018first\u2019, \u2018second", "author": ["\u2202wi"], "venue": null, "citeRegEx": "..,? \\Q2016\\E", "shortCiteRegEx": "..", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Barzilay and Elhadad (2002) discuss the importance of an explicit ordering component in MDS systems.", "startOffset": 0, "endOffset": 28}, {"referenceID": 21, "context": "Automated essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2010) is an application that can benefit from such a coherence model.", "startOffset": 24, "endOffset": 76}, {"referenceID": 3, "context": "Automated essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2010) is an application that can benefit from such a coherence model.", "startOffset": 24, "endOffset": 76}, {"referenceID": 26, "context": "The Sequence to sequence mapping framework (Sutskever et al., 2014), as well as several of its variants have fuelled RNN based approaches to a wide variety of problems including language modeling, language generation, machine translation, question answering and many others.", "startOffset": 43, "endOffset": 67}, {"referenceID": 26, "context": "The Sequence to sequence mapping framework (Sutskever et al., 2014), as well as several of its variants have fuelled RNN based approaches to a wide variety of problems including language modeling, language generation, machine translation, question answering and many others. Vinyals et al. (2015a) recently showed that the order in which tokens of the input sequence are fed to seq2seq models has a significant impact on the performance of the model.", "startOffset": 44, "endOffset": 298}, {"referenceID": 6, "context": "Foltz et al. (1998) represent words using vectors of co-occurent counts and sentences as a mean of these word vectors.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "Foltz et al. (1998) represent words using vectors of co-occurent counts and sentences as a mean of these word vectors. Sentence similarity is defined as the cosine distance between sentence vectors and text coherence is modeled as a normalized sum of similarity scores of adjacent sentences. Lapata (2003) represents sentences by vectors of", "startOffset": 0, "endOffset": 306}, {"referenceID": 7, "context": "These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns.", "startOffset": 68, "endOffset": 88}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse.", "startOffset": 54, "endOffset": 81}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM.", "startOffset": 54, "endOffset": 691}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM.", "startOffset": 54, "endOffset": 1030}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text.", "startOffset": 54, "endOffset": 1130}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text. Louis and Nenkova (2012) attempt to capture the intentional structure in documents using syntax as a proxy for the communicative goal of a sentence.", "startOffset": 54, "endOffset": 1264}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text. Louis and Nenkova (2012) attempt to capture the intentional structure in documents using syntax as a proxy for the communicative goal of a sentence. Syntax features such as parse tree production rules and constituency tags at a particular tree depth were used. Unlike previous approaches, we do not employ any handcrafted features and adopt an embedding based approach. Local coherence is taken into account by having a next sentence prediction component in the model and global dependencies are naturally captured by an RNN. We demonstrate that our model is able to capture both logical and topical structure by evaluating its performance on different types of data. Data-driven approaches Neural approaches have gained attention more recently. Li and Hovy (2014) model sentences as embeddings derived from recurrent/recursive neural nets and train a feedforward neural network that takes an input window of sentence embeddings and outputs a probability which represents the coherence of the sentence window.", "startOffset": 54, "endOffset": 2004}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text. Louis and Nenkova (2012) attempt to capture the intentional structure in documents using syntax as a proxy for the communicative goal of a sentence. Syntax features such as parse tree production rules and constituency tags at a particular tree depth were used. Unlike previous approaches, we do not employ any handcrafted features and adopt an embedding based approach. Local coherence is taken into account by having a next sentence prediction component in the model and global dependencies are naturally captured by an RNN. We demonstrate that our model is able to capture both logical and topical structure by evaluating its performance on different types of data. Data-driven approaches Neural approaches have gained attention more recently. Li and Hovy (2014) model sentences as embeddings derived from recurrent/recursive neural nets and train a feedforward neural network that takes an input window of sentence embeddings and outputs a probability which represents the coherence of the sentence window. Coherence evaluation is performed by sliding the window over the text and aggregating the score. Li and Jurafsky (2016) study the same model in a larger scale task and also consider a sequence to sequence approach where the model is trained to generate the next sentence given the current sentence and vice versa.", "startOffset": 54, "endOffset": 2369}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text. Louis and Nenkova (2012) attempt to capture the intentional structure in documents using syntax as a proxy for the communicative goal of a sentence. Syntax features such as parse tree production rules and constituency tags at a particular tree depth were used. Unlike previous approaches, we do not employ any handcrafted features and adopt an embedding based approach. Local coherence is taken into account by having a next sentence prediction component in the model and global dependencies are naturally captured by an RNN. We demonstrate that our model is able to capture both logical and topical structure by evaluating its performance on different types of data. Data-driven approaches Neural approaches have gained attention more recently. Li and Hovy (2014) model sentences as embeddings derived from recurrent/recursive neural nets and train a feedforward neural network that takes an input window of sentence embeddings and outputs a probability which represents the coherence of the sentence window. Coherence evaluation is performed by sliding the window over the text and aggregating the score. Li and Jurafsky (2016) study the same model in a larger scale task and also consider a sequence to sequence approach where the model is trained to generate the next sentence given the current sentence and vice versa. Chen et al. (2016) also propose a sentence embedding based approach where they model the probability that one sentence should come before another and define coherence based on the likelihood of the relative order of every pair of sentences.", "startOffset": 54, "endOffset": 2582}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text. Louis and Nenkova (2012) attempt to capture the intentional structure in documents using syntax as a proxy for the communicative goal of a sentence. Syntax features such as parse tree production rules and constituency tags at a particular tree depth were used. Unlike previous approaches, we do not employ any handcrafted features and adopt an embedding based approach. Local coherence is taken into account by having a next sentence prediction component in the model and global dependencies are naturally captured by an RNN. We demonstrate that our model is able to capture both logical and topical structure by evaluating its performance on different types of data. Data-driven approaches Neural approaches have gained attention more recently. Li and Hovy (2014) model sentences as embeddings derived from recurrent/recursive neural nets and train a feedforward neural network that takes an input window of sentence embeddings and outputs a probability which represents the coherence of the sentence window. Coherence evaluation is performed by sliding the window over the text and aggregating the score. Li and Jurafsky (2016) study the same model in a larger scale task and also consider a sequence to sequence approach where the model is trained to generate the next sentence given the current sentence and vice versa. Chen et al. (2016) also propose a sentence embedding based approach where they model the probability that one sentence should come before another and define coherence based on the likelihood of the relative order of every pair of sentences. We believe these models are limited by the fact that they are local in nature and our experiments show that exploiting larger contexts can be very beneficial. Hierarchical RNNs for document modeling Word level and sentence level RNNs have been used in a hierarchical fashion for modeling documents in prior work. Li et al. (2015b) proposed a hierarchical document autoencoder which has potential to be used in generation and summarization applications.", "startOffset": 54, "endOffset": 3135}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text. Louis and Nenkova (2012) attempt to capture the intentional structure in documents using syntax as a proxy for the communicative goal of a sentence. Syntax features such as parse tree production rules and constituency tags at a particular tree depth were used. Unlike previous approaches, we do not employ any handcrafted features and adopt an embedding based approach. Local coherence is taken into account by having a next sentence prediction component in the model and global dependencies are naturally captured by an RNN. We demonstrate that our model is able to capture both logical and topical structure by evaluating its performance on different types of data. Data-driven approaches Neural approaches have gained attention more recently. Li and Hovy (2014) model sentences as embeddings derived from recurrent/recursive neural nets and train a feedforward neural network that takes an input window of sentence embeddings and outputs a probability which represents the coherence of the sentence window. Coherence evaluation is performed by sliding the window over the text and aggregating the score. Li and Jurafsky (2016) study the same model in a larger scale task and also consider a sequence to sequence approach where the model is trained to generate the next sentence given the current sentence and vice versa. Chen et al. (2016) also propose a sentence embedding based approach where they model the probability that one sentence should come before another and define coherence based on the likelihood of the relative order of every pair of sentences. We believe these models are limited by the fact that they are local in nature and our experiments show that exploiting larger contexts can be very beneficial. Hierarchical RNNs for document modeling Word level and sentence level RNNs have been used in a hierarchical fashion for modeling documents in prior work. Li et al. (2015b) proposed a hierarchical document autoencoder which has potential to be used in generation and summarization applications. More relevant to our work is a similar model (but without an encoder) considered by Lin et al. (2015). A sentence level RNN predicts the bag of words in the next sentence given the previous sentences and a word level RNN predicts the word sequence conditioned on the sentence level RNN hidden state.", "startOffset": 54, "endOffset": 3359}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text. Louis and Nenkova (2012) attempt to capture the intentional structure in documents using syntax as a proxy for the communicative goal of a sentence. Syntax features such as parse tree production rules and constituency tags at a particular tree depth were used. Unlike previous approaches, we do not employ any handcrafted features and adopt an embedding based approach. Local coherence is taken into account by having a next sentence prediction component in the model and global dependencies are naturally captured by an RNN. We demonstrate that our model is able to capture both logical and topical structure by evaluating its performance on different types of data. Data-driven approaches Neural approaches have gained attention more recently. Li and Hovy (2014) model sentences as embeddings derived from recurrent/recursive neural nets and train a feedforward neural network that takes an input window of sentence embeddings and outputs a probability which represents the coherence of the sentence window. Coherence evaluation is performed by sliding the window over the text and aggregating the score. Li and Jurafsky (2016) study the same model in a larger scale task and also consider a sequence to sequence approach where the model is trained to generate the next sentence given the current sentence and vice versa. Chen et al. (2016) also propose a sentence embedding based approach where they model the probability that one sentence should come before another and define coherence based on the likelihood of the relative order of every pair of sentences. We believe these models are limited by the fact that they are local in nature and our experiments show that exploiting larger contexts can be very beneficial. Hierarchical RNNs for document modeling Word level and sentence level RNNs have been used in a hierarchical fashion for modeling documents in prior work. Li et al. (2015b) proposed a hierarchical document autoencoder which has potential to be used in generation and summarization applications. More relevant to our work is a similar model (but without an encoder) considered by Lin et al. (2015). A sentence level RNN predicts the bag of words in the next sentence given the previous sentences and a word level RNN predicts the word sequence conditioned on the sentence level RNN hidden state. The model has a structure similar to the content model of Barzilay and Lee (2004) with RNNs playing the roles of the HMM and the bigram language model.", "startOffset": 54, "endOffset": 3639}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text. Louis and Nenkova (2012) attempt to capture the intentional structure in documents using syntax as a proxy for the communicative goal of a sentence. Syntax features such as parse tree production rules and constituency tags at a particular tree depth were used. Unlike previous approaches, we do not employ any handcrafted features and adopt an embedding based approach. Local coherence is taken into account by having a next sentence prediction component in the model and global dependencies are naturally captured by an RNN. We demonstrate that our model is able to capture both logical and topical structure by evaluating its performance on different types of data. Data-driven approaches Neural approaches have gained attention more recently. Li and Hovy (2014) model sentences as embeddings derived from recurrent/recursive neural nets and train a feedforward neural network that takes an input window of sentence embeddings and outputs a probability which represents the coherence of the sentence window. Coherence evaluation is performed by sliding the window over the text and aggregating the score. Li and Jurafsky (2016) study the same model in a larger scale task and also consider a sequence to sequence approach where the model is trained to generate the next sentence given the current sentence and vice versa. Chen et al. (2016) also propose a sentence embedding based approach where they model the probability that one sentence should come before another and define coherence based on the likelihood of the relative order of every pair of sentences. We believe these models are limited by the fact that they are local in nature and our experiments show that exploiting larger contexts can be very beneficial. Hierarchical RNNs for document modeling Word level and sentence level RNNs have been used in a hierarchical fashion for modeling documents in prior work. Li et al. (2015b) proposed a hierarchical document autoencoder which has potential to be used in generation and summarization applications. More relevant to our work is a similar model (but without an encoder) considered by Lin et al. (2015). A sentence level RNN predicts the bag of words in the next sentence given the previous sentences and a word level RNN predicts the word sequence conditioned on the sentence level RNN hidden state. The model has a structure similar to the content model of Barzilay and Lee (2004) with RNNs playing the roles of the HMM and the bigram language model. Our model has a hierarchical nature in that a sentence level RNN operates over words of a sentence and a document level RNN operates over sentence embeddings. Combinatorial optimization with RNNs Vinyals et al. (2015a) equip sequence to sequence models with the capability to handle input and output sets, and discuss experiments on sorting, language modeling and parsing.", "startOffset": 54, "endOffset": 3928}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text. Louis and Nenkova (2012) attempt to capture the intentional structure in documents using syntax as a proxy for the communicative goal of a sentence. Syntax features such as parse tree production rules and constituency tags at a particular tree depth were used. Unlike previous approaches, we do not employ any handcrafted features and adopt an embedding based approach. Local coherence is taken into account by having a next sentence prediction component in the model and global dependencies are naturally captured by an RNN. We demonstrate that our model is able to capture both logical and topical structure by evaluating its performance on different types of data. Data-driven approaches Neural approaches have gained attention more recently. Li and Hovy (2014) model sentences as embeddings derived from recurrent/recursive neural nets and train a feedforward neural network that takes an input window of sentence embeddings and outputs a probability which represents the coherence of the sentence window. Coherence evaluation is performed by sliding the window over the text and aggregating the score. Li and Jurafsky (2016) study the same model in a larger scale task and also consider a sequence to sequence approach where the model is trained to generate the next sentence given the current sentence and vice versa. Chen et al. (2016) also propose a sentence embedding based approach where they model the probability that one sentence should come before another and define coherence based on the likelihood of the relative order of every pair of sentences. We believe these models are limited by the fact that they are local in nature and our experiments show that exploiting larger contexts can be very beneficial. Hierarchical RNNs for document modeling Word level and sentence level RNNs have been used in a hierarchical fashion for modeling documents in prior work. Li et al. (2015b) proposed a hierarchical document autoencoder which has potential to be used in generation and summarization applications. More relevant to our work is a similar model (but without an encoder) considered by Lin et al. (2015). A sentence level RNN predicts the bag of words in the next sentence given the previous sentences and a word level RNN predicts the word sequence conditioned on the sentence level RNN hidden state. The model has a structure similar to the content model of Barzilay and Lee (2004) with RNNs playing the roles of the HMM and the bigram language model. Our model has a hierarchical nature in that a sentence level RNN operates over words of a sentence and a document level RNN operates over sentence embeddings. Combinatorial optimization with RNNs Vinyals et al. (2015a) equip sequence to sequence models with the capability to handle input and output sets, and discuss experiments on sorting, language modeling and parsing. Their goal is to show that input and output orderings can matter in these tasks, which is demonstrated using several small scale experiments. Our work exploits this framework to address the challenging problem of modeling logical and hierarchical structure in text. Vinyals et al. (2015b) proposed pointer-networks, aimed at combinatorial optimization problems where the output dictionary size depends on the number of input elements.", "startOffset": 54, "endOffset": 4371}, {"referenceID": 28, "context": "Our model is based on the read, process and write framework proposed by Vinyals et al. (2015a) briefly discussed in section 1.", "startOffset": 72, "endOffset": 95}, {"referenceID": 28, "context": "Our model is based on the read, process and write framework proposed by Vinyals et al. (2015a) briefly discussed in section 1. We use the encoder-decoder terminology that is more common in the literature in the following discussion. The model is comprised of a sentence encoder RNN, an encoder RNN and a decoder RNN (figure 1). An RNN sentence encoder takes as input the words of a sentence s sequentially and computes an embedding representation of the sentence (Figure 1a). Henceforth, we shall use s to refer to a sentence or its embedding interchangeably. The embeddings {s1, s2, ..., sn} of a given set of n sentences constitute the sentence memory, available to be accessed by subsequent components. The encoder is identical to the originally proposed process block and is defined by equations 1-5 (See Figure 1b). Following the regular LSTM hidden state (ht\u22121 enc , c t\u22121 enc ) update, the hidden state is concatenated with an attention readout vector satt, and this concatenated vector is treated as the hidden state for the next time step (Equation 5). Attention probabilities are computed by composing the hidden state with embeddings of the candidate sentences through a scoring function f and taking the softmax (Equations 2, 3). This process is iterated for a number of times, called the number of read cycles. As described in Vinyals et al. (2015a) the encoder has the desirable property of being invariant to the order in which the sentence embeddings reside in the memory.", "startOffset": 72, "endOffset": 1363}, {"referenceID": 14, "context": "Note that the structure of this scoring function is similar to the window network in Li and Hovy (2014). While they used a local window of sentences to capture context, this scoring function exploits the RNN hidden state to score sentence candidates.", "startOffset": 85, "endOffset": 104}, {"referenceID": 22, "context": "We use pre-trained 300 dimensional GloVe word embeddings (Pennington et al., 2014).", "startOffset": 57, "endOffset": 82}, {"referenceID": 9, "context": "We used a batch size of 10 and the Adam optimizer (Kingma and Ba, 2014) with a base learning rate of 5e-4 for all experiments.", "startOffset": 50, "endOffset": 71}, {"referenceID": 1, "context": "Given a reference paragraph and a permuted version of it, the more coherently organized one needs to be identified (Barzilay and Lapata, 2008).", "startOffset": 115, "endOffset": 142}, {"referenceID": 1, "context": "We consider data from two different domains that have been widely used for this task in previous work since Barzilay and Lee (2004); Barzilay and Lapata (2008).", "startOffset": 108, "endOffset": 132}, {"referenceID": 1, "context": "We consider data from two different domains that have been widely used for this task in previous work since Barzilay and Lee (2004); Barzilay and Lapata (2008). The ACCIDENTS data (aka AIRPLANE data) is a set of aviation accident reports from the National Transportation Safety Board\u2019s database.", "startOffset": 133, "endOffset": 160}, {"referenceID": 14, "context": "The entity grid model provides a strong baseline on the ACCIDENTS dataset, only outperformed by our model and Li and Jurafsky (2016) .", "startOffset": 110, "endOffset": 133}, {"referenceID": 14, "context": "On the EARTHQUAKE data the window approach of Li and Hovy (2014) and Li and Jurafsky (2016) perform strongly.", "startOffset": 46, "endOffset": 65}, {"referenceID": 14, "context": "On the EARTHQUAKE data the window approach of Li and Hovy (2014) and Li and Jurafsky (2016) perform strongly.", "startOffset": 46, "endOffset": 92}, {"referenceID": 1, "context": "Methods ACCIDENTS EARTHQUAKES Barzilay and Lapata (2008) 0.", "startOffset": 30, "endOffset": 57}, {"referenceID": 1, "context": "Methods ACCIDENTS EARTHQUAKES Barzilay and Lapata (2008) 0.904 0.872 Louis and Nenkova (2012) 0.", "startOffset": 30, "endOffset": 94}, {"referenceID": 1, "context": "Methods ACCIDENTS EARTHQUAKES Barzilay and Lapata (2008) 0.904 0.872 Louis and Nenkova (2012) 0.842 0.957 Guinaudeau and Strube (2013) 0.", "startOffset": 30, "endOffset": 135}, {"referenceID": 1, "context": "Methods ACCIDENTS EARTHQUAKES Barzilay and Lapata (2008) 0.904 0.872 Louis and Nenkova (2012) 0.842 0.957 Guinaudeau and Strube (2013) 0.846 0.635 Li and Hovy (2014) - Recurrent 0.", "startOffset": 30, "endOffset": 166}, {"referenceID": 1, "context": "Methods ACCIDENTS EARTHQUAKES Barzilay and Lapata (2008) 0.904 0.872 Louis and Nenkova (2012) 0.842 0.957 Guinaudeau and Strube (2013) 0.846 0.635 Li and Hovy (2014) - Recurrent 0.840 0.951 Li and Hovy (2014) - Recursive 0.", "startOffset": 30, "endOffset": 209}, {"referenceID": 1, "context": "Methods ACCIDENTS EARTHQUAKES Barzilay and Lapata (2008) 0.904 0.872 Louis and Nenkova (2012) 0.842 0.957 Guinaudeau and Strube (2013) 0.846 0.635 Li and Hovy (2014) - Recurrent 0.840 0.951 Li and Hovy (2014) - Recursive 0.864 0.976 Li and Jurafsky (2016) 0.", "startOffset": 30, "endOffset": 256}, {"referenceID": 23, "context": "A second source of abstracts we consider are papers from the ACL Anthology Network (AAN) corpus (Radev et al., 2009) of ACL papers.", "startOffset": 96, "endOffset": 116}, {"referenceID": 12, "context": "Lapata (2006) discusses that this metric reliably correlates with human judgements.", "startOffset": 0, "endOffset": 14}, {"referenceID": 1, "context": "46 0 Entity Grid (Barzilay and Lapata, 2008) 20.", "startOffset": 17, "endOffset": 44}, {"referenceID": 15, "context": "10 - Seq2seq (Uni) (Li and Jurafsky, 2016) 27.", "startOffset": 19, "endOffset": 42}, {"referenceID": 14, "context": "10 Window network (Li and Hovy, 2014) 41.", "startOffset": 18, "endOffset": 37}, {"referenceID": 1, "context": "Our first baseline is the Entity Grid model of Barzilay and Lapata (2008). We use the Stanford parser Klein and Manning (2003) to get constituency trees for all sentences in our datasets.", "startOffset": 47, "endOffset": 74}, {"referenceID": 1, "context": "Our first baseline is the Entity Grid model of Barzilay and Lapata (2008). We use the Stanford parser Klein and Manning (2003) to get constituency trees for all sentences in our datasets.", "startOffset": 47, "endOffset": 127}, {"referenceID": 1, "context": "Our first baseline is the Entity Grid model of Barzilay and Lapata (2008). We use the Stanford parser Klein and Manning (2003) to get constituency trees for all sentences in our datasets. We derive entity grid representations for the parsed sentences using the Brown Coherence Toolkit.3 A ranking SVM is trained to score correct orderings higher than incorrect orderings as in the original work. We used 20 permutations per document as training data. Since the entity grid representation only provides a means of feature extraction we evaluate the model in the ordering setting as follows. We choose 1000 random permutations for each document, one of them being the correct order, and pick the order with maximum coherence. We experimented with transitions of length at most 3 in the entity-grid. Sequence to sequence. The second baseline we consider is a sequence to sequence model which is trained to predict the next sentence given the current sentence. Li and Jurafsky (2016) consider similar methods and our model is same as the uni-directional model in their work.", "startOffset": 47, "endOffset": 980}, {"referenceID": 1, "context": "Our first baseline is the Entity Grid model of Barzilay and Lapata (2008). We use the Stanford parser Klein and Manning (2003) to get constituency trees for all sentences in our datasets. We derive entity grid representations for the parsed sentences using the Brown Coherence Toolkit.3 A ranking SVM is trained to score correct orderings higher than incorrect orderings as in the original work. We used 20 permutations per document as training data. Since the entity grid representation only provides a means of feature extraction we evaluate the model in the ordering setting as follows. We choose 1000 random permutations for each document, one of them being the correct order, and pick the order with maximum coherence. We experimented with transitions of length at most 3 in the entity-grid. Sequence to sequence. The second baseline we consider is a sequence to sequence model which is trained to predict the next sentence given the current sentence. Li and Jurafsky (2016) consider similar methods and our model is same as the uni-directional model in their work. These methods were shown to yield sentence embeddings that have competitive performance in several semantic tasks in Kiros et al. (2015). Window Network.", "startOffset": 47, "endOffset": 1208}, {"referenceID": 1, "context": "Our first baseline is the Entity Grid model of Barzilay and Lapata (2008). We use the Stanford parser Klein and Manning (2003) to get constituency trees for all sentences in our datasets. We derive entity grid representations for the parsed sentences using the Brown Coherence Toolkit.3 A ranking SVM is trained to score correct orderings higher than incorrect orderings as in the original work. We used 20 permutations per document as training data. Since the entity grid representation only provides a means of feature extraction we evaluate the model in the ordering setting as follows. We choose 1000 random permutations for each document, one of them being the correct order, and pick the order with maximum coherence. We experimented with transitions of length at most 3 in the entity-grid. Sequence to sequence. The second baseline we consider is a sequence to sequence model which is trained to predict the next sentence given the current sentence. Li and Jurafsky (2016) consider similar methods and our model is same as the uni-directional model in their work. These methods were shown to yield sentence embeddings that have competitive performance in several semantic tasks in Kiros et al. (2015). Window Network. We consider the window approach of Li and Hovy (2014) and Li and Jurafsky (2016) which demonstrated strong performance in the order discrimination task as our third baseline.", "startOffset": 47, "endOffset": 1279}, {"referenceID": 1, "context": "Our first baseline is the Entity Grid model of Barzilay and Lapata (2008). We use the Stanford parser Klein and Manning (2003) to get constituency trees for all sentences in our datasets. We derive entity grid representations for the parsed sentences using the Brown Coherence Toolkit.3 A ranking SVM is trained to score correct orderings higher than incorrect orderings as in the original work. We used 20 permutations per document as training data. Since the entity grid representation only provides a means of feature extraction we evaluate the model in the ordering setting as follows. We choose 1000 random permutations for each document, one of them being the correct order, and pick the order with maximum coherence. We experimented with transitions of length at most 3 in the entity-grid. Sequence to sequence. The second baseline we consider is a sequence to sequence model which is trained to predict the next sentence given the current sentence. Li and Jurafsky (2016) consider similar methods and our model is same as the uni-directional model in their work. These methods were shown to yield sentence embeddings that have competitive performance in several semantic tasks in Kiros et al. (2015). Window Network. We consider the window approach of Li and Hovy (2014) and Li and Jurafsky (2016) which demonstrated strong performance in the order discrimination task as our third baseline.", "startOffset": 47, "endOffset": 1306}, {"referenceID": 10, "context": "However, when the sentence encoder is initialized with a good set of parameters (such as pre-trained skip-thought parameters (Kiros et al., 2015)) this compensates for the lack of an encoder and leads to better performance.", "startOffset": 125, "endOffset": 145}, {"referenceID": 13, "context": "Interestingly, Li and Jurafsky (2016) observe that the seq2seq model outperforms the window network in an order discrimination task on wikipedia data.", "startOffset": 15, "endOffset": 38}, {"referenceID": 2, "context": "While approaches based on the content model of Barzilay and Lee (2004) attempt to explicitly capture topics by discovering clusters in sentences, we observe that the neural approach implicitly discovers such structure.", "startOffset": 47, "endOffset": 71}, {"referenceID": 10, "context": "We chose the BookCorpus dataset (Kiros et al., 2015) for this purpose.", "startOffset": 32, "endOffset": 52}, {"referenceID": 10, "context": "We chose the BookCorpus dataset (Kiros et al., 2015) for this purpose. We trained the model with two key changes from the models trained on the abstracts data - 1) In addition to the sentences in the paragraph being considered, we added more contrastive sentences from other paragraphs as well. 2) We use the bilinear scoring function. These techniques helped obtain better representations when training on large amounts of data. To evaluate the quality of the sentence embeddings derived from the model, we use the evaluation pipeline of Kiros et al. (2015) for tasks that involve understanding sentence semantics.", "startOffset": 33, "endOffset": 559}, {"referenceID": 27, "context": "Method r \u03c1 MSE Purely supervised methods DT-RNN (Tai et al., 2015) 0.", "startOffset": 48, "endOffset": 66}, {"referenceID": 27, "context": "382 LSTM (Tai et al., 2015) 0.", "startOffset": 9, "endOffset": 27}, {"referenceID": 27, "context": "283 DT-LSTM (Tai et al., 2015) 0.", "startOffset": 12, "endOffset": 30}, {"referenceID": 10, "context": "253 Classifier trained on sentence embeddings skip-bow (Kiros et al., 2015) 0.", "startOffset": 55, "endOffset": 75}, {"referenceID": 10, "context": "398 uni-skip (Kiros et al., 2015) 0.", "startOffset": 13, "endOffset": 33}, {"referenceID": 10, "context": "0 Classifier trained on sentence embeddings uni-skip-bow (Kiros et al., 2015) 67.", "startOffset": 57, "endOffset": 77}, {"referenceID": 10, "context": "3 uni-skip (Kiros et al., 2015) 73.", "startOffset": 11, "endOffset": 31}, {"referenceID": 21, "context": "Method Acc F1 Purely supervised methods Socher et al. (2011) 76.", "startOffset": 40, "endOffset": 61}, {"referenceID": 18, "context": "6 Madnani et al. (2012) 77.", "startOffset": 2, "endOffset": 24}, {"referenceID": 8, "context": "1 Ji and Eisenstein (2013) 80.", "startOffset": 2, "endOffset": 27}], "year": 2016, "abstractText": "Modeling the structure of coherent texts is a task of great importance in NLP. The task of organizing a given set of sentences into a coherent order has been commonly used to build and evaluate models that understand such structure. In this work we propose an end-to-end neural approach based on the recently proposed set to sequence mapping framework to address the sentence ordering problem. Our model achieves state-of-the-art performance in the order discrimination task on two datasets widely used in the literature. We also consider a new interesting task of ordering abstracts from conference papers and research proposals and demonstrate strong performance against recent methods. Visualizing the sentence representations learned by the model shows that the model has captured high level logical structure in these paragraphs. The model also learns rich semantic sentence representations by learning to order texts, performing comparably to recent unsupervised representation learning methods in the sentence similarity and paraphrase detection tasks.", "creator": "LaTeX with hyperref package"}}}