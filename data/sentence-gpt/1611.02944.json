{"id": "1611.02944", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2016", "title": "Increasing the throughput of machine translation systems using clouds", "abstract": "The manuscript presents an experiment at implementation of a Machine Translation system in a MapReduce model. The empirical evaluation was done using fully implemented translation systems embedded into the MapReduce programming model. Two machine translation paradigms were studied: shallow transfer Rule Based Machine Translation and Statistical Machine Translation of the Model for the Mapping Project.\n\n\n\n\n\n\n\n\nThe Mapping Project is intended to generate a detailed and systematic understanding of Mapping Project in a single language for the Mapping Project. The Mapping Project is intended to generate a detailed and systematic understanding of Mapping Project in a single language for the Mapping Project. The Mapping Project is intended to generate a detailed and systematic understanding of Mapping Project in a single language for the Mapping Project. The Mapping Project is intended to generate a detailed and systematic understanding of Mapping Project in a single language for the Mapping Project. The Mapping Project is intended to generate a detailed and systematic understanding of Mapping Project in a single language for the Mapping Project. The Mapping Project is intended to generate a detailed and systematic understanding of Mapping Project in a single language for the Mapping Project. The Mapping Project is intended to generate a detailed and systematic understanding of Mapping Project in a single language for the Mapping Project. The Mapping Project is intended to generate a detailed and systematic understanding of Mapping Project in a single language for the Mapping Project. The Mapping Project is intended to generate a detailed and systematic understanding of Mapping Project in a single language for the Mapping Project.\n\nThe Mapping Project is intended to generate a detailed and systematic understanding of Mapping Project in a single language for the Mapping Project. The Mapping Project is intended to generate a detailed and systematic understanding of Mapping Project in a single language for the Mapping Project. The Mapping Project is intended to generate a detailed and systematic understanding of Mapping Project in a single language for the Mapping Project. The Mapping Project is intended to generate a detailed and systematic understanding of Mapping Project in a single language for the Mapping Project. The Mapping Project is intended to generate a detailed and systematic understanding of Mapping Project in a single language for the Mapping Project. The Mapping Project is intended to generate a detailed and systematic understanding of Mapping Project in a single language for the Mapping Project. The Mapping Project is intended to generate a detailed and systematic understanding of Mapping Project in a single language for the Mapping Project. The Mapping", "histories": [["v1", "Wed, 9 Nov 2016 14:27:03 GMT  (1402kb)", "http://arxiv.org/abs/1611.02944v1", "20 pages, 7 figures"]], "COMMENTS": "20 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.CL cs.DC", "authors": ["jernej vi\\v{c}i\\v{c}", "rej brodnik"], "accepted": false, "id": "1611.02944"}, "pdf": {"name": "1611.02944.pdf", "metadata": {"source": "CRF", "title": "Increasing the throughput of machine translation systems using clouds", "authors": ["Jernej Vi\u010di\u010d", "Andrej Brodnik"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n02 94\n4v 1\n[ cs\n.C L\n] 9\nN ov\n2 01\n6\nIncreasing the throughput of machine translation systems using\nclouds\nJernej Vic\u030cic\u030c1, 2 and Andrej Brodnik1, 3\n1University of Primorska, Andrej Marusic Institute, Slovenia\n2Research Centre of the Slovenian Academy of Sciences and Arts, The Fran Ramovs\u030c Institute\n3University of Ljubljana, Faculty of Computer Science and Informatics, Slovenia\nThe manuscript presents an experiment at implementation of a Machine Trans-\nlation system in a MapReduce model. The empirical evaluation was done using\nfully implemented translation systems embedded into the MapReduce programming\nmodel. Two machine translation paradigms were studied: shallow transfer Rule\nBased Machine Translation and Statistical Machine Translation.\nThe results show that the MapReduce model can be successfully used to increase\nthe throughput of a machine translation system. Furthermore this method enhances\nthe throughput of a machine translation system without decreasing the quality of the\ntranslation output.\nThus, the present manuscript also represents a contribution to the seminal work in\nnatural language processing, specifically Machine Translation. It first points toward\nthe importance of the definition of the metric of throughput of translation system\nand, second, the applicability of the machine translation task to the MapReduce\nparadigm."}, {"heading": "1. INTRODUCTION", "text": "Most research in the area of machine translation evaluation focuses on the translation quality of the observed translation systems. The research presented in this manuscript focuses entirely on the throughput of translation system and proposes a method to increase the throughput with no effect on the quality of the translation.\nThere are quite a few cases where the machine translation throughput is a crucial aspect such as translation of large quantities of text, e. g. translating all the texts in the Project\n2 Gutenberg1 or translating huge amounts of manuals in order to enter a new market, etc. Some of these cases can be solved using publicly available services such as Google translate2 or Microsoft Bing Translator3 although the speed of translation (actually the amount of text that can be translated) is limited. However, there are cases where such approach is not viable, such as translating sensitive information ranging from local correspondence to proprietary literature or translating domain-specific texts where a proprietary translation system must be used. The obvious solution is using faster machines, but this solution requires new investment. Using public clouds like Amazon EC3 would reduce the investment costs, but for many applications the cost would still be too high. This approach would also involve an architecture change [1]. Autodesk Brasil ventured in a one-time job of translating most of their manuals into Brazilian Portuguese, the job was done using the Apertium translation system as described in [2].\nAs said, the presented research focuses mainly on machine translation of large amounts of text on commodity machines (cost-effective). MapReduce is a programming model for processing big data sets in a distributed fashion. The basic question this research focuses on is how efficiently can a Machine Translation (MT) system be implemented in a MapReduce model? The translation task of large amounts of text can be divided into smaller units with no effect on the translation quality as all the machine translation systems base translations on independent translation units. Usually the translation of sentences is done independently although some research has been done on extending the boundaries for translation units over the sentence boundaries [3]. The natural way of increasing the translation throughput (the number of translated words in a defined amount of time) is to translate parts of the text on separate translation systems as every sentence is translated independently.\nThe rest of the manuscript is organized as follows: The domain description is presented in sections 2 through 6. The methodology is presented in section 7. The evaluation methodology with results is presented in section 8. The manuscript concludes with the discussion and description of further work in section 9.\n1 Project Gutenberg Literary Archive Foundation: http://www.gutenberg.org/ 2 Google translate: http://translate.google.com/ 3 Microsoft Bing Translator: http://www.bing.com/translator\n3"}, {"heading": "2. MACHINE TRANSLATION", "text": "Machine translation as studied in this manuscript is an unsupervised process of translating\nfrom one natural language to another using computer programs.4\nThere has been almost no research on the topic of machine translation throughput mostly due to the fact that most of the research in the field of machine translation focuses on the machine translation quality and the throughput of the systems is at least a magnitude greater to the throughput of human translators. Some comparative research has been done examining the increase on the overall speed of translation process using machine translation tools compared to standard human translation process [4] and also the effect of the using Computer Assisted Translation \u2013 CAT tools that combine MT systems with translation memory and human post-editing process [5]."}, {"heading": "3. MACHINE TRANSLATION THROUGHPUT", "text": "Definition 1 Translation throughput: T = n t , where n \u2261 number of words in a text;t \u2261 setup time + translation time; Description: T is the measured property of the translation system; n is the number of units of the original text, in our case words; t is the sum of the time needed to initialize the translation system and the amount of time the n words were translated.\nDefinition 2 Increased speed of the translation throughput: S = Tnew Torig ; is the ratio between the new translation throughput and the original (reference) translation throughput.\nThe evaluation requires a \"big enough\" testing sample that minimizes the startup effect to a desired minimum. We can rely on this simple metric because both translation paradigms base the translations on fixed-size chunks of text and both paradigms almost always ignore the global syntactic complexity of the sentences. This metric would not be fair if the experiment involved a system that parses the sentence.\nThe translation throughput as defined in Definition 1 is the primary metric used in this manuscript for the performance evaluation (throughput) of the evaluated translation systems.\n4 European Association for Machine Translation: http://eamt.org/\n4"}, {"heading": "4. OVERVIEW OF MACHINE TRANSLATION SYSTEMS", "text": "The following sections describe the translation system toolkits used in the experiment, both toolkits are often used opensource toolkits from the respective translation paradigms:\n\u2022 Shallow-transfer Rule Based Machine Translation (Shallow Transfer RBMT) [6]\nparadigm that is most suited for translation of related languages [7], represented by Apertium [8];\n\u2022 Statistical Machine Translation (SMT) [9, 10] paradigm that is based on large quanti-\nties of data and mathematical models, represented by Moses [11];"}, {"heading": "4.1. Apertium", "text": "Apertium [8] is an open-source machine translation platform, initially aimed at relatedlanguage pairs, but recently expanded to deal with more divergent language pairs (such as English \u2013 Spanish). The shallow-transfer paradigm of the toolkit is best suited for related languages as the architecture does not provide the means for deep parsing which can lead to problems especially for the more divergent language pairs. All these properties make Apertium a perfect choice for a cost effective development of a machine translation system for similar languages. The basic architecture of Apertium system is presented in Figure 1. The systems [8] and [12] follow this design.\nThe numbered rectangles describe translation modules, output of a preceding module is\nthe input to the successor:\n5 1. Morphological analyzer searches monolingual morphological dictionary of the source\nlanguage to find all possible morphological tags and lemmata for the input word.\n2. POS tagger disambiguates the output of the preceding module by selecting the most\nprobable tags.\n3. Structural and lexical transfer translates the disambiguated, morphologically analyzed\ntext into the target language lexical units.\n4. Morphological generator searches the target dictionary for the appropriate word forms\nfor the translated lexical units.\n5. Post-generator completes the automatic post-editing chores.\nApertium is licensed under the LGPL.5"}, {"heading": "4.2. Moses", "text": "Moses [11] is recently the most widely used framework for setting up systems for statistical\nmachine translation. The main features of the framework are:\n\u2022 Two types of conductive models (source models) based on phrases of the actual parts\nof the text (phrase-based), and based on trees (tree-based);\n\u2022 To a certain extent permits the integration of explicit language knowledge at the level\nof words;\n\u2022 Provides support for integration of tools with ambiguous outputs, such as morphosyn-\ntactic analyzers and parsers of speech and\n\u2022 It is supported by large language models.\nMoses can be run in a server-like mode, where all the models are loaded into memory and the communication is done through XML:RPC. Figure 2 shows a basic server deployment variation. Moses is also licensed under the LGPL.5\n5 GNU Lesser General Public License (LGPL)\n6"}, {"heading": "5. RELATED WORK", "text": "There has been a considerable amount of research in speeding up the Machine Translation processes using parallel and distributed computing paradigms. Most of the work was done in the speeding up of the automatic learning processes in the area of the Statistical Machine Translation \u2013 SMT [13]. The most used Machine Translation toolkit, Moses [11], has already implemented support for multiple processor cores and to some extent for multiple computers.\nA MapReduce-based large scale MT architecture has been proposed [14] which focuses on distributed storage for streaming and structured data that could be employed, the proposed architecture mainly focuses on the SMT paradigm. Training phase for SMT based on MapReduce has been proposed by [13].\nThe translation phase received less attention from the research community although there were a few successful attempts such as [15] using GPUs and focusing on the SMT paradigm.\nThe MapReduce paradigm was used as a speedup tool for the automatic translation of\ntexts by [16]; their work focuses on one Rule-Based Machine Translation system.\nThe Apache Hadoop [17] framework transparently provides both reliability and data delivery to applications \u2013 moving data to processors (computers) that do task execution in contrast to systems such as BOINC [18] or HTCondor [19].\nThe main contribution of this manuscript in comparison to the related work is the inclusion of two most popular MT paradigms (RBMT and SMT) and the focus on accelerating the translation phase. The main deficiency of the aforementioned systems (BOINC [18] or HTCondor [19]) in comparison to Hadoop is their lack of support for data movement. This is provided in Hadoop through the Hadoop Distributed FileSystem. From this perspective one shall understand SMT as a service in a system that is installed on individual nodes in a\n7 similar way as any other service."}, {"heading": "6. DISTRIBUTED COMPUTING", "text": "In general, distributed systems are used to solve hard and parallel computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers [20] that communicate with each other by message passing [21]. To increase the efficiency it is desired to minimise the need to exchange information between the tasks. Main advantages of distributed computing are:\n\u2022 Users are distributed;\n\u2022 Information is distributed;\n\u2022 It may be more reliable if used correctly and\n\u2022 It may be faster and cheaper, especially in comparison with supercomputers.\nThe main conceptual problem is the disassociating of a task and its data: data are on one computer while the task is on another. The main technical problems are:\n\u2022 Computers are prone to malfunctions, increasing the number of computers also in-\ncreases the probability of failure;\n\u2022 Network connections are falling;\n\u2022 Data transmission is slow (10Gbit network has 300 micro-second latency [22]) 2GHz\nprocessor does 600,000 cycles in the same amount of time and\n\u2022 There is no global clock (ticks) for the whole system.\nThese problems are also addressed by the MapReduce [23] programming model."}, {"heading": "6.1. MapReduce and Hadoop", "text": "MapReduce [23] is a programming model for processing large data sets, and the name of an implementation of the model by Google. The model was developed by Google from the Map-fold model which has its roots in functional programming. It eases communication and\n8\ncoordination, rescue of crashed computers (moving load to available nodes), status reporting, debugging and basic optimization.\nThe basic architecture of a MapReduce setting is shown in Figure 3, shards are basic data chunks, mappers extract information from data shards and feed the extracted information to the Reducers that accumulate or generalize the results.\nApache Hadoop [17] is an open-source software framework that supports data-intensive distributed applications. It supports running applications on large clusters of commodity hardware. Hadoop is based on Google MapReduce [23] and Google File System (GFS) [24]. The Hadoop framework transparently provides both reliability and data delivery to applications \u2013 moving data to processors (computers) that do task execution in contrast to BOINC [18] or HTCondor [19] systems ref. Figure 3. Hadoop is licensed under the Apache license.6"}, {"heading": "7. METHODOLOGY", "text": "The research presented in this manuscript focuses entirely on the throughput of the translation system and proposes a method to increase the throughput with no effect on the translation quality. The throughput of the translation is measured as in Definition 1.\nTo employ Machine Translation in a MapReduce model, we must first find parallelism in our data and/or algorithms. The data parallelism comes from the fact that sentences are independently translated. The first assumption for the shard length can be a sentence. Finding parallelism in translation algorithms is beyond the scope of this research.\nThe movement of data to the processing nodes is in Hadoop provided through the Hadoop Distributed FileSystem. From this perspective one shall understand SMT as a service in a\n6 Apache v2 license\n9 system that is installed on individual nodes in a similar way as any other service. Now, the question is how to translate massive amounts of data. The data is split into shards and each of them is mapped to one client machine \u2013 the map phase. The translations are then collected into a final translation \u2013 the reduce phase.\nThe two most commonly used machine translation paradigms were addressed in the experiment, each paradigm was presented by an open-source solution. The SMT [9, 10] solution was Moses [11], the RBMT solution was Apertium [8].\nThe language pair for the Apertium system was English \u2013 Spanish (EN \u2013 ES), which is available under GPL license.7 The same language pair was used for Moses to enable the reuse of the same test-data."}, {"heading": "8. EVALUATION METHODOLOGY AND RESULTS", "text": "The setting for the experiment involved constructing test data, deploying the translation system and measure the time the system needed to translate the prepared test set. Three basic objectives were sought:\n\u2022 Elimination of the startup time effect;\n\u2022 Evaluation of the translation throughput of the translation system on one machine;\n\u2022 Evaluation of the translation throughput of the translation system using a MapReduce\ncluster."}, {"heading": "8.1. Test setting", "text": "The test environments were installed on a cluster of commodity machines8 that were used as the main testing deployment and on a faster machine9 that was used as a reference. The Apertium uses algorithms that are almost independent of the input text when regarding only translation throughput. The same fact can be attributed to the algorithms used by the Moses system if the caching option is turned off.\n7 https://svn.code.sf.net/p/apertium/svn/trunk/apertium-en-es 8 Pentium(R) Dual-Core CPU E5300@2.60GHz, 4GB RAM, Gigabit ethernet. 9 Intel(R) Core(TM) i7-3930K@3.20GHz, 32GB RAM, Gigabit ethernet.\n10\nThe operating system on all test machines was Ubuntu 14.04 LTS (Trusty Tahr), the only difference was that Server version was installed on the fast machine and Desktop version on the cluster machines.\nThe version of Apertium used in the experiments was 3.2. The version of Moses used in the experiments was 2.1.1. The system was trained on the Europarl v7 corpus [25]. The system was used with all default switches except the caching option was turned off. All the translation data was binarised."}, {"heading": "8.2. Test data", "text": "The main test data set was artificially constructed in order to eliminate the possible effects of the non-uniformity of the test data. The artificial sentences were constructed from one sentence composed of 20 words which were present in the dictionary and copied the desired number of times. The sentence length was chosen as an approximate upper bound mean value of the sentence length in Opus corpus [26] (the exact value is 16.5) and as an upper limit of the sentence length in Google n-gram corpus (the exact value is 10.8) [27]. This data set would be a problematic selection if the quality of the translation was measured, or if complex parsing algorithms were involved in the translation. The selection of simple translation techniques (Apertium) and mathematical models (Moses) allows the usage of artificial test data. The only problem arising using this simple test-data set was the caching option adopted by Moses which caches previous translations and could benefit in throughput greatly from this feature. The system was tested using the same test set and same test setting with this option turned on and off. The results presented in Table 1 show a significant influence of the caching to the artificial test data although the time differences are linear to the amount of input data. All further tests were done with the caching feature turned off.\nThe possible influences of the artificial test-set were further observed by including a real-\nlife data test set [28]. Most of the results are presented for both test sets.\nAll the test data is publicly available to facilitate the re-execution of experiments at the\nLanguage technologies server of the University of Primorska.10\n10 Test data: http://jt.upr.si/research_projects/mapreduce_mt_throughput/\n11"}, {"heading": "8.3. Sequential system", "text": "The first experiment involved measuring the throughput of the standard installations. Two sequential settings were deployed, one for each translation toolbox (Apertium and Moses). The translation systems were installed on the same set of machines (one testing8 and a reference machine9) and the translation throughput was tested using the same test-sets. Both settings were tested using different sizes of source texts.\nThe results of the Apertium system using differently sized artificial test data are presented in table 2. It shows the test data set with the results of the evaluation of the translation throughput of a single system. The throughput is in words per second (using real time). A steep increase of throughput using a small number of sentences which can be attributed to a fixed setup time and a linear time spent for each sentence. The exact setup time cannot be measured as the translation pipeline starts in parallel, succeeding pipeline stages are waiting for the output of the preceding stages. The influence of the startup time is not significant when translating more than 10,000 sentences or 200,000 words in our case. The translation throughput stabilizes at around 4,500 words per second on the test environment machine. The results on the reference machine9 are almost perfectly linear (twice as fast) for all tests.\nThe results of the Apertium system using the real-life data test set [28] (the whole set and the same text copied twice) are presented in Table 3. This test was used to show the possible influences of the artificial test-set on the results.\nTable 4 shows the test data with the results of the evaluation of the translation throughput of a single system. The not applicable label (na11) denotes the long-running tests (many\n11 Some of the long-running tests (many days) were skipped due to time constraints.\n12\ndays) that were skipped due to time constraints. The system sequential - moses was deployed on the same computer as the system shown in Table 2.\nThe system sequential - moses (fast) was deployed on a faster computer9. This system is used as a reference system to show how fast we can get with much better hardware (meaning higher costs), the system was not used in MapReduce experiments. The throughput is in words per second (using real time). All the other parameters of the experiment are the same as in the experiment presented in Table 2. The throughput stabilizes at roughly 11 words/s on the standard computer and roughly twice as much (23.6) on the faster reference computer.\nTable 5 presents the same system using the [28] data set. This test was used to show the\npossible influences of the artificial test-set on the results.\n13\nThe throughput values are consistent using both test sets which shows that the artificial\ntest set does not influence the results (if the caching option is turned off in Moses system)."}, {"heading": "8.4. Distributed system", "text": "Three architectures were implemented in the MapReduce model. The actual translation\nwas done in the mapping phase of the MapReduce model in all three architectures.\nThe first architecture (Apertium service architecture) employed a simple service in order to minimize the startup effect of the translation system. Figure 4 shows the architecture with a service that communicates with mappers through sockets and with the translation\n14\nsystem through POSIX pipes. A semaphore provides a locking mechanism to ensure data integrity. The server services mappers in a FIFO style. Communication is one-way; mappers simply deliver the data to the server and continue.\nThe presented architecture minimized the startup effect, but the communication overhead was quite substantial. The MR-service-apertium system in Table 6 presents empirical evaluation of the service architecture in the MapReduce environment.\nThe break even point was 500 sentences, meaning that the new architecture was faster translating less that 500 sentences in one job and it was slower than the original architecture for bigger jobs. We decided to eliminate the startup effect by setting the shard size at 1,000 sentences (well above the break-even point) and so minimizing the startup effect.\nThe service architecture was discarded for the simpler architecture with larger shards of input data (20,000 words). Table 7 shows the results of the evaluation, the MR-apertium system is the same as MR-simple-apertium for each node of the MR setting. The throughput of the translation system is almost linear to the number of nodes in the cluster. The break even point for the new setting is 4 nodes, meaning the MapReduce installation is faster with only four nodes and the throughput increases almost linearly to 16 which was the maximum number of nodes used in our experiment.\nThe Moses framework already has a server deployment option that was used in the MapReduce implementation of the Moses-based translation system in the experiment. Servers were\n15\ninstalled on each node to minimize the network traffic. Figure 6 shows the architecture used for the experiment. Translation is done in the Map phase using an XML:RPC call to the Moses server residing on the same physical node.\nThe comparison of the impact on the throughput using the Moses server deployment and XML:RPC Java client is presented in Figure 8. The results show only a marginal throughput loss. The setting was tested on the reference (fast) machine.\nThe results of the evaluation are presented in Table 9 and in Figure 7. The throughput is linear to the number of nodes, the penalty for using the Map reduce setting is the startup time of around 3 minutes and a general 20% lower throughput due to the more complicated architecture."}, {"heading": "9. DISCUSSION AND FURTHER WORK", "text": "The aim of the experiment was to test if the MapReduce model is suitable for machine translation tasks. The most used open source toolbox was chosen for each of the two most popular translation system paradigms, RBMT and SMT. The systems were tested in a\n16\nMapReduce model. It was empirically proven, that the MapReduce programming model is suitable for machine translation task after architectural combination of Hadoop and individual MT systems.\nThe increase in throughput for the presented RBMT system was roughly 800 % using the 16 machines in the testing cluster over the single machine (one of the machines in the\n17\ncluster). The shards for the translation task were 1,000 sentences or more.\nThe increase in throughput for the presented SMT system was roughly 1,200 % using the 16 machines in the testing cluster over the single machine (one of the machines in the cluster). The experiments will be repeated on a larger cluster as the empirical results show almost linear increase in the translation throughput by increasing the number of nodes in the cluster (our limit in the test was 16).\nFurther work can be done searching for parallelism not only in data, but also in algorithms\n18\nalthough this would mean basing the research on a single MT paradigm and using MapReduce paradigm there. Additional research should be done searching for a further limitation of the setup time effect on the overall performance since we selected a simplistic approach using bigger shards.\n1. M. R. Palankar, A. Iamnitchi, M. Ripeanu, and S. Garfinkel, in Proceedings of the 2008 inter-\nnational workshop on Data-aware distributed computing - DADC \u201908 (ACM Press, New York,\nNew York, USA, 2008), pp. 55\u201364, ISBN 9781605581545, URL http://dl.acm.org/citation.\ncfm?id=1383519.1383526.\n2. F. Masselot and G. Ramirez-Sa\u0301nchez, in Proceedings of the EAMT Conference (2010), p. 8.\n3. O. Furuse and H. Iida, in Proceedings of the 15th conference on Computational linguistics (As-\nsociation for Computational Linguistics, Morristown, NJ, USA, 1994), vol. 1, p. 105, URL\nhttp://dl.acm.org/citation.cfm?id=991886.991902.\n4. L. G. Mart\u0301inez, Ph.D. thesis, Dublin City University (2003).\n19\n20\nand E. Macklovitch (AMTA, New Orleans, USA, 2003), pp. 157\u2013164.\n13. C. Dyer, A. Cordova, A. Mont, and J. Lin, in Proceedings of the Third Workshop on Statistical\nMachine Translation (2008), pp. 199\u2013207.\n14. Q. Gao, Tech. Rep., School of Computer Science, Carnegie Mellon University (2008), URL\nhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.398.685{\\&}rank=1.\n15. Chao-Yue Lai, Tech. Rep., UC Berkeley (2010).\n16. R. S. Rashid Ahmad, Pawan Kumar, Rambabu B, Phani Sajja, Mukul K Sinha, in ICON-2011:\n9th International Conference on Natural Language Processing (2011), pp. 1\u20138.\n17. T. White, Hadoop: The definitive guide (O\u2019Reilly Media, Inc., 2012).\n18. A. Marosi, J. Kova\u0301cs, and P. Kacsuk, Future Generation Computer Systems 29,\n1442 (2013), ISSN 0167739X, URL http://www.sciencedirect.com/science/article/pii/\nS0167739X12000660.\n19. D. Thain, T. Tannenbaum, and M. Livny, Concurrency - Practice and Experience 17, 323\n(2005).\n20. A. S. Tanenbaum and M. van Steen, Distributed Systems: Principles and Paradigms (2nd\nEdition) (Prentice Hall, 2006).\n21. G. R. Andrews, Foundations of multithreaded, parallel, and distributed programming (Addison-\nWesley, Reading, Mass. [u.a.], 2000), ISBN 0-201-35752-6.\n22. E. K. Lua, J. Crowcroft, M. Pias, R. Sharma, and S. Lim, IEEE Communications Surveys and\nTutorials 7, 72 (2005).\n23. J. Dean and S. Ghemawat, Commun. ACM 53, 72 (2010), ISSN 0001-0782, URL http://doi.\nacm.org/10.1145/1629175.1629198.\n24. S. Ghemawat, H. Gobioff, and S.-T. Leung, SIGOPS Oper. Syst. Rev. 37, 29 (2003), ISSN\n0163-5980, URL http://doi.acm.org/10.1145/1165389.945450.\n25. P. Koehn, MT Summit 11, 79 (2005), URL http://mt-archive.info/MTS-2005-Koehn.pdf.\n26. J. Tiedemann, Lrec pp. 2214\u20132218 (2012), URL http://lrec.elra.info/proceedings/\nlrec2012/pdf/463{\\_}Paper.pdf.\n27. A. Franz and T. Brants, Tech. Rep. (2006), URL http://googleresearch.blogspot.com/\n2006/08/all-our-n-gram-are-belong-to-you.html.\n28. L. Specia, in Proceedings of the 15th Conference of the European Association for Machine\nTranslation (2011), pp. 73\u201380."}], "references": [{"title": "Proceedings of the 2008 inter- national workshop on Data-aware distributed computing - DADC \u201908", "author": ["M.R. Palankar", "A. Iamnitchi", "M. Ripeanu", "S. Garfinkel"], "venue": "URL http://dl.acm.org/citation. cfm?id=1383519.1383526", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Mart\u0301inez, Ph.D. thesis, Dublin City", "author": ["G. L"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Trombetti, in Conference of the Association for Machine Translation in the Americas (2012), URL http://amta2012.amtaweb.org/AMTA2012Files/ papers/123.pdf", "author": ["M. Federico", "A. Cattelan"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Forcada, in (organized in conjunction with LREC", "author": ["L. M"], "venue": "(Genoa, Italy,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Vi\u010di\u010d (Academic publishing house", "author": ["P. Homola", "V. Kubon"], "venue": "EXIT, Warsaw,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Hadoop: The definitive guide (O\u2019Reilly", "author": ["T. White"], "venue": "Media, Inc.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Distributed Systems: Principles and Paradigms (2nd Edition", "author": ["A.S. Tanenbaum", "M. van Steen"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Foundations of multithreaded, parallel, and distributed programming (Addison- Wesley", "author": ["G.R. Andrews"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "This approach would also involve an architecture change [1].", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "Some comparative research has been done examining the increase on the overall speed of translation process using machine translation tools compared to standard human translation process [4] and also the effect of the using Computer Assisted Translation \u2013 CAT tools that combine MT systems with translation memory and human post-editing process [5].", "startOffset": 186, "endOffset": 189}, {"referenceID": 2, "context": "Some comparative research has been done examining the increase on the overall speed of translation process using machine translation tools compared to standard human translation process [4] and also the effect of the using Computer Assisted Translation \u2013 CAT tools that combine MT systems with translation memory and human post-editing process [5].", "startOffset": 344, "endOffset": 347}, {"referenceID": 3, "context": "\u2022 Shallow-transfer Rule Based Machine Translation (Shallow Transfer RBMT) [6] paradigm that is most suited for translation of related languages [7], represented by Apertium [8];", "startOffset": 74, "endOffset": 77}, {"referenceID": 4, "context": "\u2022 Shallow-transfer Rule Based Machine Translation (Shallow Transfer RBMT) [6] paradigm that is most suited for translation of related languages [7], represented by Apertium [8];", "startOffset": 144, "endOffset": 147}, {"referenceID": 5, "context": "The Apache Hadoop [17] framework transparently provides both reliability and data delivery to applications \u2013 moving data to processors (computers) that do task execution in contrast to systems such as BOINC [18] or HTCondor [19].", "startOffset": 18, "endOffset": 22}, {"referenceID": 6, "context": "In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers [20] that communicate with each other by message passing [21].", "startOffset": 113, "endOffset": 117}, {"referenceID": 7, "context": "In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers [20] that communicate with each other by message passing [21].", "startOffset": 170, "endOffset": 174}, {"referenceID": 5, "context": "Apache Hadoop [17] is an open-source software framework that supports data-intensive distributed applications.", "startOffset": 14, "endOffset": 18}], "year": 2016, "abstractText": "The manuscript presents an experiment at implementation of a Machine Translation system in a MapReduce model. The empirical evaluation was done using fully implemented translation systems embedded into the MapReduce programming model. Two machine translation paradigms were studied: shallow transfer Rule Based Machine Translation and Statistical Machine Translation. The results show that the MapReduce model can be successfully used to increase the throughput of a machine translation system. Furthermore this method enhances the throughput of a machine translation system without decreasing the quality of the translation output. Thus, the present manuscript also represents a contribution to the seminal work in natural language processing, specifically Machine Translation. It first points toward the importance of the definition of the metric of throughput of translation system and, second, the applicability of the machine translation task to the MapReduce paradigm.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}