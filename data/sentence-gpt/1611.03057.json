{"id": "1611.03057", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2016", "title": "When silver glitters more than gold: Bootstrapping an Italian part-of-speech tagger for Twitter", "abstract": "We bootstrap a state-of-the-art part-of-speech tagger to tag Italian Twitter data, in the context of the Evalita 2016 PoSTWITA shared task. We show that training the tagger on native Twitter data enriched with little amounts of specifically selected gold data and additional silver-labelled data scraped from Facebook, yields better results than using large amounts of manually annotated data from a mix of genres.\n\n\n\nOur dataset is derived from a mix of 2D dataset (with a range of sample weights ranging from 4.67 to 7.63). Our dataset contains only the 4.67 data we identified and contains only the 4.67 data that were collected by us, but also includes only the 4.67 data that were collected by the original project, the Twitter API, and many other tools that use Twitter API. We provide a list of our data sources with which we are working on an API in the future.", "histories": [["v1", "Wed, 9 Nov 2016 19:39:15 GMT  (176kb,D)", "http://arxiv.org/abs/1611.03057v1", "Proceedings of the 5th Evaluation Campaign of Natural Language Processing and Speech Tools for Italian (EVALITA 2016)"]], "COMMENTS": "Proceedings of the 5th Evaluation Campaign of Natural Language Processing and Speech Tools for Italian (EVALITA 2016)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["barbara plank", "malvina nissim"], "accepted": false, "id": "1611.03057"}, "pdf": {"name": "1611.03057.pdf", "metadata": {"source": "CRF", "title": "When silver glitters more than gold: Bootstrapping an Italian part-of-speech tagger for Twitter", "authors": ["Barbara Plank", "Malvina Nissim"], "emails": ["b.plank@rug.nl", "m.nissim@rug.nl"], "sections": [{"heading": null, "text": "Italiano. Nell\u2019ambito della campagna di valutazione PoSTWITA di Evalita 2016, addestriamo due modelli che differiscono nel grado di supervisione in fase di training. Il modello addestrato con due cicli di bootstrapping usando post da Facebook, e che quindi impara anche da etichette \u201csilver\u201d, ha una performance superiore alla versione supervisionata che usa solo dati annotati manualmente. Discutiamo l\u2019importanza della scelta dei dati di training e development."}, {"heading": "1 Introduction", "text": "The emergence and abundance of social media texts has prompted the urge to develop tools that are able to process language which is often nonconventional, both in terms of lexicon as well as grammar. Indeed, models trained on standard newswire data heavily suffer when used on data from a different language variety, especially Twitter (McClosky et al., 2010; Foster et al., 2011; Gimpel et al., 2011; Plank, 2016).\nAs a way to equip microblog processing with efficient tools, two ways of developing Twittercompliant models have been explored. One option\nis to transform Twitter language back to what pretrained models already know via normalisation operations, so that existing tools are more successful on such different data. The other option is to create native models by training them on labelled Twitter data. The drawback of the first option is that it\u2019s not clear what norm to target: \u201cwhat is standard language?\u201d (Eisenstein, 2013; Plank, 2016), and implementing normalisation procedures requires quite a lot of manual intervention and subjective decisions. The drawback of the second option is that manually annotated Twitter data isn\u2019t readily available, and it is costly to produce.\nIn this paper, we report on our participation in PoSTWITA1, the EVALITA 2016 shared task on Italian Part-of-Speech (POS) tagging for Twitter (Tamburini et al., 2016). We emphasise an approach geared to building a single model (rather than an ensemble) based on weakly supervised learning, thus favouring (over normalisation) the aforementioned second option of learning invariant representations, also for theoretical reasons. We address the bottleneck of acquiring manually annotated data by suggesting and showing that a semi-supervised approach that mainly focuses on tweaking data selection within a bootstrapping setting can be successfully pursued for this task. Contextually, we show that large amounts of manually annotated data might not be helpful if data isn\u2019t \u201cof the right kind\u201d."}, {"heading": "2 Data selection and bootstrapping", "text": "In adapting a POS tagger to Twitter, we mainly focus on ways of selectively enriching the training set with additional data. Rather than simply adding large amounts of existing annotated data, we investigate ways of selecting smaller amounts of more appropriate training instances, possibly even tagged with silver rather than gold labels. As\n1http://corpora.ficlit.unibo.it/PoSTWITA/\nar X\niv :1\n61 1.\n03 05\n7v 1\n[ cs\n.C L\n] 9\nN ov\n2 01\n6\nfor the model itself, we simply take an off-theshelf tagger, namely a bi-directional Long ShortTerm Memory (bi-LSTM) model (Plank et al., 2016), which we use with default parameters (see Section 3.2) apart from initializing it with Twittertrained embeddings (Section 3.1).\nOur first model is trained on the PoSTWITA training set plus additional gold data selected according to two criteria (see below: Two shades of gold). This model is used to tag a collection of Facebook posts in a bootstrapping setting with two cycles (see below: Bootstrapping via Facebook). The rationale behind using Facebook as not-so-distant source when targeting Twitter is the following: many Facebook posts of public, nonpersonal pages resemble tweets in style, because of brevity and the use of hashtags. However, differently from random tweets, they are usually correctly formed grammatically and spelling-wise, and often provide more context, which allows for more accurate tagging.\nTwo shades of gold We used the Italian portion of the latest release (v1.3) of the Universal Dependency (UD) dataset (Nivre et al., 2016), from which we extracted two subsets, according to two different criteria. First, we selected data on the basis of its origin, trying to match the Twitter training data as close as possible. For this reason, we used the Facebook subportion (UD FB). These are 45 sentences that presumably stem from the Italian Facebook help pages and contain questions and short answers.2 Second, by looking at the confusion matrix of one of the initial models, we saw that the model\u2019s performance was especially poor for cliticised verbs and interjections, tags that are also infrequent in the training set (Table 2). Therefore, from the Italian UD portion we selected any data (in terms of origin/genre) which contained the VERB CLIT or INTJ tag, with the aim to boost the identification of these categories. We refer to this set of 933 sentences as UD verb clit+intj.\nBootstrapping via Facebook We augmented our training set with silver-labelled data. With our best model trained on the original task data plus UD verb clit+intj and UD FB, we tagged a collection of Facebook posts, added those to\n2These are labelled as 4-FB in the comment section of UD. Examples include: Prima di effettuare la registrazione. E\u0300 vero che Facebook sara\u0300 a pagamento?\nthe training pool, and retrained our tagger. We used two iterations of indelible self-training (Abney, 2007), i.e., adding automatically tagged data where labels do not change once added. Using the Facebook API through the Facebook-sdk python library3, we scraped an average of 100 posts for each of the following pages, selected on the basis of our intuition and on reasonable site popularity:\n\u2022 sport: corrieredellosport \u2022 news: Ansa.it, ilsole24ore, lastampa.it \u2022 politics: matteorenziufficiale \u2022 entertainment: novella2000, alFemminile \u2022 travel: viaggiart\nWe included a second cycle of bootstrapping, scraping a few more Facebook pages (soloGossip.it, paesionline, espressonline, LaGazzettaDelloSport, again with an average of 100 posts each), and tagging the posts with the model that had been re-trained on the original training set plus the first round of Facebook data with silver labels (we refer to the whole of the automatically-labelled Facebook data as FB silver). FB silver was added to the training pool to train the final model. Statistics on the obtained data are given in Table 1.4"}, {"heading": "3 Experiments and Results", "text": "In this section we describe how we developed the two models of the final submission, including all preprocessing decisions. We highlight the importance of choosing an adequate development set to identify promising directions."}, {"heading": "3.1 Experimental Setup", "text": "PoSTWITA data In the context of PoSTWITA, training data was provided to all participants in the\n3https://pypi.python.org/pypi/facebook-sdk 4Due to time constraints we did not add further iterations; we cannot judge if we already reached a performance plateau.\nform of manually labelled tweets. The tags comply with the UD tagset, with a couple of modifications due to the specific genre (emoticons are labelled with a dedicated tag, for example), and subjective choices in the treatment of some morphological traits typical of Italian. Specifically, clitics and articulated prepositions are treated as one single form (see below: UD fused forms). The training set contains 6438 tweets, for a total of ca. 115K tokens. The distribution of tags together with examples is given in Table 2. The test set comprises 301 tweets (ca. 4800 tokens).\nUD fused forms In the UD scheme for Italian, articulated prepositions (ADP A) and cliticised verbs (VERB CLIT) are annotated as separate word forms, while in PoSTWITA the original word form (e.g., \u2018alla\u2019 or \u2018arricchirsi\u2019) is annotated as a whole. In order to get the PoSTWITA ADP A and VERB CLIT tags for these fused word forms from UD, we adjust the UCPH ud-conversion-tools5 (Agic\u0301 et al., 2016) that propagates head POS information up to the original form.\nPre-processing of unlabelled data For the Facebook data, we use a simplistic off-theshelf rule-based tokeniser that segments sentences by punctuation and tokens by whitespace.6 We normalise URLs to a single token (http://www.someurl.org) and add a rule for smileys. Finally, we remove sentences from\n5https://github.com/coastalcph/ud-conversion-tools 6https://github.com/bplank/multilingualtokenizer\nthe Facebook data were more than 90% of the tokens are in all caps. Unlabelled data used for embeddings is preprocessed only with normalisation of usernames and URLs.\nWord Embeddings We induced word embeddings from 5 million Italian tweets (TWITA) from Twita (Basile and Nissim, 2013). Vectors were created using word2vec (Mikolov and Dean, 2013) with default parameters, except for the fact that we set the dimensions to 64, to match the vector size of the multilingual (POLY) embeddings (Al-Rfou et al., 2013) used by Plank et al. (2016). We dealt with unknown words by adding a \u201cUNK\u201d token computing the mean vector of three infrequent words (\u201cvip!\u201d,\u201ccuora\u201d, \u201cWhite\u201d).\nCreation of a realistic internal development set The original task data is distributed as a single training file. In initial experiments we saw that performance varied considerably for different random subsets. This was due to a large bias towards tweets about \u2018Monti\u2019 and \u2018Grillo\u2019, see Figure 1, but also because of duplicate tweets. We opted to create the most difficult development set possible. This development set was achieved by removing duplicates, and randomly selecting a subset of tweets that do not mention \u2018Grillo\u2019 or \u2018Monti\u2019 while maximizing out-of-vocabulary (OOV) rate with respect to the training data. Hence, our internal development set consisted of 700 tweets with an OOV approaching 50%. This represents a more realistic testing scenario. Indeed, the baseline (the basic bi-LSTM model), dropped from 94.37 to 92.41 computed on the earlier development set were we had randomly selected 1/5 of the data, with an OOV of 45% (see Table 4)."}, {"heading": "3.2 Model", "text": "The bidirectional Long Short-Term Memory model bilty7 is illustrated in Figure 2. It is a\n7https://github.com/bplank/bilstm-aux\ncontext bi-LSTM taking as input word embeddings ~w. Character embeddings ~c are incorporated via a hierarchical bi-LSTM using a sequence bi-LSTM at the lower level (Ballesteros et al., 2015; Plank et al., 2016). The character representation is concatenated with the (learned) word embeddings ~w to form the input to the context bi-LSTM at the upper layers. We took default parameters, i.e., character embeddings set to 100, word embeddings set to 64, 20 iterations of training using Stochastic Gradient Descent, a single bi-LSTM layer and regularization using Gaussian noise with \u03c3 = 0.2 (cdim 100, trainer sgd, indim 64, iters 20, h layer 1, sigma 0.2). The model has been shown to achieve state-of-the-art performance on a range of languages, where the incorporation of character information was particularly effective (Plank et al., 2016). With these features and settings we train two models on different training sets.\nGOLDPICK bilty with pre-initialised TWITA embeddings, trained on the PoSTWITA training set plus selected gold data (UD FB + UD verb clit+intj).\nSILVERBOOT a bootstrapped version of GOLDPICK, where FB silver (see Section 2) is also added to the training pool, which thus includes both gold and silver data."}, {"heading": "3.3 Results on test data", "text": "Participants were allowed to submit one official, and one additional (unofficial) run. Because on development data SILVERBOOT performed better than GOLDPICK, we selected the former for our official submission and the latter for the unofficial one, making it thus also possible to assess the specific contribution of bootstrapping to performance.\nTable 3 shows the results on the official test data for both our models and TNT (Brants, 2000). The results show that adding bootstrapped silver data outperforms the model trained on gold data alone. The additional training data included in SILVERBOOT reduced the OOV rate for the testset to 41.2% (compared to 46.9% with respect to the original PoSTWITA training set). Note that, on the original randomly selected development set the results were less indicative of the contribution of the silver data (see Table 4), showing the importance of a carefully selected development set."}, {"heading": "4 What didn\u2019t work", "text": "In addition to what we found to boost the tagger\u2019s performance, we also observed what didn\u2019t yield any improvements, and in some case even lowered global accuracy. What we experimented with was triggered by intuition and previous work, as well as what we had already found to be successful, such as selecting additional data to make up for under-represented tags in the training set. However, everything we report in this section turned out to be either pointless or detrimental.\nMore data We added to the training data all (train, development, and test) sections from the Italian part of UD1.3. While training on selected gold data (978 sentences) yielded 95.06% accuracy, adding all of the UD-data (12k sentences of newswire, legal and wiki texts) yielded a disappointing 94.88% in initial experiments (see Table 4), also considerably slowing down training.\nNext, we tried to add more Twitter data from XLIME, a publicly available corpus with multiple layers of manually assigned labels, including POS tags, for a total of ca. 8600 tweets and 160K tokens (Rei et al., 2016). The data isn\u2019t provided as a single gold standard file but in the form of\nseparate annotations produced by different judges, so that we used MACE (Hovy et al., 2013) to adjudicate divergences. Additionally, the tagset is slightly different from the UD set, so that we had to implement a mapping. The results in Table 4 show that adding all of the XLIME data declines performance, despite careful preprocessing to map the tags and resolve annotation divergences.\nMore tag-specific data From the matrix computed on the dev set, it emerged that the most confused categories were NOUN and PROPN. Following the same principle that led us to add UD verb clit+intj, we tried to reduce such confusion by providing additional training data containing proper nouns. This did not yield any improvements, neither in terms of global accuracy, nor in terms of precision and recall of the two tags.\nMulti-task learning Multi-task learning (MTL) (Caruana, 1997), namely a learning setting where more than one task is learnt at the same time, has been shown to improve performance for several NLP tasks (Collobert et al., 2011; Bordes et al., 2012; Liu et al., 2015). Often, what is learnt is one main task and, additionally, a number of auxiliary tasks, where the latter should help the model\nconverge better and overfit less on the former. In this context, the additional signal we use to support the learning of each token\u2019s POS tag is the token\u2019s degree of ambiguity. Using the information stored in Morph-it!, a lexicon of Italian inflected forms with their lemma and morphological features (Zanchetta and Baroni, 2005), we obtained the number of all different tags potentially associated to each token. Because the Morph-it! labels are highly fine-grained we derived two different ambiguity scores, one on the original and one on coarser tags. In neither case the additional signal contributed to the tagger\u2019s performance, but we have not explored this direction fully and leave it for future investigations."}, {"heading": "5 Conclusions", "text": "The main conclusion we draw from the experiments in this paper is that data selection matters, not only for training but also while developing for taking informed decisions. Indeed, only after creating a carefully designed internal development set we obtained stronger evidence of the contribution of silver data which is also reflected in the official results. We also observe that choosing less but more targeted data is more effective. For instance, TWITA embeddings contribute more than generic POLY embeddings which were trained on substantially larger amounts of Wikipedia data. Also, just blindly adding training data does not help. We have seen that using the whole of the UD corpus is not beneficial to performance when compared to a small amount of selected gold data, both in terms of origin and labels covered. Finally, and most importantly, we have found that adding little amounts of not-so-distant silver data obtained via bootstrapping resulted in our best model.\nWe believe the low performance observed when adding xLIME data is likely due to the noncorrespondence of tags in the two datasets, which required a heuristic-based mapping. While this is only a speculation that requires further investigation, it seems to indicate that exploring semisupervised strategies is preferrable to producing idiosyncratic or project-specific gold annotations.\nAcknowledgments We thank the CIT of the University of Groningen for providing access to the Peregrine HPC cluster. Barbara Plank acknowledges NVIDIA corporation for support."}], "references": [{"title": "Semisupervised learning for computational linguistics", "author": ["Steven Abney"], "venue": null, "citeRegEx": "Abney.,? \\Q2007\\E", "shortCiteRegEx": "Abney.", "year": 2007}, {"title": "Multilingual projection for parsing truly low-resource languages", "author": ["Agi\u0107 et al.2016] \u017deljko Agi\u0107", "Anders Johannsen", "Barbara Plank", "H\u00e9ctor Mart\u0131\u0301nez Alonso", "Natalie Schluter", "Anders S\u00f8gaard"], "venue": "Transactions of the Association", "citeRegEx": "Agi\u0107 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agi\u0107 et al\\.", "year": 2016}, {"title": "Polyglot: Distributed word representations for multilingual NLP", "author": ["Al-Rfou et al.2013] Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": "arXiv preprint arXiv:1307.1662", "citeRegEx": "Al.Rfou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Improved transitionbased parsing by modeling characters instead of words with lstms", "author": ["Chris Dyer", "Noah A. Smith"], "venue": null, "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Sentiment analysis on italian tweets", "author": ["Basile", "Nissim2013] Valerio Basile", "Malvina Nissim"], "venue": "In Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis,", "citeRegEx": "Basile et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Basile et al\\.", "year": 2013}, {"title": "Joint learning of words and meaning representations for opentext semantic parsing", "author": ["Xavier Glorot", "Jason Weston", "Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "Bordes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2012}, {"title": "Tnt: a statistical part-of-speech tagger. In ANLP", "author": ["Thorsten Brants"], "venue": null, "citeRegEx": "Brants.,? \\Q2000\\E", "shortCiteRegEx": "Brants.", "year": 2000}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "What to do about bad language on the internet", "author": ["Jacob Eisenstein"], "venue": "In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Eisenstein.,? \\Q2013\\E", "shortCiteRegEx": "Eisenstein.", "year": 2013}, {"title": "From news to comments: Resources and benchmarks for parsing the language of Web 2.0", "author": ["Ozlem Cetinoglu", "Joachim Wagner", "Josef Le Roux", "Joakim Nivre", "Deirde Hogan", "Josef van Genabith"], "venue": "In IJCNLP", "citeRegEx": "Foster et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2011}, {"title": "Part-ofSpeech Tagging for Twitter: Annotation, Features", "author": ["Gimpel et al.2011] Kevin Gimpel", "Nathan Schneider", "Brendan O\u2019Connor", "Dipanjan Das", "Daniel Mills", "Jacob Eisenstein", "Michael Heilman", "Dani Yogatama", "Jeffrey Flanigan", "Noah A. Smith"], "venue": null, "citeRegEx": "Gimpel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2011}, {"title": "Learning whom to trust with MACE", "author": ["Hovy et al.2013] Dirk Hovy", "Taylor Berg-Kirkpatrick", "Ashish Vaswani", "Eduard Hovy"], "venue": null, "citeRegEx": "Hovy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hovy et al\\.", "year": 2013}, {"title": "Representation learning using multi-task deep neural networks for semantic classification and information", "author": ["Liu et al.2015] Xiaodong Liu", "Jianfeng Gao", "Xiaodong He", "Li Deng", "Kevin Duh", "Ye-Yi Wang"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Automatic domain adaptation for parsing", "author": ["Eugene Charniak", "Mark Johnson"], "venue": null, "citeRegEx": "McClosky et al\\.,? \\Q2010\\E", "shortCiteRegEx": "McClosky et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems", "author": ["Mikolov", "Dean2013] T Mikolov", "J Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Universal dependencies 1.3. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics, Charles University in Prague", "author": ["Nivre et al.2016] Joakim Nivre"], "venue": null, "citeRegEx": "Nivre,? \\Q2016\\E", "shortCiteRegEx": "Nivre", "year": 2016}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "author": ["Plank et al.2016] Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg"], "venue": null, "citeRegEx": "Plank et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "What to do about non-standard (or non-canonical) language in NLP", "author": ["Barbara Plank"], "venue": "In KONVENS", "citeRegEx": "Plank.,? \\Q2016\\E", "shortCiteRegEx": "Plank.", "year": 2016}, {"title": "A multilingual social media linguistic corpus. In Conference of CMC and Social Media Corpora for the Humanities", "author": ["Rei et al.2016] Luis Rei", "Dunja Mladenic", "Simon Krek"], "venue": null, "citeRegEx": "Rei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rei et al\\.", "year": 2016}, {"title": "Overview of the EVALITA 2016 Part Of Speech on TWitter for ITAlian Task", "author": ["Cristina Bosco", "Alessandro Mazzei", "Andrea Bolioli"], "venue": null, "citeRegEx": "Tamburini et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tamburini et al\\.", "year": 2016}, {"title": "Morph-it! A free corpus-based morphological resource for the Italian language", "author": ["Zanchetta", "Baroni2005] Eros Zanchetta", "Marco Baroni"], "venue": "Corpus Linguistics", "citeRegEx": "Zanchetta et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zanchetta et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 13, "context": "Indeed, models trained on standard newswire data heavily suffer when used on data from a different language variety, especially Twitter (McClosky et al., 2010; Foster et al., 2011; Gimpel et al., 2011; Plank, 2016).", "startOffset": 136, "endOffset": 214}, {"referenceID": 9, "context": "Indeed, models trained on standard newswire data heavily suffer when used on data from a different language variety, especially Twitter (McClosky et al., 2010; Foster et al., 2011; Gimpel et al., 2011; Plank, 2016).", "startOffset": 136, "endOffset": 214}, {"referenceID": 10, "context": "Indeed, models trained on standard newswire data heavily suffer when used on data from a different language variety, especially Twitter (McClosky et al., 2010; Foster et al., 2011; Gimpel et al., 2011; Plank, 2016).", "startOffset": 136, "endOffset": 214}, {"referenceID": 17, "context": "Indeed, models trained on standard newswire data heavily suffer when used on data from a different language variety, especially Twitter (McClosky et al., 2010; Foster et al., 2011; Gimpel et al., 2011; Plank, 2016).", "startOffset": 136, "endOffset": 214}, {"referenceID": 8, "context": "The drawback of the first option is that it\u2019s not clear what norm to target: \u201cwhat is standard language?\u201d (Eisenstein, 2013; Plank, 2016), and implementing normalisation procedures requires quite a lot of manual intervention and subjective decisions.", "startOffset": 106, "endOffset": 137}, {"referenceID": 17, "context": "The drawback of the first option is that it\u2019s not clear what norm to target: \u201cwhat is standard language?\u201d (Eisenstein, 2013; Plank, 2016), and implementing normalisation procedures requires quite a lot of manual intervention and subjective decisions.", "startOffset": 106, "endOffset": 137}, {"referenceID": 19, "context": "In this paper, we report on our participation in PoSTWITA1, the EVALITA 2016 shared task on Italian Part-of-Speech (POS) tagging for Twitter (Tamburini et al., 2016).", "startOffset": 141, "endOffset": 165}, {"referenceID": 16, "context": "for the model itself, we simply take an off-theshelf tagger, namely a bi-directional Long ShortTerm Memory (bi-LSTM) model (Plank et al., 2016), which we use with default parameters (see Section 3.", "startOffset": 123, "endOffset": 143}, {"referenceID": 0, "context": "We used two iterations of indelible self-training (Abney, 2007), i.", "startOffset": 50, "endOffset": 63}, {"referenceID": 1, "context": "In order to get the PoSTWITA ADP A and VERB CLIT tags for these fused word forms from UD, we adjust the UCPH ud-conversion-tools5 (Agi\u0107 et al., 2016) that propagates head POS information up to the original form.", "startOffset": 130, "endOffset": 149}, {"referenceID": 2, "context": "Vectors were created using word2vec (Mikolov and Dean, 2013) with default parameters, except for the fact that we set the dimensions to 64, to match the vector size of the multilingual (POLY) embeddings (Al-Rfou et al., 2013) used by Plank et al.", "startOffset": 203, "endOffset": 225}, {"referenceID": 2, "context": "Vectors were created using word2vec (Mikolov and Dean, 2013) with default parameters, except for the fact that we set the dimensions to 64, to match the vector size of the multilingual (POLY) embeddings (Al-Rfou et al., 2013) used by Plank et al. (2016). We dealt with unknown words by adding a \u201cUNK\u201d token computing the mean vector of three infrequent words (\u201cvip!\u201d,\u201ccuora\u201d, \u201cWhite\u201d).", "startOffset": 204, "endOffset": 254}, {"referenceID": 3, "context": "Character embeddings ~c are incorporated via a hierarchical bi-LSTM using a sequence bi-LSTM at the lower level (Ballesteros et al., 2015; Plank et al., 2016).", "startOffset": 112, "endOffset": 158}, {"referenceID": 16, "context": "Character embeddings ~c are incorporated via a hierarchical bi-LSTM using a sequence bi-LSTM at the lower level (Ballesteros et al., 2015; Plank et al., 2016).", "startOffset": 112, "endOffset": 158}, {"referenceID": 16, "context": "The model has been shown to achieve state-of-the-art performance on a range of languages, where the incorporation of character information was particularly effective (Plank et al., 2016).", "startOffset": 166, "endOffset": 186}, {"referenceID": 6, "context": "Table 3 shows the results on the official test data for both our models and TNT (Brants, 2000).", "startOffset": 80, "endOffset": 94}, {"referenceID": 18, "context": "8600 tweets and 160K tokens (Rei et al., 2016).", "startOffset": 28, "endOffset": 46}, {"referenceID": 11, "context": "separate annotations produced by different judges, so that we used MACE (Hovy et al., 2013) to adjudicate divergences.", "startOffset": 72, "endOffset": 91}, {"referenceID": 7, "context": "Multi-task learning Multi-task learning (MTL) (Caruana, 1997), namely a learning setting where more than one task is learnt at the same time, has been shown to improve performance for several NLP tasks (Collobert et al., 2011; Bordes et al., 2012; Liu et al., 2015).", "startOffset": 202, "endOffset": 265}, {"referenceID": 5, "context": "Multi-task learning Multi-task learning (MTL) (Caruana, 1997), namely a learning setting where more than one task is learnt at the same time, has been shown to improve performance for several NLP tasks (Collobert et al., 2011; Bordes et al., 2012; Liu et al., 2015).", "startOffset": 202, "endOffset": 265}, {"referenceID": 12, "context": "Multi-task learning Multi-task learning (MTL) (Caruana, 1997), namely a learning setting where more than one task is learnt at the same time, has been shown to improve performance for several NLP tasks (Collobert et al., 2011; Bordes et al., 2012; Liu et al., 2015).", "startOffset": 202, "endOffset": 265}], "year": 2016, "abstractText": "English. We bootstrap a state-of-the-art part-of-speech tagger to tag Italian Twitter data, in the context of the Evalita 2016 PoSTWITA shared task. We show that training the tagger on native Twitter data enriched with little amounts of specifically selected gold data and additional silver-labelled data scraped from Facebook, yields better results than using large amounts of manually annotated data from a mix of genres. Italiano. Nell\u2019ambito della campagna di valutazione PoSTWITA di Evalita 2016, addestriamo due modelli che differiscono nel grado di supervisione in fase di training. Il modello addestrato con due cicli di bootstrapping usando post da Facebook, e che quindi impara anche da etichette \u201csilver\u201d, ha una performance superiore alla versione supervisionata che usa solo dati annotati manualmente. Discutiamo l\u2019importanza della scelta dei dati di train-", "creator": "LaTeX with hyperref package"}}}