{"id": "1611.03218", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2016", "title": "Learning to Play Guess Who? and Inventing a Grounded Language as a Consequence", "abstract": "Learning your first language is an incredible feat and not easily duplicated. Doing this using nothing but a few pictureless books, a corpus, would likely be impossible even for humans. As an alternative we propose to use situated interactions between agents as a driving force for communication, and the framework of Deep Recurrent Q-Networks (DRQN) for learning a common language grounded in the provided environment.\n\n\nAs mentioned above the idea is simple: one needs a new language and a new culture. As with any language you choose to use, you have to learn it from it and learn from it. I\u2019ve talked to many people with such deep learning that they have been fascinated with it. In my research I have worked with a very few groups of developers and students who do not like using a new language.\nWhat\u2014s really good for you about deep learning is the flexibility. I found this a bit helpful, and I want to highlight the difference between using existing languages or not, and explain how it\u202cs not so easy.\nI hope you enjoyed this article, and you\u2019ll all like the work.", "histories": [["v1", "Thu, 10 Nov 2016 08:44:52 GMT  (2843kb,D)", "http://arxiv.org/abs/1611.03218v1", "8 pages with 1 page appendix. Accepted to Deep Reinforcement Learning Workshop at NIPS 2016"], ["v2", "Sun, 27 Nov 2016 14:50:50 GMT  (2819kb,D)", "http://arxiv.org/abs/1611.03218v2", "8 pages with 1 page appendix. Accepted to Deep Reinforcement Learning Workshop at NIPS 2016"], ["v3", "Tue, 3 Jan 2017 18:28:43 GMT  (2843kb,D)", "http://arxiv.org/abs/1611.03218v3", "8 pages with 1 page appendix. Accepted to Deep Reinforcement Learning Workshop at NIPS 2016"], ["v4", "Wed, 15 Mar 2017 11:24:49 GMT  (4109kb,D)", "http://arxiv.org/abs/1611.03218v4", "Previous version was accepted to Deep Reinforcement Learning Workshop at NIPS 2016"]], "COMMENTS": "8 pages with 1 page appendix. Accepted to Deep Reinforcement Learning Workshop at NIPS 2016", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG cs.MA", "authors": ["emilio jorge", "mikael k{\\aa}geb\\\"ack", "fredrik d johansson", "emil gustavsson"], "accepted": false, "id": "1611.03218"}, "pdf": {"name": "1611.03218.pdf", "metadata": {"source": "CRF", "title": "Learning to Play Guess Who? and Inventing a Grounded Language as a Consequence", "authors": ["Emilio Jorge", "Mikael K\u00e5geb\u00e4ck", "Emil Gustavsson"], "emails": ["firstname.lastname@fcc.chalmers.se", "kageback@chalmers.se"], "sections": [{"heading": null, "text": "Learning your first language is an incredible feat and not easily duplicated. Doing this using nothing but a few pictureless books, a corpus, would likely be impossible even for humans. As an alternative we propose to use situated interactions between agents as a driving force for communication, and the framework of Deep Recurrent Q-Networks (DRQN) for learning a common language grounded in the provided environment. We task the agents with interactive image search in the form of the game Guess Who?. The images from the game provide a non trivial environment for the agents to discuss and a natural grounding for the concepts they decide to encode in their communication. Our experiments show that it is possible to learn this task using DRQN and even more importantly that the words the agents use correspond to physical attributes present in the images that make up the agents environment."}, {"heading": "1 Introduction", "text": "Human language closely interacts with the world it is used to describe. Language arose as a way of transmitting knowledge about the state of the world between the people that live in it, and it evolves as the world changes over time. Severing this link, by analysing text as a stand alone artifact, leads to problems with grounding of concepts and effectively eliminates exploratory mapping of the language.\nIn contrast, when two humans communicate they generally do so in connection to the environment and in both directions, which provides the necessary grounding of concepts but also immediate feedback on every utterance. The importance of feedback for human language learners was shown by Sachs et al. in [1]. This paper describes the case of Jim, a hearing child that was brought up by two deaf parents and had to learn to speak from watching television without any supervision or feedback. These circumstances severely delayed his acquisition of language and he did not learn to speak properly until after intervention from the outside, which highlights the importance of guided exploration of the language via synthesis and feedback for mastering a language.\nIn this paper we investigate if a grounded language can emerge by letting two agents invent their own language to solve a common problem using DRQN. A first step in this direction was taken by Foerster et al. in [2, 3] where agents learn to communicate using binary messages to solve riddles.\nHowever, our goal is to enable the agents to evolve a richer language and to facilitate that we propose to use images as conversation pieces.\nMore precisely we let two agents play the game of Guess Who? which forces the agents to come up with grounded words that represent characteristics of objects in images in order to win the game.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 1.\n03 21\n8v 1\n[ cs\n.A I]\n1 0\nN ov\nThe model we propose is similar to the Differentiable inter-agent learning (DIAL) model presented in [2], see Section 2.3 for more details, but differs in some key areas. (1) Instead of communicating using bits we generalise the model to handle orthogonal messages of arbitrary dimension to enable vocabularies of arbitrary size, (2) by gradually increasing the noise on the communication channel we ensure that the agents learn a symbolic language but with less negative impact on the convergence rate during training and with a positive impact on the learning capacity (for more on the model see Section 4) and (3) in our model no parameters are shared between the agents since this is more reasonable from a human perspective.\nThe main contributions of this paper include:\n\u2022 An end-to-end trainable multiple-agent reinforcement learning model that learns a near optimal strategy for playing Guess who? without sharing any parameters between agents and with no predefined communications protocol.\n\u2022 An analysis of the invented language that shows how the words are grounded in the concepts visible in the images. \u2022 Experiments that show how increasing levels of noise in the communication channel leads\nto an improved training speed and learning capacity (compared to constant noise) while retaining the ability to learn discrete symbols.\n\u2022 Finally, we generalise DIAL to use orthogonal messages of arbitrary dimension, to more closely resemble human language which encompasses hundreds of thousands of words, and we show that this improve the performance of the system as well as make it more interpretable."}, {"heading": "2 Background", "text": "The results in this paper rely mainly on the theory of reinforcement learning, deep Q-networks, and the concept of differentiable inter-agent learning. In this section short descriptions of these methodologies are presented."}, {"heading": "2.1 Reinforcement learning", "text": "In a traditional single-agent reinforcement learning (RL) framework an agent observes the current state st \u2208 S at each time step t, takes action ut \u2208 U according to some policy \u03c0, receives the reward rt, and transitions according to some probability distribution (depending on the current state and action) to a new state st+1 \u2208 S. The objective of the agent is to maximize the expected discounted future sum of rewards Rt = \u2211\u221e \u03c4=t \u03b3\nt\u2212\u03c4r\u03c4 , where \u03b3 \u2208 [0, 1] is a discount factor that trades-off the importance of immediate and future rewards. For a specific policy \u03c0, the value of a state-action pair is defined as Q\u03c0(s, u) = E[Rt | st = s, ut = u]. The optimal value function Q\u2217(s, u) = max\u03c0 Q\n\u03c0(s, u) is called the Q-value of the state-action pair (s, t) and obeys the Bellman optimality equation Q\u2217(s, t) = E[r + \u03b3maxu\u2032 Q\u2217(s\u2032, u\u2032) | s, u]. Whenever an agent employs the optimal strategy the agent is guaranteed to achieve the highest expected sum of discounted rewards. To find the Q-values various RL algorithms have previously been used where the most successful ones are the iterative methods Q-Learning and Profit Sharing. RL can be extended to cooperative multi-agent settings where each agent a observes a global state st, selects individual actions uat , and then receives a team reward rt, shared among all agents. When all agents do not have full observability, i.e., when the agents do not observe the entire environment, the global state st is hidden and the agents only receive observations ot that are correlated with the state st."}, {"heading": "2.2 Deep Q-Networks", "text": "The space of state-action pairs is, in many applications, so large that storing and updating the Q values for each state-action pair is computationally intractable. One solution for this dimensionality problem is to employ the concept of Deep-Q-Networks (DQN) (for a more thorough description, see [4]). The idea of DQN is to represent the Q-function by using a neural network parameterised by \u03b8, i.e., to find Q(s, u; \u03b8) which approximates the value Q\u2217(s, u) for all state-action pairs. The network is optimised by minimising the loss function Li(\u03b8i) = E[(yDQNi \u2212Q(s, u; \u03b8i)2], at iteration i, with yDQNi = r + \u03b3maxu\u2032 Q(s \u2032, u\u2032; \u03b8\u2212i ), where \u03b8 \u2212 are the parameters of a target network which is fixed\nfor a number of iterations. The actions chosen during the training of the network are determined by an -greedy policy that selects the action that maximises the Q-value for the current state with probability 1\u2212 and chooses an action randomly with probability . When agents only have partial observability, Hausknecht and Stone ([5]) propose to use an approach called Deep Recurrent Q-Networks (DRQN) where, instead of approximating the Q-values with a feed-forward network, they approximate the Q-values with a recurrent neural network that can maintain an internal state which aggregates the observations over time. This is modelled by adding an input ht\u22121 to the network that represents the hidden state of the network."}, {"heading": "2.3 Differentiable inter-agent learning", "text": "In earlier work where communication is a part of the RL setting, messages are seen as actions and are selected via Q-functions and action polices such as -greedy. In [2], Foerster et al. introduce the idea of centralised training but decentralised execution, i.e., the agents are trained together but evaluated separately. They introduce the concept of Differentiable Inter-Agent Learning (DIAL) where messages are allowed to be continuous during training, but need to be discrete during evaluation, which allows gradients to propagate between the agents through the messages in training. This gives the agents more feedback and thus reduces the amount of learning required through trial-and-error. To reduce the discretisation error that could occur from the discretisation of messages in the evaluation phase the messages are processed by a dicretise/regularise unit (DRU). During centralised learning the DRU regularises the messages according to DRU(mat ) = Logistic(N(m a t , \u03c3)), where m a t is the message of agent a in time step t, and during decentralised execution DRU(mat ) = 1{mat > 0}. The added noise \u03c3 during the training phase pushes the messages towards the ends of the logistic function and therefore will force the agents to send almost discrete messages. The change of setup also requires a slight change in how the model is trained; the loss with respect to the Q-function is the same as in the DQN case but the gradient term for mat is the error backpropagated through the message from the recipient to the sender. More algorithm details can be found in [2, Appendix A]. Using DIAL instead of traditional independent Q-learning techniques is shown to be beneficial, both in terms of learning speed and in terms of ability to learn."}, {"heading": "3 Guess Who?", "text": "The task we consider in this paper is a version of the popular guessing game Guess Who?. In Guess Who? two players are each assigned one character from a set of 24 characters. Each character has an image and the goal of the game is to deduce which of the 24 characters the other player has. The players take turns asking questions based on the visual appearance of the characters. The questions have to be answered truthfully with yes or no until one player knows which character the other player was assigned.\nThe version of Guess Who? that we consider is slightly different. One important difference is that we remove the competing factor and have one agent be the asking-agent (agent a = 1) and the other the answering-agent (agent a = 2). This means that one agent specialises in asking questions and the other in answering them, and the two of them collaborate with the objective of solving the task. Another difference is that instead of finding the correct character amongst the full set of characters a subset is sampled and given as observation input to the asking agent; in our experiments we used a subset of two or four images. The answering agent only observes the image of its assigned character from the subset observed by the asking-agent.\nIn Figure 1 an example of our version of Guess Who? is illustrated. The asking-agent has four images and the answering-agent has the (correct) image which is the third image of the askingagent. The asking-agent sends its first question (m11) and receives the answer (m 2 1). Then another round of question and answer occurs and finally the asking-agent guesses which of the images the answering-agent holds.\nIn each time step, the agents take turns sending and receiving messages. This means that in each step either the answering-agent\u2019s receives a question or the asking-agent gets an answer. The questions that the asking-agent may send are limited to a discrete set of questions and the answers the answeringagent can send are limited to two answers (yes or no). Instead of playing the game with a variable amount of time steps a fix amount of steps is played. For the case when the asking-agent observes two images the agent is only allowed to ask one question (m11) and receive one answer (m 2 1), and for\nthe case when the asking-agent observes four images, two questions (m11,m 1 2) and two answers (m 2 1, m22) are allowed. After all answers have been received the asking-agent has to guess which of the images the answering-agent has. The guess is considered as an action taken by the asking-agent and is represented by u1 \u2208 {1, . . . , n}, where n is the number of images the agent holds. If u1 is equal to the index of the character that the asking-agent has, a reward of 1 is given to both agents, otherwise they receive 0.\nWhen the number of question-answer rounds is limited it is not always possible to win each game since only a fixed number of messages are available and only yes or no answers are permitted. If two messages are available it is essentially possible to partition the set of images into four parts. With four messages this partition can be made into 16 parts. If there are fewer parts than the size of the image set it means that several images will be partitioned into the same part, yielding in the same answers to the questions posed and therefore making them indistinguishable for the model. This implies that the maximum average reward for a game where the asking-agent holds two images from a total of 24 classes (as in Guess Who?) and two different messages are allowed is only 0.89 (for derivations see Appendix A). The equivalent score in a four image game with two rounds of questions would be 0.71 (for derivations see Appendix A). However, since our agents have recurrent networks it is possible that the questions in each round have different meanings (otherwise there would be no point in asking the same question in the second round as in the first round) such that the set can be partitioned into more parts, implying that the maximum average reward is higher than 0.71."}, {"heading": "4 Model", "text": "The architecture of the model is presented in this section and a schematic illustration of the model is shown in Figure 2. Each agent (a = 1, 2) consists of a recurrent neural network (RNN) that is unrolled for T time steps with an internal state h, an input network, and an output network. The input network takes the information available, (oat , m\u0302 a\u2032 t\u22121, u a t\u22121) and transforms it into a 256 embedding zat . The observation o a t of agent a at time step t is passed through a 2-layer multilayer perceptron (MLP) of size [#colours \u00d7|image|, 128, 256]. The regularised message m\u0302a\u2032t\u22121 from the other agent, a\u2032, is passed through an MLP of size [|M |, 256]. The previous action uat\u22121 for agent a is passed through a lookup-embedding of size 256. These embeddings are added element-wise to give a final embedding zat . The embedding z a t is then processed by a 2-layer RNN with gated reccurent units (GRU) [6] of size [256,256] to give the output embedding ha2,t. The output embedding h a 2,t is then processed by an MLP to give Qa,mat =MLP [256, 256, U +M ](h a 2,t). The Q-function Qa(u) is used to generate the action uat according to an -greedy policy. In our model we utilise messages in a one-hot encoding which are then passed to the other agent for the next time step. While using a\none-hot encoding does have disadvantages in terms of scalability (since a binary encoding of length n gives 2n different possible messages instead of only n in the one-hot encoding) we found that it gave improved results and makes it easier to study the underlying effect behind each message which is clouded by the varying closeness between messages that occurs in binary encoding. It also makes sense from a human perspective since we communicate through words which are non-binary. The message mat is passed through a variant of DRU as described in section 2.3 to generate a one-hot encoding using m\u0302ta =DRU(m a t ) = Softmax(N(m a t , \u03c3episode)) in the training case where N(\u00b5, \u03c3) is the |m| dimensional normal distribution. During evaluation m\u0302ta(i) = {1 if i = argmax(m), 0 otherwise} is used instead. Batch normalization (see, [7]) is performed in the MLP for the image embedding and on the incoming messages ma \u2032\nt\u22121. During testing non-stochastic versions of batch normalization is used which implies that running averages of values observed during training are used instead of those from the batch.\nUpdates of the parameters of the network are performed as described in section 2.3 with more details available in [2, Appendix A]."}, {"heading": "5 Increasing noise", "text": "Inspired by curriculum learning described in [8], where Bengio et al. show that gradually increasing difficulty of tasks leads to improved learning, we introduce the usage of increasing noise to DIAL. We found that using a fix value for the noise \u03c3 led to an unwelcome trade off. If \u03c3 is too large there is a risk of the model never learning anything. If \u03c3 is too small you get an excellent training error (where continuous messages are allowed) but bad testing error due to the model over-encoding information in the messages leading to a large discretisation error. Our solution to this is to allow the noise to linearly increase over the epochs. This enables the model to learn quickly in the beginning but to punish the over-encoding of information more and more as the training progresses. This can be compared to human speech where a person who is beginning to learn a new language requires an almost noise free surrounding to understand a message while a proficient speaker would have no problem understanding a message in a more noisy environment such as listening to the radio or having a conversation by a busy road."}, {"heading": "6 Experiments", "text": "To validate our ideas we run a series of experiments on our version of Guess Who? to see how well it performs but also what the model learns. Further, we evaluate the effectiveness of increasing noise by comparing it to experiments with non-increasing noise.\n0 10k 20k 30k 40k 50k\nEpisodes\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP er fo rm\na n ce\n2 images - Guess Who?\n2 questions\n4 questions\n8 questions\n(a) The performance of the model when the askingagent has two images, one round of question-answer is performed, and two, four, or eight different messages are allowed. The results are averaged over five runs.\n0 10k 20k 30k 40k 50k\nEpisodes\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP er fo rm\na n ce\n4 images - Guess Who?\n2 questions\n4 quesitons\n8 questions\n(b) The performance of the model when the askingagent has four images, two rounds of question-answer are performed, and two, four, or eight different messages are allowed. The results are medians of twelve runs.\nFigure 3: Performance of the model when allowing different number of messages. The dashed grey lines represents the baseline performance where the asking-agent guesses randomly.\nFor each episode we randomly select two or four different images from the pool of 24 Guess Who? images (which are down-sampled to 32\u00d7 32 pixels). The model is trained with \u03c3 increasing linearly from 0.1 to 1. We evaluate the model performance with two, four, or eight different messages available to the asking agent in a one-hot embedding, and the answers yes/no available to the answering agent. The game goes on for three time steps (i.e., one question, one answer, and one guess) for two images and five time steps (i.e, two questions and two answers) for four images.\nThe experiments use a -greedy policy with = 0.05, a discount factor \u03b3 = 1, and each epoch performs parallel episodes using a batch size of 32. The target network \u03b8\u2212 is updated every 100 epochs. Unless stated otherwise, we let \u03c3 increase linearly from 0.1 to 1; this keeps the average \u03c3 close to what was found to work on a task with similar tasks in [2], where they used \u03c3 = 0.5. We use a small dropout parameter of 0.1 to be able to maintain network size relatively small. The optimization is done by rms-prop with a learning rate of 5\u00d7 10\u22124. Each experiment is run multiple times and the results averaged. We intend to publish the code for all experiments online."}, {"heading": "7 Results", "text": ""}, {"heading": "7.1 Performance", "text": "In Figure 3a the average performance (over five runs) is illustrated in the case where the asking-agent sees two images, here it is clear that more messages are crucial for better performance on two images. The score for two messages is close to the theoretical bound of 0.89 for two messages (as discussed in Section 3) but is surpassed when using four and eight messages.\nThe performance for four images is show using median values. This is due to the fact that one of the runs with four messages fails to learn anything and remains at baseline performance, additionally there are three runs (two with four messages and one with eight messages) that learn a semisuccessful protocol before returning to the baseline. These outliers lead to averages not being a good representation of the ability to learn. The performance over twelve runs can be seen in Figure 3b. It can be seen that the performance obtained on four images is quite similar independently of how many questions are available, specially when taking into account that there is some instability in the training. In the case of four images with two messages some of the evaluations achieve a score that is above 0.71 which would be the best possible if the questions in both asking phases are interpreted the same (as discussed in Section 3). This shows that the agents learn to change the interpretation of questions depending on if it is the first or the second question or depending on what question was asked previously."}, {"heading": "7.2 Understanding the questions", "text": "To better understand the nature of the questions that the asking-agent asks we sample a few episodes from a model with two images and two available questions to see how the games play out, i.e., what the message protocol between the agents is depending on the images they see. The message protocol is illustrated in Table 1. In the table it can be seen that the asking-agents questions depend on what images it has, the answers it gets from the answering-agent, and its interpretation of the answer (its guess). One can see that in some cases it receives the same answer even when the answering-agent has different images, this leads to the answering-agent sometimes making a guess on the wrong image and receiving zero reward.\nThe answers to each question for the different images is illustrated in Figure 4. This shows that the two images that it has trouble separating (see the ones yielding zero reward in Table 1) have the same answers to both questions (No to question A and Yes to question B), as such it is impossible for the asking-agent to distinguish between the two. This illustrates the problem with a limited set of messages which leads to only being able to partition the set in a few ways. Judging from the appearances of the images, it appears that the question A can be interpreted as something along the lines of Is the top of his/her head very light coloured or very darkly coloured?. Question B is harder to interpret but it could be something along the lines of Are his/her cheeks and ears free from a surrounding object (such as hair or sideburns), and if this is not the case is the surrounding object very lightly coloured?."}, {"heading": "7.3 Inreasing noise", "text": "To evaluate the effectiveness of increasing the noise \u03c3 during training we compare results on Guess Who? using four images and with eight possible messages and an increasing \u03c3 with a constant \u03c3 = 0.5 (which was used in [2]) but also with \u03c3 \u2208 {0, 0.1, 1}. In Figure 5 it is clearly visible that there is a significant advantage in using variable noise compared to constant noise. The model learns both faster and achieves better performance compared to the models trained with constant noise."}, {"heading": "8 Related works", "text": "Previous work has been on communication in a multi-agent reinforcement learning [2, 3, 9]. In [9] the predator-prey environment is studied where agents learn a communication protocol consisting of bits. There it is shown that while longer messages (i.e. larger bandwidth) may take longer to learn\nthey generally give improved results. In [2, 3] the authors solve puzzles with multiple agents with one-bit communication."}, {"heading": "9 Conclusions", "text": "In this paper we have shown that a DRQN can learn to play Guess Who?. Further, by careful analysis of the results we could establish a grounding connection between the words used by the agents and characteristics of objects in images from the game. Finally, we showed that regulating the level of noise in the communications channel could have a large impact on the training speed as well as the final performance of the system."}, {"heading": "Acknowledgments", "text": "The authors would like to acknowledge the project Towards a knowledge-based culturomics supported by a framework grant from the Swedish Research Council (2012\u20132016; dnr 2012-5738). The authors would also like to thank Jakob Foerster and Yannis M. Assael for the insightful discussions."}, {"heading": "A Derivations for optimal scoring with two messages", "text": "When trying to correctly differentiate between two images from a pool of 24 images using one out of two available question the probability of guessing the correct image can be seen as the following: Let image A be the correct image and B be the incorrect image from the pool.\nP (Correct) = = P (Correct|Separable) \u00b7 P (Separable)\n+P (Correct|Not separable) \u00b7 P (Not separable) (1)\nImages are separable if the image A and B are not in the same subset of images divided up by the questions. Since a subset that divides the set up equally is optimal this means that the 24 images are divided up such that there is 6 images in each subset and P (Separable) = 1 \u2212 5/23 = 18/23. If they are inseparable asking agent will have to perform a random guess. This gives equation (1) = 1 \u00b7 18/23 + 1/2 \u00b7 5/23 \u2248 0.89. For the case with four images it is slightly different. The following calculations assume that two questions are asked but that those are the only two questions available such that the two questions available in the first question phase are the same as in the second phase. This means that the optimal procedure will be to ask one of them in the first phase and the other in the second phase. In this case the calculations are as above but taking into consideration that there may be multiple images in the correct subset. We then get\nP (Correct) = = P (Correct|Separable) \u00b7 P (Separable)\n+P (Correct|Not separable with one) \u00b7 P (Not separable with one) +P (Correct|Not separable with two) \u00b7 P (Not separable with two) +P (Correct|Not separable with three) \u00b7 P (Not separable with three)\n= 1 1\n( 5 0 )( 18 3 ) + 12 ( 5 1 )( 18 2 ) + 13 ( 5 2 )( 18 1 ) + 14 ( 5 3 )( 18 0 )( 23 3 ) \u2248 0.71"}], "references": [{"title": "Language learning with restricted input: case studies of two hearing children of deaf parents", "author": ["J. Sachs", "B. Bard", "M.L. Johnson"], "venue": "Applied Psycholinguistics, vol. 2, no. 01, pp. 33\u201354, 1981.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1981}, {"title": "Learning to communicate with deep multi-agent reinforcement learning", "author": ["J.N. Foerster", "Y.M. Assael", "N. de Freitas", "S. Whiteson"], "venue": "CoRR, vol. abs/1605.06676, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to communicate to solve riddles with deep distributed recurrent q-networks", "author": ["\u2014"], "venue": "CoRR, vol. abs/1602.02672, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, Feb. 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["M.J. Hausknecht", "P. Stone"], "venue": "CoRR, vol. abs/1507.06527, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "\u00c7. G\u00fcl\u00e7ehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "CoRR, vol. abs/1406.1078, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Batch normalization: accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "CoRR, vol. abs/1502.03167, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ser. ICML \u201909, Montreal, Quebec, Canada: ACM, 2009, pp. 41\u201348, ISBN: 978-1-60558-516-1. DOI: 10.1145/1553374. 1553380.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "in [1].", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "in [2, 3] where agents learn to communicate using binary messages to solve riddles.", "startOffset": 3, "endOffset": 9}, {"referenceID": 2, "context": "in [2, 3] where agents learn to communicate using binary messages to solve riddles.", "startOffset": 3, "endOffset": 9}, {"referenceID": 1, "context": "The model we propose is similar to the Differentiable inter-agent learning (DIAL) model presented in [2], see Section 2.", "startOffset": 101, "endOffset": 104}, {"referenceID": 0, "context": "The objective of the agent is to maximize the expected discounted future sum of rewards Rt = \u2211\u221e \u03c4=t \u03b3 r\u03c4 , where \u03b3 \u2208 [0, 1] is a discount factor that trades-off the importance of immediate and future rewards.", "startOffset": 117, "endOffset": 123}, {"referenceID": 3, "context": "One solution for this dimensionality problem is to employ the concept of Deep-Q-Networks (DQN) (for a more thorough description, see [4]).", "startOffset": 133, "endOffset": 136}, {"referenceID": 4, "context": "When agents only have partial observability, Hausknecht and Stone ([5]) propose to use an approach called Deep Recurrent Q-Networks (DRQN) where, instead of approximating the Q-values with a feed-forward network, they approximate the Q-values with a recurrent neural network that can maintain an internal state which aggregates the observations over time.", "startOffset": 67, "endOffset": 70}, {"referenceID": 1, "context": "In [2], Foerster et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "The embedding z a t is then processed by a 2-layer RNN with gated reccurent units (GRU) [6] of size [256,256] to give the output embedding h2,t.", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": "Batch normalization (see, [7]) is performed in the MLP for the image embedding and on the incoming messages m \u2032 t\u22121.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "Inspired by curriculum learning described in [8], where Bengio et al.", "startOffset": 45, "endOffset": 48}, {"referenceID": 1, "context": "1 to 1; this keeps the average \u03c3 close to what was found to work on a task with similar tasks in [2], where they used \u03c3 = 0.", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "5 (which was used in [2]) but also with \u03c3 \u2208 {0, 0.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "Previous work has been on communication in a multi-agent reinforcement learning [2, 3, 9].", "startOffset": 80, "endOffset": 89}, {"referenceID": 2, "context": "Previous work has been on communication in a multi-agent reinforcement learning [2, 3, 9].", "startOffset": 80, "endOffset": 89}, {"referenceID": 1, "context": "In [2, 3] the authors solve puzzles with multiple agents with one-bit communication.", "startOffset": 3, "endOffset": 9}, {"referenceID": 2, "context": "In [2, 3] the authors solve puzzles with multiple agents with one-bit communication.", "startOffset": 3, "endOffset": 9}], "year": 2016, "abstractText": "Learning your first language is an incredible feat and not easily duplicated. Doing this using nothing but a few pictureless books, a corpus, would likely be impossible even for humans. As an alternative we propose to use situated interactions between agents as a driving force for communication, and the framework of Deep Recurrent Q-Networks (DRQN) for learning a common language grounded in the provided environment. We task the agents with interactive image search in the form of the game Guess Who?. The images from the game provide a non trivial environment for the agents to discuss and a natural grounding for the concepts they decide to encode in their communication. Our experiments show that it is possible to learn this task using DRQN and even more importantly that the words the agents use correspond to physical attributes present in the images that make up the agents environment.", "creator": "LaTeX with hyperref package"}}}