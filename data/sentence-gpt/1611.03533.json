{"id": "1611.03533", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2016", "title": "Landmark-based consonant voicing detection on multilingual corpora", "abstract": "This paper tests the hypothesis that distinctive feature classifiers anchored at phonetic landmarks can be transferred cross-lingually without loss of accuracy. Three consonant voicing classifiers were developed: (1) manually selected acoustic features anchored at a phonetic landmark, (2) MFCCs (either averaged across the segment or anchored at the landmark), and(3) acoustic features computed using a convolutional neural network (CNN). All detectors are trained on English data (TIMIT),and tested on English, Turkish, and Spanish (performance measured using F1 and accuracy). Experiments demonstrate that manual features outperform all MFCC classifiers, while CNNfeatures outperform both. MFCC-based classifiers suffer an F1reduction of 16% absolute when generalized from English to other languages. Manual features suffer only a 5% F1 reduction,and CNN features actually perform better in Turkish and Span-ish than in the training language, demonstrating that features capable of representing long-term spectral dynamics (CNN and landmark-based features) are able to generalize cross-lingually with little or no loss of accuracy, but still perform better in both German and Turkish contexts. Moreover, this study also shows that MLTs can outperform MLTs as well.", "histories": [["v1", "Thu, 10 Nov 2016 22:11:16 GMT  (92kb)", "http://arxiv.org/abs/1611.03533v1", "ready to submit to JASA-EL"]], "COMMENTS": "ready to submit to JASA-EL", "reviews": [], "SUBJECTS": "cs.CL cs.SD", "authors": ["xiang kong", "xuesong yang", "mark hasegawa-johnson", "jeung-yoon choi", "stefanie shattuck-hufnagel"], "accepted": false, "id": "1611.03533"}, "pdf": {"name": "1611.03533.pdf", "metadata": {"source": "CRF", "title": "LANDMARK-BASED CONSONANT VOICING DETECTION ON MULTILINGUAL CORPORA", "authors": ["Xiang Kong", "Xuesong Yang", "Jeung-Yoon Choi", "Mark Hasegawa-Johnson", "Stefanie Shattuck-Hufnagel"], "emails": ["jhasegaw}@illinois.edu,", "sshuf}@mit.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n03 53\n3v 1\n[ cs\n.C L\n] 1\n0 N\nov 2\n01 6"}, {"heading": "1. Introduction", "text": "In contrast to the conventional data-driven speech recognition model, acoustic correlates of distinctive features are found in an acoustics phonetic recognizer [1] so as to extract interpretable acoustic information. There are two types of distinctive features in this model: articulator-free feature and articulatorbound feature. Articulator-free features determine phone manner class, while articulator-bound features specify the phone identity. Since features in this model depend on the properties of the vocal tract, they are, to some extent, universal and independent of the language being spoken. For obstruent consonants in English, e.g. fricatives, affricates, and stops, three articulators ([lips], [tongue body] and [tongue blade]) can form the constriction to produce consonants. Obstruent consonants are further categorized by consonant voicing which can be described by the articulator-bound feature [stiff vocal folds] [2].\nMuch related work has been done about consonant acoustic and voicing. Shadle [3] studied fricative consonants using mechanical models, theoretical models, and acoustic analysis, and found that the most important parameters for fricatives are the length of the front cavity, the presence of an obstacle and\nthe flow rate. Speech production mechanism differences between voice and voiceless stops are mainly due to muscle activity, which relaxes the tongue root during voiced stops, altering aerodynamics near the vocal folds in order to maintain voicing during closure [4]. Vowel cues (i.e. vocalic duration and F1 offset frequency) are also correlates of consonant voicing [5]. A module for detecting consonant voicing based on these acoustic correlates [6] first determines acoustic properties according to consonant production, then extracts acoustic cues, and classifies them to detect consonant voicing.\nOne of the traditional methods to detect consonant voicing uses mel-frequency cepstral coefficients (MFCCs) [7, 8], e.g. voicing can be detected with 74.7% to 80% overall accuracy [9, 10]. Overall, good performance of MFCC-based method on consonant voicing is possible,however, MFCCs mostly codify the \u201cfilter\u201d information in the source-filter theory of speech production, and are therefore less efficient in capturing information about the voice source.\nIn contrast, much of the consonant voicing information can be captured in the characteristics of the vocal fold vibration patterns, therefore capturing acoustic phonetic features indicative of vocal fold vibration has the potential to measure consonant voicing. Though voicing does not continue uninterrupted during obstruent closure in English, there are striking differences near consonant closure and release landmarks. Landmarks [1] identify times when the acoustic patterns of the linguistically motivated distinctive features are most salient; acoustic cues extracted in the vicinity of landmarks may therefore be more informative for the classification of distinctive features than cues extracted from other times in the signal. To the best of our knowledge, the highest accuracy for voicing classification of obstruents uses acoustic features extracted with reference to phonetic landmarks, with accuracies of 95% and 96% [11, 12] for stops and fricatives respectively.\nThe choice of data representation is essential for the performance of detection or classification tasks. Discriminative information from raw data can be extracted by taking advantage of human perceptual ingenuity and human prior knowledge. However, the process of designing these manual features is laborious and time-consuming. Deep learning techniques transform raw data into multiple levels of abstraction by stacking multiple layers with non-linearities, thus learning complex features automatically [13]. Though the accuracy of speech recognizers built from deep networks is high [14], results on the cross-language portability of deep networks include both positive and negative outcomes. We propose that deep networks trained to classify\ndistinctive features should be cross-language portable, because of the universality of the features they are trained to classify.\nIn order to test the hypothesis that distinctive features anchored at phonetic landmarks can be transferred cross-lingually, our models that are trained on an English corpus will be applied for the detection tasks of three different languages, including English, Spanish, and Turkish.\nIn this paper, acoustic landmark theory and the definition of its regions are described in Section 2. Within these landmark regions, section 3 illustrates acoustic feature representations that help to improve the performance of voicing detection, consisting of manually designed acoustic cues and features learned from deep neural networks. Experiments are described in Section 4, Results in Section 5, and Conclusions in Section 6."}, {"heading": "2. ACOUSTIC LANDMARKS AND DISTINCTIVE FEATURES", "text": "Landmarks [1] are defined as points in an utterance around which information about the underlying distinctive features may be extracted. Four types of landmarks were proposed in [1]: Vowel (V), Consonant release (Cr), Consonant closure (Cc), and Glide (G). Cr landmarks and Cc landmarks are further specialized by manner class: S=stop, F=fricative, N=nasal.\nIn this work, we assume that we have been given the right landmark positions in a speech signal. To convert TIMIT phonetic transcriptions to landmark transcriptions, the following rules were used. Each stop release segment has one Sr landmark at its start time; each stop closure segment has one Sc landmark at its start time; and each affricate, fricative, or nasal has a Cc landmark at its start time and a Cr landmark at its end time (where C=F for affricates and fricatives, C=N for nasals). Vowels and glides each have one landmark, located at the midpoint.\nTIMIT label files specify the start time and the end time of each phone, from which landmark locations were computed and generate landmark files. Table 1 illustrates examples of acoustic landmarks extracted from TIMIT. The first column denotes landmark time, the second column landmark type. The first row in Table 1, for example, shows that a fricative closure landmark happens at time alignment 0.916s.\nStevens [1] proposed that distinctive features obtained from closure and release landmark regions should be universal across languages. Motivated by this theory, landmark positions across multiple languages can be labeled by the same rules as above; this paper tests corpora in Spanish and Turkish. After finding landmark positions, we denote the landmark regions as follows [15]:\n\u2022 If a Cc landmark is located the start of one phone, speech signals after that time (+20ms) are extracted.\n\u2022 If a Cr landmark is at the start of one phone, speech signals before that time (-20ms) are extracted.\nAfter finding landmarks, acoustic cues which could be used to determine distinctive features are extracted. Distinctive features are a concise description of subsegmental attributes of a\nphone, with a relatively direct relation to acoustics and articulation. They take on binary values and form a minimal set which can distinguish each segment from all others in a language, therefore if we can determine distinctive features, the phonetic transcription is thereby determined."}, {"heading": "3. ACOUSTIC FEATURE REPRESENTATIONS", "text": "Within each landmark region, the acoustic features are extracted for discriminating voiced consonants from unvoiced ones, including manually designed acoustic cues (summary see Table 2) and the features learned from deep neural networks."}, {"heading": "3.1. Manually Designed Acoustic Cues", "text": "Voice onset time (VOT): At first we found duration of every consonant (e.g. stop, fricative and affricate). Duration refers to the length between the release and the onset of voicing. For stop release segments, duration is the voice onset time (VOT) [9] which carries voicing information about English stops. Duration also carries voicing information for fricatives and affricates, both of which are shorter if voiced than unvoiced.\nPeak normalized value of the cross-correlation (PNCC): Increasing cross correlation value will exist when producing voiced consonants (stops, fricatives and affricates) [[1]. PNCC is originally denoted in [16], and besides using normalized cross-correlation, we retain its peak, which captures cross correlation value transitions. Therefore, we used the peak normalized value of the cross correlation (PNCC) as another acoustic feature for consonant voicing.\nAmplitude of fundamental frequency (H1): Although the amplitude of the speech signal varies with time, the amplitude of unvoiced speech utterances is much lower than that of voiced segments. Therefore, to distinguish the strength of vocal fold vibration, the amplitude of fundamental frequency (H1) is a reasonable third acoustic feature for consonant voicing.\nFormant transitions: Formant transitions [17] are different for voiced and unvoiced consonants. There is a significant formant transition present following voice onset in voiced obstruents, less so for unvoiced obstruents; the difference is especially marked for stops, but is also observable for other obstruents. Formant transitions are therefore also retained.\nEnergy: Energy of voiced and unvoiced consonants is also different. As voice energy can be observable at both low and relatively high frequencies, while unvoiced energy is concentrated at high frequencies, the last four acoustic features are the rootmean-square (RMS) energy, energy between 0-400Hz (E1), energy between 2000-7000Hz (E2), and the ratio of E1 and E2."}, {"heading": "3.2. Convolutional neural networks (CNN)", "text": "A common raw feature representation of speech signals as inputs is the magnitude of log-scale mel filterbanks over time. However, this paper proposes to extract features across a landmark region that is too short (20ms) for multiple frames, therefore 1D discrete Fourier transformation, and 1D filterbanks are considered as inputs.\nFigure 1 illustrates the architecture of convolutional neural networks that consist of three types of layers\u2013convolution, maxpooling, and fully connected layers. In a convolutional layer, each neuron takes as inputs local patterns in the previous layer. All neurons in the same feature map share the same weight matrix. A max-pooling layer is stacked following each convolutional layer that similarly takes local patterns as inputs, and down-samples to generate a single output for that local region. Multiple fully connected layers are concatenated after multiple building blocks of convolutional-pooling pairs. A softmax layer with a single neuron is taken as the output that capture the posterior probability of the positive label (voicing). During backpropagation, a first-order gradient-based optimization method based on adaptive estimates of lower-order moments (Adam) [18] is used."}, {"heading": "4. EXPERIMENTS", "text": ""}, {"heading": "4.1. Multilingual Corpora", "text": "Three language corpora, including American English, Spanish, and Turkish, are considered. Only the TIMIT TRAIN corpus is chosen for model training, while others are selected as test sets. Table 3 illustrates the number of samples in these corpora.\nEnglish Corpus: TIMIT [19] corpus contains broadband recordings of 630 speakers of eight major dialects of American English, each reading ten phonetically rich sentences and includes time-aligned orthographic, phonetic and word transcriptions, as well as a 16-bit, 16kHz speech waveform file for each utterance.\nSpanish corpus: The phonetic Albayzin corpus of Spanish is divided into two subcorpora, one for training and another for testing purposes, because it was initially developed to train speech recognition engines. The training subcorpus is made of 200 phrases; 4 speakers produced all 200 phrases and 160 speakers produced 25 out of these 200, so the set of 200 phrases is produced 24 times. The phrases are acoustically balanced, according to a statistical study of the frequency of each sound in Castillian Spanish.\nTurkish corpus: Middle East Technical University Turkish Microphone Speech Corpus (METU) [20] was selected as Turkish test set. 120 speakers (60 male and 60 female) speak 40 sentences each (approximately 300 words per speaker), which makes around 500 minutes of speech in total. The 40 sen-\ntences are selected randomly for each speaker from a triphonebalanced set of 2462 Turkish sentences."}, {"heading": "4.2. Feature Extraction", "text": "The calculation of manual acoustic features anchored at a phonetic landmark region, and MFCCs (either averaged across the phonetic segment or anchored at the landmark region), and raw data representation of speech signals in landmark regions are illustrated as follows, respectively.\nMFCCs: A Hamming window is first applied, with duration equal either to the landmark region, or to the duration of the whole phone. The windowed signal is then transformed to compute MFCC(13) or MFCC(39).\nDuration, formant, transition, PNCC and H1: a robust RAPT [16] algorithm for pitch tracking that is based on normalized cross-correlation and dynamic programming is applied using Wavesurfer1. The fundamental frequency, probability of voicing (1.0 means voiced and 0.0 means unvoiced), local error of the pitch, and the peak normalized value of the cross correlation (PNCC) are obtained. After getting the pitch for each landmark segment, FFT amplitude at the pitch frequency was measured, and FFT spectra were used to measure formant transitions.\nEnergy: Butterworth filter is used to design a bandpass filter with \u2264 3dB of passband ripple and \u2265 40dB attenuation in the stopbands, then energy of filtered signals is computed in the bands 0-400Hz and 2000Hz-7000Hz.\nRaw features for neural networks: 1024 point magnitude FFT is performed. Mel-scale filterbank features are calculated by multiplying frequency response with a set of 40 triangular bandpass filters equally spaced in Mel frequency. In order to apply the early stopping strategy during training procedure, a held-out development set (10%) is stratified sampled from training set. The training will stop when the validation loss is not decreasing anymore within 10 consecutive epochs."}, {"heading": "5. RESULTS", "text": "Consonant voicing is detected using MFCCs, acoustic cues, and CNNs. These models are all trained in English, and tested on English, Spanish, and Turkish, respectively. Support vector machine with radial basis function kernel is used as the binary classifier based on acoustic features, while CNNs are used as an end-to-end classifier. The F1 score of voicing consonant (positive sample) and overall accuracy are used as metrics. Due to the imbalanced nature of training set, the CNN is trained with each sample weighted inversely proportional to class frequency. Relative error rate increment of performance over English has been calculated when models are applied on other languages.\nMFCCs: when calculating average MFCCs across the whole phonetic segment, MFCCs with dynamic coefficients (MC39) achieved better accuracy and F1 score than MFCCs with only static coefficients (see first and third columns in Figs. 2 and 3). However, F1 and accuracy for MFCCs dropped\n1http://www.speech.kth.se/wavesurfer/\nby 5-20% absolute when these models were evaluated on Spanish and Turkish. When calculating MFCCs anchored at landmark regions, MC39 obtained slightly better F1 score than its averaged model across the whole segment.\nAcoustic Cues vs. MFCCs: in Figs. 2 and 3, on English (training language), improvements are clearly shown for acoustic cues compared with MFCC based classifiers. When generalizing to other languages, acoustic cue based features suffer much lower accuracy decrements than MFCCs. Furthermore, large error rate increase has been detected (up to 96.1%) when MFCC-based model has been tested on other languages and the smallest error rate for it is 44.5%. However, there is only up to 35.2% relative error rate increase for Acoustic cues-based model.\nCNNs vs. Feedforward: The filterbank used to compute MFCCs can be viewed as a type of pre-determined convolutional network; conversely, CNNs extract local patterns with trainable but fixed-length convolutional windows. The last two columns in Figure 4 reveal that CNNs can hold stable performance for each language, using either FFT or filterbank features as inputs. When applied to Spanish and Turkish, CNNs show little drop in accuracy, while their F1 score is higher in the test languages than in the training language; the difference between overall accuracy and F1 score is apparently an artifact of the highly non-uniform class distribution in Turkish and Spanish, both of which have twice as many voiced as unvoiced obstruents (see Table 3).\nCNNs vs. Acoustic Cues: Since the evaluation on two models (CNN+FFT and CNN+FB) results in similar accuracy scores, we consider CNN+FFT as the best CNN model. The last columns in Figs. 2, 3, and 4 illustrate that the best CNN model outperforms acoustic cues across three languages, in both metrics, and that it generalizes well to cross-lingual corpora."}, {"heading": "6. CONCLUSION", "text": "In this work, three different features are applied to build consonant voicing detectors, in order to test the theory that distinctive feature-based classes are robust over multilingual corpora. MFCCs (in landmark region, and averaged over the whole phone utterance duration), acoustic features extracted from the landmark region, and features learned by a convolutional neural network (CNN) were tested as features. Classifiers based on these features are all trained on English and tested on English, Spanish, and Turkish. Results show that MFCCs could not capture voicing in either the training language or test languages. Manual acoustic features generalize better to novel languages than MFCC. Acoustic features learned by a CNN obtain best performance, both on training languages and nontraining languages. We conclude that features capable of representing long-term spectral dynamics relative to a phonetic landmark (CNN and landmark-based features) are able to generalize cross-lingually with little or no loss of accuracy."}, {"heading": "7. References", "text": "[1] K. N. Stevens, \u201cToward a model for lexical access based on acous-\ntic landmarks and distinctive features,\u201d The Journal of the Acoustical Society of America, vol. 114, no. 4, pp. 872\u20131891, 2002.\n[2] \u2014\u2014, \u201cModelling affricate consonants,\u201d Speech Communication, vol. 13, no. 1, pp. 33\u201343, 1993.\n[3] C. H. Shadle, \u201cThe acoustics of fricative consonants,\u201d RLE Technical Report No. 506, MIT, Cambridge,MA, 1985.\n[4] F. Bell-Berti, \u201cControl of pharyngeal cavity size for english voiced and voiceless stops,\u201d he Journal of the Acoustical Society of America, vol. 57, no. 2, pp. 456\u2013461, 1975.\n[5] C. S. Crowther1 and V. Mann1, \u201cNative language factors affecting use of vocalic cues to final consonant voicing in english,\u201d The Journal of the Acoustical Society of America, vol. 92, no. 2, pp. 711\u2013722, 1992.\n[6] J.-Y. Choi, \u201cDetection of consonant voicing: A module for a hierarchical speech recognition system,\u201d The Journal of the Acoustical Society of America, vol. 106, no. 4, pp. 2274\u20132274, 1999.\n[7] P. Auzou, C. Ozsancak, R. J. Morris, M. Jan, F. Eustache, and D. Hannequin, \u201cVoice onset time in aphasia, apraxia of speech and dysarthria: a review,\u201d Clinical Linguistics & Phonetics, vol. 14, no. 2, pp. 131\u2013150, 2000.\n[8] M. Cooke and O. Scharenborg, \u201cThe interspeech 2008 consonant challenge,\u201d in the 9th Annual Conference of the International Speech Communication Association. ISCA Archive, 2008, pp. 1765\u20131768.\n[9] N. N. Bitar and C. Y. E. Wilson, \u201cSpeech parameterization based on phonetic features: application to speech recognition,\u201d in Fourth European Conference on Speech Communication and Technology, 1995.\n[10] S. Borys, \u201cAn svm front end landmark speech recognition system.\u201d\n[11] A. M. A. Ali, J. V. der Spiegel, and P. Mueller, \u201cAn acousticphonetic feature-based system for the automatic recognition of fricative consonants,\u201d Acoustics, Speech and Signal Processing, vol. 2, pp. 961\u2013964, 1998.\n[12] \u2014\u2014, \u201cAcoustic-phonetic features for the automatic classification of stop consonants,\u201d IEEE Transactions on Speech and Audio Processing, vol. 9, no. 8, pp. 833\u2013841, 2001.\n[13] Y. LeCun, Y. Bengio, and G. Hinton, \u201cDeep learning,\u201d Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.\n[14] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.\n[15] X. Yang, X. Kong, M. Hasegawa-Johnson, and Y. Xie, \u201cLandmark-based pronunciation error identification on chinese learning,\u201d submitted in Speech Prosody, 2016.\n[16] D. Talkin, \u201cA robust algorithm for pitch tracking (rapt),\u201d Speech coding and synthesis, vol. 495, p. 518, 1995.\n[17] K. N. Stevens and D. H. Klatt, \u201cRole of formant transitions in the voicedvoiceless distinction for stops,\u201d The Journal of the Acoustical Society of America, vol. 55, no. 3, pp. 653\u2013659, 1974.\n[18] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.\n[19] J. S. Garofalo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, D. S. Pallett, and N. L. Dahlgren, \u201cThe darpa timit acoustic-phonetic continuous speech corpus cdrom,\u201d Linguistic Data Consortium, 1993.\n[20] O\u0308. Salor, B. L. Pellom, T. Ciloglu, K. Hacioglu, and M. Demirekler, \u201cOn developing new text and audio corpora and speech recognition tools for the turkish language.\u201d in INTERSPEECH. Citeseer, 2002."}], "references": [{"title": "Toward a model for lexical access based on acoustic landmarks and distinctive features", "author": ["K.N. Stevens"], "venue": "The Journal of the Acoustical Society of America, vol. 114, no. 4, pp. 872\u20131891, 2002.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1891}, {"title": "Modelling affricate consonants", "author": ["\u2014\u2014"], "venue": "Speech Communication, vol. 13, no. 1, pp. 33\u201343, 1993.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1993}, {"title": "The acoustics of fricative consonants", "author": ["C.H. Shadle"], "venue": "RLE Technical Report No. 506, MIT, Cambridge,MA, 1985.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1985}, {"title": "Control of pharyngeal cavity size for english voiced and voiceless stops", "author": ["F. Bell-Berti"], "venue": "he Journal of the Acoustical Society of America, vol. 57, no. 2, pp. 456\u2013461, 1975.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1975}, {"title": "Native language factors affecting use of vocalic cues to final consonant voicing in english", "author": ["C.S. Crowther1", "V. Mann1"], "venue": "The Journal of the Acoustical Society of America, vol. 92, no. 2, pp. 711\u2013722, 1992.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1992}, {"title": "Detection of consonant voicing: A module for a hierarchical speech recognition system", "author": ["J.-Y. Choi"], "venue": "The Journal of the Acoustical Society of America, vol. 106, no. 4, pp. 2274\u20132274, 1999.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Voice onset time in aphasia, apraxia of speech and dysarthria: a review", "author": ["P. Auzou", "C. Ozsancak", "R.J. Morris", "M. Jan", "F. Eustache", "D. Hannequin"], "venue": "Clinical Linguistics & Phonetics, vol. 14, no. 2, pp. 131\u2013150, 2000.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "The interspeech 2008 consonant challenge", "author": ["M. Cooke", "O. Scharenborg"], "venue": "the 9th Annual Conference of the International Speech Communication Association. ISCA Archive, 2008, pp. 1765\u20131768.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Speech parameterization based on phonetic features: application to speech recognition", "author": ["N.N. Bitar", "C.Y.E. Wilson"], "venue": "Fourth European Conference on Speech Communication and Technology, 1995.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "An acousticphonetic feature-based system for the automatic recognition of fricative consonants", "author": ["A.M.A. Ali", "J.V. der Spiegel", "P. Mueller"], "venue": "Acoustics, Speech and Signal Processing, vol. 2, pp. 961\u2013964, 1998.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "Acoustic-phonetic features for the automatic classification of stop consonants", "author": ["\u2014\u2014"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 9, no. 8, pp. 833\u2013841, 2001.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Landmark-based pronunciation error identification on chinese learning", "author": ["X. Yang", "X. Kong", "M. Hasegawa-Johnson", "Y. Xie"], "venue": "submitted in Speech Prosody, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "A robust algorithm for pitch tracking (rapt)", "author": ["D. Talkin"], "venue": "Speech coding and synthesis, vol. 495, p. 518, 1995.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1995}, {"title": "Role of formant transitions in the voicedvoiceless distinction for stops", "author": ["K.N. Stevens", "D.H. Klatt"], "venue": "The Journal of the Acoustical Society of America, vol. 55, no. 3, pp. 653\u2013659, 1974.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1974}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "The darpa timit acoustic-phonetic continuous speech corpus cdrom", "author": ["J.S. Garofalo", "L.F. Lamel", "W.M. Fisher", "J.G. Fiscus", "D.S. Pallett", "N.L. Dahlgren"], "venue": "Linguistic Data Consortium, 1993.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1993}], "referenceMentions": [{"referenceID": 0, "context": "In contrast to the conventional data-driven speech recognition model, acoustic correlates of distinctive features are found in an acoustics phonetic recognizer [1] so as to extract interpretable acoustic information.", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "Obstruent consonants are further categorized by consonant voicing which can be described by the articulator-bound feature [stiff vocal folds] [2].", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "Shadle [3] studied fricative consonants using mechanical models, theoretical models, and acoustic analysis, and found that the most important parameters for fricatives are the length of the front cavity, the presence of an obstacle and the flow rate.", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "Speech production mechanism differences between voice and voiceless stops are mainly due to muscle activity, which relaxes the tongue root during voiced stops, altering aerodynamics near the vocal folds in order to maintain voicing during closure [4].", "startOffset": 247, "endOffset": 250}, {"referenceID": 4, "context": "vocalic duration and F1 offset frequency) are also correlates of consonant voicing [5].", "startOffset": 83, "endOffset": 86}, {"referenceID": 5, "context": "A module for detecting consonant voicing based on these acoustic correlates [6] first determines acoustic properties according to consonant production, then extracts acoustic cues, and classifies them to detect consonant voicing.", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": "One of the traditional methods to detect consonant voicing uses mel-frequency cepstral coefficients (MFCCs) [7, 8], e.", "startOffset": 108, "endOffset": 114}, {"referenceID": 7, "context": "One of the traditional methods to detect consonant voicing uses mel-frequency cepstral coefficients (MFCCs) [7, 8], e.", "startOffset": 108, "endOffset": 114}, {"referenceID": 8, "context": "7% to 80% overall accuracy [9, 10].", "startOffset": 27, "endOffset": 34}, {"referenceID": 0, "context": "Landmarks [1] identify times when the acoustic patterns of the linguistically motivated distinctive features are most salient; acoustic cues extracted in the vicinity of landmarks may therefore be more informative for the classification of distinctive features than cues extracted from other times in the signal.", "startOffset": 10, "endOffset": 13}, {"referenceID": 9, "context": "To the best of our knowledge, the highest accuracy for voicing classification of obstruents uses acoustic features extracted with reference to phonetic landmarks, with accuracies of 95% and 96% [11, 12] for stops and fricatives respectively.", "startOffset": 194, "endOffset": 202}, {"referenceID": 10, "context": "To the best of our knowledge, the highest accuracy for voicing classification of obstruents uses acoustic features extracted with reference to phonetic landmarks, with accuracies of 95% and 96% [11, 12] for stops and fricatives respectively.", "startOffset": 194, "endOffset": 202}, {"referenceID": 11, "context": "Deep learning techniques transform raw data into multiple levels of abstraction by stacking multiple layers with non-linearities, thus learning complex features automatically [13].", "startOffset": 175, "endOffset": 179}, {"referenceID": 12, "context": "Though the accuracy of speech recognizers built from deep networks is high [14], results on the cross-language portability of deep networks include both positive and negative outcomes.", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "Landmarks [1] are defined as points in an utterance around which information about the underlying distinctive features may be extracted.", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "Four types of landmarks were proposed in [1]: Vowel (V), Consonant release (Cr), Consonant closure (Cc), and Glide (G).", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "Stevens [1] proposed that distinctive features obtained from closure and release landmark regions should be universal across languages.", "startOffset": 8, "endOffset": 11}, {"referenceID": 13, "context": "After finding landmark positions, we denote the landmark regions as follows [15]:", "startOffset": 76, "endOffset": 80}, {"referenceID": 8, "context": "For stop release segments, duration is the voice onset time (VOT) [9] which carries voicing information about English stops.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "Peak normalized value of the cross-correlation (PNCC): Increasing cross correlation value will exist when producing voiced consonants (stops, fricatives and affricates) [[1].", "startOffset": 170, "endOffset": 173}, {"referenceID": 14, "context": "PNCC is originally denoted in [16], and besides using normalized cross-correlation, we retain its peak, which captures cross correlation value transitions.", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "Formant transitions: Formant transitions [17] are different for voiced and unvoiced consonants.", "startOffset": 41, "endOffset": 45}, {"referenceID": 16, "context": "During backpropagation, a first-order gradient-based optimization method based on adaptive estimates of lower-order moments (Adam) [18] is used.", "startOffset": 131, "endOffset": 135}, {"referenceID": 17, "context": "English Corpus: TIMIT [19] corpus contains broadband recordings of 630 speakers of eight major dialects of American English, each reading ten phonetically rich sentences and includes time-aligned orthographic, phonetic and word transcriptions, as well as a 16-bit, 16kHz speech waveform file for each utterance.", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "Duration, formant, transition, PNCC and H1: a robust RAPT [16] algorithm for pitch tracking that is based on normalized cross-correlation and dynamic programming is applied using Wavesurfer.", "startOffset": 58, "endOffset": 62}], "year": 2016, "abstractText": "This paper tests the hypothesis that distinctive feature classifiers anchored at phonetic landmarks can be transferred crosslingually without loss of accuracy. Three consonant voicing classifiers were developed: (1) manually selected acoustic features anchored at a phonetic landmark, (2) MFCCs (either averaged across the segment or anchored at the landmark), and (3) acoustic features computed using a convolutional neural network (CNN). All detectors are trained on English data (TIMIT), and tested on English, Turkish, and Spanish (performance measured using F1 and accuracy). Experiments demonstrate that manual features outperform all MFCC classifiers, while CNN features outperform both. MFCC-based classifiers suffer an overall error rate increase of up to 96.1% when generalized from English to other languages. Manual features suffer only an up to 35.2% relative error rate increase, and CNN features actually perform the best on Turkish and Spanish, demonstrating that features capable of representing long-term spectral dynamics (CNN and landmark-based features) are able to generalize cross-lingually with little or no loss of accuracy.", "creator": "LaTeX with hyperref package"}}}