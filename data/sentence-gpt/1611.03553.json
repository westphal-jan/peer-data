{"id": "1611.03553", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2016", "title": "The Sum-Product Theorem: A Foundation for Learning Tractable Models", "abstract": "Inference in expressive probabilistic models is generally intractable, which makes them difficult to learn and limits their applicability. Sum-product networks are a class of deep models where, surprisingly, inference remains tractable even when an arbitrary number of hidden layers are present. In this paper, we generalize this result to a much broader set of learning problems: all those where inference consists of summing a function over a semiring. This includes satisfiability, constraint satisfaction, optimization, integration, and others. In any semiring, for summation to be tractable it suffices that the factors of every product have disjoint scopes. This unifies and extends many previous results in the literature. Enforcing this condition at learning time thus ensures that the learned models are tractable. We illustrate the power and generality of this approach by applying it to a new type of structured prediction problem: learning a nonconvex function that can be globally optimized in polynomial time. We show empirically that this greatly outperforms the standard approach of learning without regard to the cost of optimization. We explain how this process may be used in a nonconvex function, where it requires a finite amount of information in order to implement a given prediction. Here, the task of solving a new constraint on a large subset of an algorithm is to derive an explicit description of what the constraint has. The problem is that the task itself will be hard to implement without specifying its exact identity. This is the key issue with the problem: one has to define an invariant function of the function itself. For example, the constraint must be an invariant in order to be a true function, which is a true function. But, this is not a problem with this approach. For example, if we choose to define the value of a product, we can do that by assigning the value of a given constraint to its object. That is, we define a constant. So, in the case of a continuous function we have a constant of 0.5x which is a true function. The constraint must be an invariant for the value of the parameter at the end. We can then use this constraint to build on the initialization of a given constraint. The result is a more concise representation of the constraint.\n\n\nIn this example, the constraint must be an invariant in order to be a true function. The constraint must be an invariant in order to be a true function. The constraint must be an invariant in order to be", "histories": [["v1", "Fri, 11 Nov 2016 00:46:33 GMT  (267kb,D)", "http://arxiv.org/abs/1611.03553v1", "15 pages (10 body, 5 pages of appendices)"]], "COMMENTS": "15 pages (10 body, 5 pages of appendices)", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["abram l friesen", "pedro m domingos"], "accepted": true, "id": "1611.03553"}, "pdf": {"name": "1611.03553.pdf", "metadata": {"source": "META", "title": "The Sum-Product Theorem: A Foundation for Learning Tractable Models", "authors": ["Abram L. Friesen", "Pedro Domingos"], "emails": ["AFRIESEN@CS.WASHINGTON.EDU", "PEDROD@CS.WASHINGTON.EDU"], "sections": [{"heading": "1. Introduction", "text": "Graphical models are a compact representation often used as a target for learning probabilistic models. Unfortunately, inference in them is exponential in their treewidth (Chandrasekaran et al., 2008), a common measure of complexity. Further, since inference is a subroutine of learning, graphical models are hard to learn unless restricted to those with low treewidth (Bach & Jordan, 2001; Chechetka & Guestrin, 2007), but few real-world problems exhibit this property. Recent research, however, has shown that probabilistic models can in fact be much more expressive than this while remaining tractable (Domingos et al., 2014). In particular, sum-product networks (SPNs) (Gens & DominProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\ngos, 2013; Poon & Domingos, 2011) are a class of deep probabilistic models that consist of many layers of hidden variables and can have unbounded treewidth. Despite this, inference in SPNs is guaranteed to be tractable, and their structure and parameters can be effectively and accurately learned from data (Gens & Domingos, 2012; 2013; Rooshenas & Lowd, 2014).\nIn this paper, we generalize and extend the ideas behind SPNs to enable learning tractable high-treewidth representations for a much wider class of problems, including satisfiability, MAX-SAT, model counting, constraint satisfaction, marginal and MPE inference, integration, nonconvex optimization, database querying, and first-order probabilistic inference. The class of problems we address can be viewed as generalizing structured prediction beyond combinatorial optimization (Taskar et al., 2005), to include optimization for continuous models and others. Instead of approaching each domain individually, we build on a long line of work showing how, despite apparent differences, these problems in fact have much common structure (e.g., Bistarelli et al. (1997); Dechter (1999); Aji & McEliece (2000); Wilson (2005); Green et al. (2007); Dechter & Mateescu (2007); Bacchus et al. (2009)); namely, that each consists of summing a function over a semiring. For example, in the Boolean semiring the sum and product operations are disjunction and conjunction, and deciding satisfiability is summing a Boolean formula over all truth assignments. MPE inference is summation over all states in the max-product semiring, etc.\nWe begin by identifying and proving the sum-product theorem, a unifying principle for tractable inference that states a simple sufficient condition for summation to be tractable in any semiring: that the factors of every product have disjoint scopes. In \u201cflat\u201d representations like graphical models and conjunctive normal form, consisting of a single product of sums, this would allow only trivial models; but in deep representations like SPNs and negation normal form it provides remarkable flexibility. Based on the sum-product theorem, we develop an algorithm for learning representations that satisfy this condition, thus guaranteeing that the learned functions are tractable yet expressive. We demonstrate the power and generality of our approach by applying it to a new type of structured prediction problem: learning\nar X\niv :1\n61 1.\n03 55\n3v 1\n[ cs\n.L G\n] 1\n1 N\nov 2\n01 6\na nonconvex function that can be optimized in polynomial time. Empirically, we show that this greatly outperforms the standard approach of learning a continuous function without regard to the cost of optimizing it. We also show that a number of existing and novel results are corollaries of the sum-product theorem, propose a general algorithm for inference in any semiring, define novel tractable classes of constraint satisfaction problems, integrable and optimizable functions, and database queries, and present a much simpler proof of the tractability of tractable Markov logic."}, {"heading": "2. The sum-product theorem", "text": "We begin by introducing our notation and defining several important concepts. We denote a vector of variables by X = (X1, . . . , Xn) and its value by x = (x1, . . . , xn) for xi \u2208 Xi for all i, where Xi is the domain of Xi. We denote subsets (for simplicity, we treat tuples as sets) of variables as XA,Xa \u2286 X, where the domains XA,Xa are the Cartesian product of the domains of the variables in XA,Xa, respectively. We denote (partial) assignments as a \u2208 XA and restrictions of these to XB \u2282 XA as aB . To indicate compatibility between a \u2208 XA and c \u2208 XC (i.e., that aj = cj for all Xj \u2208 XA \u2229XC), we write a \u223c c. The scope of a function is the set of variables it takes as input. Definition 1. A commutative semiring (R,\u2295,\u2297, 0, 1) is a nonempty set R on which the operations of sum (\u2295) and product (\u2297) are defined and satisfy the following conditions: (i) (R,\u2295) and (R,\u2297) are associative and commutative, with identity elements 0, 1 \u2208 R such that 0 6= 1, a\u2295 0 = a, and a\u2297 1 = a for all a \u2208 R; (ii) \u2297 distributes over \u2295, such that a \u2297 (b \u2295 c) = (a \u2297 b) \u2295 (a \u2297 c) for all a, b, c \u2208 R; and (iii) 0 is absorbing for \u2297, such that a\u2297 0 = 0 for all a \u2208 R. We are interested in computing summations \u2295 x\u2208X F (x), for (R,\u2295,\u2297, 0, 1) a commutative semiring and F : X \u2192 R a function on that semiring, with X a finite set (but see Section 5.4 for extensions to continuous variables). We refer to such a function as a sum-product function. Definition 2. A sum-product function (SPF) over (R,X,\u03a6), whereR is a semiring, X is a set of variables, and \u03a6 is a set of constant (\u03c6l \u2208 R) and univariate functions (\u03c6l : Xj \u2192 R for Xj \u2208 X), is any of the following: (i) a function \u03c6l \u2208 \u03a6, (ii) a product of SPFs, or (iii) a sum of SPFs.\nAn SPF S(X) computes a mapping S : X \u2192 R and can be represented by a rooted directed acyclic graph (DAG), where each leaf node is labeled with a function \u03c6l \u2208 \u03a6 and each non-leaf node is labeled with either \u2295 or \u2297 and referred to as a sum or product node, respectively. Two SPFs are compatible iff they compute the same mapping; i.e., S1(x) = S2(x) for all x \u2208 X , where S1(X) and S2(X) are SPFs. The size of an SPF is the number of edges in the graph. The DAG rooted at each node v \u2208 S represents a\nsub-SPF Sv : Xv \u2192 R for Xv \u2286 X. Notice that restricting the leaf functions \u03c6l to be univariate incurs no loss of generality because any mapping \u03c8 : X \u2192 R is compatible with the trivial SPF F (X) = \u2295 x\u2208X (\u03c8(x)\u2297 \u2297n i=1[Xi = xi]), where the indicator function [.] has value 1 when its argument is true, and 0 otherwise (recall that 0 and 1 are the semiring identity elements). SPFs are similar to arithmetic circuits (Shpilka & Yehudayoff, 2010), but the leaves of an SPF are functions instead of variables. Darwiche (2003) used arithmetic circuits as a data structure to support inference in Bayesian networks over discrete variables. An important subclass of SPFs are those that are decomposable. Definition 3. A product node is decomposable iff the scopes of its children are disjoint. An SPF is decomposable iff all of its product nodes are decomposable.\nDecomposability is a simple condition that defines a class of functions for which inference is tractable. Theorem 1 (Sum-product theorem). Every decomposable SPF can be summed in time linear in its size. Proof. The proof is recursive, starting from the leaves of the SPF. Let S(X) be a decomposable SPF on commutative semiring (R,\u2295,\u2297, 0, 1). Every leaf node can be summed in constant time, because each is labeled with either a constant or univariate function. Now, let v \u2208 S be a node, with Sv(Xv) the sub-SPF rooted at v and Zv = \u2295 Xv Sv(Xv) its summation. Let {ci} be the children of v for ci \u2208 S, with sub-SPFs Si(Xi) for Xi \u2286 Xv and summations Zi. Let Xv\\i be the domain of variables Xv\\Xi. If v is a sum node, then Zv = \u2295 Xv \u2295 i Si(Xi) = \u2295 i \u2295 Xv Si(Xi) =\u2295\ni \u2295 Xv\\i \u2295 Xi Si(Xi) =\n\u2295 i Zi \u2297 (\u2295 Xv\\i 1 ) .\nIf v is a product node, then any two children ci, cj for i, j \u2208 {1, . . . ,m} have disjoint scopes, Xi \u2229 Xj = \u2205, and Zv = \u2295 Xv \u2297\ni Si(Xi) =\u2295 X1 \u2295 Xv\\1 \u2297 i Si(Xi) = \u2295 X1 S1(X1) \u2297\u2295\nXv\\1 \u2297m i=2 Si(Xi) = \u2297 i \u2295 Xi Si(Xi) = \u2297 i Zi. The above equations only require associativity and commutativity of\u2295 and associativity and distributivity of\u2297, which are properties of a semiring. Thus, any node can be summed over its domain in time linear in the number of its children, and S can be summed in time linear in its size.\nWe assume here that \u2295 Xv\\i 1 can be computed in constant time and that each leaf function can be evaluated in constant time, which is true for all semirings considered. We also assume that a\u2295 b and a\u2297 b take constant time for any elements a, b of semiring R, which is true for most common semirings. See Appendix A for details.\nThe complexity of summation in an SPF can be related to other notions of complexity, such as treewidth, the most common and relevant complexity measure across the domains we consider. To define the treewidth of an SPF, we first define junction trees (Lauritzen & Spiegelhalter, 1988; Aji & McEliece, 2000) and a related class of SPFs.\nDefinition 4. A junction tree over variables X is a tuple (T,Q), where T is a rooted tree, Q is a set of subsets of variables, each vertex i \u2208 T contains a subset of variables Ci \u2208 Q such that \u222aiCi = X, and for every pair of vertices i, j \u2208 T and for all k \u2208 T on the (unique) path from i to j, Ci \u2229Cj \u2286 Ck. The separator for an edge (i, j) \u2208 T is defined as Sij = Ci \u2229Cj . A junction tree provides a schematic for constructing a specific type of decomposable SPF called a tree-like SPF (a semiring-generalized version of a construction from Darwiche (2003)). Note that a tree-like SPF is not a tree, however, as many of its nodes have multiple parents. Definition 5. A tree-like SPF over variables X is constructed from a junction tree T = (T,Q) and functions {\u03c8i(Ci)} where Ci \u2208 Q and i \u2208 T , and contains the following nodes: (i) a node \u03c6vt with indicator \u03c6t(Xv) = [Xv = t] for each value t \u2208 Xv of each variable Xv \u2208 X; (ii) a (leaf) node ai with value \u03c8i(ci) and a product node ci for each value ci \u2208 XCi of each cluster Ci; (iii) a sum node sij for each value sij \u2208 XSij of each separator Sij , and (iv) a single root sum node s.\nA product node cj and a sum node sij are compatible iff their corresponding values are compatible; i.e., cj \u223c sij . The nodes are connected as follows. The children of the root s are all product nodes cr for r the root of T . The children of product node cj are all compatible sum nodes sij for each child i of j, the constant node aj with value \u03c8j(cj), and all indicator nodes \u03c6vt such that Xv \u2208 Cj , t \u223c cj , and Xv /\u2208 Ck for k any node closer to the root of T than j. The children of sum node sij are the compatible product nodes ci of child i of j connected by separator Sij .\nIf S is a tree-like SPF with junction tree (T,Q), then it is not difficult to see both that S is decomposable, since the indicators for each variable all appear at the same level, and that each sum node sjk computes Ssjk(Sjk) =\u2295\n(c\u2208XCj )\u223csjk \u03c8j(c)\u2297[Cj = c]\u2297\n(\u2297 i\u2208Ch(j) Ssij (cSij ) ) ,\nwhere the indicator children of cj have been combined into [Cj = c], Ch(j) are the children of j, and i, j, k \u2208 T with j the child of k. Further, S(x) = \u2297 i\u2208T \u03c8i(xCi) for any x \u2208 X . Thus, tree-like SPFs provide a method for decomposing an SPF. For a tree-like SPF to be compatible with an SPF F , it cannot assert independencies that do not hold in F . Definition 6. Let F (U) be an SPF over variables U with pairwise-disjoint subsets X,Y,W \u2286 U. Then X and Y are conditionally independent in F given W iff F (X,Y,w) = F (X,w)\u2297F (Y,w) for all w \u2208 W , where F (X) = \u2295 Y F (X,Y) for {X,Y} a partition of U.\nSimilarly, a junction tree T = (T,Q) is incompatible with F if it asserts independencies that are not in F , where variables X and Y are conditionally independent in T given W if W separates X from Y . A set of variables W separates X and Y in T iff after removing all vertices {i \u2208 T :\nCi \u2286W} from T there is no pair of vertices i, j \u2208 T such that X \u2208 Ci, Y \u2208 Cj , and i, j are connected. Inference complexity is commonly parameterized by treewidth, defined for a junction tree T = (T,Q) as the size of the largest cluster minus one; i.e., tw(T ) = maxi\u2208T |Ci| \u2212 1. The treewidth of an SPF S is the minimum treewidth over all junction trees compatible with S. Notice that these definitions of junction tree and treewidth reduce to the standard ones (Kask et al., 2005). If the treewidth of S is bounded then inference in S is efficient because there must exist a compatible tree-like SPF that has bounded treewidth. Note that the trivial junction tree with only a single cluster is compatible with every SPF. Corollary 1. Every SPF with bounded treewidth can be summed in time linear in the cardinality of its scope.\nDue to space limitations, all other proofs are provided in Appendix H. For any SPF, tree-like SPFs are just one type of compatible SPF, one with size exponential in treewidth; however, there are many other compatible SPFs. In fact, there can be compatible (decomposable) SPFs that are exponentially smaller than any compatible tree-like SPF. Corollary 2. Not every SPF that can be summed in time linear in the cardinality of its scope has bounded treewidth.\nGiven existing work on tractable high-treewidth inference, it is perhaps surprising that the above results do not exist in the literature at this level of generality. Most relevant is the preliminary work of Kimmig et al. (2012), which proposes a semiring generalization of arithmetic circuits for knowledge compilation and does not address learning. Their main results show that summation of circuits that are both decomposable and either deterministic or based on an idempotent sum takes time linear in their size, whereas we show that decomposability alone is sufficient, a much weaker condition. In fact, over the same set of variables, deterministic circuits may be exponentially larger and are never smaller than non-deterministic circuits (Darwiche & Marquis, 2002; Kimmig et al., 2012). We note that while decomposable circuits can be made deterministic by introducing hidden variables, this does not imply that these properties are equivalent.\nEven when restricted to specific semirings, such as those for logical and probabilistic inference (e.g., Darwiche (2001b; 2003); Poon & Domingos (2011)), some of our results have not previously been shown formally, although some have been foreshadowed informally. Further, existing semiring-specific results (discussed further below) do not make it clear that the semiring properties are all that is required for tractable high-treewidth inference. Our results are thus simpler and more general. Further, the sumproduct theorem provides the basis for general algorithms for inference in arbitrary SPFs (Section 3) and for learn-\ning tractable high-treewidth representations (i.e., decomposable SPFs) in any semiring (Section 4)."}, {"heading": "3. Inference in non-decomposable SPFs", "text": "Inference in arbitrary SPFs can be performed in a variety of ways, some more efficient than others. We present an algorithm for summing an SPF that adapts to the structure of the SPF and can thus take exponentially less time than constructing and summing a compatible tree-like SPF (Bacchus et al., 2009), which imposes a uniform decomposition structure. SPF S with root node r is summed by calling SUMSPF(r), for which pseudocode is shown in Algorithm 1. SUMSPF is a simple recursive algorithm for summing an SPF (note the similarity between its structure and the proof of the sum-product theorem). If S is decomposable, then SUMSPF simply recurses to the bottom of S, sums the leaf functions, and evaluates S in an upward pass. If S is not decomposable, SUMSPF decomposes each product node it encounters while summing S.\nAlgorithm 1 Sum an SPF. Input: node v, the root of the sub-SPF Sv(Xv) Output: sum, which is equal to \u2295 v\u2208Xv Sv(v)\n1: function SUMSPF(v) 2: if \u3008v, sum\u3009 in cache then return sum 3: if v is a sum node then // Xv\\c = Xv\\Xc 4: sum\u2190 \u2295 c\u2208Ch(v) SUMSPF(c)\u2297 \u2295 Xv\\c 1 5: else if v is a product node then 6: if v is decomposable then 7: sum\u2190 \u2297 c\u2208Ch(v) SUMSPF(c) 8: else 9: sum\u2190 SUMSPF( DECOMPOSE(v) ) 10: else // v is a leaf with constant a or function \u03c6v 11: if v is a constant then sum\u2190 a 12: else sum\u2190 \u2295 xj\u2208Xj \u03c6v(xj) 13: cache \u3008v, sum\u3009 14: return sum\nAlgorithm 2 Decompose a product node. Input: product node v, with children {c} Output: node s, such that its children are decomposable\nwith respect to Xt and Ss, Sv are compatible 1: function DECOMPOSE(v) 2: Xt\u2190 choose var. that appears in multiple Xc 3: Xv\\t \u2190Xv\\{Xt} 4: s\u2190 create new sum node 5: for all xi \u2208 Xt do 6: create simplified Svi(Xv\\t)\u2190 Sv(Xv\\t, xi) 7: set vi as child of s // vi is the root of Svi 8: set f(Xt) = [Xt = xi] as child of vi 9: set s as a child of each of v\u2019s parents\n10: remove v and all edges containing v 11: return s\nDecomposition can be achieved in many different ways, but we base our method on a common algorithmic pattern that already occurs in many of the inference problems we consider, resulting in a general, semiring-independent algorithm for summing any SPF. DECOMPOSE, shown in Algorithm 2, chooses a variable Xt that appears in the scope of multiple of v\u2019s children; creates |Xt| partially assigned and simplified copies Svi of the sub-SPF Sv for Xt assigned to each value xi \u2208 Xt; multiplies each Svi by an indicator to ensure that only one is ever non-zero when S is evaluated; and then replaces v with a sum over {Svi}. Any node u \u2208 Sv that does not have Xt in its scope is re-used across each Svi , which can drastically limit the amount of duplication that occurs. Furthermore, each Svi is simplified by removing any nodes that became 0 when setting Xt = xi. Variables are chosen heuristically; a good heuristic minimizes the amount of duplication that occurs. Similarly, SUMSPF heuristically orders the children in lines 4 and 7. A good ordering will first evaluate children that may return an absorbing value (e.g., 0 for \u2297) because SUMSPF can break out of these lines if this occurs. In general, decomposing an SPF is hard, and the resulting decomposed SPF may be exponentially larger than the input SPF, although good heuristics can often avoid this. Many extensions to SUMSPF are also possible, some of which we detail in later sections. Understanding inference in non-decomposable SPFs is important for future work on extending SPF learning to even more challenging classes of functions, particularly those without obvious decomposability structure."}, {"heading": "4. Learning tractable representations", "text": "Instead of performing inference in an intractable model, it can often be simpler to learn a tractable representation directly from data (e.g., Bach & Jordan (2001); Gens & Domingos (2013)). The general problem we consider is that of learning a decomposable SPF S : X \u2192 R on a semiring (R,\u2295,\u2297, 0, 1) from a set of i.i.d. instances T = {(x(i), y(i))} drawn from a fixed distribution DX\u00d7R, where y(i) = \u2295 Z F (x\n(i),Z), F is some (unknown) SPF, and Z is a (possibly empty) set of unobserved variables or parameters, such that S(x(i)) \u2248 y(i), for all i. After learning, \u2295 X S(X) can be computed efficiently. In the sum-product semiring, this corresponds to summation (or integration), for which estimation of a joint probability distribution over X is a special case.\nFor certain problems, such as constraint satisfaction or MPE inference, the desired quantity is the argument of the sum. This can be recovered (if meaningful in the current semiring) from an SPF by a single downward pass that recursively selects all children of a product node and the (or a) active child of a sum node (e.g., the child with the smallest value if minimizing). Learning for this domain corresponds to a generalization of learning for structured predic-\nAlgorithm 3 Learn a decomposable SPF from data. Input: a dataset T = {(x(i), y(i))} over variables X Input: integer thresholds t, v > 0 Output: S(X), an SPF over the input variables X\n1: function LEARNSPF(T,X) 2: if |T | \u2264 t or |X| \u2264 v then 3: estimate S(X) such that S(x(i)) \u2248 y(i) for all i 4: else 5: decompose X into disjoint subsets {Xi} 6: if |{Xi}| > 1 then 7: S(X)\u2190 \u2297 i LEARNSPF(T,Xi) 8: else 9: cluster T into subsets of similar instances {Tj}\n10: S(X)\u2190 \u2295\nj LEARNSPF(Tj ,X) 11: return S(X)\ntion (Taskar et al., 2005). Formally, the problem is to learn an SPF S from instances T = {(x(i),y(i))}, where y(i) = arg\u2295y\u2208Y F (x(i),y), such that arg\u2295y\u2208Y S(x(i),y) \u2248 y(i), for all i. Here, x(i) can be an arbitrarily structured input and inference is over the variables Y. Both of the above learning problems can be solved by the algorithm schema we present, with minor differences in the subroutines. We focus here on the former but discuss the latter below, alongside experiments on learning nonconvex functions that, by construction, can be efficiently optimized.\nAs shown by the sum-product theorem, the key to tractable inference is to identify the decomposability structure of an SPF. The difficulty, however, is that in general this structure varies throughout the space. For example, as a protein folds there exist conformations of the protein in which two particular amino acids are energetically independent (decomposable), and other conformations in which these amino acids directly interact, but in which other amino acids may no longer interact. This suggests a simple algorithm, which we call LEARNSPF (shown in Algorithm 3), that first tries to identify a decomposable partition of the variables and, if successful, recurses on each subset of variables in order to find finer-grained decomposability. Otherwise, LEARNSPF clusters the training instances, grouping those with analogous decomposition structure, and recurses on each cluster. Once either the set of variables is small enough to be summed over (in practice, unary leaf nodes are rarely necessary) or the number of instances is too small to contain meaningful statistical information, LEARNSPF simply estimates an SPF S(X) such that S(x(i)) \u2248 y(i) for all i in the current set of instances. LEARNSPF is a generalization of LearnSPN (Gens & Domingos, 2013), a simple but effective SPN structure learning algorithm.\nLEARNSPF is actually an algorithm schema that can be instantiated with different variable partitioning, clustering, and leaf creation subroutines for different semirings and problems. To successfully decompose the vari-\nables, LEARNSPF must find a partition {X1,X2} of the variables X such that \u2295 X S(X) \u2248 ( \u2295 X1 S1(X1)) \u2297\n( \u2295 X2 S2(X2)). We refer to this as approximate decomposability. In probabilistic inference, mutual information or pairwise independence tests can be used to determine decomposability (Gens & Domingos, 2013). For our experiments, decomposable partitions correspond to the connected components of a graph over the variables in which correlated variables are connected. Instances can be clustered by virtually any clustering algorithm, such as a naive Bayes mixture model or k-means, which we use in our experiments. Instances can also be split by conditioning on specific values of the variables, as in SUMSPF or in a decision tree. Similarly, leaf functions can be estimated using any appropriate learning algorithm, such as linear regression or kernel density estimation.\nIn Section 6, we present preliminary experiments on learning nonconvex functions that can be globally optimized in polynomial time. However, this is just one particular application of LEARNSPF, which can be used to learn a tractable representation for any problem that consists of summation over a semiring. In the following section, we briefly discuss common inference problems that correspond to summing an SPF on a specific semiring. For each, we demonstrate the benefit of the sum-product theorem, relate its core algorithms to SUMSPF, and specify the problem solved by LEARNSPF. Additional details and semirings can be found in the Appendix. Table 1 provides a summary of some of the relevant inference problems."}, {"heading": "5. Applications to specific semirings", "text": ""}, {"heading": "5.1. Logical inference", "text": "Consider the Boolean semiring B = (B,\u2228,\u2227, 0, 1), where B = {0, 1}, \u2228 is logical disjunction (OR), and \u2227 is logical conjunction (AND). If each variable is Boolean and leaf functions are literals (i.e., each \u03c6l(Xj) is Xj or \u00acXj , where \u00ac is logical negation), then SPFs on B correspond exactly to negation normal form (NNF), a DAG-based representation of a propositional formula (sentence) (Barwise, 1982). An NNF can be exponentially smaller than the same sentence in a standard (flat) representation such as conjunctive or disjunctive normal form (CNF or DNF), and is never larger (Darwiche & Marquis, 2002). Summation of an NNF F (X) on B is \u2228 X F (X), which corresponds to propositional satisfiability (SAT): the problem of determining if there exists a satisfying assignment for F . Thus, the tractability of SAT for decomposable NNFs follows directly from the sum-product theorem. Corollary 3 (Darwiche, 2001b). The satisfiability of a decomposable NNF is decidable in time linear in its size. Satisfiability of an arbitrary NNF can be determined either by decomposing the NNF or by expanding it to a CNF and using a SAT solver. DPLL (Davis et al., 1962), the stan-\ndard algorithm for solving SAT, is an instance of SUMSPF (see also Huang & Darwiche (2007)). Specifically, DPLL is a recursive algorithm that at each level chooses a variable X \u2208 X for CNF F (X) and computes F = F |X=0\u2228F |X=1 by recursing on each disjunct, where F |X=x is F with X assigned value x. Thus, each level of recursion of DPLL corresponds to a call to DECOMPOSE.\nLearning in the Boolean semiring is a well-studied area, which includes problems from learning Boolean circuits (Jukna, 2012) (of which decomposable SPFs are a restricted subclass, known as syntactically multilinear circuits) to learning sets of rules (Rivest, 1987). However, learned rule sets are typically encoded in large CNF knowledge bases, making reasoning over them intractable. In contrast, decomposable NNF is a tractable but expressive formalism for knowledge representation that supports a rich class of polynomial-time logical operations, including SAT (Darwiche, 2001b). Thus, LEARNSPF in this semiring provides a method for learning large, complex knowledge bases that are encoded in decomposable NNF and therefore support efficient querying, which could greatly benefit existing rule learning systems."}, {"heading": "5.2. Constraint satisfaction.", "text": "A constraint satisfaction problem (CSP) consists of a set of constraints {Ci} on variables X, where each constraint Ci(Xi) specifies the satisfying assignments to its variables. Solving a CSP consists of finding an assignment to X that satisfies each constraint. When constraints are functions Ci : Xi \u2192 B that are 1 when Ci is satisfied and 0 otherwise, then F (X) = \u2227 i Ci(Xi) = \u2227 i \u2228 xi\u2208Xi ( Ci(xi) \u2227 [Xi =\nxi] ) = \u2227 i \u2228 xi\u2208Xi ( Ci(xi) \u2227 \u2227 Xt\u2208Xi [Xt = xit] ) is a\nCSP and F is an SPF on the Boolean semiring B, i.e., an OR-AND network (OAN), a generalization of NNF, and a decomposable CSP is one with a decomposable OAN. Solving F corresponds to computing \u2228 X F (X), which is summation on B (see also Bistarelli et al. (1997); Chang & Mackworth (2005); Rollon et al. (2013)). The solution for F can be recovered with a downward pass that recursively selects the (or a) non-zero child of an OR node, and all children of an AND node. Corollary 4 follows immediately. Corollary 4. Every decomposable CSP can be solved in time linear in its size. Thus, for inference to be efficient it suffices that the CSP be expressible by a tractably-sized decomposable OAN; a much weaker condition than that of low treewidth. Like DPLL, backtracking-based search algorithms (Kumar, 1992) for CSPs are also instances of SUMSPF (see also Mateescu & Dechter (2005)). Further, SPFs on a number of other semirings correspond to various extensions of CSPs, including fuzzy, probabilistic, and weighted CSPs (see Table 1 and Bistarelli et al. (1997)).\nLEARNSPF for CSPs addresses a variant of structured prediction (Taskar et al., 2005); specifically, learning a function F : X \u2192 B such that arg \u2228 y F (x (i),y) \u2248 y(i) for training data {(x(i),y(i))}, where x(i) is a structured object representing a CSP and y(i) is its solution. LEARNSPF solves this problem while guaranteeing that the learned CSP remains tractable. This is a much simpler and more attractive approach than existing constraint learning methods such as Lallouet et al. (2010), which uses inductive logic programming and has no tractability guarantees."}, {"heading": "5.3. Probabilistic inference", "text": "Many probability distributions can be compactly represented as graphical models: P (X) = 1Z \u220f i \u03c8i(Xi), where \u03c8i is a potential over variables Xi \u2286 X and Z is known as the partition function (Pearl, 1988). One of the main inference problems in graphical models is to compute the probability of evidence e \u2208 XE for variables XE \u2286 X, P (e) = \u2211 XE\nP (e,XE), where XE = X\\XE . The partition function Z is the unnormalized probability of empty evidence (XE = \u2205). Unfortunately, computing Z or P (e) is generally intractable. Building on a number of earlier works (Darwiche, 2003; Dechter & Mateescu, 2007; Bacchus et al., 2009), Poon & Domingos (2011) introduced sum-product networks (SPNs), a class of distributions in which inference is guaranteed to be tractable. An SPN is an SPF on the non-negative real sum-product semiring (R+,+,\u00d7, 0, 1). A graphical model is a flat SPN, in the same way that a CNF is a flat NNF (Darwiche & Marquis, 2002). For an SPN S, the unnormalized probability of evidence e \u2208 XE for variables XE is computed by replacing each leaf function \u03c6l \u2208 {\u03c6l(Xj) \u2208 S|Xj \u2208 XE} with the constant \u03c6l(ej) and summing the SPN. The corollary below follows immediately from the sum-product theorem. Corollary 5. The probability of evidence in a decomposable SPN can be computed in time linear in its size.\nA similar result (shown in Appendix C) for finding the most probable state of the non-evidence variables also follows from the sum-product theorem. One important consequence of the sum-product theorem is that decomposability is the sole condition required for an SPN to be tractable; previously, completeness was also required (Poon & Domingos, 2011; Gens & Domingos, 2013). This expands the range of tractable SPNs and simplifies the design of tractable representations based on them, such as tractable probabilistic knowledge bases (Domingos & Webb, 2012).\nMost existing algorithms for inference in graphical models correspond to different methods of decomposing a flat SPN, and can be loosely clustered into tree-based, conditioning, and compilation methods, all of which SUMSPF generalizes. Details are provided in Appendix C. LEARNSPF for SPNs corresponds to learning a probability distribution from a set of samples {(x(i), y(i))}. Note that y(i) in this case is defined implicitly by the empirical frequency of x(i) in the dataset. Learning the parameters and structure of SPNs is a fast-growing area of research (e.g., Gens & Domingos (2013); Rooshenas & Lowd (2014); Peharz et al. (2014); Adel et al. (2015)), and we refer readers to these references for more details."}, {"heading": "5.4. Integration and optimization", "text": "SPFs can be generalized to continuous (real) domains, where each variable Xi has domain Xi \u2286 R and the semiring set is a subset of R\u221e. For the sum-product theo-\nrem to hold, the only additional conditions are that (C1)\u2295 Xj \u03c6l(Xj) is computable in constant time for all leaf\nfunctions, and (C2) \u2295 Xv\\c 1 6=\u221e for all sum nodes v \u2208 S and all children c \u2208 Ch(v), where Xv\\c is the domain of Xv\\c = Xv\\Xc. Integration. In the non-negative real sum-product semiring (R+,+,\u00d7, 0, 1), summation of an SPF with continuous variables corresponds to integration over X . Accordingly, we generalize SPFs as follows. Let \u00b51, . . . , \u00b5n be measures over X1, . . . ,Xn, respectively, where each leaf function \u03c6l : Xj \u2192 R+ is integrable with respect to \u00b5j , which satisfies (C1). Summation (integration) of an SPF S(X) then corresponds to computing\u222b X S(X)d\u00b5 = \u222b X1 \u00b7 \u00b7 \u00b7 \u222b Xn S(X)d\u00b51 \u00b7 \u00b7 \u00b7 d\u00b5n. For (C2),\u2295\nXv\\c 1 = \u222b Xv\\c\n1 d\u00b5v\\c must be integrable for all sum nodes v \u2208 S and all children c \u2208 Ch(v), where d\u00b5v\\c =\u220f {j:Xj\u2208Xv\\c} d\u00b5j . We thus assume that either \u00b5v\\c has finite support over Xv\\c or that Xv\\c = \u2205. Corollary 6 follows immediately. Corollary 6. Every decomposable SPF of real variables can be integrated in time linear in its size. Thus, decomposable SPFs define a class of functions for which exact integration is tractable. SUMSPF defines a novel algorithm for (approximate) integration that is based on recursive problem decomposition, and can be exponentially more efficient than standard integration algorithms such as trapezoidal or Monte Carlo methods (Press et al., 2007) because it dynamically decomposes the problem at each recursion level and caches intermediate computations. More detail is provided in Appendix D.\nIn this semiring, LEARNSPF learns a decomposable continuous SPF S : X \u2192 R+ on samples {(x(i), y(i) = F (x(i)))} from an SPF F : X \u2192 R+, where S can be integrated efficiently over the domain X . Thus, LEARNSPF provides a novel method for learning and integrating complex functions, such as the partition function of continuous probability distributions.\nNonconvex optimization. Summing a continuous SPF in one of the min-sum, min-product, max-sum, or maxproduct semirings corresponds to optimizing a (potentially nonconvex) continuous objective function. Our results hold for all of these, but we focus here on the real minsum semiring (R\u221e,min,+,\u221e, 0), where summation of a min-sum function (MSF) F (X) corresponds to computing minX F (X). A flat MSF is simply a sum of terms. To satisfy (C1), we assume that minxj\u2208Xj \u03c6l(xj) is computable in constant time for all \u03c6l \u2208 F . (C2) is trivially satisfied for min. The corollary below follows immediately. Corollary 7. The global minimum of a decomposable MSF can be found in time linear in its size.\nSUMSPF provides an outline for a general nonconvex optimization algorithm for sum-of-terms (or product-of-\nfactors) functions. The recent RDIS algorithm for nonconvex optimization (Friesen & Domingos, 2015), which achieves exponential speedups compared to other algorithms, is an instance of SUMSPF where values are chosen via multi-start gradient descent and variables in DECOMPOSE are chosen by graph partitioning. Friesen & Domingos (2015), however, do not specify tractability conditions for the optimization; thus, Corollary 7 defines a novel class of functions that can be efficiently globally optimized.\nFor nonconvex optimization, LEARNSPF solves a variant of structured prediction (Taskar et al., 2005), in which the variables to predict are continuous instead of discrete (e.g., protein folding, structure from motion (Friesen & Domingos, 2015)). The training data is a set {(x(i),y(i))}, where x(i) is a structured object representing a nonconvex function and y(i) is a vector of values specifying the global minimum of that function. LEARNSPF learns a function S : X \u00d7Y \u2192 R\u221e such that arg miny\u2208Y S(x(i),y) \u2248 y(i), where the arg min can be computed efficiently because S is decomposable. More detail is provided in Section 6."}, {"heading": "6. Experiments", "text": "We evaluated LEARNSPF on the task of learning a nonconvex decomposable min-sum function (MSF) from a training set of solutions of instances of a highly-multimodal test function consisting of a sum of terms. By learning an MSF, instead of just a sum of terms, we learn the general mathematical form of the optimization problem in such a way that the resulting learned problem is tractable, whereas the original sum of terms is not. The test function we learn from is a variant of the Rastrigin function (Torn & Zilinskas, 1989), a standard highly-multimodal test function for global optimization consisting of a sum of multidimensional sinusoids in quadratic basins. The function, FX(Y) = F (Y;X), has parameters X, which determine the dependencies between the variables Y and the location of the minima. To test LEARNSPF, we sampled a dataset of function instances T = {(x(i),y(i))}mi=1 from a distribution over X \u00d7 Y , where y(i) = arg miny\u2208Y Fx(i)(y). LEARNSPF partitioned variables Y based on the connected components of a graph containing a node for each Yi \u2208 Y and an edge between two nodes only if Yi and Yj were correlated, as measured by Spearman rank correlation. Instances were clustered by running k-means on the values y(i). For this preliminary test, LEARNSPF did not learn the leaf functions of the learned min-sum function (MSF) M(Y); instead, when evaluating or minimizing a leaf node in M , we evaluated or minimized the test function with all variables not in the scope of the leaf node fixed to 0 (none of the optima were positioned at 0). This corresponds to having perfectly learned leaf nodes if the scopes of the leaf nodes accurately reflect the decomposability of F , otherwise a large error is incurred. We did this to study\nthe effect of learning the decomposability structure in isolation from the error due to learning leaf nodes. The function used for comparison is also perfectly learned. Thresholds t and v were set to 30 and 2, respectively.\nThe dataset was split into 300 training samples and 50 test samples, where minY Fx(i)(Y) = 0 for all i for comparison purposes. After training, we computed yM = arg minYM(Y) for each function in the test set by first minimizing each leaf function (with respect to only those variables in the scope of the leaf function) with multi-start L-BFGS (Liu & Nocedal, 1989) and then performing an upward and a downward pass in M . Figure 1 shows the result of minimizing the learned MSF M and evaluating the test function at yM (blue line) compared to running multistart L-BFGS directly on the test function and reporting the minimum found (red line), where both optimizations are run for the same fixed amount of time (one minute per test sample). LEARNSPF accurately learned the decomposition structure of the test function, allowing it to find much better minima when optimized, since optimizing many small functions at the leaves requires exploring exponentially fewer modes than optimizing the full function. Additional experimental details are provided in Appendix G."}, {"heading": "7. Conclusion", "text": "This paper developed a novel foundation for learning tractable representations in any semiring based on the sumproduct theorem, a simple tractability condition for all inference problems that reduce to summation on a semiring. With it, we developed a general inference algorithm and an algorithm for learning tractable representations in any semiring. We demonstrated the power and generality of our approach by applying it to learning a nonconvex function that can be optimized in polynomial time, a new type of structured prediction problem. We showed empirically that our learned function greatly outperforms a continuous function learned without regard to the cost of optimizing it. We also showed that the sum-product theorem specifies an exponentially weaker condition for tractability than low treewidth and that its corollaries include many previous results in the literature, as well as a number of novel results."}, {"heading": "Acknowledgments", "text": "This research was partly funded by ONR grants N0001413-1-0720 and N00014-12-1-0312, and AFRL contract FA8750-13-2-0019. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ONR, AFRL, or the United States Government."}, {"heading": "A. Decomposable SPF summation complexity", "text": "Let S(X) be a decomposable SPF with size |S| on commutative semiring (R,\u2295,\u2297, 0, 1), let d = |Xi| for all Xi \u2208 X where X = (X1, . . . , Xn), and let the cost of a \u2295 b and a \u2297 b for any elements a, b \u2208 R be c. Further, let e denote the complexity of evaluating any unary leaf function \u03c6j(Xi) in S and let k = maxv\u2208Ssum,j\u2208Ch(v) |Xv\\Xj | < n, where Ssum, Sprod, and Sleaf are the sum, product, and leaf nodes in S, respectively, and Ch(v) are the children of v. Then the complexity of computing \u2295 x\u2208X S(x) is |S| \u00b7 c+ |Sleaf| \u00b7 d(e+ c) + |Ssum| \u00b7 (c+ kdc).\nFor certain simple SPFs that have very little internal structure and many input variables, the worst case complexity of summing S can be quadratic in |S| and occurs in the rare and restrictive case where k = O(n) = O(|S|), due to the \u2295 Xv\\i 1 term at each sum node (see proof of Theorem 1). However, in any semiring with an idempotent sum (i.e., a \u2295 a = a for every a \u2208 R) such as the minsum or max-product semirings, this term is always equal to 1 and thus no computation is necessary. Alternatively, if the semiring supports multiplication and division as in the sum-product semiring then this complexity can be reduced by first computing the product over all variables and then dividing out as needed. If the semiring has neither of these properties, these identity summations can still be computed with a single preprocessing pass through the SPF since they are constants and independent of the input variables. For all semirings we\u2019ve studied, this quadratic cost does not occur, but we include it for completeness."}, {"heading": "B. Logical inference (continued)", "text": "Model counting. Model counting (#SAT) is the problem of computing the number of satisfying assignments of a Boolean formula. The model count of an NNF F can be obtained by translating it from the Boolean semiring to the counting sum-product semiring P = (N,+,\u00d7, 0, 1) (R+ is used instead for weighted #SAT), and then summing it. Definition 8. Translating an SPF from semiring (R,\u2295,\u2297, 0, 1) to semiring (R\u2032, , , 0\u2032, 1\u2032) with R \u2286 R\u2032, involves replacing each \u2295 node with a node, each \u2297 node with a node, and each leaf function that returns 0 or 1 with one that returns 0\u2032 or 1\u2032, respectively.\nHowever, simply summing the translated function F \u2032 may compute an incorrect model count because the same satisfying assignment may be counted multiple times; this occurs when the idempotence (a semiring R is idempotent if a \u2295 a = a for a \u2208 R) of the two semirings differs, i.e., either semiring R is idempotent and R\u2032 is not, or vice versa. If exactly one of the two semirings is idempotent, F must be deterministic to ensure that summing F \u2032 gives the correct model count.\nDefinition 9. An OR node is deterministic iff the supports of its children are disjoint. An NNF is deterministic iff all of its OR nodes are deterministic.\nThe support of a function G(X) is the set of points S \u2286 X such that G(x) 6= 0 for all x \u2208 S . If F is deterministic and decomposable, then it follows from the sum-product theorem that its model count can be computed efficiently. Corollary 8 (Darwiche, 2001a). The model count of a deterministic, decomposable NNF can be computed in time linear in its size.\nMost algorithms for #SAT (e.g., Relsat (Bayardo Jr. & Pehoushek, 2000)), Cachet (Sang et al., 2004), #DPLL (Bacchus et al., 2009)) are also instances of SUMSPF, since they extend DPLL by, at each level of recursion, decomposing the CNF into independent components (i.e., no variable appears in multiple components), solving these separately, and caching the model count of each component. Component decomposition corresponds to a decomposable product node in SUMSPF and component caching corresponds to connecting a sub-SPF to multiple parents. Notice that the sum nodes created by DECOMPOSE are deterministic.\nMAX-SAT. MAX-SAT is the problem of computing the maximum number of satisfiable clauses of a CNF, over all assignments. It can be generalized to NNFs as follows. Definition 10. Let F (X) be an NNF and x \u2208 X an assignment. The SAT number (SN) of a literal \u03c6(Xj) \u2208 F is 1 if \u03c6(xj) is true and 0 otherwise. The SN of an AND node is the sum of the SNs of its children. The SN of an OR node is the max of the SNs of its children.\nMAX-SAT of an NNF F (X) is the problem of computing the maximum SAT number of the root of F over all assignments x \u2208 X . If F is a CNF, then this reduces to standard MAX-SAT. MAX-SAT of F can be solved by translating F to the max-sum semiringM = (N\u2212\u221e,max,+,\u2212\u221e, 0) (where R+,\u2212\u221e is used for weighted MAX-SAT), and then summing it. Clearly, F \u2032 is an SPF onM, i.e., a max-sum network. The corollary below follows immediately from the sum-product theorem. Corollary 9 (Darwiche, 2001b). MAX-SAT of a decomposable NNF can be computed in time linear in its size.\nMAX-SAT of an arbitrary NNF (or CNF) can be computed by first translating it to M and then calling SUMSPF, which can be extended to perform branch and bound (BnB) (Lawler & Wood, 1966) when traversing the SPF. This allows SUMSPF to prune sub-SPFs that are not relevant to the final solution, which can greatly reduce the search space. With this addition, DPLL-based BnB solvers for MAX-SAT (e.g., Heras et al. (2008) and references therein) are instances of SUMSPF. Most relevant, however, is the MPE-SAT algorithm of Sang et al. (2007), since both\nit and SUMSPF use decomposition and caching to improve their efficiency."}, {"heading": "C. Probabilistic inference (continued)", "text": "Marginal inference (continued). Tree-based methods include junction-tree clustering (Lauritzen & Spiegelhalter, 1988) and variable elimination (Dechter, 1999), which correspond (explicitly and implicitly, respectively) to constructing a junction tree and then summing its corresponding tree-like SPN. Conditioning algorithms such as recursive conditioning (Darwiche, 2001c), value elimination (Bacchus et al., 2002), AND/OR search (Dechter & Mateescu, 2007), and #DPLL (Bacchus et al., 2009) traverse the space of partial assignments by recursively conditioning on variables and their values. These algorithms vary in the flexibility of their variable ordering, decomposition, and caching (see Bacchus et al. (2009) for a comparison), but are all instances of SUMSPF, which can use a fully-dynamic variable ordering, as value elimination can and #DPLL does, or a fixed ordering, as in variants of recursive conditioning and AND/OR search. Decomposition and caching correspond to decomposable product nodes and connecting sub-SPNs to multiple parents, respectively, in SUMSPF. Thirdly, inference in graphical models can be performed by compilation to an arithmetic circuit (AC) (Darwiche, 2003). In discrete domains, Rooshenas & Lowd (2014) showed that SPNs and ACs are equivalent, but that SPNs are always smaller or equal in size. In continuous domains, however, it is unlikely that even this relationship exists, because a AC would require an infinite number of indicator functions. Furthermore, existing compilation methods require first encoding the graphical model in very restrictive languages (such as CNF or SDDs), which can make them exponentially slower than SUMSPF. Finally, no tractability properties have been established for ACs so there is no guarantee before compiling that inference will be tractable, nor have they been generalized to other semirings.\nMPE. Beyond computing the probability of evidence, another key probabilistic inference problem is finding the most probable or MPE state of the non-evidence variables of P (X) given the evidence, arg maxXE P (e,XE) for evidence e \u2208 XE where XE = X\\XE . The MPE value (maximum probability of any state) of an SPN S can be computed by translating S to the non-negative max-product semiring (R+,max,\u00d7, 0, 1) and maximizing the resulting SPF S\u2032. The MPE state can then be recovered by a downward pass in S\u2032, recursively selecting the (or a) highest-valued child of each max node and all children of each product node (Poon & Domingos, 2011). As when translating an NNF for model counting, an SPN must be selective (Peharz et al., 2014) (the SPN equivalent of\ndeterministic) for summation in the max-product semiring to give the correct MPE. Corollary 10. The MPE state of a selective, decomposable SPN can be found in time linear in its size.\nA sum node in an SPN can be viewed as the result of summing out an implicit hidden variable Yv , whose values Yv = {yc}c\u2208Ch(v) correspond to Ch(v), the children of v (Poon & Domingos, 2011). It is often of interest to find the MPE state of both the hidden and observed variables. This can be done in linear time and requires only that the SPN be decomposable, because making each Yv explicit by multiplying each child c of v by the indicator [Yv = yc] makes the resulting SPN S(X,Y) selective.\nD. Integration and optimization Integration (continued). For non-decomposable SPFs, DECOMPOSE must be altered to select only a finite number of values and then use the trapezoidal rule for approximate integration. Values can be chosen using grid search and if S is Lipschitz continuous the grid spacing can be set such that the error incurred by the approximation is bounded by a pre-specified amount. This can significantly reduce the number of values explored in SUMSPF if combined with approximate decomposability (Section 4), since SUMSPF can treat some non-decomposable product nodes as decomposable, avoiding the expensive call to DECOMPOSE while incurring only a bounded integration error."}, {"heading": "E. Relational inference", "text": "Let X be a finite set of constants and let Rk = X k be the complete relation1 of arity k on X , i.e., the set of all tuples in X k, where X k is the Cartesian product of X with itself k\u2212 1 times. The universe of relations with arity up to m is Um = 1R \u222a \u22c3m i=1 2 Ri , where 2R k is the power set of Rk and 1R is the (identity) relation containing the empty tuple. Since union distributes over join, and both are associative and commutative, R = (Um,\u222a, ./,\u2205,1R) is a semiring over relations, where ./ is natural join and \u2205 is the empty set (recall that R = R ./ 1R for any relation R). Given an extensional database R = {Ri} containing relations Ri of arity up to m, an SPF on R, referred to as a union-join network (UJN), is a query on R. In a UJN Q, each Ri \u2208 R is composed of a union of joins of unary tuples, such that Ri = \u22c3 \u3008c1,...,cr\u3009\u2208Ri ./ r j=1cj , where the leaves of Q are the unary tuples cj . The Ri are then combined with unions and joins to form the full UJN (query). A UJN Q(X) over query variables X = (X1, . . . , Xn) defines an intensional output relation Qans = \u22c3 Xn Q(X). Clearly, computing Qans corresponds to summation in R. Let nQJ denote the 1A relation is a set of tuples; see Abiteboul et al. (1995) for details on relational databases.\nmaximum number of variables involved in a particular join over all joins in Q. The corollary below follows immediately, since a decomposable join is a Cartesian product. Corollary 11. Qans of a decomposable UJNQ can be computed in time linear in the size of Q if nQJ is bounded.\nNote that nQJ can be smaller than the treewidth of Q, since Q composes the final output relation from many small relations (starting with unary tuples) via a relational form of determinism. Since the size of a UJN depends both on the input relations and the query, this is a statement about the combined complexity of queries defined by UJNs.\nRegarding expressivity, selection in a UJN can be implemented as a join with the relation P\u03c3 \u2208 Um, which contains all tuples that satisfy the selection predicate \u03c3. Projection is not immediately supported by R, but since union distributes over projection, it is straightforward to extend the results of Theorem 1 to allow UJNs to contain projection nodes. UJNs with projection correspond to non-recursive Datalog queries (i.e., unions of conjunctive queries), for which decomposable UJNs are a tractable sub-class. Thus, SUMSPF defines a recursive algorithm for evaluating non-recursive Datalog queries and the GenericJoin algorithm (Ngo et al., 2014) \u2013 a recent join algorithm that achieves worst-case optimal performance by recursing on individual tuples \u2013 is an instance of DECOMPOSE.\nAnother consequence of the sum-product theorem is a much simpler proof of the tractability of tractable Markov logic (Domingos & Webb, 2012)."}, {"heading": "F. Relational probabilistic models", "text": "A tractable probabilistic knowledge base (TPKB) (Niepert & Domingos, 2015; Webb & Domingos, 2013; Domingos & Webb, 2012) is a set of class and object declarations such that the classes form a forest and the objects form a tree of subparts when given the leaf class of each object. A class declaration for a class C specifies the subparts Parts(C) = {Pi}, (weighted) subclasses Subs(C) = {Si}, attributes Atts(C) = {Ai}, and (weighted) relations Rels(C) = {Ri}. The subparts of C are parts that every object of class Cmust have and are specified by a name Pi, a class Ci, and a number ni of unique copies. A class C with subclasses S1, . . . , Sj must belong to exactly one of these subclasses, where the weights wi specify the distribution over subclasses. Every attribute has a domain Di and a weight function ui : Di \u2192 R. Each relation Ri(. . . ) has the form Ri(Pa, . . . , Pz) where each of Pa, . . . , Pz is a part of C. Relations specify what relationships may hold among the subparts. A weight vi on Ri defines the probability that the relation is true. A relation can also apply to the object as a whole, instead of to its parts. Object declarations introduce evidence by specify-\ning an object\u2019s subclass memberships, attribute values, and relations as well as specifying the names and path of the object from the top object in the part decomposition.\nA TPKB K is a DAG of objects and their properties (classes, attributes, and relations), and a possible world W is a subtree of the DAG with values for the attributes and relations. The literals are the class membership, attribute, and relation atoms and their negations and thus specify the subclasses of each object, the truth value of each relation, and the value of each attribute. A single (root) top object (O0, C0) has all other objects as descendants. No other objects are of top class C0. The unnormalized distribution \u03c6 over possible subworlds W is defined recursively as \u03c6(O, C,W) = 0 if \u00acIs(O, C) \u2208W or if a relation R of C is hard and \u00acR(O, . . . ) \u2208W, and otherwise as\n\u03c6(O, C,W) =  \u2211 Si\u2208Subs(C) ewi\u03c6(O, Si,W) \u00d7  \u220f\nPi\u2208Parts(C)\n\u03c6(O.Pi, Ci,W) \u00d7  \u220f\nAi\u2208Atts(C)\n\u03b1(O, Ai,W) \u00d7  \u220f\nRi\u2208Rels(C)\n\u03c1(O, Ri,W)  , (1) where \u03b1(O, Ai,W) = eui(D) if Ai(O, D) \u2208 W and \u03c1(O, Ri,W) = e\nvi [Ri(. . . )] + [\u00acRi(. . . )]. Note that Parts(C) contains all subparts of C, including all duplicated parts. The probability of a possible world W is 1 ZK \u03c6(O0, C0,W) where the sub-partition function for (O, C)\nis ZKO,C = \u2211 W\u2208W \u03c6(O, C,W) and ZK = ZKO0,C0 .\nBy construction, \u03c6(O0, C0,W) defines an SPN over the literals. With the sum-product theorem in hand, it is possible to greatly simplify the two-page proof of tractability given in Niepert & Domingos (2015), as we show here. To prove that computing ZK is tractable it suffices to show that (1) is decomposable or can be decomposed efficiently. We first note that each of the four factors in (1) is decomposable, since the first is a sum, the second is a product over the subparts of O and therefore its subfunctions have disjoint scopes, and the third and fourth are products over the attributes and relations, respectively, and are decomposable because none of the \u03b1 or \u03c1 share variables. It only remains to show that the factors can be decomposed with respect to each other without increasing the size of the SPN. Let nO, nC , nr denote the number of object declarations, class declarations, and relation rules, respectively. The SPN corresponding to \u03c6(O0, C0,W) has sizeO(nO(nC+nr)), since for each object (O, C) there are a constant number of edges\nCorollary 12. The partition function of a TPKB can be computed in time linear in its size."}, {"heading": "G. Experimental details", "text": "All experiments were run on the same MacBook Pro with 2.2 GHz Intel Core i7 processor with 16 GB of RAM. Each optimization was limited to a single thread.\nThe non-separable variant of the Rastrigin function (Torn & Zilinskas, 1989) used on pairs of variables is defined as\nfRxi,xj (Yi, Yj) = c0[(Yi \u2212 xi) 2 \u2212 (Yj \u2212 xj)2]+\nc1 \u2212 c1 cos(Yi \u2212 xi) cos(Yj \u2212 xj),\nwhich has a global minimum at y\u2217 = (xi, xj) with value fRx (y\n\u2217) = 0. The constants c0 and c1 control the shape of the quadratic basin and the amplitude of the sinusoids, respectively. For our tests, we used c0 = 0.1 and c1 = 20. Figure 2 shows a contour plot of fR.\nOmitting the parameters x from fRx for simplicity, the full test function for n = 4m variables is defined as\nFx(Y) = m\u2211 i=0 fR(Y4i, Y4i+k) + f R(Y4i+3, Y4i+3\u2212k),\nwhere k = 1 with probability 0.5 and k = 2 otherwise. This creates a function that is non-decomposable between each pair of variables (Y4i, Y4i+k) and (Y4i+3, Y4i+3\u2212k). For the simplest case with n = 4 variables, if k = 1\nthen pairs (Y0, Y1) and (Y2, Y3) are non-decomposable. Alternatively, if k = 2 then pairs (Y0, Y2) and (Y1, Y3) are non-decomposable. The global minimum (xi, xj) for each function fRxi,xj (Yi, Yj) was sampled uniformly over an interval of length 2 from the line Yi = Yj with zero-mean additive Gaussian noise (\u03c3 = 0.1). Thus, each instance of Fx is highly nonconvex and is decomposable with respect to certain variables and not with respect to others. For a set of instances of Fx, there is structure in the decomposability between variables, but different instances have different decomposability structure, so LEARNSPF must first group those function instances that have similar decomposability structure and then identify that structure in order to learn a min-sum function that is applicable to any instance in the training data."}, {"heading": "H. Proofs", "text": "Corollary 1. Every SPF with bounded treewidth can be summed in time linear in the cardinality of its scope.\nProof. Let X = (X1, . . . , Xn), let (R,\u2295,\u2297, 0, 1) be a commutative semiring, and let F (X) be an SPF with bounded treewidth tw(F ) = a for 0 < a < \u221e. Let S(X) be a tree-like SPF that is compatible with F and has junction tree T = (T,Q) with treewidth tw(T ) = a. The size of the largest cluster in T is \u03b1 = a + 1. Let m = |Q| \u2264 n and d = |Xv| for all Xv \u2208 X. Further, other than the root s \u2208 S, there is a one-to-one correspondence between separator instantiations sij \u2208 XSij and sum nodes sij \u2208 S, and between cluster instantiations cj \u2208 XCj and product nodes cj \u2208 S. Now, the number of edges in S can be obtained by counting the edges that correspond to each edge in T and summing over all edges in T , as follows. By construction, each edge (j, k) \u2208 T corresponds to the product nodes {ck}; their children, which are the leaf nodes (indicators and constants) and the sum nodes {sjk}; and the children of {sjk}, which are the product nodes {cj}. By definition, the {cj} have only a single parent, so there are |XCj | \u2264 d\u03b1 edges between {sjk} and {cj}. Further, each ck has only |Ck| + 1 leaf node children and |Ch(k)| sum node children, so there are |XCk |(|Ck|+1)(|Ch(k)|) \u2264 d\u03b1(\u03b1+1)(|Ch(k)|) edges between {ck} and {sjk}. In addition, there are also XCr = d\u03b1 edges between the root s \u2208 S and the product nodes cr. Thus, since T is a tree with m \u2212 1 edges, size(S) \u2264 d\u03b1 + \u2211 (j,k)\u2208T 2d\n\u03b1(\u03b1+ 1)(|Ch(k)|) = O(md\u03b1), which is O(n). Since S is decomposable and has size O(n), then, from the sum-product theorem, S can be summed in time O(n). Furthermore, S is compatible with F , so F can be summed in time O(n), and the claim follows.\nCorollary 2. Not every SPF that can be summed in time linear in the cardinality of its scope has bounded treewidth.\nProof. By counterexample. Let X = (X1, . . . , Xn) be a vector of variables, (R,\u2295,\u2297, 0, 1) be a commutative semiring, and k = |Xi| for all Xi \u2208 X. The SPF F (X) = \u2295r j=1 \u2297n i=1 \u03c8ji(Xi) can be summed in time linear in n because F is decomposable and has size r(n+ 1). At the same time, F (X) has treewidth n \u2212 1 (i.e., unbounded) because there are no pairwise-disjoint subsets A,B,C \u2286 X with domains XA,XB ,XC such that A and B are conditionally independent in F given C, and thus the smallest junction tree compatible with F (X) is a complete clique over X. This can be seen as follows. Without loss of generality, let A \u222a B be the first m variables in X, (X1, . . . , Xm). For any c \u2208 XC , F (A,B, c) \u221d \u2295r j=1 \u2297 i:Xi\u2208A\u222aB \u03c8ji(Xi) = (\u03c811(X1)\u2297 \u00b7 \u00b7 \u00b7\u2297\u03c81m(Xm))\u2295\u00b7 \u00b7 \u00b7\u2295(\u03c8r1(X1)\u2297\u00b7 \u00b7 \u00b7\u2297\u03c8rm(Xm)). For F (A,B, c) to factor, the terms in the right-hand side must have common factors; however, in general, each \u03c8ji is different, so there are no such factors. Thus, F (A,B, c) 6= F (A, c)\u2297F (B, c) for all c \u2208 XC , and there are no conditional independencies in F .\nCorollary 8. The model count of a deterministic, decomposable NNF can be computed in time linear in its size.\nProof. Let F (X) be a deterministic, decomposable NNF and F \u2032(X) be F translated to the sum-product semiring. Clearly, F and F \u2032 have equal size and F \u2032 is deterministic and decomposable. Thus, from the sum-product theorem, \u2211 X F\n\u2032(X) takes time linear in the size of F . Let v be a node in F and v\u2032 its corresponding node in F \u2032. It remains to show that \u2211 X F \u2032 v(X) = #SAT(Fv(X)) for all v, v\u2032, which we do by induction. The base case with v a leaf node holds trivially. For the induction step, assume that \u2211 Xi F \u2032 i (Xi) = #SAT(Fi(Xi)) for each child ci \u2208 Ch(v) (resp. c\u2032i \u2208 Ch(v\u2032)). If v is an AND node then v\u2032 is a multiplication node and #SAT(Fv(X)) = #SAT( \u2227 ci Fi(xi)) = \u220f ci #SAT(Fi(xi)) = \u2211 X F \u2032 v(X), because v and v\u2032 are decomposable. If v is an OR node then v\u2032 is an addition node and #SAT(Fv(X)) = #SAT( \u2228 ci Fi(xi)) = \u2211 ci #SAT(Fi(xi)) = \u2211 X F \u2032 v(X), because v is deterministic, so its children are logically disjoint.\nCorollary 10. The MPE state of a selective, decomposable SPN can be found in time linear in its size.\nProof. Let S(X) be a selective, decomposable SPN and S\u2032(X) its max-product version, which has the same size and is also selective and decomposable. For clarity, we assume no evidence since it is trivial to incorporate. From the sum-product theorem, maxX S\u2032(X) takes time linear in the size of S. Let v be a node in S and v\u2032 its corresponding node in S\u2032. It remains to show that maxX S\u2032v(X) = maxX S(X) for all v, v\u2032, which we do by induction on v. The base case with v a leaf holds trivially because v\nand v\u2032 are identical. For the induction step, assume that maxXi S \u2032 i(Xi) = maxXi Si(Xi) for each child ci \u2208 Ch(v) (resp. c\u2032i \u2208 Ch(v\u2032)). If v is a product node then so is v\u2032 and maxX S \u2032 v(X) = maxX Sv(X). If v is a sum node then v \u2032\nis a max node and maxX S(X) = maxx\u2208X \u2211 ci Si(xi) = maxx\u2208X {maxci Si(xi)} = maxci{maxxi\u2208Xi Si(xi)} = maxX S\n\u2032(X), where the second equality occurs because v is selective. After summing S\u2032, the MPE state is recovered by a downward pass in S\u2032, which takes linear time.\nCorollary 11. The partition function of TPKB K can be computed in time linear in its size.\nProof. The only sources of non-decomposability in (1) are if an object (O, C) and one of its subclasses (O, Sj) both contain (i) the same relation Ri(O, . . . ) or (ii) the same attribute Ai(O, D). Note that they cannot contain the same part since two classes such that one is a descendant of the other in the class hierarchy never have a part with the same name. In each of the above cases, the shared relation (or attribute) can be pushed into each subclass (O,Sj) by distributing \u03c1(O, Ri,W) (or \u03b1(Ai, Ci,W)) over the subclass sum and into each subclass (this can be repeated for multiple levels of the class hierarchy). This makes the product over relations (attributes) in \u03c6(O, Sj,W) non-decomposable, but does not affect the decomposability of any other objects. Now, \u03c6(O, Sj,W) can be decomposed, as follows. For (i), the product over relations in \u03c6(O, Sj,W) now contains the non-decomposable factor F (Ri) = \u03c1(O, Ri,W) \u00b7 \u03c1\u2032(O, Ri,W), where \u03c1\u2032 was pushed down from (O, C). However, F (Ri) = (ewi [Ri] + [\u00acRi]) \u00b7 (ew \u2032 i [Ri] + [\u00acRi]) = ewi+w \u2032 i [Ri] + [\u00acRi] since [a]2 = [a] and [a][\u00aca] = 0 for a literal a. Thus, F (Ri) is simply \u03c1(O, Ri,W) with weightwi+w\u2032i for Ri, which results in the same decomposable SPN structure with a different weight. For (ii), let the weight function for attribute Ai of class C with domain Di be ui and the weight function from the attribute that was pushed down be u\u2032i. To render this decomposable, simply replace ui with ui u\u2032i, the element-wise product of the two weight functions. Again, decomposability is achieved simply by updating the weight functions. Decomposing (i) and (ii) each adds only a linear number of edges to the original non-decomposable SPN, so the size of the corresponding decomposable SPN is |K|. Thus, from the sum-product theorem, computing the partition function of TPKB K takes time linear in |K|."}], "references": [{"title": "Learning the structure of sum-product networks via an SVD-based algorithm", "author": ["T. Adel", "D. Balduzzi", "A. Ghodsi"], "venue": "In Proceedings of the 31st Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Adel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Adel et al\\.", "year": 2015}, {"title": "The generalized distributive law", "author": ["S.M. Aji", "R.J. McEliece"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Aji and McEliece,? \\Q2000\\E", "shortCiteRegEx": "Aji and McEliece", "year": 2000}, {"title": "Value elimination: Bayesian inference via backtracking search", "author": ["F. Bacchus", "S. Dalmao", "T. Pitassi"], "venue": "In Proceedings of the 19th Annual Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Bacchus et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bacchus et al\\.", "year": 2002}, {"title": "Solving #SAT and Bayesian inference with backtracking search", "author": ["F. Bacchus", "S. Dalmao", "T. Pitassi"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bacchus et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bacchus et al\\.", "year": 2009}, {"title": "Thin junction trees", "author": ["F. Bach", "M.I. Jordan"], "venue": "In Advances in Neural Information Processing Systems, pp", "citeRegEx": "Bach and Jordan,? \\Q2001\\E", "shortCiteRegEx": "Bach and Jordan", "year": 2001}, {"title": "Counting models using connected components", "author": ["R.J. Bayardo Jr.", "J.D. Pehoushek"], "venue": "In Proceedings of the 17th National Conference on Artificial Intelligence,", "citeRegEx": "Jr. and Pehoushek,? \\Q2000\\E", "shortCiteRegEx": "Jr. and Pehoushek", "year": 2000}, {"title": "Semiring-based constraint satisfaction and optimization", "author": ["S. Bistarelli", "U. Montanari", "F. Rossi"], "venue": "Journal of the ACM,", "citeRegEx": "Bistarelli et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bistarelli et al\\.", "year": 1997}, {"title": "Complexity of inference in graphical models", "author": ["V. Chandrasekaran", "N. Srebro", "P. Harsha"], "venue": "In Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Chandrasekaran et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 2008}, {"title": "A generalization of generalized arc consistency: From constraint satisfaction to constraint-based inference", "author": ["L. Chang", "A.K. Mackworth"], "venue": "In Proceedings of the IJCAI05 Workshop on Modeling and Solving Problems with Constraints,", "citeRegEx": "Chang and Mackworth,? \\Q2005\\E", "shortCiteRegEx": "Chang and Mackworth", "year": 2005}, {"title": "Efficient principled learning of thin junction trees", "author": ["A. Chechetka", "C. Guestrin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chechetka and Guestrin,? \\Q2007\\E", "shortCiteRegEx": "Chechetka and Guestrin", "year": 2007}, {"title": "A knowledge compilation map", "author": ["A. Darwiche", "P. Marquis"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Darwiche and Marquis,? \\Q2002\\E", "shortCiteRegEx": "Darwiche and Marquis", "year": 2002}, {"title": "On the tractable counting of theory models and its application to truth maintenance and belief revision", "author": ["A. Darwiche"], "venue": "Journal of Applied Non-Classical Logics,", "citeRegEx": "Darwiche,? \\Q2001\\E", "shortCiteRegEx": "Darwiche", "year": 2001}, {"title": "Decomposable negation normal form", "author": ["A. Darwiche"], "venue": "Journal of the ACM,", "citeRegEx": "Darwiche,? \\Q2001\\E", "shortCiteRegEx": "Darwiche", "year": 2001}, {"title": "Recursive conditioning", "author": ["A. Darwiche"], "venue": "Artificial Intelligence,", "citeRegEx": "Darwiche,? \\Q2001\\E", "shortCiteRegEx": "Darwiche", "year": 2001}, {"title": "A differential approach to inference in Bayesian networks", "author": ["A. Darwiche"], "venue": "Journal of the ACM,", "citeRegEx": "Darwiche,? \\Q2003\\E", "shortCiteRegEx": "Darwiche", "year": 2003}, {"title": "A machine program for theorem-proving", "author": ["M. Davis", "G. Logemann", "D. Loveland"], "venue": "Communications of the ACM,", "citeRegEx": "Davis et al\\.,? \\Q1962\\E", "shortCiteRegEx": "Davis et al\\.", "year": 1962}, {"title": "Bucket elimination: A unifying framework for reasoning", "author": ["R. Dechter"], "venue": "Artificial Intelligence,", "citeRegEx": "Dechter,? \\Q1999\\E", "shortCiteRegEx": "Dechter", "year": 1999}, {"title": "AND/OR search spaces for graphical models", "author": ["R. Dechter", "R. Mateescu"], "venue": "Artificial intelligence,", "citeRegEx": "Dechter and Mateescu,? \\Q2007\\E", "shortCiteRegEx": "Dechter and Mateescu", "year": 2007}, {"title": "A tractable first-order probabilistic logic", "author": ["P. Domingos", "W.A. Webb"], "venue": "In Proceedings of the 26th Conference on Artificial Intelligence,", "citeRegEx": "Domingos and Webb,? \\Q1902\\E", "shortCiteRegEx": "Domingos and Webb", "year": 1902}, {"title": "Recursive Decomposition for Nonconvex Optimization", "author": ["A.L. Friesen", "P. Domingos"], "venue": "Proceedings of the 24th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Friesen and Domingos,? \\Q2015\\E", "shortCiteRegEx": "Friesen and Domingos", "year": 2015}, {"title": "Discriminative learning of sumproduct networks", "author": ["R. Gens", "P. Domingos"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gens and Domingos,? \\Q2012\\E", "shortCiteRegEx": "Gens and Domingos", "year": 2012}, {"title": "Learning the structure of sumproduct networks", "author": ["R. Gens", "P. Domingos"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Gens and Domingos,? \\Q2013\\E", "shortCiteRegEx": "Gens and Domingos", "year": 2013}, {"title": "Provenance semirings", "author": ["T. Green", "G. Karvounarakis", "V. Tannen"], "venue": "In Proceedings of the 26th ACM SIGMODSIGACT-SIGART symposium on Principles of Database Systems,", "citeRegEx": "Green et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Green et al\\.", "year": 2007}, {"title": "MiniMaxSAT: An efficient weighted MAX-SAT solver", "author": ["F. Heras", "J. Larrosa", "A. Oliveras"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Heras et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Heras et al\\.", "year": 2008}, {"title": "The language of search", "author": ["J. Huang", "A. Darwiche"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Huang and Darwiche,? \\Q2007\\E", "shortCiteRegEx": "Huang and Darwiche", "year": 2007}, {"title": "Boolean Function Complexity", "author": ["S. Jukna"], "venue": null, "citeRegEx": "Jukna,? \\Q2012\\E", "shortCiteRegEx": "Jukna", "year": 2012}, {"title": "Unifying tree decompositions for reasoning in graphical models", "author": ["K. Kask", "R. Dechter", "J. Larrosa", "A. Dechter"], "venue": "Artificial Intelligence,", "citeRegEx": "Kask et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kask et al\\.", "year": 2005}, {"title": "Algebraic model counting", "author": ["A. Kimmig", "G. Van Den Broeck", "L. De Raedt"], "venue": "arXiv preprint arXiv:1211.4475,", "citeRegEx": "Kimmig et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kimmig et al\\.", "year": 2012}, {"title": "Algorithms for constraint-satisfaction problems: A survey", "author": ["V. Kumar"], "venue": "AI Magazine,", "citeRegEx": "Kumar,? \\Q1992\\E", "shortCiteRegEx": "Kumar", "year": 1992}, {"title": "On learning constraint problems", "author": ["A. Lallouet", "M. Lopez", "L. Martin", "C. Vrain"], "venue": "Proceedings of the International Conference on Tools with Artificial Intelligence,", "citeRegEx": "Lallouet et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lallouet et al\\.", "year": 2010}, {"title": "Local computations with probabilities on graphical structures and their application to expert systems", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Lauritzen and Spiegelhalter,? \\Q1988\\E", "shortCiteRegEx": "Lauritzen and Spiegelhalter", "year": 1988}, {"title": "Branch-and-Bound Methods: A Survey", "author": ["E.L. Lawler", "D.E. Wood"], "venue": "Operations Research,", "citeRegEx": "Lawler and Wood,? \\Q1966\\E", "shortCiteRegEx": "Lawler and Wood", "year": 1966}, {"title": "On the limited memory BFGS method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical Programming,", "citeRegEx": "Liu and Nocedal,? \\Q1989\\E", "shortCiteRegEx": "Liu and Nocedal", "year": 1989}, {"title": "The relationship between AND/OR search spaces and variable elimination", "author": ["R. Mateescu", "R. Dechter"], "venue": "In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Mateescu and Dechter,? \\Q2005\\E", "shortCiteRegEx": "Mateescu and Dechter", "year": 2005}, {"title": "Learning and inference in tractable probabilistic knowledge bases", "author": ["M. Niepert", "P. Domingos"], "venue": "In Proceedings of the 31st Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Niepert and Domingos,? \\Q2015\\E", "shortCiteRegEx": "Niepert and Domingos", "year": 2015}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Learning selective sum-product networks", "author": ["R. Peharz", "R. Gens", "P. Domingos"], "venue": "In Proceedings of the ICML-14 Workshop on Learning Tractable Probabilistic Models,", "citeRegEx": "Peharz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Peharz et al\\.", "year": 2014}, {"title": "Sum-product networks: A new deep architecture", "author": ["H. Poon", "P. Domingos"], "venue": "In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Poon and Domingos,? \\Q2011\\E", "shortCiteRegEx": "Poon and Domingos", "year": 2011}, {"title": "Numerical Recipes: The Art of Scientific Computing (3rd ed.)", "author": ["W.H. Press", "S.A. Teukolsky", "W.T. Vetterling", "B.P. Flannery"], "venue": null, "citeRegEx": "Press et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Press et al\\.", "year": 2007}, {"title": "Learning decision lists", "author": ["R.L. Rivest"], "venue": "Machine Learning,", "citeRegEx": "Rivest,? \\Q1987\\E", "shortCiteRegEx": "Rivest", "year": 1987}, {"title": "Semiring-based mini-bucket partitioning schemes", "author": ["E. Rollon", "J. Larrosa", "R. Dechter"], "venue": "In Proceedings of the 23rd International Joint Conference on Artificial Intelligence,", "citeRegEx": "Rollon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rollon et al\\.", "year": 2013}, {"title": "Learning sum-product networks with direct and indirect variable interactions", "author": ["A. Rooshenas", "D. Lowd"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Rooshenas and Lowd,? \\Q2014\\E", "shortCiteRegEx": "Rooshenas and Lowd", "year": 2014}, {"title": "Combining component caching and clause learning for effective model counting", "author": ["T. Sang", "F. Bacchus", "P. Beame", "H. Kautz", "T. Pitassi"], "venue": "In Proceedings of the International Conference on Theory and Applications of Satisfiability Testing,", "citeRegEx": "Sang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2004}, {"title": "A dynamic approach to MPE and weighted MAX-SAT", "author": ["T. Sang", "P. Beame", "H. Kautz"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Sang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2007}, {"title": "Arithmetic circuits: A survey of recent results and open questions", "author": ["A. Shpilka", "A. Yehudayoff"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "Shpilka and Yehudayoff,? \\Q2010\\E", "shortCiteRegEx": "Shpilka and Yehudayoff", "year": 2010}, {"title": "Learning structured prediction models: A large margin approach", "author": ["B. Taskar", "V. Chatalbashev", "D. Koller", "C. Guestrin"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "Taskar et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "Tractable probabilistic knowledge bases with existence uncertainty", "author": ["W.A. Webb", "P. Domingos"], "venue": "In Proceedings of the UAI-13 International Workshop on Statistical Relational AI,", "citeRegEx": "Webb and Domingos,? \\Q2013\\E", "shortCiteRegEx": "Webb and Domingos", "year": 2013}, {"title": "Decision diagrams for the computation of semiring valuations", "author": ["N. Wilson"], "venue": "In Proceedings of the 19th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Wilson,? \\Q2005\\E", "shortCiteRegEx": "Wilson", "year": 2005}, {"title": "Domingos & Webb, 2012) is a set of class and object declarations such that the classes form a forest and the objects form a tree of subparts when given the leaf class of each object. A class declaration for a class C", "author": ["Domingos"], "venue": "Webb & Domingos,", "citeRegEx": "Domingos,? \\Q2015\\E", "shortCiteRegEx": "Domingos", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Unfortunately, inference in them is exponential in their treewidth (Chandrasekaran et al., 2008), a common measure of complexity.", "startOffset": 67, "endOffset": 96}, {"referenceID": 45, "context": "The class of problems we address can be viewed as generalizing structured prediction beyond combinatorial optimization (Taskar et al., 2005), to include optimization for continuous models and others.", "startOffset": 119, "endOffset": 140}, {"referenceID": 4, "context": ", Bistarelli et al. (1997); Dechter (1999); Aji & McEliece (2000); Wilson (2005); Green et al.", "startOffset": 2, "endOffset": 27}, {"referenceID": 4, "context": ", Bistarelli et al. (1997); Dechter (1999); Aji & McEliece (2000); Wilson (2005); Green et al.", "startOffset": 2, "endOffset": 43}, {"referenceID": 4, "context": ", Bistarelli et al. (1997); Dechter (1999); Aji & McEliece (2000); Wilson (2005); Green et al.", "startOffset": 2, "endOffset": 66}, {"referenceID": 4, "context": ", Bistarelli et al. (1997); Dechter (1999); Aji & McEliece (2000); Wilson (2005); Green et al.", "startOffset": 2, "endOffset": 81}, {"referenceID": 4, "context": ", Bistarelli et al. (1997); Dechter (1999); Aji & McEliece (2000); Wilson (2005); Green et al. (2007); Dechter & Mateescu (2007); Bacchus et al.", "startOffset": 2, "endOffset": 102}, {"referenceID": 4, "context": ", Bistarelli et al. (1997); Dechter (1999); Aji & McEliece (2000); Wilson (2005); Green et al. (2007); Dechter & Mateescu (2007); Bacchus et al.", "startOffset": 2, "endOffset": 129}, {"referenceID": 2, "context": "(2007); Dechter & Mateescu (2007); Bacchus et al. (2009)); namely, that each consists of summing a function over a semiring.", "startOffset": 35, "endOffset": 57}, {"referenceID": 11, "context": "Darwiche (2003) used arithmetic circuits as a data structure to support inference in Bayesian networks over discrete variables.", "startOffset": 0, "endOffset": 16}, {"referenceID": 11, "context": "A junction tree provides a schematic for constructing a specific type of decomposable SPF called a tree-like SPF (a semiring-generalized version of a construction from Darwiche (2003)).", "startOffset": 168, "endOffset": 184}, {"referenceID": 26, "context": "Notice that these definitions of junction tree and treewidth reduce to the standard ones (Kask et al., 2005).", "startOffset": 89, "endOffset": 108}, {"referenceID": 27, "context": "In fact, over the same set of variables, deterministic circuits may be exponentially larger and are never smaller than non-deterministic circuits (Darwiche & Marquis, 2002; Kimmig et al., 2012).", "startOffset": 146, "endOffset": 193}, {"referenceID": 23, "context": "Most relevant is the preliminary work of Kimmig et al. (2012), which proposes a semiring generalization of arithmetic circuits for knowledge compilation and does not address learning.", "startOffset": 41, "endOffset": 62}, {"referenceID": 11, "context": ", Darwiche (2001b; 2003); Poon & Domingos (2011)), some of our results have not previously been shown formally, although some have been foreshadowed informally.", "startOffset": 2, "endOffset": 49}, {"referenceID": 3, "context": "We present an algorithm for summing an SPF that adapts to the structure of the SPF and can thus take exponentially less time than constructing and summing a compatible tree-like SPF (Bacchus et al., 2009), which imposes a uniform decomposition structure.", "startOffset": 182, "endOffset": 204}, {"referenceID": 48, "context": ", Bach & Jordan (2001); Gens & Domingos (2013)).", "startOffset": 31, "endOffset": 47}, {"referenceID": 45, "context": "tion (Taskar et al., 2005).", "startOffset": 5, "endOffset": 26}, {"referenceID": 15, "context": "DPLL (Davis et al., 1962), the stan-", "startOffset": 5, "endOffset": 25}, {"referenceID": 43, "context": "on MPE-SAT (Sang et al., 2007) and Generic-Join (Ngo et al.", "startOffset": 11, "endOffset": 30}, {"referenceID": 11, "context": "dard algorithm for solving SAT, is an instance of SUMSPF (see also Huang & Darwiche (2007)).", "startOffset": 75, "endOffset": 91}, {"referenceID": 25, "context": "Learning in the Boolean semiring is a well-studied area, which includes problems from learning Boolean circuits (Jukna, 2012) (of which decomposable SPFs are a restricted subclass, known as syntactically multilinear circuits) to learning sets of rules (Rivest, 1987).", "startOffset": 112, "endOffset": 125}, {"referenceID": 39, "context": "Learning in the Boolean semiring is a well-studied area, which includes problems from learning Boolean circuits (Jukna, 2012) (of which decomposable SPFs are a restricted subclass, known as syntactically multilinear circuits) to learning sets of rules (Rivest, 1987).", "startOffset": 252, "endOffset": 266}, {"referenceID": 6, "context": "Solving F corresponds to computing \u2228 X F (X), which is summation on B (see also Bistarelli et al. (1997); Chang & Mackworth (2005); Rollon et al.", "startOffset": 80, "endOffset": 105}, {"referenceID": 6, "context": "Solving F corresponds to computing \u2228 X F (X), which is summation on B (see also Bistarelli et al. (1997); Chang & Mackworth (2005); Rollon et al.", "startOffset": 80, "endOffset": 131}, {"referenceID": 6, "context": "Solving F corresponds to computing \u2228 X F (X), which is summation on B (see also Bistarelli et al. (1997); Chang & Mackworth (2005); Rollon et al. (2013)).", "startOffset": 80, "endOffset": 153}, {"referenceID": 28, "context": "Like DPLL, backtracking-based search algorithms (Kumar, 1992) for CSPs are also instances of SUMSPF (see also Mateescu & Dechter (2005)).", "startOffset": 48, "endOffset": 61}, {"referenceID": 15, "context": "Like DPLL, backtracking-based search algorithms (Kumar, 1992) for CSPs are also instances of SUMSPF (see also Mateescu & Dechter (2005)).", "startOffset": 121, "endOffset": 136}, {"referenceID": 6, "context": "Further, SPFs on a number of other semirings correspond to various extensions of CSPs, including fuzzy, probabilistic, and weighted CSPs (see Table 1 and Bistarelli et al. (1997)).", "startOffset": 154, "endOffset": 179}, {"referenceID": 45, "context": "LEARNSPF for CSPs addresses a variant of structured prediction (Taskar et al., 2005); specifically, learning a function F : X \u2192 B such that arg \u2228 y F (x ,y) \u2248 y", "startOffset": 63, "endOffset": 84}, {"referenceID": 29, "context": "This is a much simpler and more attractive approach than existing constraint learning methods such as Lallouet et al. (2010), which uses inductive logic programming and has no tractability guarantees.", "startOffset": 102, "endOffset": 125}, {"referenceID": 35, "context": "Many probability distributions can be compactly represented as graphical models: P (X) = 1 Z \u220f i \u03c8i(Xi), where \u03c8i is a potential over variables Xi \u2286 X and Z is known as the partition function (Pearl, 1988).", "startOffset": 192, "endOffset": 205}, {"referenceID": 14, "context": "Building on a number of earlier works (Darwiche, 2003; Dechter & Mateescu, 2007; Bacchus et al., 2009), Poon & Domingos (2011) introduced sum-product networks (SPNs), a class of distributions in which inference is guaranteed to be tractable.", "startOffset": 38, "endOffset": 102}, {"referenceID": 3, "context": "Building on a number of earlier works (Darwiche, 2003; Dechter & Mateescu, 2007; Bacchus et al., 2009), Poon & Domingos (2011) introduced sum-product networks (SPNs), a class of distributions in which inference is guaranteed to be tractable.", "startOffset": 38, "endOffset": 102}, {"referenceID": 2, "context": "Building on a number of earlier works (Darwiche, 2003; Dechter & Mateescu, 2007; Bacchus et al., 2009), Poon & Domingos (2011) introduced sum-product networks (SPNs), a class of distributions in which inference is guaranteed to be tractable.", "startOffset": 81, "endOffset": 127}, {"referenceID": 46, "context": ", Gens & Domingos (2013); Rooshenas & Lowd (2014); Peharz et al.", "startOffset": 9, "endOffset": 25}, {"referenceID": 46, "context": ", Gens & Domingos (2013); Rooshenas & Lowd (2014); Peharz et al.", "startOffset": 9, "endOffset": 50}, {"referenceID": 35, "context": ", Gens & Domingos (2013); Rooshenas & Lowd (2014); Peharz et al. (2014); Adel et al.", "startOffset": 51, "endOffset": 72}, {"referenceID": 0, "context": "(2014); Adel et al. (2015)), and we re-", "startOffset": 8, "endOffset": 27}, {"referenceID": 38, "context": "SUMSPF defines a novel algorithm for (approximate) integration that is based on recursive problem decomposition, and can be exponentially more efficient than standard integration algorithms such as trapezoidal or Monte Carlo methods (Press et al., 2007) because it dynamically decomposes the problem at each recursion level and caches intermediate computations.", "startOffset": 233, "endOffset": 253}, {"referenceID": 48, "context": "The recent RDIS algorithm for nonconvex optimization (Friesen & Domingos, 2015), which achieves exponential speedups compared to other algorithms, is an instance of SUMSPF where values are chosen via multi-start gradient descent and variables in DECOMPOSE are chosen by graph partitioning. Friesen & Domingos (2015), however, do not specify tractability conditions for the optimization; thus, Corollary 7 defines a novel class of functions that can be efficiently globally optimized.", "startOffset": 64, "endOffset": 316}, {"referenceID": 45, "context": "For nonconvex optimization, LEARNSPF solves a variant of structured prediction (Taskar et al., 2005), in which the variables to predict are continuous instead of discrete (e.", "startOffset": 79, "endOffset": 100}], "year": 2016, "abstractText": "Inference in expressive probabilistic models is generally intractable, which makes them difficult to learn and limits their applicability. Sumproduct networks are a class of deep models where, surprisingly, inference remains tractable even when an arbitrary number of hidden layers are present. In this paper, we generalize this result to a much broader set of learning problems: all those where inference consists of summing a function over a semiring. This includes satisfiability, constraint satisfaction, optimization, integration, and others. In any semiring, for summation to be tractable it suffices that the factors of every product have disjoint scopes. This unifies and extends many previous results in the literature. Enforcing this condition at learning time thus ensures that the learned models are tractable. We illustrate the power and generality of this approach by applying it to a new type of structured prediction problem: learning a nonconvex function that can be globally optimized in polynomial time. We show empirically that this greatly outperforms the standard approach of learning without regard to the cost of optimization.", "creator": "LaTeX with hyperref package"}}}