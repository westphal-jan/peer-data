{"id": "1611.03814", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2016", "title": "Towards the Science of Security and Privacy in Machine Learning", "abstract": "Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive---new systems and models are being deployed in every domain imaginable, leading to rapid and widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited. We systematize recent findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date. We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. We conclude by formally exploring the opposing relationship between model accuracy and resilience to adversarial manipulation. Through these explorations, we show that there are (possibly unavoidable) tensions between model complexity, accuracy, and resilience that must be calibrated for the environments in which they will be used.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 11 Nov 2016 18:57:15 GMT  (573kb,D)", "http://arxiv.org/abs/1611.03814v1", null]], "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["nicolas papernot", "patrick mcdaniel", "arunesh sinha", "michael wellman"], "accepted": false, "id": "1611.03814"}, "pdf": {"name": "1611.03814.pdf", "metadata": {"source": "CRF", "title": "SoK: Towards the Science of Security and Privacy in Machine Learning", "authors": ["Nicolas Papernot", "Patrick McDaniel", "Arunesh Sinha", "Michael Wellman"], "emails": ["ngp5056@cse.psu.edu,", "mcdaniel@cse.psu.edu,", "arunesh@umich.edu", "wellman@umich.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nThe coming of age of the science of machine learning (ML) coupled with advances in computational and storage capacities have transformed the technology landscape. For example, ML-driven data analytics have fundamentally altered the practice of health care and financial management. Within the security domain, detection and monitoring systems now consume massive amounts of data and extract actionable information that in the past would have been impossible. Yet, in spite of these spectacular advances, the technical community\u2019s understanding of the vulnerabilities inherent to the design of systems built on ML and the means to defend against them are still in its infancy. There is a broad and pressing call to advance a science of the security and privacy in ML.\nSuch calls have not gone unheeded. A number of activities have been launched to understand the threats, attacks and defenses of systems built on machine learning. However, work in this area is fragmented across several research communities including machine learning, security, statistics, and theory of computation, and there has been few efforts to develop a unified lexicon or science spanning these disciplines. This fragmentation presents both a motivation and challenge for our effort to systematize knowledge about the myriad of security and privacy issues that involve ML. In this paper we develop a\nunified perspective on this field. We introduce a unified threat model that considers the attack surface and adversarial goals and capabilities of systems built on machine learning. The security model serves as a roadmap in the following sections for exploring attacks and defenses of ML systems. We draw major themes and highlight results in the form of take-away messages about this new area of research. We conclude by providing a theorem of the \u201cno free lunch\u201d properties of many ML systems\u2014identifying where there is a tension between complexity and resilience to adversarial manipulation, how this tension impacts the accuracy of models and the effect of the size of datasets on this trade-off.\nIn exploring security and privacy in this domain, it is instructive to view systems built on machine learning through the prism of the classical confidentiality, integrity, and availability (CIA) model. In this work, confidentiality is defined with respect to the model or its training data. Attacks on confidentiality attempt to expose the model structure or parameters (which may be highly valuable intellectual property) or the data used to train it, e.g., patient data. The latter class of attacks have a potential to impact the privacy of the data source, e.g., the privacy of patient clinical data used to train medical diagnostic models is often of paramount importance. Conversely, we define attacks on the integrity as those that induce particular outputs or behaviors of the adversary\u2019s choosing. Where those adversarial behaviors attempt to prevent access to meaningful model outputs or the features of the system itself, such attacks fall within the realm of availability.\nA second perspective in evaluating security and privacy focuses on attacks and defenses with respect to the machine learning pipeline. Here, we consider the lifecycle of a MLbased system from training to inference, and identify the adversarial goals and means at each phase. We observe that attacks on training generally attempt to influence the model by altering or injecting training samples\u2013in essence guiding the learning process towards a vulnerable model. Attacks at inference time (runtime) are more diverse. Adversaries use exploratory attacks to induce targeted outputs, and oracle attacks to extract the model itself.\nThe science of defenses for machine learning are somewhat less well developed. Here we consider several defensive goals. First, we consider methods at training and inference time that are robust to distribution drifts\u2013the property that ensures that the model performs adequately when the training and\nar X\niv :1\n61 1.\n03 81\n4v 1\n[ cs\n.C R\n] 1\n1 N\nov 2\n01 6\nruntime input distributions differ. Second, we explore models that provide formal privacy preserving guarantees\u2013the property that the amount of data exposed by the model is bounded by a privacy budget (expressed in terms of differential privacy). Lastly, we explore defenses that provide fairness (preventing biased outputs) and accountability (explanations of why particular outputs were generated, also known as transparency).\nIn exploring these facets of machine learning attacks and defense, we make the following contributions: \u2022 We introduce a unifying threat model to allow structured\nreasoning about the security and privacy of systems that incorporate machine learning. This model, presented in Section III, departs from previous efforts by considering the entire data pipeline, of which ML is a component, instead of ML algorithms in isolation. \u2022 We taxonomize attacks and defenses identified by the various technical communities as informed elements of PAC learning theory. Section IV details the challenges of in adversarial settings and Section V considers trained and deployed systems. Section VI presents desirable properties to improve the security and privacy of ML. \u2022 In Section VII, we introduce a no free lunch theorem for adversarial machine learning. It characterizes the tradeoff between accuracy and robustness to adversarial efforts, when learning from limited data.\nNote that ML systems address a great many different problem domains, e.g., classification, regression and policy learning. However, for brevity and ease of exposition, we focus much of the current paper on ML classification. We further state that the related study of the implications of ML and AI on safety in societies is outside the scope of this paper, and refer interested readers to the comprehensive review by Amodei et al. [1].\nThe remainder of this paper focuses on a systematization of the knowledge of security and privacy in ML. While there is an enormous body of work in many aspects of this research, for space we focus on attacks and defenses. As a result of a careful analysis of the threat model, we have selected seminal and representative works that illustrate the branches of this research. While we attempt to be exhaustive, it is a practical impossibility to cite all works. For instance, we do not cover trusted computing platforms for ML [2]. We begin below by introducing the basic structure and lexicon of ML systems."}, {"heading": "II. ABOUT MACHINE LEARNING", "text": "We start with a brief overview of how systems apply ML algorithms. In particular, we compare different kinds of learning tasks, and some specifics of their practical implementation."}, {"heading": "A. An Overview of Machine Learning Tasks", "text": "Machine learning provides automated methods of analysis for large sets of data [3]. Tasks solved with machine learning techniques are commonly divided into three types. These are characterized by the structure of the data analyzed by the corresponding learning algorithm.\nSupervised learning: Methods that induce an association between inputs and outputs based on training examples in the\nform of inputs labeled with corresponding outputs are supervised learning techniques. If the output data is categorical, the task is called classification, and real-valued output domains define regression problems. Classic examples of supervised learning tasks include: object recognition in images [4], machine translation [5], and spam filtering [6].\nUnsupervised learning: When the method is given unlabeled inputs, its task is unsupervised. Unsupervised learning considers problems such as clustering points according to a similarity metric [7], dimensionality reduction to project data in lower dimensional subspaces [8], and model pre-training [10]. For instance, clustering may be applied to anomaly detection [11].\nReinforcement learning: Methods that learn a policy for action over time given sequences of actions, observations, and rewards fall in the scope of reinforcement learning [12], [13]. Reinforcement learning can be viewed as the subfield of ML concerned with planning and control. It was reinforcement learning in combination with supervised and unsupervised methods that recently enabled a computer to defeat a human champion at the game of Go [14].\nReaders interested in a broad survey of ML are well served by many books covering this rich topic [3], [15], [16]. Work on ML security and privacy to date has for the most part conducted in supervised settings, especially in the context of classification tasks, as reflected by our presentation in Sections IV and V below. Since security issues are just as relevant for unsupervised and reinforcement learning tasks, we strive to present results in more general settings when meaningful."}, {"heading": "B. Data Collection: Three Use Cases", "text": "Before one can learn a model that solves a task of interest, training data must be collected. This consists in gathering a generally large set of examples of solutions to the task that one wants to solve with machine learning. For each of the task types introduced above, we describe one example of a task and how the corresponding training dataset would be collected.\nThe first example task is to classify software executables in two categories: malicious and benign. This is a supervised classification problem, where the model must learn some mapping between inputs (software executables) and categorical outputs (this binary task only has two possible classes). The training data comprises a set of labeled instances, each an executable clearly marked as malicious or benign [17].\nSecond, consider the task of extracting a pattern representative of normal activity in a computer network. The training data could consist of TCP dumps [18]. Such a scenario is commonly encountered in anomaly-based network intrusion detection [19]. Since the model\u2019s desired outputs are not given along with the input\u2014that is, the TCP dumps are not associated with any pattern specification\u2014the problem falls under the scope of unsupervised learning.\nFinally, consider the same intrusion-detection problem given access to metrics of system state indicator (CPU load, free memory, network load, etc.) [20]. This variant of the intrusion detector can then be viewed as an agent and the system state\nindicator as rewards for actions taken based on a prediction made by the intrusion detector (e.g., shut down part of the network infrastructure). In this form, the scenario then falls under the reinforcement learning tasks."}, {"heading": "C. Machine Learning Empirical Process", "text": "We describe the general approach taken to create a machine learning model solving one of the tasks described above.\nTraining: Once the data is collected and pre-processed, a ML model is chosen and trained. Most1 ML models can be seen as parametric functions h\u03b8(x) taking an input x and a parameter vector \u03b8. The input x is often represented as a vector of values called features. The space of functions {\u2200\u03b8, x 7\u2192 h\u03b8(x)} is the set of candidate hypotheses to model the distribution from which the dataset was sampled. A learning algorithm analyzes the training data to find the value(s) of parameter(s) \u03b8. When learning is supervised, the parameters are adjusted to reduce the gap between model predictions h\u03b8(x) and the expected output indicated by the dataset. In reinforcement learning, the agent adjusts its policy to take actions that yield the highest reward. The model performance is then validated on a test dataset, which must be disjoint from the training dataset in order to measure the model\u2019s generalization. For a supervised problem like malware classification (see above), the learner computes the model accuracy on a test dataset, i.e. the proportion of predictions h\u03b8(x) that matched the label y (malware or benign) associated with the executable x in the dataset. When learning is done in an online fashion, parameters \u03b8 are updated as new training points become available.\nInference: Once training completes, the model is deployed to infer predictions on inputs unseen during training: i.e., the value of parameters \u03b8 are fixed, and the model computes h\u03b8(x) for new inputs x. In our running example, the model would predict whether an executable x is more likely to be malware or benign. The model prediction may take different forms but the most common for classification is a vector assigning a probability for each class of the problem, which characterizes how likely the input is to belong to that class. For our unsupervised network intrusion detection system, the model would instead return the pattern representation h\u03b8(x) that corresponds to a new input network traffic x."}, {"heading": "D. A Theoretical Model of Learning", "text": "Next, we formalize the semantics of supervised ML algorithms. We give an overview of the Probably Approximately Correct (PAC) model, a theoretical underpinning of these algorithms, here and later use the model in Sections IV, V, and VI to interpret attacks and defenses. Such an interpretation helps discover, from specific attacks and defenses, general principles of adversarial learning that apply across all supervised ML.\nPAC model of learning: PAC learning model has a very rich and extensive body of work [22]. Briefly, the PAC model states that data points (x, y) are samples obtained by sampling from\n1A few models are non-parametric: for instance the nearest neighbor [21].\na fixed but unknown probability distribution D over the space Z = X \u00d7 Y . Here, X is the space of feature values and Y is the space of labels (e.g., Y = {0, 1} for classification or Y = R for regression). The mapping from X to Y is captured by a function h : X \u2192 Y associated with a loss function lh : X\u00d7Y \u2192 R which captures the error made by the prediction h(x) when the true label is y. Examples include the hinge loss [23] used in SVMs or the cross-entropy loss [3]. The learner aims to learn a function h\u2217 from a family of functions H such that the the expected loss (also called risk) r(h) = Ex,y\u223cD[lh(x, y)] is minimal, that is, h\u2217 \u2208 argminh\u2208H r(h).\nOf course, in practice D is not known and only samples ~z = z1, . . . , zn (the training data) is observed. The learning algorithm then uses the empirical loss r\u0302(h, ~z) = 1n \u2211n i=1 lh(xi, yi), where zi = (xi, yi) as a proxy for the expected loss and finds the h that is close to h\u0302 \u2208 argminh\u2208H r\u0302(h, ~z). Thus, all supervised learning algorithm perform this empirical risk minimization (ERM) with the loss function varying across different algorithms. The PAC guarantee states that:\nP (|r(h\u2217)\u2212 r(h\u0302)| \u2264 ) \u2265 1\u2212 \u03b4 (1)\nwhere the probability is over samples ~z used to learn h\u0302. This guarantee holds when two pre-conditions are met: [Condition 1: Uniform bound] given enough samples (called the sample complexity, which depends on , \u03b4 above) that enable a uniform bound of the difference between the actual and empirical risk for all functions in H, and [Condition 2: Good ERM] h\u0302 is close to the true empirical risk minimizer h\u0302.\nStatistical learning is primarily concerned with the uniform bound pre-condition above, wherein a good ERM is assumed to exist and the goal is to find the sample complexity required to learn certain classes of function with certain loss function.\nThe training step in supervised learning algorithms performs the ERM step. The accuracy measured on the test data in machine learning typically estimates the (or some error value correlated with ). In particular, the train test procedure relies on the assumption that training and test data arise from the same, though unknown, distribution D and more importantly the distribution faced in an actual deployment in the inference step is also D. Later we show that most attacks arise from an adversarial modification of D either in training or in inference resulting in a mismatch between the distribution of data used in the learning and the inference phases.\nAnother noteworthy point is that the PAC guarantee (and thus most ML algorithms) is only about the expected loss. Thus, for most data points (x, y) that lie in low probability regions, the predictor h\u0302(x) can be far from the true y yet the output h\u0302 could have high accuracy (as measured by test accuracy) because the accuracy is an estimate of the expected loss. In the extreme, a learning accuracy of 100% could be achieved by predicting correctly in the positive probability region with lot of misclassification in the zero probability regions (or more precisely sets with measure zero; a detailed discussion of this fact is present in a recent paper [24]). An adversary may exploit such misclassification to its advantage. We elaborate on the two points above later in Section VII."}, {"heading": "III. THREAT MODEL", "text": "The security of any system is measured with respect the adversarial goals and capabilities that it is designed to defend against\u2013the systems\u2019 threat model. In this section we taxonomize the definition and scope of threat models in machine learning systems and map the space of security models. We begin by identifying the threat surface of systems built on machine learning to inform where and how an adversary will attempt to subvert the system under attack. For the purpose of exposition of the following Sections, we expand upon previous approaches at articulating a threat model for ML [25], [26]."}, {"heading": "A. The ML Attack Surface", "text": "The attack surface of a system built with data and machine learning is reflective of its purpose. However, one can view systems using ML within a generalized data processing pipeline (see Figure 1, top). At inference, (a) input features are collected from sensors or data repositories, (b) processed in the digital domain, (c) used by the model to produce an output, and (d) the output is communicated to an external system or user and acted upon. To illustrate, consider a generic pipeline, autonomous vehicle, and network intrusion detection systems in Figure 1 (middle and bottom). These systems collect sensor inputs (video image, network events) from which model features (pixels, flows) are extracted and used within the models. The meaning of the model output (stop sign, network attack) is then interpreted and action taken (stopping the car, filtering future traffic from an IP). Here, the attack surface for the system can be defined with respect to the data processing pipeline. Adversaries can attempt to manipulate the collection and processing of data, corrupt the model, or tamper with the outputs.\nRecall that the training of the model is performed using either an offline or online process. In an offline setting, training data is collected or generated. The training data used to learn the model includes vectors of features used as inputs during inference, as well as expected outputs for supervised learning or a reward function for reinforcement learning. The training data may also include additional features not available at runtime (referred to as privileged information in some settings [27]). As discussed below, the means of collection and validation processes offer another attack surface\u2013adversaries who can manipulate the data collection process can do so to induce targeted model behaviors. Similar attacks in an online setting (such as may be encountered in reinforcement learning) can be quite damaging, where the adversary can slowly alter the model with crafted inputs submitted at runtime (e.g., using false training [28]). Online attacks such as these have been commonly observed in domains such as SPAM detection and network intrusion detection [28]."}, {"heading": "B. Adversarial Capabilities", "text": "A threat model is also defined by the actions and information the adversary has at their disposal. The definition of security is made with respect to stronger or weaker adversaries who have more or less access to the system and its data.\nModificationRead Injection Logic Corruption\nTraining\nModel parameters I/O through pipeline I/O through model Model architecture Oracle / Black-box White-box Inference\nThe term capabilities refers to the whats and hows of the available attacks, and indicates the attack vectors one may use on a threat surface. For instance, in the network intrusion detection scenario, an internal adversary may have access to the model used to distinguish attacks from normal behavior, whereas a weaker eaves-dropping adversary would only have access to TCP dumps of the network traffic. Here the attack surface remains the same for both the attacks, but the former attacker is assumed to have much more information and is thus a strictly \u201dstronger\u201d adversary. We explore the range of attacker capabilities in machine learning systems as they relate to inference and training phases (see Figure 2).\nInference Phase: Attacks at inference time\u2014exploratory attacks [25]\u2014do not tamper with the targeted model but instead either cause it to produce adversary selected outputs (incorrect outputs, see Integrity attacks below) or simply use the attack to collect evidence about the model characteristics (reconnaissance, see privacy below). As explored at length in Section V, the effectiveness of such attacks are largely determined by the amount of information that is available to the adversary about the model and its use in the target environment.\nInference phase attacks can be classified into either white box or black box attacks. In white box attacks, the adversary has some information about the model or its original training data, e.g., ML algorithm h, model parameters \u03b8, network structure, or summary, partial, or full training data. Grossly speaking, this information can be divided into attacks that use information about the model architecture (algorithm and structure of the hypothesis h), model parameters \u03b8 (weights), or both. The adversary exploits available information to identify where a model may be exploited. For example, an adversary who has access to the model h and its parameters \u03b8 may identify parts of the feature space for which the model has high error, and exploit that by altering an input to into that space, e.g., adversarial example crafting [30].\nConversely black box attacks assume no knowledge about the model. The adversary in these attacks use information about the setting or past inputs to infer model vulnerability. For example, in a oracle attack, the adversary explores a model by providing a series of carefully crafted inputs and observing outputs [31]. Oracle attacks work because a good\ndeal of information about a model can be extracted from input / output pairs, and relatively little information is required because of the transferability property exhibited by many model architectures (See Section V-B).\nTraining Phase: Attacks on training attempt to learn, influence or corrupt the model itself. The simplest and arguably weakest attack on training is simply accessing a summary, partial or all of the training data. Here, depending on the quality and volume of training data, the adversary can create a substitute model (also referred to as a surrogate or auxiliary model) to use to mount attacks on the victim system. For example, the adversary can use a substitute model to test potential inputs before submitting them to the victim system [32]. Note that these attacks are offline attempts at model reconnaissance, and thus may be used to undermine privacy (see below).\nThere are two broad attack strategies for altering the model. The first alters the training data either by inserting adversarial inputs into the existing training data (injection) or altering the training data directly (modification). In the case of reinforcement learning, the adversary may modify the environment in which the agent is evolving. Lastly, the adversaries can tamper with the learning algorithm. We refer to these attacks as logic corruption. Obviously, any adversary that can alter the learning logic (and thus controls the model itself) is very powerful and difficult to defend against."}, {"heading": "C. Adversarial Goals", "text": "The last piece of a threat model is an articulation of the goals of the adversary. We adopt a classical approach to modeling adversarial goals by modeling desired ends as impacting confidentiality, integrity, and availability (called a CIA model), and adding a fourth property, privacy. Interestingly, a duality emerges when taking a view in this way: attacks on system integrity and availability are closely related in goal and method, as are confidentiality and privacy.\nAs is often the case in security, ML systems face three types of risks: failure to provide integrity, availability, and privacy. Integrity and privacy can both be understood at the level of the ML model itself, as well as for the entire system deploying it. Availability is however ill defined for a model in isolation but makes sense for the ML-based system as a whole. We discuss below the range of adversarial goals that relate to each risk.\nConfidentiality and Privacy: Attacks on confidentiality and privacy are with respect to the model. Put another way, the attacks achieving these goals attempt to extract information about the model or training data as highlighted above. When the model itself represents intellectual property, it requires that the model and its parameters be confidential, e.g., financial market systems [33]. In other contexts it is imperative that the privacy of the training data be preserved, e.g., medical applications [34]. Regardless of the goal, the attacks and defenses for them relate to exposing or preventing the exposure of the model and training data.\nMachine learning models have enough capacity to capture and memorize elements of their training data [35]. As such, it is hard to provide guarantees that participation in a dataset does not harm the privacy of an individual. Potential risks are adversaries performing membership test (to know whether an individual is in a dataset or not) [36], recovering of partially known inputs (use the model to complete an input vector with the most likely missing bits), and extraction of the training data using the model\u2019s predictions [35].\nIntegrity and Availability: Attacks on integrity and ability are with respect to model outputs. Here the goal is to induce model behavior as chosen by the adversary. Attacks attempting to control model outputs are at the heart of integrity attacks\u2014the integrity of the inference process is undermined. For example, attacks that attempt to induce false positives in a face recognition system affect the authentication process\u2019s integrity [37].\nClosely related, attacks on availability attempt to reduce the quality (e.g., confidence or consistency), performance (e.g., speed), or access (e.g., denial of service). Here again, while the goals of these two classes of attacks may be different, the means by which the adversary achieves them is often similar.\nIntegrity is essential in ML, and is the center of attention for most performance metrics used: e.g., accuracy [38]. However, researchers have shown that the integrity of ML systems may be compromised by adversaries capable of manipulating model inputs [30] or its training data [39]. First, the ML model\u2019s confidence may be targeted by an adversary: reducing this value may change the behavior of the overall system. For instance, an intrusion detection system may only raise an alarm when its confidence is over a specified threshold. Input misprocessing aims at misleading the model into producing wrong outputs for some inputs, either modified at the entrance of the pipeline, or at the input of the model directly. Depending on the task type, the wrong outputs differ. For a ML classifier, it may assign the wrong class to a legitimate image, or classify noise with confidence. For an unsupervised feature extractor, it may produce a meaningless representation of the input. For a reinforcement learning agent, it may act unintelligently given the environment state. However, when the adversary is capable of subverting the input-output mapping completely, it can control the model and the system\u2019s behavior. For instance, it may force an automotive\u2019s computer vision system to misprocess a traffic sign, resulting in the car accelerating.\nAvailability is somewhat different than integrity, as it is about the prevention of access to an asset\u2013the asset being an output or an action induced by a model output. Hence, the goal of these attacks is to make the model inconsistent or unreliable in the target environment. For example, the goal of the adversary attacking an autonomous vehicle may be to get it to behave erratically or non-deterministically in a given environment. Yet most of the attacks in this space require corrupting the model through training input poisoning and other confidence reduction attacks using many of the same methods used for integrity attacks.\nIf the system depends on the output of the ML model to take decisions that impact its availability, it may be subject to attacks falling under the broad category of denial of service. Continuing with the previous example, an attack that produces vision inputs that force a autonomous vehicle to stop immediately may cause a denial of service by completely stopping traffic on the highway. More broadly, machine learning models may also not perform correctly when some of their input features are corrupted or missing [40]. Thus, by denying access to these features we can subvert the system."}, {"heading": "IV. TRAINING IN ADVERSARIAL SETTINGS", "text": "As parameters \u03b8 of the hypothesis h are fine-tuned during learning, the training dataset analyzed is potentially vulnerable to manipulations by adversaries. This scenario corresponds to a poisoning attack [25], and is an instance of learning in the presence of non-necessarily adversarial but nevertheless noisy data [41]. Poisoning attacks alter the training dataset by\ninserting, editing, or removing points with the intent of modifying the decision boundaries of the targeted model [39], thus targeting the learning system\u2019s integrity per our threat model from Section III. It is somewhat obvious that an unbounded adversary can cause the learner to learn any arbitrary function h leading to complete unavailability of the system. Thus, all the attacks below bound the adversary in their attacks [42]. Also, in the PAC model, modifications of the training data can be seen as altering the distribution D that generated the training data, thereby creating a mismatch between the distributions used for training and inference. In Section VI-A, we present a line of work that builds on that observation to propose learning strategies robust to distribution drifts [43].\nUpon surveying the field, we note that works almost exclusively discuss poisoning attacks against classifiers (supervised models trained with labeled data). However, as we strive to generalize our observations to other types of machine learning tasks (see Section II), we note that the strategies described below may apply, as for instance many algorithms used for reinforcement learning use supervised submodels."}, {"heading": "A. Targeting Integrity", "text": "Kearns et al. formally analyzed PAC-learnability when the adversary is allowed to modify training samples with probability \u03b2 [39]. For large datasets this adversarial capability can be interpreted as the ability to modify a fraction \u03b2 of the training data. One of the fundamental results in the paper states that achieving learning accuracy (in the PAC model) requires \u03b2 \u2264 1+ for any learning algorithm. For example, to achieve 90% accuracy ( = 0.1) the adversary manipulation rate must be less than 10%. The efforts below explored this result from a practical standpoint and introduced poisoning attacks against ML algorithms. We organize our discussion around the adversarial capabilities highlighted in the preceding section. Unlike some attacks at inference (see Section V-B), training time attacks require some degree of knowledge about the learning model, in order to find manipulations of the data that are damaging to the learned model.\nLabel manipulation: When adversaries are only able to modify the labeling information contained in the training dataset, the attack surface is limited: they must find the most harmful label given partial or full knowledge of the learning algorithm ran by the defender. The baseline strategy is to randomly perturb the labels, i.e. select a new label for a fraction of the training data by drawing from a random distribution. Biggio et al. showed that this was sufficient to degrade inference performance of classifiers learned with SVMs [44], as long as the adversary randomly flips about 40% of the training labels. It is unclear whether this attack would generalize to multi-class classifiers, with k > 2 output classes (these results only considered problems with k = 2 classes, where swapping the labels is guaranteed to be very harmful to the model). The authors also demonstrate that perturbing points classified with confidence by the model in priority is a compelling heuristic to later degrade the model\u2019s performance during inference. It\nreduces the ratio of poisoned points to 30% for comparable drops in inference accuracy on the tasks also used to evaluate random swaps. A similar attack approach has been applied in the context of healthcare [45]. As is the case for the approach in [44], this attack requires that a new ML model be learned for each new candidate poisoning point in order to measure the proposed point\u2019s impact on the updated model\u2019s performance during inference. This high computation cost is due to the largely unknown relationship between performance metrics respectively computed on the training and test data.\nOur take-away IV.1. Search algorithms for poisoning points are computationally expensive because of the complex and often poorly understood relationship between a model\u2019s accuracy on training and test distributions.\nInput manipulation: In this threat model, the adversary can corrupt the input features of training points processed by the learning algorithm, in addition to its labels. These works assume knowledge of the learning algorithm and training set.\nDirect poisoning of the learning inputs: Kloft et al. show that by inserting points in a training dataset used for anomaly detection, they can gradually shift the decision boundary of a simple centroid model, i.e. a model that classifies a test input as malicious when it is too far from the empirical mean of the training data [28]. The detection model is learned in an online fashion\u2014new training data is collected at regular intervals and the parameter values \u03b8 are computed based on a sliding window of that data. Therefore, injection of poisoning data in the training dataset is a particularly easy task for adversaries in these online settings. Poisoning points are found by solving a linear programming problem that maximizes the displacement of the centroid (empirical mean of the training data). This formulation is made possible by the simplicity of the centroid model, which essentially evaluates an Euclidean distance.\nOur take-away IV.2. The poisoning attack surface of a ML system is often exacerbated when learning is performed online, i.e. new training points are added by observing the environment in which the system evolves. In the settings of offline learning, Biggio et al. introduce an attack that also inserts inputs in the training set [46]. These malicious samples are crafted using a gradient ascent method that identifies inputs corresponding to local maxima in the test error of the model. Adding these inputs to the training set results in a degraded classification accuracy at inference. Their approach is specific to SVMs, because it relies on the existence of a closed-form formulation of the model\u2019s test error, which in their case follows from the assumption that support vectors2 do not change as a result of the insertion of poisoning points. Mei et al. introduce a more general framework for poisoning, which finds the optimal changes to the training set in terms of cardinality or the Frobenius norm, as long as the targeted ML model is trained using a convex loss (e.g., linear and logistic regression or SVMs) and its input\n2Support vectors are the subset of training points that suffice to define the decision boundary of a support vector machine.\ndomain is continuous [47]. Their attack is formulated as two nested optimization problems, which are solved by gradient descent after reducing them to a single optimization problem using the inner problem\u2019s Karush-Kuhn-Tucker conditions.\nIndirect poisoning of the learning inputs: Adversaries with no access to the pre-processed data must instead poison the model\u2019s training data before its pre-processing (see Figure 1). For instance, Perdisci et al. prevented Polygraph, a worm signature generation tool [48], from learning meaningful signatures by inserting perturbations in worm traffic flows [49]. Polygraph combines a flow tokenizer together with a classifier that determines whether a flow should be in the signature. Polymorphic worms are crafted with noisy traffic flows such that (1) their tokenized representations will share tokens not representative of the worm\u2019s traffic flow, and (2) they modify the classifier\u2019s threshold for using a signature to flag worms. This attack forces Polygraph to generate signatures with tokens that do not correspond to invariants of the worm\u2019s behavior. Later, Xiao et al. adapted the gradient ascent strategy introduced in [46] to feature selection algorithms like LASSO [50]."}, {"heading": "B. Targeting Privacy", "text": "During training, the confidentiality and privacy of the data and ML model are not impacted by the fact that ML is used, but rather the extent of the adversary\u2019s access to the system hosting the data and model. This is a traditional access control problem, which falls outside the scope of our discussion.\nV. INFERRING IN ADVERSARIAL SETTINGS\nAdversaries may also attack ML systems at inference time. In such settings, they cannot poison the training data or tamper with the model parameters. Hence, the key characteristic that differentiates attackers is their capability of accessing (but not modifying) the deployed model\u2019s internals. White-box adversaries possess knowledge of the internals: e.g., the ML technique used or the parameters learned. Instead, black-box access is a weaker assumption corresponding to the capability of issuing queries to the model or collecting a surrogate training dataset. Black-box adversaries may surprisingly jeopardize the integrity of the model output, but white-box access allows for finer-grain control of the outputs. With respect to privacy, most existing efforts focus on the black-box (oracle) attacks that expose properties of the training data or the model itself."}, {"heading": "A. White-box adversaries", "text": "White-box adversaries have varying degrees of access to the model h as well as its parameters \u03b8. This strong threat model allows the adversary to conduct particularly devastating attacks. While it is often difficult to obtain, white-box access is not always unrealistic. For instance, ML models trained on data centers are compressed and deployed to smartphones [62], in which case reverse engineering may enable adversaries to recover the model\u2019s internals and thus obtain white-box access. Integrity: To target a white-box system\u2019s prediction integrity, adversaries perturb the ML model inputs. In the theoretical\nPAC model, this can be interpreted as modifying the distribution that generates data during inference. Our discussion of attacks against classifiers is two-fold: (1) we describe strategies that require direct manipulation of model inputs, and (2) we consider indirect perturbations resilient to the pre-processing stages of the system\u2019s data pipeline. Although most of the research efforts study classification tasks, we conclude with a discussion of regression and reinforcement learning.\nDirect manipulation of model inputs: Here, adversaries alter the feature values processed by the ML model directly. When the model is a classifier, the adversary seeks to have it assign a wrong class to perturbed inputs [25]. Szegedy et al. coined the term adversarial example to refer to such inputs [30]. Similar to concurrent work [51], they formalize the search for adversarial examples as the following minimization problem:\narg min r h(x+ r) = l s.t. x+ r \u2208 D (2)\nThe input x, correctly classified by h, is perturbed with r such that the resulting adversarial example x\u2217 = x + r remains in the input domain D but is assigned the target label l. This is a source-target misclassification as the target class l 6= h(x) is chosen [26]. For non-convex models h like DNNs, the authors apply the L-BFGS algorithm [63] to solve Equation 2. Surprisingly, DNNs with state-of-the-art accuracy on object recognition tasks are misled by small perturbations r.\nTo solve Equation 2 efficiently, Goodfellow et al. introduced the fast gradient sign method [52]. A linearization assumption reduces the computation of an adversarial example x\u2217 to:\nx\u2217 = x+ \u00b7 sign(\u2207~xJh(\u03b8, x, y)) (3)\nwhere Jh is the cost function used to train the model h. Despite the approximation made, a model with close to state-of-the-art performance on MNIST3 misclassifies 89.4% of this method\u2019s adversarial examples. This empirically validates the hypothesis that erroneous model predictions on adversarial examples are likely due to the linear extrapolation made by components of ML models (e.g., individual neurons of a DNN) for inputs far from the training data. In its canonical form, the technique is designed for misclassification (in any class differing from the correct class), but it can be extended to choose the target class.\nOur take-away V.1. Adversarial examples exist in halfspaces of the model\u2019s output surface because of the overly\n3The MNIST dataset [64] is a widely used corpus of 70,000 handwritten digits used for validating image processing machine learning systems.\nlinear extrapolation that models, including non-linear ones, make outside of their training data [52], [65]. Follow-up work reduced the size of a perturbation r according to different metrics [53], [66]. Papernot et al. introduced a Jacobian-based adversarial example crafting algorithm that minimizes the number of features perturbed, i.e. the L0 norm of r [26]. On average, only 4% of the features of an MNIST test set input are perturbed to have it classified in a chosen target class with 97% success. This proves essential when the ML model has a discrete input domain for which only a subset of the features can be modified easily by adversaries. This is the case of malware detectors: in this application, adversarial examples are malware applications classified as benign [56].\nClassifiers always output a class, even if the input is out of the expected distribution. It is therefore not surprising that randomly sampled inputs can be constrained to be classified in a class with confidence [52], [67]. The security consequences are not so important since humans would not classify these samples in any of the problem\u2019s classes. Unfortunately, training models with a class specific to rubbish (out of distribution) samples does not mitigate adversarial examples [52].\nIndirect manipulation of model inputs: When the adversary cannot directly modify feature values used as inputs of the ML model, it must find perturbations that are preserved by the data pipeline that precedes the classifier in the overall targeted system. Strategies operating in this threat model construct adversarial examples in the physical domain stage of Figure 1.\nKurakin et al. showed how printouts of adversarial examples produced by the fast gradient sign algorithm were still misclassified by an object recognition model [57]. They fed the model with photographs of the printouts, thus reproducing the typical pre-processing stage of a computer vision system\u2019s data pipeline. They also found these physical adversarial examples to be resilient to pre-processing deformations like contrast modifications or blurring. Sharif et al. applied the approach introduced in [30] to find adversarial examples that are printed on glasses frames, which once worn by an individual result in its face being misclassified by a face recognition model [37]. Adding penalties to ensure the perturbations are physically realizable (i.e., printable) in Equation 2 is sufficient to conduct misclassification attacks (the face is misclassified in any wrong class), and to a more limited extent source-target misclassification attacks (the face is misclassified in a chosen target class).\nOur take-away V.2. To be resilient to the pipeline\u2019s deformations, adversarial examples in physical domains\nneed to introduce adapted, often larger, perturbations. As a natural extension to [67] (see above), it was shown that rubbish audio can be classified with confidence by a speech recognition system [68]. Consequences are not as important in terms of security then [37]: the audio does not correspond to any legitimate input expected by the speech system or humans.\nBeyond classification: While most work has focused on attacking classifiers, Alfeld et al. [54] look at autoregressive models. An autoregressive model is one where the prediction xt of a time series depends on previous k realizations of x, that is, xt = \u2211k i=1 cixt\u2212i; such models are widely used in market predictions. The adversary can manipulate the input data with the goal of achieving their desired prediction, given budget constraints for the adversary. The author\u2019s formulate the adversary\u2019s manipulation problem as a quadratic optimization problem and provide efficient solutions for it.\nAdversarial behaviors were also considered in reinforcement learning, albeit not to address security but rather to improve the utility of models. Pinto et al. improve a model for grasping objects by introducing a competing model that attempts to snatch objects before the original model successfully grasps them [69]. The two models are trained, a\u0300 la Generative Adversarial Networks [70], with competitive cost functions.\nOur take-away V.3. Although research has focused on classification problems, algorithms developed to craft adversarial examples naturally extend to other settings like reinforcement learning: e.g., the adversary perturbs a video game frame to force an agent to take wrong actions.\nPrivacy: As discussed in Section III, adversaries targeting the privacy of a ML system are commonly interested in recovering information about either the training data or the learned model itself. The simplest attack consists in performing a membership test, i.e. determining whether a particular input was used in the training dataset of a model. Stronger opponents may seek to extract fully or partially unknown training points. Few attacks operate in the white-box threat model, as the black-box model (see below) is more realistic when considering privacy.\nAteniese et al. infer statistical information about the training dataset from a trained model h\u03b8 [55]: i.e., they analyze the model to determine whether its training data verified a certain statistical property. Their attack generates several datasets, where some exhibit the statistical property and others don\u2019t. A model is trained on each dataset independently. The adversary then trains a meta-classifier: it takes as its inputs these models and predicts if their dataset verified the statistical property. The meta-classifier is finally applied to the model of interest h\u03b8 to fulfill the initial adversarial goal. One limitation is that all classifiers must be trained with the same technique than h\u03b8."}, {"heading": "B. Black-box adversaries", "text": "When performing attacks against black-box systems, adversaries do not know the model internals. This prohibits the strategies described in Section V-A: for instance, integrity attacks require that the attacker compute gradients defined using the model h and its parameters \u03b8. However, black-box\naccess is perhaps a more realistic threat model, as all it requires is access to the output responses. For instance, an adversary seeking to penetrate a computer network rarely has access to the specifications of the intrusion detection system deployed\u2013 but they can often observe how it responds to network events. Similar attacks are key to performing reconnaissance in networks to determine their environmental detection and response policies. We focus on strategies designed irrespectively of the domain ML is being applied to, albeit heuristics specific to certain applications exist [71], [29], e.g., spam filtering.\nA common threat model for black-box adversaries is the one of an oracle, borrowed from the cryptography community: the adversary may issue queries to the ML model and observe its output for any chosen input. This is particularly relevant in the increasingly popular environment of ML as a Service cloud platforms, where the model is potentially accessible through a query interface. A PAC based work shows that with no access to the training data or ML algorithm, querying the target model and knowledge of the class of target models allows the adversary to reconstruct the model with similar amount of query data as used in training [72] Thus, a key metric when comparing different attacks is the wealth of information returned by the oracle, and the number of oracle queries.\nIntegrity: Lowd et al. estimate the cost of misclassification in terms of the number of queries to the black-box model [73]. The adversary has oracle access to the model. A cost function is associated with modifying an input x to a target instance x\u2217. The cost function is a weighted l1difference between x\u2217 and x. The authors introduce ACRE learnability, which poses the problem of finding the least cost modification to have a malicious input classified as benign using a polynomial number of queries to the ML oracle. It is shown that continous features allow for ACRE learnability while discrete features make the problem NP-hard. Because ACRE learnability also depends on the cost function, it is a different problem from reverse engineering the model. Following up on this thread, Nelson et al. [74] identify the space of convex inducing classifiers\u2014 those where one of the classes is a convex set\u2014that are ACRE learnable but not necessarily reverse engineerable.\nDirect manipulation of model inputs: It has been hypothesized that in classification, adversaries with access to class probabilities for label outputs are only slightly weaker than white-box adversaries. In these settings, Xu et al. apply a computationally expensive genetic algorithm. The fitness of genetic variants obtained by mutation is defined in terms of the oracle\u2019s class probability predictions [58]. The approach evades a random forest and SVM used for malware detection.\nWhen the adversary cannot access probabilities, it is more difficult to extract information about the decision boundary, a pre-requisite to find input perturbations that result in erroneous predictions. In the following works, the adversary only observes the first and last stage of the pipeline from Figure 1: e.g., the input (which they produce) and the class label in classification tasks. Szegedy et al. first observed adversarial example transferability: i.e., the property that adversarial ex-\namples crafted to be misclassified by a model are likely to be misclassified by a different model. This transferability property holds even when models are trained on different datasets.\nAssuming the availability of surrogate data to the adversary, Srndic et al. explored the strategy of training a substitute model for the targeted one [32]. They exploit a semantic gap to evade a malware PDF detector: they inject additional features that are not interpreted by PDF renderers. As such, their attack does not generalize well to other application domains or models.\nPapernot et al. used the cross-model transferability of adversarial samples [30], [52] to design a black-box attack [31]. They demonstrated how attackers can force a remotely hosted ML model to misclassify inputs without access to its architecture, parameters, or training data. The attack trains a substitute model using synthetic inputs generated by the adversary and labeled by querying the oracle. The substitute model is then used to craft adversarial examples that transfer back to\u2014are misclassified by\u2014the originally targeted model. They force a DNN trained by MetaMind, an online API for deep learning, to misclassify inputs at a rate of 84.24%. In a follow-up work [61], they show that the attack generalizes to many ML models by having a logistic regression oracle trained by Amazon misclassify 96% of the adversarial examples crafted.\nOur take-away V.4. Black-box attacks make it more difficult for the adversary to choose a target class in which the perturbed input will be classified by the model, when compared to white-box settings.\nData pipeline manipulation: Using transferability, Kurakin et al. [57] demonstrated that physical adversarial example (i.e., printouts of an adversarial example) can also mislead an object recognition model included in a smarphone app, which differs from the one used to craft the adversarial example. These findings suggest that black-box adversaries are able to craft inputs misclassified by the ML model despite the preprocessing stages of the system\u2019s data pipeline.\nPrivacy: In black-box settings, adversaries targeting privacy may pursue the goals already discussed in white-box settings: membership attacks and training data extraction. In addition, since the model internals are now unknown to them, extracting model parameters themselves is now a valid goal.\nMembership attacks: This type of adversary is looking to test whether or not a specific point was part of the training dataset analyzed to learn the model\u2019s parameter values. Shokri et al. show how to conduct this type of attack, named membership inference, against black-box models [36]. Their strategy exploits differences in the model\u2019s response to points that were or were not seen during training. For each class of the targeted black-box model, they train a shadow model, with the same ML technique. Each shadow model is trained to solve the membership inference test for samples of the corresponding class. The procedure that generates synthetic data is initialized with a random input and performs hill climbing by querying the original model to find modifications of the input that yield a classification with strong confidence in a class of the problem.\nThese synthetic inputs are assumed to be statistically similar to the inputs contained in the black-box model\u2019s training dataset.\nTraining data extraction: Fredrikson et al. present the model inversion attack [59]. For a medicine dosage prediction task, they show that given access to the model and auxiliary information about the patient\u2019s stable medicine dosage, they can recover genomic information about the patient. Although the approach illustrates privacy concerns that may arise from giving access to ML models trained on sensitive data, it is unclear whether the genomic information is recovered because of the ML model or the strong correlation between the auxiliary information that the adversary also has access to (the patient\u2019s dosage) [75]. Model inversion enables adversaries to extract training data from observed model predictions [35]. However, the input extracted is not actually a specific point of the training dataset, but rather an average representation of the inputs that are classified in a class\u2014similar to what is done by saliency maps [76]. The demonstration is convincing in [35] because each class corresponds to a single individual.\nModel extraction: Among other considerations like intellectual property, extracting ML model has privacy implications\u2014as models have been shown to memorize training data at least partially. Tramer et al. show how to extract parameters of a model from the observation of its predictions [60]. Their attack is conceptually simple: it consists in applying equation solving to recover parameters \u03b8 from sets of observed input-output pairs (x, h\u03b8(x)). However, the approach does not scale to scenarios where the adversary looses access to the probabilities returned for each class, i.e. when it can only access the label. This leaves room for future work to improve upon such extraction techniques to make them practical."}, {"heading": "VI. TOWARDS ROBUST, PRIVATE, AND ACCOUNTABLE MACHINE LEARNING MODELS", "text": "After presenting attacks conducted at training in Section IV and inference in Section V, we cover efforts at the intersection of security, privacy, and ML that are relevant to the mitigation of these previously discussed attacks. We draw parallels between the seemingly unrelated goals of: (a) robustness to distribution drifts, (b) learning privacy-preserving models, and (c) fairness and accountability. Many of these remain largely open problems, thus we draw insights useful for future work."}, {"heading": "A. Robustness of models to distribution drifts", "text": "To mitigate the integrity attacks presented in Section V, ML needs to be robust to distribution drifts: i.e., situations where the training and test distributions differ. Indeed, adversarial manipulations are instances of such drifts. During inference, an adversary might introduce positively connotated words in spam emails to evade detection, thus creating a test distribution different from the one analyzed during training [29]. The opposite, modifying the training distribution, is also possible: the adversary might include an identical keyword in many spam emails used for training, and then submit spam ommiting that keyword at test time. Within the PAC framework, a\ndistribution drift violates the assumption that more training data reduces the learning algorithm\u2019s error rate. We include a PAC-based analysis of learning robust to distribution drifts.\nDefending against training-time attacks: Most defense mechanism at training-time rely on the fact that poisoning samples are typically out of the expected input distribution.\nRubinstein et al. [77] pull from robust statistics to build a PCA-based detection model robust to poisoning. To limit the influence of outliers to the training distribution, they constrain the PCA algorithm to search for a direction whose projections maximize a univariate dispersion measure based on robust projection pursuit estimators instead of the standard deviation. In a similar approach, Biggio et al. limit the vulnerability of SVMs to training label manipulations by adding a regularization term to the loss function, which in turn reduces the model sensitivity to out-of-diagonal kernel matrix elements [44]. Their approach does not impact the convexity of the optimization problem unlike previous attempts [78], [79], which reduces the impact of the defense mechanism on performance.\nBarreno et al. make proposals to secure learning [25]. These include the use of regularization in the optimization problems solved to train ML models. This removes some of the complexity exploitable by an adversary (see Section VII). The authors also mention an attack detection strategy based on isolating a special holdout set to detect poisoning attempts. Lastly, they suggest the use of disinformation with for instance honeypots [80] and randomization of the ML model behavior.\nDefending against inference-time attacks: The difficulty in attaining robustness to adversarial manipulations at inference, i.e. malicious test distribution drifts, stems from the inherent complexity of output surfaces learned by ML techniques. Yet, a paradox arises from the observation that this complexity of ML hypotheses is necessary to confer modeling capacity sufficient to train robust models (see Section VII). Defending against attacks at inference remains largely an open problem. We explain why mechanisms that smooth model outputs in infinitesimal neighborhoods of the training data fail to guarantee integrity. Then, we present more effective strategies that make models robust to larger perturbations of their inputs\nDefending by gradient masking: Most integrity attacks in Section V rely on the adversary being able to find small perturbations that lead to significant changes in the model output. Thus, a natural class of defenses seeks to reduce the sensitivity of models to small changes made to their inputs. This sensitivity is estimated by computing first order derivatives of the model h with respect to its inputs. These gradients are minimized during the learning phase: hence the gradient masking terminology. We detail why this intuitive strategy is bound to have limited success because of adversarial example transferability.\nGu et al. introduce a new ML model, which they name deep contractive networks, trained using a smoothness penalty [81]. The penalty is defined with the Frobenius norm of the model\u2019s Jacobian matrix, and is approximated layer by layer to preserve computational efficiency. This approach was later generalized\n(a) Defended model (b) Substitute model\nx x\u21e4x\u21e4 x r r\nh(x\u21e4)\nh(x)\nFig. 4. Evading infinitesimal defenses using transferability: the defended model is very smooth in neighborhoods of training points: i.e., gradients of the model outputs with respect to its inputs are zero and the adversary does not know in which direction to look for adversarial examples. However, the adversary can use the substitute model\u2019s gradients to find adversarial examples that transfer back to the defended model. Note that this effect would be exacerbated by models with more than one dimension.\nto other gradient-based penalties in [82], [83]. Although Gu et al. show that contractive models are more robust to adversaries, the penalty greatly reduces the capacity of these models, with consequences on their performance and applicability.\nThe approach introduced in [84] does not involve the expensive computation of gradient-based penalties. The technique is an adaptation of distillation [62], a mechanism designed to compress large models into smaller ones while preserving prediction accuracy. In a nutshell, the large model labels data with class probabilities, which are then used to train the small model. Instead of compression, the authors apply distillation to increase the robustness of DNNs to adversarial samples. They report that the additional entropy in probability vectors (compared to labels) yields models with smoother output surfaces. In experiments with the fast gradient sign method [85] and the Jacobian attack [84], larger perturbations are required to achieve misclassification of adversarial examples by the distilled model. However, [86] identified a variant of the attack of [26] which distillation fails to mitigate on one dataset.\nA simpler variant of distillation, called label smoothing [87], improves robustness to adversarial samples crafted using the fast gradient sign method [88]. It replaces hard class labels (a vector where the only non-null element is the correct class index) with soft labels (each class is assigned a value close to 1/N for a N -class problem). However this variant was found to not defend against more computation expensive but precise attacks like the Jacobian-based iterative attack [26].\nThese results suggest limitations of defense strategies that seek to conceal gradient-based information exploited by adversaries. In fact, Papernot et al. report that defensive distillation can be evaded using a black-box attack [31]. We here detail the reason behind this evasion. When applying defense mechanisms that smooth a model\u2019s output surface, as illustrated in Figure 4.(a), the adversary cannot craft adversarial examples because the gradients it needs to compute (e.g., the derivative of the model output with respect to its input) have values close to zero. In [31], this is referred to as gradient masking. The adversary may instead use a substitute model, illustrated in Figure 4.(b), to craft adversarial examples, since the substitute is not impacted by the defensive mechanism and will still have the gradients necessary to find adversarial directions. Due to the adversarial example transferability property [30] described\nin Section V, the adversarial examples crafted using the substitute are also misclassified by the defended model. This attack vector is likely to apply to any defense performing gradient masking, i.e. any mechanism defending against adversarial examples in infinitesimal neighborhoods of the training points.\nOur take-away VI.1. Any defense that tampers with adversarial example crafting heuristics (e.g., by masking gradients used by adversaries) but does not mitigate the underlying erroneous model predictions can be evaded using a transferability-based black-box attack.\nDefending against larger perturbations: Szegedy et al. [30] first suggested injecting adversarial samples, correctly labeled, in the training set as a means to make the model robust. They showed that models fitted with this mixture of legitimate and adversarial samples were regularized and more robust to future adversaries. The efficiency of the fast gradient sign method allows for the integration of an adversarial objective during training. The adversarial objective minimizes the error between the model\u2019s prediction on the adversarial example and the original sample label. This adversarial training continuously makes the model more robust to adversarial examples crafted with the latest model parameters. Goodfellow et al. show that this reduces the misclassification rate of a MNIST model from 89.4% to 17.9% on adversarial examples [52].\nHuang et al. [66] developed the intuition behind adversarial training, i.e. penalize misclassification of adversarial examples. They formulate a min-max problem between the adversary applying perturbations to each training point to maximize the model\u2019s classification error, and the learning procedure attempting to minimize the model\u2019s misclassification error:\nmin h \u2211 i max \u2016r(i)\u2016\u2264c l(h(xi + r (i)), yi) (4)\nThey solve the problem using stochastic gradient descent [89]. Their experimentation shows improvements over [52], but they are often statistically non-significative. The non-adaptiveness of adversarial training explains some of the results reported by Moosavi et al. [53], where they apply the defense with an attack and evaluate robustness with another one.\nOur take-away VI.2. In adversarial training, it is essential to include adversarial examples produced by all known attacks, as the defensive training is non-adaptive.\nInterpreting robust learning in the PAC model: As stated earlier, inference attacks can be interpreted in the PAC model as the adversary choosing a different data distribution during inference from the one used in training. Thus, an approach to handle such adversaries is to modify the distribution D that is used to generate the training data so that training data samples reflect a probability distribution that maybe encountered during inference, i.e., a distribution that places more probability mass on possibly adversarially classified data. The additions of adversarial samples in the training data [30], [53] or modifying the training loss function [66] can be viewed as modifying the training distribution D towards such an end.\nRecall that the PAC model captures the fact that learning algorithms only optimize for expected loss and hence existence of mis-classfied instances can never be ruled out completely. Thus, a formal approach of modifying the training data must also consider the adversary\u2019s cost in modifying the distribution in order to tractably deal with adversarial manipulation. Game theory is a natural tool to capture such defender-adversary interaction. Next, we use the Stackelberg game framework to capture the adversarial interaction. It is a model for defender-adversary interaction where the defender lays out her strategy (in this paper the classifier) and the adversary responds optimally (choose a least cost evasion). Stackelberg games also allow for scalability compared to the corresponding simultaneous move game [90], [91].\nBruckner et al. [92] first recognized that test data manipulation can be seen in the PAC framework as modifying the distribution D to .\nD. Then, the learner\u2019s expected cost for output h is E\nx,y\u223c . D [lh(x, y)]. The basic approach in all game based adversarial learning technique is to associate a cost function c : X \u00d7 X \u2192 R that provides the cost c(x, x\u2032) for the adversary in modifying the feature vector x to x\u2032. The game involves the following two stages (we provide a generalization of the statistical classification game by Hardt et al. [93]):\n1) The defender publishes its hypothesis h\u0302. She has knowledge of c, and training samples drawn according to D.\n2) The adversary publishes a modification function \u2206. The defender\u2019s loss is Ex,y\u223cD[lh(\u2206(x), y)] and the adversary\u2019s cost is Ex,y\u223cD[lah(\u2206(x), y)+c(x,\u2206(x))] where l a h(x) is a loss function for the adversary that captures the loss when the prediction is h(x) and the true output is y. It is worth pointing out that \u2206 : X \u2192 X can depend on the hypothesis h\u0302. This game follows the test data manipulation framework described earlier. The function \u2206 induces a change of the test distribution D to some other distribution .\nD4 and the defender\u2019s loss function can be written as E\nx,y\u223c . D [lh(x, y)]. Thus, just like the PAC framework, the costs for both players are stated in terms of the unknown D (or .\nD) and then the empirical counterparts of these functions are:\n1\nn n\u2211 i=1 lh(\u2206(xi), yi) and 1 n n\u2211 i=1 lah(\u2206(xi), yi) + c(xi,\u2206(xi))\nThe Stackelberg equilibrium computation problem is stated below. This problem is the analogue of the empirical risk minimization that the learner solves in the standard setting.\nmin n\u2211 i=1 lh(\u2206(xi), yi)\n\u2206 \u2208 argmin \u2206 n\u2211 i=1 lah(\u2206(xi), yi) + c(xi,\u2206(xi))\nUnlike the standard empirical risk minimization, this problem is a bi-level optimization problem and is in general NP Hard.\n4If (X,Y ) is a random value distributed according to D, then the distribution of (\u2206(X), Y ) is . D.\nBruckner et al. [92] add a regularizer for the learner and the cost function c(x, x\u2032) as the l2 distance between x and x\u2032. They solve the problem with a sequential quadratic approach.\nFollowing the approach of Lowd et al. [73], Li et al. [94] use a cost function that is relevant for many security settings. The adversary is interested in classifying a malicious data point as non-malicious. Thus, the cost function only imposes costs for modifying the malicious data points."}, {"heading": "B. Learning and Inferring with Privacy", "text": "One way of defining privacy-preserving models is that they do not reveal any additional information about the subjects involved in their training data. This is captured by differential privacy [95], a rigorous framework to analyze the privacy guarantees provided by algorithms. Informally, it formulates privacy as the property that an algorithm\u2019s output does not differ significantly statistically for two versions of the data differing by only one record. In our case, the record is a training point and the algorithm the ML model.\nA randomized algorithm is said to be (\u03b5, \u03b4) differentially private if for two neighboring training datasets T, T \u2032, i.e. which differ by at most one training point, the algorithm A satisfies for any acceptable set S of algorithm outputs:\nPr[A(T ) \u2208 S] \u2264 e\u03b5Pr[A(T \u2032) \u2208 S] + \u03b4 (5)\nThe parameters (\u03b5, \u03b4) define an upper bound on the probability that the output of A differs between T and T \u2032. Parameter \u03b5 is a privacy budget: smaller budgets yield stronger privacy guarantees. The second parameter \u03b4 is a failure rate for which it is tolerated that the bound defined by \u03b5 does not hold.\nTraining: The behavior of a ML system needs to be randomized in order to provide privacy guarantees. At training, random noise may be injected to the data, the cost minimized by the learning algorithm, or the values of parameters learned.\nAn instance of training data randomization is formalized by local privacy [96]. In the scenario where users send reports to a centralized server that trains a model with the data collected, randomized response protects privacy: users respond to server queries with the true answer at a probability q, and otherwise return a random value with probability 1 \u2212 q. Erlingsson et al. showed that this allowed the developers of a browser to collect meaningful and privacy-preserving usage statistics from users [97]. Another way to obtain randomized training data is to first learn an ensemble of teacher models on data partitions, and then use these models to make noisy predictions on public unlabeled data, which is used to train a private student model. This strategy was explored in [98], [99].\nChaudhuri et al. show that objective perturbation, i.e. introducing random noise\u2014drawn from an exponential distribution and scaled using the model sensitivity5\u2014in the cost function minimized during learning, can provide \u03b5-differential privacy [100]. Bassily et al. provide improved algorithms and\n5In differential privacy research, sensitivity denotes the maximum change in the model output when one training point is changed. This is not identical to the sensitivity of ML models to adversarial perturbations (see Section V).\nprivacy analysis, along with references to many of the works intervening in private empirical risk minimization [101]\nOur take-away VI.3. Learning models with differential privacy guarantees is difficult because the sensitivity of models is unknown for most interesting ML techniques. Despite noise injected in parameters, Shokri et al. showed that large-capacity models like deep neural networks can be trained with multi-party computations from perturbed parameters and provide differential privacy guarantees [102]. Later, Abadi et al. introduced an alternative approach to improve the privacy guarantees provided: the strategy followed is to randomly perturb parameters during the stochastic gradient descent performed by the learning algorithm [103].\nInference: To provide differential privacy, the ML\u2019s behavior may also be randomized at inference by introducing noise to predictions. Yet, this degrades the accuracy of predictions, since the amount of noise introduced increases with the number of inference queries answered by the ML model. Note that different forms of privacy can be provided during inference. For instance, Dowlin et al. use homomorphic encryption [104] to encrypt the data in a form that allows a neural network to process it without decrypting it [105]. Although, this does not provide differential privacy, it protects the confidentiality of each individual input. The main limitations are the performance overhead and the restricted set of arithmetic operations supported by homomorphic encryption, which introduce additional constraints in the architecture design of the ML model."}, {"heading": "C. Fairness and Accountability in Machine Learning", "text": "The opaque nature of ML generates concerns regarding a lack of fairness and accountability of decisions taken based on the model predictions. This is especially important in applications like credit decisions or healthcare [106].\nFairness: In the pipeline from Figure 1, fairness is relevant to the action taken in the physical domain based on the model prediction. It should not nurture discrimination against specific individuals [107]. Training data is perhaps the strongest source of bias in ML. For instance, a dishonest data collector might adversarially attempt to manipulate the learning into producing a model that discriminates against certain groups. Historical data also inherently reflects social biases [108]. To learn fair models, Zemel et al. first learn an intermediate representation that encodes a sanitized variant of the data, as first discussed in [109]. Edwards et al. showed that fairness could be achieved by learning in competition with an adversary trying to predict the sensitive variable from the fair model\u2019s prediction [110]. They find connections between fairness and privacy, as their approach also applies to the task of removing sensitive annotations from images. We expect future work at the intersection of fairness and topics discussed in this paper to be fruitful.\nAccountability: Accountability explains model predictions using the ML model internals h\u03b8. This is fundamentally relevant to understanding model failures on adversarial examples. Few models are interpretable by design, i.e., match human reason-\ning [111]. Datta et al. introduced quantitative input influence measures to estimate the influence of specific inputs on the model output [112]. Another avenue to provide accountability is to compute inputs that the machine learning model\u2019s components are most sensitive to. An approach named activation maximization synthesizes an input that highly activates a specific neuron in a neural network [113]. The challenge lies in producing synthetic inputs easily interpreted by humans [114] but faithfully representing the model\u2019s behavior."}, {"heading": "VII. NO FREE LUNCH IN ADVERSARIAL LEARNING", "text": "We begin by pointing out that if a classifier is perfect, i.e., predicts the right class for every possible input, then it cannot be manipulated. Thus, the presence of adversarial examples is a manifestation of the classifier being inaccurate on many inputs. Hence the dichotomy between robustness to adversarial examples and better prediction is a false one. Also, it is wellknown in ML that, given enough data, more complex hypothesis classes (e.g., non-linear classifier as opposed to linear ones) provide better prediction (see Figure VII). As a result, we explore the interaction between prediction loss, adversarial manipulation at inference and complexity of hypothesis class.\nRecall the theoretical characterization for data poisoning by Kearns and Li [39] (see Section IV). While poisoning attacks can be measured by the percentage of data modified, mathematically describing an attack at inference is non-obvious. Thus, our first result in this section is an identification of the characteristics of an effective attack at inference. Our second result reveals that, given a positive probability of presence of an adversary, any supervised ML algorithm suffers from performance degradation under an effective attack. Finally, our third result is that increased capacity is required for resilience to adversarial examples (and can also give more precision as a by-product). But to prevent empirical challenges, e.g., overfitting, more data is needed to accompany the increase in capacity. Yet, in most practical settings, one is given a dataset of fixed size, which creates a tension between resilience and precision. A trade-off between the two must be found by empirically searching for the optimal capacity to model the underlying distribution. Note that this result (presented below) is analogous to the no free lunch result in data privacy that captures the tension between utility and privacy [115], [116].\nIn the PAC learning model, data x, y is sampled from a distribution D. Recall that the learner learns h\u0302 such that P (|r(h\u2217) \u2212 r(h\u0302)| \u2264 ) \u2265 1 \u2212 \u03b4. For this section, we assume that there is enough data6 so that , \u03b4 are negligible, hence we assume h\u0302 is same as h\u2217 for all practical purpose. Recall that the performance of any learning algorithm is characterized by the expected loss of its output h: Ex,y\u223cD[lh(x, y)]. In the real world, it is often not known whether an adversary will be present or not. Thus, we assume that an adversary is present with probability q \u2208 (0, 1); where q is not extremely close to 0 or 1 so that both q and 1 \u2212 q are not negligible. Also,\n6This assumption is practical in many applications today. Moreover, insufficient data presents fundamental information theoretic limitations [22] on learning accuracy problems in the benign (without adversaries) setting itself.\nrecall that an attack in the inference phase is captured by an adversarially modified distribution .\nD of test data. Our first result is the following definition that characterizes an effective attack by the adversary. \u2022 \u03b1-effective attack against H and D: the best\nhypothesis . h \u2217\nin the adversarial setting, i.e., .\nh \u2217 \u2208 argminh\u2208HEx,y\u223c .D[lh(x, y)] still suffers a loss E x,y\u223c . D [l . h \u2217(x, y)] such that for the best hypothesis in the benign setting h\u2217 \u2208 argminh\u2208HEx,y\u223cD[lh(x, y)]\nE x,y\u223c . D [l . h \u2217(x, y)] = Ex,y\u223cD[lh\u2217(x, y)] + \u03b1\nThis definition implies that for \u03b1 > 0 it is not trivial to defend against the modified distribution that the adversary presents compared to the benign scenario, since even the best hypothesis . h \u2217 against the modified distribution .\nD suffers a greater loss than the best hypothesis h\u2217 in the benign case.\nNote that the definition above does not restrict the adversary to choose any particular .\nD, that is, the adversary is not restricted to a particular attack. The definition is parametrized by H and D, that is, the attack is effective against the given choice of hypothesis class H and data distribution D. We leave open the research question about whether a mathematical characterization of when such attacks exists or is feasible given the attackers cost\u2013however, we illustrate in Figure VII-C why it is reasonable to assume that such attacks abound in practice.\nFor the sake of comparison, we contrast our definition with a possible alternate attack definition: one may call an attack effective if E\nx,y\u223c . D\n[lh\u2217(x, y)] = Ex,y\u223cD[lh\u2217(x, y)] + \u03b1, that\nis, the adversarial choice of .\nD cause an increase in prediction loss against the deployed h\u2217. While such an attack is indeed\npractical as shown in Figure VII-C, the impact of such an attack can be reduced if a different data distribution D is used or by gathering more data from the feature space (such as adding the adversarial samples back into training data). We present such an example in Appendix A. Our definition of \u03b1-effective attack leads to an impossibility of defense result that rules out any defense for any data distribution D and any defense measures based on gathering more data; further, later the same definition reveals the fundamental importance of the complexity of the hypothesis class used in adversarial settings.\nNo free lunch: In our result below, we reveal that higher the probability that an adversary is present leads to higher expected loss for any learner. In fact, we prove a stronger result: we show the above statement holds even if the learner is allowed to combine outputs in a randomized manner. Thus, we define the set of hypothesis that can be formed by choosing a set of hypothesis and randomly choosing one hypothesis: let hq(S) = h such that h \u2208 S and h is chosen from S according to distribution q, then R(H) = \u222aS\u2286H,S finite \u222aq {hq(S)}.\nTheorem 1. Fix any hypothesis (function) class H and distribution D, and assume that an attacker exists with probability q. Given the attacker uses an \u03b1-effective attack against H and D with \u03b1 \u2265 \u03b10 > 0, for all hypothesis h \u2208 R(H) the learner\u2019s loss is at least\nEx,y\u223cD[lh\u2217(x, y)] + q\u03b10\nThis theorem is proved in Appendix B. While the above result is a lower bound negative result, next, we present a positive upper bound result that shows that increasing the complexity of the hypothesis class considered by the learner can lead to better protection against adversarial manipulation. Towards that end, we begin by defining the lowest loss possible against a given distribution D: lD = minhEx,y\u223cD[lh(x, y)]. \u2022 \u03b2-rich hypothesis class: A hypothesis class H\u2032\nwith the following properties: (1) H \u2282 H\u2032 and (2) Ex,y\u223cD[lh\u2032(x, y)] \u2264 min{lD, Ex,y\u223cD[lh\u2217(x, y)] \u2212 \u03b2} for h\u2032 \u2208 argminh\u2208H\u2032 Ex,y\u223cD[lh(x, y)], h\u2217 \u2208 argminh\u2208HEx,y\u223cD[lh(x, y)] and all D.\nIntuitively, H\u2032 is a more complex hypothesis class that provides lower minimum loss against any possible distribution.\nTheorem 2. Fix any hypothesis (function) class H and distribution D and a \u03b2-rich hypothesis class H\u2032. Assume the attacker is present with probability q and lD << Ex,y\u223cD[lh\u2217(x, y)] \u2212 \u03b2 and l .D << Ex,y\u223c .D[l .h\u2217(x, y)] \u2212 \u03b2. Given the attacker that uses an \u03b1-effective against H and D with \u03b1 = \u03b10 and the learner uses the \u03b2-rich hypothesis class H\u2032, there exists a h \u2208 H\u2032 such that the loss for h is less than\nEx,y\u223cD[lh\u2217(x, y)] + q\u03b10 \u2212 \u03b2\nThis theorem is proved in Appendix C. There are a number of subtle points in the above result that we elaborate below: \u2022 The result is an upper bound result and hence requires\nbounding the attacker\u2019s capabilities by imposing a bound on its effectiveness \u03b1 (compare with Theorem 1).\n\u2022 The attack used is effective against the less complex class H whereas the defender uses the more complex class H\u2032. Following the lower bound in Theorem 1, if the attacker were to use an effective attack against the class H\u2032 then the defender cannot benefit from using the rich class H\u2032.\nStandard techniques to increase the hypothesis complexity include: considering more features, using non-linear kernels in SVM and, using a neural network with more neurons. In addition, a well known general technique\u2014ensemble methods\u2014 is to combine any classifiers to form complex hypothesis, in which combinations of classifiers are used as the final output.\nComplexity is not free. The above results reveal that more complex models can defend against adversaries; however, an important clarification is necessary. We assumed the existence of enough data to learn the model with high fidelity; as long as it is the case, increasing complexity leads to lower bias and better accuracy. Otherwise, learning may lead to over-fitting or high variance in the model. Thus, while the above theoretical result and recent empirical work [94] suggests more complex models for defeating adversaries, in practice, availability of data may prohibit the use of this general result.\nOur take-away VII.1. Adversaries can exploit fundamental limitations of simple hypothesis classes in providing accurate predictions in sub-regions of the feature space. Such attacks can be thwarted by moving to a more complex (richer) hypothesis class, but over-fitting issues must be addressed with the more complex class."}, {"heading": "VIII. CONCLUSIONS", "text": "The security and privacy of machine learning is an active yet nascent area. We have explored the attack surface of systems built upon machine learning. That analysis yields a natural structure for reasoning about their threat models, and we have placed numerous works in this framework as organized around attacks and defenses. We formally showed that there is often a fundamental tension between security or privacy and precision of ML predictions in machine learning systems with finite capacity. In the large, the vast body of work from the diverse scientific communities jointly paint a picture that many vulnerabilities of machine learning and the countermeasures used to defend against them are as yet unknown\u2013but a science for understanding them is slowly emerging."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Mart\u0131\u0301n Abadi, Z. Berkay Celik, Ian Goodfellow, Damien Octeau, and Kunal Talwar for feedback on early versions of this document. Nicolas Papernot is supported by a Google PhD Fellowship in Security. We also thank Megan McDaniel for taking good care of our diet before the deadline. Research was supported in part by the Army Research Laboratory, under Cooperative Agreement Number W911NF-132-0045 (ARL Cyber Security CRA), and the Army Research Office under grant W911NF-13-1-0421. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory\nor the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for government purposes notwithstanding any copyright notation hereon."}, {"heading": "A. EXAMPLE OF DEFENSE USING ADDITIONAL DATA", "text": "In Figure 6 we show in sub-figure A the original data-set and the true classifier. The hypothesis class being used contains either a single linear separator or two linear separators. Thus, this hypothesis class can provide a classifier that is very close to the true classifier. However, for the data-set in A, the learned classifier is shown in sub-figure B, which is clearly far from optimal. This is not a problem of the hypothesis class; a different distribution of data shown in sub-figure C can provide for the learning of a much better classifier as shown. Another way is to add back adversarial examples as shown in subfigure D (adversarial examples in red), which again makes the learned classifier much better."}, {"heading": "B. PROOF OF THEOREM 1", "text": "Theorem 1. Fix any hypothesis (function) class H and distribution D, and assume that an attacker exists with probability q. Given the attacker uses an \u03b1-effective attack against H and D with \u03b1 \u2265 \u03b10 > 0, for all hypothesis h \u2208 R(H) the learner\u2019s loss is at least\nEx,y\u223cD[lh\u2217(x, y)] + q\u03b10\nProof. Choose any hypothesis h \u2208 R(H). The learner\u2019s loss is E x,y\u223c . D [lh(x, y)] = \u222b lh(x, y) . p(x, y)dxdy. For a randomized classifier h which randomizes over h1, . . . , hn with probabilities q1, . . . , qn respectively, the loss for any (x, y) is the expected loss \u2211 i qilhi(x, y). Thus, Ex,y\u223c . D\n[lh(x, y)] =\u2211 i qi \u222b lhi(x, y) . p(x, y)dxdy = \u2211 i qiEx,y\u223c . D\n[lhi(x, y)]. Now, from the definition of \u03b1-effective attack we have that E\nx,y\u223c . D [lhi(x, y)] \u2265 Ex,y\u223cD[lh\u2217(x, y)] + \u03b10. Thus, E x,y\u223c . D\n[lh(x, y)] \u2265 Ex,y\u223cD[lh\u2217(x, y)] + \u03b10. Also, by definition Ex,y\u223cD[lh(x, y)] \u2265 Ex,y\u223cD[lh\u2217(x, y)]\nNext, for any choice of h the adversary is present with probability q. Thus, the expected loss of any hypothesis is qE\nx,y\u223c . D [lh(x, y)] + (1\u2212 q)Ex,y\u223cD[lh(x, y)], which using the inequalities above is \u2265 Ex,y\u223cD[lh\u2217(x, y)] + q\u03b10"}, {"heading": "C. PROOF OF THEOREM 2", "text": "Theorem 2. Fix any hypothesis (function) class H and distribution D and a \u03b2-rich hypothesis class H\u2032. Assume the attacker is present with probability q and lD << Ex,y\u223cD[lh\u2217(x, y)] \u2212 \u03b2 and l .D << Ex,y\u223c .D[l .h\u2217(x, y)] \u2212 \u03b2. Given the attacker that uses an \u03b1-effective against H and D with \u03b1 = \u03b10 and the learner uses the \u03b2-rich hypothesis class H\u2032, there exists a h \u2208 H\u2032 such that the loss for h is less than\nEx,y\u223cD[lh\u2217(x, y)] + q\u03b10 \u2212 \u03b2\nProof. Let the adversary\u2019s choice of distribution in his attack be .\nD. Choose the hypothesis h \u2208 H \u2032 such that h\u2032 \u2208 argminh\u2208H\u2032 Ex,y\u223c .D[lh(x, y)]. The learner\u2019s loss is E x,y\u223c . D [lh\u2032(x, y)]. By definition of \u03b2-richness, we have\nE x,y\u223c . D [lh\u2032(x, y)] \u2264 Ex,y\u223c .D[l .h\u2217(x, y)]\u2212 \u03b2\nwhere . h \u2217 \u2208 argminh\u2208HEx,y\u223c .D[lh(x, y)]. Also, by definition of \u03b1-effective attack against H we have\nE x,y\u223c . D [l . h \u2217(x, y)] = Ex,y\u223cD[lh\u2217(x, y)] + \u03b10\nfor h\u2217 \u2208 argminh\u2208HEx,y\u223cD[lh(x, y)]. Thus, we have\nE x,y\u223c . D\n[lh\u2032(x, y)] \u2264 Ex,y\u223cD[lh\u2217(x, y)] + \u03b10 \u2212 \u03b2\nAlso, by \u03b2-richness we have\nEx,y\u223cD[lh\u2032(x, y)] \u2264 Ex,y\u223cD[lh\u2217(x, y)]\u2212 \u03b2\nNext, the adversary is present with probability q. Thus, the expected loss of the hypothesis h\u2032 is qE\nx,y\u223c . D [lh\u2032(x, y)] +\n(1 \u2212 q)Ex,y\u223cD[lh\u2032(x, y)], which using the inequalities above is \u2264 Ex,y\u223cD[lh\u2217(x, y)] + q\u03b10 \u2212 \u03b2"}], "references": [{"title": "Concrete problems in AI safety", "author": ["D. Amodei", "C. Olah", "J. Steinhardt", "P. Christiano", "J. Schulman", "D. Man\u00e9"], "venue": "arXiv preprint arXiv:1606.06565, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Oblivious multi-party machine learning on trusted processors", "author": ["O. Ohrimenko", "F. Schuster", "C. Fournet", "A. Mehta", "S. Nowozin", "K. Vaswani", "M. Costa"], "venue": "25th USENIX Security Symposium (USENIX Security 16), 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Machine Learning: A Probabilistic Perspective", "author": ["K.P. Murphy"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 1097\u20131105.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 3104\u20133112.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Support vector machines for spam categorization", "author": ["H. Drucker", "D. Wu", "V.N. Vapnik"], "venue": "IEEE Transactions on Neural Networks, vol. 10, no. 5, pp. 1048\u20131054, 1999.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Data clustering: A review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM Computing Surveys, vol. 31, no. 3, pp. 264\u2013323, 1999.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Stacked convolutional auto-encoders for hierarchical feature extraction", "author": ["J. Masci", "U. Meier", "D. Cire\u015fan", "J. Schmidhuber"], "venue": "International Conference on Artificial Neural Networks and Machine Learning, 2011, pp. 52\u201359.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Why does unsupervised pre-training help deep learning?", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P.-A. Manzagol", "P. Vincent", "S. Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Anomaly detection: A survey", "author": ["V. Chandola", "A. Banerjee", "V. Kumar"], "venue": "ACM Computing Surveys, vol. 41, no. 3, pp. 15:1\u201315:58, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Nash Q-learning for general-sum stochastic games", "author": ["J. Hu", "M.P. Wellman"], "venue": "Journal of Machine Learning Research, vol. 4, pp. 1039\u20131069, 2003.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Mastering the game of Go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": "Nature, vol. 529, no. 7587, pp. 484\u2013489, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Pattern recognition", "author": ["C.M. Bishop"], "venue": "Machine Learning, 2006.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "2016, Book in preparation for MIT Press (www.deeplearningbook.org).", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Static analysis of executables to detect malicious patterns", "author": ["M. Christodorescu", "S. Jha"], "venue": "12th USENIX Security Symposium (USENIX Security 06), 2006.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Anomaly based network intrusion detection with unsupervised outlier detection", "author": ["J. Zhang", "M. Zulkernine"], "venue": "IEEE International Conference on Communications, vol. 5, 2006, pp. 2388\u20132393.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Outside the closed world: On using machine learning for network intrusion detection", "author": ["R. Sommer", "V. Paxson"], "venue": "2010 IEEE symposium on security and privacy. IEEE, 2010, pp. 305\u2013316.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Next generation intrusion detection: Autonomous reinforcement learning of network attacks", "author": ["J. Cannady"], "venue": "Proceedings of the 23rd national information systems security conference, 2000, pp. 1\u201312.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "An introduction to kernel and nearest-neighbor nonparametric regression", "author": ["N.S. Altman"], "venue": "The American Statistician, vol. 46, no. 3, pp. 175\u2013185, 1992.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1992}, {"title": "Neural network learning: Theoretical foundations", "author": ["M. Anthony", "P.L. Bartlett"], "venue": "cambridge university press,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Are loss functions all the same?", "author": ["L. Rosasco", "E. De Vito", "A. Caponnetto", "M. Piana", "A. Verri"], "venue": "Neural Computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Learning adversary behavior in security games: A pac model perspective", "author": ["A. Sinha", "D. Kar", "M. Tambe"], "venue": "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems, 2016, pp. 214\u2013222.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Can machine learning be secure?", "author": ["M. Barreno", "B. Nelson", "R. Sears", "A.D. Joseph", "J.D. Tygar"], "venue": "Proceedings of the 2006 ACM Symposium on Information, computer and communications security. ACM,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}, {"title": "The limitations of deep learning in adversarial settings", "author": ["N. Papernot", "P. McDaniel", "S. Jha", "M. Fredrikson", "Z.B. Celik", "A. Swami"], "venue": "Proceedings of the 1st IEEE European Symposium on Security and Privacy. IEEE, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "A new learning paradigm: Learning using privileged information", "author": ["V. Vapnik", "A. Vashist"], "venue": "Neural Networks, vol. 22, no. 5, pp. 544\u2013557, 2009.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Online anomaly detection under adversarial impact", "author": ["M. Kloft", "P. Laskov"], "venue": "International Conference on Artificial Intelligence and Statistics, 2010, pp. 405\u2013412.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Good word attacks on statistical spam filters.", "author": ["D. Lowd", "C. Meek"], "venue": "CEAS,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "Proceedings of the 2014 International Conference on Learning Representations. Computational and Biological Learning Society, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow", "S. Jha", "Z.B. Celik", "A. Swami"], "venue": "arXiv preprint arXiv:1602.02697, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Practical evasion of a learning-based classifier: A case study", "author": ["P. Laskov"], "venue": "2014 IEEE Symposium on Security and Privacy. IEEE, 2014, pp. 197\u2013211.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Statistical fraud detection: A review", "author": ["R.J. Bolton", "D.J. Hand"], "venue": "Statistical science, pp. 235\u2013249, 2002.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "Privacy, information technology, and health care", "author": ["T.C. Rindfleisch"], "venue": "Communications of the ACM, vol. 40, no. 8, pp. 92\u2013100, 1997.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1997}, {"title": "Model inversion attacks that exploit confidence information and basic countermeasures", "author": ["M. Fredrikson", "S. Jha", "T. Ristenpart"], "venue": "Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security. ACM, 2015, pp. 1322\u20131333.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Membership inference attacks against machine learning models", "author": ["R. Shokri", "M. Stronati", "V. Shmatikov"], "venue": "arXiv preprint arXiv:1610.05820, 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition", "author": ["M. Sharif", "S. Bhagavatula", "L. Bauer", "M.K. Reiter"], "venue": "Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. ACM, 2016, pp. 1528\u20131540.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Evaluation: from precision, recall and f-measure to roc, informedness, markedness and correlation", "author": ["D.M. Powers"], "venue": "2011.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning in the presence of malicious errors", "author": ["M. Kearns", "M. Li"], "venue": "SIAM Journal on Computing, vol. 22, no. 4, pp. 807\u2013837, 1993.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1993}, {"title": "Nightmare at test time: robust learning by feature deletion", "author": ["A. Globerson", "S. Roweis"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 353\u2013360.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "Noise tolerance under risk minimization", "author": ["N. Manwani", "P.S. Sastry"], "venue": "IEEE Transactions on Cybernetics, vol. 43, no. 3, pp. 1146\u20131151, 2013.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Bounding an attack\u2019s complexity for a simple learning model", "author": ["B. Nelson", "A.D. Joseph"], "venue": "Proc. of the First Workshop on Tackling Computer Systems Problems with Machine Learning Techniques (SysML), Saint-Malo, France, 2006.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2006}, {"title": "Mining time-changing data streams", "author": ["G. Hulten", "L. Spencer", "P. Domingos"], "venue": "Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2001, pp. 97\u2013106.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2001}, {"title": "Support vector machines under adversarial label noise.", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "ACML,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "Systematic poisoning attacks on and defenses for machine learning in healthcare", "author": ["M. Mozaffari-Kermani", "S. Sur-Kolay", "A. Raghunathan", "N.K. Jha"], "venue": "IEEE journal of biomedical and health informatics, vol. 19, no. 6, pp. 1893\u20131905, 2015.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1893}, {"title": "Poisoning attacks against support vector machines", "author": ["B. Biggio", "B. Nelson", "L. Pavel"], "venue": "Proceedings of the 29th International Conference on Machine Learning, 2012. 16", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2012}, {"title": "Using machine teaching to identify optimal training-set attacks on machine learners.", "author": ["S. Mei", "X. Zhu"], "venue": "in AAAI,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Polygraph: Automatically generating signatures for polymorphic worms", "author": ["J. Newsome", "B. Karp", "D. Song"], "venue": "Security and Privacy, 2005 IEEE Symposium on. IEEE, 2005, pp. 226\u2013241.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2005}, {"title": "Misleading worm signature generators using deliberate noise injection", "author": ["R. Perdisci", "D. Dagon", "W. Lee", "P. Fogla", "M. Sharif"], "venue": "Security and Privacy, 2006 IEEE Symposium on. IEEE, 2006, pp. 15\u2013pp.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2006}, {"title": "Is feature selection secure against training data poisoning?", "author": ["H. Xiao", "B. Biggio", "G. Brown", "G. Fumera", "C. Eckert", "F. Roli"], "venue": "Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "Evasion attacks against machine learning at test time", "author": ["B. Biggio", "I. Corona", "D. Maiorca", "B. Nelson", "N. \u0160rndi\u0107", "P. Laskov", "G. Giacinto", "F. Roli"], "venue": "Machine Learning and Knowledge Discovery in Databases. Springer, 2013, pp. 387\u2013402.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "International Conference on Learning Representations. Computational and Biological Learning Society, 2015.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["S.-M. Moosavi-Dezfooli", "A. Fawzi", "P. Frossard"], "venue": "arXiv preprint arXiv:1511.04599, 2015.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}, {"title": "Data poisoning attacks against autoregressive models", "author": ["S. Alfeld", "X. Zhu", "P. Barford"], "venue": "Thirtieth AAAI Conference on Artificial Intelligence, 2016.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2016}, {"title": "Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers", "author": ["G. Ateniese", "L.V. Mancini", "A. Spognardi", "A. Villani", "D. Vitali", "G. Felici"], "venue": "International Journal of Security and Networks, vol. 10, no. 3, pp. 137\u2013150, 2015.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2015}, {"title": "Adversarial perturbations against deep neural networks for malware classification", "author": ["K. Grosse", "N. Papernot", "P. Manoharan", "M. Backes", "P. McDaniel"], "venue": "arXiv preprint arXiv:1606.04435, 2016.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarial examples in the physical world", "author": ["A. Kurakin", "I. Goodfellow", "S. Bengio"], "venue": "arXiv preprint arXiv:1607.02533, 2016.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatically evading classifiers", "author": ["W. Xu", "Y. Qi", "D. Evans"], "venue": "Proceedings of the 2016 Network and Distributed Systems Symposium, 2016.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}, {"title": "Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing", "author": ["M. Fredrikson", "E. Lantz", "S. Jha", "S. Lin", "D. Page", "T. Ristenpart"], "venue": "23rd USENIX Security Symposium (USENIX Security 14), 2014, pp. 17\u201332.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "Stealing machine learning models via prediction apis", "author": ["F. Tram\u00e8r", "F. Zhang", "A. Juels", "M.K. Reiter", "T. Ristenpart"], "venue": "arXiv preprint arXiv:1609.02943, 2016.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2016}, {"title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow"], "venue": "arXiv preprint arXiv:1605.07277, 2016.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2016}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "NIPS-14 Workshop on Deep Learning and Representation Learning. arXiv:1503.02531, 2014.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2014}, {"title": "On the limited memory bfgs method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical programming, vol. 45, no. 1-3, pp. 503\u2013528, 1989.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 1989}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes"], "venue": "1998.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 1998}, {"title": "Adversarial perturbations of deep neural networks", "author": ["D. Warde-Farley", "I. Goodfellow"], "venue": "Advanced Structured Prediction, T. Hazan, G. Papandreou, and D. Tarlow, Eds, 2016.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning with a strong adversary", "author": ["R. Huang", "B. Xu", "D. Schuurmans", "C. Szepesvari"], "venue": "arXiv preprint arXiv:1511.03034, 2015.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "In Computer Vision and Pattern Recognition (CVPR 2015). IEEE, 2015.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2015}, {"title": "Hidden voice commands", "author": ["N. Carlini", "P. Mishra", "T. Vaidya", "Y. Zhang", "M. Sherr", "C. Shields", "D. Wagner", "W. Zhou"], "venue": "25th USENIX Security Symposium (USENIX Security 16), Austin, TX, 2016.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2016}, {"title": "Supervision via competition: Robot adversaries for learning tasks", "author": ["L. Pinto", "J. Davidson", "A. Gupta"], "venue": "arXiv preprint arXiv:1610.01685, 2016.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 2672\u2013 2680.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2014}, {"title": "On attacking statistical spam filters.", "author": ["G.L. Wittel", "S.F. Wu"], "venue": "CEAS,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2004}, {"title": "Optimal randomized classification in adversarial settings", "author": ["Y. Vorobeychik", "B. Li"], "venue": "Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems. International Foundation for Autonomous Agents and Multiagent Systems, 2014, pp. 485\u2013492.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2014}, {"title": "Adversarial learning", "author": ["D. Lowd", "C. Meek"], "venue": "Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining. ACM, 2005, pp. 641\u2013647.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2005}, {"title": "Query strategies for evading convex-inducing classifiers", "author": ["B. Nelson", "B.I. Rubinstein", "L. Huang", "A.D. Joseph", "S.J. Lee", "S. Rao", "J. Tygar"], "venue": "Journal of Machine Learning Research, vol. 13, no. May, pp. 1293\u2013 1332, 2012.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistical inference considered harmful", "author": ["F. McSherry"], "venue": "2016. [Online]. Available: https://github.com/frankmcsherry/blog/blob/ master/posts/2016-06-14.md", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Computer vision\u2013ECCV 2014. Springer, 2014, pp. 818\u2013833.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2014}, {"title": "Antidote: Understanding and defending against poisoning of anomaly detectors", "author": ["B.I.P. Rubinstein", "B. Nelson", "L. Huang", "A.D. Joseph", "S.-h. Lau", "S. Rao", "N. Taft", "J.D. Tygar"], "venue": "9th ACM SIGCOMM Conference on Internet measurement. ACM, 2009, pp. 1\u201314.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning SVMs from sloppily labeled data", "author": ["G. Stempfel", "L. Ralaivola"], "venue": "International Conference on Artificial Neural Networks. Springer, 2009, pp. 884\u2013893.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust support vector machine training via convex outlier ablation", "author": ["L. Xu", "K. Crammer", "D. Schuurmans"], "venue": "Twenty-First AAAI National Conference on Artificial Intelligence, vol. 6, 2006, pp. 536\u2013 542.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2006}, {"title": "A virtual honeypot framework.", "author": ["N. Provos"], "venue": "USENIX Security Symposium,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2004}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["S. Gu", "L. Rigazio"], "venue": "Proceedings of the 2015 International Conference on Learning Representations. Computational and Biological Learning Society, 2015.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2015}, {"title": "A unified gradient regularization family for adversarial examples", "author": ["C. Lyu", "K. Huang", "H.-N. Liang"], "venue": "Data Mining (ICDM), 2015 IEEE International Conference on. IEEE, 2015, pp. 301\u2013309.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2015}, {"title": "Unifying adversarial training algorithms with flexible deep data gradient regularization", "author": ["I. Ororbia", "G. Alexander", "C.L. Giles", "D. Kifer"], "venue": "arXiv preprint arXiv:1601.07213, 2016.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. Papernot", "P. McDaniel", "X. Wu", "S. Jha", "A. Swami"], "venue": "Proceedings of the 37th IEEE Symposium on Security and Privacy. IEEE, 2016.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2016}, {"title": "On the effectiveness of defensive distillation", "author": ["N. Papernot", "P. McDaniel"], "venue": "arXiv preprint arXiv:1607.05113, 2016.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2016}, {"title": "Defensive distillation is not robust to adversarial examples", "author": ["N. Carlini", "D. Wagner"], "venue": "arXiv preprint arXiv:1607.04311, 2016.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2016}, {"title": "Rethinking the inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": "arXiv preprint arXiv:1512.00567, 2015.", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2015}, {"title": "Adversarial perturbations of deep neural networks", "author": ["D. Warde-Farley", "I. Goodfellow"], "venue": "Advanced Structured Prediction, T. Hazan, G. Papandreou, and D. Tarlow, Eds., 2016.", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2016}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of COMPSTAT\u20192010. Springer, 2010, pp. 177\u2013186.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2010}, {"title": "From physical security to cybersecurity", "author": ["A. Sinha", "T.H. Nguyen", "D. Kar", "M. Brown", "M. Tambe", "A.X. Jiang"], "venue": "Journal of Cybersecurity, p. tyv007, 2015.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2015}, {"title": "Security and game theory: algorithms, deployed systems, lessons learned", "author": ["M. Tambe"], "venue": null, "citeRegEx": "91", "shortCiteRegEx": "91", "year": 2011}, {"title": "Stackelberg games for adversarial prediction problems", "author": ["M. Br\u00fcckner", "T. Scheffer"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2011, pp. 547\u2013555.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2011}, {"title": "Strategic classification", "author": ["M. Hardt", "N. Megiddo", "C. Papadimitriou", "M. Wootters"], "venue": "Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science, ser. ITCS \u201916, 2016, pp. 111\u2013122. 17", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2016}, {"title": "Feature cross-substitution in adversarial classification", "author": ["B. Li", "Y. Vorobeychik"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 2087\u20132095.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2014}, {"title": "The algorithmic foundations of differential privacy", "author": ["C. Dwork", "A. Roth"], "venue": "Foundations and Trends in Theoretical Computer Science, vol. 9, no. 3-4, pp. 211\u2013407, 2014.", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2014}, {"title": "What can we learn privately?", "author": ["S.P. Kasiviswanathan", "H.K. Lee", "K. Nissim", "S. Raskhodnikova", "A. Smith"], "venue": "SIAM Journal on Computing,", "citeRegEx": "96", "shortCiteRegEx": "96", "year": 2011}, {"title": "Rappor: Randomized aggregatable privacy-preserving ordinal response", "author": ["\u00da. Erlingsson", "V. Pihur", "A. Korolova"], "venue": "Proceedings of the 2014 ACM SIGSAC conference on computer and communications security. ACM, 2014, pp. 1054\u20131067.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning privately from multiparty data", "author": ["J. Hamm", "P. Cao", "M. Belkin"], "venue": "arXiv preprint arXiv:1602.03552, 2016.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-supervised knowledge transfer for deep learning from private training data", "author": ["N. Papernot", "M. Abadi", "\u00da. Erlingsson", "I. Goodfellow", "K. Talwar"], "venue": "arXiv preprint arXiv:1610.05755, 2016.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2016}, {"title": "Differentially private empirical risk minimization", "author": ["K. Chaudhuri", "C. Monteleoni", "A.D. Sarwate"], "venue": "Journal of Machine Learning Research, vol. 12, no. Mar, pp. 1069\u20131109, 2011.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2011}, {"title": "Differentially private empirical risk minimization: Efficient algorithms and tight error bounds", "author": ["R. Bassily", "A. Smith", "A. Thakurta"], "venue": "arXiv preprint arXiv:1405.7085, 2014.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2014}, {"title": "Privacy-preserving deep learning", "author": ["R. Shokri", "V. Shmatikov"], "venue": "22nd ACM SIGSAC Conference on Computer and Communications Security, 2015, pp. 1310\u20131321.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning with differential privacy", "author": ["M. Abadi", "A. Chu", "I. Goodfellow", "H.B. McMahan", "I. Mironov", "K. Talwar", "L. Zhang"], "venue": "ACM SIGSAC Conference on Computer and Communications Security. ACM, 2016, pp. 308\u2013318.", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2016}, {"title": "On data banks and privacy homomorphisms", "author": ["R.L. Rivest", "L. Adleman", "M.L. Dertouzos"], "venue": "Foundations of secure computation, vol. 4, no. 11, pp. 169\u2013180, 1978.", "citeRegEx": "104", "shortCiteRegEx": null, "year": 1978}, {"title": "Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy", "author": ["R. Gilad-Bachrach", "N. Dowlin", "K. Laine", "K. Lauter", "M. Naehrig", "J. Wernsing"], "venue": "Proceedings of The 33rd International Conference on Machine Learning, 2016, pp. 201\u2013210.", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2016}, {"title": "Estimation of the warfarin dose with clinical and pharmacogenetic data", "author": ["I.W.P. Consortium"], "venue": "N Engl J Med, vol. 2009, no. 360, pp. 753\u2013 764, 2009.", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2009}, {"title": "Inherent trade-offs in the fair determination of risk scores", "author": ["J. Kleinberg", "S. Mullainathan", "M. Raghavan"], "venue": "arXiv preprint arXiv:1609.05807, 2016.", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2016}, {"title": "Big data\u2019s disparate impact", "author": ["S. Barocas", "A.D. Selbst"], "venue": "California Law Review, vol. 104, 2016.", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2016}, {"title": "Fairness through awareness", "author": ["C. Dwork", "M. Hardt", "T. Pitassi", "O. Reingold", "R. Zemel"], "venue": "Proceedings of the 3rd Innovations in Theoretical Computer Science Conference. ACM, 2012, pp. 214\u2013226.", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2012}, {"title": "Censoring representations with an adversary", "author": ["H. Edwards", "A. Storkey"], "venue": "arXiv preprint arXiv:1511.05897, 2015.", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2015}, {"title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model", "author": ["B. Letham", "C. Rudin", "T.H. McCormick", "D. Madigan"], "venue": "The Annals of Applied Statistics, vol. 9, no. 3, pp. 1350\u20131371, 2015.", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2015}, {"title": "Algorithmic transparency via quantitative input influence", "author": ["A. Datta", "S. Sen", "Y. Zick"], "venue": "Proceedings of 37th IEEE Symposium on Security and Privacy, 2016.", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing higherlayer features of a deep network", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P. Vincent"], "venue": "University of Montreal, vol. 1341, 2009.", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2009}, {"title": "Visualizing deep convolutional neural networks using natural pre-images", "author": ["A. Mahendran", "A. Vedaldi"], "venue": "International Journal of Computer Vision, pp. 1\u201323, 2016.", "citeRegEx": "114", "shortCiteRegEx": null, "year": 2016}, {"title": "Differential privacy: A survey of results", "author": ["C. Dwork"], "venue": "International Conference on Theory and Applications of Models of Computation. Springer Berlin Heidelberg, 2008, pp. 1\u201319.", "citeRegEx": "115", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "For instance, we do not cover trusted computing platforms for ML [2].", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "Machine learning provides automated methods of analysis for large sets of data [3].", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "Classic examples of supervised learning tasks include: object recognition in images [4], machine translation [5], and spam filtering [6].", "startOffset": 84, "endOffset": 87}, {"referenceID": 4, "context": "Classic examples of supervised learning tasks include: object recognition in images [4], machine translation [5], and spam filtering [6].", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "Classic examples of supervised learning tasks include: object recognition in images [4], machine translation [5], and spam filtering [6].", "startOffset": 133, "endOffset": 136}, {"referenceID": 6, "context": "Unsupervised learning considers problems such as clustering points according to a similarity metric [7], dimensionality reduction to project data in lower dimensional subspaces [8], and model pre-training [10].", "startOffset": 100, "endOffset": 103}, {"referenceID": 7, "context": "Unsupervised learning considers problems such as clustering points according to a similarity metric [7], dimensionality reduction to project data in lower dimensional subspaces [8], and model pre-training [10].", "startOffset": 177, "endOffset": 180}, {"referenceID": 9, "context": "Unsupervised learning considers problems such as clustering points according to a similarity metric [7], dimensionality reduction to project data in lower dimensional subspaces [8], and model pre-training [10].", "startOffset": 205, "endOffset": 209}, {"referenceID": 10, "context": "For instance, clustering may be applied to anomaly detection [11].", "startOffset": 61, "endOffset": 65}, {"referenceID": 11, "context": "Reinforcement learning: Methods that learn a policy for action over time given sequences of actions, observations, and rewards fall in the scope of reinforcement learning [12], [13].", "startOffset": 171, "endOffset": 175}, {"referenceID": 12, "context": "Reinforcement learning: Methods that learn a policy for action over time given sequences of actions, observations, and rewards fall in the scope of reinforcement learning [12], [13].", "startOffset": 177, "endOffset": 181}, {"referenceID": 13, "context": "It was reinforcement learning in combination with supervised and unsupervised methods that recently enabled a computer to defeat a human champion at the game of Go [14].", "startOffset": 164, "endOffset": 168}, {"referenceID": 2, "context": "Readers interested in a broad survey of ML are well served by many books covering this rich topic [3], [15], [16].", "startOffset": 98, "endOffset": 101}, {"referenceID": 14, "context": "Readers interested in a broad survey of ML are well served by many books covering this rich topic [3], [15], [16].", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "Readers interested in a broad survey of ML are well served by many books covering this rich topic [3], [15], [16].", "startOffset": 109, "endOffset": 113}, {"referenceID": 16, "context": "The training data comprises a set of labeled instances, each an executable clearly marked as malicious or benign [17].", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "The training data could consist of TCP dumps [18].", "startOffset": 45, "endOffset": 49}, {"referenceID": 18, "context": "commonly encountered in anomaly-based network intrusion detection [19].", "startOffset": 66, "endOffset": 70}, {"referenceID": 19, "context": ") [20].", "startOffset": 2, "endOffset": 6}, {"referenceID": 21, "context": "PAC model of learning: PAC learning model has a very rich and extensive body of work [22].", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "1A few models are non-parametric: for instance the nearest neighbor [21].", "startOffset": 68, "endOffset": 72}, {"referenceID": 22, "context": "hinge loss [23] used in SVMs or the cross-entropy loss [3].", "startOffset": 11, "endOffset": 15}, {"referenceID": 2, "context": "hinge loss [23] used in SVMs or the cross-entropy loss [3].", "startOffset": 55, "endOffset": 58}, {"referenceID": 23, "context": "In the extreme, a learning accuracy of 100% could be achieved by predicting correctly in the positive probability region with lot of misclassification in the zero probability regions (or more precisely sets with measure zero; a detailed discussion of this fact is present in a recent paper [24]).", "startOffset": 290, "endOffset": 294}, {"referenceID": 24, "context": "For the purpose of exposition of the following Sections, we expand upon previous approaches at articulating a threat model for ML [25], [26].", "startOffset": 130, "endOffset": 134}, {"referenceID": 25, "context": "For the purpose of exposition of the following Sections, we expand upon previous approaches at articulating a threat model for ML [25], [26].", "startOffset": 136, "endOffset": 140}, {"referenceID": 26, "context": "The training data may also include additional features not available at runtime (referred to as privileged information in some settings [27]).", "startOffset": 136, "endOffset": 140}, {"referenceID": 27, "context": ", using false training [28]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 27, "context": "Online attacks such as these have been commonly observed in domains such as SPAM detection and network intrusion detection [28].", "startOffset": 123, "endOffset": 127}, {"referenceID": 24, "context": "Inference Phase: Attacks at inference time\u2014exploratory attacks [25]\u2014do not tamper with the targeted model but instead either cause it to produce adversary selected outputs (incorrect outputs, see Integrity attacks below) or simply use the attack to collect evidence about the model characteristics (reconnaissance, see privacy below).", "startOffset": 63, "endOffset": 67}, {"referenceID": 29, "context": ", adversarial example crafting [30].", "startOffset": 31, "endOffset": 35}, {"referenceID": 30, "context": "model by providing a series of carefully crafted inputs and observing outputs [31].", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "3D tensor in [0,1] Class probabilities Traffic sign JPEG Car brakes Camera Pre-processing Apply model Output analysis", "startOffset": 13, "endOffset": 18}, {"referenceID": 31, "context": "For example, the adversary can use a substitute model to test potential inputs before submitting them to the victim system [32].", "startOffset": 123, "endOffset": 127}, {"referenceID": 32, "context": ", financial market systems [33].", "startOffset": 27, "endOffset": 31}, {"referenceID": 33, "context": ", medical applications [34].", "startOffset": 23, "endOffset": 27}, {"referenceID": 34, "context": "Machine learning models have enough capacity to capture and memorize elements of their training data [35].", "startOffset": 101, "endOffset": 105}, {"referenceID": 35, "context": "Potential risks are adversaries performing membership test (to know whether an individual is in a dataset or not) [36], recovering of partially known inputs (use the model to complete an input vector with the most likely missing bits), and extraction of the training data using the model\u2019s predictions [35].", "startOffset": 114, "endOffset": 118}, {"referenceID": 34, "context": "Potential risks are adversaries performing membership test (to know whether an individual is in a dataset or not) [36], recovering of partially known inputs (use the model to complete an input vector with the most likely missing bits), and extraction of the training data using the model\u2019s predictions [35].", "startOffset": 302, "endOffset": 306}, {"referenceID": 36, "context": "tion system affect the authentication process\u2019s integrity [37].", "startOffset": 58, "endOffset": 62}, {"referenceID": 37, "context": ", accuracy [38].", "startOffset": 11, "endOffset": 15}, {"referenceID": 29, "context": "However, researchers have shown that the integrity of ML systems may be compromised by adversaries capable of manipulating model inputs [30] or its training data [39].", "startOffset": 136, "endOffset": 140}, {"referenceID": 38, "context": "However, researchers have shown that the integrity of ML systems may be compromised by adversaries capable of manipulating model inputs [30] or its training data [39].", "startOffset": 162, "endOffset": 166}, {"referenceID": 39, "context": "More broadly, machine learning models may also not perform correctly when some of their input features are corrupted or missing [40].", "startOffset": 128, "endOffset": 132}, {"referenceID": 24, "context": "a poisoning attack [25], and is an instance of learning in the presence of non-necessarily adversarial but nevertheless noisy data [41].", "startOffset": 19, "endOffset": 23}, {"referenceID": 40, "context": "a poisoning attack [25], and is an instance of learning in the presence of non-necessarily adversarial but nevertheless noisy data [41].", "startOffset": 131, "endOffset": 135}, {"referenceID": 38, "context": "Poisoning attacks alter the training dataset by inserting, editing, or removing points with the intent of modifying the decision boundaries of the targeted model [39], thus targeting the learning system\u2019s integrity per our threat model from Section III.", "startOffset": 162, "endOffset": 166}, {"referenceID": 41, "context": "Thus, all the attacks below bound the adversary in their attacks [42].", "startOffset": 65, "endOffset": 69}, {"referenceID": 42, "context": "In Section VI-A, we present a line of work that builds on that observation to propose learning strategies robust to distribution drifts [43].", "startOffset": 136, "endOffset": 140}, {"referenceID": 38, "context": "formally analyzed PAC-learnability when the adversary is allowed to modify training samples with probability \u03b2 [39].", "startOffset": 111, "endOffset": 115}, {"referenceID": 43, "context": "showed that this was sufficient to degrade inference performance of classifiers learned with SVMs [44], as long", "startOffset": 98, "endOffset": 102}, {"referenceID": 44, "context": "A similar attack approach has been applied in the context of healthcare [45].", "startOffset": 72, "endOffset": 76}, {"referenceID": 43, "context": "As is the case for the approach in [44], this attack requires that a new ML model be learned for each new candidate poisoning point in order to measure the proposed point\u2019s impact on the updated model\u2019s performance", "startOffset": 35, "endOffset": 39}, {"referenceID": 27, "context": "a model that classifies a test input as malicious when it is too far from the empirical mean of the training data [28].", "startOffset": 114, "endOffset": 118}, {"referenceID": 45, "context": "introduce an attack that also inserts inputs in the training set [46].", "startOffset": 65, "endOffset": 69}, {"referenceID": 46, "context": "domain is continuous [47].", "startOffset": 21, "endOffset": 25}, {"referenceID": 47, "context": "prevented Polygraph, a worm signature generation tool [48], from learning meaningful signatures by inserting perturbations in worm traffic flows [49].", "startOffset": 54, "endOffset": 58}, {"referenceID": 48, "context": "prevented Polygraph, a worm signature generation tool [48], from learning meaningful signatures by inserting perturbations in worm traffic flows [49].", "startOffset": 145, "endOffset": 149}, {"referenceID": 45, "context": "adapted the gradient ascent strategy introduced in [46] to feature selection algorithms like LASSO [50].", "startOffset": 51, "endOffset": 55}, {"referenceID": 49, "context": "adapted the gradient ascent strategy introduced in [46] to feature selection algorithms like LASSO [50].", "startOffset": 99, "endOffset": 103}, {"referenceID": 61, "context": "For instance, ML models trained on data centers are compressed and deployed to smartphones [62], in which case reverse engineering may enable adversaries to", "startOffset": 91, "endOffset": 95}, {"referenceID": 50, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 18, "endOffset": 22}, {"referenceID": 51, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 24, "endOffset": 28}, {"referenceID": 52, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 30, "endOffset": 34}, {"referenceID": 29, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 35, "endOffset": 39}, {"referenceID": 25, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 41, "endOffset": 45}, {"referenceID": 53, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 47, "endOffset": 51}, {"referenceID": 54, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 52, "endOffset": 56}, {"referenceID": 55, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 82, "endOffset": 86}, {"referenceID": 56, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 88, "endOffset": 92}, {"referenceID": 36, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 94, "endOffset": 98}, {"referenceID": 36, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 99, "endOffset": 103}, {"referenceID": 57, "context": "Black-Box Yes No [58] [36] [59] [60]", "startOffset": 17, "endOffset": 21}, {"referenceID": 35, "context": "Black-Box Yes No [58] [36] [59] [60]", "startOffset": 22, "endOffset": 26}, {"referenceID": 58, "context": "Black-Box Yes No [58] [36] [59] [60]", "startOffset": 27, "endOffset": 31}, {"referenceID": 59, "context": "Black-Box Yes No [58] [36] [59] [60]", "startOffset": 32, "endOffset": 36}, {"referenceID": 31, "context": "Input x only Yes [32], [30], [52] [60] No [31], [61] Through pipeline only No [57]", "startOffset": 17, "endOffset": 21}, {"referenceID": 29, "context": "Input x only Yes [32], [30], [52] [60] No [31], [61] Through pipeline only No [57]", "startOffset": 23, "endOffset": 27}, {"referenceID": 51, "context": "Input x only Yes [32], [30], [52] [60] No [31], [61] Through pipeline only No [57]", "startOffset": 29, "endOffset": 33}, {"referenceID": 59, "context": "Input x only Yes [32], [30], [52] [60] No [31], [61] Through pipeline only No [57]", "startOffset": 34, "endOffset": 38}, {"referenceID": 30, "context": "Input x only Yes [32], [30], [52] [60] No [31], [61] Through pipeline only No [57]", "startOffset": 42, "endOffset": 46}, {"referenceID": 60, "context": "Input x only Yes [32], [30], [52] [60] No [31], [61] Through pipeline only No [57]", "startOffset": 48, "endOffset": 52}, {"referenceID": 56, "context": "Input x only Yes [32], [30], [52] [60] No [31], [61] Through pipeline only No [57]", "startOffset": 78, "endOffset": 82}, {"referenceID": 24, "context": "When the model is a classifier, the adversary seeks to have it assign a wrong class to perturbed inputs [25].", "startOffset": 104, "endOffset": 108}, {"referenceID": 29, "context": "coined the term adversarial example to refer to such inputs [30].", "startOffset": 60, "endOffset": 64}, {"referenceID": 50, "context": "Similar to concurrent work [51], they formalize the search for adversarial examples as the following minimization problem:", "startOffset": 27, "endOffset": 31}, {"referenceID": 25, "context": "This is a source-target misclassification as the target class l 6= h(x) is chosen [26].", "startOffset": 82, "endOffset": 86}, {"referenceID": 62, "context": "For non-convex models h like DNNs, the authors apply the L-BFGS algorithm [63] to solve Equation 2.", "startOffset": 74, "endOffset": 78}, {"referenceID": 51, "context": "introduced the fast gradient sign method [52].", "startOffset": 41, "endOffset": 45}, {"referenceID": 63, "context": "3The MNIST dataset [64] is a widely used corpus of 70,000 handwritten digits used for validating image processing machine learning systems.", "startOffset": 19, "endOffset": 23}, {"referenceID": 51, "context": "ones, make outside of their training data [52], [65].", "startOffset": 42, "endOffset": 46}, {"referenceID": 64, "context": "ones, make outside of their training data [52], [65].", "startOffset": 48, "endOffset": 52}, {"referenceID": 52, "context": "Follow-up work reduced the size of a perturbation r according to different metrics [53], [66].", "startOffset": 83, "endOffset": 87}, {"referenceID": 65, "context": "Follow-up work reduced the size of a perturbation r according to different metrics [53], [66].", "startOffset": 89, "endOffset": 93}, {"referenceID": 25, "context": "the L0 norm of r [26].", "startOffset": 17, "endOffset": 21}, {"referenceID": 55, "context": "This is the case of malware detectors: in this application, adversarial examples are malware applications classified as benign [56].", "startOffset": 127, "endOffset": 131}, {"referenceID": 51, "context": "It is therefore not surprising that randomly sampled inputs can be constrained to be classified in a class with confidence [52], [67].", "startOffset": 123, "endOffset": 127}, {"referenceID": 66, "context": "It is therefore not surprising that randomly sampled inputs can be constrained to be classified in a class with confidence [52], [67].", "startOffset": 129, "endOffset": 133}, {"referenceID": 51, "context": "Unfortunately, training models with a class specific to rubbish (out of distribution) samples does not mitigate adversarial examples [52].", "startOffset": 133, "endOffset": 137}, {"referenceID": 56, "context": "showed how printouts of adversarial examples produced by the fast gradient sign algorithm were still misclassified by an object recognition model [57].", "startOffset": 146, "endOffset": 150}, {"referenceID": 29, "context": "introduced in [30] to find adversarial examples that are printed on glasses frames, which once worn by an individual result in its face being misclassified by a face recognition model [37].", "startOffset": 14, "endOffset": 18}, {"referenceID": 36, "context": "introduced in [30] to find adversarial examples that are printed on glasses frames, which once worn by an individual result in its face being misclassified by a face recognition model [37].", "startOffset": 184, "endOffset": 188}, {"referenceID": 66, "context": "As a natural extension to [67] (see above), it was shown that rubbish audio can be classified with confidence by a speech", "startOffset": 26, "endOffset": 30}, {"referenceID": 67, "context": "recognition system [68].", "startOffset": 19, "endOffset": 23}, {"referenceID": 36, "context": "Consequences are not as important in terms of security then [37]: the audio does not correspond to any legitimate input expected by the speech system or humans.", "startOffset": 60, "endOffset": 64}, {"referenceID": 53, "context": "[54] look at autoregressive models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 68, "context": "improve a model for grasping objects by introducing a competing model that attempts to snatch objects before the original model successfully grasps them [69].", "startOffset": 153, "endOffset": 157}, {"referenceID": 69, "context": "The two models are trained, \u00e0 la Generative Adversarial Networks [70], with competitive cost functions.", "startOffset": 65, "endOffset": 69}, {"referenceID": 54, "context": "infer statistical information about the training dataset from a trained model h\u03b8 [55]: i.", "startOffset": 81, "endOffset": 85}, {"referenceID": 70, "context": "We focus on strategies designed irrespectively of the domain ML is being applied to, albeit heuristics specific to certain applications exist [71], [29], e.", "startOffset": 142, "endOffset": 146}, {"referenceID": 28, "context": "We focus on strategies designed irrespectively of the domain ML is being applied to, albeit heuristics specific to certain applications exist [71], [29], e.", "startOffset": 148, "endOffset": 152}, {"referenceID": 71, "context": "A PAC based work shows that with no access to the training data or ML algorithm, querying the target model and knowledge of the class of target models allows the adversary to reconstruct the model with similar amount of query data as used in training [72] Thus, a key metric when comparing different attacks is the wealth of information returned by the oracle, and the number of oracle queries.", "startOffset": 251, "endOffset": 255}, {"referenceID": 72, "context": "estimate the cost of misclassification in terms of the number of queries to the black-box model [73].", "startOffset": 96, "endOffset": 100}, {"referenceID": 73, "context": "[74] identify the space of convex inducing classifiers\u2014 those where one of the classes is a convex set\u2014that are ACRE", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "The fitness of genetic variants obtained by mutation is defined in terms of the oracle\u2019s class probability predictions [58].", "startOffset": 119, "endOffset": 123}, {"referenceID": 31, "context": "explored the strategy of training a substitute model for the targeted one [32].", "startOffset": 74, "endOffset": 78}, {"referenceID": 29, "context": "used the cross-model transferability of adversarial samples [30], [52] to design a black-box attack [31].", "startOffset": 60, "endOffset": 64}, {"referenceID": 51, "context": "used the cross-model transferability of adversarial samples [30], [52] to design a black-box attack [31].", "startOffset": 66, "endOffset": 70}, {"referenceID": 30, "context": "used the cross-model transferability of adversarial samples [30], [52] to design a black-box attack [31].", "startOffset": 100, "endOffset": 104}, {"referenceID": 60, "context": "In a follow-up work [61], they show that the attack generalizes to many ML models by having a logistic regression oracle trained by Amazon misclassify 96% of the adversarial examples crafted.", "startOffset": 20, "endOffset": 24}, {"referenceID": 56, "context": "[57] demonstrated that physical adversarial example (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "show how to conduct this type of attack, named membership inference, against black-box models [36].", "startOffset": 94, "endOffset": 98}, {"referenceID": 58, "context": "present the model inversion attack [59].", "startOffset": 35, "endOffset": 39}, {"referenceID": 74, "context": "Although the approach illustrates privacy concerns that may arise from giving access to ML models trained on sensitive data, it is unclear whether the genomic information is recovered because of the ML model or the strong correlation between the auxiliary information that the adversary also has access to (the patient\u2019s dosage) [75].", "startOffset": 329, "endOffset": 333}, {"referenceID": 34, "context": "Model inversion enables adversaries to extract training data from observed model predictions [35].", "startOffset": 93, "endOffset": 97}, {"referenceID": 75, "context": "However, the input extracted is not actually a specific point of the training dataset, but rather an average representation of the inputs that are classified in a class\u2014similar to what is done by saliency maps [76].", "startOffset": 210, "endOffset": 214}, {"referenceID": 34, "context": "The demonstration is convincing in [35] because each class corresponds to a single individual.", "startOffset": 35, "endOffset": 39}, {"referenceID": 59, "context": "show how to extract parameters of a model from the observation of its predictions [60].", "startOffset": 82, "endOffset": 86}, {"referenceID": 28, "context": "During inference, an adversary might introduce positively connotated words in spam emails to evade detection, thus creating a test distribution different from the one analyzed during training [29].", "startOffset": 192, "endOffset": 196}, {"referenceID": 76, "context": "[77] pull from robust statistics to build a PCA-based detection model robust to poisoning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "limit the vulnerability of SVMs to training label manipulations by adding a regularization term to the loss function, which in turn reduces the model sensitivity to out-of-diagonal kernel matrix elements [44].", "startOffset": 204, "endOffset": 208}, {"referenceID": 77, "context": "Their approach does not impact the convexity of the optimization problem unlike previous attempts [78], [79], which reduces the impact of the defense mechanism on performance.", "startOffset": 98, "endOffset": 102}, {"referenceID": 78, "context": "Their approach does not impact the convexity of the optimization problem unlike previous attempts [78], [79], which reduces the impact of the defense mechanism on performance.", "startOffset": 104, "endOffset": 108}, {"referenceID": 24, "context": "make proposals to secure learning [25].", "startOffset": 34, "endOffset": 38}, {"referenceID": 79, "context": "Lastly, they suggest the use of disinformation with for instance honeypots [80] and randomization of the ML model behavior.", "startOffset": 75, "endOffset": 79}, {"referenceID": 80, "context": "introduce a new ML model, which they name deep contractive networks, trained using a smoothness penalty [81].", "startOffset": 104, "endOffset": 108}, {"referenceID": 81, "context": "to other gradient-based penalties in [82], [83].", "startOffset": 37, "endOffset": 41}, {"referenceID": 82, "context": "to other gradient-based penalties in [82], [83].", "startOffset": 43, "endOffset": 47}, {"referenceID": 83, "context": "The approach introduced in [84] does not involve the expensive computation of gradient-based penalties.", "startOffset": 27, "endOffset": 31}, {"referenceID": 61, "context": "The technique is an adaptation of distillation [62], a mechanism designed to compress large models into smaller ones while preserving prediction accuracy.", "startOffset": 47, "endOffset": 51}, {"referenceID": 84, "context": "In experiments with the fast gradient sign method [85] and the Jacobian attack [84], larger perturbations are required to achieve misclassification of adversarial examples by the distilled model.", "startOffset": 50, "endOffset": 54}, {"referenceID": 83, "context": "In experiments with the fast gradient sign method [85] and the Jacobian attack [84], larger perturbations are required to achieve misclassification of adversarial examples by the distilled model.", "startOffset": 79, "endOffset": 83}, {"referenceID": 85, "context": "However, [86] identified a variant of the attack of [26] which distillation fails to mitigate on one dataset.", "startOffset": 9, "endOffset": 13}, {"referenceID": 25, "context": "However, [86] identified a variant of the attack of [26] which distillation fails to mitigate on one dataset.", "startOffset": 52, "endOffset": 56}, {"referenceID": 86, "context": "A simpler variant of distillation, called label smoothing [87], improves robustness to adversarial samples crafted using the fast gradient sign method [88].", "startOffset": 58, "endOffset": 62}, {"referenceID": 87, "context": "A simpler variant of distillation, called label smoothing [87], improves robustness to adversarial samples crafted using the fast gradient sign method [88].", "startOffset": 151, "endOffset": 155}, {"referenceID": 25, "context": "However this variant was found to not defend against more computation expensive but precise attacks like the Jacobian-based iterative attack [26].", "startOffset": 141, "endOffset": 145}, {"referenceID": 30, "context": "report that defensive distillation can be evaded using a black-box attack [31].", "startOffset": 74, "endOffset": 78}, {"referenceID": 30, "context": "In [31], this is referred to as gradient masking.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "Due to the adversarial example transferability property [30] described", "startOffset": 56, "endOffset": 60}, {"referenceID": 29, "context": "[30] first suggested injecting adversarial samples, correctly labeled, in the training set as a means to make the model robust.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "9% on adversarial examples [52].", "startOffset": 27, "endOffset": 31}, {"referenceID": 65, "context": "[66] developed the intuition behind adversarial training, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 88, "context": "They solve the problem using stochastic gradient descent [89].", "startOffset": 57, "endOffset": 61}, {"referenceID": 51, "context": "Their experimentation shows improvements over [52], but they are often statistically non-significative.", "startOffset": 46, "endOffset": 50}, {"referenceID": 52, "context": "[53], where they apply the defense with an attack and evaluate robustness with another one.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "The additions of adversarial samples in the training data [30], [53] or modifying", "startOffset": 58, "endOffset": 62}, {"referenceID": 52, "context": "The additions of adversarial samples in the training data [30], [53] or modifying", "startOffset": 64, "endOffset": 68}, {"referenceID": 65, "context": "the training loss function [66] can be viewed as modifying the training distribution D towards such an end.", "startOffset": 27, "endOffset": 31}, {"referenceID": 89, "context": "Stackelberg games also allow for scalability compared to the corresponding simultaneous move game [90], [91].", "startOffset": 98, "endOffset": 102}, {"referenceID": 90, "context": "Stackelberg games also allow for scalability compared to the corresponding simultaneous move game [90], [91].", "startOffset": 104, "endOffset": 108}, {"referenceID": 91, "context": "[92] first recognized that test data manipulation can be seen in the PAC framework as modifying the", "startOffset": 0, "endOffset": 4}, {"referenceID": 92, "context": "[93]):", "startOffset": 0, "endOffset": 4}, {"referenceID": 91, "context": "[92] add a regularizer for the learner and the cost function c(x, x\u2032) as the l2 distance between x and x\u2032.", "startOffset": 0, "endOffset": 4}, {"referenceID": 72, "context": "[73], Li et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 93, "context": "[94] use a cost function that is relevant for many security settings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 94, "context": "This is captured by differential privacy [95], a rigorous framework to analyze the privacy guarantees provided by algorithms.", "startOffset": 41, "endOffset": 45}, {"referenceID": 95, "context": "An instance of training data randomization is formalized by local privacy [96].", "startOffset": 74, "endOffset": 78}, {"referenceID": 96, "context": "showed that this allowed the developers of a browser to collect meaningful and privacy-preserving usage statistics from users [97].", "startOffset": 126, "endOffset": 130}, {"referenceID": 97, "context": "This strategy was explored in [98], [99].", "startOffset": 30, "endOffset": 34}, {"referenceID": 98, "context": "This strategy was explored in [98], [99].", "startOffset": 36, "endOffset": 40}, {"referenceID": 99, "context": "troducing random noise\u2014drawn from an exponential distribution and scaled using the model sensitivity5\u2014in the cost function minimized during learning, can provide \u03b5-differential privacy [100].", "startOffset": 185, "endOffset": 190}, {"referenceID": 100, "context": "privacy analysis, along with references to many of the works intervening in private empirical risk minimization [101]", "startOffset": 112, "endOffset": 117}, {"referenceID": 101, "context": "showed that large-capacity models like deep neural networks can be trained with multi-party computations from perturbed parameters and provide differential privacy guarantees [102].", "startOffset": 175, "endOffset": 180}, {"referenceID": 102, "context": "introduced an alternative approach to improve the privacy guarantees provided: the strategy followed is to randomly perturb parameters during the stochastic gradient descent performed by the learning algorithm [103].", "startOffset": 210, "endOffset": 215}, {"referenceID": 103, "context": "use homomorphic encryption [104] to encrypt the data in a form that allows a neural network to process it without decrypting it [105].", "startOffset": 27, "endOffset": 32}, {"referenceID": 104, "context": "use homomorphic encryption [104] to encrypt the data in a form that allows a neural network to process it without decrypting it [105].", "startOffset": 128, "endOffset": 133}, {"referenceID": 105, "context": "This is especially important in applications like credit decisions or healthcare [106].", "startOffset": 81, "endOffset": 86}, {"referenceID": 106, "context": "It should not nurture discrimination against specific individuals [107].", "startOffset": 66, "endOffset": 71}, {"referenceID": 107, "context": "data also inherently reflects social biases [108].", "startOffset": 44, "endOffset": 49}, {"referenceID": 108, "context": "first learn an intermediate representation that encodes a sanitized variant of the data, as first discussed in [109].", "startOffset": 111, "endOffset": 116}, {"referenceID": 109, "context": "showed that fairness could be achieved by learning in competition with an adversary trying to predict the sensitive variable from the fair model\u2019s prediction [110].", "startOffset": 158, "endOffset": 163}, {"referenceID": 110, "context": "ing [111].", "startOffset": 4, "endOffset": 9}, {"referenceID": 111, "context": "measures to estimate the influence of specific inputs on the model output [112].", "startOffset": 74, "endOffset": 79}, {"referenceID": 112, "context": "An approach named activation maximization synthesizes an input that highly activates a specific neuron in a neural network [113].", "startOffset": 123, "endOffset": 128}, {"referenceID": 113, "context": "producing synthetic inputs easily interpreted by humans [114] but faithfully representing the model\u2019s behavior.", "startOffset": 56, "endOffset": 61}, {"referenceID": 38, "context": "Recall the theoretical characterization for data poisoning by Kearns and Li [39] (see Section IV).", "startOffset": 76, "endOffset": 80}, {"referenceID": 114, "context": "Note that this result (presented below) is analogous to the no free lunch result in data privacy that captures the tension between utility and privacy [115], [116].", "startOffset": 151, "endOffset": 156}, {"referenceID": 21, "context": "Moreover, insufficient data presents fundamental information theoretic limitations [22] on learning accuracy problems in the benign (without adversaries) setting itself.", "startOffset": 83, "endOffset": 87}, {"referenceID": 93, "context": "Thus, while the above theoretical result and recent empirical work [94] suggests more complex models for defeating adversaries, in practice, availability of data may prohibit the use of this general result.", "startOffset": 67, "endOffset": 71}], "year": 2016, "abstractText": "Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive\u2014new systems and models are being deployed in every domain imaginable, leading to rapid and widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community\u2019s understanding of the nature and extent of these vulnerabilities remains limited. We systematize recent findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date. We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. We conclude by formally exploring the opposing relationship between model accuracy and resilience to adversarial manipulation. Through these explorations, we show that there are (possibly unavoidable) tensions between model complexity, accuracy, and resilience that must be calibrated for the environments in which they will be used.", "creator": "LaTeX with hyperref package"}}}