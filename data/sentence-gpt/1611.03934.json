{"id": "1611.03934", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2016", "title": "Personalized Donor-Recipient Matching for Organ Transplantation", "abstract": "Organ transplants can improve the life expectancy and quality of life for the recipient but carries the risk of serious post-operative complications, such as septic shock and organ rejection. The probability of a successful transplant depends in a very subtle fashion on compatibility between the donor and the recipient but current medical practice is short of domain knowledge regarding the complex nature of recipient-donor compatibility. Hence a data-driven approach for learning compatibility has the potential for significant improvements in match quality of life.\n\n\n\n\n\nThe research is supported by the Institute of Science (in collaboration with University of South Africa and the Royal Society), the Foundation for Science, and the Centre for Organisation for Organisation for Organisation.", "histories": [["v1", "Sat, 12 Nov 2016 01:53:54 GMT  (20kb,D)", "http://arxiv.org/abs/1611.03934v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jinsung yoon", "ahmed m alaa", "martin cadeiras", "mihaela van der schaar"], "accepted": true, "id": "1611.03934"}, "pdf": {"name": "1611.03934.pdf", "metadata": {"source": "CRF", "title": "Personalized Donor-Recipient Matching for Organ Transplantation", "authors": ["Jinsung Yoon", "Ahmed M. Alaa", "Martin Cadeiras", "Mihaela van der Schaar"], "emails": [], "sections": [{"heading": "Introduction", "text": "Organ transplantation is the therapy of choice for patients with end stage diseases who are refractory to medical therapies (Shah 2012). Even though organ transplantation can increase the life expectancy and quality of life for the recipient, the operation can entail various complications, including infection, acute and chronic rejection and malignancy (Huynh 2014). Pre-operative anticipation of the risk associated with organ transplantation is a regular task that transplant centers perform in order to determine which patients would benefit from transplantation and accurately identify these for\nar X\niv :1\n61 1.\n03 93\n4v 1\n[ cs\n.L G\n] 1\n2 N\nwhom the risk of transplantation is too high and would therefore provide no survival benefits. Such a risk assessment task is quite complicated for that post-operative patient survival depends on different types of risk factors: recipient-related factors (e.g. cardiovascular disease severity of heart recipients (Wozniak 2014; Nwakanma 2007; Russo 2006; Silva 2016)), recipient-donor matching factors (e.g. weight ratio and HLA (Jayarajan 2013), blood group compatibility (Jawitz 2013), race (Allen 2010), etc), and donor-related factors (e.g. diabetes (Arnaoutakis 2012; Taghavi 2013)). The interactions among all these risk factors make the prognosis problem for the organ transplant outcomes highly complex; the NHLBI working group suggests resolving this problem by enhancing the phenotypic compatibility characterization of the pre-transplant recipient-donor population (Mancini 2010; Collins 2015; Shah 2012).\nIn the light of the above, we seek an enhanced phenotypic characterization for the compatibility of patient-donor pairs via a precision medicine approach (Collins 2015) in which we construct personalized predictive models that are tailored to the individual traits of both the donor and the recipient to the finest possible granularity. The advent of electronic health records (EHR) inspire a data-driven approach for constructing such predictive models in which the complex recipient-donor compatibility patterns are discovered from observational data. To that end, we develop ConfidentMatch: an automated system that learns the recipient-donor compatibility patterns from the EHR data in terms of the probability of transplant success for given recipient-donor pairs. ConfidentMatch can be utilized by the clinicians as a prognostic tool for managing organ transplantation selection decisions in which information about the donor and recipient are fed to the system, and the output comes as the probability of the transplant\u2019s success. The system can also be used in conjunction with a matching algorithm, such as the Nobel prize winning algorithm of Shapley and Roth (Shapley 1962; Roth 2003), where a set of patients are matched to a set of donors given the compatibility scores that ConfidentMatch computes.\nIn order to learn the highly complex recipient-donor compatibility patterns, ConfidentMatch adopts a novel learning framework in which the recipient-donor feature space is partitioned into disjoint subsets, and a separate predictive model (learner) is assigned to each partition. Such a learning approach gives rise to a highly complex overall predictive model for mapping recipient-donor features to transplant success probabilities. Over-fitting is controlled by penalizing the number of partitions in the recipient-donor feature space and the complexity of the learners assigned to the different partitions. Unlike existing meta-learning algorithms, ConfidentMatch solves an optimization problem through which it jointly determines how to partition the recipientdonor feature space, and what predictive model to assign to each partition. Since such an optimization problem is NP-hard, we propose an efficient greedy algorithm that proceed iteratively by first fixing the number of partitions in the recipient-donor feature space, optimizing the predictive models assigned to each partition, and then further stratifying each partition and re-assigning more \u201cspecialized\u201d predictive models to the new, finer partitions. The algorithm stops stratifying the recipient-donor feature space when further stratification would lead to new partitions with no enough per-partition training data for learning finer predictive models.\nExperiments conducted on the UNOS heart transplant dataset (Cecka 1996) show that ConfidentMatch can provide predictions of success with 95% confidence for 5,489\npatients of a total population of 9,620 patients \u2013 410 more patients than for the best state-of-the-art machine learning algorithm (DeepBoost)."}, {"heading": "Related Work", "text": "We identify two broad categories of learning algorithms that are capable of combining multiple predictive models and/or stratifying the feature space into fine clusters: ensemble learning algorithms and clustering algorithms. We compare ConfidentMatch with these methods hereunder.\nEnsemble learning algorithms Methods based on ensemble learning, such as Random Forest (Liaw 2002), LogitBoost (Friedman et al. 2000), Adaptive Boosting (Freund 1997) and DeepBoost (Kuznetsov 2014), operate by allocating different sets of training data to different weak learners, and then aggregating the predictions of these weak learners through a weighted sum to issue a final prediction (Dai et al. 2016) and (Tekin et al. 2016). While these methods can learn complex functions through the synergy of multiple weak learners, they do not integrate the allocation of the training data to the different learners (in both the bagging and boosting approaches) as part of their loss minimization problems. Therefore, these methods do not \u2013 in principle \u2013 learn a granular predictive model that performs well uniformly over the feature space, but rather learn a predictive model that works well \u201con average\u201d. Contrarily, ConfidentMatch jointly optimizes the partitioning of the feature space together with the predictive model associated with each partition, and hence, it learns a refined recipient-donor phenotypic compatibility characterization in which the predictions are tailored to fine segments of the recipient-donor feature space, leading to an overall improved performance as compared to conventional ensemble methods. We will demonstrate the superiority of ConfidentMatch to ensemble learning algorithms in the \u201cResults and Discussion\u201d Section.\nClustering algorithms Clustering is a natural approach for identifying phenotypic characterizations by grouping \u201csimilar\u201d patients (or recipient-donor pairs) into distinct clusters. Clustering algorithms can be divided into two categories: unsupervised and supervised clustering. Unsupervised clustering algorithms, such as the k-means algorithm (MacQueen 1967), utilize the feature space solely to learn the partitions that maximize some given objective, and hence they cannot address the organ transplant prognosis problem since they do not consider the transplant outcomes (i.e. labels) in the clustering process.\nSupervised clustering algorithms utilize both the feature space and the label space for constructing clusters (Eick 2004; Finley 2005); however, the predictive models assigned to each partition of the feature space are limited to indicator functions (an example for such algorithms is the regression tree (Strobl 2009)). For the organ transplant setting, the complex interactions between the donor and recipient features create highly complex patterns of recipient-donor compatibility (transplant success probabilities) that would exhibit very high per-partition impurity under conventional supervised clustering or tree learning algorithms. This means that learning such complex medical\nconcepts would face the dilemma of exhibiting large over-fitting errors when adopting a large number of clusters (or very deep decision trees) to resolve the per-partition impurity, and exhibiting a large bias error when restricting the number of clusters (or restricting the depth of decision trees). ConfidentMatch approaches this problem by providing a versatile framework for complexity control where complex predictive models can be assigned to every partition to reduce the per-partition bias error, which enables learning complex functions with less partitions and hence utilize the training data more efficiently."}, {"heading": "Methods", "text": "Let theD-dimensional recipient-donor feature space be denoted asX ; every instance in X corresponds to a recipient-donor pair with certain given characteristics. Denote the corresponding label space which designates the success or failure of an organ transplant for a given recipient-donor pair as Y; every label instance can be defined as the specific event of transplant success or failure if Y = {0, 1}, or the probability of the transplant\u2019s success if Y = [0, 1]. Let T be a dataset extracted from the EHR; we split the dataset T into two separate sets: a training set S = {(xs1, ys1), ..., (xsm, ysm)} and a validation set V = {(xv1 , yv1), ..., (xvn, yvn)}, where S and V are disjoint (S \u2229 V = \u2205). Every entry in S and V comprise a recipient-donor feature pair and a transplant outcome.\nThe goal of ConfidentMatch is to construct a predictive model h \u2208 H, h : X \u2192 Y that maps recipient-donor pairs to anticipated transplant outcomes; such a model has to be learned from the dataset T = {S,V}, and can be used for out-of-sample recipient-donor pair in order to assess the risk of a recipient\u2019s transplant operation. The problem of learning the predictive model h \u2208 H from the labeled dataset T is a standard supervised learning problem (Shalev-Shwartz 2014).\nThe expected loss of a predictive model h is defined as LF (h) = EF [l(h(x), y)] where l(h(x), y) is a general loss function, and F is the joint recipient-donor featurelabel distribution, which is unknown to the clinicians. The optimal predictive model is defined as h\u2217 = argminh\u2208H LF (h); since F is unknown, we cannot directly find the optimal predictive model, and hence we resort to minimizing the empirical loss as measured over the training and validation sets. The empirical loss for the training set\nis defined as LS(h) = 1\nm\n\u2211m i=1 l(h(x s i ), y s i ), and it can be defined similarly for the\nvalidation set. Note that as pointed out in the previous section, the optimal predictive model h\u2217 is likely to be of a very complex structure as it abstracts a complex medical concept, i.e. the interactions between the recipient and donor features and their effect on the transplant outcomes. A poor initial choice for the space of possible models H, e.g. letting H be a hypothesis class with a small VC dimension, may lead to a large bias in the loss function of h\u2217, and hence we need a more versatile learning framework for which the complexity of the predictive model h adapts to the complexity of the underlying medical concept being learned.\nConfidentMatch adopts a novel framework for crafting complex predictive models out of simpler baseline models by creating a phenotypic characterization of the\nrecipient-donor feature space in which separate predictive models are assigned to disjoint partitions of the feature space. That is, ConfidentMatch outputs a set of partitions that cover the entire recipient-donor feature space, together with a set of predictive models, each tailored to a given partition, thereby leading to an overall complex, granular predictive model. Formally, we divide the recipient-donor feature space X into k disjoint subsets, where k is to be determined based on the given dataset, in such a way that for each subset, we can have a separate optimal predictive model that minimizes the overall expected risk. We write {X1, ...,Xk} to denote a partition of the feature space X , where all such partitions are ensured to be disjoint and cover X . The partition {X1, ...,Xk} can be translated to a partitioning of the training set S and validation set V , i.e. the training set is partitioned as {S1, ...,Sk}, where Si = {(x, y) : (x, y) \u2208 S and x \u2208 Xi}.\nGiven the above construct, the learning problem becomes a problem of (jointly) finding the optimal partitioning {X1, ...,Xk} of the recipient-donor feature space, together with the optimal predictive model hi \u2208 H associated with every partition i, i.e. assuming that we know the distribution F , the optimal predictive model is found by solving the following optimization problem\nmin {X1,...,Xk}\n[ min\nh1,...,hk\u2208H k\u2211 i=1 F(X \u2208 Xi)\u00d7 EFi [l(hi(x), y)]\n]\nsubject to X = k\u22c3 i=1 Xi, and Xi \u2229 Xj = \u2205 \u2200i 6= j.\n(1)\nWe break down the problem in (1) into two nested optimization problems; we first focus on the solution of the inner optimization problem and define its solution (for a given partitioning {X1, ...,Xk}) as\nd({X1, ...,Xk}) = min h1,...,hk\u2208H k\u2211 i=1 F(X \u2208 Xi)\u00d7 EFi [l(hi(x), y)].\nNote that given a partition {X1, ...,Xk}, the solutions to the inner optimizations are separable, i.e. the optimal predictor of one partition can be determined independent of the choice of the predictors for the other partitions. Hence, we can simplify the inner optimization problem as follows.\nmin h1,...,hk\u2208H k\u2211 i=1 F(X \u2208 Xi)\u00d7 EFi [l(hi(x), y)] =\nk\u2211 i=1 min hi\u2208H F(X \u2208 Xi)\u00d7 EFi [l(hi(x), y)].\nSince ConfidentMatch has no access to the true distribution F , the algorithm has to learn the partitioning {X1, ...,Xk} and the corresponding predictive models {hi}ki=1 from the dataset T = S\u222aV) in such a way that it reaches a loss function that is as close as possible to the true loss in (1). To achieve this, we construct a proxy for the objective\nin (1) by replacing the terms F(X \u2208 Xi) and EFi [l(hi(x), y)], which depend on the unknown F , with their sample estimates |Vi| n and LVi(hi) = 1 n \u2211n i=1 l(hi(x v i ), y v i ). Hence, empirical loss minimization over the validation dataset V can be formulated as follows\nmin k,X1,...,Xk [ k\u2211 i=1 min hi\u2208H |Vi| n \u00d7 1 |Vi| \u2211\n(xvj ,y v j )\u2208Vi\nl(hi(x v j ), y v j )\n]\nsubject to X = k\u22c3 i=1 Xi, and Xi \u2229 Xj = \u2205 for \u2200i 6= j.\n(2)\nConfidentMatch constructs the hypothesis class H in (2), from which we select a hypothesis (or a predictive model) hi for every partition i, by combining the outputs of a finite set of M learners {A1, ...,AM}, each of which can learn a predictive model that belongs to some hypothesis class H(A). That is, for a fixed partition Xi, using the corresponding training set Si, the predictive model that is learned by the learning algorithm Ai for partition i is Ai(Si). Therefore, the set of all predictive models that can be learned by all the learning algorithms operating on data set Si is given as H\u0302(Si) = {A1(Si), ...,AM (Si)}. ConfidentMatch decides the optimal partitioning and the optimal predictor for each partition i that belongs to a set of learnable predictors H\u0302(Si) by minimizing the empirical loss with respect to the validation data set as follows\nmin k,X1,...,Xk [ k\u2211 i=1 min hi\u2208H\u0302(Si) |Vi| n \u00d7 1 |Vi| \u2211\n(xvj ,y v j )\u2208Vi\nl(hi(x v j ), y v j )\n]\nsubject to X = k\u22c3 i=1 Xi,and Xi \u2229 Xj = \u2205 for \u2200i 6= j.\n(3)\nNote that the formulation in (3) does not account for the out-of-sample error (or over-fitting); to handle that, we reformulate problem (3) by replacing the objective function with a tight upper bound on the true loss that appropriately penalizes overfitting. Define\nd\u0302({X1, ..Xk}) = k\u2211 i=1 min hi\u2208H\u0302(Si)\n[ 1\nn \u2211 (xvj ,y v j )\u2208Vi l(hi(x v j ), y v j )\n]\n+ \u03b1\n\u221a k2 logM\nn , (4)\nwhere \u03b1 \u2265 0 is a penalty parameter. The expression in (4) comprises the sample es-\ntimate of the objective and a penalty term \u03b1\n\u221a k logM\nn/k that penalizes: the number of\npartitions k, the average size of a partition n/k, and the number of predictive model M from which we chose one model to assign to a given partition. It can be shown that\nif the penalty parameter \u03b1 \u2265\n\u221a 1\n2 +\n1\n2 logM log(\n2\n1\u2212 (1\u2212 \u03b4)1/k ), then the probabil-\nity that d({X1, ..Xk}) is bounded above by d\u0302({X1, ..Xk}) is greater than 1 \u2212 \u03b4, i.e. P(d({X1, ...,Xk}) < d\u0302({X1, ...,Xk})) \u2265 1\u2212 \u03b4 (the proof can be found in the supporting material). By using the upper bound d\u0302({X1, ..Xk}) as the objective, the empirical loss minimization problem becomes\nmin {X1,...,Xk}\n[ k\u2211 i=1 min hi\u2208 \u02c6H(Si) 1 n \u2211 (xvj ,y v j )\u2208Vi l(h(xvj ), y v j ) ]\n+ \u03b1\n\u221a k2 logM\nn\nsubject to X = k\u22c3 i=1 Xi, and Xi \u2229 Xj = \u2205 for \u2200i 6= j\n(5)\nSolving (5) is computationally intractable because the number of possible partitions for n points grows as the Bell number. To address this problem, ConfidentMatch adopts an efficient greedy algorithm for approximating the solution to (5). As a first step to construct such an algorithm, we reformulate (5) by incorporating two more constraints. First, we restrict the partitions of the recipient-donor feature space to be hypercubes. A hypercubic partition of the feature space X is defined as {X1, ..,Xk} where Xi =\u220fD j=1[aij , bij ], aij \u2264 bij , aij \u2208 R\u2217, bij \u2208 R\u2217. (R\u2217 is defined as R \u222a {\u2212\u221e,\u221e}). Second, we restrict the number of partitions to be \u03b3 \u2208 N. The optimization problem in (5) with these additional constraints can be stated as follows.\nmin {X1,...,Xk} [ k\u2211 i=1 min hi\u2208H\u0302(Si) 1 n \u2211 (xvj ,y v j )\u2208Vi l(h(xvj ), y v j ) ]\n+ \u03b1\n\u221a k2 logM\nn\nsubject to Xi = D\u220f j=1 [aij , bij ], aij \u2264 bij , aij \u2208 R\u2217, bij \u2208 R\u2217\nk \u2264 \u03b3, where k \u2208 Z+ X = k\u22c3 i=1 Xi, and Xi \u2229 Xj = \u2205 for \u2200i 6= j.\n(6)\nLet Opt(X ) be the optimal partition that solves the optimization problem in (6); we construct a greedy algorithm which iteratively solve the optimization problem (6) to achieve the approximate solution for (5) as follows. We write the partition that is generated when X is input to the optimization problem (6) as Opt(X ) = {X1,X2, ..,Xk} where k < \u03b3. We apply the same procedure recursively on each Xi separately up to the\npoint where we do not expect to improve the objective function. The final partition is the union of the partitions that are generated by applying this procedure recursively to each Xi. We write the final partition achieved by the greedy algorithm as {X\u0302 \u22171 , ..., X\u0302 \u2217k }.\nThe pseudo-code for ConfidentMatch is given in Fig. ??. The algorithm learns the recipient-donor compatibility patterns in an offline manner using the procedure described above: it jointly optimizes the partitioning of the recipient-donor feature space (offline stage I), and then optimizes the predictive model associated with each partition (offline stage II). Having learned the recipient-donor compatibilities, the algorithm operates in an online stage for new recipients and donors by computing a compatibility score, i.e. the probability of transplant success, for a given recipient-donor pair. The compatibility score is displayed to the clinicians and based on it, the clinicians/patients can make decisions on whether a transplant should be conducted, or whether the recipient should be matched with another donor.\nIn what follows, we specify the computational complexity of ConfidentMatch. Let the complexity of the algorithmAl(Si) for learning a predictive model using the dataset Si be Tl(|Si|, D). Based on this, it can be shown that the worst-case complexity for computingOpt(X ) in the optimization problem (6) isO(\u03b3\u03b3+1n\u03b3D\u03b3\u22121 \u2211M i=1 Ti(n,D)),\nand the complexity of the greedy algorithm described in Fig. ?? isO(\u03b3\u03b3+1n\u03b3+1D\u03b3\u22121 \u2211M i=1 Ti(n,D)) (proofs are provided in the supporting material)."}, {"heading": "Results and Discussion", "text": "Experiments were conducted using the UNOS database for patients who underwent a heart transplant over the years from 1987 to 2015 (Cecka 1996). We use the \u201dThoracic DATA\u201d dataset in the UNOS database as our root dataset. In this dataset, all patients were followed-up until death, i.e. the post-transplant survival times for all patients are available in the dataset. Of the 148,512 patients in the Thoracic DATA who underwent either heart or lung transplant, we extract 60,516 patients who underwent a heart transplant. Of the 60,516 patients who underwent a heart transplant, we exclude 3,800 patients who are still alive, and we only use the 56,716 patients for whom we have the exact survival (lifetime) information.\nFor each patient in the dataset, a total of 504 features are provided; these include a combination of both the patient\u2019s and the donor\u2019s information. We discard 12 features that are normally obtained after the transplant. Of the rest 492 features, we extract 70 features for which we have less than 10% missing information in order to reduce the noise of imputation. We use the KNN imputation method to impute the missing data (Hastie 1999).\nWe compared the performance of ConfidentMatch in predicting the success of transplants with the following benchmark algorithms: logistic regression (Logit), Lasso regularized logistic re-gression (Lasso), decision tree (DTree), Random Forest (RForest), AdaBoost (ABoost), and DeepBoost (DBoost). We use the correlation feature selection (CFS) method to discover the relevant features for both ConfidentMatch and the benchmark algorithms (Hall 1999). We adopt the following metric for quantifying the performance of the different algorithms. The transplant\u2019s success probability is quantified via the 3-year post-transplant survival rate (long-term survival rate). We say that\nan algorithm provides a prediction for the transplant\u2019s success (probability of 3-year post-transplant survival) for a certain recipient-donor pair with a confidence level X% if the algorithm\u2019s probability of correct prediction is X% for that recipient-donor pair. Based on this definition, we define the gain of ConfidentMatch at a confidence level of X% as the number of recipient-donor instances in the testing dataset for which ConfidentMatch provides a confident prediction for the transplant success, whereas the best competing benchmark does not.\nWe split the dataset into a training set comprising the recipient-donor instances in which the recipient underwent the transplant before the year 2010 (past patients), and a testing set that comprises recipients who underwent the transplant after 2010 (current patients). Of the 56,716 recipient-donor pairs, 47,096 pairs (83.04%) were used for training, and 9,620 pairs (16.96%) were used for testing. We varied the confidence level from 80% to 95% and evaluated the performance within this range of confidence levels.\nTable 1 and Fig 2 show that ConfidentMatch consistently outperforms the benchmarks to predict the success of heart transplant regarding 3-year mortality prediction. For instance, ConfidentMatch boosts the number of recipient-donor pairs who get 95% confident predictions by 410 as compared to the best performing benchmark (DBoost); this means that ConfidentMatch can allow an additional number of 410 recipient-donor pairs to make better pre-transplant decisions such as whether or not the transplant should be conducted, and whether the recipient should be matched with another donor."}, {"heading": "Conf.\u2217 CM\u2217 LASSO RF\u2217 AB\u2217 DB\u2217 DTree", "text": ""}, {"heading": "80% 9269 7721 9057 9063 9067 7893", "text": ""}, {"heading": "85% 7798 7096 7605 7577 7614 7316", "text": ""}, {"heading": "90% 6467 4431 6065 6076 6080 4526", "text": ""}, {"heading": "95% 5489 2796 5065 5058 5079 3124", "text": "The performance gains achieved by ConfidentMatch can be attributed to the improved phenotypic characterization of the recipient-donor pairs that the algorithm achieves by stratifying the recipient-donor feature space. The fine and granular phenotypic characterization achieved by ConfidentMatch is restricted by the size of the training data; the more recipient-donor instances are available in the training set, the larger is the number of partitions that ConfidentMatch can construct and cast a specialized predictive model to. Fig 3 illustrates the trade-off associated with increasing the complexity of ConfidentMatch\u2019s predictive model by increasing the number of partitions; if the number of partitions increases, the gain of ConfidentMatch also increases as it copes with the underlying complexity of the recipient-donor compatibility patterns, until a certain number of partitions when the gain starts to decrease due to over-fitting.\nConfidentMatch does not only improve the quality of prognosis, but can also draw clinical insights on the patterns of recipient-donor compatibility. To illustrate this, we list the first two partitions through which ConfidentMatch stratifies the recipient feature\nspace: ConfidentMatch forms two recipient groups, group A comprises patients whose length of stay in status 1A (urgent transplant wait-list) is shorter than 10 days, whereas group B comprises the remaining patients. Table 2 lists the most relevant features that are predictive of the transplant outcome for each group. It can be seen that group B patients are more sensitive to the donor characteristics; the donor\u2019s VDRL result and blood type are relevant to the transplant outcome, whereas group A patient appear to be less sensitive to these features. Thus, the more the patient waits in status 1A, the more it becomes essential to consider the extent of her compatibility with the donors. As more training data becomes available, ConfidentMatch can reveal finer partitions and identify the relevant features for more granular recipient subgroups."}, {"heading": "Conclusions", "text": "Organ transplants for patients with end stage diseases carries the risk of various serious post-operative complications, pre-operative anticipation of the transplant outcome depends on the compatibility between the donor and the recipient. In this paper, we have developed ConfidentMatch, a data-driven system that learns complex recipient-donor compatibility patterns from the outcomes of previous transplants. ConfidentMatch captures the complexity of such compatibility patterns by optimally dividing the recipientdonor feature space into clusters and assigning different optimal predictive models to each cluster, thereby ensuring that predictions are \u201cpersonalized\u201d and tailored to individual characteristics of both the donors and the recipients. Experiments conducted on a public heart transplant dataset demonstrate the superiority of ConfidentMatch to other competing benchmark algorithms."}], "references": [{"title": "A comparison of antecedents to cardiac arrests, deaths and emergency intensive care admissions in Australia and New Zealand, and the United Kingdomthe ACADEMIA study", "author": ["J Kause"], "venue": "Resuscitation 62: 275\u2013282.", "citeRegEx": "Kause,? 2004", "shortCiteRegEx": "Kause", "year": 2004}, {"title": "Selection of cardiac transplantation candidates in 2010", "author": ["D. Mancini", "K. Lietz"], "venue": "Circulation 122: 173\u201383.", "citeRegEx": "Mancini and Lietz,? 2010", "shortCiteRegEx": "Mancini and Lietz", "year": 2010}, {"title": "The opportunity cost of futile treatment in the intensive care unit", "author": ["T.N. Huynh", "E.C. Kleerup", "P.P. Raj", "N.S. Wenger"], "venue": "Critical Care Medicine, 42: 1977\u20131982.", "citeRegEx": "Huynh et al\\.,? 2014", "shortCiteRegEx": "Huynh et al\\.", "year": 2014}, {"title": "Heart transplantation research in the next decade\u2013a goal to achieving evidence-based outcomes: National Heart, Lung, And Blood Institute Working Group", "author": ["R. Shah M.", "C. Starling R.", "L. Schwartz L.", "R. Mehra M."], "venue": "J Am Coll Cardiol, 59: 1263\u20131269.", "citeRegEx": "M. et al\\.,? 2012", "shortCiteRegEx": "M. et al\\.", "year": 2012}, {"title": "A new initiative on precision medicine", "author": ["S. Collins F.", "H. Varmus"], "venue": "New England Journal of Medicine. 372: 793\u2013795.", "citeRegEx": "F. and Varmus,? 2015", "shortCiteRegEx": "F. and Varmus", "year": 2015}, {"title": "The International Heart Transplant Survival Algorithm (IHTSA): a new model to improve organ sharing and survival", "author": ["J. Nilsson", "M. Ohlsson", "P. Hglund", "B. Ekmehag", "B. Koul", "B. Andersson"], "venue": "PLoS One.", "citeRegEx": "Nilsson et al\\.,? 2015", "shortCiteRegEx": "Nilsson et al\\.", "year": 2015}, {"title": "Ventricular Assist Devices or Inotropic Agents in Status 1A Patients? Survival Analysis of the United Network of Organ Sharing Database. The Annals of thoracic surgery", "author": ["J. Wozniak C", "J. Stehlik", "C Baird B"], "venue": null, "citeRegEx": "C. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "C. et al\\.", "year": 2014}, {"title": "Influence of pretransplant panel-reactive antibody on outcomes in 8,160 heart transplant recipients in recent era", "author": ["U. Nwakanma L.", "A. Williams J.", "S. Weiss E.", "D. Russell S.", "A. Baumgartner W.", "V. Conte J."], "venue": "The Annals of Thoracic Surgery 84: 1556\u20131562.", "citeRegEx": "L. et al\\.,? 2007", "shortCiteRegEx": "L. et al\\.", "year": 2007}, {"title": "Survival after heart transplantation", "author": ["J Russo M"], "venue": null, "citeRegEx": "M.,? \\Q2006\\E", "shortCiteRegEx": "M.", "year": 2006}, {"title": "Effect of Peripheral Vascular Disease on Mortality in Cardiac Transplant Recipients (from the United Network of Organ Sharing Database)", "author": ["Enciso", "J. S"], "venue": "The American journal of cardiology 114: 1111\u20131115.", "citeRegEx": "Enciso and S,? 2014", "shortCiteRegEx": "Enciso and S", "year": 2014}, {"title": "Impact of low donor to recipient weight ratios on cardiac transplantation", "author": ["N. Jayarajan S.", "S. Taghavi", "E. Komaroff", "A. Mangi A."], "venue": "Journal on Thoracic Cardiovascular Surgery 146: 1538\u20131543.", "citeRegEx": "S. et al\\.,? 2013", "shortCiteRegEx": "S. et al\\.", "year": 2013}, {"title": "Impact of ABO compatibility on outcomes after heart transplantation in a national cohort during the past decade", "author": ["K. Jawitz O.", "N. Jawitz", "D. Yuh D.", "P. Bonde"], "venue": "Journal on Thoracic Cardiovascular Surgery 146: 1239-1245.", "citeRegEx": "O. et al\\.,? 2013", "shortCiteRegEx": "O. et al\\.", "year": 2013}, {"title": "The impact of race on survival after heart transplantation: an analysis of more than 20,000 patients", "author": ["G. Allen J.", "S. Weiss E.", "J. Arnaoutakis G.", "D. Russell S.", "A. Baumgartner W.", "V. Conte J.", "S. Shah A."], "venue": "The Annals of thoracic surgery 89:1956\u20131963.", "citeRegEx": "J. et al\\.,? 2010", "shortCiteRegEx": "J. et al\\.", "year": 2010}, {"title": "Cardiac transplantation can be safely performed using selected diabetic donors", "author": ["S. Taghavi", "N. Jayarajan S.", "M. Wilson L.", "E. Komaroff", "M. Testani J.", "A. Mangi A."], "venue": "Journal on Thoracic Cardiovascular Surgery 146: 442\u2013447.", "citeRegEx": "Taghavi et al\\.,? 2013", "shortCiteRegEx": "Taghavi et al\\.", "year": 2013}, {"title": "Institutional volume and the effect of recipient risk on shortterm mortality after orthotopic heart transplant", "author": ["J. Arnaoutakis G", "J. George T", "G. Allen J", "D. Russell S", "S. Shah A", "V. Conte J", "S. Weiss E"], "venue": "Journal on Thoracic Cardiovascular Surgery", "citeRegEx": "G. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "G. et al\\.", "year": 2012}, {"title": "College Admissions and the Stability of Marriage", "author": ["D. Gale", "L.S. Shapley"], "venue": "American Mathematical Monthly 69: 9\u201314.", "citeRegEx": "Gale and Shapley,? 1962", "shortCiteRegEx": "Gale and Shapley", "year": 1962}, {"title": "Classication and regression by random-forest", "author": ["A. Liaw", "M. Wiener"], "venue": "R news 3: 18\u201322.", "citeRegEx": "Liaw and Wiener,? 2002", "shortCiteRegEx": "Liaw and Wiener", "year": 2002}, {"title": "Additive logistic regression: a statistical view of boosting", "author": ["J. Friedman", "T. Hastie", "R Tibshirani"], "venue": "The annals of statistics 28: 337\u2013407.", "citeRegEx": "Friedman et al\\.,? 2000", "shortCiteRegEx": "Friedman et al\\.", "year": 2000}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of computer and system sciences 55: 119\u2013 139.", "citeRegEx": "Freund and Schapire,? 1997", "shortCiteRegEx": "Freund and Schapire", "year": 1997}, {"title": "Multi-class deep boosting", "author": ["V. Kuznetsov", "M. Mohri", "U. Syed"], "venue": "NIPS, 2501\u2013 2509.", "citeRegEx": "Kuznetsov et al\\.,? 2014", "shortCiteRegEx": "Kuznetsov et al\\.", "year": 2014}, {"title": "Some methods for classication and analysis of multivariate observations", "author": ["J MacQueen"], "venue": "Proceedings of the fth Berkeley symposium on mathematical statistics and probability 1: 281\u2013297.", "citeRegEx": "MacQueen,? 1967", "shortCiteRegEx": "MacQueen", "year": 1967}, {"title": "Supervised clustering-algorithms and benets", "author": ["C.F. Eick", "N. Zeidat", "Z. Zhao"], "venue": "IEEE ICTAI 774\u2013776.", "citeRegEx": "Eick et al\\.,? 2004", "shortCiteRegEx": "Eick et al\\.", "year": 2004}, {"title": "Supervised clustering with support vector machines", "author": ["T. Finley", "T. Joachims"], "venue": "ICML 217\u2013224.", "citeRegEx": "Finley and Joachims,? 2005", "shortCiteRegEx": "Finley and Joachims", "year": 2005}, {"title": "An introduction to recur-sive partitioning: rationale, application, and characteristics of classication and regression trees, bagging, and random forests", "author": ["C. Strobl", "J. Malley", "G. Tutz"], "venue": "Psychological methods 14: 323.", "citeRegEx": "Strobl et al\\.,? 2009", "shortCiteRegEx": "Strobl et al\\.", "year": 2009}, {"title": "The unos scientic renal transplant registryten years of kidney transplants", "author": ["M. Cecka J."], "venue": "Clinical transplants 114.", "citeRegEx": "J.,? 1996", "shortCiteRegEx": "J.", "year": 1996}, {"title": "Imputing missing data for gene expression arrays", "author": ["T. Hastie", "R. Tibshirani", "G. Sherlock", "M. Eisen", "P. Brown", "Botstein D"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 1999}, {"title": "Correlation-based feature selection for machine learning", "author": ["M.A. Hall"], "venue": "PhD thesis, The University of Waikato.", "citeRegEx": "Hall,? 1999", "shortCiteRegEx": "Hall", "year": 1999}, {"title": "Understanding machine learning: From theory to algorithms", "author": ["S. Shalev-Shwartz", "S. Ben-David"], "venue": "Cambridge University Press.", "citeRegEx": "Shalev.Shwartz and Ben.David,? 2014", "shortCiteRegEx": "Shalev.Shwartz and Ben.David", "year": 2014}, {"title": "Kidney exchange", "author": ["A.E. Roth", "T. Sonmez", "M.U. Unver"], "venue": "National Bureau of Economic Research.", "citeRegEx": "Roth et al\\.,? 2003", "shortCiteRegEx": "Roth et al\\.", "year": 2003}, {"title": "Bagging Ensembles for the Diagnosis and Prognostication of Alzheimer\u2019s Disease", "author": ["P. Dai", "F. Gwadry-Sridhar", "M. Bauer", "M. Borrie"], "venue": "Thirtieth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Dai et al\\.,? 2016", "shortCiteRegEx": "Dai et al\\.", "year": 2016}, {"title": "Adaptive ensemble learning with confidence bounds for personalized diagnosis. AAAI Workshop on Expanding the Boundaries of Health Informatics using AI (HIAI16): Making Proactive, Personalized, and Participatory Medicine A Reality", "author": ["C. Tekin", "J. Yoon", "M. van der Schaar"], "venue": null, "citeRegEx": "Tekin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tekin et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "Ensemble learning algorithms Methods based on ensemble learning, such as Random Forest (Liaw 2002), LogitBoost (Friedman et al. 2000), Adaptive Boosting (Freund 1997) and DeepBoost (Kuznetsov 2014), operate by allocating different sets of training data to different weak learners, and then aggregating the predictions of these weak learners through a weighted sum to issue a final prediction (Dai et al.", "startOffset": 111, "endOffset": 133}, {"referenceID": 29, "context": "2000), Adaptive Boosting (Freund 1997) and DeepBoost (Kuznetsov 2014), operate by allocating different sets of training data to different weak learners, and then aggregating the predictions of these weak learners through a weighted sum to issue a final prediction (Dai et al. 2016) and (Tekin et al.", "startOffset": 264, "endOffset": 281}, {"referenceID": 30, "context": "2016) and (Tekin et al. 2016).", "startOffset": 10, "endOffset": 29}, {"referenceID": 20, "context": "Unsupervised clustering algorithms, such as the k-means algorithm (MacQueen 1967), utilize the feature space solely to learn the partitions that maximize some given objective, and hence they cannot address the organ transplant prognosis problem since they do not consider the transplant outcomes (i.", "startOffset": 66, "endOffset": 81}, {"referenceID": 26, "context": "We use the correlation feature selection (CFS) method to discover the relevant features for both ConfidentMatch and the benchmark algorithms (Hall 1999).", "startOffset": 141, "endOffset": 152}], "year": 2016, "abstractText": "Organ transplants can improve the life expectancy and quality of life for the recipient but carries the risk of serious post-operative complications, such as septic shock and organ rejection. The probability of a successful transplant depends in a very subtle fashion on compatibility between the donor and the recipient \u2013 but current medical practice is short of domain knowledge regarding the complex nature of recipient-donor compatibility. Hence a data-driven approach for learning compatibility has the potential for significant improvements in match quality. This paper proposes a novel system (ConfidentMatch) that is trained using data from electronic health records. ConfidentMatch predicts the success of an organ transplant (in terms of the 3-year survival rates) on the basis of clinical and demographic traits of the donor and recipient. ConfidentMatch captures the heterogeneity of the donor and recipient traits by optimally dividing the feature space into clusters and constructing different optimal predictive models to each cluster. The system controls the complexity of the learned predictive model in a way that allows for assuring more granular and confident predictions for a larger number of potential recipient-donor pairs, thereby ensuring that predictions are \u201cpersonalized\u201d and tailored to individual characteristics to the finest possible granularity. Experiments conducted on the UNOS heart transplant dataset show the superiority of the prognostic value of ConfidentMatch to other competing benchmarks; ConfidentMatch can provide predictions of success with 95% confidence for 5,489 patients of a total population of 9,620 patients, which corresponds to 410 more patients than the most competitive benchmark algorithm (DeepBoost).", "creator": "TeX"}}}