{"id": "1611.04010", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2016", "title": "Multi-Language Identification Using Convolutional Recurrent Neural Network", "abstract": "Language Identification, being an important aspect of Automatic Speaker Recognition has had many changes and new approaches to ameliorate performance over the last decade. We compare the performance of using audio spectrum in the log scale and using Polyphonic sound sequences from raw audio samples to train the neural network and to classify speech as either English or Spanish. To achieve this, we use the novel approach of using a Convolutional Recurrent Neural Network using Long Short Term Memory (LSTM) or a Gated Recurrent Unit (GRU) for forward propagation of the neural network from a single source. An initial neural network was constructed to evaluate its performance with the Convolutional Recurrent Neural Network, and to identify a common underlying underlying neural network. We use a Gaussian convolutional neural network using the Gradient LSTM network to describe the general principles of LSTM. We use the first LSTM algorithm to test for the strength and accuracy of the LSTM network, and to compare the reliability and accuracy of the LSTM network and the statistical power of the LSTM network. We use LSTM as a basis for the classification of LSTM to predict its performance on a daily basis. We use the LSTM model to identify a common feature in LSTM that is most often encountered when dealing with large populations in large groups. We also use the Convolutional Recurrent Neural Network to identify common features related to LSTM. We also use the Linear Learning, Convolutional Neural Network (LDFN) as a model to identify a common feature of LSTM in all LSTM network. We also use LSTM as a base learning model to identify a common feature in LSTM that is most commonly encountered when dealing with large populations in large groups. We also use the Convolutional Recurrent Neural Network to identify common features related to LSTM. We also use the Convolutional Recurrent Neural Network to identify common features related to LSTM. We also use LSTM as a base learning model to identify a common feature in LSTM that is most often encountered when dealing with large populations in large groups. We also use the Convolutional Recurrent Neural Network to identify common features related to LSTM. We also use LSTM as a base learning model to identify a common feature in LSTM that is most often encountered when dealing with large populations in large groups. We also use LSTM as a base learning model to", "histories": [["v1", "Sat, 12 Nov 2016 15:59:22 GMT  (861kb)", "http://arxiv.org/abs/1611.04010v1", "10 pages, 6 figures"], ["v2", "Thu, 18 May 2017 09:15:26 GMT  (0kb,I)", "http://arxiv.org/abs/1611.04010v2", "Further experiments were performed on the model using LibriVox speech dataset and it was found that a Time Distributed CRNN model performed better and represented our initial ideas about the speaker recognition task better. The dataset contains speech in three languages - English, Spanish and Czech. A report on our findings along with experimental results will be published soon"]], "COMMENTS": "10 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vrishabh ajay lakhani", "rohan mahadev"], "accepted": false, "id": "1611.04010"}, "pdf": {"name": "1611.04010.pdf", "metadata": {"source": "CRF", "title": "MULTI-LANGUAGE IDENTIFICATION USING CONVOLUTIONAL RECURRENT NEURAL NETWORK", "authors": ["Rohan Mahadev", "Vrishabh Lakhani"], "emails": ["rohan.mahadev@somaiya.edu", "vrishabh.l@somaiya.edu"], "sections": [{"heading": null, "text": "KEYWORDS\nLanguage Identification, Neural Networks, Machine Learning, Speaker Recognition, Acoustic Features."}, {"heading": "1. INTRODUCTION", "text": "The problem of language identication (LID) can be dened as the process of automatically identifying the language of a given spoken utterance.\nMany state-of-the-art LID systems rely on Acoustic Modeling. In particular, guided by the advances on speaker verication and recognition, the use of i-vector extractors for extracting features from the sample and using classification techniques such as Bayesian classification has become the norm in acoustic LID systems [1].\nLanguage Identification has for a long time been an aspect of Automatic Speaker Recognition which has been constrained more by the processing capabilities of machines rather than using the best technology available at that time. One such case has been the less frequent use of Convolutional Recurrent Neural Networks (CRNN)[7] which are computationally heavy due to multiple complex layers and each layer having to compute the parameters multiple times. But, due to the advent of GPUs with processing powers much greater than what it has been in\nthe past, using CRNNs is not only much more feasible but outperform traditional methods of Language Identification such as using GMMs and Hidden Markov Models. The study by Schmidhuber, Graves and Fernandez shows this gulf in the performance [3].\nThe most common features used for this task till now has been using Mel Frequency Cepstrum Coefficients which discard a lot of information and only extract certain parameters which, definitely being useful for the task, do not paint the entire picture. We believe that using log scale of polyphonic sound features; instead of using MFCC prove to be a much more useful tool in the job of classification of languages as it provides more information as lesser features could lead to higher variance in the test data."}, {"heading": "2. RELATED WORK", "text": ""}, {"heading": "2.1. The i-vector based standard LID system", "text": "This approach is based on the approach described in [4]. This approach is based on the Gaussian components, which are trained on PLP coefficients like MFCC and \u2206(delta) of MFCC and \u2206\u2206(double-delta) MFCC which are extracted over a specific frame rate of 10ms over 25ms window length and following the standard protocol described in [5] we can extract the i-vector of each languages.\nAfter extracting the i-vector of each of the language; we can train our machine for the classification task of the language using logistic regression (LR) can be applied over it; or as an alternate approach, we can use Linear discrimination analysis(LDA) followed by cosine of Linear discrimination analysis, and Gaussian modelling to fit the i-vectors of each language."}, {"heading": "2.2. Automatic Language Identification using Convolutional Recurrent Neural Networks", "text": "The approach presented by [6] is an improvement over the traditional i-vector analysis of the LID system since it tries to find the temporal hidden patterns between each window frames using the recurrent hidden layers. However in this approach, the main shortcoming is that it cannot sufficiently find the intra-frame pattern which can provide the information required for identifying the variance necessary for removing the bias towards the phonetic spectrum of a specific frequency of voices, specific to the phoneme of the speaker on which the training dataset is based on."}, {"heading": "3. APPROACH", "text": "We use a labeled set of data from speech databases and extract the essential audio features in the log scale from them and that is used as the input layer for our CRNN. Traditionally, Convolutional recurrent neural network are used for scene labelling[7], that is, using Convolutional Neural Networks with Intra-layer Recurrent Connections, we use this novel approach for finding the relation between the frames of audio sample in the training dataset.\nThe advantage of doing is that, theoretically, it can provide the solution to the problem in the approach described by [6].\nIn the figure, we see that, we have extracted the log scale of the audio sample, eliminating all the redundant frequency and drawing a spectrogram of the frequency log scale, here, we can visualise that the convolutional part of the CRNN model tried to find the pattern within each time frames, while the recurrent part\u2019s memory tries to seek pattern in between each time frame, thus outperforming any other traditional approach previously developed.\n4. ARCHITECTURE\nThe architecture of the neural network model we propose is as follows."}, {"heading": "4.1 RNN:", "text": "The main relation to be established in a speaker recognition problem is between the temporal audio features, which are the features which change with time. The feedback into the network at each time interval gives a sequence of values as the input. In conventional RNNs, input sequences are mapped to hidden layers by the following equations:\nh t = g(W x h xt + W hh h t\u22121 + b h ) z t = g(W hz h t + b z )\nWhere g is usually \u03c3 or tanh. Due to this recurrent nature of the input sequences travelling through many layers makes RNNs very efficient at context based learning tasks such as speaker recognition and that being the reason that we use a Recurrent Network as the basis for our architecture. Although a temporal relationship is established, the vanishing gradient problem, mentioned by [9] is even more prevalent in an RNN, because each feedback to each layer can be visualized as an entire new hidden layer from a typical feed forward network. The solution proposed for this is the use of gating mechanisms which lets the network to decide when to forget a particular input sequence and based on relative redundancy. 4.2 LSTM Unit\nLong Short Term Memory Units, or LSTMs help in protecting the integrity of input sequences which is a problem because of vanishing gradients which basically carries prior states forward if there is no interference from other state units. Now, the information which is to be selectively chosen from the input states which are to be sent forward are selected by the gates. Since the writes are incremental to the previous states The equations for the gates used in the LSTM are: i t = \u03c3(W ix x t + W ir r t\u22121 + W ic c t\u22121 + b i ) \u2026.(1) f t = \u03c3(W fx x t + W fr r t\u22121 + W fc c t\u22121 + b f ) \u2026.(2) c t = f t .c t\u22121  + i t . tanh(W cx x t + W cr r t\u22121 + b c ) \u2026.(3) o t = \u03c3(W ox x t + W or r t\u22121 + W oc c t + b o ) \u2026.(4) h t = \u03c6(W yr r t + b y ) \u2026.(6) In addition to a hidden unit ht, the LSTM includes an input gate it, forget gate ft, output gate ot and memory cell ct. The memory cell unit ct is a sum of two terms: the previous memory cell unit ct\u22121 which is modulated by ft a function of the current input and previous hidden state, modulated by the input gate it. Because it and ft are sigmoidal, their values lie within the range [0, 1]. Roughly, these input modulations can be imagined as functions selectively choosing which past input sequences to retain in the memory and which to discard. Similarly, the output gate ot learns how much of the memory cell to transfer to the hidden state. These\nadditional cells seem to enable the LSTM to learn complex and long-term relationships in time, which complements them well with the RNNs for speaker recognition tasks. b for each unit is the bias attached to that unit. What the LSTM [8] unit does is, taking the input layer at a particular instance of time and the output from the previous instance, the input to each cell is multiplied by the activation of the input gate. Similarly with the output being multiplied by the activation of the output gate and previous cell values multiplied by the activation of the forget gate.Using this a relationship between temporally prior and present audio sequences can be established."}, {"heading": "4.3 Convolutional Neural Network Layer", "text": "Convolutional Neural Networks (CNNs) exhibit extraordinary performance on a variety of machine learning tasks. There is some work, in the form of a framework, for analyzing the operations that they perform. The goal of this project is to present key results from this theory, and provide intuition for why CNNs work. The CNN filters, which look for specific patterns in an image in its typical use is hypothesized to work in a similar manner - extracting patterns of sound from each input audio sequence. Similarly, each filter has the same bias and weights which help discover a frequency pattern which are then fed to a rectified linear unit and a max pooling layer for dimensionality reduction. The complex patterns hence established are then fed to a fully connected layer for classification. Since backpropagation is used in training CNNs, the vanishing gradient problem may still be prevalent. Hence, we propose the Recurrent Convolutional Layer which uses gating mechanisms to the output of the convolutional layer.\n4.4 Recurrent Convolutional Layer\nOur hypothesis suggests a Long-Term Convolutional Recurrent Neural Network (LRCNN) Model which uses a Convolutional Layer as a feature extractor within each time frame of the audio features that is further processed to learn to recognize the temporal relationship between each sequential time frame data. Figure 1 depicts the core abstract of our model. For using the model, first we pre-process the audio data to create a 2D vector which retains the data of each of the audio clip as a frequency\u2013time assortment. This vector is used as the input to the model which is passed as the audio features xt through a feature transformation with a CNN to produce a fixed length vector representation using a transformation \u03c6V (.) with parameters V. This transformation\u2019s result is passed as the input to the Recurrent Neural Network layer which will be able to learn the sequential patterns. The Recurrent Neural network outputs as a \u201cstate\u201d that is passed to the next time step. Thus an RNN cell accepts a prior state and an (optional) current input and produces a current state and an (optional) current output. LCRNN are proven to be powerful models in perceptual challenges [10],[11],[12], especially visual feature pattern recognition, because of the their strategic implementation of the non-linear functions. In case of a CNN used for such a problem, if the model has to recognize relationship between N number of features, it would be required to be a N layer deep network. While in case of LRCNN however, since the weights of the sequence model are used at every time step, the model is obligated to descend towards the solution between time\nsteps, thus the result is that the parameter size in case of LCRNN does not grow much larger than the maximum sequence length. Our model has Sequential input and a static output (Figure 6): <x1,x2,\u2026.,xt> \u2192 y. Such a model can be used for language recognition from audios of arbitrary length T as input, and an objective of classifying the language(Spanish or English) from a fixed set of languages.\nIn the hypothesis, the frequency features transformation \u03c6 equates to an activation in one of the filter of the CNN. Since this transformation doesn\u2019t have any temporal pattern whatsoever, it is time-invariant and thus provides improvement over traditional CNN models as this doesn\u2019t require very deep layers and thus can be easily parallelized, thus increasing the overall efficiency of the entire model which is mainly dependent upon the processing power required to train a single batch of data while at the same time providing a sequential pattern parameters."}, {"heading": "5. CONCLUSION", "text": "In this paper, we devised a novel approach towards automatic speaker recognition problems, specifically Language Identification, which in comparison with other approaches such as i-Vector classification and unidirectional recurrent neural networks should prove to be more accurate due to the increased information extraction from the log scale features using convolutional recurrent networks. With the pre-existing studies on bidirectional networks, we state that using an LSTM unit in the forward direction and a GRU unit in the backward direction using lesser features than traditional classification approaches, our model, theoretically, should have higher accuracy and time-efficiency over existing models. Such a convolutional recurrent neural network hence can be said to be better at classification where the input data is in the raw audio form.\nThe approach presented will either improve or ease the use of the classic methods while maintaining their reliability. Please bear in mind that it is not our intention to state that the classic techniques are failing, but we simply wish to demonstrate that the same results may be reached using less computation, and if our approach is used where less taxing log based acoustic features are used, the same results may be achieved with even less computation."}], "references": [{"title": "Automatic language identification using long short-term memory recurrent neural networks.\" \u200bINTERSPEECH  \u200b", "author": ["Gonzalez-Dominguez", "Javier"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling.\" \u200barXiv preprint arXiv:1412.3555  \u200b", "author": ["Chung", "Junyoung"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Bidirectional LSTM networks for improved phoneme classification and recognition.\"\u200bInternational Conference on Artificial Neural Networks  \u200b", "author": ["Graves", "Alex", "Santiago Fern\u00e1ndez", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Language recognition in ivectors space.\" \u200bProceedings of Interspeech, Firenze, Italy  \u200b", "author": ["Mart\u0131nez", "David"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Front-end factor analysis for speaker verification.\" \u200bIEEE Transactions on Audio, Speech, and Language Processing  \u200b", "author": ["Dehak", "Najim"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Language Identification in Short Utterances Using Long Short-Term Memory (LSTM) Recurrent Neural Networks.\" \u200bPloS one  \u200b", "author": ["Zazo", "Ruben"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Recurrent Convolutional Neural Networks for Scene Labeling.\" \u200bICML  \u200b", "author": ["Pinheiro", "Pedro HO", "Ronan Collobert"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "LSTM with Working Memory.\" \u200barXiv preprint arXiv:1605.01988  \u200b", "author": ["Pulver", "Andrew", "Siwei Lyu"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions.\" \u200bInternational Journal of Uncertainty, Fuzziness and Knowledge-Based Systems  \u200b", "author": ["Hochreiter", "Sepp"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, 2012", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CVPR, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In particular, guided by the advances on speaker veri\uf001cation and recognition, the use of i-vector extractors for extracting features from the sample and using classification techniques such as Bayesian classification has become the norm in acoustic LID systems [1].", "startOffset": 261, "endOffset": 264}, {"referenceID": 6, "context": "One such case has been the less frequent use of Convolutional Recurrent Neural Networks (CRNN)[7] which are computationally heavy due to multiple complex layers and each layer having to compute the parameters multiple times.", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "\u200bThe study by Schmidhuber, Graves and Fernandez shows this gulf in the performance [3].", "startOffset": 83, "endOffset": 86}, {"referenceID": 3, "context": "This approach is based on the approach described in [4].", "startOffset": 52, "endOffset": 55}, {"referenceID": 4, "context": "This approach is based on the Gaussian components, which are trained on PLP coefficients like MFCC and \u2206(delta) of MFCC and \u2206\u2206(double-delta) MFCC which are extracted over a specific frame rate of 10ms over 25ms window length and following the standard protocol described in [5] we can extract the i-vector of each languages.", "startOffset": 274, "endOffset": 277}, {"referenceID": 5, "context": "The approach presented by [6] is an improvement over the traditional i-vector analysis of the LID system since it tries to find the temporal hidden patterns between each window frames using the recurrent hidden layers.", "startOffset": 26, "endOffset": 29}, {"referenceID": 6, "context": "Traditionally, Convolutional recurrent neural network are used for scene labelling[7], that is, using Convolutional Neural Networks with Intra-layer Recurrent Connections, we use this novel approach for finding the relation between the frames of audio sample in the training dataset.", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "The advantage of doing is that, theoretically, it can provide the solution to the problem in the approach described by [6].", "startOffset": 119, "endOffset": 122}, {"referenceID": 8, "context": "Although a temporal relationship is established, the vanishing gradient problem, mentioned by [9] is even more prevalent in an RNN, because each feedback to each layer can be visualized as an entire new hidden layer from a typical feed forward network.", "startOffset": 94, "endOffset": 97}, {"referenceID": 0, "context": "Because i\u200bt and f\u200bt are sigmoidal, their values lie within the range [0, 1].", "startOffset": 69, "endOffset": 75}, {"referenceID": 7, "context": "What the LSTM [8] unit does is, taking the input layer at a particular instance of time and the output from the previous instance, the input to each cell is multiplied by the activation of the input gate.", "startOffset": 14, "endOffset": 17}, {"referenceID": 9, "context": "LCRNN are proven to be powerful models in perceptual challenges [10],[11],[12], especially visual feature pattern recognition, because of the their strategic implementation of the non-linear functions.", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "LCRNN are proven to be powerful models in perceptual challenges [10],[11],[12], especially visual feature pattern recognition, because of the their strategic implementation of the non-linear functions.", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "LCRNN are proven to be powerful models in perceptual challenges [10],[11],[12], especially visual feature pattern recognition, because of the their strategic implementation of the non-linear functions.", "startOffset": 74, "endOffset": 78}], "year": 0, "abstractText": "Language Identification, being an important aspect of Automatic Speaker Recognition has had many changes and new approaches to ameliorate performance over the last decade. We compare the performance of using audio spectrum in the log scale and using Polyphonic sound sequences from raw audio samples to train the neural network and to classify speech as either English or Spanish. To achieve this, we use the novel approach of using a Convolutional Recurrent Neural Network using Long Short Term Memory (LSTM) or a Gated Recurrent Unit (GRU) for forward propagation of the neural network. Our hypothesis is that the performance of using polyphonic sound sequence as features and both LSTM and GRU as the gating mechanisms for the neural network outperform the traditional MFCC features using a unidirectional Deep Neural Network.", "creator": null}}}