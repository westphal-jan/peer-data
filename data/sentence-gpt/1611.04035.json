{"id": "1611.04035", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2016", "title": "Entropic Causal Inference", "abstract": "We consider the problem of identifying the causal direction between two discrete random variables using observational data. Unlike previous work, we keep the most general functional model but make an assumption on the unobserved exogenous variable: Inspired by Occam's razor, we assume that the exogenous variable is simple in the true causal direction. We quantify simplicity using R\\'enyi entropy. For example, we use an exponential function to identify the effect of an unobserved variable on a pair of variables by using a linear function. In the linear function, this would be a linear function, whereas a linear function would be a linear function. This is a well-known, more important concept for investigating the causal direction in a nonlinear model. We found that by taking a linear function from the same random variable as the model, it produces a smooth linear function. This result is consistent with an experimental model that does not predict the relationship between the observed and unobserved variables. We conclude that the unobserved variables of the experimental model may be independent from the observed variables of the experiment (H.O.B. et al. 2011).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Sat, 12 Nov 2016 18:56:34 GMT  (42kb)", "https://arxiv.org/abs/1611.04035v1", "To appear in AAAI 2017"], ["v2", "Tue, 15 Nov 2016 03:09:53 GMT  (42kb)", "http://arxiv.org/abs/1611.04035v2", "To appear in AAAI 2017"]], "COMMENTS": "To appear in AAAI 2017", "reviews": [], "SUBJECTS": "cs.AI cs.IT math.IT stat.ML", "authors": ["murat kocaoglu", "alexandros g dimakis", "sriram vishwanath", "babak hassibi"], "accepted": true, "id": "1611.04035"}, "pdf": {"name": "1611.04035.pdf", "metadata": {"source": "CRF", "title": "Entropic Causal Inference", "authors": ["Murat Kocaoglu", "Alexandros G. Dimakis", "Sriram Vishwanath", "Babak Hassibi"], "emails": ["mkocaoglu@utexas.edu", "dimakis@austin.utexas.edu", "sriram@ece.utexas.edu", "hassibi@systems.caltech.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n04 03\n5v 2\n[ cs\n.A I]\n1 5"}, {"heading": "1 Introduction", "text": "Causality has been studied under several frameworks including potential outcomes [22] and structural equation modeling [19]. Under the Pearlian framework [19] it is possible to discover some causal directions between variables using only observational data with conditional independence tests. The PC algorithm [27] and its variants fully characterize which causal directions can be learned in the general case. For large graphs, GES algorithm [3] provides a score-based test to greedily identify the highest scoring causal graph given the data. Unfortunately, these approaches do not guarantee the recovery of true causal direction between every pair of variables, since typically data could be generated by several statistically equivalent causal graphs.\nA general solution to the causal inference problem is to conduct experiments, also called interventions. An intervention forces the value of a variable without affecting the other system variables. This removes the effect of its causes, effectively creating a new causal graph. These changes in the causal graph create a post-interventional distribution among variables, which can be used to identify\nsome additional causal relations in the original graph. The procedure can be applied repeatedly to fully identify any causal graph [8], [9], [11], [25].\nUnfortunately, for many problems, it can be very difficult to create interventions since they require additional experiments after the original data collection. Researchers would still like to discover causal relations between variables using only observational data, using so-called data-driven causality. Several recent works [2, 24] have developed such methods. To be able to make any conclusions on causal directions in this case, additional assumptions must be made about the mechanisms that generate the data.\nIn this paper we focus on the simplest causal discovery problem that involves only two variables. The two causal graphs X \u2192 Y and X \u2190 Y are statistically indistinguishable so conditional independence tests cannot make any causal inference from observational data without interventions. Statistical indistinguishability easily follows from the fact that any joint distribution on two variables p(x, y) can be factorized both as p(x)p(y/x) and p(y)p(x/y).\nThe most popular assumption for two-variable data-driven causality is the additive noise model (ANM) [26]. In ANM, any outside factor is assumed to affect the effect variable additively, which leads to the equation Y = f(X)+E,E \u22a5\u22a5 X. Although restrictive, this assumption leads to strong theoretical guarantees in terms of identifiability, and provides the state of the art accuracy in real datasets. [26] showed that if f is linear and the noise is non-Gaussian the causal direction is identifiable. [10] showed that when f is non-linear, irrespective of the noise, identifiability holds in a non-adverserial setting of system parameters. [20] extended ANM to discrete variables.\nAnother approach is to exploit the postulate that the cause and mechanism are in general independently assigned by nature. The notion of independence here is vague and one needs to assign maps, or conditional distributions to random variables to argue about independence of cause and mechanism. In this direction an information-geometry based approach is suggested [12]. Independence of cause and mechanism is captured by treating the log-slope of the function as a random variable, and assuming that it is independent from the cause. In the case of a deterministic relation Y = f(X), there are theoretical guarantees on identifiability. However, this assumption is restrictive for real data.\nPrevious work exploited these two ideas, additive noise, and independence of cause and mechanism, to draw data-driven causal conclusions about problems in a diverse range of areas from astronomy to neuroscience [24], [23]. [24] uses the same idea that the cause and effect are independent in the time series of a linear filter. They suggest the spectral independence criterion, which is robust to time shifts. [2] uses kernel space embeddings with the assumption that the cause distribution p(x) and mechanism p(y|x) are selected independently to distinguish cause from effect.\nAs noted by [2], although conceptually proposed before, using Kolmogorov complexity of the factorization of the joint distribution p(y|x)p(x) and p(x|y)p(y) as a criterion for deciding causal direction has not been used successfully until now.\nThe use of information theory as a tool for causal discovery is currently gaining increasing attention. This is through different appoaches, e.g., for time-series data, Granger causality and Directed Information can be used [7, 5, 21], see also [14]. However, researchers have not used entropy as a measure of simplicity in the causal discovery literature, probably because the entropies H(Y |X) and H(X|Y ) do not give us any more information than H(X) and H(Y ), due to the symmetry H(Y ) + H(X|Y ) = H(X) + H(Y |X). In our work, as we will explain, we minimize H(E) which initially sounds similar, but is fundamentally different from H(Y |X). Entropy has found some additional uses in the causality literature recently: In [6], authors use maximum mutual information between X,Y in order to quantify the causal strength of a known causal graph.\nThe work that is most similar to ours in spirit is [16], which also drops the additive noise assumption. Their approach and setup are different in many ways: Authors work with continuous\ndata. To be able to handle this generic form, they have to make strong assumptions on the exogenous variable, function, and distribution of the cause: [16] assume that the exogenous variable is a standard Gaussian, a Gaussian mixture prior for the cause, and a Gaussian process as the prior of the function."}, {"heading": "1.1 Our contributions", "text": "In this paper, we propose a novel approach to the causal identifiability problem for discrete variables. Similar to [16], we keep the most general functional model, but only put an assumption on the exogenous (background) variable. Based on Occam\u2019s razor, we employ a simplicity assumption on the unobserved exogenous variable. We use R\u00e9nyi entropy, which is defined as Ha(X) = 1 1\u2212a log ( \u2211 i p a i ), for a random variable X with state probabilities pi. We focus on two special cases of R\u00e9nyi entropy: H0, which corresponds to the logarithm of the number of states, and H1 which corresponds to Shannon entropy, but our framework can be extended.\nSpecifically, if the true causal direction is X \u2192 Y , then the random variable Y is an arbitrary function of X and an exogenous variable E: Y = f(X,E) where E is independent from the cause X. Our key assumption is that the exogenous variable E is simple, i.e., has low R\u00e9nyi entropy. The postulate is that for any model in the wrong direction X = f \u2032(Y, E\u0303), the exogenous variable E\u0303 has high R\u00e9nyi entropy. We are able to prove this result for the H0 special case of R\u00e9nyi entropy, assuming generic distributions for X,Y . Furthermore, we empirically show that using H1 Shannon entropy we obtain practical causality tests that work with high probability in synthetic datasets and that slightly outperforms the previous state of the art in real datasets.\nOur assumption is an entropic interpretation of Occam\u2019s razor, motivated by what E represents in the causal model. The exogenous variable captures the combined effect of all the variables not included in the system model, which affect the distribution of Y . Our causal assumption can be stated as \u201cthere should not be too much complexity not included in the causal model\". For a \u2192 1, i.e., Shannon entropy, H(X)+H(E), H(Y )+H(E\u0303) are the number of random bits required to generate an input for the causal system X \u2192 Y and X \u2190 Y , respectively. The simplest explanation of an observed joint distribution, i.e., the direction which requires nature to generate smaller number of random bits is selected as the true causal model. More precisely we have the following:\nAssumption 1. Entropy of the exogenous variable E is small in the true causal direction.\nThe notions of simplicity that we consider are H0, which is log-cardinality, and H1, which is Shannon entropy. One significant advantage of using Shannon entropy as a simplicity metric is that it can be estimated more robustly in the presence of measurement errors, unlike cardinality H0.\nWe prove an identifiability result for H0 entropy, i.e., cardinality of E: If the probability values are not adversarially chosen, for most functions, the true causal direction is identifiable under Assumption 1. Based on experimental evidence, we conjecture that a similar identifiability result must hold for Shannon entropy H1.\nTo use our framework we need algorithms that explain a dataset by finding an exogenous variable E with minimum cardinality H0 and minimum Shannon entropy H1. Since the entropies of X and Y can be very different, any metric to determine the true causal direction cannot only consider the entropy of the exogenous variable without incorporating the entropy of the cause. We explain the exogenous variable in both directions and declare the causal direction to be the one with the smallest joint entropy Ha(X) +Ha(E) versus Ha(Y ) +Ha(E\u0303). Our method can be applied for any R\u00e9nyi entropy Ha but in this paper we only use a = 0 and a = 1.\nUnfortunately, minimizing H0(E) seems very hard for real datasets since it offers no noise robustness. For Shannon entropy we can do much better for real data. The first step in obtaining\na practical algorithm is showing that the minimum H1 explanation is equivalent to the following problem: For n random variables with given marginal distributions, find a joint distribution with the minimum Shannon entropy that is consistent with the given marginals. This problem is called the minimum Shannon entropy coupling and is known to be NP hard [13]. We propose a greedy approximation algorithm for this problem that empirically performs very well. We also prove that, for n = 2, our algorithm always produces a local minimum.\nIn summary our contributions in this paper include:\n\u2022 We show identifiability for generic low-entropy causal models under Assumption 1 with H0. \u2022 We show that the problems of identifying the minimum cardinality (H0) exogenous variable,\nand identifying the minimum Shannon entropy (H1) exogenous variable given a joint distribution are both NP hard. \u2022 We design a novel greedy algorithm for the minimum entropy coupling problem, which turns out to be equivalent to the problem of finding exogenous variable with minimum H1 entropy. \u2022 We empirically validate the conjecture that the causal direction is identifiable under Assumption 1 with H1, using experiments on synthetic datasets. \u2022 We empirically show that our causal inference algorithm based on Shannon entropy minimization has slightly better performance than the existing best algorithms on a real causal dataset. Interestingly, our algorithm uses only the probability distributions rather than the actual values of the random variables, and hence is applicable to categorical variables."}, {"heading": "1.2 Background and Notation", "text": "A tupleM = (X,U,F ,D, p) is a causal model when, 1) F = {fi} are deterministic functions, 2)X = {Xi} are a set of endogenous (observed) variables U = {Ui} are a set of exogenous (latent) variables with Xi = fi(Pai, Ui),\u2200i where Pai are the endogenous parents and Ui is the exogenous parent of Xi in directed acyclic graph D, 3) U are mutually independent with respect to p. The observable variable set X has a joint distribution implied by the distributions of U , and the functional relations fi. D is then a Bayesian network for the induced joint distribution of endogenous variables. A standard assumption employed in Pearl\u2019s model causal sufficiency is also used here: Every exogenous variable is a direct parent of at most one endogenous variable.\nIn this paper, we consider a simple two variable causal system which contains only two endogenous variables X,Y . Assume X causes Y , which is represented as X \u2192 Y . The model is determined only by one exogenous variable E, and a function f , where Y = f(X,E). The probability distribution of X and E, and f determines the distribution of Y . This model is shown by the tuple M = ({X,Y }, E, f,X \u2192 Y, pX,E). Notice that we do not assign an exogenous variable to X, since it is the source node in the graph.\nWe denote the set {1, 2, \u00b7 \u00b7 \u00b7 , n} by [n]. \u2211\ni xi is meant to run through every possible index. log refers to the logarithm base 2. For two variables X,Y , Y|X and X|Y denote the conditional probability distribution matrices, i.e., Y|X(i, j) = p(y = i|x = j) and X|Y(i, j) = p(x = i|y = j). The statistical independence of two random variables X and E are shown by X \u22a5\u22a5 E. For notational convenience, probability distribution of random variable X is shown by p(x) as well as pX(x). x shows the distribution of X in vector form , i.e., xi = x(i) = P(X = i). n\u2212 1 simplex is the set of points x in n dimensional Euclidean space that satisfy \u2211\ni x(i) = 1. card is the cardinality of a set."}, {"heading": "2 Causal Model with Minimum Cardinality Exogenous Variable", "text": "Consider the causal model M = ({X,Y }, E0, f0,X \u2192 Y, pX,E). The task is to identify the underlying causal graph X \u2192 Y using independent identically distributed samples {(xi, yi)}i. Assuming causal sufficiency, this task reduces to deciding whether X causes Y or Y causes X. To isolate the identifiability problem from estimation errors due to finite samples, we assume that the joint distribution of (X,Y ) is available. Most proofs are deferred to the Appendix.\nOne way to identify that X causes Y is by showing that although there exists a function f and random variable E with Y = f(X,E),X \u22a5\u22a5 E, there is no function, random variable pair (g, E\u0303) such that X = g(Y, E\u0303), Y \u22a5\u22a5 E\u0303. However, without more assumptions, this is not possible: For any joint distribution one can find valid causal models for both X \u2192 Y,X \u2190 Y . This is widely known, although for completeness, we provide a proof (Lemma 4 in the Appendix).\nEven when the true causal graph is known, one can create different constructions of f,E with Y = f(X,E),X \u22a5\u22a5 E. There is no way to distinguish the true causal model. However, even though we cannot recover the actual function and the exogenous variable, we can still show identifiability.\nFirst, we give an equivalent characterization of a causal model on two variables.\nDefinition 1 (Block Partition Matrices). Consider a matrix M \u2208 {0, 1}n 2\u00d7m. Let mi,j represent the i+ (j \u2212 1)n th row of M. Let Si,j = {k \u2208 [m] : mi,j(k) 6= 0}. M is called a block partition matrix if it belongs to C := {M : M \u2208 {0, 1}n 2\u00d7m, \u22c3\ni\u2208[n] Si,j = [m], Si,j \u2229 Sl,j = \u2205,\u2200i 6= l}.\nC thus stands for 0, 1matrices with n2 rows andm columns where each block of n rows correspond to a partitioning of the set [m]. We make the following key observation:\nLemma 1. Given discrete random variables X,Y with distribution p(x, y), \u2203 a causal model M = ({X,Y }, E, f,X \u2192 Y, pX,E), E \u2208 E with card(E) = m if and only if \u2203M \u2208 C, e \u2208 R m + with \u2211\ni e(i) = 1 that satisfy vec(Y|X) = Me.\nIn other words, the existence of a causal pair X \u2192 Y is equivalent to the existence of a block partition matrix M and a vector e of proper dimensions with vec(Y|X) = Me.\nFor simplicity, assume |X | = |Y| = n. We later remove this constraint. We first show that any joint distribution can be explained using a variable E with n(n\u2212 1) + 1 states.\nLemma 2 (Upper Bound on Minimum Cardinality of E). Let X \u2208 X , Y \u2208 Y be two random variables with joint probability distribution pX,Y (x, y), where |X | = |Y| = n. Then \u2203 a causal model Y = f(X,E),X \u22a5\u22a5 E that induces pX,Y , where E has support size n(n\u2212 1) + 1.\nWe can show that, if the columns of Y|X are uniformly sampled points in the n\u22121 dimensional simplex, then n(n \u2212 1) states are also necessary for E (see Proposition 3 in the Appendix). This shows, unless designed by nature through the causal mechanism, exogenous variable cannot have small cardinality. Based on this observation, the hope is to prove that in the wrong causal direction, say X \u2192 Y and we find an E\u0303 \u22a5\u22a5 Y such that X = g(Y, E\u0303) for some g, the exogenous variable E\u0303 has to have large cardinality. In the next section, we show this is actually through, under mild conditions on f ."}, {"heading": "2.1 Identifiability for H0 entropy", "text": "In a causal system Y = f(X,E), nature chooses the random variables X,E, and function f , and the conditional probability distributions are then determined by these. We are interested in the cardinality of variables E\u0303 \u22a5\u22a5 X in the wrong causal direction X = g(Y, E\u0303). Considering X|Y, we\ncan show that the same lower bound of n(n \u2212 1) still holds despite nature now chooses E and X randomly, rather than choosing the columns of X|Y directly. A mild assumption on f is needed to avoid degenerate cases (For counterexamples see the appendix).\nDefinition 2 (Generic Function). Let Y = f(X,E) where variables X,Y,E have supports X ,Y, E, respectively. Let Sy,x = f \u22121 x (y) \u2282 E be the inverse map for x, e, i.e., Sy,x = {e \u2208 E : y = f(x, e)}. A function f is called \u201cgeneric\", if for each (x1, x2, y) triple f \u22121 x1\n(y) 6= f\u22121x2 (y) and for every (x, y) pair f\u22121x (y) 6= \u2205.\nIn other words f is called generic if yth row in the xth1 block of matrix M in the decomposition vec(Y|X) = Me is different from yth row in the xth2 block, and both are nonzero. This is not a restrictive condition, for example if p(y|x) are all different, no two rows of M can be the same. For any given conditional distribution, if the probabilities are perturbed by arbitrarily small continuous noise, the corresponding f will be generic almost surely. We have the following main identifiability result:\nTheorem 1 (Identifiability). Consider the causal model M = ({X,Y }, E0, f0,X \u2192 Y, pX,E0) where the random variables X,Y have n states, E0 \u22a5\u22a5 X has \u03b8 states and f is a generic function .\nIf the distributions of X and E are uniformly randomly selected from the n\u22121 and \u03b8\u22121 simplices, then with probability 1, any E\u0303 \u22a5\u22a5 Y that satisfies X = g(Y, E\u0303) for some deterministic function g has cardinality at least n(n\u2212 1).\nTheorem 1 implies that the causal direction is identifiable, when the exogenous variable has cardinality < n(n\u2212 1):\nCorollary 1. Assume that there exists an algorithm A that given n random variables {Zi}, i \u2208 [n] with distributions {pi}, i \u2208 [n] each with n states, outputs the distribution of the random variable E with minimum cardinality and functions {fi, i \u2208 [n]} where Zi = fi(E).\nConsider the causal pair X \u2192 Y where Y = f(X,E0). Assume that the cardinality of E0 is less than n(n\u2212 1), and f is generic. Then, A can be used to identify the true causal direction with probability 1, if X,E0 are selected uniformly randomly from the proper dimensional simplices.\nProof. Feed the set of conditional distributions {P(Y |X = i) : i \u2208 [n]} and {P(X|Y = i) : i \u2208 [n]} to A to obtain E, E\u0303. From Theorem 1, with probability 1, A identifies E\u0303 with card(E\u0303) \u2265 n(n\u2212 1). Then since card(E) \u2264 card(E0) < card(E\u0303), comparing cardinalities give the true direction.\nCorollary 1 gives an algorithm for finding the true causal direction: Estimate E, E\u0303 with minimum H0 entropy and declare X \u2192 Y if |E\u0303| > |E| and declare X \u2190 Y if |E\u0303| < |E|. The result easily extends to the case where X and Y are allowed to have different number of states:\nProposition 1 (Inference algorithm). Suppose X \u2192 Y . Let X \u2208 X , Y \u2208 Y, |X | = n, |Y| = m. Assume that A is the algorithm that finds the exogenous variables E, E\u0303 with minimum cardinality. Then, if the underlying exogenous variable E0 satisfies |E0| < n(m\u2212 1), with probability 1, we have |X|+ |E| < |Y |+ |E\u0303|.\nProof follows from Corollary 1, and by extending the proof of Theorem 1 to different cardinalities for X, Y .\nUnfortunately, it turns out there does not exist an efficient algorithm A, unless P=NP:\nTheorem 2. Given a conditional distribution matrix Y|X, identifying E \u22a5\u22a5 X with minimum support size such that there exist a function f with Y = f(X,E) is NP hard.\nThe hardness of this problem sets us to search for alternative approaches."}, {"heading": "3 Causal Model with Minimum H1 Entropy", "text": "In this section, we propose a way to identify the causal model that explains the observational data with minimum Shannon entropy ( entropy in short ). Entropy of a causal model is measured by the number of random bits required to generate its input. In the causal graph X \u2192 Y , where Y = f(X,E), we identify the exogenous variable E \u22a5\u22a5 X with minimum entropy. We show that this corresponds to a known problem which has been shown to be NP hard. Later we propose a greedy algorithm.\nNotice that H(E) is different from the conditional entropy H(Y |X). Certainly, since Y = f(X,E), H(Y |X) \u2264 H(E). The key is that since E is forced to be independent from X, H(E) cannot be lowered to H(Y |X). To see this, we can write H(Y |X) = \u2211\ni pX(i)H(Y |X = i), whereas since conditional probability distribution of Y |X = i is the same as the distribution of fi(E) for some function fi, we have H(E) \u2265 maxi H(Y |X = i)."}, {"heading": "3.1 Finding E with minimum entropy", "text": "Consider the equation Y = f(X,E),X \u22a5\u22a5 E. Let fx : E \u2192 Y be the function mapping E to Y when X = x, i.e., fx(E) := f(x,E). Then P(Y = y|X = x) = P(fx(E) = y|X = x) = P(fx(E) = y). The last equality follows from the fact that X \u22a5\u22a5 E. Thus, we can treat the conditional distributions P(Y |X = x) as distributions that emerge by applying some function fx to some unobserved variable E. Then the problem of identifying E with minimum entropy given the joint distribution p(x, y) becomes equivalent to, given distributions of the variables fi(E), finding the distribution with minimum entropy (distribution of E), such that there exists functions fi which map this distribution to the observed distributions of Y |X = i. It can be shown that H(E) \u2265 H(f1(E), f2(E), . . . , fn(E)). Regarding fi(E) as a random variable Ui, the best lower bound on H(E) can be obtained by minimizing H(U1, U2, . . . , Un). We can show that we can always construct an E that acheives this minimum. Thus the problem of finding the exogenous variable E with minimum entropy given the joint distribution p(x, y) is equivalent to the problem of finding the minimum entropy joint distribution of the random variables Ui = (Y |X = i), given the marginal distributions p(Y |X = i):\nTheorem 3 (Minimum Entropy Causal Model). Assume that there exists an algorithm A that given n random variables {Zi}, i \u2208 [n] with distributions {pi}, i \u2208 [n] each with n states, outputs the joint distribution over Zi consistent with the given marginals, with minimum entropy.\nThen, A can be used to find the causal model M = ({X,Y }, E,X \u2192 Y, pX,E) with minimum input entropy, given any joint distribution pX,Y .\nThe problem of minimizing entropy subject to marginal constraints is non-convex. In fact, it is shown in [13] that minimizing the joint entropy of a set of variables given their marginals is NP hard. Thus we have the following corollary:\nCorollary 2. Finding the causal model M = ({X,Y }, E, f,X \u2192 Y, pE,X) with minimum H(E) that induce a given distribution p(x, y) is NP hard.\nFor this, we propose a greedy algorithm. Using entropy to identify E instead of cardinality, despite both turning out to be NP hard, is useful since entropy is more robust to noise in data. In real data, we estimate the probability values from samples, and noise is unavoidable."}, {"heading": "3.2 A Conjecture on Identifiability with H1 Entropy", "text": "We have the following conjecture, supported by artificial and real data experiments in Section 4.\nConjecture 1. Consider the causal model M = ({X,Y }, E, f,X \u2192 Y, pX,E) where discrete random variables X,Y have n states, E \u22a5\u22a5 X has \u03b8 states."}, {"heading": "If the distribution of X is uniformly randomly selected from the n\u2212 1 dimensional simplex and", "text": "distribution of E is uniformly selected from the probability distributions that satisfy H1(E) \u2264 log n+ O(1) and f is randomly selected from all functions f : [n]\u00d7 [\u03b8] \u2192 [n], then with high probability, any E\u0303 \u22a5\u22a5 Y that satisfies X = g(Y, E\u0303) for some deterministic g entails H(X) +H(E) < H(Y ) +H(E\u0303).\nProposition 2 (Assuming Conjecture 1). Assume there exists an algorithm A that given n random variables {Zi}, i \u2208 [n] with distributions {pi}, i \u2208 [n] each with n states, outputs the distribution of the random variable E with minimum entropy and functions {fi}, i \u2208 [n] where Zi = fi(E).\nConsider the causal pair X \u2192 Y where Y = f(X,E0), and cardinality of E0 is cn for some constant c, and f is selected randomly. Then, A can be used to identify the true causal direction with high probability, if X,E0 are uniformly random samples from the proper dimensional simplices."}, {"heading": "3.3 Greedy Entropy Minimization Algorithm", "text": "Given m discrete random variables with n states, we provide a heuristic algorithm to minimize their joint entropy given their marginal distributions. The main idea is the following: Each marginal probability constraint must be satisfied. For example, for the case of two variables with distributions p1, p2, ith row of joint distribution matrix should sum to p1(i). The contribution of a probability mass to the joint entropy only increases when probability mass is divided into smaller chunks: \u2212p1(i) log p1(i) \u2264 \u2212a log a \u2212 b log b, when p1(i) = a + b, for a, b \u2265 0. Thus, we try to keep large probability masses intact to assure that their contribution to the joint distribution is minimized.\nWe propose Algorithm 1. The sorting step is only to simplify the presentation. Hence, although the given algorithm runs in time O(m2n2 log n), it can easily be reduced to O(max(mn log n,m2n)) by dropping the sorting step. The algorithm simply proceeds by removing the most probability mass it can at each round. This makes sure the large probability masses remain intact.\nOne can easily construct the joint distribution using a variant: Instead of sorting, at each\nAlgorithm 1 Joint Entropy Minimization Algorithm\n1: Input: Marginal distributions of m variables each with n states, in matrix form M = [pT 1 ; pT 2 ; ..., pT m ]. 2: e = [ ] 3: Sort each row of M in decreasing order. 4: Find minimum of maximum of each row: r \u2190 mini(pi(1)) 5: while r > 0 do 6: e \u2190 [e, r] 7: Update maximum of each row: pi(1) \u2190 pi(1)\u2212 r, \u2200i 8: Sort each row of M in decreasing order. 9: r \u2190 mini(pi(1))\n10: end while 11: return e.\nstep, find r = mini{maxj{pi(j)}} and assign r to the element with coordinates (ai), where ai = argmaxj pi(j).\nLemma 3. Greedy entropy minimization outputs a point with entropy at most logm+ log n.\nLemma 3 follows from the fact that the algorithm returns a support of size at most m(n\u22121)+1.\nWe also prove that, when there are two variables with n dimensions, the algorithm returns a point that satisfies the KKT conditions of the optimization problem, which implies that it is a local optimum (see Proposition 4 in the Appendix)."}, {"heading": "4 Experiments", "text": "In this section, we test the performance of our algorithms on real and artificial data. First, we test the greedy entropy minimization algorithm and show that it performs close to the trivial lower bound. Then, we test our conjecture of identifiability using entropy. Lastly, we test our entropyminimization based causal identification technique on real data.\nIn order to test our algorithms, we sample points in proper dimensional simplices, which correspond to distributions for X and E. Distribution of points are uniform for selecting the distribution of X. It is well-known that a vector [xi/Z]i is uniformly randomly distributed over the simplex, if xi are i.i.d. exponential random variables with parameter 1, and Z = \u2211\ni xi [18]. To sample lowentropy distributions for E, instead of exponential, we use a heavy tailed distribution for sampling each coordinate. Specifically, we use [ei/Z]i, where ei are i.i.d. log-normal random variables with parameter \u03c3. We observe that this allows us to sample a variety of distributions with small entropy.\nPerformance of Greedy Entropy Minimization: We sample distributions for n random variables {Xi}, i \u2208 [n] each with n states and apply Algorithm 1 to minimize their joint entropy. We compare our greedy joint entropy minimization algorithm with the simple lower bound of maxiH(Xi). Figure 1a shows average, maximum and minimum excess bits relative to this lower bound. Contrary to the pessimistic bound of log n bits, joint entropy is at most 1 bit away from maxi H(Xi) for the given range of n.\nVerifying Entropy-Based Identifiability Conjecture: In this section, we empirically verify Conjecture 1. The distributions for X are uniformly randomly sampled from the simplex in n dimensions. We also select f randomly (see implementation details). For the log-normal parameter \u03c3 used for sampling the distribution of E from the n(n \u2212 1) dimensional simplex, we sweep the integer values from 2 to 8. This allows us to get distribution samples from different regimes. We only consider the samples which satisfy H(E) \u2264 log n.\nAfter sampling E,X, f , we identify the corresponding Y|X and X|Y for Y = f(X,E). We apply greedy entropy minimization on the columns of the induced distributions Y|X,X|Y to get the estimates E, E\u0303 for both causal models Y = f(X,E) and X = g(Y, E\u0303), respectively. Figure 1b shows the variation of success probability, i.e., the fraction of samples which satisfy H(X) + H(E) < H(Y ) +H(E\u0303). As observed, as n is increased, probability of success converges to 1, when H(E) \u2264 log n, which supports the conjecture.\nExperiments on Real Cause Effect Pairs: We test our entropy-based causal inference algorithm on the CauseEffectPairs repository [15]. ANM have been reported to achieve an accuracy of 63% with a confidence interval of \u00b110% [17]. We also use the binomial confidence intervals as in [4].\nThe cause effect pairs show very different characteristics. From the scatter plots, one can observe that they can be a mix of continuous and discrete variables. The challenge in applying our framework on this dataset is choosing the correct quantization. Small number of quantization levels may result in loss of information regarding the joint distribution, and a very large number of states might be computationally hard to work with. We pick the same number of states for both X and Y , and use a uniform quantization that assures each state of the variables has \u2265 10 samples on average. From the samples, we estimate the conditonal transition matrices Y|X and X|Y and feed the columns to the greedy entropy minimization algorithm (Algorithm 1), which outputs an approximate of the smallest entropy exogenous variable. Later we compare H(X,E) and H(Y, E\u0303) and declare the model with smallest input entropy to be the true model, based on Conjecture 1.\nFor a causal pair, we invoke the algorithm if |H(X,E) \u2212 H(Y, E\u0303)| \u2265 t log(n) for threshold parameter t, which determines the decision rate. Accuracy becomes unstable for very small decision rates, since the number of evaluated pairs becomes too small. At 100% decision rate, algorithm achieves 64.21% which is slightly better than the 63% performance of ANM as reported in [17]. In addition, our algorithm only uses probability values, and is applicable to categorical as well as ordinal variables."}, {"heading": "Acknowledgements", "text": "This research has been supported by NSF Grants CCF 1344179, 1344364, 1407278, 1422549 and ARO YIP W911NF-14-1-0258."}, {"heading": "5 Appendix", "text": ""}, {"heading": "5.1 Proof of Lemma 1", "text": "(\u21d2)Assume there exists a causal model M = ({X,Y }, E, f,X \u2192 Y, pX,E). Without loss of generality, assume E = [m] and pE = [e1, e2, . . . , em]. We have\np(y|x) = \u2211\ne\u2208E\np(y|x, e)p(e|x) = \u2211\ne\u2208E\np(y|x, e)p(e) (1)\nsince E \u22a5\u22a5 X. Define the matrix Y|Xk(i, j) := P(Y = i|X = j, E = k). Then, from (1), we can decompose the conditional probability distribution matrix Y|X as follows:\nY|X = \u2211\nk\u2208E\nekY|Xk. (2)\nSince f is deterministic, each value of X is mapped to exactly one value of Y , when E is conditioned on. Thus each column of Y|Xk has exactly a single 1 with remaining entries being zeros. Thus, each entry of Y|X is a subset sum of pE . Let Si,j represent this subset, i.e., p(y = i|x = j) = \u2211\nk\u2208Si,j ek. Notice that the xth column of Y|X is the conditional distribution P(Y |X = x) =\nP(f(x,E)|X = x) = P(f(x,E)) = P(fx(E)), where fx(E) := f(x,E). Since fx is a deterministic function, each value in its domain maps to exactly one value in its range. This implies that Sy,x = f\u22121x (y) are disjoint for fixed x, i.e., Si,j 6= Sl,j\u2200l 6= i. Also, since each value of E must be mapped to a value of Y by fx, the union of Si,j over i must be the whole support [m].\nDefine mi,j to be the length m vector which is 1 in the columns indexed by Si,j. Construct M from the rows mi,j such that mi,j is the i + (j \u2212 1)nth row of M. By construction, M is a block partition matrix. Pick e = [e1, e2, . . . , em]. Then we have vec(Y|X) = Me.\n(\u21d0) For reverse direction, assume there exists matrices M, e with block partition M and \u2211\ni ei = 1, such that vec(Y|X) = Me. Define E to be the random variable independent from X, with probability distribution pE := e and support E = [m].\nLet wi be the ith column of M. Then we have vec(Y|X) = \u2211\ni eiwi. Now de-vectorize Y|X and wi to have\nY|X = m \u2211\ni=1\neiUi, (3)\nwhere wi = vec(Ui). Each column of Ui, comes from distinct size-n blocks of M and since M is block partition, each column of Ui contains a single 1 \u2200i. Thus each Ui represent a valid map fi : X \u2192 Y, where fi(x) is given by the nonzero row of xth column of Ui.\nA function f with two input variables X,E, where E \u2208 E = [m] is completely determined by the set of functions {f(X, 1), f(X, 2), ..., f(X,m)}. Let f(X, e) be the function described by the matrix Ue and f be the function determined by {f(X, e), e \u2208 E}. Apply the constructed f on X,E to get Z = f(X,E). Then we have P(Y = y|X = x) = P(Z = y|X = x)\u2200x, since both Z and Y induce the same conditional distribution Y|X.\nFor any set of realizations of (X,Y ) = {xi, yi}, we can construct a set of realizations of E, {ei} based on the conditional distribution P(E|Z = yi,X = xi). Then we have yi = f(xi, ei). Also, since P(Z = y,X = x) = P(Y = y,X = x) this process induces the same joint distributions between variables X,Y,E and X,Z,E, i.e., P(X = x,E = e, Z = y) = P(X = x,E = e, Y = y), and conditional independence statement implied by one holds for the other. Thus X \u22a5\u22a5 E."}, {"heading": "5.2 Unidentifiability without assumptions", "text": "Here we prove that we can fit causal models in both directions X \u2192 Y, Y \u2192 X given any joint distribution.\nLemma 4. Let X \u2208 X , Y \u2208 Y be discrete random variables with an arbitrary joint distribution p(x, y), where |X | = m, |Y| = n. Then there exists two causal models M1 = ({X,Y }, E, f,X \u2192 Y, pX,E) and M2 = ({X,Y }, E\u0303, g,X \u2190 Y, pY,E\u0303) with E \u22a5\u22a5 X and E\u0303 \u22a5\u22a5 Y that induce the same joint distribution p(x, y).\nWe will prove by construction. Consider the conditional probability transition matrix Y|X. Without loss of generality, assume X = [m],Y = [n]. From Lemma 1, it is sufficient to show that there exists M \u2208 C and e such that vec(Y|X) = Me.\nFor now, assume that each entry of Y|X is a rational number. Scale the fractional form of each term in Y|X so that each denominator becomes the same as the least common multiple of denominators. Denote this least common multiple by \u03bb. Let \u01eb be 1/\u03bb.\nLet G0 = Y|X and apply the following procedure for i from 1 to \u03bb in order to construct {Fi, i \u2208 [\u03bb]}: Set Gi+1 to Gi \u2212 \u01ebFi, where Fi is the n\u00d7m, {0, 1} matrix containing only a single 1 in the largest entry of every column of current Gi. This procedure is called to be successful if Gi is set to all zero matrix for i = \u03bb.\nThe above procedure iteratively removes 1 from the numerator of the fractional form of one probability value per column (of Y|X). Since each column sums to 1, numerators of each column sum to \u03bb. Thus the procedure is successful. Thus we have,\nY|X = \u03bb \u2211\nk=1\n\u01ebFk. (4)\nLet e = [\u01eb, \u01eb, ..., \u01eb] be a length-\u03bb vector. Each entry of Y|X is a subset sum of e. Also, since Fi contains a single 1 per column by construction, every subset of e is disjoint within a column. Thus, we can construct M \u2208 C such that vec(Y|X) = Me.\nIf the entries are not fractional, we can still find small enough \u01eb to complete the above procedure.\nThe same process can be implemented with X|Y to obtain E\u0303, g such that X = Y (g, E\u0303)."}, {"heading": "5.3 Proof of Lemma 2", "text": "We prove by construction for any given joint probability distribution. Consider the decomposition described in the proof of Lemma 4. Assume without loss of generality that X = Y = [n]. Now, instead of taking out \u03bb = 1/\u01eb, at step i, remove minimum of the maximum probability values at each column of the current Gi matrix from the maximum probability locations. Thus, at each iteration i, at least one entry of the matrix Gi is zeroed out. Since sum of the values in each column remains the same after each iteration, after at most n(n\u2212 1) steps, each column must have the same single nonzero value. Thus the algorithm finalizes in n(n\u2212 1) + 1 steps.\nNotice that this algorithm is the same as the entropy minimization algorithm we propose in Algorithm 1. For a more detailed explanation of the algorithm steps, see Algorithm 1.\nIn the case when the matrix has zero entries, it can be shown that one can always find a decomposition with nnz \u2212 1 terms, where nnz is the number of non-zero elements in the matrix."}, {"heading": "5.4 Proposition 3", "text": "Proposition 3. Let the columns of Y|X be n points independently sampled from the uniform distribution over the n \u2212 1 simplex. Then, with probability 1, \u2204(M, e) with vec(Y|X) = Me for M \u2208 {0, 1}n 2\u00d7m, when m < n(n\u2212 1).\nProof. We use the following technique to generate uniformly randomly sampled points on the simplex in n dimensions [18]:\nLemma 5. Let xi \u223c U [0, 1] for i \u2208 [n \u2212 1] be i.i.d random variables, ordered such that xi \u2265 xj for i > j. Let x0 = 0 and xn = 1. Then u = [ui]i\u2208[n] where ui = xi \u2212 xi\u22121,\u2200i \u2208 [n], is a random vector uniformly distributed over n\u2212 1 simplex.\nFirst, we construct vec(Y|X) directly using n(n\u2212 1) uniform i.i.d. random variables: Consider x\u0303i for i \u2208 [n\n2] where each consecutive block {x\u0303jn+1, x\u0303jn+2, . . . , x\u0303jn+n}, j \u2208 {0, 1, . . . , n \u2212 1} of size n is sampled from the generative model in Lemma 5 before reordering. Thus x\u0303i are i.i.d. with x\u0303i \u223c U [0, 1] for n(n\u2212 1) indexes by construction. Order x\u0303i within each block in ascending order to get xi in accordance with Lemma 5. Defining the vectors x = [xi] and x\u0303 = [x\u0303i], we have x = Px\u0303 for some permutation matrix P. From x, we can construct z\u0303 = vec(Y|X) using the same map used in Lemma 5 for each block (the map from ui from xi in Lemma 5). This construction is a linear map H where each submatrix of H is full rank.\nFor the sake of contradiction, let z\u0303 = M\u0303e be a decomposition where M\u0303 \u2208 {0, 1}n(n\u22121)\u00d7m where m < n(n\u22121). Ignoring the entries where z\u0303i = 1\u2212xi\u22121, we can relabel z\u0303 to get z with z \u2208 [0, 1]\nn(n\u22121). Correspondingly, we can write z = Me, where M is the submatrix of M\u0303 obtained by ignoring the corresponding rows.\nThe construction of z from x based on the above construction yields z = Wx for a full rank matrix W. Notice that any subset of rows are linearly independent due to specific structure of H.\nLet r be the rank of M. Clearly r < n(n \u2212 1). Then, some of the (at least n(n \u2212 1) \u2212 r) rows of M can be written as a unique linear combination of r linearly independent rows.\nConsider one such row m0, where m0 = \u2211r\ni=1 \u03b1imi and mi are a set of linearly independent rows of M. Define a = [\u22121, \u03b11, \u03b12, ...\u03b1r ]\nT . Take r + 1 rows of W corresponding to the selected zi\u2019s to form Wr. Then we have, a TWrx = a TWrPx\u0303 = 0. Recall that any subset of rows of W is full rank, and P is a permutation matrix. Hence, left nullspace of WrP is empty, implying that x\u0303 has to be orthogonal to the vector aTWrP 6= 0. This is a probability 0 event for any nonzero a, since each x\u0303i is independently sampled from a continuous distribution. Probability that all such constraints are satisfied is zero since it is less than the probability that one particular constraints is satisfied. This argument holds for any fixed M. Since there are finitely many such {0, 1} matrices, probability that there exists such an M is 0.\nThus, unless M has rank at least n(n\u2212 1), there does not exist a decomposition Me = z with probability 1."}, {"heading": "5.5 Proof of Theorem 1", "text": "For sampling from the simplex, we use the following model:\nLemma 6 ([18]). Let xi for i \u2208 [n] be independent, exponential random variables with mean 1. Then the random vector [\nx1\u2211 i xi , x2\u2211 i xi , ..., xn\u2211 i xi\n]\nis uniformly distributed over the n\u2212 1 simplex.\nAssume above generative model for sampling the distributions of X and E: Let {xi, i \u2208 [n]} and {ei, i \u2208 [\u03b8]} be sets of independent identically distributed exponential random variables with mean 1. Assign P(X = i) = xi\u2211\nj xj , and P(E = k) = ek\u2211 j ej .\nLet p = vec(Y|X). Then, as shown in Section 2.1, we can write\np = Me (5)\nNotice that jth block of n rows of p give the conditional probability distribution of Y given X = j. Let Si,j represent the set of indices of e that contribute to the probability of observing Y = i given X = j, i.e., pi,j =\n1\u2211 j ej\n\u2211\nk\u2208Si,j\nek. Thus ith row in jth block, or equivalently i+(j\u2212 1)nth row\nof M is 1 in the indices Si,j.\nThe fact that f is generic implies each row of M is distinct and non-empty.\nFor the sake of contradiction, assume that there exists E\u0303 \u22a5\u22a5 Y with cardinality m < n(n \u2212 1), with some deterministic function g such that X = g(Y, E\u0303) induces the same joint distribution p(x, y). By Lemma 1, this implies that there exists M\u0303 \u2208 {0, 1}n 2\u00d7m end e\u0303 such that\nq = M\u0303e\u0303, (6)\nwhere q = vec(X|Y) and e\u0303 is the probability distribution vector of E\u0303. Let qi,j = P(X = i|Y = j). Then from Bayes\u2019 rule, we have\nqi,j = pj,ixi \u2211\ni pj,ixi =\nxi \u2211\nk\u2208Sj,i ek\n\u2211 i xi \u2211 k\u2208Sj,i ek\n. (7)\nNotice that the denominators \u2211 j xj and \u2211\nj ej in each term disappear due to cancellation of numerator and denominator.\nThus we have\nq =\n[\np11x1 \u2211\nk p1kxk ,\np12x2 \u2211\nk p1kxk , \u00b7 \u00b7 \u00b7 , (8)\np1nxn \u2211\nk p1kxk ,\np21x1 \u2211\nk p2kxk , \u00b7 \u00b7 \u00b7\npnnxn \u2211\nk pnkxk\n]T\n(9)\nFrom (6), drop the rows of q that contain xn in the numerator as well as the corresponding rows of M\u0303. The new linear system becomes:\nq\u0304 = M\u0304e\u0303, (10)\nwhere q\u0304 and M\u0304 are the desribed submatrices of q and M\u0303 respectively.\nM\u0304 has n(n\u2212 1) rows and m columns. We have\nrank(M\u0304) \u2264 m < n(n\u2212 1). (11)\nSince rank of M\u0304 is less than n(n\u2212 1), the rows of M\u0304 are linearly dependent. This implies there is at least one set of coefficients {\u03b1i} not identically zero that satisfies \u03b1M\u0304 = 0, where\n\u03b1 = [ \u03b11,1, \u03b11,2, \u00b7 \u00b7 \u00b7 , \u03b11,n\u22121, \u03b12,1, \u00b7 \u00b7 \u00b7 , \u03b1n,n\u22121 ] . (12)\nThen,\n\u03b1q\u0304 = \u03b1M\u0304e\u0303 = 0 (13)\nHence, the elements of q\u0304 should satisfy the linear equation. Then this linear equation in terms of qi,j can be written as:\nn \u2211\ni=1\n\u2211n\u22121 j=1 \u03b1i,jpi,jxj \u2211n\nj=1 pi,jxj =\n\u2211n\u22121 j=1 \u03b11,jp1,jxj \u2211n\nj=1 p1,jxj +\n\u2211n\u22121 j=1 \u03b12,jp2,jxj \u2211n\nj=1 p2,jxj\n+ \u2211n\u22121 j=1 \u03b13,jp3,jxj \u2211n\nj=1 p3,jxj + . . .+\n\u2211n\u22121 j=1 \u03b1n,jpn,jxj \u2211n\nj=1 pn,jxj = 0 (14)\nSlightly abusing the notation, relabel pi,j as pi,j = \u2211\nk\u2208Si,j ek, since\n\u2211\nk ek terms cancel in the\nexpression above. We know from Section 2.1 that Si,j \u2229 Sk,j = \u2205 for k 6= i. Additionally, due to the assumption that f is generic, we have Si,j 6= Si,k.\nTo prove contradiction, in the following we show that for any given non-zero \u03b1, this equation is non-zero with probability 1.\nTo show this, we show that after equating the denominators, each term brings a unique monomial. Hence, the result is a polynomial where each \u03b1i,j is accompanied with at least one unique monomial. Thus, the polynomial cannot be identically zero, and the probability of choosing a root of this polynomial is zero.\nFor now, assume that the denominator is finite. Later, we will show denominator is almost surely finite to complete the argument.\nLemma 7. If xi and ei are i.i.d. exponential random variables with mean 1, and the function f is generic, (14) holds with probability 0, i.e., P((14) holds ) = 0.\nProof. Multiply each term with the denominators of others to get a single fraction. Then, with a finite denominator, numerator should be zero for equation to hold. Define c1 to be the coefficient of xn\u22121n x1 in the numerator after equating the denominators. Since xn only appears due to terms from the denominator, we have\nc1 = \u03b11,1(p1,1p2,np3,n . . . pn,n)\n+ \u03b12,1(p1,np2,1p3,n . . . pn,n)\n+ \u00b7 \u00b7 \u00b7+ \u03b1n,1(p1,np2,np3,n . . . pn,1) (15)\nSince S1,1 6= S1,n due to f being generic, we have, (a) Either \u2203ei \u2208 S1,1, ei /\u2208 S1,n (b) Or \u2203ei /\u2208 S1,1, ei \u2208 S1,n\nWithout loss of generality, assume some ei1 \u2208 Si,1, after a potential relabeling. This is possible since Si,1 are disjoint for different i and non-zero. (Then, as we will see e\n2 i1\nonly appears together with \u03b1i,1 in (15)). Similarly, assume some ein \u2208 Si,n.\nConsider the mulitiplier of coefficient \u03b1i,1. Assume case (a) holds for Si,1, i.e., \u2203ei \u2208 Si,1, ei /\u2208 Si,n. Then, \u03b1i,1 is accompanied with term e 2 i since ei \u2208 Sj,n for some j 6= i. Also, it is easy to see that no other term contains e2i since Si,1 does not appear again and no Sj,1, j 6= i contains ei. Assume case (b) holds for the multiplier of \u03b1i,1, i.e., \u2203ein \u2208 Si,n, ein /\u2208 Si,1. Then every term except \u03b1i,1 is accompanied by either ein or ein 2, since pi,n appears in every other term.\nAbove argument implies that every term is different from the rest. This implies that every distinct \u03b1i,j is accompanied by a different monomial in the form x n\u22121 n xj \u220f k\u2208Ti,j ek, for some Ti,j \u2282 [\u03b8], where Ti,j are distinct for different i. Thus, since at least one \u03b1i,j is nonzero the resulting polynomial in the numerator is not identically zero. Then the numerator is a non-zero polynomial of the terms {x1, x2, . . . , xn, e1, e2, . . . , en}. We know that the roots of a non-zero polynomial defined over a compact domain has Lebesque measure zero [1]. Hence probability of numerator being zero is 0.\nThen we have,\nP((14) = 0) = P( Numerator of (14) = 0\nOR Denominator of (14) = \u221e)\n\u2264 P( Numerator of (14) = 0)\n+ P( Denominator of (14) = \u221e).\nP( Numerator of (14) = 0) is shown to be zero by the argument above. We need to argue that the denominator cannot be infinity.\nFor this, denote denominator random variable to be \u03be. We can write\nP(\u03be < \u221e) = 1\u2212 P(lim sup n\u2192\u221e \u03b5n), (16)\nwhere \u03b5n is the event that {\u03be \u2265 tn} for a sequence of tn such that limn\u2192\u221e tn = \u221e. Pick tn = n 2.\nSince \u03be is nonnegative, we can apply Markov inequality to get\nP(\u03b5n) \u2264 E[\u03be]\ntn . (17)\nClearly, E[\u03be] < \u221e. Since \u2211 n P(\u03b5n) = E[\u03be] \u2211 n 1 tn < \u221e, applying Borel-Cantelli lemma, we have\nP(lim sup n\u2192\u221e \u03b5n) = 0, (18)\nwhich implies P(\u03be < \u221e) = 1. Thus, we have\nP((14) = 0) \u2264 0 \u21d2 P((14) = 0) = 0. (19)\nNow we can prove the main theorem: Assume for the sake of contradiction that there exists a pair (g, E\u0303) that satisfy X = g(Y, E\u0303), E\u0303 \u22a5\n\u22a5 Y . By Lemma 1, this is equivalent to the statement that there exists pair (M\u0303, e) that satisfy (6). Define events \u03b51(M\u0303k, e\u0303) = { Event that q = M\u0303ke\u0303 } and \u03b52(M\u0303k) = { Event that \u03b1kq = 0 }, Mk is a fixed {0, 1}n\n2\u00d7m matrix and \u03b1k is one set of coefficients imposed by linear dependence of rows of M\u0303k. Notice that here q is a random vector, determined by {xi}, i \u2208 [n] and {ei}, i \u2208 [\u03b8]. Clearly, \u03b51 implies \u03b52, thus P(\u03b51) \u2264 P(\u03b52). Now we can write:\nP(\u2203(g, E\u0303) such that X = g(Y,E))\n= P(\u2203(M\u0303, e\u0303) such that q = M\u0303e\u0303) (20)\n= P(\u2203(M\u0303, e\u0303) such that \u03b51 is true) (21)\n\u2264 P(\u2203(M\u0303, e\u0303) such that \u03b52 is true) (22)\n= P(\u2203(M\u0303) such that \u03b52 is true) (23) \u2264 \u2211\nk\nP( Given M\u0303k, \u03b1kq = 0) (24)\n= \u2211\nk\nP(\u03b1kq = 0) = 0. (25)\n(20) follows from the fact that both representations are equivalent by Lemma 1. (22) is due to the fact that if \u03b51, then \u03b52. (23) is due to the fact that \u03b52 does not depend on e\u0303, but only on M\u0303. (24) follows from union bound over all matrices M\u0303k. The last equation follows from Lemma 7 and the fact that there are finitely many M\u0303k \u2208 {0, 1} n2\u00d7m with m < n(n\u2212 1) columns.\n5.6 A counterexample when Si,j = Si,k\nFollowing counterexample shows that without the additional assumption, identifiability result in Theorem 1 does not hold.\nConsider the following equation:\n\n             p1,1 p2,1 p3,1 p1,2 p2,2 p3,2 p1,3 p2,3 p3,3\n\n             =\n\n             1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1\n\n             \u00d7   e1 e2 e3   (26)\nIn the reverse direction, we can fit the following system, which has smaller cardinality for the exogenous variable:\n\n             q1,1 q2,1 q3,1 q1,2 q2,2 q3,2 q1,3 q2,3 q3,3\n\n             =\n\n             1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1\n\n             \u00d7   x1 x1+x2+x3 x2 x1+x2+x3 x3 x1+x2+x3   (27)\nNotice that ei terms completely disappear in this symmetric case. Thus, exogenous variable with n states is sufficient to describe the reverse conditional probability distribution matrix, independent from the cardinality of exogenous variable in the true direction, i.e., the value of \u03b8.\nOur main theorem suggests, under the condition that no Si,j is the exact subset of {e1, e2, ...}, no such case can arise, and the reverse direction requires exogenous variable with at least n(n\u2212 1) states.\n5.7 A counterexample when pi,j \u2265 0\nThe critical component of the proof was that each linear equation implied by the rank deficiency of the system had unique non-zero coefficients. Here, we provide a counterexample to the theorem when this condition is violated.\nConsider the following system:\n\n   p1,1 p2,1 p1,2 p2,2\n\n   =\n\n   1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1\n\n   \u00d7\n\n   e1 e2 e3 e4\n\n  \n(28)\nIn the reverse direction, we can fit the following system, which has smaller cardinality for the exogenous variable.\n    q1,1 q2,1 q1,2 q2,2     =     1 0 0 0 0 1 1 1     \u00d7 ( x1 x1+x2(e1+e2) x2(e1+e2) x1+x2(e1+e2) )\n(29)\nNotice that this is true independent of the selection of ei, hence the theorem cannot be extended to case where pi,j = 0 is allowed."}, {"heading": "5.8 Proof of Theorem 2", "text": "We first define decomposition problem, and show it is NP hard.\nDefinition 3. Decomposition problem: For a given nonnegative matrix M , with column sums equal to 1, consider the decomposition \u2211\nx\u2208X xFx, where Fx are 0,1 matrices with single 1 per column, and \u2211\nx\u2208X x = 1 with x \u2265 0. Identify the decomposition that minimizes card(X ).\nLemma 8. The decomposition problem is NP hard.\nProof. We use subset sum problem for the reduction:\nDefinition 4. Subset sum problem: For a given set of integers V , and an integer a, decide whether there exists a subset S of V such that \u2211\nu\u2208S u = a.\nSubset sum is a well known NP complete problem. Consider any instance of the subset sum problem with the set V = {u1, u2, ..., um} and an integer a. Assume without loss of generality ui 6= 0,\u2200i \u2208 V . If not, one can work with the set of nonzero values in V . Construct them by 2matrix M with M(i, 1) = ui and M(1, 2) = a,M(2, 2) = \u2212a+ \u2211\ni\u2208[m] ui,M(i, 2) = 0,\u2200i \u2208 {2, 3, ...,m}. Update M by dividing each element by \u2211\ni\u2208V ui. This does not change the answer to the subset sum problem. Now column sum of M is 1 for both columns. It can be shown that the decomposition of Lemma 2 can always be applied here to get a decomposition with |X | = m+ 1 (number of nonzero terms \u22121). We also know that any decomposition need to touch each nonzero element in each column at least once. Hence the column with largest number of nonzero elements yields the lower bound |X \u2217| \u2265 m.\nWe show that optimal decomposition size is m if and only if there is a subset of V that sums to a.\nFirst, assume \u2203S \u2282 V such that \u2211\nu\u2208S u = a. Consider the following decomposition: Let xk = uk for k \u2208 [m]. Let Fk(k, 1) = 1 be the only nonzero element in the first column. Pick the nonzero element in the second column of Fk based on the membership of uk in S: Let Fk(1, 2) = 1 if uk \u2208 S and Fk(2, 2) = 1 if uk \u2208 V \\S. This decomposition is optimal due to lower bound.\nNow we show that if optimum has size m, then \u2203 a subset sum with value a: Recall that each Fk has a single 1 per column and decomposition has m terms. Since first column of M has m non-zero terms, each term in the decomposition must be equal to the elements of this column, which is the elements of the given set. Since Fk are 0,1 matrices with single 1 per column by construction, every element of M must be a subset sum of the decomposition terms. Hence, a is a subset sum of set elements.\nThis shows that a subset sum exists if and only if optimal decomposition has size m. Thus, if we could find the optimal decomposition size in all instances of the decomposition problem, we would solve all instances of the subset sum problem.\nNow, we give a definition related to decomposability:\nDefinition 5. \u03b1\u2212decomposability: A matrix M is called \u03b1\u2212decomposable if it can be written as a convex combination of \u03b1 {0, 1} matrices, each with a single 1 per column.\nIdentifying the exogenous variable with minimum cardinality is equivalent to finding minimum size e with M \u2208 C such that vec(Y|X) = Me from Lemma 1. Notice that matrix Y|X is \u03b1\u2212decomposable if and only if there exists e of size \u03b1 along with M \u2208 C such that vec(Y|X) = Me.\nConsider any decomposition problem. If we had an algorithm that could solve all instances of the problem of identifying E with minimum support, we could feed the normalized matrix from the decomposition problem as Y|X to this algorithm and solve the decomposition problem."}, {"heading": "5.9 Proof of Theorem 3", "text": "Consider m random variables {U1, U2, . . . , Um} each with n states. Let {p1, p2, . . . , pm} stand for the marginal probability distributions of each random variable. Consider the following optimization problem:\nH\u2217(U1, U2, ..., Um) = min p(u1,u2,...,um) H(U1, U2, ..., Um)\ns. t. p(ui) = pi,\u2200i (30)\nIn words, optimization problem finds the joint distribution of m variables with minimum entropy, subject to marginal distribution constraints for every variable. Notice that this problem is nonconvex: It minimizes a concave function with respect to linear constraints.\nLet X,E be discrete, independent random variables where X \u2208 [m] and E \u2208 [t]. Recall that every conditional distribution P(Y |X = x) can be written as the distribution of a function of random variable E. Let us investigate the underlying probability space, in order to generate the probability space to optimize the joint distribution over.\nWe can consider the product probability space\nP = (\u2126 = \u2126X \u00d7\u2126E = [m]\u00d7 [t] , F = 2 \u2126 , p) (31)\nfor random variables X : \u2126X \u2192 [n], E : \u2126E \u2192 [t] with X(\u03c9) = \u03c9, Y (\u03c9) = \u03c9. Then for \u03c9 = (\u03c9x, \u03c9e) \u2208 \u2126, p(\u03c9) = pX(\u03c9x)pE(\u03c9e).\nConsider the equation Y = f(X,E). Let fx : \u2126E \u2192 \u2126Y be the function mapping E to Y when X = x, i.e., fx(E) := f(x,E). Then\nP(Y = y|X = x) = P(fx(E) = y|X = x) (32)\n= P(fx(E) = y), (33)\nwhere last equality follows from the fact that X \u22a5\u22a5 E. Let sigma algebra generated by the random variable f(x,E) be F\u03c9x . Formally, the sigma algebras generated by f(x,E) are subsigma algebras of disjoint, but identical sigma algebras. In other words, even though F\u03c9x \u2286 2\n(\u03c9x,\u2126E) are disjoint for different \u03c9x, (\u03c9x,\u2126E) are identical for every \u03c9x \u2208 \u2126X . Thus the sigma algebras generated by f(x,E) = gx(E) can be thought of as subsigma algebras of the same sigma algebra, i.e., the one generated by E. In other words, we can equivalently construct F\u03c9x \u2286 2\n\u2126E . This construction allows us to talk about the joint probability distribution of the random variables {fx(E) : x \u2208 X}.\nLet Ui = fi(E). Then we have,\nH(E) = H(E|U1, U2, ..., Um) +H(U1, U2, ..., Um) (34)\n\u2212H(U1, U2, ..., Um|E) (35)\n\u2265 H(U1, U2, ..., Um). (36)\nInequality follows from the fact that H(U1, U2, ..., Um|E) = 0 and H(E|U1, U2, ..., Um) \u2265 0.\nH(E) \u2265 H(U1, U2, ..., Um). Thus, the best lower bound on the entropy of exogenous variable E can be obtained by solving the optimization problem below.\nH(E) \u2265 min p(u1,u2,...,um) H(U1, U2, ..., Um)\nsubject to p(ui) = pi, i = 1, . . . ,m. (37)\nSince we are picking E with minimum possible entropy without restricting (not observing) the functions fx, we can actually construct an E that achieves this minimum: Let the optimal joint distribution be p\u2217(u1, u2, ..., um). We can construct E with n\nm states with state probabilities equal to p\u2217(u) for each configuration of u. This E has the same entropy as the joint entropy, since probability values are the same. Thus,\nH(E\u2217) = min p(u1,u2,...,um) H(U1, U2, ..., Um)\nsubject to p(ui) = pi\nThe functions fi, where Ui has the same distribution as fi(E), can be constructed from the distribution of E and Ui. This determines the function f , hence the causal model M that induces the conditional distribution Y|X. Note that E \u22a5\u22a5 X by construction. (For a complete argument on how one can always generate E \u22a5\u22a5 X based on a set of samples of X,Y , see the proof of Lemma 1)."}, {"heading": "5.10 Proof of Corollary 2", "text": "Assume there exists a black box that could find the causal model with minimum entropy exogenous variable E. Consider an arbitrary instance of the problem of minimizing the joint entropy of a set of variables {U1, U2, . . . , Un} subject to marginal constraints. Construct matrix M = [p1, p2, . . . , pn], where pi is the distribution of variable Ui as a column vector. Feed M into this black box as a hypothetical conditional distribution Y|X. From the proof of Theorem 3, H(E) output by this black box gives the minimum entropy joint distribution of the variables Ui. Hence, finding the causal model with minimum entropy would give the solution to this NP hard problem."}, {"heading": "5.11 Proof of Proposition 2", "text": "Given a joint distribution p(x, y), consider the following algorithm: In stage 1, feed the set of conditional distributions {P(Y |X = i) : i \u2208 [n]} to algorithm A to obtain E with minimum entropy. Algorithm outputs a variable E along with {f1, f2, . . . , fn} such that the distribution of fj(E) is the same as the conditional distribution of Y given X = j,\u2200j. This set of fj determine f where Y = f(X,E), E \u22a5\u22a5 X. Since algorithm optimizes entropy of E, H(E) \u2264 H(E0). In stage 2, feed the conditional distributions {P(X|Y = i) : i \u2208 [n]} to algorithm A to obtain E\u0303. From the conjecture any E\u0303 satisfies H(X) + H(E0) < H(Y ) + H(E\u0303). Since H(E) \u2264 H(E0), we have H(X) +H(E) < H(Y ) +H(E\u0303), which can be used for identifiability."}, {"heading": "5.12 Greedy Entropy Minimization Outputs a Local Optimum", "text": "Proposition 4. For two variables, Algorithm 1 always returns a local optimum.\nConsider random variables U, V with marginal distributions pu, pv. We first show that the KKT conditions on the problem (30) imply that the optimal solution is quasi-orthogonal :\np\u2217(u, v) = M = U(x, y)\u03b4x,y, (38)\nwhere U is rank 1 and \u03b4u,v is an indicator for the support of optimal joint distribution. We call such M as masked submatrix of a rank 1 matrix.\nLet (i, j)th entry of the joint distribution be xi,j . We have n 2 variables {xi,j, i \u2208 [n], j \u2208 [n]} to\noptimize over. In a general optimization problem\nmin x f0(x)\ns. t. hi(x) = 0, i \u2208 [p]\nfi(x) \u2264 0, i \u2208 [m],\nLagrangian becomes\nL(x, \u03bb, v) = f0(x) +\nm \u2211\ni=1\n\u03bbifi(x) +\np \u2211\ni=1\nvihi(x), (39)\nwhich gives the KKT conditions\nfi(x \u2217) \u2264 0, i \u2208 [m] hi(x \u2217) = 0, i \u2208 [p] \u03bb\u2217i \u2265 0, i \u2208 [m] \u03bb\u2217i fi(x \u2217) = 0, i \u2208 [m] \u2207L(x\u2217, \u03bb\u2217, v\u2217) = 0\nThis implies, for fixed i, either fi(x \u2217) = 0 or \u03bb\u2217i = 0. The optimization problem we have is\nmin xi,j\n\u2211\ni,j\n\u2212xi,j log xi,j\ns. t. \u2211\nj\nxi,j = pu(i),\u2200i, \u2211\ni\nxi,j = pv(j),\u2200j\nxi,j \u2265 0,\u2200i, j.\nSubstituting corresponding fi, hi, we get the following conclusion: At optimal point xi,j, either\nxi,j = 0, or 1 \u2212 log xi,j + v (1) i + v (2) j = 0. This implies that xi,j = 2\n1+v (1) i +v (2) j . Hence the optimal\njoint satisfies p\u2217i,j = uivj for some vectors u, v, whenever p \u2217 i,j 6= 0.\nNext we show that such u, v can be constructed from any algorithm output:\nTheorem 4. For any matrix M output by Algorithm 1, there exists u, v where M is a masked submatrix of uvT .\nProof of Proposition 4. Consider the following variant of Algorithm 1 for two distributions p,q: Initialize an n \u00d7 n zero matrix M0. In every iteration, find m1 = maxi p(i),m2 = maxi q(i). Let a = argmaxi p(i) and b = argmaxi q(i). Assign r = min{m1,m2} to the (a, b)th entry of M. Update p(a) \u2190 p(a)\u2212 r,q(b) \u2190 q(b)\u2212 r. Repeat the process until p = 0,q = 0.\nThis variant constructs the joint distribution matrix rather than the distribution of E directly. We need the following definitions:\nDefinition 6. An ordered set of coordinates ((i1, j1), (i2, j2), . . . , (im, jm)) of a matrix M is called a path on matrix M, if either it+1 = it or jt+1 = jt,\u2200t \u2208 [m\u2212 1].\nDefinition 7. An ordered set of coordinates ((i1, j1), (i2, j2), . . . , (im, jm)) of a matrix M is called a cycle on matrix M, if either it+1 = it or jt+1 = jt,\u2200t \u2208 [m\u2212 1] and either i1 = im or j1 = jm.\nFirst we prove a structural characterization for matrix M.\nLemma 9. Any matrix M output by Algorithm 1 contains no cycles.\nProof. Consider a bipartite graph G with n left and n right vertices constructed as follows: G has an edge (i, j) if and only if M(i, j) is nonzero.\nIt is easy to see that the matrix M has no cycles if and only if the corresponding bipartite graph G has no cycles. We claim that, for an M output by Algorithm 1, corresponding G cannot have any cycles.\nConstruct the bipartite graph in the same order as matrix M is created by Algorithm 1. Notice that when the algorithm assigns a value to M(i, j), it satisfies either ith row constraint or jth column constraint. Then no other value can be assigned to that row or column. We call this zeroing out the corresponding row/column.\nIn the bipartite representation, say an edge (i, j) is added at time t. We call a vertex of the added edge closed, if the corresponding dimension (row or column) is zeroed out. Thus each added edge to the bipartite graph closes one of the endpoints of that edge. A closed vertex cannot participate to the formation of any other edges. This captures the fact that when zeroed out, a row/column cannot be assigned a non-zero value again by the algorithm.\nAssume at time t, a cycle is formed in the bipartite graph. Then at time t\u2212 1, there must be a path with a single edge missing. Let the edges of this path be labeled as {e1, e2, . . . em}. There is a time order in which these edges are constructed. Let ek be the first edge formed in this path. Then one of its endpoints must be closed. However it belongs to a path, which means an edge was attached to a closed vertex, which is a contradiction. Assume ek is the edge at the end of the path and the closed endpoint is also the endpoint of the path. However an edge is attached to this closed vertex at time t to form the cycle, which is a contradiction.\nConsider a matrix M output by the algorithm. We prove that M is a masked submatrix of uvT by construction: Let the first entry selected by Algorithm 1 have coordinates (i0, j0). Since algorithm zeroes out either the row or column, (i0, j0) is the only non-zero entry in either its column or row. Assume without loss of generality that selection of (i0, j0) by Algorithm 1 zeroes out row i0.\nInitialize by assigning ui0 = 1 and vj0 = M(i0, j0). Let Sj0 represent the non-zero row indices for column j0. Now assign the values of u(k) for k \u2208 Sj0 such that u(Sj0)vj0 = M(Sj0 , j0), where u(S) stands for the subvector containing entries with index in S.\nRepeat the above procedure by exhausting either a row or column at each time.\nLemma 10. The above construction never runs into an entry (i, j) for which both ui and vj were assigned before.\nProof. Assume otherwise. Then, due to the process of assigning the entries of u, v, there must be two paths from (i, j) to the starting point of (i0, j0): One path that follows column j, and one path that follows row i. In other words, there exists k, l \u2208 [n] such that there are two paths ((i, j), (k, j), . . . , (i0, j0)) and ((i, j), (i, l), . . . , (i0, j0)). Combining these paths, we get the cycle ((i, j), (k, j), . . . , (i0, j0), . . . , (i, l)), which is a contradicton.\nHence, algorithm selects every element in u and v at most once. Thus it produces a valid assignment for u, v."}, {"heading": "5.13 Implementation Details and Sampling Distributions with Diverse Entropy Values", "text": "We implemented our algorithms in MATLAB. In order to sample from a wide range of entropy values, when we need to sample a distribution from the n \u2212 1 dimensional simplex, we generate n independent identically distributed log Gaussian random variables with parameter \u03c3. For verifying the conjecture on artificial data, we generate the distributions for E swiping the \u03c3 parameter from 2 to 8, taking only the integer values. The distribution for X is uniformly randomly sampled over the simplex.\nFor the true causal direction, the function f is sampled as follows: We generate \u03b8 matrices Fi, where each has a single 1 per column, randomly selected out of all n rows. Each Fe represent the function fe(X) := f(X, e). Together with e, this determines the conditional probability distribution matrix Y|X.\nThe number of states for quantization is chosen as n = min{N/10, 512}, where N is the number of samples for that particular cause-effect pair. Hence, we pick n to assure each state has at least 10 samples on average. An upper bound of 512 is used to limit the computational complexity."}], "references": [{"title": "The zero set of a polynomial, 2005", "author": ["Richard Caron", "Tim Traynor"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Causal discovery via reproducing kernel hilbert space embeddings", "author": ["Zhitang Chen", "Kun Zhang", "Laiwan Chan", "Bernhard Sch\u00f6lkopf"], "venue": "Neural Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Optimal structure identification with greedy search", "author": ["David Maxwell Chickering"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "The use of confidence or fiducial limits illustrated in the case of the binomial", "author": ["C. Clopper", "E.S. Pearson"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1934}, {"title": "Discovering influence structure", "author": ["Jalal Etesami", "Negar Kiyavash"], "venue": "In IEEE ISIT,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Causal strength via shannon capacity: Axioms, estimators and applications", "author": ["Weihao Gao", "Sreeram Kannan", "Sewoong Oh", "Pramod Viswanath"], "venue": "In Proceedings of the 33 rd International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Investigating causal relations by econometric models and cross-spectral methods", "author": ["Clive W. Granger"], "venue": "Econometrica: Journal of the Econometric Society,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1969}, {"title": "Characterization and greedy learning of interventional markov equivalence classes of directed acyclic graphs", "author": ["Alain Hauser", "Peter B\u00fchlmann"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Two optimal strategies for active learning of causal networks from interventional data", "author": ["Alain Hauser", "Peter B\u00fchlmann"], "venue": "In Proceedings of Sixth European Workshop on Probabilistic Graphical Models,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Nonlinear causal discovery with additive noise models", "author": ["Patrik O Hoyer", "Dominik Janzing", "Joris Mooij", "Jonas Peters", "Bernhard Sch\u00f6lkopf"], "venue": "In Proceedings of NIPS", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Experiment selection for causal discovery", "author": ["Antti Hyttinen", "Frederick Eberhardt", "Patrik Hoyer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Information-geometric approach to inferring causal directions", "author": ["Dominik Janzing", "Joris Mooij", "Kun Zhang", "Jan Lemeire", "Jakob Zscheischler", "Povilas Daniu\u0161is", "Bastian Steudel", "Bernhard Sch\u00f6lkopf"], "venue": "Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "On the hardness of entropy minimization and related problems", "author": ["Mladen Kovacevic", "Ivan Stanojevic", "Vojin Senk"], "venue": "In IEEE Information Theory Workshop,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Cause effect pairs repository, 2016", "author": ["J.M. Mooij", "D. Janzing", "J. Zscheischler", "B. Sch\u00f6lkopf"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Probabilistic latent variable models for distinguishing between cause and effect", "author": ["Joris M. Mooij", "Oliver Stegle", "Dominik Janzing", "Kun Zhang", "Bernhard Sch\u00f6lkopf"], "venue": "In Proceedings of NIPS", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Distinguishing cause from effect using observational data: methods and benchmarks", "author": ["M. Joris Mooij", "Jonas Peters", "Dominik Janzing", "Jakob Zscheischler", "Bernhard Scholkopf"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Generating uniform random vectors over a simplex with implications to the volume of a certain polytope and to multivariate extremes", "author": ["Shmuel Onn", "Ishay Weissman"], "venue": "Annals of Operations Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Causality: Models, Reasoning and Inference", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Causal inference on discrete data using additive noise models", "author": ["Jonas Peters", "Dominik Janzing", "Bernhard Sch\u00f6lkopf"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Directed information graphs", "author": ["Christopher Quinn", "Negar Kiyavash", "Todd Coleman"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Estimating causal effects of treatments in randomized and nonrandomized studies", "author": ["Donald Rubin"], "venue": "Journal of Educational Psychology,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1974}, {"title": "Removing systematic errors for exoplanet search via latent causes", "author": ["Bernhard Sch\u00f6lkopf", "David W. Hogg", "Dun Wang", "Daniel Foreman-Mackey", "Dominik Janzing", "Carl- Johann Simon-Gabriel", "Jonas Peters"], "venue": "In Proceedings of the 32 nd International Conference on Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Telling cause from effect in deterministic linear dynamical systems", "author": ["Naji Shajarisales", "Dominik Janzing", "Bernhard Sch\u00f6lkopf", "Michel Besserve"], "venue": "In Proceedings of the 32 nd International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Learning causal graphs with small interventions", "author": ["Karthikeyan Shanmugam", "Murat Kocaoglu", "Alex Dimakis", "Sriram Vishwanath"], "venue": "In NIPS 2015,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "A linear non-gaussian acyclic model for causal discovery", "author": ["S Shimizu", "P. O Hoyer", "A Hyvarinen", "A. J Kerminen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Causation, Prediction, and Search", "author": ["Peter Spirtes", "Clark Glymour", "Richard Scheines"], "venue": "A Bradford Book,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}], "referenceMentions": [{"referenceID": 20, "context": "1 Introduction Causality has been studied under several frameworks including potential outcomes [22] and structural equation modeling [19].", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "1 Introduction Causality has been studied under several frameworks including potential outcomes [22] and structural equation modeling [19].", "startOffset": 134, "endOffset": 138}, {"referenceID": 17, "context": "Under the Pearlian framework [19] it is possible to discover some causal directions between variables using only observational data with conditional independence tests.", "startOffset": 29, "endOffset": 33}, {"referenceID": 25, "context": "The PC algorithm [27] and its variants fully characterize which causal directions can be learned in the general case.", "startOffset": 17, "endOffset": 21}, {"referenceID": 2, "context": "For large graphs, GES algorithm [3] provides a score-based test to greedily identify the highest scoring causal graph given the data.", "startOffset": 32, "endOffset": 35}, {"referenceID": 7, "context": "The procedure can be applied repeatedly to fully identify any causal graph [8], [9], [11], [25].", "startOffset": 75, "endOffset": 78}, {"referenceID": 8, "context": "The procedure can be applied repeatedly to fully identify any causal graph [8], [9], [11], [25].", "startOffset": 80, "endOffset": 83}, {"referenceID": 10, "context": "The procedure can be applied repeatedly to fully identify any causal graph [8], [9], [11], [25].", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "The procedure can be applied repeatedly to fully identify any causal graph [8], [9], [11], [25].", "startOffset": 91, "endOffset": 95}, {"referenceID": 1, "context": "Several recent works [2, 24] have developed such methods.", "startOffset": 21, "endOffset": 28}, {"referenceID": 22, "context": "Several recent works [2, 24] have developed such methods.", "startOffset": 21, "endOffset": 28}, {"referenceID": 24, "context": "The most popular assumption for two-variable data-driven causality is the additive noise model (ANM) [26].", "startOffset": 101, "endOffset": 105}, {"referenceID": 24, "context": "[26] showed that if f is linear and the noise is non-Gaussian the causal direction is identifiable.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] showed that when f is non-linear, irrespective of the noise, identifiability holds in a non-adverserial setting of system parameters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] extended ANM to discrete variables.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "In this direction an information-geometry based approach is suggested [12].", "startOffset": 70, "endOffset": 74}, {"referenceID": 22, "context": "Previous work exploited these two ideas, additive noise, and independence of cause and mechanism, to draw data-driven causal conclusions about problems in a diverse range of areas from astronomy to neuroscience [24], [23].", "startOffset": 211, "endOffset": 215}, {"referenceID": 21, "context": "Previous work exploited these two ideas, additive noise, and independence of cause and mechanism, to draw data-driven causal conclusions about problems in a diverse range of areas from astronomy to neuroscience [24], [23].", "startOffset": 217, "endOffset": 221}, {"referenceID": 22, "context": "[24] uses the same idea that the cause and effect are independent in the time series of a linear filter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] uses kernel space embeddings with the assumption that the cause distribution p(x) and mechanism p(y|x) are selected independently to distinguish cause from effect.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "As noted by [2], although conceptually proposed before, using Kolmogorov complexity of the factorization of the joint distribution p(y|x)p(x) and p(x|y)p(y) as a criterion for deciding causal direction has not been used successfully until now.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": ", for time-series data, Granger causality and Directed Information can be used [7, 5, 21], see also [14].", "startOffset": 79, "endOffset": 89}, {"referenceID": 4, "context": ", for time-series data, Granger causality and Directed Information can be used [7, 5, 21], see also [14].", "startOffset": 79, "endOffset": 89}, {"referenceID": 19, "context": ", for time-series data, Granger causality and Directed Information can be used [7, 5, 21], see also [14].", "startOffset": 79, "endOffset": 89}, {"referenceID": 5, "context": "Entropy has found some additional uses in the causality literature recently: In [6], authors use maximum mutual information between X,Y in order to quantify the causal strength of a known causal graph.", "startOffset": 80, "endOffset": 83}, {"referenceID": 14, "context": "The work that is most similar to ours in spirit is [16], which also drops the additive noise assumption.", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "To be able to handle this generic form, they have to make strong assumptions on the exogenous variable, function, and distribution of the cause: [16] assume that the exogenous variable is a standard Gaussian, a Gaussian mixture prior for the cause, and a Gaussian process as the prior of the function.", "startOffset": 145, "endOffset": 149}, {"referenceID": 14, "context": "Similar to [16], we keep the most general functional model, but only put an assumption on the exogenous (background) variable.", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "This problem is called the minimum Shannon entropy coupling and is known to be NP hard [13].", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "In fact, it is shown in [13] that minimizing the joint entropy of a set of variables given their marginals is NP hard.", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "i xi [18].", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "Experiments on Real Cause Effect Pairs: We test our entropy-based causal inference algorithm on the CauseEffectPairs repository [15].", "startOffset": 128, "endOffset": 132}, {"referenceID": 15, "context": "ANM have been reported to achieve an accuracy of 63% with a confidence interval of \u00b110% [17].", "startOffset": 88, "endOffset": 92}, {"referenceID": 3, "context": "We also use the binomial confidence intervals as in [4].", "startOffset": 52, "endOffset": 55}, {"referenceID": 15, "context": "21% which is slightly better than the 63% performance of ANM as reported in [17].", "startOffset": 76, "endOffset": 80}, {"referenceID": 0, "context": "References [1] Richard Caron and Tim Traynor.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Zhitang Chen, Kun Zhang, Laiwan Chan, and Bernhard Sch\u00f6lkopf.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] David Maxwell Chickering.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Jalal Etesami and Negar Kiyavash.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Weihao Gao, Sreeram Kannan, Sewoong Oh, and Pramod Viswanath.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Clive W.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Alain Hauser and Peter B\u00fchlmann.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Alain Hauser and Peter B\u00fchlmann.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Patrik O Hoyer, Dominik Janzing, Joris Mooij, Jonas Peters, and Bernhard Sch\u00f6lkopf.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Antti Hyttinen, Frederick Eberhardt, and Patrik Hoyer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Dominik Janzing, Joris Mooij, Kun Zhang, Jan Lemeire, Jakob Zscheischler, Povilas Daniu\u0161is, Bastian Steudel, and Bernhard Sch\u00f6lkopf.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Mladen Kovacevic, Ivan Stanojevic, and Vojin Senk.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] Joris M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] Shmuel Onn and Ishay Weissman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] Judea Pearl.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] Jonas Peters, Dominik Janzing, and Bernhard Sch\u00f6lkopf.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] Christopher Quinn, Negar Kiyavash, and Todd Coleman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] Donald Rubin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] Bernhard Sch\u00f6lkopf, David W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] Naji Shajarisales, Dominik Janzing, Bernhard Sch\u00f6lkopf, and Michel Besserve.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] Karthikeyan Shanmugam, Murat Kocaoglu, Alex Dimakis, and Sriram Vishwanath.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] S Shimizu, P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] Peter Spirtes, Clark Glymour, and Richard Scheines.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "We use the following technique to generate uniformly randomly sampled points on the simplex in n dimensions [18]: Lemma 5.", "startOffset": 108, "endOffset": 112}, {"referenceID": 0, "context": "Let xi \u223c U [0, 1] for i \u2208 [n \u2212 1] be i.", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "with x\u0303i \u223c U [0, 1] for n(n\u2212 1) indexes by construction.", "startOffset": 13, "endOffset": 19}, {"referenceID": 0, "context": "Ignoring the entries where z\u0303i = 1\u2212xi\u22121, we can relabel z\u0303 to get z with z \u2208 [0, 1] n(n\u22121).", "startOffset": 77, "endOffset": 83}, {"referenceID": 16, "context": "5 Proof of Theorem 1 For sampling from the simplex, we use the following model: Lemma 6 ([18]).", "startOffset": 89, "endOffset": 93}, {"referenceID": 0, "context": "We know that the roots of a non-zero polynomial defined over a compact domain has Lebesque measure zero [1].", "startOffset": 104, "endOffset": 107}], "year": 2016, "abstractText": "We consider the problem of identifying the causal direction between two discrete random variables using observational data. Unlike previous work, we keep the most general functional model but make an assumption on the unobserved exogenous variable: Inspired by Occam\u2019s razor, we assume that the exogenous variable is simple in the true causal direction. We quantify simplicity using R\u00e9nyi entropy. Our main result is that, under natural assumptions, if the exogenous variable has low H0 entropy (cardinality) in the true direction, it must have high H0 entropy in the wrong direction. We establish several algorithmic hardness results about estimating the minimum entropy exogenous variable. We show that the problem of finding the exogenous variable with minimum entropy is equivalent to the problem of finding minimum joint entropy given n marginal distributions, also known as minimum entropy coupling problem. We propose an efficient greedy algorithm for the minimum entropy coupling problem, that for n = 2 provably finds a local optimum. This gives a greedy algorithm for finding the exogenous variable with minimum H1 (Shannon Entropy). Our greedy entropy-based causal inference algorithm has similar performance to the state of the art additive noise models in real datasets. One advantage of our approach is that we make no use of the values of random variables but only their distributions. Our method can therefore be used for causal inference for both ordinal and also categorical data, unlike additive noise models.", "creator": "LaTeX with hyperref package"}}}