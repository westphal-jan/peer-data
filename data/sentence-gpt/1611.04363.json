{"id": "1611.04363", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "Learning for Expertise Matching with Declination Prediction", "abstract": "We study the problem of finding appropriate experts who are able to complete timely reviews and would not say \"no\" to the invitation. The problem is a central issue in many question-and-answer systems, but has received little research attention. Different from most existing studies that focus on expertise matching, we want to further predict the expert's response: given a question, how can we find the expert who is able to provide a quality review and will agree to do it. We formalize the problem as a ranking problem. We first present an embedding-based question-to-expert distance metric for expertise matching and propose a ranking factor graph (RankFG) model to predict expert response. For online evaluation, we developed a Chrome Extension for reviewer recommendation and deployed it in the Google Chrome Web Store, and then collected the reviewers' feedback. We also used the review bidding of a CS conference for evaluation. In the experiments, the proposed method demonstrates its superiority (+6.6-21.2% by MAP) over several state-of-the-art algorithms. The system also shows an improvement (+7.7-22.0% by MAP). The problem requires some degree of refinement. In the recent research, it was discovered that the CS conference had three state-of-the-art algorithms:\n\n\nWe can safely compare the ranking factors in our study to their performance.\n\nWe also present an additional version of the ranking factor graph (The Ranking Factor graph).\nThere are other reasons for the higher rate of ranking than that of the CS conference, including our own. We want the system to use less of the information we already have (for example, how much more we need to find the expert in our area and how much they need to do before going to the CS conference.)\nIn our new study, we plan to test the system for the rank factor graph. For the first time, we plan to test our methodology by testing it, and then by testing it more closely. In our new study, we plan to test the system by testing it more closely. In our new study, we plan to test the system by testing it more closely. In our new study, we plan to test the system by testing it more closely. In our new study, we plan to test the system by testing it more closely. In our new study, we plan to test the system by testing it more closely. In our new study, we plan to test the system by testing it more closely. In our new study", "histories": [["v1", "Mon, 14 Nov 2016 12:46:24 GMT  (657kb,D)", "http://arxiv.org/abs/1611.04363v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yujie qian", "jie tang"], "accepted": false, "id": "1611.04363"}, "pdf": {"name": "1611.04363.pdf", "metadata": {"source": "CRF", "title": "Learning for Expertise Matching with Declination Prediction", "authors": ["Yujie Qian", "Jie Tang"], "emails": ["qyj13@mails.tsinghua.edu.cn,", "jietang@tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Finding appropriate experts for given questions is a central task in the question-andanswer problem. For example, Quora and Zhihu1 have attracted big-time experts in numerous fields to answer various questions. Another example is peer review, an important part of scientific publishing [21,12]. Many researchers consider peer review as part of their professional responsibility. Publishing groups also use the quality of peer reviews as an indicator of the success of a journal. At a high level, the problem is referred to as expertise matching \u2014 matching experts with questions. However, existing solutions for expertise matching are far from satisfactory. Peer review has long been criticized for being ineffective, slow and of low-quality. Statistics also show that about half of the questions on Quora only have one or do not have any answer2. One challenge is how to find appropriate experts who are qualified to answer the corresponding questions. The other one is how to find experts who will agree to provide their answers. The latter is even more serious. One would expect that experts with sufficient knowledge would be the best to answer a given question. However, in practice, just to mention a few, American Political Science Review reported 2051 of 4516 reviewers accepted in 2013 (decline rate: 54.6%) [7] and a decline rate of 49.6% in 2014 [8]. Our preliminary statistics also show that 52.7% of the review invitations are declined or ignored.\n1 http://quora.com, http://zhihu.com 2 https://www.quora.com/What-percentage-of-questions-on-Quora-have-no-answers\nar X\niv :1\n61 1.\n04 36\n3v 1\n[ cs\n.A I]\n1 4\nN ov\n2 01\n6\nFig. 1(a) illustrates the declination rate of review invitation from nine journals in our data. J1 and J2 (two materials science journals) suffer from a decline rate of almost 80%. Many top experts are inclined to say \u201cno\u201d for various reasons. As a result, finding appropriate experts who are able to complete timely reviews remains a challenge.\nQuite a few studies have been conducted for automating the expert-question matching (details can be find in Section 5). However, most of this research focuses on the following setting: given a list of experts and a list of questions, how to find an optimal matching (with some constraints) between experts and questions.\nIn this paper, we study a more open question: given a new question, how to find appropriate experts who will agree to answer this question? The problem is much more challenging than the traditional matching problem, which is mainly concerned with the relevance between a question and an expert\u2019s interest. However, factors that affect an expert to accept (or reject) a review invitation are more complicated, including not only the relevance of the question topic to the expert\u2019s interest, but also the popularity of the question and whether the expert can gain attention from others. For peer review, other reasons for experts (researchers) to decline a review invitation include having too many reviews at hand and a tight deadline for completing the review [23,24]. Another survey of political science journals also finds that decline rate is related to the expert\u2019s personal experience, e.g., top experts are more likely to decline than junior experts [2]. Technically, the challenge is how to design a principled approach to deal with the problem of expertise matching by considering the declination.\nIn this paper, we formalize the problem as a ranking problem and propose a ranking factor graph (RankFG) model to predict the willingness of experts to answer a given question. We introduce an embedding-based metric to measure the degree of expertise matching between the questions and experts, using them as features in the ranking model. RankFG is able to identify who is at a high risk of declining a review invitation. To empirically evaluate the proposed methodologies, we developed a Chrome Extension of reviewer recommendation and deployed it in the Google Chrome Web Store. About 30 journal editors downloaded the extension and used it for reviewer recommendations. Based on their feedback logs, the proposed method can clearly better predict the declination response than several state-of-the-art ranking algorithms. The proposed method is very general and can be flexibly applied to various scenarios. For example, we applied the proposed method to a major CS conference with around 1,000 submissions and 440 PCs to quantitatively evaluate the prediction quality. We use the bidding information as the ground-truth to evaluate whether an PC will be willing to review a submission or not. Experiments show that our method can significantly improve (+6.6- 21.2%) the accuracy of expertise matching by considering declination."}, {"heading": "2 Problem Definition", "text": "Given a question, our goal is to find experts with sufficient knowledge who are willing to review this paper.\nWe consider a social network G = (V,E), where V is a set of |V | = N experts and E \u2286 V \u00d7V is a set of relationships between experts. There can be various kinds of relationships in different social network. For example, in an academic social network,\nthe relationships include collaborations, same-affiliation, and same-nationality; while in a Quora-like network, the relationships include friendship, reply, and co-reply. Let ai denote expert vi\u2019s attributes, which could be one\u2019s interests, the questions she/he has replied, or simply the number of his/her posts. We use A = {a1, \u00b7 \u00b7 \u00b7 , aN} to denote the attributes of all experts. Given these definitions, we define our problem of expertise matching with declination as follows.\nProblem 1. Expertise Matching with Declination Prediction. Let G = (V,E,A) be an attribute-augmented social network. Given a particular question q, the goal is to design a predictive function such that we can suggest experts of high relevance to question q and of low risk of declining the review invitation, i.e.,\nf : (G, q)\u2192 Y (1)\nwhere Y = {y1, \u00b7 \u00b7 \u00b7 , yN} represents the prediction results for all experts in the network G and yi \u2208 {0, 1} is a binary score indicating whether expert vi \u2208 V has a high risk of declining the invitation.\nThe predictive function f takes the networkG as input, which means that we should consider the network information in designing the predictive function. Additionally, another technical challenge is how to consider both expertise matching and the potential of declination."}, {"heading": "3 The Approach", "text": "The straightforward method to deal with the expertise matching problem is to design a metric to quantify the similarity between question and expert, and then rank experts based on the similarity scores. However, the situation becomes different when taking declination into consideration. Besides, designing a high-quality similarity metric between question and expert is also a non-trivial task.\nIn this paper, we propose a ranking factor graph (RankFG) model. Specifically, given a question q, we first extract candidate experts through an information retrieval model. In order to quantify the similarity scores between questions and experts, we present an embedding-based matching algorithm. The algorithm leverages a constraintbased optimization to model the matching score between the two different types of entities \u2013 i.e., question and expert. Based on the similarity score, the RankFG model can learn and predict who has a high risk of declining the review."}, {"heading": "3.1 Candidate Generation", "text": "Given a question q, we first use all words in the question to select a list of candidate experts.3 In particular, we use a language model to retrieve relevant experts from V .\n3 In our implementation, we also tried to use a keyword extraction tool [29] to extract a number of keywords from the question and then use the keywords to select candidate experts.\nLanguage model is one of the state-of-the-art approaches in information retrieval. It interprets the relevance between a document and a query word as a generative probability:\nP (w|d) = Nd Nd + \u03bb \u00b7 N w d Nd + (1\u2212 Nd Nd + \u03bb ) \u00b7 N w D ND (2)\nwhere Nd is the number of word tokens in document d, Nwd is the word frequency (i.e., occurrence number) of word w in d, NwD is the number of word tokens in the entire collection, and NwD is the word frequency of word w in the collection D; \u03bb is the Dirichlet smoothing factor and is commonly set according to the average document length in the collection [28]. Thus, the probability of the document model d generating a question q can be defined as P (q|d) = \u03a0w\u2208qP (w|d)."}, {"heading": "3.2 Expertise Matching", "text": "The language model only represents the matching between question and expert at a course-level. We present a more elaborate question-to-expert distance metric. The metric is inspired by [16], which was designed for estimating document distance.\nGiven all questions and all experts\u2019 interests, we first learn a numeric vector representation (also called word embedding) for each word. We use Word2vec with Skipgram [18] to learn the word embeddings4. Word2vec is a shallow neural network architecture that consists of an input layer, a projection layer, and an output layer. The training objective is to use an input word to predict surrounding words in the context. Formally, we can have the following log-likelihood objective function,\n1\nT T\u2211 t=1 \u2211 \u2212c\u2264j\u2264c,j 6=0 log p(wt+j |wt) (3)\nwhere T is the size of the training corpus and c is the window size around the center word wt. The probability p(wt+j |wt) is defined by a softmax function p(wO|wI) =\nexp(v\u2032wO >vwI )\u2211W\nw=1 exp(v\u2032w>vwI ) , where vw and v\u2032w are the input vector and output vector of word w, and W is the vocabulary size. After the word embedding training, we obtain a m-dimensional embedding vector xi for each word i \u2208 {1, \u00b7 \u00b7 \u00b7 , n}. We consider both question and expert as a document. (For expert, we combine all the questions he/she answered or the papers he/she published.) Each document can be represented as a normalized bag-of-words (nBOW) vector d \u2208 Rn, with each element di = N id/( \u2211n j=1N j d) standing for the weight of word i, where N id is the word frequency of word i in the document. Finally, the question-toexpert distance QtoE(q, v) can be defined by the following linear program,\nQtoE(q, v) = min Tij\u22650 n\u2211 i=1 n\u2211 j=1 Tij\u2016xi \u2212 xj\u20162\nsubject to: n\u2211\nj=1\nTij = d q i , n\u2211 i=1 Tij = d v j , \u2200i, j \u2208 {1, . . . , n}.\n(4)\n4 We also tried CBOW, which results in an inferior performance comparing with Skip-gram.\nwhere dq is the nBOW vector for question q and dv the vector of expert v. The linear program to find an optimal matching between the words in q and v, while each word i in q can be matched with different words j in v, with the weight Tij . Complexity. The best time complexity for solving the above optimization problem is O(p3 log p), where p is the number of unique words [20]. An approximate solution can result in a complexity of O(p2) [16], by removing each of the two constraints, and then combines the results."}, {"heading": "3.3 Ranking Factor Graph", "text": "Previous works usually focused on expertise matching, but seldom considered whether the expert would decline the invitation. We propose a ranking factor graph (RankFG) model to predict how likely the expert is to accept or decline the invitation. The graphical model RankFG consists of two layers of variables: observations and latent variables. In our problem, each observation represent a question-expert pair {(q, vi)}, and is associated with a latent variable yi to represent whether the expert will agree or decline to answer the question (or review the paper). Local factor functions are defined to capture the relationships between an observation and its corresponding latent variable.\nLocal factor function: Captures the characteristics of each question-expert pair, including a relevance score between the paper and the expert, and any attributes associated with the expert. Defined as an exponential function\nf(q, vi, yi) = 1\nZa exp\n{ \u03b1T\u03c8(q, vi, yi) } (5)\nwhere \u03c8(.) is the vector of feature functions defined between q and vi with respect to the value of yi; \u03b1 is the weight of the features; and Za is a normalization factor.\nMoreover, RankFG can leverage the data correlation to improve the prediction performance. For example, candidate experts from the same country (like USA) may say \u201cno\u201d at the same time to a question. From the paper reviewer data in our experiments, we have found three interesting correlations, same nationality, same affiliation, and collaboration. Fig. 1(b) and Fig. 1(c) shows statistics on the data in our experiments. We can see that the decline probability of a reviewer having a correlated expert who already declined is much higher than that of those without. On average, the probability that a reviewer will decline the invitation almost doubles if another reviewer of the same nationality has declined the invitation. In RankFG, such correlations can be defined as correlation factor functions among latent variables.\nCorrelation factor function: Captures the correlation between latent variables. Also defined as an exponential function\ng(yi, yj) = 1\nZb exp\n{ \u03b2T\u03c6(yi, yj) } (6)\nwhere \u03c6(.) is the vector of feature functions defined between yi and yj ; \u03b2 is the weight of the features; and Zb is a normalization factor.\nBy integrating the defined factor functions, we can define the following joint probability:\nP (Y |G) =\n\u220f\nvi\u2208V f(q, vi, yi)\n\u220f\n(vi,vj)\u2208E g(yi, yj)\n= 1\nZ exp \u2211 vi\u2208V \u03b1T\u03c8(q, vi, yi) + \u2211 (vi,vj)\u2208E \u03b2T\u03c6(yi, yj)  = 1\nZ exp\n{ \u03b8T\u03a6\n} (7)\nwhere Z = ZaZb is the normalization factor; \u03b8 = ( \u03b1T,\u03b2T )T are parameters to\nestimate, and \u03a6 = (\u2211 vi\u2208V \u03c8(q, vi, yi) T, \u2211 (vi,vj)\u2208E \u03c6(yi, yj) T )T .\nCombining Expertise Matching into RankFG Now we introduce how to combine the expertise matching scores into the proposed RankFG model. Specifically, given a question q and an expert v, we obtain a score QtoE(q, v). We also defined other feature functions in the RankFG model. In principle, the feature functions can be instantiated in different ways to reflect our prior knowledge or intuitions for different applications. They can be defined as either binary or a real-valued. In our experiments, we define the local feature functions (\u03c8) which can be divided into the following three categories:\n\u2013 Basic statistics. We define a set of statistics features for each potential expert. For example, in reviewer finding task, we use the features such as h-index, publication number, citation number, and the length of research experience.\n\u2013 Expertise matching. We measure expertise matching between question and expert by QtoE(q, v). We also consider other basic similarity scores as features, such as Jaccard similarity.\n\u2013 Organization. We define a binary feature to indicate whether the reviewer comes from academia or industry.\nThree correlation feature functions (\u03c6) are also defined according to experts\u2019 relationships, which are:\n\u2013 Same-nationality. This relationship exists when two experts come from the same country.\n\u2013 Same-affiliation. This relationship exists when two experts come from the same affiliation.\n\u2013 Friendship. This relationship exists when two experts have a friend relationship or have collaborated before.\nFor all three correlation factors, we use binary functions, i.e., \u03c6l(yi, yj) = 1, if and only if such correlation exists.\nModel learning Training the RankFG model involves finding a parameter configuration \u03b8 from a given training dataset, such that the log-likelihood objective function L(\u03b8) = logP (Y |G) can be maximized, i.e.,\n\u03b8\u2217 = argmax \u03b8 logP (Y |G) (8)\nThe optimization can be solved using gradient ascent algorithm. The gradient of each parameter \u03b8 wrt L(\u03b8) is\n\u2202L(\u03b8)\n\u2202\u03b8 =\n\u2202\n\u2202\u03b8\n( \u03b8T\u03a6\u2212 logZ ) = \u03a6\u2212 EP (Y |G)\u03a6 (9)\nIn this equation, the second term EP (Y |G)\u03a6 is unmanageable to estimate because of the difficulty in determining the marginal probability P (Y |G). There are several approximation methods. In our work, we choose Loop Belief Propagation (LBP) [27]. We first derive a factor graph from the original graph G, representing the factorization of the likelihood P (Y |G). Then we apply the sum-product algorithm [15] to factor graph to compute the approximate marginal distributions.\nAlgorithm 1 describes the learning process of the RankFG model. During the training, we update the parameters iteratively until convergence. In each iteration, messages are transferred sequentially in a certain order. We randomly select a node as the root and perform breadth-first search on the factor graph to construct a tree. We update the messages from the leaves to the root, then from the root to the leaves. Based on the received messages from factors, we can calculate the marginal probabilities. Then we compute the gradient\u2207 and update the parameters with learning step \u03b7.\nPrediction Given the observed value x and the learned parameters \u03b8, the declination prediction is to find the most likely configuration of Yq for a given question q. This can be obtained by:\nYq = argmax Yq\nP (Yq|G) (10)\nFor inference, we use the max-sum algorithm to find the values of Yq that maximize the likelihood. This max-sum algorithm is similar to the sum-product algorithm, except for calculating the message according to max instead of sum in message passing functions."}, {"heading": "4 Experiments", "text": "To empirically evaluate the proposed methodologies, we conduct the experiments in the peer review task, which is finding reviewers for academic papers.\nAlgorithm 1 Learning algorithm for RankFG. Input: Query questions Q = {q}, G = (V,E,A), and the learning rate \u03b7; Output: learned parameters \u03b8; \u03b8 \u2190 0; repeat\nfor q \u2208 Q do L\u2190 initialization list; Factor graph FG\u2190 BuildFactorGraph(L); repeat\nfor vi \u2208 L do Update the messages of vi according to sum-product update rule [15];\nend for until all messages \u00b5 do not change; for \u03b8i \u2208 \u03b8 do\nCalculate gradient\u2207i according to Eq. 9; Update \u03b8newi = \u03b8 old i + \u03b7 \u00b7 \u2207i;\nend for end for until converge;"}, {"heading": "4.1 Experimental Setup", "text": "Datasets We evaluate our method in the following three different datasets: Relevance: this dataset includes 86 papers and 2,048 candidate reviewers. We asked the PhD students from the author\u2019s lab to make relevance judgments. We simply considered the relevance as binary: relevance and irrelevance. In this dataset, we focus on evaluating how the method can find relevant and qualified experts to review the paper.\nResponse: this dataset was collected from our Chrome extension for reviewer recommendation. It includes 183 papers submitted to nine journals and 827 reviewers\u2019 responses. Among the responses, 391 are \u201cagree\u201d, while the rest are viewed as \u201cdecline\u201d (including \u201cunavailable\u201d and \u201cno response\u201d). Figure 1(a) shows the declination rate. In Response, we focus on evaluating how our method can find experts who are willing to review, and reduce reviewer declination rate.\nConference: this dataset comes from the reviewer bidding of a major CS conference. It contains 935 submitted papers and 440 Program Committee members. PC members each choose several papers they would like to review. We consider their choices as \u201cagree\u201d, and randomly sample the same number of PCs from the others as \u201cdecline\u201d. In Conference, we aim to apply the model in a conference reviewer assignment task.\nIn each dataset, we randomly select 60% as training data and consider the remaining 40% as test data. We perform each experiment 10 times (including partition of the training and test data) and report the average score.\nComparison Methods We compare the following methods in the experiment: Jaccard Similarity: We calculate Jaccard similarity coefficient between the paper\u2019s keywords and the reviewer\u2019s research interests. The recommendation is then made based on the rank of similarity score.\nWord Mover\u2019s Distance (WMD): We apply Word Mover\u2019s Distance [16] as the measure of expertise matching, and then rank the candidates by WMD.\nSVM-Rank: We consider a learning to rank approach that uses the same training data as RankFG. Specifically, we use the implementation of SVM-Rank [9].\nRankFG: The proposed method, which trains a RankFG model to make reviewer recommendations.\nEvaluation Measures We consider the problem a ranking problem and evaluate different methods with the following metrics: Precision for the top-N results (P@N), Mean Average Precision (MAP), and R-prec. In the evaluations, we only consider the experts collected in our datasets.\nAll the experiment codes are implemented in C++ and Python, and the evaluations are performed on an x86-64 machine with 2.70GHz Intel Core i5 CPU and 8GB RAM."}, {"heading": "4.2 Experiment Results", "text": "Performance Analysis We compare the performance of all methods on three datasets. Table 1 lists the performance comparison in three datasets. We find in Relevance, WMD significantly outperforms Jaccard and has the best performance of P@N within four methods. SVM-Rank and RankFG have lower P@N because the relevance between paper and reviewer mostly depends on content similarity while not other features and correlations. On the other hand, in Response, the proposed RankFG achieves the best result. RankFG leverages the reviewer correlation and thus improves the performance. It also confirms the discovery in the survey [23] that whether a reviewer agrees to review depends not only on expertise matching, but also on other factors. In the Conference dataset, RankFG still outperforms other methods. It suggests that the RankFG can also be applied in conference paper-reviewer assigning.\nFactor Contribution Analysis We now analyze how different factors can help find reviewers. We examine the contribution of different features by removing each of them. Fig. 2(a) shows the result, using MAP score as the evaluation metric. We can see in Relevance and Conference, the performance mainly depends on expertise matching. Removing statistical or organization features does not significantly affect the performance. In Response, the situation is different. The MAP score drops when ignoring each kind of feature, especially statistics. It confirms the finding in a recent survey [2],\nwhich says \u201cdecline\u201d is related to the reviewer\u2019s personal academic status. Senior researchers tend to decline invitations more often.\nWe also analyze how correlation contributes to the model. Fig. 2(b) shows the analysis result. From the figure, we find that the contribution of correlation varies in different circumstances. Same-affiliation and collaborator have a positive effect in Relevance and Conference, but a negative effect in Response. However, the combination of these two correlations with same-nationality helps improve the prediction performance in Response. Same-nationality seems only work in Response among three datasets. It is reasonable since nationality is not related to someone\u2019s research interests or review ability. So we remove same-nationality from other experiments on Relevance and Conference.\nTraining/Test Ratio Analysis We provide further analysis on the effect of the training ratio on our RankFG model. Fig. 3 shows the experiment results when varying the percentage of training data. In general, we can see a rising trend as the percentage of training data increases. In Conference dataset, the increase is relatively subtle since this dataset is the largest among the three. This result indicates the positive effect of training data size on the recommendation performance of our model. We also conduct an experiment to analyze the convergent property of the RankFG model. The learning\nalgorithm shows a very good convergent property. Roughly speaking, it takes 200-2000 iterations to converge on the three different datasets."}, {"heading": "5 Related Work", "text": "The research of expertise matching can be traced back to 20 years ago. Existing methods mainly fall into two categories: probabilistic models and optimization models.\nProbabilistic models try to improve the expertise matching accuracy between experts and papers. Early works such as latent semantic indexing [3] and keyword matching [5] tried to solve the problem in an information retrieval system. Later, Mimno et al. compared several language models and proposed Author-Persona-Topic model [19]. Karimzadehgan et al. proposed reviewer aspect modeling and paper aspect modeling, and tried to model multiple aspects of expertise [11].\nOptimization model concentrates on solving the optimization problem of constructing panels between a list of reviewers and a list of papers. These models usually consider the constraints in conference paper-reviewer assignment tasks [1], such as conflict of interests, paper demand and reviewer workload. Many methods have been proposed, such as search and greedy algorithm [17,13], integer linear programming [10], network flow [4] and minimum cost flow [22].\nRecently, a few systems have also been developed to make reviewer recommendations [26], help with conference reviewer assignment [14] or identifying reviewers for proposals [6]. Wu et al. [25] presents a patent partner recommendation framework in enterprise social networks, which also uses a ranking factor graph model.\nIn this paper, our general goal is to recommend appropriate experts and predict the willingness of them to accept the invitation."}, {"heading": "6 Conclusions", "text": "In this paper, we study predicting experts response in expertise matching. Our goal is to find experts not only relevant to a question/paper, but also will not decline the invitation. We present an embedding-based question-to-expert distance metric, and propose a ranking factor graph (RankFG) model to find the proper experts. RankFG leverages both expertise matching and other attributes and correlation between experts. To fairly evaluate the proposed methodologies, we conduct experiments on three datasets: Relevance, Response, and Conference. Comparing with several state-of-the-art methods, our method can significantly improve the performance of expertise matching."}], "references": [{"title": "Conference paper assignment", "author": ["S. Benferhat", "J. Lang"], "venue": "Int. J. Intell. Syst. 16(10), 1183\u20131192", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Peer reviewing in political science: New survey results", "author": ["P.A. Djupe"], "venue": "PS: Political Science & Politics 48(02), 346\u2013352", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Automating the assignment of submitted manuscripts to reviewers", "author": ["S.T. Dumais", "J. Nielsen"], "venue": "SIGIR\u201992. pp. 233\u2013244", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1992}, {"title": "The ai conference paper assignment problem", "author": ["J. Goldsmith", "R.H. Sloan"], "venue": "AAAI\u201907. pp. 53\u201357", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Recommending papers by mining the web", "author": ["C.B. Haym", "H. Hirsh", "W.W. Cohen", "C. Nevill-manning"], "venue": "IJCAI\u201999. pp. 1\u201311", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "Mining for proposal reviewers: lessons learned at the national science foundation", "author": ["S. Hettich", "M.J. Pazzani"], "venue": "KDD\u201906. pp. 862\u2013871", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Annual report of the editors of the american political science review, 2012\u2013 2013", "author": ["J. Ishiyama"], "venue": "PS: Political Science & Politics 47(02), 542\u2013545", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Report of the editors of the american political science review, 2013\u20132014", "author": ["J. Ishiyama"], "venue": "PS: Political Science & Politics 48(02), 396\u2013399", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "KDD\u201906. pp. 217\u2013226. ACM", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Constrained multi-aspect expertise matching for committee review assignment", "author": ["M. Karimzadehgan", "C. Zhai"], "venue": "CIKM\u201909. pp. 1697\u20131700", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-aspect expertise matching for review assignment", "author": ["M. Karimzadehgan", "C. Zhai", "G. Belford"], "venue": "CIKM\u201908. pp. 1113\u20131122", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Peer review: crude and understudied, but indispensable", "author": ["J.P. Kassirer", "E.W. Campion"], "venue": "JAMA 272(2), 96\u201397", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "Weighted coverage based reviewer assignment", "author": ["N.M. Kou", "U.L. Hou", "N. Mamoulis", "Z. Gong"], "venue": "SIGMOD\u201915. pp. 2031\u20132046. ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "A topic-based reviewer assignment system", "author": ["N.M. Kou", "N. Mamoulis", "Y. Li", "Y. Li", "Z Gong"], "venue": "VLDB\u201915. pp. 1852\u20131855", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F.R. Kschischang", "B.J. Frey", "H.A. Loeliger"], "venue": "IEEE Transactions on Information Theory 47(2), 498\u2013519", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "From word embeddings to document distances", "author": ["M.J. Kusner", "Y. Sun", "N.I. Kolkin", "K.Q. Weinberger"], "venue": "ICML\u201915", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "On good and fair paper-reviewer assignment", "author": ["C. Long", "R.C.W. Wong", "Y. Peng", "L. Ye"], "venue": "ICDM\u201913. pp. 1145\u20131150. IEEE", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Expertise modeling for matching papers with reviewers", "author": ["D. Mimno", "A. McCallum"], "venue": "KDD\u201907. pp. 500\u2013509", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast and robust earth mover\u2019s distances", "author": ["O. Pele", "M. Werman"], "venue": "ICCV\u201909. pp. 460\u2013467. IEEE", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Editorial peer review: its development and rationale", "author": ["D. Rennie"], "venue": "Peer review in health sciences pp. 3\u201313", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "On optimization of expertise matching with various constraints", "author": ["W. Tang", "J. Tang", "T. Lei", "C. Tan", "B. Gao", "T. Li"], "venue": "Neurocomputing 76(1), 71\u201383", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Why do peer reviewers decline to review? a survey", "author": ["L. Tite", "S. Schroter"], "venue": "Journal of epidemiology and community health 61(1), 9\u201312", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Why do peer reviewers decline to review manuscripts? a study of reviewer invitation responses", "author": ["M. Willis"], "venue": "Learned Publishing 29(1), 5\u20137", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Patent partner recommendation in enterprise social networks", "author": ["S. Wu", "J. Sun", "J. Tang"], "venue": "WSDM\u201913. pp. 43\u201352", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "A reviewer recommendation system based on collaborative intelligence", "author": ["K.H. Yang", "T.L. Kuo", "H.M. Lee", "J.M. Ho"], "venue": "WI-IAT\u201909. vol. 1, pp. 564\u2013567. IET", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Generalized belief propagation", "author": ["J.S. Yedidia", "W.T. Freeman", "Y Weiss"], "venue": "NIPS\u201900. vol. 13, pp. 689\u2013695", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}, {"title": "A study of smoothing methods for language models applied to ad hoc information retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "SIGIR\u201901. pp. 334\u2013342", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2001}, {"title": "Keyword extraction using support vector machine", "author": ["K. Zhang", "H. Xu", "J. Tang", "J. Li"], "venue": "WAIM\u201906. pp. 85\u201396", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 20, "context": "Another example is peer review, an important part of scientific publishing [21,12].", "startOffset": 75, "endOffset": 82}, {"referenceID": 11, "context": "Another example is peer review, an important part of scientific publishing [21,12].", "startOffset": 75, "endOffset": 82}, {"referenceID": 6, "context": "6%) [7] and a decline rate of 49.", "startOffset": 4, "endOffset": 7}, {"referenceID": 7, "context": "6% in 2014 [8].", "startOffset": 11, "endOffset": 14}, {"referenceID": 22, "context": "For peer review, other reasons for experts (researchers) to decline a review invitation include having too many reviews at hand and a tight deadline for completing the review [23,24].", "startOffset": 175, "endOffset": 182}, {"referenceID": 23, "context": "For peer review, other reasons for experts (researchers) to decline a review invitation include having too many reviews at hand and a tight deadline for completing the review [23,24].", "startOffset": 175, "endOffset": 182}, {"referenceID": 1, "context": ", top experts are more likely to decline than junior experts [2].", "startOffset": 61, "endOffset": 64}, {"referenceID": 28, "context": "3 In our implementation, we also tried to use a keyword extraction tool [29] to extract a number of keywords from the question and then use the keywords to select candidate experts.", "startOffset": 72, "endOffset": 76}, {"referenceID": 27, "context": ", occurrence number) of word w in d, N D is the number of word tokens in the entire collection, and N D is the word frequency of word w in the collection D; \u03bb is the Dirichlet smoothing factor and is commonly set according to the average document length in the collection [28].", "startOffset": 272, "endOffset": 276}, {"referenceID": 15, "context": "The metric is inspired by [16], which was designed for estimating document distance.", "startOffset": 26, "endOffset": 30}, {"referenceID": 17, "context": "We use Word2vec with Skipgram [18] to learn the word embeddings4.", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "The best time complexity for solving the above optimization problem is O(p log p), where p is the number of unique words [20].", "startOffset": 121, "endOffset": 125}, {"referenceID": 15, "context": "An approximate solution can result in a complexity of O(p) [16], by removing each of the two constraints, and then combines the results.", "startOffset": 59, "endOffset": 63}, {"referenceID": 26, "context": "In our work, we choose Loop Belief Propagation (LBP) [27].", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "Then we apply the sum-product algorithm [15] to factor graph to compute the approximate marginal distributions.", "startOffset": 40, "endOffset": 44}, {"referenceID": 14, "context": "Input: Query questions Q = {q}, G = (V,E,A), and the learning rate \u03b7; Output: learned parameters \u03b8; \u03b8 \u2190 0; repeat for q \u2208 Q do L\u2190 initialization list; Factor graph FG\u2190 BuildFactorGraph(L); repeat for vi \u2208 L do Update the messages of vi according to sum-product update rule [15]; end for until all messages \u03bc do not change; for \u03b8i \u2208 \u03b8 do Calculate gradient\u2207i according to Eq.", "startOffset": 273, "endOffset": 277}, {"referenceID": 15, "context": "Word Mover\u2019s Distance (WMD): We apply Word Mover\u2019s Distance [16] as the measure of expertise matching, and then rank the candidates by WMD.", "startOffset": 60, "endOffset": 64}, {"referenceID": 8, "context": "Specifically, we use the implementation of SVM-Rank [9].", "startOffset": 52, "endOffset": 55}, {"referenceID": 22, "context": "It also confirms the discovery in the survey [23] that whether a reviewer agrees to review depends not only on expertise matching, but also on other factors.", "startOffset": 45, "endOffset": 49}, {"referenceID": 1, "context": "It confirms the finding in a recent survey [2],", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "Early works such as latent semantic indexing [3] and keyword matching [5] tried to solve the problem in an information retrieval system.", "startOffset": 45, "endOffset": 48}, {"referenceID": 4, "context": "Early works such as latent semantic indexing [3] and keyword matching [5] tried to solve the problem in an information retrieval system.", "startOffset": 70, "endOffset": 73}, {"referenceID": 18, "context": "compared several language models and proposed Author-Persona-Topic model [19].", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "proposed reviewer aspect modeling and paper aspect modeling, and tried to model multiple aspects of expertise [11].", "startOffset": 110, "endOffset": 114}, {"referenceID": 0, "context": "These models usually consider the constraints in conference paper-reviewer assignment tasks [1], such as conflict of interests, paper demand and reviewer workload.", "startOffset": 92, "endOffset": 95}, {"referenceID": 16, "context": "Many methods have been proposed, such as search and greedy algorithm [17,13], integer linear programming [10], network flow [4] and minimum cost flow [22].", "startOffset": 69, "endOffset": 76}, {"referenceID": 12, "context": "Many methods have been proposed, such as search and greedy algorithm [17,13], integer linear programming [10], network flow [4] and minimum cost flow [22].", "startOffset": 69, "endOffset": 76}, {"referenceID": 9, "context": "Many methods have been proposed, such as search and greedy algorithm [17,13], integer linear programming [10], network flow [4] and minimum cost flow [22].", "startOffset": 105, "endOffset": 109}, {"referenceID": 3, "context": "Many methods have been proposed, such as search and greedy algorithm [17,13], integer linear programming [10], network flow [4] and minimum cost flow [22].", "startOffset": 124, "endOffset": 127}, {"referenceID": 21, "context": "Many methods have been proposed, such as search and greedy algorithm [17,13], integer linear programming [10], network flow [4] and minimum cost flow [22].", "startOffset": 150, "endOffset": 154}, {"referenceID": 25, "context": "Recently, a few systems have also been developed to make reviewer recommendations [26], help with conference reviewer assignment [14] or identifying reviewers for proposals [6].", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "Recently, a few systems have also been developed to make reviewer recommendations [26], help with conference reviewer assignment [14] or identifying reviewers for proposals [6].", "startOffset": 129, "endOffset": 133}, {"referenceID": 5, "context": "Recently, a few systems have also been developed to make reviewer recommendations [26], help with conference reviewer assignment [14] or identifying reviewers for proposals [6].", "startOffset": 173, "endOffset": 176}, {"referenceID": 24, "context": "[25] presents a patent partner recommendation framework in enterprise social networks, which also uses a ranking factor graph model.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "We study the problem of finding appropriate experts who are able to complete timely reviews and would not say \u201cno\u201d to the invitation. The problem is a central issue in many question-and-answer systems, but has received little research attention. Different from most existing studies that focus on expertise matching, we want to further predict the expert\u2019s response: given a question, how can we find the expert who is able to provide a quality review and will agree to do it. We formalize the problem as a ranking problem. We first present an embedding-based question-to-expert distance metric for expertise matching and propose a ranking factor graph (RankFG) model to predict expert response. For online evaluation, we developed a Chrome Extension for reviewer recommendation and deployed it in the Google Chrome Web Store, and then collected the reviewers\u2019 feedback. We also used the review bidding of a CS conference for evaluation. In the experiments, the proposed method demonstrates its superiority (+6.6-21.2% by MAP) over several state-of-the-art algorithms.", "creator": "LaTeX with hyperref package"}}}