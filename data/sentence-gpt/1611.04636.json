{"id": "1611.04636", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "When Saliency Meets Sentiment: Understanding How Image Content Invokes Emotion and Sentiment", "abstract": "Sentiment analysis is crucial for extracting social signals from social media content. Due to the prevalence of images in social media, image sentiment analysis is receiving increasing attention in recent years. However, most existing systems are black-boxes that do not provide insight on how image content invokes sentiment and emotion in the viewers. For example, social networks rely on the appearance of content to determine which is relevant for the context of an opinion. In recent years, the presence of imagery such as images such as political slogans, slogans or articles has been linked to a tendency to increase their presence in the viewer, which might explain why images in social media are perceived as irrelevant.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 14 Nov 2016 22:02:09 GMT  (3568kb)", "http://arxiv.org/abs/1611.04636v1", "7 pages, 5 figures, submitted to AAAI-17"]], "COMMENTS": "7 pages, 5 figures, submitted to AAAI-17", "reviews": [], "SUBJECTS": "cs.AI cs.CV", "authors": ["honglin zheng", "tianlang chen", "jiebo luo"], "accepted": false, "id": "1611.04636"}, "pdf": {"name": "1611.04636.pdf", "metadata": {"source": "CRF", "title": "When Saliency Meets Sentiment: Understanding How Image Content Invokes Emotion and Sentiment", "authors": ["Honglin Zheng", "Tianlang Chen", "Jiebo Luo"], "emails": ["hzheng10@u.rochester.edu,", "t.chen@rochester.edu,", "jluo@cs.rochester.edu"], "sections": [{"heading": "Introduction", "text": ""}, {"heading": "Visual Sentiment Analysis", "text": "In such an era of information explosion, it is increasingly common for people to express their emotions through posting images on social media like Twitter and Flickr. It is important for both psychologists and computer vision researchers to utilize such tremendous information to interpret the human emotion carried in the images. In the early days, manually crafted features, e.g., pixel level features such as color, texture and composition (Machajdik and Hanbury 2010), were designed to study the emotional reaction to visual content. Similar methods were pervasive for a while until recently, when large scale image datasets such as ImageNet (Deng et al. 2009) and Places Dataset (Zhou et al. 2014) became available. The power of deep learning, especially convolutional neural networks,\nhas been harnessed recently to discover the sentiment carried in images (You et al. 2015; Jindal and Singh 2015). However, most of the work are dedicated to fine-tuning pre-trained deep neural network models, such as AlexNet, VGGNet and ResNet. Most models are used as black boxes and little research has paid attention to what elements or attributes of the image are responsible for invoking emotions. In this work, we focus our effort on understanding a few\nFigure 1: Red boxes indicate detected salient objects. Left column shows example images whose salient objects share similar sentiment with the whole images. Right column row shows example images whose salient objects do not agree with the whole images in terms of sentiment. Text under each image indicates its dominant attribute over a corresponding attribute pair. Please refer to the text for detailed description of sentiment agreement and dominant attribute.\ndominant factors and attributes behind image sentiment evocation."}, {"heading": "Saliency Detection", "text": "In the meantime, saliency detection is also shifting from common visual feature based classifiers to deep learning based models. Previously, low-level saliency priors such as contrast prior and center prior are leveraged to approximate human observed saliency. Recently, inspired by convolutional neural network\u2019s state-of-the-art performance on various computer vision tasks such as image classification (Krizhevsky, Sutskever, and Hinton 2012) and scene recognition (Zhou et al. 2014), researchers start to utilize CNN to capture high-level visual concepts and produce saliency models with better detection performance (Wang et al. 2015; Zhang et al. 2015). Saliency detection focuses on attention analysis, while visual sentiment analysis focuses on emotion analysis. Various psychologists and neuroscientists have been actively studying the underlying relationship between attention and emotion. There is an ongoing discrepancy between those who suggest that emotional perception is automatic, namely in the manner that it is independent of top-down factors such as attention (Vuilleumier et al. 2001), and those illustrating the dependence on attention (Grossberg, 1980; Fox et al. 2001). To the best of our knowledge, however, no computer vision research has been done to discover how emotion depends on attention, especially salient object(s), in an image. Even though it has been an active research topic in neuroscience, most of the neuroscientists fail to, or do not pay enough attention to the following questions: 1. Given an image, is there any salient object in it? If there is, does the salient object share similar sentiment with the entire image? 2. For those images that share similar sentiment with their salient object(s), what category do they fall into? Do they share some common attributes like man-made objects or human faces? What about those images that do not share similar sentiment with their salient objects? Therefore, we target our work on the questions above and make several contributions: 1. We investigate fine-grained interaction between visual saliency and visual sentiment over several primary scene types, including open-closed, natural-manmade, indooroutdoor, and face-noface. 2. We employ the state of the art saliency algorithm and visual sentiment classification model to facilitate accurate region-level computational analysis of their interactions. 3. We utilize a large public image emotion dataset to discover the relationship between saliency and sentiment over different scene types in order to understand the evocation mechanism of visual sentiment.\nRelated Work To the best of our knowledge, there is no related work on combining saliency detection and image sentiment classification to understand how image invokes human emotion. The most relevant work is image sentiment localization. Sun et al. (2016) proposed the notion of AR, Affective Region, which is a specific region in the image that contains one or more salient objects that can attract viewers\u2019 attention and carry significant emotion. In their work, an offthe-shelf object detection algorithm (Alexe, Deselaers and Ferrari 2010) is adopted to generate random proposals, and then the sentiment of both the proposals and the entire image will be evaluated to discover affective regions. However, this method utilizes explicit object recognition, which has to generate thousands of proposal windows in order to yield to a high recall rate of affective regions proposals. It also incurs a high computational overhead and may result in regions containing tiny objects that do not even catch people\u2019s attention.\nIn Peng et al. (2016)\u2019s work, they try to predict an Emotion Stimuli Map (ESM), which describes pixels-wise contribution to emotion. They use a pre-trained and fine-tuned model to predict ESM and conclude that neither saliency nor objectness can correctly predict the image regions that evoke emotion. However, they fail to justify the choice of a dataset that consists of predominantly landscape scenes, and does not give any insight on why the saliency detection method does not work for detecting affective emotion region of such images. Yuan et al. (2013) extract scene descriptor low-level features from the SUN Dataset (Xiao et al. 2010) and use those features to train an SVM-based classifier in order to generate 102 mid-level scene attributes, based on which they perform sentiment prediction. In Zhou et al. (2015)\u2019s work, they learn deep features for images from the Places Dataset (Zhou et al. 2014) and are able to generate receptive field of the image for scene recognition task. The generated receptive fields reflect which part of the image the model is looking at when recognizing scenes. It is critical to understand what the scene recognition model takes into account when performing scene recognition, and similarly, it is of great significance for a sentiment analysis model to understand what scene attributes account for the prediction of image sentiment. Therefore, we propose a framework to understand for those images whose salient object(s) share the same sentiment with the entire image, what scene attributes they possess. On the other hand, for those images where salient object(s) does/do not agree with the sentiment of the entire image, what other scene attributes they possess."}, {"heading": "Methodology", "text": ""}, {"heading": "Framework Overview", "text": "Figure 2 illustrates our framework with annotation. Details of the steps are as follows: Step 1: For a given image, we use a state-of-the-art saliency detection model (Zhang et al. 2016) to detect salient object(s), if any, for each image. Red boxes show the detected salient objects. Step 2: We then use a state-of-the-art sentiment classification model (Campos, Jou and Giro-i-Nieto 2016) to obtain the sentiment scores for both the whole image and all of its salient objects, if any. Step 3 & 4: Based on the obtained scores we are able to find out for each image, if any salient object shares the same sentiment with the entire image. Consequently, we can partition the entire dataset into two parts: those images where at least one of their salient objects shares the same sentiment with the whole image, and those images where none of their salient objects can express the same sentiment as the whole image. For simplicity, images without any salient object detected are simply excluded from further analysis. Please refer to the Visual Sentiment Analysis section for details about the definition of sentiment agreement. Step 5 & 6: Within the two partitions we obtain from previous steps, we apply a state-of-the-art scene attribute detector and human face detector to further partition each part based on the detected attribute of the image. Step 7: We evaluate the results of the classification mentioned above and gather any interesting findings."}, {"heading": "Dataset", "text": "We use the data set released by You et al. (2016) for visual sentiment prediction. This dataset has 8 different categories including \u201cAmusement\u201d, \u201cAnger\u201d, \u201cAwe\u201d, \u201cContentment\u201d, \u201cDisgust\u201d, \u201cExcitement\u201d, \u201cFear\u201d and \u201cSadness\u201d. For each category we randomly sample 30% of the first 8000 images for experiment. The statistics of our current data set is shown in Table 1. Notice that this work actually does not need the exact emotion label for each image, so we can build our data set using both labeled and unlabeled images."}, {"heading": "Salient Object Detection", "text": "We adopt a state-of-the-art saliency detection model (Zhang et al. 2016) to detect whether an image has any salient object and if there is, locate those salient objects. This model leverages the high expressiveness of the VGGNet to generate a set of scored salient object proposals for an image, based on which it produces a compact set of detected regions using a subset optimization formulation. For every image in the given dataset, the model will detect whether there is any salient object in it and save all of the detected salient objects as sub images in a corresponding folder for our subsequent scene-attribute based partitioning."}, {"heading": "Visual Sentiment Analysis", "text": "We employ Convolutional Neural Network (CNN) to obtain visual sentiment prediction. Specifically, we use the\nstate-of-the-art Convolutional Neural Network model proposed by Campos, Jou and Giro-i-Bieto (2016). In their work, they fine-tuned a CNN model, using several performance boosting techniques to improve the performance of visual sentiment prediction. For each image, we extract the two output nodes of the last fully connected layer, and denote the sentiment probability outputs of image i as \ud835\udc5d\" = (Pr \ud835\udc5d \" , Pr (\ud835\udc5b)\"), which represent the probability of the emotion being negative and positive, respectively. Based on this notation we can define SAR, Sentiment Agreement Rate. For each image i, \ud835\udc46\ud835\udc34\ud835\udc45 \ud835\udc56 = min(|Pr \ud835\udc5d \" \u2212 Pr \ud835\udc5d 4 |) , \ud835\udc60 \u2208 \ud835\udc46(\ud835\udc56) \ud835\udc64\u210e\ud835\udc52\ud835\udc5f\ud835\udc52 \ud835\udc46(\ud835\udc56) \ud835\udc56\ud835\udc60 \ud835\udc61\u210e\ud835\udc52 \ud835\udc60\ud835\udc52\ud835\udc61 \ud835\udc5c\ud835\udc53 \ud835\udc4e\ud835\udc59\ud835\udc59 \ud835\udc60\ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc61 \ud835\udc5c\ud835\udc4f\ud835\udc57\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc60 \ud835\udc5c\ud835\udc53 \ud835\udc56\nWe define that for an image, its salient object(s) agrees with the sentiment of the whole image if \ud835\udc46\ud835\udc34\ud835\udc45 \ud835\udc56 \u2264 \ud835\udf03 where \ud835\udf03 is the threshold. In other words, for an image, if the sentiment score of at least one of its salient object(s) is within the range of \ud835\udf03 of the whole image\u2019s sentiment score, then this image is regarded as an image whose salient object(s) agrees with the sentiment of the entire image, and is classified to the partition of agree. Otherwise, it is an image whose salient object(s) disagrees with the sentiment of the entire image and is classified to the partition of disagree. In the experiment, we use the agreement threshold of 0.08, 0.1, 0.15, 0.18 and 0.2."}, {"heading": "Attribute Extraction and Categorization", "text": "For each partition we obtain from the previous step, we fine-tune a state of the art scene recognition model, PlacesCNN (Zhou et al. 2014), to further classify it into indoors and outdoors. This model has the same architecture as the one used in the Caffe reference network (Jia et al. 2014), and it is trained on the Places Database (Zhou et al. 2014), which is a benchmark for scene recognition. For a given image, the model will produce a 205x1 vector, and each of its dimension corresponds to the probability of one of 205 scenes. According to the given indoors/outdoors label reference, we use the labels of top 5 predicted places categories to vote whether it is indoor or outdoor. Next, we use the deep features from the last fully connected layers to detect 102 SUN scene attributes (Patterson and Hays 2012). We ranked the probability scores in a descending order. We then compare scores of open vs closed, natural vs man-made. Whichever attribute of a pair has a higher probability score than the other one is used to represent the image. We define that attribute as a dominant attribute in its attribute pair. For example, an image has score of 1.2 for open and 3.4 for closed, 5.6 for natural and 7.8 for man-made, then this image will be classified to have dominant attribute of closed and manmade. Finally, we utilize a leading cloud-based face recognition service Face++ re-\nsearch tool kit (Megvii Inc.) to detect whether there is any face in a given image, based on which we can further classify the partition into images that contain face(s) and those that do not. In summary, the attribute extraction and categorization process is as follows:\nExperiment and Evaluation As we can see from Figure 4, the dataset is fairly well distributed among the evaluated scene attributes. With sentiment agreement threshold set to 0.15, the categorization experiment generates the results shown in Figure 5. We further conduct experiments on different sentiment agree-\nment thresholds and obtain the results shown in Table 2. To understand the table, we introduce an evaluation metric: discrimination ratio, DR,\n\ud835\udc37\ud835\udc45 \ud835\udc43_\ud835\udc34 = ( #\ud835\udc43_\ud835\udc34#\ud835\udc34\ud835\udc59\ud835\udc59_\ud835\udc34 \u2212 #\ud835\udc43 #\ud835\udc34\ud835\udc59\ud835\udc59)\n#\ud835\udc43 #\ud835\udc34\ud835\udc59\ud835\udc59\n\ud835\udc64\u210e\ud835\udc52\ud835\udc5f\ud835\udc52 \ud835\udc43 \ud835\udc43\ud835\udc4e\ud835\udc5f\ud835\udc61\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b \u2208 \ud835\udc4e\ud835\udc54\ud835\udc5f\ud835\udc52\ud835\udc52, \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc5f\ud835\udc52\ud835\udc52 \ud835\udc34 \ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc4f\ud835\udc62\ud835\udc61\ud835\udc52 \u2208 {\ud835\udc5c\ud835\udc5d\ud835\udc52\ud835\udc5b, \ud835\udc50\ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc52\ud835\udc51, \ud835\udc5b\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc4e\ud835\udc59, \ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc5a\ud835\udc4e\ud835\udc51\ud835\udc52, \ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc5c\ud835\udc5c\ud835\udc5f, \ud835\udc5c\ud835\udc62\ud835\udc61\ud835\udc51\ud835\udc5c\ud835\udc5c\ud835\udc5f, \ud835\udc53\ud835\udc4e\ud835\udc50\ud835\udc52, \ud835\udc5b\ud835\udc5c\ud835\udc53\ud835\udc4e\ud835\udc50\ud835\udc52} #P_A: in partition P, the number of images that have dominant attribute A over its opposite. (open-closed, naturalmanmade, indoor-outdoor, face-noface are opposite of each other) #All_A: for the entire dataset, the number of images that have dominant attribute A #P: number of images in partition P #All: number of images in the entire dataset\nIf images are randomly split into two partitions of negative and positive, then the discrimination ratio for each at-\ntribute in each partition should be roughly equal to the percentage of such partition over the entire dataset. A more\npositive discrimination ratio of an attribute A in a partition P indicates that an image with such an attribute is easier to be classified into P. In other words, the higher the discrimination ratio is, the more likely the images in partition P are going to have attribute A, and vice versa. Table 2 suggests some similar patterns for various sentiment agreement thresholds ranging from 0.08 to 0.2: 1) As the agreement threshold decreases, the agree partition decreases while the disagree partition increases, which follows the intuition that the stricter the rule is, the more difficult for a specific part of an image to represent the whole image, and it would be more like a random even split for the dataset. 2) For images whose salient object(s) agree with the whole images, they tend to contain face(s) or outstanding man-made object(s), or tend to be more closed or more likely to be an indoor scene than images whose salient objects disagree with the whole images. 3) From the face-noface column, we can see that these two attributes are influential in pulling images away from each other. With an average discrimination ratio of -35.76% in partition disagree and +17.96% in partition agree, images containing faces are highly likely to express the same sentiment as the faces express, while images without faces are more unlikely to invoke human emotion solely due to the salient objects. This observation also follows the intuition that faces tend to dominate the sentiment perception by humans. For example, we can easily tell the sentiment\nof the left images in Figure 1 simply by looking at the human face(s) without paying attention to other objects or elements. We conduct another experiment to further evaluate the partitioning power of attribute combinations of closed, manmade, outdoor and noface, which have great tendency of partitioning the images to agree. In this experiment, we partition the agree and disagree classes according to all the possible combinations of these four pairs of attributes. For example, for each class, we calculate the total number of closed, manmade, indoor and face images, and also the total number of its entire opposite, namely open, natural, outdoor and noface images. For each combination in each partition, we will also calculate its DR metric.\nThe observations from Table 3 lead to the following interesting findings: 1) The combination with the most negative DR is 1, 1, 0 and 0. That is to say, an image with an attribute combination of open, natural, outdoor and noface is more likely to be classified to the partition of disagree. 2) Combinations with the face attribute are all with high DR, which further confirms that images with faces will be more likely to be classified into the partition of agree. 3) Surprisingly, the opposite of the most negative DR is not necessarily the combination with the most positive DR. Instead, {closed, natural, outdoor, face} and {open, natural, indoor, face} share the most positive DR.\nConclusion and Future Work In this work, using state of the art saliency detection model and visual sentiment classification model, we obtain salient object proposals and analyze their sentiment agreement with the whole images. We extract scene attributes from the images and examine the fine-grained interaction between visual saliency and visual sentiment. Our results suggest that images that contain outstanding man-made objects or human faces, or are indoors and closed, tend to express sentiment through their salient objects. On the other hand, images in which natural objects are more outstanding than man-made objects or do not contain human faces, or are outdoors and open, usually do not convey their sentiment information solely through their salient objects. We are encouraged by this study to evaluate the expressiveness of salient object(s) in terms of image sentiment. Moreover, we will conduct further experiments on the scene attribute distribution for each emotion class, e.g. amusement and anger, to study the evocation mechanism for each specific emotion. We will also study more scene attributes. It has the potential to give us more insight into what kind of attributes are responsible for invoking human emotion, providing guidance for both in-depth image sentiment analysis and psycho-visual understadning."}], "references": [{"title": "Affective image classification using features inspired by psychology and art theory", "author": ["J. Machajdik", "A. Hanbury"], "venue": "Proceedings of the 18th ACM international conference on Multimedia (pp. 83-92). ACM.", "citeRegEx": "Machajdik and Hanbury,? 2010", "shortCiteRegEx": "Machajdik and Hanbury", "year": 2010}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.J. Li", "K. Li", "L. Fei-Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on (pp. 248-255). IEEE.", "citeRegEx": "Deng et al\\.,? 2009", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "Advances in neural information processing systems (pp. 487-495).", "citeRegEx": "Zhou et al\\.,? 2014", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}, {"title": "Image Sentiment Analysis using Deep Convolutional Neural Networks with Domain Specific Fine Tuning", "author": ["S. Jindal", "S. Singh"], "venue": "IEEE International Conference on Information Processing (ICIP)", "citeRegEx": "Jindal and Singh,? 2015", "shortCiteRegEx": "Jindal and Singh", "year": 2015}, {"title": "Robust Image Sentiment Analysis using Progressively Trained and Domain Transferred Deep Networks", "author": ["Q. You", "J. Luo", "H. Jin", "J. Yang"], "venue": "the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI).", "citeRegEx": "You et al\\.,? 2015", "shortCiteRegEx": "You et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems (pp. 10971105).", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Deep networks for saliency detection via local estimation and global search", "author": ["L. Wang", "H. Lu", "X. Ruan", "M.H. Yang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3183-3192).", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Co-saliency detection via looking deep and wide", "author": ["D. Zhang", "J. Han", "C. Li", "J. Wang"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 29943002).", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Effects of attention and emotion on face processing in the human brain: an event-related fMRI study", "author": ["P. Vuilleumier", "J.L. Armony", "J. Driver", "R.J. Dolan"], "venue": "Neuron, 30(3), 829-841.", "citeRegEx": "Vuilleumier et al\\.,? 2001", "shortCiteRegEx": "Vuilleumier et al\\.", "year": 2001}, {"title": "How does a brain build a cognitive code", "author": ["S. Grossberg"], "venue": "Studies of mind and brain (pp. 1-52). Springer Netherlands.", "citeRegEx": "Grossberg,? 1982", "shortCiteRegEx": "Grossberg", "year": 1982}, {"title": "Do threatening stimuli draw or hold visual attention in subclinical anxiety", "author": ["E. Fox", "R. Russo", "R. Bowles", "K. Dutton"], "venue": "Journal of Experimental Psychology: General, 130(4), 681.", "citeRegEx": "Fox et al\\.,? 2001", "shortCiteRegEx": "Fox et al\\.", "year": 2001}, {"title": "Discovering affective regions in deep convolutional neural networks for visual sentiment prediction", "author": ["M. Sun", "J. Yang", "K. Wang", "Shen. H"], "venue": "In IEEE International Conference on Multimedia and Expo (ICME) (pp. 1-6)", "citeRegEx": "Sun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2016}, {"title": "What is an object", "author": ["B. Alexe", "T. Deselaers", "V. Ferrari"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on (pp. 73-80). IEEE.", "citeRegEx": "Alexe et al\\.,? 2010", "shortCiteRegEx": "Alexe et al\\.", "year": 2010}, {"title": "Where do emotions come from? Predicting the Emotion Stimuli Map", "author": ["K.C. Peng", "A. Sadovnik", "A. Gallagher", "T. Chen"], "venue": "In IEEE International Conference on Image Processing (ICIP) (pp. 614-618)", "citeRegEx": "Peng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2016}, {"title": "Sentribute: image sentiment analysis from a mid-level perspective", "author": ["J. Yuan", "S. Mcdonough", "Q. You", "J. Luo"], "venue": "Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining (p. 10). ACM.", "citeRegEx": "Yuan et al\\.,? 2013", "shortCiteRegEx": "Yuan et al\\.", "year": 2013}, {"title": "Object Detectors Emerge in Deep Scene CNNs", "author": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "Unconstrained salient object detection via proposal subset optimization", "author": ["J. Zhang", "S. Sclaroff", "Z. Lin", "X. Shen", "B. Price", "R. Mech"], "venue": "Proceeding of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "From Pixels to Sentiment: Fine-tuning CNNs for Visual Sentiment Prediction", "author": ["V. Campos", "B. Jou", "X. Giro-i-Nieto"], "venue": "arXiv preprint arXiv:1604.03489.", "citeRegEx": "Campos et al\\.,? 2016", "shortCiteRegEx": "Campos et al\\.", "year": 2016}, {"title": "Building a large scale dataset for image emotion recognition: The fine print and the benchmark", "author": ["Q. You", "J. Luo", "H. Jin", "J. Yang"], "venue": "the Thirtieth AAAI Conference on Artificial Intelligence (AAAI).", "citeRegEx": "You et al\\.,? 2016", "shortCiteRegEx": "You et al\\.", "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "Proceedings of the 22nd ACM international conference on Multimedia (pp. 675-678). ACM.", "citeRegEx": "Jia et al\\.,? 2014", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Sun attribute database: Discovering, annotating, and recognizing scene attributes", "author": ["G. Patterson", "J. Hays"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on (pp. 2751-2758). IEEE.", "citeRegEx": "Patterson and Hays,? 2012", "shortCiteRegEx": "Patterson and Hays", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": ", pixel level features such as color, texture and composition (Machajdik and Hanbury 2010), were designed to study the emotional reaction to visual content.", "startOffset": 62, "endOffset": 90}, {"referenceID": 1, "context": "Similar methods were pervasive for a while until recently, when large scale image datasets such as ImageNet (Deng et al. 2009) and Places Dataset (Zhou et al.", "startOffset": 108, "endOffset": 126}, {"referenceID": 2, "context": "2009) and Places Dataset (Zhou et al. 2014) became available.", "startOffset": 25, "endOffset": 43}, {"referenceID": 4, "context": "The power of deep learning, especially convolutional neural networks, has been harnessed recently to discover the sentiment carried in images (You et al. 2015; Jindal and Singh 2015).", "startOffset": 142, "endOffset": 182}, {"referenceID": 3, "context": "The power of deep learning, especially convolutional neural networks, has been harnessed recently to discover the sentiment carried in images (You et al. 2015; Jindal and Singh 2015).", "startOffset": 142, "endOffset": 182}, {"referenceID": 2, "context": "Recently, inspired by convolutional neural network\u2019s state-of-the-art performance on various computer vision tasks such as image classification (Krizhevsky, Sutskever, and Hinton 2012) and scene recognition (Zhou et al. 2014), researchers start to utilize CNN to capture high-level visual concepts and produce saliency models with better detection performance (Wang et al.", "startOffset": 207, "endOffset": 225}, {"referenceID": 6, "context": "2014), researchers start to utilize CNN to capture high-level visual concepts and produce saliency models with better detection performance (Wang et al. 2015; Zhang et al. 2015).", "startOffset": 140, "endOffset": 177}, {"referenceID": 7, "context": "2014), researchers start to utilize CNN to capture high-level visual concepts and produce saliency models with better detection performance (Wang et al. 2015; Zhang et al. 2015).", "startOffset": 140, "endOffset": 177}, {"referenceID": 8, "context": "There is an ongoing discrepancy between those who suggest that emotional perception is automatic, namely in the manner that it is independent of top-down factors such as attention (Vuilleumier et al. 2001), and those illustrating the dependence on attention (Grossberg, 1980; Fox et al.", "startOffset": 180, "endOffset": 205}, {"referenceID": 10, "context": "2001), and those illustrating the dependence on attention (Grossberg, 1980; Fox et al. 2001).", "startOffset": 58, "endOffset": 92}, {"referenceID": 2, "context": "(2015)\u2019s work, they learn deep features for images from the Places Dataset (Zhou et al. 2014) and are able to generate receptive field of the image for scene recognition task.", "startOffset": 75, "endOffset": 93}, {"referenceID": 10, "context": "Sun et al. (2016) proposed the notion of AR, Affective Region, which is a specific region in the image that contains one or more salient objects that can attract viewers\u2019 attention and carry significant emotion.", "startOffset": 0, "endOffset": 18}, {"referenceID": 10, "context": "Sun et al. (2016) proposed the notion of AR, Affective Region, which is a specific region in the image that contains one or more salient objects that can attract viewers\u2019 attention and carry significant emotion. In their work, an offthe-shelf object detection algorithm (Alexe, Deselaers and Ferrari 2010) is adopted to generate random proposals, and then the sentiment of both the proposals and the entire image will be evaluated to discover affective regions. However, this method utilizes explicit object recognition, which has to generate thousands of proposal windows in order to yield to a high recall rate of affective regions proposals. It also incurs a high computational overhead and may result in regions containing tiny objects that do not even catch people\u2019s attention. In Peng et al. (2016)\u2019s work, they try to predict an Emotion Stimuli Map (ESM), which describes pixels-wise contribution to emotion.", "startOffset": 0, "endOffset": 805}, {"referenceID": 10, "context": "Sun et al. (2016) proposed the notion of AR, Affective Region, which is a specific region in the image that contains one or more salient objects that can attract viewers\u2019 attention and carry significant emotion. In their work, an offthe-shelf object detection algorithm (Alexe, Deselaers and Ferrari 2010) is adopted to generate random proposals, and then the sentiment of both the proposals and the entire image will be evaluated to discover affective regions. However, this method utilizes explicit object recognition, which has to generate thousands of proposal windows in order to yield to a high recall rate of affective regions proposals. It also incurs a high computational overhead and may result in regions containing tiny objects that do not even catch people\u2019s attention. In Peng et al. (2016)\u2019s work, they try to predict an Emotion Stimuli Map (ESM), which describes pixels-wise contribution to emotion. They use a pre-trained and fine-tuned model to predict ESM and conclude that neither saliency nor objectness can correctly predict the image regions that evoke emotion. However, they fail to justify the choice of a dataset that consists of predominantly landscape scenes, and does not give any insight on why the saliency detection method does not work for detecting affective emotion region of such images. Yuan et al. (2013) extract scene descriptor low-level features from the SUN Dataset (Xiao et al.", "startOffset": 0, "endOffset": 1343}, {"referenceID": 2, "context": "In Zhou et al. (2015)\u2019s work, they learn deep features for images from the Places Dataset (Zhou et al.", "startOffset": 3, "endOffset": 22}, {"referenceID": 16, "context": "Details of the steps are as follows: Step 1: For a given image, we use a state-of-the-art saliency detection model (Zhang et al. 2016) to detect salient object(s), if any, for each image.", "startOffset": 115, "endOffset": 134}, {"referenceID": 4, "context": "Dataset We use the data set released by You et al. (2016) for visual sentiment prediction.", "startOffset": 40, "endOffset": 58}, {"referenceID": 16, "context": "We adopt a state-of-the-art saliency detection model (Zhang et al. 2016) to detect whether an image has any salient object and if there is, locate those salient objects.", "startOffset": 53, "endOffset": 72}, {"referenceID": 2, "context": "For each partition we obtain from the previous step, we fine-tune a state of the art scene recognition model, PlacesCNN (Zhou et al. 2014), to further classify it into indoors and outdoors.", "startOffset": 120, "endOffset": 138}, {"referenceID": 19, "context": "This model has the same architecture as the one used in the Caffe reference network (Jia et al. 2014), and it is trained on the Places Database (Zhou et al.", "startOffset": 84, "endOffset": 101}, {"referenceID": 2, "context": "2014), and it is trained on the Places Database (Zhou et al. 2014), which is a benchmark for scene recognition.", "startOffset": 48, "endOffset": 66}, {"referenceID": 20, "context": "Next, we use the deep features from the last fully connected layers to detect 102 SUN scene attributes (Patterson and Hays 2012).", "startOffset": 103, "endOffset": 128}], "year": 2016, "abstractText": "Sentiment analysis is crucial for extracting social signals from social media content. Due to the prevalence of images in social media, image sentiment analysis is receiving increasing attention in recent years. However, most existing systems are black-boxes that do not provide insight on how image content invokes sentiment and emotion in the viewers. Psychological studies have confirmed that salient objects in an image often invoke emotions. In this work, we investigate more fine-grained and more comprehensive interaction between visual saliency and visual sentiment. In particular, we partition images in several primary scene-type dimensions, including: open-closed, natural-manmade, indooroutdoor, and face-noface. Using state of the art saliency detection algorithm and sentiment classification algorithm, we examine how the sentiment of the salient region(s) in an image relates to the overall sentiment of the image. The experiments on a representative image emotion dataset have shown interesting correlation between saliency and sentiment in different scene types and in turn shed light on the mechanism of visual sentiment evocation.", "creator": "Word"}}}