{"id": "1611.04887", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Interpreting the Syntactic and Social Elements of the Tweet Representations via Elementary Property Prediction Tasks", "abstract": "Research in social media analysis is experiencing a recent surge with a large number of works applying representation learning models to solve high-level syntactico-semantic tasks such as sentiment analysis, semantic textual similarity computation, hashtag prediction and so on. Although the performance of the representation learning models are better than the traditional baselines for the tasks, little is known about the core properties of a tweet encoded within the representations. Understanding these core properties would empower us in making generalizable conclusions about the quality of representations. Our work presented here constitutes the first step in opening the black-box of vector embedding for social media posts, with emphasis on tweets in particular.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 15 Nov 2016 15:34:47 GMT  (11kb)", "http://arxiv.org/abs/1611.04887v1", "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems"]], "COMMENTS": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems", "reviews": [], "SUBJECTS": "cs.CL cs.SI", "authors": ["j ganesh", "manish gupta", "vasudeva varma"], "accepted": false, "id": "1611.04887"}, "pdf": {"name": "1611.04887.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["ganesh.j@research.iiit.ac.in", "gmanish@microsoft.com", "vv@iiit.ac.in"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n04 88\n7v 1\n[ cs\n.C L\n] 1\n5 N\nov 2\n01 6\nInterpreting the Syntactic and Social Elements of the Tweet Representations via Elementary Property\nPrediction Tasks\nGanesh J IIIT Hyderabad, India\nganesh.j@research.iiit.ac.in Manish Gupta Microsoft, Hyderabad, India gmanish@microsoft.com Vasudeva Varma IIIT Hyderabad, India vv@iiit.ac.in"}, {"heading": "1 Introduction", "text": "Research in social media analysis is recently seeing a surge in the number of research works applying representation learning models to solve high-level syntactico-semantic tasks such as sentiment analysis [1], semantic textual similarity computation [2], hashtag prediction [3] and so on. Though the performance of the representation learning models are better than the traditional models for all the tasks, little is known about the core properties of a tweet encoded within the representations. In a recent work, Hill et al. [4] perform a comparison of different sentence representation models by evaluating them for different high-level semantic tasks such as paraphrase identification, sentiment classification, question answering, document retrieval and so on. This type of coarse-grained analysis is opaque as it does not clearly reveal the kind of information encoded by the representations. Our work presented here constitutes the first step in opening the black-box of vector embeddings for social media posts, particularly tweets.\nEssentially we ask the following question: \u201cWhat are the core properties encoded in the given tweet representation?\u201d. We explicitly group the set of these properties into two categories: syntactic and social. Syntactic category includes properties such as tweet length, the order of words in it, the words themselves, slang words, hashtags and named entities in the tweet. On the other hand, social properties consist of \u2018is reply\u2019, and \u2018reply time\u2019. We investigate the degree to which the tweet representations encode these properties. We assume that if we cannot train a classifier to predict a property based on its tweet representation, then this property is not encoded in this representation. For example, the model which preserves the tweet length should perform well in predicting the length given the representation generated from the model. Though these elementary property prediction tasks are not directly related to any downstream application, knowing that the model is good at modeling a particular property (e.g., the social properties) indicates that it could excel in correlated applications (e.g., user profiling task). In this work we perform an extensive evaluation of 9 unsupervised and 4 supervised tweet representation models, using 8 different properties. The most relevant work is that of Adi et al. [5], which investigates three sentence properties in comparing unsupervised sentence representation models such as average of words vectors and LSTM auto-encoders. We differ from their work in two ways: (1) While they focus on sentences, we focus on social media posts which opens up the challenge of considering multiple salient properties such as hashtags, named entities, conversations and so on. (2) While they work with only unsupervised representation-learning models, we investigate the traditional unsupervised methods (BOW, LDA), unsupervised representation learning methods (Siamese CBOW, Tweet2Vec), as well as supervised methods (CNN, BLSTM).\nOur main contributions are summarized below.\n\u2022 Our work is the first towards interpreting the tweet embeddings in a fine-grained fashion. To this end, we propose a set of tweet-specific elementary property prediction tasks which help in unearthing the basic characteristics of different tweet representations.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\n\u2022 To the best of our knowledge, this work is the first to do a holistic study of traditional, unsupervised and supervised representation learning models for tweets.\n\u2022 We compare various tweet representations with respect to such properties across various dimensions like tweet length and word ordering sensitivity.\nThe paper is organized as follows. Sections 2 and 3 discuss the set of proposed elementary property prediction tasks and the models considered for this study respectively. Section 4 and 5 presents the experiment setup and result analysis respectively. We conclude the work with a brief summary in Section 5."}, {"heading": "2 Elementary Property Prediction Tasks", "text": "In this section we list down the set of proposed elementary property prediction tasks to test the characteristics of a tweet embedding. Table 1 explains all the tasks considered in this study. Note that we use a neural network to build the elementary property prediction task classifier which has the following two layers in order: the representation layer and the softmax layer on top whose size varies according to the specific task. When there are more than one input for a task, we concatenate embeddings for each input."}, {"heading": "3 Representation Models", "text": "In this section we list down the set of models considered in the study."}, {"heading": "3.1 Unsupervised", "text": "\u2022 Bag Of Words (BOW) [17] - This simple representation captures the TF-IDF value of an n-gram. We pick top 50K n-grams, with the value of \u2018n\u2019 going up to 5.\n\u2022 Latent Dirichlet Allocation (LDA) [18] - We use the topic distribution resulting by running LDA with number of topics as 200, as tweet representation.\n\u2022 Bag Of Means (BOM) - We take the average of the word embeddings obtained by running the Glove [12] model on 2 billion tweets with embedding size as 200.\n\u2022 Deep Structured Semantic Models (DSSM) [9] - This is a deep encoder trained to represent query and document in common space, for document ranking. We use the publicly available pre-trained encoder to encode the tweets.\n\u2022 Convolutional DSSM (CDSSM) [10] - This is the convolutional variant of DSSM.\n\u2022 Paragraph2Vec (PV) [13] - This model based on Word2Vec [15] learns embedding for a document which is good in predicting the words within it. We use the BOW variant with embedding size and window size of 200 and 10 respectively.\n\u2022 Skip-Thought Vectors (STV) [6] - This is a GRU [16] encoder trained to predict adjacent sentences in a books corpus. We use the recommended combine-skip (4800-dimensional) vectors from the publicly available encoder.\n1https://noisy-text.github.io/norm-shared-task.html\n\u2022 Tweet2Vec (T2V) [3] - This is a character composition model working directly on the character sequences to predict the user-annotated hashtags in a tweet. We use publicly available encoder, which was trained on 2 million tweets.\n\u2022 Siamese CBOW (SCBOW) [2] - This model uses averaging of word vectors to represent a sentence, and the objective and data used here is the same as that for STV. Note that this is different from BOW because the word vectors here are optimized for sentence representation."}, {"heading": "3.2 Supervised", "text": "\u2022 Convolutional Neural Network (CNN) - This is a simple CNN proposed in [7].\n\u2022 Long Short Term Memory Network (LSTM) [14] - This is a vanilla LSTM based recurrent model, applied from start to the end of a tweet, and the last hidden vector is used as tweet representation.\n\u2022 Bi-directional LSTM (BLSTM) [14] - This extends LSTM by using two LSTM networks, processing a tweet left-to-right and right-to-left respectively. Tweet is represented by concatenating the last hidden vector of both the LSTMs.\n\u2022 FastText (FT) [8] - This is a simple architecture which averages the n-gram vectors to represent a tweet, followed by the softmax in the final layer. This simple model has been shown to be effective for the text classification task."}, {"heading": "4 Experiments", "text": "In this section we perform an extensive evaluation of all the models in an attempt to find the significance of different representation models. Essentially we study every model (with optimal settings reported in the corresponding paper) with respect to the following three perspectives.\n1. Property Prediction Task Accuracy - This test identifies the model with the best F1-score for each elementary property prediction task.\n(a) Best of all in: Property prediction tasks for which this model has outperformed all the other models.\n(b) Best of unsupervised approaches in: Property prediction tasks for which this model has outperformed all the other unsupervised models.\n(c) Best of supervised approaches in: Property prediction tasks for which this model has outperformed all the other supervised models.\n2. Property Prediction Task Accuracy versus Tweet Length - This test helps to compare the performance of the model for shorter and longer tweets.\n(a) Positively correlated tasks: Property prediction tasks for which the performance of the model increases as tweet length increases.\n(b) Negatively correlated tasks: Property prediction tasks for which the performance of the model decreases as tweet length increases.\n3. Sensitivity of Property Prediction Task to Word Order - This refers to the setting where the words in the tweets are randomly ordered. This helps in testing the extent to which a model relies on the word ordering properties of the natural language.\n(a) Invariant tasks: Property prediction tasks for which the model performance does not decline even when the words in the tweets are randomly reordered.\n(b) Significantly deviant tasks: Property prediction tasks for which the model performance declines significantly when the words in the tweets are randomly reordered."}, {"heading": "5 Results and Analysis", "text": "Fine-grained analysis of various supervised and unsupervised models discussed in Section 3, across various dimensions discussed in Section 4, is presented in Table 2. The codes used to conduct our experiments are publicly accessible at: https://github.com/ganeshjawahar/fine-tweet/."}, {"heading": "5.1 Property Prediction Task Accuracy", "text": "We summarize the results of property prediction tasks in Table 3. Length prediction turns out to be a difficult task for most of the models. Models which rely on the recurrent architectures such as LSTM, STV, T2V have sufficient capacity to perform well in modeling the tweet length. Also BLSTM is the best in modeling slang words. BLSTM outperforms the LSTM variant in all the tasks except \u2018Content\u2019, which signifies the power of using the information flowing from both the directions of the tweet. T2V which is expected to perform well in this task because of its ability to work at a more fine level (i.e., characters) performs the worst. In fact T2V does not outperform other models in any task, which could be mainly due to the fact that the hashtags which are used for supervision in learning tweet representations reduces the generalization capability of the tweets beyond hashtag prediction. Prediction tasks such as \u2018Content\u2019 and \u2018Hashtag\u2019 seem to be less difficult as all the models perform nearly optimal for them. The superior performance of all the models for the \u2018Content\u2019 task in particular is unlike the relatively lower performance reported for in [5], mainly because of the short length of the tweets. The most surprising result is when the BOM model turned out to be the best in \u2018Word Order\u2019 task, as the model by nature loses the word order. This might be due to the correlation between word order patterns and the occurrences of specific words. BOM has also proven to perform well for identifying the named entities in the tweet.\nSTV is good for most of the social tasks. We believe the main reason for STV\u2019s performance is two-fold: (a) the inter-sentential features extracted from STV\u2019s encoder by the prediction of the surrounding sentences in the books corpus contains rich social elements that are vital for social tasks (e.g., user profiling), (b) the recurrent structure in both the encoder and decoder persists useful information in the memory nicely. The second claim is further substantiated by observing the poor performance of SCBOW whose objective is also similar to STV, but with a simpler architecture (i.e., word vector averaging). In future it would be interesting to create such a model for Twitter conversations or chronologically ordered topical tweets so as to directly capture the latent social features from Twitter."}, {"heading": "5.2 Property Prediction Task Accuracy versus Tweet Length", "text": "This setup captures the behavior of the model with the increase in the context size, which is defined in terms of number of words. For \u2018Word Order\u2019 task, we see the performance of all the models to be negatively correlated with the tweet length, which is expected. On the other hand, there is no correlation between the tweet length and the performance of all the models for the tasks such as \u2018Slang Words\u2019, \u2018Content\u2019, \u2018Hashtag\u2019, \u2018NE\u2019, and \u2018Is Reply\u2019. For social tasks such as \u2018Is Reply\u2019 and \u2018Reply Time\u2019, we see a positive correlation between the tweet length and the performance of all the models. This finding is intuitive in social media analysis where additional context is mostly helpful in modeling the social behavior."}, {"heading": "5.3 Sensitivity of Property Prediction Task to Word Order", "text": "This test essentially captures the importance of \u201cnatural word order\u201d. We found that LDA was invariant to the reordering of the words in the tweet for most of the tasks. This result is not surprising as LDA considers each word in the tweet independently. CNN, LSTM and BLSTM rely on the word order significantly to perform well for most of the prediction tasks."}, {"heading": "6 Conclusion", "text": "This work proposed a set of elementary property prediction tasks to understand different tweet representations in an application independent, fine-grained fashion. The open nature of social media not only poses a plethora of opportunities to understand the basic characteristics of the posts, but also helped us draw novel insights about different representation models. We observed that among supervised models, CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models. Tweet length affects the task prediction accuracies, but we found that all models behave similarly under variation in tweet length. Finally while LDA is insensitive to input word order, CNN, LSTM and BLSTM are extremely sensitive to word order."}], "references": [{"title": "Sentiment Embeddings with Applications to Sentiment Analysis. In: TKDE", "author": ["D. Tang", "F. Wei", "B. Qin", "N. Yang", "T. Liu", "M. Zhou"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations", "author": ["T. Kenter", "A. Borisov", "M. de Rijke"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Tweet2Vec: Character- Based Distributed Representations for Social Media", "author": ["B. Dhingra", "Z. Zhou", "D. Fitzpatrick", "M. Muehl", "W.W. Cohen"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["F. Hill", "K. Cho", "A. Korhonen"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "author": ["Y. Adi", "E. Kermany", "Y. Belinkov", "O. Lavi", "Y. Goldberg"], "venue": "arXiv preprint arXiv:1608.04207", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "EMNLP", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Bag of Tricks for Efficient Text Classification", "author": ["A. Joulin", "E. Grave", "P. Bojanowski", "T. Mikolov"], "venue": "arXiv preprint arXiv:1607.01759", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["P.S. Huang", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "A latent semantic model with convolutionalpooling structure for information retrieval", "author": ["Y. Shen", "X. He", "J. Gao", "L. Deng", "G. Mesnil"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Named entity recognition in tweets: an experimental study", "author": ["A. Ritter", "S. Clark", "Mausam", "O. Etzioni"], "venue": "EMNLP", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Glove: Global Vectors for Word Representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "In: ICML", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks. In: ICASSP", "author": ["A. Graves", "A.R. Mohamed", "G. Hinton"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Latent dirichlet allocation. In: JMLR", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Research in social media analysis is recently seeing a surge in the number of research works applying representation learning models to solve high-level syntactico-semantic tasks such as sentiment analysis [1], semantic textual similarity computation [2], hashtag prediction [3] and so on.", "startOffset": 206, "endOffset": 209}, {"referenceID": 1, "context": "Research in social media analysis is recently seeing a surge in the number of research works applying representation learning models to solve high-level syntactico-semantic tasks such as sentiment analysis [1], semantic textual similarity computation [2], hashtag prediction [3] and so on.", "startOffset": 251, "endOffset": 254}, {"referenceID": 2, "context": "Research in social media analysis is recently seeing a surge in the number of research works applying representation learning models to solve high-level syntactico-semantic tasks such as sentiment analysis [1], semantic textual similarity computation [2], hashtag prediction [3] and so on.", "startOffset": 275, "endOffset": 278}, {"referenceID": 3, "context": "[4] perform a comparison of different sentence representation models by evaluating them for different high-level semantic tasks such as paraphrase identification, sentiment classification, question answering, document retrieval and so on.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5], which investigates three sentence properties in comparing unsupervised sentence representation models such as average of words vectors and LSTM auto-encoders.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "5 Hashtag Predict whether the word is a hashtag in the tweet or not Randomly choose the word in the tweet which is not a hashtag 6 NE [11] Predict whether the n-gram is a Named Entity (NE) in the tweet or not Randomly choose the n-gram in the tweet which is not a NE", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "\u2022 Latent Dirichlet Allocation (LDA) [18] - We use the topic distribution resulting by running LDA with number of topics as 200, as tweet representation.", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "\u2022 Bag Of Means (BOM) - We take the average of the word embeddings obtained by running the Glove [12] model on 2 billion tweets with embedding size as 200.", "startOffset": 96, "endOffset": 100}, {"referenceID": 7, "context": "\u2022 Deep Structured Semantic Models (DSSM) [9] - This is a deep encoder trained to represent query and document in common space, for document ranking.", "startOffset": 41, "endOffset": 44}, {"referenceID": 8, "context": "\u2022 Convolutional DSSM (CDSSM) [10] - This is the convolutional variant of DSSM.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "\u2022 Paragraph2Vec (PV) [13] - This model based on Word2Vec [15] learns embedding for a document which is good in predicting the words within it.", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "\u2022 Paragraph2Vec (PV) [13] - This model based on Word2Vec [15] learns embedding for a document which is good in predicting the words within it.", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "\u2022 Skip-Thought Vectors (STV) [6] - This is a GRU [16] encoder trained to predict adjacent sentences in a books corpus.", "startOffset": 49, "endOffset": 53}, {"referenceID": 2, "context": "\u2022 Tweet2Vec (T2V) [3] - This is a character composition model working directly on the character sequences to predict the user-annotated hashtags in a tweet.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "\u2022 Siamese CBOW (SCBOW) [2] - This model uses averaging of word vectors to represent a sentence, and the objective and data used here is the same as that for STV.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "\u2022 Convolutional Neural Network (CNN) - This is a simple CNN proposed in [7].", "startOffset": 72, "endOffset": 75}, {"referenceID": 12, "context": "\u2022 Long Short Term Memory Network (LSTM) [14] - This is a vanilla LSTM based recurrent model, applied from start to the end of a tweet, and the last hidden vector is used as tweet representation.", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "\u2022 Bi-directional LSTM (BLSTM) [14] - This extends LSTM by using two LSTM networks, processing a tweet left-to-right and right-to-left respectively.", "startOffset": 30, "endOffset": 34}, {"referenceID": 6, "context": "\u2022 FastText (FT) [8] - This is a simple architecture which averages the n-gram vectors to represent a tweet, followed by the softmax in the final layer.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "The superior performance of all the models for the \u2018Content\u2019 task in particular is unlike the relatively lower performance reported for in [5], mainly because of the short length of the tweets.", "startOffset": 139, "endOffset": 142}], "year": 2016, "abstractText": "Research in social media analysis is recently seeing a surge in the number of research works applying representation learning models to solve high-level syntactico-semantic tasks such as sentiment analysis [1], semantic textual similarity computation [2], hashtag prediction [3] and so on. Though the performance of the representation learning models are better than the traditional models for all the tasks, little is known about the core properties of a tweet encoded within the representations. In a recent work, Hill et al. [4] perform a comparison of different sentence representation models by evaluating them for different high-level semantic tasks such as paraphrase identification, sentiment classification, question answering, document retrieval and so on. This type of coarse-grained analysis is opaque as it does not clearly reveal the kind of information encoded by the representations. Our work presented here constitutes the first step in opening the black-box of vector embeddings for social media posts, particularly tweets.", "creator": "LaTeX with hyperref package"}}}