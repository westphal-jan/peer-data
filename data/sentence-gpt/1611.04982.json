{"id": "1611.04982", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Oracle Complexity of Second-Order Methods for Finite-Sum Problems", "abstract": "Finite-sum optimization problems are ubiquitous in machine learning, and are commonly solved using first-order methods which rely on gradient computations. Recently, there has been growing interest in \\emph{second-order} methods, which rely on both gradients and Hessians. In principle, second-order methods can require much fewer iterations than first-order methods, and hold the promise for more efficient algorithms. Although computing and manipulating Hessians is prohibitive for high-dimensional problems in general, the Hessians of individual functions in finite-sum problems can often be efficiently computed, e.g. because they possess a low-rank structure. Can second-order information indeed be used to solve such problems more efficiently? In this paper, we provide evidence that the answer -- perhaps surprisingly -- is negative, at least in terms of worst-case guarantees. However, we also discuss what additional assumptions and algorithmic approaches might potentially circumvent this negative result. The paper is available in the full text of the paper here.", "histories": [["v1", "Tue, 15 Nov 2016 18:41:55 GMT  (24kb)", "http://arxiv.org/abs/1611.04982v1", "23 pages"], ["v2", "Wed, 8 Mar 2017 11:05:59 GMT  (29kb)", "http://arxiv.org/abs/1611.04982v2", "30 pages"]], "COMMENTS": "23 pages", "reviews": [], "SUBJECTS": "math.OC cs.LG stat.ML", "authors": ["yossi arjevani", "ohad shamir"], "accepted": true, "id": "1611.04982"}, "pdf": {"name": "1611.04982.pdf", "metadata": {"source": "CRF", "title": "Oracle Complexity of Second-Order Methods for Finite-Sum Problems", "authors": ["Yossi Arjevani"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n04 98\n2v 1\n[ m\nat h.\nO C\n] 1\n5 N"}, {"heading": "1 Introduction", "text": "We consider finite-sum problems of the form\nmin w\u2208W\nF (w) = 1\nn\nn \u2211\ni=1\nfi(w), (1)\nwhere W is a closed convex subset of some Euclidean or Hilbert space, each fi is convex and \u00b5-smooth, and F is \u03bb-strongly convex1. Such problems are ubiquitous in machine learning, for example in order to perform empirical risk minimization using convex losses.\nTo study the complexity of this and other optimization problems, it is common to consider an oracle model, where the optimization algorithm has no a-priori information about the objective function, and obtains information from an oracle which provides values and derivatives of the function at various domain points [Nemirovsky and Yudin, 1983]. The complexity of the algorithm is measured in terms of the number of oracle calls required to optimize the function to within some prescribed accuracy.\nExisting lower bounds for finite-sum problems show that using a first-order oracle, which given a point w and index i = 1, . . . , n returns fi(w) and \u2207fi(w), the number of oracle queries required to find an \u01eb-optimal solution is at least of order\n\u2126\n(\nn+\n\u221a\nn\u00b5\n\u03bb log\n(\n1\n\u01eb\n))\n,\n1For a twice-differentiable function f , it is \u00b5-smooth if \u22072f(w) \u00b5I for all w \u2208 W , and \u03bb-strongly convex if \u22072f(w) \u03bbI for all w \u2208 W .\neither under algorithmic assumptions or assuming the dimension is sufficiently large2 [Agarwal and Bottou, 2014, Lan, 2015, Woodworth and Srebro, 2016, Arjevani and Shamir, 2016a]. This is matched (up to log factors) by existing approaches, and cannot be improved in general.\nAn alternative to first-order methods are second-order methods, which also utilize Hessian information. A prototypical example is the Newton method, which given a (single) function F , performs iterations of the form\nwt+1 = wt \u2212 \u03b1t ( \u22072F (w) )\u22121\u2207F (w), (2)\nwhere \u2207F (w),\u22072F (w) are the gradient and the Hessian of F at w, and \u03b1t is a step size parameter. Secondorder methods can have extremely fast convergence, better than those of first-order methods (i.e. quadratic instead of linear). Moreover, they can be invariant to affine transformations of the objective function, and provably independent of its strong convexity and smoothness parameters (assuming e.g. self-concordance) [Boyd and Vandenberghe, 2004]. A drawback of these methods, however, is that they can be computationally prohibitive. In the context of machine learning, we are often interested in high-dimensional problems (where the dimension d is very large), and the Hessians are d\u00d7 d matrices which in general may not even fit into computer memory. However, for optimization problems as in Eq. (1), the Hessians of individual fi often have a special structure. For example, a very common special case of finite-sum problems in machine learning is empirical risk minimization for linear predictors, where fi(w) = \u2113i(\u3008w,xi\u3009), where xi is a training instance and \u2113i is some loss function. In that case, assuming \u2113i is twice-differentiable, the Hessian has the rank-1 form \u2113\u2032\u2032i (\u3008w,xi\u3009)xix\u22a4i . Therefore, the memory and computational effort involved with storing and manipulating the Hessian of this function is merely linear (rather than quadratic) in d. Thus, it is tractable even for high-dimensional problems.\nBuilding on this, several recent papers proposed and analyzed second-order methods for finite-sum problems, which utilize Hessians of the individual functions fi (see for instance Erdogdu and Montanari [2015], Agarwal et al. [2016], Pilanci and Wainwright [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016], Xu et al. [2016] and references therein). These can all be viewed as approximate Newton methods, which replace the actual Hessian \u22072F (w) = 1n \u2211n i=1 \u22072fi(w) in Eq. (2) by some approximation, based for instance on the Hessians of a few individual functions fi sampled at random. One may hope that such methods can inherit the favorable properties of second-order methods, and improve on the performance of commonly used first-order methods.\nIn this paper, we consider the opposite direction, and study lower bounds on the number of iterations required by algorithms using second-order (or possibly even higher-order) information, focusing on finitesum problems which are strongly-convex and smooth. We make the following contributions:\n\u2022 First, as a more minor contribution, we prove that in the standard setting of optimizing a single smooth and strongly convex function, second-order information cannot improve the oracle complexity compared to first-order methods (at least in high dimensions). Although this may seem unexpected at first, the reason is that the smoothness constraint must be extended to higher-order derivatives, in order for higher-order information to be useful. We note that this observation in itself is not new, and is briefly mentioned (without proof) in Nemirovsky and Yudin [1983, Section 7.2.6]. Our contribution here is in providing a clean, explicit statement and proof of this result.\n\u2022 We then turn to present our main results, which state (perhaps surprisingly) that under some mild algorithmic assumptions, and if the dimension is sufficiently large, the oracle complexity of second-\n2Depending on how \u01eb-optimality is defined precisely, and where the algorithm is assumed to start, these bounds may have additional factors inside the log. For simplicity, we present the existing bounds assuming \u01eb is sufficiently small, so that a log(1/\u01eb) term dominates.\norder methods for finite-sum problems is no better than first-order methods, even if the finite-sum problem is composed of quadratics (which are trivially smooth to any order).\n\u2022 Despite this pessimistic conclusion, our results also indicate what assumptions and algorithmic approaches might be helpful in circumventing it. In particular, it appears that better, dimension-dependent performance may be possible, if the dimension is moderate and the n individual functions in Eq. (1) are accessed in a non-oblivious way, which depends on the functions rather than fixed in advance (e.g. sampling them from a non-uniform distribution depending on their Hessians, as opposed to sampling them uniformly at random). This provides evidence to the necessity of non-oblivious sampling schemes, and a dimension-dependent analysis, which indeed accords with some recently proposed algorithms and derivations, e.g. Agarwal et al. [2016], Xu et al. [2016]. We note that the limitations arising from oblivious optimization schemes (in a somewhat stronger sense) was also explored in Arjevani and Shamir [2016a,b].\nThe paper is structured as follows: We begin in Sec. 2 with a lower bound for algorithms utilizing second-order information, in the simpler setting where there is a single function F to be optimized, rather than a finite-sum problem. We then turn to provide our main lower bounds in Sec. 3, and discuss their applicability to some existing approaches in Sec. 4. We conclude in Sec. 5, where we also discuss possible approaches to circumvent our lower bounds. The proofs of our results appear in Appendix A."}, {"heading": "2 Strongly Convex and Smooth Optimization with a Second-Order Oracle", "text": "Before presenting our main results for finite-sum optimization problems, we consider the simpler problem of minimizing a single strongly-convex and smooth function F (or equivalently, Eq. (1) when n = 1), and prove a result which may be of independent interest.\nTo formalize the setting, we follow a standard oracle model, and assume that the algorithm does not have a-priori information on the objective function F , except the strong-convexity parameter \u03bb and smoothness parameter \u00b5. Instead, it has access to an oracle, which given a point w \u2208 W , returns values and derivatives of F at w (either \u2207F (w) for a first-order oracle, or \u2207F (w),\u22072F (w) for a second-order oracle). The algorithm sequentially queries the oracle using w1,w2, . . . ,wT\u22121, and returns the point wT . Our goal is to lower bound the number of oracle calls T , required to ensure that wT is an \u01eb-suboptimal solution.\nGiven a first-order oracle and a strongly convex and smooth objective in sufficiently high dimensions, it is well-known that the worst-case oracle complexity is \u2126( \u221a\n\u00b5/\u03bb \u00b7 log(1/\u01eb)) [Nemirovsky and Yudin, 1983]. What if we replace this by a second-order oracle, which returns both \u22072F (w) on top of F (w),\u2207F (w)?\nPerhaps unexpectedly, it turns out that this additional information does not substantially improve the worst-case oracle complexity bound, as evidenced by the following theorem:\nTheorem 1. For any \u00b5, \u03bb such that \u00b5 > 8\u03bb > 0, any \u01eb \u2208 (0, 1), and any deterministic algorithm, there exists a \u00b5-smooth, \u03bb strongly-convex function F on Rd (for d = O\u0303( \u221a\n\u00b5/\u03bb), hiding factors logarithmic in \u00b5, \u03bb, \u01eb), such that the number of calls T to a second-order oracle, required to ensure that F (wT ) \u2212 min\nw\u2208Rd F (w) \u2264 \u01eb \u00b7 (F (0) \u2212minw\u2208Rd F (w)), must be at least\nc\n( \u221a\n\u00b5\n8\u03bb \u2212 1\n) \u00b7 log ( (\u03bb/\u00b5)3/2\nc\u2032\u01eb\n)\n,\nwhere c, c\u2032 are positive universal constants.\nFor sufficiently large \u00b5\u03bb and small \u01eb, this complexity lower bound is \u2126 (\u221a \u00b5 \u03bb \u00b7 log ( 1 \u01eb ) ) , which matches existing lower and upper bounds for optimizing strongly-convex and smooth functions using first-order methods. As mentioned earlier, the observation that such first-order oracle bounds can be extended to higherorder oracles is also briefly mentioned (without proof) in Nemirovsky and Yudin [1983, Section 7.2.6]. Also, the theorem considers deterministic algorithms (which includes standard second-order methods, such as the Newton method), but otherwise makes no assumption on the algorithm. Generalizing this result to randomized algorithms should be quite doable, based on the techniques developed in Woodworth and Srebro [2016]. We leave a formal derivation to future work.\nAlthough this result may seem surprising at first, it has a simple explanation: In order for Hessian information, which is local in nature, to be useful, there should be some Lipschitz constraint on the Hessian, which ensures that it cannot change arbitrarily quickly as we move around the domain (e.g. \u2016\u22072F (w) \u2212 \u22072F (w\u2032)\u2016 \u2264 L\u2016w\u2212w\u2032\u2016 for some constant L). Indeed, the construction relies on a function which does not have Lipschitz Hessians: It is based on a standard lower bound construction for first-order oracles, but the function is locally \u201cflattened\u201d in certain directions around points which are to be queried by the algorithm. This is done in such a way, that the Hessian observed by the algorithm does not provide more information than the gradient, and cannot be used to improve the algorithm\u2019s performance."}, {"heading": "3 Second-Order Oracle Complexity Bounds for Finite-Sum Problems", "text": "We now turn to study finite-sum problems of the form given in Eq. (1), and provide lower bounds on the number of oracle calls required to solve them, assuming a second-order oracle. To adapt the setting to a finite-sum problem, we assume that the second-order oracle is given both a point w and an index i \u2208 {1, . . . , n}, and returns {fi(w),\u2207fi(w),\u22072fi(w)}. The algorithm iteratively produces and queries the oracle with point-index pairs {(wt, it)}Tt=1, with the goal of making the suboptimality (or expected suboptimality, if the algorithm is randomized) smaller than \u01eb using a minimal number of oracle calls T .\nIn fact, the lower bound construction we use is such that each function fi is quadratic. Unlike the construction of the previous section, such functions have a constant (and hence trivially Lipschitz) Hessian. Moreover, since any p-order derivative of a quadratic for p > 2 is zero, this means that our lower bounds automatically hold even if the oracle provides p-th order derivatives at any w, for arbitrarily large p.\nHowever, in order to provide a lower bound using quadratic functions, it is necessary to pose additional assumptions on the structure of the algorithm (unlike Thm. 1 which is purely information-based). To see why, note that without computational constraints, the algorithm can simply query the Hessians and gradients of each fi(w) at w = 0, take the average to get \u2207F (0) = 1n \u2211n i=1\u2207fi(0) and \u22072F (0) = 1 n \u2211n i=1\u22072fi(0), and return the exact optimum, which for quadratics equals \u22072F (0)\u22121\u2207F (0). Therefore, with second-order information, the best possible information-based lower bound for quadratics is no better than \u2126(n). This is not a satisfying bound, since in order to attain it we need to invert the (possibly high-rank) d \u00d7 d matrix \u22072F (0). Therefore, if we are interested in bounds for computationally-efficient algorithms, we need to forbid such operations.\nSpecifically, we will make two assumptions about the algorithm, which are stated below (their applicability to existing algorithms is discussed in the next section). The first assumption constrains the indices chosen by the algorithm to be oblivious, in the following sense:\nAssumption 1 (Index Obliviousness). The indices i1, i2, . . . chosen by the algorithm are independent of f1, . . . , fn.\nTo put this assumption differently, the indices may just as well be chosen before the algorithm begins querying the oracle. This can include, for instance, sampling functions fi uniformly at random from f1, . . . , fn, and performing deterministic passes over f1, . . . , fn in order. All optimal first-order algorithms, as well as most second-order methods we are aware of, fall into this framework. Some second-order methods are based on non-uniform sampling schemes, which are covered by the following relaxation:\nAssumption 1a (Partial Index Obliviousness). The indices i1, . . . , iT chosen by the algorithm depend on f1, . . . , fn only through {\u22072f1(w), . . . ,\u22072fn(w)}w\u2208W .\nWe will show that one can use Assumption 1a in lieu of Assumption 1, at the cost of a slightly more complicated construction.\nThe second assumption we use constrains the algorithm to query and return points w which are computable using linear-algebraic manipulations of previous points, gradients and Hessians. Moreover, these manipulations can only depend on (at most) the last \u230an/2\u230b Hessians returned by the oracle. As discussed previously, this assumption is necessary to prevent the algorithm from computing and inverting the full Hessian of F , which is computationally prohibitive. Formally, the assumption is the following:\nAssumption 2 (Linear-Algebraic Computations). wt belongs to the set Wt \u2286 Rd, defined recursively as follows: W1 = {0}, and Wt+1 is the closure of the set of vectors derived from Wt \u222a {\u2207fit(wt)} by a finite number of operations of the following form:\n\u2022 w,w\u2032 \u2192 \u03b1w + \u03b1\u2032w\u2032, where \u03b1,\u03b1\u2032 are arbitrary.\n\u2022 w \u2192 Hw, where H is any d\u00d7 d which has the same block-diagonal structure as t\n\u2211\n\u03c4=max{1,t\u2212\u230an/2\u230b+1} \u03b1\u03c4\u22072fi\u03c4 (w\u03c4 ), (3)\nfor some arbitrary {\u03b1\u03c4}.\nThe first bullet allows to take arbitrary linear combinations of previous points and gradients, and already covers standard first-order methods and their variants. As to the second bullet, by \u201csame block-diagonal structure\u201d, we mean that if the matrix in Eq. (3) can be decomposed to r diagonal blocks of size d1, . . . , dr in order, then H can also be decomposed into r blocks of size d1, . . . , dr in order (note that this does not exclude the possibility that each such block is composed of additional sub-blocks). To give a few examples, if we let Ht be the matrix in Eq. (3), then we may have H = Ht, H = H \u22121 t (if Ht is invertible), H = (Ht + D) \u22121 (where D is some arbitrary diagonal matrix, possibly acting as a regularizer), H can be a truncated SVD decomposition of Ht (or again, Ht + D or (Ht + D)\u22121 for some arbitrary diagonal matrix D) or its pseudoinverse, etc. Also, note that the assumption places no limits on the number of such operations allowed between oracle calls. However, crucially, all these operations can be performed starting from a linear combination of at most \u230an/2\u230b recent Hessians. As mentioned earlier, this is necessary, since if we could compute the average of all Hessians, then we could implement the Newton method. It is also realistic, as existing computationally-efficient methods seek to use \u226a n individual Hessians. We note that the choice of \u230an/2\u230b is rather arbitrary, and can be replaced by \u03b1n for any constant \u03b1 \u2208 (0, 1).\nFinally, we mention that the way the assumption is formulated, the algorithm is assumed to \u201cstart\u201d from the origin 0. However, this is merely for simplicity, and can be replaced by any other fixed vector (the lower bound will hold by shifting the constructed \u201chard\u201d function appropriately).\nWith these assumptions stated, we can finally turn to present the main result of this section:\nTheorem 2. For any n > 1, any \u00b5, \u03bb such that \u00b5 \u2265 2\u03bbn > 0, any \u01eb \u2208 (0, c) (for some universal constant c > 0), and any (possibly randomized) algorithm satisfying Assumptions 1 and 2, there exists \u00b5-smooth, \u03bb-strongly convex quadratic functions f1, . . . , fn : Rd \u2192 R (for d = O\u0303(1 + \u221a\n\u00b5/\u03bbn), hiding factors logarithmic in n, \u00b5, \u03bb, \u01eb), such that the number of calls T to a second-order oracle, so that E [F (wT )\u2212minw\u2208Rd F (w)] \u2264 \u01eb \u00b7 (F (0) \u2212minw\u2208Rd F (w)), must be at least\n\u2126\n(\nn+\n\u221a\nn\u00b5\n\u03bb \u00b7 log\n( (\u03bb/\u00b5)3/2 \u221a n\n\u01eb\n))\nComparing this with the (tight) first-order oracle complexity bounds discussed in the introduction, we see that the lower bound is the same up to log-factors, despite the availability of second-order information. In particular, the lower bound exhibits none of the favorable properties associated with full second-order methods, which can compute and invert Hessians of F : Whereas the full Newton method can attain O(log log(1/\u01eb)) rates, and be independent of \u00b5, \u03bb if F satisfies a self-concordance property [Boyd and Vandenberghe, 2004], here we only get a linear O(log(1/\u01eb)) rate, and there is a strong dependence on \u00b5, \u03bb, even though the function is quadratic and hence self-concordant.\nThe proof of the theorem is based on a randomized construction, which can be sketched as follows: We choose indices j1, . . . , jd\u22121 \u2208 {1, . . . , n} independently and uniformly at random, and define\nfi(w) = a \u00b7 w21 + a\u0302 \u00b7 d\u22121 \u2211\nl=1\n1jl=i(wl \u2212 wl+1)2 + a\u0304 \u00b7 w2d \u2212 a\u0303 \u00b7 w1 + \u03bb\n2 \u2016w\u20162,\nwhere 1A is the indicator function of the event A, and a, a\u0302, a\u0304, a\u0303 are parameters chosen based on \u03bb, \u00b5, n. The average function F (w) = 1n \u2211n i=1 fi(w) equals\nF (w) = a \u00b7 w21 + a\u0302 n \u00b7 d\u22121 \u2211\nl=1\n(wl \u2212wl+1)2 + a\u0304 \u00b7 w2d \u2212 a\u0303 \u00b7 w1 + \u03bb\n2 \u2016w\u20162.\nBy setting the parameters appropriately, it can be shown that F is \u03bb-strongly convex and each fi is \u00b5-smooth. Moreover, the optimum of F has the form (q, q2, q3, . . . , qd) for some q > 0. The proof is based on arguing that after T oracle calls, the points computable by any algorithm satisfying Assumptions 1 and 2 must have 0 values at all coordinates larger than some lT , hence the squared distance of wT from the optimum must be at least\n\u2211d i=lT+1 q2i, which leads to our lower bound. Thus, the proof revolves around upper bounding lT . We note that a similar construction of F was used in some previous first-order lower bounds under algorithmic assumptions (e.g. Nesterov [2013], Lan [2015], as well as Arjevani and Shamir [2015] in a somewhat different context). The main difference is in how we construct the individual functions fi, and in analyzing the effect of second-order rather than just first-order information.\nTo upper bound lT , we let lt (where t = 1, . . . , T ) be the largest non-zero coordinate in wt, and track how lt increases with t. The key insight is that if w1, . . . ,wt\u22121 are zero beyond some coordinate l, then any linear combinations of them, as well as multiplying them by matrices based on second-order information, as specified in Assumption 2, will still result in vectors with zeros beyond coordinate l. The only way to \u201cadvance\u201d and increase the set of non-zero coordinates is by happening to query the function fjl . However, since the indices of the queried functions are chosen obliviously, whereas each jl is chosen uniformly at random, the probability of this happening is quite small, of order 1/n. Moreover, we show that even if this event occurs, we are unlikely to \u201cadvance\u201d by more than O(1) coordinates at a time. Thus, the algorithm essentially needs to make \u2126(n) oracle calls in expectation, in order to increase the number of non-zero\ncoordinates by O(1). It can be shown that the number of coordinates needed to get an \u01eb-optimal solution is \u2126\u0303( \u221a\n\u00b5/n\u03bb \u00b7 log(1/\u01eb)) (hiding some log-factors). Therefore, the total number of oracle calls is about n times larger, namely \u2126\u0303( \u221a\nn\u00b5/\u03bb \u00b7 log(1/\u01eb)). To complete the theorem, we also provide a simple and separate \u2126(n) lower bound, which holds since each oracle call gives us information on just one of the n individual functions f1, . . . , fn, and we need some information on most of them in order to get a close-to-optimal solution.\nAs mentioned previously, we can provide a similar result if we replace Assumption 1 by the milder Assumption 1a, which allows the choice of indices to depend on the Hessians of the individual functions:\nTheorem 3. Thm. 2 also holds (for some finite dimension) if we replace Assumption 1 by the relaxed assumption 1a.\nThe cost for relaxing Assumption 1 is that the dimension needs to be much higher (although still finite). The proof is rather straightforward: Making the dependence on the random indices j1, . . . , jd\u22121 explicit, the quadratic construction used in the previous theorem can be written as\nF j1,...,jd\u22121(w) = 1\nn\nn \u2211\ni=1\nf j1,...,jd\u22121 i (w) =\n1\nn\nn \u2211\ni=1\nw \u22a4A j1,...,jd\u22121 i w \u2212 a\u0303\u3008e1,w\u3009+\n\u03bb 2 \u2016w\u20162\nfor some d \u00d7 d matrix Aj1,...,jd\u22121i dependent on j1, . . . , jd\u22121, and a fixed parameter a\u0303. Crucially, note that the linear term in the function above does not depend on j1, . . . , jd\u22121. Now, we create n huge blockdiagonal matrices A1, . . . , An, where each Ai contains A j1,...,jd\u22121 i for each of the n\nd\u22121 possible choices of j1, . . . , jd\u22121 along its diagonal (in some canonical order). We then choose r \u2208 {1, 1 + d, 1 + 2d, . . . , 1 + (nd\u22121 \u2212 1)d} uniformly at random, and let\nF (w) = 1\nn\nn \u2211\ni=1\nfi(w) = 1\nn\nn \u2211\ni=1\nw \u22a4Aiw \u2212 c\u3008er,w\u3009+\n\u03bb 2 \u2016w\u20162.\nDue to the block-diagonal structure of each Ai, this function inherits the strong-convexity and smoothness properties of the original construction. Moreover, depending on the choice of r, the function is equivalent to the construction involving some j1, . . . , jd\u22121 on some subset of d coordinates (and along the other coordinates, the optimum is simply 0). The only main difference is that the quadratic terms Ai are no longer random \u2013 all the randomness is \u201cpushed\u201d to the linear term er, and the random r encodes which choices of j1, . . . , jd\u22121 are used. Thus, even if the indices i1, i2, . . . used by the algorithm depends on the Hessians Ai, they are independent of r, and hence independent of j1, . . . , jd\u22121 corresponding to the construction. Therefore, the analysis of Thm. 2 (which assumes such independence via Assumption 1) holds verbatim, and the result follows."}, {"heading": "4 Comparison to Existing Approaches", "text": "As discussed in the introduction, there has been a recent burst of activity involving second-order methods for solving finite-sum problems, relying on Hessians of individual functions fi. In this section, we review the main algorithmic approaches and compare them to our results. The bottom line is that most existing approaches satisfy the assumptions stated in Sec. 3, and therefore our lower bounds will apply, at least in a worst-case sense. A possible exception to this is the Newton sketch algorithm [Pilanci and Wainwright, 2015], which relies on random projections, but on the flip side is computationally expensive.\nTurning to the details, existing approaches are based on taking the standard Newton iteration for such problems,\nwt+1 = wt \u2212 \u03b1t ( \u22072F (w) )\u22121 \u2207F (w) = wt \u2212 \u03b1t\n(\n1\nn\nn \u2211\ni=1\n\u22072fi(w) )\u22121( 1\nn\nn \u2211\ni=1\n\u2207fi(w) ) ,\nand replacing the inverse Hessian term ( 1 n \u2211n i=1\u22072fi(w) )\u22121 (and sometimes the vector term 1n \u2211n i=1 \u2207fi(w) as well) by some approximation which is computationally cheaper to compute. One standard and wellknown approach is to use only gradient information to construct such an approximation, leading to the family of quasi-Newton methods [Nocedal and Wright, 2006]. However, as they rely on first-order rather than second-order information, they are orthogonal to the topic of our work, and are already covered by existing complexity lower bounds for first-order oracles.\nTurning to consider Hessian approximation techniques using second-order information, perhaps the simplest and most intuitive approach is sampling: Since the Hessian equals the average of many individual Hessians (\u22072F (w) = 1n \u2211n i=1\u22072fi(w)), we can approximate it by taking a sample S of indices in {1, . . . , n} uniformly at random, compute the Hessians of the corresponding individual functions, and use the approximation\n1\n|S| \u2211 i\u2208S \u22072fi(w).\nIf |S| is large enough, then by concentration of measure arguments, this sample average should be close to the actual Hessian \u22072F (w). On the other hand, if |S| is not too large, then the resulting matrix is easier to invert (e.g. because it has a rank of only O(|S|), if each individual Hessian has rank O(1), as in the case of linear predictors). Thus, one can hope that the right sample size will lead to computational savings. There have been several rigorous studies of such \u201csubsampled Newton\u201d methods, such as Erdogdu and Montanari [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016] and references therein. However, our lower bound in Thm. 2 holds for such an approach, since it satisfies both Assumption 1 and 2. As expected, the existing worst-case complexity upper bounds are no better than our lower bound.\nXu et al. [2016] recently proposed a subsampled Newton method, together with non-uniform sampling, which assigns more weight to individual functions which are deemed more \u201cimportant\u201d. This is measured via properties of the Hessians of the functions, such as their norms or via leverage scores. This approach breaks Assumption 1, as the sampled indices are now chosen in a way dependent on the individual functions, but since this dependence is only through the Hessians, it still satisfies Assumption 1a. Therefore, our lower bound as stated in Thm. 3 still applies to such a method.\nA variant of the subsampled Newton approach, studied in Erdogdu and Montanari [2015], uses a lowrank approximation of the sample Hessian (attained by truncated SVD), in lieu of the sample Hessian itself. However, this still falls in the framework of Assumption 2, and our lower bound still applies.\nA different approach to approximate the full Hessian is via randomized sketching techniques, which replace the Hessian \u22072F (w) by a low-rank approximation of the form (\u22072F (w))1/2SS\u22a4(\u22072F (w))1/2, where S \u2208 Rd\u00d7m,m \u226a d is a random sketching matrix, and (\u22072F (w))1/2 is the matrix square root of \u22072F (w). This approach forms the basis of the Newton sketch algorithm proposed in Pilanci and Wainwright [2015]. This approach currently escapes our lower bound, since it violates Assumption 2. That being said, this approach is inherently expensive in terms of computational resources, as it requires us to compute the square root of the full Hessian matrix. Even under favorable conditions, this requires us to perform a full pass over all functions f1, . . . , fn at every iteration. Moreover, existing iteration complexity upper bounds\nhave a strong dependence on both \u00b5/\u03bb as well as the dimension d, and are considerably worse than the lower bound of Thm. 2. Therefore, we conjecture that this approach cannot lead to better worst-case results.\nAgarwal et al. [2016] develop another line of stochastic second-order methods, which are based on the observation that the Newton step (\u22072F (w))\u22121\u2207F (w) is the solution of the system of linear equations \u22072F (w)x = \u2207F (w). Thus, one can reduce the optimization problem to solving this system as efficiently as possible. The basic variant of their algorithm (denoted as LiSSA) relies on operations of the form w 7\u2192 (I \u2212 \u22072fi(w))w (for i sampled uniformly at random), as well as linear combinations of such vectors, which satisfy our assumptions. A second variant, LiSSA-Quad, re-phrases this linear system as the finitesum optimization problem\nmin x\nx \u22a4\u22072F (w)x+\u2207F (w)\u22a4x = 1\nn\nn \u2211\ni=1\nx \u22a4\u22072fi(w)x+\u2207fi(w)\u22a4x,\nand uses some first-order method for finite-sum problems in order to solve it. Since individual gradients of this objective are of the form \u22072fi(w)x + \u2207fi(w), and most state-of-the-art first-order methods pick indices i obliviously, this approach also satisfies our assumptions, and our lower bounds apply. Yet another proposed algorithm, LiSSA-Sample, is based on replacing the optimization problem above by\nmin x\nx \u22a4\u22072F (w)B\u22121x+\u2207F (w)\u22a4x, (4)\nwhere B is some invertible matrix, solving it (with the optimum being equal to B(\u22072F (w))\u22121\u2207F (w)), and multiplying the solution by B\u22121 to recover the solution (\u22072F (w))\u22121\u2207F (w) to the original problem. In order to get computational savings, B is chosen to be a linear combination of O(d log(d)) sampled individual hessians \u22072fi(w), where it is assumed that d log(d) \u226a n, and the sampling and weighting is carefully chosen (based on the Hessians) so that Eq. (4) has strong convexity and smoothness parameters within a constant of each other. As a result, Eq. (4) can be solved quickly using standard gradient descent, taking steps along the gradient, which equals \u22072F (w)B\u22121x + \u2207F (w) at any point x. This gradient is again computable under Assumptions 1a and 2 (using O(n) oracle calls), since B is a linear combination of d log(d) \u226a n sampled individual Hessians, with the sampling determined based on the Hessians. Thus, our lower bound (in the form of Thm. 3) still applies to such methods.\nThat being said, it is important to note that the complexity upper bound attained in Agarwal et al. [2016] for LiSSA-Sample is on the order of O\u0303((n+ \u221a\nd\u00b5/\u03bb) log(1/\u01eb)) (asymptotically as \u01eb \u2192 0), which is better than our lower bound if d \u226a n. There is no contradiction, since the lower bound in Thm. 3 only applies for a dimension d much larger than n. Interestingly, note that if O\u0303((n + \u221a\nd\u00b5/\u03bb) log(1/\u01eb)) was attainable with an oblivious sampling scheme, in the regime where d \u226a n, it would violate Thm. 2, which establishes a lower bound of O\u0303(n + \u221a n\u00b5/\u03bb) even if the dimension is quite moderate (d = O\u0303(1 + \u221a\n\u00b5/\u03bbn), which is \u226a n under the mild assumption that \u00b5/\u03bb \u226a n3). This indicates that a non index-oblivious sampling scheme is indeed necessary to get such improved results.\nThe observation that a non-oblivious scheme (breaking assumption 1) can help performance when d \u226a n is also seen in the lower bound construction used to prove Thm. 2: If \u00b5, \u03bb, n are such that the required dimension d is \u226a n, then it means that only the functions fj1 , . . . , fjd\u22121 , which are a small fraction of all n individual functions, are informative and help us reduce the objective value. Thus, sampling these functions in an adaptive manner is imperative to get better complexity than the bound in Thm. 2. Based on the fact that only at most d \u2212 1 out of n functions are relevant in the construction, we conjecture that the possible improvement in the oracle complexity of such schemes may amount to replacing dependencies on n with dependencies on d, which is indeed the type of improvement attained (for small enough \u01eb) in Agarwal et al. [2016].\nFinally, we note that Agarwal et al. [2016] proposes another algorithm tailored to self-concordant functions, with runtime independent of the smoothness and strong convexity parameters of the problem. However, it requires performing \u2265 1 full Newton steps, so the runtime is prohibitive for large-scale problems (indeed, for quadratics as used in our lower bounds, even a single Newton step suffices to compute an exact solution)."}, {"heading": "5 Summary and Discussion", "text": "In this paper, we studied the oracle complexity for optimization problems, assuming availability of a secondorder oracle. This is in contrast to most existing oracle complexity results, which focus on a first-order oracle. First, we formally proved that in the standard setting of strongly-convex and smooth optimization problems, second-order information does not significantly improve the oracle complexity, and further assumptions (i.e. Lipschitzness of the Hessians) are in fact necessary. We then presented our main lower bounds, which show that for finite-sum problems with a second-order oracle, under some reasonable algorithmic assumptions, the resulting oracle complexity is \u2013 again \u2013 not significantly better than what can be obtained using a first-order oracle. Moreover, this is shown using quadratic functions, which have 0 derivatives of order larger than 2. Hence, our lower bounds apply even if we have access to an oracle returning derivatives of order p for all p \u2265 0, and the function is smooth to any order. In Sec. 4, we studied how our framework and lower bounds are applicable to most existing approaches.\nAlthough this conclusion may appear very pessimistic, they are actually useful in pinpointing potential assumptions and approaches which may circumvent these lower bounds. In particular:\n\u2022 Our lower bound for algorithms employing non index-oblivious sampling schemes (Thm. 3) only hold when the dimension d is very large. This leaves open the possibility of better (non index-oblivious) algorithms when d is moderate, as was recently demonstrated in the context of the LiSSA-Sample algorithm of Agarwal et al. [2016] (at least for small enough \u01eb). As discussed in the previous section, we conjecture that the possible improvement in the oracle complexity of such schemes may amount to replacing dependencies on n with dependencies on d.\n\u2022 It might be possible to construct algorithms breaking the one of the assumptions in Sec. 3, e.g. by using operations other than the linear-algebraic ones specified in Assumption 2. However, we currently conjecture that these assumptions can be considerably relaxed, and similar results would hold for any algorithm which has \u201csignificantly\u201d cheaper iterations (in terms of runtime) compared to the Newton method. Nevertheless, our conjecture could be false.\n\u2022 Our lower bounds are worst-case over smooth and strongly-convex individual functions fi. It could be that by assuming more structure, better bounds can be obtained. For example, as discussed in the introduction, an important special case is when fi(w) = \u2113i(x\u22a4i w) for some scalar function \u2113i and vector xi. Our construction in Thm. 2 does not quite fit this structure, although it is easy to show that we still get functions of the form fi(w) = \u2113i(X\u22a4i w), where Xi has O(1 + d/n) = O\u0303(1 + \u221a\n\u00b5/\u03bbn3) rows in expectation, which is O\u0303(1) under a broad parameter regime. We believe that the difference between O\u0303(1) rows and 1 row is not significant in terms of the attainable oracle complexity, but we may be wrong. Another possibility is to provide results depending on more delicate spectral properties of the function, beyond its strong convexity and smoothness, which may lead to better results and algorithms under favorable assumptions.\n\u2022 Our lower bounds in Sec. 3, which establish a linear convergence rate (logarithmic dependence on log(1/\u01eb)), are non-trivial only if the optimization error \u01eb is sufficiently small. This does not preclude the possibility of attaining better initial performance when \u01eb is relatively large. Indeed, some of the recent work mentioned previously (e.g. Bollapragada et al. [2016], Xu et al. [2016], Roosta-Khorasani and Mahoney [2016a], Pilanci and Wainwright [2015]) show that various algorithms can have an initial super-linear convergence rate, which slows down to linear as one gets closer to the optimum.\nIn any case, we believe our work lays the foundation for a fuller study of the complexity of efficient second-order methods, for finite-sum and related optimization and learning problems."}, {"heading": "Acknowledgments", "text": "This research is supported in part by an FP7 Marie Curie CIG grant and an Israel Science Foundation grant 425/13."}, {"heading": "A Proofs", "text": ""}, {"heading": "A.1 Auxiliary Lemmas", "text": "The following lemma was essentially proven in Lan [2015], Nesterov [2013], but we provide a proof for completeness:\nLemma 1. Fix \u00b5\u0303 \u2265 \u03bb\u0303 > 0, and consider the following function on Rd:\nF (w) = \u03bb\u0303(\u03ba\u0303\u2212 1)\n8\n(\nw21 +\nd\u22121 \u2211\ni=1\n(wi \u2212 wi+1)2 + (a\u03ba\u0303 \u2212 1)w2d \u2212 w1 ) + \u03bb\u0303\n2 \u2016w\u20162,\nwhere \u03ba\u0303 = \u00b5\u0303/\u03bb\u0303 and a\u03ba\u0303 = \u221a \u03ba\u0303+3\u221a \u03ba\u0303+1 . Then F is \u03bb\u0303 strongly convex, \u00b5\u0303-smooth, and has a unique minimum at (q, q2, q3, . . . , qd) where q = \u221a \u03ba\u0303\u22121\u221a \u03ba\u0303+1 .\nProof. The function is equivalent to\nF (w) = \u03bb\u0303(\u03ba\u0303\u2212 1)\n8\n(\nw \u22a4Aw \u2212w1\n) + \u03bb\u0303\n2 \u2016w\u20162,\nwhere\nA =\n\n      \n2 \u22121 \u22121 2 \u22121\n\u22121 . . . . . . . . . 2 \u22121\n\u22121 a\u03ba\u0303\n\n      \nis a symmetric tridiagonal matrix with positive entries on the diagonal. Since it is diagonally dominant, it follows that it is positive semidefinite, and therefore F is \u03bb\u0303 strongly-convex due to the \u03bb\u03032\u2016w\u20162 term.\nMoreover, for any w, using the fact that (a+ b)2 \u2264 2(a2 + b2), we have\nw \u22a4Aw = w21 +\nd\u22121 \u2211\ni=1\n(wi \u2212 wi+1)2 + (a\u03ba\u0303 \u2212 1)w2d\n\u2264 w21 + 2 d\u22121 \u2211\ni=1\n(w2i + w 2 i+1) + (a\u03ba\u0303 \u2212 1)w2d\n\u2264 4w21 + 4 d\u22121 \u2211\ni=2\nw2i + (a\u03ba\u0303 + 1)w 2 d \u2264 4\u2016w\u20162,\nwhere in the last inequality we used the fact that a\u03ba\u0303 \u2264 3 by definition of a\u03ba\u0303. Therefore, 4I A 0, and hence the function w 7\u2192 \u03bb\u0303(\u03ba\u0303\u22121)8 ( w \u22a4Aw \u2212 w1 )\nis a convex, \u03bb\u0303(\u03ba\u0303\u2212 1)-smooth function. Since \u03bb\u03032\u2016w\u20162 is \u03bb\u0303 strongly-convex, it follows that F is \u03bb\u0303 strongly convex and \u03bb\u0303(\u03ba\u0303\u2212 1) + \u03bb\u0303 = \u03bb\u0303\u03ba\u0303 = \u00b5\u0303 smooth.\nIt remains to compute the optimum of F . By differentiating F and setting to zero, we get that the optimum w must satisfy the following set of equations:\nw2 \u2212 2 \u00b7 \u03ba\u0303+ 1\n\u03ba\u0303\u2212 1 \u00b7 w1 + 1 = 0\nwi+1 \u2212 2 \u00b7 \u03ba\u0303+ 1\n\u03ba\u0303\u2212 1 \u00b7 wi + wi\u22121 = 0 \u2200 i = 2, . . . , d\u2212 1 (\na\u03ba\u0303 + 4\n\u03ba\u0303\u2212 1\n)\nwd \u2212 wd\u22121 = 0.\nIt is easily verified that this is satisfied by the vector (q, q2, q3, . . . , qd), where q = \u221a \u03ba\u0303\u22121\u221a \u03ba\u0303+1\n. Since F is strongly convex, this stationary point must be the unique global optimum of F .\nLemma 2. For some q \u2208 (0, 1) and positive d, define\ng(z) =\n{\nq2(z+1) z < d 0 z \u2265 d .\nLet l be a non-negative random variable, and suppose d \u2265 2E[l]. Then E[g(l)] \u2265 12q4E[l]+4.\nProof. Since q \u2208 (0, 1), the function z 7\u2192 qz is convex for non-negative z. Therefore, by definition of g, Jensen\u2019s inequality, and the law of total expectation, we have\nE[g(l)] = Pr(l < d) \u00b7 E[q2(l+1)|l < d] + Pr(l \u2265 d) \u00b7 0 \u2265 Pr(l < d) \u00b7 qE[2(l+1)|l<d]\n= Pr(l < d) \u00b7 q(E[2(l+1)]\u2212Pr(l\u2265d)\u00b7E[2(l+1)|l\u2265d])/Pr(l<d)\n\u2265 Pr(l < d) \u00b7 qE[2(l+1)]/Pr(l<d).\nBy Markov\u2019s inequality, Pr(l < d) = 1 \u2212 Pr(l \u2265 d) \u2265 1 \u2212 E[l]d \u2265 12 . Plugging into the above, we get the lower bound 12q 4(E[l+1]) = 12q 4E[l]+4 as required."}, {"heading": "A.2 Proof of Thm. 1", "text": "The proof is inspired by a technique introduced in Woodworth and Srebro [2016] for analyzing randomized first-order methods, in which a quadratic function is \u201clocally flattened\u201d in order to make first-order (gradient) information non-informative. We use a similar technique to make second-order (Hessian) information noninformative, hence preventing second-order methods from having an advantage over first-order methods.\nGiven a (deterministic) algorithm and a bound T on the number of oracle calls, we construct the function F in the following manner. We first choose some dimension d \u2265 2T . We then define\n\u03ba = \u00b5\n8\u03bb , q = \u221a \u03ba\u2212 1\u221a \u03ba+ 1 ,\nand choose r > 0 sufficiently small so that\nT\u00b5r2\n8\u03bb \u2264 1 and\n\u221a\nT\u00b5r2\n16\u03bb \u2264 1 2 qT .\nWe also let v1, . . . ,vT be orthonormal vectors in Rd (to be specified later). We finally define our function as\nF (w) = H(w) + \u03bb\n2 \u2016w\u20162,\nwhere\nH(w) = \u03bb(\u03ba\u2212 1)\n8\n( \u3008v1,w\u30092 + T\u22121 \u2211\ni=1\n\u03c6r(\u3008vi \u2212 vi+1,w\u3009) + (a\u03ba \u2212 1)\u03c6r(\u3008vT ,w\u3009)\u2212 \u3008v1,w\u3009 ) ,\na\u03ba = \u221a \u03ba+3\u221a \u03ba+1 , and\n\u03c6r(z) =\n\n \n  0 |z| \u2264 r 2(|z| \u2212 r)2 r < |z| \u2264 2r z2 \u2212 2r2 |z| > 2r .\nIt is easy to show that \u03c6r is 4-smooth and satisfies 0 \u2264 z2 \u2212 \u03c6r(z) \u2264 2r2 for all z. First, we establish that F is indeed strongly convex and smooth as required:\nLemma 3. F as defined above is \u03bb-strongly convex and \u00b5-smooth.\nProof. Since \u03c6r is convex, and the composition of a convex and linear function is convex, we have that w 7\u2192 \u03c6r(\u3008vi \u2212 vi+1,w\u3009) are convex for all i, as well as w 7\u2192 \u3008v1,w\u30092 and w 7\u2192 \u03c6r(\u3008vT ,w\u3009). Therefore, H(w) is convex. As a result, F is \u03bb-strongly convex due to the \u03bb2\u2016w\u20162 term. As to smoothness, note first that H(w) can be equivalently written as H\u0303(Vw), where V is some orthogonal d \u00d7 d matrix with the first T rows equal to v1, . . . ,vT , and\nH\u0303(x) = \u03bb(\u03ba\u2212 1)\n8\n( x21 + T\u22121 \u2211\ni=1\n\u03c6r(xi \u2212 xi+1) + (a\u03ba \u2212 1)\u03c6r(xT )\u2212 x1 ) .\nTherefore, \u22072F (w) = \u22072H(w) + \u03bbI = V \u22a4\u22072H\u0303(Vw)V + \u03bbI . It is easily verified that \u22072H\u0303 at any point (and in particular Vw) is tridiagonal, with each element having absolute value at most 2\u03bb(\u03ba\u22121). Therefore,\nusing the orthogonality of V and the fact that (a+ b)2 \u2264 2(a2 + b2),\nsup x:\u2016x\u2016=1\nx \u22a4\u22072F (w)x = sup x:\u2016x\u2016=1 x \u22a4(V \u22a4\u22072H\u0303(Vw)V + \u03bbI)x\n= sup x:\u2016x\u2016=1\nx \u22a4\u22072H\u0303(Vw)x+ \u03bb\n\u2264 sup x:\u2016x\u2016=1\n2\u03bb(\u03ba\u2212 1) ( d \u2211\ni=1\nx2i + 2\nd\u22121 \u2211\ni=1\n|xixi+1| ) + \u03bb\n\u2264 sup x:\u2016x\u2016=1\n2\u03bb(\u03ba\u2212 1) d\u22121 \u2211\ni=1\n(|xi|+ |xi+1|)2 + \u03bb\n\u2264 sup x:\u2016x\u2016=1\n4\u03bb(\u03ba\u2212 1) d\u22121 \u2211\ni=1\n(x2i + x 2 i+1) + \u03bb\n\u2264 8\u03bb(\u03ba \u2212 1) + \u03bb \u2264 8\u03bb\u03ba.\nPlugging in the definition of \u03ba, this equals \u00b5. Therefore, the spectral norm of the Hessian of F at any point is at most \u00b5, and therefore F is \u00b5-smooth.\nBy construction, the function F also has the following key property:\nLemma 4. For any w \u2208 Rd orthogonal to vt,vt+1, . . . ,vT (for some t \u2208 {1, 2, . . . , T \u2212 1}), it holds that F (w),\u2207F (w),\u22072F (w) do not depend on vt+1,vt+2, . . . ,vT .\nProof. Recall that F is derived from H by adding a \u03bb2\u2016w\u20162 term, which clearly does not depend on v1, . . . ,vT . Therefore, it is enough to prove the result for H(w),\u2207H(w),\u22072H(w). By taking the definition of H and differentiating, we have that H(w) is proportional to\n\u3008v1,w\u30092 + T\u22121 \u2211\ni=1\n\u03c6r(\u3008vi \u2212 vi+1,w\u3009) + (a\u03ba \u2212 1)\u03c6r(\u3008vT ,w\u3009)\u2212 \u3008v1,w\u3009,\n\u2207H(w) is proportional to\n2\u3008v1,w\u3009v1 + T\u22121 \u2211\ni=1\n\u03c6\u2032r(\u3008vi \u2212 vi+1,w\u3009)(vi \u2212 vi+1) + (a\u03ba \u2212 1)\u03c6\u2032r(\u3008vT ,w\u3009)vT \u2212 v1,\nand \u22072H(w) is proportional to\n2v1v \u22a4 1 +\nT\u22121 \u2211\ni=1\n\u03c6\u2032\u2032r(\u3008vi \u2212 vi+1,w\u3009)(vi \u2212 vi+1)(vi \u2212 vi+1)\u22a4 + (a\u03ba \u2212 1)\u03c6\u2032\u2032r (\u3008vT ,w\u3009)vTv\u22a4T .\nBy the assumption \u3008vt,w\u3009 = \u3008vt+1,w\u3009 = . . . = \u3008vT ,w\u3009 = 0, and the fact that \u03c6r(0) = \u03c6\u2032r(0) = \u03c6\u2032\u2032r(0) = 0, we have \u03c6r(\u3008vi\u2212vi+1,w\u3009) = \u03c6\u2032r(\u3008vi\u2212vi+1,w\u3009) = \u03c6\u2032\u2032r (\u3008vi\u2212vi+1,w\u3009) = 0 for all i \u2208 {t, t+1, . . . , T}, as well as \u03c6r(\u3008vT ,w\u3009) = \u03c6\u2032r(\u3008vT ,w\u3009) = \u03c6\u2032\u2032r (\u3008vT , bw\u3009) = 0. Therefore, it is easily verified that the expressions above indeed do not depend on vt+1, . . . ,vT .\nWith this lemma at hand, we now turn to describe how v1, . . . ,vT are constructed:\n\u2022 First, we compute w1 (which is possible since the algorithm is deterministic and w1 is chosen before any oracle calls are made).\n\u2022 We pick v1 to be some unit vector orthogonal to w1. Assuming v2, . . . ,vT will also be orthogonal to w1 (which will be ensured by the construction which follows), we have by Lemma 4 that the information F (w1),\u2207F (w1),\u22072F (w1) provided by the oracle to the algorithm does not depend on {v2, . . . ,vT }, and thus depends only on v1 which was already fixed. Since the algorithm is deterministic, this fixes the next query point w2.\n\u2022 For t = 2, 3, . . . , T \u2212 1, we repeat the process above: We compute wt, and pick vt to be some unit vectors orthogonal to w1,w2, . . . ,wt, as well as all previously constructed v\u2019s (this is always possible since the dimension is sufficiently large). By Lemma 4, as long as all vectors thus constructed are orthogonal to wt, the information {F (wt),\u2207F (wt),\u22072F (wt)} provided to the algorithm does not depend on vt+1, . . . ,vT , and only depends on v1, . . . ,vt which were already determined. Therefore, the next query point wt+1 is fixed.\n\u2022 At the end of the process, we pick vT to be some unit vector orthogonal to all previously chosen v\u2019s as well as w1, . . . ,wT .\nBased on this construction, the following lemma is self-evident:\nLemma 5. It holds that \u3008wT ,vT \u3009 = 0.\nBased on this lemma, we now turn to argue that wT must be a sub-optimal point. We first establish the following result:\nLemma 6. Letting w\u22c6 = argminw F (w), it holds that \u2225\n\u2225 \u2225 \u2225 \u2225 w \u22c6 \u2212\nT \u2211\ni=1\nqivi\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2264 \u221a T\u00b5r2 16\u03bb\nwhere q = \u221a \u03ba\u22121\u221a \u03ba+1 .\nProof. Let Fr denote F , where we make the dependence on the parameter r explicit. We first argue that\nsup w\u2208Rd\n|Fr(w)\u2212 F0(w)| \u2264 T\u00b5r2\n32 . (5)\nThis is because\n|Fr(w)\u2212 F0(w)| \u2264 \u03bb(\u03ba\u2212 1)\n8\n( T\u22121 \u2211\ni=1\n|\u03c6r(\u3008vi \u2212 vi+1,w\u3009)\u2212 \u03c60(\u3008vi \u2212 vi+1,w\u3009)|\n+ |\u03c6r(\u3008vT ,w\u3009)\u2212 \u03c60(\u3008vT ,w\u3009)| ) ,\nand since supz\u2208R |\u03c6r(z)\u2212 \u03c60(z)| = supz\u2208R |\u03c6r(z)\u2212 z2| \u2264 2r2, the above is at most \u03bb(\u03ba\u22121)4 Tr2 \u2264 \u03bb\u03ba4 Tr2. Recalling that \u03ba = \u00b5/8\u03bb, Eq. (5) follows.\nLet wr = argminFr(w). By \u03bb-strong convexity of F0 and Fr ,\nF0(wr)\u2212 F0(w0) \u2265 \u03bb\n2 \u2016wr \u2212w0\u20162 , Fr(w0)\u2212 Fr(wr) \u2265\n\u03bb 2 \u2016w0 \u2212wr\u20162.\nSumming the two inequalities and using Eq. (5),\n\u03bb\u2016wr \u2212w0\u20162 \u2264 F0(wr)\u2212 Fr(wr) + Fr(w0)\u2212 F0(w0) \u2264 T\u00b5r2\n16 ,\nand therefore\n\u2016wr \u2212w0\u20162 \u2264 T\u00b5r2\n16\u03bb . (6)\nBy definition, wr = w\u22c6 from the statement of our lemma, so it only remains to prove that w0 = argminF0(w) equals\n\u2211T i=1 q i vi. To see this, note that F0(w) can be equivalently written as F\u0303 (Vw), where V is some\northogonal d\u00d7 d matrix with its first T rows equal to v1, . . . ,vT , and\nF\u0303 (x) = \u03bb(\u03ba\u2212 1)\n8\n( x21 + T\u22121 \u2211\ni=1\n(xi \u2212 xi+1)2 + (a\u03ba \u2212 1)x2T \u2212 w1 ) + \u03bb\n2 \u2016x\u20162.\nBy an immediate corollary of Lemma 1, F\u0303 (\u00b7) is minimized at (q, q2, . . . , qT , 0, . . . , 0), where q = \u221a \u03ba\u22121\u221a \u03ba+1 , and therefore F (w) = F\u0303 (Vw) is minimized at V \u22a4(q, q2, . . . , qT , 0, . . . , 0), which equals \u2211T\ni=1 q i vi as\nrequired.\nNote that this lemma also allows us to bound the norm of w\u22c6 = argminF (w), since it implies that\n\u2016w\u22c6\u2016 \u2264 \u2225 \u2225 \u2225\n\u2225 \u2225\nT \u2211\ni=1\nqivi\n\u2225 \u2225 \u2225 \u2225 \u2225 + \u221a T\u00b5r2 16\u03bb ,\nand since (a+ b)2 \u2264 2a2 + 2b2 and q < 1, we have\n\u2016w\u22c6\u20162 \u2264 2 \u2225 \u2225 \u2225\n\u2225 \u2225\nT \u2211\ni=1\nqivi\n\u2225 \u2225 \u2225 \u2225 \u2225\n2\n+ T\u00b5r2\n8\u03bb = 2\nT \u2211\ni=1\nq2i + T\u00b5r2\n8\u03bb\n\u2264 2 \u221e \u2211\ni=1\nq2i + T\u00b5r2\n8\u03bb =\n2q2 1\u2212 q2 + T\u00b5r2 8\u03bb\n\u2264 2 1\u2212 q +\nT\u00b5r2\n8\u03bb =\n\u221a \u03ba+ 1 + T\u00b5r2\n8\u03bb ,\nwhich is at most \u221a \u03ba + 2 \u2264 3\u221a\u03ba, since we assume that c is sufficiently small so that T\u00b5r28\u03bb \u2264 1, and that \u03ba = \u00b5/8\u03bb \u2265 1. The proof of the theorem follows by combining Lemma 5 and Lemma 6. Specifically, Lemma 5 (which states that \u3008wT ,vT \u3009 = 0) and the fact that v1, . . . ,vT are orthonormal tells us that \u2225\n\u2225 \u2225 \u2225 \u2225\nwT \u2212 T \u2211\ni=1\nqivi\n\u2225 \u2225 \u2225 \u2225 \u2225\n2\n=\n\u2225 \u2225 \u2225 \u2225 \u2225 ( wT \u2212 T\u22121 \u2211\ni=1\nqivi\n)\n\u2212 qTvT\n\u2225 \u2225 \u2225 \u2225 \u2225\n2\n=\n\u2225 \u2225 \u2225 \u2225 \u2225 wT \u2212 T\u22121 \u2211\ni=1\nqivi\n\u2225 \u2225 \u2225 \u2225 \u2225\n2\n+ \u2016qTvT \u20162\n\u2265 \u2016qTvT \u20162 = q2T ,\nand hence \u2225\n\u2225 \u2225 \u2225 \u2225\nwT \u2212 T \u2211\ni=1\nqivi\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2265 qT .\nOn the other hand, Lemma 6 states that \u2225\n\u2225 \u2225 \u2225 \u2225 w \u22c6 \u2212\nT \u2211\ni=1\nqivi\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2264 \u221a T\u00b5r2 16\u03bb .\nCombining the last two displayed equations by the triangle inequality, we get that\n\u2016wT \u2212w\u22c6\u2016 \u2265 qT \u2212 \u221a T\u00b5r2\n16\u03bb .\nBy the assumption that c is sufficiently small so that \u221a T\u00b5r2\n16\u03bb \u2264 12qT , the left hand side is at least 12qT . Squaring both sides, we get\n\u2016wT \u2212w\u22c6\u20162 \u2265 1\n4 q2T ,\nso by strong convexity of F ,\nF (wT )\u2212 F (w\u22c6) \u2265 \u03bb\n2 \u2016wT \u2212w\u22c6\u20162 \u2265\n\u03bb 8 q2T .\nPlugging in the value of q, we get\nF (wT )\u2212 F (w\u22c6) \u2265 \u03bb\n8 (\u221a \u03ba\u2212 1\u221a \u03ba+ 1 )2T .\nOn the other hand, we showed earlier that \u2016w\u22c6\u20162 \u2264 3\u221a\u03ba, so by smoothness, F (0)\u2212 F (w\u22c6) \u2264 \u00b52\u2016w\u22c6\u20162 \u2264 3\u00b5 2 \u221a \u03ba. Therefore,\nF (wT )\u2212 F (w\u22c6) F (0)\u2212 F (w\u22c6) \u2265\n\u03bb\n12\u00b5 \u221a \u03ba (\u221a \u03ba\u2212 1\u221a \u03ba+ 1 )2T\nTo make the right-hand side less than \u01eb, T must be such that (\u221a\n\u03ba\u2212 1\u221a \u03ba+ 1\n)2T \u2264 12\u00b5 \u221a \u03ba\u01eb\n\u03bb ,\nwhich is equivalent to\n2T \u00b7 log (\u221a\n\u03ba+ 1\u221a \u03ba\u2212 1\n) \u2265 log (\n\u03bb\n12\u00b5 \u221a \u03ba\u01eb\n)\n.\nSince log (\u221a\n\u03ba+1\u221a \u03ba\u22121\n) = log (\n1 + 2\u221a \u03ba\u22121\n)\n\u2264 2\u221a \u03ba\u22121 , it follows that T must be such that\n4T\u221a \u03ba\u2212 1 \u2265 log\n(\n\u03bb\n12\u00b5 \u221a \u03ba\u01eb\n)\n.\nPlugging in \u03ba = \u00b5/8\u03bb and simplifying a bit, we get that\nT \u2265 1 4\n(\u221a\n\u00b5\n8\u03bb \u2212 1\n) \u00b7 log (\u221a 8(\u03bb/\u00b5)3/2\n12\u01eb\n)\n,\nfrom which the result follows."}, {"heading": "A.3 Proof of Thm. 2", "text": "We will define a randomized choice of quadratic functions f1, . . . , fn, and prove a lower bound on the expected optimization error of any algorithm (where the expectation is over both the algorithm and the randomized functions). This implies that for any algorithm, the same lower bound (in expectation over the algorithm only) holds for some deterministic choice of f1, . . . , fn.\nThere will actually be two separate constructions, one leading to a lower bound of \u2126(n), and one leading\nto a lower bound of \u2126 (\u221a\nn\u00b5 \u03bb \u00b7 log\n( (\u03bb/\u00b5)3/2 \u221a n\n\u01eb\n))\n. Choosing the construction which leads to the larger lower\nbound, the theorem follows."}, {"heading": "A.3.1 An \u2126(n) Lower Bound", "text": "Starting with the \u2126(n) lower bound, let \u03b4i, where i \u2208 {1, . . . , n}, be chosen uniformly at random from {\u22121,+1}, and define\nfi(w) = \u2212\u03b4iw1 + \u03bb\n2 \u2016w\u20162.\nClearly, these are \u03bb-smooth (and hence \u00b5-smooth) functions, as well as \u03bb-strongly convex. Also, the optimum of F (w) = \u00b5n \u2211n i=1 fi(w) equals w \u22c6 = ( 1 n\u03bb \u2211n i=1 \u03b4i ) e1, where e1 is the first unit vector. As a result, \u2016w\u22c6\u20162 = 1 \u03bb2 ( 1 n \u2211n i=1 \u03b4i )2 , so by \u03bb-smoothness of F\nF (0)\u2212 F (w\u22c6) \u2264 \u03bb 2 \u2016w\u22c6\u20162 = 1 2\u03bb\n(\n1\nn\nn \u2211\ni=1\n\u03b4i\n)2\n.\nSince \u03b4i are i.i.d., we have by Hoeffding\u2019s bound that with probability at least 3/4, \u2223 \u2223 1 n \u2211n i=1 \u03b4i \u2223 \u2223 is at most \u221a\n2 log(8/3)/n \u2264 \u221a 2/n. Plugging into the equation above, we get that with probability at least 3/4,\nF (0)\u2212 F (w\u22c6) \u2264 1 \u03bbn . (7)\nTurning to lower bound F (wT )\u2212 F (w\u22c6), we have by strong convexity that\nF (wT )\u2212 F (w\u22c6) \u2265 \u03bb\n2 \u2016wT \u2212w\u22c6\u20162 \u2265\n\u03bb 2 (wT,1 \u2212 w\u22c61)2\n= 1\n2\u03bb\n(\n\u03bbwT,1 \u2212 1\nn\nn \u2211\ni=1\n\u03b4i\n)2\n.\nNow, if at most \u230an/2\u230b indices {1, . . . , n} were queried by the algorithm, then the wT returned by the algorithm must be independent of at least \u2308n/2\u2309 random variables \u03b4j1 , . . . , \u03b4j\u2308n/2\u2309 (for some distinct indices j1, j2, . . . depending on the algorithm\u2019s behavior, but independent of the values of \u03b4j1 , . . . , \u03b4j\u2308n/2\u2309). Therefore, conditioned on j1, . . . , j\u2308n/2\u2309 and the values of \u03b4j1 , . . . , \u03b4j\u2308n/2\u2309 , the expression above can be written as\n1\n2\u03bb\n\n\u03b7 \u2212 1 n\n\u2211\ni/\u2208{j1,...,j\u2308n/2\u2309} \u03b4i\n\n\n2\n,\nwhere \u03b7 is a fixed quantity independent of the values of \u03b4i for i /\u2208 {j1, . . . , j\u2308n/2\u2309}. By a standard anticoncentration argument, with probability at least 3/4, this expression will be at least 12\u03bb ( c\u2032\u221a n )2 = c \u20322 2\u03bbn for\nsome universal positive c\u2032 > 0. Since this is true for any j1, . . . , j\u2308n/2\u2309 and \u03b4j1 , . . . , \u03b4j\u2308n/2\u2309 , we get that with probability at least 3/4 over \u03b41, . . . , \u03b4n,\nF (wT )\u2212 F (w\u22c6) \u2265 c\u20322\n2\u03bbn .\nCombining this with Eq. (7) using a union bound, we have that with probability at least 1/2,\nF (wT )\u2212 F (w\u22c6) F (0) \u2212 F (w\u22c6) \u2265 c\u20322\u03bbn 2\u03bbn = c\u20322 2 .\nAs a result, since the ratio above is always a non-negative quantity,\nE\n[\nF (wT )\u2212 F (w\u22c6) F (0)\u2212 F (w\u22c6)\n] \u2265 c \u20322\n4 .\nUsing the assumption stated in the theorem (taking c = c\u20322/4), we have that the right hand side cannot be smaller than \u01eb, unless more than \u230an/2\u230b = \u2126(n) oracle calls are made.\nA.3.2 An \u2126 (\u221a\nn\u00b5 \u03bb \u00b7 log\n( (\u03bb/\u00b5)3/2 \u221a n\n\u01eb\n))\nLower Bound\nWe now turn to prove the \u2126 (\u221a\nn\u00b5 \u03bb \u00b7 log\n(\n\u03bb \u01eb\n)\n)\nlower bound, using a different function construction: Let\nj1, . . . , jd\u22121 be chosen uniformly and independently at random from {1, . . . , n}, and define\nfi(w) = \u03bbn(\u03ba\u2212 1)\n8\n( d\u22121 \u2211\nl=1\n1jl=i(wl \u2212 wl+1)2 + 1\nn\n( w21 + (a\u03ba \u2212 1)w2d \u2212 w1 )\n)\n+ \u03bb\n2 \u2016w\u20162.\nwhere 1A is the indicator of the event i, and \u03ba = \u00b5 \u03bbn (which is \u2265 2 by assumption). Note that these are all \u03bb-strongly convex functions, as all terms in their definition are convex in w, and there is an additional \u03bb 2\u2016w\u20162 term. Moreover, they are also \u00b5-smooth: To see this, note that \u22072fi(w) \u03bbn(\u03ba\u22121) 4 A+ \u03bb 2 I , where A 4I is as defined in the proof of Lemma 1. Therefore, the smoothness parameter is at most\n\u03bbn(\u03ba\u2212 1) 4 \u00b7 4 + \u03bb \u2264 \u03bbn\u03ba \u2264 \u00b5.\nThe average function F (w) = 1n \u2211n i=1 fi(w) equals\nF (w) = \u03bb(\u03ba\u2212 1)\n8\n( w21 + d\u22121 \u2211\ni=1\n(wi \u2212 wi+1)2 + (a\u03ba \u2212 1)w2d \u2212 w1 ) + \u03bb\n2 \u2016w\u20162,\nwhich is \u03bb-strongly convex and is the same as F from Lemma 1 (with \u03bb\u0303 = \u03bb, \u00b5\u0303 = \u00b5/n, and \u03ba\u0303 = \u03ba). Therefore, by Lemma 1, the global minimum w\u22c6 of F equals (q, q2, . . . , qd), where q = \u221a \u03ba\u22121\u221a \u03ba+1\n. Note that since q < 1 and \u03ba \u2265 2, the squared norm of w\u22c6 is at most\nd \u2211\ni=1\nq2i \u2264 \u221e \u2211\ni=1\nq2i = q2 1\u2212 q2 \u2264 1 1\u2212 q = \u221a \u03ba+ 1 2 \u2264 \u221a \u03ba,\nhence by smoothness,\nF (0)\u2212 F (w\u22c6) \u2264 \u00b5 2 \u2016w\u22c6\u20162 \u2264 \u00b5 2\n\u221a \u03ba. (8)\nWith these preliminaries out of the way, we now turn to compute a lower bound on the expected optimization error. The proof is based on arguing that wT can only have a first few coordinates being non-zero. To see how this gives a lower bound, let lT \u2208 {1, . . . , d} be the largest index of a non-zero coordinate of wT (or 0 if wT = 0). By definition of w\u22c6, we have\n\u2016wT \u2212w\u22c6\u20162 \u2265 d \u2211\ni=lT+1\nq2i \u2265 g(lT ),\nwhere\ng(z) =\n{\nq2(z+1) z < d 0 z \u2265 d .\nBy strong convexity of F , this implies that\nF (wT )\u2212 F (w\u22c6) \u2265 \u03bb\n2 \u2016wT \u2212w\u22c6\u20162 \u2265\n\u03bb 2 g(lT ).\nFinally, taking expectation over the randomness of j1, . . . , jd\u22121 above (and over the internal randomness of the algorithm, if any), applying Lemma 2, and choosing the dimension d = \u23082E[lT ]\u2309 (which we will later show to equal the value specified in the theorem), we have\nE [F (wT )\u2212 F (w\u22c6)] \u2265 \u03bb\n4 q4E[lT ]+4 =\n\u03bb\n4 (\u221a \u03ba\u2212 1\u221a \u03ba+ 1 )4E[lT ]+4 .\nCombined with Eq. (8), this gives\nE\n[\nF (wT )\u2212 F (w\u22c6) F (0)\u2212 F (w\u22c6)\n]\n\u2265 \u03bb 2\u00b5 \u221a \u03ba (\u221a \u03ba\u2212 1\u221a \u03ba+ 1 )4E[lT ]+4 . (9)\nThus, it remains to upper bound E[lT ]. To get a bound, we rely on the following key lemma (where ei is the i-th unit vector, and recall that Wt defines the set of allowed query points wt, and j1, . . . , jd are the random indices used in constructing f1, . . . , fn):\nLemma 7. For all t, it holds that Wt \u2286 span{ed, e1, e2, e3, . . . , e\u2113t} for all t, where \u2113t is defined recursively as follows: \u21131 = 1, and \u2113t+1 equals the largest number in {1, . . . , d\u22121} such that {j\u2113t , j\u2113t+1, . . . , j\u2113t+1\u22121} \u2286 {it, it\u22121, . . . , imax{1,t\u2212\u230an/2\u230b+1}} (and \u2113t+1 = \u2113t if no such number exists).\nAs will be seen later, \u2113T (which is a random variable as a function of the random indices j1, . . . , jd) upper-bounds the number of non-zero coordinates of wT , and therefore we can upper bound E[lT ] by E[\u2113T ].\nProof. The proof is by induction over t. Since W1 = {0} \u2286 span(ed), the result trivially holds for t = 1. Now, suppose that Wt \u2286 span{ed, e1, . . . , e\u2113t} for some t and \u2113t. Note that in particular, this means that wt is non-zero only in its first \u2113t coordinates. By definition of fi for any i,\n\u2207fi(w) = \u03bbn(\u03ba\u2212 1)\n8\n(\n2\nd\u22121 \u2211\nl=1\n1jl=i(wl \u2212 wl+1)(el \u2212 el+1) + 1\nn (2w1e1 + 2(a\u03ba \u2212 1)wded \u2212 e1)\n)\n+ \u03bbw\n\u22072fi(w) = \u03bbn(\u03ba\u2212 1)\n8\n( d\u22121 \u2211\nl=1\n1jl=i(2El,l \u2212 El+1,l \u2212 El,l+1) + 1\nn (2E1,1 + 2(a\u03ba \u2212 1)Ed,d)\n)\n+ \u03bbI,\nwhere Er,s is the d \u00d7 d which is all zeros, except for an entry of 1 in location (r, s). It is easily seen that these expressions imply the following:\n\u2022 If j\u2113t 6= it, then \u2207fit(wt) \u2208 span{ed, e1, . . . , e\u2113t}, otherwise \u2207fit(wt) \u2208 span{ed, e1, . . . , e\u2113t+1}.\n\u2022 For any w and l \u2208 {1, . . . , d\u2212 1}, if jl 6= i, then \u22072fi(w) is block-diagonal, with a block in the first l\u00d7 l entries. In other words, any entry (r, s) in the matrix, where r \u2264 l and s > l (or r > l and s \u2264 l) is zero.\n\u2022 As a result, if jl /\u2208 {it, it\u22121, . . . , imax{1,t\u2212\u230an/2\u230b+1}}, then \u2211t \u03c4=max{1,t\u2212\u230an/2\u230b+1} \u03b1\u03c4\u22072fi\u03c4 (w\u03c4 ), for arbitrary scalars \u03c4 , is block-diagonal with a block in the first l \u00d7 l entries. The same clearly holds for any matrix with the same block-diagonal structure.\nTogether, these observations imply that the operations specified in Assumption 2 can lead to vectors outside span{ed, e1, . . . , e\u2113t}, only if j\u2113t \u2208 {it, it\u22121, . . . , imax{1,t\u2212\u230an/2\u230b+1}}. Moreover, these vectors must belong to span{ed, e1, . . . , e\u2113t+1}, where \u2113t+1 is as specified in the lemma: By definition, j\u2113t+1 is not in {it, it\u22121, . . . , imax{1,t\u2212\u230an/2\u230b+1}}, and therefore all relevant Hessians have a block in the first \u2113t+1\u00d7 \u2113t+1 entries, hence it is impossible to create a vector with non-zero coordinates (using the operations of Assumption 2) beyond the first \u2113t+1.\nSince wT \u2286 WT , the lemma above implies that E[lT ] from Eq. (9) (where lT is the largest index of a non-zero coordinate of wT ) can be upper-bounded by E[\u2113T ], where the expectation is over the random draw of the indices j1, . . . , jd\u22121. This can be bounded using the following lemma: Lemma 8. It holds that E[\u2113T ] \u2264 1 + 2(T\u22121)n . Proof. By definition of \u2113t and linearity of expectation, we have\nE[\u2113T ] = E\n[ T\u22121 \u2211\nt=1\n(\u2113t+1 \u2212 \u2113t) ] + \u21131 = T\u22121 \u2211\nt=1\nE[\u2113t+1 \u2212 \u2113t] + 1. (10)\nLet us consider any particular term in the sum above. Since \u2113t+1 \u2212 \u2113t is a non-negative integer, we have E[\u2113t+1 \u2212 \u2113t] = Pr (\u2113t+1 > \u2113t) \u00b7 E [\u2113t+1 \u2212 \u2113t | \u2113t+1 > \u2113t] .\nBy definition of \u2113t, the event \u2113t+1 > \u2113t can occur only if j\u2113t /\u2208 {it\u22121, it\u22122, . . . , imax{1,t\u2212\u230an/2\u230b}}, yet j\u2113t \u2208 {it, it\u22121, . . . , imax{1,t\u2212\u230an/2\u230b+1}}. This is equivalent to j\u2113t = it (that is, in iteration t we happened to choose the index j\u2113t of the unique individual function, which contains the block linking coordinate \u2113t and \u2113t + 1, hence allowing us to \u201cadvance\u201d and have more non-zero coordinates). But since the algorithm is oblivious, it is fixed whereas j\u2113t is chosen uniformly at random, hence the probability of this event is 1/n. Therefore, Pr (\u2113t+1 > \u2113t) \u2264 1/n. Turning to the conditional expectation of \u2113t+1 \u2212 \u2113t above, it equals the expected number of indices j\u2113t , j\u2113t+1, . . . belonging to {it, it\u22121, . . . , imax{1,t\u2212\u230an/2\u230b+1}}, conditioned on j\u2113t belonging to that set. But since the i indices are fixed and the j indices are chosen uniformly at random, this equals one plus the expected number of times where a randomly drawn j \u2208 {1, . . . , n} belongs to {it, it\u22121, . . . , it\u2212\u230an/2\u230b+1}. Since this set contains at most \u230an/2\u230b distinct elements in {1, . . . , n}, this is equivalent to (one plus) the expectation of a geometric random variable, where the success probability is at most 1/2. By a standard derivation, this is at most 1 + 1/21\u22121/2 = 2. Plugging into the displayed equation above, we get that\nE[\u2113t+1 \u2212 \u2113t] \u2264 1 n \u00b7 2 = 2 n ,\nand therefore the bound in Eq. (10) is at most 2(T\u22121)n + 1 as required.\nPlugging this bound into Eq. (9), we get\nE\n[\nF (wT )\u2212 F (w\u22c6) F (0)\u2212 F (w\u22c6)\n]\n\u2265 \u03bb 2\u00b5 \u221a \u03ba (\u221a \u03ba\u2212 1\u221a \u03ba+ 1 ) 8(T\u22121) n +8 .\nTo make the right-hand side less than \u01eb, T must be such that\n(\u221a \u03ba\u2212 1\u221a \u03ba+ 1 ) 8(T\u22121) n +8 \u2264 2\u00b5 \u221a \u03ba\u01eb \u03bb ,\nwhich is equivalent to (\n8(T \u2212 1) n + 8\n)\nlog (\u221a \u03ba+ 1\u221a \u03ba\u2212 1 ) \u2265 log ( \u03bb 2\u00b5 \u221a \u03ba\u01eb ) .\nSince log (\u221a\n\u03ba+1\u221a \u03ba\u22121\n) = log (\n1 + 2\u221a \u03ba\u22121\n)\n\u2264 2\u221a \u03ba\u22121 , it follows that T must be such that\n(\n8(T \u2212 1) n + 8\n)\n2\u221a \u03ba\u2212 1 \u2265 log\n(\n\u03bb\n2\u00b5 \u221a \u03ba\u01eb\n)\n.\nPlugging in \u03ba = \u00b5/\u03bbn and simplifying, we get that\nT \u2265 1 + n 8\n(\n\u221a\n\u00b5/\u03bbn\u2212 1 2 \u00b7 log (\n(\u03bb/\u00b5)3/2 \u221a n\n2\u01eb\n) \u2212 8 ) .\nUsing asymptotic notation, and recalling the assumptions \u00b5/\u03bbn \u2265 2, the right-hand side equals\n\u2126\n(\nn \u00b7 \u221a \u00b5\n\u03bbn \u00b7 log\n( (\u03bb/\u00b5)3/2 \u221a n\n\u01eb\n))\n= \u2126\n(\n\u221a\nn\u00b5\n\u03bb \u00b7 log\n( (\u03bb/\u00b5)3/2 \u221a n\n\u01eb\n))\nas required. The bound on the dimension d follows from the fact that we chose it to be O(E[lT ]) = O(1 + T/n), and to make the lower bound valid it is enough to pick some T = O (\u221a\nn\u00b5 \u03bb \u00b7 log\n( (\u03bb/\u00b5)3/2 \u221a n\n\u01eb\n))\n."}], "references": [{"title": "A lower bound for the optimization of finite sums", "author": ["Alekh Agarwal", "Leon Bottou"], "venue": "arXiv preprint arXiv:1410.0723,", "citeRegEx": "Agarwal and Bottou.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal and Bottou.", "year": 2014}, {"title": "Second order stochastic optimization in linear time", "author": ["Naman Agarwal", "Brian Bullins", "Elad Hazan"], "venue": "arXiv preprint arXiv:1602.03943,", "citeRegEx": "Agarwal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2016}, {"title": "Communication complexity of distributed convex learning and optimization", "author": ["Yossi Arjevani", "Ohad Shamir"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Arjevani and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Arjevani and Shamir.", "year": 2015}, {"title": "Dimension-free iteration complexity of finite sum optimization problems", "author": ["Yossi Arjevani", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1606.09333,", "citeRegEx": "Arjevani and Shamir.,? \\Q2016\\E", "shortCiteRegEx": "Arjevani and Shamir.", "year": 2016}, {"title": "On the iteration complexity of oblivious first-order optimization algorithms", "author": ["Yossi Arjevani", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1605.03529,", "citeRegEx": "Arjevani and Shamir.,? \\Q2016\\E", "shortCiteRegEx": "Arjevani and Shamir.", "year": 2016}, {"title": "Exact and inexact subsampled newton methods for optimization", "author": ["Raghu Bollapragada", "Richard Byrd", "Jorge Nocedal"], "venue": "arXiv preprint arXiv:1609.08502,", "citeRegEx": "Bollapragada et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bollapragada et al\\.", "year": 2016}, {"title": "Convergence rates of sub-sampled newton methods", "author": ["Murat A Erdogdu", "Andrea Montanari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Erdogdu and Montanari.,? \\Q2015\\E", "shortCiteRegEx": "Erdogdu and Montanari.", "year": 2015}, {"title": "An optimal randomized incremental gradient method", "author": ["Guanghui Lan"], "venue": "arXiv preprint arXiv:1507.02000,", "citeRegEx": "Lan.,? \\Q2015\\E", "shortCiteRegEx": "Lan.", "year": 2015}, {"title": "Problem Complexity and Method Efficiency in Optimization", "author": ["A. Nemirovsky", "D. Yudin"], "venue": "WileyInterscience,", "citeRegEx": "Nemirovsky and Yudin.,? \\Q1983\\E", "shortCiteRegEx": "Nemirovsky and Yudin.", "year": 1983}, {"title": "Introductory lectures on convex optimization: A basic course, volume 87", "author": ["Yurii Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "Nesterov.,? \\Q2013\\E", "shortCiteRegEx": "Nesterov.", "year": 2013}, {"title": "Newton sketch: A linear-time optimization algorithm with linearquadratic convergence", "author": ["Mert Pilanci", "Martin J Wainwright"], "venue": "arXiv preprint arXiv:1505.02250,", "citeRegEx": "Pilanci and Wainwright.,? \\Q2015\\E", "shortCiteRegEx": "Pilanci and Wainwright.", "year": 2015}, {"title": "Sub-sampled newton methods i: globally convergent algorithms", "author": ["Farbod Roosta-Khorasani", "Michael W Mahoney"], "venue": "arXiv preprint arXiv:1601.04737,", "citeRegEx": "Roosta.Khorasani and Mahoney.,? \\Q2016\\E", "shortCiteRegEx": "Roosta.Khorasani and Mahoney.", "year": 2016}, {"title": "Sub-sampled newton methods ii: Local convergence rates", "author": ["Farbod Roosta-Khorasani", "Michael W Mahoney"], "venue": "arXiv preprint arXiv:1601.04738,", "citeRegEx": "Roosta.Khorasani and Mahoney.,? \\Q2016\\E", "shortCiteRegEx": "Roosta.Khorasani and Mahoney.", "year": 2016}, {"title": "Tight complexity bounds for optimizing composite objectives", "author": ["Blake Woodworth", "Nathan Srebro"], "venue": "arXiv preprint arXiv:1605.08003,", "citeRegEx": "Woodworth and Srebro.,? \\Q2016\\E", "shortCiteRegEx": "Woodworth and Srebro.", "year": 2016}, {"title": "Sub-sampled newton methods with non-uniform sampling", "author": ["Peng Xu", "Jiyan Yang", "Farbod Roosta-Khorasani", "Christopher R\u00e9", "Michael W Mahoney"], "venue": "arXiv preprint arXiv:1607.00559,", "citeRegEx": "Xu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "To study the complexity of this and other optimization problems, it is common to consider an oracle model, where the optimization algorithm has no a-priori information about the objective function, and obtains information from an oracle which provides values and derivatives of the function at various domain points [Nemirovsky and Yudin, 1983].", "startOffset": 316, "endOffset": 344}, {"referenceID": 0, "context": "either under algorithmic assumptions or assuming the dimension is sufficiently large2 [Agarwal and Bottou, 2014, Lan, 2015, Woodworth and Srebro, 2016, Arjevani and Shamir, 2016a]. This is matched (up to log factors) by existing approaches, and cannot be improved in general. An alternative to first-order methods are second-order methods, which also utilize Hessian information. A prototypical example is the Newton method, which given a (single) function F , performs iterations of the form wt+1 = wt \u2212 \u03b1t ( \u2207F (w) )\u2207F (w), (2) where \u2207F (w),\u22072F (w) are the gradient and the Hessian of F at w, and \u03b1t is a step size parameter. Secondorder methods can have extremely fast convergence, better than those of first-order methods (i.e. quadratic instead of linear). Moreover, they can be invariant to affine transformations of the objective function, and provably independent of its strong convexity and smoothness parameters (assuming e.g. self-concordance) [Boyd and Vandenberghe, 2004]. A drawback of these methods, however, is that they can be computationally prohibitive. In the context of machine learning, we are often interested in high-dimensional problems (where the dimension d is very large), and the Hessians are d\u00d7 d matrices which in general may not even fit into computer memory. However, for optimization problems as in Eq. (1), the Hessians of individual fi often have a special structure. For example, a very common special case of finite-sum problems in machine learning is empirical risk minimization for linear predictors, where fi(w) = li(\u3008w,xi\u3009), where xi is a training instance and li is some loss function. In that case, assuming li is twice-differentiable, the Hessian has the rank-1 form l\u2032\u2032 i (\u3008w,xi\u3009)xixi . Therefore, the memory and computational effort involved with storing and manipulating the Hessian of this function is merely linear (rather than quadratic) in d. Thus, it is tractable even for high-dimensional problems. Building on this, several recent papers proposed and analyzed second-order methods for finite-sum problems, which utilize Hessians of the individual functions fi (see for instance Erdogdu and Montanari [2015], Agarwal et al.", "startOffset": 87, "endOffset": 2162}, {"referenceID": 0, "context": "either under algorithmic assumptions or assuming the dimension is sufficiently large2 [Agarwal and Bottou, 2014, Lan, 2015, Woodworth and Srebro, 2016, Arjevani and Shamir, 2016a]. This is matched (up to log factors) by existing approaches, and cannot be improved in general. An alternative to first-order methods are second-order methods, which also utilize Hessian information. A prototypical example is the Newton method, which given a (single) function F , performs iterations of the form wt+1 = wt \u2212 \u03b1t ( \u2207F (w) )\u2207F (w), (2) where \u2207F (w),\u22072F (w) are the gradient and the Hessian of F at w, and \u03b1t is a step size parameter. Secondorder methods can have extremely fast convergence, better than those of first-order methods (i.e. quadratic instead of linear). Moreover, they can be invariant to affine transformations of the objective function, and provably independent of its strong convexity and smoothness parameters (assuming e.g. self-concordance) [Boyd and Vandenberghe, 2004]. A drawback of these methods, however, is that they can be computationally prohibitive. In the context of machine learning, we are often interested in high-dimensional problems (where the dimension d is very large), and the Hessians are d\u00d7 d matrices which in general may not even fit into computer memory. However, for optimization problems as in Eq. (1), the Hessians of individual fi often have a special structure. For example, a very common special case of finite-sum problems in machine learning is empirical risk minimization for linear predictors, where fi(w) = li(\u3008w,xi\u3009), where xi is a training instance and li is some loss function. In that case, assuming li is twice-differentiable, the Hessian has the rank-1 form l\u2032\u2032 i (\u3008w,xi\u3009)xixi . Therefore, the memory and computational effort involved with storing and manipulating the Hessian of this function is merely linear (rather than quadratic) in d. Thus, it is tractable even for high-dimensional problems. Building on this, several recent papers proposed and analyzed second-order methods for finite-sum problems, which utilize Hessians of the individual functions fi (see for instance Erdogdu and Montanari [2015], Agarwal et al. [2016], Pilanci and Wainwright [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al.", "startOffset": 87, "endOffset": 2185}, {"referenceID": 0, "context": "either under algorithmic assumptions or assuming the dimension is sufficiently large2 [Agarwal and Bottou, 2014, Lan, 2015, Woodworth and Srebro, 2016, Arjevani and Shamir, 2016a]. This is matched (up to log factors) by existing approaches, and cannot be improved in general. An alternative to first-order methods are second-order methods, which also utilize Hessian information. A prototypical example is the Newton method, which given a (single) function F , performs iterations of the form wt+1 = wt \u2212 \u03b1t ( \u2207F (w) )\u2207F (w), (2) where \u2207F (w),\u22072F (w) are the gradient and the Hessian of F at w, and \u03b1t is a step size parameter. Secondorder methods can have extremely fast convergence, better than those of first-order methods (i.e. quadratic instead of linear). Moreover, they can be invariant to affine transformations of the objective function, and provably independent of its strong convexity and smoothness parameters (assuming e.g. self-concordance) [Boyd and Vandenberghe, 2004]. A drawback of these methods, however, is that they can be computationally prohibitive. In the context of machine learning, we are often interested in high-dimensional problems (where the dimension d is very large), and the Hessians are d\u00d7 d matrices which in general may not even fit into computer memory. However, for optimization problems as in Eq. (1), the Hessians of individual fi often have a special structure. For example, a very common special case of finite-sum problems in machine learning is empirical risk minimization for linear predictors, where fi(w) = li(\u3008w,xi\u3009), where xi is a training instance and li is some loss function. In that case, assuming li is twice-differentiable, the Hessian has the rank-1 form l\u2032\u2032 i (\u3008w,xi\u3009)xixi . Therefore, the memory and computational effort involved with storing and manipulating the Hessian of this function is merely linear (rather than quadratic) in d. Thus, it is tractable even for high-dimensional problems. Building on this, several recent papers proposed and analyzed second-order methods for finite-sum problems, which utilize Hessians of the individual functions fi (see for instance Erdogdu and Montanari [2015], Agarwal et al. [2016], Pilanci and Wainwright [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al.", "startOffset": 87, "endOffset": 2216}, {"referenceID": 0, "context": "either under algorithmic assumptions or assuming the dimension is sufficiently large2 [Agarwal and Bottou, 2014, Lan, 2015, Woodworth and Srebro, 2016, Arjevani and Shamir, 2016a]. This is matched (up to log factors) by existing approaches, and cannot be improved in general. An alternative to first-order methods are second-order methods, which also utilize Hessian information. A prototypical example is the Newton method, which given a (single) function F , performs iterations of the form wt+1 = wt \u2212 \u03b1t ( \u2207F (w) )\u2207F (w), (2) where \u2207F (w),\u22072F (w) are the gradient and the Hessian of F at w, and \u03b1t is a step size parameter. Secondorder methods can have extremely fast convergence, better than those of first-order methods (i.e. quadratic instead of linear). Moreover, they can be invariant to affine transformations of the objective function, and provably independent of its strong convexity and smoothness parameters (assuming e.g. self-concordance) [Boyd and Vandenberghe, 2004]. A drawback of these methods, however, is that they can be computationally prohibitive. In the context of machine learning, we are often interested in high-dimensional problems (where the dimension d is very large), and the Hessians are d\u00d7 d matrices which in general may not even fit into computer memory. However, for optimization problems as in Eq. (1), the Hessians of individual fi often have a special structure. For example, a very common special case of finite-sum problems in machine learning is empirical risk minimization for linear predictors, where fi(w) = li(\u3008w,xi\u3009), where xi is a training instance and li is some loss function. In that case, assuming li is twice-differentiable, the Hessian has the rank-1 form l\u2032\u2032 i (\u3008w,xi\u3009)xixi . Therefore, the memory and computational effort involved with storing and manipulating the Hessian of this function is merely linear (rather than quadratic) in d. Thus, it is tractable even for high-dimensional problems. Building on this, several recent papers proposed and analyzed second-order methods for finite-sum problems, which utilize Hessians of the individual functions fi (see for instance Erdogdu and Montanari [2015], Agarwal et al. [2016], Pilanci and Wainwright [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016], Xu et al.", "startOffset": 87, "endOffset": 2284}, {"referenceID": 0, "context": "either under algorithmic assumptions or assuming the dimension is sufficiently large2 [Agarwal and Bottou, 2014, Lan, 2015, Woodworth and Srebro, 2016, Arjevani and Shamir, 2016a]. This is matched (up to log factors) by existing approaches, and cannot be improved in general. An alternative to first-order methods are second-order methods, which also utilize Hessian information. A prototypical example is the Newton method, which given a (single) function F , performs iterations of the form wt+1 = wt \u2212 \u03b1t ( \u2207F (w) )\u2207F (w), (2) where \u2207F (w),\u22072F (w) are the gradient and the Hessian of F at w, and \u03b1t is a step size parameter. Secondorder methods can have extremely fast convergence, better than those of first-order methods (i.e. quadratic instead of linear). Moreover, they can be invariant to affine transformations of the objective function, and provably independent of its strong convexity and smoothness parameters (assuming e.g. self-concordance) [Boyd and Vandenberghe, 2004]. A drawback of these methods, however, is that they can be computationally prohibitive. In the context of machine learning, we are often interested in high-dimensional problems (where the dimension d is very large), and the Hessians are d\u00d7 d matrices which in general may not even fit into computer memory. However, for optimization problems as in Eq. (1), the Hessians of individual fi often have a special structure. For example, a very common special case of finite-sum problems in machine learning is empirical risk minimization for linear predictors, where fi(w) = li(\u3008w,xi\u3009), where xi is a training instance and li is some loss function. In that case, assuming li is twice-differentiable, the Hessian has the rank-1 form l\u2032\u2032 i (\u3008w,xi\u3009)xixi . Therefore, the memory and computational effort involved with storing and manipulating the Hessian of this function is merely linear (rather than quadratic) in d. Thus, it is tractable even for high-dimensional problems. Building on this, several recent papers proposed and analyzed second-order methods for finite-sum problems, which utilize Hessians of the individual functions fi (see for instance Erdogdu and Montanari [2015], Agarwal et al. [2016], Pilanci and Wainwright [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016], Xu et al. [2016] and references therein).", "startOffset": 87, "endOffset": 2302}, {"referenceID": 8, "context": "Given a first-order oracle and a strongly convex and smooth objective in sufficiently high dimensions, it is well-known that the worst-case oracle complexity is \u03a9( \u221a \u03bc/\u03bb \u00b7 log(1/\u01eb)) [Nemirovsky and Yudin, 1983].", "startOffset": 182, "endOffset": 210}, {"referenceID": 1, "context": "Agarwal et al. [2016], Xu et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 1, "context": "Agarwal et al. [2016], Xu et al. [2016]. We note that the limitations arising from oblivious optimization schemes (in a somewhat stronger sense) was also explored in Arjevani and Shamir [2016a,b].", "startOffset": 0, "endOffset": 40}, {"referenceID": 8, "context": "As mentioned earlier, the observation that such first-order oracle bounds can be extended to higherorder oracles is also briefly mentioned (without proof) in Nemirovsky and Yudin [1983, Section 7.2.6]. Also, the theorem considers deterministic algorithms (which includes standard second-order methods, such as the Newton method), but otherwise makes no assumption on the algorithm. Generalizing this result to randomized algorithms should be quite doable, based on the techniques developed in Woodworth and Srebro [2016]. We leave a formal derivation to future work.", "startOffset": 158, "endOffset": 521}, {"referenceID": 5, "context": "Nesterov [2013], Lan [2015], as well as Arjevani and Shamir [2015] in a somewhat different context).", "startOffset": 0, "endOffset": 16}, {"referenceID": 4, "context": "Nesterov [2013], Lan [2015], as well as Arjevani and Shamir [2015] in a somewhat different context).", "startOffset": 17, "endOffset": 28}, {"referenceID": 2, "context": "Nesterov [2013], Lan [2015], as well as Arjevani and Shamir [2015] in a somewhat different context).", "startOffset": 40, "endOffset": 67}, {"referenceID": 10, "context": "A possible exception to this is the Newton sketch algorithm [Pilanci and Wainwright, 2015], which relies on random projections, but on the flip side is computationally expensive.", "startOffset": 60, "endOffset": 90}, {"referenceID": 5, "context": "There have been several rigorous studies of such \u201csubsampled Newton\u201d methods, such as Erdogdu and Montanari [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al.", "startOffset": 86, "endOffset": 115}, {"referenceID": 5, "context": "There have been several rigorous studies of such \u201csubsampled Newton\u201d methods, such as Erdogdu and Montanari [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016] and references therein.", "startOffset": 156, "endOffset": 183}, {"referenceID": 5, "context": "There have been several rigorous studies of such \u201csubsampled Newton\u201d methods, such as Erdogdu and Montanari [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016] and references therein. However, our lower bound in Thm. 2 holds for such an approach, since it satisfies both Assumption 1 and 2. As expected, the existing worst-case complexity upper bounds are no better than our lower bound. Xu et al. [2016] recently proposed a subsampled Newton method, together with non-uniform sampling, which assigns more weight to individual functions which are deemed more \u201cimportant\u201d.", "startOffset": 156, "endOffset": 428}, {"referenceID": 5, "context": "There have been several rigorous studies of such \u201csubsampled Newton\u201d methods, such as Erdogdu and Montanari [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016] and references therein. However, our lower bound in Thm. 2 holds for such an approach, since it satisfies both Assumption 1 and 2. As expected, the existing worst-case complexity upper bounds are no better than our lower bound. Xu et al. [2016] recently proposed a subsampled Newton method, together with non-uniform sampling, which assigns more weight to individual functions which are deemed more \u201cimportant\u201d. This is measured via properties of the Hessians of the functions, such as their norms or via leverage scores. This approach breaks Assumption 1, as the sampled indices are now chosen in a way dependent on the individual functions, but since this dependence is only through the Hessians, it still satisfies Assumption 1a. Therefore, our lower bound as stated in Thm. 3 still applies to such a method. A variant of the subsampled Newton approach, studied in Erdogdu and Montanari [2015], uses a lowrank approximation of the sample Hessian (attained by truncated SVD), in lieu of the sample Hessian itself.", "startOffset": 156, "endOffset": 1080}, {"referenceID": 5, "context": "There have been several rigorous studies of such \u201csubsampled Newton\u201d methods, such as Erdogdu and Montanari [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016] and references therein. However, our lower bound in Thm. 2 holds for such an approach, since it satisfies both Assumption 1 and 2. As expected, the existing worst-case complexity upper bounds are no better than our lower bound. Xu et al. [2016] recently proposed a subsampled Newton method, together with non-uniform sampling, which assigns more weight to individual functions which are deemed more \u201cimportant\u201d. This is measured via properties of the Hessians of the functions, such as their norms or via leverage scores. This approach breaks Assumption 1, as the sampled indices are now chosen in a way dependent on the individual functions, but since this dependence is only through the Hessians, it still satisfies Assumption 1a. Therefore, our lower bound as stated in Thm. 3 still applies to such a method. A variant of the subsampled Newton approach, studied in Erdogdu and Montanari [2015], uses a lowrank approximation of the sample Hessian (attained by truncated SVD), in lieu of the sample Hessian itself. However, this still falls in the framework of Assumption 2, and our lower bound still applies. A different approach to approximate the full Hessian is via randomized sketching techniques, which replace the Hessian \u22072F (w) by a low-rank approximation of the form (\u22072F (w))1/2SS\u22a4(\u22072F (w))1/2, where S \u2208 Rd\u00d7m,m \u226a d is a random sketching matrix, and (\u22072F (w))1/2 is the matrix square root of \u22072F (w). This approach forms the basis of the Newton sketch algorithm proposed in Pilanci and Wainwright [2015]. This approach currently escapes our lower bound, since it violates Assumption 2.", "startOffset": 156, "endOffset": 1699}, {"referenceID": 1, "context": "Agarwal et al. [2016] develop another line of stochastic second-order methods, which are based on the observation that the Newton step (\u22072F (w))\u22121\u2207F (w) is the solution of the system of linear equations \u22072F (w)x = \u2207F (w).", "startOffset": 0, "endOffset": 22}, {"referenceID": 1, "context": "That being said, it is important to note that the complexity upper bound attained in Agarwal et al. [2016] for LiSSA-Sample is on the order of \u00d5((n+ \u221a d\u03bc/\u03bb) log(1/\u01eb)) (asymptotically as \u01eb \u2192 0), which is better than our lower bound if d \u226a n.", "startOffset": 85, "endOffset": 107}, {"referenceID": 1, "context": "Based on the fact that only at most d \u2212 1 out of n functions are relevant in the construction, we conjecture that the possible improvement in the oracle complexity of such schemes may amount to replacing dependencies on n with dependencies on d, which is indeed the type of improvement attained (for small enough \u01eb) in Agarwal et al. [2016]. 9", "startOffset": 319, "endOffset": 341}, {"referenceID": 1, "context": "Finally, we note that Agarwal et al. [2016] proposes another algorithm tailored to self-concordant functions, with runtime independent of the smoothness and strong convexity parameters of the problem.", "startOffset": 22, "endOffset": 44}, {"referenceID": 1, "context": "Finally, we note that Agarwal et al. [2016] proposes another algorithm tailored to self-concordant functions, with runtime independent of the smoothness and strong convexity parameters of the problem. However, it requires performing \u2265 1 full Newton steps, so the runtime is prohibitive for large-scale problems (indeed, for quadratics as used in our lower bounds, even a single Newton step suffices to compute an exact solution). 5 Summary and Discussion In this paper, we studied the oracle complexity for optimization problems, assuming availability of a secondorder oracle. This is in contrast to most existing oracle complexity results, which focus on a first-order oracle. First, we formally proved that in the standard setting of strongly-convex and smooth optimization problems, second-order information does not significantly improve the oracle complexity, and further assumptions (i.e. Lipschitzness of the Hessians) are in fact necessary. We then presented our main lower bounds, which show that for finite-sum problems with a second-order oracle, under some reasonable algorithmic assumptions, the resulting oracle complexity is \u2013 again \u2013 not significantly better than what can be obtained using a first-order oracle. Moreover, this is shown using quadratic functions, which have 0 derivatives of order larger than 2. Hence, our lower bounds apply even if we have access to an oracle returning derivatives of order p for all p \u2265 0, and the function is smooth to any order. In Sec. 4, we studied how our framework and lower bounds are applicable to most existing approaches. Although this conclusion may appear very pessimistic, they are actually useful in pinpointing potential assumptions and approaches which may circumvent these lower bounds. In particular: \u2022 Our lower bound for algorithms employing non index-oblivious sampling schemes (Thm. 3) only hold when the dimension d is very large. This leaves open the possibility of better (non index-oblivious) algorithms when d is moderate, as was recently demonstrated in the context of the LiSSA-Sample algorithm of Agarwal et al. [2016] (at least for small enough \u01eb).", "startOffset": 22, "endOffset": 2102}, {"referenceID": 5, "context": "Bollapragada et al. [2016], Xu et al.", "startOffset": 0, "endOffset": 27}, {"referenceID": 5, "context": "Bollapragada et al. [2016], Xu et al. [2016], Roosta-Khorasani and Mahoney [2016a], Pilanci and Wainwright [2015]) show that various algorithms can have an initial super-linear convergence rate, which slows down to linear as one gets closer to the optimum.", "startOffset": 0, "endOffset": 45}, {"referenceID": 5, "context": "Bollapragada et al. [2016], Xu et al. [2016], Roosta-Khorasani and Mahoney [2016a], Pilanci and Wainwright [2015]) show that various algorithms can have an initial super-linear convergence rate, which slows down to linear as one gets closer to the optimum.", "startOffset": 0, "endOffset": 83}, {"referenceID": 5, "context": "Bollapragada et al. [2016], Xu et al. [2016], Roosta-Khorasani and Mahoney [2016a], Pilanci and Wainwright [2015]) show that various algorithms can have an initial super-linear convergence rate, which slows down to linear as one gets closer to the optimum.", "startOffset": 0, "endOffset": 114}, {"referenceID": 7, "context": "1 Auxiliary Lemmas The following lemma was essentially proven in Lan [2015], Nesterov [2013], but we provide a proof for completeness: Lemma 1.", "startOffset": 65, "endOffset": 76}, {"referenceID": 7, "context": "1 Auxiliary Lemmas The following lemma was essentially proven in Lan [2015], Nesterov [2013], but we provide a proof for completeness: Lemma 1.", "startOffset": 65, "endOffset": 93}, {"referenceID": 13, "context": "1 The proof is inspired by a technique introduced in Woodworth and Srebro [2016] for analyzing randomized first-order methods, in which a quadratic function is \u201clocally flattened\u201d in order to make first-order (gradient) information non-informative.", "startOffset": 53, "endOffset": 81}], "year": 2017, "abstractText": "Finite-sum optimization problems are ubiquitous in machine learning, and are commonly solved using first-order methods which rely on gradient computations. Recently, there has been growing interest in second-order methods, which rely on both gradients and Hessians. In principle, second-order methods can require much fewer iterations than first-order methods, and hold the promise for more efficient algorithms. Although computing and manipulating Hessians is prohibitive for high-dimensional problems in general, the Hessians of individual functions in finite-sum problems can often be efficiently computed, e.g. because they possess a low-rank structure. Can second-order information indeed be used to solve such problems more efficiently? In this paper, we provide evidence that the answer \u2013 perhaps surprisingly \u2013 is negative, at least in terms of worst-case guarantees. However, we also discuss what additional assumptions and algorithmic approaches might potentially circumvent this negative result.", "creator": "LaTeX with hyperref package"}}}