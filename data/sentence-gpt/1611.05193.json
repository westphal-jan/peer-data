{"id": "1611.05193", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "Bayesian optimization of hyper-parameters in reservoir computing", "abstract": "We describe a method for searching the optimal hyper-parameters in reservoir computing, which consists of a Gaussian process with Bayesian optimization. It provides an alternative to other frequently used optimization methods such as grid, random, or manual search. In addition to a set of optimal hyper-parameters, the method also provides a probability distribution of the cost function as a function of the hyper-parameters. We apply this method to two types of reservoirs: nonlinear delay nodes and echo state networks. It shows excellent performance on all considered benchmarks, either matching or significantly surpassing expert human optimization. We find that some values for hyper-parameters that have become standard in the research community, are in fact suboptimal for most of the problems we considered. In general, the algorithm achieves optimal results in fewer iterations when compared to other optimization methods, and scales well with increasing dimensionality of the hyper-parameter space. Due to its automated nature, this method significantly reduces the need for expert knowledge when optimizing the hyper-parameters in reservoir computing. Existing software libraries for Bayesian optimization make the implementation of the algorithm straightforward. In addition, our approach also contains an alternative to the usual Bayesian optimization algorithms, which include the assumption that the probability of error would be the largest on all other approaches. In addition, we have already introduced the following optimization method.\n\n\nThe procedure is performed for the prediction and prediction of the probability of the problem by using Bayesian approximation. This procedure is executed using a nonlinear step method in which a given node is shown to be an optimal hyper-parameter and an alternative to the nonlinear step method. The solution is then performed for the prediction and prediction of the probability of the problem by using Bayesian approximation. This procedure is executed using a nonlinear step method in which a given node is shown to be an optimal hyper-parameter and an alternative to the nonlinear step method. The solution is then performed for the prediction and prediction of the probability of the problem by using Bayesian approximation. This procedure is executed using a nonlinear step method in which a given node is shown to be an optimal hyper-parameter and an alternative to the nonlinear step method. The solution is then executed using a nonlinear step method in which a given node is shown to be an optimal hyper-parameter and an alternative to the nonlinear step method. The solution is then executed using a nonlinear step method in which a given node is shown to be an optimal hyper-parameter and an alternative", "histories": [["v1", "Wed, 16 Nov 2016 09:25:17 GMT  (306kb,D)", "https://arxiv.org/abs/1611.05193v1", null], ["v2", "Mon, 13 Feb 2017 10:23:28 GMT  (990kb,D)", "http://arxiv.org/abs/1611.05193v2", null], ["v3", "Wed, 14 Jun 2017 13:26:38 GMT  (992kb,D)", "http://arxiv.org/abs/1611.05193v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jan yperman", "thijs becker"], "accepted": false, "id": "1611.05193"}, "pdf": {"name": "1611.05193.pdf", "metadata": {"source": "CRF", "title": "Bayesian optimization of hyper-parameters in reservoir computing", "authors": ["Jan Ypermana", "Thijs Becker"], "emails": [], "sections": [{"heading": null, "text": "We describe a method for searching the optimal hyper-parameters in reservoir computing, which consists of a Gaussian process with Bayesian optimization. It provides an alternative to other frequently used optimization methods such as grid, random, or manual search. In addition to a set of optimal hyper-parameters, the method also provides a probability distribution of the cost function as a function of the hyper-parameters. We apply this method to two types of reservoirs: nonlinear delay nodes and echo state networks. It shows excellent performance on all considered benchmarks, either matching or significantly surpassing results found in the literature. In general, the algorithm achieves optimal results in fewer iterations when compared to other optimization methods. We have optimized up to six hyper-parameters simultaneously, which would have been infeasible using, e.g., grid search. Due to its automated nature, this method significantly reduces the need for expert knowledge when optimizing the hyper-parameters in reservoir computing. Existing software libraries for Bayesian optimization, such as Spearmint, make the implementation of the algorithm straightforward. A fork of the Spearmint framework along with a tutorial on how to use it in practice is available at https://bitbucket.org/uhasseltmachinelearning/spearmint/\nKeywords: reservoir computing, hyper-parameter optimization, Gaussian processes, Bayesian statistics"}, {"heading": "1. Introduction", "text": "Error backpropagation combined with stochastic gradient descent (SGD) is a simple and highly successful training method for feedforward neural networks [1, 2]. In contrast, training recurrent neural networks with this method poses considerable difficulties [3]. Different approaches such as, e.g., more complicated training schemes [4] or different architectures [5, 6] have been explored to tackle this problem, and have been quite successful. A popular method skips the training stage of the recurrent network completely. Only the connections of the network to the output are trained. As a result, the training stage is computationally fast and straightforward to implement. This training paradigm goes by the name of reservoir computing (RC) [7]. Despite its simplicity compared to other techniques, it achieves state-of-the-art results on several machine learning tasks [7]. However, on large datasets and more challenging benchmarks, RC techniques fall short of e.g. LSTMs [5], which have become feasible in recent years. Therefore, RC techniques are now mainly being developed in the form of hardware implementations, using e.g. photonics [8, 9, 10, 11] or electronics [12], which could be computationally faster compared to digital ones.\nAlthough the training stage is dramatically simplified, one still needs to set several hyper-parameters that determine the general properties of the network, such as its size and spectral radius. Hyper-parameter optimization (HO) requires an experienced researcher, i.e., acquiring optimal results still necessitates expert input [13]. It is, therefore, of interest to automate the search for hyper-parameters in reservoir computing. A straightforward HO method is grid or random search, which is suitable for finding the optimum when considering only a few hyper-parameters. If there are many hyper-parameters such an approach is not viable, because the volume of the hyper-parameter space grows exponentially with the number of hyper-parameters.\n\u2217Corresponding author Email addresses: jan_yperman@uhasselt.be (Jan Yperman), thijs.becker@uhasselt.be (Thijs Becker)\nar X\niv :1\n61 1.\n05 19\n3v 3\n[ cs\n.L G\n] 1\n4 Ju\nn 20\n17\nA step by step plan for manual HO for RC is provided in [7]. As noted in [7], automated HO is a common topic in machine learning, and these methods are applicable to RC. A few methods have been proposed for automated HO, including Particle Swarm Optimization (PSO) [14, 15, 16], various forms of Genetic Algorithms (GAs) [17, 18, 19], and stochastic gradient descent applied to the hyper-parameters [13].\nIn this paper, we show that a Gaussian process with Bayesian optimization is able to achieve an optimal choice for RC hyper-parameters in an automated way. The method is applied to two types of reservoirs: dynamical nonlinear delay nodes (NDNs) and echo state networks (ESNs). For both the NDN and ESN, our results either match the considered benchmarks, or surpass them by one or several orders of magnitude. The implementation of Gaussian processes with Bayesian optimization is non-trivial. We therefore use the Spearmint library [20], which was developed in the context of HO in machine learning. HO with Bayesian optimization matches or surpasses human performance on several machine learning tasks and for different algorithms, such as deep learning and support vector machines [20]. However, it seems to fail completely on other tasks [21]. Because we achieve good results for all considered types of reservoirs and benchmarks, our work indicates that its performance for HO in RC is robust.\nThe paper is organized as follows. In Section 2, we describe the ESN and NDN formalisms. The theory of Bayesian optimization of Gaussian processes is briefly sketched in Section 3. In Section 4, we describe the benchmark tasks. Details of the implementation are discussed in Section 5. In Section 6, we present and discuss the results. In Section 7, we discuss the behaviour of the Bayesian search process. We end with a conclusion and outlook to possible future work in Section 8."}, {"heading": "2. Reservoir Computing", "text": ""}, {"heading": "2.1. Echo state networks", "text": "Echo state networks were introduced independently by Herbert Jaeger [22] and Wolfgang Maass (under the name Liquid State Machines [23]). The idea is to use a recurrent neural network which is fixed, i.e., its connection weights and any biases are not trained. One can drive this network using an input, which will change the state of the network (i.e., the value at each of the nodes). The output is then constructed using a linear combination of all, or a subset of, the node values. The weights of this linear combination are most commonly obtained using linear regression. The values of the nodes are updated according to the rule:\nx (n+ 1) = \u03c3 ( Wx(n) + W inu(n+ 1) + b ) , (1)\nwhere x(n) is the N -dimensional state vector of the network at time n, u(n) is the K-dimensional input vector at time n, W is the N \u00d7N reservoir weight matrix, W in is the N \u00d7K input weight matrix, b is a constant bias term and \u03c3(.) is a sigmoid function (in our case we use the tanh function).\nThe reservoir weight matrix is rescaled as W \u2032 = \u03c1W / |\u03bbmax|, where |\u03bbmax| is the spectral radius of the network (i.e., the largest eigenvalue of W ) and \u03c1 is a scaling parameter (effectively the spectral radius of the rescaled network). This rescaling operation is a popular choice in the reservoir computing literature. It, however, does not guarantee the echo state property [24]. See, e.g., [25, 26] for rigorous discussions on sufficient conditions for the echo state property.\nThe output is given by: y(n) = Uoutx(n), (2)\nwhere the output weights matrix Uout is obtained using, e.g., linear regression. We performed linear regression using ridge regression (a.k.a. Tikhonov regularization), which is a modified version of the linear regression equations [7]:\nUout = ( XTX + \u03bb2I )\u22121 XTy, (3)\nwhere I is the identity matrix, \u03bb is the regularization parameter, and X is the matrix of all reservoir states. Performance is measured with the normalized mean squared error (NMSE):\nNMSE = \u3008\u2016y\u0302(t)\u2212 y(t)\u20162\u3009 \u3008\u2016y(t)\u2212 \u3008y(t)\u3009\u20162\u3009 , (4)\nwhere y\u0302(t) is the predicted value and y(t) is the target value. The denominator is simply the variance of the test set under consideration. The root mean squared version (NRMSE) is:\nNRMSE = \u221a \u3008\u2016y\u0302(t)\u2212 y(t)\u20162\u3009 \u3008\u2016y(t)\u2212 \u3008y(t)\u3009\u20162\u3009 . (5)"}, {"heading": "2.2. Nonlinear delay nodes", "text": "The concept of a nonlinear delay node as a reservoir was introduced in [27]. It can be implemented in hardware with optical and electronic components [28, 29, 30, 12, 31, 8, 9, 10, 11, 12]. We use two different types of NDNs."}, {"heading": "2.2.1. Mackey-Glass delay differential equation", "text": "Reservoir states can be computed by solving a delay differential equation. In this work, we consider the Mackey-Glass (MG) differential equation with delay time \u03c4 :\nX\u0307(t) = \u2212X(t) + \u03b7 X(t\u2212 \u03c4) + \u03b3J(t) 1 + [X(t\u2212 \u03c4) + \u03b3J(t)]p , (6)\nwhere \u03b3, \u03b7, and p are adjustable parameters, X(t) is the state of the system, and J(t) is the input. We note that other delay differential equations can be used equally well.\nThe reservoir states are calculated as follows. Consider a discrete (or discretized) one-dimensional signal u(k), with k integer. We want to map each scalar value u(k) to a reservoir state of dimension N . This reservoir state is obtained by solving the MG differential equation (6). Solving over a time interval \u03c4 enables us to compute one reservoir state. Suppose we start at time t = 0 and end at t = \u03c4 . The input J(t) is calculated by multiplying u(k) with a mask M . For a reservoir of dimension N , the mask is a set of N values, which we take to be either -1 or 1: M is a random element from the set {\u22121, 1}N . By multiplying the mask with the signal, J(t) = M \u00d7 u(k), one finds N values, equal to \u00b1u(k), which are constant over a time \u03c4/N\n[J(0), J(\u03c4/N)] = M(0)u(k)\n]J(\u03c4/N), J(2\u03c4/N)] = M(1)u(k)\n. . .\n]J((N \u2212 1)\u03c4/N), J(\u03c4)] = M(N)u(k).\nThis preprocessing sequence of the signal is illustrated in Figure 1. Given the input J(t), the MG equation can be solved. The reservoir state xi, i \u2208 {1, . . . , N}, is equal to the value of X(t) at time t = i\u03c4/N . The following reservoir state is then calculated by solving equation (6) from t = \u03c4 to t = 2\u03c4 with input M \u00d7 u(k + 1), and so on. The times at which the solution to the MG equation is read can be interpreted as nodes from a network. They are referred to as \u201cvirtual nodes\u201d. Training of the connections between the reservoir state and the output is done the same as with ESNs. The whole NDN system is illustrated in Figure 2.\nIt is clear that the reservoir states depend on the previous state, via X(t \u2212 \u03c4), and the input, via J(t). The delay-dynamical equation therefore functions as a recurrent network. It is important that the values of the input mask M alternate. Otherwise, the differential equation relaxes to its stationary value, and the dynamics cannot perform a projection to a rich set of reservoir states, cf. the discussion in [27].\nThe parameters in the MG dynamics equation (6) have specific functions. The input scaling \u03b3 determines how strongly the dynamics is influenced by new input values J(t). \u03b7 determines the influence of both J(t) and X(t \u2212 \u03c4). The parameter p determines the nonlinearity of the dynamics. If other nonlinear delay differential equations are used, the parameters in those equations will have similar roles."}, {"heading": "2.2.2. Sine", "text": "First proposed in [29], this version of the nonlinear delay node uses an architecture similar to that of [27] (reviewed in Section 2.2.1), but with a simpler dynamics. The update rule for the reservoir states is given by:\nxi(n) = { sin (\u03b1xi\u2212k (n\u2212 1) + \u03b2miu(n) + \u03c6) k \u2264 i < N sin (\u03b1xN+i\u2212k (n\u2212 2) + \u03b2miu(n) + \u03c6) 0 \u2264 i < k,\n(7)\nwhere \u03b1, \u03b2 and \u03c6 are adjustable parameters, xi(n) represents the state of the ith virtual node at discrete timestep n, N is the total number of virtual nodes, mi represents the ith value of the mask (chosen at random from {\u22121, 1} as before), u(n) represents the input at discrete timestep n and k \u2208 {1, . . . , N \u2212 1}. In our implementation we set k = bN/3c, where b.c denotes the floor function. Further details can be found in [29]."}, {"heading": "3. Gaussian Processes", "text": "When fitting a function, several approaches are possible. One option is to consider a class of functions, for example all linear functions. Another approach is to specify a probability distribution over all possible functions, where more likely functions have higher probabilities. The latter approach can be achieved with Gaussian processes (GPs). A GP is equivalent to Bayesian linear regression with an infinite number of basis functions [32]. It is therefore possible to fit a large class of functions exactly. A function f(x) fitted by a GP probability distribution is denoted by\nf(x) \u223c GP(m(x), k(x,x\u2032)). (8)\nThe vector x are the hyper-parameters of the reservoir and f(x) is the error function, e.g., f(x) = NMSE({\u03b3, \u03b7, p}) for the Mackey-Glass NDN. Note that the symbol x is also used for the reservoir states. Which one is meant should be clear from the context. Gaussian processes are able to describe probability distributions over complicated functions, which is what we need for the optimization algorithm. For an introduction to Gaussian processes, we refer to [32]. Several applications are discussed in [33].\nA GP is completely specified by its mean function m(x) and covariance function k(x,x\u2032). m(x) gives the average function value at x. Values at different positions x and x\u2032 are correlated by an amount k(x,x\u2032), also called the kernel. In practice, this kernel is used to ensure that points close to each other in hyperparameter space are likely to have similar values, which follows from an assumption about the behaviour of f(x). Intuitively, we expect small changes in the objective function if the hyper-parameters are changed slightly.\nA GP that models f(x) well is achieved by performing measurements of f(x), and consequently updating m(x) and k(x,x\u2032) to take the obtained information into account. This updating is done with Bayesian statistics [34]. Before any measurements are performed, one needs to set a prior, i.e., guess a reasonable form for m(x) and k(x,x\u2032). An example would be m(x) = 0 for all x and the covariance function\nk(xi,xj) = exp\n( \u2212 1\n2\u03be2 |xi \u2212 xj |2\n) , (9)\nwith \u03be an adjustable parameter. This covariance function is a popular choice, although it is too smooth for most machine learning problems. In this paper, the automatic relevance detection Mate\u0301rn 5/2 kernel is used [20]. After each measurement one can update m(x) and k(x,x\u2032) to obtain the posterior GP. An illustration of a GP and its evolution when measurements are performed is shown in Figure 3. We note that it is possible to incorporate the effect of noise in the measurement process.\nOne can analytically calculate the mean and variance of the function distribution at each x. The choice of where to perform a new measurement is not only determined by the improvement in the average of f(x) (exploitation). It is also influenced by how much new information we gain by performing the measurement,\nwhich is determined by the reduction in variance (exploration). The balance between exploitation and exploration is determined by the acquisition function, which needs to be chosen by the researcher. The acquisition function used here is the expected improvement. An example of this acquisition function is shown in Figure 3. A discussion of several acquisition functions can be found in [33].\nThe hyper-parameters of the GP itself, such as \u03be in Eq. (9), can also be optimized. A more practical method is to average these parameters out, e.g. with Bayesian statistics. This leads to a completely parameter free algorithm. The implementation of the GP with Bayesian updating is done with Spearmint [20]. This library contains several non-trivial extensions, such as averaging out the hyper-parameters of the GP with Bayesian statics, and input warping [35]. Input warping deals with the problem of f(x) being non-stationary, i.e., the covariance function k(xi,xj) is not invariant under translations in the input space. Error functions in machine learning are generally non-stationary: different regions in the hyper-parameter space have different length scales of function variation. Therefore, a choice of covariance function such as in Eq. (9), which is invariant under translations, cannot model most error functions well. Input warping can be seen as applying a particular kind of non-stationary kernel to the original data, which solves the problem of the non-stationarity of the error function.\nOur final choice for the optimal hyper-parameters is the best set found during the search. An interesting question is which functions can be fitted with a GP. A discussion on the convergence of the GP to the average of the underlying function f(x) can be found in [32]. There are two necessary conditions for convergence in the limit of an infinite number of measurements: (1) the covariance function k(x,x\u2032) should be non-degenerate, which is the case for us; (2) the average of the underlying function should be sufficiently well-behaved, so that it can be represented by a generalized Fourier series. Of more practical importance is whether the search for the minimum converges. This depends on the acquisition function, the function to be estimated, and the prior on the GP. Sufficient conditions for the convergence of GPs with Bayesian optimization were given by Moc\u030ckus [36]. His assumptions are quite general, so are expected to apply to a large number of error functions, although it is difficult to numerically check whether they are fulfilled. Depending on the implementation, the expected improvement acquisition function can converge near-optimally, although it is possible that the algorithm does not converge at all [37]. We note that, for practical purposes, convergence to the absolute minimum is not necessary. Rather, we are interested in a \u201cgood enough\u201d minimum. Besides asymptotic behaviour, one is also interested in the rate of convergence. Recent work has shown that for the upper confidence bound acquisition function, there are algorithms for which convergence of a GP to the optimum is exponentially fast [38, 39]. As discussed in Section 6, the convergence behaviour we observe is also approximately exponential.\nWe now briefly discuss the differences with other optimization methods. The fit for unseen parameter regions is non-local: it takes into account the information of all measurements. This should be contrasted with SGD, where only local information of the error function determines the parameter update. There is also no exploration component in SGD. If the connection weights of the reservoir output are high, the SGD learning rate becomes exponentially slow [13]. This problem does not occur for GPs. When optimizing hyperparameters step by step [7], the hyper-parameters are optimized with all others fixed. The interdependency of the hyper-parameters is therefore not fully exploited. This inevitably leads to a sub-optimal end result. In contrast, such interdependency can be represented and taken advantage of with a GP. In contrast to random search, the new measurements are directed towards interesting regions, which means we need far fewer measurements to get good results. This compensates for the slight increase in computation time needed to calculate and update the GP."}, {"heading": "4. Benchmark Tasks", "text": "In this section we describe the data sets we use to test the performance of the algorithm."}, {"heading": "4.1. Santa Fe", "text": "The Santa Fe data set consists of the output of a chaotic laser system [40]. We use the A.cont data set, which can be obtained from [41]. The goal is to predict the next step of the time series."}, {"heading": "4.2. NARMA 10", "text": "The Nonlinear Auto-Regressive Moving Average (NARMA) of order 10 was originally introduced for use as a timeseries prediction benchmark in [42]. It is generated by\ny(t+ 1) = 0.3y(t) + 0.05y(t) 9\u2211 i=0 y(t\u2212 i) + 1.5s(t\u2212 9)s(t) + 0.1, (10)\nwith s(t) a random term with uniform distribution in [0, 0.5]. The goal is to do a one-step ahead prediction of y(t)."}, {"heading": "4.3. Nonlinear channel equalization", "text": "This task was first introduced in [43] and has since been used as a benchmark several times [44, 29, 45]. The data set is created as follows: An i.i.d. sequence d(t) is generated by randomly choosing values from {\u22123,\u22121, 1, 3}. This signal is then passed through a linear channel described by:\nq(n) =0.08d (n+ 2)\u2212 0.12d (n+ 1) + d (n) + 0.18d (n\u2212 1)\u2212 0.1d (n\u2212 2) + 0.091d (n\u2212 3) \u2212 0.05d (n\u2212 4) + 0.04d (n\u2212 5) + 0.03d (n\u2212 6) + 0.01d (n\u2212 7) ,\nwhich in turn is passed through a nonlinear channel:\nu(n) = q(n) + 0.036q(n)2 \u2212 0.011q(n)3 + \u03bd(n),\nwhere \u03bd(n) is an i.d.d. Gaussian noise with zero mean and a standard deviation adjusted to yield signal-tonoise ratios (SNR) ranging from 12 to 32 dB. The objective is to reconstruct d(n\u2212 2) given u(n), for several values of the SNR (so u(n) is the input and the target is y(n) = d(n \u2212 2)). Following [44, 46], we shifted u(n) by +30. The quality measure of the algorithm is given by the Symbol Error Rate (SER), which is the fraction of incorrect symbols obtained."}, {"heading": "5. Implementation", "text": "Before discussing the benchmarks, we give some details on how the algorithms are implemented."}, {"heading": "5.1. Mackey-Glass dynamics implementation", "text": "If p is uneven, the MG dynamics Eq. (6) is unstable for negative values. We therefore rescale u(k) so that it is always positive. We furthermore add an offset parameter \u03b4 \u2265 1 to the mask M \u2208 {\u22121 + \u03b4, 1 + \u03b4}N .\nFor comparison purposes, the number of nodes N is fixed. The (time) distance between the nodes is denoted by \u03b8. A larger \u03b8 gives a longer relaxation time between the nodes. By definition one has that \u03c4 = N\u03b8.\nWe employ Euler integration to solve the MG equation. The number of integration steps between adjacent nodes determines the precision of the obtained solution. However, the method used to solve the differential equation does not necessarily influence the quality of the reservoir. Put differently, one does not need an exact solution of the differential equation to have a high-quality reservoir. The essential property of the reservoir is a \u201cgood\u201d nonlinear projection to a high-dimensional space, which is not necessarily the exact solution of the differential equation. We perform Euler integration with either several integration steps between adjacent nodes (number of steps 2|b\u03b8/0.1\u2212 1c|+ 2), or with one integration step between adjacent nodes. We refer to these methods as, respectively, multi-step integration (MSI) and one-step integration (OSI). Both choices give good results, with the latter method obviously being faster.\nFor all NDN implementations, ridge regression is performed with sci-kit learn [47]."}, {"heading": "5.2. Echo state network", "text": "To implement the echo state networks we made use of the software library Oger [48], which in turn builds on the Modular toolkit for Data Processing (MDP) [49]."}, {"heading": "5.3. Bayesian optimization", "text": "For the implementation of the GP we used the Spearmint library, available at https://github.com/ HIPS/Spearmint. A fork of this framework, as well as a step-by-step tutorial detailing both installation and a practical example (NDN MG on NARMA 10), is available at https://bitbucket.org/uhasseltmachinelearning/ spearmint/. This code replaces the MongoDB backend with an SQLite one. This removes the need for a background server process, which makes it easier to deploy in a cluster environment or as part of a Jupyter Notebook."}, {"heading": "6. Results", "text": ""}, {"heading": "6.1. Santa Fe laser data", "text": "For the MG NDN, we compare our results with those of [50]. Details of the implementation can be found in Appendix A. As in [50], we take N = 200. Before the other parameters are optimized, ten different masks are generated, and the best performing one is selected. There are six parameters to optimize: p, \u03b3, \u03b7, \u03b8, \u03b4, and the regularization parameter \u03bb. Spearmint runs are performed for both OSI and MSI. One needs to specify the boundaries of the hyper-parameter search space. We therefore start by running Spearmint for different boundary settings, to find reasonable areas for the hyper-parameter search. Correct order of magnitudes for the boundaries can also be estimated from previously published results and physical arguments. For different boundary values, the optimum for p always converged to a value close to 1. Because a smaller number of variables implies a smaller search space, especially in high dimensions, we set p = 1 in our final run.\nThe final results are shown in Table 1. The NMSE for the MSI is an order of magnitude lower than [50], while the OSI result is four times lower. A plot of the NMSE on the validation set as a function of the number of Spearmint iterations is shown in Figure 4. If we start with good hyper-parameter boundaries, the first runs show a strong decrease in error, after which further improvements come in steps. This behaviour qualitatively resembles the exponential convergence speed found in analytical work, cf. Section 3. It should be noted that different runs with small changes in the boundaries of the search space lead to parameter sets that are quite different, but have the same performance.\nSome error surfaces are shown in Appendix B. One sees that the error surface is smooth around the optimum parameter values for both OSI and MSI. The error surfaces of the OSI and MSI differ significantly. The error surface is non-stationary. For example, the variation in the \u03b3 direction is low near the optimum, Figure B.10, while it is high away from the optimum, Figure B.12. A difference in spatial variation between different parameters can be seen in Figure B.11, where \u03b3 exhibits a slow variation and p a faster variation. This difference in variation is taken care of by Spearmint, as discussed in Section 3. Note that \u03b4 has little to no influence on the performance. Given the smoothness of the error surface, we expect good convergence behaviour from the GP, as is witnessed in the results.\nWe now compare our optimal parameters to those from reference [50]. One should keep in mind that the integration schemes are slightly different from the implementation in [50], even though Euler integration is also used there. Our programs are written in Python, while Matlab is used in [50]. For reference, our\nMSI method with Appeltant\u2019s parameters gives NMSE = 0.028, which is similar to his result NMSE = 0.02 (note that we don\u2019t know all the parameters of [50], some are educated guesses). All parameters differ at most one order of magnitude. It appears that higher values of \u03b3 and \u03b7 are useful, although this seems to be specific to the Santa Fe task. A more important conclusion is that the value of \u03b8 = 0.2 is suboptimal. Larger \u03b8 values allow for better performance in most considered benchmark tasks. This is important, because the choice \u03b8 = 0.2 has become popular in the literature [28, 12, 51, 52, 53, 30, 54, 55, 56, 57]. A larger \u03b8 causes a longer calculation time for the MSI. The OSI calculation time is independent of \u03b8, and has the same order of magnitude improvement in NMSE.\nIn addition to the NDN, we studied an ESN to compare the performance of the Bayesian optimization algorithm to the results from [58], who use binary PSO to optimize the ESN architecture. They use 6400 samples for training, 1700 for validation and another 1700 for testing. The optimized parameters were the input scaling (which is a scaling parameter of W in in Eq. (1)), spectral radius \u03b1 and the ridge regression parameter \u03bb. The number of nodes in the network was set to N = 200. The validation error was averaged over 10 independent realizations of the ESN. Using Bayesian optimization on these parameters we obtained a NMSE of 0.00694 \u00b1 0.00087, which should be compared to 0.0284 in [58] for their standard ESN, and 0.0143 after applying their optimization algorithm. This is a significant improvement compared to their results. During testing an irregularity in the data set was found, on which we elaborate in Appendix A. In the partitioning we study, the irregularity is part of the validation set. It should be noted that the irregularity may have driven the optimization to a hyper-parameter set that minimizes the error caused by the irregularity, which isn\u2019t necessarily the optimal set for prediction on the test set, as no such deviations occur there. In order to check the performance of the ESN on the Santa Fe laser task without the complications of the irregularity, we also performed some tests using the Appeltant division of the data set, as was used to test the performance of the delay node algorithm. The resulting NMSE was 0.00693 \u00b1 0.00097. Even though the error is approximately the same as for the other data set division, the result is better since only 1000 points instead of 6400 points are used for training."}, {"heading": "6.2. NARMA 10", "text": "We used the same methods for the NARMA 10 time series. We train and validate on two fixed time series. The test error shows quite some variance, so we average the test error over 15 randomly generated time series of 2000 points each. We found an NRMSE = 0.08 for the OSI scheme, which compares favorably to NRMSE = 0.12 of Appeltant [50]. The histogram of test errors is shown in Figure 5. Most values lie around the average, but there are some outliers with higher NRMSE. We don\u2019t know if such an average was taken in [50]. The MSI scheme gives NRMSE = 0.146, which is worse than the result from [50]. Note that\n\u03b8 \u2248 0.2 for the optimal MSI scheme (compared to \u03b8 = 0.908 for OSI). The MSI scheme for NARMA 10 was used in [50] to arrive at the choice of \u03b8 = 0.2.\nBecause NARMA 10 is deterministic given the ten previous input steps, the linear regression weights of the output are extremely large (a factor 100 larger than Santa Fe, which itself has large weights because it is deterministic). Large regression weights occur when overfitting, but in this case they give good results. The regularization parameter should be set to zero \u03bb = 0, i.e., no regularization is needed. This \u201coverfitting\u201d issue also leads to a strong sensitivity on noise in the implementation, making hardware implementations inherently difficult [50]. It is possible that rounding errors in the MSI scheme are the reason for the worse performance, which makes the calculation of reservoir states less stable.\nFrom the plots of the error surfaces in Appendix C one sees that there is a very low sensitivity for \u03b4. The error surface is less smooth than the Santa Fe error surface, but still rather smooth. While the MSI scheme is indeed noisier compared to the OSI scheme, the amount of noise seems small enough for Spearmint to handle (this amount of noise was not problematic for the demo runs, see Section 7). The fact that the MSI error surface is noisier corroborates our idea that the worse performance of the MSI scheme is because of rounding error in the integration scheme. Away from the optimal parameter values, the error surface can be very noisy, cf. Figure C.21. We finally note that the error surfaces are non-stationary here as well."}, {"heading": "6.3. Nonlinear channel equalization", "text": "We now assess the performance on the nonlinear channel equalization task (Section 4.3). In order to compare with the results found in [29] we use the delay node system with the sine nonlinearity, as described in Section 2.2.2. Following the setup in [29] we generate 10 different realizations of the data set, for each of\nthe 6 SNRs (ranging from 12 to 32 dB). The delay node was trained using an initial washout of 200 samples, followed by 3000 points of training, and was validated on 106 validation points. This was done for each of the 10 realizations of the data set, and the validation error was the average of the error on the individual realizations. To produce the output, ridge regression was used, after which the resulting values were binned to the values {\u22123,\u22121, 1, 3}. The reservoir consists of N = 50 nodes.\nThe parameters that were optimized using Bayesian optimization are \u03b1, \u03b2, \u03b3 and the regularization parameter \u03bb. These were optimized separately for each of the SNRs. The results are shown in Table 3. The results of the SER are plotted in Figure 6. For low noise levels, Bayesian optimization finds parameters which result in a performance that is 2 orders of magnitude better than the literature. For high noise the performance is on par with [29]. Presumably, the error surface at high levels of noise becomes less well behaved, and the assumption of smoothness breaks down, reducing the algorithm to random sampling. This is apparent at SNR 16, where the \u03b1 parameter is larger by 2 orders of magnitude when compared to the other values of the SNR.\nWe also used an ESN on this data set, and compare our results with [44]. Mimicking their setup we used an ESN with 47 nodes, used 200 washout points, 5000 training points and validated on 106 points. The validation error was an average over the error on 10 independent realizations of the network. Once the hyper-parameters were determined on the validation set, a test was performed on a test set of 106 points, averaged over 100 separate realizations of the network. This was done for 10 instantiations of the data set, and the final result is the average of the performance on each of these data sets. The result is shown in Figure 6. We observe a performance which is an order of magnitude better than [44] for high values of the SNR. Similar to the results with the NDN architecture with sine nonlinearity, our method converges to the results obtained in other works for the lower values of the SNR ."}, {"heading": "7. Spearmint demos", "text": "To get a better understanding of Spearmint\u2019s behaviour, we have visualized a few runs on two-dimensional parameter regions. Animations of the search process can be found in the supplementary material. One can conclude that Spearmint settles quite fast at a certain region. This region often contains the minimum, such as in Figure 7 (see also Figures D.22, D.23, D.24). In some cases, however, the region with the true minimum is missed, see Figure 8. In Figure D.25 the region with the minimum is found, but is not fully investigated. Spearmint does find the two regions with local minima in Figures D.22 and D.23 (the area at the bottom left and the strip above it). The fast fixation to a region indicates that the algorithm is quite \u201ceager\u201d to discard large hyper-parameter regions that it hasn\u2019t visited. This gives a qualitative understanding of the observed fast convergence behaviour: Spearmint discards large regions of the hyper-parameter space with only a few measurements, and after a while settles in a local minimum.\nThe estimate of the shape of the error surface is never really good, although in some cases a qualitative resemblance can be observed, cf. Figures 7 and D.23. Sometimes there is almost no resemblance, cf. Figure 8. This is because the algorithm has a strong focus on finding a good minimum as soon as possible; it simply discards regions that are not of interest instead of trying to model them well."}, {"heading": "8. Conclusion and Outlook", "text": "To conclude, we have shown that Bayesian optimization of Gaussian processes is an excellent technique for automated hyper-parameter optimization in reservoir computing. The method was implemented with\nthe Spearmint library. On all considered benchmarks, the method performed equally well or significantly better.\nFor the nonlinear delay nodes reservoirs, we showed that one does not need an exact solution of the differential equation to have a high-quality reservoir. Although this seems to be known, we are not aware of publications were this is explicitly discussed. We furthermore found that the popular choice of node distance \u03b8 = 0.2 is often suboptimal. Because the algorithm is able to optimize over several parameters simultaneously, one can expect to find other such consistent optimal choices for other types of reservoirs.\nBecause the hyper-parameter search is automated, reservoir computing becomes a technique that requires little to no expert input. This could increase its appeal to machine learning practitioners. It remains to be seen whether this approach works well for all tasks and different types of reservoirs. We hope that the presented method is adopted by other researchers, so that a large variety of problems are studied.\nNote that there is no optimization of other architectural features of the reservoir. The goal of this work is to compare the method with exactly the same architecture used in the literature. The methods presented here could possibly speed up the process of searching for new architectures, since the automated HO is able to find better results at a faster pace, without the need for a practitioner to acquire an intuition of the behaviour of the new architecture.\nIt would be of interest to see the performance of Spearmint for HO of hardware implementations of an NDN/ESN [28, 29, 30, 12, 59, 8, 9, 10, 11, 12], in which testing a hyper-parameter set could be more expensive compared to software implementations.\nAnother advantage of using Bayesian optimization to determine the hyper-parameters is that the comparison between different frameworks becomes less biased. When introducing a new framework it is not uncommon for researchers to put more effort into finding optimal parameters for the novel framework, than\nthey do for the old method. When both methods are optimized using Bayesian optimization this bias becomes less of a factor."}, {"heading": "Appendix A. Details Santa Fe implementation", "text": "Appeltant [50] uses the first 4000 points of the data set: the first 1000 for training, the second for optimizing hyper-parameters, and the third for optimizing the regularization parameter \u03bb. The NMSE is reported on the fourth set. We optimized the regularization parameter together with the other hyperparameters on the second set, and measured the NMSE on the third and fourth set (last 2000 points).\nOne often uses initialization points to bring the reservoir to a stationary state. We use the first 200 points for initialization, and discard their reservoir states. So in fact we use 4200 points of the data set. We do not know if some initialization points are discarded in [50]. We checked that these small differences do not lead to qualitative differences in behaviour. The prediction to unseen data is robust, i.e., the first 1000 points and second 1000 points have similar errors.\nAt points 6455-6461 in A.cont, the signal is exactly zero. This seems a measurement error, and the prediction of the reservoir diverges at those points, see Figure A.9. Several papers include this part of the data set in their test or validation set. In this case, the reservoir structure is not only optimized with respect to predicting the next point, but also towards a greater stability for a null input."}, {"heading": "Appendix B. Heat maps Santa Fe laser data and MG NDN", "text": "We plot heat maps and one-dimensional slices of the error surface of the Santa Fe task. Parameter regions are near the optimal values unless stated otherwise."}, {"heading": "Appendix C. Heat maps NARMA 10 task and MG NDN", "text": "We plot heat maps and one-dimensional slices of the error surface of the Santa Fe task. Parameter regions are near the optimal values unless stated otherwise."}, {"heading": "Appendix D. Spearmint demos", "text": "We plot heat maps and Spearmint runs for the nonlinear channel equalization task."}, {"heading": "Acknowledgement", "text": "Early work was done with Christian Van den Broeck, whom we thank for letting us work on this problem. The authors would like to thank Lars Keuninckx and Guy Van der Sande for discussions and reading the manuscript, Bart Cleuren for reading the manuscript, the reservoir computing group at IFISC for discussions,\nand Peter Tino for answering our questions about echo state networks. TB is supported by the Fonds voor Wetenschappelijk Onderzoek (FWO), project R4859. The computational resources and services used in this work were provided by the VSC (Flemish Supercomputer Center), funded by the Research Foundation - Flanders (FWO) and the Flemish Government - department EWI. In this context, the authors would like to thank Geert Jan Bex for his help with the deployment of our software on the VSC."}], "references": [{"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "Neural networks: Tricks of the trade. Springer, 2012, pp. 9\u201348.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": "A field guide to dynamical recurrent neural networks, S. C. Kremer and J. F. Kolen, Eds. IEEE Press, 2001.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Unitary evolution recurrent neural networks", "author": ["M. Arjovsky", "A. Shah", "Y. Bengio"], "venue": "International Conference on Machine Learning, 2016, pp. 1120\u20131128.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural comput., vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "IEEE Trans. on Signal Proc., vol. 45, no. 11, pp. 2673\u20132681, 1997.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "A practical guide to applying echo state networks", "author": ["M. Luko\u0161evi\u010dius"], "venue": "Neural networks: Tricks of the trade. Springer, 2012, pp. 659\u2013686.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Photonic reservoir computing for ultra-fast information processing using semiconductor lasers", "author": ["I. Fischer", "J. Bueno", "D. Brunner", "M.C. Soriano", "C. Mirasso"], "venue": "ECOC 2016; 42nd European Conference on Optical Communication; Proceedings of. VDE, 2016, pp. 1\u20133.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Fully analogue photonic reservoir computer", "author": ["F. Duport", "A. Smerieri", "A. Akrout", "M. Haelterman", "S. Massar"], "venue": "Scientific reports, vol. 6, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards pattern generation and chaotic series prediction with photonic reservoir computers", "author": ["P. Antonik", "M. Hermans", "F. Duport", "M. Haelterman", "S. Massar"], "venue": "SPIE LASE. International Society for Optics and Photonics, 2016, pp. 97 320B\u201397 320B.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "A multiple-input strategy to efficient integrated photonic reservoir computing", "author": ["A. Katumba", "M. Freiberger", "P. Bienstman", "J. Dambre"], "venue": "Cognitive Computation, pp. 1\u20138, 2017.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2017}, {"title": "Delay-based reservoir computing: noise effects in a combined analog and digital implementation", "author": ["M.C. Soriano", "S. Ort\u0301\u0131n", "L. Keuninckx", "L. Appeltant", "J. Danckaert", "L. Pesquera", "G. Van der Sande"], "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 26, no. 2, pp. 388\u2013393, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimization and applications of echo state networks with leaky-integrator neurons", "author": ["H. Jaeger", "M. Luko\u0161evi\u010dius", "D. Popovici", "U. Siewert"], "venue": "Neural networks, vol. 20, no. 3, pp. 335\u2013352, 2007.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Sensitivity learning oriented nonmonotonic multi reservoir echo state network for short-term load forecasting", "author": ["M.J.A. Rabin", "M.S. Hossain", "M.S. Ahsan", "M.A.S. Mollah", "M.T. Rahman"], "venue": "Informatics, Electronics & Vision (ICIEV), 2013 International Conference on. IEEE, 2013, pp. 1\u20136.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Pso for reservoir computing optimization", "author": ["A. Sergio", "T. Ludermir"], "venue": "Artificial Neural Networks and Machine Learning\u2013 ICANN 2012, pp. 685\u2013692, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "An experimental analysis of the echo state network initialization using the particle swarm optimization", "author": ["S. Basterrech", "E. Alba", "V. Sn\u00e1\u0161el"], "venue": "Nature and Biologically Inspired Computing (NaBIC), 2014 Sixth World Congress on. IEEE, 2014, pp. 214\u2013219.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Genetic algorithm for reservoir computing optimization", "author": ["A.A. Ferreira", "T.B. Ludermir"], "venue": "Neural Networks, 2009. IJCNN 2009. International Joint Conference on. IEEE, 2009, pp. 811\u2013815.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "An approach to reservoir computing design and training", "author": ["A.A. Ferreira", "T.B. Ludermir", "R.R. De Aquino"], "venue": "Expert systems with applications, vol. 40, no. 10, pp. 4172\u20134182, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Echo state network for the remaining useful life preeiction of a turbofan engine", "author": ["M. Rigamonti", "P. Baraldi", "E. Zio", "I. Roychoudhury", "K. Goebel", "S. Poll"], "venue": "Proceedings of the Third European Prognostic and Health Management Conference, PHME, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "Advances in Neural Information Processing Systems 25, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds. Curran Associates, Inc., 2012, pp. 2951\u20132959.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "The echo state approach to analysing and training recurrent neural networks-with an erratum note", "author": ["H. Jaeger"], "venue": "Technical Report GMD Report 148, German National Research Center for Information Technology, 2001.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Real-time computing without stable states: A new framework for neural computation based on perturbations", "author": ["W. Maass", "T. Natschl\u00e4ger", "H. Markram"], "venue": "Neural comput., vol. 14, no. 11, pp. 2531\u20132560, 2002.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "Randomness in neural networks: an overview", "author": ["S. Scardapane", "D. Wang"], "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, vol. 7, no. 2, 2017. 21", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "Echo state property linked to an input: Exploring a fundamental characteristic of recurrent neural networks", "author": ["G. Manjunath", "H. Jaeger"], "venue": "Neural computation, vol. 25, no. 3, pp. 671\u2013696, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "A local echo state property through the largest lyapunov exponent", "author": ["G. Wainrib", "M.N. Galtier"], "venue": "Neural Networks, vol. 76, pp. 39\u201345, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Information processing using a single dynamical node as complex system", "author": ["L. Appeltant", "M.C. Soriano", "G. Van der Sande", "J. Danckaert", "S. Massar", "J. Dambre", "B. Schrauwen", "C.R. Mirasso", "I. Fischer"], "venue": "Nat. Commun., vol. 2, p. 468, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Photonic information processing beyond turing: an optoelectronic implementation of reservoir computing", "author": ["L. Larger", "M.C. Soriano", "D. Brunner", "L. Appeltant", "J.M. Guti\u00e9rrez", "L. Pesquera", "C.R. Mirasso", "I. Fischer"], "venue": "Optics express, vol. 20, no. 3, pp. 3241\u20133249, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Optoelectronic reservoir computing", "author": ["Y. Paquot", "F. Duport", "A. Smerieri", "J. Dambre", "B. Schrauwen", "M. Haelterman", "S. Massar"], "venue": "Sci. Rep., vol. 2, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Parallel photonic information processing at gigabyte per second data rates using transient states", "author": ["D. Brunner", "M.C. Soriano", "C.R. Mirasso", "I. Fischer"], "venue": "Nat. Commun., vol. 4, p. 1364, 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Experimental demonstration of reservoir computing on a silicon photonics chip", "author": ["K. Vandoorne", "P. Mechet", "T. Van Vaerenbergh", "M. Fiers", "G. Morthier", "D. Verstraeten", "B. Schrauwen", "J. Dambre", "P. Bienstman"], "venue": "Nat. Commun., vol. 5, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Gaussian processes for machine learning", "author": ["C.K. Williams", "C.E. Rasmussen"], "venue": "the MIT Press, vol. 2, no. 3, p. 4, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["E. Brochu", "V.M. Cora", "N. De Freitas"], "venue": "arXiv preprint arXiv:1012.2599, 2010.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Data analysis: a Bayesian tutorial", "author": ["D. Sivia", "J. Skilling"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Input warping for bayesian optimization of non-stationary functions.", "author": ["J. Snoek", "K. Swersky", "R.S. Zemel", "R.P. Adams"], "venue": "in Proc. Int. Conf. Mach. Learn.,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Application of bayesian approach to numerical methods of global and stochastic optimization", "author": ["J. Mockus"], "venue": "Journal of Global Optimization, vol. 4, no. 4, pp. 347\u2013365, 1994.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1994}, {"title": "Convergence rates of efficient global optimization algorithms", "author": ["A.D. Bull"], "venue": "Journal of Machine Learning Research, vol. 12, no. Oct, pp. 2879\u20132904, 2011.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Bayesian optimization with exponential convergence", "author": ["K. Kawaguchi", "L.P. Kaelbling", "T. Lozano-P\u00e9rez"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 2809\u20132817.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Exponential regret bounds for gaussian process bandits with deterministic observations", "author": ["N. De Freitas", "A. Smola", "M. Zoghi"], "venue": "Proceedings of the 29th International Conference on Machine Learning, 2012.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "The future of time series: learning and understanding", "author": ["N.A. Gershenfeld", "A.S. Weigend"], "venue": "Time series prediction: Forecasting the future and understanding the past, A. S. Weigend and N. A. Gershenfeld, Eds. Addison-Wesley Publishing Company, Reading, MA, USA, 1994.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1994}, {"title": "New results on recurrent network training: unifying the algorithms and accelerating convergence", "author": ["A.F. Atiya", "A.G. Parlos"], "venue": "IEEE Trans. Neural Netw., vol. 11, no. 3, pp. 697\u2013709, 2000.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2000}, {"title": "Adaptive algorithms for bilinear filtering", "author": ["V.J. Mathews", "J. Lee"], "venue": "SPIE\u2019s 1994 International Symposium on Optics, Imaging, and Instrumentation. International Society for Optics and Photonics, 1994, pp. 317\u2013327.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1994}, {"title": "Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication", "author": ["H. Jaeger", "H. Haas"], "venue": "Science, vol. 304, no. 5667, pp. 78\u201380, 2004.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2004}, {"title": "Simple deterministically constructed recurrent neural networks", "author": ["A. Rodan", "P. Tino"], "venue": "International Conference on Intelligent Data Engineering and Automated Learning. Springer, 2010, pp. 267\u2013274.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Minimum complexity echo state network", "author": ["\u2014\u2014"], "venue": "IEEE Trans. Neural Netw., vol. 22, no. 1, pp. 131\u2013144, 2011.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "Scikit-learn: Machine learning in python", "author": ["F. Pedregosa"], "venue": "J. Mach. Learn. Res., vol. 12, pp. 2825\u20132830, 2011.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "Oger: modular learning architectures for large-scale sequential processing", "author": ["D. Verstraeten", "B. Schrauwen", "S. Dieleman", "P. Brakel", "P. Buteneers", "D. Pecevski"], "venue": "J. Mach. Learn. Res., vol. 13, no. Oct, pp. 2995\u20132998, 2012.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Modular toolkit for data processing (mdp): A python data processing framework", "author": ["T. Zito", "N. Wilbert", "L. Wiskott", "P. Berkes"], "venue": "Front. Neuroinf., 2009.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "Reservoir computing based on delay-dynamical systems", "author": ["L. Appeltant"], "venue": "These de Doctorat, Vrije Universiteit Brussel/Universitat de les Illes Balears, 2012.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2012}, {"title": "Constructing optimized binary masks for reservoir computing with delay systems", "author": ["L. Appeltant", "G. Van der Sande", "J. Danckaert", "I. Fischer"], "venue": "Sci. Rep., vol. 4, p. 3629, 2014.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Optoelectronic reservoir computing: tackling noise-induced performance degradation", "author": ["M.C. Soriano", "S. Ort\u0301\u0131n", "D. Brunner", "L. Larger", "C.R. Mirasso", "I. Fischer", "L. Pesquera"], "venue": "Optics express, vol. 21, no. 1, pp. 12\u201320, 2013.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "A unified framework for reservoir computing and extreme learning machines based on a single time-delayed neuron", "author": ["S. Ort\u0301\u0131n", "M.C. Soriano", "L. Pesquera", "D. Brunner", "D. San-Mart\u0301\u0131n", "I. Fischer", "C.R. Mirasso", "J.M. Guti\u00e9rrez"], "venue": "Sci. Rep., vol. 5, 2015.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}, {"title": "Information processing using transient dynamics of semiconductor lasers subject to delayed feedback", "author": ["K. Hicke", "M.A. Escalona-Mor\u00e1n", "D. Brunner", "M.C. Soriano", "I. Fischer", "C.R. Mirasso"], "venue": "IEEE J. Sel. Topics Quantum Electron., vol. 19, no. 4, pp. 1 501 610\u20131 501 610, 2013.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast photonic information processing using semiconductor lasers with delayed optical feedback: Role of phase dynamics", "author": ["R.M. Nguimdo", "G. Verschaffelt", "J. Danckaert", "G. Van der Sande"], "venue": "Optics express, vol. 22, no. 7, pp. 8672\u2013 8686, 2014.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "Simultaneous computation of two independent tasks using reservoir computing based on a single photonic nonlinear 22  node with optical feedback", "author": ["\u2014\u2014"], "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 26, no. 12, pp. 3301\u20133307, 2015.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "Reducing the phase sensitivity of laser-based optical reservoir computing systems", "author": ["\u2014\u2014"], "venue": "Optics express, vol. 24, no. 2, pp. 1238\u20131252, 2016.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2016}, {"title": "Optimizing the echo state network with a binary particle swarm optimization algorithm", "author": ["H. Wang", "X. Yan"], "venue": "Knowledge-Based Systems, vol. 86, pp. 182\u2013193, 2015.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2015}, {"title": "Scaling up echo-state networks with multiple light scattering", "author": ["J. Dong", "S. Gigan", "F. Krzakala", "G. Wainrib"], "venue": "arXiv preprint arXiv:1609.05204, 2016. 23", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Introduction Error backpropagation combined with stochastic gradient descent (SGD) is a simple and highly successful training method for feedforward neural networks [1, 2].", "startOffset": 165, "endOffset": 171}, {"referenceID": 1, "context": "Introduction Error backpropagation combined with stochastic gradient descent (SGD) is a simple and highly successful training method for feedforward neural networks [1, 2].", "startOffset": 165, "endOffset": 171}, {"referenceID": 2, "context": "In contrast, training recurrent neural networks with this method poses considerable difficulties [3].", "startOffset": 97, "endOffset": 100}, {"referenceID": 3, "context": ", more complicated training schemes [4] or different architectures [5, 6] have been explored to tackle this problem, and have been quite successful.", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": ", more complicated training schemes [4] or different architectures [5, 6] have been explored to tackle this problem, and have been quite successful.", "startOffset": 67, "endOffset": 73}, {"referenceID": 5, "context": ", more complicated training schemes [4] or different architectures [5, 6] have been explored to tackle this problem, and have been quite successful.", "startOffset": 67, "endOffset": 73}, {"referenceID": 6, "context": "This training paradigm goes by the name of reservoir computing (RC) [7].", "startOffset": 68, "endOffset": 71}, {"referenceID": 6, "context": "Despite its simplicity compared to other techniques, it achieves state-of-the-art results on several machine learning tasks [7].", "startOffset": 124, "endOffset": 127}, {"referenceID": 4, "context": "LSTMs [5], which have become feasible in recent years.", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "photonics [8, 9, 10, 11] or electronics [12], which could be computationally faster compared to digital ones.", "startOffset": 10, "endOffset": 24}, {"referenceID": 8, "context": "photonics [8, 9, 10, 11] or electronics [12], which could be computationally faster compared to digital ones.", "startOffset": 10, "endOffset": 24}, {"referenceID": 9, "context": "photonics [8, 9, 10, 11] or electronics [12], which could be computationally faster compared to digital ones.", "startOffset": 10, "endOffset": 24}, {"referenceID": 10, "context": "photonics [8, 9, 10, 11] or electronics [12], which could be computationally faster compared to digital ones.", "startOffset": 10, "endOffset": 24}, {"referenceID": 11, "context": "photonics [8, 9, 10, 11] or electronics [12], which could be computationally faster compared to digital ones.", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": ", acquiring optimal results still necessitates expert input [13].", "startOffset": 60, "endOffset": 64}, {"referenceID": 6, "context": "A step by step plan for manual HO for RC is provided in [7].", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "As noted in [7], automated HO is a common topic in machine learning, and these methods are applicable to RC.", "startOffset": 12, "endOffset": 15}, {"referenceID": 13, "context": "A few methods have been proposed for automated HO, including Particle Swarm Optimization (PSO) [14, 15, 16], various forms of Genetic Algorithms (GAs) [17, 18, 19], and stochastic gradient descent applied to the hyper-parameters [13].", "startOffset": 95, "endOffset": 107}, {"referenceID": 14, "context": "A few methods have been proposed for automated HO, including Particle Swarm Optimization (PSO) [14, 15, 16], various forms of Genetic Algorithms (GAs) [17, 18, 19], and stochastic gradient descent applied to the hyper-parameters [13].", "startOffset": 95, "endOffset": 107}, {"referenceID": 15, "context": "A few methods have been proposed for automated HO, including Particle Swarm Optimization (PSO) [14, 15, 16], various forms of Genetic Algorithms (GAs) [17, 18, 19], and stochastic gradient descent applied to the hyper-parameters [13].", "startOffset": 95, "endOffset": 107}, {"referenceID": 16, "context": "A few methods have been proposed for automated HO, including Particle Swarm Optimization (PSO) [14, 15, 16], various forms of Genetic Algorithms (GAs) [17, 18, 19], and stochastic gradient descent applied to the hyper-parameters [13].", "startOffset": 151, "endOffset": 163}, {"referenceID": 17, "context": "A few methods have been proposed for automated HO, including Particle Swarm Optimization (PSO) [14, 15, 16], various forms of Genetic Algorithms (GAs) [17, 18, 19], and stochastic gradient descent applied to the hyper-parameters [13].", "startOffset": 151, "endOffset": 163}, {"referenceID": 18, "context": "A few methods have been proposed for automated HO, including Particle Swarm Optimization (PSO) [14, 15, 16], various forms of Genetic Algorithms (GAs) [17, 18, 19], and stochastic gradient descent applied to the hyper-parameters [13].", "startOffset": 151, "endOffset": 163}, {"referenceID": 12, "context": "A few methods have been proposed for automated HO, including Particle Swarm Optimization (PSO) [14, 15, 16], various forms of Genetic Algorithms (GAs) [17, 18, 19], and stochastic gradient descent applied to the hyper-parameters [13].", "startOffset": 229, "endOffset": 233}, {"referenceID": 19, "context": "We therefore use the Spearmint library [20], which was developed in the context of HO in machine learning.", "startOffset": 39, "endOffset": 43}, {"referenceID": 19, "context": "HO with Bayesian optimization matches or surpasses human performance on several machine learning tasks and for different algorithms, such as deep learning and support vector machines [20].", "startOffset": 183, "endOffset": 187}, {"referenceID": 20, "context": "Echo state networks Echo state networks were introduced independently by Herbert Jaeger [22] and Wolfgang Maass (under the name Liquid State Machines [23]).", "startOffset": 88, "endOffset": 92}, {"referenceID": 21, "context": "Echo state networks Echo state networks were introduced independently by Herbert Jaeger [22] and Wolfgang Maass (under the name Liquid State Machines [23]).", "startOffset": 150, "endOffset": 154}, {"referenceID": 22, "context": "It, however, does not guarantee the echo state property [24].", "startOffset": 56, "endOffset": 60}, {"referenceID": 23, "context": ", [25, 26] for rigorous discussions on sufficient conditions for the echo state property.", "startOffset": 2, "endOffset": 10}, {"referenceID": 24, "context": ", [25, 26] for rigorous discussions on sufficient conditions for the echo state property.", "startOffset": 2, "endOffset": 10}, {"referenceID": 6, "context": "Tikhonov regularization), which is a modified version of the linear regression equations [7]: U = ( XX + \u03bbI )\u22121 Xy, (3) where I is the identity matrix, \u03bb is the regularization parameter, and X is the matrix of all reservoir states.", "startOffset": 89, "endOffset": 92}, {"referenceID": 25, "context": "Nonlinear delay nodes The concept of a nonlinear delay node as a reservoir was introduced in [27].", "startOffset": 93, "endOffset": 97}, {"referenceID": 26, "context": "It can be implemented in hardware with optical and electronic components [28, 29, 30, 12, 31, 8, 9, 10, 11, 12].", "startOffset": 73, "endOffset": 111}, {"referenceID": 27, "context": "It can be implemented in hardware with optical and electronic components [28, 29, 30, 12, 31, 8, 9, 10, 11, 12].", "startOffset": 73, "endOffset": 111}, {"referenceID": 28, "context": "It can be implemented in hardware with optical and electronic components [28, 29, 30, 12, 31, 8, 9, 10, 11, 12].", "startOffset": 73, "endOffset": 111}, {"referenceID": 11, "context": "It can be implemented in hardware with optical and electronic components [28, 29, 30, 12, 31, 8, 9, 10, 11, 12].", "startOffset": 73, "endOffset": 111}, {"referenceID": 29, "context": "It can be implemented in hardware with optical and electronic components [28, 29, 30, 12, 31, 8, 9, 10, 11, 12].", "startOffset": 73, "endOffset": 111}, {"referenceID": 7, "context": "It can be implemented in hardware with optical and electronic components [28, 29, 30, 12, 31, 8, 9, 10, 11, 12].", "startOffset": 73, "endOffset": 111}, {"referenceID": 8, "context": "It can be implemented in hardware with optical and electronic components [28, 29, 30, 12, 31, 8, 9, 10, 11, 12].", "startOffset": 73, "endOffset": 111}, {"referenceID": 9, "context": "It can be implemented in hardware with optical and electronic components [28, 29, 30, 12, 31, 8, 9, 10, 11, 12].", "startOffset": 73, "endOffset": 111}, {"referenceID": 10, "context": "It can be implemented in hardware with optical and electronic components [28, 29, 30, 12, 31, 8, 9, 10, 11, 12].", "startOffset": 73, "endOffset": 111}, {"referenceID": 11, "context": "It can be implemented in hardware with optical and electronic components [28, 29, 30, 12, 31, 8, 9, 10, 11, 12].", "startOffset": 73, "endOffset": 111}, {"referenceID": 25, "context": "the discussion in [27].", "startOffset": 18, "endOffset": 22}, {"referenceID": 25, "context": "[27], copyright (2011).", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27], copyright (2011).", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Sine First proposed in [29], this version of the nonlinear delay node uses an architecture similar to that of [27] (reviewed in Section 2.", "startOffset": 23, "endOffset": 27}, {"referenceID": 25, "context": "Sine First proposed in [29], this version of the nonlinear delay node uses an architecture similar to that of [27] (reviewed in Section 2.", "startOffset": 110, "endOffset": 114}, {"referenceID": 27, "context": "Further details can be found in [29].", "startOffset": 32, "endOffset": 36}, {"referenceID": 30, "context": "A GP is equivalent to Bayesian linear regression with an infinite number of basis functions [32].", "startOffset": 92, "endOffset": 96}, {"referenceID": 30, "context": "For an introduction to Gaussian processes, we refer to [32].", "startOffset": 55, "endOffset": 59}, {"referenceID": 31, "context": "Several applications are discussed in [33].", "startOffset": 38, "endOffset": 42}, {"referenceID": 32, "context": "This updating is done with Bayesian statistics [34].", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "In this paper, the automatic relevance detection Mat\u00e9rn 5/2 kernel is used [20].", "startOffset": 75, "endOffset": 79}, {"referenceID": 31, "context": "Reprinted with permission from [33].", "startOffset": 31, "endOffset": 35}, {"referenceID": 31, "context": "A discussion of several acquisition functions can be found in [33].", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "The implementation of the GP with Bayesian updating is done with Spearmint [20].", "startOffset": 75, "endOffset": 79}, {"referenceID": 33, "context": "This library contains several non-trivial extensions, such as averaging out the hyper-parameters of the GP with Bayesian statics, and input warping [35].", "startOffset": 148, "endOffset": 152}, {"referenceID": 30, "context": "A discussion on the convergence of the GP to the average of the underlying function f(x) can be found in [32].", "startOffset": 105, "endOffset": 109}, {"referenceID": 34, "context": "Sufficient conditions for the convergence of GPs with Bayesian optimization were given by Mo\u010dkus [36].", "startOffset": 97, "endOffset": 101}, {"referenceID": 35, "context": "Depending on the implementation, the expected improvement acquisition function can converge near-optimally, although it is possible that the algorithm does not converge at all [37].", "startOffset": 176, "endOffset": 180}, {"referenceID": 36, "context": "Recent work has shown that for the upper confidence bound acquisition function, there are algorithms for which convergence of a GP to the optimum is exponentially fast [38, 39].", "startOffset": 168, "endOffset": 176}, {"referenceID": 37, "context": "Recent work has shown that for the upper confidence bound acquisition function, there are algorithms for which convergence of a GP to the optimum is exponentially fast [38, 39].", "startOffset": 168, "endOffset": 176}, {"referenceID": 12, "context": "If the connection weights of the reservoir output are high, the SGD learning rate becomes exponentially slow [13].", "startOffset": 109, "endOffset": 113}, {"referenceID": 6, "context": "When optimizing hyperparameters step by step [7], the hyper-parameters are optimized with all others fixed.", "startOffset": 45, "endOffset": 48}, {"referenceID": 38, "context": "Santa Fe The Santa Fe data set consists of the output of a chaotic laser system [40].", "startOffset": 80, "endOffset": 84}, {"referenceID": 39, "context": "NARMA 10 The Nonlinear Auto-Regressive Moving Average (NARMA) of order 10 was originally introduced for use as a timeseries prediction benchmark in [42].", "startOffset": 148, "endOffset": 152}, {"referenceID": 40, "context": "Nonlinear channel equalization This task was first introduced in [43] and has since been used as a benchmark several times [44, 29, 45].", "startOffset": 65, "endOffset": 69}, {"referenceID": 41, "context": "Nonlinear channel equalization This task was first introduced in [43] and has since been used as a benchmark several times [44, 29, 45].", "startOffset": 123, "endOffset": 135}, {"referenceID": 27, "context": "Nonlinear channel equalization This task was first introduced in [43] and has since been used as a benchmark several times [44, 29, 45].", "startOffset": 123, "endOffset": 135}, {"referenceID": 42, "context": "Nonlinear channel equalization This task was first introduced in [43] and has since been used as a benchmark several times [44, 29, 45].", "startOffset": 123, "endOffset": 135}, {"referenceID": 41, "context": "Following [44, 46], we shifted u(n) by +30.", "startOffset": 10, "endOffset": 18}, {"referenceID": 43, "context": "Following [44, 46], we shifted u(n) by +30.", "startOffset": 10, "endOffset": 18}, {"referenceID": 44, "context": "For all NDN implementations, ridge regression is performed with sci-kit learn [47].", "startOffset": 78, "endOffset": 82}, {"referenceID": 45, "context": "Echo state network To implement the echo state networks we made use of the software library Oger [48], which in turn builds on the Modular toolkit for Data Processing (MDP) [49].", "startOffset": 97, "endOffset": 101}, {"referenceID": 46, "context": "Echo state network To implement the echo state networks we made use of the software library Oger [48], which in turn builds on the Modular toolkit for Data Processing (MDP) [49].", "startOffset": 173, "endOffset": 177}, {"referenceID": 47, "context": "OSI MSI Appeltant [50] NMSE 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 47, "context": "Santa Fe laser data For the MG NDN, we compare our results with those of [50].", "startOffset": 73, "endOffset": 77}, {"referenceID": 47, "context": "As in [50], we take N = 200.", "startOffset": 6, "endOffset": 10}, {"referenceID": 47, "context": "The NMSE for the MSI is an order of magnitude lower than [50], while the OSI result is four times lower.", "startOffset": 57, "endOffset": 61}, {"referenceID": 47, "context": "We now compare our optimal parameters to those from reference [50].", "startOffset": 62, "endOffset": 66}, {"referenceID": 47, "context": "One should keep in mind that the integration schemes are slightly different from the implementation in [50], even though Euler integration is also used there.", "startOffset": 103, "endOffset": 107}, {"referenceID": 47, "context": "Our programs are written in Python, while Matlab is used in [50].", "startOffset": 60, "endOffset": 64}, {"referenceID": 47, "context": "02 (note that we don\u2019t know all the parameters of [50], some are educated guesses).", "startOffset": 50, "endOffset": 54}, {"referenceID": 26, "context": "2 has become popular in the literature [28, 12, 51, 52, 53, 30, 54, 55, 56, 57].", "startOffset": 39, "endOffset": 79}, {"referenceID": 11, "context": "2 has become popular in the literature [28, 12, 51, 52, 53, 30, 54, 55, 56, 57].", "startOffset": 39, "endOffset": 79}, {"referenceID": 48, "context": "2 has become popular in the literature [28, 12, 51, 52, 53, 30, 54, 55, 56, 57].", "startOffset": 39, "endOffset": 79}, {"referenceID": 49, "context": "2 has become popular in the literature [28, 12, 51, 52, 53, 30, 54, 55, 56, 57].", "startOffset": 39, "endOffset": 79}, {"referenceID": 50, "context": "2 has become popular in the literature [28, 12, 51, 52, 53, 30, 54, 55, 56, 57].", "startOffset": 39, "endOffset": 79}, {"referenceID": 28, "context": "2 has become popular in the literature [28, 12, 51, 52, 53, 30, 54, 55, 56, 57].", "startOffset": 39, "endOffset": 79}, {"referenceID": 51, "context": "2 has become popular in the literature [28, 12, 51, 52, 53, 30, 54, 55, 56, 57].", "startOffset": 39, "endOffset": 79}, {"referenceID": 52, "context": "2 has become popular in the literature [28, 12, 51, 52, 53, 30, 54, 55, 56, 57].", "startOffset": 39, "endOffset": 79}, {"referenceID": 53, "context": "2 has become popular in the literature [28, 12, 51, 52, 53, 30, 54, 55, 56, 57].", "startOffset": 39, "endOffset": 79}, {"referenceID": 54, "context": "2 has become popular in the literature [28, 12, 51, 52, 53, 30, 54, 55, 56, 57].", "startOffset": 39, "endOffset": 79}, {"referenceID": 55, "context": "In addition to the NDN, we studied an ESN to compare the performance of the Bayesian optimization algorithm to the results from [58], who use binary PSO to optimize the ESN architecture.", "startOffset": 128, "endOffset": 132}, {"referenceID": 55, "context": "0284 in [58] for their standard ESN, and 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 47, "context": "12 of Appeltant [50].", "startOffset": 16, "endOffset": 20}, {"referenceID": 47, "context": "We don\u2019t know if such an average was taken in [50].", "startOffset": 46, "endOffset": 50}, {"referenceID": 47, "context": "146, which is worse than the result from [50].", "startOffset": 41, "endOffset": 45}, {"referenceID": 47, "context": "one-step multi-step Appeltant [50] NRMSE 0.", "startOffset": 30, "endOffset": 34}, {"referenceID": 47, "context": "The MSI scheme for NARMA 10 was used in [50] to arrive at the choice of \u03b8 = 0.", "startOffset": 40, "endOffset": 44}, {"referenceID": 47, "context": "This \u201coverfitting\u201d issue also leads to a strong sensitivity on noise in the implementation, making hardware implementations inherently difficult [50].", "startOffset": 145, "endOffset": 149}, {"referenceID": 27, "context": "In order to compare with the results found in [29] we use the delay node system with the sine nonlinearity, as described in Section 2.", "startOffset": 46, "endOffset": 50}, {"referenceID": 27, "context": "Following the setup in [29] we generate 10 different realizations of the data set, for each of 11", "startOffset": 23, "endOffset": 27}, {"referenceID": 27, "context": "For high noise the performance is on par with [29].", "startOffset": 46, "endOffset": 50}, {"referenceID": 41, "context": "We also used an ESN on this data set, and compare our results with [44].", "startOffset": 67, "endOffset": 71}, {"referenceID": 41, "context": "We observe a performance which is an order of magnitude better than [44] for high values of the SNR.", "startOffset": 68, "endOffset": 72}, {"referenceID": 41, "context": "[44] having dashed lines (green), and our results having solid lines (blue).", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] in dashed lines (red), and our results in solid line (black).", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "It would be of interest to see the performance of Spearmint for HO of hardware implementations of an NDN/ESN [28, 29, 30, 12, 59, 8, 9, 10, 11, 12], in which testing a hyper-parameter set could be more expensive compared to software implementations.", "startOffset": 109, "endOffset": 147}, {"referenceID": 27, "context": "It would be of interest to see the performance of Spearmint for HO of hardware implementations of an NDN/ESN [28, 29, 30, 12, 59, 8, 9, 10, 11, 12], in which testing a hyper-parameter set could be more expensive compared to software implementations.", "startOffset": 109, "endOffset": 147}, {"referenceID": 28, "context": "It would be of interest to see the performance of Spearmint for HO of hardware implementations of an NDN/ESN [28, 29, 30, 12, 59, 8, 9, 10, 11, 12], in which testing a hyper-parameter set could be more expensive compared to software implementations.", "startOffset": 109, "endOffset": 147}, {"referenceID": 11, "context": "It would be of interest to see the performance of Spearmint for HO of hardware implementations of an NDN/ESN [28, 29, 30, 12, 59, 8, 9, 10, 11, 12], in which testing a hyper-parameter set could be more expensive compared to software implementations.", "startOffset": 109, "endOffset": 147}, {"referenceID": 56, "context": "It would be of interest to see the performance of Spearmint for HO of hardware implementations of an NDN/ESN [28, 29, 30, 12, 59, 8, 9, 10, 11, 12], in which testing a hyper-parameter set could be more expensive compared to software implementations.", "startOffset": 109, "endOffset": 147}, {"referenceID": 7, "context": "It would be of interest to see the performance of Spearmint for HO of hardware implementations of an NDN/ESN [28, 29, 30, 12, 59, 8, 9, 10, 11, 12], in which testing a hyper-parameter set could be more expensive compared to software implementations.", "startOffset": 109, "endOffset": 147}, {"referenceID": 8, "context": "It would be of interest to see the performance of Spearmint for HO of hardware implementations of an NDN/ESN [28, 29, 30, 12, 59, 8, 9, 10, 11, 12], in which testing a hyper-parameter set could be more expensive compared to software implementations.", "startOffset": 109, "endOffset": 147}, {"referenceID": 9, "context": "It would be of interest to see the performance of Spearmint for HO of hardware implementations of an NDN/ESN [28, 29, 30, 12, 59, 8, 9, 10, 11, 12], in which testing a hyper-parameter set could be more expensive compared to software implementations.", "startOffset": 109, "endOffset": 147}, {"referenceID": 10, "context": "It would be of interest to see the performance of Spearmint for HO of hardware implementations of an NDN/ESN [28, 29, 30, 12, 59, 8, 9, 10, 11, 12], in which testing a hyper-parameter set could be more expensive compared to software implementations.", "startOffset": 109, "endOffset": 147}, {"referenceID": 11, "context": "It would be of interest to see the performance of Spearmint for HO of hardware implementations of an NDN/ESN [28, 29, 30, 12, 59, 8, 9, 10, 11, 12], in which testing a hyper-parameter set could be more expensive compared to software implementations.", "startOffset": 109, "endOffset": 147}, {"referenceID": 47, "context": "Details Santa Fe implementation Appeltant [50] uses the first 4000 points of the data set: the first 1000 for training, the second for optimizing hyper-parameters, and the third for optimizing the regularization parameter \u03bb.", "startOffset": 42, "endOffset": 46}, {"referenceID": 47, "context": "We do not know if some initialization points are discarded in [50].", "startOffset": 62, "endOffset": 66}], "year": 2017, "abstractText": "We describe a method for searching the optimal hyper-parameters in reservoir computing, which consists of a Gaussian process with Bayesian optimization. It provides an alternative to other frequently used optimization methods such as grid, random, or manual search. In addition to a set of optimal hyper-parameters, the method also provides a probability distribution of the cost function as a function of the hyper-parameters. We apply this method to two types of reservoirs: nonlinear delay nodes and echo state networks. It shows excellent performance on all considered benchmarks, either matching or significantly surpassing results found in the literature. In general, the algorithm achieves optimal results in fewer iterations when compared to other optimization methods. We have optimized up to six hyper-parameters simultaneously, which would have been infeasible using, e.g., grid search. Due to its automated nature, this method significantly reduces the need for expert knowledge when optimizing the hyper-parameters in reservoir computing. Existing software libraries for Bayesian optimization, such as Spearmint, make the implementation of the algorithm straightforward. A fork of the Spearmint framework along with a tutorial on how to use it in practice is available at https://bitbucket.org/uhasseltmachinelearning/spearmint/", "creator": "LaTeX with hyperref package"}}}