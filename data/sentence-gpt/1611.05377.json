{"id": "1611.05377", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "Fully-adaptive Feature Sharing in Multi-Task Networks with Applications in Person Attribute Classification", "abstract": "Multi-task learning aims to improve generalization performance of multiple prediction tasks by appropriately sharing relevant information across them. In the context of deep neural networks, this idea is often realized by hand-designed network architectures with layers that are shared across tasks and branches that encode task-specific features. However, the space of possible multi-task deep architectures is combinatorially large and often the final architecture is arrived at by manual exploration of this space subject to designer's bias, which can be both error-prone and tedious.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 16 Nov 2016 17:31:44 GMT  (767kb,D)", "http://arxiv.org/abs/1611.05377v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["yongxi lu", "abhishek kumar", "shuangfei zhai", "yu cheng", "tara javidi", "rogerio feris"], "accepted": false, "id": "1611.05377"}, "pdf": {"name": "1611.05377.pdf", "metadata": {"source": "CRF", "title": "Fully-adaptive Feature Sharing in Multi-Task Networks with Applications in Person Attribute Classification", "authors": ["Yongxi Lu", "UC San Diego", "Abhishek Kumar", "Shuangfei Zhai", "Yu Cheng", "Tara Javidi", "Rogerio Feris"], "emails": ["yol070@ucsd.edu", "abhishk@us.ibm.com", "szhai2@binghamton.edu", "chengyu@us.ibm.com", "tjavidi@eng.ucsd.edu", "rsferis@us.ibm.com"], "sections": [{"heading": "1. Introduction", "text": "Humans possess a natural yet remarkable ability of seamlessly transferring and sharing knowledge across multiple related domains while doing inference for a given task. Effective mechanisms for sharing relevant information across multiple prediction tasks (referred as multi-task learning) are also arguably crucial for making significant advances towards machine intelligence. In this paper, we propose a novel approach for multi-task learning in the context of deep neural networks for computer vision tasks. We particularly aim for two desirable characteristics in the proposed approach: (i) automatic learning of multi-task archi-\ntectures based on branching, (ii) selective sharing among tasks with automated learning of whom to share with. In addition, we want our multi-task models to have low memory footprint and low latency during prediction (forward pass through the network).\nA natural approach for enabling sharing across multiple tasks is to share model parameters (partially or fully) across the corresponding layers of the task-specific deep neural networks. At an extreme, we can imagine a fully shared multi-task network architecture where all layers are shared except the last layer which predicts the labels for individual tasks. However, this unrestricted sharing may suffer from the problem of negative transfer where inadequate sharing across two unrelated tasks can worsen the performance on both. To avoid this, most of the multi-task deep architectures share the bottom layers till some layer l after which the sharing is blocked, resulting in task-specific sub-networks or branches beyond it [28, 17, 13]. This is motivated by the observation made by several earlier works that bottom layers capture low level detailed features, which can be shared across multiple tasks, whereas top layers capture features at a higher level of abstraction that are more task specific. It can be further extended to a more general tree-like architecture, e.g., a smaller group of tasks can share parameters even after the first break-point at layer l and breakup at a later layer. However, the space of such possible branching architectures is combinatorially large and current approaches largely make a decision based on limited manual exploration of this space, often biased by designer\u2019s perception of the relationship among different tasks [25].\nOur goal in this work is to develop a principled approach for designing multi-task deep learning architectures obviating the need for tedious manual explorations. The proposed approach operates in a greedy top-down manner, making branching and task-grouping decisions at each layer of the network using a novel criterion that promotes the creation\nar X\niv :1\n61 1.\n05 37\n7v 1\n[ cs\n.C V\n] 1\n6 N\nof separate branches for unrelated tasks (or groups of tasks) while penalizing for the model complexity. Since we also desire a multi-task model with low memory footprint, the proposed approach starts with a thin network and dynamically grows it during the training phase by creating new branches based on the aforementioned criterion. We also propose a method based on simultaneous orthogonal matching pursuit (SOMP) [42] for initializing a thin network from a pretrained wider network (e.g., VGG-16) as a side contribution in this work.\nWe evaluate the proposed approach on person attribute classification, where each attribute is considered a task (with non-mutually exclusive labels), achieving state-ofthe-art results with highly compact multi-task models. On the CelebA dataset [24], we match the current top results on facial attribute classification (90% accuracy) with a model 90x more compact and 3x faster than the original VGG-16 model. We draw similar conclusions for clothing category recognition on the DeepFashion dataset [23], demonstrating that we can perform simultaneous facial and clothing attribute prediction using a single compact multi-task model, while preserving accuracy. In summary, our main contributions are listed below: \u25e6 We propose to automate learning of multi-task deep net-\nwork architectures through a novel dynamic branching procedure, which makes task grouping decisions at each layer of the network (deciding with whom each task should share features) by taking into account both task relatedness and complexity of the model. \u25e6 A novel method based on Simultaneous Orthogonal Matching Pursuit is proposed for initializing a thin network from a wider pre-trained network model, leading to faster convergence and higher accuracy. \u25e6 We perform joint prediction of facial and clothing attributes, achieving state-of-the-art results on standard datasets with a significantly more compact and efficient multi-task model. We also conduct relevant ablation studies providing insights into the proposed approach."}, {"heading": "2. Related Work", "text": "Multi-Task Learning. There is a long history of research in multi-task learning [4, 40, 16, 21, 25]. Most proposed techniques assume that all tasks are related and appropriate for joint training. A few methods have addressed the problem of \u201cwith whom\u201d each task should share features [45, 16, 51, 18, 21, 26]. These methods are generally designed for shallow classification models, while our work investigates feature sharing among tasks in hierarchical models such as deep neural networks.\nRecently, several methods have been proposed for multitask learning using deep neural networks. HyperFace [28] simultaneously learns to perform face detection, landmarks localization, pose estimation and gender recognition. Uber-\nNet [19] jointly learns low-, mid-, and high-level computer vision tasks using a compact network model. MultiNet [3] exploits recurrent networks for transferring information across tasks. Cross-ResNet [17] connects tasks through residual learning for knowledge transfer. However, all these methods rely on hand-designed network architectures composed of base layers that are shared across tasks and specialized branches that learn task-specific features.\nAs network architectures become deeper, defining the right level of feature sharing across tasks through handcrafted network branches is impractical. Cross-stitching networks [25] have been recently proposed to learn an optimal combination of shared and task-specific representations. Although cross-stitching units connecting taskspecific sub-networks are designed to learn the feature sharing among tasks, the size of the network grows linearly with the number of tasks, causing scalability issues. We instead propose a novel algorithm that makes decisions about branching based on task relatedness, while optimizing for the efficiency of the model. We note that other techniques such as HD-CNN [46] and Network of Experts [1] also group related classes to perform hierarchical classification, but these methods are not applicable for the multi-label setting (where labels are not mutually exclusive).\nModel Compression and Acceleration. Existing deep convolutional neural network models are computationally and memory intensive, hindering their deployment in devices with low memory resources or in applications with strict latency requirements. Methods for compressing and accelerating convolutional networks include knowledge distillation [12, 29], low-rank-factorization [14, 39, 33], pruning and quantization [10, 27], structured matrices [6, 35, 9], and dynamic capacity networks [2]. These methods are task-agnostic and therefore most of them are complementary to our approach, which seeks to obtain a compact multitask model by widening a low-capacity network based on task relatedness. Moreover, many of these state-of-the-art compression techniques can be used to further reduce the size of our learned multi-task architectures.\nPerson Attribute Classification. Methods for recognizing attributes of people, such as facial and clothing attributes, have received increased attention in the past few years. In the visual surveillance domain, person attributes serve as features for improving person re-identification [36] and enable search of suspects based on their description [43, 8]. In e-commerce applications, these attributes have proven effective in improving clothing retrieval [13], and fashion recommendation [22]. It has also been shown that facial attribute prediction is helpful as an auxiliary task for improving face detection [47] and face alignment [50].\nState-of-the-art methods for person attribute prediction are based on deep convolutional neural networks [44, 24, 5, 49]. Most methods either train separate classifiers per at-\ntribute [49] or perform joint learning with a fully shared network [32]. Multi-task networks have been used with base layers that are shared across all attributes, and branches to encode task-specific features for each attribute category [13, 37]. However, in contrast to our work, the network branches are hand-designed and do not exploit the fact that some attributes are more related than others in order to determine the level of sharing among tasks in the network. Moreover, we show that our approach produces a single compact network that can predict both facial and clothing attributes simultaneously."}, {"heading": "3. Methodology", "text": "Let the linear operation in a layer l of the network be paramterized by W l. Let xl \u2208 Rcl be the input vector of layer l, and yl \u2208 Rcl+1 be the output vector. In feedforward networks that are of interest to this work, it is always the case that xl = yl\u22121. In other words, the output of a layer is the input to the layer above. In vision applications, the feature maps are often considered as three-way tensors and one should think of xl and yl as appropriately vectorized versions of the input and output feature tensors. The functional form of the network is a series of within-layer computations chained in a sequence linking the lowest to the highest (output) layer. The within-layer computation (for both convolutional and fully-connected layers) can be concisely represented by a simple linear operation parametrized by W l, followed by a non-linearity \u03c3l(\u00b7) as\nyl = \u03c3l(P(W l)xl), (1)\nwhere P is an operator that maps the parameters W l to the appropriate matrix P(W l) \u2208 Rcl+1\u00d7cl . For a fully connected layer P reduces to the identity operator, whereas for a convolutional layer with fl filters, W l \u2208 Rfl\u00d7dl contains the vectorized filter coefficients in each row and the operator P maps it to an appropriate matrix that represents convolution as matrix multiplication. With this unified representation, we define the width of the network at layer l as cl for the fully connected layers, and as fl for the convolutional layers. The widths at different layers are critical hyper-parameters for a network design. In general, a wider network is more expensive to train and deploy, but it has the capacity to model a richer set of visual patterns. The relative width across layers is a particularly relevant consideration in the design of a multi-task network. It is widely observed that higher layer represents a level of abstraction that is more task dependent. This is confirmed by previous works on visualization of filters at different layers [48].\nTraditional approaches tackle the width design problem largely through hand-crafted layer design and manual model selection. Notably, popular deep convolutional network architectures, such as AlexNet [20], VGG [34], Inception [38] and ResNet [11] all use wider layers at the top\nAlgorithm 1: Training with Adaptive Widening Data: Input data D = (xi, yi)Ni=1. The labels y are for a set\nof T tasks. Input: Branch factor \u03b1, and thinness factor \u03c9. Optionally, a pre-trained network Mp with parameters \u0398p. Result: A trained network Mf with parameters \u0398f . Initialization: M0 is a thin-\u03c9 model with L layers. if exist Mp,\u0398p then\n\u03980 \u2190 SompInit(M0,Mp,\u0398p). t\u2190 1, d\u2190 T . (Sec. 3.1) else\n\u03980 \u2190 Random initialization while (t \u2264 L) and (d > 1) do\n\u0398t, At \u2190 TrainAndGetAffinity(D,Mt,\u0398t) (Sec. 3.3) d\u2190 FindNumberBranches(Mt, At, \u03b1) (Sec. 3.4) Mt+1,\u0398t+1 \u2190 WidenModel(Mt,\u0398t, At, d) (Sec. 3.2) t\u2190 t+ 1\nTrain model Mt with sufficient iterations, update \u0398t. Mf \u2190Mt, \u0398f \u2190 \u0398t.\nof the network in what can be called an \u201cinverse pyramid\u201d pattern. These architectures serve as excellent reference designs in a myriad of domains, but researchers have noted that the width schedule (especially at the top layers) need to be tuned for the underlying set of tasks the network has to perform in order to achieve best accuracy [25].\nHere we propose an algorithm that dynamically finds the appropriate width of the multi-task network along with the task groupings through a multi-round training procedure. It has three main phases:\nThin Model Initialization. We start with a thin neural network model, initializing it from a pre-trained wider VGG-16 model by selecting a subset of filters using simultaneous orthogonal matching pursuit (ref. Section 3.1).\nAdaptive Model Widening. The thin initialized model goes through a multi-round widening and training procedure. The widening is done in a greedy top-down layerwise manner starting from the top layer. For the current layer to be widened, our algorithm makes a decision on the number of branches to be created at this layer along with task assignments for each branch. The network architecture is frozen when the algorithm decides to create no further branches (ref. Section 3.2).\nTraining with the Final Model. In this last phase, the fixed final network is trained until convergence.\nMore technical details are discussed in the next few sections. Algorithm 1 provides a summary of the procedure."}, {"heading": "3.1. Thin Networks and Filter Selection using Simultaneous Orthogonal Matching Pursuit", "text": "The initial model we use is a thin version of the VGG-16 network. It has the same structure as VGG-16 except for the widths at each layer. We experiment with a range of thin models that are denoted as thin-\u03c9 models. The width\nof a convolutional layer of the thin-\u03c9 model is the minimum between \u03c9 and the width of the corresponding layer of the VGG-16 network. The width of the fully connected layers are set to 2\u03c9. We shall call \u03c9 the \u201cthinness factor\u201d. Figure 1 illustrates a thin model side by side with VGG-16.\nUsing weights from pre-trained models is known to speed up training and improve model generalization. However, the standard direct copy method is only suitable when the source and the target networks have the same architecture (at least for most of the layers). Our adoption of a thin initial model forbids the use of direct copy, as there is a mismatch in the dimension of the weight matrix (for both the input and output dimensions, see Equation 1 and discussions). In the literature a set of general methods for training arbitrarily small networks using an existing larger network and the training data are known as \u201cknowledge distillation\u2019 [12, 29]. However, for the limited use case of this work we propose a faster, data-free, and simple yet reasonably effective method. Let W p,l be the parameters of the pre-trained model at layer l with d rows. For convolutional layers, each row of W p,l represents a vectorized filter kernel. The initialization procedure aims to identify a subset of d\u2032(< d) rows of W p,l to form W 0,l (the superscript 0 denotes initialized parameters for the thin model). We would like the selected rows that minimize the following objective:\nA?, \u03c9?(l) = arg min A\u2208Rd\u00d7d\u2032 ,|\u03c9|=d\u2032 ||W p,l \u2212AW p,l\u03c9: ||F , (2)\nwhere W p,l\u03c9: is a truncated weight matrix that only keeps the rows indexed by the set \u03c9. This problem is NP-hard, however, there exist approaches based on convex relaxation [41] and greedy simultaneous orthogonal matching pursuit (SOMP) [42] which can produce approximate solutions. We use the greedy SOMP to find the approximate solution \u03c9?(l) which is then used to initialize the parameter matrix of the thin model as W 0,l \u2190 W p,l\u03c9?(l):. We run this procedure layer by layer, starting from the input layer. At layer l, after initializing W 0,l, we replace W p,l+1 with a columntruncated version that only keeps the columns indexed by\nAlgorithm 2: SompInit(M0, Mp, \u0398p) Input: The architecture of the thin network M0 with L\nlayers. The pretrained network and its parameters Mp, \u0398p. Denote the weight matrix at layer l as W p,l \u2208 \u0398p.\nResult: The initial parmaeters of the thin network \u03980. foreach l \u2208 1, 2, \u00b7 \u00b7 \u00b7 , L do\nFind \u03c9?(l) in Equation 2 by SOMP, using W p,l as weight matrix. W 0,l \u2190W p,l\u03c9?(l): W p,l+1 \u2190 ( (W p,l+1)T\u03c9?(l):\n)T Aggregate W 0,l for l \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , L} to form \u03980.\n\u03c9?(l) to keep the input dimensions consistent. This initialization procedure is applicable for both convolutional and fully connected layers. See Algorithm 2."}, {"heading": "3.2. Top-Down Layer-wise Model Widening", "text": "At the core of our training algorithm is a procedure that incrementally widens the current design in a layer-wise fashion. Let us introduce the concept of a \u201cjunction\u201d. A junction is a point at which the network splits into two or more independent sub-networks. We shall call such a subnetwork a \u201cbranch\u201d. Each branch leads to a subset of prediction tasks performed by the full network. In the context of person attributes classification, each prediction is a sigmoid unit that produces a normalized confidence score on the existence of an attribute.\nWe propose to widen the network only at these junctions. More formally, consider a junction at layer l with input xl and d outputs {yli}di=1. Note that each output is the input to one of the d top sub-networks. Similar to Equation 1 the within-layer computation is given as\nyli = \u03c3l(P(W li )xl) for i \u2208 [d], (3)\nwhere W li parameterizes the connection from input x l to the i\u2019th output yli at layer l. The set [d] is the indexing set {1, 2, \u00b7 \u00b7 \u00b7 , d}. A junction is widened by creating new outputs at the layer below. To widen layer l by a factor of c, we make layer l\u2212 1 a junction with 2 \u2264 c \u2264 d outputs. We use yl\u22121j to denote an output in layer l \u2212 1 (each is an input for layer l) and W l\u22121j to denote its parameter matrix. All of the newly-created parameter matrices have the same shape as W l\u22121 (the parameter matrix before widening). The single output yl\u22121 = xl is replaced by a set of outputs {yl\u22121j }cj=1 where\nyl\u22121j = \u03c3l\u22121(P(W l\u22121 j )x l\u22121) for j \u2208 [c]. (4)\nLet gl : [d] \u2192 [c] be a given grouping function at layer l. After widening, the within-layer computation at layer l is\ngiven as (cf. Equation 3) yli = \u03c3l(P(W li )xlgl(i)) = \u03c3l ( P(W li )\u03c3l\u22121(P(W l\u22121gl(i))x l\u22121) ) (5) where the latter equality is a consequence of Equation 3. The widening operation sets the initial weight for W l\u22121j to be equal to the original weight of W l\u22121. It allows the widened network to preserve the functional form of the smaller network, enabling faster training.\nTo put the widening of one junction into the context of the multi-round progressive model widening procedure, consider a situation where there are T tasks. Before any widening, the output layer of the initial thin multi-task network has a junction with T outputs, each is the output of a sub-network (branch). It is also the only junction at initialization. The widening operation naturally starts from the output layer (denoted as layer l). It will cluster the T branches into t groups where t \u2264 T . In this manner the widening operation creates t branches at layer l \u2212 1. The operation is performed recursively in a top-down manner towards the lower layers. Note that each branch will be associated with a sub-set of tasks. There is a 1-1 correspondence between tasks and branches at the output layer, but the granularity goes coarser at lower layers. An illustration of this procedure can be found in Figure 2."}, {"heading": "3.3. Task Grouping based on the Probability of Concurrently Simple or Difficult Examples", "text": "Ideally, dissimilar tasks are separated starting from a low layer, resulting in less sharing of features. For similar tasks the situation is the opposite. We observe that if an easy example for one task is typically a difficult example for another, intuitively a distinctive set of filters are required for each task to accurately model both in a single network. Thus we define the affinity between a pair of tasks as the probability of observing concurrently simple or difficult examples for the underlying pair of tasks from a random sample of the training data.\nTo make it mathematically concrete, we need to prop-\nerly define the notion of a \u201cdifficult\u201d and a \u201csimple\u201d example. Consider an arbitrary attribute classification task i. Denote the prediction of the task for example n as sni , and the error margin as mni = |tni \u2212 sni |, where tni is the binary label for task i at sample n. Following the previous discussion, it seems natural to set a fixed threshold on mni to decide whether example n is simple or difficult. However, we observe that this is problematic since as the training progresses most of the examples will become simple as the error rate decreases, rendering this measure of affinity useless. An adaptive but universal (across all tasks) threshold is also problematic as it creates a bias that makes intrinsically easier tasks less related to all the other tasks.\nThese observations lead us to the following approach. Instead of setting a fixed threshold, we estimate the average margin for each task, E{mi}. We define the indicator variable for a difficult example for task i as eni = 1mni \u2265E{mi}. For a pair of tasks i, j, we define their affinity as\nA(i, j) = P(eni = 1, enj = 1) + P(eni = 0, enj = 0) = E{eni enj + (1\u2212 eni )(1\u2212 enj )}. (6)\nBoth E{mi} and the expectation on Equation 6 can be estimated by their sample averages. Since these expectations are functions of the current neural network model, a naive implementation would require a large number of time consuming forward passes after every training iterations. As a much more efficient implementation, we alternatively collect the sample averages from each training mini-batches. The expectations are estimated by computing a weighted average of the within-batch sample averages. To make the estimation closer to the true expectations from the current model, an exponentially decaying weight is used.\nThe estimated task affinity is used directly for the clustering at the output layer. It is natural as branches at the output layer has a 1-1 map to the tasks. But at lower layers the mapping is one to many, as a branch can be associated with more than one tasks. In this case, affinity is computed to reflect groups of tasks. In particular, let k, l denote two branches at the current layer, where ik and jl denotes the\ni-th and j-th task associated with each branch respectively. The affinity of the two branches are defined by\nA\u0303b(k, l) = mean ik ( min jl A(ik, jl) ) (7)\nA\u0303b(l, k) = mean jl ( min ik A(ik, jl) ) (8)\nThe final affinity score is computed as Ab(k, l) = (A\u0303b(k, l) + A\u0303b(l, k))/2. Note that if branches and tasks form a 1-1 map (the situation at the output layer), this reduces to the definition in Equation 6. For branches with coarser task granularity, Ab(k, l) measures the affinity between two branches by looking at the largest distance (smallest affinity) between their associated tasks."}, {"heading": "3.4. Complexity-aware Width Selection", "text": "The number of branches to be created determines how much wider the network becomes after a widening operation. This number is determined by a loss function that balances complexity and the separation of dissimilar tasks to different branches. For each number of clusters 1 \u2264 d \u2264 c, we perform spectral clustering to get a grouping function gd : [d] \u2192 [c] that associates the newly created branches with the c old branches at one layer above. At layer l the loss function is given by\nLl(gd) = (d\u2212 1)L02pl + \u03b1Ls(gd) (9)\nwhere (d\u2212 1)L02pl is a penalty term for creating branches at layer l, Ls(gd) is a penalty for separation. pl is defined as the number of pooling layers above the layer l and L0 is the unit cost for branch creation. The first term grows linearly with the number of branches, with a scalar that defines how expensive it is to create a branch at the current layer (which is heuristically set to double after every pooling layers). Note that in this formulation a larger \u03b1 encourages the creation of more branches. We call \u03b1 the branching factor. The network is widened by creating the number of branches that minimizes the loss function, or gld ? = arg min\ngd\nLl(gd).\nThe separation term is a function of the branch affinity matrix Ab. For each i \u2208 [d], we have\nLis(gd) = 1\u2212 mean k\u2208g\u22121(i)\n( min\nl\u2208g\u22121(i) Ab(k, l)\n) , (10)\nand the separation cost is the average across each newly created branches\nLs(gd) = 1\nd \u2211 i\u2208[d] Lis(gd). (11)\nNote Equation 10 measures the maximum distances (minimum affinity) between the tasks within the same group. It penalizes cases where very dissimilar tasks are included in the same branch."}, {"heading": "4. Experiments", "text": "We perform an extensive evaluation of our approach on person attribute classification tasks. We use CelebA [24] dataset for facial attribute classification tasks and Deepfashion [23] for clothing category classification tasks. CelebA consists of images of celebrities labeled with 40 attribute classes. Most images also include the torso region in addition to the face. Our models are evaluated using the standard classification accuracy (average of classification accuracy rate over all attribute classes) and the top-10 recall rate (proportion of correctly retrieved attributes from the top-10 prediction scores for each image). Top-10 is used as there are on average about 9 positive facial attributes per image on this dataset. DeepFashion is richly labeled with 50 categories of clothes, such as \u201cshorts\u201d, \u201cjeans\u201d, \u201ccoats\u201d, etc. (the labels are mutually exclusive). Faces are often visible on these images. We evaluate top-3 and top-5 classification accuracy to directly compare with benchmark results in [23]."}, {"heading": "4.1. Comparison with the State of the art", "text": "We establish three baselines. The first baseline is a VGG-16 model initialized from the a model trained from imdb-wiki gender classification [30]. The second baseline is a low-rank model with low rank factorization at all layers. This model is also initialized from the imdb-wiki gender pretrained model, but the initialization is through truncated Singular Value Decomposition (SVD) [7]. The number of basis filters is 8-16-32-64-64 for the convolutional layers, 64-64 for the two fully-connected layers and 16 for the output layer. The third is a thin model initialized using the SOMP initialization method introduced in Section 3.1, using the same pre-trained model. Our VGG-16 baselines are stronger than all previously reported methods, while the low-rank baselines closely matches the state-of-the-art while being faster and more compact. The thin baseline is up to 6 times faster, 500 times more compact than the VGG16 baseline, but still reasonably accurate.\nWe find several contributing factors to the strength of our baselines. Firstly, the choice of pre-trained model is critical. Most recent works use the VGG face descriptor, whereas in our work we use the pre-trained model from imdb-wiki [31]. For the thin baseline, it is also important to use Batch Normalization (BN) [15]. Without the adoption of BN layers the training error ceases to decrease after a small number of training iterations. We observe this phenomenon in both random initialization and SOMP initialization.\nA comparison of the models generated by our adaptive widening algorithm with baseline results are shown in Table 1 and 2. Our \u201cbranching\u201d models achieves similar or better accuracy compared to these state-of-the-art methods, while being much more compact and faster."}, {"heading": "4.2. Cross-domain Training of Joint Person Attribute Network", "text": "To examine the ability of our approach in handling crossdomain tasks, we train a network that jointly predict facial and clothing attributes. The model is trained on the union of the two training sets. Note that the CelebA dataset is not annotated with clothing labels, and the Deepfashion dataset is not annotated with facial attribute labels. To augment the annotations for both datasets, we use the predictions provided by the baseline VGG-16 models as soft training targets. We demonstrate that the joint model is comparable to the state-of-the-art on both facial and clothing tasks, while being a much more efficient combined model rather than two separate models. The comparison between the joint models with the baselines is shown in Table 1 and 2."}, {"heading": "4.3. Visual Validation of Task Grouping", "text": "We visually inspect the task groupings in the generated model. Figure 3 displays the actual task grouping in the Branch-32-2.0 model trained on CelebA. The grouping are often highly intuitive. For instance, \u201c5-o-clock Shadow\u201d, \u201cBushy Eyebrows\u201d and \u201cNo Beard\u201d, which all describe some forms of facial hairs, are grouped. The cluster with \u201cHeavy Makeup\u201d, \u201cPale Skin\u201d and \u201cWearing Lipstick\u201d is\nclearly related. Groupings at lower layers are also sensible. As an example, the group \u201cBags Under Eyes\u201d, \u201cBig Nose\u201d and \u201cYoung\u201d are joined by \u201cAttractive\u201d and \u201cReceding Hairline\u201d at fc6, probably because they all describe age cues. This is particularly interesting as no human intervention is involved in model generation."}, {"heading": "4.4. Ablation Studies", "text": "What are the advantages of grouping similar tasks? We shuffle the correspondence between training targets and the output of the network for \u201cBranch-32-2.0\u201d model from CelebA and report the reduction in accuracies for each tasks. Both random and manual shuffling are tested but we only report the one from manual shuffling as they are similar. In particular, for manual shuffling we choose a new grouping of tasks so that the network separates many tasks that are originally in the same branch. Figure 4 summarizes our findings. Clearly grouping tasks according to similarity improves accuracy for most tasks.\nCloser examination yields other interesting observations. The three tasks that actually benefit from the shuffling significantly (unlike most of the tasks), namely \u201cwavy hair\u201d, \u201cwearing necklace\u201d and \u201cpointy nose\u201d are all from the branch with the largest number of tasks. This is sensible as after the shuffling they are not forced to share filters with many other tasks. But other tasks from the same branch, namely \u201cblack hair\u201d and \u201cwearing earrings\u201d are significantly improved from the original grouping. One possible explanation is that while grouping similar tasks allow them to benefit from multi-task learning, some tasks are intrinsically more difficult and require a wider branch. Our current design lacks the ability to change the width of a branch, which is an interesting future direction.\nSub-optimal use of pretrained network or smaller capacity? The gap in accuracy between Branch-32-2.0 and VGG-16 baseline can be caused by sub-optimal use of the pretrained model or the intrinsically smaller capacity of the former. To determine if both factors contribute to the gap, we compare training the Branch-32-2.0 model and VGG-16\nfrom scratch on CelebA. As neither model benefit from the information from a pre-trained network, we expect a much smaller gap in accuracy if the sub-optimal use of the pretrained model is the main cause. Our results summarized in Table 3 suggest that the smaller capacity of the Branch-322.0 model is likely the main reason for the accuracy gap.\nHow does SOMP help the training? We compare training with and without this initialization using the Baselinethin-32 model on CelebA, under identical training conditions. The evolution of training and validation accuracies are shown in Figure 5. Clearly, the network initialized with SOMP initialization converges faster and better than the one without SOMP initialization."}, {"heading": "5. Conclusion", "text": "We have proposed a novel method for learning the structure of compact multi-task deep neural networks. Our method starts with a thin network model and expands it during training by means of a novel multi-round branching mechanism, which determines with whom each task shares features in each layer of the network, while penalizing for the complexity of the model. We demonstrated compelling results of the proposed approach on the problem of person attribute classification. As future work, we plan to adapt\nour approach to other related problems, such as incremental learning and domain adaptation."}], "references": [{"title": "Network of experts for large-scale image categorization", "author": ["K. Ahmed", "M.H. Baig", "L. Torresani"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Dynamic capacity networks", "author": ["A. Almahairi", "N. Ballas", "T. Cooijmans", "Y. Zheng", "H. Larochelle", "A. Courville"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Integrated perception with recurrent multi-task neural networks", "author": ["H. Bilen", "A. Vedaldi"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Multi-task learning", "author": ["R. Caruana"], "venue": "Machine Learning Journal,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Deep domain adaptation for describing people based on fine-grained clothing attributes", "author": ["Q. Chen", "J. Huang", "R. Feris", "L.M. Brown", "J. Dong", "S. Yan"], "venue": "In CVPR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Y. Cheng", "F. Yu", "R. Feris", "S. Kumar", "A. Choudhary", "S.F. Chang"], "venue": "In ICCV,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["E.L. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Attributebased people search: Lessons learnt from a practical surveillance system", "author": ["R. Feris", "R. Bobbitt", "L. Brown", "S. Pankanti"], "venue": "In ICMR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Tamp: A library for compact deep neural networks with structured matrices", "author": ["B. Gong", "B. Jou", "F. Yu", "S.-F. Chang"], "venue": "In ACM Multimedia,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "In arXiv preprint arXiv:1503.02531,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Cross-domain image retrieval with a dual attribute-aware ranking network", "author": ["J. Huang", "R.S. Feris", "Q. Chen", "S. Yan"], "venue": "In ICCV,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Training cnns with low-rank filters for efficient image classification", "author": ["Y. Ioannou", "D. Robertson", "J. Shotton", "R. Cipolla", "A. Criminisi"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Clustered multi-task learning: A convex formulation", "author": ["L. Jacob", "J.-p. Vert", "F.R. Bach"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Deep cross residual learning for multi-task visual recognition", "author": ["B. Jou", "S.F. Chang"], "venue": "In ACM Multimedia,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Z. Kang", "K. Grauman", "F. Sha"], "venue": "In ICML,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Ubernet: Training a universal cnn for low-, mid- , and high- level vision using diverse datasets and limited memory", "author": ["I. Kokkinos"], "venue": "In arXiv preprint arXiv:1609.02132,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Learning task grouping and overlap in multi-task", "author": ["A. Kumar", "H. Daume III"], "venue": "In ICML,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Wow! you are so beautiful today", "author": ["L. Liu", "J. Xing", "S. Liu", "H. Xu", "X. Zhou", "S. Yan"], "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Deepfashion: Powering robust clothes recognition and retrieval with rich annotations", "author": ["Z. Liu", "P. Luo", "S. Qiu", "X. Wang", "X. Tang"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Deep learning face attributes in the wild", "author": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "venue": "In ICCV,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Crossstitch networks for multi-task learning", "author": ["I. Misra", "A. Shrivastava", "A. Gupta", "M. Hebert"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Flexible modeling of latent task structures in multitask learning", "author": ["A. Passos", "P. Rai", "J. Wainer", "H. Daume III"], "venue": "arXiv preprint arXiv:1206.6486,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Channel-level acceleration of deep face representations", "author": ["A. Polyak", "L. Wolf"], "venue": "IEEE Access,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition", "author": ["R. Ranjan", "V. Patel", "R. Chellappa"], "venue": "In arXiv preprint arXiv:1603.01249,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Fitnets: Hints for thin deep nets", "author": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.6550,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Deep expectation of real and apparent age from a single image without facial landmarks", "author": ["R. Rothe", "R. Timofte", "L.V. Gool"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Deep expectation of real and apparent age from a single image without facial landmarks", "author": ["R. Rothe", "R. Timofte", "L.V. Gool"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Moon: A mixed objective optimization network for the recognition of facial attributes", "author": ["E. Rudd", "M. G\u00fcnther", "T. Boult"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["T.N. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "In ICASSP,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Structured transforms for small-footprint deep learning", "author": ["V. Sindhwani", "T. Sainath", "S. Kumar"], "venue": "In NIPS,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Deep attributes driven multi-camera person re-identification", "author": ["C. Su", "S. Zhang", "J. Xing", "W. Gao", "Q. Tian"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Person attribute recognition with a jointly-trained holistic cnn model", "author": ["P. Sudowe", "H. Spitzer", "B. Leibe"], "venue": "In ICCV ChaLearn Looking at People Workshop,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "In CVPR,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Convolutional neural networks with low-rank regularization", "author": ["C. Tai", "T. Xiao", "X. Wang"], "venue": "ICLR, 2016", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Learning to learn", "author": ["S. Thrun", "L. Pratt"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1998}, {"title": "Algorithms for simultaneous sparse approximation. part ii: Convex relaxation", "author": ["J.A. Tropp"], "venue": "Signal Processing,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2006}, {"title": "Algorithms for simultaneous sparse approximation. part i: Greedy pursuit", "author": ["J.A. Tropp", "A.C. Gilbert", "M.J. Strauss"], "venue": "Signal Processing,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2006}, {"title": "Attribute-based people search in surveillance environments", "author": ["D.A. Vaquero", "R.S. Feris", "D. Tran", "L. Brown", "A. Hampapur", "M. Turk"], "venue": "In WACV,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2009}, {"title": "Walk and learn: Facial attribute representation learning from egocentric video and contextual data", "author": ["J. Wang", "Y. Cheng", "R.S. Feris"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Multitask learning for classification with dirichlet process priors", "author": ["Y. Xue", "X. Liao", "L. Carin", "B. Krishnapuram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2007}, {"title": "Hd-cnn: Hierarchical deep convolutional neural network for large scale visual recognition", "author": ["Z. Yan", "H. Zhang", "R. Piramuthu", "V. Jagadeesh", "D. DeCoste", "W. Di", "Y. Yu"], "venue": "In ICCV,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "From facial parts responses to face detection: A deep learning approach", "author": ["S. Yang", "P. Luo", "C.-C. Loy", "X. Tang"], "venue": "In ICCV,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "In ECCV,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2014}, {"title": "Panda: Pose aligned networks for deep attribute modeling", "author": ["N. Zhang", "M. Paluri", "M. Ranzato", "T. Darrell", "L. Bourdev"], "venue": "In CVPR,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2014}, {"title": "Learning deep representation for face alignment with auxiliary attributes", "author": ["Z. Zhang", "P. Luo", "C.C. Loy", "X. Tang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2016}, {"title": "Clustered multi-task learning via alternating structure optimization", "author": ["J. Zhou", "J. Chen", "J. Ye"], "venue": "In NIPS,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2011}], "referenceMentions": [{"referenceID": 27, "context": "To avoid this, most of the multi-task deep architectures share the bottom layers till some layer l after which the sharing is blocked, resulting in task-specific sub-networks or branches beyond it [28, 17, 13].", "startOffset": 197, "endOffset": 209}, {"referenceID": 16, "context": "To avoid this, most of the multi-task deep architectures share the bottom layers till some layer l after which the sharing is blocked, resulting in task-specific sub-networks or branches beyond it [28, 17, 13].", "startOffset": 197, "endOffset": 209}, {"referenceID": 12, "context": "To avoid this, most of the multi-task deep architectures share the bottom layers till some layer l after which the sharing is blocked, resulting in task-specific sub-networks or branches beyond it [28, 17, 13].", "startOffset": 197, "endOffset": 209}, {"referenceID": 24, "context": "However, the space of such possible branching architectures is combinatorially large and current approaches largely make a decision based on limited manual exploration of this space, often biased by designer\u2019s perception of the relationship among different tasks [25].", "startOffset": 263, "endOffset": 267}, {"referenceID": 41, "context": "We also propose a method based on simultaneous orthogonal matching pursuit (SOMP) [42] for initializing a thin network from a pretrained wider network (e.", "startOffset": 82, "endOffset": 86}, {"referenceID": 23, "context": "On the CelebA dataset [24], we match the current top results on facial attribute classification (90% accuracy) with a model 90x more compact and 3x faster than the original VGG-16 model.", "startOffset": 22, "endOffset": 26}, {"referenceID": 22, "context": "We draw similar conclusions for clothing category recognition on the DeepFashion dataset [23], demonstrating that we can perform simultaneous facial and clothing attribute prediction using a single compact multi-task model, while preserving accuracy.", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "There is a long history of research in multi-task learning [4, 40, 16, 21, 25].", "startOffset": 59, "endOffset": 78}, {"referenceID": 39, "context": "There is a long history of research in multi-task learning [4, 40, 16, 21, 25].", "startOffset": 59, "endOffset": 78}, {"referenceID": 15, "context": "There is a long history of research in multi-task learning [4, 40, 16, 21, 25].", "startOffset": 59, "endOffset": 78}, {"referenceID": 20, "context": "There is a long history of research in multi-task learning [4, 40, 16, 21, 25].", "startOffset": 59, "endOffset": 78}, {"referenceID": 24, "context": "There is a long history of research in multi-task learning [4, 40, 16, 21, 25].", "startOffset": 59, "endOffset": 78}, {"referenceID": 44, "context": "A few methods have addressed the problem of \u201cwith whom\u201d each task should share features [45, 16, 51, 18, 21, 26].", "startOffset": 88, "endOffset": 112}, {"referenceID": 15, "context": "A few methods have addressed the problem of \u201cwith whom\u201d each task should share features [45, 16, 51, 18, 21, 26].", "startOffset": 88, "endOffset": 112}, {"referenceID": 50, "context": "A few methods have addressed the problem of \u201cwith whom\u201d each task should share features [45, 16, 51, 18, 21, 26].", "startOffset": 88, "endOffset": 112}, {"referenceID": 17, "context": "A few methods have addressed the problem of \u201cwith whom\u201d each task should share features [45, 16, 51, 18, 21, 26].", "startOffset": 88, "endOffset": 112}, {"referenceID": 20, "context": "A few methods have addressed the problem of \u201cwith whom\u201d each task should share features [45, 16, 51, 18, 21, 26].", "startOffset": 88, "endOffset": 112}, {"referenceID": 25, "context": "A few methods have addressed the problem of \u201cwith whom\u201d each task should share features [45, 16, 51, 18, 21, 26].", "startOffset": 88, "endOffset": 112}, {"referenceID": 27, "context": "HyperFace [28] simultaneously learns to perform face detection, landmarks localization, pose estimation and gender recognition.", "startOffset": 10, "endOffset": 14}, {"referenceID": 18, "context": "UberNet [19] jointly learns low-, mid-, and high-level computer vision tasks using a compact network model.", "startOffset": 8, "endOffset": 12}, {"referenceID": 2, "context": "MultiNet [3] exploits recurrent networks for transferring information across tasks.", "startOffset": 9, "endOffset": 12}, {"referenceID": 16, "context": "Cross-ResNet [17] connects tasks through residual learning for knowledge transfer.", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "Cross-stitching networks [25] have been recently proposed to learn an optimal combination of shared and task-specific representations.", "startOffset": 25, "endOffset": 29}, {"referenceID": 45, "context": "We note that other techniques such as HD-CNN [46] and Network of Experts [1] also group related classes to perform hierarchical classification, but these methods are not applicable for the multi-label setting (where labels are not mutually exclusive).", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "We note that other techniques such as HD-CNN [46] and Network of Experts [1] also group related classes to perform hierarchical classification, but these methods are not applicable for the multi-label setting (where labels are not mutually exclusive).", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "Methods for compressing and accelerating convolutional networks include knowledge distillation [12, 29], low-rank-factorization [14, 39, 33], pruning and quantization [10, 27], structured matrices [6, 35, 9], and dynamic capacity networks [2].", "startOffset": 95, "endOffset": 103}, {"referenceID": 28, "context": "Methods for compressing and accelerating convolutional networks include knowledge distillation [12, 29], low-rank-factorization [14, 39, 33], pruning and quantization [10, 27], structured matrices [6, 35, 9], and dynamic capacity networks [2].", "startOffset": 95, "endOffset": 103}, {"referenceID": 13, "context": "Methods for compressing and accelerating convolutional networks include knowledge distillation [12, 29], low-rank-factorization [14, 39, 33], pruning and quantization [10, 27], structured matrices [6, 35, 9], and dynamic capacity networks [2].", "startOffset": 128, "endOffset": 140}, {"referenceID": 38, "context": "Methods for compressing and accelerating convolutional networks include knowledge distillation [12, 29], low-rank-factorization [14, 39, 33], pruning and quantization [10, 27], structured matrices [6, 35, 9], and dynamic capacity networks [2].", "startOffset": 128, "endOffset": 140}, {"referenceID": 32, "context": "Methods for compressing and accelerating convolutional networks include knowledge distillation [12, 29], low-rank-factorization [14, 39, 33], pruning and quantization [10, 27], structured matrices [6, 35, 9], and dynamic capacity networks [2].", "startOffset": 128, "endOffset": 140}, {"referenceID": 9, "context": "Methods for compressing and accelerating convolutional networks include knowledge distillation [12, 29], low-rank-factorization [14, 39, 33], pruning and quantization [10, 27], structured matrices [6, 35, 9], and dynamic capacity networks [2].", "startOffset": 167, "endOffset": 175}, {"referenceID": 26, "context": "Methods for compressing and accelerating convolutional networks include knowledge distillation [12, 29], low-rank-factorization [14, 39, 33], pruning and quantization [10, 27], structured matrices [6, 35, 9], and dynamic capacity networks [2].", "startOffset": 167, "endOffset": 175}, {"referenceID": 5, "context": "Methods for compressing and accelerating convolutional networks include knowledge distillation [12, 29], low-rank-factorization [14, 39, 33], pruning and quantization [10, 27], structured matrices [6, 35, 9], and dynamic capacity networks [2].", "startOffset": 197, "endOffset": 207}, {"referenceID": 34, "context": "Methods for compressing and accelerating convolutional networks include knowledge distillation [12, 29], low-rank-factorization [14, 39, 33], pruning and quantization [10, 27], structured matrices [6, 35, 9], and dynamic capacity networks [2].", "startOffset": 197, "endOffset": 207}, {"referenceID": 8, "context": "Methods for compressing and accelerating convolutional networks include knowledge distillation [12, 29], low-rank-factorization [14, 39, 33], pruning and quantization [10, 27], structured matrices [6, 35, 9], and dynamic capacity networks [2].", "startOffset": 197, "endOffset": 207}, {"referenceID": 1, "context": "Methods for compressing and accelerating convolutional networks include knowledge distillation [12, 29], low-rank-factorization [14, 39, 33], pruning and quantization [10, 27], structured matrices [6, 35, 9], and dynamic capacity networks [2].", "startOffset": 239, "endOffset": 242}, {"referenceID": 35, "context": "In the visual surveillance domain, person attributes serve as features for improving person re-identification [36] and enable search of suspects based on their description [43, 8].", "startOffset": 110, "endOffset": 114}, {"referenceID": 42, "context": "In the visual surveillance domain, person attributes serve as features for improving person re-identification [36] and enable search of suspects based on their description [43, 8].", "startOffset": 172, "endOffset": 179}, {"referenceID": 7, "context": "In the visual surveillance domain, person attributes serve as features for improving person re-identification [36] and enable search of suspects based on their description [43, 8].", "startOffset": 172, "endOffset": 179}, {"referenceID": 12, "context": "In e-commerce applications, these attributes have proven effective in improving clothing retrieval [13], and fashion recommendation [22].", "startOffset": 99, "endOffset": 103}, {"referenceID": 21, "context": "In e-commerce applications, these attributes have proven effective in improving clothing retrieval [13], and fashion recommendation [22].", "startOffset": 132, "endOffset": 136}, {"referenceID": 46, "context": "It has also been shown that facial attribute prediction is helpful as an auxiliary task for improving face detection [47] and face alignment [50].", "startOffset": 117, "endOffset": 121}, {"referenceID": 49, "context": "It has also been shown that facial attribute prediction is helpful as an auxiliary task for improving face detection [47] and face alignment [50].", "startOffset": 141, "endOffset": 145}, {"referenceID": 43, "context": "State-of-the-art methods for person attribute prediction are based on deep convolutional neural networks [44, 24, 5, 49].", "startOffset": 105, "endOffset": 120}, {"referenceID": 23, "context": "State-of-the-art methods for person attribute prediction are based on deep convolutional neural networks [44, 24, 5, 49].", "startOffset": 105, "endOffset": 120}, {"referenceID": 4, "context": "State-of-the-art methods for person attribute prediction are based on deep convolutional neural networks [44, 24, 5, 49].", "startOffset": 105, "endOffset": 120}, {"referenceID": 48, "context": "State-of-the-art methods for person attribute prediction are based on deep convolutional neural networks [44, 24, 5, 49].", "startOffset": 105, "endOffset": 120}, {"referenceID": 48, "context": "tribute [49] or perform joint learning with a fully shared network [32].", "startOffset": 8, "endOffset": 12}, {"referenceID": 31, "context": "tribute [49] or perform joint learning with a fully shared network [32].", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "Multi-task networks have been used with base layers that are shared across all attributes, and branches to encode task-specific features for each attribute category [13, 37].", "startOffset": 165, "endOffset": 173}, {"referenceID": 36, "context": "Multi-task networks have been used with base layers that are shared across all attributes, and branches to encode task-specific features for each attribute category [13, 37].", "startOffset": 165, "endOffset": 173}, {"referenceID": 47, "context": "This is confirmed by previous works on visualization of filters at different layers [48].", "startOffset": 84, "endOffset": 88}, {"referenceID": 19, "context": "Notably, popular deep convolutional network architectures, such as AlexNet [20], VGG [34], Inception [38] and ResNet [11] all use wider layers at the top Algorithm 1: Training with Adaptive Widening", "startOffset": 75, "endOffset": 79}, {"referenceID": 33, "context": "Notably, popular deep convolutional network architectures, such as AlexNet [20], VGG [34], Inception [38] and ResNet [11] all use wider layers at the top Algorithm 1: Training with Adaptive Widening", "startOffset": 85, "endOffset": 89}, {"referenceID": 37, "context": "Notably, popular deep convolutional network architectures, such as AlexNet [20], VGG [34], Inception [38] and ResNet [11] all use wider layers at the top Algorithm 1: Training with Adaptive Widening", "startOffset": 101, "endOffset": 105}, {"referenceID": 10, "context": "Notably, popular deep convolutional network architectures, such as AlexNet [20], VGG [34], Inception [38] and ResNet [11] all use wider layers at the top Algorithm 1: Training with Adaptive Widening", "startOffset": 117, "endOffset": 121}, {"referenceID": 24, "context": "These architectures serve as excellent reference designs in a myriad of domains, but researchers have noted that the width schedule (especially at the top layers) need to be tuned for the underlying set of tasks the network has to perform in order to achieve best accuracy [25].", "startOffset": 273, "endOffset": 277}, {"referenceID": 11, "context": "In the literature a set of general methods for training arbitrarily small networks using an existing larger network and the training data are known as \u201cknowledge distillation\u2019 [12, 29].", "startOffset": 176, "endOffset": 184}, {"referenceID": 28, "context": "In the literature a set of general methods for training arbitrarily small networks using an existing larger network and the training data are known as \u201cknowledge distillation\u2019 [12, 29].", "startOffset": 176, "endOffset": 184}, {"referenceID": 40, "context": "This problem is NP-hard, however, there exist approaches based on convex relaxation [41] and greedy simultaneous orthogonal matching pursuit (SOMP) [42] which can produce approximate solutions.", "startOffset": 84, "endOffset": 88}, {"referenceID": 41, "context": "This problem is NP-hard, however, there exist approaches based on convex relaxation [41] and greedy simultaneous orthogonal matching pursuit (SOMP) [42] which can produce approximate solutions.", "startOffset": 148, "endOffset": 152}, {"referenceID": 23, "context": "We use CelebA [24] dataset for facial attribute classification tasks and Deepfashion [23] for clothing category classification tasks.", "startOffset": 14, "endOffset": 18}, {"referenceID": 22, "context": "We use CelebA [24] dataset for facial attribute classification tasks and Deepfashion [23] for clothing category classification tasks.", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "We evaluate top-3 and top-5 classification accuracy to directly compare with benchmark results in [23].", "startOffset": 98, "endOffset": 102}, {"referenceID": 29, "context": "The first baseline is a VGG-16 model initialized from the a model trained from imdb-wiki gender classification [30].", "startOffset": 111, "endOffset": 115}, {"referenceID": 6, "context": "This model is also initialized from the imdb-wiki gender pretrained model, but the initialization is through truncated Singular Value Decomposition (SVD) [7].", "startOffset": 154, "endOffset": 157}, {"referenceID": 30, "context": "Most recent works use the VGG face descriptor, whereas in our work we use the pre-trained model from imdb-wiki [31].", "startOffset": 111, "endOffset": 115}, {"referenceID": 14, "context": "For the thin baseline, it is also important to use Batch Normalization (BN) [15].", "startOffset": 76, "endOffset": 80}, {"referenceID": 43, "context": "LNet+ANet and Walk and Learn results are cited from [44].", "startOffset": 52, "endOffset": 56}, {"referenceID": 31, "context": "MOON results are cited from [32].", "startOffset": 28, "endOffset": 32}, {"referenceID": 22, "context": "WTBI and DARN results are cited from [23].", "startOffset": 37, "endOffset": 41}, {"referenceID": 22, "context": "We cite the reported results for clothing category [23].", "startOffset": 51, "endOffset": 55}], "year": 2016, "abstractText": "Multi-task learning aims to improve generalization performance of multiple prediction tasks by appropriately sharing relevant information across them. In the context of deep neural networks, this idea is often realized by handdesigned network architectures with layers that are shared across tasks and branches that encode task-specific features. However, the space of possible multi-task deep architectures is combinatorially large and often the final architecture is arrived at by manual exploration of this space subject to designer\u2019s bias, which can be both error-prone and tedious. In this work, we propose a principled approach for designing compact multi-task deep learning architectures. Our approach starts with a thin network and dynamically widens it in a greedy manner during training using a novel criterion that promotes grouping of similar tasks together. Extensive evaluation on person attributes classification tasks involving facial and clothing attributes suggests that the models produced by the proposed method are fast, compact and can closely match or exceed the state-of-theart accuracy from strong baselines by much more expensive models.", "creator": "LaTeX with hyperref package"}}}