{"id": "1611.05664", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2016", "title": "Learning to detect and localize many objects from few examples", "abstract": "The current trend in object detection and localization is to learn predictions with high capacity deep neural networks trained on a very large amount of annotated data and using a high amount of processing power. In this work, we propose a new neural model which directly predicts bounding box coordinates. The particularity of our contribution lies in the local computations of predictions with a new form of local parameter sharing which keeps the overall amount of trainable parameters low. This could be used in general to reduce the error rates for prediction. Moreover, we believe that the predictions are related to a new generation of models and their model-specific prediction models, which are currently undergoing new validation studies.", "histories": [["v1", "Thu, 17 Nov 2016 12:51:18 GMT  (754kb,D)", "http://arxiv.org/abs/1611.05664v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["bastien moysset", "christoper kermorvant", "christian wolf"], "accepted": false, "id": "1611.05664"}, "pdf": {"name": "1611.05664.pdf", "metadata": {"source": "CRF", "title": "Learning to detect and localize many objects from few examples", "authors": ["Bastien Moysset", "Christoper Kermorvant", "Christian Wolf"], "emails": [], "sections": [{"heading": null, "text": "We show that this model is more powerful than the state of the art in applications where training data is not as abundant as in the classical configuration of natural images and Imagenet/Pascal VOC tasks. We particularly target the detection of text in document images, but our method is not limited to this setting. The proposed model also facilitates the detection of many objects in a single image and can deal with inputs of variable sizes without resizing."}, {"heading": "1. Introduction", "text": "Object detection and localization in images is currently dominated by approaches which first create proposals (hypothesis bounding boxes) followed by feature extraction and pooling on these boxes and classification, the latter steps being usually performed by deep networks [9, 8, 27, 26, 1]. Very recent methods also use deep networks for the proposal step [27, 26, 15], sometimes sharing features between localization and classification. Differences exist in the detailed architectures in the way calculations are shared over layers, scales, spatial regions etc. (see section 2 for a detailed analysis). Another criterion is the coupling between hypothesis creation and confirmation/classification. Earlier works create thousands of hypotheses per image,\nsometimes using low level algorithms (e.g. R-CNN [9]), leaving the burden of validation to a subsequent classifier. Current work tends to create very few proposals per image, which satisfy a high degree of \u201cobjectness\u201d.\nIn this work we focus on the localization step, targeting cases where the existing methods tend to give weak results:\n\u2022 the current trend is to design high capacity networks trained on large amounts of training data either directly or as a pre-training step. However, in some applications, the image content is only very weakly correlated to the data available in standard dataset like Imagenet. In the case of small and medium amounts of training data, fully automatic training of deep models remains a challenge in these cases.\n\u2022 we allow for the detection and localization of a relatively high number of potentially small objects in an image, which is especially hard for existing methods [1]. Our target application is the localization of text boxes, but our method is not restricted to this kind of setting.\nSimilar to recent work, the proposed method localizes\n1\nar X\niv :1\n61 1.\n05 66\n4v 1\n[ cs\n.C V\n] 1\n7 N\nov 2\nbounding boxes by direct regression of (relative) coordinates. The main contribution we claim is a new model which performs spatially local computations, efficiently sharing parameters spatially. The main challenge in this case is to allow the model to collect features from local regions as well as globally pooled features in order to be able to efficiently model context.\nSimilar to models like YOLO [26] and Single-Shot Detector [15], our outputs are assigned to local regions of the image. However, in contrast to these methods, each output is trained to be able to predict objects in its support region, or outside. Before each gradient update step, we globally match predictions and ground truth objects. Each output of our model directly sees only a limited region of the input image, which keeps the overall number of trainable parameters low. However, outputs get additional information from outside regions through context, which is collected using spatial 2D recurrent (LSTM) units. This spatial context layer proved to be a key component of our model.\nWe propose the following contributions:\n\u2022 A new fully convolutional model for object detection using spatial 2D-LSTM layers for handling spatial context with an objective of high spatial parameter sharing.\n\u2022 The capability of predicting a large number of outputs,\nmade possible by the combination of highly local output layers (1 \u00d7 1 convolutions) and preceding spatial LSTM layers.\n\u2022 The possibility of predicting outputs from input images of variable size without resizing the input.\n\u2022 An application to document analysis with experiments on the difficult and heterogeneous Maurdor dataset, which show that the model significantly outperforms the state of the art in objects detection.\nThe paper is organized as follows: the next section briefly outlines related work. Section 2 discusses properties and trade-offs of deep models related to convolutions, poolings and subsampling, which will be related to our proposed model. Based on these conclusions, a new model is introduced in section 3."}, {"heading": "1.1. Related work", "text": "Earlier (pre-deep learning) work on object recognition proceeded through matching of local features [17] or by decomposing objects into mixtures of parts and solving combinatorial problems [7]. Early work on deep learning first extended the sliding window approach to deep neural networks. To avoid testing a large number of positions and\naspect ratios, R-CNN [9] introduced the concept object proposals, created by separate methods, followed by convolutional networks to classify each proposal. The concept was improved as Fast R-CNN [8] and Faster R-CNN [27].\nErhan et al. proposed Multibox [6, 29], which performs direct regression of bounding box locations instead of relying on object proposals. After each forward pass, network outputs are assigned to target ground-truth boxes through a matching algorithm. YOLO [26] and the Single-Shot Detector [15] can be seen as variants of this concept, they will be discussed in more detail in section 2.\nSome recent work strives to detect and localize objects with pixel-wise precision, which somewhat blurs the boundaries between object detection and semantic segmentation [23, 21]. Methods which learn to segment without pixelwise ground truth have also been proposed [22]. Pixelwise segmentation is not needed in our application, where the segmentation step is performed in a latter stage jointly with recognition (recognition results will be given in the experimental section).\nContext through spatial 2D-recurrent networks has been proposed as early as in [12]. However, up to our knowledge, no method did use it for object localization. Similarly to our method, inside-Outside-Nets [1] contain 2D spatial context layers collecting information from 4 different directions. However, the hidden transitions of recurrent layers are set to identity, whereas our model contains fully-fledged trainable 2D-LSTM layers. Moreover, localization is performed as ROI proposals with selective search, the deep model being used only for classification and bounding box correction, whereas we do not require a region proposal step. Our model directly performs bounding box regression. Other recent work uses 2D recurrent networks for semantic segmentation [32].\nCNNs have been used before for text detection, for instance in [34], a Fully convolutional network (FCN) is used to classify each position of a salient map as text or nontext. In [13], a YOLO-related method is proposed for the detection of text in natural images but only few objects are present in the images.\nThe problem of dataset sizes has been addressed before, with strategies reaching from external memories [30] and unsupervised learning, for instance by learning feature extraction from physics [25]."}, {"heading": "2. Delving again into convolutions, pooling,", "text": "strides and spatial structure\nObject detection and localization with convolutional deep neural networks is governed by a set of implicit properties and requirements, which we will try to lay out in the following lines. We will concentrate on the approach of direct prediction of object locations (as opposed to creating proposals from additional and not-tightly connected methods).\nThe goal of this section is to discuss the effects and importances of each part and the trade-offs to consider in these architectures, which will lead us then to the formulation of the proposed model.\nThe input image is passed through a series of convolutional layers, each of which extracts features from the preceding layer. Although not absolutely required, reducing the spatial size of the features maps (often combined with pooling) is frequently done in order to increase the receptive fields, i.e. the relative size of the filters w.r.t. to the inputs. Choosing when to pool and to reduce can be critical, and optimizations can lead to large decreases in the numbers of trainable parameters [14]. An alternative to in-betweenlayer pooling is changing the size of filters, especially as \u201ca trou\u201d computation in order to keep the number of parameters low [33].\nAt some point, a model needs to collect features from a spatial support region. The way this pooling is distributed over the different layers will decide important properties of the model:\n\u2022 Classical networks stop the sequence of convolutions and reductions before the spatial size of the feature map shrinks to 1\u00d71, keeping a spatial / geometrical structure in the feature representation. Subsequent fully connected layers then perform further feature extraction, implicit pooling and decision taking.\nThe spatial structure of the feature map allows to perform controlled pooling from specific regions, but limits the shift-invariance of the representation.\n\u2022 More recently, fully-convolutional networks (FCN) perform convolutions and reductions+pooling until the spatial size of the feature map is negligible, e.g. 1\u00d71, with a high feature dimension (1\u00d71\u00d74096 in the network for semantic segmentation proposed in [16]). The goal here is to fully translate geometry and appearance into features and semantics.\nTraining can in principle lead to a spatial structuring of the feature dimension, i.e. training can lead to a situation where different elements of the feature layer correspond to different regions in the input image. However, this is not a constraint in the model and each activation of the last feature layer can potentially contain features from the full image.\nObject detection and localization require certain properties, like shift invariance, spatial precision and context collected from the global scene. Several state of the art models, Multibox [6], YOLO [26] and Single-Shot Detector (SSD) [15] tackle this through an architecture sketched in figure 2a and 2b1. A sequence of convolutions and reductions decrease the spatial size of feature maps down to a small grid\n1The purpose of this figure is to show the strategy these models use to\n(7\u00d77 for [26], 9\u00d79 for [15]). This map is then fully connected to a 1\u00d71\u00d74096 feature layer and again fully connected to a set of outputs, each output predicting bounding box positions and confidence scores (as well as class scores if required). This approach has several advantages. Each of the outputs is fully connected to previous layers and therefore potentially has access to information from the full image. The last feature layer mixes spatial structure and feature dimensions in a trainable way.\nAlthough there is no principled difference in how the last fully connected layer is actually implemented in the three models, we display the output layer differently for Multibox [6] (figure 2a) and for YOLO [26] or SSD [15] (figure 2b). For the latter two, and also in accordance with the figures of the respective papers, the outputs are shown as a spatial grid (7\u00d77 for [26], 9\u00d79 for [15]). However, this structuring is an interpretation, as the spatial structure of the grid is not wired into the network architecture. It is justified through the way training is performed in these models, in particular on the way ground truth outputs are matched (assigned) to the network outputs. In the case of [26], this assignment is purely spatial: outputs of a given cell are trained to provide predictions of a spatial region corresponding to this cell (see section 4).\nThe main shortcoming of these models, which we will address in the next section, lies in the fully connected feature and output layers at the end. We argue that they limit invariance and contain too many parameters.\ntranslate geometry and resolution into features. In particular, we do not show the actual numbers of layers and units. For SSD[15] , we do not show the way how this model handles multiple scales."}, {"heading": "3. A local spatially recurrent model", "text": "We propose a new model designed to detect a large number of (potentially) small objects from a low number of training examples, i.e. with a model with a small number of trainable parameters. We achieve this with two techniques:\nA) Feature sharing \u2014 we predict different object locations from local features only. More precisely, the output layer of a single object bounding box is not fully connected to the previous layer, as illustrated in figure 2c. Outputs are connected through 1\u00d71 convolutions, i.e. each element (i, j) of the last feature map is connected to its own set ofK output modules, each module consisting of relative x and y positions, width, and height and a confidence score used to confirm the presence of an object at the predicted position. The objectives here are two-fold:\n\u2022 To drastically reduce the number of parameters in the output layer by avoiding parameter hungry fully connected layers.\n\u2022 To share parameters between locations in the image, increasing shift invariance and significantly reducing the requirements for data augmentation.\nB) Spatial recurrent context layers \u2014 the drawback of local parameter sharing is twofold: i) objects may be larger than the receptive field of each output, and ii) we may lose valuable context information from the full input image. We address both these concerns through context layers consisting of Multi-Dimensional Long-Short term memory models [11], which are inserted between the convolutional layers.\nThese MD-LSTM layers aim at recovering the context information from the area outside of the receptive field.\nFigure 3 illustrates how the context layers are organized. Each convolutional layer is followed by 4 different parallel 2D-LSTM layers, which propagate information over the feature map elements in 4 different diagonal directions, starting at the 4 edges. For each of the directions, each element gets recurrent connections from 2 different neighbouring sites. The outputs of the 4 directions are summed \u2014 concatenation would have been another possibility, albeit with a drastically higher amount of parameters. No pooling is performed between the convolutions. Spatial resolution is reduced through convolutions with strides between 2 and 4 (see table 1).\nThe network outputs are computed from the last hidden layer as a regression of the normalized relative bounding box locations. In particular, the absolute location of each predicted object is calculated by multiplying the network output with a width parameter vector \u039b and an offset vector \u2206, whose values depend on the architecture of the network. More formally, the location li,j,k for the kth object prediction of element (i, j) of the last feature map is given as follows:\nli,j,k = \u039b T\u03c3 (Ukhi,j + ck) + [i\u22121 j\u22121]T \u2206 (1)\nwhere h is the last hidden layer, \u03c3(\u00b7) is the element-wise sigmoid function and the weights Uk and biases ck are trainable parameters. Note that, since the outputs are 1\u00d71 convolutions, the parameters {Uk, ck} are shared over locations (i, j). However, each object predictor k features its own set of parameters.\nFlexibilty \u2014 another significant advantage of the proposed local method is that we can handle images of varying sizes without performing any resizing or cropping. Decreasing or increasing the size of the input image, or changing its aspect ratio, will change the size of the post convolutional feature maps accordingly. This will change the number of network outputs, i.e. object predictions."}, {"heading": "4. Training", "text": "The model is trained with stochastic gradient descent (SGD) using mini-batches of size 8 and dropout for regularization. During training, object predictors (network outputs) need to be matched to targets, i.e. to groundtruth object positions. Similar to the strategy in MultiBox [6], this is done globally over the entire image, which allows each bounding box predictor to respond to any location in an image.\nWe denote by M the number of predicted objects, given as M = I \u2217 J \u2217 K, with I and J being the width and the height of the last feature map and K the number of predictors per feature map location; we denote by N the number of reference objects in the image. Matching is a combinatorial problem over the matching matrix X , where Xnm=1\nwhen hypothesis m is matched to target n, and 0 otherwise. For each forward-backward pass for each image, X is estimated minimizing the following cost function:\nCost = N\u2211 n=0 M\u2211 m=0 Xnm ( \u03b1 \u2016lm \u2212 tn\u20162 \u2212 log(cm) ) \u2212(1\u2212Xnm) log(1\u2212 cm) (2)\nwhere lm is a vector of size 4 corresponding to a predicted location, cm is the corresponding confidence, and tn as a target location. The first term handles location alignment, the second favours high confidence and \u03b1 is a weight between both terms.\nEquation (2) is minimized subject to constraints, namely that each target box is matched to at most one hypothesis box and vice versa. This is a well known bi-partite graph matching problem, which can be solved with the Hungarian algorithm [18]. Equation (2) gives the loss function used in the SGD parameter updates. However, we prefer to set different values of \u03b1 for gradient updates and for matching. We found it important to increase \u03b1 for the matching in order to help the network to use more diverse outputs.\nAs mentioned earlier, our matching strategy is similar to the one described in MultiBox [6] and has the same global property brought by the confidence term (albeit applied to local outputs, compared to the global outputs in [6]). On the other hand, in SSD[15] and YOLO[26], matching is done locally, i.e. predictors are matched to targets falling into spatial regions they are associated with. This is the reason for the spatial interpretation of the output grid shown in figure 2. Moreover, YOLO matches only one target location with each spatial cell, which leads to non-matched targets in the case of several objects with bounding box centers in the same cell. In our target application, where a large number of objects may be present, a large number of objects will not be matched to any predictor during training, as can be seen in the example in figure 4.\nIn SSD and MultiBox, the matching process is restricted to a fixed dictionary of anchor locations obtained arbitrarily[15] or with clustering[6], which helps the network to create outputs specialized to regions in the image. This was proved unnecessary and even counter-productive in our case, where predictors share parameters spatially."}, {"heading": "5. Experimental results", "text": "We tested the proposed model and the baselines on the publicly available Maurdor dataset [4]. This highly heterogeneous dataset is composed of 8773 document images ( train:6592; valid:1110; test:1071 ) in mixed French, English and Arabic text, both handwritten and printed.\nThe dataset is annotated at paragraph level. For this reason, we use the technique detailed in [3] to get annotation\nat line level and we keep only the pages where we are confident that the automatic line position generation has worked well. We obtain a restricted dataset containing 3995 training pages, 697 validation pages and 616 test pages that are used for training, validation and test for the evaluation of intersection over union and detEval metrics.\nFor the Bag of Word metric, we evaluate on the 265 pages fully in English and on the 507 pages fully in French of the full Maurdor test set in order to avoid the line language classification task."}, {"heading": "5.1. Metrics", "text": "We evaluate the performance of our method using three different metrics:\nIntersection over union \u2014 IoU is a commonly used metric in object detection and image segmentation. It is given as the ratio of the intersection and the union of reference and hypothesis objects. Reference objects and hypothesis objects are matched by thresholding their IoU score. In most frequent versions, only one hypothesis can be associated to a reference box, the others are considered as error/insertions. Alternatively to reporting IoU directly, after thresholding IoU, an F-Measure can be computed from Precision and Recall.\nDetEval \u2014 DetEval [31] is the metric chosen for the ICDAR robust reading series of competitions. Its main advantage is that it allows many-to-many matchings between reference objects and hypothesis objects, which is important in applications where fragmentation of objects should be allowed (and eventually slightly punished), which is the case in text localization. Objects are assigned by thresholding overlap, and Precisions, Recall and F-Mesure are reported.\nBag-of-words recognition error \u2014 BoW is a goal oriented method described in [24], which measures the performance of a subsequent text recognition module. The objective is to avoid the need of judging the geometrical precision of the result and to directly evaluate the performance of the goal of any localization method. In the case of the target application this is the subsequent recognizer.\nWe use the recognition model from [20], which is based on deep convolutional networks and spatial/sequence modelling with 2D-LSTM layers. Assigning character labels to network outputs is performed with the Connectionist Temporal Classification framework [10]. Recent follow-up work solves this problem with attention based mechanisms [2], this will be investigated in future work. The recognizer is trained on both handwritten and printed text lines, separately on English and French text. We apply them on crops of localized bounding boxes. The Bag of Word metric, on the contrary to metrics based on the Levenshtein distance enables to avoid an alignment that can be ambiguous at page level. Word insertion and deletions are computed at page level and F-Measure is reported."}, {"heading": "5.2. Baselines", "text": "Traditional text segmentation methods \u2014 For comparison, we used two techniques based on image processing (w/o machine learning) for document text line segmentation. Shi et al. [28] use steerable directional filters to create an adaptive local connectivity map. Line locations are given by the positions of the connected components extracted from the binarisation of this connectivity map. These positions are refined with heuristic-based post-processing. The method proposed by Nicolaou et al. [19] follows the whitest and blackest paths in a blurred image to find lines and interlines. Yolo and MultiBox \u2014 For YOLO, we used two classes for the object classification part of the model, handwritten text lines and printed text lines. It helped the model to learn better than without classification.\nBoth systems were tested in two different configurations: the original architecture tuned for large scale image recognition, and an architecture which we optimized for our task on the validation set. In particular, the size of the filters was adapted to the shape of the objects. Hyper-parameter tuning led in both cases to architectures with heavily reduced numbers of layers and less units per layer. We also optimized learning rates and minibatch sizes."}, {"heading": "5.3. Architectures", "text": "The network architecture of the proposed model has been tuned to correspond to our task. The found hyperparameters are detailed in Table 1. The width and height of the feature maps is given for illustration but it can of course vary. In particular, the aspect ratio of the image can vary. We would like to stress again that the number of parameters is independent of the actual size of the input image.\nThe inputs of our network are raw gray-scaled images with width normalisation. The use of color images was not improving the results on our task.\nNote that the number of weights in our last layer, the position prediction layer, is rather small : 3,700. To be able to predict the same number of objects, with the same number of input features, MultiBox[6] and Yolo[26] would have needed 15,688,200 parameters.\nFor training, we used a learning rate of 10\u22124 and minibatches of size 8. Dropout with 0.5 probability is applied after each 2D-LSTM layer. The \u03b1 parameter in equation (2) is set to 1000 for matching and to 100 for weight updates during SGD.\nWe experimentaly found that resolution reduction between layers works better using strides > 1 of the convolutional layers instead of max-pooling. This can be explained by our need for precision, while max-pooling is known to lead to shift invariance."}, {"heading": "5.4. Results and discussion", "text": "Localization results on the restricted Maurdor test set, for our proposed method and baselines, are shown with the IoU metric in Table 2 and with the DetEval metric in Table 3. Text recognition results (on text objects localized by our method) are shown in table 4, respectively for pages fully in French and fully in English of the whole Maurdor test set.\nFor the IoU metric, results are given in Table 2. We re-\nport F-Measure for different thresholds on IoU, i.e. for different localization quality requirements. The image-based techniques Shi et al. [28] and Nicolaou et al. [19] perform poorly when the threshold is low, i.e. when we are interested in the ability of the system to detect all the boxes regardless of the exact location. They suffer of low general recall. However, they are relatively precise. F-Measure drops less than the learning based methods when the precision requirements are increased by increasing the threshold on IoU. This can be explained by the nature of these algorithms, which proceed by binarisation of the input images. In the normal operating range of these algorithm, when the segmentation steps work out well, precision is almost guaranteed to be high. However, once images don\u2019t fall into the situations the algorithms has been tuned for, performance\nbreaks down. On the other hand, methods based on direct regression as MultiBox [6] and our proposed method are more robust and achieve better general recall, an advantage which is bought with a slight drop in precision. Our proposed method gives the best results for realistic thresholds. The detEval metric results shown in Table 3 confirm the results from the intersection over union metric.\nFrom the application perspective, namely the full page text recognition in documents, Table 4 shows that the proposed method delivers good results with over 70% FMeasure Bag of Word score on both French and English, outperforming all other methods. This can be explained by its high recall, while the slightly better precision of image based methods is not an advantage since a recognizer can compensate for it, up to a certain limit.\nThe last two lines of Tables 2, 3 and 4 illustrate the importance of adding 2D-LSTM layers to recover information as it significantly improves performances for all the metrics. The power of the 2D-LSTM layers can also be shown in Figure 5, which gives some example detections. Figures 5a and 5b show that the model is capable of detecting objects which are larger than the receptive fields of the individual bounding box predictors. This is made possible through the context information gathered by the LSTM layers. Figures 5a and 5d show that the system is capable of detecting and locating a large number of small objects.\nMultibox [6] is significantly outperformed by our method, even if we optimize its hyper-parameters (on the validation set). We attribute this to the fact, that the output layers are not shared. The model needs to express similar prediction behavior for each output, thus relearn the same strategies several times.\nYOLO [26] proved to be impossible to apply to this kind of problem, at least in its current shape. As reported by the authors, the system gives excellent results on the tasks it has been designed for. However, despite extensive tuning of its hyperparameters, we were not able to reach satisfying results, although we worked with two different implementations: the original implementation of the authors, as well\nas our own implementation. We did identify the problem, however. YOLO has been designed for a small number of objects, with a predictor/target matching algorithm adapted to these settings (see also section 4). As mentioned, only one target can be associated to each spatial cell, which is a harmless restriction for traditional object detection tasks. However, this is a real problem in our case, as shown in the example image in Figure 4. A large part of the ground truth objects in most figures will not be assigned to any predictor, and not trained for. Not only are these boxes missing at training, network outputs predicting their locations will be punished at the next parameter update, further hurting performance and hindering the networks from converging properly."}, {"heading": "5.5. Implementation", "text": "No deep learning framework was used for the implementation of the proposed method, since, until recently and the Theano version from Doetsch et al. [5], no 2D-LSTMs implementation was, up to our knowledge, yet existing in Tensorflow, Torch, Theano or Caffe. The system has been implemented using our inhouse framework implemented in C++, including the SG optimizer, dropout etc. For this reason also, the model has been trained on CPUs.\nFor YOLO we used two different implementations. We implemented and trained our own implementation in Tensorflow, and we also used the official source code published by the authors 2."}, {"heading": "6. Conclusion", "text": "We presented a new fully-convolutional model for the detection and localization of a potentially large number of objects in images. To optimize invariance and in order to limit the number of trainable parameters, we shared parameters of the output layer over spatial blocks of the image, implementing the output layer as 1\u00d71 convolution. To deal with objects which are larger than the receptive field, and in order\n2http://pjreddie.com/darknet/yolo\nto allow the model to collect features from the global context, we added 2D-LSTM layers between the convolutional layers.\nWe compared the proposed model to the state of the art in object detection, in particular to YOLO [26] and Multibox [6]. We measured detection performance and word recognition performance of a subsequent classifier. Our experiments showed, that the proposed model significantly outperforms both methods, even if their hyper-parameters are optimized for the targeted configurations."}], "references": [{"title": "Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks", "author": ["S. Bell", "L. Zitnick", "K. Bala", "R. Girshick"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Joint line segmentation and transcription for endto-end handwritten paragraph recognition", "author": ["T. Bluche"], "venue": "In Advances in Neural Information Processing System,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Automatic line segmentation and ground-truth alignment of handwritten documents", "author": ["T. Bluche", "B. Moysset", "C. Kermorvant"], "venue": "In Int. Conf. on Frontiers in Handwriting Recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "The maurdor project - improving automatic processing of digital documents", "author": ["S. Brunessaux", "P. Giroux", "B. Grilheres", "M. Manta", "M. Bodin", "K. Choukri", "O. Galibert", "J. Kahn"], "venue": "In Document Analysis Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Returnn: The rwth extensible training framework for universal recurrent neural networks", "author": ["P. Doetsch", "A. Zeyer", "P. Voigtlaender", "I. Kulikov", "R. Schl\u00fcter", "H. Ney"], "venue": "arXiv preprint arXiv:1608.00895,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Scalable object detection using deep neural networks", "author": ["D. Erhan", "C. Szegedy", "A. Toshev", "D. Anguelov"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Object detection with discriminatively trained partbased models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Fast R-CNN", "author": ["R. Girshick"], "venue": "In Int. Conf. on Computer Vision,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fernandez", "F. Gomez", "J. Schmidhuber"], "venue": "In Int. Conf. on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["A. Graves", "J. Schmidhuber"], "venue": "In Advances in Neural Information Processing System,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["A. Graves", "J. Schmidhuber"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Synthetic data for text localisation in natural images", "author": ["A. Gupta", "A. Vedaldi", "A. Zisserman"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Squeezenet: Alexnet-level accuracy with 50\u00d7 fewer parameters and <0.5MB model size", "author": ["F. Iandola", "S. Hand", "M. Moskewicz", "K. Ashraf"], "venue": "In Openreview submission to ICLR 2017,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Ssd: Single shot multibox detector", "author": ["W. Liu", "D. Anguelov", "D. Erhan", "C. Szegedy", "S. Reed", "C.-Y. Fu", "A. Berg"], "venue": "In European Conference on Computer Vision, 2016", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D. Lowe"], "venue": "Int. Journal of Computer Visioon,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Algorithms for the assignment and transportation problems", "author": ["J. Munkres"], "venue": "Journal of the Society for Industrial and Applied Mathematics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1957}, {"title": "Handwritten Text Line Segmentation by Shredding Text into its Lines", "author": ["A. Nicolaou", "B. Gatos"], "venue": "In Int. Conf. on Document Analysis and Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["V. Pham", "T. Bluche", "C. Kermorvant", "J. Louradour"], "venue": "In Int. Conf. on Frontiers in Handwriting Recognition,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Learning to refine object segments", "author": ["P. Pinheiro", "T. Lin", "R. Collobert", "P. Dollar"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "From image-level to pixel-level labeling with convolutional networks", "author": ["P. Pinheiro", "R. Collobert"], "venue": "In CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Learning to segment object candidates", "author": ["P. Pinheiro", "R. Collobert", "P. Dollar"], "venue": "In Advances in Neural Information Processing System,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Europeana newspapers ocr workflow evaluation", "author": ["S. Pletschacher", "C. Clausner", "A. Antonacopoulos"], "venue": "In Workshop on Historical Document Imaging and Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Label-free supervision of neural networks with physics and domain knowledge", "author": ["S.E.R. Stewart"], "venue": "Pre-print: arXiv:1609.05566,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "You only look once: Unified, real-time object detection", "author": ["J. Redmon", "S. Divvala", "R. Girshick", "A. Farhadi"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition, June 2016", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Faster R-CNN: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "In Advances in Neural Information Processing System,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "A Steerable Directional Local Profile Technique for Extraction of Handwritten Arabic Text Lines", "author": ["Z. Shi", "S. Setlur", "V. Govindaraju"], "venue": "In Int. Conf. on Document Analysis and Recognition,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Scalable, high-quality object detection", "author": ["C. Szegedy", "S. Reed", "D. Erhan", "D. Anguelov"], "venue": "Pre-print: arXiv:1412.1441,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Matching networks for one shot learning", "author": ["O. Vinyals", "C. Blundell", "K. Kavukcuoglu", "T. Lillicrap", "D. Wierstra"], "venue": "arXiv preprint arXiv:1606.0408,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Object count/Area Graphs for the Evaluation of Object Detection and Segmentation Algorithms", "author": ["C. Wolf", "J.-M. Jolion"], "venue": "Int. Journ. on Document Analysis and Recognition,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Scene labeling with lstm recurrent neural networks", "author": ["W. Wonmin", "T. Breuel", "F. Raue", "M. Liwicki"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "In Int. Conf. on Learning Representations,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Multi-oriented text detection with fully convolutional networks. preprint arXiv:1604.04018, 2016", "author": ["Z. Zhang", "C. Zhang", "W. Shen", "C. Yao", "W. Liu", "X. Bai"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "Object detection and localization in images is currently dominated by approaches which first create proposals (hypothesis bounding boxes) followed by feature extraction and pooling on these boxes and classification, the latter steps being usually performed by deep networks [9, 8, 27, 26, 1].", "startOffset": 274, "endOffset": 291}, {"referenceID": 7, "context": "Object detection and localization in images is currently dominated by approaches which first create proposals (hypothesis bounding boxes) followed by feature extraction and pooling on these boxes and classification, the latter steps being usually performed by deep networks [9, 8, 27, 26, 1].", "startOffset": 274, "endOffset": 291}, {"referenceID": 26, "context": "Object detection and localization in images is currently dominated by approaches which first create proposals (hypothesis bounding boxes) followed by feature extraction and pooling on these boxes and classification, the latter steps being usually performed by deep networks [9, 8, 27, 26, 1].", "startOffset": 274, "endOffset": 291}, {"referenceID": 25, "context": "Object detection and localization in images is currently dominated by approaches which first create proposals (hypothesis bounding boxes) followed by feature extraction and pooling on these boxes and classification, the latter steps being usually performed by deep networks [9, 8, 27, 26, 1].", "startOffset": 274, "endOffset": 291}, {"referenceID": 0, "context": "Object detection and localization in images is currently dominated by approaches which first create proposals (hypothesis bounding boxes) followed by feature extraction and pooling on these boxes and classification, the latter steps being usually performed by deep networks [9, 8, 27, 26, 1].", "startOffset": 274, "endOffset": 291}, {"referenceID": 26, "context": "Very recent methods also use deep networks for the proposal step [27, 26, 15], sometimes sharing features between localization and classification.", "startOffset": 65, "endOffset": 77}, {"referenceID": 25, "context": "Very recent methods also use deep networks for the proposal step [27, 26, 15], sometimes sharing features between localization and classification.", "startOffset": 65, "endOffset": 77}, {"referenceID": 14, "context": "Very recent methods also use deep networks for the proposal step [27, 26, 15], sometimes sharing features between localization and classification.", "startOffset": 65, "endOffset": 77}, {"referenceID": 8, "context": "R-CNN [9]), leaving the burden of validation to a subsequent classifier.", "startOffset": 6, "endOffset": 9}, {"referenceID": 0, "context": "\u2022 we allow for the detection and localization of a relatively high number of potentially small objects in an image, which is especially hard for existing methods [1].", "startOffset": 162, "endOffset": 165}, {"referenceID": 5, "context": "(a) Multibox [6]; (b) Yolo [26] and SSD [15]; these three models pass through a fully connected layer, which connects to a set of bounding box outputs; architecture wise, these three models are identical.", "startOffset": 13, "endOffset": 16}, {"referenceID": 25, "context": "(a) Multibox [6]; (b) Yolo [26] and SSD [15]; these three models pass through a fully connected layer, which connects to a set of bounding box outputs; architecture wise, these three models are identical.", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "(a) Multibox [6]; (b) Yolo [26] and SSD [15]; these three models pass through a fully connected layer, which connects to a set of bounding box outputs; architecture wise, these three models are identical.", "startOffset": 40, "endOffset": 44}, {"referenceID": 25, "context": "Similar to models like YOLO [26] and Single-Shot Detector [15], our outputs are assigned to local regions of the image.", "startOffset": 28, "endOffset": 32}, {"referenceID": 14, "context": "Similar to models like YOLO [26] and Single-Shot Detector [15], our outputs are assigned to local regions of the image.", "startOffset": 58, "endOffset": 62}, {"referenceID": 16, "context": "Earlier (pre-deep learning) work on object recognition proceeded through matching of local features [17] or by decomposing objects into mixtures of parts and solving combinatorial problems [7].", "startOffset": 100, "endOffset": 104}, {"referenceID": 6, "context": "Earlier (pre-deep learning) work on object recognition proceeded through matching of local features [17] or by decomposing objects into mixtures of parts and solving combinatorial problems [7].", "startOffset": 189, "endOffset": 192}, {"referenceID": 8, "context": "aspect ratios, R-CNN [9] introduced the concept object proposals, created by separate methods, followed by convolutional networks to classify each proposal.", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "The concept was improved as Fast R-CNN [8] and Faster R-CNN [27].", "startOffset": 39, "endOffset": 42}, {"referenceID": 26, "context": "The concept was improved as Fast R-CNN [8] and Faster R-CNN [27].", "startOffset": 60, "endOffset": 64}, {"referenceID": 5, "context": "proposed Multibox [6, 29], which performs direct regression of bounding box locations instead of relying on object proposals.", "startOffset": 18, "endOffset": 25}, {"referenceID": 28, "context": "proposed Multibox [6, 29], which performs direct regression of bounding box locations instead of relying on object proposals.", "startOffset": 18, "endOffset": 25}, {"referenceID": 25, "context": "YOLO [26] and the Single-Shot Detector [15] can be seen as variants of this concept, they will be discussed in more detail in section 2.", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "YOLO [26] and the Single-Shot Detector [15] can be seen as variants of this concept, they will be discussed in more detail in section 2.", "startOffset": 39, "endOffset": 43}, {"referenceID": 22, "context": "Some recent work strives to detect and localize objects with pixel-wise precision, which somewhat blurs the boundaries between object detection and semantic segmentation [23, 21].", "startOffset": 170, "endOffset": 178}, {"referenceID": 20, "context": "Some recent work strives to detect and localize objects with pixel-wise precision, which somewhat blurs the boundaries between object detection and semantic segmentation [23, 21].", "startOffset": 170, "endOffset": 178}, {"referenceID": 21, "context": "Methods which learn to segment without pixelwise ground truth have also been proposed [22].", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "Context through spatial 2D-recurrent networks has been proposed as early as in [12].", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "Similarly to our method, inside-Outside-Nets [1] contain 2D spatial context layers collecting information from 4 different directions.", "startOffset": 45, "endOffset": 48}, {"referenceID": 31, "context": "Other recent work uses 2D recurrent networks for semantic segmentation [32].", "startOffset": 71, "endOffset": 75}, {"referenceID": 33, "context": "CNNs have been used before for text detection, for instance in [34], a Fully convolutional network (FCN) is used to classify each position of a salient map as text or nontext.", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "In [13], a YOLO-related method is proposed for the detection of text in natural images but only few objects are present in the images.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "The problem of dataset sizes has been addressed before, with strategies reaching from external memories [30] and unsupervised learning, for instance by learning feature extraction from physics [25].", "startOffset": 104, "endOffset": 108}, {"referenceID": 24, "context": "The problem of dataset sizes has been addressed before, with strategies reaching from external memories [30] and unsupervised learning, for instance by learning feature extraction from physics [25].", "startOffset": 193, "endOffset": 197}, {"referenceID": 13, "context": "Choosing when to pool and to reduce can be critical, and optimizations can lead to large decreases in the numbers of trainable parameters [14].", "startOffset": 138, "endOffset": 142}, {"referenceID": 32, "context": "An alternative to in-betweenlayer pooling is changing the size of filters, especially as \u201ca trou\u201d computation in order to keep the number of parameters low [33].", "startOffset": 156, "endOffset": 160}, {"referenceID": 15, "context": "1\u00d71, with a high feature dimension (1\u00d71\u00d74096 in the network for semantic segmentation proposed in [16]).", "startOffset": 98, "endOffset": 102}, {"referenceID": 5, "context": "Several state of the art models, Multibox [6], YOLO [26] and Single-Shot Detector (SSD) [15] tackle this through an architecture sketched in figure 2a and 2b1.", "startOffset": 42, "endOffset": 45}, {"referenceID": 25, "context": "Several state of the art models, Multibox [6], YOLO [26] and Single-Shot Detector (SSD) [15] tackle this through an architecture sketched in figure 2a and 2b1.", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "Several state of the art models, Multibox [6], YOLO [26] and Single-Shot Detector (SSD) [15] tackle this through an architecture sketched in figure 2a and 2b1.", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "In contrast to [1], we use real LSTM models with trainable transition matrices.", "startOffset": 15, "endOffset": 18}, {"referenceID": 25, "context": "(7\u00d77 for [26], 9\u00d79 for [15]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "(7\u00d77 for [26], 9\u00d79 for [15]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "Although there is no principled difference in how the last fully connected layer is actually implemented in the three models, we display the output layer differently for Multibox [6] (figure 2a) and for YOLO [26] or SSD [15] (figure 2b).", "startOffset": 179, "endOffset": 182}, {"referenceID": 25, "context": "Although there is no principled difference in how the last fully connected layer is actually implemented in the three models, we display the output layer differently for Multibox [6] (figure 2a) and for YOLO [26] or SSD [15] (figure 2b).", "startOffset": 208, "endOffset": 212}, {"referenceID": 14, "context": "Although there is no principled difference in how the last fully connected layer is actually implemented in the three models, we display the output layer differently for Multibox [6] (figure 2a) and for YOLO [26] or SSD [15] (figure 2b).", "startOffset": 220, "endOffset": 224}, {"referenceID": 25, "context": "For the latter two, and also in accordance with the figures of the respective papers, the outputs are shown as a spatial grid (7\u00d77 for [26], 9\u00d79 for [15]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 14, "context": "For the latter two, and also in accordance with the figures of the respective papers, the outputs are shown as a spatial grid (7\u00d77 for [26], 9\u00d79 for [15]).", "startOffset": 149, "endOffset": 153}, {"referenceID": 25, "context": "In the case of [26], this assignment is purely spatial: outputs of a given cell are trained to provide predictions of a spatial region corresponding to this cell (see section 4).", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "For SSD[15] , we do not show the way how this model handles multiple scales.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "We address both these concerns through context layers consisting of Multi-Dimensional Long-Short term memory models [11], which are inserted between the convolutional layers.", "startOffset": 116, "endOffset": 120}, {"referenceID": 5, "context": "Similar to the strategy in MultiBox [6], this is done globally over the entire image, which allows each bounding box predictor to respond to any location in an image.", "startOffset": 36, "endOffset": 39}, {"referenceID": 17, "context": "This is a well known bi-partite graph matching problem, which can be solved with the Hungarian algorithm [18].", "startOffset": 105, "endOffset": 109}, {"referenceID": 5, "context": "As mentioned earlier, our matching strategy is similar to the one described in MultiBox [6] and has the same global property brought by the confidence term (albeit applied to local outputs, compared to the global outputs in [6]).", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "As mentioned earlier, our matching strategy is similar to the one described in MultiBox [6] and has the same global property brought by the confidence term (albeit applied to local outputs, compared to the global outputs in [6]).", "startOffset": 224, "endOffset": 227}, {"referenceID": 14, "context": "On the other hand, in SSD[15] and YOLO[26], matching is done locally, i.", "startOffset": 25, "endOffset": 29}, {"referenceID": 25, "context": "On the other hand, in SSD[15] and YOLO[26], matching is done locally, i.", "startOffset": 38, "endOffset": 42}, {"referenceID": 14, "context": "In SSD and MultiBox, the matching process is restricted to a fixed dictionary of anchor locations obtained arbitrarily[15] or with clustering[6], which helps the network to create outputs specialized to regions in the image.", "startOffset": 118, "endOffset": 122}, {"referenceID": 5, "context": "In SSD and MultiBox, the matching process is restricted to a fixed dictionary of anchor locations obtained arbitrarily[15] or with clustering[6], which helps the network to create outputs specialized to regions in the image.", "startOffset": 141, "endOffset": 144}, {"referenceID": 3, "context": "We tested the proposed model and the baselines on the publicly available Maurdor dataset [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "For this reason, we use the technique detailed in [3] to get annotation", "startOffset": 50, "endOffset": 53}, {"referenceID": 30, "context": "DetEval \u2014 DetEval [31] is the metric chosen for the ICDAR robust reading series of competitions.", "startOffset": 18, "endOffset": 22}, {"referenceID": 23, "context": "Bag-of-words recognition error \u2014 BoW is a goal oriented method described in [24], which measures the performance of a subsequent text recognition module.", "startOffset": 76, "endOffset": 80}, {"referenceID": 19, "context": "We use the recognition model from [20], which is based on deep convolutional networks and spatial/sequence modelling with 2D-LSTM layers.", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "Assigning character labels to network outputs is performed with the Connectionist Temporal Classification framework [10].", "startOffset": 116, "endOffset": 120}, {"referenceID": 1, "context": "Recent follow-up work solves this problem with attention based mechanisms [2], this will be investigated in future work.", "startOffset": 74, "endOffset": 77}, {"referenceID": 27, "context": "[28] use steerable directional filters to create an adaptive local connectivity map.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] follows the whitest and blackest paths in a blurred image to find lines and interlines.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "To be able to predict the same number of objects, with the same number of input features, MultiBox[6] and Yolo[26] would have needed 15,688,200 parameters.", "startOffset": 98, "endOffset": 101}, {"referenceID": 25, "context": "To be able to predict the same number of objects, with the same number of input features, MultiBox[6] and Yolo[26] would have needed 15,688,200 parameters.", "startOffset": 110, "endOffset": 114}, {"referenceID": 27, "context": "[28] 40.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] 36.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Multibox [6] 11.", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "2% Multibox [6] (optimized) 48.", "startOffset": 12, "endOffset": 15}, {"referenceID": 30, "context": "Detection performance with detEval[31].", "startOffset": 34, "endOffset": 38}, {"referenceID": 27, "context": "[28] 35.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] 46.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Multibox [6] 4.", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "0% Multibox [6] (optimized) 28.", "startOffset": 12, "endOffset": 15}, {"referenceID": 27, "context": "[28] 48.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] 65.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Multibox [6] 27.", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "8% Multibox [6] (optimized) 32.", "startOffset": 12, "endOffset": 15}, {"referenceID": 27, "context": "[28] and Nicolaou et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] perform poorly when the threshold is low, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "On the other hand, methods based on direct regression as MultiBox [6] and our proposed method are more robust and achieve better general recall, an advantage which is bought with a slight drop in precision.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "Multibox [6] is significantly outperformed by our method, even if we optimize its hyper-parameters (on the validation set).", "startOffset": 9, "endOffset": 12}, {"referenceID": 25, "context": "YOLO [26] proved to be impossible to apply to this kind of problem, at least in its current shape.", "startOffset": 5, "endOffset": 9}, {"referenceID": 4, "context": "[5], no 2D-LSTMs implementation was, up to our knowledge, yet existing in Tensorflow, Torch, Theano or Caffe.", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "We compared the proposed model to the state of the art in object detection, in particular to YOLO [26] and Multibox [6].", "startOffset": 98, "endOffset": 102}, {"referenceID": 5, "context": "We compared the proposed model to the state of the art in object detection, in particular to YOLO [26] and Multibox [6].", "startOffset": 116, "endOffset": 119}], "year": 2016, "abstractText": "The current trend in object detection and localization is to learn predictions with high capacity deep neural networks trained on a very large amount of annotated data and using a high amount of processing power. In this work, we propose a new neural model which directly predicts bounding box coordinates. The particularity of our contribution lies in the local computations of predictions with a new form of local parameter sharing which keeps the overall amount of trainable parameters low. Key components of the model are spatial 2D-LSTM recurrent layers which convey contextual information between the regions of the image. We show that this model is more powerful than the state of the art in applications where training data is not as abundant as in the classical configuration of natural images and Imagenet/Pascal VOC tasks. We particularly target the detection of text in document images, but our method is not limited to this setting. The proposed model also facilitates the detection of many objects in a single image and can deal with inputs of variable sizes without resizing.", "creator": "LaTeX with hyperref package"}}}