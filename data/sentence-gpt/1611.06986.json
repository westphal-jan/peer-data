{"id": "1611.06986", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Robust end-to-end deep audiovisual speech recognition", "abstract": "Speech is one of the most effective ways of communication among humans. Even though audio is the most common way of transmitting speech, very important information can be found in other modalities, such as vision. Vision is particularly useful when the acoustic signal is corrupted. Multi-modal speech recognition however has not yet found wide-spread use, mostly because the temporal alignment and fusion of the different information sources is challenging.\n\n\n\n\"The way you use the information you need in your voice and the other person is not as easy to control as it is for many speakers. The fact that they can not access information without a strong voice is one of the most challenging challenges that has ever been addressed by a human ear.\"\nCultural and Personal Differences\nIn many cultures, people seem to have a different set of beliefs regarding what they think of their speech, but this is just a small example of this. Humans seem to have different beliefs about what they believe, so that people who believe their speech are wrong may choose to ignore that information. However, for many people, this may not be an issue when they have no interest in the truth and are not interested in making sure that the information they think they get from the source is not actually what they actually think.\nA similar problem is being overlooked in some cultures as well, as many religions, and is most often found in very different cultures, like Indonesia and Malaysia.\nHowever, in some cultures, there are not enough resources to meet all those different beliefs, and if they choose to ignore their opinions, they might become stuck.\nIn this case, there is a difference between the two beliefs, which can be easily found in many different cultures.\n\"The way you use the information you need in your voice and the other person is not as easy to control as it is for many speakers. The fact that they can not access information without a strong voice is one of the most challenging challenges that has ever been addressed by a human ear.\"\nThere are also a number of other ways of communicating with the listener, especially if the speaker is a native Indonesian.\nIn fact, when they get into an argument with a different person, some people may decide to ignore that information and become stuck. This could affect their decision to stop listening to the conversation, or to stop their conversation.\nIf the speaker is not an Indonesian, and the conversation has been interrupted, the communication between the speaker and the listener may get lost or lost. In this case, the communication between the speaker and", "histories": [["v1", "Mon, 21 Nov 2016 20:08:51 GMT  (7229kb)", "http://arxiv.org/abs/1611.06986v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["ramon sanabria", "florian metze", "fernando de la torre"], "accepted": false, "id": "1611.06986"}, "pdf": {"name": "1611.06986.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ramon Sanabria", "Florian Metze", "Fernando De La Torre"], "emails": ["ramons@andrew.cmu.edu", "fmetze@andrew.cmu.edu", "ftorre@andrew.cmu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n06 98\n6v 1\n[ cs\n.C L\n] 2\n1 N\nov 2\nThis paper presents an end-to-end audiovisual speech recognizer (AVSR), based on recurrent neural networks (RNN) with a connectionist temporal classification (CTC) [1] loss function. CTC creates sparse \u201cpeaky\u201d output activations, and we analyze the differences in the alignments of output targets (phonemes or visemes) between audio-only, video-only, and audio-visual feature representations. We present the first such experiments on the large vocabulary IBM ViaVoice database, which outperform previously published approaches on phone accuracy in clean and noisy conditions.\nIndex Terms\u2014 audiovisual speech recognition, recurrent neural networks, connectionist temporal classification"}, {"heading": "1. INTRODUCTION", "text": "Although researchers have been trying to improve the performance of automatic speech recognition systems under noisy conditions for decades, the problem is far from being solved. Some solutions are focused on removing the noise from the signal or improving the feature representation of the audio channel. However, the amount of signal masked by additive noise presents a natural limitation to those solutions. For that reason, some researchers use an alternative nonrelated with audio modality, for example the visual channel. Vision is usually not affected by the acoustic environment, and thus immune to corruption by noise. It has been demonstrated by [2] that humans also tend to put their attention to other information channels (i.e., vision) to ease the understanding of the speaker when the acoustic channel is corrupted. The addition of a new modality however into a temporal sequence classification problem creates several challenges. In addition to potentially having to estimate stream weights, the temporal alignment of both information sources is usually not constant. This is due to the nature of the speech production process, as well as the complexity of the technical solutions for transmission, storage, and encoding of audio-visual data. In this paper, we present a novel approach to audio-visual speech recognition, which will allow us to investigate these effects in new and interesting ways.\nThe authors would like to thank Susanne Burger for sharing her insights into the role of phonetics and phonology in an audio-visual setting.\nSpecifically, it is shown by different linguistic studies such as [3] that the mouth shape towards an articulatory target modify the following phone. This effect is accentuated when the speaking rate of the speaker is high [4]. In those cases, the visual modality will provide enough information to the system to determine how the phoneme should sound. However, a problem of synchronization between visemes (groups of similar movements of visual articulators) and phonemes is present. Some practical studies such as [5] state that the coarticulation is speaker- and phoneme-dependent. This adds a certain level of difficulty to the synchronization task between the audio and video modality, and makes frame-level fusion difficult. Consequently, somewhat a-synchronous approaches such as Hidden Markov models (HMMs) coupled at the state level have been attempted, but have also not met with dramatic success.\nWe present an AVSR solution that does not require an HMM, but rather uses several layers of bi-directional long short-term memory (LSTM) [6] units as building blocks, followed by a CTC loss function for the output layer. The CTC loss is defined directly over the symbol sequence, and effectively marginalizes over all permitted alignments between frames and states, adding a \u201cblank\u201d state between label states. The resulting alignment typically contains mostly blanks, and is thus \u201csparse\u201d, see Fig. 1. In a multi-stream setting, where independent models are being trained and tested for the audio and video modality, frame-synchronous approaches such as tightly coupled HMMs together with score fusion (late integration) are therefore unlikely to work, because the \u201cpeaks\u201d for the same unit will appear at a different point in time in each stream. Early integration (feature fusion) however should work just fine, and will thus be investigated in this paper.\nAs an end-to-end approach, CTC directly optimizes for the sequence of output labels without requiring any initial labeling (man-\nual or ported from another system). However it is not clear if the temporal locations of the peaks have any meaningful interpretation. The audio-visual setting provides an interesting opportunity to investigate this issue: intuitively, the locations of the peaks should correspond to \u201cdiscriminative\u201d input features, and should thus mark \u201cinformative\u201d time points. In an audio-visual setting, these may correspond to the different times at which phonemes and their corresponding visemes (we follow [7] for the mapping) are observed, without requiring any manual labeling or input.\nThis paper thus makes two main contributions: first, we demonstrate that CTC-based acoustic models can achieve state-of-the-art performance in audio-visual speech recognition tasks, as our system achieve comparable results with a traditional pre-Deep Learning baseline ( [8] report 11% Word Error Rate on the ViaVoice database) and outperforms recent cross-entropy trained DNN baseline [9] in terms of phoneme error rate. We report results for clean and noisy training and testing conditions. Our second contribution lies in an analysis of the peak structure of the CTC output labeling for a multimodal input, which we can compare to human intuition about the nature and relationship between the feature generation process."}, {"heading": "2. TECHNICAL BACKGROUND", "text": ""}, {"heading": "2.1. Architecture of AVSR systems", "text": "Traditionally, HMMs and Gaussian Mixture Models (GMMs) have been applied as main learning structure for AVSR systems. HMMs normalize the time axis of the input sequences, and GMMs model the emission probability of each state of the HMM. Two ways of fusing both modalities are used in traditional AVSR: first, early combination (feature fusion) of both feature vectors can be applied [10, 11]. In some cases algorithms such as Principal Component Analysis (PCA) or Linear Discrimination Analysis (LDA) are used to reduce the dimensionality of those representations. This approach may lead to frame synchronization problems [10, 12, 11]. Second, score combination (late fusion) is performed in order to avoid such problems, and even allow for asynchrony in the state sequences in the two streams. In [13] for example both modalities are analyzed separately and later on the results of both are fused using a bias.\nIn [14, 15, 9] present different recent deep-learning approaches to solve the AVSR problem. In [14] and [9], a joined (audio and video) representations using Deep Neural Networks (DNNs) is learned to perform word and phone recognition respectively. However, no temporal dependence, which is an inherent property of audiovisual speech recognition, is considered. More recently, [15] present a Recurrent Temporal Multimodal Restricted Boltzmann Machine (RTMRB), which takes into consideration long-term dependencies and outperforms other non-temporal solutions. All approaches explicitly align states with the data and care must be taken in aligning data and setting up experiments."}, {"heading": "2.2. Audio and Video Feature Representation", "text": "Several phonetic studies tried to understand which are the most relevant features that can be extracted from the face in order to perform audiovisual speech recognition. According to [12], lip position is a considerable source of information when performing visual-only speech recognition. In addition to the position of the lips, [12] state that teeth visibility eases the process of guessing the sound that was produced. Moreover, [16, 17, 18] conduct experiments where it is shown that the entire face provides information about speech.\nTraditionally, researchers use different processing and feature extraction methods in order to represent the features explained above. All of them are based on extracting Regions-Of-Interest (ROIs) of each frame where the mouth and other parts of the face (e.g., jaw) are located. Different techniques are used to parametrize the ROIs such as using grey-scale value of each pixel, extracting the variation of the values of each pixel between frames, or parametrize each part of the face using a specific statistical model. As we will discuss in Section 4, those techniques have problems since they provide neither a rotation nor a light-invariant feature representation of the area described.\nIn the field of deep learning, [19] proposed a MSHMM infrastructure, which uses features extracted from a Convolutial Neural Network (CNN). In addition, as we explain in Section 2.1, [14, 9, 15] learn a joint feature representation using different DNNs approaches. All those solutions, in turn, are trained on pairs of raw images and the corresponding phoneme labels, which may be unreliable, because of the inherent potential for asynchrony between audio and video (both due to the speech production and due to the technical processes when handling audio-visual content)."}, {"heading": "3. ARCHITECTURE OF OUR SYSTEM", "text": "We use the Eesen framework [20]. The Acoustic Model (AM) is composed of multiple stacked LSTM Networks, and uses CTC as loss-function. This set-up allows to our system to automatically align the sequence of vector representations and the phoneme sequences. It is important to note that the system will output the additional CTC symbol \u201cblank\u201d most of the time.\nIn place of the HMM, a series of three Weighted Finite State Transducers (WFSTs) is used to model the sequence of a symbol and blank states that make up a token (phoneme or a viseme), then the words, and the Language Model (LM) during decoding.\nIn our pipeline, four layers of RNNs are connected to build our AM. To provide the ability of learning more complex time sequences we use bidirectional LSTM units [6] for our RNN. K possible labels (45 phonemes or 12 visemes in our case) plus a blank label that is added in the position 0 compose all the possible output symbols of the network. Let X = (x1, ..., xT ) and z = (z1, ..., zU ) with U \u2264 T be the utterance (audio, video or audiovisual features) and their corresponding label sequence (phonemes), respectively. Thereby, each xi is a feature vector (audio, visual or audiovisual) and zu \u2208 {1, 2...K} . The CTC loss function aims to maximize the expression P (z|X) by optimizing the parameters of the RNN. Since the output of the RNN will be a probability distribution over all possible labels, the last layer of the network is a softmax layer with K + 1 units (original number of symbols plus blank).\nWe assume that the probabilities for each time frame are i.i.d. Let yt be the output probability vector computed for each time frame and p = (p1, .., pT ), be a possible output sequence, where pt \u2208 {1, 2...K + 1}. Then the total probability of each possible output sequence of the labels can be computed as:\nP (p|X) = T\u220f\nt=1\ny pt t (1)\nWe denote all possible p that can be mapped to a z as \u03a6(z). Therefore, the likelihood of z given an input sequence X can be described as follows:\nP (z|X) = \u2211\np\u2208(\u03a6(z))\nP (p|X) (2)\nThis is the loss function that our RNN aims to maximize. To allow blanks symbols in \u03a6(z), we add them at the beginning, the end, and between each symbol of z. Consequently, a modified label sequence of length 2U + 1 is to be used to compute P (p|X). To do so, the well-known forward-backward algorithm is used. It computes the probabilities of every past path that ends with a label u at a concrete time t as \u03b1t \u2212 1u, and the probability of all possible paths that start with label u at time t to the end as \u03b2ut . Then, the total likelihood of a sequence z given X is computed as:\nP (p|X) = 2U+1\u2211\nu=1\n\u03b1 u t \u03b2 u t (3)\nThis is differentiable and can thus be used as objective function."}, {"heading": "4. DATA AND FEATURES", "text": "The IBM ViaVoice [11] data set is used to test and train the proposed pipelines. The data set consists of 17111 utterances, which are spoken by 26 different speakers looking directly to the camera with an estimated Signal-to-Noise Ratio of 19.5dB in the audio channel. The data was initially split in 17111 utterances of 261 speakers for training (about 34.9 h) and 1893 of 26 speakers utterance (4.6 hours) for testing. However, we were only able to use 15963 utterances for training and 1840 utterances for testing (see Section 2.2). We perform a data augmentation adding white Gaussian noise at 10 different levels of to the original audio signal, which actually have an initial 19.5dB SNR (office noise), creating thus different SNR (40dB to 20dB).\nFBank features (40 dimensions), FBank + pitch features (43 dimensions) and Mel Cepstral Coefficients (12 dimensions) are used as audio features in our experiments. Cepstral mean and variance normalization is conducted for robustness, and plus/minus one frame is stacked at the inputs of the neural network.\n18 coordinate points that define the inner and outer profile of the mouth shape are extracted using IntraFace [21]. Afterwards, a position normalization is applied doing an affine transformation to the fixed points (e.g., eye corners) of an average face. Then, a translation to the center of coordinates is performed to the lips contour (inner and outer). Finally, the acceleration and speed of each mouth point is computed. All features described are concatenated to form the visual representation (72 dimensions) of each frame.\nA richer representation of the visual modality is achieved describing the mouth landmark points using a scale invariant local description (SIFT) [22]. The original vector of the SIFT descriptors of all the mouth landmarks points has 2304 dimensions. However, the dimensionality of that feature representation is reduced applying a PCA decreasing the number of feature dimensions to 222 (98% of variance). This information is added to the previous vector to create a more complete representation (294 dimensions) of each frame. IntraFace is not able to process some utterances due to the quality of the data. Therefore, as we already stated in section 4, some utterances are removed."}, {"heading": "5. EXPERIMENTS", "text": "We perform a baseline audio-only recognition experiments with FBank + pitch coefficients and an in-domain language model as used in [8], and achieved a WER of 11.8% in clean conditions using the entire database. In the following experiments a subset of the training and testing set was used, as it is explained in 4, some data\nhad to be removed. Also, in order to reduce the language model bias of this setup in the ViaVoice domain, we decided to switch to a more general n-gram language model based on TED talks [23], which we reduce to the required vocabulary and we use in the following experiments."}, {"heading": "5.1. Audio Results", "text": "All features are tested using a 33ms frame rate. This setup is chosen to be as close to the video frame rate as possible for later fusion experiments. As can be seen in Table 1, FBank + pitch leads to better results. This feature is used in the following multi-modal experiment. It is interesting to note that relatively small differences in phone accuracy translate into bigger differences in testing word error rate.\nFigure 2 shows the phone error rate, Figure 3 the word error rate for different noise conditions during testing, of systems trained with clean data only, as well as all noise conditions found in the test data (multi-condition training). \u201cClean\u201d data is very uniform (the recording condition is identical for all speakers), so that multicondition training does not improve over the baseline in this case. Those results are computed using the reduced dataset explained in section 4. However, experiments show that we achieve a 11.7 % WER using FBank Pitch features with the complete data set."}, {"heading": "5.2. Video Results", "text": "As can be seen in Table 2, different combinations of visual feature representations have been tested using visemes as target units. SIFT\ndescriptors perform particularly well."}, {"heading": "5.3. Audiovisual Results", "text": "Figures 2 and 3 show the benefits of training a model using data augmentation techniques, and the benefit of training a multimodal system on audio and video features.1 \u201cFull fusion\u201d (audio+landmarks+SIFT) models do not perform better than audio+landmarks models at higher SNR, because the high dimensionality of the SIFT feature dominates the audio features, rendering them less useful. We are currently experimenting with further dimensionality reduction techniques to solve that problem.\nFigure 4 shows the peaks with which CTC labels the units in each modality, after off-setting technical delays caused by the codec and other factors, for the audio-only, video-only, and audio-visual case. Several conclusions can be extracted from that figure. First, the video signal always precedes audio signal. This finding supports the coarticualation [3] and anticipatory coarticulation [5] studies of natural speech production, where is stated that speaker changes the mouth shape before pronounce the following phone. Moreover, from\n1Still, because of the homogeneous nature of the data, a model trained specifically for a concrete noise condition will perform better than more general, multi-condition models.\nan AVSR point of view, audio-visual models seem to generate better alignments than uni-modal inputs: in most cases, CTC places the position of each phoneme between the audio-only and the video-only position, correcting mis-alignments. Finally, we performed audiovisual training with +/-330ms offset between feature types, without any significant change in WER or PA. This shows that the \u201cpeaky\u201d structure of CTC is well suited also to multi-modal fusion, and that more detailed studies should be performed in order to investigate the, presumably,speaker- and phoneme-dependent nature of coarticulation [3, 5]."}, {"heading": "6. CONCLUSIONS", "text": "In this paper, we demonstrated that end-to-end Deep Learning can successfully be applied to the problem of audio-visual (multi-modal) speech recognition. Using the CTC loss function and early integration (feature fusion), our system achieves the lowest published word error rate on the large vocabulary IBM ViaVoice database. We show that multi-condition training can be used to improve results on noisy data, and that audio-visual fusion improves results in all conditions, as expected.\nMore interestingly, the multi-modal setting allows us to reason about the inherent meaning of the \u201cpeaky\u201d output structure of CTC models, and investigate how their location corresponds to our intuition about the speech production process.\nReasonable care has been used to tune the models used in these experiments, but further improvements seem possible by exploring more data augmentation strategies, or by further optimizing the CTC training strategy in this relatively low data scenario. Furthermore, neural methods could also be investigated to achieve late fusion."}, {"heading": "7. REFERENCES", "text": "[1] Alex Graves, Santiago Ferna\u0301ndez, Faustino Gomez, and Ju\u0308rgen Schmidhuber, \u201cConnectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369\u2013376.\n[2] Harry McGurk and John MacDonald, \u201cHearing lips and seeing voices,\u201d Nature, vol. 264, pp. 746\u2013748, 1976.\n[3] Raymond D Kent and Fred D Minifie, \u201cCoarticulation in recent speech production models,\u201d Journal of Phonetics, vol. 5, no. 2, pp. 115\u2013133, 1977.\n[4] Sarah Taylor, Barry-John Theobald, and Iain Matthews, \u201cThe effect of speaking rate on audio and visual speech,\u201d in 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 3037\u20133041.\n[5] Fredericka Bell-Berti and Katherine S Harris, \u201cTemporal patterns of coarticulation: Lip rounding,\u201d The Journal of the Acoustical Society of America, vol. 71, no. 2, pp. 449\u2013454, 1982.\n[6] Sepp Hochreiter and Ju\u0308rgen Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[7] Luca Cappelletta and Naomi Harte, \u201cPhoneme-to-viseme mapping for visual speech recognition.,\u201d in ICPRAM (2), 2012, pp. 322\u2013329.\n[8] Gerasimos Potamianos, Chalapathy Neti, Giridharan Iyengar, and Eric Helmuth, \u201cLarge-vocabulary audio-visual speech recognition by machines and humans.,\u201d in INTERSPEECH. Citeseer, 2001, pp. 1027\u20131030.\n[9] Youssef Mroueh, Etienne Marcheret, and Vaibhava Goel, \u201cDeep multimodal learning for audio-visual speech recognition,\u201d in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 2130\u20132134.\n[10] Jan Kratt, Florian Metze, Rainer Stiefelhagen, and Alex Waibel, \u201cLarge vocabulary audio-visual speech recognition using the janus speech recognition toolkit,\u201d in Joint Pattern Recognition Symposium. Springer, 2004, pp. 488\u2013495.\n[11] Chalapathy Neti, Gerasimos Potamianos, Juergen Luettin, Iain Matthews, Herve Glotin, Dimitra Vergyri, June Sison, and Azad Mashari, \u201cAudio visual speech recognition,\u201d Tech. Rep., IDIAP, 2000.\n[12] Iain Matthews, Features for audio-visual speech recognition, Ph.D. thesis, Citeseer, 1998.\n[13] Juergen Luettin, Gerasimos Potamianos, and Chalapathy Neti, \u201cAsynchronous stream modeling for large vocabulary audiovisual speech recognition,\u201d in Acoustics, Speech, and Signal Processing, 2001. Proceedings.(ICASSP\u201901). 2001 IEEE International Conference on. IEEE, 2001, vol. 1, pp. 169\u2013172.\n[14] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng, \u201cMultimodal deep learning,\u201d in Proceedings of the 28th international conference on machine learning (ICML-11), 2011, pp. 689\u2013696.\n[15] Di Hu, Xuelong Li, et al., \u201cTemporal multimodal learning in audiovisual speech recognition,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 3574\u20133582.\n[16] Eric Vatikiotis-Bateson, \u201cThe moving face during speech communication,\u201d Hearing by eye II: Advances in the psychology of speechreading and auditory-visual speech, vol. 2, pp. 123, 1998.\n[17] Eric Vatikiotis-Bateson, Kevin G Munhall, Makoto Hirayama, Y Victor Lee, and Demetri Terzopoulos, \u201cThe dynamics of audiovisual behavior in speech,\u201d in Speechreading by humans and machines, pp. 221\u2013232. Springer, 1996.\n[18] Eric Vatikiotis-Bateson, Kevin G Munhall, Y Kasahara, Frederique Garcia, and Hani Yehia, \u201cCharacterizing audiovisual information during speech.,\u201d in ICSLP, 1996.\n[19] Kuniaki Noda, Yuki Yamaguchi, Kazuhiro Nakadai, Hiroshi G Okuno, and Tetsuya Ogata, \u201cAudio-visual speech recognition using deep learning,\u201d Applied Intelligence, vol. 42, no. 4, pp. 722\u2013737, 2015.\n[20] Yajie Miao, Mohammad Gowayyed, and Florian Metze, \u201cEesen: End-to-end speech recognition using deep rnn models and wfst-based decoding,\u201d in 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 167\u2013174.\n[21] Fernando De la Torre, Wen-Sheng Chu, Xuehan Xiong, Francisco Vicente, Xiaoyu Ding, and Jeffrey Cohn, \u201cIntraface,\u201d in Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on. IEEE, 2015, vol. 1, pp. 1\u20138.\n[22] David G Lowe, \u201cDistinctive image features from scaleinvariant keypoints,\u201d International journal of computer vision, vol. 60, no. 2, pp. 91\u2013110, 2004.\n[23] Will Williams, Niranjani Prasad, David Mrva, Tom Ash, and Tony Robinson, \u201cScaling recurrent neural network language models,\u201d CoRR, vol. abs/1502.00512, 2015.\n[24] J Jeffers and M Barley, \u201cLipreading (speechreading),\u201d Charles C. Thomas, Springfield, IL, p. b10, 1971."}], "references": [{"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369\u2013376.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Hearing lips and seeing voices", "author": ["Harry McGurk", "John MacDonald"], "venue": "Nature, vol. 264, pp. 746\u2013748, 1976.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1976}, {"title": "Coarticulation in recent speech production models", "author": ["Raymond D Kent", "Fred D Minifie"], "venue": "Journal of Phonetics, vol. 5, no. 2, pp. 115\u2013133, 1977.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1977}, {"title": "The effect of speaking rate on audio and visual speech", "author": ["Sarah Taylor", "Barry-John Theobald", "Iain Matthews"], "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 3037\u20133041.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Temporal patterns of coarticulation: Lip rounding", "author": ["Fredericka Bell-Berti", "Katherine S Harris"], "venue": "The Journal of the Acoustical Society of America, vol. 71, no. 2, pp. 449\u2013454, 1982.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1982}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Phoneme-to-viseme mapping for visual speech recognition", "author": ["Luca Cappelletta", "Naomi Harte"], "venue": "ICPRAM (2), 2012, pp. 322\u2013329.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Large-vocabulary audio-visual speech recognition by machines and humans", "author": ["Gerasimos Potamianos", "Chalapathy Neti", "Giridharan Iyengar", "Eric Helmuth"], "venue": "INTERSPEECH. Citeseer, 2001, pp. 1027\u20131030.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Deep multimodal learning for audio-visual speech recognition", "author": ["Youssef Mroueh", "Etienne Marcheret", "Vaibhava Goel"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 2130\u20132134.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Large vocabulary audio-visual speech recognition using the janus speech recognition toolkit", "author": ["Jan Kratt", "Florian Metze", "Rainer Stiefelhagen", "Alex Waibel"], "venue": "Joint Pattern Recognition Symposium. Springer, 2004, pp. 488\u2013495.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Audio visual speech recognition", "author": ["Chalapathy Neti", "Gerasimos Potamianos", "Juergen Luettin", "Iain Matthews", "Herve Glotin", "Dimitra Vergyri", "June Sison", "Azad Mashari"], "venue": "Tech. Rep., IDIAP, 2000.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "Features for audio-visual speech recognition", "author": ["Iain Matthews"], "venue": "Ph.D. thesis, Citeseer,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Asynchronous stream modeling for large vocabulary audiovisual speech recognition", "author": ["Juergen Luettin", "Gerasimos Potamianos", "Chalapathy Neti"], "venue": "Acoustics, Speech, and Signal Processing, 2001. Proceedings.(ICASSP\u201901). 2001 IEEE International Conference on. IEEE, 2001, vol. 1, pp. 169\u2013172.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), 2011, pp. 689\u2013696.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Temporal multimodal learning in audiovisual speech recognition", "author": ["Di Hu", "Xuelong Li"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 3574\u20133582.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "The moving face during speech communication", "author": ["Eric Vatikiotis-Bateson"], "venue": "Hearing by eye II: Advances in the psychology of speechreading and auditory-visual speech, vol. 2, pp. 123, 1998.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "The dynamics of audiovisual behavior in speech", "author": ["Eric Vatikiotis-Bateson", "Kevin G Munhall", "Makoto Hirayama", "Y Victor Lee", "Demetri Terzopoulos"], "venue": "Speechreading by humans and machines, pp. 221\u2013232. Springer, 1996.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1996}, {"title": "Characterizing audiovisual information during speech", "author": ["Eric Vatikiotis-Bateson", "Kevin G Munhall", "Y Kasahara", "Frederique Garcia", "Hani Yehia"], "venue": "ICSLP, 1996.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1996}, {"title": "Audio-visual speech recognition using deep learning", "author": ["Kuniaki Noda", "Yuki Yamaguchi", "Kazuhiro Nakadai", "Hiroshi G Okuno", "Tetsuya Ogata"], "venue": "Applied Intelligence, vol. 42, no. 4, pp. 722\u2013737, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Yajie Miao", "Mohammad Gowayyed", "Florian Metze"], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 167\u2013174.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Intraface", "author": ["Fernando De la Torre", "Wen-Sheng Chu", "Xuehan Xiong", "Francisco Vicente", "Xiaoyu Ding", "Jeffrey Cohn"], "venue": "Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on. IEEE, 2015, vol. 1, pp. 1\u20138.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Distinctive image features from scaleinvariant keypoints", "author": ["David G Lowe"], "venue": "International journal of computer vision, vol. 60, no. 2, pp. 91\u2013110, 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Scaling recurrent neural network language models", "author": ["Will Williams", "Niranjani Prasad", "David Mrva", "Tom Ash", "Tony Robinson"], "venue": "CoRR, vol. abs/1502.00512, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Lipreading (speechreading)", "author": ["J Jeffers", "M Barley"], "venue": "Charles C. Thomas, Springfield, IL, p. b10, 1971.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1971}], "referenceMentions": [{"referenceID": 0, "context": "This paper presents an end-to-end audiovisual speech recognizer (AVSR), based on recurrent neural networks (RNN) with a connectionist temporal classification (CTC) [1] loss function.", "startOffset": 164, "endOffset": 167}, {"referenceID": 1, "context": "It has been demonstrated by [2] that humans also tend to put their attention to other information channels (i.", "startOffset": 28, "endOffset": 31}, {"referenceID": 2, "context": "Specifically, it is shown by different linguistic studies such as [3] that the mouth shape towards an articulatory target modify the following phone.", "startOffset": 66, "endOffset": 69}, {"referenceID": 3, "context": "This effect is accentuated when the speaking rate of the speaker is high [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 4, "context": "Some practical studies such as [5] state that the coarticulation is speaker- and phoneme-dependent.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "We present an AVSR solution that does not require an HMM, but rather uses several layers of bi-directional long short-term memory (LSTM) [6] units as building blocks, followed by a CTC loss function for the output layer.", "startOffset": 137, "endOffset": 140}, {"referenceID": 6, "context": "In an audio-visual setting, these may correspond to the different times at which phonemes and their corresponding visemes (we follow [7] for the mapping) are observed, without requiring any manual labeling or input.", "startOffset": 133, "endOffset": 136}, {"referenceID": 7, "context": "This paper thus makes two main contributions: first, we demonstrate that CTC-based acoustic models can achieve state-of-the-art performance in audio-visual speech recognition tasks, as our system achieve comparable results with a traditional pre-Deep Learning baseline ( [8] report 11% Word Error Rate on the ViaVoice database) and outperforms recent cross-entropy trained DNN baseline [9] in terms of phoneme error rate.", "startOffset": 271, "endOffset": 274}, {"referenceID": 8, "context": "This paper thus makes two main contributions: first, we demonstrate that CTC-based acoustic models can achieve state-of-the-art performance in audio-visual speech recognition tasks, as our system achieve comparable results with a traditional pre-Deep Learning baseline ( [8] report 11% Word Error Rate on the ViaVoice database) and outperforms recent cross-entropy trained DNN baseline [9] in terms of phoneme error rate.", "startOffset": 386, "endOffset": 389}, {"referenceID": 9, "context": "Two ways of fusing both modalities are used in traditional AVSR: first, early combination (feature fusion) of both feature vectors can be applied [10, 11].", "startOffset": 146, "endOffset": 154}, {"referenceID": 10, "context": "Two ways of fusing both modalities are used in traditional AVSR: first, early combination (feature fusion) of both feature vectors can be applied [10, 11].", "startOffset": 146, "endOffset": 154}, {"referenceID": 9, "context": "This approach may lead to frame synchronization problems [10, 12, 11].", "startOffset": 57, "endOffset": 69}, {"referenceID": 11, "context": "This approach may lead to frame synchronization problems [10, 12, 11].", "startOffset": 57, "endOffset": 69}, {"referenceID": 10, "context": "This approach may lead to frame synchronization problems [10, 12, 11].", "startOffset": 57, "endOffset": 69}, {"referenceID": 12, "context": "In [13] for example both modalities are analyzed separately and later on the results of both are fused using a bias.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "In [14, 15, 9] present different recent deep-learning approaches to solve the AVSR problem.", "startOffset": 3, "endOffset": 14}, {"referenceID": 14, "context": "In [14, 15, 9] present different recent deep-learning approaches to solve the AVSR problem.", "startOffset": 3, "endOffset": 14}, {"referenceID": 8, "context": "In [14, 15, 9] present different recent deep-learning approaches to solve the AVSR problem.", "startOffset": 3, "endOffset": 14}, {"referenceID": 13, "context": "In [14] and [9], a joined (audio and video) representations using Deep Neural Networks (DNNs) is learned to perform word and phone recognition respectively.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "In [14] and [9], a joined (audio and video) representations using Deep Neural Networks (DNNs) is learned to perform word and phone recognition respectively.", "startOffset": 12, "endOffset": 15}, {"referenceID": 14, "context": "More recently, [15] present a Recurrent Temporal Multimodal Restricted Boltzmann Machine (RTMRB), which takes into consideration long-term dependencies and outperforms other non-temporal solutions.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "According to [12], lip position is a considerable source of information when performing visual-only speech recognition.", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "In addition to the position of the lips, [12] state that teeth visibility eases the process of guessing the sound that was produced.", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "Moreover, [16, 17, 18] conduct experiments where it is shown that the entire face provides information about speech.", "startOffset": 10, "endOffset": 22}, {"referenceID": 16, "context": "Moreover, [16, 17, 18] conduct experiments where it is shown that the entire face provides information about speech.", "startOffset": 10, "endOffset": 22}, {"referenceID": 17, "context": "Moreover, [16, 17, 18] conduct experiments where it is shown that the entire face provides information about speech.", "startOffset": 10, "endOffset": 22}, {"referenceID": 18, "context": "In the field of deep learning, [19] proposed a MSHMM infrastructure, which uses features extracted from a Convolutial Neural Network (CNN).", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "1, [14, 9, 15] learn a joint feature representation using different DNNs approaches.", "startOffset": 3, "endOffset": 14}, {"referenceID": 8, "context": "1, [14, 9, 15] learn a joint feature representation using different DNNs approaches.", "startOffset": 3, "endOffset": 14}, {"referenceID": 14, "context": "1, [14, 9, 15] learn a joint feature representation using different DNNs approaches.", "startOffset": 3, "endOffset": 14}, {"referenceID": 19, "context": "We use the Eesen framework [20].", "startOffset": 27, "endOffset": 31}, {"referenceID": 5, "context": "To provide the ability of learning more complex time sequences we use bidirectional LSTM units [6] for our RNN.", "startOffset": 95, "endOffset": 98}, {"referenceID": 10, "context": "The IBM ViaVoice [11] data set is used to test and train the proposed pipelines.", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "18 coordinate points that define the inner and outer profile of the mouth shape are extracted using IntraFace [21].", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "A richer representation of the visual modality is achieved describing the mouth landmark points using a scale invariant local description (SIFT) [22].", "startOffset": 145, "endOffset": 149}, {"referenceID": 7, "context": "We perform a baseline audio-only recognition experiments with FBank + pitch coefficients and an in-domain language model as used in [8], and achieved a WER of 11.", "startOffset": 132, "endOffset": 135}, {"referenceID": 22, "context": "Also, in order to reduce the language model bias of this setup in the ViaVoice domain, we decided to switch to a more general n-gram language model based on TED talks [23], which we reduce to the required vocabulary and we use in the following experiments.", "startOffset": 167, "endOffset": 171}, {"referenceID": 23, "context": "Summary of the results obtained with the different visual feature representations, mapping phonemes to 12 visemes according to [24].", "startOffset": 127, "endOffset": 131}, {"referenceID": 2, "context": "This finding supports the coarticualation [3] and anticipatory coarticulation [5] studies of natural speech production, where is stated that speaker changes the mouth shape before pronounce the following phone.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "This finding supports the coarticualation [3] and anticipatory coarticulation [5] studies of natural speech production, where is stated that speaker changes the mouth shape before pronounce the following phone.", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "This shows that the \u201cpeaky\u201d structure of CTC is well suited also to multi-modal fusion, and that more detailed studies should be performed in order to investigate the, presumably,speaker- and phoneme-dependent nature of coarticulation [3, 5].", "startOffset": 235, "endOffset": 241}, {"referenceID": 4, "context": "This shows that the \u201cpeaky\u201d structure of CTC is well suited also to multi-modal fusion, and that more detailed studies should be performed in order to investigate the, presumably,speaker- and phoneme-dependent nature of coarticulation [3, 5].", "startOffset": 235, "endOffset": 241}], "year": 2016, "abstractText": "Speech is one of the most effective ways of communication among humans. Even though audio is the most common way of transmitting speech, very important information can be found in other modalities, such as vision. Vision is particularly useful when the acoustic signal is corrupted. Multi-modal speech recognition however has not yet found wide-spread use, mostly because the temporal alignment and fusion of the different information sources is challenging. This paper presents an end-to-end audiovisual speech recognizer (AVSR), based on recurrent neural networks (RNN) with a connectionist temporal classification (CTC) [1] loss function. CTC creates sparse \u201cpeaky\u201d output activations, and we analyze the differences in the alignments of output targets (phonemes or visemes) between audio-only, video-only, and audio-visual feature representations. We present the first such experiments on the large vocabulary IBM ViaVoice database, which outperform previously published approaches on phone accuracy in clean and noisy conditions.", "creator": "LaTeX with hyperref package"}}}