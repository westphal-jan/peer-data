{"id": "1611.07476", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2016", "title": "Singularity of the Hessian in Deep Learning", "abstract": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how over-parametrized the system is, and for the edges indicating the complexity of the input data. As such, we can make some intuitive and more general conclusions about the composition of this distribution, and that the total volume of the Hessian of a loss function is much larger.\n\n\nIn addition to the Hessian of a loss function, we will also demonstrate that the Hessian of a loss function is related to its underlying complexity. The Hessian of a loss function has a much broader range of features. In each section we demonstrate how we can define the Hessian of a loss function, and how to define it.\nIn terms of the Hessian of a loss function, we can define a variety of different possible combinations of the Hessian of a loss function, but only in two sections:\nWe will see the Hessian of a loss function from the Hessian of a loss function. As this article will show, one of the most important aspects of the Hessian of a loss function is the structure of the Hessian of a loss function, and how to define it.\nOne of the first things we must remember is how the Hessian of a loss function is organized. In this section, we will see a representation of the Hessian of a loss function that is divided into two parts. In the sections that will go into this section, we will see a simple representation of the Hessian of a loss function that is organized into two parts:\nA loss function that is distributed between parts of the Hessian of a loss function and is distributed across parts of the Hessian of a loss function and is distributed across parts of the Hessian of a loss function.\nThe other major difference between the Hessian of a loss function and the Hessian of a loss function is that the Hessian of a loss function is distributed across parts of the Hessian of a loss function. In other words, the Hessian of a loss function is distributed across parts of the Hessian of a loss function.\nIn the section, we will see a representation of the Hessian of a loss function, and how to define it.\nThe Hessian of a loss function is organized across parts of the Hessian", "histories": [["v1", "Tue, 22 Nov 2016 19:24:49 GMT  (4520kb,D)", "http://arxiv.org/abs/1611.07476v1", "ICLR 2017 Submission on Nov 4, 2016"], ["v2", "Thu, 5 Oct 2017 13:28:50 GMT  (2514kb,D)", "http://arxiv.org/abs/1611.07476v2", "ICLR submission, 2016 - updated to match the openreview.net version"]], "COMMENTS": "ICLR 2017 Submission on Nov 4, 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["levent sagun", "leon bottou", "yann lecun"], "accepted": false, "id": "1611.07476"}, "pdf": {"name": "1611.07476.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["DEEP LEARNING", "Levent Sagun"], "emails": ["sagun@cims.nyu.edu", "leon@bottou.org", "yann@cs.nyu.edu"], "sections": [{"heading": null, "text": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how overparametrized the system is, and for the edges indicating the complexity of the input data."}, {"heading": "1 INTRODUCTION", "text": "Given a (piece-wise) differentiable loss function, and a gradient based algorithm to minimize it, the knowledge of the second order information about it can tell us quite a bit about how the landscape looks like, and how we could modify our algorithm to make it go faster and find better solutions. But, one of the biggest challenges in second order optimization methods is in accessing that second order information itself. In particular, in deep learning there have been many proposals to accelerate training using second order information. Ngiam et al. (2011) has an in depth review of some of the proposals for approximating the Hessian of the loss function. Nevertheless, given the computational complexity of the problems at hand, we don\u2019t have much information on what the actual Hessian looks like. This work is part of a series of papers that explore the data-model-algorithm connection along with Sagun et al. (2014; 2015) and it builds on top of the intuition developed in Lecun et al. (1998).\nIn this short note, we show how the data and the architecture depends on the eigenvalues of the Hessian of the loss function. In particular, we observe that the top discrete eigenvalues depend on the data, and the bulk of the eigenvalues depend on the architecture. Furthermore, as we keep growing the size of the network, we observe that the discrete part that depends on data remains the same, but the concentration around zero sharpens.\nThere are various conclusions and implications of this singularity. Recent research suggest new insights into convergence properties of gradient based algorithms in non-convex systems (Lee et al., 2016; Hardt et al., 2015). The results come together with their implications on neural networks. However, the proofs require the system at hand to be non-degenerate. An immediate conclusion of our observation is that the Hessian of the loss function is very singular. Therefore, a lot of the theory and methodology that assumes non-singular Hessian cannot be applied without an appropriate modification."}, {"heading": "2 MNIST EIGENVALUES ON A FULLY-CONNECTED MODEL", "text": "We calculate the exact Hessian of the loss function of a network with two hidden layers. The inputs are 28 \u2217 28 MNIST data, the network has two hidden layers with ReLU nonlinearity, the top layer has a softmax and a negative log likelihood loss function at the end. We plot the histogram of the eigenvalues of the Hessian for a varying number of hidden units. The Hessian turns out to be extremely singular, and increasing the number of units in hidden layers only add to the singularity of the Hessian.\nar X\niv :1\n61 1.\n07 47\n6v 1\n[ cs\n.L G\n] 2\n2 N\nov 2\n01 6"}, {"heading": "2.1 VARYING THE DATA", "text": "To demonstrate how the eigenvalue distribution may depend on data itself, we keep the same architecture and change the inputs to random patterns. Initially, a random point in the weight space is selected, and we calculated the Hessian without any training. After training the system until the norm of the gradient is close to zero. We again calculate the exact Hessian and plot the histogram of their eigenvalues."}, {"heading": "3 A SIMPLER CASE", "text": "In this section, we will repeat the same experiment in two-dimensional data, in an attempt to understand better the connection between the data and the spectrum of the Hessian. We create two Gaussian blobs, centered at (1, 1) and (\u22121,\u22121), and first we keep the standard deviation the same, and increase the network size.\nThe network architecture is the same, two hidden layer fully connected network, with ReLU nonlinearities and a softmax at the top layer combined with a negative log-likelihood loss function. We train the system with gradient descent with constant step size. At the end of the training the norm of the gradient is at the order of 10\u22124.\nThere are two eigenvalues that are isolated, and away from the bulk of the spectrum. Increasing the network only adds to the concentration of eigenvalues at and around zero. To give an insight into how the Hessian\u2019s themselves look like, here we plot the full Hessian matrices for three of the systems above:\nAll of the training has been done with random initializations on the weight space with the same standard deviation. In other words, initial points are randomly chosen on the surface of a sphere with a fixed radius given the total number of parameters. This begs the question of the effect of the choice of the initial point. Therefore, now we fix the network size, and repeat the experiment with different random initializations over 5K times, and plot the fluctuations of the top eigenvalue.\nThe next question is how the spectrum responds to the increased complexity of data. To this end, we keep the architecture the same, and increase the standard deviation of the two Gaussian blobs. They are still centered at the same two points, but it becomes harder to separate them as they merge together. Gradient descent still converges to a low-cost value, but the error is higher, and it can\u2019t learn how to separate them perfectly as the blobs merge together. We observe that the top two eigenvalues grow significantly, and beyond its natural fluctuations due to the initialization."}, {"heading": "4 CONCLUSION", "text": "We show that the Hessian of the loss functions in deep learning is degenerate. This has implications on the theoretical work which requires improvements in its premises. One such step has been taken in Panageas & Piliouras (2016) in relaxing the isolated singularity condition that was assumed in Lee et al. (2016). From a practical point of view this has multiple implications:\n\u2022 The landscape may be flat beyond the notion of wide basins.\n\u2022 Training goes to a place with small gradient, but it is not zero.\n\u2022 There are still negative eigenvalues even when they are small in magnitude.\nThis suggests that we may be able to look beyond the classical notions of basins when exploring the energy landscapes of loss functions. Next obvious question is to find low energy paths between solutions to show the kind of flatness in such landscapes. This will be explored in a subsequent work in the same series.\nWe also demonstrate the two phases of the spectrum, one that is concentrated around zero that depends on the size of the model, and the second part that is away from the bulk of the spectrum, that is isolated and depends on the data.\nThis kind of two-phased non-degeneracy can, in fact, be a desirable property. A degenerate Hessian implies locally flat regions. A degenerate Hessian at the scale that we observe in deep learning may imply flat regions across space, at the global scale.\n\u2022 We can devise separate methods for the directions that correspond to the top eigenvalues.\n\u2022 We can take advantage of the directions that correspond to the zero or small eigenvalues by attempting to find paths of low energies in the weight space.\nAs a first step to the last item, initial experiments are promising: Let\u2019s take a random point on the weight space and train two systems from that point: (1) with gradient descent, and (2) with stochastic gradient descent. At each step, take a straight line between the two points and interpolate the cost value. The resulting profile is completely flat even when the points keep diverging from one another. Next, take two random initial points on the weight space, so now they are orthogonal to each other. And train two systems with different shuffling of data for SGD. This time one would expect the line interpolation to give arbitrary values since the initial points are completely orthogonal, surprisingly, the line interpolation also decreases albeit not as flat as the previous one."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Afonso Bandeira, Yann Dauphin, Ruoyu Sun, Arthur Szlam and Soumith Chintala for valuable discussions. Part of the research has been conducted when the first author was an intern at FAIR."}], "references": [{"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Moritz Hardt", "Benjamin Recht", "Yoram Singer"], "venue": "arXiv preprint arXiv:1509.01240,", "citeRegEx": "Hardt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2015}, {"title": "Efficient backprop", "author": ["Yann Lecun", "Leon Bottou", "Genevieve B Orr", "Klaus-Robert M\u00fcller"], "venue": null, "citeRegEx": "Lecun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "Gradient descent converges to minimizers", "author": ["Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht"], "venue": "University of California, Berkeley,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "On optimization methods for deep learning", "author": ["Jiquan Ngiam", "Adam Coates", "Ahbik Lahiri", "Bobby Prochnow", "Quoc V Le", "Andrew Y Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Gradient descent only converges to minimizers: Nonisolated critical points and invariant regions", "author": ["Ioannis Panageas", "Georgios Piliouras"], "venue": "arXiv preprint arXiv:1605.00405,", "citeRegEx": "Panageas and Piliouras.,? \\Q2016\\E", "shortCiteRegEx": "Panageas and Piliouras.", "year": 2016}, {"title": "Explorations on high dimensional landscapes", "author": ["Levent Sagun", "V U\u011fur G\u00fcney", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": "ICLR 2015 Workshop Contribution,", "citeRegEx": "Sagun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sagun et al\\.", "year": 2014}, {"title": "Universality in halting time and its applications in optimization", "author": ["Levent Sagun", "Thomas Trogdon", "Yann LeCun"], "venue": "arXiv preprint arXiv:1511.06444,", "citeRegEx": "Sagun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sagun et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Recent research suggest new insights into convergence properties of gradient based algorithms in non-convex systems (Lee et al., 2016; Hardt et al., 2015).", "startOffset": 116, "endOffset": 154}, {"referenceID": 0, "context": "Recent research suggest new insights into convergence properties of gradient based algorithms in non-convex systems (Lee et al., 2016; Hardt et al., 2015).", "startOffset": 116, "endOffset": 154}, {"referenceID": 0, "context": "Ngiam et al. (2011) has an in depth review of some of the proposals for approximating the Hessian of the loss function.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "(2014; 2015) and it builds on top of the intuition developed in Lecun et al. (1998). In this short note, we show how the data and the architecture depends on the eigenvalues of the Hessian of the loss function.", "startOffset": 64, "endOffset": 84}, {"referenceID": 2, "context": "One such step has been taken in Panageas & Piliouras (2016) in relaxing the isolated singularity condition that was assumed in Lee et al. (2016). From a practical point of view this has multiple implications: \u2022 The landscape may be flat beyond the notion of wide basins.", "startOffset": 127, "endOffset": 145}], "year": 2016, "abstractText": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how overparametrized the system is, and for the edges indicating the complexity of the input data.", "creator": "LaTeX with hyperref package"}}}