{"id": "1611.08656", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2016", "title": "Attention-based Memory Selection Recurrent Network for Language Modeling", "abstract": "Recurrent neural networks (RNNs) have achieved great success in language modeling. However, since the RNNs have fixed size of memory, their memory cannot store all the information about the words it have seen before in the sentence, and thus the useful long-term information may be ignored when predicting the next words. In this paper, we propose Attention-based Memory Selection Recurrent Network (AMSRN), in which the model can review the information stored in the memory at each previous time step and select the relevant information to help generate the outputs. The first step is to use the data of the trained dataset to determine the best learning time for each sentence in the sentence. With the help of the training dataset, we can generate the training time for each sentence in the sentence. In order to test the effect of AMSRN, we have to define the number of words that were used in the training dataset in order to assess their influence on the prediction of the next words in the sentence. Using the training dataset, we have to select words in the sentence. These words are found in the training dataset. To learn more about the training dataset, refer to this article.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Sat, 26 Nov 2016 04:25:00 GMT  (255kb,D)", "http://arxiv.org/abs/1611.08656v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["da-rong liu", "shun-po chuang", "hung-yi lee"], "accepted": false, "id": "1611.08656"}, "pdf": {"name": "1611.08656.pdf", "metadata": {"source": "CRF", "title": "ATTENTION-BASED MEMORY SELECTION RECURRENT NETWORK FOR LANGUAGE MODELING", "authors": ["Da-Rong Liu", "Shun-Po Chuang", "Hung-yi Lee"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 Language Modeling, Recurrent Network, Attention Model"}, {"heading": "1. INTRODUCTION", "text": "Recurrent neural networks (RNNs) [1] have been shown to perform well in many sequence modeling tasks[2]. In RNNs, the gated memory cells like long short-term memory (LSTM) [3] and gated recurrent unit (GRU) [4] are widely used. The attention mechanism has been applied on RNN models. Neural Turing Machine (NTM) [5] is one of the examples. The idea of attention mechanism is to let the model automatically find the related part of information from memory (usually represented as a vector sequence), and use the information to obtain the results. Attention mechanism shows promising results on many tasks including machine translation[6, 7, 8], caption generation[9, 10] and question answering [11, 12, 13, 14].\nLanguage modeling has been recognized as an important task in human language processing. The statistical models such as N-gram language model [15, 16] were widely used to solve this task. Recently, RNNs are introduced in language modeling [17, 18, 19, 20, 21, 22, 23, 24] and have shown great improvement compared to the traditional counterpart. However, since the RNNs have fixed size of memory, their memory cannot store all the information about the words have seen in the sentence, and thus the useful long-term information may be ignored when predicting the next words. By making the RNN models have the ability to review the information obtained in every previous time step, the attention mechanism improves RNN model. The examples of integrating attention mechanism and LSTM-based RNN model for language modeling are Long Shortterm Memory-Network (LSTMN) [25] and Recurrent Memory Network (RMN) [26]. LSTMN uses an expandable hidden memory to\nexplicitly store every past memory segments, making use of all the previous values in the memory to compute every update and generate the results. RMN uses the hidden memory of LSTM to generate the attention weights, and then uses the attention weights and another trainable memory to generate the outputs. Both LSTMN and RMN are shown to outperform original LSTM on language modeling.\nIn this paper, we propose Attention-based Memory Selection Recurrent Network (AMSRN), a novel RNN architecture that applies the attention mechanism on LSTM. In AMSRN, the attention mechanism extracts the relevant information from the LSTM memory states in all the previous time steps for predicting the next word. The information in different dimensions of LSTM memory states has different degrees of involvement in attention weight generation and relevant information extraction. The degree of the involvement for each dimension is different for each time step. The memory selection mechanism is automatically learned from data. In this paper, we mainly make the following contributions:\n1. We investigate different ways of integrating LSTM and attention mechanism. The experimental results show that the attention mechanism helps the LSTM language model on three different corpora including English and Chinese.\n2. Different from LSTMN which makes some modification on the way LSTM updates the memory, in AMSRN the attention mechanism is stacked on original LSTM, so the architecture of the original LSTM has remained. Therefore, the LSTM part in AMSRN can be initialized by a typical LSTM language model.\n3. In RMN, there are two sets of memory, one for computing the attention weights and the other for extracting the information. On the other hand, the proposed model learns to determine which memory dimensions should be involved more in computing the attention weights and which should be considered more when extracting the information, and the role of each dimension can be different at different time steps. From this point of view, RMN can be considered as a special case of the proposed model. The experimental results show that the proposed model has more stable performance across different corpora than RMN.\n4. We investigate to use entropy as regularizer for attention weights.\n5. Finally, we make a visualization analysis of how the attention mechanism helps language modeling."}, {"heading": "2. ATTENTION-BASED MEMORY SELECTION RECURRENT NETWORK", "text": "The overall structure of the proposed Attention-based Memory Selection Recurrent Network (AMSRN) is shown in Fig. 1. AMSRN\nar X\niv :1\n61 1.\n08 65\n6v 1\n[ cs\n.C L\n] 2\n6 N\nov 2\n01 6\nconsists of two major parts: the typical LSTM described in Section 2.1 and the attention mechanism module stacking on LSTM in Section 2.2. The LSTM reads through the input word sequence, and stores the hidden layer outputs generated at each time step. The attention mechanism module takes the stored information as input, and generates a vector relevant to the prediction of the next words. Then the relevant vector and the current LSTM hidden state are used to generate the distributions for the next words. In the attention mechanism module, the memory selection is applied on the LSTM memory to determine which dimensions of LSTM hidden states should be involved in computing the attention weights and extracting relevant information, which will be described in Section 2.3. Finally, the attention weights can be regularized by their entropy in Section 2.4."}, {"heading": "2.1. LSTM", "text": "The input of the LSTM is a sequence of words represented by 1-ofN encoding, {x1, x2, \u00b7 \u00b7 \u00b7xt, \u00b7 \u00b7 \u00b7 }, at the bottom of Fig. 1. At each time step t, the hidden layer output of the LSTM is a d-dimensional vector ht, where d is the number of memory cells in LSTM, and ht would be stored for further use. Therefore, at the time step t (when the model has read the first t words in the sentence in question), the information stored is Mt,\nMt = [h0, h1, \u00b7 \u00b7 \u00b7ht\u22121], (1)\nwhere h0 is the initial state of the LSTM.Mt in (1) is a d\u00d7 tmatrix, which grows as t increases. The attention module will extract the information from Mt."}, {"heading": "2.2. Attention Mechanism", "text": "In the attention mechanism, the memory selection module generates two d-dimensional vectors, wh1 and wh2, from the current LSTM state ht. wh1 and wh2 are used to select the stored information. Here all the elements in wh1 and wh2 are between 0 and 1. How to generate wh1 and wh2 will be described in the next subsection.\nThen the current hidden state ht and the two memory selection vectors, wh1 and wh2, are used to extract the relevant information, represented as a d-dimensional vector rt, fromMt in (1). The model first generates a d-dimensional vector kt from the current hidden\nstate ht as the \u2018key\u2019 for attention weight generation,\nkt =Wkhht + bk, (2)\nwhere the d\u00d7dmatrixWkh and d-dimensional vector bk are network parameters to be learned. Then the inner product similarity eti between the key kt and each hi in Mt = [h0, h1, h2, \u00b7 \u00b7 \u00b7hi, \u00b7 \u00b7 \u00b7ht\u22121] is computed. eti = (hi wh1) \u2022 kt, (3) where denotes the elementwise multiplication, and \u2022 denotes the inner product. By multiplying each element in hi by the corresponding element in wh1 (that is, hi wh1 in (3)), the model determines the degree of each dimension of hi involved in computing the similarity (for example, the dimension multiplied by 0 would be totally ignored in generating the attention weights). The similarity eti is further normalized by softmax normalization to obtain the attention weights \u03b1ti,\n\u03b1ti = exp(eti)\u2211t\u22121 i=0 exp(eti) . (4)\nTo generate the relevant vector rt, each hi is selected by wh2 to obtain h\u2032i, h\u2032i = hi wh2, (5) in which the degree each dimension of hi is involved in extracting the relevant vector rt is determined. Finally, rt is generated by the weighted sum of h\u2032i according to \u03b1ti,\nrt = t\u22121\u2211 i=0 \u03b1tih \u2032 i. (6)\nThe attention vector rt and the hidden state ht predicts the distribution of the next word Pw,\nPw = softmax(Wphht +Wprrt + bp),\nwhere Wph, Wpr and bp are network parameters to be learned, and softmax is the softmax activation function. The cost C to be minimized by optimizing the network parameters is the cross-entropy between the word distribution Pw and the reference distribution for all the words in the training set."}, {"heading": "2.3. Memory Selection", "text": "In this paper, we investigate three different ways to obtain wh1 used in (3) and wh2 in (6):\n1. wh1 and wh2 are generated independently. The current state of LSTM, ht, is passed into two different fully connected layers with sigmoid activation function to generate wh1 and wh2 as below,\nwh1 = sigmoid(Whh1ht + bh1)\nwh2 = sigmoid(Whh2ht + bh2),\nwhere the Whh1, bh1, Whh2 and bh2 denote the weights and the biases of the fully connected layer.\n2. The two vectors wh1 and wh2 are forced to be the same. wh1 is generated by the same way as the first approach, and the model simply sets wh2 = wh1.\n3. The only difference between the third and the second approaches is that here we set wh1 = 1\u2212wh2, where 1 is a ddimensional vector with all ones, and \u2018\u2212\u2019 here represents elementwise subtraction. The inspiration of the third approach\nis that in the attention models the memory used to generate attention distribution and the memory used to generate the final attention vector can be different [26, 27]. Therefore, by constraining the sum of the two weights, wh1 and wh2, it simulates the situation that there are two different sets of memory for attention weights and information extraction respectively."}, {"heading": "2.4. Regularizer", "text": "When training model, a regularization term is usually used to prevent overfitting. For example, the two-norm of the model parameters are widely used as a regularizer. Here we investigate to use the entropy of the attention weights as the regularization term [28]. The purpose of using entropy as regularizer is because only part of the information in the previous steps is relevant to the prediction of the next word. Therefore, the attention weights that extract useful information from the previous time steps are sparse. The entropy regularizer to keep the attention weights sparse is designed as below.\nLreg = \u2211 u Tu\u2211 t=1 t\u22121\u2211 i=0 \u2212wti logwti, (7)\nwhere u is a sentence in the training corpus, and Tu is the length of u. wti in (7) denotes the attention weight of hi at the time step t when reading sentence u, and \u2211t\u22121 i=0 \u2212wti logwti is the entropy of the attention weights obtained at the time step t. With the regularization term, the network is learned to minimize C + \u03bbLreg , where C has been mentioned in Subsection 2.2 and \u03bb is determined by a validation set."}, {"heading": "3. EXPERIMENTS", "text": ""}, {"heading": "3.1. Experimental Setups", "text": "We tested the proposed model on two English data sets and one Chinese data set. The first data set we used is the Penn Treebank Corpus [29], which is a widely used data set to evaluate the effectiveness of a language model. It contains about 40K training sentences, 3K validation sentences and 4K testing sentences. The other English data set we used is from the Switchboard Corpus[30]. Switchboard is a Telephone Speech Corpus which collect two-sided telephone conversations among speakers in the United States. We used about 945K sentences for training, 10K for validation and about 5.2K for testing. For Chinese, we used Chinese Gigaword data set[31] to evaluate the model. Chinese Gigaword data set consists of around 25K Chinese news articles. After parsing, there are 531K sentences for training, 165K for validation and about 260K for testing. Table 1 summaries the statistics of the three data sets we used in the following experiments. The perplexities (PPLs) on the testing data sets are used to evaluate different methods.\nAlthough it is possible to train AMSRN model from scratch, since the AMSRN model contains a LSTM part, it is possible to initialize the LSTM part by a LSTM langauge model. Therefore, in the following experiments, the AMSRN is always pretrained by a LSTM language model. Because of the limited computing resource, we fixed both the dimension of the LSTM hidden state and the embedding layer to be 50."}, {"heading": "3.2. Memory Selection Methods", "text": "In this experiment, we investigate different memory selection methods for generating wh1 and wh2 respectively. The three methods in\nSubsection 2.3 are (1) generating wh1 and wh2 are independently, (2) setting wh1 = wh2, and (3) setting wh1 = 1\u2212 wh2. The results of the three methods are shown in Table 2. Generating wh1 and wh2 independently leads to the worst result (133.80). This may be because this approach need twice parameters compared with the other two. Constraining wh1 and wh2 to be the same is better than making them complementary (133.36 v.s. 133.62). The results suggest that probably the information for computing similarity and information extraction are contained in the same dimension of the LSTM hidden states. Since the second method achieves the best result, it is used in the following experiments."}, {"heading": "3.3. Comparison of Different Models", "text": "The experimental results of different models are shown in Table 3. Columns (1), (2) and (3) are the results on Penn Treebank Corpus, Switchboard Corpus and Chinese Gigaword data set, respectively. Experiments were done step by step. First, a typical LSTM language model was trained, and PPLs of the LSTM model on the testing sets are in row (A). Then in row (B) the attention module was added on top of LSTM but without memory selection (or all the elements in wh1 and wh2 are one) and entropy regularizer. It is found that attention mechanism was helpful on both Penn Treebank and Switchboard (Rows (B) v.s. (A) on Columns (1) and (2)), but it does not improve the LSTM on Chinese Gigaword (Rows (B) v.s. (A) on columns (3)). In row (C), we show the results of wit memory selection based on the second approach in Subsection 2.3. We found that memory selection is essential for attention mechanism here. With memory selection, attention-based model outperformed LSTM on all the three corpora (Rows (C) v.s. (B)). Then the entropy regularization for the attention weights was applied on the attention-based model with memory selection. The results are in row (D). The results of entropy regularization are mixed. It improved the performance on Penn Treebank, but degrades the performance on the rest two corpora (Rows (D) v.s. (C)). The experimental results suggest that the assumption of sparse attention weights is probably not very accurate.\nWe further compare the proposed model with another two attention-based language model, recurrent memory network (RMN) [26] and Recurrent-Memory-Recurrent (RMR) [26]. Comparing LSTM with the two attention-based model in the literature, the conclusion\nis also mixed. RMN and RMR outperformed drastically the two English corpora (Rows (E), (F) v.s. (A) on Columns (1) and (2)), but contrary conclusion is obtained on the Chinese corpus (Rows (E), (F) v.s. (A) on Column (3)). This is probably RMN and RMR have only be verified on English, German, and Italian, and there are some special techniques on Chinese that should be specially considered. The proposed approach consistently improves LSTM, and better than RMN and RMR on Chinese, but worse than them on English corpora. The proposed model seems to be more robust across different corpora, but the improvements are limited."}, {"heading": "3.4. Analysis", "text": "To illustrate how attention mechanism works, we visualize the attention weights in some sentences. We first compute the perplexities of each sentence in Gigaword (Chinese) and Penn Treebank (English) data sets, then select the sentences which improved the most by the proposed model (Row (C) in Table 3) compared with the LSTM baselines. We chose ten sentences from Gigaword (Chinese) and Penn Treebank data sets, and visualize and analysis the attention weights. Four examples are shown in Fig. 2. In Fig. 2, the arrows point to the words to be predicted, and we highlight the words whose attention weights are higher than a threshold when predicting the words with arrows. We found that a word will have large attention under one of the following four conditions:\n1. Trigger (example (a) in Fig. 2: When the information is repeated, the model attends to the part where the same information is mentioned before.\n2. Causal Relationship (example (b)): If A is the cause of B, when prediction the words related toB, the model will attend to the words related to A.\n3. Phrases (example (c)): When predicting a word in the later part of a phrase, the model will attend on the former part of the same phrase.\n4. Grammar (example (d)): Some grammar rules are considered by the attention-based model. For example, to predict the word \u2019are\u2019, the model attends on a plural noun."}, {"heading": "4. CONCLUDING REMARKS", "text": "In this paper, we propose Attention-based Memory Selection Recurrent Network (AMSRN) for language modeling and investigate the integration of attention mechanism and LSTM. The results were verified on two English corpora and a Chinese corpus. The results show that AMSRN consistently outperformed LSTM-based language model, and memory selection is essential for attention mechanism. We further visualize how the attention mechanism works in language modeling. Some questions unresolved in this paper will be studied in the future, for example, the influence of the language characteristics to the attention-based model."}, {"heading": "5. REFERENCES", "text": "[1] Jeffrey L Elman, \u201cFinding structure in time,\u201d Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.\n[2] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, \u201cSequence to sequence learning with neural networks,\u201d in Advances in neural information processing systems, 2014, pp. 3104\u20133112.\n[3] Sepp Hochreiter and Ju\u0308rgen Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[4] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio, \u201cEmpirical evaluation of gated recurrent neural networks on sequence modeling,\u201d arXiv preprint arXiv:1412.3555, 2014.\n[5] Alex Graves, Greg Wayne, and Ivo Danihelka, \u201cNeural turing machines,\u201d arXiv preprint arXiv:1410.5401, 2014.\n[6] Minh-Thang Luong, Hieu Pham, and Christopher D Manning, \u201cEffective approaches to attention-based neural machine translation,\u201d arXiv preprint arXiv:1508.04025, 2015.\n[7] Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vymolova, Kaisheng Yao, Chris Dyer, and Gholamreza Haffari, \u201cIncorporating structural alignment biases into an attentional neural translation model,\u201d arXiv preprint arXiv:1601.01085, 2016.\n[8] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, \u201cNeural machine translation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014.\n[9] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S Zemel, and Yoshua Bengio, \u201cShow, attend and tell: Neural image caption generation with visual attention,\u201d arXiv preprint arXiv:1502.03044, vol. 2, no. 3, pp. 5, 2015.\n[10] Junqi Jin, Kun Fu, Runpeng Cui, Fei Sha, and Changshui Zhang, \u201cAligning where to see and what to tell: image caption with region-based attention and scene factorization,\u201d arXiv preprint arXiv:1506.06272, 2015.\n[11] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola, \u201cStacked attention networks for image question answering,\u201d arXiv preprint arXiv:1511.02274, 2015.\n[12] Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, and Ram Nevatia, \u201cAbc-cnn: An attention based convolutional neural network for visual question answering,\u201d arXiv preprint arXiv:1511.05960, 2015.\n[13] Kevin J. Shih, Saurabh Singh, and Derek Hoiem, \u201cWhere to look: Focus regions for visual question answering,\u201d in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.\n[14] Abhishek Das, Harsh Agrawal, C Lawrence Zitnick, Devi Parikh, and Dhruv Batra, \u201cHuman attention in visual question answering: Do humans and deep networks look at the same regions?,\u201d arXiv preprint arXiv:1606.03556, 2016.\n[15] Roni Rosenfeld, \u201cTwo decades of statistical language modeling: Where do we go from here?,\u201d 2000.\n[16] Fred Jelinek, \u201cUp from trigrams,\u201d . [17] Tomas Mikolov, \u201cRecurrent neural network based language\nmodel.,\u201d . [18] Toma\u0301s\u030c Mikolov, Stefan Kombrink, Luka\u0301s\u030c Burget, Jan\nC\u030cernocky\u0300, and Sanjeev Khudanpur, \u201cExtensions of recurrent neural network language model,\u201d in 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2011, pp. 5528\u20135531.\n[19] Siva Reddy Gangireddy, Fergus McInnes, and Steve Renals, \u201cFeed forward pre-training for recurrent neural network language models.,\u201d 2014.\n[20] Xunying Liu, Yongqiang Wang, Xie Chen, Mark JF Gales, and Philip C Woodland, \u201cEfficient lattice rescoring using recurrent neural network language models,\u201d in 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 4908\u20134912.\n[21] Bing Zhao and Yik-Cheung Tam, \u201cBilingual recurrent neural networks for improved statistical machine translation,\u201d in Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, pp. 66\u201370.\n[22] Jen-Tzung Chien and Yuan-Chu Ku, \u201cBayesian recurrent neural network for language modeling,\u201d IEEE transactions on neural networks and learning systems, vol. 27, no. 2, pp. 361\u2013374, 2016.\n[23] Abdel-rahman Mohamed, Frank Seide, Dong Yu, Jasha Droppo, Andreas Stoicke, Geoffrey Zweig, and Gerald Penn, \u201cDeep bi-directional recurrent networks over spectral windows,\u201d in 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 78\u201383.\n[24] Xiangang Li and Xihong Wu, \u201cConstructing long short-term memory based deep recurrent neural networks for large vocabulary speech recognition,\u201d in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4520\u20134524.\n[25] Jianpeng Cheng, Li Dong, and Mirella Lapata, \u201cLong shortterm memory-networks for machine reading,\u201d arXiv preprint arXiv:1601.06733, 2016.\n[26] Ke Tran, Arianna Bisazza, and Christof Monz, \u201cRecurrent memory network for language modeling,\u201d arXiv preprint arXiv:1601.01272, 2016.\n[27] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al., \u201cEndto-end memory networks,\u201d in Advances in neural information processing systems, 2015, pp. 2440\u20132448.\n[28] Yves Grandvalet and Yoshua Bengio, \u201cSemi-supervised learning by entropy minimization,\u201d in Advances in neural information processing systems, 2004, pp. 529\u2013536.\n[29] Ann Taylor, Mitchell Marcus, and Beatrice Santorini, \u201cThe penn treebank: an overview,\u201d in Treebanks, pp. 5\u201322. Springer, 2003.\n[30] John J Godfrey, Edward C Holliman, and Jane McDaniel, \u201cSwitchboard: Telephone speech corpus for research and development,\u201d in Acoustics, Speech, and Signal Processing, 1992. ICASSP-92., 1992 IEEE International Conference on. IEEE, 1992, vol. 1, pp. 517\u2013520.\n[31] David Graff and Ke Chen, \u201cChinese gigaword,\u201d LDC Catalog No.: LDC2003T09, ISBN, vol. 1, pp. 58563\u201358230, 2005."}], "references": [{"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "Advances in neural information processing systems, 2014, pp. 3104\u20133112.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1508.04025, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Incorporating structural alignment biases into an attentional neural translation model", "author": ["Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari"], "venue": "arXiv preprint arXiv:1601.01085, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.03044, vol. 2, no. 3, pp. 5, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Aligning where to see and what to tell: image caption with region-based attention and scene factorization", "author": ["Junqi Jin", "Kun Fu", "Runpeng Cui", "Fei Sha", "Changshui Zhang"], "venue": "arXiv preprint arXiv:1506.06272, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola"], "venue": "arXiv preprint arXiv:1511.02274, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Abc-cnn: An attention based convolutional neural network for visual question answering", "author": ["Kan Chen", "Jiang Wang", "Liang-Chieh Chen", "Haoyuan Gao", "Wei Xu", "Ram Nevatia"], "venue": "arXiv preprint arXiv:1511.05960, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Where to look: Focus regions for visual question answering", "author": ["Kevin J. Shih", "Saurabh Singh", "Derek Hoiem"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Human attention in visual question answering: Do humans and deep networks look at the same regions", "author": ["Abhishek Das", "Harsh Agrawal", "C Lawrence Zitnick", "Devi Parikh", "Dhruv Batra"], "venue": "arXiv preprint arXiv:1606.03556, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Two decades of statistical language modeling: Where do we go from here", "author": ["Roni Rosenfeld"], "venue": "2000.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Up from trigrams", "author": ["Fred Jelinek"], "venue": ".", "citeRegEx": "16", "shortCiteRegEx": null, "year": 0}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov"], "venue": ".", "citeRegEx": "17", "shortCiteRegEx": null, "year": 0}, {"title": "Extensions of recurrent neural network language model", "author": ["Tom\u00e1\u0161 Mikolov", "Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2011, pp. 5528\u20135531.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Feed forward pre-training for recurrent neural network language models", "author": ["Siva Reddy Gangireddy", "Fergus McInnes", "Steve Renals"], "venue": "2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient lattice rescoring using recurrent neural network language models", "author": ["Xunying Liu", "Yongqiang Wang", "Xie Chen", "Mark JF Gales", "Philip C Woodland"], "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 4908\u20134912.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Bilingual recurrent neural networks for improved statistical machine translation", "author": ["Bing Zhao", "Yik-Cheung Tam"], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, pp. 66\u201370.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Bayesian recurrent neural network for language modeling", "author": ["Jen-Tzung Chien", "Yuan-Chu Ku"], "venue": "IEEE transactions on neural networks and learning systems, vol. 27, no. 2, pp. 361\u2013374, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep bi-directional recurrent networks over spectral windows", "author": ["Abdel-rahman Mohamed", "Frank Seide", "Dong Yu", "Jasha Droppo", "Andreas Stoicke", "Geoffrey Zweig", "Gerald Penn"], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 78\u201383.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Constructing long short-term memory based deep recurrent neural networks for large vocabulary speech recognition", "author": ["Xiangang Li", "Xihong Wu"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4520\u20134524.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Long shortterm memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "arXiv preprint arXiv:1601.06733, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent memory network for language modeling", "author": ["Ke Tran", "Arianna Bisazza", "Christof Monz"], "venue": "arXiv preprint arXiv:1601.01272, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Endto-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "Advances in neural information processing systems, 2015, pp. 2440\u20132448.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised learning by entropy minimization", "author": ["Yves Grandvalet", "Yoshua Bengio"], "venue": "Advances in neural information processing systems, 2004, pp. 529\u2013536.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "The penn treebank: an overview", "author": ["Ann Taylor", "Mitchell Marcus", "Beatrice Santorini"], "venue": "Treebanks, pp. 5\u201322. Springer, 2003.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "Switchboard: Telephone speech corpus for research and development", "author": ["John J Godfrey", "Edward C Holliman", "Jane McDaniel"], "venue": "Acoustics, Speech, and Signal Processing, 1992. ICASSP-92., 1992 IEEE International Conference on. IEEE, 1992, vol. 1, pp. 517\u2013520.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1992}, {"title": "Chinese gigaword", "author": ["David Graff", "Ke Chen"], "venue": "LDC Catalog No.: LDC2003T09, ISBN, vol. 1, pp. 58563\u201358230, 2005.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Recurrent neural networks (RNNs) [1] have been shown to perform well in many sequence modeling tasks[2].", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": "Recurrent neural networks (RNNs) [1] have been shown to perform well in many sequence modeling tasks[2].", "startOffset": 100, "endOffset": 103}, {"referenceID": 2, "context": "In RNNs, the gated memory cells like long short-term memory (LSTM) [3] and gated recurrent unit (GRU) [4] are widely used.", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "In RNNs, the gated memory cells like long short-term memory (LSTM) [3] and gated recurrent unit (GRU) [4] are widely used.", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "Neural Turing Machine (NTM) [5] is one of the examples.", "startOffset": 28, "endOffset": 31}, {"referenceID": 5, "context": "Attention mechanism shows promising results on many tasks including machine translation[6, 7, 8], caption generation[9, 10] and question answering [11, 12, 13, 14].", "startOffset": 87, "endOffset": 96}, {"referenceID": 6, "context": "Attention mechanism shows promising results on many tasks including machine translation[6, 7, 8], caption generation[9, 10] and question answering [11, 12, 13, 14].", "startOffset": 87, "endOffset": 96}, {"referenceID": 7, "context": "Attention mechanism shows promising results on many tasks including machine translation[6, 7, 8], caption generation[9, 10] and question answering [11, 12, 13, 14].", "startOffset": 87, "endOffset": 96}, {"referenceID": 8, "context": "Attention mechanism shows promising results on many tasks including machine translation[6, 7, 8], caption generation[9, 10] and question answering [11, 12, 13, 14].", "startOffset": 116, "endOffset": 123}, {"referenceID": 9, "context": "Attention mechanism shows promising results on many tasks including machine translation[6, 7, 8], caption generation[9, 10] and question answering [11, 12, 13, 14].", "startOffset": 116, "endOffset": 123}, {"referenceID": 10, "context": "Attention mechanism shows promising results on many tasks including machine translation[6, 7, 8], caption generation[9, 10] and question answering [11, 12, 13, 14].", "startOffset": 147, "endOffset": 163}, {"referenceID": 11, "context": "Attention mechanism shows promising results on many tasks including machine translation[6, 7, 8], caption generation[9, 10] and question answering [11, 12, 13, 14].", "startOffset": 147, "endOffset": 163}, {"referenceID": 12, "context": "Attention mechanism shows promising results on many tasks including machine translation[6, 7, 8], caption generation[9, 10] and question answering [11, 12, 13, 14].", "startOffset": 147, "endOffset": 163}, {"referenceID": 13, "context": "Attention mechanism shows promising results on many tasks including machine translation[6, 7, 8], caption generation[9, 10] and question answering [11, 12, 13, 14].", "startOffset": 147, "endOffset": 163}, {"referenceID": 14, "context": "The statistical models such as N-gram language model [15, 16] were widely used to solve this task.", "startOffset": 53, "endOffset": 61}, {"referenceID": 15, "context": "The statistical models such as N-gram language model [15, 16] were widely used to solve this task.", "startOffset": 53, "endOffset": 61}, {"referenceID": 16, "context": "Recently, RNNs are introduced in language modeling [17, 18, 19, 20, 21, 22, 23, 24] and have shown great improvement compared to the traditional counterpart.", "startOffset": 51, "endOffset": 83}, {"referenceID": 17, "context": "Recently, RNNs are introduced in language modeling [17, 18, 19, 20, 21, 22, 23, 24] and have shown great improvement compared to the traditional counterpart.", "startOffset": 51, "endOffset": 83}, {"referenceID": 18, "context": "Recently, RNNs are introduced in language modeling [17, 18, 19, 20, 21, 22, 23, 24] and have shown great improvement compared to the traditional counterpart.", "startOffset": 51, "endOffset": 83}, {"referenceID": 19, "context": "Recently, RNNs are introduced in language modeling [17, 18, 19, 20, 21, 22, 23, 24] and have shown great improvement compared to the traditional counterpart.", "startOffset": 51, "endOffset": 83}, {"referenceID": 20, "context": "Recently, RNNs are introduced in language modeling [17, 18, 19, 20, 21, 22, 23, 24] and have shown great improvement compared to the traditional counterpart.", "startOffset": 51, "endOffset": 83}, {"referenceID": 21, "context": "Recently, RNNs are introduced in language modeling [17, 18, 19, 20, 21, 22, 23, 24] and have shown great improvement compared to the traditional counterpart.", "startOffset": 51, "endOffset": 83}, {"referenceID": 22, "context": "Recently, RNNs are introduced in language modeling [17, 18, 19, 20, 21, 22, 23, 24] and have shown great improvement compared to the traditional counterpart.", "startOffset": 51, "endOffset": 83}, {"referenceID": 23, "context": "Recently, RNNs are introduced in language modeling [17, 18, 19, 20, 21, 22, 23, 24] and have shown great improvement compared to the traditional counterpart.", "startOffset": 51, "endOffset": 83}, {"referenceID": 24, "context": "The examples of integrating attention mechanism and LSTM-based RNN model for language modeling are Long Shortterm Memory-Network (LSTMN) [25] and Recurrent Memory Network (RMN) [26].", "startOffset": 137, "endOffset": 141}, {"referenceID": 25, "context": "The examples of integrating attention mechanism and LSTM-based RNN model for language modeling are Long Shortterm Memory-Network (LSTMN) [25] and Recurrent Memory Network (RMN) [26].", "startOffset": 177, "endOffset": 181}, {"referenceID": 25, "context": "is that in the attention models the memory used to generate attention distribution and the memory used to generate the final attention vector can be different [26, 27].", "startOffset": 159, "endOffset": 167}, {"referenceID": 26, "context": "is that in the attention models the memory used to generate attention distribution and the memory used to generate the final attention vector can be different [26, 27].", "startOffset": 159, "endOffset": 167}, {"referenceID": 27, "context": "Here we investigate to use the entropy of the attention weights as the regularization term [28].", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "The first data set we used is the Penn Treebank Corpus [29], which is a widely used data set to evaluate the effectiveness of a language model.", "startOffset": 55, "endOffset": 59}, {"referenceID": 29, "context": "The other English data set we used is from the Switchboard Corpus[30].", "startOffset": 65, "endOffset": 69}, {"referenceID": 30, "context": "For Chinese, we used Chinese Gigaword data set[31] to evaluate the model.", "startOffset": 46, "endOffset": 50}, {"referenceID": 25, "context": "We further compare the proposed model with another two attention-based language model, recurrent memory network (RMN) [26] and Recurrent-Memory-Recurrent (RMR) [26].", "startOffset": 118, "endOffset": 122}, {"referenceID": 25, "context": "We further compare the proposed model with another two attention-based language model, recurrent memory network (RMN) [26] and Recurrent-Memory-Recurrent (RMR) [26].", "startOffset": 160, "endOffset": 164}, {"referenceID": 25, "context": "25 (E) RMN [26] 123.", "startOffset": 11, "endOffset": 15}, {"referenceID": 25, "context": "28 (F) RMR [26] 134.", "startOffset": 11, "endOffset": 15}], "year": 2016, "abstractText": "Recurrent neural networks (RNNs) have achieved great success in language modeling. However, since the RNNs have fixed size of memory, their memory cannot store all the information about the words it have seen before in the sentence, and thus the useful longterm information may be ignored when predicting the next words. In this paper, we propose Attention-based Memory Selection Recurrent Network (AMSRN), in which the model can review the information stored in the memory at each previous time step and select the relevant information to help generate the outputs. In AMSRN, the attention mechanism finds the time steps storing the relevant information in the memory, and memory selection determines which dimensions of the memory are involved in computing the attention weights and from which the information is extracted. In the experiments, AMSRN outperformed long short-term memory (LSTM) based language models on both English and Chinese corpora. Moreover, we investigate using entropy as a regularizer for attention weights and visualize how the attention mechanism helps language modeling.", "creator": "LaTeX with hyperref package"}}}