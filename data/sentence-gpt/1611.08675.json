{"id": "1611.08675", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2016", "title": "Deep Reinforcement Learning for Multi-Domain Dialogue Systems", "abstract": "Standard deep reinforcement learning methods such as Deep Q-Networks (DQN) for multiple tasks (domains) face scalability problems. We propose a method for multi-domain dialogue policy learning---termed NDQN, and apply it to an information-seeking spoken dialogue system in the domains of restaurants and hotels. Experimental results comparing DQN (baseline) versus NDQN (proposed) using simulations report that our proposed method exhibits better scalability and is promising for optimising the behaviour of multi-domain dialogue systems. We propose a method for multicasting dynamic language (DQN) and cross-functional language (DQN) languages based on the following model: A functional language with high complexity and low complexity of the choice, or a functional language with more than 3,000 words with no other choice. For example, with the DQN model, a multi-language conversation is described in the DQN model, with the DQN model in the DQN model and the DQN model in the DQN model. To achieve our proposal, we define the following features: a deep-channel conversation between a person and an NPC that could be used to inform their interaction with the NPC and provide feedback on interactions. We define the following features: an implementation that allows an NPC to communicate to their NPCs. A language that can interact with NPCs based on the same language or language. A language that can interact with NPCs based on the same language or language. A language that can communicate with NPCs based on the same language or language. A language that can interact with NPCs based on the same language or language. A language that can communicate with NPCs based on the same language or language. A language that can interact with NPCs based on the same language or language. A language that can interact with NPCs based on the same language or language. A language that can interact with NPCs based on the same language or language. An object to learn more about our proposed method.", "histories": [["v1", "Sat, 26 Nov 2016 07:53:22 GMT  (705kb)", "http://arxiv.org/abs/1611.08675v1", "NIPS Workshop on Deep Reinforcement Learning, 2016"]], "COMMENTS": "NIPS Workshop on Deep Reinforcement Learning, 2016", "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["heriberto cuay\\'ahuitl", "seunghak yu", "ashley williamson", "jacob carse"], "accepted": false, "id": "1611.08675"}, "pdf": {"name": "1611.08675.pdf", "metadata": {"source": "CRF", "title": "Deep Reinforcement Learning for Multi-Domain Dialogue Systems", "authors": ["Heriberto Cuay\u00e1huitl", "Seunghak Yu", "Ashley Williamson", "Jacob Carse"], "emails": ["HCuayahuitl@lincoln.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n08 67\n5v 1\n[ cs\n.A I]\n2 6\nN ov"}, {"heading": "1 Introduction", "text": "Dialogue systems based on the Reinforcement Learning (RL) paradigm offer the possibility to treat dialogue design as an optimisation problem, and are attractive because they can improve their performance over time with experience. But the application of RL is not trivial due to the complexity of the problem such as large state-action spaces exhibited in human-machine conversations. This is especially true in multi-domain systems, where the number of state variables (features) and dialogue actions increases rapidly as more domains are taken into account. On the one hand, unique situations in the interaction can be described by a large number of variables (e.g. words raised in the conversation by the system and user) so that enumerating them would result in very large state spaces. On the other hand, the action space can also be large due to the wide range of unique dialogue actions (e.g. requests, apologies, confirmations in multiple contexts).\nWhile one can aim to optimise the interaction via compression of the search space, it is usually unclear what features to incorporate in the state representation. This is a strong motivation for applying Deep Reinforcement Learning (DRL) to dialogue management so that the agent can simultaneously learn its feature representation and policy [15]. This paper makes use of raw noisy text as features in an attempt to avoid engineered features to represent the dialogue state. By using this representation, dialogue agents bypass spoken language understanding in order to learn dialogue policies directly from raw (noisy) text to actions [1].\nWe address dialogue optimisation using the divide-and-conquer approach, in which dialogue states can be described at different levels of granularity, and an action can execute behaviour using either a single dialogue action (taking one dialogue turn) or a composite one (equivalent to a subdialogue taking multiple dialogue turns). This approach offers at least two benefits: (a) modularity helps to optimise subdialogues that may be easier to optimise than the whole dialogue; and (b) subdialogues may include only relevant dialogue knowledge in the states and relevant actions, thus reducing significantly the size of possible solutions: consequently they can be found faster. These properties\n\u2217Funding from Samsung Electronics Ltd. and the University of Lincoln is gratefully acknowledged. We thank Raymond Kirk for helping with App development (to integrate the agents in this paper) and system testing. We also thank Heesik Jeon and the AI team at Samsung for their executive efforts in this project.\nAppears in: NIPS Workshop on Deep Reinforcement Learning, Barcelona, Spain, 2016.\nare crucial for training the behaviour of multi-domain spoken dialogue systems in which there may be a large set of state variables or a large number of actions.\nBelow we describe a data-driven method to the approach described, which we have applied to an information-seeking dialogue system in the domains of restaurants and hotels. Experimental results show that the proposed method can train policies faster and more effectively than a standard algorithm in the literature, showing promise for training multi-domain dialogue systems."}, {"heading": "2 Literature Review", "text": "Recently, multi-domain spoken conversational agents have received an increasing amount of attention. This may be due to the fact that speech technologies such as Automated Speech Recognition (ASR) and Text-To-Speech (TTS) have reached a high degree of maturity. But the question of How to design conversational systems for human-machine interaction in multiple domains (or tasks)? is still an open and interesting problem in artificial intelligence. The dialogue system proposed by [11] used a distributed architecture of domain experts modulated by a domain selector. The latter used a decision tree with classification errors over 20% in 5 domains. This indicates that not only individual domains have to exhibit robust interactions against errors, but also that errors increase by incorporating more domains.[9] used rule-based classifiers for predicting user intentions, which are executed using a Hierarchical Task Network (HTN) incorporating expert knowledge. Trainable multi-domain dialogue systems using traditional reinforcement learning include [4, 13, 22, 5]. These systems use a modest amount of features, and in contrast to neural-based systems, they require manual feature engineering.\nRecent work on deep learning applied to task-oriented conversational agents include the following. [6] uses a Recurrent Neural Network (RNN) for dialogue act prediction in a POMDP-based dialogue system, which focuses on mapping system and user sentences to dialogue acts. [2] applies Deep Reinforcement Learning with a fully-connected neural network for trading negotiations in board games, which focuses on mapping game situations to dialogue actions. [20] trains RNN-based classifiers for predicting dialogue success in multi-domain dialogue systems, which can be applied to unseen domains. [17] also trains RNN-based classifiers but for belief tracking in order to improve the robustness of recognised user responses across dialogue turns. Other neural-based conversational agents have been applied to text prediction using the sequence-to-sequence approach [19, 21], and to reasoning with inference for text-based question answering [23].\nFrom these works, we can observe that supervised learning is the dominating form of training in neural-based conversational agents. To our knowledge, we report the first multi-domain dialogue system using deep reinforcement learning. This form of learning is interesting because it can perform feature learning and policy learning simultaneously, and its effective application in real-world dialogue scenarios remains to be demonstrated."}, {"heading": "3 Method", "text": "Our proposed method to scale up Deep Reinforcement Learning (DRL) for multi-domain dialogue systems has two stages. First, multi-policy learning via a network of DRL agents; and second, more compact state representations by compressing raw inputs. Although these two stages can be applied independently, their combination aims for further scalability than any one of them individually."}, {"heading": "3.1 Network of Deep Q-Networks", "text": "We propose to optimise multi-domain dialogue systems using a network of Deep Reinforcement Learners, for example a network of Deep Q-networks (DQN) \u2014 see [15, 16] for an introduction to the standard DQN method. In our method, instead of training a single DQN,we train a set of DQNs (also referred to as NDQNs), where every DQN represents a specialised skill to converse in a particular subdialogue \u2014 see Figure 1. In addition, the network of agents enable DQNs to be executed without a fixed structure in order to support flexible and unstructured dialogues. In contrast to hierarchical DQNs [12] that follow a strict sequence of agents, in our method an NDQN allows transitions between all DQN agents except for self-transitions and loops (the latter using a stack-based approach as in [3]). Furthermore, while user responses can motivate transitions to\nanother domain in the network, completing a subdialogue within a domain motivates a transition to the previous domain to resume the interaction. Algorithm 1 describes the procedure to train and execute NDQNs.\nAn optimal policy in an NDQN performs action selection according to\n\u03c0\u2217 \u03b8(d) (s) = arg max a\u2208A(d) Q\u2217(d)(s, a; \u03b8(d)), (1)\nwhere domain or skill d \u2208 D is selected according to d = argmax\nd\u2032\u2208D F (d\u2032|d, e), (2)\nand evidence e takes into account all features that describe the environment space of domain d. While this transition function (Eq. 2) is used for high-level transitions in the interaction, Equation 1 is used for low-level transitions within a node (skill) in the network and subject to reinforcement learning. NDQN assumes that the domain transition function F can be deterministic or probabilistic (the latter due to uncertainty in the interaction), and it is a prior requirement for NDQN-Learning."}, {"heading": "3.2 NDQNs with Compressed Raw Inputs", "text": "Previous work on dialogue policy learning using DRL map raw (noisy) text to actions [1, 24]. This is not only computationally intensive, but it becomes infeasible for dialogue systems with large vocabularies. To tackle this problem we propose to use delexicalised sentences (as proposed in [8]) and synonymised sentences. This has the advantage that dialogue policies can be trained from more compact state representations than those using only raw inputs, and have coverage for a larger vocabulary than trained for.\nAlgorithm 1 Network of Deep Q-Learners (NDQN)\n1: Initialise set of Deep Q-Networks with replay memories D(d), action-value functions Q(d) with random weights \u03b8(d), and target action-value functions Q\u0302(d) with weights \u03b8\u0302(d) = \u03b8(d) 2: repeat 3: d \u2190 initial domain, predefined or defined by argmaxd\u2208D Fo(d) 4: s \u2190 initial environment state in S(d) 5: repeat 6: repeat 7: Choose action a \u2208 A(d) in s derived from Q(d) (e.g. \u01eb-greedy, Thompson) 8: Execute action a and observe reward r and next state s\u2032 9: Append transition (s, a, r, s\u2032) to D(d)\n10: B(d) \u2190 sample random minibatch of experiences from D(d) 11: d\u2032 \u2190 select next domain according to argmaxd\u2032\u2208D F (d\u2032|s\u2032, e)\n12: yj =\n{\nrj if final step of episode rj + \u03b3maxa\u2208A(d) Q\u0302 (d)(s\u2032, a\u2032; \u03b8\u0302(d)), otherwise\n13: Gradient descent step on ( yj \u2212Q (d)(s\u2032, a\u2032; \u03b8(d)) )2 using B(d) 14: Reset Q\u0302(d) = Q(d) every C steps 15: s \u2190 s\u2032 16: until s is a terminal state or d 6= d\u2032 17: d \u2190 d\u2032 18: until s is a goal state 19: until convergence"}, {"heading": "3.2.1 Delexicalisation", "text": "Consider a dialogue system for restaurant search receiving the following user request\u2014with corresponding delexicalised sentence underneath."}, {"heading": "I am looking for italian food in the city centre", "text": "I am looking for $foodtype food in the $area\nThe latter representation combining words and slot IDs (denoted with the symbol \u2018$\u2019) has several practical advantages. For example, policies can be learnt faster, they contribute to further scalability of systems with large vocabularies, and policies do not have to be retrained if the slot values change over time. In this work we use heuristics to replace slot values by slot IDs, and a trainable component for automatic slot labelling is considered beyond the scope of this paper."}, {"heading": "3.2.2 Synonymization", "text": "Consider the same system above receiving the following user request given the unknown words \u2018fancy\u2019 and \u2018cuisine\u2019\u2014with corresponding synonyms underneath.\nWe fancy italian cuisine in the centre of town\nWe want italian food in the centre of town\nWe argue that word synonyms can be useful in such situations because the unknown word \u2018fancy\u2019 can trigger the known word feature \u2018want\u2019. Similarly, the unknown word \u2018cuisine\u2019 can trigger the known word feature \u2018food\u2019. In this way, the vocabulary of our NDQNs incorporate a mapping from filler words and slot values to synonyms in order to cope with unseen wordings. Due to the complexity of automatic generation of meaningful synonyms, our synonyms have been manually specified but they can be generated automatically for example from word embeddings [14]."}, {"heading": "4 Multi-Domain Dialogue System", "text": "The proposed computational framework for training multi-domain dialogue agents is a substantial extension from the publicly available software tools SimpleDS [1] and ConvnetJS [10]. It can be executed in training or test mode using simulations or speech-based interactions (via an App). Our dialogue system runs under a client-server architecture, where the learning agents\u2014one per domain\u2014 act as the clients and the dialogue system as the server. They communicate by exchanging messages, where the clients tell the server the action to execute, and the server tells the clients the state and reward observed. The elements for training multi-domain dialogue systems are as follows.\nState Spaces They include word-based features depending on the vocabulary of each learning agent. They include 177 unique words2 without synonyms, and 150 unique words with synonyms. For example, an agent in the domain of restaurants has relevant features for its domain and it is agnostic of features in other domains. While words derived from system responses are treated as binary variables (i.e. word present or absent), the words derived from noisy user responses can be seen as continuous variables by taking ASR confidence scores into account. Since a single variable per word is used, user features override system ones in case of overlaps.\nAction Spaces They include dialogue acts for the targeted domains\u2014currently 69 unique actions in total. Example dialogue act types, dialogue acts without parameters, are as follows: Salutation(), Request(), AskFor(), Apology(), ExpConfirm(), ImpConfirm(), Retrieve(), Provide(), among others. Rather than learning with whole action sets, our framework supports learning from constrained actions by applying learning updates only on the set of valid actions. These actions are derived from the most likely actions, Pr(a|s) > 0.0001, from Naive Bayes classifiers (due to scalability purposes) trained from example dialogues. See example demonstration dialogue in Appendix A. In addition to the most probable data-like actions, the constrained actions are extended with the legitimate requests, apologies and confirmations. The fact that constrained actions are data-driven and driven by domain-independent heuristics, facilitates its usage across domains.\nState Transition Functions They are based on numerical vectors representing the last system and user responses. Taking a wider dialogue context is also possible but not explored in this paper. The system responses are straightforward, 0 if absent and 1 if present (hit-or-miss). The user responses correspond to the confidence level [0..1] of noisy user responses. While system responses are generated from stochastic templates, user responses are generated from semi-random user behaviour. These elements enable the creation of a vast amount of different conversations for agent training.\nDomain Transition Function This function specifies the next domain or task in focus. It is currently defined deterministically, and it is also implemented as a SVM classifier trained from example interactions\u2014see Appendix A. The design of this classifier follows that of a two-deep fully connected neural network with 80 nodes in each hidden layer, with tanh activation, and an SVM output layer, using Hinge Loss. While the input layer accepts domain-independent words-as-features vectors representing the unique global vocabulary shared amongst all domains in a hit-or-miss approach, the output layer has 3 classes representing system domains (meta3, restaurants and hotels). 15K dialogues of data were generated, partitioned as a 60-40 training-testing split, and trained for 180 epochs. Initial results of this classifier shows a 87.5% classification accuracy on user-simulated data.\nReward Function It is defined as R(s, a, s\u2032) = GR+DR\u2212DL, where GR is goal-based reward treated as task success [0..1] (the proportion of positively confirmed slots and information retrieved and presented); DR is a data-like probability of having observed action a in state s; and DL = t\u2217w is a dialogue length measure used to encourage efficient interactions with t time steps and weight w (-0.1 in our case). The DR scores are derived from Naive Bayes classifiers to allow statistical inference over actions given states (Pr(a|s)).\n2The unique words in our system\u2019s vocabulary excludes words from information presentation due to the vast amount of information about hotels and restaurants. Nonetheless and during testing, our system retrieves live information from http://www.bookatable.co.uk) and www.reservetravel.com.\n3We refer to meta domain as subdialogues containing domain-general system and user responses.\nModel Architectures We use fully-connected multilayer neural nets, trained with stochastic gradient descent, where nodes in the input layers depend on the vocabulary of each agent. The use of convolutional neural nets is work in progress. They include 2 hidden layers with 80 nodes with Rectified Linear Units to normalise their weights [18]. Dropout[7] and adaptive learning rates are also part of our work in progress. Other hyperparameters include experience replay size=10000, burning steps=1000, discount factor=0.7, minimum epsilon=0.001, batch size=32, and learning steps=30000."}, {"heading": "5 Experimental Results", "text": "Here we compare a multi-domain dialogue system using a standard DRL method versus our proposed method described in Sections 3 and 4. While the former (DQN) uses a single policy for learning (baseline), the latter (NDQN) uses multiple policies (proposed). Both multi-domain dialogue systems use the same data, resources and hyperparameters for training, the only difference between both systems is the learning method (DQN or NDQN) or state representation (with or without compression).\nWe use four different metrics to measure system performance: avg. reward, learning time, avg. task success, and avg. dialogue length. The higher the better in the first and third, and the lower the better in the second and fourth. Figure 2 shows learning curves for the baseline DQN-based system, and Figure 3 shows learning curves for the proposed NDQN-based system. Both the baseline and proposed system report results over 150K learning steps (about 8700 dialogues). Our results report that training multi-domain systems using a single policy is twofold harder than using a multi-policy approach. First, this is evidenced by the fact that the baseline policies do not improve over time, and the policies with the proposed method do. This is presumably due to the abstraction exhibited in the multi-policy approach\u2014more focused system actions rather than interleaving them across domains. Second, our proposed system also learned 4.6 times faster than the baseline, which was accelerated further to 4.7 times faster by using compressed inputs4. By applying synonymization we are able to use a smaller vocabulary when training and then a much wider vocabulary at runtime, which adds robustness in the presence of unseen dialogues. These results show indication of better scalability for NDQN to multiple domains.\nAlthough the currently generated dialogues using the proposed method seem reasonable, a natural question to ask is How good (qualitatively speaking) are the trained policies? This question will be answered in an evaluation reported in future work.\n4Ran on Intel Core i5-3210M CPU @ 2.50GHz x 4; 8GiB DDR4 RAM @ 2400MHz."}, {"heading": "6 Conclusion and Future Work", "text": "We have described a method for training multi-domain dialogue systems in a more scalable way than traditional deep reinforcement learning, e.g. using the DQN method. The proposed method uses a Network of DQN (NDQN) agents in order to train specialised agents, and compression of input features. Experimental results using simulations report that the proposed method (NDQN) can train policies faster and more effectively than DQN\u2014almost 5 times faster than DQNs. In addition, our results report that input compression contribute faster learning without performance degradation across metrics. These dialogue agents are the first applications of deep reinforcement learning to optimising the behaviour of multi-domain human-machine dialogues.\nIn future work we plan to carry out a data collection of speech-based interactions with recruited participants in order to evaluate and/or retrain our multi-domain dialogue policies with a larger dataset. Other future work includes applying and refining the proposed framework to larger sets of domains, and applying it to multimodal interaction in different types of devices and machines."}, {"heading": "A Example Demonstration Dialogue", "text": "We aim for a data-efficient system, trainable from a small set of demonstrations. This dialogue is one of the 12 manually-specified seed demonstrations for inducing the system\u2019s features (words) and actions (dialogue acts), and for training classifiers to predict the most likely actions given the previous system and user responses. The latter were used by the DRL agents described in Section 3 for inducing their dialogue behaviour according to the learning environment described in Section 4. Square brackets denote system actions, double quotes denote system or user responses, and words beginning with an underscore encapsulate live web-based information according to the user requests.\nDomain Agent [Action]\"Verbalisation\" Meta SYS [Salutation(greeting)]\"Welcome to MultiDS!\"\n[Provide(intro)]\"You may ask for information about hotels and restaurants.\" [Request(hmihy)]\"How can I help you?\"\nUSR \"I am looking for a hotel in edinburgh on the 2nd of january for 2 nights\" Hotels SYS [Apology(h_nights)]\"Sorry, how long?\"\nUSR \"2 nights\" SYS [ExpConfirm(h_day=$h_day,h_month=$h_month,h_nights=$h_nights)] \"Did you say on the 2nd of january for 2 nights?\" USR \"yes i did\" SYS [ExpConfirm(h_city=$h_city)]\"Did you say in edinburgh?\" USR \"i did\" SYS [Retrieve(h_info)]\"Let me see.\"\n[Provide(h_info)]\"I found the following hotels. _hotels.\" Meta [AskFor(h_more)]\"Anything else?\"\nUSR \"I am looking for cheap japanese food in the centre of town\" Restaurants SYS [Apology(r_area)]\"Sorry, what area?\"\nUSR \"In the centre\" SYS [ExpConfirm(r_price=$r_price,r_area=$r_area)] \"Did you say cheap food in the centre?\" USR \"yes i did\" SYS [ExpConfirm(r_food=$r_food)]\"Did you say japanese food?\" USR \"i did\" SYS [Retrieve(h_info)]\"Let me see.\"\n[Provide(h_info)]\"I found the following restaurants. _res.\" Meta Anything else?\nUSR \"no\" SYS [Salutation(closing)]\"Okay, talk to you soon. Bye!\""}], "references": [{"title": "SimpleDS: A simple deep reinforcement learning dialogue system", "author": ["H. Cuay\u00e1huitl"], "venue": "CoRR, abs/1601.04574", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Strategic dialogue management via deep reinforcement learning", "author": ["H. Cuay\u00e1huitl", "S. Keizer", "O. Lemon"], "venue": "CoRR, abs/1511.08099", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonstrict hierarchical reinforcement learning for interactive systems and robots", "author": ["H. Cuay\u00e1huitl", "I. Kruijff-Korbayov\u00e1", "N. Dethlefs"], "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS), 4(3)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Evaluation of a hierarchical reinforcement learning spoken dialogue system", "author": ["H. Cuay\u00e1huitl", "S. Renals", "O. Lemon", "H. Shimodaira"], "venue": "Computer Speech & Language, 24(2)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Policy committee for adaptation in multi-domain spoken dialogue systems", "author": ["M. Gasic", "N. Mrksic", "P. Su", "D. Vandyke", "T. Wen", "S.J. Young"], "venue": "ASRU", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Dialogue management based on multi-domain corpus", "author": ["W. Ge", "B. Xu"], "venue": "Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A.C. Courville", "Y. Bengio"], "venue": "International Conference on Machine Learning (ICML)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust dialog state tracking using delexicalised recurrent neural networks and unsupervised adaptation", "author": ["M. Henderson", "B. Thomson", "S.J. Young"], "venue": "IEEE Spoken Language Technology Workshop", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "An intelligent dialogue agent for the IoT home", "author": ["H. Jeon", "H.R. Oh", "I. Hwang", "J. Kim"], "venue": "AAAI Workshop on AI Applied to Assistive Technologies and Smart Environments", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "ConvNetJS: Javascript library for deep learning", "author": ["A. Karpathy"], "venue": "http://cs.stanford.edu/people/karpathy/convnetjs/", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-domain spoken dialogue system with extensibility and robustness against speech recognition errors", "author": ["K. Komatani", "N. Kanda", "M. Nakano", "K. Nakadai", "H. Tsujino", "T. Ogata", "H.G. Okuno"], "venue": "SIGdial Workshop on Discourse and Dialogue", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["T.D. Kulkarni", "K. Narasimhan", "A. Saeedi", "J.B. Tenenbaum"], "venue": "CoRR, abs/1604.06057", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-policy dialogue management", "author": ["P. Lison"], "venue": "SIGDIAL", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "In NIPS Deep Learning Workshop", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Multidomain dialog state tracking using recurrent neural networks", "author": ["N. Mrksic", "D.\u00d3. S\u00e9aghdha", "B. Thomson", "M. Gasic", "P. Su", "D. Vandyke", "T. Wen", "S.J. Young"], "venue": "CoRR, abs/1506.07190", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "International Conference on Machine Learning (ICML)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["I.V. Serban", "A. Sordoni", "Y. Bengio", "A.C. Courville", "J. Pineau"], "venue": "CoRR, abs/1507.04808", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-domain dialogue success classifiers for policy training", "author": ["D. Vandyke", "P. Su", "M. Gasic", "N. Mrksic", "T. Wen", "S.J. Young"], "venue": "ASRU", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "A neural conversational model", "author": ["O. Vinyals", "Q.V. Le"], "venue": "CoRR, abs/1506.05869", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Policy learning for domain selection in an extensible multi-domain spoken dialogue system", "author": ["Z. Wang", "H. Chen", "G. Wang", "H. Tian", "H. Wu", "H. Wang"], "venue": "Empirical Methods in Natural Language Processing EMNLP", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "CoRR, abs/1410.3916", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning", "author": ["T. Zhao", "M. Esk\u00e9nazi"], "venue": "CoRR, abs/1606.02560", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "This is a strong motivation for applying Deep Reinforcement Learning (DRL) to dialogue management so that the agent can simultaneously learn its feature representation and policy [15].", "startOffset": 179, "endOffset": 183}, {"referenceID": 0, "context": "By using this representation, dialogue agents bypass spoken language understanding in order to learn dialogue policies directly from raw (noisy) text to actions [1].", "startOffset": 161, "endOffset": 164}, {"referenceID": 10, "context": "The dialogue system proposed by [11] used a distributed architecture of domain experts modulated by a domain selector.", "startOffset": 32, "endOffset": 36}, {"referenceID": 8, "context": "[9] used rule-based classifiers for predicting user intentions, which are executed using a Hierarchical Task Network (HTN) incorporating expert knowledge.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Trainable multi-domain dialogue systems using traditional reinforcement learning include [4, 13, 22, 5].", "startOffset": 89, "endOffset": 103}, {"referenceID": 12, "context": "Trainable multi-domain dialogue systems using traditional reinforcement learning include [4, 13, 22, 5].", "startOffset": 89, "endOffset": 103}, {"referenceID": 21, "context": "Trainable multi-domain dialogue systems using traditional reinforcement learning include [4, 13, 22, 5].", "startOffset": 89, "endOffset": 103}, {"referenceID": 4, "context": "Trainable multi-domain dialogue systems using traditional reinforcement learning include [4, 13, 22, 5].", "startOffset": 89, "endOffset": 103}, {"referenceID": 5, "context": "[6] uses a Recurrent Neural Network (RNN) for dialogue act prediction in a POMDP-based dialogue system, which focuses on mapping system and user sentences to dialogue acts.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] applies Deep Reinforcement Learning with a fully-connected neural network for trading negotiations in board games, which focuses on mapping game situations to dialogue actions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[20] trains RNN-based classifiers for predicting dialogue success in multi-domain dialogue systems, which can be applied to unseen domains.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] also trains RNN-based classifiers but for belief tracking in order to improve the robustness of recognised user responses across dialogue turns.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Other neural-based conversational agents have been applied to text prediction using the sequence-to-sequence approach [19, 21], and to reasoning with inference for text-based question answering [23].", "startOffset": 118, "endOffset": 126}, {"referenceID": 20, "context": "Other neural-based conversational agents have been applied to text prediction using the sequence-to-sequence approach [19, 21], and to reasoning with inference for text-based question answering [23].", "startOffset": 118, "endOffset": 126}, {"referenceID": 22, "context": "Other neural-based conversational agents have been applied to text prediction using the sequence-to-sequence approach [19, 21], and to reasoning with inference for text-based question answering [23].", "startOffset": 194, "endOffset": 198}, {"referenceID": 14, "context": "We propose to optimise multi-domain dialogue systems using a network of Deep Reinforcement Learners, for example a network of Deep Q-networks (DQN) \u2014 see [15, 16] for an introduction to the standard DQN method.", "startOffset": 154, "endOffset": 162}, {"referenceID": 15, "context": "We propose to optimise multi-domain dialogue systems using a network of Deep Reinforcement Learners, for example a network of Deep Q-networks (DQN) \u2014 see [15, 16] for an introduction to the standard DQN method.", "startOffset": 154, "endOffset": 162}, {"referenceID": 11, "context": "In contrast to hierarchical DQNs [12] that follow a strict sequence of agents, in our method an NDQN allows transitions between all DQN agents except for self-transitions and loops (the latter using a stack-based approach as in [3]).", "startOffset": 33, "endOffset": 37}, {"referenceID": 2, "context": "In contrast to hierarchical DQNs [12] that follow a strict sequence of agents, in our method an NDQN allows transitions between all DQN agents except for self-transitions and loops (the latter using a stack-based approach as in [3]).", "startOffset": 228, "endOffset": 231}, {"referenceID": 0, "context": "Previous work on dialogue policy learning using DRL map raw (noisy) text to actions [1, 24].", "startOffset": 84, "endOffset": 91}, {"referenceID": 23, "context": "Previous work on dialogue policy learning using DRL map raw (noisy) text to actions [1, 24].", "startOffset": 84, "endOffset": 91}, {"referenceID": 7, "context": "To tackle this problem we propose to use delexicalised sentences (as proposed in [8]) and synonymised sentences.", "startOffset": 81, "endOffset": 84}, {"referenceID": 13, "context": "Due to the complexity of automatic generation of meaningful synonyms, our synonyms have been manually specified but they can be generated automatically for example from word embeddings [14].", "startOffset": 185, "endOffset": 189}, {"referenceID": 0, "context": "The proposed computational framework for training multi-domain dialogue agents is a substantial extension from the publicly available software tools SimpleDS [1] and ConvnetJS [10].", "startOffset": 158, "endOffset": 161}, {"referenceID": 9, "context": "The proposed computational framework for training multi-domain dialogue agents is a substantial extension from the publicly available software tools SimpleDS [1] and ConvnetJS [10].", "startOffset": 176, "endOffset": 180}, {"referenceID": 17, "context": "They include 2 hidden layers with 80 nodes with Rectified Linear Units to normalise their weights [18].", "startOffset": 98, "endOffset": 102}, {"referenceID": 6, "context": "Dropout[7] and adaptive learning rates are also part of our work in progress.", "startOffset": 7, "endOffset": 10}], "year": 2016, "abstractText": "Standard deep reinforcement learning methods such as Deep Q-Networks (DQN) for multiple tasks (domains) face scalability problems. We propose a method for multi-domain dialogue policy learning\u2014termed NDQN, and apply it to an information-seeking spoken dialogue system in the domains of restaurants and hotels. Experimental results comparing DQN (baseline) versus NDQN (proposed) using simulations report that our proposed method exhibits better scalability and is promising for optimising the behaviour of multi-domain dialogue systems.", "creator": "LaTeX with hyperref package"}}}