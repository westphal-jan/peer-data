{"id": "1611.08930", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Nov-2016", "title": "Deep attractor network for single-microphone speaker separation", "abstract": "Despite the overwhelming success of deep learning in various speech processing tasks, the problem of separating simultaneous speakers in a mixture remains challenging. Two major difficulties in such systems are the arbitrary source permutation and unknown number of sources in the mixture. We propose a novel deep learning framework for single channel speech separation by creating attractor points in high dimensional embedding space of the acoustic signals which pull together the time-frequency bins corresponding to each source. This framework offers an efficient alternative approach to the underlying neural networks of speech processing by building a deep learning neural network based on a distributed neural network. The task, such as forming waves of multiple sources in the high dimensional embedding space, is a very challenging task and can be challenging for the neural network to effectively perform. In a recent review, we have reviewed the results of a new Deep Learning Learning Neural Network (DNN).\n\n\n\nThe problem of separation of simultaneous speakers is a complicated one. The main problem with separating speakers from each other is that while each speaker has multiple sources, each one has multiple sources, each one has multiple sources, each one has multiple sources. As such, the system consists of multiple sources and hence each speaker has multiple sources. The process of separating speakers and each other has a unique characteristic of both sources. The process of separating speakers and each other has a unique characteristic of both sources. The process of separating speakers and each other has a unique characteristic of both sources. The process of separating speakers and each other has a unique characteristic of both sources. The process of separating speakers and each other has a unique characteristic of both sources. The process of separating speakers and each other has a unique characteristic of both sources.\nThe problem of separation of speakers and each other has a unique characteristic of both sources. The process of separating speakers and each other has a unique characteristic of both sources. The process of separating speakers and each other has a unique characteristic of both sources. The process of separating speakers and each other has a unique characteristic of both sources. The process of separating speakers and each other has a unique characteristic of both sources. The process of separating speakers and each other has a unique characteristic of both sources. The process of separating speakers and each other has a unique characteristic of both sources. The process of separating speakers and each other has a unique characteristic of both sources. The process of separating speakers and each other has a unique characteristic of both sources. The process of separating speakers and each other has a unique characteristic of both sources. The process of separating speakers and each other has a unique characteristic of", "histories": [["v1", "Sun, 27 Nov 2016 22:47:23 GMT  (2571kb,D)", "http://arxiv.org/abs/1611.08930v1", "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"], ["v2", "Tue, 28 Mar 2017 03:15:07 GMT  (2826kb,D)", "http://arxiv.org/abs/1611.08930v2", "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"]], "COMMENTS": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["zhuo chen", "yi luo", "nima mesgarani"], "accepted": false, "id": "1611.08930"}, "pdf": {"name": "1611.08930.pdf", "metadata": {"source": "CRF", "title": "DEEP ATTRACTOR NETWORK FOR SINGLE-MICROPHONE SPEAKER SEPARATION", "authors": ["Zhuo Chen", "Yi Luo", "Nima Mesgarani"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 Source separation, multi-talker, deep clustering, attractor network\n1. INTRODUCTION\nDespite the recent advances in deep learning methods for various speech processing tasks such as automatic recognition[1, 2, 3], enhancement[4, 5, 6], speech separation remains unresolved. Two main difficulties hinder the efficacy of deep learning algorithms to tackle the famous \u201ccocktail party problem\u201d[7].\nThe first difficulty is referred as the \u201cpermutation problem\u201d [8]. Most neural network methods are trained to map the input signal to a unique target output which can be a label, a sequence, or regression reference. Permutation problem in speech separation arises due to the fact that the order of targets in the mixture is irrelevant. For example, in separating speakers A and B, both (A,B) and (B,A) solutions are acceptable. However, training the neural network with more than one target label per sample will produce conflicting gra-\ndients and thus lead to convergence problem, because the order of targets cannot be determined beforehand. For instance, assigning speaker A to the first target position in (A,B) and (A,C) will cause confusions when the mixture is (B,C), since they both need to be in the second position for consistency.\nThe second problem in using neural network framework for speech separation is the output dimension mismatch problem. Since the number of sources in the mixture can vary, a neural network with fixed number of output nodes does not have the flexibility to separate arbitrary number of sources.\nTwo deep learning based methods have been proposed to resolve these problems, which are known as \u201ddeep clustering (DC)[8]\u201d and \u201dpermutation invariant training (PIT)[9]\u201d. In deep clustering, a network is trained to generate discriminative embedding for each time-frequency (T-F) bin with points belonging to the same source forced to be closer to each other. DC is able to solve both permutation and output dimension problem to produce the state of the art separation performance. The main drawback of DC is its inefficiency to perform end-to-end mapping, because the objective function is the affinity between the sources in the embedded space and not the separated signals themselves. Minimizing the separation error is done with an unfolding clustering system and a second network, which is trained iteratively and stage by stage to ensure convergence [10]. The PIT algorithm solves the permutation problem by pooling over all possible permutations for N mixing sources (N ! permutations), and use the permutation with lowest error to update the network. PIT was first proposed in [8], and was later shown to have comparable performance as DC [9]. However, PIT approach suffers the output dimension mismatch problem because it assumes a fixed number of sources. PIT also suffers from its computation efficiency, where the prediction window has to be much shorter than context window due to the inconsistency of the permutation both across and within sample segments.\nIn this work, we propose a novel deep learning framework which we refer to as the attractor network to solve the source separation problem. The term attractor refers to the well-studied perceptual effects in human speech perception which suggest that the brain circuits create perceptual attractors (magnets) that warp the stimulus space such that to draws the sound that is closest to it, a phenomenon that is called Perceptual Magnet Effect [11]. Our proposed model works on the same principle by forming a reference point (attractor)\nar X\niv :1\n61 1.\n08 93\n0v 1\n[ cs\n.S D\n] 2\n7 N\nov 2\n01 6\nfor each source in the embedding space which draws all the T-F bins toward itself. Using the similarity between the embedded points and each attractor, a mask is estimated for each sources in the mixture. Since the mask is directly related to the attractor point, the proposed framework can potentially be extended to arbitrary number of sources without the permutation problem. Moreover, the mask learning enables a very efficient end-to-end training scheme and highly reduces the computation complexity compared with DC and PIT.\nIn Section 2, the proposed model is explained and discussed in more detail. In Section 3, we evaluate the performance of proposed system, and the conclusion is drawn in Section 4.\n2. ATTRACTOR NEURAL NETWORK"}, {"heading": "2.1. Model", "text": "The neural network is trained to map the mixture sound X to a k dimensional embedding space, such that it minimizes the following objective function:\nL = \u2211 f,t,c \u2016Sf,t,c \u2212Xf,t \u00d7Mf,t,c\u201622 (1)\nwhere S is the clean spectrogram (frequency F \u00d7 time T ) of C sources, X is the mixture spectrogram (frequency F \u00d7 time T ), and M is the mask formed to extract each source. The mask is estimated in the K dimensional embedding space of each T-F bin, represented by V \u2208 RF\u00d7T\u00d7K :\nMf,t,c = 1 1 + exp( \u2211 k Ac,k \u00d7 Vf,t,k) (2)\nwhere A \u2208 RC\u00d7K are the attractors for the C sources in the embedding space, learnt during training, which are defined as\nAc,k =\n\u2211 f,t Vk,f,t \u00d7 Yc,f,t\u2211\nf,t Yc,f,t (3)\nwhich Y \u2208 RF\u00d7T\u00d7K is the source membership function for each T-F bin, i.e., Yt,f,c = 1 if source c has the highest energy at time t and frequency f compare to the other sources.\nThe objective function in Equation 1 consists of three parts. During training, we first compute an embedding V through a forward pass of the neural network for each given mixture. Then an attractor vector is estimated for each source using Equation 3. This can be done in several ways which we will elaborate in Section 2.2. The most straightforward method for attractor generation is to find the source centroid, as defined in Equation 3.\nNext, we estimate a reconstruction mask for each source by finding the similarity of each T-F bin in the embedding space to each of the attractor vectors A, where the similarity metric is defined in Equation 2. This particular metric uses the inner product followed by a sigmoid function which\nmonotonically scales the masks between [0, 1]. Intuitively, if an embedding of a T-F bin is closer to one attractor means that it belongs to that source, and the resulting mask for that source will produce larger values for that T-F bin.\nFinally, a standard L2 reconstruction error is used to generate the gradient, as shown in Equation 1. Therefore, the error for each source reflects the difference between the masked signal and the clean reference, forcing the network to optimize the global reconstruction error for better separation. We refer the proposed net as deep attractor network (DANet).\nIn comparison with previous methods, DANet network has two distinct advantages. Firstly, DANet removes the stepwise pre-training required in DC method to enable end-toend training. Another big advantage of DANet arises from the flexibility in source dependent training, where the sourcedependent knowledge could be easily incorporated by the attractor (e.g. speaker identity)."}, {"heading": "2.2. Estimation of attractor points", "text": "Attractor points can be estimated using various methods other than the average used in Equation 3. One possibility is to use weighted average. Since the attractors represents the source center of gravity, we can include only the embeddings of the most salient T-F bins, which leads to more robust estimation. We investigate this strategy by using an amplitude threshold in the estimation of the attractor. Alternatively, a neural network model may also be used to pick the representative embedding for each source, an idea which shares similarities with encoder-decoder attention networks [12, 13].\nDuring test time, because the true assignment Y is unknown, we incorporate two strategies to form the attractor points. The first is similar to the strategy used in DC, where the centers are found using post K-means algorithm. The second method is based on the observation that the location of the attractors in the embedding space is relatively stable. This observation is shown in Figure 1, where each pair of dots corresponds to the attractor found for the two speakers in a given mixture. Figure 1 shows two principle stable attractor pairs for all the mixtures used, however, this observation needs to be tested in more depth and different tasks and datasets."}, {"heading": "2.3. Relation with DC and PIT", "text": "The objective function of DC is shown in Equation 4, where Y is the indicator function which is equivalent to a binary mask, and V is the learnt embedding:\nL = \u2211 f,t \u2225\u2225\u2225\u2225\u2225\u2211 c Y Y T \u2212 V V T \u2225\u2225\u2225\u2225\u2225 2\n2\n(4)\nSince Y is orthonormal and constant for each mixture, by multiplying Y T to both term, we can get an objective function that is a special case of the attractor network, as in Equation 5:\nL = \u2211 f,t \u2225\u2225\u2225\u2225\u2225\u2211 c Y T \u2212 Y TV V T \u2225\u2225\u2225\u2225\u2225 2\n2\n(5)\nIn Equation 5, Y TV can be viewed as an averaging step, where the embeddings are summed according to the label, and the resulted center is multiplied with the embedding matrix V to measure the similarity between each embedding and the center, and compared with the ground truth binary mask. Therefore, the transformed DC object can be viewed as a special example using binary mask as target and averaging-based attractor vector.\nOn the other hand, when the attractor vectors are considered as free parameters in the network, DANet reduces to a classification network [4, 5], and Equation 1 becomes a fullyconnected layer. In this case, PIT becomes necessary since the mask has no information about the source and the problem of fixed output dimension arises. In contrast, the freedom of the network to form attractor points during the training allows the system to use the affinity between samples where no constraint is on the number of patterns found, therefore allowing the network to be independent of the number of sources. The flexibility of the network in choosing the attractor points is helpful even in two-source separation problem, because the two sources may have very different structures. As can be seen in Figure 2, our proposed method trained in speaker separation tasks has ended up finding 2 attractor pairs (4 points in the embedding space), which can be expected to increase in harder problems.\n3. EVALUATION"}, {"heading": "3.1. Experimental setup", "text": "We evaluate our proposed model on the task of single-channel overlapped two-speaker separation. We use the corpus introduced in [8], which contains a 30 h training set and a 10 h validation set generated by randomly selecting utterances from different speakers in the Wall Street Journal (WSJ0) training set si tr s, and mixing them at various signal-to-noise ratios (SNR) randomly chosen between 0 dB and 10 dB. 5 h evaluation set is generated similarly as above, using utterances from 16 unseen speakers from si dt 05 and si et 05 in WSJ0 dataset. All data are resampled to 8 kHz to reduce computational and memory costs. The log spectral magnitude is served as input feature, computed using short-time Fourier transform (STFT) with 32 ms window length, 8 ms hop size, and the square root of hanning window.\nThe network contains 4 Bi-directional LSTM [14] layers with 600 hidden units in each layer. The embedding dimension is set to 20, resulting in a fully-connected feed-forward layer of 2580 hidden units (20 \u00d7 129) after the BLSTM layers. We split the input features into non-overlapping chunks of 100-frame length as the input to the network. RMSprop algorithm [15] is used for training with an exponential learning\nrate decaying strategy, where the learning rate starts at 10\u22124 and ends at 3\u00d7 10\u22126. The total number of epochs was set to be 150, and we used the cost function in Equation 1 on the validation set for early stopping. The criteria for early stopping is no decrease in the loss function on validation set for 10 epochs. We constructed a Deep Clustering (DC) network with the same configuration which is used as the baseline.\nWe report the results in terms of signal-to-distortion ratio (SDR, which we define as scale-invariant SNR here), signal-to-artifacts ratio (SAR), and signal-to-interference ratio (SIR). The results are shown in Table 3.3"}, {"heading": "3.2. Separation examples", "text": "Figure 1 shows an example of mixture, the difference between the two speakers in the mixture, and the separated spectrograms of the two speakers using DANet. Also visualized in Figure 1 is the embeddings of the mixture projected onto its first Principal Components. In Figure 1, each point represents one T-F bin in the embedding space. Red and blue dots correspond to the T-F bins where speaker one or two have greater energy accordingly. The location of attractors are marked by x. It shows that two symmetric attractor centers are formed, each corresponding to one of the speakers in the mixture. A clear boundary can be observed in the figure, showing that the network successfully pulled the two speakers apart toward their corresponding attractor points.\nFigure 2 shows the location of attractors for 10,000 mixture examples, mapped onto the 3-dimensional space for visualization purpose using Principal Component Analysis. It suggests that the network may have learned two attractor pairs (4 symmetric centers), marked by A1 and A2. This observation confirms our intuition of the DANet mentioned in Section , that DANet has the ability to discover different number of attractors in an unsupervised way, and therefore, form complex separation strategies. Although the task considered in this study is already challenging, one can imagine much more difficult separation scenarios, where the number of speakers in the mixture is large and can change over time. The ability of DANet to form new attractors may prove to be crucial in such cases, because any effort in pre-setting the number of mixing patterns, as done in methods such as PIT, will hinder the generalization ability of the network. Figure 2 also suggests that hierarchical clustering methods can be more suitable, where attractors can drive a hierarchical grouping of sources, allowing a better representation of audio signals. We will explore these issues in future work."}, {"heading": "3.3. Results", "text": "Table 3.3 shows the evaluation results for different networks (example sounds can be found here [16]). Although the plain DANet already outperforms the DC baseline, adding a simple threshold on T-F samples included in the formation of the attractor yields further improved performance, presumably due\nto the increased influence of salient segments. On the other hand, the performance boost suggests that better attractor formation procedures can be utilized to estimate the attractors, such as joint optimizing of the network parameters.\nIn the last experiment in Table 3.3, a fixed pair of attention vector collected from the training data is used, corresponding to the A1 pair in Figure 2. This pre-set attractor is able to generalize well to the unseen mixtures and produced high quality separation, however it was slightly worse than the best model. Compared with K-means, this has the advantage that it can be easily implemented in real-time using a frame-by-frame pipeline. Based on this observation, when more attractors are required (e.g. in more complex tasks), a collection of attractor codebook and a simple classifier could be implemented for real-time processing.\n4. CONCLUSION\nIn this work, we proposed a novel neural network framework called deep attractor network for general source separation problem. The network forms attractor points in a highdimensional embedding space of the signal, and the similarity between attractors and time-frequency embeddings are then converted into a soft separation mask. We showed that the proposed framework can perform end-to-end, real-time separation, is number-of-sources independent, and is more general comparing to deep clustering and classification based approaches. The experiment on two speaker separation task confirmed its efficacy and potential for extension to general source separation problems.\n5. ACKNOWLEDGEMENT\nThe authors would like to thank Drs. John Hershey and Jonathan Le Roux of Mitsubishi Electric Research Lab for constructive discussions. This work was funded by a grant from National Institute of Health, NIDCD, DC014279, National Science Foundation CAREER Award, and the Pew Charitable Trusts.\n6. REFERENCES\n[1] G.E. Dahl, D. Yu, L. Deng, and A. Acero, \u201cContextdependent pre-trained deep neural networks for largevocabulary speech recognition,\u201d in IEEE Trans. on Audio, Speech and Language Processing, 2012, vol. 20.\n[2] S. Has\u0306im, Senior A., and F. Beaufays, \u201cLong short-term memory recurrent neural network architectures for large scale acoustic modeling,\u201d in Interspeech 2014. ISCA, 2014.\n[3] A. Senior, S. Has\u0306im, F. C. Quitry, T. N. Sainath, and K. Rao, \u201cAcoustic modelling with cd-ctc-smbr lstm rnns,\u201d in ASRU 2015. IEEE, 2015.\n[4] Zhuo Chen, Shinji Watanabe, Hakan Erdog\u0306an, and John R Hershey, \u201cSpeech enhancement and recognition using multi-task learning of long short-term memory recurrent neural networks,\u201d ISCA, 2015.\n[5] Takaaki Hori, Zhuo Chen, Hakan Erdogan, John R Hershey, Jonathan Le Roux, Vikramjit Mitra, and Shinji Watanabe, \u201cThe merl/sri system for the 3rd chime challenge using beamforming, robust feature extraction, and advanced speech recognition,\u201d in 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 475\u2013481.\n[6] X. Lu, Y. Tsao, S. Matsuda, and C. Hori, \u201cSpeech enhancement based on deep denoising autoencoder.,\u201d 2013, pp. 436\u2013440.\n[7] E Colin Cherry, \u201cSome experiments on the recognition of speech, with one and with two ears,\u201d The Journal of the acoustical society of America, vol. 25, no. 5, pp. 975\u2013979, 1953.\n[8] John R Hershey, Zhuo Chen, Jonathan Le Roux, and Shinji Watanabe, \u201cDeep clustering: Discriminative embeddings for segmentation and separation,\u201d in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 31\u2013 35.\n[9] Dong Yu, Morten Kolb\u00e6k, Zheng-Hua Tan, and Jesper Jensen, \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d arXiv preprint arXiv:1607.00325, 2016.\n[10] Yusuf Isik, Jonathan Le Roux, Zhuo Chen, Shinji Watanabe, and John R Hershey, \u201cSingle-channel multi-speaker separation using deep clustering,\u201d arXiv preprint arXiv:1607.02173, 2016.\n[11] Patricia K. Kuhl, \u201cHuman adults and human infants show a perceptual magnet effect,\u201d Perception & psychophysics, 50.2 (1991): 93-107.\n[12] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, \u201cNeural machine translation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014.\n[13] Kyunghyun Cho, Aaron Courville, and Yoshua Bengio, \u201cDescribing multimedia content using attention-based encoder-decoder networks,\u201d IEEE Transactions on Multimedia, vol. 17, no. 11, pp. 1875\u20131886, 2015.\n[14] Sepp Hochreiter and Ju\u0308rgen Schmidhuber, \u201cLong shortterm memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[15] Tijmen Tieleman and Geoffrey Hinton, \u201cLecture 6.5- rmsprop,\u201d COURSERA: Neural networks for machine learning, 2012.\n[16] \u201cnaplab.ee.columbia.edu/anet,\u201d ."}], "references": [{"title": "Contextdependent pre-trained deep neural networks for largevocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Trans. on Audio, Speech and Language Processing, 2012, vol. 20.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["S. Has\u0306im", "Senior A.", "F. Beaufays"], "venue": "Interspeech 2014. ISCA, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Acoustic modelling with cd-ctc-smbr lstm rnns", "author": ["A. Senior", "S. Has\u0306im", "F.C. Quitry", "T.N. Sainath", "K. Rao"], "venue": "ASRU 2015. IEEE, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech enhancement and recognition using multi-task learning of long short-term memory recurrent neural networks", "author": ["Zhuo Chen", "Shinji Watanabe", "Hakan Erdo\u011fan", "John R Hershey"], "venue": "ISCA, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "The merl/sri system for the 3rd chime challenge using beamforming, robust feature extraction, and advanced speech recognition", "author": ["Takaaki Hori", "Zhuo Chen", "Hakan Erdogan", "John R Hershey", "Jonathan Le Roux", "Vikramjit Mitra", "Shinji Watanabe"], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 475\u2013481.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech enhancement based on deep denoising autoencoder", "author": ["X. Lu", "Y. Tsao", "S. Matsuda", "C. Hori"], "venue": "2013, pp. 436\u2013440.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Some experiments on the recognition of speech, with one and with two ears", "author": ["E Colin Cherry"], "venue": "The Journal of the acoustical society of America, vol. 25, no. 5, pp. 975\u2013979, 1953.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1953}, {"title": "Deep clustering: Discriminative embeddings for segmentation and separation", "author": ["John R Hershey", "Zhuo Chen", "Jonathan Le Roux", "Shinji Watanabe"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 31\u2013 35.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Permutation invariant training of deep models for speaker-independent multi-talker speech separation", "author": ["Dong Yu", "Morten Kolb\u00e6k", "Zheng-Hua Tan", "Jesper Jensen"], "venue": "arXiv preprint arXiv:1607.00325, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Single-channel multi-speaker separation using deep clustering", "author": ["Yusuf Isik", "Jonathan Le Roux", "Zhuo Chen", "Shinji Watanabe", "John R Hershey"], "venue": "arXiv preprint arXiv:1607.02173, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Human adults and human infants show a perceptual magnet effect", "author": ["Patricia K. Kuhl"], "venue": "Perception & psychophysics, 50.2 (1991): 93-107.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1991}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Describing multimedia content using attention-based encoder-decoder networks", "author": ["Kyunghyun Cho", "Aaron Courville", "Yoshua Bengio"], "venue": "IEEE Transactions on Multimedia, vol. 17, no. 11, pp. 1875\u20131886, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1875}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Lecture 6.5rmsprop", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural networks for machine learning, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Despite the recent advances in deep learning methods for various speech processing tasks such as automatic recognition[1, 2, 3], enhancement[4, 5, 6], speech separation remains unresolved.", "startOffset": 118, "endOffset": 127}, {"referenceID": 1, "context": "Despite the recent advances in deep learning methods for various speech processing tasks such as automatic recognition[1, 2, 3], enhancement[4, 5, 6], speech separation remains unresolved.", "startOffset": 118, "endOffset": 127}, {"referenceID": 2, "context": "Despite the recent advances in deep learning methods for various speech processing tasks such as automatic recognition[1, 2, 3], enhancement[4, 5, 6], speech separation remains unresolved.", "startOffset": 118, "endOffset": 127}, {"referenceID": 3, "context": "Despite the recent advances in deep learning methods for various speech processing tasks such as automatic recognition[1, 2, 3], enhancement[4, 5, 6], speech separation remains unresolved.", "startOffset": 140, "endOffset": 149}, {"referenceID": 4, "context": "Despite the recent advances in deep learning methods for various speech processing tasks such as automatic recognition[1, 2, 3], enhancement[4, 5, 6], speech separation remains unresolved.", "startOffset": 140, "endOffset": 149}, {"referenceID": 5, "context": "Despite the recent advances in deep learning methods for various speech processing tasks such as automatic recognition[1, 2, 3], enhancement[4, 5, 6], speech separation remains unresolved.", "startOffset": 140, "endOffset": 149}, {"referenceID": 6, "context": "Two main difficulties hinder the efficacy of deep learning algorithms to tackle the famous \u201ccocktail party problem\u201d[7].", "startOffset": 115, "endOffset": 118}, {"referenceID": 7, "context": "The first difficulty is referred as the \u201cpermutation problem\u201d [8].", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": "Two deep learning based methods have been proposed to resolve these problems, which are known as \u201ddeep clustering (DC)[8]\u201d and \u201dpermutation invariant training (PIT)[9]\u201d.", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "Two deep learning based methods have been proposed to resolve these problems, which are known as \u201ddeep clustering (DC)[8]\u201d and \u201dpermutation invariant training (PIT)[9]\u201d.", "startOffset": 164, "endOffset": 167}, {"referenceID": 9, "context": "Minimizing the separation error is done with an unfolding clustering system and a second network, which is trained iteratively and stage by stage to ensure convergence [10].", "startOffset": 168, "endOffset": 172}, {"referenceID": 7, "context": "PIT was first proposed in [8], and was later shown to have comparable performance as DC [9].", "startOffset": 26, "endOffset": 29}, {"referenceID": 8, "context": "PIT was first proposed in [8], and was later shown to have comparable performance as DC [9].", "startOffset": 88, "endOffset": 91}, {"referenceID": 10, "context": "The term attractor refers to the well-studied perceptual effects in human speech perception which suggest that the brain circuits create perceptual attractors (magnets) that warp the stimulus space such that to draws the sound that is closest to it, a phenomenon that is called Perceptual Magnet Effect [11].", "startOffset": 303, "endOffset": 307}, {"referenceID": 0, "context": "This particular metric uses the inner product followed by a sigmoid function which monotonically scales the masks between [0, 1].", "startOffset": 122, "endOffset": 128}, {"referenceID": 11, "context": "Alternatively, a neural network model may also be used to pick the representative embedding for each source, an idea which shares similarities with encoder-decoder attention networks [12, 13].", "startOffset": 183, "endOffset": 191}, {"referenceID": 12, "context": "Alternatively, a neural network model may also be used to pick the representative embedding for each source, an idea which shares similarities with encoder-decoder attention networks [12, 13].", "startOffset": 183, "endOffset": 191}, {"referenceID": 3, "context": "On the other hand, when the attractor vectors are considered as free parameters in the network, DANet reduces to a classification network [4, 5], and Equation 1 becomes a fullyconnected layer.", "startOffset": 138, "endOffset": 144}, {"referenceID": 4, "context": "On the other hand, when the attractor vectors are considered as free parameters in the network, DANet reduces to a classification network [4, 5], and Equation 1 becomes a fullyconnected layer.", "startOffset": 138, "endOffset": 144}, {"referenceID": 7, "context": "We use the corpus introduced in [8], which contains a 30 h training set and a 10 h validation set generated by randomly selecting utterances from different speakers in the Wall Street Journal (WSJ0) training set si tr s, and mixing them at various signal-to-noise ratios (SNR) randomly chosen between 0 dB and 10 dB.", "startOffset": 32, "endOffset": 35}, {"referenceID": 13, "context": "The network contains 4 Bi-directional LSTM [14] layers with 600 hidden units in each layer.", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "RMSprop algorithm [15] is used for training with an exponential learning rate decaying strategy, where the learning rate starts at 10\u22124 and ends at 3\u00d7 10\u22126.", "startOffset": 18, "endOffset": 22}], "year": 2016, "abstractText": "Despite the overwhelming success of deep learning in various speech processing tasks, the problem of separating simultaneous speakers in a mixture remains challenging. Two major difficulties in such systems are the arbitrary source permutation and unknown number of sources in the mixture. We propose a novel deep learning framework for single channel speech separation by creating attractor points in high dimensional embedding space of the acoustic signals which pull together the time-frequency bins corresponding to each source. Attractor points in this study are created by finding the centroids of the sources in the embedding space, which are subsequently used to determine the similarity of each bin in the mixture to each source. The network is then trained to minimize the reconstruction error of each source by optimizing the embeddings. The proposed model is different from prior works in that it implements an end-to-end training, and it does not depend on the number of sources in the mixture. Two strategies are explored in the test time, K-means and fixed attractor points, where the latter requires no post-processing and can be implemented in real-time. We evaluated our system on Wall Street Journal dataset and show 5.49% improvement over the previous state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}