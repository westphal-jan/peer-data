{"id": "1611.08944", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Nonparametric General Reinforcement Learning", "abstract": "Reinforcement learning (RL) problems are often phrased in terms of Markov decision processes (MDPs). In this thesis we go beyond MDPs and consider RL in environments that are non-Markovian, non-ergodic and only partially observable. Our focus is not on practical algorithms, but rather on the fundamental underlying problems: How do we balance exploration and exploitation? How do we explore optimally? When is an agent optimal? We follow the nonparametric realizable paradigm. We discuss the practical consequences of learning how to avoid the mis-conception of an agent with a single set of parameters. As such, we seek to understand the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences of learning the practical consequences", "histories": [["v1", "Mon, 28 Nov 2016 00:36:40 GMT  (195kb,D)", "http://arxiv.org/abs/1611.08944v1", "PhD thesis"]], "COMMENTS": "PhD thesis", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jan leike"], "accepted": false, "id": "1611.08944"}, "pdf": {"name": "1611.08944.pdf", "metadata": {"source": "META", "title": "Nonparametric General Reinforcement Learning", "authors": ["Jan Leike"], "emails": [], "sections": [{"heading": null, "text": "Nonparametric General Reinforcement Learning\nJan Leike\nA thesis submitted for the degree of Doctor of Philosophy\nat the Australian National University\nNovember 2016\nar X\niv :1\n61 1.\n08 94\n4v 1\n[ cs\n.A I]\n2 8\nN ov\n2 01\n6\n\u00a9 Jan Leike\nThis work is licensed under the Creative Commons Attribution 4.0 International License\nNo reinforcement learners were harmed in the making of this thesis.\nExcept where otherwise indicated, this thesis is my own original work.\nJan Leike 29 November 2016"}, {"heading": "Acknowledgements", "text": "There are many without whom this thesis would not have been possible. I sincerely hope that this page is not the way they learn how grateful I am to them. I thank in particular . . .\n. . . first and foremost, Marcus Hutter: he is an amazing supervisor; always very supportive of my (unusual) endeavors, spent countless hours reading my drafts with a impressive attention to detail. I am also grateful to him for forcing me to be absolutely rigorous in my mathematical arguments, and, of course, for developing the theory of universal AI without which this thesis would not have existed. I could not have picked a better supervisor.\n. . . the Australian National University for granting me scholarships that let me pursue my academic interests unrestricted and without any financial worries.\n. . . Csaba Szepesv\u00e1ri and the University of Alberta for hosting me for three months.\n. . . Matthias Heizmann and the University of Freiburg for hosting me while I was traveling in Europe.\n. . . the Machine Intelligence Research Institute for enabling me to run MIRIx research workshops.\n. . . CCR, UAI, Google DeepMind, ARC, MIRI, and FHI for supporting my travel.\n. . . Tor Lattimore for numerous explanations, discussions, and pointers that left me with a much deeper understanding of the theory of reinforcement learning.\n. . . Laurent Orseau for interesting discussions, encouragement, and for sharing so many intriguing ideas.\n. . . my fellow students: Mayank Daswani, Tom Everitt, Daniel Filan, Roshan Shariff, Tian Kruger, Emily Cutts Worthington, Buck Shlegeris, Jarryd Martin, John Aslanides, Alexander Mascolo, and Sultan Javed for so many interesting discussions and for being awesome friends. I especially thank Daniel, Emily, Mayank, and Buck for encouraging me to read more of Less Wrong and Slate Star Codex.\n. . . Tosca Lechner for studying statistics with me despite so many scheduling difficulties across all these time zones.\n. . . Tom Sterkenburg, Christian Kamm, Alexandra Surdina, Freya Fleckenstein, Peter Sunehag, Tosca Lechner, Ines Nikolaus, Laurent Orseau, John Aslanides, and especially Daniel Filan for proofreading parts of this thesis.\n. . . the CSSA for being a lovely bunch that made my stay in Australia feel less isolated.\n. . . my family for lots of love and support, and for tolerating my long absences from Europe.\nAbstract\nReinforcement learning problems are often phrased in terms of Markov decision processes (MDPs). In this thesis we go beyond MDPs and consider reinforcement learning in environments that are non-Markovian, non-ergodic and only partially observable. Our focus is not on practical algorithms, but rather on the fundamental underlying problems: How do we balance exploration and exploitation? How do we explore optimally? When is an agent optimal? We follow the nonparametric realizable paradigm: we assume the data is drawn from an unknown source that belongs to a known countable class of candidates.\nFirst, we consider the passive (sequence prediction) setting, learning from data that is not independent and identically distributed. We collect results from artificial intelligence, algorithmic information theory, and game theory and put them in a reinforcement learning context: they demonstrate how an agent can learn the value of its own policy.\nNext, we establish negative results on Bayesian reinforcement learning agents, in particular AIXI. We show that unlucky or adversarial choices of the prior cause the agent to misbehave drastically. Therefore Legg-Hutter intelligence and balanced Pareto optimality, which depend crucially on the choice of the prior, are entirely subjective. Moreover, in the class of all computable environments every policy is Pareto optimal. This undermines all existing optimality properties for AIXI.\nHowever, there are Bayesian approaches to general reinforcement learning that satisfy objective optimality guarantees: We prove that Thompson sampling is asymptotically optimal in stochastic environments in the sense that its value converges to the value of the optimal policy. We connect asymptotic optimality to regret given a recoverability assumption on the environment that allows the agent to recover from mistakes. Hence Thompson sampling achieves sublinear regret in these environments.\nAIXI is known to be incomputable. We quantify this using the arithmetical hierarchy, and establish upper and corresponding lower bounds for incomputability. Further, we show that AIXI is not limit computable, thus cannot be approximated using finite computation. However there are limit computable \u03b5-optimal approximations to AIXI. We also derive computability bounds for knowledge-seeking agents, and give a limit computable weakly asymptotically optimal reinforcement learning agent.\nFinally, our results culminate in a formal solution to the grain of truth problem: A Bayesian agent acting in a multi-agent environment learns to predict the other agents\u2019 policies if its prior assigns positive probability to them (the prior contains a grain of truth). We construct a large but limit computable class containing a grain of truth and show that agents based on Thompson sampling over this class converge to play \u03b5-Nash equilibria in arbitrary unknown computable multi-agent environments.\nix\nx Keywords. Bayesian methods, sequence prediction, merging, general reinforcement learning, universal artificial intelligence, AIXI, Thompson sampling, knowledge-seeking agents, Pareto optimality, intelligence, asymptotic optimality, computability, reflective oracle, grain of truth problem, Nash equilibrium.\nContents\nTitle Page i\nAbstract ix\nContents xiii\nList of Figures xv\nList of Tables xvii"}, {"heading": "1 Introduction 1", "text": "1.1 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.1.1 Narrow Reinforcement Learning . . . . . . . . . . . . . . . . . . . 3 1.1.2 Deep Q-Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.1.3 General Reinforcement Learning . . . . . . . . . . . . . . . . . . 6\n1.2 Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.3 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11"}, {"heading": "2 Preliminaries 15", "text": "2.1 Measure Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.2 Stochastic Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.3 Information Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.4 Algorithmic Information Theory . . . . . . . . . . . . . . . . . . . . . . 20"}, {"heading": "3 Learning 23", "text": "3.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.2 Compatibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 3.3 Martingales . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.4 Merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n3.4.1 Strong Merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 3.4.2 Weak Merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 3.4.3 Almost Weak Merging . . . . . . . . . . . . . . . . . . . . . . . . 34\n3.5 Predicting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 3.5.1 Dominance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 3.5.2 Absolute Continuity . . . . . . . . . . . . . . . . . . . . . . . . . 38 3.5.3 Dominance with Coefficients . . . . . . . . . . . . . . . . . . . . . 40 3.6 Learning with Algorithmic Information Theory . . . . . . . . . . . . . . 41 3.6.1 Solomonoff Induction . . . . . . . . . . . . . . . . . . . . . . . . . 41\nxi\nxii Contents\n3.6.2 The Speed Prior . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.6.3 Universal Compression . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44"}, {"heading": "4 Acting 49", "text": "4.1 The General Reinforcement Learning Problem . . . . . . . . . . . . . . . 50\n4.1.1 Discounting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 4.1.2 Implicit Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . 53 4.1.3 Typical Environment Classes . . . . . . . . . . . . . . . . . . . . 54\n4.2 The Value Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 4.2.1 Optimal Policies . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 4.2.2 Properties of the Value Function . . . . . . . . . . . . . . . . . . 58 4.2.3 On-Policy Value Convergence . . . . . . . . . . . . . . . . . . . . 59 4.3 The Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 4.3.1 Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 4.3.2 Knowledge-Seeking Agents . . . . . . . . . . . . . . . . . . . . . . 63 4.3.3 BayesExp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 4.3.4 Thompson Sampling . . . . . . . . . . . . . . . . . . . . . . . . . 65"}, {"heading": "5 Optimality 67", "text": "5.1 Pareto Optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 5.2 Bad Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n5.2.1 The Indifference Prior . . . . . . . . . . . . . . . . . . . . . . . . 70 5.2.2 The Dogmatic Prior . . . . . . . . . . . . . . . . . . . . . . . . . 71 5.2.3 The G\u00f6del Prior . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n5.3 Bayes Optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 5.4 Asymptotic Optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n5.4.1 Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 5.4.2 BayesExp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 5.4.3 Thompson Sampling . . . . . . . . . . . . . . . . . . . . . . . . . 84 5.4.4 Almost Sure in Ces\u00e0ro Average vs. in Mean . . . . . . . . . . . . 89\n5.5 Regret . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 5.5.1 Sublinear Regret in Recoverable Environments . . . . . . . . . . 91 5.5.2 Regret of the Optimal Policy and Thompson sampling . . . . . . 95 5.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 5.6.1 The Optimality of AIXI . . . . . . . . . . . . . . . . . . . . . . . 96 5.6.2 Natural Universal Turing Machines . . . . . . . . . . . . . . . . . 97 5.6.3 Asymptotic Optimality . . . . . . . . . . . . . . . . . . . . . . . . 98 5.6.4 The Quest for Optimality . . . . . . . . . . . . . . . . . . . . . . 99"}, {"heading": "6 Computability 101", "text": "6.1 Background on Computability . . . . . . . . . . . . . . . . . . . . . . . . 103\n6.1.1 The Arithmetical Hierarchy . . . . . . . . . . . . . . . . . . . . . 103 6.1.2 Computability of Real-valued Functions . . . . . . . . . . . . . . 103\nContents xiii\n6.2 The Complexity of Solomonoff Induction . . . . . . . . . . . . . . . . . . 105 6.3 The Complexity of AINU, AIMU, and AIXI . . . . . . . . . . . . . . . . 108\n6.3.1 Upper Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 6.3.2 Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Iterative Value Function . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 6.5 The Complexity of Knowledge-Seeking . . . . . . . . . . . . . . . . . . . 122 6.6 A Limit Computable Weakly Asymptotically Optimal Agent . . . . . . . 122 6.7 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123"}, {"heading": "7 The Grain of Truth Problem 127", "text": "7.1 Reflective Oracles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n7.1.1 Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 7.1.2 A Limit Computable Reflective Oracle . . . . . . . . . . . . . . . 131 7.1.3 Proof of Theorem 7.7 . . . . . . . . . . . . . . . . . . . . . . . . . 132\n7.2 A Grain of Truth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 7.2.1 Reflective Bayesian Agents . . . . . . . . . . . . . . . . . . . . . 135 7.2.2 Reflective-Oracle-Computable Policies . . . . . . . . . . . . . . . 136 7.2.3 Solution to the Grain of Truth Problem . . . . . . . . . . . . . . 137 7.3 Multi-Agent Environments . . . . . . . . . . . . . . . . . . . . . . . . . . 137 7.4 Informed Reflective Agents . . . . . . . . . . . . . . . . . . . . . . . . . 140 7.5 Learning Reflective Agents . . . . . . . . . . . . . . . . . . . . . . . . . . 141 7.6 Impossibility Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143 7.7 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144"}, {"heading": "8 Conclusion 147", "text": "Measures and Martingales 151\nBibliography 155\nList of Notation 171\nIndex 175\nxiv Contents\nList of Figures\n1.1 Selection of Atari 2600 video games . . . . . . . . . . . . . . . . . . . . . 13\n3.1 Properties of learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n4.1 The dualistic agent model . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n5.1 Legg-Hutter intelligence measure . . . . . . . . . . . . . . . . . . . . . . 77 5.2 Relationship between different types of asymptotic optimality . . . . . . 80\n6.1 Definition of conditional M as a \u220602-formula . . . . . . . . . . . . . . . . 106 6.2 Environment from the proof of Theorem 6.15 . . . . . . . . . . . . . . . 111 6.3 Environment from the proof of Theorem 6.16 . . . . . . . . . . . . . . . 113 6.4 Environment from the proof of Theorem 6.17 . . . . . . . . . . . . . . . 114 6.5 Environment from the proof of Proposition 6.19 . . . . . . . . . . . . . . 117 6.6 Environment from the proof of Theorem 6.22 . . . . . . . . . . . . . . . 119 6.7 Environment from the proof of Theorem 6.23 . . . . . . . . . . . . . . . 121\n7.1 Answer options of a reflective oracle . . . . . . . . . . . . . . . . . . . . 131 7.2 The multi-agent model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\nxv\nxvi LIST OF FIGURES\nList of Tables\n1.1 Assumptions in reinforcement learning . . . . . . . . . . . . . . . . . . . 7 1.2 List of publications by chapter . . . . . . . . . . . . . . . . . . . . . . . 11 1.3 List of publications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3.1 Examples of learning distributions . . . . . . . . . . . . . . . . . . . . . 45 3.2 Summary on properties of learning . . . . . . . . . . . . . . . . . . . . . 45\n4.1 Discount functions and their effective horizons . . . . . . . . . . . . . . . 53\n5.1 Types of asymptotic optimality . . . . . . . . . . . . . . . . . . . . . . . 79 5.2 Compiler sizes of the UTMs of bad priors . . . . . . . . . . . . . . . . . 98 5.3 Notions of optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n6.1 Computability results on Solomonoff\u2019s prior . . . . . . . . . . . . . . . . 102 6.2 Computability results for different agent models . . . . . . . . . . . . . . 102 6.3 Computability of real-valued functions . . . . . . . . . . . . . . . . . . . 104 6.4 Computability results for the iterative value function . . . . . . . . . . . 116\n7.1 Terminology dictionary between reinforcement learning and game theory. 128\nxvii\nxviii LIST OF TABLES\nChapter 1\nIntroduction\nEverything I did was for the glamor, the money, and the sex. \u2014 Albert Einstein\nAfter the early enthusiastic decades, research in artificial intelligence (AI) now mainly aims at specific domains: playing games, mining data, processing natural language, recognizing objects in images, piloting robots, filtering email, and many others (Russell and Norvig, 2010). Progress on particular domains has been remarkable, with several high-profile breakthroughs: The chess world champion Garry Kasparov was defeated by the computer program Deep Blue in 1997 (IBM, 2012a). In 2011 the world\u2019s best Jeopardy! players were defeated by the computer program Watson (IBM, 2012b). As of 2014 Google\u2019s self-driving cars completed over a million kilometers autonomously on public roads (Google, 2014). Finally, in 2016 Google DeepMind\u2019s AlphaGo beat Lee Sedol, one of the world\u2019s best players, at the board game Go (Google, 2016).\nWhile these advancements are very impressive, they are highly-specialized algorithms tailored to their domain of expertise. Outside that domain these algorithms perform very poorly: AlphaGo cannot play chess, Watson cannot drive a car, and DeepBlue cannot answer natural language queries. Solutions in one domain typically do not generalize to other domains and no single algorithm performs well in more than one of them. We classify these kinds of algorithms as narrow AI .\nThis thesis is not about narrow AI. We expect progress on narrow AI to continue and even accelerate, taking the crown of human superiority in domain after domain. But this is not the ultimate goal of artificial intelligence research. The ultimate goal is to engineer a mind\u2014to build a machine that can learn to do all tasks that humans can do, at least as well as humans do them. We call such a machine human-level AI (HLAI) if it performs at human level and strong AI if it surpasses human level. This thesis is about strong AI.\nThe goal of developing HLAI has a long tradition in AI research and was explicitly part of the 1956 Dartmouth conference that gave birth to the field of AI (McCarthy et al., 1955):\nWe propose that a 2 month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An\n1"}, {"heading": "2 Introduction", "text": "attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.\nIn hindsight this proposal reads vastly overconfident, and disappointment was inevitable. Making progress on these problems turned out to be a lot harder than promised, and over the last decades any discussion of research targeting HLAI has been avoided by serious researchers in the field. This void was filled mostly by crackpots, which tainted the reputation of HLAI research even further. However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously. Even more: the explicit motto of Google DeepMind, one of today\u2019s leading AI research centers, is to \u201csolve intelligence.\u201d"}, {"heading": "1.1 Reinforcement Learning", "text": "The best formal model for strong AI we currently have is reinforcement learning (RL). Reinforcement learning studies algorithms that learn to act in an unknown environment through trial and error (Sutton and Barto, 1998; Szepesv\u00e1ri, 2010; Wiering and van Otterlo, 2012). Without knowing the structure of the environments or the goal, an agent has to learn what to do through the carrot-and-stick approach: it receives a reward in form of a numeric feedback signifying how well it is currently doing; from this signal the agent has to figure out autonomously what to do. More specifically, in a general reinforcement learning problem an agent interacts sequentially with an unknown environment : in every time step the agent chooses an action and receives a percept consisting of an observation and a real-valued reward . The sequence of past actions and percepts is the history . The goal in reinforcement learning is to maximize cumulative (discounted) rewards (this setup is described formally in Section 4.1).\nA central problem in reinforcement learning is the balance between exploration and exploitation: should the agent harvest rewards in the regions of the environment that it currently knows (exploitation) or try discovering more profitable regions (exploration)? Exploration is costly and dangerous: it forfeits rewards that could be had right now, and it might lead into traps from which the agent cannot recover. However, exploration may pay off in the long run. Generally, it is not clear how to make this tradeoff (see Section 5.6).\nReinforcement learning algorithms can be categorized by whether they learn onpolicy or off-policy . Learning on-policy means learning the value of the policy that the agent currently follows. Typically, the policy is slowly improved while learning, like SARSA (Sutton and Barto, 1998). In contrast, learning off-policy means following one policy but learning the value of another policy (typically the optimal policy), like Q-learning (Watkins and Dayan, 1992). Off-policy methods are more difficult to handle\n\u00a71.1 Reinforcement Learning 3\nin practice (see the discussion on function approximation below) but tend to be more data-efficient since samples from an old policy do not have to be discarded.\nReinforcement learning has to be distinguished from planning . In a planning problem we are provided with the true environment and are tasked with finding an optimal policy. Mathematically it is clear what the optimal policy is, the difficulty stems from finding a reasonable solution with limited computation. Reinforcement learning is fundamentally more difficult because the true environment is unknown and has to be learned from observation. This enables two approaches: we could learn a model of the true environment and then use planning techniques within that model; this is the modelbased approach. Alternatively, we could learn an optimal policy directly or through an intermediate quantity (typically the value function); this is the model-free approach. Model-based methods tend to be more data-efficient but also computationally more expensive. Therefore most algorithms used in practice (Q-learning and SARSA) are model-free."}, {"heading": "1.1.1 Narrow Reinforcement Learning", "text": "In the reinforcement learning literature it is typically assumed that the environment is a Markov decision process (MDP), i.e., the next percept only depends on the last percept and action and is independent of the rest of the history (see Section 4.1.3). In an MDP, percepts are usually called states. This setting is well-analyzed (Puterman, 2014; Bertsekas and Tsitsiklis, 1995; Sutton and Barto, 1998), and there is a variety of algorithms that are known to learn the MDP asymptotically, such as TD learning (Sutton, 1988) and Q-learning (Watkins and Dayan, 1992).\nMoreover, for MDPs various learning guarantees have been proved in the literature. First, there are bounds on the agent\u2019s regret , the difference between the obtained rewards and the rewards of the optimal policy. Auer et al. (2009) derive the regret bound O\u0303(dS \u221a At) for ergodic MDPs where d is the diameter of the MDP (how many steps a policy needs on average to get from one state of the MDP to any other), S is the number of states, A is the number of actions, and t is the number of time steps the algorithm runs. Second, given \u03b5 and \u03b4, a reinforcement learning algorithm is said to have sample complexity C(\u03b5, \u03b4) iff it is \u03b5-suboptimal for at most C(\u03b5, \u03b4) time steps with probability at least 1 \u2212 \u03b4 (probably approximately correct, PAC). For MDPs the first sample complexity bounds were due to Kakade (2003). Lattimore and Hutter (2012) use the algorithm UCRL\u03b3 (Auer et al., 2009) with geometric discounting with discount rate \u03b3 and derive the currently best-known PAC bound of O\u0303(\u2212T/(\u03b52(1 \u2212 \u03b3)3) log \u03b4) where T is the number of non-zero transitions in the MDP.\nTypically, algorithms for MDPs rely on visiting every state multiple times (or even infinitely often), which becomes infeasible for large state spaces (e.g. a video game screen consisting of millions of pixels). In these cases, function approximation can be used to learn an approximation to the value function (Sutton and Barto, 1998). Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995). A recent breakthrough was made by Mahmood et al. (2015) and Yu (2015)"}, {"heading": "4 Introduction", "text": "with their emphatic TD algorithm that converges off-policy. For nonlinear function approximation no convergence guarantee is known.\nAmong the historical successes of reinforcement learning is autonomous helicopter piloting (Kim et al., 2003) and TD-Gammon, a backgammon algorithm that learned through self-play (Tesauro, 1995), similar to AlphaGo (Silver et al., 2016)."}, {"heading": "1.1.2 Deep Q-Networks", "text": "The current state of the art in reinforcement learning challenges itself to playing simple video games. Video games are an excellent benchmark because they come readily with the reward structure provided: the agent\u2019s rewards are the change in the game score. Without prior knowledge of any aspect of the game, the agent needs to learn to score as many points in the game as possible from looking only at raw pixel data (sometimes after some preprocessing).\nThis approach to general AI is in accordance with the definition of intelligence given by Legg and Hutter (2007b):\nIntelligence measures an agent\u2019s ability to achieve goals in a wide range of environments.\nIn reinforcement learning the definition of the goal is very flexible, and provided by the rewards. Moreover, a diverse selection of video games arguably constitutes a \u2018wide range of environments.\u2019\nA popular such selection is the Atari 2600 video game console (Bellemare et al., 2013). There are hundreds of games released for this platform, with very diverse challenges: top-down shooting games such as Space Invaders, ball games such as Pong, agility-based games such as Boxing or Gopher, tactical games such as Ms. Pac-Man, and maze games such as Montezuma\u2019s Revenge. An overview over some of the games is given in Figure 1.1 on page 13.\nMnih et al. (2013, 2015) introduce the deep Q-network (DQN) algorithm, combining Q-learning with nonlinear function approximation through convolutional neural networks. DQN achieves 75% of the performance of a human game tester on 29 of 49 Atari games. The two innovations that made this breakthrough possible are (1) using a not so recent target Q-function in the TD update and (2) experience replay. For experience replay, a set of recent state transitions is retained and the network is regularly retrained on random samples from these old transitions.1\nDQN rides the wave of success of deep learning (LeCun et al., 2015; Schmidhuber, 2015; Goodfellow et al., 2016). Deep learning refers to the training of artificial neural networks with several layers. This allows them to automatically learn higher-level abstractions from data. Deep neural networks are conceptionally simple and have been studied since the inception of AI; only recently has computation power become cheap enough to train them effectively. Recently deep neural networks have taken the top of the machine learning benchmarks by storm (LeCun et al., 2015, and references therein):\n1The slogan for experience replay should be \u2018regularly retrained randomly on retained rewards\u2019.\n\u00a71.1 Reinforcement Learning 5\nThese methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics.\nSince the introduction of DQN there have been numerous improvements on this algorithm: increasing the gap on the Q-values of different actions (Bellemare et al., 2016), training in parallel (Nair et al., 2015; Mnih et al., 2016), improvements to the experience replay mechanism (Schaul et al., 2016), generalization to continuous action spaces (Lillicrap et al., 2016), solve the overestimation problem (van Hasselt et al., 2016), and improvements to the neural network architecture (Wang et al., 2016). TheQvalues learned by DQN\u2019s neural networks are intransparent to inspection; Zahavy et al. (2016) use visualization techniques on the Q-value networks. Finally, Liang et al. (2016) managed to reproduce DQN\u2019s success using only linear function approximation (no neural networks). The key is a selection of features similar to the ones produced by DQN\u2019s convolutional neural networks.\nRegardless of its success, the DQN algorithm fundamentally falls short of the requirements for strong AI: Q-learning with function approximation is targeted at solving large-state (fully observable) Markov decision processes. In particular, it does not address the following challenges.\n\u2022 Partial observability. All games in the ATARI framework are fully observable (except for Montezuma\u2019s revenge): all information relevant to the state of the game is visible on the screen at all times (when using the four most recent frames).\nHowever, the real world is only partially observable. For example, when going to the supermarket you have to remember what you wanted to buy because you currently cannot observe which items you are missing at home. A strong AI needs to have memory and be able to remember things that happened in the past (rather than only learning from it).\nAn obvious approach to equip DQN with memory is to use recurrent neural networks instead of simple feedforward neural networks (Heess et al., 2015). Hausknecht and Stone (2015) show that this enables the agent to play the games when using only a single frame as input. However, it is currently unclear whether recurrent neural networks are powerful enough to learn long-term dependencies in the data (Bengio et al., 1994).\n\u2022 Directed exploration. DQN fails in games with delayed rewards. For example, in Montezuma\u2019s Revenge the agent needs to avoid several obstacles to get to a key before receiving the first reward. DQN fails to score any rewards in this environment. This is not surprising: the typical approach for reinforcement learning, to use \u03b5-exploration for which the agent chooses actions at random with a certain probability, is insufficient for exploring complex environments; the probability of random walking into the first reward is just too low.\nInstead we need a more targeted exploration approach that aims at understanding the environment in a structured manner. Theoretical foundations are provided"}, {"heading": "6 Introduction", "text": "by knowledge-seeking agents (Orseau, 2011, 2014a; Orseau et al., 2013). Kulkarni et al. (2016) introduce a hierarchical approach based on intrinsic motivation to improve DQN\u2019s exploration and manage to score points in Montezuma\u2019s Revenge. However, their approach relies on quite a bit of visual preprocessing and domain knowledge.\n\u2022 Non-ergodicity. When losing in an Atari game, the agent always gets to play the same game again. From the agent\u2019s perspective, it has not actually failed, it just gets transported back to the starting state. Because of this, there are no strong incentives to be careful when exploring the environment: there can be no bad mistakes that make recovery impossible.\nHowever, in the real world some actions are irreversibly bad. If the robot drives off a cliff it can be fatally damaged and cannot learn from the mistake. The real world is full of potentially fatal mistakes (e.g. crossing the street at the wrong time) and for humans, natural reflexes and training by society make sure that we are very confident of what situations to avert. This is crucial, as some mistakes must be avoided without any training examples. Current reinforcement learning algorithms only learn about bad states by visiting them.\n\u2022 Wireheading. The goal of reinforcement learning is to maximize rewards. When playing a video game the most efficient way to get rewards is to increase the game score. However, when a reinforcement learning algorithm is acting in the real world, theoretically it can change its own hard- and software. In this setting, the most efficient way to get rewards is to modify the reward mechanism to always provide the maximal reward (Omohundro, 2008; Ring and Orseau, 2011; Bostrom, 2014). Consequently the agent no longer pursues the designers\u2019 originally intended goals and instead only attempts to protect its own existence. The name wireheading was established by analogy to a biology experiment by Olds and Milner (1954) in which rats had a wire embedded into the reward center of their brain that they could then stimulate by the push of a button.\nToday\u2019s reinforcement learning algorithms usually do not have access to their own internal workings, but more importantly they are not smart enough to understand their own architecture. They simply lack the capability to wirehead. But as we increase their capability, wireheading will increasingly become a challenge for reinforcement learning."}, {"heading": "1.1.3 General Reinforcement Learning", "text": "A theory of strong AI cannot make some of the typical assumptions. Environments are partially observable, so we are dealing with partially observable Markov decision processes (POMDPs). The POMDP\u2019s state space does not need to be finite. Moreover, the environment may not allow recovery from mistakes: we do not assume ergodicity or weak communication (not every POMDP state has to be reachable from every other state). So in general, our environments are infinite-state non-ergodic POMDPs. Table 1.1 lists the assumptions that are typical but we do not make.\nLearning POMDPs is a lot harder, and only partially successful attempts have been made: through predictive state representations (Singh et al., 2003, 2004), and Bayesian methods (Doshi-Velez, 2012). A general approach is feature reinforcement learning (Hutter, 2009c,d), which aims to reduce the general reinforcement learning problem to an MDP by aggregating histories into states. The quest for a good cost function for feature maps remains unsuccessful thus far (Sunehag and Hutter, 2010; Daswani, 2015). However, Hutter (2014) managed to derive strong bounds relating the optimal value function of the aggregated MDP to the value function of the original process even if the latter violates the Markov condition.\nA full theoretical approach to the general reinforcement learning problem is given by Hutter (2000, 2001a, 2002a, 2003, 2005, 2007a, 2012b). He introduces the Bayesian RL agent AIXI building on the theory of sequence prediction by Solomonoff (1964, 1978). Based in algorithmic information theory, Solomonoff\u2019s prior draws from famous insights by William of Ockham, Sextus Epicurus, Alan Turing, and Andrey Kolmogorov (Rathmanner and Hutter, 2011). AIXI uses Solomonoff\u2019s prior over the class of all computable environments and acts to maximize Bayes-expected rewards. We formally introduce Solomonoff\u2019s theory of induction in Chapter 3 and AIXI in Section 4.3.1. See also Legg (2008) for an accessible introduction to AIXI.\nA typical optimality property in general reinforcement learning is asymptotic optimality (Lattimore and Hutter, 2011): as time progresses the agent converges to achieve the same rewards as the optimal policy. Asymptotic optimality is usually what is meant by \u201cQ-learning converges\u201d (Watkins and Dayan, 1992) or \u201cTD learning converges\u201d (Sutton, 1988). Orseau (2010, 2013) showed that AIXI is not asymptotically optimal. Yet asymptotic optimality in the general setting can be achieved through optimism (Sunehag and Hutter, 2012a,b, 2015), Thompson sampling (Section 5.4.3), or an extra exploration component on top of AIXI (Lattimore, 2013, Ch. 5).\nIn our setting, learning the environment does not just involve learning a fixed finite set of parameters; the real world is too complicated to fit into a template. Therefore we fall back on the nonparametric approach where we start with an infinite but countable class of candidate environments. Our only assumption is that the true environment is contained in this class (the realizable case). As long as this class of environments is large enough (such as for the class of all computable environments), this assumption is\n8 Introduction\nrather weak."}, {"heading": "1.2 Contribution", "text": "The goal of this thesis is not to increase AI capability. As such, we are not trying to improve on the state of the art, and we are not trying to derive practical algorithms. Instead, the emphasis of this thesis is to further our understanding of general reinforcement learning and thus strong AI. How a future implementation of strong AI will actually work is in the realm of speculation at this time. Therefore we should make as few and as weak assumptions as possible.\nWe disregard computational constraints in order to focus on the fundamental underlying problems. This is unrealistic, of course. With unlimited computation power many traditional AI problems become trivial: playing chess, Go, or backgammon can be solved by exhaustive expansion of the game tree. But the general RL problem does not become trivial: the agent has to learn the environment and balance between exploration and exploitation. That being said, the algorithms that we study do have a relationship with algorithms being used in practice and our results can and should educate implementation.\nOn a high level, our insights can be viewed from three different perspectives.\n\u2022 Philosophically. Concisely, our understanding of strong AI can be summarized as follows.\nintelligence = learning + acting (1.1)\nHere, intelligence refers to an agent that optimizes towards some goal in accordance with the definition by Legg and Hutter (2007b). For learning we distinguish two (very related) aspects: (1) arriving at accurate beliefs about the future and (2) making accurate predictions about the future. Of course, the former implies the latter: if you have accurate beliefs, then you can also make good predictions. For RL accurate beliefs is what we care about because they enable us to plan for the future. Learning is a passive process that only observes the data and does not interfere with its generation. In particular, learning does not require a goal. With acting we mean the selection of actions in pursuit of some goal. This goal can be reward maximization as in reinforcement learning, understanding the environment as for knowledge-seeking agents, or something else entirely. Together they enable an agent to learn the environment\u2019s behavior in response to itself (on-policy learning) and to choose a policy that furthers its goal. We discuss the formal aspects of learning in Chapter 3 and some approaches to acting in Chapter 4.\nGiven infinite computational resources, learning is easy and Solomonoff induction provides a complete theoretical solution. However, acting is not straightforward. We show that in contrast to popular belief, AIXI, the natural extension of Solomonoff induction to reinforcement learning, does not provide the objectively best answer to this question. We discuss some alternatives and their problems in\n\u00a71.2 Contribution 9\nChapter 5. Unfortunately, the general question of how to act optimally remains open.\nAIXItl (Hutter, 2005, Ch. 7.2) is often mentioned as a computable approximation to AIXI. But AIXItl does not converge to AIXI in the limit. Inspired by Hutter search (Hutter, 2002b), it relies on an automated theorem prover to find the provably best policy computable in time t with a program of length \u2264 l. In contrast to AIXI, which only requires the choice of universal Turing machine, proof search requires an axiom system that must not be too weak or too strong. In Section 5.2.3 we discuss some of the problems with AIXItl. Moreover, in Corollary 6.13 we show that \u03b5-optimal AIXI is limit computable, which shows that AIXI can be computably approximated by running this algorithm for a fixed number of time steps or until a timeout is reached. While neither AIXItl nor this AIXI approximation algorithm are practically feasible, the latter is a better example for a computable strong AI.\nIn our view, AIXI should be taken as a descriptive rather than prescriptive model. It is descriptive as an abstraction from an actual implementation of strong AI where we ignore all the details of the learning algorithm and the computational approximations of choosing how to act. It should not be viewed as a prescription of how strong AI should be built and AIXI approximations (Veness et al., 2011, 2015) are easily outperformed by neural-network-based approaches (Mnih et al., 2015).\n\u2022 Mathematically. Some of the proof techniques we employ are novel and could be used to analyze other algorithms. Examples include the proofs for the lower bounds on the computability results (Section 6.3.2) and to a lesser extent the upper bounds (Section 6.3.1), which should work analogously for a wide range of algorithms. Furthermore, the proof of the asymptotic optimality of Thompson sampling (Theorem 5.25) brings together a variety of mathematical tools from measure theory, probability theory, and stochastic processes.\nNext, the recoverability assumption (Definition 5.31) is a novel technical assumption on the environment akin to ergodicity and weak communication in finite-state environments. It is more general, yet mathematically simple and works for arbitrary environments. This assumption turns out to be what we need to prove the connection from asymptotic optimality to sublinear regret in Section 5.5.\nMoreover, we introduce the use of the recursive instead of the iterative value function (Section 6.4). The iterative value function is the natural extension of expectimax search to the sequential setting and was originally used by Hutter (2005, Sec. 5.5). Yet it turned out to be an incorrect and inconvenient definition: it does not correctly maximize expected rewards (Proposition 6.19) and it is not limit computable (Theorem 6.22 and Theorem 6.23). However, this is only a minor technical correction.\nFinally, this work raises new mathematically intriguing questions about the properties of reflective oracles (Section 7.1)."}, {"heading": "10 Introduction", "text": "\u2022 Practically. One insight from this thesis is regarding the effective horizon. In practice geometric discounting is ubiquitous which has a constant effective horizon. However, when facing a finite horizon problem or an episodic task, sometimes the effective horizon changes. One lesson from our result on Thompson sampling (Section 5.4.3 and Section 5.5) is that you should explore for an effective horizon instead of using \u03b5-greedy. While the latter exploration method is often used in practice, it has proved ineffective in environments with delayed rewards (see Section 1.1.2).\nFurthermore, our application of reinforcement learning results to game theory in Chapter 7 reinforces this trend to solve game theory problems (Tesauro, 1995; Bowling and Veloso, 2001; Busoniu et al., 2008; Silver et al., 2016; Heinrich and Silver, 2016; Foerster et al., 2016, and many more). In particular, the approximation algorithm for reflective oracles (Section 7.1.3) could guide future applications for computing Nash equilibria (see also Fallenstein et al., 2015b).\nOn a technical level, we advance the theory of general reinforcement learning. In its center is the Bayesian reinforcement learning agent AIXI. AIXI is meant as an answer to the question of how to do general RL disregarding computational constraints. We analyze the computational complexity of AIXI and related agents in Chapter 6 and show that even with an infinite horizon AIXI can be computationally approximated with a regular Turing machine (Section 6.3.1). We also derive corresponding lower bounds for most of our upper bounds (Section 6.3.2).\nChapter 5 is about notions of optimality in general reinforcement learning. We dispel AIXI\u2019s status as the gold standard for reinforcement learning. Hutter (2002a) showed that AIXI is Pareto optimal, balanced Pareto optimal, and self-optimizing. Orseau (2013) established that AIXI does not achieve asymptotic optimality in all computable environments (making the self-optimizing result inapplicable to this general environment class). In Section 5.1 we show that every policy is Pareto optimal and in Section 5.3 we show that balanced Pareto optimality is highly subjective, depending on the choice of the prior; bad choices for priors are discussed in Section 5.2. Notable is the dogmatic prior that locks a Bayesian reinforcement learning agent into a particular (bad) policy as long as this policy yields some rewards. Our results imply that there are no known nontrivial and non-subjective optimality results for AIXI. We have to regard AIXI as a relative theory of intelligence. More generally, our results imply that general reinforcement learning is difficult even when disregarding computational costs.\nBut this is not the end to Bayesian methods in general RL. We show in Section 5.4 that a Bayes-inspired algorithm called Thompson sampling achieves asymptotic optimality. Thompson sampling, also known as posterior sampling or the Bayesian control rule repeatedly draws one environment from the posterior distribution and then acts as if this was the true environment for a certain period of time (depending on the discount function). Moreover, given a recoverability assumption on the environment and some mild assumptions on the discount function, we show in Section 5.5 that Thompson sampling achieves sublinear regret.\nFinally, we tie these results together to solve an open problem in game theory:\nWhen acting in a multi-agent environment with other Bayesian agents, each agent needs to assign positive prior probability to the other agents\u2019 actual policies (they need to have a grain of truth). Finding a reasonably large class of policies that contains the Bayes optimal policies with respect to this class is known as the grain of truth problem (Hutter, 2009b, Q. 5j). Only small classes are known to have a grain of truth and the literature contains several related impossibility results (Nachbar, 1997, 2005; Foster and Young, 2001). Moreover, while AIXI assumes the environment to be computable, our computability results on AIXI confirm that it is incomputable (Theorem 6.15 and Theorem 6.17). This asymmetry elevates AIXI above its environment computationally, and prevents the environment from containing other AIXIs.\nIn Chapter 7 we give a formal and general solution to the grain of truth problem: we construct a class of policies that avoid this asymmetry. This class contains all computable policies as well as Bayes optimal policies for every lower semicomputable prior over the class. When the environment is unknown, our dogmatic prior from Section 5.2 makes Bayes optimal agents fail to act optimally even asymptotically. However, our convergence results on Thompson sampling (Section 5.4.3) imply that Thompson samplers converge to play \u03b5-Nash equilibria in arbitrary unknown computable multiagent environments. While these results are purely theoretical, we use techniques from Chapter 6 to show that they can be computationally approximated arbitrarily closely."}, {"heading": "1.3 Thesis Outline", "text": "This thesis is based on the papers Leike and Hutter (2014a,b, 2015a,b,c, 2016); Leike et al. (2016a,b). During my PhD, I was also involved in the publications Leike and Heizmann (2014a,b, 2015); Heizmann et al. (2015, 2016) based on my research in termination analysis (in collaboration with Matthias Heizmann), Daswani and Leike (2015) (coauthored with Mayank Daswani in equal parts), Everitt et al. (2015) (co-authored with Tom Everitt in equal parts), Filan et al. (2016) (written by Daniel Filan as part of his honour\u2019s thesis supervised by Marcus Hutter and me). Leike and Hutter (2016) is"}, {"heading": "12 Introduction", "text": "still under review. Leike and Hutter (2014a, 2015d) are tangential to this thesis\u2019 main thrust, so the results are mentioned only in passing. A list of papers written during my PhD is given in Table 1.3 on page 14, with a corresponding chapter outline in Table 1.2. The core of our contribution is found in chapters 5, 6, and 7.\nEvery thesis chapter starts with a quote. In case this is not blatantly obvious: these are false quotes, a desperate attempt to make the thesis less dry and humorless. None of the quotes were actually stated by the person they are attributed to (according to our knowledge).\n\u00a71.3 Thesis Outline 13"}, {"heading": "14 Introduction", "text": "[1] Jan Leike and Marcus Hutter. Indefinitely oscillating martingales. In Algorithmic Learning Theory, pages 321\u2013335, 2014a\n[2] Jan Leike and Matthias Heizmann. Ranking templates for linear loops. Logical Methods in Computer Science, 11(1):1\u201327, March 2015\n[3] Mayank Daswani and Jan Leike. A definition of happiness for reinforcement learning agents. In Artificial General Intelligence, pages 231\u2013240. Springer, 2015\n[4] Tom Everitt, Jan Leike, and Marcus Hutter. Sequential extensions of causal and evidential decision theory. In Algorithmic Decision Theory, pages 205\u2013221. Springer, 2015\n[5] Jan Leike and Marcus Hutter. On the computability of AIXI. In Uncertainty in Artificial Intelligence, pages 464\u2013473, 2015a\n[6] Jan Leike and Marcus Hutter. On the computability of Solomonoff induction and knowledgeseeking. In Algorithmic Learning Theory, pages 364\u2013378, 2015b\n[7] Jan Leike and Marcus Hutter. Bad universal priors and notions of optimality. In Conference on Learning Theory, pages 1244\u20131259, 2015c\n[8] Jan Leike and Marcus Hutter. Solomonoff induction violates Nicod\u2019s criterion. In Algorithmic Learning Theory, pages 349\u2013363. Springer, 2015d\n[9] Matthias Heizmann, Daniel Dietsch, Jan Leike, Betim Musa, and Andreas Podelski. Ultimate Automizer with array interpolation (competition contribution). In Tools and Algorithms for the Construction and Analysis of Systems, pages 455\u2013457. Springer, 2015\n[10] Matthias Heizmann, Daniel Dietsch, Marius Greitschus, Jan Leike, Betim Musa, Claus Sch\u00e4tzle, and Andreas Podelski. Ultimate Automizer with two-track proofs (competition contribution). In Tools and Algorithms for the Construction and Analysis of Systems, pages 950\u2013953. Springer, 2016\n[11] Daniel Filan, Jan Leike, and Marcus Hutter. Loss bounds and time complexity for speed priors. In Artificial Intelligence and Statistics, 2016\n[12] Jan Leike and Marcus Hutter. On the computability of Solomonoff induction and AIXI. 2016. Under review\n[13] Jan Leike, Tor Lattimore, Laurent Orseau, and Marcus Hutter. Thompson sampling is asymptotically optimal in general environments. In Uncertainty in Artificial Intelligence, 2016a\nChapter 2\nPreliminaries\nMathematics is a waste of time. \u2014 Leonhard Euler\nThis chapter establishes the notation and background material that is used throughout this thesis. Section 2.1 is about probability and measure theory, Section 2.2 is about stochastic processes, Section 2.3 is about information theory, and Section 2.4 is about algorithmic information theory. We defer the formal introduction to reinforcement learning to Chapter 4. Additional preliminary notation and terminology is also established in individual chapters wherever necessary. A list of notation is provided in the appendix on page 171.\nMost of the content from this chapter can be found in standard textbooks and reference works. We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vit\u00e1nyi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al. (2009) on machine learning, Sutton and Barto (1998) on reinforcement learning, and Hutter (2005) and Lattimore (2013) on general reinforcement learning.\nWe understand definitions to follow natural language; e.g., when defining the adjective \u2018continuous\u2019, we define at the same time the noun \u2018continuity\u2019 and the adverb \u2018continuously\u2019 wherever appropriate.\nNumbers. N := {1, 2, 3, . . .} denotes the set of natural numbers (starting from 1), Q := {\u00b1p/q | p \u2208 N\u222a{0}, q \u2208 N} denotes the set of rational numbers, and R denotes the set of real numbers. For two real numbers r1, r2, the set [r1, r2] := {r \u2208 R | r1 \u2264 r \u2264 r2} denotes the closed interval with end points r1 and r2; the sets (r1, r2] := [r1, r2] \\ {r1} and [r1, r2) := [r1, r2]\\{r2} denote half-open intervals; the set (r1, r2) := [r1, r2]\\{r1, r2} denotes an open interval.\nStrings. Fix X to be a finite nonempty set, called alphabet . We assume that X contains at least two distinct elements. The set X \u2217 := \u22c3\u221e n=0X n is the set of all finite strings over the alphabet X , the set X\u221e is the set of all infinite strings over the alphabet X , and the set X ] := X \u2217 \u222aX\u221e is their union. The empty string is denoted by , not to be confused with the small positive real number \u03b5. Given a string x \u2208 X ], we denote its length by |x|. For a (finite or infinite) string x of length \u2265 k, we denote with xk the k-th character of x, with x1:k the first k characters of x, and with x<k the first k \u2212 1\n15"}, {"heading": "16 Preliminaries", "text": "characters of x. The notation x1:\u221e stresses that x is an infinite string. We use x v y to denote that x is a prefix of y, i.e., x = y1:|x|. Our examples often (implicitly) involve the binary alphabet {0, 1}. In this case we define the functions ones, zeros : X \u2217 \u2192 N that count the number of ones and zeros in a string respectively.\nComputability. A function f : X \u2217 \u2192 R is lower semicomputable iff the set {(x, q) \u2208 X \u2217 \u00d7Q | f(x) > q} is recursively enumerable. If f and \u2212f are lower semicomputable, then f is called computable. See Section 6.1.2 for more computability definitions.\nAsymptotic Notation. Let f, g : N \u2192 R\u22650. We use f \u2208 O(g) to denote that there is a constant c such that f(t) \u2264 cg(t) for all t \u2208 N. We use f \u2208 o(g) to denote that lim supt\u2192\u221e f(t)/g(t) = 0. For functions on strings P,Q : X \u2217 \u2192 R we use Q\n\u00d7\u2265 P to denote that there is a constant c > 0 such that Q(x) \u2265 cP (x) for all x \u2208 X \u2217. We also use Q\n\u00d7\u2264 P for P \u00d7\u2265 Q and Q \u00d7= P for Q \u00d7\u2264 P and P \u00d7\u2264 Q. Note that Q \u00d7= P does not imply that there is a constant c such that Q(x) = cP (x) for all x \u2208 X \u2217. For a sequence (at)t\u2208N with limit limt\u2192\u221e at = a we also write at \u2192 a as t\u2192\u221e. If no limiting variable is provided, we mean t\u2192\u221e by convention.\nOther Conventions. Let A be some set. We use #A to denote the cardinality of the set A, i.e., the number of elements in A, and 2A to denote the power set of A, i.e., the set of all subsets of A. We use log to denote the binary logarithm and ln to denote the natural logarithm."}, {"heading": "2.1 Measure Theory", "text": "For a countable set \u2126, we use \u2206\u2126 to denote the set of probability distributions over \u2126. If \u2126 is uncountable (such as the set of all infinite strings X\u221e), we need to use the machinery of measure theory. This section provides a concise introduction to measure theory; see Durrett (2010) for an extensive treatment.\nDefinition 2.1 (\u03c3-algebra). Let \u2126 be a set. The set F \u2286 2\u2126 is a \u03c3-algebra over \u2126 iff\n(a) \u2126 \u2208 F ,\n(b) A \u2208 F implies \u2126 \\A \u2208 F , and\n(c) for any countable number of sets A0, A1, . . . ,\u2208 F , the union \u22c3 i\u2208NAi \u2208 F .\nFor a set A \u2286 2\u2126, we define \u03c3(A) to be the smallest (with respect to set inclusion) \u03c3-algebra containing A.\nFor the real numbers, the default \u03c3-algebra (used implicitly) is the Borel \u03c3-algebra B generated by the open sets of the usual topology. Formally, B := \u03c3({(a, b) | a, b \u2208 R}).\nA set \u2126 together with a \u03c3-algebra F forms a measurable space. The sets from the \u03c3algebra F are called measurable sets. A function f : \u21261 \u2192 \u21262 between two measurable spaces is called measurable iff any preimage of an (in \u21262) measurable set is measurable (in \u21261).\n\u00a72.1 Measure Theory 17\nDefinition 2.2 (Probability Measure). Let \u2126 be a measurable space with \u03c3-algebra F . A probability measure on the space \u2126 is a function \u00b5 : F \u2192 [0, 1] such that\n(a) \u00b5(\u2126) = 1 (normalization), and (b) \u00b5( \u22c3 i\u2208NAi) = \u2211 i\u2208N \u00b5(Ai) for any collection {Ai | i \u2208 N} \u2286 F that is pairwise\ndisjoint (\u03c3-additivity).\nA probability measure \u00b5 is deterministic iff it assigns all probability mass to a single element of \u2126, i.e., iff there is an x \u2208 \u2126 with \u00b5({x}) = 1.\nWe define the conditional probability \u00b5(A | B) for two measurable sets A,B \u2208 F with \u00b5(B) > 0 as \u00b5(A | B) := \u00b5(A \u2229B)/\u00b5(B).\nDefinition 2.3 (Random Variable). Let \u2126 be a measurable space with probability measure \u00b5. A (real-valued) random variable is a measurable function X : \u2126\u2192 R.\nWe often (but not always) denote random variables with uppercase Latin letters. Given a \u03c3-algebra F , a probability measure P on F , and an F-measurable random variable X, the conditional expectation E[X | F ] of X given F is a random variable Y such that (1) Y is F-measurable and (2) \u222b AXdP = \u222b A Y dP for all A \u2208 F . The conditional expectation exists and is unique up to a set of P -measure 0 (Durrett, 2010, Sec. 5.1). Intuitively, if F describes the information we have at our disposal, then E[X | F ] denotes the expectation of X given this information.\nWe proceed to define the \u03c3-algebra on X\u221e (the \u03c3-algebra on X ] is defined analogously). For a finite string x \u2208 X \u2217, the cylinder set\n\u0393x := {xy | y \u2208 X\u221e}\nis the set of all infinite strings of which x is a prefix. Furthermore, we fix the \u03c3-algebras\nFt := \u03c3 ( {\u0393x | x \u2208 X t} ) and F\u221e := \u03c3 ( \u221e\u22c3 t=1 Ft ) .\nThe sequence (Ft)t\u2208N is a filtration: from \u0393x = \u22c3 a\u2208X \u0393xa follows that Ft \u2286 Ft+1 for every t \u2208 N, and all Ft \u2286 F\u221e by the definition of F\u221e. For our purposes, the \u03c3-algebra Ft means \u2018all symbols up to and including time step t.\u2019 So instead of conditioning an expectation on Ft, we can just as well condition it on the sequence x1:t drawn at time t. Hence we write E[X | x1:t] instead of E[X | Ft]. Moreover, for conditional probabilities we also write Q(xt | x<t) instead of Q(x1:t | x<t).\nIn the context of probability measures, a measurable set E \u2208 F\u221e is also called an event . The event Ec := X\u221e \\ E denotes the complement of E. In case the event E is defined by a predicate Q dependent on the random variable X, E = {x \u2208 \u2126 | Q(X(x))}, we also use the shorthand notation\nP [Q(X)] := P ({x \u2208 \u2126 | Q(X(x))}) = P (E).\nWe assume all sets to be measurable; when we write P (A) for some set A \u2286 X\u221e, we understand implicitly that A be measurable. This is not true: not all subsets of"}, {"heading": "18 Preliminaries", "text": "X\u221e are measurable (assuming the axiom of choice). While we choose to do this for readability purposes, note that under some axioms compatible with Zermelo-Fraenkel set theory, notably the axiom of determinacy, all subsets of X\u221e are measurable."}, {"heading": "2.2 Stochastic Processes", "text": "This section introduces some notions about sequences of random variables.\nDefinition 2.4 (Stochastic Process). (Xt)t\u2208N is a stochastic process iff Xt is a random variable for every t \u2208 N.\nA stochastic process (Xt)t\u2208N is nonnegative iff Xt \u2265 0 for all t \u2208 N. The process is bounded iff there is a constant c \u2208 R such that |Xt| \u2264 c for all t \u2208 N.\nIn the real numbers, a sequence (zt)t\u2208N converges if and only if it is a Cauchy sequence, i.e., iff |zt+1\u2212zt| \u2192 0 as t\u2192\u221e. For sequences of random variables convergence is a lot more subtle and there are several different notions of convergence.\nDefinition 2.5 (Stochastic Convergence). Let P be a probability measure. A stochastic process (Xt)t\u2208N converges to the random variable X\n\u2022 in P -probability iff for every \u03b5 > 0,\nP [ |Xt \u2212X| > \u03b5 ] \u2192 0 as t\u2192\u221e;\n\u2022 in P -mean iff EP [ |Xt \u2212X| ] \u2192 0 as t\u2192\u221e;\n\u2022 P -almost surely iff P [\nlim t\u2192\u221e\nXt = X ] = 1.\nAlmost sure convergence and convergence in mean both imply convergence in probability (Wasserman, 2004, Thm. 5.17). If the stochastic process is bounded, then convergence in probability implies convergence in mean (Wasserman, 2004, Thm. 5.19).\nA sequence of real numbers (at)t\u2208N converges in Ces\u00e0ro average to a \u2208 R iff 1/t \u2211t\nk=1 ak \u2192 a as t \u2192 \u221e. The definition for sequences of random variables is analogous.\nDefinition 2.6 (Martingale). Let P be a probability measure over (X\u221e,F\u221e). A stochastic process (Xt)t\u2208N is a P -supermartingale (P -submartingale) iff\n(a) each Xt is Ft-measurable, and\n(b) E[Xt | Fs] \u2264 Xs (E[Xt | Fs] \u2265 Xs) P -almost surely for all s, t \u2208 N with s < t.\nA P -martingale is a process that is both a P -supermartingale and a P -submartingale.\n\u00a72.3 Information Theory 19\nExample 2.7 (Fair Gambling). Suppose Mary bets on the outcome of a fair coin flip. If she predicts correctly, her wager is doubled and otherwise it is lost. Let Xt denote Mary\u2019s wealth at time step t. Since the game is fair, E[Xt+1 | Ft] = Xt where Ft represents the information available at time step t. Hence E[Xt] = X1, so in expectation she never loses money regardless of her betting strategy. 3\nFor martingales the following famous convergence result was proved by Doob (1953).\nTheorem 2.8 (Martingale Convergence; Durrett, 2010, Thm. 5.2.9). If (Xt)t\u2208N is a nonnegative supermartingale, then it converges almost surely to a limit X with E[X] \u2264 E[X1].\nBy Theorem 2.8 the martingale from Example 2.7 representing Mary\u2019s wealth converges almost surely, regardless of her betting strategy. Either she refrains from betting at some point (assuming she cannot place smaller and smaller bets) or she cannot play anymore because her wealth is 0. Is there a lesson to learn here about gambling?"}, {"heading": "2.3 Information Theory", "text": "This section introduces the notions of entropy and two notions of distance between probability measures: KL-divergence and total variation distance.\nDefinition 2.9 (Entropy). Let \u2126 be a countable set. For a probability distribution p \u2208 \u2206\u2126, the entropy of p is defined as\nEnt(p) := \u2212 \u2211\nx\u2208\u2126: p(x)>0\np(x) log p(x).\nDefinition 2.10 (KL-Divergence). Let P,Q be two measures and let m \u2208 N be a lookahead time step. The Kullback-Leibler-divergence (KL-divergence) of P and Q between time steps t and m is defined as\nKLm(P,Q | x<t) := \u2211\nxt:m\u2208Xm\u2212t+1 P (x1:m | x<t) log\nP (x1:m | x<t) Q(x1:m | x<t) .\nMoreover, we define KL\u221e(P,Q | x<t) := limm\u2192\u221eKLm(P,Q | x<t).\nKL-divergence is also known as relative entropy . KL-divergence is always nonnegative by Gibbs\u2019 inequality, but it is not a distance since it is not symmetric. If the alphabet X is finite, then KLm(P,Q | x) is always finite. However, KL\u221e(P,Q | x) may be infinite.\nDefinition 2.11 (Total Variation Distance). Let P,Q be two measures and let 1 \u2264 m \u2264 \u221e be a lookahead time step. The total variation distance between P and Q between time steps t and m is defined as\nDm(P,Q | x) := sup A\u2286Xm\n\u2223\u2223\u2223P (A | x<t)\u2212Q(A | x<t)\u2223\u2223\u2223."}, {"heading": "20 Preliminaries", "text": "Total variation distance is always bounded between 0 and 1 since P and Q are probability measures. Moreover, in contrast to KL-divergence total variation distance satisfies the axioms of distance: symmetry (D(P,Q) = D(Q,P )), identity of indiscernibles (D(P,Q) = 0 if and only if P = Q), and the triangle inequality (D(P,Q) +D(Q,R) \u2265 D(P,R)).\nThe following lemma shows that total variation distance can be used to bound differences in expectation.\nLemma 2.12 (Total Variation Bound on the Expectation). For a random variable X with 0 \u2264 X \u2264 1 and two probability measures P and Q\u2223\u2223EP [X]\u2212 EQ[X]\u2223\u2223 \u2264 D(P,Q).\nKL-divergence and total variation distance are linked by the following inequality.\nLemma 2.13 (Pinsker\u2019s inequality; Tsybakov, 2008, Lem. 2.5i). For all probability measures P and Q on X\u221e, for every x \u2208 X \u2217, and for every m \u2208 N\nDm(P,Q | x) \u2264 \u221a 1\n2 KLm(P,Q | x)"}, {"heading": "2.4 Algorithmic Information Theory", "text": "A universal Turing machine (UTM) is a Turing machine that can simulate all other Turing machines. Formally, a Turing machine U is a UTM iff for every Turing machine T there is a binary string p (called program) such that U(p, x) = T (x) for all x \u2208 X \u2217, i.e., the output of U when run on (p, x) is the same as the output of T when run on x. We assume the set of programs on U is prefix-free. The Kolmogorov complexity K(x) of a string x is the length of the shortest program on U that prints x and then halts:\nK(x) := min{|p| | U(p) = x}.\nA monotone Turing machine is a Turing machine with a one-way read-only input tape, a one-way write-only output tape, and a read/write work tape. Monotone Turing machines sequentially read symbols from their input tape and write to their output tape. Interpreted as a function, a monotone Turing machine T maps a string x to the longest string that T writes to the output tape while reading x and no more from the input tape (Li and Vit\u00e1nyi, 2008, Ch. 4.5.2).\nWe also use U to denote a universal monotone Turing machine (programs on the universal monotone Turing machine do not have to be prefix-free). The monotone Kolmogorov complexity Km(x) denotes the length of the shortest program on the monotone machine U that prints a string starting with x (Li and Vit\u00e1nyi, 2008, Def. 4.5.9):\nKm(x) := min{|p| | x v U(p)}. (2.1)\nSince monotone complexity does not require the machine to halt, there is a constant c such that Km(x) \u2264 K(x) + c for all x \u2208 X\u2217.\n\u00a72.4 Algorithmic Information Theory 21\nThe following notion of a (semi)measure is particular to algorithmic information theory.\nDefinition 2.14 (Semimeasure; Li and Vit\u00e1nyi, 2008, Def. 4.2.1). A semimeasure over the alphabet X is a function \u03bd : X \u2217 \u2192 [0, 1] such that\n(a) \u03bd( ) \u2264 1, and (b) \u03bd(x) \u2265 \u2211\na\u2208X \u03bd(xa) for all x \u2208 X \u2217.\nA semimeasure is a (probability) measure iff equalities hold in (a) and (b) for all x \u2208 X \u2217.\nSemimeasures are not probability measures in the classical measure theoretic sense. However, semimeasures correspond canonically to classical probability measures on the probability space X ] = X \u2217 \u222aX\u221e whose \u03c3-algebra is generated by the cylinder sets (Li and Vit\u00e1nyi, 2008, Ch. 4.2 and Hay, 2007).\nLower semicomputable semimeasures correspond naturally to monotone Turing machines (Li and Vit\u00e1nyi, 2008, Thm. 4.5.2): for a monotone Turing machine T , the semimeasure \u03bbT maps a string x to the probability that T outputs something starting with x when fed with fair coin flips as input (and vice versa). Hence we can enumerate all lower semicomputable semimeasures \u03bd1, \u03bd2, . . . by enumerating all monotone Turing machines. We define the Kolmogorov complexity K(\u03bd) of a lower semicomputable semimeasure \u03bd as the Kolmogorov complexity of the index of \u03bd in this enumeration.\nWe often mix the (semi)measures of algorithmic information theory with concepts from probability theory. For convenience, we identify a finite string x \u2208 X \u2217 with its cylinder set \u0393x. Then \u03bd(x) in the algorithmic information theory sense coincides with \u03bd(\u0393x) in the measure theory sense if we use the identification of semimeasures with probability measures above.\nExample 2.15 (Lebesgue Measure). The Lebesgue measure or uniform measure is defined as\n\u03bb(x) := (#X )\u2212|x|. 3\nThe following definition turns a semimeasure into a measure, preserving the predictive ratio \u03bd(xa)/\u03bd(xb) for a, b \u2208 X .\nDefinition 2.16 (Solomonoff Normalization). The Solomonoff normalization \u03bdnorm of a semimeasure \u03bd is defined as \u03bdnorm( ) := 1 and for all x \u2208 X \u2217 and a \u2208 X ,\n\u03bdnorm(xa) := \u03bdnorm(x) \u03bd(xa)\u2211 b\u2208X \u03bd(xb) . (2.2)\nBy definition, \u03bdnorm is a measure. Moreover, \u03bdnorm dominates \u03bd according to the following lemma.\nLemma 2.17 (\u03bdnorm \u2265 \u03bd). \u03bdnorm(x) \u2265 \u03bd(x) for all x \u2208 X \u2217 and all semimeasures \u03bd."}, {"heading": "22 Preliminaries", "text": "Proof. We use induction on the length of x: if x = then \u03bdnorm( ) = 1 = \u03bd( ), and otherwise\n\u03bdnorm(xa) = \u03bdnorm(x)\u03bd(xa)\u2211 b\u2208X \u03bd(xb) \u2265 \u03bd(x)\u03bd(xa)\u2211 b\u2208X \u03bd(xb) \u2265 \u03bd(x)\u03bd(xa) \u03bd(x) = \u03bd(xa).\nThe first inequality holds by induction hypothesis and the second inequality uses the fact that \u03bd is a semimeasure.\nChapter 3\nLearning\nThe problem of induction is essentially solved. \u2014 David Hume\nMachine learning refers to the process of learning models of and/or making predictions about (large) sets of data points that are typically independent and identically distributed (i.i.d.); see Bishop (2006) and Hastie et al. (2009). In this chapter we do not make the i.i.d. assumption. Instead, we aim more generally at the theoretical fundamentals of the sequence prediction problem: how will a sequence of symbols generated by an unknown stochastic process be continued? Given a finite string x<t = x1x2 . . . xt\u22121 of symbols, what is the next symbol xt? How likely does a given property hold for the entire sequence x1:\u221e? Arguably, any learning or prediction problem can be phrased in this fashion: anything that can be stored on a computer can be turned into a sequence of bits.\nWe distinguish two major elements of learning. First, the process of converging to accurate beliefs, called merging . Second, the process of making accurate forecasts about the next symbol, called predicting . These two notions are not distinct: if you have accurate beliefs about the unseen data, then you can make good predictions, but not necessarily vice versa (see Example 3.41). We discuss different notions of merging in Section 3.4 and state bounds on the prediction regret in Section 3.5.\nIn the general reinforcement learning problem we target in this thesis, the environment is unknown and the agent needs to learn it. The literature on non-i.i.d. learning has focused on predicting individual symbols and bounds on the number of prediction errors (Hutter, 2001b, 2005; Cesa-Bianchi and Lugosi, 2006), and the results on merging are from the game theory literature (Blackwell and Dubins, 1962; Kalai and Lehrer, 1994; Lehrer and Smorodinsky, 1996). However, we argue that merging is the essential property for general AI. In order to make good decisions, the agent needs to have accurate beliefs about what its actions will entail. On a technical level, merging leads to on-policy value convergence (Section 4.2.3), the fact that the agents learns to estimate the values for its own policy correctly.\nThe setup we consider is the realizable case: we assume that the data is generated by an unknown probability distribution that belongs to a known (countable) class of distributions. In contrast, the nonrealizable case allows no assumptions on the underlying process that generates the data. A well-known approach to the nonrealizable case is prediction with expert advice (Cesa-Bianchi and Lugosi, 2006), which we do not con-\n23"}, {"heading": "24 Learning", "text": "sider here. Generally, the nonrealizable case is harder, but Ryabko (2011) argues that for some problems, both cases coincide.\nAfter introducing the formal setup in Section 3.1, we discuss several examples for learning distributions and notions that relate the learning distribution with the process generating the data in Section 3.2. In Section 3.3 we connect these notions to the theory of martingale processes.\nSection 3.6 connects the results from the first sections to the learning framework developed by Solomonoff (1964, 1978), Hutter (2001b, 2005, 2007b), and Schmidhuber (2002) (among others). This framework relies on results from algorithmic information theory and computability theory to learn any computable distribution quickly and effectively. It is incomputable (see Section 6.2), but can serve as a gold standard for learning.\nMost of this chapter echoes the literature. We collect results from economics and computer science that previously had not been assembled in one place. We provide proofs that connect the various properties (Proposition 3.23, Proposition 3.16, and Proposition 3.37), and we fill in a few gaps in the picture: the prediction bounds for absolute continuity (Section 3.5.2) and the improved regret bounds for nonuniform measures (Theorem 3.48 and Theorem 3.51). Section 3.7 summarizes the results in Table 3.2 on page 45 as well as Figure 3.1 on page 46."}, {"heading": "3.1 Setup", "text": "For the rest of this chapter, fix P and Q to be two probability measures over the measurable space of infinite sequences (X\u221e,F\u221e). We think of P as the true distribution from which the data sequence x1:\u221e is drawn, and of Q as our belief distribution or learning algorithm. In other words, we use the distribution Q to learn a string drawn from the distribution P .\nLet H denote a hypothesis, i.e., any measurable set from F\u221e. Our prior belief in the hypothesis H is Q(H). In each time step t, we make one observation xt \u2208 X . Our history x<t = x1x2 . . . xt\u22121 is the sequence of all previous observations. We update our belief in accordance with Bayesian learning; our posterior belief in the hypothesis H is\nQ(H | x1:t) = Q(H \u2229 x1:t) Q(x1:t) .\nThe observation xt confirms the hypothesis H iff Q(H | x1:t) > Q(H | x<t) (the belief in H increases), and the observation xt disconfirms the hypothesis H iff Q(H | x1:t) < Q(H | x<t) (the belief in H decreases). If Q(H | x1:t) = 0, then H is refuted or falsified .\nWhen we assign a prior belief of 0 to a hypothesis H, this means that we think that H is impossible; it is refuted from the beginning. If Q(H) = 0, then the posterior Q(H | x<t) = 0, so no evidence whatsoever can change our mind that H is impossible. This is bad if the hypothesis H is actually true.\nTo be able to learn we need to make some assumptions on the learning distribution Q: we need to have an open mind about anything that might actually happen, i.e.,\n\u00a73.2 Compatibility 25\nQ(H) > 0 on any hypothesis H with P (H) > 0. This property is called absolute continuity . We discuss this and other notions of compatibility of P and Q in Section 3.2.\nWe motivate this chapter with the following example.\nExample 3.1 (The Black Ravens; Rathmanner and Hutter, 2011, Sec. 7.4). If we live in a world in which all ravens are black, how can we learn this fact? Since at every time step we have observed only a finite subset of the (possibly infinite) set of all ravens, how can we confidently state anything about all ravens?\nWe formalize this problem in line with Rathmanner and Hutter (2011, Sec. 7.4) and Leike and Hutter (2015d). We define two predicates, blackness B and ravenness R. There are four possible observations: a black raven BR, a non-black raven BR, a black non-raven BR, and a non-black non-raven BR. Therefore our alphabet consists of four symbols corresponding to each of the possible observations, X := {BR,BR,BR,BR}.\nWe are interested in the hypothesis \u2018all ravens are black\u2019. Formally, it corresponds to the measurable set\nH := {x \u2208 X\u221e | xt 6= BR \u2200t} = {BR,BR,BR}\u221e, (3.1)\nthe set of all infinite strings in which the symbol BR does not occur. If we observe a non-black raven, xt = BR, the hypothesisH is refuted sinceH\u2229x1:t = \u2205 and this implies Q(H | x1:t) = 0. In this case, our inquiry regarding H is settled. The interesting case is when the hypothesis H is in fact true (P (H) = 1), i.e., P does not generate any non-black ravens. The property we desire is that in a world in which all ravens are black, we arrive at this belief: P (H) = 1 implies Q(H | x<t) \u2192 1 as t\u2192\u221e. 3"}, {"heading": "3.2 Compatibility", "text": "In this section we define dominance, absolute continuity , dominance with coefficients, weak dominance, and local absolute continuity , in decreasing order of their strength. These notions make the relationship of the two probability measures P and Q precise. We also give examples for various choices for the learning algorithm Q.\nIn our examples, we frequently rely on the following process.\nExample 3.2 (Bernoulli Process). Assume X = {0, 1}. For a real number r \u2208 [0, 1] we define the Bernoulli process with parameter r as the measure\nBernoulli(r)(x) := rones(x)(1\u2212 r)zeros(x).\nNote that Bernoulli(1/2) = \u03bb, the Lebesgue measure from Example 2.15. 3\nDefinition 3.3 (Dominance). The measure Q dominates P (Q \u00d7\u2265 P ) iff there is a constant c > 0 such that Q(x) \u2265 cP (x) for all finite strings x \u2208 X \u2217.\nDominance is also called having a grain of truth (Lehrer and Smorodinsky, 1996, Def. 2a and Kalai and Lehrer, 1993); we discuss this property in the context of game theory in Chapter 7."}, {"heading": "26 Learning", "text": "Example 3.4 (Bayesian mixture). LetM be a countable set of probability measures on (X\u221e,F\u221e) and let w \u2208 \u2206M be a prior over M. If w(P ) > 0 for all P \u2208 M, the prior w is called positive or universal . Then the Bayesian mixture \u03be := \u2211 P\u2208Mw(P )P dominates each P \u2208M. 3\nThe Bayesian mixture is a mathematically simple yet very powerful concept. It is very easy to derive from a countable set of distributions, and it has been considered extensively in the literature (Solomonoff, 1964; Jaynes, 2003; Hutter, 2005, . . . ). Ryabko (2009) shows that even for uncountably infinite classes, if there are good predictors, then a Bayesian mixture over a countable subclass asymptotically also does well.\nExample 3.5 (Solomonoff Prior). Solomonoff (1964) defines a distribution M over X ] that assigns to a string x the probability that the universal monotone Turing machine U outputs x when fed with fair coin flips on the input tape. Formally,\nM(x) := \u2211\np:xvU(p)\n2\u2212|p| (3.2)\nwhere p is a binary string.1 The functionM is a lower semicomputable semimeasure, but not computable and not a measure (Li and Vit\u00e1nyi, 2008, Lem. 4.5.3); see Section 6.2 for the computability properties of M . More importantly, M dominates every lower semicomputable semimeasure (Li and Vit\u00e1nyi, 2008, Thm. 4.5.1).\nSolomonoff\u2019s prior M has a number of appealing philosophical properties. In line with Ockham\u2019s razor it favors simple environments over complex ones: Turing machines that have a short program on the UTM U have a higher contribution in the sum (3.2). In line with Epicurus\u2019 principle it never discards possible explanations: every program that produces the string x contributes to the sum. See Rathmanner and Hutter (2011) for a discussion on the philosophical underpinnings of Solomonoff\u2019s prior. 3\nWood et al. (2011) show that the Solomonoff priorM can equivalently be defined as a Bayesian mixture over all lower semicomputable semimeasures with a prior w(P ) \u221d 2\u2212K(P ). (If we use w(P ) = 2\u2212K(P ) we get a semiprior because \u2211 P\u2208M 2\n\u2212K(P ) can be less than 1. This prior also carries the name Solomonoff prior .)\nDefinition 3.6 (Absolute Continuity). The measure P is absolutely continuous with respect to Q (Q P ) iff Q(A) = 0 implies P (A) = 0 for all measurable sets A.\nRemark 3.7 (Absolute Continuity ; Dominance). Absolute continuity is strictly weaker than dominance: let X := {0, 1} and define a probability measure P that assigns probability 2/3 to 1 and probability 1/3 to 0 until seeing the first 0, then P behaves like the Lebesgue measure \u03bb. Formally,\nP (x1:t) :=\n{( 2 3 )t if x1:t = 1t, and( 2 3 )n 1 3\u03bb(xn+2:t) if \u2203n \u2265 0. 1 n0 v x1:t.\n1We use the name Solomonoff prior for both a distribution over X\u221e and a distribution over a computably enumerable setM. Maybe M should better be called Solomonoff mixture to avoid confusion.\n\u00a73.2 Compatibility 27\nSince \u03bb(1t)/P (1t) = (3/4)t \u2192 0 as t\u2192\u221e, there is no constant c such that \u03bb(x)/P (x) > c > 0 for all finite strings x \u2208 X \u2217, hence \u03bb does not dominate P . But P is absolutely continuous with respect to \u03bb because P -almost surely we draw a 0 eventually, and then P behaves like \u03bb. Hence P -almost surely \u03bb/P 6\u2192 0. The claim now follows from Proposition 3.23b. 3\nThe idea of Remark 3.7 is to \u2018punch a hole into \u03bb\u2019 at the infinite string 1\u221e. This infinite string has probability 0, hence this hole does not break absolute continuity. But it breaks dominance on this infinite string. Analogously we could punch countably many holes into a probability measure without breaking absolute continuity.\nDefinition 3.8 (Weak Dominance). The measure Q weakly dominates P (Q \u00d7\u2265W P ) iff lim t\u2192\u221e 1 t log Q(x1:t) P (x1:t) = 0 with P -probability 1.\nLehrer and Smorodinsky (1996, Rem. 8) point out that for any P and Q,\nlim sup t\u2192\u221e\n1 t log Q(x1:t) P (x1:t) \u2264 0 P -almost surely,\nso crucial is whether the lim inf is also 0.\nRemark 3.9 (Weak Dominance). The measure Q weakly dominates P if and only if P -almost surely log(P (x)/Q(x)) \u2208 o(t). 3\nRyabko and Hutter (2007, 2008) consider the following definition. It is analogous to Definition 3.3, except that the constant c is allowed to depend on time.\nDefinition 3.10 (Dominance with Coefficients; Ryabko and Hutter, 2008, Def. 2). The measure Q dominates P with coefficients f (Q \u2265 P/f) iff Q(x) \u2265 P (x)/f(|x|) for all x \u2208 X \u2217.\nIf Q dominates P with coefficients f and f grows subexponentially (f \u2208 o(exp)), then Q weakly dominates P by Remark 3.9.\nExample 3.11 (Speed Prior). Schmidhuber (2002) defines a variant of Solomonoff\u2019s priorM that penalizes programs by their running time, called the speed prior. Consider the speed prior\nSKt(x) := \u2211\np:xvU(p)\n2\u2212|p|\nt(U, p, x)\nwhere t(U, p, x) is the number of time steps the Turing machine U takes to produce x from the program p. For any deterministic measure P computable in time q we have SKt(x)\n\u00d7\u2265 P (x)/q(|x|). Therefore SKt dominates P with coefficients O(q). If q is a polynomial (P is computable in polynomial time), then it grows subexponentially and thus SKt weakly dominates P . 3\nThe semimeasure loss SKt(x)\u2212 \u2211\na\u2208X SKt(xa) in the speed prior is quite substantial: since it takes at least n steps to output a string of length n, M(x) \u2265 |x|SKt(x)."}, {"heading": "28 Learning", "text": "Example 3.12 (Laplace Rule). The Laplace rule \u03c1L is defined by\n\u03c1L(xt | x<t) := #{i < t | xi = xt}\nt+ #X .\nFor X = {0, 1} and r \u2208 [0, 1] the measure \u03c1L dominates Bernoulli(r) with coefficients f(t) = t\u2212#X+1 (Ryabko and Hutter, 2008, Prop. 3). 3\nDefinition 3.13 (Local Absolute Continuity). The measure P is locally absolutely continuous with respect to Q (Q L P ) iff Q(x) = 0 implies P (x) = 0 for all finite strings x \u2208 X \u2217.\nThe notable difference between local absolute continuity and absolute continuity is that Definition 3.6 talks about arbitrary measurable sets while Definition 3.13 only talks about finite strings. The former is a much stronger property.\nFor example, every measure is locally absolutely continuous with respect to the Lebesgue measure since \u03bb(x) > 0 for all finite strings x \u2208 X \u2217.\nLocal absolute continuity is an extremely weak property. If it is not satisfied, we have to be very careful when using Q for prediction: then there is a positive probability that we have to condition on a probability zero event.\nExample 3.14 (The Minimum Description Length Principle; Gr\u00fcnwald, 2007). Let M be a countable set of probability measures on (X\u221e,F\u221e) and let K : M \u2192 [0, 1] be a function such that \u2211 P\u2208M 2\n\u2212K(P ) \u2264 1 called regularizer. Following notation from Hutter (2009a), we define for each x \u2208 X \u2217 the minimal description length model as\nMDLx := arg min P\u2208M {\u2212 logP (x) +K(P )}.\n\u2212 logP (x) is the (arithmetic) code length of x given model P , andK(P ) is a complexity penalty for P . Given data x \u2208 X \u2217, MDLx is the measure P \u2208 M that minimizes the total code length of data and model.\nNote that the Lebesgue measure is not locally absolutely continuous with respect to the MDL distribution Q(x) := MDLx(x): for some x \u2208 X \u2217 the minimum description P \u2208M may assign probability zero to a continuation xy \u2208 X \u2217. 3\nRemark 3.15 (MDL is Inductively Inconsistent; Leike and Hutter, 2014a, Cor. 13). The MDL estimator for countable classes as defined in Example 3.14 is inductively inconsistent: the selected model P \u2208 M can change infinitely often and thus the limit limt\u2192\u221eMDLx<t may not exist. This can be a major obstacle for using MDL for prediction, since the model used for prediction has to be changed over and over again, incurring the corresponding computational cost. 3\nThe following proposition establishes the relationship between our notions of compatibility; see also Figure 3.1 on page 46.\nProposition 3.16 (Relationships between Compatibilities).\n(a) If Q \u00d7\u2265 P , then Q P .\n\u00a73.3 Martingales 29\n(b) If Q P , then Q \u00d7\u2265W P .\n(c) If Q \u00d7\u2265 P , then Q dominates P with coefficients f for a constant function f .\n(d) If Q dominates P with coefficients f and f \u2208 o(exp), then Q \u00d7\u2265W P .\n(e) If Q \u00d7\u2265W P , then Q L P .\nProof. (a) From Proposition 3.23 (a) and (b).\n(b) From Proposition 3.23b and Kalai and Lehrer (1994, Prop. 3a).\n(c) Follows immediately from the definitions.\n(d) From Remark 3.9.\n(e) Follows immediately from the definitions.\nNote that the converse of Proposition 3.16d is false: in Remark 3.7 we defined a measure P that is absolutely continuous with respect to \u03bb (and hence is weakly dominated by \u03bb), but the coefficients for P/\u03bb grow exponentially on the string 1t. This infinite string has P -probability 0, but dominance with coefficients demands the inequality Q \u2265 P/f to hold for all strings.\nRemark 3.17 (Local Absolute Continuity ; Absolute Continuity). Define P := Bernoulli(2/3) and Q := Bernoulli(1/3). Both measures P and Q are nonzero on all cylinder sets: Q(x) \u2265 3\u2212|x| > 0 and P (x) \u2265 3\u2212|x| > 0 for every x \u2208 X \u2217. Therefore Q is locally absolutely continuous with respect to P . However, Q is not absolutely continuous with respect to P : define\nA := { x \u2208 X \u03c9 \u2223\u2223\u2223\u2223 lim sup t\u2192\u221e 1 t ones(x1:t) \u2264 1 2 } .\nThe set A is F\u221e-measurable since A = \u22c2\u221e n=1 \u22c3 x\u2208Un \u0393x with Un := {x \u2208 X\n\u2217 | |x| \u2265 n and ones(x) \u2264 |x|/2}, the set of all finite strings of length at least n that have at least as many zeros as ones. We have that P (A) = 0 and Q(A) = 1, hence Q is not absolutely continuous with respect to P . 3"}, {"heading": "3.3 Martingales", "text": "The following two theorems state the connection between probability measures on infinite strings and martingales. For two probability measures P and Q the quotient Q/P is a nonnegative P -martingale if Q is locally absolutely continuous with respect to P . Conversely, for every nonnegative P -martingale there is a probability measure Q L P such that the martingale is P -almost surely a multiple of Q/P ."}, {"heading": "30 Learning", "text": "Theorem 3.18 (Measures 7\u2192 Martingales; Doob, 1953, II\u00a77 Ex. 3). Let Q and P be two probability measures on (X\u221e,F\u221e) such that Q is locally absolutely continuous with respect to P . Then the stochastic process (Xt)t\u2208N,\nXt(x) := Q(x1:t)\nP (x1:t)\nis a nonnegative P -martingale with E[Xt] = 1.\nTheorem 3.19 (Martingales 7\u2192Measures). Let P be a probability measure on (X\u221e,F\u221e) and let (Xt)t\u2208N be a nonnegative P -martingale with E[Xt] = 1. There is a probability measure Q on (X\u221e,F\u221e) that is locally absolutely continuous with respect to P and for all x \u2208 X\u221e and all t \u2208 N with P (x1:t) > 0,\nXt(x) = Q(x1:t)\nP (x1:t) .\nThe proofs for Theorem 3.18 and Theorem 3.19 are provided in the Appendix.\nExample 3.20 (The Posterior Martingale). Suppose we are interested in a hypothesis H \u2286 X\u221e (such as the proposition \u2018all ravens are black\u2019 in Example 3.1). If Q(H) =\u2211\nP\u2208Mw(P )P (H) is a Bayesian mixture over a set of probability distributions M with prior weights w \u2208 \u2206M (see Example 3.4), then the posterior belief Q(H | x) =\u2211\nP\u2208Mw(P | x)P (H | x). The weights w(P | x) are called posterior weights, and satisfy the identity\nw(P | x) = w(P )P (x) Q(x)\n(3.3)\nsince\nQ(H | x) = Q(H \u2229 x) Q(x)\n= 1\nQ(x) \u2211 P\u2208M w(P )P (H \u2229 x)\n= \u2211 P\u2208M w(P ) P (x)P (H \u2229 x) Q(x)P (x)\n= \u2211 P\u2208M w(P | x)P (H | x).\nAccording to Theorem 3.18 the posterior weight w(P | x) is a Q-martingale with expectation w(P ). In particular, this means that the posterior weights converge Qalmost surely by the martingale convergence theorem (Theorem 2.8). SinceQ dominates P , by Proposition 3.16a P is absolutely continuous with respect to Q and hence the posterior also converges P -almost surely. 3\nRemark 3.21 (Martingales and Absolute Continuity). While Theorem 3.18 trivially also holds if Q is absolutely continuous with respect to P , Theorem 3.19 does not imply that Q is absolutely continuous with respect to P .\n\u00a73.3 Martingales 31\nLet P and Q be defined as in Remark 3.17. Consider the process X0(x) := 1,\nXt+1(x) := { 2Xt, if xt+1 = 0, and 1 2Xt, if xt+1 = 1.\nThe process (Xt)t\u2208N is a nonnegative P -martingale since every Xt is Ft-measurable and for x = y1:t we have\nE[Xt+1 | Ft](y) = P (x0 | x)2Xt(y) + P (x1 | x)12Xt(y) = 132Xt(y) + 2 3 \u00b7 1 2Xt(y) = Xt(y).\nMoreover,\nQ(x) = (\n1 3 )ones(x) (2 3 )zeros(x) = ( 2 3 )ones(x) (1 3 )zeros(x) 2\u2212ones(x)2zeros(x) = P (x)Xt(y).\nHence Xt(y) = Q(y1:t)/P (y1:t) P -almost surely. The measure Q is uniquely defined by its values on the cylinder sets, and as shown in Remark 3.17, Q is not absolutely continuous with respect to P . 3\nTheorem 3.22 (Radon-Nikodym Derivative). If Q P , then there is a function dP/dQ : X\u221e \u2192 [0,\u221e) called the Radon-Nikodym derivative such that\u222b\nfdP = \u222b f dP\ndQ dQ\nfor all measurable functions f .\nThis function dP/dQ can be seen as a density function of P with respect to the background measure Q. Moreover, dP/dQ is the limit of the martingale P/Q (Durrett, 2010, Sec. 5.3.3) which exists Q-almost surely according to Theorem 2.8.\nThe following proposition characterizes the notions of compatibility from Section 3.2 in terms of the martingale Q/P .\nProposition 3.23 (Martingales and Compatibility). The following relationships hold between Q, P , and the P -martingale Yt := Q(x1:t)/P (x1:t). (a) Q \u00d7\u2265 P if and only if Yt \u2265 c > 0 for all t \u2208 N.\n(b) Q P if and only if P -almost surely Yt 6\u2192 0 as t\u2192\u221e.\n(c) Q dominates P with coefficients f if and only if Yt \u2265 1/f(t) for all t.\n(d) Q \u00d7\u2265W P if and only if P -almost surely log(Yt+1/Yt)\u2192 0 in Ces\u00e0ro average.\n(e) Q L P if and only if P -almost surely Yt > 0 for all t \u2208 N.\nProof. (Yt)t\u2208N is a P -martingale according to Theorem 3.18.\n(a) Q(x) \u2265 cP (x) with c > 0 for all x \u2208 X \u2217 is equivalent to Q(x)/P (x) \u2265 c > 0 for all x \u2208 X \u2217."}, {"heading": "32 Learning", "text": "(b) Proved by Hutter (2009a, Lem. 3i).\n(c) Analogously to the proof of (a).\n(d) If Q weakly dominates P , we get \u2212 log Yt \u2208 o(t) according to Remark 3.9. Together with Y0 = 1 we get \u2212 log Yt = \u2211t\u22121 k=0\u2212 log(Yk+1/Yk) \u2208 o(t), therefore\nt\u22121 \u2211t\u22121\nk=0\u2212 log(Yk+1/Yk) \u2192 0 as t \u2192 \u221e. Conversely, if the Ces\u00e0ro average converges to 0, then t\u22121 log Yt \u2192 0, hence \u2212 log Yt \u2208 o(t).\n(e) Let x \u2208 X \u2217 be any finite string. If Q L P and P (x) > 0, then Q(x) > 0, and hence Q(x)/P (x) > 0. Conversely, if P (x) > 0 then Yt is well-defined, so if Y|x|(x) > 0 then Q(x) > 0.\nFrom Proposition 3.23b and Theorem 3.22 we get that Q P if and only if the Radon-Nikodym derivative dQ/dP is positive on a set of P -measure 1."}, {"heading": "3.4 Merging", "text": "If Q is capable of learning, it should use the sequence x drawn from P to change its opinions more in the direction of P . More precisely, we want Q( \u00b7 | x<t) \u2248 P ( \u00b7 | x<t) for large t. In the rest of this chapter, we make this notion of closeness precise and discuss different conditions on Q that are sufficient for learning.\nStrong merging implies that the belief of any hypothesis merges. This is very strong, as hypotheses can talk about tail events: events that are independent of any finite initial part of the infinite sequence (such as the event A in Remark 3.17). Weak merging only considers hypothesis about the next couple of symbols, and almost weak merging allows Q to deviate from P in a vanishing fraction of the time. Much of this section is based on Kalai and Lehrer (1994) and Lehrer and Smorodinsky (1996)."}, {"heading": "3.4.1 Strong Merging", "text": "Definition 3.24 (Strong Merging). Q merges strongly with P iff D\u221e(P,Q | x<t)\u2192 0 as t\u2192\u221e P -almost surely.\nThe following theorem is the famous merging of opinions theorem by Blackwell and Dubins (1962).\nTheorem 3.25 (Absolute Continuity\u21d2 Strong Merging; Blackwell and Dubins, 1962). If P is absolutely continuous with respect to Q, then Q merges strongly with P .\nExample 3.26 (The Black Ravens 2; Rathmanner and Hutter, 2011, Sec. 7.4). Recall the black raven problem from Example 3.1. Let Q be a learning distribution that dominates the true distribution P , such as a Bayesian mixture (Example 3.4). By Proposition 3.16a we get Q P , and hence Q merges strongly to P by Theorem 3.25. Thus we get as t \u2192 \u221e that P -almost surely |Q(H | x<t) \u2212 P (H | x<t)| \u2192 0 for the hypothesis H that \u2018all ravens are black\u2019 defined in (3.1). Thus if all ravens are black in the real world (P (H) = 1), Q learns this asymptotically (Q(H | x<t) \u2192 1). This is\n\u00a73.4 Merging 33\nthe solution we desired: the learning distribution Q converges to a true belief about an infinite set by only looking from a finite (but growing) number of data points. 3\nThe following is the converse of Theorem 3.25.\nTheorem 3.27 (Strong Merging \u2227 Local Absolute Continuity\u21d2 Absolute Continuity; Kalai and Lehrer, 1994, Thm. 2). If Q is locally absolutely continuous with respect to P and Q merges strongly with P , then P is absolutely continuous with respect to Q.\nThe following result shows that local absolute continuity is not required for strong merging: recall that according to Example 3.14 the MDL distribution is not locally absolutely continuous with respect to every P from the classM.\nTheorem 3.28 (Strong Merging for MDL; Hutter, 2009a, Thm. 1). If P \u2208M, then\nD\u221e(P,MDL x | x)\u2192 0 as |x| \u2192 \u221e P -almost surely.\nLet M be a (possibly uncountable) set of probability measures on (X\u221e,F\u221e). Ryabko (2010, Thm. 4) shows that if there is a Q that merges strongly with every P \u2208 M, then there is a Bayesian mixture over a countable subset of M that also merges strongly with every P \u2208M."}, {"heading": "3.4.2 Weak Merging", "text": "In Definition 3.24 the supremum ranges over all measurable sets A \u2208 F\u221e which includes tail events. Instead, we may restrict the supremum to the next few symbols. This is known as weak merging.\nDefinition 3.29 (Weak Merging). Q weakly merges with P iff for every d \u2208 N, Dt+d(Q,P | x<t)\u2192 0 as t\u2192\u221e P -almost surely.\nThe following lemma gives an equivalent formulation of weak merging.\nLemma 3.30 (Lehrer and Smorodinsky, 1996, Rem. 5). Q weakly merges with P if and only if Dt(Q,P | x<t)\u2192 0 as t\u2192\u221e P -almost surely.\nUnfortunately, weak dominance is not sufficient for weak merging (Lehrer and Smorodinsky, 1996, Ex. 10). We need the following stronger condition, that turns out to be (almost) necessary. In the following, let Yt := Q(x1:t)/P (x1:t) denote the P -martingale from Proposition 3.23.\nTheorem 3.31 (Kalai and Lehrer, 1994, Prop. 5a). If P -almost surely Yt+1/Yt \u2192 1, then Q merges weakly with P .\nExample 3.32 (Laplace Rule 2). Suppose we use the Laplace rule from Example 3.12 to predict a Bernoulli(r) process. By the strong law of large numbers, \u03c1L(xt | x<t)\u2192 r almost surely. Therefore we can use Theorem 3.31 to conclude that \u03c1L merges weakly with Bernoulli(r) for all r \u2208 [0, 1]. (Note that strongly merging with every Bernoulli process is impossible; Ryabko, 2010, p. 7) 3"}, {"heading": "34 Learning", "text": "The following is a converse to Theorem 3.31.\nTheorem 3.33 (Kalai and Lehrer, 1994, Prop. 5b). If Q merges weakly with P , then Yt+1/Yt \u2192 1 in P -probability.\nUnfortunately, weak dominance is not enough to guarantee weak merging.\nExample 3.34 (Weak Dominance;WeakMerging; Ryabko and Hutter, 2007, Prop. 7). Let X = {0, 1} and let f be any arbitrarily slowly monotone growing function with f(t)\u2192\u221e. Define P (1\u221e) := 1, the sequence (ti)i\u2208N such that f(ti+1) \u2265 2f(ti), and\nQ(xt | x<t) :=  1 2 if t = ti for some i \u2208 N, 1 if t 6= ti and xt = 1, and 0 otherwise.\nNow Q dominates P with coefficients f by construction and Q weakly dominates P if f grows subexponentially. However, |Q(1 | 1t) \u2212 P (1 | 1t)| \u2265 1/2 for infinitely many t \u2208 N. 3"}, {"heading": "3.4.3 Almost Weak Merging", "text": "The following definition is due to Lehrer and Smorodinsky (1996, Def. 10).\nDefinition 3.35 (Almost Weak Merging). Q almost weakly merges with P iff for every d \u2208 N\n1\nt t\u2211 k=1 Dt+d(Q,P | x<t)\u2192 0 as t\u2192\u221e P -almost surely.\nThere is also an analogue of Lemma 3.30 for almost weakly merging in the sense that we can equivalently set d = 0 (Lehrer and Smorodinsky, 1996, Rem. 6).\nRemark 3.36 (Weak Merging and Merging in KL-Divergence). From Lemma 2.13 follows that weak merging is implied by KLd(P,Q | x) \u2192 0 P -almost surely and almost weak merging is implied by \u2211t k=1 KL1(P,Q | x<k) \u2208 o(t) P -almost surely, i.e., KLt(P,Q) \u2208 o(t) (Ryabko and Hutter, 2008, Lem. 1). The converse is generally false. 3\nThe following proposition relates the three notions of merging.\nProposition 3.37 (Strong Merging \u21d2 Weak Merging \u21d2 Almost Weak Merging). If Q merges strongly with P , then Q merges weakly with P . If Q merges weakly with P , then Q merges almost weakly with P .\nProof. Follows immediately from the definitions.\nTheorem 3.38 (Weak Dominance\u21d2 Almost Weak Merging; Lehrer and Smorodinsky, 1996, Thm. 4). If Q weakly dominates P , then Q merges almost weakly with P .\n\u00a73.5 Predicting 35\nFrom Theorem 3.38 we get that the speed prior (Example 3.11) merges almost weakly with any probability distribution estimable in polynomial time.\nWe also have the following converse to Theorem 3.38.\nTheorem 3.39 (Almost Weak Merging\u21d2Weak Dominance; Lehrer and Smorodinsky, 1996, Cor. 7). If Q is locally absolutely continuous with respect to P , Q merges almost weakly with P , and P -almost surely lim inft\u2192\u221e Yt+1/Yt > 0, then Q weakly dominates P ."}, {"heading": "3.5 Predicting", "text": "In Section 3.4 we wanted Q to acquire the correct beliefs about P . In this section, we exploit the accuracy of our beliefs for predicting individual symbols. We derive bounds on the number of errors Q makes when trying to predict a string drawn from P .\nSince the data drawn from P is stochastic, we cannot expect to make a finite number of errors. Even the perfect predictor that knows P generally makes an infinite number of errors. For example, trying to predict the Lebesgue measure \u03bb (Example 2.15), in expectation we make half an error in every time step. So instead we are asking about the asymptotic error rate of a predictor based on Q compared to a predictor based on P , the prediction regret .\nLet xRt be the t-th symbol predicted by the probability measure R according to the maximum likelihood estimator:\nxRt :\u2208 arg max a\u2208X R(x<ta | x<t). (3.4)\nThe instantaneous error of a R-based predictor is defined as\neRt := { 0 if xt = xRt , and 1 otherwise.\nand the cumulative error is\nERt := t\u2211 k=1 eRk .\nNote that both et and Et are random variables.\nDefinition 3.40 (Prediction Regret). In time step t the prediction regret is EQt \u2212 EPt and the expected prediction regret is E [ EQt \u2212 EPt ] .\nMore generally, we could also follow Hutter (2001b) and phrase predictive performance in terms of loss: given a loss function ` : X \u00d7X \u2192 R the predictor Q suffers an (instantaneous) loss of `(xQt , xPt ) in time step t. If the loss function ` is bounded in [0, 1], many of the results for prediction regret also hold for cumulative loss (for Section 3.5.2 we also need `(a, a) = 0 for all a \u2208 X ). In this chapter we chose to phrase the results in terms of prediction errors instead of loss because prediction errors are conceptionally simpler."}, {"heading": "36 Learning", "text": "Example 3.41 (Good Prediction Regret ; Merging/Compatibility). Good prediction regret does not imply (weak/strong) merging or (weak) dominance: Let P := Bernoulli(1/3) and Q := Bernoulli(1/4). Clearly P and Q do not merge (weakly) or (weakly) dominate each other. However, a P -based predictor always predicts 0, and so does a Q-based predictor. Therefore the prediction regret EQt \u2212 EPt is always 0. 3\nExample 3.42 (Adversarial Sequence; Legg, 2006, Lem. 4). No learning distribution Q will learn to predict everything. We can always define a Q-adversarial sequence z1:\u221e recursively according to\nzt := { 0 if Q(0 | z<t) < 1/2, and 1 if Q(0 | z<t) \u2265 1/2.\nIn every time step the probability that a Q-based predictor makes an error is at least 1/2, hence eQt \u2265 1/2 and E Q t \u2265 t/2. But z1:\u221e is a deterministic sequence, thus an informed predictor makes zero errors. Therefore the prediction regret of Q on the sequence z1:\u221e is linear. 3"}, {"heading": "3.5.1 Dominance", "text": "We start with the prediction regret bounds proved by Hutter (2001b) in case the learning distribution Q dominates the true distribution P . In the following, let cP denote the constant from Definition 3.3.\nTheorem 3.43 (Hutter, 2007b, Eq. 5 & 8). For all P and Q,\u221a EPEQn \u2212 \u221a EPEPn \u2264 \u221a 2KLn(P,Q).\nThe following bound on prediction regret then follows easily, but it is a factor of \u221a 2\nworse than the bound stated by Hutter (2005, Thm. 3.36).\nCorollary 3.44 (Expected Prediction Regret). For all P and Q,\n0 \u2264 EP [ EQn \u2212 EPn ] \u2264 2KLn(P,Q) + 2 \u221a 2KLn(P,Q)EPEPn .\nProof. From Theorem 3.43 we get\nEP [ EQn \u2212 EPn ] = (\u221a EPEQn + \u221a EPEPn )(\u221a EPEQn \u2212 \u221a EPEPn ) \u2264 (\u221a EPEQn + \u221a EPEPn )\u221a 2KLn(P,Q)\n\u2264 (\u221a 2KLn(P,Q) + \u221a EPEPn + \u221a EPEPt )\u221a 2KLn(P,Q)\n= 2KLn(P,Q) + 2 \u221a 2KLn(P,Q)EPEPn .\n\u00a73.5 Predicting 37\nIf Q dominates P , then we have KLn(P,Q) \u2264 \u2212 ln cP :\nKLn(P,Q) = \u2211 x\u2208Xn P (x) log P (x) Q(x) \u2264 \u2211 x\u2208Xn P (x) log 1 cP = \u2212 log cP (3.5)\nThis invites the following corollary.\nCorollary 3.45 (Prediction Regret for Dominance; Hutter, 2005, Cor. 3.49). If Q dominates P , then the following statements hold.\n(a) EPEQ\u221e is finite if and only if EPEP\u221e is finite. (b) \u221a EPEQ\u221e \u2212 \u221a EPEP\u221e \u2208 O(1)\n(c) EPEQt /EPEPt \u2192 1 for EPEPt \u2192\u221e. (d) EP [ EQt \u2212 EPt ] \u2208 O (\u221a EPEPt ) .\nIf the true distribution P is deterministic, we can improve on these bounds:\nExample 3.46 (Predicting a Deterministic Measure). Suppose we are predicting a deterministic measure P that assigns probability 1 to the infinite string x1:\u221e. If P is dominated by Q, the total expected prediction regret EPEQ\u221e is bounded by \u22122 ln cP by Corollary 3.44. This is easy to see: every time we predict a wrong symbol a 6= xt, then Q(a | x<t) \u2265 Q(xt | x<t), so Q(xt | x<t) \u2264 1/2. Therefore Yt \u2264 Yt\u22121/2 and by dominance Yt \u2265 cP . Hence a prediction error can occur at most \u2212 log cP times. 3\nGenerally, the O(EPEPt ) bounds on expected prediction regret given in Corollary 3.45 are essentially unimprovable:\nExample 3.47 (Lower Bounds on Prediction Regret). Set X := {0, 1} and consider the uniform measure \u03bb from Example 2.15. For each time step t, we have \u03bb(0 | x<t) = \u03bb(1 | x<t) = 1/2, so the argmax in (3.4) ties and hence it does not matter whether we predict 0 or 1. We take two predictors P and Q, where P always predicts 0 and Q always predicts 1. Let Zt := E Q t \u2212 EPt . Since their predictions never match, Zt is an ordinary random walk with step size 1. We have (Weisstein, 2002)\nlim sup t\u2192\u221e EP [EQt \u2212 EPt ]\u221a t\n= \u221a 2/\u03c0\nand for the law of the iterated logarithm (Durrett, 2010, Thm. 8.8.3)\nlim sup t\u2192\u221e EQt \u2212 EPt\u221a 2t log log t = 1 P -almost surely.\nBoth bounds are known to be asymptotically tight. 3\nWhile Example 3.47 shows that the bounds from Corollary 3.45 are asymptotically tight, they are misleading because in most cases, we can do much better. According"}, {"heading": "38 Learning", "text": "to the following theorem, the worst case bounds are only attained if P (xt | x<t) is sufficiently close to 1/2.\nTheorem 3.48 (Expected Prediction Regret for Nonuniform Measures). If X = {0, 1} and there is an \u03b5 > 0 such that |P (xt | x<t)\u2212 1/2| \u2265 \u03b5 for all x1:t \u2208 X \u2217, then\nEP [ EQt \u2212 EPt ] \u2264 KLt(P,Q)\n\u03b5 .\nProof. Recall the definition of entropy in nats:\nEnt(p) := \u2212p ln p\u2212 (1\u2212 p) ln(1\u2212 p).\nThe second order Taylor approximation of Ent at 1/2 is\nf(p) = ln 2\u2212 2(p\u2212 12) 2.\nOne can check that f(p) \u2265 Ent(p) for all 0 \u2264 p \u2264 1. Define p := P (xPt | x<t) \u2265 1/2 and q := Q(xQt | x<t) \u2265 1/2 to ease notation. Consider the function\ng(p, q, \u03b5) := p\u2212 (1\u2212 p)\u2212 \u03b5\u22121 ( p ln p1\u2212q + (1\u2212 p) ln 1\u2212p q ) which is strictly increasing as q decreases, so from q \u2265 1/2 we get\ng(p, q, \u03b5) \u2264 2p\u2212 1\u2212 \u03b5\u22121 ln 2 + \u03b5\u22121Ent(p) ln 2 \u2264 2p\u2212 1\u2212 \u03b5\u22121 ln 2 + \u03b5\u22121f(p) ln 2 = 2p\u2212 1\u2212 \u03b5\u221212(p\u2212 12) 2,\nwhich decreases as p increases, hence it is maximized for p = 1/2 + \u03b5,\ng(p, q, \u03b5) \u2264 2\u03b5\u2212 \u03b5\u221212\u03b52 = 0\nTherefore g is nonpositive. If xQt = xPt , the one-step error is 0. Otherwise EP [et | x<t] = p \u2212 (1 \u2212 p) and g(p, q, \u03b5) = EP [et | x<t] \u2212 \u03b5\u22121KL1(P,Q | x<t), so we get EP [et | x<t] \u2264 \u03b5\u22121KL1(P,Q | x<t). Summing this from t = 1 to n yields the claim.\nE [ EQn \u2212 EPn ] \u2264 \u03b5\u22121KLn(P,Q)."}, {"heading": "3.5.2 Absolute Continuity", "text": "Theorem 3.49 (Prediction with Absolute Continuity). If Q P , then\u221a EQt \u2212 \u221a EPt \u2264 O (\u221a log log t ) P -almost surely.\nThe proof idea is inspired by Miller and Sanchirico (1999). We think of P and Q as two players in a zero-sum betting game. In every time step t, the players will make a\n\u00a73.5 Predicting 39\nbet on the outcome of xt. If xt = x Q t 6= xPt , then Q wins $1 from P , if xt = xPt 6= x Q t , then Q loses $1 to P . Otherwise xQt = xPt or x Q t 6= xt 6= xPt and neither player gains or loses money. Since Q predicts according to the maximum likelihood principle (3.4), it is rational to accept the bet from Q\u2019s perspective. In Q\u2019s eyes, the worst case is a fair bet, so Q will not lose more money than it would lose on a random walk. The law of the iterated logarithm gives a Q-probability one statement about this bound, which transfers to P by absolute continuity.\nProof. Define the stochastic process Zt := E Q t \u2212 EPt . Since EQ[eRt ] = Q(xRt | x<t), we get\nEQ[Zt+1 | Ft] = Q(xQt | x<t)\u2212Q(xPt | x<t) + Zt \u2265 Q(xQt | x<t)\u2212Q(x Q t | x<t) + Zt = Zt,\nhence (Zt)t\u2208N is a Q-submartingale. In the worst case (for Q), (Zt)t\u2208N is just a random walk with step size 1. But Zt can only move if Q and P predict a different symbol. If this happens, at least one of them makes an error. Let mt be the number of steps Zt has moved (Zt+1 6= Zt). Then mt \u2264 EQt + EPt and mt \u2264 t. By the law of the iterated logarithm (Durrett, 2010, Thm. 8.8.3),\nlim inf t\u2192\u221e Zt\u221a 2mt log logmt = \u22121\nQ-almost surely. We define the event\nA := { \u2203C \u2200t. Zt \u2265 \u2212C \u221a mt log logmt } .\nThen Q(A) = 1, hence P (A) = 1 by absolute continuity.\nEQt \u2212 EPt = Zt \u2264 C \u221a (EQt + E P t ) log log t \u2264 C (\u221a EQt + \u221a EPt )\u221a log log t\nDividing both sides by \u221a EQt + \u221a EPt yields that there is a P -almost surely finite random\nvariable C such that \u221a EQt \u2212 \u221a EPt \u2264 C \u221a log log t.\nThis invites the following immediate corollary.\nCorollary 3.50 (Prediction Regret for Absolute Continuity). If Q P , then\nEQt \u2212 EPt \u2208 O ( log log t+ \u221a EPt log log t ) P -almost surely.\nProof. Analogously to the proof of Corollary 3.44.\nWhile Corollary 3.50 establishes an almost sure prediction regret bound, it is different from the bound on expected prediction regret from Corollary 3.44; bounds on"}, {"heading": "40 Learning", "text": "E[EQt \u2212 EPt ] are incomparable to almost sure bound given in Theorem 3.49: for a sequence of nonnegative (unbounded) random variables convergence in mean does not imply almost sure convergence (Stoyanov, 2013, Sec. 14.7) or vice versa (Stoyanov, 2013, Sec. 14.8ii).\nWe proceed to establish an improved prediction regret bound in case P is nonuniform analogously to Theorem 3.48.\nTheorem 3.51 (Prediction Regret for Nonuniform Measures). If Q P , X = {0, 1}, and there is an \u03b5 > 0 such that with P -probability 1\n|P (xt | x<t)\u2212 1/2| \u2265 \u03b5\nfor all t \u2208 N, then P -almost surely EQt \u2212 EPt \u2208 O(1).\nProof. If |P (xt | x<t) \u2212 1/2| \u2265 \u03b5, then for large enough t, Q will have merged with P (Theorem 3.25) and hence |Q(xt | x<t)\u2212 1/2| \u2265 \u03b5/2 infinitely often.\nThus Zt has an expected gain of \u03b5/2 if the predictors disagree. Therefore Zt \u2192\u221e Q-almost surely. Consequently, the set\nA := {\u2203t0 \u2200t \u2265 t0. Zt \u2265 0}\nhas Q-measure 1. By absolute continuity, it also has P -measure 1, hence there is a P -almost surely finite random variable C such that for all t, Zt \u2265 \u2212C.\nThere is another argument that we could use to show that under the condition of Theorem 3.51 EQt \u2212EPt is almost surely finite: If P is absolutely continuous with respect to Q, then Q merges strongly with P and hence Q merges weakly with P . Therefore almost surely there is a t0 such that for all t \u2265 t0 we have |Q(xPt | x<t)\u2212P (xPt | x<t)| < \u03b5, thus xQt = xPt for t \u2265 t0."}, {"heading": "3.5.3 Dominance with Coefficients", "text": "Lemma 3.52 (KL Divergence and Dominance With Coefficients). If Q dominates P with coefficients f , then KLt(Q,P ) \u2264 ln f(t).\nProof. Analogous to (3.5).\nThis lets us derive an analogous regret bound to Corollary 3.44.\nCorollary 3.53 (Expected Prediction Regret for Dominance With Coefficients). If Q dominates P with coefficients f , then\nEP [ EQn \u2212 EPn ] \u2264 2 ln f(t) + 2 \u221a 2EPEPn ln f(t).\nProof. Apply Lemma 3.52 to Corollary 3.44.\nFor weak dominance we get sublinear prediction regret.\n\u00a73.6 Learning with Algorithmic Information Theory 41\nCorollary 3.54 (Sublinear Prediction Regret for Weak Dominance). If Q weakly dominates P , then EP [EQn \u2212 EPn ] \u2208 o(t).\nProof. By Remark 3.9 ln f \u2208 o(t). Applying Corollary 3.53 we get\nEP [ EQn \u2212 EPn ] \u2264 2o(t) + 2 \u221a 2EPEPn o(t) \u2264 2o(t) + 2 \u221a 2O(t)o(t) \u2208 o(t)."}, {"heading": "3.6 Learning with Algorithmic Information Theory", "text": "Algorithmic information theory provides a theoretical framework to apply the probability theory results from the previous sections. In the following we discuss Solomonoff\u2019s famous theory of induction (Section 3.6.1), the speed prior (Section 3.6.2), and learning with a universal compression algorithm (Section 3.6.3)."}, {"heading": "3.6.1 Solomonoff Induction", "text": "Solomonoff (1964, 1978) proposed a theory of learning, also known as universal induction or Solomonoff induction. It encompasses Ockham\u2019s razor by favoring simple explanations over complex ones, and Epicurus\u2019 principle of multiple explanations by never discarding possible explanations. See Rathmanner and Hutter (2011) for a very readable introduction to Solomonoff\u2019s theory and its philosophical motivations and Sterkenburg (2016) for a critique of its optimality.\nAt the core of this theory is Solomonoff\u2019s distribution M , as defined in Example 3.5. SinceM dominates all lower semicomputable semimeasures, we get all the merging and prediction results from Section 3.4 and Section 3.5: when drawing a string from any computable measure P , M arrives at the correct belief for any hypothesis.\nCorollary 3.55 (Strong Merging for Solomonoff Induction). M merges strongly with every computable measure.\nProof. From Proposition 3.16a and Theorem 3.25.\nCorollary 3.56 (Expected Prediction Regret for Solomonoff Induction). For all computable measures P ,\nEP [ EMt \u2212 EPt ] \u2264 K(P ) ln 4 + \u221a 2EPEPt K(P ) ln 16.\nProof. From Corollary 3.44 and cP = 2\u2212K(P ).\nRemark 3.57 (Converging Fast and Slow). The convergence of M to a computable P is fast in the sense of Corollary 3.56: M cannot make many more prediction errors than P in expectation. When predicting an infinite computable sequence x1:\u221e, the total number of prediction errors is bounded by |p|2 ln 2 \u2248 1.4|p| where p is a program that generates x1:\u221e (Example 3.46).\nThe convergence of M to P is also slow in the sense that M(xt | x<t) \u2192 1 slower than any computable function since 1\u2212M(xt | x<t) \u00d7\u2265 2\u2212minn\u2265tK(n) for all t. 3"}, {"heading": "42 Learning", "text": "The bound from Corollary 3.56 is not optimal. Even if we knew the program p generating the sequence x1:\u221e, there might be a shorter program p\u2032 that computes x1:\u221e; hence the improved bound EM\u221e \u2264 |p\u2032|2 ln 2 also holds. Since Kolmogorov complexity is incomputable, we can\u2019t find the \u2018best\u2019 bound algorithmically.\nSolomonoff induction may even converge on some incomputable measures.\nExample 3.58 (M Converges on Some Incomputable Measures). Let r be an incomputable real number. Then the measure P := Bernoulli(r) is not computable and M is not absolutely continuous with respect to P : for\nA := { x \u2208 X\u221e \u2223\u2223\u2223 lim t\u2192\u221e ones(x1:t) = r }\nwe have P (A) = 1 but M(A) = 0. Since M L P we get from Theorem 3.27 that M does not merge with P . Nevertheless, M still succeeds at prediction because it dominates Bernoulli(q) for each rational q and the rationals are dense around r. According to Lehrer and Smorodinsky (1996, Lem. 3), this implies that M weakly dominates P and by Theorem 3.38 M almost weakly merges to P . 3\nThe fact that M does not merge strongly with every Bernoulli(r) process is not a failure of Solomonoff\u2019s prior. Ryabko (2010, p. 7) shows that for the class of all Bernoulli measures there is no probability measure that merges strongly with each of them.\nThe definition of M has only one parameter: the choice of the universal Turing machine. The effect of this choice on the function K can be uniformly bounded by a constant by the invariance theorem (Li and Vit\u00e1nyi, 2008, Thm. 3.1.1). Hence the choice of the UTM changes the prediction regret bound from Corollary 3.56 only by a constant. This constant can be large, preventing any finite-time guarantees that are independent of the UTM. However, asymptotically Solomonoff induction succeeds even for terrible choices of the UTM.\nThe Solomonoff normalization Mnorm of M is defined according to Definition 2.16. While Mnorm dominates M according to Lemma 2.17 and thus every lower semicomputable semimeasure, in some respects, Mnorm behaves a little differently from M . Another way to complete the semimeasure M into a measure is given in the following example.\nExample 3.59 (The Measure Mixture; G\u00e1cs, 1983, p. 74). The measure mixture M is defined as\nM(x) := lim n\u2192\u221e \u2211 y\u2208Xn M(xy). (3.6)\nIt is the same as M except that the contributions by programs that do not produce infinite strings are removed: for any such program p, let k denote the length of the finite string generated by p. Then for |xy| > k, the program p does not contribute to M(xy), hence it is excluded from M(x).\nSimilarly toM , the measure mixtureM is not a (probability) measure sinceM( ) < 1; but in this case normalization (2.2) is just multiplication with the constant 1/M( ), leading to the normalized measure mixture Mnorm. 3\n\u00a73.6 Learning with Algorithmic Information Theory 43\nEven thoughM merges strongly with any computable measure P with P -probability 1, Lattimore and Hutter (2013, 2015) show that generally it does not hold for all MartinL\u00f6f random sequences (which also form a set of P -probability 1). Hutter and Muchnik (2007, Thm. 6) construct non-universal lower semicomputable semimeasures that have this convergence property for all P -Martin-L\u00f6f random sequences. For infinite nonrandom sequences whose bits are selectively predicted by some total recursive function, Lattimore et al. (2011, Thm. 10) show that the normalized Solomonoff measure Mnorm converges to 1 on the selected bits. This does not hold for the unnormalized measure M (Lattimore et al., 2011, Thm. 12)."}, {"heading": "3.6.2 The Speed Prior", "text": "Solomonoff\u2019s prior M is incomputable (Theorem 6.3); a computable alternative is the speed prior from Example 3.11. In this section we state merging and prediction results for SKt, a speed prior introduced by Filan et al. (2016) formally defined in Example 3.11. It is slightly different from the speed prior defined by Schmidhuber (2002), but for the latter no compatibility properties are known for nondeterministic measures.\nDefinition 3.60 (Estimable in Polynomial Time). A function f : X \u2217 \u2192 R is estimable in polynomial time iff there is a function g : X \u2217 \u2192 R computable in polynomial time such that f \u00d7= g.\nFor a measure P estimable in polynomial time the speed prior SKt dominates P with coefficients polynomial in |x| \u2212 logP (x) (Filan et al., 2016, Eq. 12). Thus SKt weakly dominates P and we get the following results.\nCorollary 3.61 (Almost Weak Merging for SKt). SKt almost weakly merges with every measure estimable in polynomial time.\nProof. From Theorem 3.38 and Filan et al. (2016, Eq. 12) since logP does not grow superexponentially P -almost surely.\nCorollary 3.62 (Expected Prediction Regret for SKt; Filan et al., 2016, Thm. 9). For all measures P estimable in polynomial time,\nEP [ ESKtn \u2212 EPn ] \u2208 O ( log n+ \u221a EPEP\u221e log n ) .\nProof. From Corollary 3.44 and Filan et al. (2016, Eq. 14)."}, {"heading": "3.6.3 Universal Compression", "text": "Solomonoff\u2019s distribution can be approximated using a standard compression algorithm, motivated by the similarityM(x) \u2248 2\u2212Km(x), where Km denotes monotone Kolmogorov complexity. The function Km is a universal compressor , compressing at least as well as any other recursively enumerable program.\nG\u00e1cs (1983) shows that the similarity M \u2248 2\u2212Km is not an equality. However, the difference between \u2212 logM and Km is very small: the best known lower bound is due"}, {"heading": "44 Learning", "text": "to Day (2011) who shows that Km(x) > \u2212 logM(x) +O(log log |x|) for infinitely many x \u2208 X \u2217.\nNevertheless, 2\u2212Km dominates every computable measure (Li and Vit\u00e1nyi, 2008, Thm. 4.5.4 and Lem. 4.5.6ii(d); originally proved by Levin, 1973). Hence all the strong results that hold for Solomonoff induction (prediction regret and strong merging) also hold for compression: we apply Theorem 3.25 and Corollary 3.44 to get the following results. See Hutter (2006a) for further discussion on using the universal compressor Km for learning.\nCorollary 3.63 (Strong Merging for Universal Compression). The distribution 2\u2212Km(x) merges strongly with every computable measure.\nCorollary 3.64 (Expected Prediction Regret for Universal Compression). For Q(x) := 2\u2212Km(x) and for all computable measures P there is a constant cP such that\nEP [ EQt \u2212 EPt ] \u2264 cP + \u221a cPEPEPt .\nThis provides a theoretical basis for viewing compression as a general purpose learning algorithm. In this spirit, the Hutter prize is awarded for the compression of a 100MB excerpt from the English Wikipedia (Hutter, 2006c).\nPractical compression algorithms (such as the algorithm by Ziv and Lempel (1977) used in gzip) are not universal. Hence they do not dominate every computable distribution. As with the speed prior, what matters is the rate at which Yt = Q(x1:t)/P (x1:t) goes to 0, i.e., does the compressor weakly dominate the true distribution in the sense of Definition 3.8?\nVeness et al. (2015) successfully apply the Lempel-Ziv compression algorithm as a learning algorithm for reinforcement learning; however, some preprocessing of the data is required. More remotely, Vit\u00e1nyi et al. (2009) use standard compression algorithms to classify mammal genomes, languages, and classical music."}, {"heading": "3.7 Summary", "text": "Ultimately, whether learning succeeds depends on the rate at which the nonnegative P -martingale Q/P goes to 0 (when drawing from P ). If Q/P does not converge to zero, then Q merges strongly with P and thus arrives at correct beliefs about any hypothesis, including tail events. If Q/P converges to zero subexponentially, then Q merges almost weakly with P and thus asymptotically has incorrect beliefs about the immediate future only a vanishing fraction of the time.\nCorollary 3.44 bounds the expected prediction regret by the KL-divergence between P and Q plus a \u221a EPEPt term. The KL-divergence is in turn bounded by the rate at which Q/P goes to zero. It is constant if Q dominates P and bounded by ln f if Q dominates P with coefficients f . If Q weakly dominates P , then the KL-divergence is sublinear. We also derived bounds on the prediction regret for absolute continuity (Section 3.5.2). Remarkably, the bounds are only log log t worse than the bound we get from dominance. Moreover, they hold almost surely instead of in expectation.\n46 Learning\n\u00a73.7 Summary 47\nNext, we showed that the \u221a EPEPt term is generally unimprovable (Example 3.47). However, it comes only from predicting measures that assign probabilities close to 1/2. If we can bound P away from 1/2, then the \u221a EPEPt term disappears (Theorem 3.48 and Theorem 3.51). Table 3.1 lists our learning distributions. The Bayesian mixture is the strongest since it dominates every measure from the given classM (Example 3.4). The minimum description length model MDLx does not have this property, yet it still merges strongly with every measure from the class (Example 3.14 and Theorem 3.28). The Laplace rule is only useful for learning i.i.d. measures; it merges weakly with every Bernoulli process (Example 3.12 and Example 3.32). We also discussed some learning distributions from algorithmic information theory. Solomonoff\u2019s prior is a Bayesian mixture over all lower semicomputable semimeasures (Example 3.5 and Wood et al., 2011). Like the universal compressor it dominates and hence merges strongly with all computable measures. The speed prior dominates all probability measures estimable in polynomial time with polynomial coefficients (Example 3.11), and thus merges weakly with each of them.\nTable 3.2 summarizes the results from this chapter and Figure 3.1 illustrates their logical relationship and their origin.\nWe conclude this chapter with a paradox from the philosophy of science.\nRemark 3.65 (The Paradox of Confirmation). Recall the black raven problem introduced in Example 3.1; the hypothesis \u2018all ravens are black\u2019 is denoted with H. The paradox of confirmation, also known as Hempel\u2019s paradox (Hempel, 1945), relies on the following three principles.\n\u2022 Nicod\u2019s criterion (Nicod, 1961, p. 67): observing an F that is a G increases our belief in the hypothesis that all F s are Gs.\n\u2022 The equivalence condition: logically equivalent hypotheses are confirmed or disconfirmed by the same evidence.\n\u2022 The paradoxical conclusion: a green apple confirms H.\nThe argument goes as follows. The hypothesisH is logically equivalent to the hypothesis H \u2032 that all non-black objects are non-ravens. According to Nicod\u2019s criterion, any nonblack non-raven, such as a green apple, confirms H \u2032. But then the equivalence condition entails the paradoxical conclusion.\nThe paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey. Support for Nicod\u2019s criterion is not uncommon (Mackie, 1963; Hempel, 1967; Maher, 1999) and no consensus is in sight.\nA Bayesian reasoner might be tempted to argue that a green apple does confirm the hypothesis H, but only to a small degree, since there are vastly more non-black objects than ravens (Good, 1960). This leads to the acceptance of the paradoxical conclusion,"}, {"heading": "48 Learning", "text": "and this solution to the confirmation paradox is known as the standard Bayesian solution. Vranas (2004) shows that this solution is equivalent to the assertion that blackness is equally probable regardless of whether H holds: P (black|H) \u2248 P (black).\nThe following is a very concise example against the standard Bayesian solution by Good (1967): There are two possible worlds, the first has 100 black ravens and a million other birds, while the second has 1000 black ravens, one white raven, and a million other birds. Now we draw a bird uniformly at random, and it turns out to be a black raven. Contrary to what Nicod\u2019s criterion claims, this is strong evidence that we are in fact in the second world, and in this world non-black ravens exist.\nFor another, more intuitive example: Suppose you do not know anything about ravens and you have a friend who collects atypical objects. If you see a black raven in her collection, surely this would not increase your belief in the hypothesis that all ravens are black.\nIn Leike and Hutter (2015d) we investigate the paradox of confirmation in the context of Solomonoff induction. We show that the paradoxical conclusion is avoided because Solomonoff induction violates Nicod\u2019s criterion: There are time steps when (counterfactually) observing a black raven disconfirms the hypothesis that all ravens are black. When predicting a deterministic computable sequence Nicod\u2019s criterion is even violated infinitely often. However, if we normalize Solomonoff\u2019s prior and observe a deterministic computable infinite string, Nicod\u2019s criterion is violated at most finitely many times. These results are independent of the choice of the universal Turing machine.\nWe must conclude that violating Nicod\u2019s criterion is not a fault of Solomonoff induction. Instead, we should accept that for Bayesian reasoning Nicod\u2019s criterion, in its generality, is false! Quoting the great Bayesian master Jaynes (2003, p. 144):\nIn the literature there are perhaps 100 \u2018paradoxes\u2019 and controversies which are like this, in that they arise from faulty intuition rather than faulty mathematics. Someone asserts a general principle that seems to him intuitively right. Then, when probability analysis reveals the error, instead of taking this opportunity to educate his intuition, he reacts by rejecting the probability analysis. 3\nChapter 4\nActing"}, {"heading": "I ought never to act except in such a way that I could also will that my maxim should", "text": "become a universal prior. \u2014 Immanuel Kant\nRecall our decomposition of intelligence into learning and acting from Equation 1.1. The previous chapter made the notion of learning precise and provided several examples of learning distributions for the non-i.i.d. setting (see Table 3.1). Learning is passive: there is no interaction with the data-generating process. In this chapter we transition into the active setting: we consider an agent acting in an unknown environment in order to achieve a goal . In our case, this goal is maximizing reward; this is known as reinforcement learning. Where this reward signal originates does not concern us here.\nIn this thesis we consider is the general reinforcement learning problem in which we do not make several of the typical simplifying assumptions (see Table 1.1). Environments are only partially observable, have infinitely many states, and might contain traps from which the agent cannot escape. The context for making decision is the agent\u2019s entire history; its behavior is given by a policy that specifies how the agent behaves in any possible situation.\nA central quantity in reinforcement learning is the value function. The value function quantifies the expected future discounted reward. Since the agent seeks to maximize reward, it aims to adopt a policy that has high value. Since the agent\u2019s environment is unknown to the agent, learning the value function is part of the challenge; otherwise we would call this planning.\nIf our agent is capable of learning in the sense of Chapter 3, then it learns the value of its own policy (on-policy value convergence). However, generally the agent does not learn to predict the value of counterfactual actions, actions that it does not take. Learning off-policy is hard because the agent receives no evidence about what would have happened on counterfactual actions. Nevertheless, off-policy learning is highly desirable because we want the agent to be confident that the policy it is currently following is in fact the best one; we want it to accurately predict that the counterfactual actions have less value.\nThis brings us back to the central theme of reinforcement learning: the tradeoff between exploration and exploitation. Asymptotically the agent needs to focus on exploitation, i.e., take the actions that it thinks yield the highest expected rewards. If the agent explores enough, then all actions are on-policy because they are all actions that\n49"}, {"heading": "50 Acting", "text": "the agent sometimes takes. Then on-policy learning ensures that the agent understands the consequences of every action and can confidently choose the best action. Effective exploration is performed by knowledge-seeking agents; these agents ignore the rewards and just focus on exploration.\nThis chapter introduces the central concepts of general reinforcement learning. It is mostly based on Hutter (2005) and Lattimore (2013). Section 4.1 specifies the general reinforcement learning problem, discusses discounting (Section 4.1.1), our implicit assumptions (Section 4.1.2), and typical environment classes (Section 4.1.3). Section 4.2 discusses the value function and its properties. In Section 4.3 we introduce the agents: AIXI (Section 4.3.1), knowledge-seeking agents (Section 4.3.2), BayesExp (Section 4.3.3), and Thompson sampling (Section 4.3.4)."}, {"heading": "4.1 The General Reinforcement Learning Problem", "text": "In reinforcement learning, an agent interacts with an environment: at time step t \u2208 N the agent takes an action at \u2208 A and subsequently receives a percept et = (ot, rt) \u2208 E consisting of an observation ot \u2208 O and a reward rt \u2208 R. This cycle then repeats for time step t+ 1 (see Figure 4.1).\nA history is an element of (A \u00d7 E)\u2217 and lists the actions the agent took and the percepts it received. We use \u00e6 \u2208 A \u00d7 E to denote one interaction cycle, and \u00e6<t = \u00e61\u00e62 . . .\u00e6t\u22121 to denote a history of length t \u2212 1. For our agent, the history is a sufficient statistic about the past and in general reinforcement learning there is no simpler sufficient statistic.\nFor example, consider the agent to be a robot interacting with the real world. Its actions are moving the motors in its limbs and wheels and sending data packets over a network connection. Its observations are data from cameras and various other sensors. The reward could be provided either by a human supervisor or through a reward module that checks whether a predefined goal has been reached. The history is the collection of all the data it received and emitted in the past. The division of the robot\u2019s interaction with the environment into discrete time steps might seem a bit unnatural at first since the real world evolves according to a continuous process. However, note that the electronic components used in robots operate at discrete frequencies anyway.\n\u00a74.1 The General Reinforcement Learning Problem 51\nIn order to specify how the agent behaves in any possible situation, we define a policy : a policy is a function \u03c0 : (A\u00d7E)\u2217 \u2192 \u2206Amapping a history \u00e6<t to a distribution over actions \u03c0( \u00b7 | \u00e6<t) taken after seeing this history. Usually we do not distinguish between agent and policy. An environment is a function \u03bd : (A \u00d7 E)\u2217 \u00d7 A \u2192 \u2206E mapping a history \u00e6<t and an action at to a distribution \u03bd( \u00b7 | \u00e6<tat) over the percepts received after the history \u00e6<t and action at. We use \u00b5 to denote the true environment.\nEquivalently, Hutter (2005) defines environments as chronological contextual semimeasures.1 A contextual semimeasure \u03bd takes a sequence of actions a1:\u221e as input and returns a semimeasure \u03bd( \u00b7 \u2016 a1:\u221e) over E]. A contextual semimeasure \u03bd is chronological iff percepts at time t do not depend on future actions, i.e., \u03bd(e1:t \u2016 a1:\u221e) = \u03bd(e1:t \u2016 a\u20321:\u221e) whenever a1:t = a\u20321:t. For chronological contextual semimeasures we write \u03bd(e1:t \u2016 a1:t) instead of \u03bd(e1:t \u2016 a1:\u221e). The two definition can be translated using the identities\n\u03bd(e1:t \u2016 a1:t) = t\u220f\nk=1\n\u03bd(ek | \u00e6<kak) and \u03bd(et | \u00e6<tat) = \u03bd(e1:t \u2016 a1:t) \u03bd(e<t \u2016 a<t) . (4.1)\nIf the policy \u03c0 always assigns probability 1 to one of the actions, then \u03c0 is called deterministic. Likewise, if the environment \u03bd always assigns probability 1 to one of the percepts, then \u03bd is called deterministic. For deterministic policies and environments we also use the notation at = \u03c0(\u00e6<t) and et = \u03bd(\u00e6<tat). A deterministic policy \u03c0 is consistent with history \u00e6<t iff ak = \u03c0(\u00e6<k) for all k < t. Likewise, a deterministic environment \u03bd is consistent with history \u00e6<t iff ek = \u03bd(\u00e6<kak) for all k < t.\nDefinition 4.1 (History Distribution). An environment \u03bd together with a policy \u03c0 induces a history distribution\n\u03bd\u03c0(\u00e6<t) := t\u220f\nk=1\n\u03c0(ak | \u00e6<k)\u03bd(ek | \u00e6<kak).\nWe denote an expectation with respect to the history distribution \u03bd\u03c0 with E\u03c0\u03bd .\nThe history distribution is a (semi)measure on (A\u00d7E)\u221e. In the language of measure theory, our \u03c3-algebra is the \u03c3-algebra F\u221e generated by the cylinder sets introduced in Section 2.1. The filtration (Ft)t\u2208N formalizes that at time step t we have seen exactly the history \u00e6<t (we use the \u03c3-algebra Ft\u22121). To simplify notation and help intuition, we simply condition expectations and probability measures with the history \u00e6<t instead of Ft\u22121 and sweep most of the measure-theoretic details under the rug.\nWith these preliminaries out of the way, we can now specify the general reinforcement learning problem.\nProblem 4.2 (General Reinforcement Learning Problem). Given an arbitrary class of environmentsM, choose a policy \u03c0 that maximizes \u00b5\u03c0-expected reward when interacting with any environment \u00b5 \u2208M.\n1Hutter (2005) calls them chronological conditional semimeasures. This is confusing because contextual semimeasures do not specify conditional probabilities; the environment is not a joint probability distribution over actions and percepts."}, {"heading": "52 Acting", "text": "Problem 4.2 is kept vague on purpose: it does not say how we should balance between achieving more rewards in some environments while achieving less in others. In other words, we leave open what an optimal solution to the general reinforcement learning problem is. This turns out to be a notoriously difficult question that we discuss in Chapter 5.\nAs promised in the title of this thesis, we take the nonparametric approach. For the rest of this thesis, fixM to be any countable set of environments. While the true environment is unknown, we assume it belongs to the classM (the realizable case). As long as the classM is sufficiently large (such as the class of all computable environments), this assumption is weak. Some typical choices are discussed in Section 4.1.3.\nOur agent-environment setup shown in Figure 4.1 is known as the dualistic model : the agent is distinct from the environment and influences it only through its actions. In turn, the environment influences the agent only through the percepts. The dualism assumption is accurate for an algorithm that is playing chess, Go, or other (video) games, which explains why it is ubiquitous in AI research. But often it is not true: real-world agents are embedded in (and computed by) the environment, and then a physicalistic model (also called materialistic model or naturalistic model) is more appropriate. Decision making in the physicalistic model is still underdeveloped; see Everitt et al. (2015) and Orseau and Ring (2012a). In this thesis we restrict ourselves to the dualistic model."}, {"heading": "4.1.1 Discounting", "text": "The goal in reinforcement learning is to maximize rewards. However, the infinite reward sum \u2211\u221e t=1 rt may diverge. To get around this technical problem, we let our agent prioritize the present over the future. This is done with a discount function that quantifies how much the agent prefers rewards now over rewards later.\nDefinition 4.3 (Discount Function). A discount function is a function \u03b3 : N\u2192 R with \u03b3t := \u03b3(t) \u2265 0 and \u2211\u221e t=1 \u03b3t <\u221e. The discount normalization factor is \u0393t := \u2211\u221e k=t \u03b3k.\nThere is no requirement that \u0393t > 0. In fact, we use \u03b3 for both, discounted infinite horizon (\u0393t > 0 for all t), and finite horizon m (\u0393m\u22121 > 0 and \u0393m = 0) where the agent does not care what happens after time step m.\nNote that the way in which we employ discounting is time consistent : the agent does not change its mind about how much it values the reward at time step k over time: reward rk is always discounted with \u03b3k regardless of the current time step. For a discussion of general discounting we refer the reader to Lattimore and Hutter (2014).\nDefinition 4.4 (Effective Horizon). The \u03b5-effective horizon Ht(\u03b5) is a horizon that is long enough to encompass all but an \u03b5 of the discount function\u2019s mass:\nHt(\u03b5) := min{k | \u0393t+k/\u0393t \u2264 \u03b5}\nThe effective horizon is bounded iff for all \u03b5 > 0 there is a constant c\u03b5 such that Ht(\u03b5) \u2264 c\u03b5 for all t \u2208 N.\nExample 4.5 (Geometric Discounting). The most common discount function is geometric discounting with \u03b3t := \u03b3t for some constant \u03b3 \u2208 [0, 1). We get that \u0393t =\u2211\u221e\nk=t \u03b3 k = \u03b3t/(1 \u2212 \u03b3) and the \u03b5-effective horizon is Ht(\u03b5) = dlog\u03b3 \u03b5e. Hence the effective horizon is bounded. 3\nMore examples for discount functions are given in Table 4.1. From now on, we fix a discount function \u03b3."}, {"heading": "4.1.2 Implicit Assumptions", "text": "Throughout this thesis, we make the following assumptions implicitly.\nAssumption 4.6. (a) The discount function \u03b3 is computable.\n(b) Rewards are bounded between 0 and 1.\n(c) The set of actions A and the set of percepts E are both finite.\nLet\u2019s motivate these assumptions in turn. Their purpose is to ensure that discounted reward sums are finite and optimal policies exist.\nAssumption 4.6a is a technical assumption that ensures that discounted reward sums are computable. This is important for Chapter 6 and Chapter 7 where we analyse the computability of optimal policies. Note that all discount functions given in Table 4.1 are computable.\nAssumption 4.6b could be relaxed to require only that rewards are bounded. We can rescale rewards rt 7\u2192 crt + d for any c \u2208 R+ and d \u2208 R without changing optimal policies if the environment \u03bd is a probability measure. (For our computability-related results in Chapter 6, we must assume that rewards are nonnegative.) In this sense Assumption 4.6b is not very restrictive. However, this normalization of rewards into the [0, 1]-interval has the convenient consequence that the normalized discounted reward sum \u2211\u221e k=t \u03b3krk/\u0393k is bounded between 0 and 1. If rewards are unbounded, then the discounted reward sum might diverge. Moreover, with unbounded rewards there all kinds of pathological problems where defining optimal actions is no longer straightforward; see Arntzenius et al. (2004) for a discussion.\nAssumption 4.6c is a technical requirement for the existence of optimal policies since it implies that there are only finitely many deterministic policies that differ in the first t"}, {"heading": "54 Acting", "text": "time steps. Note that finite action and percept spaces are very natural since it ensures that our agent only receives and emits a finite amount of information in every time step. This is in line with the problems a strong AI is facing: the agent has to remember important information and act sequentially.\nAssumption 4.6b, Assumption 4.6c, and the fact that the discount function is summable guarantee that a deterministic optimal policy exists for every environment according to Lattimore and Hutter (2014, Thm. 10). It would be interesting to relax these assumptions while preserving the existence of optimal policies or at least \u03b5-optimal policies (e.g. use compact action and percept spaces)."}, {"heading": "4.1.3 Typical Environment Classes", "text": "The simplest reinforcement learning problems are multi-armed bandits.\nDefinition 4.7 (Multi-Armed Bandit). An environment \u03bd is a multi-armed bandit iff O = {\u22a5} and \u03bd(et | \u00e6<tat) = \u03bd(et | at) for all histories \u00e61:t \u2208 (A\u00d7 E)\u2217.\nIn a multi-armed bandit problem there are no observations and the next reward only depends on the previous action. Intuitively, we are deciding between #A different slot machines (so-called one-armed bandits), pull the lever and obtain a reward. The reward is stochastic, but it is drawn from a distribution that is time-invariant and fixed for each arm.\nA multi-armed bandit is also called bandit for short. Although bandits are the simplest reinforcement learning problem, they already exhibit the exploration-exploitationtradeoff that makes reinforcement learning difficult: do you pull an arm that has the best empirical mean or do you pull an arm that has the highest uncertainty? In bandits it is very easy to come up with policies that perform (close to) optimal asymptotically (e.g., \u03b5t-greedy with \u03b5t = 1/t). But coming up with algorithms that perform well in practice is difficult, and research focuses on the multiplicative and additive constants on the asymptotic guarantees. Bandits exist in many flavors; see Bubeck and Bianchi (2012) for a survey.\nDefinition 4.8 (Markov Decision Process). An environment \u03bd is a Markov decision process (MDP) iff \u03bd(et | \u00e6<tat) = \u03bd(et | ot\u22121at) for all histories \u00e61:t \u2208 (A\u00d7 E)\u2217.\nIntuitively, in MDPs, the previous observation ot\u22121 provides a sufficient statistic for the history: given ot\u22121 and the current action at, the next percept et is independent of the rest of the history. In other words, everything that the agent needs to know to make optimal decisions is readily available in the previous percept. This is why observations are called states in MDPs. Note that bandits are MDPs with a single state.\nMuch of today\u2019s literature on reinforcement learning focuses on MDPs (Sutton and Barto, 1998). They provide a particularly good framework to study reinforcement learning because they are simple enough to be tractable for today\u2019s algorithms, yet general enough to encompass many interesting problems. For example, most of the Atari games (see Figure 1.1 for an overview) are (deterministic) MDPs when combining\n\u00a74.1 The General Reinforcement Learning Problem 55\nthe previous four frames into one percept. While they have a huge state space2 they can still be learned using Q-learning with function approximation (Mnih et al., 2015).\nThe MDP framework is restrictive because it requires the agent to be more powerful than the environment. Since the agent learns, its actions are not independent of the rest of the history given the last action and percept. In other words, learning agents are not Markov. The following definition lifts this restriction and allows the environment to be partially observable.\nDefinition 4.9 (Partially Observable Markov Decision Process). An environment \u03bd is a partially observable Markov decision process (POMDP) iff there is a set of states S, an initial state s0 \u2208 S, a state transition function \u03bd \u2032 : S \u00d7 A \u2192 \u2206S, and a percept distribution \u03bd \u2032\u2032 : S \u2192 \u2206E such that\n\u03bd(e1:t \u2016 a1:t) = t\u220f\nk=1\n\u03bd \u2032\u2032(ek | sk)\u03bd \u2032(sk | sk\u22121, ak).\nUsually the set S is assumed to be finite; with infinite-state POMDPs we can model any environment \u03bd by setting the set of states to be the set of histories, S := (A\u00d7E)\u2217.\nA common assumption for MDPs and POMDPs is that they do not contain traps. Formally, a (PO)MDP is ergodic iff for any policy \u03c0 and any two states s1, s2 \u2208 S, the expected number of time steps to reach s2 from s1 is \u00b5\u03c0-almost surely finite. A (PO)MDP is weakly communicating iff for any two states s1, s2 \u2208 S there is a policy \u03c0 such that the expected number of time steps to reach s2 from s1 is \u00b5\u03c0-almost surely finite. Note that any ergodic (PO)MDP is also weakly communicating, but not vice versa.\nIn general, our environments are stochastic. Stochasticity can originate from noise in the environment, noise in the sensors, or modeling errors. Sometimes we also consider classes of deterministic environments. These are usually easier to deal with because they do not require as much mathematical machinery. For example, in a deterministic environment the next percept is certain; if a different percept is received this environment is immediately falsified and can be discarded. In a stochastic environment, an unlikely percept reduces our posterior belief in this environment but does not rule it out completely.\nIn Chapter 6 and Chapter 7 we make the assumption that the environment is computable. This encompasses all finite-state POMDPs and most if not all AI problems can be formulated in this setting. Moreover, the current theories of quantum mechanics and general relativity are computable and there is no evidence that suggests that our physical universe is incomputable. For any physical system of finite volume and finite (average) energy, the amount of information it can contain is finite (Bekenstein, 1981), and so is the number of state transitions per unit of time (Margolus and Levitin, 1998). This gives us reason to believe that even the environment that we humans currently face (and will ever face) falls under these assumptions.\n2The size of the state space is at most 256128 since the Atari 2600 has only 128 bytes of memory. However, the vast majority of these states are not reachable."}, {"heading": "56 Acting", "text": "Formally we define the setMCCSLSC as the set of environments that are lower semicomputable chronological contextual semimeasures andMCCMcomp as the set of environments that are computable chronological contextual measures. Note that for chronological contextual semimeasures it makes a difference whether \u03bd( \u00b7 \u2016 a1:\u221e) is lower semicomputable or the conditionals \u03bd( \u00b7 | \u00e6<tat) are. The latter implies the former, but not vice versa."}, {"heading": "4.2 The Value Function", "text": "The value of a policy in an environment is the future expected discounted reward when following a given policy in a given environment conditional on the past. Since this quantity captures exactly what our agent aims to maximize, we prefer policies whose value is high.\nDefinition 4.10 (Value Function). The value of a policy \u03c0 in an environment \u03bd given history \u00e6<t and horizon m with t \u2264 m \u2264 \u221e is defined as\nV \u03c0,m\u03bd (\u00e6<t) := 1\n\u0393t E\u03c0\u03bd [ m\u22121\u2211 k=t \u03b3krk \u2223\u2223\u2223\u2223\u2223 \u00e6<t ]\nif \u0393t > 0 and V \u03c0,m \u03bd (\u00e6<t) := 0 if \u0393t = 0. The optimal value is defined as V \u2217,m \u03bd (\u00e6<t) := sup\u03c0 V \u03c0,m \u03bd (\u00e6<t).\nSometimes we omit the history argument \u00e6<t for notational convenience if it is clear from context. Moreover, when we omit m, we implicitly use an infinite horizon m =\u221e, i.e., V \u03c0\u03bd := V \u03c0,\u221e \u03bd and V \u2217\u03bd := V \u2217,\u221e \u03bd . The value of a policy \u03c0 in an environment \u03bd after the empty history, V \u03c0\u03bd ( ) is also called the t0-value.\nRemark 4.11 (Values are Bounded Between 0 and 1). From Assumption 4.6b we get that for all histories \u00e6<t all policies \u03c0 and all environments \u03bd, the value function V \u03c0\u03bd (\u00e6<t) \u2208 [0, 1]. 3\nSince environment and policy are stochastic, the history \u00e6<t is random. With abuse of notation we treat \u00e6<t sometimes as a concrete outcome and sometimes as a random variable. We also view the value of a policy \u03c0 in an environment \u03bd as a sequence of random variables (Xt)t\u2208N with Xt := V \u03c0\u03bd (\u00e61:t) where the history \u00e61:t is generated stochastically by the agent\u2019s actual policy interacting with the true environment \u00b5. This view is helpful for some of the convergence results (e.g., Theorem 4.19 and Definition 5.18) in which we talk about the type of convergence of this sequence of random variables.\nThe value function defined in Definition 4.10 is also called the recursive value function, in contrast to iterative value function that we discuss in Section 6.4. The name of the recursive value function originates from the following recursive identity (analogously\n\u00a74.2 The Value Function 57\nto Hutter, 2005, Eq. 4.12), also called the Bellman equation:\nV \u03c0\u03bd (\u00e6<t) = \u2211 at\u2208A \u03c0(at | \u00e6<t)V \u03c0\u03bd (\u00e6<tat)\nV \u03c0\u03bd (\u00e6<tat) = 1\n\u0393t \u2211 et\u2208E \u03bd(et | \u00e6<tat) ( \u03b3trt + \u0393t+1V \u03c0 \u03bd (\u00e61:t) ) An explicit expression for the optimal value in environment \u03bd is\nV \u2217,m\u03bd (\u00e6<t) = 1 \u0393t max \u2211\n\u00e6t:m\u22121 m\u22121\u2211 k=t \u03b3krk k\u220f i=t \u03bd(ei | \u00e6<iai), (4.2)\nwhere \u2211 max denotes the max-sum-operator:\nmax \u2211\n\u00e6t:m\u22121\n:= max at\u2208A \u2211 et\u2208E . . . max am\u22121\u2208A \u2211 em\u22121\u2208E\nFor an explicit expression of V \u2217,\u221e\u03bd (\u00e6<t) we can simply take the limit m\u2192\u221e."}, {"heading": "4.2.1 Optimal Policies", "text": "An optimal policy is a policy that achieves the highest value:\nDefinition 4.12 (Optimal Policy; Hutter, 2005, Def. 5.19 & 5.30). A policy \u03c0 is optimal in environment \u03bd (\u03bd-optimal) iff for all histories \u03c0 attains the optimal value: V \u03c0\u03bd (\u00e6<t) = V \u2217\u03bd (\u00e6<t) for all \u00e6<t \u2208 (A\u00d7E)\u2217. The action at is an optimal action iff \u03c0\u2217\u03bd(at | \u00e6<t) = 1 for some \u03bd-optimal policy \u03c0\u2217\u03bd .\nFollowing the tradition of Hutter (2005), AINU denotes a \u03bd-optimal policy for the environment \u03bd \u2208 MCCSLSC and AIMU denotes an \u00b5-optimal policy for the environment \u00b5 \u2208MCCMcomp that is a measure (as opposed to a semimeasure).\nBy definition of the optimal policy and the optimal value function, we have the following identity for all histories \u00e6<t:\nV \u2217\u03bd (\u00e6<t) = V \u03c0\u2217\u03bd \u03bd (\u00e6<t) (4.3)\nThere can be more than one optimal policy; generally the choice of \u03c0\u2217\u03bd from Definition 4.12 is not unique. More specifically, for a \u03bd-optimal policy we have\n\u03c0\u2217\u03bd(at | \u00e6<t) > 0 =\u21d2 at \u2208 arg max a\u2208A V \u2217\u03bd (\u00e6<ta). (4.4)\nIf there are multiple actions \u03b1, \u03b2 \u2208 A that attain the optimal value, V \u2217\u03bd (\u00e6<\u03b1) = V \u2217\u03bd (\u00e6<t\u03b2), then there is an argmax tie. Which action we settle on in case of a tie (how we break the tie) is irrelevant and can be arbitrary. Since we allow stochastic policies, we can also randomize between \u03b1 and \u03b2.\nThe following definition allows policies to be slightly suboptimal."}, {"heading": "58 Acting", "text": "Definition 4.13 (\u03b5-Optimal Policy). A policy \u03c0 is \u03b5-optimal in environment \u03bd iff V \u2217\u03bd (\u00e6<t)\u2212 V \u03c0\u03bd (\u00e6<t) < \u03b5 for all histories \u00e6<t \u2208 (A\u00d7 E)\u2217.\nA policy \u03c0 that achieves optimal t0-value, V \u03c0\u03bd ( ) = V \u2217\u03bd ( ), takes \u03bd-optimal actions on any history reachable by \u03c0 in \u03bd. However, this is not true for \u03b5-optimal policies: a policy that is \u03b5-optimal at t = 0 is not necessarily \u03b5-optimal in later time steps."}, {"heading": "4.2.2 Properties of the Value Function", "text": "The following two lemmas are stated by Hutter (2005, Thm. 31) without proof and for the iterative value function.\nLemma 4.14 (Linearity of V \u03c0\u03bd in \u03bd). If \u03bd = z1\u03bd1+z2\u03bd2 for some real numbers z1, z2 \u2265 0, then for all policies \u03c0 and all histories \u00e6<t\nV \u03c0,m\u03bd (\u00e6<t) = z1 \u03bd1(e<t \u2016 a<t) \u03bd(e<t \u2016 a<t) V \u03c0,m\u03bd1 (\u00e6<t) + z2 \u03bd2(e<t \u2016 a<t) \u03bd(e<t \u2016 a<t) V \u03c0,m\u03bd2 (\u00e6<t).\nProof. Since \u03bd\u03c0 = z1\u03bd\u03c01 + z2\u03bd\u03c02 , we have for the conditional measure\n\u03bd\u03c0(A | \u00e6<t) = \u03bd\u03c0(A \u2229 \u00e6<t) \u03bd\u03c0(\u00e6<t) = z1\u03bd \u03c0 1 (A \u2229 \u00e6<t) + z2\u03bd\u03c02 (A \u2229 \u00e6<t) \u03bd\u03c0(\u00e6<t)\n= z1 \u03bd\u03c01 (\u00e6<t) \u03bd\u03c0(\u00e6<t) \u03bd\u03c01 (A | \u00e6<t) + z2 \u03bd\u03c02 (\u00e6<t) \u03bd\u03c0(\u00e6<t) \u03bd\u03c02 (A | \u00e6<t).\nThe claim now follows from the linearity of expectation in the probability measure.\nLemma 4.15 (Convexity of V \u2217\u03bd in \u03bd). If \u03bd = z1\u03bd1 +z2\u03bd2 for some real numbers z1, z2 \u2265 0, then for all histories \u00e6<t\nV \u2217\u03bd (\u00e6<t) \u2264 z1 \u03bd1(e<t \u2016 a<t) \u03bd(e<t \u2016 a<t) V \u2217,m\u03bd1 (\u00e6<t) + z2 \u03bd2(e<t \u2016 a<t) \u03bd(e<t \u2016 a<t) V \u2217,m\u03bd2 (\u00e6<t).\nProof. Let \u03c0\u2217\u03bd be an optimal policy for environment \u03bd. From Lemma 4.14 we get\nV \u2217\u03bd (\u00e6<t) = V \u03c0\u2217\u03bd \u03bd (\u00e6<t) = z1 \u03bd1(e<t \u2016 a<t) \u03bd(e<t \u2016 a<t) V \u03c0 \u2217 \u03bd \u03bd1 (\u00e6<t) + z2 \u03bd2(e<t \u2016 a<t) \u03bd(e<t \u2016 a<t) V \u03c0 \u2217 \u03bd \u03bd2 (\u00e6<t)\n\u2264 z1 \u03bd1(e<t \u2016 a<t) \u03bd(e<t \u2016 a<t) V \u2217\u03bd1(\u00e6<t) + z2 \u03bd2(e<t \u2016 a<t) \u03bd(e<t \u2016 a<t) V \u2217\u03bd2(\u00e6<t).\nThe following lemma bounds the error when truncating the value function. This implies that planning for an \u03b5-effective horizon (m = t+Ht(\u03b5)), we get all but an \u03b5 of the value: |V \u03c0\u03bd (\u00e6<t)\u2212 V \u03c0,m \u03bd (\u00e6<t)| < \u03b5.\nLemma 4.16 (Truncated Values). For every environment \u03bd, every policy \u03c0, and every history \u00e6<t \u2223\u2223V \u03c0,m\u03bd (\u00e6<t)\u2212 V \u03c0\u03bd (\u00e6<t)\u2223\u2223 \u2264 \u0393m\u0393t .\n\u00a74.2 The Value Function 59\nProof.\nV \u03c0\u03bd (\u00e6<t) = 1\n\u0393t E\u03c0\u03bd [ \u221e\u2211 k=t \u03b3krk \u2223\u2223\u2223\u2223\u2223 \u00e6<t ] = V \u03c0,m\u03bd (\u00e6<t) + 1 \u0393t E\u03c0\u03bd [ \u221e\u2211 k=m \u03b3krk \u2223\u2223\u2223\u2223\u2223 \u00e6<t ]\nThe result now follows from Assumption 4.6b and\n0 \u2264 E\u03c0\u03bd [ \u221e\u2211 k=m \u03b3krk \u2223\u2223\u2223\u2223\u2223 \u00e6<t ] \u2264 \u0393m.\nThis lemma bounds the (truncated) value function by the total variation distance.\nLemma 4.17 (Bounds on Value Difference). For any policies \u03c01, \u03c02, any environments \u03bd1 and \u03bd2, and any horizon t \u2264 m \u2264 \u221e\n|V \u03c01,m\u03bd1 (\u00e6<t)\u2212 V \u03c02,m \u03bd2 (\u00e6<t)| \u2264 Dm\u22121(\u03bd \u03c01 1 , \u03bd \u03c02 2 | \u00e6<t)\nProof. According to Definition 4.10, the value function is the expectation of the random variable \u2211m\u22121 k=t \u03b3krk/\u0393t that is bounded between 0 and 1. Therefore we can use Lemma 2.12 with P := \u03bd\u03c011 ( \u00b7 | \u00e6<t) and R := \u03bd \u03c02 2 ( \u00b7 | \u00e6<t) on the space (A\u00d7E)m\u22121 to conclude that |V \u03c01,m\u03bd1 (\u00e6<t)\u2212 V \u03c02,m \u03bd2 (\u00e6<t)| is bounded by Dm\u22121(\u03bd\u03c011 , \u03bd \u03c02 2 | \u00e6<t).\nLemma 4.18 (Discounted Values; Lattimore, 2013, Lem. 2.5). Let \u00e6<t be some history and let \u03c01 and \u03c02 be two policies that coincide from time step t to time step m: \u03c01(a | \u00e61:k) = \u03c02(a | \u00e61:k) for all a \u2208 A, all histories \u00e6<t\u00e6t:k consistent with \u03c01, and t \u2264 k \u2264 m. Then for all environments \u03bd\u2223\u2223V \u03c01\u03bd (\u00e6<t)\u2212 V \u03c02\u03bd (\u00e6<t)\u2223\u2223 \u2264 \u0393m\u0393t . Proof. Since \u03c01 and \u03c02 coincide for time steps t throughm\u22121, Dm\u22121(\u03bd\u03c01 , \u03bd\u03c02 | \u00e6<t) = 0 for all environments \u03bd. Thus the result follows from Lemma 4.16 and Lemma 4.17:\u2223\u2223V \u03c01\u03bd (\u00e6<t)\u2212 V \u03c02\u03bd (\u00e6<t)\u2223\u2223 \u2264 \u2223\u2223V \u03c01,m\u03bd (\u00e6<t)\u2212 V \u03c02,m\u03bd (\u00e6<t)\u2223\u2223+ \u0393m\u0393t\n\u2264 Dm\u22121(\u03bd\u03c01 , \u03bd\u03c02 | \u00e6<t) + \u0393m \u0393t = \u0393m \u0393t"}, {"heading": "4.2.3 On-Policy Value Convergence", "text": "This section states some general results on learning the value function. On-policy value convergence refers to the fact that if we use a learning distribution \u03c1 to learn to environment \u00b5, and \u03c1\u03c0 merges with \u00b5\u03c0 in the sense discussed Section 3.4, then V \u03c0\u03c1 converges to V \u03c0\u00b5 , i.e., using \u03c1 we learn to estimate values correctly.\nA weaker variant of the following theorem was proved by Hutter (2005, Thm. 5.36). It states convergence in mean (not almost surely), and only for the Bayesian mixture."}, {"heading": "60 Acting", "text": "Theorem 4.19 (On-Policy Value Convergence). Let \u00b5 be any environment and \u03c0 be any policy.\n(a) If \u03c1\u03c0 merges strongly with \u00b5\u03c0, then\nV \u03c0\u03c1 (\u00e6<t)\u2212 V \u03c0\u00b5 (\u00e6<t)\u2192 0 as t\u2192\u221e \u00b5\u03c0-almost surely.\n(b) If the effective horizon is bounded and \u03c1\u03c0 merges weakly with \u00b5\u03c0, then\nV \u03c0\u03c1 (\u00e6<t)\u2212 V \u03c0\u00b5 (\u00e6<t)\u2192 0 as t\u2192\u221e \u00b5\u03c0-almost surely.\n(c) If the effective horizon is bounded and \u03c1\u03c0 merges almost weakly with \u00b5\u03c0, then\n1\nt t\u2211 k=1 ( V \u03c0\u03c1 (\u00e6<k)\u2212 V \u03c0\u00b5 (\u00e6<k) ) \u2192 0 as t\u2192\u221e in \u00b5\u03c0-almost surely.\nProof. (a) Apply Lemma 4.17 with m :=\u221e.\n(b) Let \u03b5 > 0 and let c\u03b5 be a bound on suptHt(\u03b5). From Lemma 4.16\n|V \u03c0\u03c1 (\u00e6<t)\u2212 V \u03c0\u00b5 (\u00e6<t)| \u2264 |V \u03c0,t+Ht(\u03b5)\u03c1 (\u00e6<t)\u2212 V \u03c0,t+Ht(\u03b5)\u00b5 (\u00e6<t)|+ 2 \u0393t+Ht(\u03b5)\n\u0393t\n< Dt+Ht\u22121(\u03b5)(\u03c1 \u03c0, \u00b5\u03c0 | \u00e6<t) + 2\u03b5 \u2264 Dt+c\u03b5(\u03c1\u03c0, \u00b5\u03c0 | \u00e6<t) + 2\u03b5\naccording to Definition 4.4 and Lemma 4.17. Since \u03c1\u03c0 merges weakly with \u00b5\u03c0, we get that \u00b5\u03c0-almost surely there is a time step t0 \u2208 N such that Dt+c\u03b5(\u03c1\u03c0, \u00b5\u03c0 | \u00e6<t) < \u03b5 for all t \u2265 t0. Hence |V \u03c0Q (\u00e6<t)\u2212 V \u03c0\u00b5 (\u00e6<t)| < 3\u03b5 for all t \u2265 t0.\n(c) Analogously to the proof of (b).\nIt is important to observe that on-policy convergence does not imply that the agent converges to the optimal policy. V \u03c0\u03c1 converges to V \u03c0\u00b5 , but V \u03c0\u00b5 need not be close to V \u2217\u00b5 . Indeed, there might be another policy \u03c0\u0303 that has a higher value than \u03c0 in the true environment \u00b5 (V \u03c0\u0303\u00b5 > V \u03c0\u00b5 ). If the agent thinks \u03c0\u0303 has lower value (V \u03c0\u0303\u03c1 < V \u03c0\u03c1 ) it might not follow \u03c0\u0303 and hence not learn that the actual value of \u03c0\u0303 is much higher. In other words, on-policy convergence implies that the agent learns the value of its own actions, but not the value of counterfactual actions that it does not take.\nTheorem 4.19 now enables us to tie in the results of Chapter 3. This yields a surge of corollaries, but first we need to make the learning distributions contextual on the actions.\nLet w \u2208 \u2206M be a positive prior over the environment class M. We define the corresponding Bayesian mixture analogously to Example 3.4:\n\u03be(e<t \u2016 a<t) := \u2211 \u03bd\u2208M w(\u03bd)\u03bd(e<t \u2016 a<t) (4.5)\n\u00a74.2 The Value Function 61\nNote that the Bayesian mixture \u03be depends on the prior w. For the rest of this thesis, this dependence will not be made explicit.\nFrom Lemma 4.14 and (3.3) we immediately get the following identity:\nV \u03c0\u03be (\u00e6<t) = \u2211 \u03bd\u2208M w(\u03bd | \u00e6<t)V \u03c0\u03bd (\u00e6<t) (4.6)\nSimilarly, we get from Lemma 4.15\nV \u2217\u03be (\u00e6<t) \u2264 \u2211 \u03bd\u2208M w(\u03bd | \u00e6<t)V \u2217\u03bd (\u00e6<t). (4.7)\nCorollary 4.20 (On-Policy Value Convergence for Bayes). For any environment \u00b5 \u2208 M and any policy \u03c0,\nV \u03c0\u03be (\u00e6<t)\u2212 V \u03c0\u00b5 (\u00e6<t)\u2192 0 as t\u2192\u221e \u00b5\u03c0-almost surely.\nProof. Since \u00b5 \u2208 M, we have dominance \u03be\u03c0 \u2265 w(\u00b5)\u00b5\u03c0 with w(\u00b5) > 0 and by Proposition 3.16a absolute continuity \u03be\u03c0 \u00b5\u03c0. From Theorem 3.25 we get that \u03be\u03c0 merges strongly with \u00b5\u03c0. Therefore we can apply Theorem 4.19a.\nAnalogously, we define MDL\u00e6<t := arg min\u03bd\u2208M{\u2212 log \u03bd(e<t \u2016 a<t) +K(\u03bd)}.\nCorollary 4.21 (On-Policy Value Convergence for MDL). For any environment \u00b5 \u2208M and any policy \u03c0,\nV \u03c0MDL\u00e6<t (\u00e6<t)\u2212 V \u03c0 \u00b5 (\u00e6<t)\u2192 0 as t\u2192\u221e \u00b5\u03c0-almost surely.\nProof. By Theorem 3.28 MDL\u03c0 merges strongly with \u03bd\u03c0 for each \u03bd \u2208M, therefore we can apply Theorem 4.19a.\nBy providing the action sequence contextually on a separate input tape, we can define Km(e<t \u2016 a<t) := min{|p| | e<t v U(p, a<t)} analogously to (2.1).\nCorollary 4.22 (On-Policy Value Convergence for Universal Compression). Let \u03c1(e<t \u2016 a<t) := 2 \u2212Km(e<t\u2016a<t). Then for any environment \u00b5 \u2208MCCMcomp and any policy \u03c0,\nV \u03c0\u03c1 (\u00e6<t)\u2212 V \u03c0\u00b5 (\u00e6<t)\u2192 0 as t\u2192\u221e \u00b5\u03c0-almost surely.\nProof. Since \u03c1 dominates every \u00b5 \u2208 MCCMcomp (Section 3.6.3) we can apply Proposition 3.16a, Theorem 3.25, and Theorem 4.19a as in the proof of Corollary 4.20.\nSimilarly to Km there is a speed prior for environments (Filan, 2015, Ch. 6):\nSKt(e<t \u2016 a<t) := \u2211\np: e<tvU(p,a<t)\n2\u2212|p|\nt(U, p, a<t, e<t)\nwhere t(U, p, a<t, e<t) denotes the number of time steps U(p, a<t) takes to produce e<t."}, {"heading": "62 Acting", "text": "Corollary 4.23 (On-Policy Value Convergence for the Speed Prior). If the effective horizon is bounded, then for any environment \u00b5 \u2208MCCMcomp estimable in polynomial time and any policy \u03c0,\n1\nt t\u2211 k=1 ( V \u03c0SKt(\u00e6<k)\u2212 V \u03c0 \u00b5 (\u00e6<k) ) \u2192 0 as t\u2192\u221e \u00b5\u03c0-almost surely.\nProof. By Corollary 3.61 the speed prior SKt merges almost weakly with every measure estimable in polynomial time. Therefore we can apply Theorem 4.19c."}, {"heading": "4.3 The Agents", "text": "If we knew the true environment \u00b5, we would choose the \u00b5-optimal policy, the policy that maximizes \u00b5-expected discounted rewards. But generally we do not know the true environment, and the challenging part of reinforcement learning is to learn the environment while trying to collect rewards.\nIn this section we introduce a number of agents that attempt to solve the general reinforcement learning problem (Problem 4.2). These agents are discussed throughout the rest of this thesis."}, {"heading": "4.3.1 Bayes", "text": "A Bayes optimal policy with respect to the prior w is the policy \u03c0\u2217\u03be where \u03be is the Bayesian mixture defined in Section 4.2.3. There can be one or more Bayes optial policies. From Corollary 4.20 we get on-policy value convergence for the Bayes optimal policy.\nAfter history \u00e6<t, the Bayes policy \u03c0\u2217\u03be maximizes expected discounted rewards in the posterior mixture:\n\u03be( \u00b7 | e<t \u2016 a1:\u221e) = \u2211 \u03bd\u2208M w(\u03bd | \u00e6<t)\u03bd( \u00b7 | e<t \u2016 a1:\u221e)\nwhere w(\u03bd | \u00e6<t) are the posterior weights (3.3). Maximizing expected rewards according to the posterior is the same as maximizing expected rewards according to the prior conditional on the history: if \u03c0(\u00e6<t) = \u03c0\u2217\u03be (\u00e6<t), then V \u03c0 \u03be (\u00e6<t) = V \u2217 \u03be (\u00e6<t). Actually visiting the history \u00e6<t does not change what \u03c0\u2217\u03be planned to do before it visited \u00e6<t. Note that this relies on the fact that the way we use discounting is time consistent (Lattimore and Hutter, 2014, Def. 12).\nWhen using the prior w(\u03bd) \u221d 2\u2212K(\u03bd) (Example 3.5) over the classMCCSLSC , the Bayes optimal policy is also known as AIXI , introduced and analyzed by Hutter (2000, 2001a, 2002a, 2003, 2005, 2007a, 2012b) in his work on universal artificial intelligence. In this case, the Bayesian mixture (4.5) can be defined equivalently according to (Wood et al., 2011)\n\u03be(e<t \u2016 a<t) := \u2211\np: e<tvU(p,a<t)\n2\u2212|p|. (4.8)\n\u00a74.3 The Agents 63\nGenerally there is more than one \u03be-optimal policy and Solomonoff\u2019s prior depends on the choice of the (reference) universal Turing machine, so this definition is not unique. Moreover, not every universal Turing machine is a good choice for AIXI, see Section 5.2 for a few bad choices. The following lemma will be used later.\nLemma 4.24 (Mixing Mixtures). Let q, q\u2032 \u2208 Q such that q > 0, q\u2032 \u2265 0, and q + q\u2032 \u2264 1. Let w be any lower semicomputable positive prior, let \u03be be the Bayesian mixture corresponding to w, and let \u03c1 \u2208 MCCSLSC . Then \u03be\u2032 := q\u03be + q\u2032\u03c1 \u2208 MCCSLSC is a Bayesian mixture.\nProof. \u03be\u2032 is given by the positive prior w\u2032 with w\u2032 := qw + q\u20321\u03c1.\nBayesian approaches have a long tradition in reinforcement learning, although they are often prohibitively expensive to compute. For multi-armed bandits, Gittins (1979) achieved a breakthrough with an index strategy that enables the computation of the optimal policy by computing one quantity for each arm independently of the rest. This strategy even achieves the optimal asymptotic regret bounds (Lattimore, 2016). Larger classes have also been attempted: using Monte-Carlo tree search, Veness et al. (2011) approximate the Bayes optimal policy in the class of all context trees. Doshi-Velez (2012) uses Bayesian techniques to learn infinite-state POMDPs. See Vlassis et al. (2012) for a survey on Bayesian techniques in RL.\nIn the rest of this thesis, the Bayes optimal policy is often treated as an optimal exploitation strategy. This is not true: Bayes does explore (when it is Bayes optimal to do so). It just does not explore general environment classes completely (see Section 5.4.1)."}, {"heading": "4.3.2 Knowledge-Seeking Agents", "text": "In this section we discuss two variants of knowledge-seeking agents: entropy-seeking agents introduced by Orseau (2011, 2014a) and information-seeking agents introduced by Orseau et al. (2013). The entropy-seeking agent maximizes the Shannon entropy gain, while the information-seeking agent maximizes the expected information gain. These quantities are expressed in different value functions. In places where confusion can arise, we call the value function V \u03c0\u03bd from Definition 4.10 the reward-seeking value function.\nIn this section we use a finite horizon m <\u221e (possibly dependent on time step t): the knowledge-seeking agent maximizes entropy/information received up to time step m. We assume implicitly that m (as a function of t) is computable. Moreover, in this section we assume that the Bayesian mixture \u03be is a measure rather than a semimeasure; Example 4.27 discusses this assumption.\nDefinition 4.25 (Entropy-Seeking Value Function; Orseau, 2014a, Sec. 6). The entropyseeking value of a policy \u03c0 given history \u00e6<t is\nV \u03c0,mEnt (\u00e6<t) := E \u03c0 \u03be [\u2212 log2 \u03be(e1:m | e<t \u2016 a1:m) | \u00e6<t]."}, {"heading": "64 Acting", "text": "The entropy-seeking value is the Bayes-expectation of \u2212 log \u03be. Orseau (2011, 2014a) also considers a related value function based on the \u03be-expectation of \u03be that we do not discuss here.\nDefinition 4.26 (Information-Seeking Value Function; Orseau et al., 2013, Def. 1). The information-seeking value of a policy \u03c0 given history \u00e6<t is\nV \u03c0,mIG (\u00e6<t) := \u2211 \u03bd\u2208M w(\u03bd | \u00e6<t)KLm(\u03bd\u03c0, \u03be\u03c0 | \u00e6<t).\nAnalogously to before we define V \u2217Ent := sup\u03c0 V \u03c0 Ent and V \u2217 IG := sup\u03c0 V \u03c0 IG. An optimal entropy-seeking policy is defined as \u03c0\u2217Ent :\u2208 arg max\u03c0 V \u03c0Ent and an optimal informationseeking policy is defined as \u03c0\u2217IG :\u2208 arg max\u03c0 V \u03c0IG. Since we use a finite horizon (m <\u221e), these optimal policies exist.\nThe information gain is defined as the difference in entropy between the prior and the posterior:\nIGt:m(\u00e61:m) := Ent(w( \u00b7 | \u00e6<t))\u2212 Ent(w( \u00b7 | \u00e61:m))\nWe get the following identity (Lattimore, 2013, Eq. 3.5).\nE\u03c0\u03be [IGt:m(\u00e61:m) | \u00e6<t] = V \u03c0,m IG (\u00e6<t)\nFor infinite horizons (m = \u221e), the values functions from Definition 4.25 and Definition 4.26 may not converge. To ensure convergence, we can either use discounting, or in case of VIG a prior with finite entropy (Lattimore, 2013, Thm. 3.4). Moreover, note that while VIG and VEnt are expectations with respect to the measure \u03be, there is no bound on the one-step change in value V \u03c0,mIG (\u00e6<t)\u2212 V \u03c0,m IG (\u00e61:t), which can also be negative. For the reward-seeking value function V \u03c0,m\u03bd , the one-step change in value is bounded between 0 and 1 by Remark 4.11.\nFor classes of deterministic environments Definition 4.25 and Definition 4.26 coincide. In stochastic environments the entropy-seeking agent does not work well because it gets distracted by noise in the environment rather than trying to distinguish environments (Orseau et al., 2013, Sec. 5). Moreover, the entropy-seeking agent may fail to seek knowledge in deterministic semimeasures as the following example demonstrates.\nExample 4.27 (Unnormalized Entropy-Seeking). If the Bayesian mixture \u03be is a semimeasure instead of a measure (such as the Solomonoff prior from Example 3.5), then the entropy-seeking agent does not explore correctly. Fix A := {\u03b1, \u03b2}, E := {0, 1}, and m = t (we only care about the entropy of the next percept). We illustrate the problem on a simple class of environments {\u03bd1, \u03bd2}:\n\u03bd1\u03b1/0/0.1 \u03b2/0/0.5 \u03bd2\u03b1/1/0.1 \u03b2/0/0.5\nwhere transitions are labeled with action/percept/probability. Both \u03bd1 and \u03bd2 return a percept deterministically or nothing at all (the environment ends). Only action \u03b1\n\u00a74.3 The Agents 65\ndistinguishes between the environments. With the prior w(\u03bd1) := w(\u03bd2) := 1/2, we get a mixture \u03be for the entropy-seeking value function V \u03c0Ent. Then V \u2217 Ent(\u03b1) \u2248 0.432 < 0.5 = V \u2217Ent(\u03b2), hence action \u03b2 is preferred over \u03b1 by the entropy-seeking agent. But taking action \u03b2 yields percept 0 (if any), hence nothing is learned about the environment. 3\nOn-policy value convergence (Theorem 4.19) ensures that asymptotically, the agent learns the value of its own policy. Knowledge-seeking agents do even better: they don\u2019t have to balance between exploration and exploitation, so they can focus solely on exploration. As a result, they learn off-policy, i.e., the value of counterfactual actions (Orseau et al., 2013, Thm. 7)."}, {"heading": "4.3.3 BayesExp", "text": "Lattimore (2013, Thm. 5.6) defines BayesExp combining AIXI with the informationseeking agent. BayesExp alternates between phases of exploration and phases of exploitation: Let \u03b5t be a monotone decreasing sequence of positive reals such that \u03b5t \u2192 0 as t\u2192\u221e. If the optimal information-seeking value V \u2217IG is larger than \u03b5t, then BayesExp starts an exploration phase, otherwise it starts an exploitation phase. During an exploration phase, BayesExp follows an optimal information-seeking policy for an \u03b5t-effective horizon. During an exploitation phase, BayesExp follows an \u03be-optimal reward-seeking policy for one step (see Algorithm 1).\nAlgorithm 1 BayesExp policy \u03c0BE (Lattimore, 2013, Alg. 2). 1: while true do 2: if V \u2217,t+Ht(\u03b5t)IG (\u00e6<t) > \u03b5t then 3: follow \u03c0\u2217IG for Ht(\u03b5t) steps 4: else 5: follow \u03c0\u2217\u03be for 1 step"}, {"heading": "4.3.4 Thompson Sampling", "text": "Thompson sampling, also known as posterior sampling or the Bayesian control rule, was originally proposed by Thompson (1933) as a bandit algorithm. It is easy to implement and often achieves quite good results (Chapelle and Li, 2011). In multi-armed bandits it attains optimal regret (Agrawal and Goyal, 2011; Kaufmann et al., 2012). Thompson sampling has also been discussed for MDPs (Strens, 2000; Dearden et al., 1998) and Bayesian and frequentist regret bounds have been established (Osband et al., 2013; Gopalan and Mannor, 2015).\nFor general RL Thompson sampling was first suggested by Ortega and Braun (2010) with resampling at every time step. Strens (2000) proposes following the optimal policy for one episode or \u201crelated to the number of state transitions the agent is likely to need to plan ahead\u201d. We follow Strens\u2019 suggestion and resample at the effective horizon.\nLet \u03b5t be a monotone decreasing sequence of positive reals such that \u03b5t \u2192 0 as t \u2192 \u221e. Our variant of Thomson sampling is given in Algorithm 2. It samples an"}, {"heading": "66 Acting", "text": "environment \u03c1 from the posterior, follows the \u03c1-optimal policy for an \u03b5t-effective horizon, and then repeats.\nAlgorithm 2 Thompson sampling policy \u03c0T . 1: while true do 2: sample \u03c1 \u223c w( \u00b7 | \u00e6<t) 3: follow \u03c0\u2217\u03c1 for Ht(\u03b5t) steps\nNote that \u03c0T is a stochastic policy since we occasionally sample from a distribution. We assume that this sampling is independent of everything else.\nChapter 5\nOptimality\nMachines will never be intelligent. \u2014 Shane Legg\nProblem 4.2 defines the general reinforcement learning problem. But our definition of this problem did not specify what a solution would be. This chapter is dedicated to this question:\nWhat is an optimal solution to the general reinforcement learning problem?\nHow can we say that one policy is better than another? What is the best policy? Are the policies from Section 4.3 optimal? Several notions of optimality for a policy \u03c0 in an environment classM are conceivable:\nO1. Maximal reward. The policy \u03c0 receives a reward of 1 in every time step (which is maximal according to Assumption 4.6b):\n\u2200t \u2208 N. rt = 1\nO2. Optimal policy. The policy \u03c0 achieves the highest possible value in the true environment \u00b5:\n\u2200\u00e6<t \u2208 (A\u00d7 E)\u2217. V \u03c0\u00b5 (\u00e6<t) = V \u2217\u00b5 (\u00e6<t)\nO3. Pareto optimality (Hutter, 2002a, Thm. 2). There is no other policy that performs at least as good in all environments and strictly better in at least one:\n@\u03c0\u0303. ( \u2200\u03bd \u2208M. V \u03c0\u0303\u03bd ( ) \u2265 V \u03c0\u03bd ( ) and \u2203\u03c1 \u2208M. V \u03c0\u0303\u03c1 ( ) > V \u03c0\u03c1 ( ) ) O4. Balanced Pareto optimality (Hutter, 2002a, Thm. 3). The policy \u03c0 achieves a\nbetter value acrossM weighted by w \u2208 \u2206M than any other policy:\n\u2200\u03c0\u0303. \u2211 \u03bd\u2208M w(\u03bd) ( V \u03c0\u03bd ( )\u2212 V \u03c0\u0303\u03bd ( ) ) \u2265 0\nO5. Bayes optimality . The policy \u03c0 is \u03be-optimal for some Bayes mixture \u03be:\n\u2200\u00e6<t \u2208 (A\u00d7 E)\u2217. V \u03c0\u03be (\u00e6<t) = V \u2217\u03be (\u00e6<t)\n67"}, {"heading": "68 Optimality", "text": "O6. Probably approximately correct . For a given \u03b5, \u03b4 > 0 the value of the policy \u03c0 is \u03b5-close to the optimal value with probability at least \u03b4 after time step t0(\u03b5, \u03b4):\n\u00b5\u03c0 [ \u2200t \u2265 t0(\u03b5, \u03b4). V \u2217\u00b5 (\u00e6<t)\u2212 V \u03c0\u00b5 (\u00e6<t) < \u03b5 ] > 1\u2212 \u03b4\nO7. Asymptotic optimality (Hutter, 2005, Sec. 5.3.4). The value of the policy \u03c0 converges to the optimal value:\nV \u2217\u00b5 (\u00e6<t)\u2212 V \u03c0\u00b5 (\u00e6<t)\u2192 0 as t\u2192\u221e\nO8. Sublinear regret . The difference between the reward sum of the policy \u03c0 and the best policy in hindsight grows sublinearly:\nsup \u03c0\u2032\nE\u03c0 \u2032 \u00b5 [ m\u2211 t=1 rt ] \u2212 E\u03c0\u00b5 [ m\u2211 t=1 rt ] \u2208 o(m)\nWe discuss these notions of optimality in turn. Achieving the maximal reward at every time step is impossible if there is no action that makes the environment \u00b5 respond with the maximal reward; generally there is no policy that achieves maximal rewards at every time step. In order to follow the optimal policy, we need to know the true environment. In our setting, the true environment is unknown and has to be learned. During the learning process the agent cannot also act optimally because it needs to explore. In particular, the policy \u03c0 cannot be optimal simultaneously in all environments fromM. This rules out O1 and O2 as a notion of optimality.\nIn Section 5.1 we show that all policies are Pareto optimal. This disqualifies O3 as a useful notion of optimality in general reinforcement learning.\nBalanced Pareto optimality (O4), Bayes optimality (O5), and maximal Legg-Hutter intelligence (Legg and Hutter, 2007b) turn out to coincide. In Section 5.3 we show that Legg-Hutter intelligence is highly subjective, because it depends on the choice of the prior. By changing the prior of a Bayesian agent, we can make the agent\u2019s intelligence arbitrarily low. In Section 5.2 we present a choice of particularly bad priors. This rules out O4 and O5 because they are prior-dependent and not objective.\nO6 is a stronger version of asymptotic optimality that provides a rate of convergence (it implies O7). Since our environment class can be very large and non-compact, concrete PAC results are likely impossible. Orseau (2010, 2013) shows that the Bayes optimal agent does not achieve asymptotic optimality in all computable environments. The underlying problem is that in the beginning the agent does not know enough about its environment and therefore relies heavily on its prior. Lack of exploration then retains the prior\u2019s bias. This problem can be alleviated by adding extra exploration to the Bayesian agent. In Section 5.4 we discuss two agents that achieve asymptotic optimality: BayesExp (Section 4.3.3) and Thompson sampling (Section 4.3.4). This establishes that O7 is possible.\nIn general environments sublinear regret is impossible because the agent can get stuck in traps from which it is unable to recover. This rules out O8. However, in Sec-\n\u00a75.1 Pareto Optimality 69\ntion 5.5 we show that if we assume that the environment allows recovering from mistakes (and some minor conditions on the discount function are fulfilled), then asymptotic optimality implies sublinear regret. This means that Thompson sampling has sublinear regret in these recoverable environments.\nNotably, only asymptotic optimality (O7) holds up to be a nontrivial and objective criterion of optimality that applies to the general reinforcement learning problem. While there are several agents that are known to be asymptotically optimal, some undesirable properties remain. Section 5.6 discusses this further. See also Mahadevan (1996) for a discussion of notions of optimality in MDPs."}, {"heading": "5.1 Pareto Optimality", "text": "In this section we show that Pareto optimality is not a useful criterion for optimality since for any environment class containingMCCMcomp, all policies are Pareto optimal.\nDefinition 5.1 (Pareto Optimality; Hutter, 2005, Def. 5.22). A policy \u03c0 is Pareto optimal in the set of environmentsM iff there is no policy \u03c0\u0303 such that V \u03c0\u0303\u03bd ( ) \u2265 V \u03c0\u03bd ( ) for all \u03bd \u2208M and V \u03c0\u0303\u03c1 ( ) > V \u03c0\u03c1 ( ) for at least one \u03c1 \u2208M.\nThe literature provides the following result.\nTheorem 5.2 (AIXI is Pareto Optimal; Hutter, 2002a, Thm. 2). Every \u03be-optimal policy is Pareto optimal inMCCSLSC .\nThe following theorem was proved for deterministic policies in Leike and Hutter (2015c). Here we extend it to stochastic policies.\nTheorem 5.3 (Pareto Optimality is Trivial). Every policy is Pareto optimal in any classM\u2287MCCMcomp.\nThe proof proceeds as follows: for a given policy \u03c0, we construct a set of \u2018buddy environments\u2019 that reward \u03c0 and punish other policies. Together they can defend against any policy \u03c0\u0303 that tries to take the crown of Pareto optimality from \u03c0.\nProof. We assume (0, 0) and (0, 1) \u2208 E . Moreover, assume there is a policy \u03c0 that is not Pareto optimal. Then there is a policy \u03c0\u0303 that Pareto dominates \u03c0, i.e., V \u03c0\u0303\u03c1 ( ) > V \u03c0\u03c1 ( ) for some \u03c1 \u2208 M, and V \u03c0\u0303\u03bd ( ) \u2265 V \u03c0\u03bd ( ) for all \u03bd \u2208 M. From V \u03c0\u0303\u03c1 ( ) > V \u03c0\u03c1 ( ) and Lemma 4.18 we get that there is a shortest and lexicographically first history \u00e6\u2032<k consistent with \u03c0 and \u03c0\u0303 such that \u03c0(\u03b1 | \u00e6\u2032<k) > \u03c0\u0303(\u03b1 | \u00e6\u2032<k) for some action \u03b1 \u2208 A and V \u03c0\u0303\u03c1 (\u00e6\u2032<k) > V \u03c0 \u03c1 (\u00e6\u2032<k). Consequently there is an i \u2265 k such that \u03b3i > 0, and hence \u0393k > 0. We define the environment \u00b5 that first reproduces the separating history \u00e6\u2032<k and then, if \u03b1 is the next action, returns reward 1 forever, and otherwise returns reward 0 forever. Formally, \u00b5 is defined by\n\u00b5(e1:t | e<t \u2016 a1:t) :=  1, if t < k and et = e\u2032t, 1, if t \u2265 k and ak = \u03b1 and rt = 1 and ot = 0, 1, if t \u2265 k and ak 6= \u03b1 and rt = 0 = ot, and 0, otherwise."}, {"heading": "70 Optimality", "text": "The environment \u00b5 is computable, even if the policy \u03c0 is not: for a fixed history \u00e6\u2032<t and action \u03b1, there exists a program computing \u00b5; therefore \u00b5 \u2208 MCCMcomp. We get the following value difference for the policies \u03c0 and \u03c0\u0303:\nV \u03c0\u00b5 ( )\u2212 V \u03c0\u0303\u00b5 ( ) = E\u03c0\u00b5 [ k\u22121\u2211 t=1 \u03b3trt + \u221e\u2211 t=k \u03b3trt ] \u2212 E\u03c0\u0303\u00b5 [ k\u22121\u2211 t=1 \u03b3trt \u2212 \u221e\u2211 t=k \u03b3trt ]\n= ( \u03c0(\u03b1 | \u00e6\u2032<k) \u221e\u2211 t=k \u03b3t \u2212 \u03c0\u0303(\u03b1 | \u00e6\u2032<k) \u221e\u2211 t=k \u03b3t ) \u00b5\u03c0(\u00e6\u2032<k)\n= ( \u03c0(\u03b1 | \u00e6\u2032<k)\u2212 \u03c0\u0303(\u03b1 | \u00e6\u2032<k) ) \u00b5\u03c0(\u00e6\u2032<k)\u0393k > 0\nHence V \u03c0\u0303\u00b5 ( ) < V \u03c0\u00b5 ( ), which contradicts the fact that \u03c0\u0303 Pareto dominates \u03c0 since M\u2287MCCMcomp 3 \u00b5.\nNote that the environment \u00b5 we defined in the proof of Theorem 5.3 is actually just a finite-state POMDP, so Pareto optimality is also trivial for smaller environment classes."}, {"heading": "5.2 Bad Priors", "text": "In this section we give three examples of universal priors that cause a AIXI to misbehave drastically. In case of a finite horizon, the indifference prior makes all actions equally preferable to AIXI (Section 5.2.1). The dogmatic prior makes AIXI stick to any given computable policy \u03c0 as long as expected future rewards do not fall too close to zero (Section 5.2.2). The G\u00f6del prior prevents AIXItl from taking any actions (Section 5.2.3)."}, {"heading": "5.2.1 The Indifference Prior", "text": "The following theorem constructs the indifference prior that yields a Bayesian mixture \u03be\u2032 that causes argmax ties for the first m steps. If we use a discount function that only cares about the first m steps, \u0393m = 0, then all policies are \u03be\u2032-optimal policies. In this case AIXI\u2019s behavior only depends on how we break argmax ties.\nTheorem 5.4 (Indifference Prior). If there is an m such that \u0393m = 0, then there is a Bayesian mixture \u03be\u2032 such that all policies are \u03be\u2032-optimal.\nProof. First, we assume that the action space is binary, A = {0, 1}. Let U be the reference UTM and define the UTM U \u2032 by\nU \u2032(s<mp, a1:t) := U(p, a1:t xor s1:t),\nwhere s<m is a binary string of lengthm\u22121 and sk := 0 for k \u2265 m. (U \u2032 has no programs of length less than m \u2212 1.) Let \u03be\u2032 be the Bayesian mixture given by U \u2032 according to\n\u00a75.2 Bad Priors 71\n(4.8). Then\n\u03be\u2032(e<m \u2016 a<m) = \u2211\np: e<mvU \u2032(p,a<m)\n2\u2212|p|\n= \u2211\ns<mp\u2032: e<mvU \u2032(s<mp\u2032,a<m)\n2\u2212m\u22121\u2212|p \u2032|\n= \u2211 s<m \u2211 p\u2032: e<mvU(p\u2032,a<m xor s<m) 2\u2212m\u22121\u2212|p \u2032|\n= \u2211 s<m \u2211 p\u2032: e<mvU(p\u2032,s<m) 2\u2212m\u22121\u2212|p \u2032|,\nwhich is independent of a<m. Hence the first m \u2212 1 percepts are independent of the first m\u2212 1 actions. But the percepts\u2019 rewards from time step m on do not matter since \u0393m = 0 (Lemma 4.16). Because the environment is chronological, the value function must be independent of all actions. Thus every policy is \u03be\u2032-optimal.\nFor finite action spaces A with more than 2 elements, the proof works analogously by making A a cyclic group and using the group operation instead of xor.\nThe choice of U \u2032 in the proof of Theorem 5.4 depends on m. If we increase AIXI\u2019s horizon while fixing the UTM U \u2032, Theorem 5.4 no longer holds. For Solomonoff induction, there is an analogous problem: when using Solomonoff\u2019s prior M to predict a deterministic binary sequence x, we make at most K(x) errors (Corollary 3.56). In case the shortest program has length > m, there is no guarantee that we make less than m errors (see Section 5.6.2)."}, {"heading": "5.2.2 The Dogmatic Prior", "text": "In this section we define a universal prior that assigns very high probability of going to hell (reward 0 forever) if we deviate from a given computable policy \u03c0. For a Bayesian agent like AIXI, it is thus only worth deviating from the policy \u03c0 if the agent thinks that the prospects of following \u03c0 are very poor already. We call this prior the dogmatic prior, because the fear of going to hell makes AIXI conform to any arbitrary \u2018dogmatic ideology\u2019 \u03c0. AIXI will only break out if it expects \u03c0 to give very low future payoff; in that case the agent does not have much to lose.\nTheorem 5.5 (Dogmatic Prior). Let \u03c0 be any computable deterministic policy, let \u03be be any Bayesian mixture over MCCSLSC , and let \u03b5 > 0. There is a Bayesian mixture \u03be\u2032 such that for any history \u00e6<t consistent with \u03c0 and for which V \u03c0\u03be (\u00e6<t) > \u03b5, the action \u03c0(\u00e6<t) is the unique \u03be\u2032-optimal action.\nThe following proof was adapted from Leike and Hutter (2015c) to work for environment classes that do not contain the Bayesian mixture. Essentially, for every environment \u03bd \u2208 MCCSLSC the dogmatic prior puts much higher weight on an environment \u03c1\u03bd that behaves just like \u03bd on the policy \u03c0, but sends any policy deviating from \u03c0 to hell. Importantly, while following the policy \u03c0 the environments \u03bd and \u03c1\u03bd are indistinguishable, so the posterior belief in \u03bd is equal to the posterior belief in \u03c1\u03bd ."}, {"heading": "72 Optimality", "text": "Proof of Theorem 5.5. We assume (o, 0) \u2208 E for some o \u2208 O. For every environment \u03bd \u2208MCCSLSC define the environment\n\u03c1\u03bd(e1:t \u2016 a1:t) :=  \u03bd(e1:t \u2016 a1:t), if ak = \u03c0(\u00e6<k) \u2200k \u2264 t, \u03bd(e<k \u2016 a<k), if k := min{i | ai 6= \u03c0(\u00e6<i)} exists\nand ei = (o, 0) \u2200i \u2208 {k, . . . , t}, and 0, otherwise.\nThe environment \u03c1\u03bd mimics environment \u03bd until it receives an action that the policy \u03c0 would not take. From then on, it provides rewards 0. Since \u03c0 is a computable policy, we have that \u03c1\u03bd \u2208MCCSLSC for every \u03bd \u2208MCCSLSC .\nNow we need to reweigh the prior w so that it assigns a much higher prior weight to \u03c1\u03bd than to \u03bd. Without loss of generality we assume that \u03b5 is computable, otherwise we make it slightly smaller. We define w\u2032(\u03bd) := \u03b5w(\u03bd) if \u03bd 6= \u03c1\u03bd\u0303 for all \u03bd\u0303 \u2208 MCCSLSC and w\u2032(\u03c1\u03bd) := (1\u2212 \u03b5)w(\u03bd) + \u03b5w(\u03c1\u03bd). Then\u2211\n\u03bd\u2208MCCSLSC\nw\u2032(\u03bd) = \u2211 \u03bd=\u03c1\u03bd\u0303 w\u2032(\u03bd) + \u2211 \u03bd 6=\u03c1\u03bd\u0303 w\u2032(\u03bd)\n= \u2211 \u03bd=\u03c1\u03bd\u0303 ( (1\u2212 \u03b5)w(\u03bd\u0303) + \u03b5w(\u03bd) ) + \u2211 \u03bd 6=\u03c1\u03bd\u0303 \u03b5w(\u03bd)\n= \u2211\n\u03bd\u2208MCCSLSC\n\u03b5w(\u03bd) + \u2211\n\u03bd\u0303\u2208MCCSLSC\n(1\u2212 \u03b5)w(\u03bd\u0303)\n= \u03b5+ (1\u2212 \u03b5) = 1,\nand with w\u2032 \u2265 \u03b5w, we get that w\u2032 is a positive prior over MCCSLSC . We define \u03be\u2032 as the corresponding Bayesian mixture analogous to (4.5).\nWith \u03c1 := \u2211\n\u03bd\u2208MCCSLSC w(\u03bd)\u03c1\u03bd we get \u03be\u2032 = \u03b5\u03be + (1 \u2212 \u03b5)\u03c1. The mixtures \u03be and \u03c1\ncoincide on the policy \u03c0 since every \u03bd coincides with \u03c1\u03bd on the policy \u03c0:\n\u03be\u03c0(\u00e6<t) = \u2211\n\u03bd\u2208MCCSLSC\nw(\u03bd)\u03bd\u03c0(\u00e6<t) = \u2211\n\u03bd\u2208MCCSLSC\nw(\u03bd)\u03c1\u03c0\u03bd (\u00e6<t) = \u03c1 \u03c0(\u00e6<t)\nMoreover, V \u2217\u03c1\u03bd (\u00e6<t) = 0 and thus V \u2217 \u03c1 (\u00e6<t) = 0 for any history inconsistent with \u03c0 by construction of \u03c1\u03bd .\nLet \u00e6<t \u2208 (A\u00d7E)\u2217 be any history consistent with \u03c0 such that V \u03c0\u03be (\u00e6<t) > \u03b5. Then \u03c1\u03c0 = \u03be\u03c0 implies\n\u03c1(e<t \u2016 a<t) \u03be\u2032(e<t \u2016 a<t) = \u03be(e<t \u2016 a<t) \u03be\u2032(e<t \u2016 a<t) = \u03be(e<t \u2016 a<t) \u03b5\u03be(e<t \u2016 a<t) + (1\u2212 \u03b5)\u03c1(e<t \u2016 a<t) = 1.\n\u00a75.2 Bad Priors 73\nTherefore Lemma 4.14 implies that for all a \u2208 A and all policies \u03c0\u0303\nV \u03c0\u0303\u03be\u2032 (\u00e6<ta) = \u03b5 \u03be(e<t \u2016 a<t) \u03be\u2032(e<t \u2016 a<t) V \u03c0\u0303\u03be (\u00e6<ta) + (1\u2212 \u03b5) \u03c1(e<t \u2016 a<t) \u03be\u2032(e<t \u2016 a<t) V \u03c0\u0303\u03c1 (\u00e6<ta)\n= \u03b5V \u03c0\u0303\u03be (\u00e6<ta) + (1\u2212 \u03b5)V \u03c0\u0303\u03c1 (\u00e6<ta). (5.1)\nLet \u03b1 := \u03c0(\u00e6<t) be the next action according to \u03c0, and let \u03b2 6= \u03b1 be any other action. We have that V \u03c0\u03be (\u00e6<t\u03b1) = V \u03c0 \u03c1 (\u00e6<t\u03b1) since \u03be\u03c0 = \u03c1\u03c0 and \u00e6<t\u03b1 is consistent with \u03c0. Therefore we get from (5.1)\nV \u2217\u03be\u2032(\u00e6<t\u03b1) \u2265 V \u03c0\u03be\u2032 (\u00e6<t\u03b1) = \u03b5V \u03c0\u03be (\u00e6<t\u03b1) + (1\u2212 \u03b5)V \u03c0\u03c1 (\u00e6<t\u03b1) = V \u03c0\u03be (\u00e6<t\u03b1) > \u03b5, V \u2217\u03be\u2032(\u00e6<t\u03b2) = \u03b5V \u03c0\u2217 \u03be\u2032 \u03be (\u00e6<t\u03b2) + (1\u2212 \u03b5)V \u03c0\u2217 \u03be\u2032 \u03c1 (\u00e6<t\u03b2) = \u03b5V \u03c0\u2217 \u03be\u2032 \u03be (\u00e6<ta) + (1\u2212 \u03b5)0 \u2264 \u03b5.\nHence V \u2217\u03be\u2032(\u00e6<t\u03b1) > V \u2217 \u03be\u2032(\u00e6<t\u03b2) and thus the action \u03b1 taken by \u03c0 is the only \u03be \u2032-optimal action for the history \u00e6<t.\nCorollary 5.6 (With Finite Horizon Every Policy is Bayes Optimal). If \u0393m = 0 for some m \u2208 N, then for any deterministic policy \u03c0 there is a Bayesian mixture \u03be\u2032 such that \u03c0(\u00e6<t) is the only \u03be\u2032-optimal action for all histories \u00e6<t consistent with \u03c0 and t \u2264 m.\nIn contrast to Theorem 5.4 where every policy is \u03be\u2032-optimal for a fixed Bayesian mixture \u03be\u2032, Corollary 5.6 gives a different Bayesian mixture \u03be\u2032 for every policy \u03c0 such that \u03c0 is the only \u03be\u2032-optimal policy.\nProof. Let \u03b5 > 0 be small enough such that V \u03c0\u03be (\u00e6<t) > \u03b5 for all \u00e6<t and t \u2264 m. (This is possible because (A\u00d7 E)m is finite by Assumption 4.6c.) We use the dogmatic prior from Theorem 5.5 to construct a Bayesian mixture \u03be\u2032 for the policy \u03c0 and \u03b5 > 0. Thus for any history \u00e6<t \u2208 (A\u00d7 E)\u2217 consistent with \u03c0 and t \u2264 m, the action \u03c0(\u00e6<t) is the only \u03be\u2032-optimal action.\nCorollary 5.7 (AIXI Emulating Computable Policies). Let \u03b5 > 0 and let \u03c0 be any computable policy. There is a Bayesian mixture \u03be\u2032 such that for any \u03be\u2032-optimal policy \u03c0\u2217\u03be\u2032 and for any environment \u03bd,\u2223\u2223\u2223\u2223V \u03c0\u2217\u03be\u2032\u03bd ( )\u2212 V \u03c0\u03bd ( )\u2223\u2223\u2223\u2223 < \u03b5. Proof. From the proof of Corollary 5.6 and Lemma 4.18."}, {"heading": "5.2.3 The G\u00f6del Prior", "text": "This section introduces a prior that prevents any fixed formal system from making any statements about the outcome of all but finitely many computations. It is named after G\u00f6del (1931) who famously showed that for any sufficiently rich formal system there are statements that it can neither prove nor disprove."}, {"heading": "74 Optimality", "text": "This prior is targeted at AIXItl, a computable approximation to AIXI defined by Hutter (2005, Sec. 7.2). AIXItl aims to perform as least as well as the best agent who is limited by time t and space l that can be verified using a proof of length at most n for some fixed n \u2208 N. The core idea is to enumerate all deterministic policies and proofs and then execute the policy for which the best value has been proved.\nIn order to be verified, a policy \u03c0 has to be computed by a program p which fulfills the verification condition VA(p) (Hutter, 2005, Eq. 7.7). This program p not only computes future actions of \u03c0, but also hypothetical past actions a\u2032i and lower bounds vi for the value of the policy \u03c0:\nVA(p) := \u201c\u2200k\u2200(va\u2032\u00e6)1:k. ( p(\u00e6<k) = v1a\u20321 . . . vka \u2032 k \u2192 vk \u2264 V \u03c0\u03be (\u00e6<k) ) \u201d,\nwhere \u03c0 is the policy derived from p according to \u03c0(\u00e6<k) := a\u2032k. We fix some formal system that we use to prove the verification condition. We want it to be sufficiently powerful, but this incurs G\u00f6del incompleteness. For simplicity of exposition we pick PA, the system of Peano arithmetic (Shoenfield, 1967, Ch. 8.1), but our result generalizes trivially to all formal systems which cannot prove their own consistency.\nLet n be a fixed constant. The algorithm for AIXItl is specified as follows.\n1. Let P = \u2205. This will be the set of verified programs.\n2. For all proofs in PA of length \u2264 n: if the proof proves VA(p) for some p, and |p| \u2264 l, then add the program p to P .\n3. For each input history \u00e6<k repeat: run all programs from P for at most t steps each, take the one with the highest promised value vk, and return that program\u2019s policy\u2019s action.\nTheorem 5.8 (The G\u00f6del Prior). There is a UTM U \u2032 such that if PA is consistent, then the set of verified programs P is empty for all t, l, and n.\nProof. Let q denote an algorithm that never halts, but for which this cannot be proved in PA; e.g., let q enumerate all consequences of PA and halt as soon as it finds a contradiction. Since we assumed that PA is consistent, q never halts. Define the UTM U \u2032(p, a1:k) as follows.\n\u2022 Run q for k steps.\n\u2022 If q halts, output vk = 2.\n\u2022 Run U(p, a1:k).\nSince q never halts, U and U \u2032 are functionally identical, therefore U \u2032 is universal. Note that PA proves \u2200p. U(p, a1:k) = U \u2032(p, a1:k) for any fixed k, but PA does not prove \u2200k\u2200p. U(p, a1:k) = U \u2032(p, a1:k).\n\u00a75.3 Bayes Optimality 75\nIf q did eventually halt, it would output a value vk = 2 that is too high, since the value function V \u03c0\u03be is bounded by 1 from above, which PA knows. Hence PA proves that\nq halts\u2192 \u2200p. \u00acVA(p) (5.2)\nIf PA could prove VA(p) for any p, then PA would prove that q does not halt since this is the contrapositive of (5.2). Therefore the set P remains empty.\nAIXItl exhibits all the problems of the arbitrariness of the UTM illustrated by the indifference prior (Theorem 5.4) and the dogmatic prior (Theorem 5.5). In addition, it is also susceptible to G\u00f6del incompleteness as illustrated by the G\u00f6del prior in Theorem 5.8. The formal system that is a parameter to AIXItl just provides another point of failure.\nAs a computable approximation to AIXI, AIXItl is needlessly complicated. As we prove in Corollary 6.13, \u03b5-optimal AIXI is limit computable, so we can approximate it with an anytime algorithm. Bounding the computational resources to the approximation algorithm already yields a computable version of AIXI. Moreover, unlike AIXItl, this approximation actually converges to AIXI in the limit. Furthermore, we can \u2018speed up\u2019 this approximation algorithm using Hutter search (Hutter, 2002b); this is very similar but not identical to AIXItl."}, {"heading": "5.3 Bayes Optimality", "text": "The aim of the Legg-Hutter intelligence measure is to formalize the intuitive notion of intelligence mathematically. Legg and Hutter (2007a) collect various definitions of intelligence across many academic fields and destill it into the following statement (Legg and Hutter, 2007b)\nIntelligence measures an agent\u2019s ability to achieve goals in a wide range of environments.\nThis definition is formalized as follows.\nDefinition 5.9 (Legg-Hutter Intelligence; Legg and Hutter, 2007b, Sec. 3.3)). The (Legg-Hutter) intelligence of a policy \u03c0 is defined as\n\u03a5\u03be(\u03c0) := \u2211 \u03bd\u2208M w(\u03bd)V \u03c0\u03bd ( )\nThe Legg-Hutter intelligence of a policy \u03c0 is the t0-value that \u03c0 achieves across all environments from the classM weighted by the prior w. Legg and Hutter (2007b) consider a subclass ofMCCSLSC , the class of computable measures together with a Solomonoff prior w(\u03bd) = 2\u2212K(\u03bd) and do not use discounting explicitly.\nTypically, the index \u03be is omitted when writing \u03a5. However, in this section we consider the intelligence measure with respect to different priors, therefore we make this dependency explicit. The following proposition motivates the use of the index \u03be instead of w."}, {"heading": "76 Optimality", "text": "Proposition 5.10 (Bayes Optimality = Maximal Intelligence). \u03a5\u03be(\u03c0) = V \u03c0\u03be ( ) for all policies \u03c0.\nProof. Follows directly from (4.6) and Definition 5.9.\nDefinition 5.11 (Balanced Pareto Optimality; Hutter, 2005, Def. 5.22). LetM be a set of environments. A policy \u03c0 is balanced Pareto optimal in the set of environments M iff for all policies \u03c0\u0303, \u2211\n\u03bd\u2208M w(\u03bd)\n( V \u03c0\u03bd ( )\u2212 V \u03c0\u0303\u03bd ( ) ) \u2265 0.\nProposition 5.12 (Balanced Pareto Optimality = Maximal Intelligence). A policy \u03c0 is balanced Pareto optimal inM if and only if \u03c0 has maximal Legg-Hutter intelligence.\nProof. From (4.6) we get\u2211 \u03bd\u2208M w(\u03bd) ( V \u03c0\u03bd ( )\u2212 V \u03c0\u2217\u03be \u03bd ( ) ) = \u2211 \u03bd\u2208M w(\u03bd)V \u03c0\u03bd ( )\u2212 \u2211 \u03bd\u2208M w(\u03bd)V \u03c0\u2217\u03be \u03bd ( )\n= V \u03c0\u03be ( )\u2212 V \u2217\u03be ( ) = V \u03c0\u03be ( )\u2212 sup\n\u03c0\u0303 V \u03c0\u0303\u03be ( )\n= \u03a5\u03be(\u03c0)\u2212 sup \u03c0\u0303 \u03a5(\u03c0\u0303)\nby Proposition 5.10. This term is nonnegative iff \u03a5\u03be(\u03c0) is maximal.\nAs a consequence of Proposition 5.10 and Proposition 5.12 we get that AIXI is balanced Pareto optimal (Hutter, 2005, Thm. 5.24) and has maximal Legg-Hutter intelligence.\n\u03a5\u03be := sup \u03c0 \u03a5\u03be(\u03c0) = sup \u03c0 V \u03c0\u03be ( ) = V\n\u03c0\u2217\u03be \u03be ( ) = \u03a5\u03be(\u03c0 \u2217 \u03be ).\nThis is not surprising since Legg-Hutter intelligence was defined in terms of the t0value in the Bayes mixture. Moreover, because the value function is scaled to be in the interval [0, 1], intelligence is a real number between 0 and 1.\nIt is just as hard to score very high on the Legg-Hutter intelligence measure as it is to score very low: we can always turn a reward minimizer into a reward maximizer by inverting the rewards r\u2032t := 1 \u2212 rt. Hence the lowest possible intelligence score is achieved by AIXI\u2019s twin sister, a \u03be-expected reward minimizer:\n\u03a5\u03be := inf\u03c0 \u03a5\u03be(\u03c0) = inf \u03c0 V \u03c0\u03be ( )\nThe heaven environment (reward 1 forever) and the hell environment (reward 0 forever) are computable and thus in the environment classMCCSLSC ; therefore it is impossible to get a reward 0 or reward 1 in every environment. Consequently, for all policies \u03c0,\n0 < \u03a5\u03be \u2264 \u03a5\u03be(\u03c0) \u2264 \u03a5\u03be < 1.\n\u00a75.3 Bayes Optimality 77\nFor every real number r \u2208 [\u03a5\u03be,\u03a5\u03be] there is a policy \u03c0 with \u03a5\u03be(\u03c0) = r: analogously to Lemma 4.14 we can define \u03c0 such that with probability (r \u2212 \u03a5\u03be)/(\u03a5\u03be \u2212 \u03a5\u03be) it follows \u03c0\u2217\u03be and otherwise it follows arg min\u03c0\u0303 V \u03c0\u0303 \u03be ( ).\nFigure 5.1 illustrates the intelligence measure \u03a5. It is natural to fix the policy random that takes actions uniformly at random to have an intelligence score of 1/2 by choosing a \u2018symmetric\u2019 universal prior (Legg and Veness, 2013).\nAIXI is not computable (Theorem 6.15), hence there is no computable policy \u03c0 such that \u03a5\u03be(\u03c0) = \u03a5\u03be or \u03a5\u03be(\u03c0) = \u03a5\u03be for any Bayesian mixture \u03be overMCCSLSC . But the following theorem states that computable policies can come arbitrarily close. This is no surprise: by Lemma 4.17 we can do well on a Legg-Hutter intelligence test simply by memorizing what AIXI would do for the first k steps; as long as k is chosen large enough such that discounting makes the remaining rewards contribute very little to the value function.\nTheorem 5.13 (Computable Policies are Dense). The set\n{\u03a5\u03be(\u03c0) | \u03c0 is a computable policy}\nis dense in the set [\u03a5\u03be,\u03a5\u03be].\nProof. Let \u03c0 be any policy and let \u03b5 > 0. We need to show that there is a computable policy \u03c0\u0303 with |\u03a5\u03be(\u03c0\u0303)\u2212\u03a5\u03be(\u03c0)| < \u03b5. We choose m large enough such that \u0393m/\u03931 < \u03b5/3. Let \u03b1 \u2208 A be arbitrary and define the policy\n\u03c0\u0303(a | \u00e6<t) :=  \u03c0(a | \u00e6<t)\u00b1 (\u03b5/3)\u2212m if t < m, 1 if t \u2265 m and a = \u03b1, and 0 otherwise.\nBy choosing an appropriate rational number in the interval [\u03c0(a | \u00e6<t)\u2212 (\u03b5/3)\u2212m, \u03c0(a | \u00e6<t) + (\u03b5/3)\u2212m] we can make the policy \u03c0\u0303 computable because we can store these approximations to the action probabilities of \u03c0 for the first m \u2212 1 steps in a lookup table. From Lemma 4.17 we get\u2223\u2223\u2223V \u03c0,m\u03be ( )\u2212 V \u03c0\u0303,m\u03be ( )\u2223\u2223\u2223 \u2264 Dm\u22121(\u03be\u03c0, \u03be\u03c0\u0303 | ) \u2264 ((\u03b5/3)\u2212m)m = \u03b53"}, {"heading": "78 Optimality", "text": "and together with Lemma 4.16 this yields\n|\u03a5\u03be(\u03c0)\u2212\u03a5\u03be(\u03c0\u0303)| = \u2223\u2223V \u03c0\u03be ( )\u2212 V \u03c0\u0303\u03be ( )\u2223\u2223 \u2264 \u2223\u2223\u2223V \u03c0,m\u03be ( )\u2212 V \u03c0\u0303,m\u03be ( )\u2223\u2223\u2223+ 2\u0393m\u03931 \u2264 \u03b53 + 2\u0393m\u03931 < \u03b5.\nRemark 5.14 (Deterministic Policies are not Dense in [\u03a5\u03be,\u03a5\u03be]). The intelligence values of deterministic policies are generally not dense in the interval [\u03a5\u03be,\u03a5\u03be]. We show this by defining an environment \u03bd where the first action determines whether the agent goes to heaven or hell: action \u03b1 leads to heaven and action \u03b2 leads to hell. Define Bayesian mixture \u03be\u2032 := 0.999\u03bd + 0.001\u03be and let \u03c0 be any policy. If \u03c0 takes action \u03b1 first, then \u03a5\u03be\u2032(\u03c0) > 0.999. If \u03c0 takes action \u03b2 first, then \u03a5\u03be\u2032(\u03c0) < 0.001. Hence there are no deterministic policies that score an intelligence value in the closed interval [0.001, 0.999]. 3\nLegg-Hutter intelligence is measured with respect to a fixed prior. The Bayes agent is the most intelligent policy if it uses the same prior. We use the results from Section 5.2 to show that the intelligence score of the Bayes agent can be arbitrary close to the minimum intelligence score \u03a5\u03be.\nCorollary 5.15 (Some AIXIs are Stupid). For any Bayesian mixture \u03be over MCCSLSC and every \u03b5 > 0, there is a Bayesian mixture \u03be\u2032 such that \u03a5\u03be(\u03c0\u2217\u03be\u2032) < \u03a5\u03be + \u03b5.\nProof. Let \u03b5 > 0. According to Theorem 5.13, there is a computable policy \u03c0 such that \u03a5\u03be(\u03c0) < \u03a5\u03be + \u03b5/2. From Corollary 5.7 we get a Bayesian mixture \u03be\u2032 such that |\u03a5\u03be(\u03c0\u2217\u03be\u2032)\u2212\u03a5\u03be(\u03c0)| = |V \u03c0\u2217 \u03be\u2032 \u03be ( )\u2212 V \u03c0 \u03be ( )| < \u03b5/2, hence\n|\u03a5\u03be(\u03c0\u2217\u03be\u2032)\u2212\u03a5\u03be| \u2264 |\u03a5\u03be(\u03c0\u2217\u03be\u2032)\u2212\u03a5\u03be(\u03c0)|+ |\u03a5\u03be(\u03c0)\u2212\u03a5\u03be| < \u03b5/2 + \u03b5/2 = \u03b5.\nWe get the same result if we fix AIXI, but rig the intelligence measure.\nCorollary 5.16 (AIXI is Stupid for Some \u03a5). For any deterministic \u03be-optimal policy \u03c0\u2217\u03be and for every \u03b5 > 0 there is a Bayesian mixture \u03be\n\u2032 such that \u03a5\u03be\u2032(\u03c0\u2217\u03be ) \u2264 \u03b5 and \u03a5\u03be\u2032 \u2265 1\u2212 \u03b5. Proof. Let a1 := \u03c0\u2217\u03be ( ) be the first action that \u03c0 \u2217 \u03be takes. We define an environment \u03bd such that taking the first action a1 leads to hell and taking any other first action leads to heaven as in Remark 5.14. We define the Bayesian mixture \u03be\u2032 := (1\u2212 \u03b5)\u03bd+ \u03b5\u03be. Since \u03c0\u2217\u03be takes action a1 first, it goes to hell, i.e., V \u03c0\u2217\u03be \u03bd ( ) = 0. Hence with Lemma 4.14\n\u03a5\u03be\u2032(\u03c0 \u2217 \u03be ) = V \u03c0\u2217\u03be \u03be\u2032 ( ) = (1\u2212 \u03b5)V \u03c0\u2217\u03be \u03bd ( ) + \u03b5V \u03c0\u2217\u03be \u03be ( ) \u2264 \u03b5.\nFor any policy \u03c0 that takes an action other than a1 first, we get\n\u03a5\u03be\u2032(\u03c0) = V \u03c0 \u03be\u2032 ( ) = (1\u2212 \u03b5)V \u03c0\u03bd ( ) + \u03b5V \u03c0\u03be ( ) \u2265 1\u2212 \u03b5.\nOn the other hand, we can make any computable policy smart if we choose the right Bayesian mixture. In particular, we get that there is a Bayesian mixture such that \u2018do nothing\u2019 is the most intelligent policy save for some \u03b5.\nCorollary 5.17 (Computable Policies can be Smart). For any computable policy \u03c0 and any \u03b5 > 0 there is a Bayesian mixture \u03be\u2032 such that \u03a5\u03be\u2032(\u03c0) > \u03a5\u03be\u2032 \u2212 \u03b5.\nProof. Corollary 5.7 yields a Bayesian mixture \u03be\u2032 with |\u03a5\u03be\u2032\u2212\u03a5\u03be\u2032(\u03c0)| = |V \u2217\u03be\u2032( )\u2212V \u03c0\u03be\u2032 ( )| < \u03b5."}, {"heading": "5.4 Asymptotic Optimality", "text": "An asymptotically optimal policy is a policy learns to act optimally in every environment fromM, i.e., the value of this policy converges to the optimal value.\nDefinition 5.18 (Asymptotic Optimality). A policy \u03c0 is asymptotically optimal in an environment classM iff for all \u00b5 \u2208M\nV \u2217\u00b5 (\u00e6<t)\u2212 V \u03c0\u00b5 (\u00e6<t)\u2192 0 as t\u2192\u221e (5.3)\non histories drawn from \u00b5\u03c0.\nThere are different types of asymptotic optimality based on the type of stochastic convergence in (5.3); see Definition 2.5. If this convergence occurs almost surely, it is called strong asymptotic optimality (Lattimore and Hutter, 2011, Def. 7); if this convergence occurs in mean, it is called asymptotic optimality in mean; if this convergence occurs in probability, it is called asymptotic optimality in probability ; and if the Ces\u00e0ro averages converge almost surely, it is called weak asymptotic optimality (Lattimore and Hutter, 2011, Def. 7). Since the value function is a nonnegative bounded random variable, asymptotic optimality in mean and asymptotic optimality in probability are equivalent. See Table 5.1 for the explicit definitions and see Figure 5.2 for an overview over their relationship.\nAsymptotic optimality in probability is in spirit a probably approximately correct (PAC) result: for all \u03b5 > 0 and \u03b4 > 0 the probability that our policy is \u03b5-suboptimal converges to zero; eventually this probability will be less than \u03b4. For a PAC result it is typically demanded that the number of time steps until the probability is less than \u03b4\nbe polynomial in 1/\u03b5 and 1/\u03b4. In general environments this is impossible, and here we have no ambition to provide concrete convergence rates.\nIntuitively, a necessary condition for asymptotic optimality is that the agent needs to explore infinitely often for an entire effective horizon. If we explore only finitely often, then the environment might change after we stopped exploring. Moreover, the agent needs to predict the value of counterfactual policies accurately; but by Lemma 4.16 only for an \u03b5-effective horizon. By committing to exploration for the entire effective horizon, we learn about the value of counterfactual policies.\nExample 5.19 (Exploration Infinitely Often for an Entire Effective Horizon). If there is an \u03b5 > 0 such that the policy \u03c0 does not explore for Ht(\u03b5) steps infinitely often, then V \u2217\u00b5 (\u00e6<t) \u2212 V \u03c0\u00b5 (\u00e6<t) > \u03b5 infinitely often. Define A := {\u03b1, \u03b2} and E := {0, \u03b5/2, 1} (observations are vacuous) and consider the following class of environments M := {\u03bd\u221e, \u03bd1, \u03bd2, . . .} (transitions are labeled with condition: action, reward):\ns0\n\u03b2, \u03b52\n\u03b1, 0\ns0 s1 . . . sn\n\u03b2, \u03b52\nt < k : \u03b1, 0\nt \u2265 k : \u03b1, 0 \u03b1, 0 \u03b1, 0 \u03b2, 0\n\u03b2, 0\n\u03b2, 0\n\u03b1, 1\n\u03bd\u221e \u03bdk\nEnvironment \u03bdk works just like environment \u03bd\u221e, except that at time step k the path to state s1 gets unlocked. The length of the state sequence in \u03bdk is defined as an \u03b5-effective horizon, n := Ht(\u03b5) where t is the time step in which the agent leaves state s0. The optimal policy in environment \u03bd\u221e is to always take action \u03b2, the optimal policy for environment \u03bdk is to take action \u03b2 for t < k and then take action \u03b1. Suppose the agent is in time step t and in state s0. Since these environments are partially observable, it needs to explore for n steps (take action \u03b1 n times) to distinguish \u03bd\u221e from \u03bdk for any k \u2264 t. Since there are infinitely many \u03bdk, the agent needs to do this infinitely often. Moreover, V \u2217\u03bd1 \u2265 \u03b5 and V \u2217 \u03bd\u221e = \u03b5/2, so if \u03bdt is the true environment, then not exploring to the right for an \u03b5-effective horizon is suboptimal by \u03b5/2. But if \u03bd\u221e is the true environment, then exploring incurs an opportunity cost of one reward of \u03b5/2. 3\n\u00a75.4 Asymptotic Optimality 81\nNext, we state two negative results about asymptotic optimality proved by Lattimore and Hutter (2011). It is important to emphasize that Theorem 5.20 and Theorem 5.21 only hold for deterministic policies.\nTheorem 5.20 (Deterministic Policies are not Strongly Asymptotically Optimal; Lattimore and Hutter, 2011, Thm. 8). There is no deterministic policy that is strong asymptotically optimal in the classMCCMcomp.\nIf the horizon grows linearly (for example, power discounting \u03b3(t) = t\u2212\u03b2 with \u03b2 > 1; see Table 4.1), then a deterministic policy cannot be weakly asymptotically optimal policy: the agent has to explore for an entire effective horizon, which prevents the Ces\u00e0ro average from converging.\nTheorem 5.21 (Necessary Condition for Weak Asymptotic Optimality; Lattimore, 2013, Thm. 5.5). If there is an \u03b5 > 0 such that Ht(\u03b5) /\u2208 o(t), then there is no deterministic policy that is weakly asymptotically optimal in the classMCCMcomp.\nThere are several agents that achieve asymptotic optimality. In the rest of this section, we discuss the Bayes agent, BayesExp, and Thompson sampling. Asymptotic optimality can also be achieved through optimism (Sunehag and Hutter, 2012a,b, 2015)."}, {"heading": "5.4.1 Bayes", "text": "In this section, we list two results from the literature regarding the asymptotic optimality of the Bayes optimal policy. The following negative result is due to Orseau (2010, 2013).\nTheorem 5.22 (Bayes is not Asymptotically Optimal in General Environments; Orseau, 2013, Thm. 4). For any classM\u2287MCCMcomp no Bayes optimal policy \u03c0\u2217\u03be is asymptotically optimal: there is an environment \u00b5 \u2208 M and a time step t0 \u2208 N such that \u00b5\u03c0 \u2217 \u03be -almost surely for all time steps t \u2265 t0\nV \u2217\u00b5 (\u00e6<t)\u2212 V \u03c0\u2217\u03be \u00b5 (\u00e6<t) = 1\n2 .\nOrseau calls this result the good enough effect : A Bayesian agent eventually decides that the current strategy is good enough and that any additional exploration is not worth its expected payoff. However, if the environment changes afterwards, the Bayes agent is acting suboptimally.\nProof. Without loss of generality assume A := {\u03b1, \u03b2} and E := {0, 1/2, 1} (observations are vacuous). We consider the following environment \u00b5 (transitions are labeled with action, reward)."}, {"heading": "82 Optimality", "text": "s0 s1 . . . sn\n\u03b2, 12\n\u03b1, 0 \u2217, 0 \u2217, 0\n\u2217, 0\nIn state s0 the action \u03b2 is the exploitation action and the action \u03b1 the exploration action. The length of the state sequence is defined as an 1/t-effective horizon, n := Ht(1/t) where t is the time step in which the agent leaves state s0. Since the discount function \u03b3 is computable by Assumption 4.6a, \u00b5 \u2208MCCSLSC .\nAssume that when acting in \u00b5, the Bayes agent explores infinitely often. Let \u00e6<t be a history in which the agent is in state s0 and takes action \u03b1. Then V \u03c0\u2217\u03be \u00b5 \u2264 1/t. By on-policy value convergence (Corollary 4.20), V \u2217\u03be (\u00e6<t)\u2212 V \u03c0\u2217\u03be \u00b5 (\u00e6<t)\u2192 0 \u00b5\u03c0 \u2217 \u03be -almost surely. Hence there is a time step t0 such that for all t \u2265 t0 we have V \u2217\u03be < w(\u00b5)/2. Since \u00b5 is deterministic, w(\u00b5 | \u00e6<t) \u2265 w(\u00b5). Now we get a contradiction from (4.7):\nV \u2217\u03be (\u00e6<t) \u2265 w(\u00b5|\u00e6<t)V \u2217\u00b5 (\u00e6<t) \u2265 w(\u00b5)V \u2217\u00b5 (\u00e6<t) = w(\u00b5) 2 > V \u2217\u03be (\u00e6<t)\nTherefore the Bayes agent stops taking the exploration action \u03b1 after time step t0, and so it is not optimal in any \u03bd \u2208 MCCSLSC that behaves like \u00b5 until time step t0 and then changes:\ns0 s1 . . . sn\n\u03b2, 12\nt > t0 : \u03b1, 1 t \u2264 t0 : \u03b1, 0 \u2217, 0 \u2217, 0\n\u2217, 0\nThe following theorem is also known as the self-optimizing theorem. This theorem has been a source of great confusion because its statement in Hutter (2005, Thm. 5.34) is not very explicit about how the histories are generated. The formulation of Lattimore (2013, Thm. 5.2) is explicit, but less general.\nTheorem 5.23 (Sufficient Condition for Strong Asymptotic Optimality of Bayes; Hutter, 2005, Thm. 5.34). Let \u00b5 be some environment. If there is a policy \u03c0 and a sequence of policies \u03c01, \u03c02, . . . such that for all \u03bd \u2208M\nV \u2217\u03bd (\u00e6<t)\u2212 V \u03c0t\u03bd (\u00e6<t)\u2192 0 as t\u2192\u221e \u00b5\u03c0-almost surely, (5.4)\nthen V \u2217\u00b5 (\u00e6<t)\u2212 V \u03c0\u2217\u03be \u00b5 (\u00e6<t)\u2192 0 as t\u2192\u221e \u00b5\u03c0-almost surely.\n\u00a75.4 Asymptotic Optimality 83\nIf \u03c0 = \u03c0\u2217\u03be and (5.4) holds for all \u00b5 \u2208 M, then \u03c0\u2217\u03be is strongly asymptotically optimal in the classM.\nIt is important to emphasize that the policies \u03c01, \u03c02, . . . need to converge to the optimal value on the history generated by \u00b5 and \u03c0, and not (as one might think) \u03bd and \u03c0t. Intuitively, the policy \u03c0 is an \u2018exploration policy\u2019 that ensures that the environment class is explored sufficiently. Typically, a policy is asymptotically optimal on its own history. So if \u03c0 = \u03c01 = \u03c02 = . . ., then we get that Bayes is asymptotically optimal on the history generated by the policy \u03c0, not its own history. In light of Theorem 5.5 and Theorem 5.22 this is not too surprising; Bayesian reinforcement learning agents might not explore enough to be asymptotically optimal, but given a policy that does explore enough, Bayes learns enough to be asymptotically optimal.\nThis invites us to define the following policies \u03c0t: follow the information-seeking policy \u03c0\u2217IG until time step t, and then follow \u03c0 \u2217 \u03be (explore until t, then exploit). Since the information-seeking policy explores enough to prove off-policy prediction (Orseau et al., 2013, Thm. 7), we get V \u03c0\u03be \u2212V \u03c0\u00b5 \u2192 0 for every policy \u03c0 uniformly. Hence arg max\u03c0 V \u03c0\u03be \u2192 arg max\u03c0 V \u03c0 \u00b5 and thus V \u2217\u00b5 \u2212 V \u03c0\u2217\u03be \u00b5 \u2192 0 and (5.4) is satisfied. From Theorem 5.23 we get V \u2217\u00b5 \u2212 V \u03c0\u2217\u03be \u00b5 \u2192 0, which we already knew. In order to get strong asymptotic optimality, all we need to do is choose the switching time step t appropriately, i.e., wait until V \u2217\u00b5 and V \u03c0\u2217\u03be \u00b5 are close enough. Unfortunately, this is an invalid strategy: the agent does not know the true environment \u00b5 and hence cannot check this condition. Hutter (2005, Sec. 5.6) uses Theorem 5.23 to show that the Bayes optimal policy is strongly asymptotically optimal in the class of ergodic finite-state MDPs if the effective horizon is growing, i.e., Ht(\u03b5)\u2192\u221e for all \u03b5 > 0. This relies on the fact that in ergodic finite-state MDPs we need a fixed number of steps to explore the entire environment up to \u03b5-confidence. Therefore we can define a sequence of policies \u03c01, \u03c02, . . . that completely disregard the history and start exploring everything from scratch. Since the effective horizon is growing, this exploration phase takes a vanishing fraction of effective horizon and most of the value is retained. Therefore the sequence of policies \u03c01, \u03c02, . . . satisfies the condition of Theorem 5.23 regardless of the history, thus in particular for the history generated by \u03c0 = \u03c0\u2217\u03be and any \u00b5 \u2208 M. Note that the condition on the horizon is important: If the effective horizon is bounded, then Bayes is not asymptotically optimal in the class of ergodic finite-state MDPs because it can be locked into a dogmatic prior similarly to Theorem 5.5.\nProof of Theorem 5.23. From (4.6) we get for any history \u00e6<t\nw(\u00b5 | \u00e6<t) ( V \u2217\u00b5 (\u00e6<t)\u2212 V \u03c0\u2217\u03be \u00b5 (\u00e6<t) ) \u2264 \u2211 \u03bd\u2208M w(\u03bd | \u00e6<t) ( V \u2217\u03bd (\u00e6<t)\u2212 V \u03c0\u2217\u03be \u03bd (\u00e6<t) ) =\n(\u2211 \u03bd\u2208M w(\u03bd | \u00e6<t)V \u2217\u03bd (\u00e6<t) ) \u2212 V \u03c0\u2217\u03be \u03be (\u00e6<t)\n\u2264 \u2211 \u03bd\u2208M w(\u03bd | \u00e6<t)V \u2217\u03bd (\u00e6<t)\u2212 V \u03c0t \u03be (\u00e6<t)\n84 Optimality\n= \u2211 \u03bd\u2208M w(\u03bd | \u00e6<t) ( V \u2217\u03bd (\u00e6<t)\u2212 V \u03c0t\u03bd (\u00e6<t) ) . (5.5)\nFrom (5.4) follows that V \u2217\u03bd \u2212V \u03c0t\u03bd \u2192 0 \u00b5\u03c0-almost surely for all \u03bd \u2208M, so (5.5) converges to 0 \u00b5\u03c0-almost surely (Hutter, 2005, Lem. 5.28ii). Similar to Example 3.20, 1/w(\u00b5 | \u00e6<t) is a nonnegative \u00b5\u03c0-martingale and thus converges (to a finite value) \u00b5\u03c0-almost surely by Theorem 2.8. Therefore V \u2217\u00b5 (\u00e6<t) \u2212 V \u03c0\u2217\u03be \u00b5 (\u00e6<t) \u2192 0 \u00b5\u03c0-almost surely. If this is true for all \u00b5 \u2208 M, the strong asymptotic optimality of \u03c0\u2217\u03be follows from \u03c0 = \u03c0\u2217\u03be by definition."}, {"heading": "5.4.2 BayesExp", "text": "The definition of BayesExp is given in Section 4.3.3. In this subsection we state a result by Lattimore (2013) that motivated the definition of BayesExp.\nTheorem 5.24 (BayesExp is Weakly Asymptotically Optimal; Lattimore, 2013, Thm. 5.6). Let \u03c0BE denote the policy from Algorithm 1. If Ht(\u03b5) grows monotone in t and Ht(\u03b5t)/\u03b5t \u2208 o(t), then for all environments \u00b5 \u2208M\n1\nt t\u2211 k=1 ( V \u2217\u00b5 (\u00e6<k)\u2212 V \u03c0BE\u00b5 (\u00e6<k) ) \u2192 0 as t\u2192\u221e \u00b5\u03c0BE -almost surely.\nIf the horizon grows sublinearly (Ht(\u03b5) \u2208 o(t) for all \u03b5 > 0), then we can always find a sequence \u03b5t \u2192 0 that decreases slowly enough such that Ht(\u03b5)/\u03b5t \u2208 o(t) holds."}, {"heading": "5.4.3 Thompson Sampling", "text": "In this section we prove that the Thompson sampling policy defined in Section 4.3.4 is asymptotically optimal. Ortega and Braun (2010) prove that the action probabilities of Thompson sampling converge to the action probability of the optimal policy almost surely, but require a finite environment class and two (arguably quite strong) technical assumptions on the behavior of the posterior distribution (akin to ergodicity) and the similarity of environments in the class. Our convergence results do not require these assumptions.\nTheorem 5.25 (Thompson Sampling is Asymptotically Optimal in Mean). For all environments \u00b5 \u2208M,\nE\u03c0T\u00b5 [ V \u2217\u00b5 (\u00e6<t)\u2212 V \u03c0T\u00b5 (\u00e6<t) ] \u2192 0 as t\u2192\u221e.\nThis theorem immediately implies that Thompson sampling is also asymptotically optimal in probability according to Figure 5.2. However, this does not imply almost sure convergence (see Example 5.28).\nWe first give an intuition for the asymptotic optimality of Thompson sampling. At every resampling step we can split the classM into three partitions:\n1. Environments \u03c1 where V \u03c0\u2217\u03c1 \u00b5 \u2248 V \u2217\u00b5\n\u00a75.4 Asymptotic Optimality 85\n2. Environments \u03c1 where V \u2217\u03c1 > V \u2217\u00b5\n3. Environments \u03c1 where V \u2217\u03c1 < V \u2217\u00b5\nThe first class is the class of \u2018good\u2019 environments: if we draw one of them, we follow a policy that is close to optimal in \u00b5. The second class is the class of environments that overestimate the value of \u00b5. Following their optimal policy the agent gains information because rewards will be lower than expected. The third class is the class of environments that underestimate the value of \u00b5. Following their optimal policy the agent might not gain information since \u00b5 might behave just like environment \u03c1 on the \u03c1-optimal policy. However, when sampling from the first class instead, the agent gains information about the third class because rewards tend to be better than environments from the third class predicted.\nSince the true environment \u00b5 \u2208M, the first class is not empty, and the probability of drawing a sample from the first class does not become too small. Whenever the second and third class have sufficiently high weight in the posterior, there is a good chance of picking a policy that leads the agent to gain information. Asymptotically, the posterior converges, so the agent ends up having learned everything it could, i.e., the posterior weight of the second and third class vanishes.\nThis argument is not too hard to formalize for deterministic environment classes. However, for stochastic environment classes the effect on the posterior when following a bad policy is harder to quantify because there is always a chance that the rewards are different simply because of bad luck. In order to prove this theorem in its generality for stochastic classes, we employ an entirely different proof strategy that relies on statistical tools rather than the argument given above.\nDefinition 5.26 (Expected Total Variation Distance). Let \u03c0 be any policy and let m \u2208 N \u222a\u221e. The expected total variation distance on the policy \u03c0 is\nF \u03c0m(\u00e6<t) := \u2211 \u03c1\u2208M w(\u03c1 | \u00e6<t)Dm(\u03c1\u03c0, \u03be\u03c0 | \u00e6<t).\nIf we replace the distance measure Dm by cross-entropy, then the quantity F \u03c0m(\u00e6<t) becomes the expected information gain (see Section 4.3.2).\nFor the proof of Theorem 5.25 we need the following lemma.\nLemma 5.27 (Expected Total Variation Distance Vanishes On-Policy). For any policy \u03c0 and any environment \u00b5,\nE\u03c0\u00b5[F \u03c0\u221e(\u00e6<t)]\u2192 0 as t\u2192\u221e.\nProof. From Theorem 3.25 we get D\u221e(\u00b5\u03c0, \u03be\u03c0 | \u00e6<t)\u2192 0 \u00b5\u03c0-almost surely, and since D is bounded, this convergence also occurs in mean. Thus for every environment \u03bd \u2208M,\nE\u03c0\u03bd [ D\u221e(\u03bd \u03c0, \u03be\u03c0 | \u00e6<t) ] \u2192 0 as t\u2192\u221e."}, {"heading": "86 Optimality", "text": "Now\nE\u03c0\u00b5[F \u03c0\u221e(\u00e6<t)] \u2264 1\nw(\u00b5) E\u03c0\u03be [F \u03c0\u221e(\u00e6<t)]\n= 1\nw(\u00b5) E\u03c0\u03be [\u2211 \u03bd\u2208M w(\u03bd | \u00e6<t)D\u221e(\u03bd\u03c0, \u03be\u03c0 | \u00e6<t) ]\n= 1\nw(\u00b5) E\u03c0\u03be [\u2211 \u03bd\u2208M w(\u03bd) \u03bd\u03c0(\u00e6<t) \u03be\u03c0(\u00e6<t) D\u221e(\u03bd \u03c0, \u03be\u03c0 | \u00e6<t) ]\n= 1\nw(\u00b5) \u2211 \u03bd\u2208M w(\u03bd)E\u03c0\u03bd [ D\u221e(\u03bd \u03c0, \u03be\u03c0 | \u00e6<t) ] \u2192 0\nby Hutter (2005, Lem. 5.28ii) since total variation distance is bounded.\nProof of Theorem 5.25. Let \u03b2, \u03b4 > 0 and let \u03b5t > 0 denote the sequence used to define \u03c0T in Algorithm 2. We assume that t is large enough such that \u03b5k \u2264 \u03b2 for all k \u2265 t and that \u03b4 is small enough such that w(\u00b5 | \u00e6<t) > 4\u03b4 for all t, which holds since w(\u00b5 | \u00e6<t) 6\u2192 0 \u00b5\u03c0-almost surely for any policy \u03c0 (Hutter, 2009a, Lem. 3i).\nThe stochastic process w(\u03bd | \u00e6<t) is a \u03be\u03c0T -martingale according to Example 3.20. By the martingale convergence theorem (Theorem 2.8) w(\u03bd | \u00e6<t) converges \u03be\u03c0T -almost surely and because \u03be\u03c0T \u2265 w(\u00b5)\u00b5\u03c0T it also converges \u00b5\u03c0T -almost surely.\nWe argue that we can choose t0 to be one of \u03c0T \u2019s resampling time steps large enough such that for all t \u2265 t0 the following three events hold simultaneously with \u00b5\u03c0T -probability at least 1\u2212 \u03b4.\n(i) There is a finite setM\u2032 \u2282 M with w(M\u2032 | \u00e6<t) > 1 \u2212 \u03b4 and w(\u03bd | \u00e6<k) 6\u2192 0 as k \u2192\u221e for all \u03bd \u2208M\u2032.\n(ii) |w(M\u2032\u2032 | \u00e6<t)\u2212 w(M\u2032\u2032 | \u00e6<t0)| \u2264 \u03b4 for allM\u2032\u2032 \u2286M\u2032.\n(iii) F \u03c0T\u221e (\u00e6<t) < \u03b4\u03b2w2min.\nwhere wmin := inf{w(\u03bd | \u00e6<k) | k \u2208 N, \u03bd \u2208M\u2032}, which is positive by (i). (i) and (ii) are satisfied eventually because the posterior w( \u00b7 | \u00e6<t) converges \u00b5\u03c0T - almost surely. Note that the set M\u2032 is random: the limit of w(\u03bd | \u00e6<t) as t \u2192 \u221e depends on the history \u00e61:\u221e. Without loss of generality, we assume the true environment \u00b5 is contained inM\u2032 since w(\u00b5 | \u00e6<t) 6\u2192 0 \u00b5\u03c0T -almost surely. (iii) follows from Lemma 5.27 since convergence in mean implies convergence in probability.\nMoreover, we define the horizon m := t + Ht(\u03b5t) as the time step of the effective horizon at time step t. Let \u00e6<t be a fixed history for which (i-iii) is satisfied. Then we have\n\u03b4\u03b2w2min > F \u03c0T \u221e (\u00e6<t) = \u2211 \u03bd\u2208M w(\u03bd | \u00e6<t)D\u221e(\u03bd\u03c0T , \u03be\u03c0T | \u00e6<t)\n= E\u03bd\u223cw( \u00b7|\u00e6<t) [D\u221e(\u03bd \u03c0T , \u03be\u03c0T | \u00e6<t)]\n\u00a75.4 Asymptotic Optimality 87\n\u2265 E\u03bd\u223cw( \u00b7|\u00e6<t) [Dm(\u03bd \u03c0T , \u03be\u03c0T | \u00e6<t)] \u2265 \u03b2w2minw(M\\M\u2032\u2032 | \u00e6<t)\nby Markov\u2019s inequality where\nM\u2032\u2032 := { \u03bd \u2208M \u2223\u2223 Dm(\u03bd\u03c0T , \u03be\u03c0T | \u00e6<t) < \u03b2w2min} . For our fixed history \u00e6<t we have\n1\u2212 \u03b4 < w(M\u2032\u2032 | \u00e6<t) (i)\n\u2264 w(M\u2032\u2032 \u2229M\u2032 | \u00e6<t) + \u03b4 (ii) \u2264 w(M\u2032\u2032 \u2229M\u2032 | \u00e6<t0) + 2\u03b4 (i) \u2264 w(M\u2032\u2032 | \u00e6<t0) + 3\u03b4\nand thus we get\n1\u2212 4\u03b4 < w ( {\u03bd \u2208M | Dm(\u03bd\u03c0T , \u03be\u03c0T | \u00e6<t) < \u03b2w2min} \u2223\u2223 \u00e6<t0) . (5.6) In particular, this bound holds for \u03bd = \u00b5 since w(\u00b5 | \u00e6<t0) > 4\u03b4 by assumption.\nIt remains to show that with high probability the value V \u03c0\u2217\u03c1 \u00b5 of the sample \u03c1\u2019s optimal policy \u03c0\u2217\u03c1 is sufficiently close to the \u00b5-optimal value V \u2217\u00b5 . The worst case is that we draw the worst sample from M\u2032 \u2229 M\u2032\u2032 twice in a row. From now on, let \u03c1 denote the sample environment we draw at time step t0, and let t denote some time step between t0 and t1 := t0 +Ht0(\u03b5t0) (before the next resampling). With probability w(\u03bd \u2032 | \u00e6<t0)w(\u03bd \u2032 | \u00e6<t1) we sample \u03bd \u2032 both at t0 and t1 when following \u03c0T . Therefore we have for all \u00e6t:m and all \u03bd \u2208M\n\u03bd\u03c0T (\u00e61:m | \u00e6<t) \u2265 w(\u03bd \u2032 | \u00e6<t0)w(\u03bd \u2032 | \u00e6<t1)\u03bd \u03c0\u2217 \u03bd\u2032 (\u00e61:m | \u00e6<t).\nThus we get for all \u03bd \u2208M\u2032 (in particular \u03c1 and \u00b5)\nDm(\u00b5 \u03c0T , \u03c1\u03c0T | \u00e6<t) \u2265 sup\n\u03bd\u2032\u2208M sup A\u2286(A\u00d7E)m \u2223\u2223\u2223w(\u03bd \u2032 | \u00e6<t0)w(\u03bd \u2032 | \u00e6<t1) (\u00b5\u03c0 \u2217 \u03bd\u2032 (A | \u00e6<t)\u2212 \u03c1\u03c0 \u2217 \u03bd\u2032 (A | \u00e6<t))\n\u2223\u2223\u2223 \u2265 w(\u03bd | \u00e6<t0)w(\u03bd | \u00e6<t1) sup\nA\u2286(A\u00d7E)m \u2223\u2223\u2223\u00b5\u03c0\u2217\u03bd (A | \u00e6<t)\u2212 \u03c1\u03c0\u2217\u03bd (A | \u00e6<t)\u2223\u2223\u2223 \u2265 w2minDm(\u00b5\u03c0 \u2217 \u03bd , \u03c1\u03c0 \u2217 \u03bd | \u00e6<t).\nFor \u03c1 \u2208M\u2032\u2032 we get with (5.6)\nDm(\u00b5 \u03c0T , \u03c1\u03c0T | \u00e6<t) \u2264 Dm(\u00b5\u03c0T , \u03be\u03c0T | \u00e6<t) +Dm(\u03c1\u03c0T , \u03be\u03c0T | \u00e6<t)\n< \u03b2w2min + \u03b2w 2 min = 2\u03b2w 2 min,"}, {"heading": "88 Optimality", "text": "which together with Lemma 4.17 and the fact that rewards in [0, 1] implies\u2223\u2223\u2223V \u03c0\u2217\u03bd\u00b5 (\u00e6<t)\u2212 V \u03c0\u2217\u03bd\u03c1 (\u00e6<t)\u2223\u2223\u2223 \u2264 \u0393t+Ht(\u03b5t)\u0393t + \u2223\u2223\u2223V \u03c0\u2217\u03bd ,m\u00b5 (\u00e6<t)\u2212 V \u03c0\u2217\u03bd ,m\u03c1 (\u00e6<t)\u2223\u2223\u2223\n\u2264 \u03b5t +Dm(\u00b5\u03c0 \u2217 \u03bd , \u03c1\u03c0 \u2217 \u03bd | \u00e6<t) \u2264 \u03b5t + 1w2minDm(\u00b5 \u03c0T , \u03c1\u03c0T | \u00e6<t)\n< \u03b2 + 2\u03b2 = 3\u03b2.\nHence we get (omitting history arguments \u00e6<t for simplicity)\nV \u2217\u00b5 = V \u03c0\u2217\u00b5 \u00b5 < V \u03c0\u2217\u00b5 \u03c1 + 3\u03b2 \u2264 V \u2217\u03c1 + 3\u03b2 = V \u03c0\u2217\u03c1 \u03c1 + 3\u03b2 < V \u03c0\u2217\u03c1 \u00b5 + 3\u03b2 + 3\u03b2 = V \u03c0\u2217\u03c1 \u00b5 + 6\u03b2. (5.7)\nWith \u00b5\u03c0T -probability at least 1\u2212 \u03b4 (i), (ii), and (iii) are true, with \u00b5\u03c0T -probability at least 1\u2212\u03b4 our sample \u03c1 happens to be inM\u2032 by (i), and with w( \u00b7 | \u00e6<t0)-probability at least 1\u22124\u03b4 the sample is inM\u2032\u2032 by (5.6). All of these events are true simultaneously with probability at least 1\u2212 (\u03b4 + \u03b4 + 4\u03b4) = 1\u2212 6\u03b4. Hence the bound (5.7) transfers for \u03c0T such that with \u00b5\u03c0T -probability \u2265 1\u2212 6\u03b4 we have\nV \u2217\u00b5 (\u00e6<t)\u2212 V \u03c0T\u00b5 (\u00e6<t) < 6\u03b2.\nTherefore \u00b5\u03c0T [V \u2217\u00b5 (\u00e6<t)\u2212V \u03c0T\u00b5 (\u00e6<t) \u2265 6\u03b2] < 6\u03b4 and with \u03b4 \u2192 0 we get that V \u2217\u00b5 (\u00e6<t)\u2212 V \u03c0T\u00b5 (\u00e6<t) \u2192 0 as t \u2192 \u221e in probability. The value function is bounded, thus it also converges in mean.\nThe following example shows that the Thompson sampling policy is not strongly asymptotically optimal. However, we expect that strong asymptotic optimality can be achieved with Thompson sampling by resampling at every time step (with strong assumptions on the discount function). However, for practical purposes resampling in every time step is very inefficient.\nExample 5.28 (Thompson Sampling is not Strongly Asymptotically Optimal). Define A := {\u03b1, \u03b2}, E := {0, 1/2, 1}, and assume geometric discounting (Example 4.5). Consider the following class of environmentsM := {\u03bd\u221e, \u03bd1, \u03bd2, . . .} (transitions are labeled with action, reward):\ns0\ns1\ns2\n\u03b2, 12\n\u03b1, 0\n\u03b2, 0\n\u03b1, 0\n\u2217, 0\ns0\ns1\ns2\ns3\ns4\n\u03b2, 12\nt < k : \u03b1, 0\n\u03b2, 0\n\u03b1, 0\n\u2217, 0\nt \u2265 k : \u03b1, 0\n\u03b1, 0 \u03b2, 0\n\u03b1, 1\n\u03b2, 0\n\u03bd\u221e \u03bdk\n\u00a75.4 Asymptotic Optimality 89\nEnvironment \u03bdk works just like environment \u03bd\u221e except that after time step k, the path to state s3 gets unlocked. The classM is a class of deterministic weakly communicating POMDPs (but as a POMDP \u03bdk has more than 5 states). The optimal policy in environment \u03bd\u221e is to always take action \u03b2, the optimal policy for environment \u03bdk is to take action \u03b2 for t < k and then take action \u03b2 in state s1 and action \u03b1 otherwise.\nSuppose the policy \u03c0T is acting in environment \u03bd\u221e. Since it is asymptotically optimal in the class M, it has to take actions \u03b1\u03b1 from s0 infinitely often: for t < k environment \u03bdk is indistinguishable from \u03bd\u221e, so the posterior for \u03bdk is larger or equal to the prior. Hence there is always a constant chance of sampling \u03bdk until taking actions \u03b1\u03b1, at which point all environments \u03bdk for k \u2264 t become falsified.\nIf the policy \u03c0T decides to explore and take the first action \u03b1, it will be in state s1. Let \u00e6<t denote the current history. Then the \u03bd\u221e-optimal action is \u03b2 and\nV \u2217\u03bd\u221e(\u00e6<t) = (1\u2212 \u03b3) ( 0 + \u03b3 1\n2 + \u03b32\n1 2 + . . .\n) = \u03b3\n2 .\nThe next action taken by \u03c0T is \u03b1 since any optimal policy for any sampled environment that takes action \u03b1 once, takes that action again (and we are following that policy for an \u03b5t-effective horizon). Hence\nV \u03c0T\u03bd\u221e (\u00e6<t) \u2264 (1\u2212 \u03b3) ( 0 + 0 + \u03b32 1\n2 + \u03b33\n1 2 + . . .\n) = \u03b32\n2 .\nTherefore V \u2217\u03bd\u221e \u2212 V \u03c0T \u03bd\u221e \u2265 (\u03b3 \u2212 \u03b3 2)/2 > 0. This happens infinitely often with probability one and thus we cannot get almost sure convergence. 3\nIf the Bayesian mixture \u03be is inside the classM (as it is the case for the classMCCSLSC ), then we can assign \u03be a prior probability that is arbitrarily close to 1. Since the posterior of \u03be is the same as the prior, Thompson sampling will act according to the Bayes optimal policy most of the time. This means the Bayes-value of Thompson sampling can be very good; formally, V \u2217\u03be ( )\u2212 V \u03c0T \u03be ( ) = \u03a5\u03be \u2212\u03a5\u03be(\u03c0T ) can be made arbitrarily small.\nIn contrast, the Bayes-value of Thompson sampling can also be very bad: Suppose you have a class of (n+ 1)-armed bandits indexed 1, . . . , n where bandit i gives reward 1\u2212 \u03b5 on arm 1, reward 1 on arm i+ 1, and reward 0 on all other arms. For geometric discounting and \u03b5 < (1\u2212 \u03b3)/(2\u2212 \u03b3), it is Bayes optimal to pull arm 1 while Thompson sampling will explore on average n/2 arms until it finds the optimal arm. The Bayesvalue of Thompson sampling is 1/(n\u2212\u03b3n\u22121) in contract to (1\u2212\u03b5) achieved by Bayes. For a horizon of n, the Bayes optimal policy suffers a regret of \u03b5n and Thompson sampling a regret of n/2, which is much larger for small \u03b5."}, {"heading": "5.4.4 Almost Sure in Ces\u00e0ro Average vs. in Mean", "text": "It might appear that convergence in mean is more natural than the convergence of Ces\u00e0ro averages of weak asymptotic optimality. However, both notions are not so fundamentally different because they both allow an infinite number of bad mistakes (actions that lead to V \u2217\u00b5 \u2212 V \u03c0\u00b5 being large). Asymptotic optimality in mean allows bad"}, {"heading": "90 Optimality", "text": "mistakes as long as their probability converges to zero; weak asymptotic optimality allows bad mistakes as long as the total time spent on bad mistakes grows sublinearly. Note that according to Example 5.19 making bad mistakes infinitely often is necessary for asymptotic optimality.\nTheorem 5.24 shows that weak asymptotic optimality is possible in any countable class of stochastic environments. However, this requires the additional condition that the effective horizon grows sublinearly, Ht(\u03b5t) \u2208 o(t), while Theorem 5.25 does not require any condition on the discount function.\nGenerally, weak asymptotic optimality and asymptotic optimality in mean are incomparable because the notions of convergence are incomparable for (bounded) random variables. First, for deterministic sequences (i.e. deterministic policies in deterministic environments), convergence in mean is equivalent to (regular) convergence, which is impossible by Theorem 5.20. Second, convergence in probability (and hence convergence in mean for bounded random variables) does not imply almost sure convergence of Ces\u00e0ro averages (Stoyanov, 2013, Sec. 14.18). We leave open the question whether the policy \u03c0T is weakly asymptotically optimal."}, {"heading": "5.5 Regret", "text": "Regret is how many expected rewards the agent forfeits by not following the best informed policy.\nDefinition 5.29 (Regret). The regret of a policy \u03c0 in environment \u00b5 is\nRm(\u03c0, \u00b5) := sup \u03c0\u2032\nE\u03c0 \u2032 \u00b5 [ m\u2211 t=1 rt ] \u2212 E\u03c0\u00b5 [ m\u2211 t=1 rt ] .\nNote that regret is undiscounted and always nonnegative. Moreover, the space of possible different policies for the first m actions is finite and we assumed the set of actions A and the set of percepts E to be finite (Assumption 4.6c), so the supremum is always attained by some policy (not necessarily the \u00b5-optimal policy \u03c0\u2217\u00b5 because that policy uses discounting).\nDifferent problem classes have different regret rates, depending on the structure and the difficulty of the problem class. Multi-armed bandits provide a (problemindependent) worst-case regret bound of \u2126( \u221a km) where k is the number of arms (Bubeck and Bianchi, 2012). In MDPs the lower bound is \u2126( \u221a SAdm) where S is the number of states, A the number of actions, and d the diameter of the MDP (Auer et al., 2010). For a countable class of environments given by state representation functions that map histories to MDP states, a regret of O\u0303(m2/3) is achievable assuming the resulting MDP is weakly communicating (Nguyen et al., 2013).\nA problem class is considered learnable if there is an algorithm that has a sublinear regret guarantee. The following example shows that the general reinforcement learning problem is not learnable because the agent can get caught in a trap and be unable to recover.\n\u00a75.5 Regret 91\nExample 5.30 (Linear Regret; Hutter, 2005, Sec. 5.3.2). Consider the following two environments \u00b51 and \u00b52. In environment \u00b51 action \u03b1 leads to hell (reward 0 forever) and action \u03b2 leads to heaven (reward 1 forever). Environment \u00b52 behaves just the same, except that both actions are swapped.\nhell heaven\nreward = 0 reward = 1\n\u03b2\u03b1\nhell heaven\nreward = 0 reward = 1\n\u03b2 \u03b1\n\u00b51 \u00b52\nThe policy \u03b1 that takes action \u03b1 in the first time step performs well in \u00b52 but performs poorly in \u00b51. Likewise, the policy \u03b2 that takes action \u03b2 in the first time step performs well in \u00b51 but performs poorly in \u00b52. Regardless of which policy we adopt, our regret is always linear in one of the environments \u00b51 or \u00b52:\nRm(\u03b1, \u00b51) = m Rm(\u03b1, \u00b52) = 0\nRm(\u03b2, \u00b51) = 0 Rm(\u03b2, \u00b52) = m 3\nTo achieve sublinear regret we need to ensure that the agent can recover from mistakes. Formally, we make the following assumption.\nDefinition 5.31 (Recoverability). An environment \u00b5 satisfies the recoverability assumption iff\nsup \u03c0 \u2223\u2223\u2223E\u03c0\u2217\u00b5\u00b5 [V \u2217\u00b5 (\u00e6<t)]\u2212 E\u03c0\u00b5[V \u2217\u00b5 (\u00e6<t)]\u2223\u2223\u2223\u2192 0 as t\u2192\u221e. Recoverability compares following the worst policy \u03c0 for t\u2212 1 time steps and then switching to the optimal policy \u03c0\u2217\u03bd to having followed \u03c0\u2217\u03bd from the beginning. The recoverability assumption states that switching to the optimal policy at any time step enables the recovery of most of the value: it has to become less costly to recover from mistakes as time progresses. This should be regarded as an effect of the discount function: if the (effective) horizon grows, recovery becomes easier because the optimal policy has more time to perform a recovery. Moreover, recoverability is on the optimal policy, in contrast to the notion of ergodicity in MDPs which demands returning to a starting state regardless of the policy.\nRemark 5.32 (Weakly Communicating POMDPs are Recoverable). If the effective horizon is growing, Ht(\u03b5)\u2192\u221e as t\u2192\u221e, then any weakly communicating finite state POMDP satisfies the recoverability assumption. 3"}, {"heading": "5.5.1 Sublinear Regret in Recoverable Environments", "text": "This subsection is dedicated to the following theorem that connects asymptotic optimality in mean to sublinear regret."}, {"heading": "92 Optimality", "text": "Theorem 5.33 (Sublinear Regret in Recoverable Environments). If the discount function \u03b3 satisfies Assumption 5.34, the environment \u00b5 satisfies the recoverability assumption, and \u03c0 is asymptotically optimal in mean in the class {\u00b5}, then Rm(\u03c0, \u00b5) \u2208 o(m).\nAssumption 5.34 (Discount Function). Let the discount function \u03b3 be such that\n(a) \u03b3t > 0 for all t,\n(b) \u03b3t is monotone decreasing in t, and\n(c) Ht(\u03b5) \u2208 o(t) for all \u03b5 > 0.\nThis assumption demands that the discount function is somewhat well-behaved: the function has no oscillations, does not become 0, and the horizon is not growing too fast. It is satisfied by geometric discounting (Example 4.5): (a) \u03b3t > 0, (b) \u03b3 monotone decreasing, and (c) Ht(\u03b5) = dlog\u03b3 \u03b5e \u2208 o(t).\nThe problem with geometric discounting is that it makes the recoverability assumption very strong: since the horizon is not growing, the environment has to enable faster recovery as time progresses; in this case weakly communicating POMDPs are not recoverable. A choice with Ht(\u03b5) \u2192 \u221e that satisfies Assumption 5.34 is subgeometric discounting \u03b3t := e\u2212 \u221a t/ \u221a t (see Table 4.1).\nIf the items in Assumption 5.34 are violated, Theorem 5.33 can fail:\n\u2022 If \u03b3t = 0 for some time steps t, our policy does not care about those time steps and might take actions that have large regret.\n\u2022 Similarly if \u03b3 oscillates between high values and very low values: our policy might take high-regret actions in time steps with comparatively lower \u03b3-weight.\n\u2022 If the horizon grows linearly, infinitely often our policy might spend some constant fraction of the current effective horizon exploring, which incurs a cost that is a constant fraction of the total regret so far.\nTo prove Theorem 5.33 we require the following technical lemma.\nLemma 5.35 (Value and Regret). Let \u03b5 > 0 and assume the discount function \u03b3 satisfies Assumption 5.34. Let (dt)t\u2208N be a sequence of numbers with |dt| \u2264 1 for all t. If there is a time step t0 with\n1\n\u0393t \u221e\u2211 k=t \u03b3kdk < \u03b5 \u2200t \u2265 t0 (5.8)\nthen m\u2211 t=1 dt \u2264 t0 + \u03b5(m\u2212 t0 + 1) + 1 + \u03b5 1\u2212 \u03b5 Hm(\u03b5).\nProof. This proof essentially follows the proof of Hutter (2006b, Thm. 17).\n\u00a75.5 Regret 93\nBy Assumption 5.34a we have \u03b3t > 0 for all t and hence \u0393t > 0 for all t. By Assumption 5.34b have that \u03b3 is monotone decreasing, so we get for all n \u2208 N\n\u0393t = \u221e\u2211 k=t \u03b3k \u2264 t+n\u22121\u2211 k=t \u03b3t + \u221e\u2211 k=t+n \u03b3k = n\u03b3t + \u0393t+n.\nAnd with n := Ht(\u03b5) this yields\n\u03b3tHt(\u03b5)\n\u0393t \u2265 1\u2212\n\u0393t+Ht(\u03b5)\n\u0393t \u2265 1\u2212 \u03b5 > 0. (5.9)\nIn particular, this bound holds for all t and \u03b5 > 0.\nNext, we define a series of nonnegative weights (bt)t\u22651 such that\nm\u2211 t=t0 dk = m\u2211 t=t0 bt \u0393t m\u2211 k=t \u03b3kdk.\nThis yields the constraints t\u2211\nk=t0\nbk \u0393k \u03b3t = 1 \u2200t \u2265 t0.\nThe solution to these constraints is\nbt0 = \u0393t0 \u03b3t0 , and bt = \u0393t \u03b3t \u2212 \u0393t \u03b3t\u22121 for t > t0. (5.10)\nThus we get\nm\u2211 t=t0 bt = \u0393t0 \u03b3t0 + m\u2211 t=t0+1 ( \u0393t \u03b3t \u2212 \u0393t \u03b3t\u22121 )\n= \u0393m+1 \u03b3m + m\u2211 t=t0 ( \u0393t \u03b3t \u2212 \u0393t+1 \u03b3t ) =\n\u0393m+1 \u03b3m +m\u2212 t0 + 1\n\u2264 Hm(\u03b5) 1\u2212 \u03b5 +m\u2212 t0 + 1\nfor all \u03b5 > 0 according to (5.9).\nFinally,\nm\u2211 t=1 dt \u2264 t0\u2211 t=1 dt + m\u2211 t=t0 bt \u0393t m\u2211 k=t \u03b3kdk\n\u2264 t0 + m\u2211 t=t0 bt \u0393t \u221e\u2211 k=t \u03b3kdk \u2212 m\u2211 t=t0 bt \u0393t \u221e\u2211 k=m+1 \u03b3kdk"}, {"heading": "94 Optimality", "text": "and using the assumption (5.8) and dt \u2265 \u22121,\n< t0 + m\u2211 t=t0 bt\u03b5+ m\u2211 t=t0 bt\u0393m+1 \u0393t\n\u2264 t0 + \u03b5Hm(\u03b5)\n1\u2212 \u03b5 + \u03b5(m\u2212 t0 + 1) + m\u2211 t=t0 bt\u0393m+1 \u0393t\nFor the latter term we substitute (5.10) to get\nm\u2211 t=t0 bt\u0393m+1 \u0393t = \u0393m+1 \u03b3t0 + m\u2211 t=t0+1 ( \u0393m+1 \u03b3t \u2212 \u0393m+1 \u03b3t\u22121 ) = \u0393m+1 \u03b3m \u2264 Hm(\u03b5) 1\u2212 \u03b5\nwith (5.9).\nProof of Theorem 5.33. Let (\u03c0m)m\u2208N denote any sequence of policies, such as a sequence of policies that attain the supremum in the definition of regret. We want to show that\nE\u03c0m\u00b5 [ m\u2211 t=1 rt ] \u2212 E\u03c0\u00b5 [ m\u2211 t=1 rt ] \u2208 o(m).\nFor d\n(m) k := E \u03c0m \u00b5 [rk]\u2212 E\u03c0\u00b5[rk] (5.11)\nwe have \u22121 \u2264 d(m)k \u2264 1 since we assumed rewards to be bounded between 0 and 1. Because the environment \u00b5 satisfies the recoverability assumption we have\u2223\u2223\u2223E\u03c0\u2217\u00b5\u00b5 [V \u2217\u00b5 (\u00e6<t)]\u2212 E\u03c0\u00b5[V \u2217\u00b5 (\u00e6<t)]\u2223\u2223\u2223\u2192 0 as t\u2192\u221e, and\nsup m \u2223\u2223\u2223E\u03c0\u2217\u00b5\u00b5 [V \u2217\u00b5 (\u00e6<t)]\u2212 E\u03c0m\u00b5 [V \u2217\u00b5 (\u00e6<t)]\u2223\u2223\u2223\u2192 0 as t\u2192\u221e, so we conclude that\nsup m \u2223\u2223E\u03c0\u00b5[V \u2217\u00b5 (\u00e6<t)]\u2212 E\u03c0m\u00b5 [V \u2217\u00b5 (\u00e6<t)]\u2223\u2223\u2192 0 by the triangle inequality and thus\nsup m\nE\u03c0m\u00b5 [V \u2217\u00b5 (\u00e6<t)]\u2212 E\u03c0\u00b5[V \u2217\u00b5 (\u00e6<t)]\u2192 0 as t\u2192\u221e. (5.12)\nBy assumption the policy \u03c0 is asymptotically optimal in mean, so we have\nE\u03c0\u00b5[V \u2217\u00b5 (\u00e6<t)]\u2212 E\u03c0\u00b5[V \u03c0\u00b5 (\u00e6<t)]\u2192 0 as t\u2192\u221e,\nand with (5.12) this combines to\nsup m\nE\u03c0m\u00b5 [V \u2217\u00b5 (\u00e6<t)]\u2212 E\u03c0\u00b5[V \u03c0\u00b5 (\u00e6<t)]\u2192 0 as t\u2192\u221e.\n\u00a75.5 Regret 95\nFrom V \u2217\u00b5 (\u00e6<t) \u2265 V \u03c0m\u00b5 (\u00e6<t) we get\nlim sup t\u2192\u221e ( sup m E\u03c0m\u00b5 [V \u03c0m\u00b5 (\u00e6<t)]\u2212 E\u03c0\u00b5[V \u03c0\u00b5 (\u00e6<t)] ) \u2264 0. (5.13)\nFor \u03c0\u2032 \u2208 {\u03c0, \u03c01, \u03c02, . . .} we have\nE\u03c0 \u2032 \u00b5 [V \u03c0\u2032 \u00b5 (\u00e6<t)] = E\u03c0 \u2032 \u00b5\n[ 1\n\u0393t E\u03c0 \u2032 \u00b5 [ \u221e\u2211 k=t \u03b3krk \u2223\u2223\u2223\u2223\u2223\u00e6<t ]] = E\u03c0 \u2032 \u00b5 [ 1 \u0393t \u221e\u2211 k=t \u03b3krk ] = 1 \u0393t \u221e\u2211 k=t \u03b3kE\u03c0 \u2032 \u00b5 [rk],\nso from (5.11) and (5.13) we get\nlim sup t\u2192\u221e sup m\n1\n\u0393t \u221e\u2211 k=t \u03b3kd (m) k \u2264 0.\nLet \u03b5 > 0. We choose t0 independent of m and large enough such that we get supm \u2211\u221e k=t \u03b3kd (m) k /\u0393t < \u03b5 for all t \u2265 t0. Now we let m \u2208 N be given and apply Lemma 5.35 to get\nRm(\u03c0, \u00b5)\nm =\n\u2211m k=1 d (m) k\nm \u2264 t0 + \u03b5(m\u2212 t0 + 1) + 1+\u03b51\u2212\u03b5Hm(\u03b5) m .\nSince Ht(\u03b5) \u2208 o(t) according to Assumption 5.34c we get lim supm\u2192\u221eRm(\u03c0, \u00b5)/m \u2264 0.\nExample 5.36 (The Converse of Theorem 5.33 is False). Let \u00b5 be a two-armed Bernoulli bandit with means 0 and 1 and suppose we are using geometric discounting with discount factor \u03b3 \u2208 [0, 1). This environment is recoverable. If our policy \u03c0 pulls the suboptimal arm exactly on time steps 1, 2, 4, 8, 16, . . ., regret will be logarithmic. However, on time steps t = 2n for n \u2208 N the value difference V \u2217\u00b5 \u2212 V \u03c0\u00b5 is deterministically at least 1\u2212 \u03b3 > 0. 3\nNote that Example 5.36 does not rule out weak asymptotic optimality."}, {"heading": "5.5.2 Regret of the Optimal Policy and Thompson sampling", "text": "We get the following immediate consequence.\nCorollary 5.37 (Sublinear Regret for the Optimal Discounted Policy). If the discount function \u03b3 satisfies Assumption 5.34 and the environment \u00b5 satisfies the recoverability assumption, then Rm(\u03c0\u2217\u00b5, \u00b5) \u2208 o(m).\nProof. From Theorem 5.33 since the policy \u03c0\u2217\u00b5 is (trivially) asymptotically optimal in {\u00b5}.\nIf the environment does not satisfy the recoverability assumption, regret may be linear even on the optimal policy : the optimal policy maximizes discounted rewards"}, {"heading": "96 Optimality", "text": "and this short-sightedness might incur a tradeoff that leads to linear regret later on if the environment does not allow recovery.\nCorollary 5.38 (Sublinear Regret for Thompson Sampling). If the discount function \u03b3 satisfies Assumption 5.34 and the environment \u00b5 \u2208 M satisfies the recoverability assumption, then Rm(\u03c0T , \u00b5) \u2208 o(m) for the Thompson sampling policy \u03c0T .\nProof. From Theorem 5.25 and Theorem 5.33."}, {"heading": "5.6 Discussion", "text": "In this work, we disregard computational constraints. Because of this, our agents learn very efficiently and we can focus on the way they balance exploration and exploitation. So which balance is best?"}, {"heading": "5.6.1 The Optimality of AIXI", "text": "Bayesian reinforcement learning agents make the tradeoff between exploration and exploitation in the Bayes optimal way. Maximizing expected rewards according to any positive prior does not lead to enough exploration to achieve asymptotic optimality (Theorem 5.22); the prior\u2019s bias is retained indefinitely. For bad priors this can cause serious malfunctions: the dogmatic prior defined in Section 5.2.2 can prevent a Bayesian agent from taking a single exploratory action; exploration is restricted to cases where the expected future payoff falls below some prespecified \u03b5 > 0. However, this problem can be alleviated by adding an extra exploration component to AIXI: Lattimore (2013) shows that BayesExp is weakly asymptotically optimal (Theorem 5.24).\nSo instead, we may ask the following weaker questions. Does AIXI succeed in every (ergodic) finite-state (PO)MDP, bandit problem, or sequence prediction task? Our results imply that without further assumptions on the prior, we cannot answer any of the preceding questions in the affirmative. Using a dogmatic prior (Theorem 5.5), we can make AIXI follow any computable policy as long as that policy produces rewards that are bounded away from zero.\n\u2022 In a sequence prediction task that gives a reward of 1 for every correctly predicted bit and 0 otherwise, a policy \u03c0 that correctly predicts every third bit will receive an average reward of 1/3. With a \u03c0-dogmatic prior, AIXI thus only predicts a third of the bits correctly, and hence is outperformed by a uniformly random predictor.\nHowever, if we have a constant horizon of length 1, AIXI does succeed in sequence prediction (Hutter, 2005, Sec. 6.2.2). If the horizon is this short, the agent is so hedonistic that no threat of hell can deter it.\n\u2022 In a (PO)MDP a dogmatic prior can make AIXI get stuck in any loop that provides nonzero expected rewards.\n\u00a75.6 Discussion 97\n\u2022 In a bandit problem, a dogmatic prior can make AIXI get stuck on any arm which provides nonzero expected rewards.\nThese results apply not only to AIXI, but generally to Bayesian reinforcement learning agents. Any Bayesian mixture over nonrecoverable environments is susceptible to dogmatic priors if we allow an arbitrary reweighing of the prior. Notable exceptions are classes of environment that allow policies that are strongly asymptotically optimal regardless of the history (Theorem 5.23). For example, the class of all ergodic MDPs for an unbounded effective horizon; in this case the Bayes optimal policy is strongly asymptotically optimal (Hutter, 2005, Thm. 5.38). Note that in contrast to our results, this requires that that the agent uses a Bayes-mixture over a class of ergodic MDPs.\nMoreover, Bayesian agents still perform well at learning and achieve on-policy value convergence (Corollary 4.20): the posterior belief about the value of a policy \u03c0 converges to the true value of \u03c0 while following \u03c0: V \u03c0\u03be (\u00e6<t)\u2212 V \u03c0\u00b5 (\u00e6<t)\u2192 0 as t\u2192\u221e \u00b5\u03c0-almost surely. Since this holds for any policy, in particular it holds for the Bayes optimal policy \u03c0\u2217\u03be . This means that the Bayes agent learns to predict those parts of the environment that it sees. But if it does not explore enough, then it will not learn other parts of the environment that are potentially more rewarding.\nHutter (2005, Claim 5.12) claims:\nWe expect AIXI to be universally optimal.\nOur work seriously challenges Hutter\u2019s claim: no nontrivial and non-subjective optimality results for AIXI remain (see Table 5.3). Until new arguments for AIXI\u2019s optimality are put forward, we have to regard AIXI as a relative theory of intelligence, dependent on the choice of the prior."}, {"heading": "5.6.2 Natural Universal Turing Machines", "text": "The choice of the UTM has been a big open question in algorithmic information theory for a long time. The Kolmogorov complexity of a string depends on this choice. However, there are invariance theorems (Li and Vit\u00e1nyi, 2008, Thm. 2.1.1 & Thm. 3.1.1) which state that changing the UTM changes Kolmogorov complexity only by a constant. When using the Solomonoff prior M to predict any deterministic computable binary sequence, the number of wrong predictions is bounded by the Kolmogorov complexity of the sequence (Corollary 3.56). Due to the invariance theorem, changing the UTM changes the number of errors only by a constant. In this sense, compression and prediction work for any choice of UTM.\nFor AIXI, there can be no invariance theorem; in Section 5.2 we showed that a bad choice for the UTM can have drastic consequences. Our negative results can guide future search for a natural UTM: the UTMs used to define the indifference prior (Theorem 5.4), the dogmatic prior (Theorem 5.5), and the G\u00f6del prior (Theorem 5.8) should be considered unnatural. But what are other desirable properties of a UTM?\nA remarkable but unsuccessful attempt to find natural UTMs is due to M\u00fcller (2010). It takes the probability that one universal machine simulates another according"}, {"heading": "98 Optimality", "text": "to the length of their respective compilers and searches for a stationary distribution. Unfortunately, no stationary distribution exists.\nAlternatively, we could demand that the UTM U \u2032 that we use for the universal prior has a small compiler on the reference machine U (Hutter, 2005, p. 35). Moreover, we could demand the reverse, that the reference machine U has a small compiler on U \u2032. The idea is that this should limit the amount of bias one can introduce by defining a UTM that has very small programs for very complicated and \u2018unusual\u2019 environments. Unfortunately, this just pushes the choice of the UTM to the reference machine. Table 5.2 lists compiler sizes of the UTMs constructed in this thesis."}, {"heading": "5.6.3 Asymptotic Optimality", "text": "A policy is asymptotically optimal if the agent learns to act optimally in any environment from the classM. We discussed two asymptotically optimal policies. BayesExp is weakly asymptotically optimal if the horizon grows sublinearly (Theorem 5.24) and Thompson sampling is asymptotically optimal in mean (Theorem 5.25). Both policies commit to exploration for several steps. As stated in Example 5.19:\nTo achieve asymptotic optimality, the agent needs to explore infinitely often for an entire effective horizon.\nThis is why weak asymptotic optimality is impossible if the horizon grows linearly (Theorem 5.21): if the agent explores for an entire effective horizon, it spoils a significant fraction of the average. Thompson sampling explores whenever it draws a bad sample. BayesExp explores if the maximal expected information gain is above some threshold. Both policies commit to exploration for the entire effective horizon.\nThe exploration performed by Thompson sampling is qualitatively different from the exploration by BayesExp (Lattimore, 2013, Ch. 5). BayesExp performs phases of exploration in which it maximizes the expected information gain. This explores the environment class completely, even achieving off-policy prediction (Orseau et al., 2013, Thm. 7). In contrast, Thompson sampling only explores on the optimal policies, and in some environment classes this will not yield off-policy prediction. So in this sense the\nexploration mechanism of Thompson sampling is more reward-oriented than maximizing information gain.\nHowever, asymptotic optimality has to be taken with a grain of salt. It provides no incentive to the agent to avoid traps in the environment. Once the agent gets caught in a trap, all actions are equally bad and thus optimal: asymptotic optimality has been achieved. Even worse, an asymptotically optimal agent has to explore all the traps because they might contain hidden treasure. This brings us to the following impossibility result for non-recoverable environment classes.\nEither the agent gets caught in a trap or it is not asymptotically optimal.1"}, {"heading": "5.6.4 The Quest for Optimality", "text": "Theorem 5.3 shows that Pareto optimality is trivial in the class of all computable environments. Bayes optimality, Balanced Pareto optimality, and maximal Legg-Hutter intelligence are equivalent (Proposition 5.12 and Proposition 5.10). Corollary 5.15 and Corollary 5.16 show that this notion is highly subjective because it depends on the choice of the prior. Moreover, according to Corollary 5.17, any computable policy is nearly balanced Pareto optimal. For finite horizons, there are priors such that every policy is balanced Pareto optimal (Theorem 5.4). Sublinear regret is impossible in general environments (Example 5.30). However, if the environment is recoverable (Definition 5.31), then Theorem 5.33 shows that asymptotic optimality in mean implies sublinear regret. In summary, asymptotic optimality is the only nontrivial and objective notion of optimality for the general reinforcement learning problem (Problem 4.2):\n1This formulation was suggested by Toby Ord.\n100 Optimality\nit is both satisfiable (Theorem 5.24 and Theorem 5.25) and objective because it does not depend on a prior probability measure over the environment class M. Table 5.3 summarized the notions of optimality discussed in this chapter.\nOur optimality notions are tail events: any finite number of time steps are irrelevant; the agent can be arbitrarily lazy. Asymptotic optimality requires only convergence in the limit. In recoverable environments we can always achieve sublinear regret after any finite interaction. All policies with finite horizon are Bayes optimal according to Theorem 5.4 and Corollary 5.6. Overall, there is a dichotomy between the asymptotic nature of our optimality notions and the use of discounting to prioritize the present over the future. Ideally, we would aim for finite guarantees instead, such as precise regret bounds or PAC convergence rates, but without additional assumptions this is impossible in this general setting. This leaves us with the main question of this chapter unanswered (Hutter, 2009b, Sec. 5):\nWhat is a good optimality criterion for general reinforcement learning?\nChapter 6\nComputability\nI simply keep a few spare halting oracles around. \u2014 Marcus Hutter\nGiven infinite computation power, many traditional AI problems become trivial: playing chess, go, or backgammon can be solved by exhaustive expansion of the game tree. Yet other problems seem difficult still; for example, predicting the stock market, driving a car, or babysitting your nephew. How can we solve these problems in theory? Solomonoff induction and AIXI are proposed answers to this question.\nBoth Solomonoff induction and AIXI are known to be incomputable. But not all incomputabilities are equal. The arithmetical hierarchy specifies different levels of computability based on oracle machines: each level in the arithmetical hierarchy is computed by a Turing machine which may query a halting oracle for the respective lower level. Our agents are useless if they cannot be approximated in practice, i.e., by a regular Turing machine. Therefore we posit that any ideal for a \u2018perfect agent\u2019 needs to be limit computable (\u220602). The class of limit computable functions is the class of functions that admit an anytime algorithm.\nIn Section 6.2 we consider various different flavors of Solomonoff induction: Solomonoff\u2019s prior M (Example 3.5) is only a semimeasure and not a measure: it assigns positive probability that the observed string has only finite length. This can be circumvented by normalizing M . Solomonoff\u2019s normalization Mnorm (Definition 2.16) preserves the ratio M(x1)/M(x0) and is limit computable. If instead we mix only over programs that compute infinite strings, we get a semimeasure M (3.6), which can be normalized to Mnorm. Moreover, when predicting a sequence, we are primarily interested in the conditional probability M(xy | x) (respectively Mnorm(xy | y), M(xy | x), or Mnorm(xy | x)) that the currently observed string x is continued with y. We show that both M and Mnorm are limit computable, while M and Mnorm are not. Table 6.1 summarizes our computability results for Solomonoff induction.\nFor MDPs, planning is already P-complete for finite and infinite horizons (Papadimitriou and Tsitsiklis, 1987). In POMDPs, planning is undecidable (Madani et al., 1999, 2003). The existence of a policy whose expected value exceeds a given threshold is PSPACE-complete (Mundhenk et al., 2000), even for purely epistemic POMDPs in which actions do not change the hidden state (Sabbadin et al., 2007). In Section 6.3 we derive hardness results for planning in general semicomputable environments; this environment class is even more general than POMDPs. We show that optimal policies\n101\nare \u03a002-hard and \u03b5-optimal policies are undecidable. Moreover, we show that by default, AIXI is not limit computable. When picking the next action, two or more actions might have the same value (expected future rewards). The choice between them is easy, but determining whether such a tie exists is difficult. This problem can be circumvented by settling for an \u03b5-optimal policy; we get a limitcomputable agent with infinite horizon. However, these results rely on the recursive definition of the value function. In contrast, Hutter (2005) defines the value function as the limit of the iterative value function. In Section 6.4 we compare these two definitions and show that the recursive definition correctly maximizes expected rewards and has better computability properties.\nIn Section 6.5 we show that for finite horizons both the entropy-seeking and the information-seeking agent are \u220603-computable and have limit-computable \u03b5-optimal policies. BayesExp (Section 4.3.3) relies on optimal policies that are generally not\n\u00a76.1 Background on Computability 103\nlimit computable. In Section 6.6 we give a weakly asymptotically optimal agent based on BayesExp that is limit computable. Table 6.2 summarizes our results on the computability of these agents.\nIn this chapter we illustrate the environments used in the proofs of our theorems in the form of flowcharts. They should be read as follows. Circles denote stochastic nodes, rectangles denote environment nodes, and diamonds denote the agent\u2019s choice nodes. Transitions out of stochastic nodes are labeled with transition probabilities, transitions out of environment nodes are labeled with percepts, and transitions out of choice nodes are labeled with actions. The initial node is marked with a small incoming arrow (see for example Figure 6.3). By Assumption 4.6b the worst possible outcome is getting reward 0 forever, thus we label such states as hell . Analogously, getting reward 1 forever is the best possible outcome, thus we label such states as heaven."}, {"heading": "6.1 Background on Computability", "text": ""}, {"heading": "6.1.1 The Arithmetical Hierarchy", "text": "A set A \u2286 N is \u03a30n iff there is a quantifier-free formula \u03b7 such that\nk \u2208 A \u21d0\u21d2 \u2203k1\u2200k2 . . . Qnkn \u03b7(k, k1, . . . , kn) (6.1)\nwhere Qn = \u2200 if n is even, Qn = \u2203 if n is odd (Nies, 2009, Def. 1.4.10). (We can also think of \u03b7 as a computable relation.) A set A \u2286 N is \u03a00n iff its complement N \\ A is \u03a30n. The formula \u03b7 on the right side of (6.1) is a \u03a30n-formula and its negation is a \u03a00n-formula. It can be shown that we can add any bounded quantifiers and duplicate quantifiers of the same type without changing the classification of A. The set A is \u22060n iff A is \u03a30n and A is \u03a00n. We get that \u03a301 as the class of recursively enumerable sets, \u03a001 as the class of co-recursively enumerable sets and \u220601 as the class of recursive sets.\nThe set A \u2286 N is \u03a30n-hard (\u03a00n-hard, \u22060n-hard) iff for any set B \u2208 \u03a30n (B \u2208 \u03a00n, B \u2208 \u22060n), B is many-one reducible to A, i.e., there is a computable function f such that k \u2208 B \u2194 f(k) \u2208 A (Nies, 2009, Def. 1.2.1). We get \u03a30n \u2282 \u22060n+1 \u2282 \u03a30n+1 \u2282 . . . and \u03a00n \u2282 \u22060n+1 \u2282 \u03a00n+1 \u2282 . . .. This hierarchy of subsets of natural numbers is known as the arithmetical hierarchy .\nBy Post\u2019s Theorem (Nies, 2009, Thm. 1.4.13), a set is \u03a30n if and only if it is recursively enumerable on an oracle machine with an oracle for a \u03a30n\u22121-complete set. An oracle for \u03a301 is called a halting oracle."}, {"heading": "6.1.2 Computability of Real-valued Functions", "text": "We fix some encoding of rational numbers into binary strings and an encoding of binary strings into natural numbers. From now on, this encoding will be done implicitly wherever necessary.\nDefinition 6.1 (\u03a30n-, \u03a00n-, \u22060n-computable). A function f : X \u2217 \u2192 R is called \u03a30ncomputable (\u03a00n-computable, \u22060n-computable) iff the set {(x, q) \u2208 X \u2217 \u00d7Q | f(x) > q} is \u03a30n (\u03a00n, \u22060n).\nA \u220601-computable function is called computable, a \u03a301-computable function is called lower semicomputable, and a \u03a001-computable function is called upper semicomputable. A \u220602-computable function f is called limit computable, because there is a computable function \u03c6 such that\nlim k\u2192\u221e \u03c6(x, k) = f(x).\nThe program \u03c6 that limit computes f can be thought of as an anytime algorithm for f : we can stop \u03c6 at any time k and get a preliminary answer. If the program \u03c6 ran long enough (which we do not know), this preliminary answer will be close to the correct one.\nLimit-computable sets are the highest level in the arithmetical hierarchy that can be approached by a regular Turing machine. Above limit-computable sets we necessarily need some form of halting oracle. See Table 6.3 for the definition of lower/upper semicomputable and limit-computable functions in terms of the arithmetical hierarchy.\nLemma 6.2 (Computability of Arithmetical Operations). Let n > 0 and let f, g : X \u2217 \u2192 R be two \u22060n-computable functions. Then\n(a) {(x, y) | f(x) > g(y)} is \u03a30n,\n(b) {(x, y) | f(x) \u2264 g(y)} is \u03a00n,\n(c) f + g, f \u2212 g, and f \u00b7 g are \u22060n-computable, and\n(d) f/g is \u22060n-computable if g(x) 6= 0 for all x.\n(e) log f is \u22060n-computable if f(x) > 0 for all x.\nProof. We only prove this for n > 1. Since f, g are \u22060n-computable, they are limit computable on a level n\u22121 oracle machine. Let \u03c6 be the function limit computing f on the oracle machine, and let \u03c8 be the function limit computing g on the oracle machine:\nf(x) = lim k\u2192\u221e \u03c6(k, x) and g(y) = lim k\u2192\u221e \u03c8(k, y).\nBy assumption, both \u03c6 and \u03c8 are \u22060n\u22121-computable.\n\u00a76.2 The Complexity of Solomonoff Induction 105\n(a) Let G := {(x, y, q) | g(y) < q}, and let F := {(x, y, q) | q < f(x)}, both of which are in \u22060n by assumption. Hence there are \u03a30n-formulas \u03d5G and \u03d5F such that\n(x, y, q) \u2208 G \u21d0\u21d2 \u03d5G(x, y, q) (x, y, q) \u2208 F \u21d0\u21d2 \u03d5F (x, y, q)\nNow f(x) > g(y) if and only if \u2203q. (x, y, q) \u2208 G \u2229 F , which is equivalent to the \u03a30n-forumla\n\u2203q. \u03d5G(x, y, q) \u2227 \u03d5F (x, y, q).\n(b) Follows from (a).\n(c) Addition, subtraction, and multiplication are continuous operations.\n(d) Division is discontinuous only at g(x) = 0. We show this explicitly. By assumption, for any \u03b5 > 0 there is a k0 such that for all k > k0\n|\u03c6(x, k)\u2212 f(x)| < \u03b5 and |\u03c8(x, k)\u2212 g(x)| < \u03b5.\nWe assume without loss of generality that \u03b5 < |g(x)|, since g(x) 6= 0 by assumption.\u2223\u2223\u2223\u2223\u03c6(x, k)\u03c8(x, k) \u2212 f(x)g(x) \u2223\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223\u03c6(x, k)g(x)\u2212 f(x)\u03c8(x, k)\u03c8(x, k)g(x) \u2223\u2223\u2223\u2223 \u2264 |\u03c6(x, k)g(x)\u2212 f(x)g(x)|+ |f(x)g(x)\u2212 f(x)\u03c8(x, k)| |\u03c8(x, k)g(x)| < \u03b5|g(x)|+ |f(x)|\u03b5 |\u03c8(x, k)g(x)|\nwith |\u03c8(x, k)g(x)| = |\u03c8(x, k)| \u00b7 |g(x)| > (|g(x)| \u2212 \u03b5)|g(x)|,\n< \u03b5 \u00b7 |g(x)|+ |f(x)| (|g(x)| \u2212 \u03b5)|g(x)| \u03b5\u21920\u2212\u2212\u2212\u2192 0,\ntherefore f(x)/g(x) = limk\u2192\u221e \u03c6(x, k)/\u03c8(x, k).\n(e) Follows from the fact that the logarithm is computable."}, {"heading": "6.2 The Complexity of Solomonoff Induction", "text": "In this section, we derive the computability results for Solomonoff\u2019s prior as stated in Table 6.1.\nSince M is lower semicomputable, Mnorm is limit computable by Lemma 6.2 (c) and (d). When using the Solomonoff prior M (or one of its sisters Mnorm, M , or Mnorm defined in Definition 2.16 and Equation 3.6) for sequence prediction, we need\nto compute the conditional probability M(xy | x) = M(xy)/M(x) for finite strings x, y \u2208 X \u2217. Because M(x) > 0 for all finite strings x \u2208 X \u2217, this quotient is well-defined.\nTheorem 6.3 (Complexity of M , Mnorm, M , and Mnorm).\n(a) M(x) is lower semicomputable\n(b) M(xy | x) is limit computable\n(c) Mnorm(x) is limit computable\n(d) Mnorm(xy | x) is limit computable\n(e) M(x) is \u03a002-computable\n(f) M(xy | x) is \u220603-computable\n(g) Mnorm(x) is \u220603-computable\n(h) Mnorm(xy | x) is \u220603-computable\nProof. (a) By Li and Vit\u00e1nyi (2008, Thm. 4.5.2). Intuitively, we can run all programs in parallel and get monotonely increasing lower bounds for M(x) by adding 2\u2212|p|\nevery time a program p has completed outputting x.\n(b) From (a) and Lemma 6.2d since M(x) > 0 (see also Figure 6.1).\n(c) By Lemma 6.2cd, and M(x) > 0.\n(d) By (iii), Lemma 6.2d since Mnorm(x) \u2265M(x) > 0.\n(e) Let \u03c6 be a computable function that lower semicomputesM . SinceM is a semimeasure, M(xy) \u2265 \u2211 zM(xyz), hence \u2211 y\u2208XnM(xy) is nonincreasing in n and thus\nM(x) > q iff \u2200n\u2203k \u2211\ny\u2208Xn \u03c6(xy, k) > q.\n(f) From (v) and Lemma 6.2d since M(x) > 0.\n(g) From (v) and Lemma 6.2d.\n(h) From (vi) and Lemma 6.2d since Mnorm(x) \u2265M(x) > 0.\n\u00a76.2 The Complexity of Solomonoff Induction 107\nWe proceed to show that these bounds are in fact the best possible ones. If M were \u220601-computable, then so would be the conditional semimeasureM( \u00b7 | \u00b7 ). Thus the M -adversarial sequence z1:\u221e defined in Example 3.42 would be computable and hence corresponds to a computable deterministic measure \u00b5. However, we haveM(z1:t) \u2264 2\u2212t by construction, so dominance M(x) \u2265 w(\u00b5)\u00b5(x) with w(\u00b5) > 0 yields a contradiction with t\u2192\u221e:\n2\u2212t \u2265M(z1:t) \u2265 w(\u00b5)\u00b5(z1:t) = w(\u00b5) > 0\nBy the same argument, the normalized Solomonoff prior Mnorm cannot be \u220601-computable. However, since it is a measure, \u03a301- or \u03a001-computability would entail \u220601- computability.\nFor M and Mnorm we prove the following two lower bounds for specific universal Turing machines.\nTheorem 6.4 (M is not Limit Computable). There is a universal Turing machine U \u2032 such that the set {(x, q) |MU \u2032(x) > q} is not in \u220602.\nProof. Assume the contrary and let A \u2208 \u03a002 \\\u220602 and \u03b7 be a quantifier-free first-order formula such that\nn \u2208 A \u21d0\u21d2 \u2200k\u2203i. \u03b7(n, k, i). (6.2)\nFor each n \u2208 N, we define the program pn as follows.\n1: procedure pn 2: output 1n+10 3: k \u2190 0 4: while true do 5: i\u2190 0 6: while not \u03b7(n, k, i) do 7: i\u2190 i+ 1 8: k \u2190 k + 1 9: output 0\nEach program pn always outputs 1n+10. Furthermore, the program pn outputs the infinite string 1n+10\u221e if and only if n \u2208 A by (6.2). We define U \u2032 as follows using our reference machine U .\n\u2022 U \u2032(1n+10): Run pn.\n\u2022 U \u2032(00p): Run U(p).\n\u2022 U \u2032(01p): Run U(p) and bitwise invert its output.\nBy construction, U \u2032 is a universal Turing machine. No pn outputs a string starting with 0n+11, therefore MU \u2032(0n+11) = 14 ( MU (0 n+11) +MU (1 n+10) ) . Hence\nMU \u2032(1 n+10) = 2\u2212n\u221221A(n) + 1 4MU (1 n+10) + 14MU (0 n+11)\n= 2\u2212n\u221221A(n) +MU \u2032(0 n+11)\n108 Computability\nIf n /\u2208 A, then MU \u2032(1n+10) = MU \u2032(0n+11). Otherwise, we have |MU \u2032(1n+10) \u2212 MU \u2032(0\nn+11)| = 2\u2212n\u22122. Now we assume that MU \u2032 is limit computable, i.e., there is a computable function\n\u03c6 : X \u2217 \u00d7 N\u2192 Q such that limk\u2192\u221e \u03c6(x, k) = MU \u2032(x). We get that\nn \u2208 A \u21d0\u21d2 lim k\u2192\u221e\n\u03c6(0n+11, k)\u2212 \u03c6(1n+10, k) \u2265 2\u2212n\u22122,\nthus A is limit computable, a contradiction.\nCorollary 6.5 (Mnorm is not \u03a302- or \u03a002-computable). There is a universal Turing machine U \u2032 such that {(x, q) |MnormU \u2032(x) > q} is not in \u03a302 or \u03a002.\nProof. Since Mnorm = c \u00b7M , there exists a k \u2208 N such that 2\u2212k < c (even if we do not know the value of k). We can show that the set {(x, q) |MnormU \u2032(x) > q} is not in \u220602 analogously to the proof of Theorem 6.4, using\nn \u2208 A \u21d0\u21d2 lim k\u2192\u221e\n\u03c6(0n+11, k)\u2212 \u03c6(1n+10, k) \u2265 2\u2212k\u2212n\u22122.\nIf Mnorm were \u03a302-computable or \u03a002-computable, this would imply that Mnorm is \u220602- computable since Mnorm is a measure, a contradiction.\nSinceM( ) = 1, we haveM(x | ) = M(x), so the conditional probabilityM(xy | x) has at least the same complexity as M . Analogously for Mnorm and Mnorm since they are measures. For M , we have that M(x | ) = Mnorm(x), so Corollary 6.5 applies. All that remains to prove is that conditional M is not lower semicomputable.\nTheorem 6.6 (Conditional M is not Lower Semicomputable). The set {(x, xy, q) | M(xy | x) > q} is not recursively enumerable.\nWe gave a different, more complicated proof in Leike and Hutter (2015b). The following, much simpler and more elegant proof is due to Sterkenburg (2016, Prop. 3).\nProof. Assume to the contrary thatM(xy | x) is lower semicomputable. Let a 6= b \u2208 X . We construct an infinite string x by defining its initial segments =: x(0) @ x(1) @ x(2) @ . . . @ x. At every step n, we enumerate strings y \u2208 X \u2217 until one is found satisfying M(a | x(n)y) \u2265 1/2; then set x(n + 1) := x(n)yb. This implies that for infinitely many t there is an n such that M(b | x<t) = M(b | x(n)y) \u2264 1 \u2212M(a | x(n)y) \u2264 1/2. Since we assumed M( \u00b7 | \u00b7 ) to be lower semicomputable, the infinite string x is computable, and hence M(xt | x<t) \u2192 1 by Corollary 3.55. But this contradicts M(b | x<t) \u2264 1/2 infinitely often."}, {"heading": "6.3 The Complexity of AINU, AIMU, and AIXI", "text": ""}, {"heading": "6.3.1 Upper Bounds", "text": "In this section, we derive upper bounds on the computability of AINU, AIMU, and AIXI. Except for Corollary 6.14, all results in this section apply generally to any \u03bd \u2208 MCCSLSC .\n\u00a76.3 The Complexity of AINU, AIMU, and AIXI 109\nSince the Bayesian mixture \u03be \u2208MCCSLSC , they apply to AIXI even though they are stated for AINU.\nIn order to position AINU in the arithmetical hierarchy, we need to encode policies as sets of natural numbers. For the rest of this chapter, we assume that policies are deterministic, thus can be represented as relations over (A\u00d7E)\u2217\u00d7A. These relations are easily identified with sets of natural numbers by encoding the history into one natural number. From now on this translation of policies into sets of natural numbers will be done implicitly wherever necessary.\nLemma 6.7 (Policies are in \u22060n). If a policy \u03c0 is \u03a30n or \u03a00n, then \u03c0 is \u22060n.\nProof. Let \u03d5 be a \u03a30n-formula (\u03a00n-formula) defining \u03c0, i.e., \u03d5(h, a) holds iff \u03c0(h) = a. We define the formula \u03d5\u2032,\n\u03d5\u2032(h, a) := \u2227\na\u2032\u2208A\\{a}\n\u00ac\u03d5(h, a\u2032).\nThe set of actions A is finite, hence \u03d5\u2032 is a \u03a00n-formula (\u03a30n-formula). Moreover, \u03d5\u2032 is equivalent to \u03d5.\nTo compute the optimal policy, we need to compute the optimal value function. The following lemma gives an upper bound on the computability of the value function for environments inMCCSLSC .\nLemma 6.8 (Complexity of V \u2217\u03bd ). For every \u03bd \u2208 MCCSLSC , and every lower semicomputable discount function \u03b3, the function V \u2217\u03bd is \u220602-computable.\nProof. The explicit form of the value function (4.2) has numerator\nlim m\u2192\u221e max \u2211 \u00e6t:m m\u2211 i=t \u03b3(i)ri\u03bd(e1:i \u2016 a1:i),\nand denominator \u03bd(e<t \u2016 a<t) \u00b7 \u0393t. The numerator is nondecreasing in m because we assumed rewards to be nonnegative (Assumption 4.6b). Hence both numerator and denominator are lower semicomputable functions, so Lemma 6.2d implies that V \u2217\u03bd is \u220602-computable.\nFrom the optimal value function V \u2217\u03bd we get the optimal policy \u03c0\u2217\u03bd according to (4.4). However, in cases where there is more than one optimal action, we have to break an argmax tie. This happens iff V \u2217\u03bd (h\u03b1) = V \u2217\u03bd (h\u03b2) for two potential actions \u03b1 6= \u03b2 \u2208 A. This equality test is more difficult than determining which is larger in cases where they are unequal. Thus we get the following upper bound.\nTheorem 6.9 (Complexity of Optimal Policies). For any environment \u03bd, if V \u2217\u03bd is \u22060n-computable, then there is an optimal policy \u03c0\u2217\u03bd for the environment \u03bd that is \u22060n+1.\n110 Computability\nProof. To break potential ties, we pick an (arbitrary) total order on A that specifies which actions should be preferred in case of a tie. We define\n\u03c0\u03bd(h) = a :\u21d0\u21d2 \u2227\na\u2032:a\u2032 a V \u2217\u03bd (ha) > V \u2217 \u03bd (ha \u2032)\n\u2227 \u2227\na\u2032:a a\u2032 V \u2217\u03bd (ha) \u2265 V \u2217\u03bd (ha\u2032).\n(6.3)\nThen \u03c0\u03bd is a \u03bd-optimal policy according to (4.4). By assumption, V \u2217\u03bd is \u22060n-computable. By Lemma 6.2ab V \u2217\u03bd (ha) > V \u2217\u03bd (ha\u2032) is \u03a30n and V \u2217\u03bd (ha) \u2265 V \u2217\u03bd (ha\u2032) is \u03a00n. Therefore the policy \u03c0\u03bd defined in (6.3) is a conjunction of a \u03a30n-formula and a \u03a00n-formula and thus \u22060n+1.\nCorollary 6.10 (Complexity of AINU). AINU is \u220603 for every environment \u03bd \u2208MCCSLSC .\nProof. From Lemma 6.8 and Theorem 6.9.\nUsually we do not mind taking slightly suboptimal actions. Therefore actually trying to determine if two actions have the exact same value seems like a waste of resources. In the following, we consider policies that attain a value that is always within some \u03b5 > 0 of the optimal value.\nTheorem 6.11 (Complexity of \u03b5-Optimal Policies). For any environment \u03bd, if V \u2217\u03bd is \u22060n-computable, then there is an \u03b5-optimal policy \u03c0\u03b5\u03bd for the environment \u03bd that is \u22060n.\nProof. Let \u03b5 > 0 be given. Since the value function V \u2217\u03bd (h) is \u22060n-computable, the set V\u03b5 := {(ha, q) | |q \u2212 V \u2217\u03bd (ha)| < \u03b5/2} is in \u22060n according to Definition 6.1. Hence we compute the values V \u2217\u03bd (ha\u2032) until we get within \u03b5/2 for every a\u2032 \u2208 A and then choose the action with the highest value so far. Formally, let be an arbitrary total order on A that specifies which actions should be preferred in case of a tie. Without loss of generality, we assume \u03b5 = 1/k, and define Q to be an \u03b5/2-grid on [0, 1], i.e., Q := {0, 1/2k, 2/2k, . . . , 1}. We define\n\u03c0\u03b5\u03bd(h) = a :\u21d0\u21d2 \u2203(qa\u2032)a\u2032\u2208A \u2208 QA. \u2227 a\u2032\u2208A (ha\u2032, qa\u2032) \u2208 V\u03b5\n\u2227 \u2227\na\u2032:a\u2032 a qa > qa\u2032 \u2227 \u2227 a\u2032:a a\u2032 qa \u2265 qa\u2032\n\u2227 the tuple (qa\u2032)a\u2032\u2208A is minimal with respect to the lex. ordering on QA.\n(6.4)\nThis makes the choice of a unique. Moreover, QA is finite since A is finite, and hence (6.4) is a \u22060n-formula.\nCorollary 6.12 (Complexity of \u03b5-Optimal AINU). For any environment \u03bd \u2208 MCCSLSC , there is an \u03b5-optimal policy for AINU that is \u220602.\nProof. From Lemma 6.8 and Theorem 6.11.\n\u00a76.3 The Complexity of AINU, AIMU, and AIXI 111\nCorollary 6.13 (Complexity of \u03b5-Optimal AIXI). For any lower semicomputable prior there is an \u03b5-optimal policy for AIXI that is \u220602.\nProof. From Corollary 6.12 since for any lower semicomputable prior, the corresponding Bayesian mixture \u03be is inMCCSLSC .\nIf the environment \u03bd \u2208MCCMcomp is a measure, i.e., \u03bd assigns zero probability to finite strings, then we get computable \u03b5-optimal policies.\nCorollary 6.14 (Complexity of AIMU). If the environment \u00b5 \u2208 MCCMcomp is a measure and the discount function \u03b3 is computable, then AIMU is limit computable (\u220602), and \u03b5-optimal AIMU is computable (\u220601).\nProof. Let \u03b5 > 0 be the desired accuracy. We can truncate the limit m \u2192 \u221e in (4.2) at the \u03b5/2-effective horizon Ht(\u03b5/2), since everything after Ht(\u03b5/2) can contribute at most \u03b5/2 to the value function. Any lower semicomputable measure is computable (Li and Vit\u00e1nyi, 2008, Lem. 4.5.1). Therefore V \u2217\u00b5 as given in (4.2) is composed only of computable functions, hence it is computable according to Lemma 6.2. The claim now follows from Theorem 6.9 and Theorem 6.11."}, {"heading": "6.3.2 Lower Bounds", "text": "We proceed to show that the bounds from the previous section are the best we can hope for. In environment classes where ties have to be broken, AINU has to solve \u03a002-hard problems (Theorem 6.16). These lower bounds are stated for particular environments \u03bd \u2208MCCSLSC . Throughout this section, we assume that \u0393t > 0 for all t.\nWe also construct universal mixtures that yield bounds on \u03b5-optimal policies. There is an \u03b5-optimal AIXI that solves \u03a301-hard problems (Theorem 6.17). For arbitrary universal mixtures, we prove the following weaker statement that only guarantees incomputability.\nTheorem 6.15 (No AIXI is computable). AIXI is not computable for any universal Turing machine U .\n112 Computability\nThis theorem follows from the incomputability of Solomonoff induction. By the on-policy value convergence theorem (Corollary 4.20) AIXI succeeds to predict the environment\u2019s behavior for its own policy. If AIXI were computable, then there would be computable environments more powerful than AIXI: they can simulate AIXI and anticipate its prediction, which leads to a contradiction.\nProof. Assume there is a computable policy \u03c0\u2217\u03be that is optimal in the mixture \u03be. We define a deterministic environment \u00b5, the adversarial environment to \u03c0\u2217\u03be . The environment \u00b5 gives rewards 0 as long as the agent follows the policy \u03c0\u2217\u03be , and rewards 1 once the agent deviates. Formally, we ignore observations by setting O := {0}, and define\n\u00b5(r1:t \u2016 a1:t) :=  1 if \u2200k \u2264 t. ak = \u03c0\u2217\u03be ((ar)<k) and rk = 0, 1 if \u2200k \u2264 t. rk = 1k\u2265i\nwhere i := min{j | aj 6= \u03c0\u2217\u03be ((ar)<j)}, and 0 otherwise.\nSee Figure 6.2 for an illustration of this environment. The environment \u00b5 is computable because the policy \u03c0\u2217\u03be was assumed to be computable. Suppose \u03c0 \u2217 \u03be acts in \u00b5, then by Theorem 4.19 AIXI learns to predict perfectly on policy :\nV \u03c0\u2217\u03be \u03be (\u00e6<t)\u2212 V \u03c0\u2217\u03be \u00b5 (\u00e6<t)\u2192 0 as t\u2192\u221e \u00b5\u03c0 \u2217 \u03be -almost surely,\nsince both \u03c0\u2217\u03be and \u00b5 are deterministic. Because V \u03c0\u2217\u03be \u00b5 (h<t) = 0 by definition of \u00b5, we get V \u2217\u03be (\u00e6<t)\u2192 0. Therefore we find a t large enough such that V \u2217\u03be (\u00e6<t) < w(\u00b5) where \u00e6<t is the interaction history of \u03c0\u2217\u03be in \u00b5. A policy \u03c0 with \u03c0(\u00e6<t) 6= \u03c0\u2217\u03be (\u00e6<t), gets a reward of 1 in environment \u00b5 for all time steps after t, hence V \u03c0\u00b5 (\u00e6<t) = 1. With linearity of V \u03c0\u03be (\u00e6<t) in \u03be (Lemma 4.14),\nV \u03c0\u03be (\u00e6<t) \u2265 w(\u00b5) \u00b5(e1:t\u2016a1:t) \u03be(e1:t\u2016a1:t)V \u03c0 \u00b5 (\u00e6<t) \u2265 w(\u00b5),\nsince \u00b5(e1:t \u2016 a1:t) = 1 (\u00b5 is deterministic), V \u03c0\u00b5 (\u00e6<t) = 1, and \u03be(e1:t \u2016 a1:t) \u2264 1. Now we get a contradiction:\nw(\u00b5) > V \u2217\u03be (\u00e6<t) = sup \u03c0\u2032 V \u03c0 \u2032 \u03be (\u00e6<t) \u2265 V \u03c0\u03be (\u00e6<t) \u2265 w(\u00b5)\nFor the remainder of this section, we fix the action space to be A := {\u03b1, \u03b2} with action \u03b1 favored in ties. The percept space is fixed to a tuple of binary observations and rewards, E := O \u00d7 {0, 1} with O := {0, 1}.\nTheorem 6.16 (AINU is \u03a002-hard). There is an environment \u03bd \u2208 MCCSLSC such that AINU is \u03a002-hard.\nProof. Let A be a any \u03a002-set, and let \u03b7 be a quantifier-free formula such that\nn \u2208 A \u21d0\u21d2 \u2200i \u2203k \u03b7(n, i, k). (6.5)\n\u00a76.3 The Complexity of AINU, AIMU, and AIXI 113\nWe define a class of environmentsM := {\u03c11, \u03c12, . . .} where each \u03c1i is defined as follows.\n\u03c1i((or)1:m \u2016 a1:m) :=  2\u2212m if o1:m = 1m and \u2200t \u2264 m. rt = 0, 2\u2212n\u22121 if \u2203n. 1n0 v o1:m v 1n0\u221e and an+2 = \u03b1 and rt = 1t>n+1 and \u2203k \u03b7(n, i, k), 2\u2212n\u22121 if \u2203n. 1n0 v o1:m v 1n0\u221e and an+2 = \u03b2\nand rt = 1t>n+1, and 0 otherwise.\nSee Figure 6.3 for an illustration of these environments. Every \u03c1i is a chronological conditional semimeasure by definition and every \u03c1i is lower semicomputable since \u03b7 is quantifier-free, soM\u2286MCCSLSC .\nWe define our environment \u03bd as a mixture overM,\n\u03bd := \u2211 i\u2208N 2\u2212i\u22121\u03c1i;\nthe choice of the weights on the environments \u03c1i is arbitrary but positive. Let \u03c0\u2217\u03bd be an optimal policy for the environment \u03bd and recall that the action \u03b1 is preferred in ties. We claim that for the \u03bd-optimal policy \u03c0\u2217\u03bd ,\nn \u2208 A \u21d0\u21d2 \u03c0\u2217\u03bd(1n0) = \u03b1. (6.6)\nThis enables us to decide whether n \u2208 A given the policy \u03c0\u2217\u03bd , hence proving (6.6) concludes this proof.\nLet n, i \u2208 N be given, and suppose we are in environment \u03c1i and observe 1n0. Taking action \u03b2 next yields reward 1 forever; taking action \u03b1 next yields a reward of 1 if there\n114 Computability\nis a k such that \u03b7(n, i, k) holds. If this is the case, then\nV \u2217\u03c1i(1 n0\u03b1) = \u0393n+2 = V \u2217 \u03c1i(1 n0\u03b2),\nand otherwise V \u2217\u03c1i(1 n0\u03b1) = 0 < \u0393n+2 = V \u2217 \u03c1i(1 n0\u03b2)\n(omitting the first n + 1 actions and rewards in the argument of the value function). We can now show (6.6): By (6.5), n \u2208 A if and only if for all i there is a k such that \u03b7(n, i, k), which happens if and only if V \u2217\u03c1i(1\nn0\u03b1) = \u0393n+2 for all i \u2208 N, which is equivalent to V \u2217\u03bd (1n0\u03b1) = \u0393n+2, which in turn is equivalent to \u03c0\u2217\u00b5(1n0) = \u03b1 since V \u2217\u03bd (1 n0\u03b2) = \u0393n+2 and action \u03b1 is favored in ties.\nTheorem 6.17 (Some \u03b5-optimal AIXI are \u03a301-hard). There is a universal Turing machine U \u2032 and an \u03b5 > 0 such that any \u03b5-optimal policy for AIXI is \u03a301-hard.\nProof. Let A be a \u03a301-set and \u03b7 be a quantifier-free formula such that n + 1 \u2208 A iff \u2203k \u03b7(n, k). We define the environment\n\u03bd((or)1:t \u2016 a1:t) :=  \u03be((or)1:n \u2016 a1:n) if \u2203n. o1:n = 1n\u221210 and an = \u03b1 and \u2200t\u2032 > n. ot\u2032 = 0 \u2227 rt\u2032 = 12 , \u03be((or)1:n \u2016 a1:n) if \u2203n. o1:n = 1n\u221210 and an = \u03b2 and \u2200t\u2032 > n. ot = 0 \u2227 rt = 1 and \u2203k \u03b7(n\u2212 1, k),\n\u03be((or)1:t \u2016 a1:t) if @n. o1:n = 1n\u221210, and 0 otherwise.\nSee Figure 6.4 for an illustration. The environment \u03bd mimics the universal environment\n\u00a76.4 Iterative Value Function 115\n\u03be until the observation history is 1n\u221210. Taking the action \u03b1 next gives rewards 1/2 forever. Taking the action \u03b2 next gives rewards 1 forever if n \u2208 A, otherwise the environment \u03bd ends at some future time step. Therefore we want to take action \u03b2 if and only if n \u2208 A. We have that \u03bd \u2208MCCSLSC since \u03be \u2208MCCSLSC and \u03b7 is quantifier-free.\nWe define \u03be\u2032 := 12\u03bd + 1 8\u03be. By Lemma 4.24 \u03be \u2032 is a universal lower semicomputable semimeasure. Let n \u2208 A be given and let h \u2208 (A\u00d7E)n be any history with observations o1:n = 1\nn\u221210. Since \u03bd(1n\u221210 | a1:n) = \u03be(1n\u221210 | a1:n) by definition, the posterior weights of \u03bd and \u03be in \u03be\u2032 are equal to the prior weights, analogously to the proof of Theorem 5.5. In the following, we use the linearity of V \u03c0\u2217 \u03be\u2032\n\u03c1 in \u03c1 (Lemma 4.14), and the fact that values are bounded between 0 and 1 (Assumption 4.6b). If there is a k such that \u03b7(n \u2212 1, k) holds,\nV \u2217\u03be\u2032(h\u03b2)\u2212 V \u2217\u03be\u2032(h\u03b1) = 12V \u03c0\u2217 \u03be\u2032 \u03bd (h\u03b2)\u2212 12V \u03c0\u2217 \u03be\u2032 \u03bd (h\u03b1) + 1 8V \u03c0\u2217 \u03be\u2032 \u03be (h\u03b2)\u2212 1 8V \u03c0\u2217 \u03be\u2032 \u03be (h\u03b1)\n\u2265 12 \u2212 1 4 + 0\u2212 1 8 = 1 8 ,\nand similarly if there is no k such that \u03b7(n\u2212 1, k) holds, then\nV \u2217\u03be\u2032(h\u03b1)\u2212 V \u2217\u03be\u2032(h\u03b2) = 12V \u03c0\u2217 \u03be\u2032 \u03bd (h\u03b1)\u2212 12V \u03c0\u2217 \u03be\u2032 \u03bd (h\u03b2) + 1 8V \u03c0\u2217 \u03be\u2032 \u03be (h\u03b1)\u2212 1 8V \u03c0\u2217 \u03be\u2032 \u03be (h\u03b2)\n\u2265 14 \u2212 0 + 0\u2212 1 8 = 1 8 .\nIn both cases |V \u2217\u03be\u2032(h\u03b2) \u2212 V \u2217\u03be\u2032(h\u03b1)| > 1/9. Hence we pick \u03b5 := 1/9 and get for every \u03b5-optimal policy \u03c0\u03b5\u03be\u2032 that \u03c0 \u03b5 \u03be\u2032(h) = \u03b2 if and only if n \u2208 A.\nNote the differences between Theorem 6.15 and Theorem 6.17: the former talks about optimal policies and shows that they are not computable, but is agnostic towards the underlying universal Turing machine. The latter talks about \u03b5-optimal policies and gives a stronger hardness result, at the cost of depending on one particular universal Turing machine."}, {"heading": "6.4 Iterative Value Function", "text": "Historically, AIXI\u2019s value function has been defined slightly differently to Definition 4.10, using a limit extension of an iterative definition of the value function. This definition is the more straightforward to come up with in AI: it is the natural adaptation of (optimal) minimax search in zero-sum games to the (optimal) expectimax algorithm for stochastic environments. In this section we discuss the problems with this definition.\nTo avoid confusion with the recursive value function V \u03c0\u03bd , we denote the iterative value function with W \u03c0\u03bd .1\nDefinition 6.18 (Iterative Value Function; Hutter, 2005, Def. 5.30). The iterative\n1In Leike and Hutter (2015a) the use of the symbols V and W is reversed.\nvalue of a policy \u03c0 in an environment \u03bd given history \u00e6<t is\nW \u03c0\u03bd (\u00e6<t) := 1\n\u0393t lim m\u2192\u221e \u2211 et:m \u03bd(e1:m | e<t \u2016 a1:m) m\u2211 k=t \u03b3(k)rk\nif \u0393t > 0 and W \u03c0\u03bd (\u00e6<t) := 0 if \u0393t = 0 where ai := \u03c0(e<i) for all i \u2265 t. The optimal iterative value is defined as W \u2217\u03bd (h) := sup\u03c0W \u03c0\u03bd (h).\nAnalogously to (4.2), we can write W \u2217\u03bd using the max-sum-operator:\nW \u2217\u03bd (\u00e6<t) = 1\n\u0393t lim m\u2192\u221e max \u2211 \u00e6t:m \u03bd(e1:m | e<t \u2016 a1:m) m\u2211 k=t \u03b3(k)rk (6.7)\nWe use iterative AINU for the \u03bd-optimal policy according to the iterative value function, and iterative AIXI for the \u03be-optimal policy according to the iterative value function. Note that iterative AIMU coincides with AIMU since \u00b5 is a measure by convention.\nGenerally, our environment \u03bd \u2208 MCCSLSC is only a semimeasure and not a measure, i.e., there is a history \u00e6<tat such that\n1 > \u2211 et\u2208E \u03bd(et | e<t \u2016 a1:t).\nIn such cases, with positive probability the environment \u03bd does not produce a new percept et. If this occurs, we shall use the informal interpretation that the environment \u03bd ended, but our formal argument does not rely on this interpretation.\nThe following proposition shows that for a semimeasure \u03bd \u2208 MCCSLSC that is not a measure, iterative AINU does not maximize \u03bd-expected rewards. Recall that \u03b3(1) states the discount of the first reward. In the following, we assume without loss of generality that \u03b3(1) > 0, i.e., we are not indifferent about the reward received in time step 1.\nProposition 6.19 (Iterative AINU is not a \u03bd-Expected Reward Maximizer). For any \u03b5 > 0 there is an environment \u03bd \u2208 MCCSLSC that is not a measure and a policy \u03c0 that receives a total of \u03b3(1) rewards in \u03bd, but iterative AINU receives only \u03b5\u03b3(1) rewards in \u03bd.\n\u00a76.4 Iterative Value Function 117\nInformally, the environment \u03bd is defined as follows. In the first time step, the agent chooses between the two actions \u03b1 and \u03b2. Taking action \u03b1 gives a reward of 1, and subsequently the environment ends. Action \u03b2 gives a reward of \u03b5, but the environment continues forever. There are no other rewards in this environment. See Figure 6.5. From the perspective of \u03bd-expected reward maximization, it is better to take action \u03b1, however iterative AINU takes action \u03b2.\nProof of Proposition 6.19. Let \u03b5 > 0. We ignore observations and set E := {0, \u03b5, 1}, A := {\u03b1, \u03b2}. The environment \u03bd is formally defined by\n\u03bd(r1:t \u2016 a1:t) :=  1 if a1 = \u03b1 and r1 = 1 and t = 1 1 if a1 = \u03b2 and r1 = \u03b5 and rk = 0 \u22001 < k \u2264 t 0 otherwise.\nTaking action \u03b1 first, we have \u03bd(r1:t \u2016 \u03b1a2:t) = 0 for t > 1 (the environment \u03bd ends in time step 2 given history \u03b1). Hence we conclude\nV \u2217\u03bd (\u03b1) = 1\n\u0393t lim m\u2192\u221e \u2211 r1:m \u03bd(r1:m \u2016 \u03b1a2:m) m\u2211 k=1 \u03b3(k)rk = 0.\nTaking action \u03b2 first we get\nV \u2217\u03bd (\u03b2) = 1\n\u0393t lim m\u2192\u221e \u2211 r1:m \u03bd(r1:m \u2016 \u03b2a2:m) m\u2211 k=1 \u03b3(k)rk = \u03b3(1) \u03931 \u03b5.\nSince \u03b3(1) > 0 and \u03b5 > 0, we have V \u2217\u03bd (\u03b2) > V \u2217\u03bd (\u03b1), and thus iterative AINU will use a policy that plays action \u03b2 first, receiving a total discounted reward of \u03b5\u03b3(1). In contrast, any policy \u03c0 that takes action \u03b1 first receives a larger total discounted reward of \u03b3(1).\nWhether it is reasonable to assume that our environment has a nonzero probability\n118 Computability\nof ending is a philosophical debate we do not want to engage in here; see Martin et al. (2016) for a discussion. Instead, we have a different motivation to use the recursive over the iterative value function: the latter has worse computability properties. Concretely, we show that \u03b5-optimal iterative AIXI has to solve \u03a002-hard problems and that there is an environment \u03bd \u2208MCCSLSC such that iterative AINU has to solve \u03a303-hard problems. In contrast, using the recursive value function, \u03b5-optimal AIXI is \u220602 according to Corollary 6.12 and AINU is \u220603 according to Corollary 6.10.\nThe central difference between V \u03c0\u03bd and W \u03c0\u03bd is that for V \u03c0\u03bd all obtained rewards matter, but for W \u03c0\u03bd only the rewards in timelines that continue indefinitely. In this sense the value function W \u03c0\u03bd conditions on surviving forever. If the environment \u00b5 is a measure, then the history is infinite with probability one, and so V \u03c0\u03bd and W \u03c0\u03bd coincide. Hence this distinction is not relevant for AIMU, only for AINU and AIXI.\nLemma 6.20 (Complexity of W \u2217\u03bd ). For every \u03bd \u2208 MCCSLSC , the function W \u2217\u03bd is \u03a002computable.\nProof. Multiplying (6.7) with \u0393t\u03bd(e<t \u2016 a<t) yields W \u2217\u03bd (\u00e6<t) > q if and only if\nlim m\u2192\u221e max \u2211 \u00e6t:m \u03bd(e1:m \u2016 a1:m) m\u2211 k=t \u03b3(k)rk > q \u0393t \u03bd(e<t \u2016 a<t). (6.8)\nThe inequality\u2019s right side is lower semicomputable, hence there is a computable function \u03c8 such that \u03c8(`)\u2197 q \u0393t \u03bd(e<t \u2016 a<t) =: q\u2032 as `\u2192\u221e. (In contrast to the recursive value function, this quantity is not increasing in m.) For a fixed m, the left side is also lower semicomputable, therefore there is a computable function \u03c6 such that\n\u03c6(m, k)\u2197 max \u2211 \u00e6t:m \u03bd(e1:m \u2016 a1:m) m\u2211 k=t \u03b3(k)rk =: f(m) as k \u2192\u221e.\nWe already know that the limit of f(m) for m \u2192 \u221e exists (uniquely), hence we can write (6.8) as\nlim m\u2192\u221e\nf(m) > q\u2032\n\u21d0\u21d2 \u2200m0 \u2203m \u2265 m0. f(m) > q\u2032 \u21d0\u21d2 \u2200m0 \u2203m \u2265 m0 \u2203k. \u03c6(m, k) > q\u2032\n\u21d0\u21d2 \u2200`\u2200m0 \u2203m \u2265 m0 \u2203k. \u03c6(m, k) > \u03c8(`),\nwhich is a \u03a002-formula.\nNote that in the finite horizon case where m is fixed, the value function W \u2217\u03bd is \u220602- computable by Lemma 6.2d, since W \u2217\u03bd (\u00e6<t) = f(m)/q\u2032. In this case, we get the same computability results for iterative AINU as we did in Section 6.3.1.\nCorollary 6.21 (Complexity of Iterative AINU). For any environment \u03bd \u2208 MCCSLSC , iterative AINU is \u220604 and there is an \u03b5-optimal iterative AINU that is \u2206 0 3.\n\u00a76.4 Iterative Value Function 119\nProof. From Theorem 6.9, Theorem 6.11, and Lemma 6.20.\nWe proceed to show corresponding lower bounds as in Section 6.3.2. For the rest of this section we assume \u0393t > 0 for all t.\nTheorem 6.22 (Iterative AINU is \u03a303-hard). There is an environment \u03bd \u2208MCCSLSC such that iterative AINU is \u03a303-hard.\nProof. The proof is analogous to the proof of Theorem 6.16. Let A be any \u03a303 set, then there is a quantifier-free formula \u03b7 such that\nn \u2208 A \u21d0\u21d2 \u2203i \u2200t \u2203k \u03b7(n, i, t, k).\nWe define the environments \u03c1i similar to the proof of Theorem 6.16, except for two changes:\n\u2022 We replace \u2203k \u03b7(n, i, k) with \u2200t\u2032 \u2264 t \u2203k \u03b7(n, i, t\u2032, k).\n\u2022 We switch actions \u03b1 and \u03b2: action \u03b2 \u2018checks\u2019 the formula \u03b7 and action \u03b1 gives a sure reward of 0.\n120 Computability\nFormally,\n\u03c1i((or)1:t \u2016 a1:t) :=  2\u2212t if o1:t = 1t and \u2200t\u2032 \u2264 t. rt\u2032 = 0, 2\u2212n\u22121 if \u2203n. 1n0 v o1:t v 1n0\u221e and an+2 = \u03b1 and \u2200t\u2032 \u2264 t. rt\u2032 = 0, 2\u2212n\u22121 if \u2203n. 1n0 v o1:t v 1n0\u221e and an+2 = \u03b2 and \u2200t\u2032 \u2264 t. rt\u2032 = 1t\u2032>n+1 and \u2200t\u2032 \u2264 t \u2203k \u03b7(n, i, t\u2032, k), and\n0 otherwise.\nSee Figure 6.6 for an illustration of the environment \u03c1i. Every \u03c1i is a chronological conditional semimeasure by definition, so M := {\u03c10, \u03c11, . . .} \u2286 MCCSLSC . Furthermore, every \u03c1i is lower semicomputable since \u03b7 is quantifier-free.\nWe define our environment \u03bd as a mixture overM,\n\u03bd := \u2211 i\u2208N 2\u2212i\u22121\u03c1i;\nthe choice of the weights on the environments \u03c1i is arbitrary but positive. We get for the \u03bd-optimal policy \u03c0\u2217\u03bd analogously to the proof of Theorem 6.16\n\u03c0\u2217\u03bd(1 n0) = \u03b2 \u21d0\u21d2 \u2203i\u2200t\u2032 \u2264 t\u2203k \u03b7(n, i, t\u2032, k) \u21d0\u21d2 n \u2208 A,\nsince action \u03b1 is preferred in ties.\nAnalogously to Theorem 6.15, we can show that iterative AIXI is not computable. We also get the following lower bound.\nTheorem 6.23 (Some \u03b5-optimal iterative AIXI are \u03a002-hard). There is a universal mixture \u03be\u2032 and an \u03b5 > 0 such that any policy that is \u03b5-optimal according to the iterative value for environment \u03be\u2032 is \u03a002-hard.\nProof. Let A be a \u03a002-set and \u03b7 a quantifier-free formula such that\nn \u2208 A \u21d0\u21d2 \u2200t \u2203k \u03b7(n, t, k).\nWe proceed analogous to the proof of Theorem 6.17 except that we choose \u2200t\u2032 \u2264 t\u2203k \u03b7(n, t, k) as a condition for reward 1 after playing action \u03b2.\n\u00a76.4 Iterative Value Function 121\nDefine the environment\n\u03bd((or)1:t \u2016 a1:t) :=  \u03be((or)1:n+1 \u2016 a1:n+1) if \u2203n. 1n0 v o1:t v 1n0\u221e and an+1 = \u03b1 and \u2200n+ 1 < k \u2264 t. rk = 1/2, \u03be((or)1:n+1 \u2016 a1:n+1) if \u2203n. 1n0 v o1:t v 1n0\u221e and an+1 = \u03b2 and \u2200n+ 1 < k \u2264 t. rk = 1 and \u2200t\u2032 \u2264 t\u2203k \u03b7(n, t, k),\n\u03be((or)1:t \u2016 a1:t) if @n. 1n0 v o1:t v 1n0\u221e, and 0 otherwise.\nSee Figure 6.7 for an illustration of the environment \u03bd. The environment \u03bd mimics the universal environment \u03be until the observation history is 1n0. The next action \u03b1 always gives rewards 1/2 forever, while action \u03b2 gives rewards 1 forever iff n \u2208 A. We have that \u03bd is a lower semicomputable semimeasure since \u03be is a lower semicomputable semimeasure and \u03b7 is quantifier-free. We define \u03be\u2032 = 12\u03bd+ 1 8\u03be. By Lemma 4.24, \u03be\n\u2032 is a universal lower semicomputable semimeasure. Let n \u2208 A be given and let h \u2208 (A \u00d7 O)x+1 be any history with observations o1:n+1 = 1n0. In the following, we use the linearity of W \u2217\u03c1 in \u03c1 (analogously to Lemma 4.14). If \u2200t\u2203k \u03b7(n, t, k), then\nW \u2217\u03be\u2032(h\u03b2)\u2212W \u2217\u03be\u2032(h\u03b1) = 12W \u2217 \u03bd (h\u03b2)\u2212 12W \u2217 \u03bd (h\u03b1) + 1 8W \u2217 \u03be (h\u03b2)\u2212 18W \u2217 \u03be (h\u03b1)\n\u2265 12 \u2212 1 4 + 0\u2212 1 8 = 1 8 ,\n122 Computability\nand similarly if \u00ac\u2200t\u2203k \u03b7(n, t, k), then\nW \u2217\u03be\u2032(h\u03b1)\u2212W \u2217\u03be\u2032(h\u03b2) = 12W \u2217 \u03bd (h\u03b1)\u2212 12W \u2217 \u03bd (h\u03b2) + 1 8W \u2217 \u03be (h\u03b1)\u2212 18W \u2217 \u03be (h\u03b2)\n\u2265 14 \u2212 0 + 0\u2212 1 8 = 1 8 .\nIn both cases |W \u2217\u03be\u2032(h\u03b2)\u2212W \u2217\u03be\u2032(h\u03b1)| > 1/9, hence with \u03b5 := 1/9 we have for an \u03b5-optimal policy \u03c0\u03b5\u03be\u2032 that \u03c0 \u03b5 \u03be\u2032(h) = \u03b2 if and only if n \u2208 A."}, {"heading": "6.5 The Complexity of Knowledge-Seeking", "text": "Recall the definition of the optimal entropy-seeking value V \u2217,mEnt and the optimal information-seeking value V \u2217,mIG from Section 4.3.2. Using the results from Section 6.3 we can show that \u03b5-optimal knowledge-seeking agents are limit computable, and optimal knowledge-seeking agents are \u220603.\nCorollary 6.24 (Computability of Knowledge-Seeking Values). For fixed m, the value functions V \u2217,mEnt and V \u2217,m IG are limit computable.\nProof. This follows from Lemma 6.2 (c-e) since \u03be, \u03bd, and w are lower semicomputable.\nCorollary 6.25 (Computability of Knowledge-Seeking Policies). For entropy-seeking and information-seeking agents there are limit-computable \u03b5-optimal policies and \u220603- computable optimal policies.\nProof. Follows from Corollary 6.24, Theorem 6.9, and Theorem 6.11.\nNote that if we used an infinite horizon with discounting in Definition 4.25 or Definition 4.26, then we cannot retain this computability result without further assumptions: we would need that the value functions increase monotonically as m \u2192 \u221e, as they do for the recursive value function from Definition 4.10. However, entropy is not a monotone function and may decrease if there are events whose probability converges to something > 1/2. For the entropy-seeking value function this happens for histories drawn from a deterministic environment \u00b5 since \u03be \u2192 \u00b5, so the conditionals converge to 1. Similarly, for the information-seeking value function, the posterior belief in one (deterministic) environment might become larger than 1/2 (depending on the prior and the environment class). Therefore we generally only get that discounted versions of V \u2217Ent and V \u2217IG are \u2206 0 3 analogously to Lemma 6.20. Hence optimal discounted entropy-seeking and optimal discounted information-seeking policies are in \u220604 by Theorem 6.9 and their corresponding \u03b5-optimal siblings are \u220603 by Theorem 6.11."}, {"heading": "6.6 A Limit Computable Weakly Asymptotically Optimal Agent", "text": "According to Theorem 6.16, optimal reward-seeking policies are generally \u03a002-hard, and for optimal knowledge-seeking policies Corollary 6.25 shows that they are \u220603. Therefore\n\u00a76.7 Discussion 123\nwe get that BayesExp is \u220603:\nCorollary 6.26 (BayesExp is \u220603). For any universal mixture \u03be, BayesExp is \u220603.\nProof. From Corollary 6.10, Corollary 6.24, and Corollary 6.25.\nHowever, we do not know BayesExp to be limit computable, and we expect it not to be. However, we can approximate it using \u03b5-optimal policies preserving weak asymptotic optimality.\nTheorem 6.27 (A Limit-Computable Weakly Asymptotically Optimal Agent). If there is a nonincreasing computable sequence of positive reals (\u03b5t)t\u2208N such that \u03b5t \u2192 0 and Ht(\u03b5t)/(t\u03b5t) \u2192 0 as t \u2192 \u221e, then there is a limit-computable policy that is weakly asymptotically optimal in the class of all computable stochastic environments.\nProof. By Corollary 6.10, there is a limit-computable 2\u2212t-optimal reward-seeking policy \u03c0t\u03be for the universal mixture \u03be. By Corollary 6.25 there are limit-computable t/2optimal information-seeking policies \u03c0tIG with horizon t + Ht(\u03b5t). We define a policy \u03c0 analogously to Algorithm 1 with \u03c0tIG and \u03c0 t \u03be instead of the optimal policies. From Corollary 6.24 we get that V \u2217IG is limit computable, so the policy \u03c0 is limit computable. Furthermore, \u03c0t\u03be is 2 \u2212t-optimal and 2\u2212t \u2192 0, so V \u03c0t\u03be \u03be (\u00e6<t)\u2192 V \u2217 \u03be (\u00e6<t) as t\u2192\u221e.\nNow we can proceed analogously to the proof of Lattimore (2013, Thm. 5.6), which consists of three parts. First, it is shown that the value of the \u03be-optimal rewardseeking policy \u03c0\u2217\u03be converges to the optimal value for exploitation time steps (line 6 in Algorithm 1) in the sense that V \u03c0\u2217\u03be \u00b5 \u2192 V \u2217\u00b5 . This carries over to the 2\u2212t-optimal policy \u03c0t\u03be, since the key property is that on exploitation steps, V \u2217\nIG < \u03b5t; i.e., \u03c0 only exploits if potential knowledge-seeking value is low. In short, we get for exploitation steps\nV \u03c0t\u03be \u03be (\u00e6<t)\u2192 V \u03c0\u2217\u03be \u03be (\u00e6<t)\u2192 V \u03c0\u2217\u03be \u00b5 (\u00e6<t)\u2192 V \u2217\u00b5 (\u00e6<t) as t\u2192\u221e.\nSecond, it is shown that the density of exploration steps vanishes. This result carries over since the condition V \u2217IG(\u00e6<t) > \u03b5t that determines exploration steps is exactly the same as for BayesExp and \u03c0tIG is \u03b5t/2-optimal.\nThird, the results of part one and two are used to conclude that \u03c0 is weakly asymptotically optimal. This part carries over to our proof."}, {"heading": "6.7 Discussion", "text": "When using Solomonoff\u2019s prior for induction, we need to evaluate conditional probabilities. We showed that conditional M and Mnorm are limit computable (Theorem 6.3), and that M and Mnorm are not limit computable (Theorem 6.4 and Corollary 6.5). Table 6.1 on page 102 summarizes our computability results on various versions of Solomonoff\u2019s prior. Theses results implies that we can approximate M or Mnorm for prediction, but not the measure mixture M or Mnorm.\nIn some cases, normalized priors have advantages. As illustrated in Example 4.27, unnormalized priors can make the entropy-seeking agent mistake the entropy gained\n124 Computability\nfrom the probability assigned to finite strings for knowledge. From Mnorm \u2265M we get that Mnorm predicts just as well as M , and by Theorem 6.3 we can use Mnorm without losing limit computability.\nTable 6.2 on page 102 summarizes our computability results for the agents AINU, AIXI, and AINU. AINU is \u220603 and restricting to \u03b5-optimal policies decreases the level by one (Corollary 6.10 and Corollary 6.12). For environments from MCCMcomp, AIMU is limit-computable and \u03b5-optimal AIMU is computable (Corollary 6.14). In Section 6.3.2 we proved that these computability bounds on AINU are generally unimprovable (Theorem 6.16 and Theorem 6.17). Additionally, we proved weaker lower bounds for AIXI independent of the universal Turing machine (Theorem 6.15) and for \u03b5-optimal AIXI for specific choices of the universal Turing machine (Theorem 6.17).\nWhen the environment \u03bd has nonzero probability of not producing a new percept, the iterative definition of AINU (Definition 6.18) originally given by Hutter (2005, Def. 5.30) fails to maximize \u03bd-expected rewards (Proposition 6.19). Moreover, the policies are one level higher in the arithmetical hierarchy (see Table 6.4 on page 116). We proved upper (Corollary 6.21) and lower bounds (Theorem 6.22 and Theorem 6.23). The difference between the recursive value function V and the iterative value function W is readily exposed in the difference between the universal prior M and the measure mixture M : Just like W conditions on surviving forever, so does M eliminate the weight of programs that do not produce infinite strings. Both M and W are not limit computable for this reason.\nWe considered \u03b5-optimality to avoid having to determine argmax ties. This \u03b5 does not have to be constant over time, we may let \u03b5 \u2192 0 as t \u2192 \u221e at any computable rate. With this we retain the computability results of \u03b5-optimal policies and get that the value of the \u03b5(t)-optimal policy \u03c0\u03b5(t)\u03bd converges rapidly to the \u03bd-optimal value:\nV \u2217\u03bd (\u00e6<t)\u2212 V \u03c0 \u03b5(t) \u03bd \u03bd (\u00e6<t)\u2192 0 as t\u2192\u221e.\nIn Section 4.1 we defined the set MCCSLSC as the set of all lower semicomputable chronological contextual semimeasure over percepts with actions povided as side-information. When determining the probability of the next percept et in an environment \u03bd, we have to compute \u03bd(e1:t | e<t \u2016 a1:t). Alternatively, we could have defined the environment as a lower semicomputable mapping from histories \u00e6<tat to probabilities over the next percept et (this is done in Chapter 7). For the proof of Lemma 6.8 and Lemma 6.20 we only need that \u03bd(e1:t \u2016 a1:t) is lower semicomputable computable. While this new definition makes no difference for the computability of AINU, it matters for AIXI because in the mixture \u03be over all of these environments is no longer lower semicomputable.\nAny method that tries to tackle the reinforcement learning problem has to balance between exploration and exploitation. AIXI strikes this balance in the Bayesian way. However, as we showed in Section 5.2, this may not lead to enough exploration. To counteract this, we can add an explorative component to the agent, akin to knowledgeseeking agents. In Section 6.5 we show that \u03b5-optimal knowledge-seeking agents are limit computable if we use the recursive definition of the value function.\n\u00a76.7 Discussion 125\nWe set out with the goal of finding a perfect reinforcement learning agent that is limit computable. The Bayesian agent AIXI could be considered one suitable candidate, despite its optimality problems discussed in Chapter 5. Another suitable candidate are weakly asymptotically optimal agents, which in contrast to AIXI are optimal in an objective sense (see Section 5.6). We discussed BayesExp, which relies on a Solomonoff prior to learn its environment and on a information-seeking component for extra exploration. Our results culminated in a limit-computable weakly asymptotically optimal agent based on BayesExp (Theorem 6.27). In this sense our goal has been achieved.\n126 Computability\nChapter 7\nThe Grain of Truth Problem1\nAIs become friendly by playing lots of Newcomblike problems. \u2014 Eliezer Yudkowsky\nConsider the general setup of multiple reinforcement learning agents interacting sequentially in a known environment with the goal to maximize discounted reward.2 Each agent knows how the environment behaves, but does not know the other agents\u2019 behavior. The natural (Bayesian) approach would be to define a class of possible policies that the other agents could adopt and take a prior over this class. During the interaction, this prior gets updated to the posterior as our agent learns the others\u2019 behavior. Our agent then acts optimally with respect to this posterior belief. Kalai and Lehrer (1993) show that in infinitely repeated games Bayesian agents converge to an \u03b5-Nash equilibrium as long as each agent assigns positive prior probability to the other agents\u2019 policies (a grain of truth).\nAs an example, consider an infinitely repeated prisoners dilemma between two agents. In every time step the payoff matrix is as follows, where C means cooperate and D means defect.\nC D C 3/4, 3/4 0, 1 D 1, 0 1/4, 1/4\nDefine the set of policies \u03a0 := {\u03c0\u221e, \u03c00, \u03c01, . . .} where policy \u03c0t cooperates until time step t or the opponent defects (whatever happens first) and defects thereafter. The Bayes optimal behavior is to cooperate until the posterior belief that the other agent defects in the time step after the next is greater than some constant (depending on the discount function) and then defect afterwards. Therefore Bayes optimal behavior leads to a policy from the set \u03a0 (regardless of the prior). If both agents are Bayes optimal with respect to some prior, they both have a grain of truth and therefore they converge to a Nash equilibrium: either they both cooperate forever or after some finite time they both defect forever. Alternating strategies like TitForTat (cooperate first, then play\n1The idea for reflective oracles was developed by Jessica Taylor and Benya Fallenstein based on ideas by Paul Christiano (Christiano et al., 2013). Reflective oracles were first described in Fallenstein et al. (2015a). The proof of Theorem 7.7 was sketched by Benya Fallenstein and developed by me. Except for minor editing, everything else in this chapter is my own work.\n2We mostly use the terminology of reinforcement learning. For readers from game theory we provide a dictionary in Table 7.1.\n127\nthe opponent\u2019s last action) are not part of the policy class \u03a0, and adding them to the class breaks the grain of truth property: the Bayes optimal behavior is no longer in the class. This is rather typical; a Bayesian agent usually needs to be more powerful than its environment (see Section 6.3). We are facing the following problem.\nProblem 7.1 (Grain of Truth Problem; Hutter, 2009b, Q. 5j). Find a large class of policies \u03a0 containing Bayesian agents with positive prior over \u03a0.\nUntil now, classes that admit a grain of truth were known only for small toy examples such as the iterated prisoner\u2019s dilemma above (Shoham and Leyton-Brown, 2009, Ch. 7.3). Foster and Young (2001) and Nachbar (1997, 2005) prove several impossibility results on the grain of truth problem that identify properties that cannot be simultaneously satisfied for classes that allow a grain of truth (see Section 7.6 for a discussion).\nIn this chapter we present a formal solution to the grain of truth problem (Section 7.2). We assume that our multi-agent environment is computable, but it does not need to be stationary/Markov, ergodic, or finite-state. Our class of policies \u03a0 is large enough to contain all computable (stochastic) policies, as well as all relevant Bayes optimal policies. At the same time, our class is small enough to be limit computable. This is important because it allows our result to be computationally approximated.\nIn Section 7.3 we consider the setting where the multi-agent environment is unknown to the agents and has to be learned in addition to the other agents\u2019 behavior. A Bayes optimal agent may not learn to act optimally in unknown multi-agent environments even though it has a grain of truth. This effect occurs in non-recoverable environments where taking one wrong action can mean a permanent loss of future value. In this case, a Bayes optimal agent avoids taking these dangerous actions and therefore will not explore enough to wash out the prior\u2019s bias (using the dogmatic prior from Section 5.2.2). Therefore, Bayesian agents are not asymptotically optimal , i.e., they do not always learn to act optimally (Theorem 5.22).\nHowever, asymptotic optimality is achieved by Thompson sampling because the inherent randomness of Thompson sampling leads to enough exploration to learn the entire environment class (see Section 5.4.3). This leads to our main result: if all agents use Thompson sampling over our class of multi-agent environments, then for every \u03b5 > 0\n\u00a77.1 Reflective Oracles 129\nthey converge to an \u03b5-Nash equilibrium. This is not the first time Thompson sampling is used in game theory (Ortega and Braun, 2014), but the first time to show that it achieves such general positive results.\nThe central idea to our construction is based on reflective oracles introduced by Fallenstein et al. (2015a,b). Reflective oracles are probabilistic oracles similar to halting oracles that answer whether the probability that a given probabilistic Turing machine T outputs 1 is higher than a given rational number p. The oracles are reflective in the sense that the machine T may itself query the oracle, so the oracle has to answer queries about itself. This invites issues caused by self-referential liar paradoxes of the form \u201cif the oracle says that this machine return 1 with probability > 1/2, then return 0, else return 1.\u201d Reflective oracles avoid these issues by being allowed to randomize if the machines do not halt or the rational number is exactly the probability to output 1. We introduce reflective oracles formally in Section 7.1 and prove that there is a limit computable reflective oracle.\nFor infinitely repeated games practical algorithms rely on ficticious play (Fudenberg and Levine, 1998, Ch. 2): the agent takes a best-response action based on the assumption that its opponent is playing a stationary but unknown mixed strategy estimated according to the observed empirical frequencies. If all agents converge to a stationary policy, then this is a Nash equilibrium. However, convergence is not guaranteed.\nThe same problem occurs in multi-agent reinforcement learning (Busoniu et al., 2008). Reinforcement learning algorithms typically assume a (stationary) Markov decision process. This assumption is violated when interacting with other reinforcement learning agents because as these agents learn, their behavior changes and thus they are not stationary. Assuming convergence to a stationary policy is a necessary criterion to enable all agents to learn, but the process is unstable for many reinforcement learning algorithms and only empirical positive results are known (Bowling and Veloso, 2001)."}, {"heading": "7.1 Reflective Oracles", "text": ""}, {"heading": "7.1.1 Definition", "text": "First we connect semimeasures as defined in Definition 2.14 to Turing machines. In Chapter 2 we used monotone Turing machines which naturally correspond to lower semicomputable semimeasures (Li and Vit\u00e1nyi, 2008, Sec. 4.5.2) that describe the distribution that arises when piping fair coin flips into the monotone machine. Here we take a different route.\nA probabilistic Turing machine is a Turing machine that has access to an unlimited number of uniformly random coin flips. Let T denote the set of all probabilistic Turing machines that take some input in X \u2217 and may query an oracle (formally defined below). We take a Turing machine T \u2208 T to correspond to a semimeasure \u03bbT where \u03bbT (a | x) is the probability that T outputs a \u2208 X when given x \u2208 X \u2217 as input. The value of\n130 The Grain of Truth Problem\n\u03bbT (x) is then given by the chain rule\n\u03bbT (x) := |x|\u220f k=1 \u03bbT (xk | x<k). (7.1)\nThus T gives rise to the set of semimeasuresMLSC where the conditionals \u03bb(a | x) are lower semicomputable. In contrast, in Chapter 6 we considered semimeasures whose joint probability (7.1) is lower semicomputable. This set MLSC contains all computable measures. However, MLSC is a proper subset of the set of all lower semicomputable semimeasures because the product (7.1) is lower semicomputable, but there are some lower semicomputable semimeasures whose conditional is not lower semicomputable (Theorem 6.6):\nMCCMcomp \u2282MLSC \u2282MCCSLSC\nIn the following we assume that our alphabet is binary, i.e., X := {0, 1}.\nDefinition 7.2 (Oracle). An oracle is a function O : T \u00d7 {0, 1}\u2217 \u00d7Q\u2192 \u2206{0, 1}.\nOracles are understood to be probabilistic: they randomly return 0 or 1. Let TO denote the machine T \u2208 T when run with the oracle O, and let \u03bbOT denote the semimeasure induced by TO. This means that drawing from \u03bbOT involves two sources of randomness: one from the distribution induced by the probabilistic Turing machine T and one from the oracle\u2019s answers.\nThe intended semantics of an oracle are that it takes a query (T, x, p) and returns 1 if the machine TO outputs 1 on input x with probability greater than p when run with the oracle O, i.e., when \u03bbOT (1 | x) > p. Furthermore, the oracle returns 0 if the machine TO outputs 1 on input x with probability less than p when run with the oracle O, i.e., when \u03bbOT (1 | x) < p. To fulfill this, the oracle O has to make statements about itself, since the machine T from the query may again query O. Therefore we call oracles of this kind reflective oracles. This has to be defined very carefully to avoid the obvious diagonalization issues that are caused by programs that ask the oracle about themselves. We impose the following self-consistency constraint.\nDefinition 7.3 (Reflective Oracle). An oracle O is reflective iff for all queries (T, x, p) \u2208 T \u00d7 {0, 1}\u2217 \u00d7Q,\n(a) \u03bbOT (1 | x) > p implies O(T, x, p) = 1, and\n(b) \u03bbOT (0 | x) > 1\u2212 p implies O(T, x, p) = 0.\nIf p under- or overshoots the true probability of \u03bbOT ( \u00b7 | x), then the oracle must reveal this information. However, in the critical case when p = \u03bbOT (1 | x), the oracle is allowed to return anything and may randomize its result. Furthermore, since T might not output any symbol, it is possible that \u03bbOT (0 | x) + \u03bbOT (1 | x) < 1. In this case the oracle can reassign the non-halting probability mass to 0, 1, or randomize; see Figure 7.1.\nExample 7.4 (Reflective Oracles and Diagonalization). Let T \u2208 T be a probabilistic Turing machine that outputs 1 \u2212 O(T, , 1/2) (T can know its own source code by quining; Kleene, 1952, Thm. 27). In other words, T queries the oracle about whether it is more likely to output 1 or 0, and then does whichever the oracle says is less likely. In this case we can use an oracle O(T, , 1/2) := 1/2 (answer 0 or 1 with equal probability), which implies \u03bbOT (1 | ) = \u03bbOT (0 | ) = 1/2, so the conditions of Definition 7.3 are satisfied. In fact, for this machine T we must have O(T, , 1/2) = 1/2 for all reflective oracles O. 3\nThe following theorem establishes that reflective oracles exist.\nTheorem 7.5 (Fallenstein et al., 2015c, App. B). There is a reflective oracle.\nDefinition 7.6 (Reflective-Oracle-Computable). A semimeasure is called reflectiveoracle-computable iff it is computable on a probabilistic Turing machine with access to a reflective oracle.\nFor any probabilistic Turing machine T \u2208 T we can complete the semimeasure \u03bbOT ( \u00b7 | x) into a reflective-oracle-computable measure \u03bb O T ( \u00b7 | x): Using the oracle O and a binary search on the parameter p we search for the crossover point p where O(T, x, p) goes from returning 1 to returning 0. The limit point p\u2217x \u2208 R of the binary search is random since the oracle\u2019s answers may be random. But the main point is that the expectation of p\u2217x exists, so \u03bb O T (1 | x) = E[p\u2217x] = 1\u2212 \u03bb O T (0 | x). Hence \u03bb O T is a measure. Moreover, if the oracle is reflective, then \u03bbOT (x) \u2265 \u03bbOT (x) for all x \u2208 X \u2217. In this sense the oracle O can be viewed as a way of \u2018completing\u2019 all semimeasures \u03bbOT to measures by arbitrarily assigning the non-halting probability mass. If the oracle O is reflective this is consistent in the sense that Turing machines who run other Turing machines will be completed in the same way. This is especially important for a universal machine that runs all other Turing machines to induce a Solomonoff prior (Example 3.5)."}, {"heading": "7.1.2 A Limit Computable Reflective Oracle", "text": "The proof of Theorem 7.5 given by Fallenstein et al. (2015c, App. B) is nonconstructive and uses the axiom of choice. In Section 7.1.3 we give a new proof for the existence of reflective oracles and provide a construction that there is a reflective oracle that is limit computable.\n132 The Grain of Truth Problem\nTheorem 7.7 (A Limit Computable Reflective Oracle). There is a reflective oracle that is limit computable.\nThis theorem has the immediate consequence that reflective oracles cannot be used as halting oracles. At first, this result may seem surprising: according to the definition of reflective oracles, they make concrete statements about the output of probabilistic Turing machines. However, the fact that the oracles may randomize some of the time actually removes enough information such that halting can no longer be decided from the oracle output.\nCorollary 7.8 (Reflective Oracles are not Halting Oracles). There is no probabilistic Turing machine T such that for every prefix program p and every reflective oracle O, we have that \u03bbOT (1 | p) > 1/2 if p halts and \u03bbOT (1 | p) < 1/2 otherwise.\nProof. Assume there was such a machine T and let O be the limit computable oracle from Theorem 7.7. Since O is reflective we can turn T into a deterministic halting oracle by calling O(T, p, 1/2) which deterministically returns 1 if p halts and 0 otherwise. Since O is limit computable, we can finitely compute the output of O on any query to arbitrary finite precision using our deterministic halting oracle. We construct a probabilistic Turing machine T \u2032 that uses our halting oracle to compute (rather than query) the oracle O on (T \u2032, , 1/2) to a precision of 1/3 in finite time. If O(T \u2032, , 1/2)\u00b1 1/3 > 1/2, the machine T \u2032 outputs 0, otherwise T \u2032 outputs 1. Since our halting oracle is entirely deterministic, the output of T \u2032 is entirely deterministic as well (and T \u2032 always halts), so \u03bbOT \u2032(0 | ) = 1 or \u03bbOT \u2032(1 | ) = 1. Therefore O(T \u2032, , 1/2) = 1 or O(T \u2032, , 1/2) = 0 because O is reflective. A precision of 1/3 is enough to tell them apart, hence T \u2032 returns 0 if O(T \u2032, , 1/2) = 1 and T \u2032 returns 1 if O(T \u2032, , 1/2) = 0. This is a contradiction.\nA similar argument can also be used to show that reflective oracles are not computable."}, {"heading": "7.1.3 Proof of Theorem 7.7", "text": "The idea for the proof of Theorem 7.7 is to construct an algorithm that outputs an infinite series of partial oracles converging to a reflective oracle in the limit.\nThe set of queries is countable, so we can assume that we have some computable enumeration of it:\nT \u00d7 {0, 1}\u2217 \u00d7Q =: {q1, q2, . . .}\nDefinition 7.9 (k-Partial Oracle). A k-partial oracle O\u0303 is function from the first k queries to the multiples of 2\u2212k in [0, 1]:\nO\u0303 : {q1, q2, . . . , qk} \u2192 {n2\u2212k | 0 \u2264 n \u2264 2k}\nDefinition 7.10 (Approximating an Oracle). A k-partial oracle O\u0303 approximates an oracle O iff |O(qi)\u2212 O\u0303(qi)| \u2264 2\u2212k\u22121 for all i \u2264 k.\n\u00a77.1 Reflective Oracles 133\nLet k \u2208 N, let O\u0303 be a k-partial oracle, and let T \u2208 T be an oracle machine. The machine T O\u0303 that we get when we run T with the k-partial oracle O\u0303 is defined as follows (this is with slight abuse of notation since k is taken to be understood implicitly).\n1. Run T for at most k steps.\n2. If T calls the oracle on qi for i \u2264 k,\n(a) return 1 with probability O\u0303(qi)\u2212 2\u2212k\u22121, (b) return 0 with probability 1\u2212 O\u0303(qi)\u2212 2\u2212k\u22121, and (c) halt otherwise.\n3. If T calls the oracle on qj for j > k, halt.\nFurthermore, we define \u03bbO\u0303T analogously to \u03bb O T as the distribution generated by the machine T O\u0303.\nLemma 7.11. If a k-partial oracle O\u0303 approximates a reflective oracle O, then \u03bbOT (1 | x) \u2265 \u03bbO\u0303T (1 | x) and \u03bbOT (0 | x) \u2265 \u03bbO\u0303T (0 | x) for all x \u2208 {0, 1}\u2217 and all T \u2208 T .\nProof. This follows from the definition of T O\u0303: when running T with O\u0303 instead of O, we can only lose probability mass. If T makes calls whose index is > k or runs for more than k steps, then the execution is aborted and no further output is generated. If T makes calls whose index i \u2264 k, then O\u0303(qi) \u2212 2\u2212k\u22121 \u2264 O(qi) since O\u0303 approximates O. Therefore the return of the call qi is underestimated as well.\nDefinition 7.12 (k-Partially Reflective). A k-partial oracle O\u0303 is k-partially reflective iff for the first k queries (T, x, p)\n\u2022 \u03bbO\u0303T (1 | x) > p implies O\u0303(T, x, p) = 1, and\n\u2022 \u03bbO\u0303T (0 | x) > 1\u2212 p implies O\u0303(T, x, p) = 0.\nIt is important to note that we can check whether a k-partial oracle is k-partially reflective in finite time by running all machines T from the first k queries for k steps and tallying up the probabilities to compute \u03bbO\u0303T .\nLemma 7.13. If O is a reflective oracle and O\u0303 is a k-partial oracle that approximates O, then O\u0303 is k-partially reflective.\nLemma 7.13 only holds because we use semimeasures whose conditionals are lower semicomputable.\nProof. Assuming \u03bbO\u0303T (1 | x) > p we get from Lemma 7.11 that \u03bbOT (1 | x) \u2265 \u03bbO\u0303T (1 | x) > p. Thus O(T, x, p) = 1 because O is reflective. Since O\u0303 approximates O, we get 1 = O(T, x, p) \u2264 O\u0303(T, x, p) + 2\u2212k\u22121, and since O\u0303 assigns values in a 2\u2212k-grid, it follows that O\u0303(T, x, p) = 1. The second implication is proved analogously.\n134 The Grain of Truth Problem\nDefinition 7.14 (Extending Partial Oracles). A k + 1-partial oracle O\u0303\u2032 extends a kpartial oracle O\u0303 iff |O\u0303(qi)\u2212 O\u0303\u2032(qi)| \u2264 2\u2212k\u22121 for all i \u2264 k.\nLemma 7.15. There is an infinite sequence of partial oracles (O\u0303k)k\u2208N such that for each k, O\u0303k is a k-partially reflective k-partial oracle and O\u0303k+1 extends O\u0303k.\nProof. By Theorem 7.5 there is a reflective oracle O. For every k, there is a canonical k-partial oracle O\u0303k that approximates O: restrict O to the first k queries and for any such query q pick the value in the 2\u2212k-grid which is closest to O(q). By construction, O\u0303k+1 extends O\u0303k and by Lemma 7.13, each O\u0303k is k-partially reflective.\nLemma 7.16. If the k + 1-partial oracle O\u0303k+1 extends the k-partial oracle O\u0303k, then \u03bb O\u0303k+1 T (1 | x) \u2265 \u03bb O\u0303k T (1 | x) and \u03bb O\u0303k+1 T (0 | x) \u2265 \u03bb O\u0303k T (0 | x) for all x \u2208 {0, 1}\u2217 and all T \u2208 T .\nProof. T O\u0303k+1 runs for one more step than T O\u0303k , can answer one more query and has increased oracle precision. Moreover, since O\u0303k+1 extends O\u0303k, we have |O\u0303k+1(qi) \u2212 O\u0303k(qi)| \u2264 2\u2212k\u22121, and thus O\u0303k+1(qi)\u2212 2\u2212k\u22121 \u2265 O\u0303k(qi)\u2212 2\u2212k. Therefore the success to answers to the oracle calls (case 2(a) and 2(b)) will not decrease in probability.\nNow everything is in place to state the algorithm that constructs a reflective oracle in the limit. It recursively traverses a tree of partial oracles. The tree\u2019s nodes are the partial oracles; level k of the tree contains all k-partial oracles. There is an edge in the tree from the k-partial oracle O\u0303k to the i-partial oracle O\u0303i if and only if i = k + 1 and O\u0303i extends O\u0303k.\nFor every k, there are only finitely many k-partial oracles, since they are functions from finite sets to finite sets. In particular, there are exactly two 1-partial oracles (so the search tree has two roots). Pick one of them to start with, and proceed recursively as follows. Given a k-partial oracle O\u0303k, there are finitely many (k + 1)-partial oracles that extend O\u0303k (finite branching of the tree). Pick one that is (k+1)-partially reflective (which can be checked in finite time). If there is no (k+1)-partially reflective extension, backtrack.\nBy Lemma 7.15 our search tree is infinitely deep and thus the tree search does not terminate. Moreover, it can backtrack to each level only a finite number of times because at each level there is only a finite number of possible extensions. Therefore the algorithm will produce an infinite sequence of partial oracles, each extending the previous. Because of finite backtracking, the output eventually stabilizes on a sequence of partial oracles O\u03031, O\u03032, . . .. By the following lemma, this sequence converges to a reflective oracle, which concludes the proof of Theorem 7.7.\nLemma 7.17. Let O\u03031, O\u03032, . . . be a sequence where O\u0303k is a k-partially reflective k-partial oracle and O\u0303k+1 extends O\u0303k for all k \u2208 N. Let O := limk\u2192\u221e O\u0303k be the pointwise limit. Then\n(a) \u03bbO\u0303kT (1 | x) \u2192 \u03bbOT (1 | x) and \u03bb O\u0303k T (0 | x) \u2192 \u03bbOT (0 | x) as k \u2192 \u221e for all x \u2208 {0, 1}\u2217\nand all T \u2208 T , and\n\u00a77.2 A Grain of Truth 135\n(b) O is a reflective oracle.\nProof. First note that the pointwise limit must exists because |O\u0303k(qi) \u2212 O\u0303k+1(qi)| \u2264 2\u2212k\u22121 by Definition 7.14.\n(a) Since O\u0303k+1 extends O\u0303k, each O\u0303k approximates O. Let x \u2208 {0, 1}\u2217 and T \u2208 T and consider the sequence ak := \u03bb O\u0303k T (1 | x) for k \u2208 N. By Lemma 7.16, ak \u2264 ak+1, so the\nsequence is monotone increasing. By Lemma 7.11, ak \u2264 \u03bbOT (1 | x), so the sequence is bounded. Therefore it must converge. But it cannot converge to anything strictly below \u03bbOT (1 | x) by the definition of TO.\n(b) By definition, O is an oracle; it remains to show that O is reflective. Let qi = (T, x, p) be some query. If p < \u03bbOT (1 | x), then by (a) there is a k large enough such that p < \u03bbO\u0303tT (1 | x) for all t \u2265 k. For any t \u2265 max{k, i}, we have O\u0303t(T, x, p) = 1 since O\u0303t is t-partially reflective. Therefore 1 = limk\u2192\u221e O\u0303k(T, x, p) = O(T, x, p). The case 1\u2212 p < \u03bbOT (0 | x) is analogous."}, {"heading": "7.2 A Grain of Truth", "text": ""}, {"heading": "7.2.1 Reflective Bayesian Agents", "text": "Fix O to be a reflective oracle. From now on, we assume that the action space A := {\u03b1, \u03b2} is binary. We can treat computable measures over binary strings as environments: the environment \u03bd corresponding to a probabilistic Turing machine T \u2208 T is defined by\n\u03bd(et | \u00e6<tat) := \u03bb O T (y | x) = k\u220f i=1 \u03bb O T (yi | xy1 . . . yi\u22121)\nwhere y1:k is a binary encoding of et and x is a binary encoding of \u00e6<tat. The actions a1:\u221e are only contextual, and not part of the environment distribution. We define \u03bd(e<t \u2016 a<t) analogously to (4.1).\nLet T1, T2, . . . be an enumeration of all probabilistic Turing machines in T that use an oracle. We define the class of reflective environments\nMOrefl := { \u03bb O T1 , \u03bb O T2 , . . . } .\nThis is the class of all environments computable on a probabilistic Turing machine with reflective oracle O, that have been completed from semimeasures to measures using O.\nAnalogously to Section 4.3.1, we define a Bayesian mixture over the class MOrefl. Let w \u2208 \u2206MOrefl be a lower semicomputable prior probability distribution on MOrefl. Possible choices for the prior include the Solomonoff prior w ( \u03bb O T ) := 2\u2212K(T ), where K(T ) denotes the length of the shortest input to some universal Turing machine that encodes T . We define the corresponding Bayesian mixture\n\u03be(et | \u00e6<tat) := \u2211\n\u03bd\u2208MOrefl\nw(\u03bd | \u00e6<t)\u03bd(et | \u00e6<tat) (7.2)\n136 The Grain of Truth Problem\nwhere w(\u03bd | \u00e6<t) is the (renomalized) posterior,\nw(\u03bd | \u00e6<t) := w(\u03bd) \u03bd(e<t \u2016 a<t) \u03be(e<t \u2016 a<t) . (7.3)\nThe mixture \u03be is lower semicomputable on an oracle Turing machine because the posterior w( \u00b7 | \u00e6<t) is lower semicomputable. Hence there is an oracle machine T such that \u03be = \u03bbOT . We define its completion \u03be := \u03bb O T as the completion of \u03bbOT . This is the distribution that is used to compute the posterior. There are no cyclic dependencies since \u03be is called on the shorter history \u00e6<t. We arrive at the following statement.\nProposition 7.18 (Bayes is in the Class). \u03be \u2208MOrefl.\nMoreover, since O is reflective, we have that \u03be dominates all environments \u03bd \u2208MOrefl:\n\u03be(e1:t \u2016 a1:t) = \u03be(et | \u00e6<tat)\u03be(e<t \u2016 a<t) \u2265 \u03be(et | \u00e6<tat)\u03be(e<t | a<t)\n= \u03be(e<t \u2016 a<t) \u2211\n\u03bd\u2208MOrefl\nw(\u03bd | \u00e6<t)\u03bd(et | \u00e6<tat)\n= \u03be(e<t \u2016 a<t) \u2211\n\u03bd\u2208MOrefl\nw(\u03bd) \u03bd(e<t \u2016 a<t) \u03be(e<t \u2016 a<t) \u03bd(et | \u00e6<tat)\n= \u2211\n\u03bd\u2208MOrefl\nw(\u03bd)\u03bd(e1:t \u2016 a1:t)\n\u2265 w(\u03bd)\u03bd(e1:t \u2016 a1:t)\nTherefore we get on-policy value convergence according to Corollary 4.20: for all \u00b5 \u2208MOrefl and all policies \u03c0\nV \u03c0 \u03be (\u00e6<t)\u2212 V \u03c0\u00b5 (\u00e6<t)\u2192 0 as t\u2192\u221e \u00b5\u03c0-almost surely. (7.4)"}, {"heading": "7.2.2 Reflective-Oracle-Computable Policies", "text": "This subsection is dedicated to the following result that was previously stated by Fallenstein et al. (2015a, Alg. 6) but not proved. It contrasts results on arbitrary semicomputable environments where optimal policies are not limit computable (see Section 6.3).\nTheorem 7.19 (Optimal Policies are Oracle Computable). For every \u03bd \u2208MOrefl, there is a \u03bd-optimal (stochastic) policy \u03c0\u2217\u03bd that is reflective-oracle-computable.\nNote that even though deterministic optimal policies always exist, those policies are typically not reflective-oracle-computable.\nTo prove Theorem 7.19 we need the following lemma.\nLemma 7.20 (Reflective-Oracle-Computable Optimal Value Function). For every environment \u03bd \u2208MOrefl the optimal value function V \u2217\u03bd is reflective-oracle-computable.\n\u00a77.3 Multi-Agent Environments 137\nProof. This proof follows the proof of Corollary 6.14. We write the optimal value explicitly as in (4.2). For a fixed m, all involved quantities are reflective-oracle-computable. Moreover, this quantity is monotone increasing in m and the tail sum from m+ 1 to\u221e is bounded by \u0393m+1 which is computable according to Assumption 4.6a and converges to 0 as m\u2192\u221e. Therefore we can enumerate all rationals above and below V \u2217\u03bd .\nProof of Theorem 7.19. According to Lemma 7.20 the optimal value function V \u2217\u03bd is reflective-oracle-computable. Hence there is a probabilistic Turing machine T such that\n\u03bbOT (1 | \u00e6<t) = ( V \u2217\u03bd (\u00e6<t\u03b1)\u2212 V \u2217\u03bd (\u00e6<t\u03b2) + 1 ) /2.\nWe define the policy\n\u03c0(\u00e6<t) := { \u03b1 if O(T,\u00e6<t, 1/2) = 1, and \u03b2 if O(T,\u00e6<t, 1/2) = 0\nThis policy is stochastic because the answer of the oracle O is stochastic. It remains to show that \u03c0 is a \u03bd-optimal policy. If V \u2217\u03bd (\u00e6<t\u03b1) > V \u2217\u03bd (\u00e6<t\u03b2), then \u03bbOT (1 | \u00e6<t) > 1/2, thus O(T,\u00e6<t, 1/2) = 1 since O is reflective, and hence \u03c0 takes action \u03b1. Conversely, if V \u2217\u03bd (\u00e6<t\u03b1) < V \u2217\u03bd (\u00e6<t\u03b2), then \u03bbOT (1 | \u00e6<t) < 1/2, thus O(T,\u00e6<t, 1/2) = 0 since O is reflective, and hence \u03c0 takes action \u03b2. Lastly, if V \u2217\u03bd (\u00e6<t\u03b1) = V \u2217\u03bd (\u00e6<t\u03b2), then both actions are optimal and thus it does not matter which action is returned by policy \u03c0. (This is the case where the oracle may randomize.)"}, {"heading": "7.2.3 Solution to the Grain of Truth Problem", "text": "Together, Proposition 7.18 and Theorem 7.19 provide the necessary ingredients to solve the grain of truth problem (Problem 7.1).\nCorollary 7.21 (Solution to the Grain of Truth Problem). For every lower semicomputable prior w \u2208 \u2206MOrefl the Bayes optimal policy \u03c0\u2217\u03be is reflective-oracle-computable where \u03be is the Bayes-mixture corresponding to w defined in (7.2).\nProof. From Proposition 7.18 and Theorem 7.19.\nHence the environment classMOrefl contains any reflective-oracle-computable modification of the Bayes optimal policy \u03c0\u2217\n\u03be . In particular, this includes computable multi-\nagent environments that contain other Bayesian agents over the class MOrefl. So any Bayesian agent over the classMOrefl has a grain of truth even though the environment may contain other Bayesian agents of equal power. We proceed to sketch the implications for multi-agent environments in the next section."}, {"heading": "7.3 Multi-Agent Environments", "text": "In a multi-agent environment there are n agents each taking sequential actions from the finite action space A. In each time step t = 1, 2, . . ., the environment receives action ait\n138 The Grain of Truth Problem\nfrom agent i and outputs n percepts e1t , . . . , ent \u2208 E , one for each agent. Each percept eit = (o i t, r i t) contains an observation oit and a reward rit \u2208 [0, 1]. Importantly, agent i only sees its own action ait and its own percept eit (see Figure 7.2). We use the shorthand notation at := (a1t , . . . , ant ) and et := (e1t , . . . , ent ) and denote \u00e6i<t = ai1ei1 . . . ait\u22121eit\u22121 and \u00e6<t = a1e1 . . . at\u22121et\u22121. Formally, multi-agent environments are defined as follows.\nDefinition 7.22 (Multi-Agent Environment). A multi-agent environment is a function\n\u03c3 : (An \u00d7 En)\u2217 \u00d7An \u2192 \u2206(En).\nTogether with the policies \u03c01, . . . , \u03c0n the multi-agent environment \u03c3 induces a history distribution \u03c3\u03c01:n where\n\u03c3\u03c01:n( ) : = 1\n\u03c3\u03c01:n(\u00e61:t) : = \u03c3\u03c01:n(\u00e6<tat)\u03c3(et | \u00e6<tat)\n\u03c3\u03c01:n(\u00e6<tat) : = \u03c3\u03c01:n(\u00e6<t) n\u220f i=1 \u03c0i(a i t | \u00e6i<t).\nAgent i acts in a subjective environment \u03c3i given by joining the multi-agent environment \u03c3 with the policies \u03c01, . . . , \u03c0n and marginalizing over the histories that \u03c0i does not see. Together with policy \u03c0i, the environment \u03c3i yields a distribution over the histories of agent i\n\u03c3\u03c0ii (\u00e6 i <t) := \u2211 \u00e6j<t,j 6=i \u03c3\u03c01:n(\u00e6<t).\nWe get the definition of the subjective environment \u03c3i with the identity \u03c3i(eit | \u00e6i<tait) := \u03c3\u03c0ii (e i t | \u00e6i<tait). The subjective environment \u03c3i depends on \u03c0i because other policies\u2019 actions may depend on the actions of \u03c0i. It is crucial to note that the subjective environment \u03c3i and the policy \u03c0i are ordinary environments and policies, so we can use the notation from Chapter 4.\n\u00a77.3 Multi-Agent Environments 139\nOur definition of a multi-agent environment is very general and encompasses most of game theory. It allows for cooperative, competitive, and mixed games; infinitely repeated games or any (infinite-length) extensive form games with finitely many players.\nExample 7.23 (Matching Pennies). In the game of matching pennies there are two agents (n = 2), and two actions A = {\u03b1, \u03b2} representing the two sides of a penny. In each time step agent 1 wins if the two actions are identical and agent 2 wins if the two actions are different. The payoff matrix is as follows.\n\u03b1 \u03b2\n\u03b1 1,0 0,1 \u03b2 0,1 1,0\nWe use E = {0, 1} to be the set of rewards (observations are vacuous) and define the multi-agent environment \u03c3 to give reward 1 to agent 1 iff a1t = a2t (0 otherwise) and reward 1 to agent 2 iff a1t 6= a2t (0 otherwise). Formally,\n\u03c3(r1t r 2 t | \u00e6<tat) :=  1 if r1t = 1, r2t = 0, a1t = a2t , 1 if r1t = 0, r2t = 1, a1t 6= a2t , and 0 otherwise.\nLet \u03c0\u03b1 denote the policy that always takes action \u03b1. If two agents each using policy \u03c0\u03b1 play matching pennies, agent 1 wins in every step. Formally, setting \u03c01 := \u03c02 := \u03c0\u03b1 we get a history distribution that assigns probability one to the history\n\u03b1\u03b110\u03b1\u03b110 . . . .\nThe subjective environment of agent 1 is\n\u03c31(r 1 t | \u00e61<ta1t ) =  1 if r1t = 1, a1t = \u03b1, 1 if r1t = 0, a1t = \u03b2, and 0 otherwise.\nTherefore policy \u03c0\u03b1 is optimal in agent 1\u2019s subjective environment. 3\nDefinition 7.24 (\u03b5-Best Response). A policy \u03c0i acting in multi-agent environment \u03c3 with policies \u03c01, . . . , \u03c0n is an \u03b5-best response after history \u00e6i<t iff\nV \u2217\u03c3i(\u00e6 i <t)\u2212 V \u03c0i\u03c3i (\u00e6 i <t) < \u03b5.\nIf at some time step t, all agents\u2019 policies are \u03b5-best responses, we have an \u03b5Nash equilibrium. The property of multi-agent systems that is analogous to asymptotic optimality is convergence to an \u03b5-Nash equilibrium.\n140 The Grain of Truth Problem"}, {"heading": "7.4 Informed Reflective Agents", "text": "Let \u03c3 be a multi-agent environment and let \u03c0\u2217\u03c31 , . . . \u03c0 \u2217 \u03c3n be such that for each i the policy \u03c0\u2217\u03c3i is an optimal policy in agent i\u2019s subjective environment \u03c3i. At first glance this seems ill-defined: The subjective environment \u03c3i depends on each policy \u03c0\u2217\u03c3j , which depends on the subjective environment \u03c3j , which in turn depends on the policy \u03c0\u2217\u03c3i . However, this circular definition actually has a well-defined solution.\nTheorem 7.25 (Optimal Multi-Agent Policies). For any reflective-oracle-computable multi-agent environment \u03c3, the optimal policies \u03c0\u2217\u03c31 , . . . , \u03c0 \u2217 \u03c3n exist and are reflectiveoracle-computable.\nTo prove Theorem 7.25, we need the following proposition.\nProposition 7.26 (Reflective-Oracle-Computability). If the multi-agent environment \u03c3 and the policies \u03c01, . . . , \u03c0n are reflective-oracle-computable, then \u03c3\u03c01:n and \u03c3\u03c0ii are reflective-oracle-computable, and \u03c3i \u2208MOrefl.\nProof. All involved quantities in the definition of \u03c3\u03c01:n are reflective-oracle-computable by assumption, therefore also their marginalizations \u03c3\u03c0ii and \u03c3i.\nProof of Theorem 7.25. According to Theorem 7.19 the optimal policy \u03c0\u2217\u03c3i in agent i\u2019s subjective environment is reflective-oracle-computable if the subjective environment \u03c3i is. In particular the process that takes \u03c3i in form of a probabilistic Turing machine and returns \u03c0\u2217\u03c3i is reflective-oracle-computable. Moreover, \u03c3i is reflective-oracle-computable if \u03c3 and \u03c01, . . . , \u03c0n are, according to Proposition 7.26. Again, this construction is itself reflective-oracle-computable. Connecting these two constructions we get probabilistic Turing machines T1, . . . , Tn \u2208 T where each Ti takes the multi-agent environment \u03c3 and \u03c01, . . . , \u03c0n in form of probabilistic Turing machines and returns \u03c0\u2217\u03c3i . We define the probabilistic Turing machines T \u20321, . . . , T \u2032n where T \u2032i runs T O i on (\u03c3, T \u2032 1, . . . , T \u2032 n); hence T \u2032i computes \u03c0\u2217\u03c3i . Note that this construction only works because we relied on the reflective oracle in the proof of Theorem 7.19. Since the machines TOi always halt, so do T \u2032 i O despite their infinitely recursive construction.\nNote the strength of Theorem 7.25: each of the policies \u03c0\u2217\u03c3i is acting optimally given the knowledge of everyone else\u2019s policies. Hence optimal policies play 0-best responses by definition, so if every agent is playing an optimal policy, we have a Nash equilibrium. Moreover, this Nash equilibrium is also a subgame perfect Nash equilibrium, because each agent also acts optimally on the counterfactual histories that do not end up being played. In other words, Theorem 7.25 states the existence and reflective-oracle-computability of a subgame perfect Nash equilibrium in any reflectiveoracle-computable multi-agent environment. The following immediate corollary states that these subgame perfect Nash equilibria are limit computable.\nCorollary 7.27 (Solution to Computable Multi-Agent Environments). For any computable multi-agent environment \u03c3, the optimal policies \u03c0\u2217\u03c31 , . . . , \u03c0 \u2217 \u03c3n exist and are limit computable.\n\u00a77.5 Learning Reflective Agents 141\nProof. From Theorem 7.25 and Theorem 7.7.\nExample 7.28 (Nash Equilibrium in Matching Pennies). Consider the matching pennies game from Example 7.23. The only pair of optimal policies is the pair of two uniformly random policies that play \u03b1 and \u03b2 with equal probability in every time step: if one of the agents picks a policy that plays one of the actions with probability > 1/2, then the other agent\u2019s best response is to play the other action with probability 1. But now the first agent\u2019s policy is no longer a best response. 3"}, {"heading": "7.5 Learning Reflective Agents", "text": "Since our classMOrefl solves the grain of truth problem, the result by Kalai and Lehrer (1993) immediately implies that for any Bayesian agents \u03c01, . . . , \u03c0n interacting in an infinitely repeated game and for all \u03b5 > 0 and all i \u2208 {1, . . . , n} there is almost surely a t0 \u2208 N such that for all t \u2265 t0 the policy \u03c0i is an \u03b5-best response. However, this hinges on the important fact that every agent has to know the game and also that all other agents are Bayesian agents. Otherwise the convergence to an \u03b5-Nash equilibrium may fail, as illustrated by the following example.\nAt the core of the construction is a dogmatic prior (Section 5.2.2). A dogmatic prior assigns very high probability to going to hell (reward 0 forever) if the agent deviates from a given computable policy \u03c0. For a Bayesian agent it is thus only worth deviating from the policy \u03c0 if the agent thinks that the prospects of following \u03c0 are very poor already. This implies that for general multi-agent environments and without additional assumptions on the prior, we cannot prove any meaningful convergence result about Bayesian agents acting in an unknown multi-agent environment.\nExample 7.29 (Reflective Bayesians Playing Matching Pennies). Consider the multiagent environment matching pennies from Example 7.23. Let \u03c01 be the policy that takes the action sequence (\u03b1\u03b1\u03b2)\u221e and let \u03c02 := \u03c0\u03b1 be the policy that always takes action \u03b1. The average reward of policy \u03c01 is 2/3 and the average reward of policy \u03c02 is 1/3. Let \u03be be a universal mixture (7.2). By on-policy value convergence (7.4), V \u03c01 \u03be \u2192 c1 \u2248 2/3 and V \u03c02\u03be \u2192 c2 \u2248 1/3 almost surely when following policies (\u03c01, \u03c02). Therefore there is an \u03b5 > 0 such that V \u03c01 \u03be > \u03b5 and V \u03c02 \u03be\n> \u03b5 for all time steps. Now we can apply Theorem 5.5 to conclude that there are (dogmatic) mixtures \u03be\u20321 and \u03be\u20322 such that \u03c0\u2217\u03be\u20321 always follows policy \u03c01 and \u03c0 \u2217 \u03be\u20322\nalways follows policy \u03c02. This does not converge to a (\u03b5-)Nash equilibrium. 3\nAn important property required for the construction in Example 7.29 is that the environment class contains environments that threaten the agent with going to hell, which is outside of the class of matching pennies environments. In other words, since the agent does not know a priori that it is playing a matching pennies game, it might behave more conservatively than appropriate for the game.\nThe following theorem is our main convergence result. It states that for asymptotically optimal agents we get convergence to \u03b5-Nash equilibria in any reflective-oraclecomputable multi-agent environment.\n142 The Grain of Truth Problem\nTheorem 7.30 (Convergence to Equilibrium). Let \u03c3 be an reflective-oracle-computable multi-agent environment and let \u03c01, . . . , \u03c0n be reflective-oracle-computable policies that are asymptotically optimal in mean in the class MOrefl. Then for all \u03b5 > 0 and all i \u2208 {1, . . . , n} the \u03c3\u03c01:n-probability that the policy \u03c0i is an \u03b5-best response converges to 1 as t\u2192\u221e.\nProof. Let i \u2208 {1, . . . , n}. By Proposition 7.26, the subjective environment \u03c3i is reflective-oracle-computable, therefore \u03c3i \u2208 MOrefl. Since \u03c0i is asymptotically optimal in mean in the class MOrefl, we get that E[V \u2217\u03c3i(\u00e6<t) \u2212 V \u03c0i \u03c3i (\u00e6<t)] \u2192 0. Convergence in mean implies convergence in probability for bounded random variables, hence for all \u03b5 > 0 we have\n\u03c3\u03c0ii [V \u2217 \u03c3i(\u00e6 i <t)\u2212 V \u03c0i\u03c3i (\u00e6 i <t) \u2265 \u03b5]\u2192 0 as t\u2192\u221e.\nTherefore the probability that the policy \u03c0i plays an \u03b5-best response converges to 1 as t\u2192\u221e.\nIn contrast to Theorem 7.25 which yields policies that play a subgame perfect equilibrium, this is not the case for Theorem 7.30: the agents typically do not learn to predict off-policy and thus will generally not play \u03b5-best responses in the counterfactual histories that they never see. This weaker form of equilibrium is unavoidable if the agents do not know the environment because it is impossible to learn the parts that they do not interact with.\nCorollary 7.31 (Convergence to Equilibrium). There are limit computable policies \u03c01, . . . , \u03c0n such that for any computable multi-agent environment \u03c3 and for all \u03b5 > 0 and all i \u2208 {1, . . . , n} the \u03c3\u03c01:n-probability that the policy \u03c0i is an \u03b5-best response converges to 1 as t\u2192\u221e.\nProof. Pick \u03c01, . . . , \u03c0n to be the Thompson sampling policy \u03c0T defined in Algorithm 2 over the countable classMOrefl. By Theorem 5.25 these policies are asymptotically optimal in mean. By Theorem 7.32 below they are reflective-oracle-computable and by Theorem 7.7 they are also limit computable. The statement now follows from Theorem 7.30.\nTheorem 7.32 (Thompson Sampling is Reflective-Oracle-Computable). The policy \u03c0T defined in Algorithm 2 over the classMOrefl is reflective-oracle-computable.\nProof. The posterior w( \u00b7 | \u00e6<t) is reflective-oracle-computable by the definition (7.3) and according to Theorem 7.19 the optimal policies \u03c0\u2217\u03bd are reflective-oracle-computable. On resampling steps we can compute the action probabilities of \u03c0T by enumerating all \u03bd \u2208MOrefl and computing \u03c0\u2217\u03bd weighted by the posterior w(\u03bd | \u00e6<t). Between resampling steps we need to condition the policy \u03c0T computed above by the actions it has already taken since the last resampling step (compare Example 5.28).\nBecause the posterior w( \u00b7 | \u00e6<t) is a \u03be \u03c0-martingale when acting according to the policy \u03c0, it converges \u03be\u03c0-almost surely according to the martingale convergence theorem (Theorem 2.8). Since \u03be dominates the subjective environment \u03c3i, it also converges\n\u00a77.6 Impossibility Results 143\n\u03c3\u03c0i -almost surely (see Example 3.20). Hence all the Thompson sampling agents eventually \u2018calm down\u2019 and settle on some posterior belief.\nAccording to Theorem 7.30, the policies \u03c01, . . . , \u03c0n only need to be asymptotically optimal in mean. For Thompson sampling this is independent of the discount function according to Theorem 5.25 (but the discount function has to be known to the agent). So the different agents may use different discount functions, resample at different time steps and converge at different speeds.\nExample 7.33 (Thompson Samplers Playing Matching Pennies). Consider the matching pennies game from Example 7.23 and let both agents use the Thompson sampling policy defined in Algorithm 2, i.e., define \u03c01 := \u03c0T and \u03c02 := \u03c0T .\nThe value of the uniformly random policy \u03c0R is always 1/2, so V \u2217\u03c3i \u2265 V \u03c0R \u03c3i = 1/2. According to Theorem 7.30, for every \u03b5 > 0, each agent will eventually always play an \u03b5-best response, i.e., V \u03c0i\u03c3i > V \u2217 \u03c3i \u2212 \u03b5 \u2265 1/2 \u2212 \u03b5. Since matching pennies is a zero-sum game, V \u03c01\u03c31 + V \u03c02 \u03c32 = 1, so V \u03c02 \u03c32 = 1\u2212 V \u03c01 \u03c31 < 1/2 + \u03b5.\nTherefore each agent will end up randomizing their actions; \u03c0i(at | \u00e6<t) \u2248 1/2 most of the time: If one of the agents (say agent 1) does not sufficiently randomize their actions in some time steps, then agent 2 could exploit this by picking a deterministic adversarial policy in those time steps. Suppose that this way it can gain a value of \u03b5 compared to the random policy \u03c0R, i.e., V \u03c02\u03c32 \u2265 V \u03c0R \u03c32 + \u03b5 = 1/2 + \u03b5. But this is a contradiction because then agent 1 is not playing an \u03b5-best response:\nV \u2217\u03c31 \u2212 V \u03c01 \u03c31 = V \u2217 \u03c31 \u2212 1 + V \u03c02 \u03c32 \u2265 1/2\u2212 1 + 1/2 + \u03b5 = \u03b5 3"}, {"heading": "7.6 Impossibility Results", "text": "Why does our solution to the grain of truth problem not violate the impossibility results from the literature? Assume we are playing an infinitely repeated game where in the stage game no agent has a weakly dominant action and the pure action maxmin reward is strictly less then the minmax reward. The impossibility result of Nachbar (1997, 2005) states that there is no class of policies \u03a0 such that the following are simultaneously satisfied.\n\u2022 Learnability. Each agent learns to predict the other agent\u2019s actions.\n\u2022 Caution and Symmetry. The set \u03a0 is closed under simple policy modifications such as renaming actions.\n\u2022 Purity. There is an \u03b5 > 0 such that for any stochastic policy \u03c0 \u2208 \u03a0 there is a deterministic policy \u03c0\u2032 \u2208 \u03a0 such that if \u03c0\u2032(\u00e6<t) = a, then \u03c0(a | \u00e6<t) > \u03b5.\n\u2022 Consistency. Each agent always has an \u03b5-best response available in \u03a0.\nIn order to converge to an \u03b5-Nash equilibrium, each agent has to have an \u03b5-best response available to them, so consistency is our target. Learnability is immediately satisfied for any environment in our class if we have a dominant prior according to Corollary 4.20.\n144 The Grain of Truth Problem\nFor MOrefl caution and symmetry are also satisfied since this set is closed under any computable modifications to policies. However, our classMOrefl avoids this impossibility result because it violates the purity condition: Let T1, T2, . . . be an enumeration of T . Consider the policy \u03c0 that maps history \u00e6i<t to the action 1\u2212O(Tt,\u00e6i<t, 1/2). If Tt is deterministic, then \u03c0 will take a different action than Tt for any history of length t\u2212 1. Therefore no deterministic reflective-oracle-computable policy can take an action that \u03c0 assigns positive probability to in every time step.\nFoster and Young (2001) present a condition that makes convergence to a Nash equilibrium impossible: if the player\u2019s rewards are perturbed by a small real number drawn from some continuous density \u03bd, then for \u03bd-almost all realizations the players do not learn to predict each other and do not converge to a Nash equilibrium. For example, in a matching pennies game, rational agents randomize only if the (subjective) values of both actions are exactly equal. But this happens only with \u03bd-probability zero, since \u03bd is a density. Thus with \u03bd-probability one the agents do not randomize. If the agents do not randomize, they either fail to learn to predict each other, or they are not acting rationally according to their beliefs: otherwise they would seize the opportunity to exploit the other player\u2019s deterministic action.\nBut this does not contradict our convergence result: the class MOrefl is countable and each \u03bd \u2208 MOrefl has positive prior probability. Perturbation of rewards with arbitrary real numbers is not possible. Even more, the argument given by Foster and Young (2001) cannot work in our setting: the Bayesian mixture \u03be mixes over \u03bbT for all probabilistic Turing machines T . For Turing machines T that sometimes do not halt, the oracle decides how to complete \u03bbT into a measure \u03bbT . Thus the oracle has enough influence on the exact values in the Bayesian mixture that the values of two actions in matching pennies can be made exactly equal."}, {"heading": "7.7 Discussion", "text": "This chapter introduced the class of all reflective-oracle-computable environmentsMOrefl. This class fully solves the grain of truth problem (Problem 7.1) because it contains (any computable modification of) Bayesian agents defined overMOrefl: the optimal agents and Bayes optimal agents over the class are all reflective-oracle-computable (Theorem 7.19 and Corollary 7.21).\nIf the environment is unknown, then a Bayesian agent may end up playing suboptimally (Example 7.29). However, if each agent uses a policy that is asymptotically optimal in mean (such as the Thompson sampling policy from Section 4.3.4) then for every \u03b5 > 0 the agents converge to an \u03b5-Nash equilibrium (Theorem 7.30 and Corollary 7.31).\nHowever, Corollary 7.31 does not imply that two Thompson sampling policies will converge to cooperation in an iterated prisoner\u2019s dilemma since always defecting is also a Nash equilibrium. The exact outcome will depend on the priors involved and the randomness of the policies.\nOur solution to the grain of truth problem is purely theoretical. However, Theorem 7.7 shows that our classMOrefl allows for computable approximations. This suggests\n\u00a77.7 Discussion 145\nthat practical approaches can be derived from this result, and reflective oracles have already seen applications in one-shot games (Fallenstein et al., 2015b).\n146 The Grain of Truth Problem\nChapter 8\nConclusion\nThe biggest existential risk is that future superintelligences stop simulating us. \u2014 Nick Bostrom\nToday computer programs exceeding humans in general intelligence are known only from science fiction. But research on AI has progressed steadily over the last decades and there is good reason to believe that we will be able to build HLAI eventually, and even sooner than most people think (M\u00fcller and Bostrom, 2016).\nThe advent of strong AI would be the biggest event in human history. Potential benefits are huge, as the new level of automation would free us from any kind of undesirable labor. But there is no reason to believe that humans are at the far end of the intelligence spectrum. Rather, humans barely cross the threshold for general intelligence to be able to use language and do science. Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines?\nThis could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.g., through self-amplification effects from AIs improving themselves (if doing AI research is one of humans\u2019 capabilities, then a machine that can do everything humans can do can also do AI research). Once machine intelligence is above or far above human level, machines would steer the course of history. There is no reason to believe that machines would be adversarial to us, but nevertheless humanity\u2019s fate might rest at the whims of the machines, just as chimpanzees today have no say in the large-scale events on this planet.\nBostrom (2002) defines:\nExistential risk \u2014 One where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential.\nExistential risks are events that have the power to extinguish human life as we know it. Examples are cosmic events such as an asteroid colliding with Earth. But cosmic events are unlikely on human timescales compared to human-made existential risks from nuclear weapons, synthetic biology, and nanotechnology.\nIt is possible that artificial intelligence also falls into this category. Vinge (1993) was the first person to recognize this:\n147\n148 Conclusion\nWithin thirty years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended.\nAfter Vinge, Yudkowsky (2001, 2008) can be regarded as one of the key people to popularize the potential dangers of AI technology. Bostrom (2003) picked up on this issue very early and gave the topic credibility through his well-researched and carefully written book Superintelligence (Bostrom, 2014). He argues that we need to solve the Control Problem\u2014the unique principal-agent problem that arises with the creation of strong AI (Bostrom, 2014, Ch. 9). In other words: How do we align strong AI with human values? How do we ensure that AI remains robust and beneficial? This research is collected under the umbrella term AI safety . Currently, we have no idea how to solve these problems even in theory.\nThrough Yudkowsky\u2019s and, more importantly, Bostrom\u2019s efforts, AI-related longterm safety concerns have now entered the mainstream media. In 2014 high-profile scientists such as Stephen Hawking, Max Tegmark, Stuart Russell, and Frank Wilczek have warned against the dangers posed by AI (Hawking et al., 2014). (See also Alexander (2015) for a collection of positions by prominent AI researchers.) Many scientists inside and outside the field have signed an open letter that research ensuring that AI systems remain robust and beneficial is both important and timely (Future of Life Institute, 2015c). This lead entrepreneur Elon Musk to donate $10 million to kick-start research in this field (Future of Life Institute, 2015a); most of this money has now been distributed across the planet to 37 different projects (Future of Life Institute, 2015b). Moreover, the Future of Life Institute and the Machine Intelligence Research Institute have formulated concrete technical research priorities to make AI more robust and beneficial (Soares and Fallenstein, 2014; Russell et al., 2015).\nAt the end of 2015 followed the announcement of OpenAI, a nonprofit organization with financial backing from several famous silicon valley billionaires (OpenAI, 2015):\nOpenAI is a non-profit artificial intelligence research company. Our goal is to advance digital intelligence in the way that is most likely to benefit humanity as a whole, unconstrained by a need to generate financial return.\nThe mission of OpenAI is to enable everyone to benefit from AI technology. But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea.\nDespite all of the recent efforts in AI safety research, critical voices within the AI community remain. Prominently, Davis (2014), Ng (2016), Walsh (2016), and Lawrence (2016) have proposed counterarguments that range from \u2018HLAI is so far away that any worry is misplaced\u2019 to claims that \u2018the safety problem would not be so hard\u2019. See Sotala and Yampolskiy (2014) for a discussion.\nIf AI poses an existential risk then a formal theory of strong AI is paramount to develop technical approaches to mitigate this risk. Which path will ultimately lead us to HLAI is in the realm of speculation at this time; therefore we should make as few and as weak assumptions as possible and abstract away from possible future implementation details.\n149\nThis thesis lays some of the groundwork for this endeavor. We built on top of Hutter\u2019s theory of universal artificial intelligence. Chapter 3 discussed the formal theory of learning. Chapter 4 presented several approaches to acting in unknown environments (Bayes, Thompson sampling, knowledge-seeking agents, and BayesExp). Chapter 5 analysed these approaches and discussed notions of optimality and principled problems with acting under uncertainty in general environment. Chapter 6 provided the mathematical tools to analyze the computational properties of these models. Finally, Chapter 7 solved the grain of truth problem, which lead to convergence to Nash equilibria in unknown general multi-agent environments.\nOur work is theoretical by nature and there is still a long way to go until these results make their way into applications. But a solution in principle is a crucial first step towards solving a problem in practice. Consider the research paper by Shannon (1950) on how to solve chess in principle. The algorithm he describes expands the full game tree of chess (until some specified depth), which is completely infeasible even with today\u2019s computation power. His contribution was to show that winning at chess is a feat that computers can achieve in principle, which was not universally accepted at the time. Even more, his approach already considered the correct ideas (minimax-search over the game tree) that ultimately lead to the defeat of the chess champion Garry Kasparov by the computer Deep Blue 46 years later (IBM, 2012a).\nThe theory of general reinforcement learning can serve and has served as a starting point for future investigation in AI safety. In particular, value learning (Dewey, 2011), self-reflection (Soares, 2015; Fallenstein et al., 2015b), self-modification (Orseau and Ring, 2011, 2012a; Everitt et al., 2016), interruptibility (Orseau and Armstrong, 2016; Armstrong and Orseau, 2016), decision theory (Everitt et al., 2015), memory manipulation (Orseau and Ring, 2012b), wireheading (Ring and Orseau, 2011; Everitt and Hutter, 2016), and questions of identity (Orseau, 2014b,c).\nIt is possible that HLAI is decades or centuries away. It might also be a few years around the corner. Whichever is the case, we are currently completely unprepared for the consequences. As an AI researcher, it is tempting to devote your life to increasing the capability of AI, advancing it domain after domain, and showing off with flashy demos and impressive victories over human contestants. But every technology incurs risks, and the more powerful the technology, the higher the risks. The potential power of AI technology is enormous, and correspondingly we need to consider the risks, take them seriously, and proceed to mitigate them.\n150 Conclusion\nAppendix A\nMeasures and Martingales\nIn this chapter we provide the proofs for Theorem 3.18 and Theorem 3.19.\nProof of Theorem 3.18. Xt is only undefined if P (\u0393v1:t) = 0. The set\n{v \u2208 \u03a3\u221e | \u2203t. P (\u0393v1:t) = 0}\nhas P -measure 0 and hence (Xt)t\u2208N is well-defined almost everywhere. Xt is constant on \u0393u for all u \u2208 \u03a3t, and Ft is generated by a collection of finitely many disjoint sets: \u03a3\u221e =\n\u228e u\u2208\u03a3t \u0393u.\n(a) Therefore Xt is Ft-measurable. (b) \u0393u = \u228e a\u2208\u03a3 \u0393ua for all u \u2208 \u03a3t and v \u2208 \u0393u, and therefore\nE[Xt+1 | Ft](v) = 1\nP (\u0393u) \u2211 a\u2208\u03a3 Xt+1(ua)P (\u0393ua) = 1 P (\u0393u) \u2211 a\u2208\u03a3 Q(\u0393ua) P (\u0393ua) P (\u0393ua)\n(\u2217) =\n1\nP (\u0393u) \u2211 a\u2208\u03a3 Q(\u0393ua) = Q(\u0393u) P (\u0393u) = Xt(v).\nAt (\u2217) we used the fact that P is locally absolutely continuous with respect to Q. (If P were not locally absolutely continuous with respect to Q, then there are cases where P (\u0393u) > 0, P (\u0393ua) = 0, and Q(\u0393ua) 6= 0. Therefore Xt+1(ua) does not contribute to the expectation and thus Xt+1(ua)P (\u0393ua) = 0 6= Q(\u0393ua).)\nP \u2265 0 and Q \u2265 0 by definition, thus Xt \u2265 0. Since P (\u0393 ) = Q(\u0393 ) = 1, we have E[X0] = 1.\nThe following lemma gives a convenient condition for the existence of a measure on (\u03a3\u03c9,F\u221e). It is a special case of the Daniell-Kolmogorov Extension Theorem (Rogers and Williams, 1994, Thm. 26.1).\nLemma A.1 (Extending measures). Let q : \u03a3\u2217 \u2192 [0, 1] be a function such that q( ) = 1 and \u2211 a\u2208\u03a3 q(ua) = q(u) for all u \u2208 \u03a3\u2217. Then there exists a unique probability measure Q on (\u03a3\u221e,F\u221e) such that q(u) = Q(\u0393u) for all u \u2208 \u03a3\u2217.\n151\n152 Measures and Martingales\nTo prove this lemma, we need the following two ingredients.\nDefinition A.2 (Semiring). A set R \u2286 2\u2126 is called semiring over \u2126 iff\n(a) \u2205 \u2208 R,\n(b) for all A,B \u2208 R, the set A \u2229B \u2208 R, and\n(c) for all A,B \u2208 R, there are pairwise disjoint sets C1, . . . , Cn \u2208 R such that A \\B =\u228en i=1Ci.\nTheorem A.3 (Carath\u00e9odory\u2019s Extension Theorem; Durrett, 2010, Thm. A.1.1). Let R be a semiring over \u2126 and let \u00b5 : R \u2192 [0, 1] be a function such that\n(a) \u00b5(\u2126) = 1 (normalization), (b) \u00b5( \u228en i=1Ai) = \u2211n i=1 \u00b5(Ai) for pairwise disjoint sets A1, . . . , An \u2208 R such that\u228en\ni=1Ai \u2208 R (finite additivity), and\n(c) \u00b5( \u22c3 i\u22650Ai) \u2264 \u2211 i\u22650 \u00b5(Ai) for any collection (Ai)i\u22650 such that each Ai \u2208 R and\u22c3\ni\u22650Ai \u2208 R (\u03c3-subadditivity).\nThen there is a unique extension \u00b5 of \u00b5 that is a probability measure on (\u2126, \u03c3(R)) such that \u00b5(A) = \u00b5(A) for all A \u2208 R.\nProof of Lemma A.1. We show the existence of Q using Carath\u00e9odory\u2019s Extension Theorem. Define R := {\u0393u | u \u2208 \u03a3\u2217} \u222a {\u2205}.\n(a) \u2205 \u2208 R.\n(b) For any \u0393u,\u0393v \u2208 R, either\n\u2022 u is a prefix of v and \u0393u \u2229 \u0393v = \u0393v \u2208 R, or\n\u2022 v is a prefix of u and \u0393u \u2229 \u0393v = \u0393u \u2208 R, or\n\u2022 \u0393u \u2229 \u0393v = \u2205 \u2208 R.\n(c) For any \u0393u,\u0393v \u2208 R,\n\u2022 \u0393u \\ \u0393v = \u228e w\u2208\u03a3|v|\u2212|u|\\{x} \u0393uw if v = ux, i.e., u is a prefix of v, and\n\u2022 \u0393u \\ \u0393v = \u2205 otherwise.\nTherefore R is a semiring. By definition of R, we have \u03c3(R) = F\u221e. The function q : \u03a3\u2217 \u2192 [0, 1] naturally gives rise to a function \u00b5 : R \u2192 [0, 1] with \u00b5(\u2205) := 0 and \u00b5(\u0393u) := q(u) for all u \u2208 \u03a3\u2217. We will now check the prerequisites of Carath\u00e9odory\u2019s Extension Theorem.\n(a) (Normalization.) \u00b5(\u03a3\u221e) = \u00b5(\u0393 ) = q( ) = 1.\n153\n(b) (Finite additivity.) Let \u0393u1 , . . . ,\u0393uk \u2208 R be pairwise disjoint sets such that \u0393w :=\u228ek i=1 \u0393ui \u2208 R. Let ` := max{|ui| | 1 \u2264 i \u2264 k}, then \u0393w = \u228e v\u2208\u03a3` \u0393wv. By\nassumption, \u2211 a\u2208\u03a3 q(ua) = q(u), thus \u2211\na\u2208\u03a3 \u00b5(\u0393ua) = \u00b5(\u0393u) and inductively we have\n\u00b5(\u0393ui) = \u2211\ns\u2208\u03a3`\u2212|ui| \u00b5(\u0393uis), (A.1)\nand \u00b5(\u0393w) = \u2211 v\u2208\u03a3` \u00b5(\u0393wv). (A.2)\nFor every string v \u2208 \u03a3`, the concatenation wv \u2208 \u0393w = \u228ek i=1 \u0393ui , so there is a unique i such that wv \u2208 \u0393ui . Hence there is a unique string s \u2208 \u03a3`\u2212|ui| such that wv = uis. Together with (A.1) and (A.2) this yields\n\u00b5 ( k\u228e i=1 \u0393ui ) = \u00b5(\u0393w) = \u2211 v\u2208\u03a3` \u00b5(\u0393wv) = k\u2211 i=1 \u2211 s\u2208\u03a3`\u2212|ui| \u00b5(\u0393uis) = k\u2211 i=1 \u00b5(\u0393ui).\n(c) (\u03c3-subadditivity.) We will show that each \u0393u is compact with respect to the topology O generated by R. \u03c3-subadditivity then follows from (b) because every countable union is in fact a finite union.\nWe will show that the topology O is the product topology of the discrete topology on \u03a3. (This establishes that (\u03a3\u03c9,O) is a Cantor Space.) Every projection \u03c0k : \u03a3\u221e \u2192 \u03a3 selecting the k-th symbol is continuous, since \u03c0\u22121k (a) = \u22c3 u\u2208\u03a3k\u22121 \u0393ua for every a \u2208 \u03a3. Moreover, O is the coarsest topology with this property, since we can generate every open set \u0393u \u2208 R in the base of the topology by\n\u0393u = |u|\u22c2 i=1 \u03c0\u22121i ({ui}).\nThe set \u03a3 is finite and thus compact. By Tychonoff\u2019s Theorem, \u03a3\u221e is also compact. Therefore \u0393u is compact since it is homeomorphic to \u03a3\u221e via the canonical map \u03b2u : \u03a3 \u221e \u2192 \u0393u, v 7\u2192 uv.\nFrom (a), (b), and (c) Carath\u00e9odory\u2019s Extension Theorem yields a unique probability measure Q on (\u03a3\u221e,F\u221e) such that Q(\u0393u) = \u00b5(\u0393u) = q(u) for all u \u2208 \u03a3\u2217.\nUsing Lemma A.1, the proof of Theorem 3.19 is now straightforward.\nProof of Theorem 3.19. We define a function q : \u03a3\u2217 \u2192 R, with\nq(u) := X|u|(v)P (\u0393u)\nfor any v \u2208 \u0393u. The choice of v is irrelevant because X|u| is constant on \u0393u since it is Ft-measurable. In the following, we also write Xt(u) if |u| = t to simplify notation.\n154 Measures and Martingales\nThe function q is non-negative because Xt and P are both non-negative. Moreover, for any u \u2208 \u03a3t,\n1 = E[Xt] = \u222b\n\u03a3\u221e XtdP \u2265 \u222b \u0393u XtdP = P (\u0393u)Xt(u) = q(u).\nHence the range of q is a subset of [0, 1]. We have q( ) = X0( )P (\u0393 ) = E[X0] = 1 since P is a probability measure and F0 = {\u2205,\u03a3\u221e} is the trivial \u03c3-algebra. Let u \u2208 \u03a3t.\u2211 a\u2208\u03a3 q(ua) = \u2211 a\u2208\u03a3 Xt+1(ua)P (\u0393ua) = \u222b \u0393u Xt+1dP\n= \u222b \u0393u E[Xt+1 | Ft]dP = \u222b \u0393u XtdP = P (\u0393u)Xt(u) = q(u).\nBy Lemma A.1, there is a probability measure Q on (\u03a3\u221e,F\u221e) such that q(u) = Q(\u0393u) of all u \u2208 \u03a3\u2217. Therefore, for all v \u2208 \u03a3\u221e and for all t \u2208 N with P (\u0393v1:t) > 0,\nXt(v) = q(v1:t) P (\u0393v1:t) = Q(\u0393v1:t) P (\u0393v1:t) .\nMoreover, P is locally absolutely continuous with respect to Q since P (\u0393u) = 0 implies\nQ(\u0393u) = q(u) = X|u|(u)P (\u0393u) = 0."}], "references": [{"title": "Analysis of Thompson sampling for the multi-armed bandit problem", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Agrawal and Goyal.,? \\Q2011\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2011}, {"title": "AI researchers on AI risk", "author": ["Scott Alexander"], "venue": "http://slatestarcodex.com/2015/05/ 22/ai-researchers-on-ai-risk/,", "citeRegEx": "Alexander.,? \\Q2015\\E", "shortCiteRegEx": "Alexander.", "year": 2015}, {"title": "Interruptibility and corrigibility for AIXI and Monte Carlo agents. 2016", "author": ["Stuart Armstrong", "Laurent Orseau"], "venue": null, "citeRegEx": "Armstrong and Orseau.,? \\Q2016\\E", "shortCiteRegEx": "Armstrong and Orseau.", "year": 2016}, {"title": "Bayesianism, infinite decisions, and binding", "author": ["Frank Arntzenius", "Adam Elga", "John Hawthorne"], "venue": "Mind, 113(450):251\u2013283,", "citeRegEx": "Arntzenius et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Arntzenius et al\\.", "year": 2004}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Peter Auer", "Thomas Jaksch", "Ronald Ortner"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Auer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2009}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Peter Auer", "Thomas Jaksch", "Ronald Ortner"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Auer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2010}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["Leemon Baird"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Baird.,? \\Q1995\\E", "shortCiteRegEx": "Baird.", "year": 1995}, {"title": "Universal upper bound on the entropy-to-energy ratio for bounded systems", "author": ["Jacob D Bekenstein"], "venue": "Physical Review D,", "citeRegEx": "Bekenstein.,? \\Q1981\\E", "shortCiteRegEx": "Bekenstein.", "year": 1981}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Increasing the action gap: New operators for reinforcement learning", "author": ["Marc G Bellemare", "Georg Ostrovski", "Arthur Guez", "Philip S Thomas", "R\u00e9mi Munos"], "venue": "In AAAI,", "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Dynamic Programming and Optimal Control", "author": ["Dimitri P Bertsekas", "John Tsitsiklis"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas and Tsitsiklis.,? \\Q1995\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis.", "year": 1995}, {"title": "Pattern Recognition and Machine Learning", "author": ["Christopher M Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "Merging of opinions with increasing information", "author": ["David Blackwell", "Lester Dubins"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Blackwell and Dubins.,? \\Q1962\\E", "shortCiteRegEx": "Blackwell and Dubins.", "year": 1962}, {"title": "Existential risks", "author": ["Nick Bostrom"], "venue": "Journal of Evolution and Technology,", "citeRegEx": "Bostrom.,? \\Q2002\\E", "shortCiteRegEx": "Bostrom.", "year": 2002}, {"title": "Ethical issues in advanced artificial intelligence. Science Fiction and Philosophy: From Time Travel to Superintelligence", "author": ["Nick Bostrom"], "venue": null, "citeRegEx": "Bostrom.,? \\Q2003\\E", "shortCiteRegEx": "Bostrom.", "year": 2003}, {"title": "Superintelligence: Paths, Dangers, Strategies", "author": ["Nick Bostrom"], "venue": null, "citeRegEx": "Bostrom.,? \\Q2014\\E", "shortCiteRegEx": "Bostrom.", "year": 2014}, {"title": "Strategic implications of openness in AI development", "author": ["Nick Bostrom"], "venue": "Technical report, Future of Humanity Institute,", "citeRegEx": "Bostrom.,? \\Q2016\\E", "shortCiteRegEx": "Bostrom.", "year": 2016}, {"title": "Rational and convergent learning in stochastic games", "author": ["Michael Bowling", "Manuela Veloso"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "Bowling and Veloso.,? \\Q2001\\E", "shortCiteRegEx": "Bowling and Veloso.", "year": 2001}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Cesa-Nicol\u00f2 Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Bianchi.", "year": 2012}, {"title": "A comprehensive survey of multiagent reinforcement learning", "author": ["Lucian Busoniu", "Robert Babuska", "Bart De Schutter"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews,", "citeRegEx": "Busoniu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Busoniu et al\\.", "year": 2008}, {"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "The singularity: A philosophical analysis", "author": ["David Chalmers"], "venue": "Journal of Consciousness Studies,", "citeRegEx": "Chalmers.,? \\Q2010\\E", "shortCiteRegEx": "Chalmers.", "year": 2010}, {"title": "An empirical evaluation of Thompson sampling", "author": ["Olivier Chapelle", "Lihong Li"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chapelle and Li.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle and Li.", "year": 2011}, {"title": "Definability of truth in probabilistic logic", "author": ["Paul Christiano", "Eliezer Yudkowsky", "Marcello Herreshoff", "Mihaly Barasz"], "venue": "Technical report, Machine Intelligence Research Institute,", "citeRegEx": "Christiano et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Christiano et al\\.", "year": 2013}, {"title": "Elements of Information Theory", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "Cover and Thomas.,? \\Q2006\\E", "shortCiteRegEx": "Cover and Thomas.", "year": 2006}, {"title": "Generic Reinforcement Learning Beyond Small MDPs", "author": ["Mayank Daswani"], "venue": "PhD thesis, Australian National University,", "citeRegEx": "Daswani.,? \\Q2015\\E", "shortCiteRegEx": "Daswani.", "year": 2015}, {"title": "A definition of happiness for reinforcement learning agents", "author": ["Mayank Daswani", "Jan Leike"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Daswani and Leike.,? \\Q2015\\E", "shortCiteRegEx": "Daswani and Leike.", "year": 2015}, {"title": "Ethical guidelines for a superintelligence", "author": ["Ernest Davis"], "venue": "Technical report,", "citeRegEx": "Davis.,? \\Q2014\\E", "shortCiteRegEx": "Davis.", "year": 2014}, {"title": "Increasing the gap between descriptional complexity and algorithmic probability", "author": ["Adam Day"], "venue": "Transactions of the American Mathematical Society,", "citeRegEx": "Day.,? \\Q2011\\E", "shortCiteRegEx": "Day.", "year": 2011}, {"title": "Bayesian Q-learning", "author": ["Richard Dearden", "Nir Friedman", "Stuart Russell"], "venue": "In AAAI,", "citeRegEx": "Dearden et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Dearden et al\\.", "year": 1998}, {"title": "Learning what to value", "author": ["Daniel Dewey"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Dewey.,? \\Q2011\\E", "shortCiteRegEx": "Dewey.", "year": 2011}, {"title": "Bayesian Nonparametric Approaches for Reinforcement Learning in Partially Observable Domains", "author": ["Finale Doshi-Velez"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Doshi.Velez.,? \\Q2012\\E", "shortCiteRegEx": "Doshi.Velez.", "year": 2012}, {"title": "Probability: Theory and Examples", "author": ["Rick Durrett"], "venue": null, "citeRegEx": "Durrett.,? \\Q2010\\E", "shortCiteRegEx": "Durrett.", "year": 2010}, {"title": "The singularity controversy", "author": ["Amnon H Eden"], "venue": "Technical report, Sapience Project,", "citeRegEx": "Eden.,? \\Q2016\\E", "shortCiteRegEx": "Eden.", "year": 2016}, {"title": "Singularity Hypotheses: A Scientific and Philosophical Assessment", "author": ["Amnon H Eden", "James H Moor", "Johnny H S\u00f8raker", "Eric Steinhart", "editors"], "venue": null, "citeRegEx": "Eden et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Eden et al\\.", "year": 2013}, {"title": "Avoiding wireheading with value reinforcement learning", "author": ["Tom Everitt", "Marcus Hutter"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Everitt and Hutter.,? \\Q2016\\E", "shortCiteRegEx": "Everitt and Hutter.", "year": 2016}, {"title": "Sequential extensions of causal and evidential decision theory", "author": ["Tom Everitt", "Jan Leike", "Marcus Hutter"], "venue": "In Algorithmic Decision Theory,", "citeRegEx": "Everitt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Everitt et al\\.", "year": 2015}, {"title": "Self-modification of policy and utility function in rational agents", "author": ["Tom Everitt", "Daniel Filan", "Mayank Daswani", "Marcus Hutter"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Everitt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Everitt et al\\.", "year": 2016}, {"title": "Reflective variants of Solomonoff induction and AIXI", "author": ["Benja Fallenstein", "Nate Soares", "Jessica Taylor"], "venue": "In Artificial General Intelligence. Springer,", "citeRegEx": "Fallenstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fallenstein et al\\.", "year": 2015}, {"title": "Reflective oracles: A foundation for game theory in artificial intelligence", "author": ["Benja Fallenstein", "Jessica Taylor", "Paul F Christiano"], "venue": "In Logic, Rationality, and Interaction,", "citeRegEx": "Fallenstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fallenstein et al\\.", "year": 2015}, {"title": "Reflective oracles: A foundation for classical game theory", "author": ["Benja Fallenstein", "Jessica Taylor", "Paul F Christiano"], "venue": "Technical report, Machine Intelligence Research Institute,", "citeRegEx": "Fallenstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fallenstein et al\\.", "year": 2015}, {"title": "Agents using speed priors", "author": ["Daniel Filan"], "venue": "Honours thesis,", "citeRegEx": "Filan.,? \\Q2015\\E", "shortCiteRegEx": "Filan.", "year": 2015}, {"title": "Loss bounds and time complexity for speed priors", "author": ["Daniel Filan", "Jan Leike", "Marcus Hutter"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "Filan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Filan et al\\.", "year": 2016}, {"title": "Learning to communicate to solve riddles with deep distributed recurrent Q-networks", "author": ["Jakob N Foerster", "Yannis M Assael", "Nando de Freitas", "Shimon Whiteson"], "venue": "Technical report, University of Oxford,", "citeRegEx": "Foerster et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Foerster et al\\.", "year": 2016}, {"title": "On the impossibility of predicting the behavior of rational agents", "author": ["Dean P Foster", "H Peyton Young"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Foster and Young.,? \\Q2001\\E", "shortCiteRegEx": "Foster and Young.", "year": 2001}, {"title": "Bandit processes and dynamic allocation indices", "author": ["John Gittins"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Gittins.,? \\Q1979\\E", "shortCiteRegEx": "Gittins.", "year": 1979}, {"title": "The paradox of confirmation", "author": ["Irving John Good"], "venue": "British Journal for the Philosophy of Science,", "citeRegEx": "Good.,? \\Q1960\\E", "shortCiteRegEx": "Good.", "year": 1960}, {"title": "Speculations concerning the first ultraintelligent machine", "author": ["Irving John Good"], "venue": "Advances in Computers,", "citeRegEx": "Good.,? \\Q1965\\E", "shortCiteRegEx": "Good.", "year": 1965}, {"title": "The white shoe is a red herring", "author": ["Irving John Good"], "venue": "The British Journal for the Philosophy of Science,", "citeRegEx": "Good.,? \\Q1967\\E", "shortCiteRegEx": "Good.", "year": 1967}, {"title": "Deep learning. Book in preparation for MIT Press, http://www.deeplearningbook.org, 2016", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Thompson sampling for learning parameterized Markov decision processes", "author": ["Aditya Gopalan", "Shie Mannor"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Gopalan and Mannor.,? \\Q2015\\E", "shortCiteRegEx": "Gopalan and Mannor.", "year": 2015}, {"title": "Reinforcement learning with function approximation converges to a region", "author": ["Geoffrey J Gordon"], "venue": "In Advanced in Neural Information Processing Systems,", "citeRegEx": "Gordon.,? \\Q2001\\E", "shortCiteRegEx": "Gordon.", "year": 2001}, {"title": "The Minimum Description Length Principle", "author": ["Peter D. Gr\u00fcnwald"], "venue": null, "citeRegEx": "Gr\u00fcnwald.,? \\Q2007\\E", "shortCiteRegEx": "Gr\u00fcnwald.", "year": 2007}, {"title": "On the relation between descriptional complexity and algorithmic probability", "author": ["P\u00e9ter G\u00e1cs"], "venue": "Theoretical Computer Science,", "citeRegEx": "G\u00e1cs.,? \\Q1983\\E", "shortCiteRegEx": "G\u00e1cs.", "year": 1983}, {"title": "\u00dcber formal unentscheidbare S\u00e4tze der Principia Mathematica und verwandter Systeme I", "author": ["Kurt G\u00f6del"], "venue": "Monatshefte fu\u0308r Mathematik und Physik,", "citeRegEx": "G\u00f6del.,? \\Q1931\\E", "shortCiteRegEx": "G\u00f6del.", "year": 1931}, {"title": "Deep recurrent Q-learning for partially observable MDPs", "author": ["Matthew Hausknecht", "Peter Stone"], "venue": "In 2015 AAAI Fall Symposium Series,", "citeRegEx": "Hausknecht and Stone.,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht and Stone.", "year": 2015}, {"title": "Transcending complacency on superintelligent machines. http://www.huffingtonpost.com/ stephen-hawking/artificial-intelligence_b_5174265.html", "author": ["Stephen Hawking", "Max Tegmark", "Stuart Russell", "Frank Wilczek"], "venue": null, "citeRegEx": "Hawking et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hawking et al\\.", "year": 2014}, {"title": "Universal semimeasures: An introduction", "author": ["Nicholas J Hay"], "venue": "Master\u2019s thesis, University of Auckland,", "citeRegEx": "Hay.,? \\Q2007\\E", "shortCiteRegEx": "Hay.", "year": 2007}, {"title": "Memory-based control with recurrent neural networks", "author": ["Nicolas Heess", "Jonathan J Hunt", "Timothy P Lillicrap", "David Silver"], "venue": "Technical report, Google DeepMind,", "citeRegEx": "Heess et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2015}, {"title": "Deep reinforcement learning from self-play in imperfect-information games", "author": ["Johannes Heinrich", "David Silver"], "venue": "Technical report, DeepMind,", "citeRegEx": "Heinrich and Silver.,? \\Q2016\\E", "shortCiteRegEx": "Heinrich and Silver.", "year": 2016}, {"title": "Ultimate Automizer with array interpolation (competition contribution)", "author": ["Matthias Heizmann", "Daniel Dietsch", "Jan Leike", "Betim Musa", "Andreas Podelski"], "venue": "In Tools and Algorithms for the Construction and Analysis of Systems,", "citeRegEx": "Heizmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heizmann et al\\.", "year": 2015}, {"title": "Ultimate Automizer with two-track proofs (competition contribution)", "author": ["Matthias Heizmann", "Daniel Dietsch", "Marius Greitschus", "Jan Leike", "Betim Musa", "Claus Sch\u00e4tzle", "Andreas Podelski"], "venue": "In Tools and Algorithms for the Construction and Analysis of Systems,", "citeRegEx": "Heizmann et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Heizmann et al\\.", "year": 2016}, {"title": "Studies in the logic of confirmation (I.)", "author": ["Carl G Hempel"], "venue": "Mind, pages 1\u201326,", "citeRegEx": "Hempel.,? \\Q1945\\E", "shortCiteRegEx": "Hempel.", "year": 1945}, {"title": "The white shoe: No red herring", "author": ["Carl G Hempel"], "venue": "The British Journal for the Philosophy of Science,", "citeRegEx": "Hempel.,? \\Q1967\\E", "shortCiteRegEx": "Hempel.", "year": 1967}, {"title": "A theory of universal artificial intelligence based on algorithmic complexity", "author": ["Marcus Hutter"], "venue": "Technical report,", "citeRegEx": "Hutter.,? \\Q2000\\E", "shortCiteRegEx": "Hutter.", "year": 2000}, {"title": "Universal sequential decisions in unknown environments", "author": ["Marcus Hutter"], "venue": "In European Workshop on Reinforcement Learning,", "citeRegEx": "Hutter.,? \\Q2001\\E", "shortCiteRegEx": "Hutter.", "year": 2001}, {"title": "New error bounds for Solomonoff prediction", "author": ["Marcus Hutter"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Hutter.,? \\Q2001\\E", "shortCiteRegEx": "Hutter.", "year": 2001}, {"title": "Self-optimizing and Pareto-optimal policies in general environments based on Bayes-mixtures", "author": ["Marcus Hutter"], "venue": "In Computational Learning Theory,", "citeRegEx": "Hutter.,? \\Q2002\\E", "shortCiteRegEx": "Hutter.", "year": 2002}, {"title": "The fastest and shortest algorithm for all well-defined problems", "author": ["Marcus Hutter"], "venue": "International Journal of Foundations of Computer Science,", "citeRegEx": "Hutter.,? \\Q2002\\E", "shortCiteRegEx": "Hutter.", "year": 2002}, {"title": "A gentle introduction to the universal algorithmic agent AIXI", "author": ["Marcus Hutter"], "venue": "Technical report, IDSIA,", "citeRegEx": "Hutter.,? \\Q2003\\E", "shortCiteRegEx": "Hutter.", "year": 2003}, {"title": "Sequential predictions based on algorithmic complexity", "author": ["Marcus Hutter"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Hutter.,? \\Q2006\\E", "shortCiteRegEx": "Hutter.", "year": 2006}, {"title": "General discounting versus average reward. In Algorithmic Learning Theory, pages 244\u2013258", "author": ["Marcus Hutter"], "venue": null, "citeRegEx": "Hutter.,? \\Q2006\\E", "shortCiteRegEx": "Hutter.", "year": 2006}, {"title": "50\u2019000\u20ac prize for compressing human knowledge", "author": ["Marcus Hutter"], "venue": "http://prize. hutter1.net/,", "citeRegEx": "Hutter.,? \\Q2006\\E", "shortCiteRegEx": "Hutter.", "year": 2006}, {"title": "Universal algorithmic intelligence: A mathematical top\u2192down approach", "author": ["Marcus Hutter"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Hutter.,? \\Q2007\\E", "shortCiteRegEx": "Hutter.", "year": 2007}, {"title": "On universal prediction and Bayesian confirmation", "author": ["Marcus Hutter"], "venue": "Theoretical Computer Science,", "citeRegEx": "Hutter.,? \\Q2007\\E", "shortCiteRegEx": "Hutter.", "year": 2007}, {"title": "Discrete MDL predicts in total variation", "author": ["Marcus Hutter"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hutter.,? \\Q2009\\E", "shortCiteRegEx": "Hutter.", "year": 2009}, {"title": "Open problems in universal induction & intelligence", "author": ["Marcus Hutter"], "venue": null, "citeRegEx": "Hutter.,? \\Q2009\\E", "shortCiteRegEx": "Hutter.", "year": 2009}, {"title": "Feature dynamic Bayesian networks", "author": ["Marcus Hutter"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Hutter.,? \\Q2009\\E", "shortCiteRegEx": "Hutter.", "year": 2009}, {"title": "Feature reinforcement learning: Part I: Unstructured MDPs", "author": ["Marcus Hutter"], "venue": "Journal of Artificial General Intelligence,", "citeRegEx": "Hutter.,? \\Q2009\\E", "shortCiteRegEx": "Hutter.", "year": 2009}, {"title": "Can intelligence explode", "author": ["Marcus Hutter"], "venue": "Journal of Consciousness Studies,", "citeRegEx": "Hutter.,? \\Q2012\\E", "shortCiteRegEx": "Hutter.", "year": 2012}, {"title": "One decade of universal artificial intelligence", "author": ["Marcus Hutter"], "venue": "In Theoretical Foundations of Artificial General Intelligence,", "citeRegEx": "Hutter.,? \\Q2012\\E", "shortCiteRegEx": "Hutter.", "year": 2012}, {"title": "Extreme state aggregation beyond MDPs", "author": ["Marcus Hutter"], "venue": "In Algorithmic Learning Theory. Springer,", "citeRegEx": "Hutter.,? \\Q2014\\E", "shortCiteRegEx": "Hutter.", "year": 2014}, {"title": "On semimeasures predicting Martin-L\u00f6f random sequences", "author": ["Marcus Hutter", "Andrej A. Muchnik"], "venue": "Theoretical Computer Science,", "citeRegEx": "Hutter and Muchnik.,? \\Q2007\\E", "shortCiteRegEx": "Hutter and Muchnik.", "year": 2007}, {"title": "Probability Theory: The Logic of Science", "author": ["Edwin T Jaynes"], "venue": null, "citeRegEx": "Jaynes.,? \\Q2003\\E", "shortCiteRegEx": "Jaynes.", "year": 2003}, {"title": "On the Sample Complexity of Reinforcement Learning", "author": ["Sham Machandranath Kakade"], "venue": "PhD thesis,", "citeRegEx": "Kakade.,? \\Q2003\\E", "shortCiteRegEx": "Kakade.", "year": 2003}, {"title": "Rational learning leads to Nash equilibrium", "author": ["Ehud Kalai", "Ehud Lehrer"], "venue": "Econometrica, pages 1019\u20131045,", "citeRegEx": "Kalai and Lehrer.,? \\Q1993\\E", "shortCiteRegEx": "Kalai and Lehrer.", "year": 1993}, {"title": "Weak and strong merging of opinions", "author": ["Ehud Kalai", "Ehud Lehrer"], "venue": "Journal of Mathematical Economics,", "citeRegEx": "Kalai and Lehrer.,? \\Q1994\\E", "shortCiteRegEx": "Kalai and Lehrer.", "year": 1994}, {"title": "Thompson sampling: An asymptotically optimal finite-time analysis", "author": ["Emilie Kaufmann", "Nathaniel Korda", "R\u00e9mi Munos"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "Autonomous helicopter flight via reinforcement learning", "author": ["HJ Kim", "Michael I Jordan", "Shankar Sastry", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems, page None,", "citeRegEx": "Kim et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2003}, {"title": "Introduction to Metamathematics", "author": ["Stephen Cole Kleene"], "venue": "Wolters-Noordhoff Publishing,", "citeRegEx": "Kleene.,? \\Q1952\\E", "shortCiteRegEx": "Kleene.", "year": 1952}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Tejas D Kulkarni", "Karthik R Narasimhan", "Ardavan Saeedi", "Joshua B Tenenbaum"], "venue": "Technical report, Massachusetts Institute of Technology,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "The Singularity is Near: When Humans Transcend Biology", "author": ["Ray Kurzweil"], "venue": "Viking Books,", "citeRegEx": "Kurzweil.,? \\Q2005\\E", "shortCiteRegEx": "Kurzweil.", "year": 2005}, {"title": "Theory of General Reinforcement Learning", "author": ["Tor Lattimore"], "venue": "PhD thesis, Australian National University,", "citeRegEx": "Lattimore.,? \\Q2013\\E", "shortCiteRegEx": "Lattimore.", "year": 2013}, {"title": "Regret analysis of the finite-horizon Gittins index strategy for multiarmed bandits", "author": ["Tor Lattimore"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Lattimore.,? \\Q2016\\E", "shortCiteRegEx": "Lattimore.", "year": 2016}, {"title": "Asymptotically optimal agents", "author": ["Tor Lattimore", "Marcus Hutter"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Lattimore and Hutter.,? \\Q2011\\E", "shortCiteRegEx": "Lattimore and Hutter.", "year": 2011}, {"title": "PAC bounds for discounted MDPs", "author": ["Tor Lattimore", "Marcus Hutter"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Lattimore and Hutter.,? \\Q2012\\E", "shortCiteRegEx": "Lattimore and Hutter.", "year": 2012}, {"title": "On Martin-L\u00f6f convergence of Solomonoff\u2019s mixture", "author": ["Tor Lattimore", "Marcus Hutter"], "venue": "In Theory and Applications of Models of Computation,", "citeRegEx": "Lattimore and Hutter.,? \\Q2013\\E", "shortCiteRegEx": "Lattimore and Hutter.", "year": 2013}, {"title": "General time consistent discounting", "author": ["Tor Lattimore", "Marcus Hutter"], "venue": "Theoretical Computer Science,", "citeRegEx": "Lattimore and Hutter.,? \\Q2014\\E", "shortCiteRegEx": "Lattimore and Hutter.", "year": 2014}, {"title": "On Martin-L\u00f6f (non-)convergence of Solomonoff\u2019s universal mixture", "author": ["Tor Lattimore", "Marcus Hutter"], "venue": "Theoretical Computer Science,", "citeRegEx": "Lattimore and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Lattimore and Hutter.", "year": 2015}, {"title": "Universal prediction of selected bits", "author": ["Tor Lattimore", "Marcus Hutter", "Vaibhav Gavane"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Lattimore et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lattimore et al\\.", "year": 2011}, {"title": "Is there an elegant universal theory of prediction", "author": ["Shane Legg"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Legg.,? \\Q2006\\E", "shortCiteRegEx": "Legg.", "year": 2006}, {"title": "Machine Super Intelligence", "author": ["Shane Legg"], "venue": "PhD thesis, University of Lugano,", "citeRegEx": "Legg.,? \\Q2008\\E", "shortCiteRegEx": "Legg.", "year": 2008}, {"title": "A collection of definitions of intelligence", "author": ["Shane Legg", "Marcus Hutter"], "venue": "Frontiers in Artificial Intelligence and Applications,", "citeRegEx": "Legg and Hutter.,? \\Q2007\\E", "shortCiteRegEx": "Legg and Hutter.", "year": 2007}, {"title": "Universal intelligence: A definition of machine intelligence", "author": ["Shane Legg", "Marcus Hutter"], "venue": "Minds & Machines,", "citeRegEx": "Legg and Hutter.,? \\Q2007\\E", "shortCiteRegEx": "Legg and Hutter.", "year": 2007}, {"title": "An approximation of the universal intelligence measure. In Algorithmic Probability and Friends", "author": ["Shane Legg", "Joel Veness"], "venue": "Bayesian Prediction and Artificial Intelligence,", "citeRegEx": "Legg and Veness.,? \\Q2013\\E", "shortCiteRegEx": "Legg and Veness.", "year": 2013}, {"title": "Merging and learning. Statistics, Probability and Game Theory, pages", "author": ["Ehud Lehrer", "Rann Smorodinsky"], "venue": null, "citeRegEx": "Lehrer and Smorodinsky.,? \\Q1996\\E", "shortCiteRegEx": "Lehrer and Smorodinsky.", "year": 1996}, {"title": "Ranking templates for linear loops. In Tools and Algorithms for the Construction and Analysis of Systems, pages 172\u2013186", "author": ["Jan Leike", "Matthias Heizmann"], "venue": null, "citeRegEx": "Leike and Heizmann.,? \\Q2014\\E", "shortCiteRegEx": "Leike and Heizmann.", "year": 2014}, {"title": "Geometric series as nontermination arguments for linear lasso programs", "author": ["Jan Leike", "Matthias Heizmann"], "venue": "Technical report, University of Freiburg,", "citeRegEx": "Leike and Heizmann.,? \\Q2014\\E", "shortCiteRegEx": "Leike and Heizmann.", "year": 2014}, {"title": "Ranking templates for linear loops", "author": ["Jan Leike", "Matthias Heizmann"], "venue": "Logical Methods in Computer Science,", "citeRegEx": "Leike and Heizmann.,? \\Q2015\\E", "shortCiteRegEx": "Leike and Heizmann.", "year": 2015}, {"title": "Geometric nontermination arguments. 2016", "author": ["Jan Leike", "Matthias Heizmann"], "venue": "Under preparation", "citeRegEx": "Leike and Heizmann.,? \\Q2016\\E", "shortCiteRegEx": "Leike and Heizmann.", "year": 2016}, {"title": "Indefinitely oscillating martingales", "author": ["Jan Leike", "Marcus Hutter"], "venue": "In Algorithmic Learning Theory, pages 321\u2013335,", "citeRegEx": "Leike and Hutter.,? \\Q2014\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2014}, {"title": "Indefinitely oscillating martingales", "author": ["Jan Leike", "Marcus Hutter"], "venue": "Technical report, Australian National University,", "citeRegEx": "Leike and Hutter.,? \\Q2014\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2014}, {"title": "On the computability of AIXI", "author": ["Jan Leike", "Marcus Hutter"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Leike and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2015}, {"title": "On the computability of Solomonoff induction and knowledge-seeking", "author": ["Jan Leike", "Marcus Hutter"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Leike and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2015}, {"title": "Bad universal priors and notions of optimality", "author": ["Jan Leike", "Marcus Hutter"], "venue": "In Conference on Learning Theory, pages 1244\u20131259,", "citeRegEx": "Leike and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2015}, {"title": "Solomonoff induction violates Nicod\u2019s criterion", "author": ["Jan Leike", "Marcus Hutter"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Leike and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2015}, {"title": "On the computability of Solomonoff induction and AIXI", "author": ["Jan Leike", "Marcus Hutter"], "venue": null, "citeRegEx": "Leike and Hutter.,? \\Q2016\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2016}, {"title": "Thompson sampling is asymptotically optimal in general environments", "author": ["Jan Leike", "Tor Lattimore", "Laurent Orseau", "Marcus Hutter"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Leike et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Leike et al\\.", "year": 2016}, {"title": "A formal solution to the grain of truth problem", "author": ["Jan Leike", "Jessica Taylor", "Benya Fallenstein"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Leike et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Leike et al\\.", "year": 2016}, {"title": "On the notion of a random sequence", "author": ["Leonid A Levin"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "Levin.,? \\Q1973\\E", "shortCiteRegEx": "Levin.", "year": 1973}, {"title": "An Introduction to Kolmogorov Complexity and Its Applications", "author": ["Ming Li", "Paul M.B. Vit\u00e1nyi"], "venue": "Texts in Computer Science. Springer,", "citeRegEx": "Li and Vit\u00e1nyi.,? \\Q2008\\E", "shortCiteRegEx": "Li and Vit\u00e1nyi.", "year": 2008}, {"title": "State of the art control of Atari games using shallow reinforcement learning", "author": ["Yitao Liang", "Marlos C Machado", "Erik Talvitie", "Michael Bowling"], "venue": "In Autonomous Agents and Multiagent Systems,", "citeRegEx": "Liang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Lillicrap et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2016}, {"title": "The paradox of confirmation", "author": ["John L Mackie"], "venue": "British Journal for the Philosophy of Science,", "citeRegEx": "Mackie.,? \\Q1963\\E", "shortCiteRegEx": "Mackie.", "year": 1963}, {"title": "On the undecidability of probabilistic planning and infinite-horizon partially observable Markov decision problems", "author": ["Omid Madani", "Steve Hanks", "Anne Condon"], "venue": "In AAAI,", "citeRegEx": "Madani et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Madani et al\\.", "year": 1999}, {"title": "On the undecidability of probabilistic planning and related stochastic optimization problems", "author": ["Omid Madani", "Steve Hanks", "Anne Condon"], "venue": "Artificial Intelligence,", "citeRegEx": "Madani et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Madani et al\\.", "year": 2003}, {"title": "Optimality criteria in reinforcement learning", "author": ["Sridhar Mahadevan"], "venue": "In AAAI Fall Symposium on Learning Complex Behaviors in Adaptive Intelligent Systems,", "citeRegEx": "Mahadevan.,? \\Q1996\\E", "shortCiteRegEx": "Mahadevan.", "year": 1996}, {"title": "Inductive logic and the ravens paradox", "author": ["Patrick Maher"], "venue": "Philosophy of Science,", "citeRegEx": "Maher.,? \\Q1999\\E", "shortCiteRegEx": "Maher.", "year": 1999}, {"title": "Emphatic temporal-difference learning", "author": ["A Rupam Mahmood", "Huizhen Yu", "Martha White", "Richard Sutton"], "venue": "Technical report, University of Alberta,", "citeRegEx": "Mahmood et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mahmood et al\\.", "year": 2015}, {"title": "The maximum speed of dynamical evolution", "author": ["Norman Margolus", "Lev B Levitin"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "Margolus and Levitin.,? \\Q1998\\E", "shortCiteRegEx": "Margolus and Levitin.", "year": 1998}, {"title": "Death and suicide in universal artificial intelligence", "author": ["Jarryd Martin", "Tom Everitt", "Marcus Hutter"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Martin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2016}, {"title": "A proposal for the Dartmouth summer research project on artificial intelligence", "author": ["John McCarthy", "Marvin Minsky", "Nathaniel Rochester", "Claude Shannon"], "venue": null, "citeRegEx": "McCarthy et al\\.,? \\Q1955\\E", "shortCiteRegEx": "McCarthy et al\\.", "year": 1955}, {"title": "The role of absolute continuity in \u201cmerging of opinions\u201d and \u201crational learning", "author": ["Ronald I Miller", "Chris William Sanchirico"], "venue": "Games and Economic Behavior,", "citeRegEx": "Miller and Sanchirico.,? \\Q1999\\E", "shortCiteRegEx": "Miller and Sanchirico.", "year": 1999}, {"title": "Playing Atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "Technical report, Google DeepMind,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Intelligence explosion: Evidence and import", "author": ["Luke Muehlhauser", "Anna Salamon"], "venue": "In Singularity Hypotheses,", "citeRegEx": "Muehlhauser and Salamon.,? \\Q2012\\E", "shortCiteRegEx": "Muehlhauser and Salamon.", "year": 2012}, {"title": "Complexity of finite-horizon Markov decision process problems", "author": ["Martin Mundhenk", "Judy Goldsmith", "Christopher Lusena", "Eric Allender"], "venue": "Journal of the ACM,", "citeRegEx": "Mundhenk et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Mundhenk et al\\.", "year": 2000}, {"title": "Stationary algorithmic probability", "author": ["Markus M\u00fcller"], "venue": "Theoretical Computer Science,", "citeRegEx": "M\u00fcller.,? \\Q2010\\E", "shortCiteRegEx": "M\u00fcller.", "year": 2010}, {"title": "Future progress in artificial intelligence: A survey of expert opinion", "author": ["Vincent C M\u00fcller", "Nick Bostrom"], "venue": "Fundamental Issues of Artificial Intelligence,", "citeRegEx": "M\u00fcller and Bostrom.,? \\Q2016\\E", "shortCiteRegEx": "M\u00fcller and Bostrom.", "year": 2016}, {"title": "Prediction, optimization, and learning", "author": ["John H Nachbar"], "venue": "in repeated games. Econometrica,", "citeRegEx": "Nachbar.,? \\Q1997\\E", "shortCiteRegEx": "Nachbar.", "year": 1997}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Arun Nair", "Praveen Srinivasan", "Sam Blackwell", "Cagdas Alcicek", "Rory Fearon", "Alessandro De Maria", "Vedavyas Panneershelvam", "Mustafa Suleyman", "Charles Beattie", "Stig Petersen", "Shane Legg", "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver"], "venue": "Technical report, Google DeepMind,", "citeRegEx": "Nair et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2015}, {"title": "Is A.I. an existential threat to humanity? https://www.quora.com/ Is-A-I-an-existential-threat-to-humanity/answer/Andrew-Ng, January 2016", "author": ["Andrew Ng"], "venue": null, "citeRegEx": "Ng.,? \\Q2016\\E", "shortCiteRegEx": "Ng.", "year": 2016}, {"title": "Competing with an infinite set of models in reinforcement learning", "author": ["Phuong Nguyen", "Odalric-Ambrym Maillard", "Daniil Ryabko", "Ronald Ortner"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "Nguyen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2013}, {"title": "Le Probl\u00e8me Logique de L\u2019Induction", "author": ["Jean Nicod"], "venue": "Presses Universitaires de France,", "citeRegEx": "Nicod.,? \\Q1961\\E", "shortCiteRegEx": "Nicod.", "year": 1961}, {"title": "Computability and Randomness", "author": ["Andr\u00e9 Nies"], "venue": null, "citeRegEx": "Nies.,? \\Q2009\\E", "shortCiteRegEx": "Nies.", "year": 2009}, {"title": "Positive reinforcement produced by electrical stimulation of septal area and other regions of rat brain", "author": ["James Olds", "Peter Milner"], "venue": "Journal of Comparative and Physiological Psychology,", "citeRegEx": "Olds and Milner.,? \\Q1954\\E", "shortCiteRegEx": "Olds and Milner.", "year": 1954}, {"title": "The basic AI drives", "author": ["Stephen M Omohundro"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Omohundro.,? \\Q2008\\E", "shortCiteRegEx": "Omohundro.", "year": 2008}, {"title": "Optimality issues of universal greedy agents with static priors", "author": ["Laurent Orseau"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Orseau.,? \\Q2010\\E", "shortCiteRegEx": "Orseau.", "year": 2010}, {"title": "Universal knowledge-seeking agents", "author": ["Laurent Orseau"], "venue": null, "citeRegEx": "Orseau.,? \\Q2011\\E", "shortCiteRegEx": "Orseau.", "year": 2011}, {"title": "Asymptotic non-learnability of universal agents with computable horizon functions", "author": ["Laurent Orseau"], "venue": "Theoretical Computer Science,", "citeRegEx": "Orseau.,? \\Q2013\\E", "shortCiteRegEx": "Orseau.", "year": 2013}, {"title": "Universal knowledge-seeking agents", "author": ["Laurent Orseau"], "venue": "Theoretical Computer Science,", "citeRegEx": "Orseau.,? \\Q2014\\E", "shortCiteRegEx": "Orseau.", "year": 2014}, {"title": "The multi-slot framework: A formal model for multiple, copiable AIs", "author": ["Laurent Orseau"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Orseau.,? \\Q2014\\E", "shortCiteRegEx": "Orseau.", "year": 2014}, {"title": "Teleporting universal intelligent agents", "author": ["Laurent Orseau"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Orseau.,? \\Q2014\\E", "shortCiteRegEx": "Orseau.", "year": 2014}, {"title": "Safely interruptible agents", "author": ["Laurent Orseau", "Stuart Armstrong"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Orseau and Armstrong.,? \\Q2016\\E", "shortCiteRegEx": "Orseau and Armstrong.", "year": 2016}, {"title": "Self-modification and mortality in artificial agents", "author": ["Laurent Orseau", "Mark Ring"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Orseau and Ring.,? \\Q2011\\E", "shortCiteRegEx": "Orseau and Ring.", "year": 2011}, {"title": "Space-time embedded intelligence", "author": ["Laurent Orseau", "Mark Ring"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Orseau and Ring.,? \\Q2012\\E", "shortCiteRegEx": "Orseau and Ring.", "year": 2012}, {"title": "Memory issues of intelligent agents", "author": ["Laurent Orseau", "Mark Ring"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Orseau and Ring.,? \\Q2012\\E", "shortCiteRegEx": "Orseau and Ring.", "year": 2012}, {"title": "Universal knowledge-seeking agents for stochastic environments", "author": ["Laurent Orseau", "Tor Lattimore", "Marcus Hutter"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Orseau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Orseau et al\\.", "year": 2013}, {"title": "A minimum relative entropy principle for learning and acting", "author": ["Pedro A Ortega", "Daniel A Braun"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Ortega and Braun.,? \\Q2010\\E", "shortCiteRegEx": "Ortega and Braun.", "year": 2010}, {"title": "Generalized Thompson sampling for sequential decision-making and causal inference", "author": ["Pedro A Ortega", "Daniel A Braun"], "venue": "Complex Adaptive Systems Modeling,", "citeRegEx": "Ortega and Braun.,? \\Q2014\\E", "shortCiteRegEx": "Ortega and Braun.", "year": 2014}, {"title": "More) efficient reinforcement learning via posterior sampling", "author": ["Ian Osband", "Dan Russo", "Benjamin van Roy"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Osband et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2013}, {"title": "The complexity of Markov decision processes", "author": ["Christos H Papadimitriou", "John N Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Papadimitriou and Tsitsiklis.,? \\Q1987\\E", "shortCiteRegEx": "Papadimitriou and Tsitsiklis.", "year": 1987}, {"title": "Markov Decision Processes", "author": ["Martin L Puterman"], "venue": null, "citeRegEx": "Puterman.,? \\Q2014\\E", "shortCiteRegEx": "Puterman.", "year": 2014}, {"title": "A philosophical treatise of universal induction", "author": ["Samuel Rathmanner", "Marcus Hutter"], "venue": "Entropy, 13(6):1076\u20131136,", "citeRegEx": "Rathmanner and Hutter.,? \\Q2011\\E", "shortCiteRegEx": "Rathmanner and Hutter.", "year": 2011}, {"title": "Delusion, survival, and intelligent agents", "author": ["Mark Ring", "Laurent Orseau"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Ring and Orseau.,? \\Q2011\\E", "shortCiteRegEx": "Ring and Orseau.", "year": 2011}, {"title": "Diffusions, Markov Processes, and Martingales: Volume 1, Foundations", "author": ["Chris Rogers", "David Williams"], "venue": null, "citeRegEx": "Rogers and Williams.,? \\Q1994\\E", "shortCiteRegEx": "Rogers and Williams.", "year": 1994}, {"title": "Research priorities for robust and beneficial artificial intelligence", "author": ["Stuart Russell", "Daniel Dewey", "Max Tegmark"], "venue": "Technical report, Future of Life Institute,", "citeRegEx": "Russell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russell et al\\.", "year": 2015}, {"title": "Artificial Intelligence. A Modern Approach", "author": ["Stuart J Russell", "Peter Norvig"], "venue": null, "citeRegEx": "Russell and Norvig.,? \\Q2010\\E", "shortCiteRegEx": "Russell and Norvig.", "year": 2010}, {"title": "Characterizing predictable classes of processes", "author": ["Daniil Ryabko"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Ryabko.,? \\Q2009\\E", "shortCiteRegEx": "Ryabko.", "year": 2009}, {"title": "On finding predictors for arbitrary families of processes", "author": ["Daniil Ryabko"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Ryabko.,? \\Q2010\\E", "shortCiteRegEx": "Ryabko.", "year": 2010}, {"title": "On the relation between realizable and nonrealizable cases of the sequence prediction problem", "author": ["Daniil Ryabko"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Ryabko.,? \\Q2011\\E", "shortCiteRegEx": "Ryabko.", "year": 2011}, {"title": "On sequence prediction for arbitrary measures", "author": ["Daniil Ryabko", "Marcus Hutter"], "venue": "In IEEE International Symposium on Information Theory, pages 2346\u20132350,", "citeRegEx": "Ryabko and Hutter.,? \\Q2007\\E", "shortCiteRegEx": "Ryabko and Hutter.", "year": 2007}, {"title": "Predicting non-stationary processes", "author": ["Daniil Ryabko", "Marcus Hutter"], "venue": "Applied Mathematics Letters,", "citeRegEx": "Ryabko and Hutter.,? \\Q2008\\E", "shortCiteRegEx": "Ryabko and Hutter.", "year": 2008}, {"title": "Purely epistemic Markov decision processes", "author": ["R\u00e9gis Sabbadin", "J\u00e9r\u00f4me Lang", "Nasolo Ravoanjanahry"], "venue": "In AAAI,", "citeRegEx": "Sabbadin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sabbadin et al\\.", "year": 2007}, {"title": "Prioritized experience replay", "author": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Schaul et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2016}, {"title": "The speed prior: A new simplicity measure yielding near-optimal computable predictions", "author": ["J\u00fcrgen Schmidhuber"], "venue": "In Computational Learning Theory,", "citeRegEx": "Schmidhuber.,? \\Q2002\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2002}, {"title": "Philosophers & futurists, catch up", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Journal of Consciousness Studies,", "citeRegEx": "Schmidhuber.,? \\Q2012\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2012}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "The Technological Singularity", "author": ["Murray Shanahan"], "venue": null, "citeRegEx": "Shanahan.,? \\Q2015\\E", "shortCiteRegEx": "Shanahan.", "year": 2015}, {"title": "Programming a computer for playing chess", "author": ["Claude E Shannon"], "venue": "The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science,", "citeRegEx": "Shannon.,? \\Q1950\\E", "shortCiteRegEx": "Shannon.", "year": 1950}, {"title": "Multiagent Systems: Algorithmic, GameTheoretic, and Logical Foundations", "author": ["Yoav Shoham", "Kevin Leyton-Brown"], "venue": null, "citeRegEx": "Shoham and Leyton.Brown.,? \\Q2009\\E", "shortCiteRegEx": "Shoham and Leyton.Brown.", "year": 2009}, {"title": "Learning predictive state representations", "author": ["Satinder Singh", "Michael L Littman", "Nicholas K Jong", "David Pardoe", "Peter Stone"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Singh et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2003}, {"title": "Predictive state representations: A new theory for modeling dynamical systems", "author": ["Satinder Singh", "Michael R James", "Matthew R Rudary"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Singh et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2004}, {"title": "Formalizing two problems of realistic world-models", "author": ["Nate Soares"], "venue": "Technical report, Machine Intelligence Research Institute,", "citeRegEx": "Soares.,? \\Q2015\\E", "shortCiteRegEx": "Soares.", "year": 2015}, {"title": "Aligning superintelligence with human interests: A technical research agenda", "author": ["Nate Soares", "Benja Fallenstein"], "venue": "Technical report, Machine Intelligence Research Institute,", "citeRegEx": "Soares and Fallenstein.,? \\Q2014\\E", "shortCiteRegEx": "Soares and Fallenstein.", "year": 2014}, {"title": "A formal theory of inductive inference. Parts 1 and 2", "author": ["Ray Solomonoff"], "venue": "Information and Control,", "citeRegEx": "Solomonoff.,? \\Q1964\\E", "shortCiteRegEx": "Solomonoff.", "year": 1964}, {"title": "Complexity-based induction systems: Comparisons and convergence theorems", "author": ["Ray Solomonoff"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Solomonoff.,? \\Q1978\\E", "shortCiteRegEx": "Solomonoff.", "year": 1978}, {"title": "Responses to catastrophic AGI risk: A survey", "author": ["Kaj Sotala", "Roman V Yampolskiy"], "venue": "Physica Scripta,", "citeRegEx": "Sotala and Yampolskiy.,? \\Q2014\\E", "shortCiteRegEx": "Sotala and Yampolskiy.", "year": 2014}, {"title": "Putnam\u2019s diagonal argument and the impossibility of a universal learning machine", "author": ["Tom F Sterkenburg"], "venue": "Technical report, Centrum Wiskunde & Informatica,", "citeRegEx": "Sterkenburg.,? \\Q2016\\E", "shortCiteRegEx": "Sterkenburg.", "year": 2016}, {"title": "Counterexamples in Probability", "author": ["Jordan M Stoyanov"], "venue": "Courier Corporation,", "citeRegEx": "Stoyanov.,? \\Q2013\\E", "shortCiteRegEx": "Stoyanov.", "year": 2013}, {"title": "A Bayesian framework for reinforcement learning", "author": ["Malcolm Strens"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Strens.,? \\Q2000\\E", "shortCiteRegEx": "Strens.", "year": 2000}, {"title": "Consistency of feature Markov processes", "author": ["Peter Sunehag", "Marcus Hutter"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Sunehag and Hutter.,? \\Q2010\\E", "shortCiteRegEx": "Sunehag and Hutter.", "year": 2010}, {"title": "Optimistic agents are asymptotically optimal", "author": ["Peter Sunehag", "Marcus Hutter"], "venue": "In Australasian Joint Conference on Artificial Intelligence,", "citeRegEx": "Sunehag and Hutter.,? \\Q2012\\E", "shortCiteRegEx": "Sunehag and Hutter.", "year": 2012}, {"title": "Optimistic AIXI", "author": ["Peter Sunehag", "Marcus Hutter"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Sunehag and Hutter.,? \\Q2012\\E", "shortCiteRegEx": "Sunehag and Hutter.", "year": 2012}, {"title": "Rationality, optimism and guarantees in general reinforcement learning", "author": ["Peter Sunehag", "Marcus Hutter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sunehag and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Sunehag and Hutter.", "year": 2015}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Richard Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton.,? \\Q1988\\E", "shortCiteRegEx": "Sutton.", "year": 1988}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "The paradoxes of confirmation: A survey", "author": ["Richard G Swinburne"], "venue": "American Philosophical Quarterly,", "citeRegEx": "Swinburne.,? \\Q1971\\E", "shortCiteRegEx": "Swinburne.", "year": 1971}, {"title": "Algorithms for Reinforcement Learning", "author": ["Csaba Szepesv\u00e1ri"], "venue": null, "citeRegEx": "Szepesv\u00e1ri.,? \\Q2010\\E", "shortCiteRegEx": "Szepesv\u00e1ri.", "year": 2010}, {"title": "Temporal difference learning and TD-Gammon", "author": ["Gerald Tesauro"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro.", "year": 1995}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William R Thompson"], "venue": null, "citeRegEx": "Thompson.,? \\Q1933\\E", "shortCiteRegEx": "Thompson.", "year": 1933}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["John N Tsitsiklis", "Benjamin Van Roy"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Tsitsiklis and Roy.,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis and Roy.", "year": 1997}, {"title": "Introduction to Nonparametric Estimation", "author": ["Alexandre Tsybakov"], "venue": null, "citeRegEx": "Tsybakov.,? \\Q2008\\E", "shortCiteRegEx": "Tsybakov.", "year": 2008}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["Hado van Hasselt", "Arthur Guez", "David Silver"], "venue": "In AAAI,", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "A MonteCarlo AIXI approximation", "author": ["Joel Veness", "Kee Siong Ng", "Marcus Hutter", "William Uther", "David Silver"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Veness et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Veness et al\\.", "year": 2011}, {"title": "Compress and control", "author": ["Joel Veness", "Marc G Bellemare", "Marcus Hutter", "Alvin Chua", "Guillaume Desjardins"], "venue": "In AAAI,", "citeRegEx": "Veness et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Veness et al\\.", "year": 2015}, {"title": "The coming technological singularity", "author": ["Vernor Vinge"], "venue": "Vision 21: Interdisciplinary Science and Engineering in the Era of Cyberspace,", "citeRegEx": "Vinge.,? \\Q1993\\E", "shortCiteRegEx": "Vinge.", "year": 1993}, {"title": "Normalized information distance", "author": ["Paul MB Vit\u00e1nyi", "Frank J Balbach", "Rudi L Cilibrasi", "Ming Li"], "venue": "In Information Theory and Statistical Learning,", "citeRegEx": "Vit\u00e1nyi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Vit\u00e1nyi et al\\.", "year": 2009}, {"title": "Bayesian reinforcement learning", "author": ["Nikos Vlassis", "Mohammad Ghavamzadeh", "Shie Mannor", "Pascal Poupart"], "venue": "Reinforcement Learning,", "citeRegEx": "Vlassis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Vlassis et al\\.", "year": 2012}, {"title": "Hempel\u2019s raven paradox: A lacuna in the standard Bayesian solution", "author": ["Peter BM Vranas"], "venue": "The British Journal for the Philosophy of Science,", "citeRegEx": "Vranas.,? \\Q2004\\E", "shortCiteRegEx": "Vranas.", "year": 2004}, {"title": "The singularity may never be near", "author": ["Toby Walsh"], "venue": "Technical report,", "citeRegEx": "Walsh.,? \\Q2016\\E", "shortCiteRegEx": "Walsh.", "year": 2016}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Ziyu Wang", "Nando de Freitas", "Tom Schaul", "Matteo Hessel", "Hado van Hasselt", "Marc Lanctot"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Random walk\u20141-dimensional", "author": ["Eric W Weisstein"], "venue": "In MathWorld\u2014A Wolfram Web Resource. Wolfram Research, Inc.,", "citeRegEx": "Weisstein.,? \\Q2002\\E", "shortCiteRegEx": "Weisstein.", "year": 2002}, {"title": "Non-)equivalence of universal priors", "author": ["Ian Wood", "Peter Sunehag", "Marcus Hutter"], "venue": "In Solomonoff 85th Memorial Conference,", "citeRegEx": "Wood et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wood et al\\.", "year": 2011}, {"title": "On convergence of emphatic temporal-difference learning", "author": ["Huizhen Yu"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Yu.,? \\Q2015\\E", "shortCiteRegEx": "Yu.", "year": 2015}, {"title": "Creating friendly AI 1.0: The analysis and design of benevolent goal architectures", "author": ["Eliezer Yudkowsky"], "venue": "Technical report, Singularity Institute for Artificial Intelligence,", "citeRegEx": "Yudkowsky.,? \\Q2001\\E", "shortCiteRegEx": "Yudkowsky.", "year": 2001}, {"title": "Artificial intelligence as a positive and negative factor in global risk. In Global Catastrophic Risks, pages 308\u2013345", "author": ["Eliezer Yudkowsky"], "venue": null, "citeRegEx": "Yudkowsky.,? \\Q2008\\E", "shortCiteRegEx": "Yudkowsky.", "year": 2008}, {"title": "Graying the black box: Understanding DQNs", "author": ["Tom Zahavy", "Nir Ben Zrihem", "Shie Mannor"], "venue": "Technical report, Israel Institute of Technology,", "citeRegEx": "Zahavy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zahavy et al\\.", "year": 2016}, {"title": "A universal algorithm for sequential data compression", "author": ["Jacob Ziv", "Abraham Lempel"], "venue": "IEEE Transactions on information theory,", "citeRegEx": "Ziv and Lempel.,? \\Q1977\\E", "shortCiteRegEx": "Ziv and Lempel.", "year": 1977}], "referenceMentions": [{"referenceID": 169, "context": "After the early enthusiastic decades, research in artificial intelligence (AI) now mainly aims at specific domains: playing games, mining data, processing natural language, recognizing objects in images, piloting robots, filtering email, and many others (Russell and Norvig, 2010).", "startOffset": 254, "endOffset": 280}, {"referenceID": 132, "context": "The goal of developing HLAI has a long tradition in AI research and was explicitly part of the 1956 Dartmouth conference that gave birth to the field of AI (McCarthy et al., 1955):", "startOffset": 156, "endOffset": 179}, {"referenceID": 18, "context": "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.", "startOffset": 48, "endOffset": 64}, {"referenceID": 18, "context": "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.", "startOffset": 48, "endOffset": 80}, {"referenceID": 18, "context": "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.", "startOffset": 48, "endOffset": 100}, {"referenceID": 14, "context": "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.", "startOffset": 101, "endOffset": 116}, {"referenceID": 14, "context": "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.", "startOffset": 101, "endOffset": 163}, {"referenceID": 14, "context": "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.", "startOffset": 101, "endOffset": 180}, {"referenceID": 14, "context": "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.", "startOffset": 101, "endOffset": 198}, {"referenceID": 198, "context": "Reinforcement learning studies algorithms that learn to act in an unknown environment through trial and error (Sutton and Barto, 1998; Szepesv\u00e1ri, 2010; Wiering and van Otterlo, 2012).", "startOffset": 110, "endOffset": 183}, {"referenceID": 200, "context": "Reinforcement learning studies algorithms that learn to act in an unknown environment through trial and error (Sutton and Barto, 1998; Szepesv\u00e1ri, 2010; Wiering and van Otterlo, 2012).", "startOffset": 110, "endOffset": 183}, {"referenceID": 198, "context": "Typically, the policy is slowly improved while learning, like SARSA (Sutton and Barto, 1998).", "startOffset": 68, "endOffset": 92}, {"referenceID": 164, "context": "This setting is well-analyzed (Puterman, 2014; Bertsekas and Tsitsiklis, 1995; Sutton and Barto, 1998), and there is a variety of algorithms that are known to learn the MDP asymptotically, such as TD learning (Sutton, 1988) and Q-learning (Watkins and Dayan, 1992).", "startOffset": 30, "endOffset": 102}, {"referenceID": 11, "context": "This setting is well-analyzed (Puterman, 2014; Bertsekas and Tsitsiklis, 1995; Sutton and Barto, 1998), and there is a variety of algorithms that are known to learn the MDP asymptotically, such as TD learning (Sutton, 1988) and Q-learning (Watkins and Dayan, 1992).", "startOffset": 30, "endOffset": 102}, {"referenceID": 198, "context": "This setting is well-analyzed (Puterman, 2014; Bertsekas and Tsitsiklis, 1995; Sutton and Barto, 1998), and there is a variety of algorithms that are known to learn the MDP asymptotically, such as TD learning (Sutton, 1988) and Q-learning (Watkins and Dayan, 1992).", "startOffset": 30, "endOffset": 102}, {"referenceID": 197, "context": "This setting is well-analyzed (Puterman, 2014; Bertsekas and Tsitsiklis, 1995; Sutton and Barto, 1998), and there is a variety of algorithms that are known to learn the MDP asymptotically, such as TD learning (Sutton, 1988) and Q-learning (Watkins and Dayan, 1992).", "startOffset": 209, "endOffset": 223}, {"referenceID": 4, "context": "Lattimore and Hutter (2012) use the algorithm UCRL\u03b3 (Auer et al., 2009) with geometric discounting with discount rate \u03b3 and derive the currently best-known PAC bound of \u00d5(\u2212T/(\u03b52(1 \u2212 \u03b3)3) log \u03b4) where T is the number of non-zero transitions in the MDP.", "startOffset": 52, "endOffset": 71}, {"referenceID": 198, "context": "In these cases, function approximation can be used to learn an approximation to the value function (Sutton and Barto, 1998).", "startOffset": 99, "endOffset": 123}, {"referenceID": 203, "context": "Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995).", "startOffset": 84, "endOffset": 138}, {"referenceID": 197, "context": "Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995).", "startOffset": 84, "endOffset": 138}, {"referenceID": 52, "context": "Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995).", "startOffset": 84, "endOffset": 138}, {"referenceID": 6, "context": "Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995).", "startOffset": 184, "endOffset": 197}, {"referenceID": 4, "context": "Auer et al. (2009) derive the regret bound \u00d5(dS \u221a At) for ergodic MDPs where d is the diameter of the MDP (how many steps a policy needs on average to get from one state of the MDP to any other), S is the number of states, A is the number of actions, and t is the number of time steps the algorithm runs.", "startOffset": 0, "endOffset": 19}, {"referenceID": 4, "context": "Auer et al. (2009) derive the regret bound \u00d5(dS \u221a At) for ergodic MDPs where d is the diameter of the MDP (how many steps a policy needs on average to get from one state of the MDP to any other), S is the number of states, A is the number of actions, and t is the number of time steps the algorithm runs. Second, given \u03b5 and \u03b4, a reinforcement learning algorithm is said to have sample complexity C(\u03b5, \u03b4) iff it is \u03b5-suboptimal for at most C(\u03b5, \u03b4) time steps with probability at least 1 \u2212 \u03b4 (probably approximately correct, PAC). For MDPs the first sample complexity bounds were due to Kakade (2003). Lattimore and Hutter (2012) use the algorithm UCRL\u03b3 (Auer et al.", "startOffset": 0, "endOffset": 600}, {"referenceID": 4, "context": "Auer et al. (2009) derive the regret bound \u00d5(dS \u221a At) for ergodic MDPs where d is the diameter of the MDP (how many steps a policy needs on average to get from one state of the MDP to any other), S is the number of states, A is the number of actions, and t is the number of time steps the algorithm runs. Second, given \u03b5 and \u03b4, a reinforcement learning algorithm is said to have sample complexity C(\u03b5, \u03b4) iff it is \u03b5-suboptimal for at most C(\u03b5, \u03b4) time steps with probability at least 1 \u2212 \u03b4 (probably approximately correct, PAC). For MDPs the first sample complexity bounds were due to Kakade (2003). Lattimore and Hutter (2012) use the algorithm UCRL\u03b3 (Auer et al.", "startOffset": 0, "endOffset": 629}, {"referenceID": 4, "context": "Auer et al. (2009) derive the regret bound \u00d5(dS \u221a At) for ergodic MDPs where d is the diameter of the MDP (how many steps a policy needs on average to get from one state of the MDP to any other), S is the number of states, A is the number of actions, and t is the number of time steps the algorithm runs. Second, given \u03b5 and \u03b4, a reinforcement learning algorithm is said to have sample complexity C(\u03b5, \u03b4) iff it is \u03b5-suboptimal for at most C(\u03b5, \u03b4) time steps with probability at least 1 \u2212 \u03b4 (probably approximately correct, PAC). For MDPs the first sample complexity bounds were due to Kakade (2003). Lattimore and Hutter (2012) use the algorithm UCRL\u03b3 (Auer et al., 2009) with geometric discounting with discount rate \u03b3 and derive the currently best-known PAC bound of \u00d5(\u2212T/(\u03b52(1 \u2212 \u03b3)3) log \u03b4) where T is the number of non-zero transitions in the MDP. Typically, algorithms for MDPs rely on visiting every state multiple times (or even infinitely often), which becomes infeasible for large state spaces (e.g. a video game screen consisting of millions of pixels). In these cases, function approximation can be used to learn an approximation to the value function (Sutton and Barto, 1998). Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995). A recent breakthrough was made by Mahmood et al. (2015) and Yu (2015)", "startOffset": 0, "endOffset": 1445}, {"referenceID": 4, "context": "Auer et al. (2009) derive the regret bound \u00d5(dS \u221a At) for ergodic MDPs where d is the diameter of the MDP (how many steps a policy needs on average to get from one state of the MDP to any other), S is the number of states, A is the number of actions, and t is the number of time steps the algorithm runs. Second, given \u03b5 and \u03b4, a reinforcement learning algorithm is said to have sample complexity C(\u03b5, \u03b4) iff it is \u03b5-suboptimal for at most C(\u03b5, \u03b4) time steps with probability at least 1 \u2212 \u03b4 (probably approximately correct, PAC). For MDPs the first sample complexity bounds were due to Kakade (2003). Lattimore and Hutter (2012) use the algorithm UCRL\u03b3 (Auer et al., 2009) with geometric discounting with discount rate \u03b3 and derive the currently best-known PAC bound of \u00d5(\u2212T/(\u03b52(1 \u2212 \u03b3)3) log \u03b4) where T is the number of non-zero transitions in the MDP. Typically, algorithms for MDPs rely on visiting every state multiple times (or even infinitely often), which becomes infeasible for large state spaces (e.g. a video game screen consisting of millions of pixels). In these cases, function approximation can be used to learn an approximation to the value function (Sutton and Barto, 1998). Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995). A recent breakthrough was made by Mahmood et al. (2015) and Yu (2015)", "startOffset": 0, "endOffset": 1459}, {"referenceID": 89, "context": "Among the historical successes of reinforcement learning is autonomous helicopter piloting (Kim et al., 2003) and TD-Gammon, a backgammon algorithm that learned through self-play (Tesauro, 1995), similar to AlphaGo (Silver et al.", "startOffset": 91, "endOffset": 109}, {"referenceID": 201, "context": ", 2003) and TD-Gammon, a backgammon algorithm that learned through self-play (Tesauro, 1995), similar to AlphaGo (Silver et al.", "startOffset": 77, "endOffset": 92}, {"referenceID": 65, "context": "This approach to general AI is in accordance with the definition of intelligence given by Legg and Hutter (2007b):", "startOffset": 99, "endOffset": 114}, {"referenceID": 8, "context": "\u2019 A popular such selection is the Atari 2600 video game console (Bellemare et al., 2013).", "startOffset": 64, "endOffset": 88}, {"referenceID": 179, "context": "1 DQN rides the wave of success of deep learning (LeCun et al., 2015; Schmidhuber, 2015; Goodfellow et al., 2016).", "startOffset": 49, "endOffset": 113}, {"referenceID": 50, "context": "1 DQN rides the wave of success of deep learning (LeCun et al., 2015; Schmidhuber, 2015; Goodfellow et al., 2016).", "startOffset": 49, "endOffset": 113}, {"referenceID": 9, "context": "Since the introduction of DQN there have been numerous improvements on this algorithm: increasing the gap on the Q-values of different actions (Bellemare et al., 2016), training in parallel (Nair et al.", "startOffset": 143, "endOffset": 167}, {"referenceID": 142, "context": ", 2016), training in parallel (Nair et al., 2015; Mnih et al., 2016), improvements to the experience replay mechanism (Schaul et al.", "startOffset": 30, "endOffset": 68}, {"referenceID": 136, "context": ", 2016), training in parallel (Nair et al., 2015; Mnih et al., 2016), improvements to the experience replay mechanism (Schaul et al.", "startOffset": 30, "endOffset": 68}, {"referenceID": 176, "context": ", 2016), improvements to the experience replay mechanism (Schaul et al., 2016), generalization to continuous action spaces (Lillicrap et al.", "startOffset": 57, "endOffset": 78}, {"referenceID": 123, "context": ", 2016), generalization to continuous action spaces (Lillicrap et al., 2016), solve the overestimation problem (van Hasselt et al.", "startOffset": 52, "endOffset": 76}, {"referenceID": 213, "context": ", 2016), and improvements to the neural network architecture (Wang et al., 2016).", "startOffset": 61, "endOffset": 80}, {"referenceID": 8, "context": "Since the introduction of DQN there have been numerous improvements on this algorithm: increasing the gap on the Q-values of different actions (Bellemare et al., 2016), training in parallel (Nair et al., 2015; Mnih et al., 2016), improvements to the experience replay mechanism (Schaul et al., 2016), generalization to continuous action spaces (Lillicrap et al., 2016), solve the overestimation problem (van Hasselt et al., 2016), and improvements to the neural network architecture (Wang et al., 2016). TheQvalues learned by DQN\u2019s neural networks are intransparent to inspection; Zahavy et al. (2016) use visualization techniques on the Q-value networks.", "startOffset": 144, "endOffset": 602}, {"referenceID": 8, "context": "Since the introduction of DQN there have been numerous improvements on this algorithm: increasing the gap on the Q-values of different actions (Bellemare et al., 2016), training in parallel (Nair et al., 2015; Mnih et al., 2016), improvements to the experience replay mechanism (Schaul et al., 2016), generalization to continuous action spaces (Lillicrap et al., 2016), solve the overestimation problem (van Hasselt et al., 2016), and improvements to the neural network architecture (Wang et al., 2016). TheQvalues learned by DQN\u2019s neural networks are intransparent to inspection; Zahavy et al. (2016) use visualization techniques on the Q-value networks. Finally, Liang et al. (2016) managed to reproduce DQN\u2019s success using only linear function approximation (no neural networks).", "startOffset": 144, "endOffset": 685}, {"referenceID": 59, "context": "An obvious approach to equip DQN with memory is to use recurrent neural networks instead of simple feedforward neural networks (Heess et al., 2015).", "startOffset": 127, "endOffset": 147}, {"referenceID": 10, "context": "However, it is currently unclear whether recurrent neural networks are powerful enough to learn long-term dependencies in the data (Bengio et al., 1994).", "startOffset": 131, "endOffset": 152}, {"referenceID": 55, "context": "Hausknecht and Stone (2015) show that this enables the agent to play the games when using only a single frame as input.", "startOffset": 0, "endOffset": 28}, {"referenceID": 159, "context": "by knowledge-seeking agents (Orseau, 2011, 2014a; Orseau et al., 2013).", "startOffset": 28, "endOffset": 70}, {"referenceID": 148, "context": "In this setting, the most efficient way to get rewards is to modify the reward mechanism to always provide the maximal reward (Omohundro, 2008; Ring and Orseau, 2011; Bostrom, 2014).", "startOffset": 126, "endOffset": 181}, {"referenceID": 166, "context": "In this setting, the most efficient way to get rewards is to modify the reward mechanism to always provide the maximal reward (Omohundro, 2008; Ring and Orseau, 2011; Bostrom, 2014).", "startOffset": 126, "endOffset": 181}, {"referenceID": 16, "context": "In this setting, the most efficient way to get rewards is to modify the reward mechanism to always provide the maximal reward (Omohundro, 2008; Ring and Orseau, 2011; Bostrom, 2014).", "startOffset": 126, "endOffset": 181}, {"referenceID": 87, "context": "Kulkarni et al. (2016) introduce a hierarchical approach based on intrinsic motivation to improve DQN\u2019s exploration and manage to score points in Montezuma\u2019s Revenge.", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "In this setting, the most efficient way to get rewards is to modify the reward mechanism to always provide the maximal reward (Omohundro, 2008; Ring and Orseau, 2011; Bostrom, 2014). Consequently the agent no longer pursues the designers\u2019 originally intended goals and instead only attempts to protect its own existence. The name wireheading was established by analogy to a biology experiment by Olds and Milner (1954) in which rats had a wire embedded into the reward center of their brain that they could then stimulate by the push of a button.", "startOffset": 167, "endOffset": 419}, {"referenceID": 32, "context": ", 2003, 2004), and Bayesian methods (Doshi-Velez, 2012).", "startOffset": 36, "endOffset": 55}, {"referenceID": 193, "context": "The quest for a good cost function for feature maps remains unsuccessful thus far (Sunehag and Hutter, 2010; Daswani, 2015).", "startOffset": 82, "endOffset": 123}, {"referenceID": 26, "context": "The quest for a good cost function for feature maps remains unsuccessful thus far (Sunehag and Hutter, 2010; Daswani, 2015).", "startOffset": 82, "endOffset": 123}, {"referenceID": 165, "context": "Based in algorithmic information theory, Solomonoff\u2019s prior draws from famous insights by William of Ockham, Sextus Epicurus, Alan Turing, and Andrey Kolmogorov (Rathmanner and Hutter, 2011).", "startOffset": 161, "endOffset": 190}, {"referenceID": 95, "context": "A typical optimality property in general reinforcement learning is asymptotic optimality (Lattimore and Hutter, 2011): as time progresses the agent converges to achieve the same rewards as the optimal policy.", "startOffset": 89, "endOffset": 117}, {"referenceID": 197, "context": "Asymptotic optimality is usually what is meant by \u201cQ-learning converges\u201d (Watkins and Dayan, 1992) or \u201cTD learning converges\u201d (Sutton, 1988).", "startOffset": 126, "endOffset": 140}, {"referenceID": 26, "context": "The quest for a good cost function for feature maps remains unsuccessful thus far (Sunehag and Hutter, 2010; Daswani, 2015). However, Hutter (2014) managed to derive strong bounds relating the optimal value function of the aggregated MDP to the value function of the original process even if the latter violates the Markov condition.", "startOffset": 109, "endOffset": 148}, {"referenceID": 26, "context": "The quest for a good cost function for feature maps remains unsuccessful thus far (Sunehag and Hutter, 2010; Daswani, 2015). However, Hutter (2014) managed to derive strong bounds relating the optimal value function of the aggregated MDP to the value function of the original process even if the latter violates the Markov condition. A full theoretical approach to the general reinforcement learning problem is given by Hutter (2000, 2001a, 2002a, 2003, 2005, 2007a, 2012b). He introduces the Bayesian RL agent AIXI building on the theory of sequence prediction by Solomonoff (1964, 1978). Based in algorithmic information theory, Solomonoff\u2019s prior draws from famous insights by William of Ockham, Sextus Epicurus, Alan Turing, and Andrey Kolmogorov (Rathmanner and Hutter, 2011). AIXI uses Solomonoff\u2019s prior over the class of all computable environments and acts to maximize Bayes-expected rewards. We formally introduce Solomonoff\u2019s theory of induction in Chapter 3 and AIXI in Section 4.3.1. See also Legg (2008) for an accessible introduction to AIXI.", "startOffset": 109, "endOffset": 1018}, {"referenceID": 65, "context": "1) Here, intelligence refers to an agent that optimizes towards some goal in accordance with the definition by Legg and Hutter (2007b). For learning we distinguish two (very related) aspects: (1) arriving at accurate beliefs about the future and (2) making accurate predictions about the future.", "startOffset": 120, "endOffset": 135}, {"referenceID": 135, "context": ", 2011, 2015) are easily outperformed by neural-network-based approaches (Mnih et al., 2015).", "startOffset": 73, "endOffset": 92}, {"referenceID": 65, "context": "Hutter (2002a) showed that AIXI is Pareto optimal, balanced Pareto optimal, and self-optimizing.", "startOffset": 0, "endOffset": 15}, {"referenceID": 65, "context": "Hutter (2002a) showed that AIXI is Pareto optimal, balanced Pareto optimal, and self-optimizing. Orseau (2013) established that AIXI does not achieve asymptotic optimality in all computable environments (making the self-optimizing result inapplicable to this general environment class).", "startOffset": 0, "endOffset": 111}, {"referenceID": 42, "context": "Chapter Publication(s) Chapter 1 Chapter 2 Chapter 3 with links to Leike and Hutter (2014a, 2015d); Filan et al. (2016) Chapter 4 Chapter 5 Leike and Hutter (2015c); Leike et al.", "startOffset": 100, "endOffset": 120}, {"referenceID": 42, "context": "Chapter Publication(s) Chapter 1 Chapter 2 Chapter 3 with links to Leike and Hutter (2014a, 2015d); Filan et al. (2016) Chapter 4 Chapter 5 Leike and Hutter (2015c); Leike et al.", "startOffset": 100, "endOffset": 165}, {"referenceID": 42, "context": "Chapter Publication(s) Chapter 1 Chapter 2 Chapter 3 with links to Leike and Hutter (2014a, 2015d); Filan et al. (2016) Chapter 4 Chapter 5 Leike and Hutter (2015c); Leike et al. (2016a) Chapter 6 Leike and Hutter (2015b,a, 2016) Chapter 7 Leike et al.", "startOffset": 100, "endOffset": 187}, {"referenceID": 42, "context": "Chapter Publication(s) Chapter 1 Chapter 2 Chapter 3 with links to Leike and Hutter (2014a, 2015d); Filan et al. (2016) Chapter 4 Chapter 5 Leike and Hutter (2015c); Leike et al. (2016a) Chapter 6 Leike and Hutter (2015b,a, 2016) Chapter 7 Leike et al. (2016b) Chapter 8 Appendix A Leike and Hutter (2014b)", "startOffset": 100, "endOffset": 261}, {"referenceID": 42, "context": "Chapter Publication(s) Chapter 1 Chapter 2 Chapter 3 with links to Leike and Hutter (2014a, 2015d); Filan et al. (2016) Chapter 4 Chapter 5 Leike and Hutter (2015c); Leike et al. (2016a) Chapter 6 Leike and Hutter (2015b,a, 2016) Chapter 7 Leike et al. (2016b) Chapter 8 Appendix A Leike and Hutter (2014b)", "startOffset": 100, "endOffset": 307}, {"referenceID": 45, "context": "Only small classes are known to have a grain of truth and the literature contains several related impossibility results (Nachbar, 1997, 2005; Foster and Young, 2001).", "startOffset": 120, "endOffset": 165}, {"referenceID": 26, "context": "(2015, 2016) based on my research in termination analysis (in collaboration with Matthias Heizmann), Daswani and Leike (2015) (coauthored with Mayank Daswani in equal parts), Everitt et al.", "startOffset": 101, "endOffset": 126}, {"referenceID": 26, "context": "(2015, 2016) based on my research in termination analysis (in collaboration with Matthias Heizmann), Daswani and Leike (2015) (coauthored with Mayank Daswani in equal parts), Everitt et al. (2015) (co-authored with Tom Everitt in equal parts), Filan et al.", "startOffset": 101, "endOffset": 197}, {"referenceID": 26, "context": "(2015, 2016) based on my research in termination analysis (in collaboration with Matthias Heizmann), Daswani and Leike (2015) (coauthored with Mayank Daswani in equal parts), Everitt et al. (2015) (co-authored with Tom Everitt in equal parts), Filan et al. (2016) (written by Daniel Filan as part of his honour\u2019s thesis supervised by Marcus Hutter and me).", "startOffset": 101, "endOffset": 264}, {"referenceID": 26, "context": "(2015, 2016) based on my research in termination analysis (in collaboration with Matthias Heizmann), Daswani and Leike (2015) (coauthored with Mayank Daswani in equal parts), Everitt et al. (2015) (co-authored with Tom Everitt in equal parts), Filan et al. (2016) (written by Daniel Filan as part of his honour\u2019s thesis supervised by Marcus Hutter and me). Leike and Hutter (2016) is", "startOffset": 101, "endOffset": 381}, {"referenceID": 31, "context": "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vit\u00e1nyi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al.", "startOffset": 56, "endOffset": 71}, {"referenceID": 24, "context": "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vit\u00e1nyi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al.", "startOffset": 119, "endOffset": 143}, {"referenceID": 24, "context": "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vit\u00e1nyi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al.", "startOffset": 119, "endOffset": 188}, {"referenceID": 24, "context": "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vit\u00e1nyi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al.", "startOffset": 119, "endOffset": 249}, {"referenceID": 12, "context": "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vit\u00e1nyi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al.", "startOffset": 277, "endOffset": 291}, {"referenceID": 12, "context": "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vit\u00e1nyi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al. (2009) on machine learning, Sutton and Barto (1998) on reinforcement learning, and Hutter (2005) and Lattimore (2013) on general reinforcement learning.", "startOffset": 277, "endOffset": 316}, {"referenceID": 12, "context": "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vit\u00e1nyi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al. (2009) on machine learning, Sutton and Barto (1998) on reinforcement learning, and Hutter (2005) and Lattimore (2013) on general reinforcement learning.", "startOffset": 277, "endOffset": 361}, {"referenceID": 12, "context": "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vit\u00e1nyi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al. (2009) on machine learning, Sutton and Barto (1998) on reinforcement learning, and Hutter (2005) and Lattimore (2013) on general reinforcement learning.", "startOffset": 277, "endOffset": 406}, {"referenceID": 12, "context": "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vit\u00e1nyi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al. (2009) on machine learning, Sutton and Barto (1998) on reinforcement learning, and Hutter (2005) and Lattimore (2013) on general reinforcement learning.", "startOffset": 277, "endOffset": 427}, {"referenceID": 33, "context": "This section provides a concise introduction to measure theory; see Durrett (2010) for an extensive treatment.", "startOffset": 68, "endOffset": 83}, {"referenceID": 21, "context": "learning has focused on predicting individual symbols and bounds on the number of prediction errors (Hutter, 2001b, 2005; Cesa-Bianchi and Lugosi, 2006), and the results on merging are from the game theory literature (Blackwell and Dubins, 1962; Kalai and Lehrer, 1994; Lehrer and Smorodinsky, 1996).", "startOffset": 100, "endOffset": 152}, {"referenceID": 13, "context": "learning has focused on predicting individual symbols and bounds on the number of prediction errors (Hutter, 2001b, 2005; Cesa-Bianchi and Lugosi, 2006), and the results on merging are from the game theory literature (Blackwell and Dubins, 1962; Kalai and Lehrer, 1994; Lehrer and Smorodinsky, 1996).", "startOffset": 217, "endOffset": 299}, {"referenceID": 87, "context": "learning has focused on predicting individual symbols and bounds on the number of prediction errors (Hutter, 2001b, 2005; Cesa-Bianchi and Lugosi, 2006), and the results on merging are from the game theory literature (Blackwell and Dubins, 1962; Kalai and Lehrer, 1994; Lehrer and Smorodinsky, 1996).", "startOffset": 217, "endOffset": 299}, {"referenceID": 106, "context": "learning has focused on predicting individual symbols and bounds on the number of prediction errors (Hutter, 2001b, 2005; Cesa-Bianchi and Lugosi, 2006), and the results on merging are from the game theory literature (Blackwell and Dubins, 1962; Kalai and Lehrer, 1994; Lehrer and Smorodinsky, 1996).", "startOffset": 217, "endOffset": 299}, {"referenceID": 21, "context": "A well-known approach to the nonrealizable case is prediction with expert advice (Cesa-Bianchi and Lugosi, 2006), which we do not con-", "startOffset": 81, "endOffset": 112}, {"referenceID": 12, "context": "); see Bishop (2006) and Hastie et al.", "startOffset": 7, "endOffset": 21}, {"referenceID": 12, "context": "); see Bishop (2006) and Hastie et al. (2009). In this chapter we do not make the i.", "startOffset": 7, "endOffset": 46}, {"referenceID": 152, "context": "Generally, the nonrealizable case is harder, but Ryabko (2011) argues that for some problems, both cases coincide.", "startOffset": 49, "endOffset": 63}, {"referenceID": 65, "context": "6 connects the results from the first sections to the learning framework developed by Solomonoff (1964, 1978), Hutter (2001b, 2005, 2007b), and Schmidhuber (2002) (among others).", "startOffset": 111, "endOffset": 163}, {"referenceID": 65, "context": "1 (The Black Ravens; Rathmanner and Hutter, 2011, Sec. 7.4). If we live in a world in which all ravens are black, how can we learn this fact? Since at every time step we have observed only a finite subset of the (possibly infinite) set of all ravens, how can we confidently state anything about all ravens? We formalize this problem in line with Rathmanner and Hutter (2011, Sec. 7.4) and Leike and Hutter (2015d). We define two predicates, blackness B and ravenness R.", "startOffset": 36, "endOffset": 414}, {"referenceID": 65, "context": "It is very easy to derive from a countable set of distributions, and it has been considered extensively in the literature (Solomonoff, 1964; Jaynes, 2003; Hutter, 2005, . . . ). Ryabko (2009) shows that even for uncountably infinite classes, if there are good predictors, then a Bayesian mixture over a countable subclass asymptotically also does well.", "startOffset": 155, "endOffset": 192}, {"referenceID": 65, "context": "It is very easy to derive from a countable set of distributions, and it has been considered extensively in the literature (Solomonoff, 1964; Jaynes, 2003; Hutter, 2005, . . . ). Ryabko (2009) shows that even for uncountably infinite classes, if there are good predictors, then a Bayesian mixture over a countable subclass asymptotically also does well. Example 3.5 (Solomonoff Prior). Solomonoff (1964) defines a distribution M over X ] that assigns to a string x the probability that the universal monotone Turing machine U outputs x when fed with fair coin flips on the input tape.", "startOffset": 155, "endOffset": 403}, {"referenceID": 65, "context": "See Rathmanner and Hutter (2011) for a discussion on the philosophical underpinnings of Solomonoff\u2019s prior.", "startOffset": 19, "endOffset": 33}, {"referenceID": 65, "context": "Ryabko and Hutter (2007, 2008) consider the following definition. It is analogous to Definition 3.3, except that the constant c is allowed to depend on time. Definition 3.10 (Dominance with Coefficients; Ryabko and Hutter, 2008, Def. 2). The measure Q dominates P with coefficients f (Q \u2265 P/f) iff Q(x) \u2265 P (x)/f(|x|) for all x \u2208 X \u2217. If Q dominates P with coefficients f and f grows subexponentially (f \u2208 o(exp)), then Q weakly dominates P by Remark 3.9. Example 3.11 (Speed Prior). Schmidhuber (2002) defines a variant of Solomonoff\u2019s priorM that penalizes programs by their running time, called the speed prior.", "startOffset": 11, "endOffset": 503}, {"referenceID": 53, "context": "14 (The Minimum Description Length Principle; Gr\u00fcnwald, 2007).", "startOffset": 3, "endOffset": 61}, {"referenceID": 53, "context": "14 (The Minimum Description Length Principle; Gr\u00fcnwald, 2007). Let M be a countable set of probability measures on (X,F\u221e) and let K : M \u2192 [0, 1] be a function such that \u2211 P\u2208M 2 \u2212K(P ) \u2264 1 called regularizer. Following notation from Hutter (2009a), we define for each x \u2208 X \u2217 the minimal description length model as MDL := arg min P\u2208M {\u2212 logP (x) +K(P )}.", "startOffset": 46, "endOffset": 247}, {"referenceID": 86, "context": "Much of this section is based on Kalai and Lehrer (1994) and Lehrer and Smorodinsky (1996).", "startOffset": 33, "endOffset": 57}, {"referenceID": 86, "context": "Much of this section is based on Kalai and Lehrer (1994) and Lehrer and Smorodinsky (1996).", "startOffset": 33, "endOffset": 91}, {"referenceID": 13, "context": "25 (Absolute Continuity\u21d2 Strong Merging; Blackwell and Dubins, 1962).", "startOffset": 3, "endOffset": 68}, {"referenceID": 13, "context": "The following theorem is the famous merging of opinions theorem by Blackwell and Dubins (1962). Theorem 3.", "startOffset": 67, "endOffset": 95}, {"referenceID": 65, "context": "More generally, we could also follow Hutter (2001b) and phrase predictive performance in terms of loss: given a loss function ` : X \u00d7X \u2192 R the predictor Q suffers an (instantaneous) loss of `(xQt , xt ) in time step t.", "startOffset": 37, "endOffset": 52}, {"referenceID": 65, "context": "1 Dominance We start with the prediction regret bounds proved by Hutter (2001b) in case the learning distribution Q dominates the true distribution P .", "startOffset": 65, "endOffset": 80}, {"referenceID": 214, "context": "We have (Weisstein, 2002)", "startOffset": 8, "endOffset": 25}, {"referenceID": 133, "context": "The proof idea is inspired by Miller and Sanchirico (1999). We think of P and Q as two players in a zero-sum betting game.", "startOffset": 30, "endOffset": 59}, {"referenceID": 65, "context": "See Rathmanner and Hutter (2011) for a very readable introduction to Solomonoff\u2019s theory and its philosophical motivations and Sterkenburg (2016) for a critique of its optimality.", "startOffset": 19, "endOffset": 33}, {"referenceID": 65, "context": "See Rathmanner and Hutter (2011) for a very readable introduction to Solomonoff\u2019s theory and its philosophical motivations and Sterkenburg (2016) for a critique of its optimality.", "startOffset": 19, "endOffset": 146}, {"referenceID": 42, "context": "In this section we state merging and prediction results for SKt, a speed prior introduced by Filan et al. (2016) formally defined in Example 3.", "startOffset": 93, "endOffset": 113}, {"referenceID": 42, "context": "In this section we state merging and prediction results for SKt, a speed prior introduced by Filan et al. (2016) formally defined in Example 3.11. It is slightly different from the speed prior defined by Schmidhuber (2002), but for the latter no compatibility properties are known for nondeterministic measures.", "startOffset": 93, "endOffset": 223}, {"referenceID": 54, "context": "G\u00e1cs (1983) shows that the similarity M \u2248 2\u2212Km is not an equality.", "startOffset": 0, "endOffset": 12}, {"referenceID": 29, "context": "to Day (2011) who shows that Km(x) > \u2212 logM(x) +O(log log |x|) for infinitely many x \u2208 X \u2217.", "startOffset": 3, "endOffset": 14}, {"referenceID": 29, "context": "to Day (2011) who shows that Km(x) > \u2212 logM(x) +O(log log |x|) for infinitely many x \u2208 X \u2217. Nevertheless, 2\u2212Km dominates every computable measure (Li and Vit\u00e1nyi, 2008, Thm. 4.5.4 and Lem. 4.5.6ii(d); originally proved by Levin, 1973). Hence all the strong results that hold for Solomonoff induction (prediction regret and strong merging) also hold for compression: we apply Theorem 3.25 and Corollary 3.44 to get the following results. See Hutter (2006a) for further discussion on using the universal compressor Km for learning.", "startOffset": 3, "endOffset": 456}, {"referenceID": 65, "context": "In this spirit, the Hutter prize is awarded for the compression of a 100MB excerpt from the English Wikipedia (Hutter, 2006c). Practical compression algorithms (such as the algorithm by Ziv and Lempel (1977) used in gzip) are not universal.", "startOffset": 20, "endOffset": 208}, {"referenceID": 65, "context": "In this spirit, the Hutter prize is awarded for the compression of a 100MB excerpt from the English Wikipedia (Hutter, 2006c). Practical compression algorithms (such as the algorithm by Ziv and Lempel (1977) used in gzip) are not universal. Hence they do not dominate every computable distribution. As with the speed prior, what matters is the rate at which Yt = Q(x1:t)/P (x1:t) goes to 0, i.e., does the compressor weakly dominate the true distribution in the sense of Definition 3.8? Veness et al. (2015) successfully apply the Lempel-Ziv compression algorithm as a learning algorithm for reinforcement learning; however, some preprocessing of the data is required.", "startOffset": 20, "endOffset": 508}, {"referenceID": 65, "context": "In this spirit, the Hutter prize is awarded for the compression of a 100MB excerpt from the English Wikipedia (Hutter, 2006c). Practical compression algorithms (such as the algorithm by Ziv and Lempel (1977) used in gzip) are not universal. Hence they do not dominate every computable distribution. As with the speed prior, what matters is the rate at which Yt = Q(x1:t)/P (x1:t) goes to 0, i.e., does the compressor weakly dominate the true distribution in the sense of Definition 3.8? Veness et al. (2015) successfully apply the Lempel-Ziv compression algorithm as a learning algorithm for reinforcement learning; however, some preprocessing of the data is required. More remotely, Vit\u00e1nyi et al. (2009) use standard compression algorithms to classify mammal genomes, languages, and classical music.", "startOffset": 20, "endOffset": 706}, {"referenceID": 13, "context": "3i) Blackwell and Dubins (1962)", "startOffset": 4, "endOffset": 32}, {"referenceID": 63, "context": "The paradox of confirmation, also known as Hempel\u2019s paradox (Hempel, 1945), relies on the following three principles.", "startOffset": 60, "endOffset": 74}, {"referenceID": 63, "context": "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.", "startOffset": 106, "endOffset": 199}, {"referenceID": 47, "context": "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.", "startOffset": 106, "endOffset": 199}, {"referenceID": 124, "context": "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.", "startOffset": 106, "endOffset": 199}, {"referenceID": 49, "context": "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.", "startOffset": 106, "endOffset": 199}, {"referenceID": 64, "context": "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.", "startOffset": 106, "endOffset": 199}, {"referenceID": 128, "context": "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.", "startOffset": 106, "endOffset": 199}, {"referenceID": 211, "context": "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.", "startOffset": 106, "endOffset": 199}, {"referenceID": 124, "context": "Support for Nicod\u2019s criterion is not uncommon (Mackie, 1963; Hempel, 1967; Maher, 1999) and no consensus is in sight.", "startOffset": 46, "endOffset": 87}, {"referenceID": 64, "context": "Support for Nicod\u2019s criterion is not uncommon (Mackie, 1963; Hempel, 1967; Maher, 1999) and no consensus is in sight.", "startOffset": 46, "endOffset": 87}, {"referenceID": 128, "context": "Support for Nicod\u2019s criterion is not uncommon (Mackie, 1963; Hempel, 1967; Maher, 1999) and no consensus is in sight.", "startOffset": 46, "endOffset": 87}, {"referenceID": 47, "context": "A Bayesian reasoner might be tempted to argue that a green apple does confirm the hypothesis H, but only to a small degree, since there are vastly more non-black objects than ravens (Good, 1960).", "startOffset": 182, "endOffset": 194}, {"referenceID": 47, "context": "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.", "startOffset": 121, "endOffset": 222}, {"referenceID": 179, "context": "Vranas (2004) shows that this solution is equivalent to the assertion that blackness is equally probable regardless of whether H holds: P (black|H) \u2248 P (black).", "startOffset": 0, "endOffset": 14}, {"referenceID": 47, "context": "The following is a very concise example against the standard Bayesian solution by Good (1967): There are two possible worlds, the first has 100 black ravens and a million other birds, while the second has 1000 black ravens, one white raven, and a million other birds.", "startOffset": 82, "endOffset": 94}, {"referenceID": 47, "context": "The following is a very concise example against the standard Bayesian solution by Good (1967): There are two possible worlds, the first has 100 black ravens and a million other birds, while the second has 1000 black ravens, one white raven, and a million other birds. Now we draw a bird uniformly at random, and it turns out to be a black raven. Contrary to what Nicod\u2019s criterion claims, this is strong evidence that we are in fact in the second world, and in this world non-black ravens exist. For another, more intuitive example: Suppose you do not know anything about ravens and you have a friend who collects atypical objects. If you see a black raven in her collection, surely this would not increase your belief in the hypothesis that all ravens are black. In Leike and Hutter (2015d) we investigate the paradox of confirmation in the context of Solomonoff induction.", "startOffset": 82, "endOffset": 792}, {"referenceID": 65, "context": "It is mostly based on Hutter (2005) and Lattimore (2013).", "startOffset": 22, "endOffset": 36}, {"referenceID": 65, "context": "It is mostly based on Hutter (2005) and Lattimore (2013). Section 4.", "startOffset": 22, "endOffset": 57}, {"referenceID": 65, "context": "Equivalently, Hutter (2005) defines environments as chronological contextual semimeasures.", "startOffset": 14, "endOffset": 28}, {"referenceID": 65, "context": "Hutter (2005) calls them chronological conditional semimeasures.", "startOffset": 0, "endOffset": 14}, {"referenceID": 37, "context": "Decision making in the physicalistic model is still underdeveloped; see Everitt et al. (2015) and Orseau and Ring (2012a).", "startOffset": 72, "endOffset": 94}, {"referenceID": 37, "context": "Decision making in the physicalistic model is still underdeveloped; see Everitt et al. (2015) and Orseau and Ring (2012a). In this thesis we restrict ourselves to the dualistic model.", "startOffset": 72, "endOffset": 122}, {"referenceID": 65, "context": "For a discussion of general discounting we refer the reader to Lattimore and Hutter (2014).", "startOffset": 77, "endOffset": 91}, {"referenceID": 3, "context": "Moreover, with unbounded rewards there all kinds of pathological problems where defining optimal actions is no longer straightforward; see Arntzenius et al. (2004) for a discussion.", "startOffset": 139, "endOffset": 164}, {"referenceID": 19, "context": "Bandits exist in many flavors; see Bubeck and Bianchi (2012) for a survey.", "startOffset": 35, "endOffset": 61}, {"referenceID": 198, "context": "Much of today\u2019s literature on reinforcement learning focuses on MDPs (Sutton and Barto, 1998).", "startOffset": 69, "endOffset": 93}, {"referenceID": 135, "context": "While they have a huge state space2 they can still be learned using Q-learning with function approximation (Mnih et al., 2015).", "startOffset": 107, "endOffset": 126}, {"referenceID": 7, "context": "For any physical system of finite volume and finite (average) energy, the amount of information it can contain is finite (Bekenstein, 1981), and so is the number of state transitions per unit of time (Margolus and Levitin, 1998).", "startOffset": 121, "endOffset": 139}, {"referenceID": 130, "context": "For any physical system of finite volume and finite (average) energy, the amount of information it can contain is finite (Bekenstein, 1981), and so is the number of state transitions per unit of time (Margolus and Levitin, 1998).", "startOffset": 200, "endOffset": 228}, {"referenceID": 65, "context": "12 (Optimal Policy; Hutter, 2005, Def. 5.19 & 5.30). A policy \u03c0 is optimal in environment \u03bd (\u03bd-optimal) iff for all histories \u03c0 attains the optimal value: V \u03c0 \u03bd (\u00e6<t) = V \u2217 \u03bd (\u00e6<t) for all \u00e6<t \u2208 (A\u00d7E)\u2217. The action at is an optimal action iff \u03c0\u2217 \u03bd(at | \u00e6<t) = 1 for some \u03bd-optimal policy \u03c0\u2217 \u03bd . Following the tradition of Hutter (2005), AINU denotes a \u03bd-optimal policy for the environment \u03bd \u2208 MCCS LSC and AIMU denotes an \u03bc-optimal policy for the environment \u03bc \u2208MCCM comp that is a measure (as opposed to a semimeasure).", "startOffset": 20, "endOffset": 335}, {"referenceID": 215, "context": "5) can be defined equivalently according to (Wood et al., 2011) \u03be(e<t \u2016 a<t) := \u2211", "startOffset": 44, "endOffset": 63}, {"referenceID": 94, "context": "This strategy even achieves the optimal asymptotic regret bounds (Lattimore, 2016).", "startOffset": 65, "endOffset": 82}, {"referenceID": 45, "context": "For multi-armed bandits, Gittins (1979) achieved a breakthrough with an index strategy that enables the computation of the optimal policy by computing one quantity for each arm independently of the rest.", "startOffset": 25, "endOffset": 40}, {"referenceID": 45, "context": "For multi-armed bandits, Gittins (1979) achieved a breakthrough with an index strategy that enables the computation of the optimal policy by computing one quantity for each arm independently of the rest. This strategy even achieves the optimal asymptotic regret bounds (Lattimore, 2016). Larger classes have also been attempted: using Monte-Carlo tree search, Veness et al. (2011) approximate the Bayes optimal policy in the class of all context trees.", "startOffset": 25, "endOffset": 381}, {"referenceID": 32, "context": "Doshi-Velez (2012) uses Bayesian techniques to learn infinite-state POMDPs.", "startOffset": 0, "endOffset": 19}, {"referenceID": 32, "context": "Doshi-Velez (2012) uses Bayesian techniques to learn infinite-state POMDPs. See Vlassis et al. (2012) for a survey on Bayesian techniques in RL.", "startOffset": 0, "endOffset": 102}, {"referenceID": 149, "context": "In this section we discuss two variants of knowledge-seeking agents: entropy-seeking agents introduced by Orseau (2011, 2014a) and information-seeking agents introduced by Orseau et al. (2013). The entropy-seeking agent maximizes the Shannon entropy gain, while the information-seeking agent maximizes the expected information gain.", "startOffset": 106, "endOffset": 193}, {"referenceID": 23, "context": "It is easy to implement and often achieves quite good results (Chapelle and Li, 2011).", "startOffset": 62, "endOffset": 85}, {"referenceID": 0, "context": "In multi-armed bandits it attains optimal regret (Agrawal and Goyal, 2011; Kaufmann et al., 2012).", "startOffset": 49, "endOffset": 97}, {"referenceID": 88, "context": "In multi-armed bandits it attains optimal regret (Agrawal and Goyal, 2011; Kaufmann et al., 2012).", "startOffset": 49, "endOffset": 97}, {"referenceID": 192, "context": "Thompson sampling has also been discussed for MDPs (Strens, 2000; Dearden et al., 1998) and Bayesian and frequentist regret bounds have been established (Osband et al.", "startOffset": 51, "endOffset": 87}, {"referenceID": 30, "context": "Thompson sampling has also been discussed for MDPs (Strens, 2000; Dearden et al., 1998) and Bayesian and frequentist regret bounds have been established (Osband et al.", "startOffset": 51, "endOffset": 87}, {"referenceID": 162, "context": ", 1998) and Bayesian and frequentist regret bounds have been established (Osband et al., 2013; Gopalan and Mannor, 2015).", "startOffset": 73, "endOffset": 120}, {"referenceID": 51, "context": ", 1998) and Bayesian and frequentist regret bounds have been established (Osband et al., 2013; Gopalan and Mannor, 2015).", "startOffset": 73, "endOffset": 120}, {"referenceID": 193, "context": "4 Thompson Sampling Thompson sampling, also known as posterior sampling or the Bayesian control rule, was originally proposed by Thompson (1933) as a bandit algorithm.", "startOffset": 2, "endOffset": 145}, {"referenceID": 0, "context": "In multi-armed bandits it attains optimal regret (Agrawal and Goyal, 2011; Kaufmann et al., 2012). Thompson sampling has also been discussed for MDPs (Strens, 2000; Dearden et al., 1998) and Bayesian and frequentist regret bounds have been established (Osband et al., 2013; Gopalan and Mannor, 2015). For general RL Thompson sampling was first suggested by Ortega and Braun (2010) with resampling at every time step.", "startOffset": 50, "endOffset": 381}, {"referenceID": 0, "context": "In multi-armed bandits it attains optimal regret (Agrawal and Goyal, 2011; Kaufmann et al., 2012). Thompson sampling has also been discussed for MDPs (Strens, 2000; Dearden et al., 1998) and Bayesian and frequentist regret bounds have been established (Osband et al., 2013; Gopalan and Mannor, 2015). For general RL Thompson sampling was first suggested by Ortega and Braun (2010) with resampling at every time step. Strens (2000) proposes following the optimal policy for one episode or \u201crelated to the number of state transitions the agent is likely to need to plan ahead\u201d.", "startOffset": 50, "endOffset": 431}, {"referenceID": 127, "context": "See also Mahadevan (1996) for a discussion of notions of optimality in MDPs.", "startOffset": 9, "endOffset": 26}, {"referenceID": 65, "context": "1 (Pareto Optimality; Hutter, 2005, Def. 5.22). A policy \u03c0 is Pareto optimal in the set of environmentsM iff there is no policy \u03c0\u0303 such that V \u03c0\u0303 \u03bd ( ) \u2265 V \u03c0 \u03bd ( ) for all \u03bd \u2208M and V \u03c0\u0303 \u03c1 ( ) > V \u03c0 \u03c1 ( ) for at least one \u03c1 \u2208M. The literature provides the following result. Theorem 5.2 (AIXI is Pareto Optimal; Hutter, 2002a, Thm. 2). Every \u03be-optimal policy is Pareto optimal inMCCS LSC . The following theorem was proved for deterministic policies in Leike and Hutter (2015c). Here we extend it to stochastic policies.", "startOffset": 22, "endOffset": 476}, {"referenceID": 65, "context": "The following proof was adapted from Leike and Hutter (2015c) to work for environment classes that do not contain the Bayesian mixture.", "startOffset": 47, "endOffset": 62}, {"referenceID": 55, "context": "3 The G\u00f6del Prior This section introduces a prior that prevents any fixed formal system from making any statements about the outcome of all but finitely many computations. It is named after G\u00f6del (1931) who famously showed that for any sufficiently rich formal system there are statements that it can neither prove nor disprove.", "startOffset": 6, "endOffset": 203}, {"referenceID": 65, "context": "The aim of the Legg-Hutter intelligence measure is to formalize the intuitive notion of intelligence mathematically. Legg and Hutter (2007a) collect various definitions of intelligence across many academic fields and destill it into the following statement (Legg and Hutter, 2007b) Intelligence measures an agent\u2019s ability to achieve goals in a wide range of environments.", "startOffset": 20, "endOffset": 141}, {"referenceID": 65, "context": "The Legg-Hutter intelligence of a policy \u03c0 is the t0-value that \u03c0 achieves across all environments from the classM weighted by the prior w. Legg and Hutter (2007b) consider a subclass ofMCCS LSC , the class of computable measures together with a Solomonoff prior w(\u03bd) = 2\u2212K(\u03bd) and do not use discounting explicitly.", "startOffset": 9, "endOffset": 164}, {"referenceID": 105, "context": "It is natural to fix the policy random that takes actions uniformly at random to have an intelligence score of 1/2 by choosing a \u2018symmetric\u2019 universal prior (Legg and Veness, 2013).", "startOffset": 157, "endOffset": 180}, {"referenceID": 65, "context": "Next, we state two negative results about asymptotic optimality proved by Lattimore and Hutter (2011). It is important to emphasize that Theorem 5.", "startOffset": 88, "endOffset": 102}, {"referenceID": 93, "context": "In this subsection we state a result by Lattimore (2013) that motivated the definition of BayesExp.", "startOffset": 40, "endOffset": 57}, {"referenceID": 160, "context": "Ortega and Braun (2010) prove that the action probabilities of Thompson sampling converge to the action probability of the optimal policy almost surely, but require a finite environment class and two (arguably quite strong) technical assumptions on the behavior of the posterior distribution (akin to ergodicity) and the similarity of environments in the class.", "startOffset": 0, "endOffset": 24}, {"referenceID": 19, "context": "Multi-armed bandits provide a (problemindependent) worst-case regret bound of \u03a9( \u221a km) where k is the number of arms (Bubeck and Bianchi, 2012).", "startOffset": 117, "endOffset": 143}, {"referenceID": 5, "context": "In MDPs the lower bound is \u03a9( \u221a SAdm) where S is the number of states, A the number of actions, and d the diameter of the MDP (Auer et al., 2010).", "startOffset": 126, "endOffset": 145}, {"referenceID": 144, "context": "For a countable class of environments given by state representation functions that map histories to MDP states, a regret of \u00d5(m2/3) is achievable assuming the resulting MDP is weakly communicating (Nguyen et al., 2013).", "startOffset": 197, "endOffset": 218}, {"referenceID": 93, "context": "However, this problem can be alleviated by adding an extra exploration component to AIXI: Lattimore (2013) shows that BayesExp is weakly asymptotically optimal (Theorem 5.", "startOffset": 90, "endOffset": 107}, {"referenceID": 55, "context": "5), and the G\u00f6del prior (Theorem 5.8) should be considered unnatural. But what are other desirable properties of a UTM? A remarkable but unsuccessful attempt to find natural UTMs is due to M\u00fcller (2010). It takes the probability that one universal machine simulates another according", "startOffset": 12, "endOffset": 203}, {"referenceID": 93, "context": "25) and BayesExp (Lattimore, 2013), but not AIXI (Orseau, 2013) sublinear regret impossible in general environments, but possible with recoverability (Theorem 5.", "startOffset": 17, "endOffset": 34}, {"referenceID": 151, "context": "25) and BayesExp (Lattimore, 2013), but not AIXI (Orseau, 2013) sublinear regret impossible in general environments, but possible with recoverability (Theorem 5.", "startOffset": 49, "endOffset": 63}, {"referenceID": 163, "context": "For MDPs, planning is already P-complete for finite and infinite horizons (Papadimitriou and Tsitsiklis, 1987).", "startOffset": 74, "endOffset": 110}, {"referenceID": 138, "context": "The existence of a policy whose expected value exceeds a given threshold is PSPACE-complete (Mundhenk et al., 2000), even for purely epistemic POMDPs in which actions do not change the hidden state (Sabbadin et al.", "startOffset": 92, "endOffset": 115}, {"referenceID": 175, "context": ", 2000), even for purely epistemic POMDPs in which actions do not change the hidden state (Sabbadin et al., 2007).", "startOffset": 90, "endOffset": 113}, {"referenceID": 65, "context": "In contrast, Hutter (2005) defines the value function as the limit of the iterative value function.", "startOffset": 13, "endOffset": 27}, {"referenceID": 65, "context": "We gave a different, more complicated proof in Leike and Hutter (2015b). The following, much simpler and more elegant proof is due to Sterkenburg (2016, Prop.", "startOffset": 57, "endOffset": 72}, {"referenceID": 65, "context": "In Leike and Hutter (2015a) the use of the symbols V and W is reversed.", "startOffset": 13, "endOffset": 28}, {"referenceID": 131, "context": "of ending is a philosophical debate we do not want to engage in here; see Martin et al. (2016) for a discussion.", "startOffset": 74, "endOffset": 95}, {"referenceID": 86, "context": "Kalai and Lehrer (1993) show that in infinitely repeated games Bayesian agents converge to an \u03b5-Nash equilibrium as long as each agent assigns positive prior probability to the other agents\u2019 policies (a grain of truth).", "startOffset": 0, "endOffset": 24}, {"referenceID": 24, "context": "Alternating strategies like TitForTat (cooperate first, then play The idea for reflective oracles was developed by Jessica Taylor and Benya Fallenstein based on ideas by Paul Christiano (Christiano et al., 2013).", "startOffset": 186, "endOffset": 211}, {"referenceID": 24, "context": "Alternating strategies like TitForTat (cooperate first, then play The idea for reflective oracles was developed by Jessica Taylor and Benya Fallenstein based on ideas by Paul Christiano (Christiano et al., 2013). Reflective oracles were first described in Fallenstein et al. (2015a). The proof of Theorem 7.", "startOffset": 187, "endOffset": 283}, {"referenceID": 45, "context": "Foster and Young (2001) and Nachbar (1997, 2005) prove several impossibility results on the grain of truth problem that identify properties that cannot be simultaneously satisfied for classes that allow a grain of truth (see Section 7.", "startOffset": 0, "endOffset": 24}, {"referenceID": 161, "context": "This is not the first time Thompson sampling is used in game theory (Ortega and Braun, 2014), but the first time to show that it achieves such general positive results.", "startOffset": 68, "endOffset": 92}, {"referenceID": 20, "context": "The same problem occurs in multi-agent reinforcement learning (Busoniu et al., 2008).", "startOffset": 62, "endOffset": 84}, {"referenceID": 18, "context": "Assuming convergence to a stationary policy is a necessary criterion to enable all agents to learn, but the process is unstable for many reinforcement learning algorithms and only empirical positive results are known (Bowling and Veloso, 2001).", "startOffset": 217, "endOffset": 243}, {"referenceID": 86, "context": "Since our classMrefl solves the grain of truth problem, the result by Kalai and Lehrer (1993) immediately implies that for any Bayesian agents \u03c01, .", "startOffset": 70, "endOffset": 94}, {"referenceID": 45, "context": "Foster and Young (2001) present a condition that makes convergence to a Nash equilibrium impossible: if the player\u2019s rewards are perturbed by a small real number drawn from some continuous density \u03bd, then for \u03bd-almost all realizations the players do not learn to predict each other and do not converge to a Nash equilibrium.", "startOffset": 0, "endOffset": 24}, {"referenceID": 45, "context": "Foster and Young (2001) present a condition that makes convergence to a Nash equilibrium impossible: if the player\u2019s rewards are perturbed by a small real number drawn from some continuous density \u03bd, then for \u03bd-almost all realizations the players do not learn to predict each other and do not converge to a Nash equilibrium. For example, in a matching pennies game, rational agents randomize only if the (subjective) values of both actions are exactly equal. But this happens only with \u03bd-probability zero, since \u03bd is a density. Thus with \u03bd-probability one the agents do not randomize. If the agents do not randomize, they either fail to learn to predict each other, or they are not acting rationally according to their beliefs: otherwise they would seize the opportunity to exploit the other player\u2019s deterministic action. But this does not contradict our convergence result: the class Mrefl is countable and each \u03bd \u2208 Mrefl has positive prior probability. Perturbation of rewards with arbitrary real numbers is not possible. Even more, the argument given by Foster and Young (2001) cannot work in our setting: the Bayesian mixture \u03be mixes over \u03bbT for all probabilistic Turing machines T .", "startOffset": 0, "endOffset": 1082}, {"referenceID": 140, "context": "But research on AI has progressed steadily over the last decades and there is good reason to believe that we will be able to build HLAI eventually, and even sooner than most people think (M\u00fcller and Bostrom, 2016).", "startOffset": 187, "endOffset": 213}, {"referenceID": 48, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 208, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 92, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 22, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 178, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 137, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 35, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 180, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 34, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 212, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 14, "context": "But research on AI has progressed steadily over the last decades and there is good reason to believe that we will be able to build HLAI eventually, and even sooner than most people think (M\u00fcller and Bostrom, 2016). The advent of strong AI would be the biggest event in human history. Potential benefits are huge, as the new level of automation would free us from any kind of undesirable labor. But there is no reason to believe that humans are at the far end of the intelligence spectrum. Rather, humans barely cross the threshold for general intelligence to be able to use language and do science. Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.g., through self-amplification effects from AIs improving themselves (if doing AI research is one of humans\u2019 capabilities, then a machine that can do everything humans can do can also do AI research). Once machine intelligence is above or far above human level, machines would steer the course of history. There is no reason to believe that machines would be adversarial to us, but nevertheless humanity\u2019s fate might rest at the whims of the machines, just as chimpanzees today have no say in the large-scale events on this planet. Bostrom (2002) defines: Existential risk \u2014 One where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential.", "startOffset": 199, "endOffset": 1537}, {"referenceID": 14, "context": "But research on AI has progressed steadily over the last decades and there is good reason to believe that we will be able to build HLAI eventually, and even sooner than most people think (M\u00fcller and Bostrom, 2016). The advent of strong AI would be the biggest event in human history. Potential benefits are huge, as the new level of automation would free us from any kind of undesirable labor. But there is no reason to believe that humans are at the far end of the intelligence spectrum. Rather, humans barely cross the threshold for general intelligence to be able to use language and do science. Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.g., through self-amplification effects from AIs improving themselves (if doing AI research is one of humans\u2019 capabilities, then a machine that can do everything humans can do can also do AI research). Once machine intelligence is above or far above human level, machines would steer the course of history. There is no reason to believe that machines would be adversarial to us, but nevertheless humanity\u2019s fate might rest at the whims of the machines, just as chimpanzees today have no say in the large-scale events on this planet. Bostrom (2002) defines: Existential risk \u2014 One where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential. Existential risks are events that have the power to extinguish human life as we know it. Examples are cosmic events such as an asteroid colliding with Earth. But cosmic events are unlikely on human timescales compared to human-made existential risks from nuclear weapons, synthetic biology, and nanotechnology. It is possible that artificial intelligence also falls into this category. Vinge (1993) was the first person to recognize this:", "startOffset": 199, "endOffset": 2106}, {"referenceID": 16, "context": "Bostrom (2003) picked up on this issue very early and gave the topic credibility through his well-researched and carefully written book Superintelligence (Bostrom, 2014).", "startOffset": 154, "endOffset": 169}, {"referenceID": 57, "context": "In 2014 high-profile scientists such as Stephen Hawking, Max Tegmark, Stuart Russell, and Frank Wilczek have warned against the dangers posed by AI (Hawking et al., 2014).", "startOffset": 148, "endOffset": 170}, {"referenceID": 186, "context": "Moreover, the Future of Life Institute and the Machine Intelligence Research Institute have formulated concrete technical research priorities to make AI more robust and beneficial (Soares and Fallenstein, 2014; Russell et al., 2015).", "startOffset": 180, "endOffset": 232}, {"referenceID": 168, "context": "Moreover, the Future of Life Institute and the Machine Intelligence Research Institute have formulated concrete technical research priorities to make AI more robust and beneficial (Soares and Fallenstein, 2014; Russell et al., 2015).", "startOffset": 180, "endOffset": 232}, {"referenceID": 13, "context": "Bostrom (2003) picked up on this issue very early and gave the topic credibility through his well-researched and carefully written book Superintelligence (Bostrom, 2014).", "startOffset": 0, "endOffset": 15}, {"referenceID": 1, "context": "(See also Alexander (2015) for a collection of positions by prominent AI researchers.", "startOffset": 10, "endOffset": 27}, {"referenceID": 14, "context": "But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea.", "startOffset": 92, "endOffset": 107}, {"referenceID": 14, "context": "But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea. Despite all of the recent efforts in AI safety research, critical voices within the AI community remain. Prominently, Davis (2014), Ng (2016), Walsh (2016), and Lawrence (2016) have proposed counterarguments that range from \u2018HLAI is so far away that any worry is misplaced\u2019 to claims that \u2018the safety problem would not be so hard\u2019.", "startOffset": 92, "endOffset": 303}, {"referenceID": 14, "context": "But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea. Despite all of the recent efforts in AI safety research, critical voices within the AI community remain. Prominently, Davis (2014), Ng (2016), Walsh (2016), and Lawrence (2016) have proposed counterarguments that range from \u2018HLAI is so far away that any worry is misplaced\u2019 to claims that \u2018the safety problem would not be so hard\u2019.", "startOffset": 92, "endOffset": 314}, {"referenceID": 14, "context": "But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea. Despite all of the recent efforts in AI safety research, critical voices within the AI community remain. Prominently, Davis (2014), Ng (2016), Walsh (2016), and Lawrence (2016) have proposed counterarguments that range from \u2018HLAI is so far away that any worry is misplaced\u2019 to claims that \u2018the safety problem would not be so hard\u2019.", "startOffset": 92, "endOffset": 328}, {"referenceID": 14, "context": "But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea. Despite all of the recent efforts in AI safety research, critical voices within the AI community remain. Prominently, Davis (2014), Ng (2016), Walsh (2016), and Lawrence (2016) have proposed counterarguments that range from \u2018HLAI is so far away that any worry is misplaced\u2019 to claims that \u2018the safety problem would not be so hard\u2019.", "startOffset": 92, "endOffset": 349}, {"referenceID": 14, "context": "But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea. Despite all of the recent efforts in AI safety research, critical voices within the AI community remain. Prominently, Davis (2014), Ng (2016), Walsh (2016), and Lawrence (2016) have proposed counterarguments that range from \u2018HLAI is so far away that any worry is misplaced\u2019 to claims that \u2018the safety problem would not be so hard\u2019. See Sotala and Yampolskiy (2014) for a discussion.", "startOffset": 92, "endOffset": 537}, {"referenceID": 31, "context": "In particular, value learning (Dewey, 2011), self-reflection (Soares, 2015; Fallenstein et al.", "startOffset": 30, "endOffset": 43}, {"referenceID": 185, "context": "In particular, value learning (Dewey, 2011), self-reflection (Soares, 2015; Fallenstein et al., 2015b), self-modification (Orseau and Ring, 2011, 2012a; Everitt et al.", "startOffset": 61, "endOffset": 102}, {"referenceID": 38, "context": ", 2015b), self-modification (Orseau and Ring, 2011, 2012a; Everitt et al., 2016), interruptibility (Orseau and Armstrong, 2016; Armstrong and Orseau, 2016), decision theory (Everitt et al.", "startOffset": 28, "endOffset": 80}, {"referenceID": 155, "context": ", 2016), interruptibility (Orseau and Armstrong, 2016; Armstrong and Orseau, 2016), decision theory (Everitt et al.", "startOffset": 26, "endOffset": 82}, {"referenceID": 2, "context": ", 2016), interruptibility (Orseau and Armstrong, 2016; Armstrong and Orseau, 2016), decision theory (Everitt et al.", "startOffset": 26, "endOffset": 82}, {"referenceID": 37, "context": ", 2016), interruptibility (Orseau and Armstrong, 2016; Armstrong and Orseau, 2016), decision theory (Everitt et al., 2015), memory manipulation (Orseau and Ring, 2012b), wireheading (Ring and Orseau, 2011; Everitt and Hutter, 2016), and questions of identity (Orseau, 2014b,c).", "startOffset": 100, "endOffset": 122}, {"referenceID": 166, "context": ", 2015), memory manipulation (Orseau and Ring, 2012b), wireheading (Ring and Orseau, 2011; Everitt and Hutter, 2016), and questions of identity (Orseau, 2014b,c).", "startOffset": 67, "endOffset": 116}, {"referenceID": 36, "context": ", 2015), memory manipulation (Orseau and Ring, 2012b), wireheading (Ring and Orseau, 2011; Everitt and Hutter, 2016), and questions of identity (Orseau, 2014b,c).", "startOffset": 67, "endOffset": 116}, {"referenceID": 57, "context": "We built on top of Hutter\u2019s theory of universal artificial intelligence. Chapter 3 discussed the formal theory of learning. Chapter 4 presented several approaches to acting in unknown environments (Bayes, Thompson sampling, knowledge-seeking agents, and BayesExp). Chapter 5 analysed these approaches and discussed notions of optimality and principled problems with acting under uncertainty in general environment. Chapter 6 provided the mathematical tools to analyze the computational properties of these models. Finally, Chapter 7 solved the grain of truth problem, which lead to convergence to Nash equilibria in unknown general multi-agent environments. Our work is theoretical by nature and there is still a long way to go until these results make their way into applications. But a solution in principle is a crucial first step towards solving a problem in practice. Consider the research paper by Shannon (1950) on how to solve chess in principle.", "startOffset": 19, "endOffset": 919}], "year": 2016, "abstractText": "Reinforcement learning problems are often phrased in terms of Markov decision processes (MDPs). In this thesis we go beyond MDPs and consider reinforcement learning in environments that are non-Markovian, non-ergodic and only partially observable. Our focus is not on practical algorithms, but rather on the fundamental underlying problems: How do we balance exploration and exploitation? How do we explore optimally? When is an agent optimal? We follow the nonparametric realizable paradigm: we assume the data is drawn from an unknown source that belongs to a known countable class of candidates. First, we consider the passive (sequence prediction) setting, learning from data that is not independent and identically distributed. We collect results from artificial intelligence, algorithmic information theory, and game theory and put them in a reinforcement learning context: they demonstrate how an agent can learn the value of its own policy. Next, we establish negative results on Bayesian reinforcement learning agents, in particular AIXI. We show that unlucky or adversarial choices of the prior cause the agent to misbehave drastically. Therefore Legg-Hutter intelligence and balanced Pareto optimality, which depend crucially on the choice of the prior, are entirely subjective. Moreover, in the class of all computable environments every policy is Pareto optimal. This undermines all existing optimality properties for AIXI. However, there are Bayesian approaches to general reinforcement learning that satisfy objective optimality guarantees: We prove that Thompson sampling is asymptotically optimal in stochastic environments in the sense that its value converges to the value of the optimal policy. We connect asymptotic optimality to regret given a recoverability assumption on the environment that allows the agent to recover from mistakes. Hence Thompson sampling achieves sublinear regret in these environments. AIXI is known to be incomputable. We quantify this using the arithmetical hierarchy, and establish upper and corresponding lower bounds for incomputability. Further, we show that AIXI is not limit computable, thus cannot be approximated using finite computation. However there are limit computable \u03b5-optimal approximations to AIXI. We also derive computability bounds for knowledge-seeking agents, and give a limit computable weakly asymptotically optimal reinforcement learning agent. Finally, our results culminate in a formal solution to the grain of truth problem: A Bayesian agent acting in a multi-agent environment learns to predict the other agents\u2019 policies if its prior assigns positive probability to them (the prior contains a grain of truth). We construct a large but limit computable class containing a grain of truth and show that agents based on Thompson sampling over this class converge to play \u03b5-Nash equilibria in arbitrary unknown computable multi-agent environments.", "creator": "LaTeX with hyperref package"}}}