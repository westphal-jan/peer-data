{"id": "1611.09194", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Times series averaging and denoising from a probabilistic perspective on time-elastic kernels", "abstract": "In the light of regularized dynamic time warping kernels, this paper re-considers the concept of time elastic centroid for a setof time series. We derive a new algorithm based on a probabilistic interpretation of kernel alignment matrices. This algorithm expressesthe averaging process in terms of a stochastic alignment automata.\n\n\n\nWe assume that a kernel and a process start at an intermediate point in time, and then return it to the same process. This is in turn the function of the kernel and a process, as is the case for the kernel and a process, which returns a random number of times. The algorithm has a few functions:\nThe value of the system is always 0, so the system is always zero. The system and the process get the value from 0:0 to 0:0 (i.e., the system and the process get the value from 0:0 to 0:0 (i.e., the system and the process get the value from 0:0 to 0:0). Note that since the system and process get a value from 0:0 to 0:0, there are no parameters for the algorithm and process get a value from 0:0 to 0:0 (i.e., the system and the process get the value from 0:0 to 0:0). If the system and process get the value from 0:0 to 0:0 (i.e., the system and the process get the value from 0:0 to 0:0 (i.e., the system and the process get the value from 0:0 to 0:0 (i.e., the system and the process get the value from 0:0 to 0:0 (i.e., the system and the process get the value from 0:0 to 0:0 (i.e., the system and the process get the value from 0:0 to 0:0 (i.e., the system and the process get the value from 0:0 to 0:0 (i.e., the system and the process get the value from 0:0 to 0:0 (i.e., the system and the process get the value from 0:0 to 0:0 (i.e., the system and the process get the value from 0:0 to 0:0 (i.e., the system and the process get the value from 0:0 to 0:0 (i.e., the system and the process get the value", "histories": [["v1", "Mon, 28 Nov 2016 15:34:30 GMT  (3633kb,D)", "https://arxiv.org/abs/1611.09194v1", null], ["v2", "Tue, 29 Nov 2016 12:41:07 GMT  (3751kb,D)", "http://arxiv.org/abs/1611.09194v2", "arXiv admin note: text overlap witharXiv:1505.06897. arXiv admin note: text overlap witharXiv:1505.06897"], ["v3", "Thu, 22 Dec 2016 11:50:08 GMT  (3750kb,D)", "http://arxiv.org/abs/1611.09194v3", "arXiv admin note: text overlap witharXiv:1505.06897"], ["v4", "Mon, 24 Apr 2017 08:40:10 GMT  (3710kb,D)", "http://arxiv.org/abs/1611.09194v4", "arXiv admin note: text overlap witharXiv:1505.06897"]], "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["pierre-fran\\c{c}ois marteau"], "accepted": false, "id": "1611.09194"}, "pdf": {"name": "1611.09194.pdf", "metadata": {"source": "CRF", "title": "Times series averaging and denoising from a probabilistic perspective on time-elastic kernels", "authors": ["Pierre-Francois Marteau"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Time series averaging Time elastic kernel Dynamic Time Warping Hidden Markov Model Classification Denoising.\nF"}, {"heading": "1 INTRODUCTION", "text": "Since Maurice Fre\u0301chet\u2019s pioneering work [1] in the early 1900s, time-elastic matching of time series or symbolic sequences has attracted much attention from the scientific community in numerous fields such as information indexing and retrieval, pattern analysis, extraction and recognition, data mining, etc. This approach has impacted a very wide spectrum of applications addressing socio-economic issues such as the environment, industry, health, energy, defense and so on.\nAmong other time elastic measures, Dynamic Time Warping (DTW) was widely popularized during the 1970s with the advent of speech recognition systems [2], [3], and numerous variants that have since been proposed to match time series with a certain degree of time distortion tolerance.\nThe main issue addressed in this paper is time series or shape averaging in the context of a time elastic distance. Time series averaging or signal averaging is a long-standing issue that is currently becoming increasingly prevalent in the big data context; it is relevant for de-noising [4], [5], summarizing subsets of time series [6], defining significant prototypes, identifying outliers [7], performing data mining tasks (mainly exploratory data analysis such as clustering) and speeding up classification [8], as well as regression or data analysis processes in a big data context.\nIn this paper, we specifically tackle the question of averaging subsets of time series, not from considering the DTW measure itself as has already been largely exploited, but from the perspective of the so-called regularized DTW kernel (KDTW). From this new perspective, the estimation\n\u2022 P.-F. Marteau is with UMR CNRS IRISA, Universite\u0301 Bretagne Sud, F56000 Vannes, France.\nof a time series average or centroid can be readily addressed with a probabilistic interpretation of kernel alignment matrices allowing a precise definition of the average of a pair of time series from the expected value of local alignments of samples. The tests carried out so far demonstrate the robustness and the efficiency of this approach compared to the state of the art approach.\nThe structure of this paper is as follows: the introductory section, the second section summarizes the most relevant related studies on time series averaging as well as DTW kernelization. In the third section, we derive a probabilistic interpretation of kernel alignment matrices evaluated on a pair of time series by establishing a parallel with a forwardbackward procedure on a stochastic alignment automata. In the fourth section, we define the average of a pair of time series based on the alignment expectation of pairs of samples, and we propose an algorithm designed for the averaging of any subset of time series using a pairwise aggregating procedure. We present in the fifth section three complementary experiments to assess our approach against the state of the art, and conclude."}, {"heading": "2 RELATED WORKS", "text": "Time series averaging in the context of (multiple) time elastic distance alignments has been mainly addressed in the scope of the Dynamic Time Warping (DTW) measure [2], [3]. Although other time elastic distance measures such as the Edit Distance With Real Penalty (ERP) [9] or the Time Warp Edit Distance (TWED) [10] could be considered instead, without loss of generality, we remain focused throughout this paper on DTW and its kernelization.\nar X\niv :1\n61 1.\n09 19\n4v 4\n[ cs\n.L G\n] 2\n4 A\npr 2\n01 7\n2"}, {"heading": "2.1 DTW and time elastic average of a pair of time series", "text": "A classical formulation of DTW can be given as follows. If d is a fixed positive integer, we define a time series of length n as a multidimensional sequence Xn1 = X1X2 \u00b7 \u00b7 \u00b7Xn, such that, \u2200i \u2208 {1, .., n}, Xi \u2208 Rd.\nDefinition 2.1. If Xn1 and Y m 1 are two time series with\nrespective lengths n and m, an alignment path \u03c0 = (\u03c0k) of length p = |\u03c0| between Xn1 and Y m1 is represented by a sequence\n\u03c0 : {1, . . . , p} \u2192 {1, . . . , n} \u00d7 {1, . . . ,m}\nsuch that \u03c01 = (1, 1), \u03c0p = (n,m), and (using the notation \u03c0k = (ik, jk), for all k \u2208 {1, . . . , p \u2212 1}, \u03c0k+1 = (ik+1, jk+1) \u2208 {(ik + 1, jk), (ik, jk + 1), (ik + 1, jk + 1)}.\nWe define \u2200k \u03c0k(1) = ik and \u03c0k(2) = jk, as the index access functions at step k of the mapped elements in the pair of aligned time series.\nIn other words, a warping path defines a way to travel along both time series simultaneously from beginning to end; it cannot skip a point, but it can advance one time step along one series without advancing along the other, thereby justifying the term time-warping.\nIf \u03b4 is a distance on Rd, the global cost of a warping path \u03c0 is the sum of distances (or squared distances or local costs) between pairwise elements of the two time series along \u03c0, i.e.:\ncost(\u03c0) = \u2211\n(ik,jk)\u2208\u03c0\n\u03b4(Xik , Yjk)\nA common choice of distance on Rd is the one generated by the L2 norm.\nDefinition 2.2. For a pair of finite time series X and Y , any warping path has a finite length, and thus the number of existing warping paths is finite. Hence, there exists at least one path \u03c0\u2217 whose cost is minimal, so we can define DTW(X,Y ) as the minimal cost taken over all existing warping paths. Hence\nDTW(Xn1 , Y m 1 ) = min\u03c0 cost(\u03c0(Xn1 , Y m 1 ))\n= cost(\u03c0\u2217(Xn1 , Y m 1 )). (1)\nDefinition 2.3. From the DTW measure, [11] have defined the time elastic average a(X,Y ) of a pair of time series Xn1 and Y m 1 as the time series A |\u03c0\u2217| 1 whose elements\nare Ak = mean(X\u03c0\u2217k(1), Y\u03c0\u2217k(2)), \u2200k \u2208 1, \u00b7 \u00b7 \u00b7 , |\u03c0 \u2217|, where mean corresponds to the definition of the mean in Euclidean space."}, {"heading": "2.2 Time elastic centroid of a set of time series", "text": "A single alignment path is required to calculate the time elastic centroid of a pair of time series (Def. 2.1). However, multiple path alignments need to be considered to evaluate the centroid of a larger set of time series. Multiple alignments have been widely studied in bioinformatics [12], and it has been shown that determining the optimal alignment of a set of sequences under the sum of all pairs (SP) score scheme is a NP-complete problem [13] [14]. The time and space complexity of this problem is O(Lk), where k is the number of sequences in the set and L is the length of the sequences when using dynamic programming to search for an optimal solution [15]. This latter result applies to the estimation of the time elastic centroid of a set of k time series with respect to the DTW measure. Since the search for an optimal solution becomes rapidly intractable with increasing k, sub-optimal heuristic solutions have been subsequently proposed, most of them falling into one of the following three categories."}, {"heading": "2.2.1 Progressive heuristics", "text": "Progressive heuristic methods estimate the time elastic centroid of a set of k time series by combining pairwise centroids (Def. 2.3). This kind of approach constructs a binary tree whose leaves correspond to the time series of the data set, and whose nodes correspond to the calculation of a local pairwise centroid, such that, when the tree is complete, the root is associated with the estimated data set centroid. The proposed strategies differ in the way the tree is constructed. One popular approach consists of providing a random order for the leaves, and then constructing the binary tree up to the root using this ordering [11]. Another approach involves constructing a dendrogram (a hierarchical ascendant clustering) from the data set and then using this dendrogram to calculate pairwise centroids starting with the closest pairs of time series and progressively aggregating series that are farther away [16] as illustrated on the left of Figure 1. Note that these heuristic methods are entirely based on the calculation of a pairwise centroid, so they do not explicitly require the evaluation of a DTW centroid for more than two time series. Their degree of complexity varies linearly with the number of time series in the data set."}, {"heading": "2.2.2 Iterative heuristics", "text": "Iterative heuristics are based on an iterated three-step process. For a given temporary centroid candidate, the first step consists of calculating the inertia, i.e. the sum of the DTW distances between the temporary centroid and each time series in the data set. The second step (Figure 1a top) evaluates the best pairwise alignment with the temporary centroid c(i), of length L, for each time series uj(i) in the data set (j \u2208 {1 \u00b7 \u00b7 \u00b7n}), where i is the timestamp. A new time series of length L, u\u2032j(i) is thus constructed that contains the contributions of all the samples of time series uj(i), but with time being possibly stretched (duplicate samples) or compressed (average of successive samples) according to the best alignment path as exemplified in Figure 1a, top left side. The third step consists in producing a new temporary centroid candidate c(i) from the set {u\u2032j(i)} by successively averaging (in the sense of the Euclidean\n3 (a) Pairwise average (top) and Progressive agglomeration (bottom) (b) Iterative agglomeration with refinement\nFig. 1. Pairwise averaging (top left), progressive hierarchical with similar first agglomeration (bottom left) v.s. iterative agglomeration (right) strategies. Final centroid approximations are presented in red bold color. Temporary estimations are presented using a bold dotted black line\ncentroid), the samples at every timestamp i of the u\u2032j(i) time series. Basically, we have c(i) = 1/n \u00b7 \u2211 j=1..n u \u2032 j(i).\nThen, the new centroid candidate replaces the previous one and the process is iterated until the inertia is no longer reduced or the maximum number of iterations is reached. Generally, the first temporary centroid candidate is taken as the DTW medoid of the considered data set. This process is illustrated on Figure 1b. The three steps of this heuristic method were first proposed in [17]. The iterative aspect of this heuristic approach was initially introduced by [18] and refined by [6] who introduced the DTW Barycenter Averaging (DBA) algorithm. Note that, in contrast to the progressive method, this kind of approach needs to evaluate, at each iteration, all the alignments with the current centroid candidate. The complexity of the iterative approach is higher than the progressive approach, the extra computational cost being linear with the number of iterations. More sophisticated approaches have been proposed to escape some local minima. For instance [19] have evaluated a genetic algorithm for managing a population of centroid candidates, thus improving with some success the straightforward iterative heuristic methods."}, {"heading": "2.2.3 Optimization approaches", "text": "Given the entire set of time series S and a subset of n time series S = {Xj}j=1\u00b7\u00b7\u00b7n \u2286 S, optimization approaches attempt to estimate the centroid of S from the definition of an optimization problem, which is generally expressed by equation (2) given below:\nc = argmin s\u2208S n\u2211 j=1 DTW(s,Xj) (2)\nAmong other works, some attempt to use this kind of direct approach for the estimation of time elastic centroid was recently addressed in [20], [21] and [22].\nIn [20] the authors detail a Canonical Time Warp (CTW) and a Generalized version of it (GCTW) [21] that combines DTW and CCA (Canonical Correlation Analysis) for temporally aligning multi-modal motion sequences. From a least square formulation for DTW, a non-convex optimization problem is handled by means of a coordinatedescent approach that alternates between multiple temporal alignments using DTW (or a variant exploiting a set of basis functions to parameterized the warping paths) and spatial projections using CCA (or a multi-set extension of CCA). Whilst these approaches have not been designed to explicitly propose a centroid estimation, they do provide multi-alignment paths that can straightforwardly be used to compute a centroid estimate. As an extension to CTW, GCTW requires the set-up of generally \u201dsmooth\u201d function basis that constrain the shape of the admissible alignment paths. This ensures the computational efficiency of GCTW, but in return it may induce some drawback, especially when considering the averaging of \u201dunsmoothed\u201d time series that may involve very \u201djerky\u201d alignment paths. The choice of this function basis may require some expertise on the data.\nIn [22], a non-convex constrained optimization problem is derived, by integrating a temporal weighting of local sample alignments to highlight the temporal region of interest in a time series data set, thus penalizing the other temporal regions. Although the number of parameters to optimize is linear with the size and the dimensionality of the time series, the two steps gradient-based optimization process they derived is very computationally efficient and shown to outperform the state of the art approaches on some challenging scalar and multivariate data sets. However, as numerous local optima exist in practice, the method is not guaranteed to converge towards the best possible centroid,\n4 which is anyway the case in all other approaches. Furthermore, their approach, due to combinatorial explosion, cannot be adapted for time elastic kernels like the one addressed in this paper and described in section 2.4."}, {"heading": "2.3 Discussion and motivation", "text": "According to the state of the art in time elastic centroid estimation, an exact centroid, if it exists, can be calculated by solving a NP-complete problem whose complexity is exponential with the number of time series to be averaged. Heuristic methods with increasing time complexity have been proposed since the early 2000s. Simple pairwise progressive aggregation is a less complex approach, but which suffers from its dependence on initial conditions. Iterative aggregation is reputed to be more efficient, but entails a higher computational cost. It could be combined with ensemble methods or soft optimization such as genetic algorithms. The non-convex optimization approach has the merit of directly addressing the mathematical formulation of the centroid problem in a time elastic distance context. This approach nevertheless involves a higher complexity and must deal with a relatively large set of parameters to be optimized (the weights and the sample of the centroid). Its scalability could be questioned, specifically for high dimensional multivariate time series.\nIt should also be mentioned that some criticism of these heuristic methods has been made in [23]. Among other drawbacks, the fact that DTW is not a metric could explain the occurrence of unwanted behavior such as centroid drift outside the time series cluster to be averaged. We should also bear in mind that keeping a single best alignment can increase the dependence of the solution on the initial conditions. It may also increase the aggregating order of the time series proposed by the chosen method, or potentially enhance the convergence rate.\nIn this study, we do not directly address the issue of time elastic centroid estimation from the DTW perspective, but rather from the point of view of the regularized dynamic time warping kernel (KDTW) [24]. Although this perspective allows us to consider centroid estimation as a preimage problem, which is in itself another optimization perspective, we rather show that the KDTW alignment matrices computation can be described as the result of applying a forwardbackward algorithm on a stochastic alignment automata. This probabilistic interpretation of the pairwise alignment of time series leads us to propose a robust averaging scheme for any set of time series that interpolate jointly along the time axis and in the sample space. Furthermore, this scheme significantly outperforms the current state of the art method, as shown by our experiments."}, {"heading": "2.4 Time elastic kernels and their regularization", "text": "The Dynamic Time Warping (DTW) distance between two time series Xp1 = X1X2 \u00b7 \u00b7 \u00b7Xp and Y q 1 = Y1Y2 \u00b7 \u00b7 \u00b7Yq of lengths p and q respectively, [2], [3] as defined in equation (1) can be recursively evaluated as\nddtw(X p 1 , Y q 1 ) =\nd2E(Xp, Yq) + Min  ddtw(X p\u22121 1 , Y q 1 ) ddtw(X p\u22121 1 , Y q\u22121 1 )\nddtw(X p 1 , Y q\u22121 1 )\n(3)\nwhere dE(Xp, Yq) is the Euclidean distance defined on Rd between the two positions in sequences Xp1 and Y q 1 taken at times p and q, respectively. Apart from the fact that the triangular inequality does not hold for the DTW distance measure, it is not possible to define a positive definite kernel directly from this distance. Hence, the optimization problem, which is inherent to the learning of a kernel machine, is no longer convex and could be a source of limitation due to the emergence of local minima.\nRegularized DTW: seminal work by [25], prolonged recently by [24] leads us to propose new guidelines to ensure that kernels constructed from elastic measures such as DTW are positive definite. A simple instance of such a regularized kernel, derived from [24], can be expressed as a convolution kernel, which makes use of two recursive terms:\nKDTW(Xp1 , Y q 1 ) = Kdtw(X p 1 , Y q 1 ) +K \u2032 dtw(X p 1 , Y q 1 )\nKdtw(X p 1 , Y q 1 ) =\n1 3e \u2212\u03bdd2E(Xp,Yq) \u00b7 ( h(p\u2212 1, q)Kdtw(Xp\u221211 , Y q 1 )\n+h(p\u2212 1, q \u2212 1)Kdtw(Xp\u221211 , Y q\u22121 1 ) +h(p, q \u2212 1)Kdtw(Xp1 , Y q\u22121 1 ) ) K \u2032dtw(X p 1 , Y q 1 ) =\n1 3 \u00b7 ( h(p\u2212 1, q)K \u2032dtw(X p\u22121 1 , Y q 1 )e \u2212\u03bdd2E(Xp,Yp) +\u2206p,qh(p\u2212 1, q \u2212 1)K \u2032dtw(X p\u22121 1 , Y q\u22121 1 )e \u2212\u03bdd2E(Xp,Yq) +h(p, q \u2212 1)K \u2032dtw(X p 1 , Y q\u22121 1 )e \u2212\u03bdd2E(Xq,Yq) )\n(4)\nwhere \u2206p,q is the Kronecker symbol, \u03bd \u2208 R+ is a stiffness parameter which weights the local contributions, i.e. the distances between locally aligned positions, dE(., .) is a distance defined on Rk, and h is a symmetric binary non negative function, usually in {0, 1}, used to define a symmetric corridor around the main diagonal to limit the \u201dtime elasticity\u201d of the kernel. For the remaining of the paper we will not consider any corridor, hence h(., .) = 1 everywhere.\nThe initialization is simply Kdtw(X01 , Y 0 1 ) =\nK \u2032dtw(X 0 1 , Y 0 1 ) = 1.\nThe main idea behind this regularization is to replace the operators min and max (which prevent symmetrization of the kernel) by a summation operator. This allows us to consider the best possible alignment, as well as all the best (or nearly the best) paths by summing their overall cost. The parameter \u03bd is used to check what is termed as nearly-the-best alignment, thus penalizing alignments that are too far away from the optimal ones. This parameter can be easily optimized through a cross-validation.\n5 For each alignment path, KDTW evaluates the product of local alignment costs e\u2212\u03bdd 2 E(Xp,Yq)) \u2264 1 occurring along the path. This product can be very small depending on the size of the time series and the selected value for \u03bd. This is the source for a diagonal dominance problem in the Gram matrix. But, above all, this requires to balance the choice of the \u03bd value according to the lengths of the matched time series. This is the main (and probably the only) limitation of the KDTW kernel: the selectivity or bandwidth of the local alignment kernels needs to be adjusted according to the lengths of the matched time series."}, {"heading": "3 STOCHASTIC ALIGNMENT PROCESS", "text": "To introduce a probabilistic paradigm to the time elastic averaging of time series, we first consider the pairwise alignment process as the output of a stochastic automata. The stochastic alignment process that we propose finds its roots in the forward-backward algorithm defined for the learning of Hidden Markov Models (HMM) [26] and in the parallel between HMM and DTW that is proposed in [27], [28] and in a more distant way in [29]. However we differ from these founding works (and others) in the following\n1) we do not construct a parallel with DTW, but with its kernelized variant KDTW 2) [28] only consider an optimal alignment path (exploiting the Viterbi algorithm) while we consider the whole set of possible alignments (as in [27]) 3) [27] construct an asymmetric classical left-right HMM (one time series plays the role of the observation sequence, while the other plays the role of the state sequence). With a similar idea [29] proposes a generative mixture model along a discrete time grid axis with local and global time warp capability. We construct instead an alignment process, that sticks on the DTW recursive definition without any other hypothesis on the structure of the automata, and for which the two aligned time series play the role of the observation sequence, and the set of states corresponds to the set of all possible sample pairs alignments."}, {"heading": "3.1 pairwise alignment of time series as a Markov model", "text": "Let on1 = o1o2 \u00b7 \u00b7 \u00b7 on and o\u2032 n\u2032 1 = o \u2032 1o \u2032 2 \u00b7 \u00b7 \u00b7 o\u2032n\u2032 be two discrete time series (observations) of length n and n\u2032 respectively. To align this two time series, we define a stochastic alignment automata as follows. First we consider the set of state variables S = {S1,1, S1,2, \u00b7 \u00b7 \u00b7 , Sn,n\u2032}. Each Si,j characterizes the alignment between observed samples oi and o\u2032j . The posterior probability for all state variables, Si,j , given the sequences of observations on1 and o \u2032n\u2032 1 is P (Si,j |on1 ; o\u2032 n\u2032\n1 ). The transitions probabilities between states are driven by a tensor A = [aij;kl], where aij;kl = P (Sk,l|Si,j), \u2200(k, l) and (i, j) \u2208 {1 \u00b7 \u00b7 \u00b7n} \u00d7 {1 \u00b7 \u00b7 \u00b7n\u2032}. A can be defined accordingly to the standard DTW definition, namely\naij;kl =  1 3 IF  (k = i AND l = j + 1) OR (k = i+ 1 AND l = j + 1) OR (k = i+ 1 AND l = j)\n0 OTHERWISE.\n(5)\nThe 1/3 factor ensures that the transition matrix equivalent to A is stochastic, basically\n\u2200i, j \u2211 kl aij;kl = 1 (6)\nNotice that any tensor A satisfying equation (6) could be considered at this level instead of the previous DTW surrogate tensor.\nFurthermore, each state is observable through the so-called emission probabilities which are defined by a set of functions bij(x, y), where bij(x, y) = P (x, y|Si,j), \u2200(x, v) \u2208 Rd \u00d7 Rd and (i, j) \u2208 {1 \u00b7 \u00b7 \u00b7n} \u00d7 {1 \u00b7 \u00b7 \u00b7n\u2032}. The bij functions are normalized such that \u222b\u222b x,y bij(x, y) dx dy = 1.\nHere we differ from the classical HMM: the first difference lies in the nature of the observation sequence itself. Unlike HMM, our observation consists of a pair of subsequences that are not traveled necessarily synchronously, but according to the structure of the transition tensor A. For instance, given the DTW tensor described by equation (5), from a current state associated to the alignment (ou, o\u2032v), three possible alignments can be reached at the next transition: (ou+1, o\u2032v), (ou, o \u2032 v+1) or (ou+1, o \u2032 v+1).\nThe second difference with classical HMM is that the emission probabilities are independent from the state, such that \u2200i, j bi,j(x, y) = b(x, y). We use a local (density) kernel to estimate these probabilities as follows\nb(x, y) = \u03ba(x, y) = \u03b3e\u2212\u03bdd 2 E(x,y) (7)\nwhere \u03b3 is the normalization coefficient. Consequently, given the two observation sequences on1 and o \u2032n\u2032 1 , we define the emission probability matrix B = [bkl] = b(ok, o \u2032 l) = \u03b3e \u2212\u03bdd2E(ok,o \u2032 l), for k \u2208 {1, \u00b7 \u00b7 \u00b7 , n} and l \u2208 {1, \u00b7 \u00b7 \u00b7 , n\u2032}\nFinally let u be the initial state probability vector defined by \u2200(i, j) \u2208 {1 \u00b7 \u00b7 \u00b7n} \u00d7 {1 \u00b7 \u00b7 \u00b7n\u2032}, uij = 1 if i = j = 1, 0 otherwise.\nThereby, the stochastic alignment automata is fully specified by the triplet \u03b8 = (A,B,u), where A only depends on the lengths n and n\u2032 of the observations, and B depends on the complete pair of observations on1 and o \u2032n\u2032 1 ."}, {"heading": "3.2 Forward-backward alignment algorithm", "text": "We derive the forward-backward alignment algorithm for our stochastic alignment automata from its classical derivation that was defined for Hidden Markov Models [26].\nFor all S \u2208 S, the posterior probability P (S|on1 , o\u2032 n\u2032\n1 , \u03b8) is decomposed into forward/backward recursions as follows:\nP (S|on1 , o\u2032 n\u2032 1 , \u03b8) = P (on1 ,o\n\u2032n\u2032 1 ,S|\u03b8)\nP (on1 ,o \u2032n\u2032 1 |\u03b8)\n= P (ot1,o\nn t ,o \u2032t\u2032 1 ,o \u2032n\u2032 t\u2032 ,S|\u03b8)\nP (on1 ,o \u2032n\u2032 1 |\u03b8)\n= P (ont ,o\n\u2032n\u2032 t\u2032 |S,\u03b8)P (S,o t 1,o \u2032t\u2032 1 |\u03b8)\nP (on1 ,o \u2032n\u2032 1 |\u03b8)\n(8)\n6 The last equality results from the application of the Bayes rule and the conditional independence of ont , o \u2032n\u2032 t\u2032 and ot1, o \u2032t\u2032 1 given S, \u03b8.\nLet \u03b1t,t\u2032 = P (ot1, o \u2032t\u2032 1 , St,t\u2032 |\u03b8) be the probability of the alignment of the pair of partial observation sequences (ot1, o \u2032t\u2032 1 ) produced by all possible state sequences that end at state St,t\u2032 . \u03b1t,t\u2032 can be recursively evaluated as the forward procedure  \u03b11,1 = u11b11\u03b1t,t\u2032 = btt\u2032 \u2211\nu,v\u2208Ft,t\u2032 \u03b1u,vauv;tt\u2032 (9)\nwhere Ft,t\u2032 is the subset of states allowing to reach the state St,t\u2032 in a single transition. For the DTW tensor A (Eq. 5), we have Ft,t\u2032 = {St\u22121,t\u2032 , St,t\u2032\u22121, St\u22121,t\u2032\u22121}. Notice that in this case \u03b1n,n\u2032 = Kdtw(on1 , o \u2032n\u2032 1 ).\nSimilarly let \u03b2t,t\u2032 = P (ont , o \u2032n\u2032 t\u2032 |S, \u03b8) be the probability of the alignment of the pair of partial sequences (ont , o \u2032n\u2032 t\u2032 ) given starting state St,t\u2032 . \u03b2t,t\u2032 can be recursively evaluated as the backward procedure \u03b2n,n\u2032 = 1\u03b2t,t\u2032 = \u2211\nu,v\u2208Bt,t\u2032 \u03b2u,vatt\u2032;uvbtt\u2032 (10)\nwhere Bt,t\u2032 is the subset of states that can be reached from the state St,t\u2032 in a single transition. For the DTW tensor A (Eq. 5), we have Bt,t\u2032 = {St+1,t\u2032 , St,t\u2032+1, St+1,t\u2032+1}.\nHence from Eq. 8, we get\nP (St,t\u2032 |on1 , o\u2032 n\u2032 1 , \u03b8) = \u03b1t,t\u2032\u03b2t,t\u2032\nP (on1 , o \u2032n\u2032 1 |\u03b8)\n(11)\nAny tensor A satisfying equation (6) is not eligible: for the \u03b1t,t\u2032 and \u03b2t,t\u2032 recursions to be calculable, one has to impose linearity. Basically \u03b1t,t\u2032 cannot depend on any \u03b1u,v\u2032 that is not previously evaluated. The constraint we need to impose is that the time stamps are locally increasing, i.e. if \u03b1t,t\u2032 depends on any \u03b1u,v\u2032 , then necessarily [(t < u and t\u2032 \u2264 v\u2032) or (t \u2264 u and t\u2032 < v\u2032)]. The same applies for the \u03b2t,t\u2032 recursion.\nAs an example, Figure 2 presents the Forward Backward (FB) matrix (FB(t, t\u2032) = P (St,t\u2032 |on1 , o\u2032 n\u2032\n1 , \u03b8)) corresponding to the alignment of a positive half-wave with a sinus\nwave. The three areas of likely alignment paths are clearly identified in dark red colors."}, {"heading": "3.3 Parallel with KDTW", "text": "A direct parallel exists between KDTW and the previous Markov process. It follows from the forward equation (Eq. 9) that\nKdtw(X k 1 , Y l 1 ) = \u2211 i,j aij,klbklKdtw(X i 1), Y j 1 )\n= \u03ba(Xk, Yl) \u2211 i,j aij,klKdtw(X i 1, Y j 1 ) (12)\nwhere A = [aij;kl] is defined in equation (5), and B = [bkl], defined in equation (7), is such that bkl = e\u2212\u03bdd 2 E(Xk,Yl). Hence, the Kdtw recursion coincides exactly with the forward recursion (Eq. 9). Similarly, we can assimilate the backward recursion (eq. 10) to the Kdtw evaluation of the pair of time series obtained by inverting X and Y along the time axis. Hence, the forward-backward matrix elements (eq. 11) can be directly expressed in terms Kdtw recursions.\nFurthermore, the corridor function h() that occurs in the Kdtw recursion (Eq. 4) modifies directly the structure of the transition tensor A by setting aij;kl = 0 whenever h(i, j) = 0 or h(k, l) = 0. Neighbor states may be affected also by the normalization that is required to maintain A stochastic."}, {"heading": "3.4 Time elastic centroid estimate of a set of time series", "text": "Let us introduce the marginal probability of subset St,\u2022 = {St,1, St,2, \u00b7 \u00b7 \u00b7 , St,n\u2032} given the observations o and o\u2032, namely that sample ot is aligned with the samples of o\u2032 n\u2032\n1 P (St,\u2022) = \u2211 t\u2032 P (St,t\u2032 |on1 , o\u2032 n\u2032 1 , \u03b8) (13)\nand let us consider, for all t and t\u2032, the conditional probability of state St,t\u2032 given the two observation sequences, parameter \u03b8 and St,\u2022, namely the probability that ot and o\u2032t\u2032 are aligned given the knowledge that ot is aligned with one of the samples of o\u2032.\nP (St,t\u2032 |on1 , o\u2032 n\u2032 1 , St,\u2022, \u03b8) =\nP (St,t\u2032 |on1 , o\u2032 n\u2032 1 , \u03b8)/P (St,\u2022|on1 , o\u2032 n\u2032 1 , \u03b8) (14)\nThe previous equality is easily established because P (St,t\u2032 , St,\u2022|on1 , o\u2032 n\u2032 1 , \u03b8) = P (St,t\u2032 |on1 , o\u2032 n\u2032 1 , \u03b8).\nNote that for estimating P (St,t\u2032 , St,\u2022|on1 , o\u2032 n\u2032\n1 , \u03b8) we only need to evaluate the forward (\u03b1t,t\u2032 ) and backward (\u03b2t,t\u2032 ) recursions, since P (on1 , o \u2032n\u2032 1 |\u03b8), the numerator term in Eq.11, is eliminated.\nWe can then define the expectation of the samples of o\u2032n \u2032\n1\nthat are aligned with sample ot (given that ot is aligned) as well as the expectation of time of occurrence of the samples of o\u2032n \u2032\n1 that are aligned with ot as follows:\n7 E(o\u2032|ot) = 1n\u2032 n\u2032\u2211 t\u2032=1 o\u2032t\u2032P (St,t\u2032 |on1 , o\u2032 n\u2032 1 , St,\u2022, \u03b8)\nE(t\u2032|ot) = 1n\u2032 n\u2032\u2211 t\u2032=1 t\u2032P (St,t\u2032 |on1 , o\u2032 n\u2032 1 , St,\u2022, \u03b8)\n(15)\nThe Expectation equations (Eq. 15) are at the basis of our procedure for averaging a set of time series.\nLet O = {konk1 }k=1\u00b7\u00b7\u00b7N be a set of time series and rn1 a reference time series (rn1 can be initially setup as the medoid of set O). The centroid estimate of O is defined as the pair (c, \u03c4) where c is a time series of length n and \u03c4 is the sequence of time stamps associated to the samples of c\nc(t) = 1N N\u2211 k=1 E(ko|rt)\n= 1N N\u2211 k=1 1 nk nk\u2211 kt=1 kojP (St,kt|rn1 , ko nk 1 , St,\u2022)\n\u03c4(t) = 1N N\u2211 k=1 E(kt|rt)\n= 1N N\u2211 k=1 1 nk nk\u2211 kt=1 ktP (St,kt|rn1 , ko nk 1 , St,\u2022)\n(16)\nObviously, (c, \u03c4) is a non uniformly sampled time series for which \u03c4(t) is the time stamp associated to observation c(t). \u03c4(t) could be understood as the expected time of occurrence of the expected observation c(t). A uniform re-sampling can straightforwardly be used to get back to a uniformly sampled time series.\nThe proposed iterative agglomerative algorithm (cf. Fig. 1-b), called TEKA (Time Elastic Kernel Averaging), that provides a refinement of the centroid estimation at each iteration until reaching a (local) optimum is presented in algorithm (1).\nAs an example, figure (3) presents the time elastic centroid estimates obtained, using algorithm (1) with K = Kdtw, for the Cylinder c(t), Bell, b(t) Funnel, f(t), synthetic functions [30] defined as follows\nc(t) = (6 + \u03b7) \u00b7 \u03c7[a,b](t) + (t) b(t) = (6 + \u03b7) \u00b7 \u03c7[a,b](t) \u00b7 (t\u2212 a)/(b\u2212 a) + (t) f(t) = (6 + \u03b7) \u00b7 \u03c7[a,b](t) \u00b7 (b\u2212 t)/(b\u2212 a) + (t)\nwhere \u03c7[a,b] = 0 if t < a \u2228 t > b, 1 if a \u2264 t \u2264 b, \u03b7 and (t) are obtained from a standard normal distribution N(0, 1), a is an integer obtained from a uniform distribution in [16, 32] and b\u2212 a is another integer obtained from another uniform distribution in [32, 96]. Hence such shapes are characterized with start and end time stamps of 24 and 88 respectively, and a shape duration of 64 samples. Figure (3) clearly shows that, from a subset of 300 time series (100 for each category), the algorithm has correctly recovered the start and end shape events (hence the expected shape duration) for all three shapes.\nAlgorithm 1 Iterative Time Elastic Kernel Averaging (TEKA) of a set of time series\n1: Let K be a similarity time elastic kernel for time series satisfying eq. (12) 2: Let O be a set of time series of d dimensional samples 3: Let c be an initial centroid estimate (e.g. the medoid of O) of length n 4: Let \u03c4 and \u03c40 be two sequences of time stamps of length n initialized with zero values 5: Let MeanK0 = 0 and MeanK be two double values; 6: repeat 7: c0 = c, \u03c40 = \u03c4 , MeanK0 = MeanK ; 8: Evaluate c and \u03c4 according to Eq. (16) 9: //Average similarity between c and O elements\n10: MeanK= 1|O| \u2211 o\u2208OK(c, o) 11: until MeanK < MeanK0 12: (c0, \u03c40) is the centroid estimation 13: Uniformly re-sample c0 using the time stamps \u03c40\nThe figures presented in Table 1 compare the centroid estimates provided by the iterated DBA [19], CTW [20] and TEKA algorithms. For the experiment, the DBA and TEKA algorithms were iterated at most 10 times. The centroid estimates provided by the TEKA algorithm are much smoother than the ones provided by DBA or CTW. This denoising property, expected from any averaging algorithm, will be addressed in a dedicated experiment (c.f. subsection 4.3)."}, {"heading": "3.5 Role of parameter \u03bd", "text": "In practice, the selectivity or bandwidth of the local alignment kernels (that is controlled by parameter \u03bd) has to be adapted according to the the lengths of the time series. If the time series are long, then \u03bd should be reduced to maintain the calculability of the forward-backward matrices, and the local selectivity decreases. Hence, more alignment paths are likely and more sample pairs participate to the calculation of the average such that local details are filtered out by the averaging. Conversely if the time series are short, \u03bd can be increased, hence fewer sample pairs participate to the calculation of the average, and details can be preserved."}, {"heading": "3.6 Computational complexity", "text": "TEKA has intrinsically the same algorithmic complexity than the DBA algorithm, basically O(L2) for each pairwise averaging, where L is the average length of the time series.\n8 DBA\nCTW\nTEKA\nTABLE 1 Centroid estimation for the three categories of the CBF dataset and for the three tested algorithms: DBA (top), CTW (center) TEKA (bottom). The centroid estimations are indicated as a bold black line superimposed on top of the time series (in light red) that are averaged.\nNevertheless, computationally speaking, TEKA algorithm is slightly more costly mainly because of two reasons:\n\u2022 the FB matrix induces a factor three in complexity because of the reverse alignment and the multiplication term by term of the forward and backward matrices. \u2022 the exponential terms that enter into the computation of KDTW (Eq. (4)) are costly, basically O(M(n)n1/2), where M(n) is the cost of the floating point multiplication, and n is the number of digits. This induces another factor 2 or 3 depending on the chosen floating point precision.\nThe overall algorithmic cost for averaging a set of N time series of average length L with an average number of iterations I is, for the two algorithms, O(I \u00b7N \u00b7 L2).\nSome optimization are indeed possible, in particular replacing the exponential function by another local kernel easier to compute is an important source of algorithmic simplification. We do not address further this issue in this paper and let it stand as a perspective."}, {"heading": "4 EXPERIMENTS", "text": "The two first proposed experiments aim at demonstrating the benefits of using time elastic centroids in a data reduction paradigm: 1-NC/NM (first near centroid or medoid) classification for the first one, and isolated gesture recognition for the second one using 1-NC/NM and SVM classifiers in conjunction with the KDTW kernel. The third experiment explores the noise reduction angle brought by time elastic centroids."}, {"heading": "4.1 1-Nearest Centroid/Medoid classification", "text": "The purpose of this experiment is to evaluate the effectiveness of the proposed time elastic averaging method (TEKA) against a triple baseline. The first baseline allow us to compare centroid-based with medoid-based approaches. The second and third baselines are provided by the DBA [19] and CTW [20] algorithms (thanks to the implementation proposed by the authors), currently considered as state of the art methods to average a set of sequences consistently with DTW. We have tested the CTW averaging with a 1- NC-DTW (CTW1) and a 1-NC-KDTW (CTW2) classifier to highlight the impact of the selected similarity measure.\nFor this purpose, we empirically evaluate the effectiveness of the methods using a first nearest centroid/medoid (1-NC/NM) classification task on a set of time series derived from widely diverse fields of application. The task consists of representing each category contained in a training data set by estimating its medoid or centroid and then evaluating the error rate of a 1-NC classifier on an independent testing data set. Hence, the classification rule consists of assigning to the tested time series the category which corresponds to the closest (or most similar) medoid or centroid according to the DTW measure for DTW medoid (DTW-M), DBA and CTW centroids (CTW1) or to KDTW measure for KDTW medoid (KDTW-M), CTW (CTW2) and TEKA centroids.\nIn [8] a generalized k-NC task is described. The authors demonstrate that by selecting the appropriate number k of centroids (using DBA and k-means), they achieve, without\n9 loss, a 70% speed-up in average, compared to the original k-Nearest Neighbor task. Although, in general, the classification accuracy is improved when several centroids are used to represent the training datasets, our main purpose is to highlight and amplify the discrimination between time series averaging methods: this is why we stick here with the 1-NC task.\nA collection of 45 heterogeneous data sets is used to assess the proposed algorithms. The collection includes synthetic and real data sets, as well as univariate and multivariate time series. These data sets are distributed as follows:\n\u2022 42 of these data sets are available at the UCR repository [31]. Basically, we used all the data sets except for StarLightCurves, Non-Invasive Fetal ECG Thorax1 and Non-Invasive Fetal ECG Thorax2. Although these last three data sets are still tractable, their computational cost is high because of their size and the length of the time series they contain. All these data sets are composed of scalar time series. \u2022 One data set, uWaveGestureLibrary 3D was constructed from the uWaveGestureLibrary X\u2014Y\u2014Z scalar data sets to compose a new set of multivariate (3D) time series. \u2022 One data set, CharTrajTT, is available at the UCI Repository [32] under the name Character Trajectories Data Set. This data set contains multivariate (3D) time series and is divided into two equal sized data sets (TRAIN and TEST) for the experiment. \u2022 The last data set, PWM2, which stands for Pulse Width Modulation [33], was specifically defined to demonstrate a weakness in dynamic time warping (DTW) pseudo distance. This data set is composed of synthetic scalar time series.\nFor each dataset, a training subset (TRAIN) is defined as well as an independent testing subset (TEST). We use the training sets to extract single medoids or centroid estimates for each of the categories defined in the data sets.\nFurthermore, for KDTW-M, CTW2 and TEKA, the \u03bd parameter is optimized using a leave-one-out (LOO) procedure carried out on the TRAIN data sets. The \u03bd value is selected within the discrete set {.01, .05, .1, .25, .5, .75, 1, 2, 5, 10, 15, 20, 25, 50, 100}. The value that minimizes the LOO classification error rate on the TRAIN data is then used to provide the error rates that are estimated on the TEST data.\nThe classification results are given in Table 2. It can be seen from this experiment, that\ni) Centroid-based methods outperform medoid-based methods: DBA and CTW (CTW2) yield lower error rates compared to DTW-M, as do TEKA compared to KDTWM and DTW-M.\nii) CTW pairs much better with KDTW (CTW2 outperforms CTW1)\niii) TEKA outperforms DBA (under the same experimental conditions (maximum of 10 iterations)), and CTW.\nThe average ranking for all six tested methods, which supports our preliminary conclusion, is given at the bottom of Table 2.\nIn Table 3 we report the P-values for each pair of tested algorithms using a Wilcoxon signed-rank test. The null hypothesis is that for a tested pair of classifiers, the difference between classification error rates obtained on the 45 datasets follows a symmetric distribution around zero. With a .05 significance level, the P-values that lead to reject the null hypothesis are shown in bolded fonts in the table. This analysis confirms our previous analysis of the classification results. We observe that centroid-based approaches perform significantly better than medoid-based approaches. Furthermore, KDTW-M appears to be significantly better than DTW-M.\nFurthermore, TEKA is evaluated as significantly better than DBA and CTW2 in this experiment. Note also that DBA does not seem to perform significantly better than KDTWM or CTW2, and that CTW1 performed similarly to DTW-M and poorly compared to the other centroid methods. Hence, it confirms out that CTW method seems to pair well with KDTW measure but poorly with the DTW measure."}, {"heading": "4.2 Instance set reduction", "text": "In this second experiment, we address an application that consists in summarizing subsets of training time series to speed-up an isolated gesture recognition process.\nThe dataset that we consider enables to explore the handshape and the upper body movement using 3D positions of skeletal joints captured using a Microsoft Kinect 2 sensor. 20 subjects have been selected (15 males and 5 females) to perform in front of the sensor (at a three meters distance) the six selected NATOPS gestures. Each subject repeated each gesture three times. Hence the isolated gesture dataset is composed of 360 gesture utterances that have been manually segmented to a fixed length of 51 frames 1.\nTo evaluate this task, we have performed a subject cross validation experiment consisting of 100 tests: for each test, 10 subjects have been randomly drawn among 20 for training and the remaining 10 subjects have been retained for testing. 1-NN/NC (our baselines) and SVM classifiers are evaluated, with or without summarizing the subsets composed with the three repetitions performed by each subjects using a single centroid (DBA, CTW, TEKA) or Medoid (KDTW-M). The \u03bd parameter of the KDTW kernel as well as the SVM meta parameter (RBF bandwidth \u03c3 and C) are optimized using a leave one subject procedure on the training dataset. The kernels exp(\u2212DTW (., .)/\u03c3) and exp(\u2212KDTW (., .)/\u03c3) are used respectively in the SVM DTW and SVM KDTW classifiers.\nTable 4 gives the assessment measures (ERR: average error rate, PRE: macro average precision, REC: macro average recall and F1 = 2 \u00b7 precision\u00b7recallprecision+recall ) for the isolated gestures classification task. In addition, the number of reference instances used by the 1-NN/NC classifiers or the number of support vectors exploited by the SVM (#Ref column in\n1. These datasets will be made available for the community at the earliest feasible opportunity\n10\n11\nthe table) are reported to demonstrate the data reduction that is induced by the methods in the training sets.\nThe results show that the DTW measure does not fit well with SVM comparatively to KDTW: the error rate or the F1 score are about 9% higher or lower for the isolated gesture task. Hence, to compare the DBA, CTW and TEKA centroids using a SVM classification, the KDTW kernel has been used. When using the centroids (SVM KDTW-DBA, SVM KDTW-CTW, SVM KDTW-TEKA), or Medoids (SVM KDTW-M) the error rate or F1 score increases or decreases\nonly by around 2.5% and 2% comparatively to the SVMKDTW that achieves the best scores. Meanwhile the number of support vectors exploited by the SVM drops by a two factor, leading to an expected speed-up of 2. Compared to 1-NN classification without centroids, the SVM KDTW with centroids achieves a much better performance, with an expected speed-up of 4 (\u223c 50 support vectors comparatively to 180 gesture instances). This demonstrates the capacity of centroid methods to reduce significantly the size of the training sets while maintaining a very similar level of accuracy.\nIn more details, the TEKA is the centroid-based method that achieves the lowest error rates for the two classification tasks, while DBA is the centroid-based method that exploits the fewest support vectors (46.5).\nTable 5 and 6 give the P-values for the Wilcoxon signedrank tests. With the same null hypothesis as above (difference between the error rates follows a symmetric distribution around zero), and with a .05 significance level, the Pvalues that lead to reject the null hypothesis are presented in bolded fonts in the tables. From Table 5 we note that 1NNKDTW (which exploits the full training set) performs significantly better than 1NN DTW, 1-NC DTW-DBA and 1-NC KDTW-CTW but not significantly than 1-NC KDTW-TEKA. Conversely, 1-NC KDTW-TEKA performs significantly better that 1-NC DTW-DBA but not significantly better that 1-NC KDTW-CTW. Similarly, from Table 6 we observe that SVM KDTW, which exploits the full training set, performs significantly better than all centroid or medoid based methods. Also, SVM KDTW-TEKA performs significantly better than SVM KDTW-CTW but not significantly better than SVM KDTW-DBA. Finally SVM KDTW-TEKA and SVM KDTW-DBA outperform the medoid based method (SVM KDTW-M) but not SVM KDTW-CTW.\nIf the three centroid methods show rather close accuracies on this experiment, TEKA is significantly better than DBA on the 1NC task and significantly better than CTW on the SVM task."}, {"heading": "4.3 Denoising experiment", "text": "To demonstrate the utility of centroid based methods for denoising data, we construct a demonstrative synthetic experiment that provides some insights. The test is based on the following 2D periodic signal:\nXk(t) = ( Ak +Bk\n\u221e\u2211 i=1 \u03b4(t\u2212 2\u03c0i 6\u03c9k )\n) cos(\u03c9kt+ \u03c6k) (17)\nYk(t) = ( Ak +Bk\n\u221e\u2211 i=1 \u03b4(t\u2212 2\u03c0i 6\u03c9k )\n) sin(\u03c9kt+ \u03c6k)\nwhere Ak = A0 +ak, Bk = (A0 +5)+bk and \u03c9k = \u03c90 +wk, A0 and \u03c90 are constant and ak, bk, \u03c9k, \u03c6k are small perturbation in amplitude, frequency and phase respectively and randomly drawn from ak \u2208 [0, A0/10], bk \u2208 [0, A0/10], \u03c9k \u2208 [\u2212\u03c90/6.67, \u03c90/6.67], \u03c6k \u2208 [\u2212\u03c90/10, \u03c90/10].\nIn practice we have adopted the following setting: f0 = \u03c9o/(2.\u03c0) = 20Hz, and A0 = 1. We then center and normalize this 2D signal to get (X\u0303k(t), Y\u0303k(t)) corresponding to the plots given in Figure 4. The log power spectrum of the X\u0303k component, that is presented in Figure 5, shows the\n12\nDirac spike located at f0 = 20Hz (corresponding to the sine component), and the convolution of this spike with a Dirac comb in the frequency domain that results in pairs of Dirac spikes symmetrically located (\u00b120Hz) around multiples of 6f0, namely 120Hz, 240Hz, etc. This shows that this signal is characterized by an infinite spectrum.\nWe consider then noise utterances k(t) with zero mean and variance one added to each instances of the 2D signal:\nxk(t) = X\u0303k(t) + k(t)\nyk(t) = Y\u0303k(t) + k(t)\nleading to a signal to noise ratio of 0dB. An example of such noisy instance is given in Figure 6. Because of the scattering\nFig. 6. Noisy (xk(t), yk(t)) waveforms (top) and corresponding 2D shape (bottom) of the synthetic signal.\nof the random components of the signal in a wide spectral band, traditional noise reduction techniques, such as those presented in [5] for instance, will not allow to recover the signal properly.\nThe task consists in reducing the noise as far as possible to recover the 2D shape of the noise free signal from a small set of noisy instances {(xk, yk)}k=1\u00b7\u00b7\u00b78 containing two \u201dperiods\u201d of the clean signal. Figure 7 presents the centroid shapes obtained using, from left to right, Euclidean, DBA, CTW and TEKA methods respectively. We can see that the Euclidean centroid retrieves partially the low frequency sine component without properly sorting out the spikes components, while DBA more accurately retrieves the spikes, however without achieving to suppress the low frequency noise around the sine component. CTW centroid appears to be in between and achieves partially to reduce the low frequency noise and to extract the spikes. TEKA achieves the best retrieval of the sine and spikes components that are better timely and spatially separated. The spectral analysis presented in Figure 7 (top) gives further insight: for DBA and CTW centroids, top center sub-figures, the series of pairs of Dirac spikes (in dotted red) are still hidden into the noise level (black curve), while it is much more separated from the noise for the TEKA centroid, as shown in the top right side sub-figure.\nMoreover, if we take the clean shapes as ground truth, the signal to noise ratio (SNR) gains estimated from the log power spectra (to get rid of the phase) is 0dB for the noisy shapes , while it is 1.58dB for the Euclidean centroid, 1.17dB for the DBA centroid, 1.57dB for the CTW\n13\ncentroid, and 3.88dB for the TEKA centroid. Note that in the calculation of the SNR, preserving the spikes has a lower impact compared to preserving the low frequency sine wave, which explains why the SNR values obtained by the DBA and CTW centroid are lower than for the Euclidean centroid.\nIn terms of noise reduction, this experiment demonstrates the ability of the TEKA centroid to better recover, from few noisy utterances, a signal whose components are scattered in a wide band spectrum. Indeed, if the noise level increases, the quality of the denoising will be reduced."}, {"heading": "4.4 Discussion", "text": "We believe that the noise filtering ability of TEKA is mainly due to the averaging technique described in the equation (16), which aggregates many plausible alignments between samples (instead of a best one) while averaging also the time of occurrence of the samples, in particular those corresponding to expected pattern location and duration such as the CBF shapes or the spike locations in the third experiment. This ability is also likely to explain the best accuracy results obtained by TEKA comparatively to the state of the art methods, CTW and DBA.\nFurthermore, it seems that the KDTW measure is more adapted to match centroids than DTW. Here again, handling several good to best alignments rather than a single optimal one allows for matching the centroids in many ways that are averaged by the measure. This has been verified for CTW in 1-NC classification tasks and is true for TEKA and DBA also.\nThe main limitation in exploiting TEKA (and KDTW) is the tuning of the \u03bd parameter that control the selectivity of the local kernel. \u03bd is dependent on the length of the time\nseries and need to be adapted to the task itself. Basically, if \u03bd is too small TEKA will filter out high frequency events just as a moving average filter. Conversely, if \u03bd is too high, the computation of the products of local probabilities along the alignment paths will bear some loss of significance in terms of the numerical calculation. Despite this tuning requirement, the three experiments, that we have carried out in this study, demonstrate its applicability and usefulness."}, {"heading": "5 CONCLUSION", "text": "In this paper, we have addressed the problem of averaging a set of time series in the context of a time elastic distance measure such as Dynamic Time Warping. The new perspective provided by the kernelization of the elastic distance allows a re-interpretation of pairwise kernel alignment matrices as the result of a forward-backward procedure applied on the states of an equivalent stochastic alignment automata. From this re-interpretation, we have proposed a new algorithm, TEKA, based on an iterative agglomerative heuristic method that allows for efficiently computing good solutions to the multi-alignment of time series. This algorithm exhibits quite interesting denoising capabilities which enlarges the area of its potential applications.\nWe have presented extensive experiments carried out on synthetic and real data sets, containing univariate but also multivariate time series. Our results show that centroid-based methods significantly outperform medoidbased methods in the context of a first nearest neighbor and SVM classification tasks. More strikingly, the TEKA algorithm, which integrates joint averaging in the sample space and along the time axis, is significantly better than the state-of-the art DBA and CTW algorithms, with a similar\n14\nalgorithmic complexity. It enables robust training set reduction which has been experimented on an isolated gesture recognition task. Finally we have developed a dedicated synthetic test to demonstrate the denoising capability of our algorithm, a property that is not supported at a same level by the other time-elastic centroid methods on this test."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank the French Ministry of Research, the Brittany Region and the European Regional Development Fund that partially funded this research. The authors also thank the promoters of the UCR and UCI data repositories for providing the datasets used in this study."}], "references": [{"title": "Sur quelques points du calcul fonctionnel", "author": ["M. Fr\u00e9chet"], "venue": "Ed. The\u0300se, Faculte\u0301 des sciences de Paris.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1906}, {"title": "Automatic recognition of 200 words", "author": ["V.M. Velichko", "N.G. Zagoruyko"], "venue": "Int. Jour. of Man-Mach. Stud., vol. 2, pp. 223\u2013234, 1970.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1970}, {"title": "A dynamic programming approach to continuous speech recognition", "author": ["H. Sakoe", "S. Chiba"], "venue": "Proceedings of the 7th International Congress of Acoustic, 1971, pp. 65\u201368.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1971}, {"title": "Digital signal averaging", "author": ["R. Kaiser", "W. Knight"], "venue": "Journal of Magnetic Resonance (1969), vol. 36, no. 2, pp. 215 \u2013 220, 1979.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1969}, {"title": "Reducing noise by repetition: introduction to signal averaging", "author": ["U. Hassan", "M.S. Anwar"], "venue": "European Journal of Physics, vol. 31, no. 3, p. 453, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "A global averaging method for dynamic time warping, with applications to clustering", "author": ["F. Petitjean", "A. Ketterlin", "P. Gan\u00e7arski"], "venue": "Pattern Recogn., vol. 44, no. 3, pp. 678\u2013693, Mar. 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Outlier detection for temporal data: A survey", "author": ["M. Gupta", "J. Gao", "C.C. Aggarwal", "J. Han"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 26, no. 9, pp. 2250\u20132267, Sept 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Dynamic time warping averaging of time series allows faster and more accurate classification", "author": ["F. Petitjean", "G. Forestier", "G. Webb", "A. Nicholson", "Y. Chen", "E. Keogh"], "venue": "Proceedings of the 14th IEEE International Conference on Data Mining, 2014, pp. 470\u2013479.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "On the marriage of lp-norms and edit distance", "author": ["L. Chen", "R. Ng"], "venue": "Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30, ser. VLDB \u201904. VLDB Endowment, September 2004, pp. 792\u2013803.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Time warp edit distance with stiffness adjustment for time series matching", "author": ["P.-F. Marteau"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 31, no. 2, pp. 306\u2013318, Feb 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonlinear alignment and averaging for estimating the evoked potential", "author": ["L. Gupta", "D. Molfese", "R. Tammana", "P. Simos"], "venue": "Biomedical Eng., IEEE Trans. on, vol. 43, no. 4, pp. 348\u2013356, 1996.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "An introduction to biological sequence analysis", "author": ["S.S.L.K.H. Fasman"], "venue": "Comp. Methods in Mol. Biology,. In Salzberg, S.L., Searls, D.B., and Kasif, S., eds., Elsevier, 1998, pp. 21\u201342.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "On the complexity of multiple sequence alignment.", "author": ["L. Wang", "T. Jiang"], "venue": "Jour. of Comp. Biology,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1994}, {"title": "Computational complexity of multiple sequence alignment with sp-score", "author": ["W. Just", "W. Just"], "venue": "Journal of Computational Biology, vol. 8, pp. 615\u2013623, 1999.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1999}, {"title": "The multiple sequence alignment problem in biology", "author": ["H. Carrillo", "D. Lipman"], "venue": "SIAM J. Appl. Math., vol. 48, no. 5, pp. 1073\u2013 1082, Oct. 1988.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1988}, {"title": "Shape averaging under time warping", "author": ["V. Niennattrakul", "C. Ratanamahatana"], "venue": "Electronics, Computer, Telecommunications and Information Technology, 2009. ECTI-CON 2009. 6th Int. Conf. on, vol. 02, May 2009, pp. 626\u2013629.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Cross-words reference template for dtw-based speech recognition systems", "author": ["W. Abdulla", "D. Chow", "G. Sin"], "venue": "TENCON 2003. Conference on Convergent Technologies for the Asia-Pacific Region, vol. 4, Oct 2003, pp. 1576\u20131579 Vol.4.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Time-series clustering by approximate prototypes", "author": ["V. Hautamaki", "P. Nykanen", "P. Franti"], "venue": "Pattern Recognition, 2008. ICPR 2008. 19th International Conference on, Dec 2008, pp. 1\u20134.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Summarizing a set of time series by averaging: From Steiner sequence to compact multiple alignment", "author": ["F. Petitjean", "P. Gan\u00e7arski"], "venue": "Journal of theoretical computer science, vol. 414, no. 1, pp. 76\u201391, Jan. 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Canonical time warping for alignment of human behavior", "author": ["F. Zhou", "F. Torre"], "venue": "Advances in Neural Information Processing Systems 22, Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta, Eds. Curran Associates, Inc., 2009, pp. 2286\u20132294.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Generalized canonical time warping", "author": ["F. Zhou", "F.D. la Torre"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 38, no. 2, pp. 279\u2013294, Feb 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Generalized k-means-based clustering for temporal data under weighted and kernel time warp", "author": ["S. Soheily-Khah", "A. Douzal-Chouakria", "E. Gaussier"], "venue": "Patt. Recog. Lett., vol. 75, pp. 63 \u2013 69, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Inaccuracies of shape averaging method using dynamic time warping for time series data", "author": ["V. Niennattrakul", "C. Ratanamahatana"], "venue": "Computational Science \u2013 ICCS 2007, ser. Lecture Notes in Computer Science, Y. Shi, G. van Albada, J. Dongarra, and P. Sloot, Eds. Springer Berlin Heidelberg, 2007, vol. 4487, pp. 513\u2013520.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "On Recursive Edit Distance Kernels with Application to Time Series Classification", "author": ["P.-F. Marteau", "S. Gibet"], "venue": "IEEE Trans. on Neural Networks and Learning Systems, pp. 1\u201314, Jun. 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "A kernel for time series based on global alignments", "author": ["M. Cuturi", "J.-P. Vert", "O. Birkenes", "T. Matsui"], "venue": "IEEE ICASSP 2007, vol. 2, April 2007, pp. II\u2013413\u2013II\u2013416.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of the IEEE, vol. 77, no. 2, pp. 257\u2013286, Feb 1989.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1989}, {"title": "On the hidden Markov model and dynamic time warping for speech recognition \u2013 A unified view", "author": ["B. Juang"], "venue": "AT&T Bell Labs Technical Jour., vol. 63, no. 7, pp. 1213\u20131242, 1985.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1985}, {"title": "Speaker-Independent English Consonant and Japanese Word Recognition by a Stochastic Dynamic Time Warping Method", "author": ["S. Nakagawa", "H. Nakanishi"], "venue": "Journal of Institution of Electronics and Telecommunication Engineers, vol. 34, no. 1, pp. 87\u201395, Jan. 1989.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1989}, {"title": "Probabilistic models for joint clustering and time-warping of multidimensional curves", "author": ["D. Chudova", "S. Gaffney", "P. Smyth"], "venue": "Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence, ser. UAI\u201903. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2003, pp. 134\u2013141. [Online]. Available: http://dl.acm.org/citation.cfm?id=2100584.2100600", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "Local feature extraction and its applications using a library of bases", "author": ["N. Saito"], "venue": "Ph.D. dissertation, Dept. of Mathematics, Yale University, 1994.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1994}, {"title": "The UCR time series classification-clustering datasets", "author": ["E.J. Keogh", "X. Xi", "L. Wei", "C. Ratanamahatana"], "venue": "2006, http://wwwcs.ucr.edu/ eamonn/time series data/.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Uci machine learning repository", "author": ["M. Lichman"], "venue": "2013. [Online]. Available: http://archive.ics.uci.edu/ml", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Pulse width modulation data sets", "author": ["P.-F. Marteau"], "venue": "2007. [Online]. Available: http://people.irisa.fr/Pierre-Francois.Marteau/PWM/", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Since Maurice Fr\u00e9chet\u2019s pioneering work [1] in the early 1900s, time-elastic matching of time series or symbolic sequences has attracted much attention from the scientific community in numerous fields such as information indexing and retrieval, pattern analysis, extraction and recognition, data mining, etc.", "startOffset": 40, "endOffset": 43}, {"referenceID": 1, "context": "Among other time elastic measures, Dynamic Time Warping (DTW) was widely popularized during the 1970s with the advent of speech recognition systems [2], [3], and numerous variants that have since been proposed to match time series with a certain degree of time distortion tolerance.", "startOffset": 148, "endOffset": 151}, {"referenceID": 2, "context": "Among other time elastic measures, Dynamic Time Warping (DTW) was widely popularized during the 1970s with the advent of speech recognition systems [2], [3], and numerous variants that have since been proposed to match time series with a certain degree of time distortion tolerance.", "startOffset": 153, "endOffset": 156}, {"referenceID": 3, "context": "Time series averaging or signal averaging is a long-standing issue that is currently becoming increasingly prevalent in the big data context; it is relevant for de-noising [4], [5], summarizing subsets of time series [6], defining significant prototypes, identifying outliers [7], performing data mining tasks (mainly exploratory data analysis such as clustering) and speeding up classification [8], as well as regression or data analysis processes in a big data context.", "startOffset": 172, "endOffset": 175}, {"referenceID": 4, "context": "Time series averaging or signal averaging is a long-standing issue that is currently becoming increasingly prevalent in the big data context; it is relevant for de-noising [4], [5], summarizing subsets of time series [6], defining significant prototypes, identifying outliers [7], performing data mining tasks (mainly exploratory data analysis such as clustering) and speeding up classification [8], as well as regression or data analysis processes in a big data context.", "startOffset": 177, "endOffset": 180}, {"referenceID": 5, "context": "Time series averaging or signal averaging is a long-standing issue that is currently becoming increasingly prevalent in the big data context; it is relevant for de-noising [4], [5], summarizing subsets of time series [6], defining significant prototypes, identifying outliers [7], performing data mining tasks (mainly exploratory data analysis such as clustering) and speeding up classification [8], as well as regression or data analysis processes in a big data context.", "startOffset": 217, "endOffset": 220}, {"referenceID": 6, "context": "Time series averaging or signal averaging is a long-standing issue that is currently becoming increasingly prevalent in the big data context; it is relevant for de-noising [4], [5], summarizing subsets of time series [6], defining significant prototypes, identifying outliers [7], performing data mining tasks (mainly exploratory data analysis such as clustering) and speeding up classification [8], as well as regression or data analysis processes in a big data context.", "startOffset": 276, "endOffset": 279}, {"referenceID": 7, "context": "Time series averaging or signal averaging is a long-standing issue that is currently becoming increasingly prevalent in the big data context; it is relevant for de-noising [4], [5], summarizing subsets of time series [6], defining significant prototypes, identifying outliers [7], performing data mining tasks (mainly exploratory data analysis such as clustering) and speeding up classification [8], as well as regression or data analysis processes in a big data context.", "startOffset": 395, "endOffset": 398}, {"referenceID": 1, "context": "Time series averaging in the context of (multiple) time elastic distance alignments has been mainly addressed in the scope of the Dynamic Time Warping (DTW) measure [2], [3].", "startOffset": 165, "endOffset": 168}, {"referenceID": 2, "context": "Time series averaging in the context of (multiple) time elastic distance alignments has been mainly addressed in the scope of the Dynamic Time Warping (DTW) measure [2], [3].", "startOffset": 170, "endOffset": 173}, {"referenceID": 8, "context": "Although other time elastic distance measures such as the Edit Distance With Real Penalty (ERP) [9] or the Time Warp Edit Distance (TWED) [10] could be considered instead, without loss of generality, we remain focused throughout this paper on DTW and its kernelization.", "startOffset": 96, "endOffset": 99}, {"referenceID": 9, "context": "Although other time elastic distance measures such as the Edit Distance With Real Penalty (ERP) [9] or the Time Warp Edit Distance (TWED) [10] could be considered instead, without loss of generality, we remain focused throughout this paper on DTW and its kernelization.", "startOffset": 138, "endOffset": 142}, {"referenceID": 10, "context": "From the DTW measure, [11] have defined the time elastic average a(X,Y ) of a pair of time series", "startOffset": 22, "endOffset": 26}, {"referenceID": 11, "context": "Multiple alignments have been widely studied in bioinformatics [12], and it has been shown that determining the optimal alignment of a set of sequences under the sum of all pairs (SP) score scheme is a NP-complete problem [13] [14].", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "Multiple alignments have been widely studied in bioinformatics [12], and it has been shown that determining the optimal alignment of a set of sequences under the sum of all pairs (SP) score scheme is a NP-complete problem [13] [14].", "startOffset": 222, "endOffset": 226}, {"referenceID": 13, "context": "Multiple alignments have been widely studied in bioinformatics [12], and it has been shown that determining the optimal alignment of a set of sequences under the sum of all pairs (SP) score scheme is a NP-complete problem [13] [14].", "startOffset": 227, "endOffset": 231}, {"referenceID": 14, "context": "The time and space complexity of this problem is O(L), where k is the number of sequences in the set and L is the length of the sequences when using dynamic programming to search for an optimal solution [15].", "startOffset": 203, "endOffset": 207}, {"referenceID": 10, "context": "One popular approach consists of providing a random order for the leaves, and then constructing the binary tree up to the root using this ordering [11].", "startOffset": 147, "endOffset": 151}, {"referenceID": 15, "context": "Another approach involves constructing a dendrogram (a hierarchical ascendant clustering) from the data set and then using this dendrogram to calculate pairwise centroids starting with the closest pairs of time series and progressively aggregating series that are farther away [16] as illustrated on the left of Figure 1.", "startOffset": 277, "endOffset": 281}, {"referenceID": 16, "context": "The three steps of this heuristic method were first proposed in [17].", "startOffset": 64, "endOffset": 68}, {"referenceID": 17, "context": "The iterative aspect of this heuristic approach was initially introduced by [18] and refined by [6] who introduced the DTW Barycenter Averaging (DBA) algorithm.", "startOffset": 76, "endOffset": 80}, {"referenceID": 5, "context": "The iterative aspect of this heuristic approach was initially introduced by [18] and refined by [6] who introduced the DTW Barycenter Averaging (DBA) algorithm.", "startOffset": 96, "endOffset": 99}, {"referenceID": 18, "context": "For instance [19] have evaluated a genetic algorithm for managing a population of centroid candidates, thus improving with some success the straightforward iterative heuristic methods.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "Among other works, some attempt to use this kind of direct approach for the estimation of time elastic centroid was recently addressed in [20], [21] and [22].", "startOffset": 138, "endOffset": 142}, {"referenceID": 20, "context": "Among other works, some attempt to use this kind of direct approach for the estimation of time elastic centroid was recently addressed in [20], [21] and [22].", "startOffset": 144, "endOffset": 148}, {"referenceID": 21, "context": "Among other works, some attempt to use this kind of direct approach for the estimation of time elastic centroid was recently addressed in [20], [21] and [22].", "startOffset": 153, "endOffset": 157}, {"referenceID": 19, "context": "In [20] the authors detail a Canonical Time Warp (CTW) and a Generalized version of it (GCTW) [21] that combines DTW and CCA (Canonical Correlation Analysis) for temporally aligning multi-modal motion sequences.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "In [20] the authors detail a Canonical Time Warp (CTW) and a Generalized version of it (GCTW) [21] that combines DTW and CCA (Canonical Correlation Analysis) for temporally aligning multi-modal motion sequences.", "startOffset": 94, "endOffset": 98}, {"referenceID": 21, "context": "In [22], a non-convex constrained optimization problem is derived, by integrating a temporal weighting of local sample alignments to highlight the temporal region of interest in a time series data set, thus penalizing the other temporal regions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "It should also be mentioned that some criticism of these heuristic methods has been made in [23].", "startOffset": 92, "endOffset": 96}, {"referenceID": 23, "context": "In this study, we do not directly address the issue of time elastic centroid estimation from the DTW perspective, but rather from the point of view of the regularized dynamic time warping kernel (KDTW) [24].", "startOffset": 202, "endOffset": 206}, {"referenceID": 1, "context": "The Dynamic Time Warping (DTW) distance between two time series X 1 = X1X2 \u00b7 \u00b7 \u00b7Xp and Y q 1 = Y1Y2 \u00b7 \u00b7 \u00b7Yq of lengths p and q respectively, [2], [3] as defined in equation (1) can be recursively evaluated as ddtw(X p 1 , Y q 1 ) =", "startOffset": 141, "endOffset": 144}, {"referenceID": 2, "context": "The Dynamic Time Warping (DTW) distance between two time series X 1 = X1X2 \u00b7 \u00b7 \u00b7Xp and Y q 1 = Y1Y2 \u00b7 \u00b7 \u00b7Yq of lengths p and q respectively, [2], [3] as defined in equation (1) can be recursively evaluated as ddtw(X p 1 , Y q 1 ) =", "startOffset": 146, "endOffset": 149}, {"referenceID": 24, "context": "Regularized DTW: seminal work by [25], prolonged recently by [24] leads us to propose new guidelines to ensure that kernels constructed from elastic measures such as DTW are positive definite.", "startOffset": 33, "endOffset": 37}, {"referenceID": 23, "context": "Regularized DTW: seminal work by [25], prolonged recently by [24] leads us to propose new guidelines to ensure that kernels constructed from elastic measures such as DTW are positive definite.", "startOffset": 61, "endOffset": 65}, {"referenceID": 23, "context": "A simple instance of such a regularized kernel, derived from [24], can be expressed as a convolution kernel, which makes use of two recursive terms:", "startOffset": 61, "endOffset": 65}, {"referenceID": 25, "context": "The stochastic alignment process that we propose finds its roots in the forward-backward algorithm defined for the learning of Hidden Markov Models (HMM) [26] and in the parallel between HMM and DTW that is proposed in [27], [28] and in a more distant way in [29].", "startOffset": 154, "endOffset": 158}, {"referenceID": 26, "context": "The stochastic alignment process that we propose finds its roots in the forward-backward algorithm defined for the learning of Hidden Markov Models (HMM) [26] and in the parallel between HMM and DTW that is proposed in [27], [28] and in a more distant way in [29].", "startOffset": 219, "endOffset": 223}, {"referenceID": 27, "context": "The stochastic alignment process that we propose finds its roots in the forward-backward algorithm defined for the learning of Hidden Markov Models (HMM) [26] and in the parallel between HMM and DTW that is proposed in [27], [28] and in a more distant way in [29].", "startOffset": 225, "endOffset": 229}, {"referenceID": 28, "context": "The stochastic alignment process that we propose finds its roots in the forward-backward algorithm defined for the learning of Hidden Markov Models (HMM) [26] and in the parallel between HMM and DTW that is proposed in [27], [28] and in a more distant way in [29].", "startOffset": 259, "endOffset": 263}, {"referenceID": 27, "context": "1) we do not construct a parallel with DTW, but with its kernelized variant KDTW 2) [28] only consider an optimal alignment path (exploiting the Viterbi algorithm) while we consider the whole set of possible alignments (as in [27]) 3) [27] construct an asymmetric classical left-right HMM (one time series plays the role of the observation sequence, while the other plays the role of the state sequence).", "startOffset": 84, "endOffset": 88}, {"referenceID": 26, "context": "1) we do not construct a parallel with DTW, but with its kernelized variant KDTW 2) [28] only consider an optimal alignment path (exploiting the Viterbi algorithm) while we consider the whole set of possible alignments (as in [27]) 3) [27] construct an asymmetric classical left-right HMM (one time series plays the role of the observation sequence, while the other plays the role of the state sequence).", "startOffset": 226, "endOffset": 230}, {"referenceID": 26, "context": "1) we do not construct a parallel with DTW, but with its kernelized variant KDTW 2) [28] only consider an optimal alignment path (exploiting the Viterbi algorithm) while we consider the whole set of possible alignments (as in [27]) 3) [27] construct an asymmetric classical left-right HMM (one time series plays the role of the observation sequence, while the other plays the role of the state sequence).", "startOffset": 235, "endOffset": 239}, {"referenceID": 28, "context": "With a similar idea [29] proposes a generative mixture model along a discrete time grid axis with local and global time warp capability.", "startOffset": 20, "endOffset": 24}, {"referenceID": 25, "context": "We derive the forward-backward alignment algorithm for our stochastic alignment automata from its classical derivation that was defined for Hidden Markov Models [26].", "startOffset": 161, "endOffset": 165}, {"referenceID": 29, "context": "K = Kdtw, for the Cylinder c(t), Bell, b(t) Funnel, f(t), synthetic functions [30] defined as follows c(t) = (6 + \u03b7) \u00b7 \u03c7[a,b](t) + (t) b(t) = (6 + \u03b7) \u00b7 \u03c7[a,b](t) \u00b7 (t\u2212 a)/(b\u2212 a) + (t) f(t) = (6 + \u03b7) \u00b7 \u03c7[a,b](t) \u00b7 (b\u2212 t)/(b\u2212 a) + (t) where \u03c7[a,b] = 0 if t < a \u2228 t > b, 1 if a \u2264 t \u2264 b, \u03b7 and (t) are obtained from a standard normal distribution N(0, 1), a is an integer obtained from a uniform distribution in [16, 32] and b\u2212 a is another integer obtained from another uniform distribution in [32, 96].", "startOffset": 78, "endOffset": 82}, {"referenceID": 15, "context": "K = Kdtw, for the Cylinder c(t), Bell, b(t) Funnel, f(t), synthetic functions [30] defined as follows c(t) = (6 + \u03b7) \u00b7 \u03c7[a,b](t) + (t) b(t) = (6 + \u03b7) \u00b7 \u03c7[a,b](t) \u00b7 (t\u2212 a)/(b\u2212 a) + (t) f(t) = (6 + \u03b7) \u00b7 \u03c7[a,b](t) \u00b7 (b\u2212 t)/(b\u2212 a) + (t) where \u03c7[a,b] = 0 if t < a \u2228 t > b, 1 if a \u2264 t \u2264 b, \u03b7 and (t) are obtained from a standard normal distribution N(0, 1), a is an integer obtained from a uniform distribution in [16, 32] and b\u2212 a is another integer obtained from another uniform distribution in [32, 96].", "startOffset": 408, "endOffset": 416}, {"referenceID": 31, "context": "K = Kdtw, for the Cylinder c(t), Bell, b(t) Funnel, f(t), synthetic functions [30] defined as follows c(t) = (6 + \u03b7) \u00b7 \u03c7[a,b](t) + (t) b(t) = (6 + \u03b7) \u00b7 \u03c7[a,b](t) \u00b7 (t\u2212 a)/(b\u2212 a) + (t) f(t) = (6 + \u03b7) \u00b7 \u03c7[a,b](t) \u00b7 (b\u2212 t)/(b\u2212 a) + (t) where \u03c7[a,b] = 0 if t < a \u2228 t > b, 1 if a \u2264 t \u2264 b, \u03b7 and (t) are obtained from a standard normal distribution N(0, 1), a is an integer obtained from a uniform distribution in [16, 32] and b\u2212 a is another integer obtained from another uniform distribution in [32, 96].", "startOffset": 408, "endOffset": 416}, {"referenceID": 31, "context": "K = Kdtw, for the Cylinder c(t), Bell, b(t) Funnel, f(t), synthetic functions [30] defined as follows c(t) = (6 + \u03b7) \u00b7 \u03c7[a,b](t) + (t) b(t) = (6 + \u03b7) \u00b7 \u03c7[a,b](t) \u00b7 (t\u2212 a)/(b\u2212 a) + (t) f(t) = (6 + \u03b7) \u00b7 \u03c7[a,b](t) \u00b7 (b\u2212 t)/(b\u2212 a) + (t) where \u03c7[a,b] = 0 if t < a \u2228 t > b, 1 if a \u2264 t \u2264 b, \u03b7 and (t) are obtained from a standard normal distribution N(0, 1), a is an integer obtained from a uniform distribution in [16, 32] and b\u2212 a is another integer obtained from another uniform distribution in [32, 96].", "startOffset": 491, "endOffset": 499}, {"referenceID": 18, "context": "The figures presented in Table 1 compare the centroid estimates provided by the iterated DBA [19], CTW [20] and TEKA algorithms.", "startOffset": 93, "endOffset": 97}, {"referenceID": 19, "context": "The figures presented in Table 1 compare the centroid estimates provided by the iterated DBA [19], CTW [20] and TEKA algorithms.", "startOffset": 103, "endOffset": 107}, {"referenceID": 18, "context": "The second and third baselines are provided by the DBA [19] and CTW [20] algorithms (thanks to the implementation proposed by the authors), currently considered as state of the art methods to average a set of sequences consistently with DTW.", "startOffset": 55, "endOffset": 59}, {"referenceID": 19, "context": "The second and third baselines are provided by the DBA [19] and CTW [20] algorithms (thanks to the implementation proposed by the authors), currently considered as state of the art methods to average a set of sequences consistently with DTW.", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "In [8] a generalized k-NC task is described.", "startOffset": 3, "endOffset": 6}, {"referenceID": 30, "context": "\u2022 42 of these data sets are available at the UCR repository [31].", "startOffset": 60, "endOffset": 64}, {"referenceID": 31, "context": "\u2022 One data set, CharTrajTT, is available at the UCI Repository [32] under the name Character Trajectories Data Set.", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "\u2022 The last data set, PWM2, which stands for Pulse Width Modulation [33], was specifically defined to demonstrate a weakness in dynamic time warping (DTW) pseudo distance.", "startOffset": 67, "endOffset": 71}, {"referenceID": 4, "context": "of the random components of the signal in a wide spectral band, traditional noise reduction techniques, such as those presented in [5] for instance, will not allow to recover the signal properly.", "startOffset": 131, "endOffset": 134}], "year": 2017, "abstractText": "In the light of regularized dynamic time warping kernels, this paper re-considers the concept of time elastic centroid for a set of time series. We derive a new algorithm based on a probabilistic interpretation of kernel alignment matrices. This algorithm expresses the averaging process in terms of a stochastic alignment automata. It uses an iterative agglomerative heuristic method for averaging the aligned samples, while also averaging the times of occurrence of the aligned samples. By comparing classification accuracies for 45 heterogeneous time series datasets obtained by first nearest centroid/medoid classifiers we show that: i) centroid-based approaches significantly outperform medoid-based approaches, ii) for the considered datasets, our algorithm that combines averaging in the sample space and along the time axes, emerges as the most significantly robust model for time-elastic averaging with a promising noise reduction capability. We also demonstrate its benefit in an isolated gesture recognition experiment and its ability to significantly reduce the size of training instance sets. Finally we highlight its denoising capability using demonstrative synthetic data: we show that it is possible to retrieve, from few noisy instances, a signal whose components are scattered in a wide spectral band.", "creator": "LaTeX with hyperref package"}}}