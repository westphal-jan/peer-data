{"id": "1611.09268", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset", "abstract": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension.This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The first dataset was used to determine what words and phrases we would describe as a question answering data in an online database (e.g., PubMed). However, this dataset was not developed in a lab using real data from a real world server. The second dataset was used to determine what language you use. This dataset was then compared to the online dataset for the word comprehension task. In this new study, we present a full dataset of the language used by MS MARCO and the word comprehension task to evaluate whether there is a difference between a question answer and a question answering data in the online database.", "histories": [["v1", "Mon, 28 Nov 2016 18:14:11 GMT  (1852kb,D)", "http://arxiv.org/abs/1611.09268v1", null], ["v2", "Tue, 29 Nov 2016 02:39:53 GMT  (1852kb,D)", "http://arxiv.org/abs/1611.09268v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["tri nguyen", "mir rosenberg", "xia song", "jianfeng gao", "saurabh tiwary", "rangan majumder", "li deng"], "accepted": false, "id": "1611.09268"}, "pdf": {"name": "1611.09268.pdf", "metadata": {"source": "CRF", "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset", "authors": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng"], "emails": ["deng}@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Building intelligent agents with the ability for reading comprehension (RC) or open-domain question answering (QA) over real world data is a major goal of artificial intelligence. Such agents can have tremendous value for consumers because they can power personal assistants such as Cortana [3], Siri [6], Alexa [1], or Google Assistant [4] found on phones or headless devices like Amazon Echo [2], all of which have been facilitated by recent advances in deep speech recognition technology [18, 9]. As these types of assistants rise in popularity, consumers are finding it more convenient to ask a question and quickly get an answer through voice assistance as opposed to navigating through a search engine result page and web browser. Intelligent agents with RC and QA abilities can also have incredible business value by powering bots that automate customer service agents for business found through messaging or chat interfaces.\nReal world RC and QA is an extremely challenging undertaking involving the amalgamation of multiple difficult tasks such as reading, processing, comprehending, inferencing/reasoning, and finally summarizing the answer.\nThe public availability of large datasets has led to many breakthroughs in AI research. One of the best examples is ImageNet\u2019s [10] exceptional release of 1.5 million labeled examples and 1000 object categories which has led to better than human level performance on object classification from images [15]. Another example is the very large speech databases collected over 20 years by DARPA that enabled successes of deep learning in speech recognition [11]. Recently there has been an influx of datasets for RC and QA as well. These databases, however, all have notable drawbacks. For example, some are not large enough to train deep models [27], and others are larger but are synthetic.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 1.\n09 26\n8v 1\n[ cs\n.C L\n] 2\n8 N\nov 2\nOne characteristic in most, if not all, of the existing databases for RC and QA research is that the distribution of questions asked in the databases are not from real users. In the creation of most RC or QA datasets, usually crowd workers are asked to create questions for a given piece of text or document. We have found that the distribution of actual questions users ask intelligent agents can be very different from those conceived from crowdsourcing them from the text.\nFurthermore, real-world questions can be messy: they may include typos and abbreviations. Another characteristic of current datasets is that text is often from high-quality stories or content such as Wikipedia. Again, real-world text may have noisy or even conflicting content across multiple documents and our experience is that intelligent agents will often need to operate over this type of problematic data.\nFinally, another unrealistic characteristic of current datasets is that answers are often restricted to an entity or a span from the existing reading text. What makes QA difficult in the real world is that an existing entity or a span of text may not be sufficient to answer the question. Finding the best answer as the output of QA systems may require reasoning across multiple pieces of text/passages. Users also prefer answers that can be read in a stand-alone fashion; this sometimes means stitching together information from multiple passages, as the ideal output not only answers the question, but also has supporting information or an explanation.\nIn this paper we introduce Microsoft MAchine Reading Comprehension (MS MARCO) - a large scale real-world reading comprehension dataset that addresses the shortcomings of the existing datasets for RC and QA discussed above. The questions in the dataset are real anonymized queries issued through Bing or Cortana and the documents are related web pages which may or may not be enough to answer the question. For every question in the dataset, we have asked a crowdsourced worker to answer it, if they can, and to mark relevant passages which provide supporting information for the answer. If they can\u2019t answer it we consider the question unanswerable and we also include a sample of those in MS MARCO. We believe a characteristic of reading comprehension is to understand when there is not enough information or even conflicting information so a question is unanswerable. The answer is strongly encouraged to be in the form of a complete sentence, so the workers may write a longform passage on their own. MS MARCO includes 100,000 questions, 1 million passages, and links to over 200,000 documents. Compared to previous publicly available datasets, this dataset is unique in the sense that (a) all questions are real user queries, (b) the context passages, which answers are derived from, are extracted from real web documents, (c) all the answers to the queries are human generated, (d) a subset of these queries has multiple answers, (e) all queries are tagged with segment information."}, {"heading": "2 Related Work", "text": "Datasets have played a significant role in making forward progress in difficult domains. The ImageNet dataset [10] is one of the best known for enabling advances in image classification and detection and inspired new classes of deep learning algorithms [22] [13] [15]. Reading comprehension and open\ndomain question answering is one of those domains existing systems still struggle to solve [31]. Here we summarize a couple of the previous approaches towards datasets for reading comprehension and open domain question answering.\nOne can find a reasonable amount of semi-synthetic reading comprehension and question answering datasets. Since these can be automatically generated they can be large enough to apply modern data intensive models. Hermann et al. created a corpus of cloze style questions from CNN / Daily News summaries [16] and Hill et al. has built the Children\u2019s Book Test [17]. Another popular question answering dataset involving reasoning is by Weston et al. [31]. One drawback with these sets is it does not capture the same question characteristics we find with questions people ask in the real world.\nMCTest is a challenging dataset which contains 660 stories created by crowdworkers, 4 questions per story, and 4 answer choices per question [27], but real-world QA systems needs to go beyond multiple choice answers or selecting from known responses. WikiQA is another set which includes 3047 questions [32]. While other sets are synthetic or editor-generated questions WikiQA is constructed using a more natural process using actual query logs. It also includes questions for which there are no correct sentences which is an important component in any QA system like MS MARCO. Unfortunately, these sets are too small to try data demanding approaches like deep learning.\nA more recently introduced reading comprehension dataset is the Stanford Question Answering Dataset (SQuAD) [26] which consists of 107785 question/answer pairs from 536 articles where the answer is span of paragraph. A few differences between MS MARCO and SQuAD is (a) SQuAD consisting of questions posed by crowdworkers while MS MARCO is sampled from the real world, (b) SQuAD is on a small set of high quality Wikipedia articles while MS MARCO is from a large set of real web documents, (c) MS MARCO includessome unanswerable queries and (d) SQuAD consists of spans while MS MARCO has human generated answers (if there is one)."}, {"heading": "3 The MS MARCO Dataset", "text": "In order to deliver true machine Reading Comprehension (RC), we start with QA as the initial problem to solve. Our introduction covered some of the key advantages of making very large RC or QA datasets freely available that contain only real-world questions and human crowdsourced answers versus artificially generated data. Given those advantages, our goal is that MS MARCO [5] - a large scale, real-world and human sourced QA dataset - will become a key vehicle to empower researchers to deliver many more AI breakthroughs in the future, just like ImageNet [10] enabled for image comprehension before.\nAdditionally, building an RC-oriented dataset helps us understand a contained yet complex RC problem while learning about all of the infrastructure pieces needed to build such a large one-million query set that helps the community make progress on state-of-the-art research problems. This task is also helping us experiment with natural language processing and deep learning models as well as to understand detailed characteristics of the very large training data required to deliver a true AI breakthrough in RC.\nThis first MS MARCO release contains 100,000 queries with answers to share the rich information and benchmarking capabilities it enables. Our first goal is to inspire the research community to try and solve reading comprehension by building great question answering and related models with the ability to carry out complex reasoning. We also aim to gather feedback and learn from the community towards completing the one-million query dataset in the near future.\nThis dataset has specific value-added features that distinguish itself from previous datasets freely available to researchers. The following factors describe the uniqueness of the MS MARCO dataset:\n\u2022 All questions are real, anonymized user queries issued to the Bing search engine. \u2022 The context passages, which answers are derived from, are extracted from real Web docu-\nments in the Bing Index. \u2022 All of the answers to the queries are human generated. \u2022 A subset of these queries has multiple answers. \u2022 A subset of these queries have no as. \u2022 All queries are tagged with segment information.\nThe next sections outline the structure, building process and distribution of the MS MARCO dataset along with metrics needed to benchmark answer or passage synthesis and our initial experimentation results."}, {"heading": "3.1 Dataset Structure and Building Process", "text": "The MS MARCO dataset structure is described in Table 2 below.\nStarting with the real-world Bing user queries we filter them down to only those that are asking for a question (1) and the Web index documents mentioned in Table 2 as data sources, we automatically extracted context passages from those documents (2). Then, human judges selected relevant passages that helped them write natural language answers to each query in a concise way (3). Following detailed guidelines, judges used a Web-based user interface (UI) to complete this task (3 and 4). A simplified example of such a UI is shown in figure 2.\nA feedback cycle and auditing process evaluated dataset quality regularly to ensure answers were accurate and followed the guidelines. In the back-end, we tagged queries with segment classification labels (5) to understand the resulting distribution and the type of data analysis, measurement and experiments this dataset would enable for researchers. Segment tags include\n\u2022 NUMERIC \u2022 ENTITY \u2022 LOCATION \u2022 PERSON \u2022 DESCRIPTION (Phrase)\nIt is important to note that the question queries above are not artificially handcrafted questions based on Web documents but real user queries issued to Bing over the years. Humans are not always clear, concise or to the point when asking questions to a search engine. An example of a real question query issued to Bing is {in what type of circulation does the oxygenated blood flow between the heart and the cells of the body?}. Unlike previously available datasets, we believe these questions better represent actual human information seeking needs and are more complex to answer compared to artificially generated questions based on a set of documents.\nTo solve for these types of questions we need a system with human level reading comprehension and reasoning abilities. E.g., given a query such as {will I qualify for osap if i\u2019m new in canada} as shown in figure 2 one of the relevant passages include:\nYou must be a 1. Canadian citizen, 2. Permanent Resident or 3. Protected person\nA RC model needs to parse and understand that being new to a country is usually the opposite of citizen, permanent resident, etc. This is not a simple task to do in a general way. As part of our dataset quality control process, we noticed that even human judges had a hard time reaching this type of conclusions, especially for content belonging to areas they were not familiar with.\nThe MS MARCO dataset that we are publishing consists of four major components:\n\u2022 Queries:These are a subset of user queries issued to a commercial search engine wherein the user is looking for a specific answer. This is in contrast to navigational intent which is another major chunk of user queries where the intent is to visit a destination website. The queries were selected through a classifier which was trained towards answer seeking intent of the query based on human labeled data. The query set was further pruned to only contain queries for which the human judges were able to generate an answer based on the passages that were provided to the judges.\n\u2022 Passages: For each query, we also present a set of approximately 10 passages which might potentially have the answer to the query. These passages are extracted from relevant webpages. The passages were selected through a separate IR (information retrieval) based machine learned system.\n\u2022 Answers: For each query, the data set also contain one or multiple answers that were generated by human judges. The judge task involved looking at the passages and synthesizing an answer using the content of the passages that best answers the given query.\n\u2022 Query type: For each query, the dataset also contains the query intent type across five different categories \u2013 (a) description, (b) numeric, (c) entity, (d) person and (e) location. For example, \"xbox one release date\" will be labeled as numeric while \"how to cook a turkey\" will be of type description.This classification is done using a machine learned classifier using human labeled training data. The features of the classifier included unigram/bigram features, brown clustering features, LDA cluster features, dependency parser features, amongst others. The classifier was a multi-class SVM classifier with an accuracy of 90.31% over test data.\nSince the query set is coming from real user queries, not all queries explicitly contain \"what\", \"where\", \"how\" kind of keywords even though the intents are similar. For example, users could type in a query like \"what is the age of barack obama\" as \"barack obama age\". Table 3.1 lists the percentage of queries that explicitly contain the words \"what\", \"where\", etc.\nThe following table shows the distribution of queries across different answer types as described earlier in this section."}, {"heading": "4 Experimental Results", "text": "In this section, we present our results over a range of experiments designed to showcase characteristics of MS MARCO dataset. As we discussed in section 3, human judgments are being accumulated in\norder to grow the dataset to the expected scale. Along the time line various snapshots of the dataset were taken and used in thoughtfully designed experiments for validation and insights. With dataset developing, the finalized experiment results may differ on the complete dataset, however, we expect observations and conclusions to be reasonably representative.\nWe group the queries in MS MARCO dataset into various categories based on their answer types, as described in subsection 3.1. The complexity of the answers varies greatly from category to category. For example, the answers to Yes/No questions are simply binary. The answers to entity questions can be a single entity name or phrase, such as the answer \"Rome\" for query \"What is the capital of Italy\". However, for other categories such as description queries, a longer textual answer is often required to answer to full extent, such as query \"What is the agenda for Hollande\u2019s state visit to Washington?\". These long textual answers may need to be derived through reasoning across multiple pieces of text. Since we impose no restrictions on the vocabulary used, different human editors often compose for the same query multiple reference answers with different expressions.\nTherefore, in our experiments different evaluation metrics are used for different categories, building on metrics from our initial proposal [24]. As shown in subsection 4.1 and 4.2, we use accuracy and precision-recall to measure the quality of the numeric answers, and apply metrics like ROUGE-L [23] and phrasing-aware evaluation framework [24] for long textual answers. The phrasing-aware evaluation framework aims to deal with the diversity of natural language in evaluating long textual answers. The evaluation requires a large number of reference answers per question that are each curated by a different human editor, thus providing a natural way to estimate how diversely a group of individuals may phrase the answer to the same question. A family of pairwise similarity based metrics can used to incorporate consensus between different reference answers for evaluation. These metrics are simple modifications to metrics like BLEU [25] and METEOR [8], and are shown to achieve better correlation with human judgments. Accordingly as part of our experiments, a subset of MS MARCO where each query has multiple answers was used to evaluate model performance with both BLEU and pa-BLEU as metrics."}, {"heading": "4.1 Generative Model Experiments", "text": "Recurrent Neural Networks (RNNs) are capable of predicting future elements from sequence prior. It is often used as a generative language model for various NLP tasks, such as machine translation [7], query answering [16], etc. In this QA experiment setup, we mainly target training and evaluation of such generative models which predict the human-generated answers given queries and/or contextual passages as model input.\nTable 5 shows the result quality from these models using ROUGE-L metric. While passages provided in MS MARCO generally contains useful information for given queries, the answer generation nature of the problem makes it relatively challenging for simple generative models to achieve great\nresults. Model advancement from Seq2Seq to Memory Networks are captured by MS MARCO on ROUGE-L.\nAdditionally we evaluated Memory Networks model on an MS MARCO subset where queries have multiple answers. Table 6 shows answers quality of the model measured by BLEU and its pairwise variant pa-BLEU [24]."}, {"heading": "4.2 Cloze-Style Model Experiments", "text": "Cloze-style test is a representative and fundamental problem in machine reading comprehension. In this test, a model attempts to predict missing symbols in a partially given text sequence by reading context texts that potentially have helpful information. CNN and Daily Mail dataset is one of the most commonly used cloze-style QA dataset. Sizable progress has been made recently from various model proposals in participating cloze-style test competition on these datasets. In this section, we present the performance of two machine reading comprehension models using both CNN test dataset and a MS MARCO subset. The subset is filtered to numeric answer type category, to which cloze-style test is applicable.\nWe show model accuracy numbers on both datasets in table 7, and precision-recall curves on MS MARCO subset in figure 1."}, {"heading": "5 Summary and Future Work", "text": "The MS MARCO dataset described in this paper above provides training data with question-answer pairs, where only a single answer text is provided via crowdsourcing. This simplicity makes the evaluation relatively easy. However, in the real world, multiple and equally valid answers are possible to a single question. This is akin to machine translation where multiple ways of translation are equally valid. Our immediate future work is to enrich the test set of the current dataset by providing multiple answers. We plan to add 1000 to 5000 such multiple answers in the dateset described in this paper.\nSubsequent evaluation experiments on comparing single vs. multiple answers will be conducted to understand whether the model we have built has better resolution with multiple answers. The evaluation metric can be the same METEOR as described in the experiments reported earlier in this paper.\nWhile MS MARCO has overcome a set of undesirable characteristics of the existing RC and QA datasets, notably the requirement that the answers to questions have to be restricted to an entity or a span from the existing reading text. Our longer-term goal is to be able to develop more advanced datasets to assess and facilitate research towards real, human-like reading comprehension. Currently, much of the successes of deep learning has been demonstrated in classification tasks [12]. Extending this success, the more complex reasoning process in many current deep-learning-based RC and QA methods has relied on multiple stages of memory networks with attention mechanisms and with close supervision information for classification. These artificial memory elements are far away from the human memory mechanism, and they derive their power mainly from the labeled data (single or multiple answers as labels) which guides the learning of network weights using a largely supervised learning paradigm. This is completely different from how human does reasoning. If we ask the current connectionist reasoning models trained on question-answer pairs to do another task such as recommendation or translation that are away from the intended classification task (i.e. answering questions expressed in a pre-fixed vocabulary), they will completely fail. Human cognitive reasoning would not fail in such cases. While recent work is moving towards this important direction [14], how to develop new deep learning methods towards human-like natural language understanding and reasoning, and how to design more advanced datasets to evaluate and facilitate this research is our longer-term goal."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation", "author": ["Satanjeev Banerjee", "Alon Lavie"], "venue": "and/or summarization,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Context-dependent pre-trained deep neural networks for largevocabulary speech recognition", "author": ["G. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Challenges in adopting speech recognition", "author": ["L. Deng", "XD Huang"], "venue": "Communications of the ACM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Deep Learning: Methods and Applications", "author": ["L. Deng", "D. Yu"], "venue": "NOW Publishers, New York,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Hybrid computing using a neural network with dynamic external memory. Nature, 2016", "author": ["Alex Graves", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka Grabska-Barwi\u0144ska", "Sergio G\u00f3mez Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou", "Adri\u00e0 Puigdom\u00e8nech Badia", "Karl Moritz Hermann", "Yori Zwols", "Georg Ostrovski", "Adam Cain", "Helen King", "Christopher Summerfield", "Phil Blunsom", "Koray Kavukcuoglu", "Demis Hassabis"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1\u0161 Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dalh", "A. Mohamed"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst"], "venue": "arXiv preprint arXiv:1603.01547,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Rouge: A package for automatic evaluation of summaries. In Text summarization branches out", "author": ["Chin-Yew Lin"], "venue": "Proceedings of the ACL-04 workshop,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "Association for Computational Linguistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "Squad: 100,000+ questions for machine comprehension", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher J.C. Burges", "Erin Renshaw"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Reasonet: Learning to stop reading in machine comprehension", "author": ["Yelong Shen", "Po-Sen Huang", "Jianfeng Gao", "Weizhu Chen"], "venue": "arXiv preprint arXiv:1609.05284,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "CoRR, abs/1409.3215,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Towards ai-complete question answering: A set of prerequisite toy", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M. Rush", "Bart van Merrienboer", "Armand Joulin", "Tomas Mikolov"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yi Yang", "Wen tau Yih", "Christopher Meek"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "Such agents can have tremendous value for consumers because they can power personal assistants such as Cortana [3], Siri [6], Alexa [1], or Google Assistant [4] found on phones or headless devices like Amazon Echo [2], all of which have been facilitated by recent advances in deep speech recognition technology [18, 9].", "startOffset": 311, "endOffset": 318}, {"referenceID": 2, "context": "Such agents can have tremendous value for consumers because they can power personal assistants such as Cortana [3], Siri [6], Alexa [1], or Google Assistant [4] found on phones or headless devices like Amazon Echo [2], all of which have been facilitated by recent advances in deep speech recognition technology [18, 9].", "startOffset": 311, "endOffset": 318}, {"referenceID": 7, "context": "5 million labeled examples and 1000 object categories which has led to better than human level performance on object classification from images [15].", "startOffset": 144, "endOffset": 148}, {"referenceID": 3, "context": "Another example is the very large speech databases collected over 20 years by DARPA that enabled successes of deep learning in speech recognition [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 18, "context": "For example, some are not large enough to train deep models [27], and others are larger but are synthetic.", "startOffset": 60, "endOffset": 64}, {"referenceID": 14, "context": "The ImageNet dataset [10] is one of the best known for enabling advances in image classification and detection and inspired new classes of deep learning algorithms [22] [13] [15].", "startOffset": 164, "endOffset": 168}, {"referenceID": 5, "context": "The ImageNet dataset [10] is one of the best known for enabling advances in image classification and detection and inspired new classes of deep learning algorithms [22] [13] [15].", "startOffset": 169, "endOffset": 173}, {"referenceID": 7, "context": "The ImageNet dataset [10] is one of the best known for enabling advances in image classification and detection and inspired new classes of deep learning algorithms [22] [13] [15].", "startOffset": 174, "endOffset": 178}, {"referenceID": 22, "context": "domain question answering is one of those domains existing systems still struggle to solve [31].", "startOffset": 91, "endOffset": 95}, {"referenceID": 8, "context": "created a corpus of cloze style questions from CNN / Daily News summaries [16] and Hill et al.", "startOffset": 74, "endOffset": 78}, {"referenceID": 9, "context": "has built the Children\u2019s Book Test [17].", "startOffset": 35, "endOffset": 39}, {"referenceID": 22, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "MCTest is a challenging dataset which contains 660 stories created by crowdworkers, 4 questions per story, and 4 answer choices per question [27], but real-world QA systems needs to go beyond multiple choice answers or selecting from known responses.", "startOffset": 141, "endOffset": 145}, {"referenceID": 23, "context": "WikiQA is another set which includes 3047 questions [32].", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "A more recently introduced reading comprehension dataset is the Stanford Question Answering Dataset (SQuAD) [26] which consists of 107785 question/answer pairs from 536 articles where the answer is span of paragraph.", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": "2, we use accuracy and precision-recall to measure the quality of the numeric answers, and apply metrics like ROUGE-L [23] and phrasing-aware evaluation framework [24] for long textual answers.", "startOffset": 118, "endOffset": 122}, {"referenceID": 16, "context": "These metrics are simple modifications to metrics like BLEU [25] and METEOR [8], and are shown to achieve better correlation with human judgments.", "startOffset": 60, "endOffset": 64}, {"referenceID": 1, "context": "These metrics are simple modifications to metrics like BLEU [25] and METEOR [8], and are shown to achieve better correlation with human judgments.", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "It is often used as a generative language model for various NLP tasks, such as machine translation [7], query answering [16], etc.", "startOffset": 99, "endOffset": 102}, {"referenceID": 8, "context": "It is often used as a generative language model for various NLP tasks, such as machine translation [7], query answering [16], etc.", "startOffset": 120, "endOffset": 124}, {"referenceID": 21, "context": "\u2022 Sequence-to-Sequence (Seq2Seq) Model: Seq2Seq [30] model is one of the most commonly used RNN models.", "startOffset": 48, "endOffset": 52}, {"referenceID": 21, "context": "We trained a vanilla Seq2Seq model similar to the one described in [30] with query as source sequence and answer as target sequence.", "startOffset": 67, "endOffset": 71}, {"referenceID": 20, "context": "\u2022 Memory Networks Model: End-to-End Memory Networks [29] was proposed for and has shown good performance in QA task for its ability of learning memory representation of contextual information.", "startOffset": 52, "endOffset": 56}, {"referenceID": 12, "context": "This is a variant of [20] where we use LSTM [19] in place of Multilayer Perceptron (MLP).", "startOffset": 21, "endOffset": 25}, {"referenceID": 11, "context": "This is a variant of [20] where we use LSTM [19] in place of Multilayer Perceptron (MLP).", "startOffset": 44, "endOffset": 48}, {"referenceID": 13, "context": "\u2022 Attention Sum Reader (AS Reader): AS Reader [21] is a simple model that uses attention to directly pick the answer from the context.", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "\u2022 ReasoNet: ReasoNet [28] also relies on attention, but is also a dynamic multi-turn model that attempts to exploit and reason over the relation among queries, contexts and answers.", "startOffset": 21, "endOffset": 25}, {"referenceID": 4, "context": "Currently, much of the successes of deep learning has been demonstrated in classification tasks [12].", "startOffset": 96, "endOffset": 100}, {"referenceID": 6, "context": "While recent work is moving towards this important direction [14], how to develop new deep learning methods towards human-like natural language understanding and reasoning, and how to design more advanced datasets to evaluate and facilitate this research is our longer-term goal.", "startOffset": 61, "endOffset": 65}], "year": 2016, "abstractText": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension. This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community.", "creator": "LaTeX with hyperref package"}}}