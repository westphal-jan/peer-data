{"id": "1611.09321", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Improving Policy Gradient by Exploring Under-appreciated Rewards", "abstract": "This paper presents a novel form of policy gradient for model-free reinforcement learning (RL) with improved exploration properties. Current policy-based methods use entropy regularization to encourage undirected exploration of the reward landscape, which is ineffective in high dimensional spaces with sparse rewards. We propose a more directed exploration strategy that promotes exploration of {\\em under-appreciated reward} regions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 28 Nov 2016 20:15:55 GMT  (391kb,D)", "http://arxiv.org/abs/1611.09321v1", "Under review at ICLR 2017"], ["v2", "Wed, 25 Jan 2017 22:35:03 GMT  (992kb,D)", "http://arxiv.org/abs/1611.09321v2", "Under review at ICLR 2017"], ["v3", "Wed, 15 Mar 2017 22:55:17 GMT  (995kb,D)", "http://arxiv.org/abs/1611.09321v3", "Published as a conference paper at ICLR 2017"]], "COMMENTS": "Under review at ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["ofir nachum", "mohammad norouzi", "dale schuurmans"], "accepted": true, "id": "1611.09321"}, "pdf": {"name": "1611.09321.pdf", "metadata": {"source": "CRF", "title": "IMPROVING POLICY GRADIENT BY EXPLORING UNDER-APPRECIATED REWARDS", "authors": ["Ofir Nachum", "Mohammad Norouzi", "Dale Schuurmans"], "emails": ["ofirnachum@google.com", "mnorouzi@google.com", "schuurmans@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Humans can reason about symbolic objects and solve algorithmic problems. After learning to count and then manipulate numbers via simple arithmetic, people eventually learn to invent new algorithms and even reason about their correctness and efficiency. The ability to invent new algorithms is fundamental to artificial intelligence (AI). Although symbolic reasoning has a long history in AI (Russell et al., 2003), only recently have statistical machine learning and neural network approaches begun to make headway in automated algorithm discovery (Reed & de Freitas, 2016; Kaiser & Sutskever, 2016; Neelakantan et al., 2016) heading to cross an important milestone on the path to AI. Nevertheless, most of the recent successes depend on the use of strong supervision to learn a mapping from a set of training inputs to outputs by maximizing a conditional log-likelihood, very much like neural machine translation systems (Sutskever et al., 2014; Bahdanau et al., 2015). Such a dependence on strong supervision is a significant limitation that does not match the ability of people to invent new algorithmic procedures based solely on trial and error.\nBy contrast, reinforcement learning (RL) methods (Sutton & Barto, 1998) hold the promise of searching over discrete objects such as symbolic representations of algorithms by considering much weaker feedback in the form of a simple verifier that tests the correctness of a program execution on a given problem instance. Despite the recent excitement around the use of RL to tackle Atari games (Mnih et al., 2015) and Go (Silver et al., 2016), standard RL methods are not yet able to consistently and reliably solve algorithmic tasks in all but the simplest cases (Zaremba & Sutskever, 2014). A key property of algorithmic problems that makes them challenging for RL is reward sparsity, i.e., a policy usually has to get a long action sequence exactly right to obtain a non-zero reward.\n\u2217Work done as a member of the Google Brain Residency program (g.co/brainresidency) \u2020Also at the Department of Computing Science, University of Alberta, daes@ualberta.ca\nar X\niv :1\n61 1.\n09 32\n1v 1\n[ cs\n.L G\n] 2\n8 N\nov 2\n01 6\nWe believe one of the key limitations of the current RL methods, preventing them from making much progress in the sparse reward settings, is the use of undirected exploration strategies (Thrun, 1992), such as -greedy and entropy regularization (Williams & Peng, 1991). For long action sequences with delayed sparse reward, it is hopeless to explore the space uniformly and blindly. Instead, we propose a formulation to encourage exploration of action sequences that are under-appreciated by the current policy. We consider an action sequence to be under-appreciated if the model\u2019s logprobability assigned to an action sequence under-estimates the resulting reward from the action sequence. Exploring under-appreciated states and actions encourages the policy to have a better calibration between its log-probabilities and observed reward values, even for action sequences with negligible rewards. This effectively increases exploration around neglected action sequences.\nWe term our proposed technique under-appreciated reward exploration (UREX). We show that the objective given by UREX is a combination of a mode seeking objective (standard REINFORCE) and a mean seeking term, which provides a good trade-off between exploitation and exploration. We apply the method to recurrent neural network (RNN)-based RL agents tackling algorithmic tasks such as sequence reversal, multi-digit addition, and binary search. The experiments demonstrate that UREX significantly outperforms baseline RL methods, such as entropy regularized REINFORCE and one-step Q-learning, especially on the seemingly more difficult tasks, such as multi-digit addition. Moreover, UREX is shown to be more robust to changes of hyper-parameters, which makes hyper-parameter tuning less tedious in practice. To our knowledge, the addition task has not been solved by any pure reinforcement learning approach. We observe that some of the policies learned by UREX can successfully generalize to long sequences; e.g., in 2 out of 5 random restarts, the policy learned by UREX for the addition task correctly generalizes to addition of numbers with 2000 digits with no mistakes, even though training sequences are at most 33 digits long."}, {"heading": "2 NEURAL NETWORKS FOR LEARNING ALGORITHMS", "text": "Although research on using neural networks to learn algorithms has had a surge of recent interest, the problem of program induction from examples has a long history in many fields, including program induction, inductive logic programming (Lavrac & Dzeroski, 1994), relational learning (Kemp et al., 2007) and regular language learning (Angulin, 1987). Rather than presenting a comprehensive survey of program induction here, we focus on neural network approaches to algorithmic tasks and highlight the relative simplicity of our neural network architecture.\nMost successful applications of neural networks to algorithmic tasks rely on strong supervision, where the inputs and target outputs are completely known a priori. Given a dataset of examples, one learns the network parameters by maximizing the conditional likelihood of the outputs via backpropagation (e.g., Reed & de Freitas (2016); Kaiser & Sutskever (2016); Vinyals et al. (2015)). However, target outputs may not be available for novel tasks, for which no algorithm exists yet. A more desirable approach to inducing algorithms, followed in this paper, advocates using self-driven learning strategies that only receive reinforcement based on the outputs produced. Hence, just by having access to a verifier for an algorithmic problem, one can aim to learn an algorithm. For example, if one does not know how to sort an array, but can check the extent to which an array is sorted, then one can provide the reward signal necessary for learning sorting algorithms.\nWe formulate learning algorithms as an RL problem and make use of model-free policy gradient methods to optimize a set parameters associated with the algorithm. In this setting, the goal is to learn a policy \u03c0\u03b8 that given an observed state st at step t, estimates a distribution over the next action at, denoted \u03c0\u03b8(at | st). Actions represent the commands within the algorithm and states represent the joint state of the algorithm and the environment. Previous work in this area has focused on augmenting a neural network with additional structure and increased capabilities (Zaremba & Sutskever, 2015; Graves et al., 2016). In contrast, we utilize a simple architecture based on a standard recurrent neural network (RNN) with LSTM cells (Hochreiter & Schmidhuber, 1997) as depicted in Figure 1. At each episode, the environment is initialized with a latent state h, unknown to the agent, which determines s1 and the subsequent state transition and reward functions. Once the agent observes s1 as the input to the RNN, the network outputs a distribution \u03c0\u03b8(a1 | s1), from which an action a1 is sampled. This action is applied to the environment, and the agent receives a new state observation s2. The state s2 and the previous action a1 are then fed into the RNN and the process repeats until the end of the episode. Upon termination, a reward signal is received."}, {"heading": "3 LEARNING A POLICY BY MAXIMIZING EXPECTED REWARD", "text": "We start by discussing the most common form of policy gradient, REINFORCE (Williams, 1992), and its entropy regularized variant (Williams & Peng, 1991). REINFORCE has been applied to model-free policy-based learning with neural networks and algorithmic domains (Zaremba & Sutskever, 2015; Graves et al., 2016).\nAs mentioned above, we aim to learn a policy \u03c0\u03b8 that given an observed state st at step t, estimates a distribution over the next action at, denoted \u03c0\u03b8(at | st). The environment is initialized with a latent vector, h, which determines the initial observed state s1 = g(h), and the transition function st+1 = f(st,at | h). Given a latent state h, and s1:T \u2261 (s1, . . . , sT ), the model probability of an action sequence a1:T \u2261(a1, . . . ,aT ) is expressed as,\n\u03c0\u03b8(a1:T | h) = T\u220f t=1 \u03c0\u03b8(at | st) , where s1 = g(h), st+1 = f(st,at | h) for 1 \u2264 t < T .\nThe environment provides a reward at the end of the episode, denoted r(a1:T | h). For ease of readability we drop the subscript from a1:T and simply write \u03c0\u03b8(a | h) and r(a | h). The objective used to optimize the policy parameters, \u03b8, consists of maximizing expected reward under actions drawn from the policy, plus an optional maximum entropy regularizer. Given a distribution over initial latent environment states p(h), we express the regularized expected reward as,\nORL(\u03b8; \u03c4) = Eh\u223cp(h) {\u2211\na\u2208A \u03c0\u03b8(a | h)\n[ r(a | h)\u2212 \u03c4 log \u03c0\u03b8(a | h) ]} . (1)\nWhen \u03c0\u03b8 is a non-linear function defined by a neural network, finding the global optimum of \u03b8 is challenging, and one often resorts to gradient-based methods to find a local optimum of ORL(\u03b8; \u03c4). Given that dd\u03b8\u03c0\u03b8(a) = \u03c0\u03b8(a) d d\u03b8 log \u03c0\u03b8(a) for any a such that \u03c0\u03b8(a) > 0, one can verify that,\nd\nd\u03b8 ORL(\u03b8; \u03c4 | h) = \u2211 a\u2208A \u03c0\u03b8(a | h) d d\u03b8 log \u03c0\u03b8(a | h) [ r(a | h)\u2212 \u03c4 log \u03c0\u03b8(a | h)\u2212 \u03c4 ] . (2)\nBecause the space of possible actionsA is large, enumerating over all of the actions to compute this gradient is infeasible. Williams (1992) proposed to compute the stochastic gradient of the expected reward by using Monte Carlo samples. Using Monte Carlo samples, one first drawsN i.i.d. samples from the latent environment states {h(n)}Nn=1, and then draws K i.i.d. samples {a(k)}Kk=1 from \u03c0\u03b8(a | h(n)) to approximate the gradient of (1) by using (2) as,\nd\nd\u03b8 ORL(\u03b8; \u03c4) \u2248\n1\nNK N\u2211 n=1 K\u2211 k=1 d d\u03b8 log \u03c0\u03b8(a (k) | h(n)) [ r(a(k) | h(n))\u2212 \u03c4 log \u03c0\u03b8(a(k) | h(n))\u2212 \u03c4 ] .\n(3)\nThis reparametrization of the gradients is the key to the REINFORCE algorithm. To reduce the variance of (3), one uses rewards r\u0302 that are shifted by some offset values,\nr\u0302 (a(k) | h) = r(a(k) | h)\u2212 b(h) , (4) where b is known as a baseline or sometimes called a critic. Note that subtracting any offset from the rewards in (1) simply results in shifting the objective ORL by a constant. Unfortunately, directly maximizing expected reward (i.e., when \u03c4 = 0) is prone to getting trapped in a local optimum. To combat this tendency, Williams & Peng (1991) augmented the expected reward objective by including a maximum entropy regularizer (\u03c4 > 0) to promote greater exploration. We will refer to this variant of REINFORCE as MENT (maximum entropy exploration)."}, {"heading": "4 UNDER-APPRECIATED REWARD EXPLORATION (UREX)", "text": "To explain our novel form of policy gradient, we first note that the optimal policy \u03c0\u2217\u03c4 , which globally maximizes ORL(\u03b8; \u03c4 | h) in (1) for any \u03c4 > 0, can be expressed as,\n\u03c0\u2217\u03c4 (a | h) = 1\nZ(h) exp {1 \u03c4 r(a | h) } , (5)\nwhere Z(h) is a normalization constant making \u03c0\u2217\u03c4 a distribution over the space of action sequences A. One can verify this by first acknowledging that,\nORL(\u03b8; \u03c4 | h) = \u2212\u03c4 DKL (\u03c0\u03b8(\u00b7 | h) \u2016 \u03c0\u2217\u03c4 (\u00b7 | h)) . (6) Since DKL (p \u2016 q) is non-negative and zero iff p = q, then \u03c0\u2217\u03c4 defined in (5) maximizes ORL. That said, given a particular form of \u03c0\u03b8, finding \u03b8 that exactly characterizes \u03c0\u2217\u03c4 may not be feasible.\nThe KL divergence DKL (\u03c0\u03b8 \u2016 \u03c0\u2217\u03c4 ) is known to be mode seeking (Murphy, 2012, Section 21.2.2) even with entropy regularization (\u03c4 > 0). Learning a policy by optimizing this direction of the KL is prone to falling into a local optimum resulting in a sub-optimal policy that omits some of the modes of \u03c0\u2217\u03c4 . Although entropy regularization helps mitigate the issues as confirmed in our experiments, it is not an effective exploration strategy as it is undirected and requires a small regularization coefficient \u03c4 to avoid too much random exploration. Instead, we propose a directed exploration strategy that improves the mean seeking behavior of policy gradient in a principled way.\nWe start by considering the alternate mean seeking direction of the KL divergence, DKL (\u03c0\u2217\u03c4 \u2016 \u03c0\u03b8). Norouzi et al. (2016) considered this direction of the KL to directly learn a policy by optimizing\nORML(\u03b8; \u03c4) = Eh\u223cp(h) { \u03c4 \u2211 a\u2208A \u03c0\u2217\u03c4 (a | h) log \u03c0\u03b8(a | h) } , (7)\nfor structured prediction. This objective has the same optimal solution \u03c0\u2217\u03c4 as ORL since, ORML(\u03b8; \u03c4 | h) = \u2212\u03c4 DKL (\u03c0\u2217\u03c4 (\u00b7 | h) \u2016 \u03c0\u03b8(\u00b7 | h)) + const . (8)\nNorouzi et al. (2016) argue that in some structured prediction problems when one can draw samples from \u03c0\u2217\u03c4 , optimizing (7) is more effective than (1), since no sampling from a non-stationary policy \u03c0\u03b8 is required. If \u03c0\u03b8 is a log-linear model of a set of features, ORML is convex in \u03b8 whereas ORL is not, even in the log-linear case. Unfortunately, in scenarios that the reward landscape is unknown or computing the normalization constant Z(h) is intractable, sampling from \u03c0\u2217\u03c4 is not straightforward.\nIn the RL problems, the reward landscape is completely unknown, hence sampling from \u03c0\u2217\u03c4 is intractable. This paper proposes to approximate the expectation with respect to \u03c0\u2217\u03c4 in (7) by using self-normalized importance sampling (Owen, 2013), where the proposal distribution is \u03c0\u03b8 and the reference distribution is \u03c0\u2217\u03c4 . For importance sampling, one draws K i.i.d. samples {a(k)}Kk=1 from \u03c0\u03b8(a | h) and computes a set of normalized importance weights to approximateORML(\u03b8; \u03c4 | h) as,\nORML(\u03b8; \u03c4 | h) \u2248 \u03c4 K\u2211 k=1 w\u03c4 (a (k) | h)\u2211K m=1 w\u03c4 (a (m) | h) log \u03c0\u03b8(a (k) | h) , (9)\nwhere w\u03c4 (a(k) | h) \u221d \u03c0\u2217\u03c4/\u03c0\u03b8 denotes an importance weight defined by,\nw\u03c4 (a (k) | h) = exp {1 \u03c4 r(a(k) | h)\u2212 log \u03c0\u03b8(a(k) | h) } . (10)\nOne can view these importance weights as evaluating the discrepancy between scaled rewards r/\u03c4 and the policy\u2019s log-probabilities log \u03c0\u03b8. Among the K samples, a sample that is least appreciated by the model, i.e., has the largest r/\u03c4 \u2212 log \u03c0\u03b8, receives the largest positive feedback in (9). In practice, we have found that just using the importance sampling RML objective in (9) does not always yield promising solutions. Particularly, at the beginning of training, when \u03c0\u03b8 is still far away from \u03c0\u2217\u03c4 , the variance of importance weights is too large, and the self-normalized importance sampling procedure results in poor approximations. To stabilize early phases of training and ensure that the model distribution \u03c0\u03b8 achieves large expected reward scores, we combine the expected reward and RML objectives to benefit from the best of their mode and mean seeking behaviors. Accordingly, we propose the following objective that we call under-appreciated reward exploration (UREX),\nOUREX(\u03b8; \u03c4) = Eh\u223cp(h) {\u2211\na\u2208A\n[ \u03c0\u03b8(a | h) r(a | h) + \u03c4 \u03c0\u2217\u03c4 (a | h) log \u03c0\u03b8(a | h) ]} , (11)\nwhich is the sum of the expected reward and RML objectives. In our preliminary experiments, we considered a composite objective of ORL +ORML, but we found that removing the entropy term is beneficial. Hence, the OUREX objective does not include entropy regularization. Accordingly, the optimum policy for OUREX is no longer \u03c0\u2217\u03c4 , as it was for ORL and ORML. Appendix A derives the optimal policy for OUREX as a function of the optimal policy for ORL. We find that the optimal policy of UREX is more sharply concentrated on the high reward regions of the action space, which may be an advantage for UREX, but we leave more analysis of this behavior to future work.\nTo compute the gradient ofOUREX(\u03b8; \u03c4), we use the self-normalized importance sampling estimate outlined in (9). We assume that the importance weights are constant and contribute no gradient to d d\u03b8OUREX(\u03b8; \u03c4). To approximate the gradient, one draws N i.i.d. samples from the latent environment states {h(n)}Nn=1, and then draws K i.i.d. samples {a(k)}Kk=1 from \u03c0\u03b8(a |h (n)) to obtain\nd\nd\u03b8 OUREX(\u03b8; \u03c4) \u2248\n1\nN N\u2211 n=1 K\u2211 k=1 d d\u03b8 log \u03c0\u03b8(a (k) |h(n)) [ 1 K r\u0302 (a(k) | h(n))+\u03c4 w\u03c4 (a (k) |h(n))\u2211K m=1w\u03c4 (a (m) |h(n)) ] .\n(12) As with REINFORCE, the rewards are shifted by an offset b(h). In this gradient, the model logprobability of a sample action sequence a(k) is reinforced if the corresponding reward is large, or the corresponding importance weights are large, meaning that the action sequence is under-appreciated. The normalized importance weights are computed using a softmax operator softmax(r/\u03c4 \u2212 log \u03c0\u03b8)."}, {"heading": "5 RELATED WORK ON EXPLORATION IN REINFORCEMENT LEARNING", "text": "The most common exploration strategy considered in value-based RL is -greedy Q-learning, where at each step the agent either takes the best action according to its current value approximation or with probability takes an action sampled uniformly at random. Like entropy regularization, such an approach applies undirected exploration, but it has achieved recent success in game playing environments (Mnih et al., 2013; Van Hasselt et al., 2016; Mnih et al., 2016).\nProminent approaches to improving exploration beyond -greedy in value-based or model-based RL have focused on reducing uncertainty by prioritizing exploration toward states and actions where the agent knows the least. This basic intuition underlies work on counter and recency methods (Thrun, 1992), exploration methods based on uncertainty estimates of values (Kaelbling, 1993; Tokic, 2010), methods that prioritize learning environment dynamics (Kearns & Singh, 2002; Stadie et al., 2015), and methods that provide an intrinsic motivation or curiosity bonus for exploring unknown states (Schmidhuber, 2006; Bellemare et al., 2016). We relate the concepts of value and policy in RL and propose an exploration strategy based on the discrepancy between the two.\nIn contrast to value-based methods, exploration for policy-based RL methods is often a by-product of the optimization algorithm itself. Since algorithms like REINFORCE and Thompson sampling choose actions according to a stochastic policy, sub-optimal actions are chosen with some non-zero probability. The Q-learning algorithm may also be modified to sample an action from the softmax of the Q values rather than the argmax (Sutton & Barto, 1998).\nAsynchronous training has also been reported to have an exploration effect on both value- and policy-based methods. Mnih et al. (2016) report that asynchronous training can stabilize training\nby reducing the bias experienced by a single trainer. By using multiple separate trainers, an agent is less likely to become trapped at a policy found to be locally optimal only due to local conditions. In the same spirit, Osband et al. (2016) use multiple Q value approximators and sample only one to act for each episode as a way to implicitly incorporate exploration."}, {"heading": "6 SIX ALGORITHMIC TASKS", "text": "We assess the effectiveness of the proposed approach on five algorithmic tasks from the OpenAI Gym (Brockman et al., 2016), as well as a new binary search problem. Each task is summarized below with further details available on the Gym website1 or in their open-source code.2 In each case, the environment has a hidden tape and a hidden sequence. The agent observes the sequence via a pointer to a single character, which can be moved by a set of pointer control actions.\n1. Copy: The agent should emit a copy of the sequence. The pointer actions are move left and right. 2. DuplicatedInput: In the hidden tape, each character is repeated twice. The agent must dedupli-\ncate the sequence and emit every other character. The pointer actions are move left and right.\n3. RepeatCopy: The agent should emit the hidden sequence once, then emit the sequence in the reverse order, then emit the original sequence again. The pointer actions are move left and right.\n4. Reverse: The agent should emit the hidden sequence in the reverse order. As before, the pointer actions are move left and right.\n5. ReversedAddition: The hidden tape is a 2\u00d7n grid of digits representing two numbers in base 3 in little-endian order. The agent must emit the sum of the two numbers, in little-endian order. The allowed pointer actions are move left, right, up, or down.\nThe OpenAI Gym provides an additional harder task called ReversedAddition3, which involves adding three numbers. We omit this task, since none of the methods make much progress on it.\nFor these tasks, during training, input sequences range from a length of 2 characters to 33. A reward of 1 is given for each correct emission. On an incorrect emission, a small penalty of\u22120.5 is incurred and the episode is terminated. The agent is also terminated and penalized with a reward of \u22121 if the episode exceeds a certain number of steps. Each of the Gym tasks has a success threshold, which determines the required average reward over 100 episodes for the agent to be considered successful.\nWe also conduct experiments on an additional algorithmic task described below: 6. BinarySearch: Given an integer n, the environment has a hidden array of n distinct numbers\nstored in ascending order. The environment also has a query number x unknown to the agent that is contained somewhere in the array. The goal of the agent is to find the query number in the array in a small number of actions. The environment has three integer registers initialized at (n, 0, 0). At each step, the agent can interact with the environment via the four following actions: \u2022 INC(i): increment the value of the register i for i \u2208 {1, 2, 3}. \u2022 DIV(i): divide the value of the register i by 2 for i \u2208 {1, 2, 3}. \u2022 AVG(i): replace the value of the register i with the average of the two other registers. \u2022 CMP(i): compare the value of the register i with x and receive a signal indicating which\nvalue is greater. The agent succeeds when it calls CMP on an array cell holding the value x. The agent is terminated when the number of steps exceeds a maximum threshold of 2n+1 steps and recieves a reward of 0. If the agent finds x at step t, it recieves a reward of 10(1\u2212t/(2n+1)).\nWe set the maximum number of steps to 2n+1 to allow the agent to perform a full linear search. A policy performing full linear search achieves an average reward of 5, because x is chosen uniformly at random from the elements of the array. A policy employing binary search can find the number x in at most 2 log2 n + 1 steps. If n is selected uniformly at random from the range 32 \u2264 n \u2264 512, binary search yields an optimal average reward above 9.55. We set the success threshold for this task to an average reward of 9.\n1gym.openai.com 2github.com/openai/gym"}, {"heading": "7 EXPERIMENTS", "text": "We compare our policy gradient method using under-appreciated reward exploration (UREX) against two main RL baselines: (1) REINFORCE with entropy regularization termed MENT (Williams & Peng, 1991), where the value of \u03c4 determines the degree of regularization. When \u03c4 = 0, standard REINFORCE is obtained. (2) one-step double Q-learning based on bootstrapping one step future rewards."}, {"heading": "7.1 ROBUSTNESS TO HYPER-PARAMETERS", "text": "Hyper-parameter tuning is often tedious for RL algorithms. We found that the proposed UREX method significantly improves robustness to changes in hyper-parameters when compared to MENT. For our experiments, we perform a careful grid search over a set of hyper-parameters for both MENT and UREX. For any hyper-parameter setting, we run the MENT and UREX methods 5 times with different random restarts. We explore the following main hyper-parameters:\n\u2022 The learning rate denoted \u03b7 chosen from a set of 3 possible values \u03b7 \u2208 {0.1, 0.01, 0.001}. \u2022 The maximum L2 norm of the gradients, beyond which the gradients are clipped. This parame-\nter, denoted c, matters for training RNNs. The value of c is selected from c \u2208 {1, 10, 40, 100}. \u2022 The temperature parameter \u03c4 that controls the degree of exploration for both MENT and UREX.\nFor MENT, we use \u03c4 \u2208 {0, 0.005, 0.01, 0.1}. For UREX, we only consider \u03c4 = 0.1, which consistently performs well across the tasks.\nIn all of the experiments, both MENT and UREX are treated exactly the same. In fact, the change of implementation is just a few lines of code. Given a value of \u03c4 , for each task, we run 60 training jobs comprising 3 learning rates, 4 clipping values, and 5 random restarts. We run each algorithm for a maximum number of steps determined based on the difficulty of the task. The training jobs for Copy, DuplicatedInput, RepeatCopy, Reverse, ReversedAddition, and BinarySearch are run for 2K, 500, 50K, 5K, 50K, and 2K stochastic gradient steps, respectively. We find that running a trainer job longer does not result in a better performance. Our policy network comprises a single LSTM layer with 128 nodes. We use the Adam optimizer (Kingma & Ba, 2015) for the experiments.\nTable 1 shows the percentage of 60 trials on different hyper-parameters (\u03b7, c) and random restarts which successfully solve each of the algorithmic tasks. It is clear that UREX is more robust than MENT to changes in hyper-parameters, even though we only report the results of UREX for a single temperature. See Appendix B for more detailed tables on hyper-parameter robustness."}, {"heading": "7.2 RESULTS", "text": "Table 2 presents the number of successful attempts (out of 5 random restarts) and the expected reward values (averaged over 5 trials) for each RL algorithm given the best hyper-parameters. Onestep Q-learning results are also included in the table. It is clear that UREX outperforms the baselines on these tasks. On the more difficult tasks, such as Reverse and ReverseAddition, UREX is able to consistently find an appropriate algorithm, but MENT and Q-learning fall behind. Importantly, for the BinarySearch task, which exhibits many local maxima and necessitates smart exploration, UREX is the only method that can solve it consistently. The Q-learning baseline solves some of the simple\ntasks, but it makes little headway on the harder tasks. We believe that entropy regularization for policy gradient and -greedy for Q-learning are relatively weak exploration strategies in long episodic tasks with delayed rewards. On such tasks, one random exploratory step in the wrong direction can take the agent off the optimal policy, hampering its ability to learn. In contrast, UREX provides a form of adaptive and smart exploration. In fact, we observe that the variance of the importance weights decreases as the agent approaches the optimal policy, effectively reducing exploration when it is no longer necessary; see Appendix E."}, {"heading": "7.3 GENERALIZATION TO LONGER SEQUENCES", "text": "To confirm whether our method is able to find the correct algorithm for multi-digit addition, we investigate its generalization to longer input sequences than provided during training. We evaluate the trained models on inputs up to a length of 2000 digits, even though training sequences were at most 33 characters. For each length, we test the model on 100 randomly generated inputs, stopping when the accuracy falls below 100%. Out of the 60 models trained on addition with UREX, we find that 5 models generalize to numbers up to 2000 digits without any observed mistakes. On the best UREX hyper-parameters, 2 out of the 5 random restarts are able to generalize successfully. For more detailed results on the generalization performance on 3 different tasks including Copy, DuplicatedInput, and ReversedAddition, see Appendix C. During these evaluations, we take the action with largest probability from \u03c0\u03b8(a | h) at each time step rather than sampling randomly. We also looked into the generalization of the models trained on the BinarySearch task. We found that none of the agents perform proper binary search. Rather, those that solved the task perform a hybrid of binary and linear search: first actions follow a binary search pattern, but then the agent switches to a linear search procedure once it narrows down the search space; see Appendix D for some execution traces for BinarySearch and ReversedAddition. Thus, on longer input sequences, the agent\u2019s running time complexity approaches linear rather than logarithmic. We hope that future work will make more progress on this task. This task is especially interesting because the reward signal should incorporate both correctness and efficiency of the algorithm."}, {"heading": "7.4 IMPLEMENTATION DETAILS", "text": "In all of the experiments, we make use of curriculum learning. The environment begins by only providing small inputs and moves on to longer sequences once the agent achieves close to maximal reward over a number of steps. For policy gradient methods including MENT and UREX, we only provide the agent with a reward at the end of the episode, and there is no notion of intermediate reward. For the value-based baseline, we implement one-step Q-learning as described in Mnih et al. (2016)-Alg. 1, employing double Q-learning with -greedy exploration. We use the same RNN in our policy-based approaches to estimate the Q values. A grid search over exploration rate, exploration rate decay, learning rate, and sync frequency (between online and target network) is conducted to find the best hyper-parameters. Unlike our other methods, the Q-learning baseline\nuses intermediate rewards, as given by the OpenAI Gym on a per-step basis. Hence, the Q-learning baseline has a slight advantage over the policy gradient methods.\nIn all of the tasks except Copy, our stochastic optimizer uses mini-batches comprising 400 policy samples from the model. These 400 samples correspond to 40 different random sequences drawn from the environment, and 10 random policy trajectories per sequence. In other words, we set K = 10 and N = 40 as defined in (3) and (12). For MENT, we use the 10 samples to subtract the mean of the coefficient of dd\u03b8 log \u03c0\u03b8(a | h) which includes the contribution of the reward and entropy regularization. For UREX, we use the 10 trajectories to subtract the mean reward and normalize the importance sampling weights. We do not subtract the mean of the normalized importance weights. For the Copy task, we use mini-batches with 200 samples using K = 10 and N = 20. Experiments are conducted using Tensorflow (Abadi et al., 2016)."}, {"heading": "8 CONCLUSION", "text": "We present a variant of policy gradient, called UREX, which promotes exploring action sequences that yield rewards larger than what the model expects. This exploration strategy is the result of importance sampling from the optimal policy. Our experimental results demonstrate that UREX significantly outperforms other value and policy based methods, while being more robust to changes of hyper-parameters. By using UREX, we can solve algorithmic tasks like multi-digit addition, which other methods cannot reliably solve even given the best hyper-parameters. We introduce a new algorithmic task based on binary search to advocate more research in this area, especially when the computational complexity of the solutions matters too. Solving these tasks is not only important to develop human like intelligence to enable learning algorithms, but also important for generic reinforcement learning, where smart and efficient exploration is the key to successful methods."}, {"heading": "9 ACKNOWLEDGMENT", "text": "We thank Irwan Bello, Corey Lynch, George Tucker, Volodymyr Mnih, and the Google Brain team for insightful comments and discussions."}, {"heading": "A OPTIMAL POLICY FOR THE UREX OBJECTIVE", "text": "To derive the form of the optimal policy for the UREX objective (11), note that for each h one would like to maximize \u2211\na\u2208A\n[ \u03c0\u03b8(a) r(a) + \u03c4 \u03c0 \u2217 \u03c4 (a) log \u03c0\u03b8(a) ] , (13)\nsubject to the constraint \u2211\na\u2208A \u03c0\u03b8(a) = 1. To enforce the constraint, we introduce a Lagrange multiplier \u03b1 and aim to maximize\u2211\na\u2208A\n[ \u03c0\u03b8(a) r(a) + \u03c4 \u03c0 \u2217 \u03c4 (a) log \u03c0\u03b8(a)\u2212 \u03b1\u03c0\u03b8(a) ] + \u03b1 . (14)\nSince the gradient of the Lagrangian (14) with respect to \u03b8 is given by\u2211 a\u2208A d\u03c0\u03b8(a) d\u03b8 [ r(a) + \u03c4 \u03c0\u2217\u03c4 (a) \u03c0\u03b8(a) \u2212 \u03b1 ] , (15)\nthe optimal choice for \u03c0\u03b8 is achieved by setting\n\u03c0\u03b8(a) = \u03c4 \u03c0\u2217\u03c4 (a)\n\u03b1\u2212 r(a) for all a \u2208 A , (16)\nforcing the gradient to be zero. The Lagrange multiplier \u03b1 can then be chosen so that \u2211\na\u2208A \u03c0\u03b8(a) = 1 while also satisfying \u03b1 > maxa\u2208A r(a); see e.g. (Golub, 1987)."}, {"heading": "B ROBUSTNESS TO HYPER-PARAMETERS", "text": "Tables 3\u20138 provide more details on different cells of Table 1. Each table presents the results of MENT using the best temperature \u03c4 vs. UREX with \u03c4 = 0.1 on a variety of learning rates and clipping values. Each cell is the number of trials out of 5 random restarts that succeed at solving the task using a specific \u03b7 and c."}, {"heading": "C GENERALIZATION TO LONGER SEQUENCES", "text": "Table 9 provides a more detailed look into the generalization performance of the trained models on Copy, DuplicatedInput, and ReversedAddition. The tables show how the number of models which can solve the task correctly drops off as the length of the input increases."}, {"heading": "D EXAMPLE EXECUTION TRACES", "text": "We provide the traces of two trained agents on the ReversedAddition task (Figure 2) and the BinarySearch task (Table 10).\nE VARIANCE OF IMPORTANCE WEIGHTS"}], "references": [{"title": "Tensorflow: A system for largescale machine learning", "author": ["Mart\u0131\u0301n Abadi", "Paul Barham", "Jianmin Chen", "Zhifeng Chen", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Geoffrey Irving", "Michael Isard"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Learning regular sets form queries and counterexamples", "author": ["Dana Angulin"], "venue": "Information and Computation,", "citeRegEx": "Angulin.,? \\Q1987\\E", "shortCiteRegEx": "Angulin.", "year": 1987}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["Marc G. Bellemare", "Sriram Srinivasan", "Georg Ostrovski", "Tom Schaul", "David Saxton", "R\u00e9mi Munos"], "venue": null, "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Some modified matrix eigenvalue problems", "author": ["Gene Golub"], "venue": "SIAM Review,", "citeRegEx": "Golub.,? \\Q1987\\E", "shortCiteRegEx": "Golub.", "year": 1987}, {"title": "Hybrid computing using a neural network with dynamic external memory. Nature, 2016", "author": ["Alex Graves", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka GrabskaBarwinska", "Sergio G. Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou", "Adria P. Badia", "Karl M. Hermann", "Yori Zwols", "Georg Ostrovski", "Adam Cain", "Helen King", "Christopher Summerfield", "Phil Blunsom", "Koray Kavukcuoglu", "Demis Hassabis"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Learning in embedded systems", "author": ["Leslie Pack Kaelbling"], "venue": "MIT press,", "citeRegEx": "Kaelbling.,? \\Q1993\\E", "shortCiteRegEx": "Kaelbling.", "year": 1993}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Machine Learning,", "citeRegEx": "Kearns and Singh.,? \\Q2002\\E", "shortCiteRegEx": "Kearns and Singh.", "year": 2002}, {"title": "Learning and using relational theories", "author": ["Charles Kemp", "Noah Goodman", "Joshua Tenebaum"], "venue": null, "citeRegEx": "Kemp et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kemp et al\\.", "year": 2007}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Inductive Logic Programming: Theory and Methods", "author": ["N. Lavrac", "S. Dzeroski"], "venue": null, "citeRegEx": "Lavrac and Dzeroski.,? \\Q1994\\E", "shortCiteRegEx": "Lavrac and Dzeroski.", "year": 1994}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin A. Riedmiller"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Machine Learning: A Probabilistic Perspective", "author": ["Kevin P. Murphy"], "venue": null, "citeRegEx": "Murphy.,? \\Q2012\\E", "shortCiteRegEx": "Murphy.", "year": 2012}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever"], "venue": null, "citeRegEx": "Neelakantan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "Reward augmented maximum likelihood for neural structured prediction", "author": ["Mohammad Norouzi", "Samy Bengio", "Zhifeng Chen", "Navdeep Jaitly", "Mike Schuster", "Yonghui Wu", "Dale Schuurmans"], "venue": null, "citeRegEx": "Norouzi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2016}, {"title": "Deep exploration via bootstrapped DQN", "author": ["Ian Osband", "Charles Blundell", "Alexander Pritzel", "Benjamin Van Roy"], "venue": null, "citeRegEx": "Osband et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2016}, {"title": "Monte Carlo theory, methods and examples", "author": ["Art B. Owen"], "venue": null, "citeRegEx": "Owen.,? \\Q2013\\E", "shortCiteRegEx": "Owen.", "year": 2013}, {"title": "Artificial intelligence: a modern approach, volume 2. Prentice hall", "author": ["Stuart Jonathan Russell", "Peter Norvig", "John F Canny", "Jitendra M Malik", "Douglas D Edwards"], "venue": "Upper Saddle River,", "citeRegEx": "Russell et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Russell et al\\.", "year": 2003}, {"title": "Optimal artificial curiosity, creativity, music, and the fine arts", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Connection Science,", "citeRegEx": "Schmidhuber.,? \\Q2006\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2006}, {"title": "Mastering the game of Go with deep neural networks and tree search", "author": ["David Silver", "Aja Huang"], "venue": null, "citeRegEx": "Silver and Huang,? \\Q2016\\E", "shortCiteRegEx": "Silver and Huang", "year": 2016}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["Bradly C. Stadie", "Sergey Levine", "Pieter Abbeel"], "venue": null, "citeRegEx": "Stadie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Introduction to Reinforcement Learning", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Efficient exploration in reinforcement learning", "author": ["Sebastian B Thrun"], "venue": "Technical report,", "citeRegEx": "Thrun.,? \\Q1992\\E", "shortCiteRegEx": "Thrun.", "year": 1992}, {"title": "Adaptive \u03b5-greedy exploration in reinforcement learning based on value differences", "author": ["Michel Tokic"], "venue": null, "citeRegEx": "Tokic.,? \\Q2010\\E", "shortCiteRegEx": "Tokic.", "year": 2010}, {"title": "Deep reinforcement learning with double qlearning", "author": ["Hado Van Hasselt", "Arthur Guez", "David Silver"], "venue": null, "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Function optimization using connectionist reinforcement learning algorithms", "author": ["Ronald J Williams", "Jing Peng"], "venue": "Connection Science,", "citeRegEx": "Williams and Peng.,? \\Q1991\\E", "shortCiteRegEx": "Williams and Peng.", "year": 1991}, {"title": "Reinforcement learning neural turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": null, "citeRegEx": "Zaremba and Sutskever.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "Although symbolic reasoning has a long history in AI (Russell et al., 2003), only recently have statistical machine learning and neural network approaches begun to make headway in automated algorithm discovery (Reed & de Freitas, 2016; Kaiser & Sutskever, 2016; Neelakantan et al.", "startOffset": 53, "endOffset": 75}, {"referenceID": 16, "context": ", 2003), only recently have statistical machine learning and neural network approaches begun to make headway in automated algorithm discovery (Reed & de Freitas, 2016; Kaiser & Sutskever, 2016; Neelakantan et al., 2016) heading to cross an important milestone on the path to AI.", "startOffset": 142, "endOffset": 219}, {"referenceID": 24, "context": "Nevertheless, most of the recent successes depend on the use of strong supervision to learn a mapping from a set of training inputs to outputs by maximizing a conditional log-likelihood, very much like neural machine translation systems (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 237, "endOffset": 284}, {"referenceID": 2, "context": "Nevertheless, most of the recent successes depend on the use of strong supervision to learn a mapping from a set of training inputs to outputs by maximizing a conditional log-likelihood, very much like neural machine translation systems (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 237, "endOffset": 284}, {"referenceID": 13, "context": "Despite the recent excitement around the use of RL to tackle Atari games (Mnih et al., 2015) and Go (Silver et al.", "startOffset": 73, "endOffset": 92}, {"referenceID": 26, "context": "We believe one of the key limitations of the current RL methods, preventing them from making much progress in the sparse reward settings, is the use of undirected exploration strategies (Thrun, 1992), such as -greedy and entropy regularization (Williams & Peng, 1991).", "startOffset": 186, "endOffset": 199}, {"referenceID": 9, "context": "Although research on using neural networks to learn algorithms has had a surge of recent interest, the problem of program induction from examples has a long history in many fields, including program induction, inductive logic programming (Lavrac & Dzeroski, 1994), relational learning (Kemp et al., 2007) and regular language learning (Angulin, 1987).", "startOffset": 285, "endOffset": 304}, {"referenceID": 1, "context": ", 2007) and regular language learning (Angulin, 1987).", "startOffset": 38, "endOffset": 53}, {"referenceID": 5, "context": "Previous work in this area has focused on augmenting a neural network with additional structure and increased capabilities (Zaremba & Sutskever, 2015; Graves et al., 2016).", "startOffset": 123, "endOffset": 171}, {"referenceID": 1, "context": ", 2007) and regular language learning (Angulin, 1987). Rather than presenting a comprehensive survey of program induction here, we focus on neural network approaches to algorithmic tasks and highlight the relative simplicity of our neural network architecture. Most successful applications of neural networks to algorithmic tasks rely on strong supervision, where the inputs and target outputs are completely known a priori. Given a dataset of examples, one learns the network parameters by maximizing the conditional likelihood of the outputs via backpropagation (e.g., Reed & de Freitas (2016); Kaiser & Sutskever (2016); Vinyals et al.", "startOffset": 39, "endOffset": 596}, {"referenceID": 1, "context": ", 2007) and regular language learning (Angulin, 1987). Rather than presenting a comprehensive survey of program induction here, we focus on neural network approaches to algorithmic tasks and highlight the relative simplicity of our neural network architecture. Most successful applications of neural networks to algorithmic tasks rely on strong supervision, where the inputs and target outputs are completely known a priori. Given a dataset of examples, one learns the network parameters by maximizing the conditional likelihood of the outputs via backpropagation (e.g., Reed & de Freitas (2016); Kaiser & Sutskever (2016); Vinyals et al.", "startOffset": 39, "endOffset": 623}, {"referenceID": 1, "context": ", 2007) and regular language learning (Angulin, 1987). Rather than presenting a comprehensive survey of program induction here, we focus on neural network approaches to algorithmic tasks and highlight the relative simplicity of our neural network architecture. Most successful applications of neural networks to algorithmic tasks rely on strong supervision, where the inputs and target outputs are completely known a priori. Given a dataset of examples, one learns the network parameters by maximizing the conditional likelihood of the outputs via backpropagation (e.g., Reed & de Freitas (2016); Kaiser & Sutskever (2016); Vinyals et al. (2015)).", "startOffset": 39, "endOffset": 646}, {"referenceID": 29, "context": "We start by discussing the most common form of policy gradient, REINFORCE (Williams, 1992), and its entropy regularized variant (Williams & Peng, 1991).", "startOffset": 74, "endOffset": 90}, {"referenceID": 5, "context": "REINFORCE has been applied to model-free policy-based learning with neural networks and algorithmic domains (Zaremba & Sutskever, 2015; Graves et al., 2016).", "startOffset": 108, "endOffset": 156}, {"referenceID": 29, "context": "Williams (1992) proposed to compute the stochastic gradient of the expected reward by using Monte Carlo samples.", "startOffset": 0, "endOffset": 16}, {"referenceID": 29, "context": "To combat this tendency, Williams & Peng (1991) augmented the expected reward objective by including a maximum entropy regularizer (\u03c4 > 0) to promote greater exploration.", "startOffset": 25, "endOffset": 48}, {"referenceID": 15, "context": "The KL divergence DKL (\u03c0\u03b8 \u2016 \u03c0\u2217 \u03c4 ) is known to be mode seeking (Murphy, 2012, Section 21.2.2) even with entropy regularization (\u03c4 > 0). Learning a policy by optimizing this direction of the KL is prone to falling into a local optimum resulting in a sub-optimal policy that omits some of the modes of \u03c0\u2217 \u03c4 . Although entropy regularization helps mitigate the issues as confirmed in our experiments, it is not an effective exploration strategy as it is undirected and requires a small regularization coefficient \u03c4 to avoid too much random exploration. Instead, we propose a directed exploration strategy that improves the mean seeking behavior of policy gradient in a principled way. We start by considering the alternate mean seeking direction of the KL divergence, DKL (\u03c0\u2217 \u03c4 \u2016 \u03c0\u03b8). Norouzi et al. (2016) considered this direction of the KL to directly learn a policy by optimizing", "startOffset": 64, "endOffset": 804}, {"referenceID": 19, "context": "This paper proposes to approximate the expectation with respect to \u03c0\u2217 \u03c4 in (7) by using self-normalized importance sampling (Owen, 2013), where the proposal distribution is \u03c0\u03b8 and the reference distribution is \u03c0\u2217 \u03c4 .", "startOffset": 124, "endOffset": 136}, {"referenceID": 17, "context": "(8) Norouzi et al. (2016) argue that in some structured prediction problems when one can draw samples from \u03c0\u2217 \u03c4 , optimizing (7) is more effective than (1), since no sampling from a non-stationary policy \u03c0\u03b8 is required.", "startOffset": 4, "endOffset": 26}, {"referenceID": 12, "context": "Like entropy regularization, such an approach applies undirected exploration, but it has achieved recent success in game playing environments (Mnih et al., 2013; Van Hasselt et al., 2016; Mnih et al., 2016).", "startOffset": 142, "endOffset": 206}, {"referenceID": 14, "context": "Like entropy regularization, such an approach applies undirected exploration, but it has achieved recent success in game playing environments (Mnih et al., 2013; Van Hasselt et al., 2016; Mnih et al., 2016).", "startOffset": 142, "endOffset": 206}, {"referenceID": 26, "context": "This basic intuition underlies work on counter and recency methods (Thrun, 1992), exploration methods based on uncertainty estimates of values (Kaelbling, 1993; Tokic, 2010), methods that prioritize learning environment dynamics (Kearns & Singh, 2002; Stadie et al.", "startOffset": 67, "endOffset": 80}, {"referenceID": 7, "context": "This basic intuition underlies work on counter and recency methods (Thrun, 1992), exploration methods based on uncertainty estimates of values (Kaelbling, 1993; Tokic, 2010), methods that prioritize learning environment dynamics (Kearns & Singh, 2002; Stadie et al.", "startOffset": 143, "endOffset": 173}, {"referenceID": 27, "context": "This basic intuition underlies work on counter and recency methods (Thrun, 1992), exploration methods based on uncertainty estimates of values (Kaelbling, 1993; Tokic, 2010), methods that prioritize learning environment dynamics (Kearns & Singh, 2002; Stadie et al.", "startOffset": 143, "endOffset": 173}, {"referenceID": 23, "context": "This basic intuition underlies work on counter and recency methods (Thrun, 1992), exploration methods based on uncertainty estimates of values (Kaelbling, 1993; Tokic, 2010), methods that prioritize learning environment dynamics (Kearns & Singh, 2002; Stadie et al., 2015), and methods that provide an intrinsic motivation or curiosity bonus for exploring unknown states (Schmidhuber, 2006; Bellemare et al.", "startOffset": 229, "endOffset": 272}, {"referenceID": 21, "context": ", 2015), and methods that provide an intrinsic motivation or curiosity bonus for exploring unknown states (Schmidhuber, 2006; Bellemare et al., 2016).", "startOffset": 106, "endOffset": 149}, {"referenceID": 3, "context": ", 2015), and methods that provide an intrinsic motivation or curiosity bonus for exploring unknown states (Schmidhuber, 2006; Bellemare et al., 2016).", "startOffset": 106, "endOffset": 149}, {"referenceID": 3, "context": ", 2015), and methods that provide an intrinsic motivation or curiosity bonus for exploring unknown states (Schmidhuber, 2006; Bellemare et al., 2016). We relate the concepts of value and policy in RL and propose an exploration strategy based on the discrepancy between the two. In contrast to value-based methods, exploration for policy-based RL methods is often a by-product of the optimization algorithm itself. Since algorithms like REINFORCE and Thompson sampling choose actions according to a stochastic policy, sub-optimal actions are chosen with some non-zero probability. The Q-learning algorithm may also be modified to sample an action from the softmax of the Q values rather than the argmax (Sutton & Barto, 1998). Asynchronous training has also been reported to have an exploration effect on both value- and policy-based methods. Mnih et al. (2016) report that asynchronous training can stabilize training", "startOffset": 126, "endOffset": 861}, {"referenceID": 18, "context": "In the same spirit, Osband et al. (2016) use multiple Q value approximators and sample only one to act for each episode as a way to implicitly incorporate exploration.", "startOffset": 20, "endOffset": 41}, {"referenceID": 12, "context": "For the value-based baseline, we implement one-step Q-learning as described in Mnih et al. (2016)-Alg.", "startOffset": 79, "endOffset": 98}, {"referenceID": 0, "context": "Experiments are conducted using Tensorflow (Abadi et al., 2016).", "startOffset": 43, "endOffset": 63}], "year": 2017, "abstractText": "This paper presents a novel form of policy gradient for model-free reinforcement learning (RL) with improved exploration properties. Current policy-based methods use entropy regularization to encourage undirected exploration of the reward landscape, which is ineffective in high dimensional spaces with sparse rewards. We propose a more directed exploration strategy that promotes exploration of under-appreciated reward regions. An action sequence is considered under-appreciated if its log-probability under the current policy under-estimates its resulting reward. The proposed exploration strategy is easy to implement, requiring small modifications to an implementation of the REINFORCE algorithm. We evaluate the approach on a set of algorithmic tasks that have long challenged RL methods. Our approach reduces hyper-parameter sensitivity and demonstrates significant improvements over baseline methods. Our algorithm successfully solves a benchmark multi-digit addition task and generalizes to long sequences. This is, to our knowledge, the first time that a pure RL method has solved addition using only reward feedback.", "creator": "LaTeX with hyperref package"}}}