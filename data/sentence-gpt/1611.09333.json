{"id": "1611.09333", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Dictionary Learning with Equiprobable Matching Pursuit", "abstract": "Sparse signal representations based on linear combinations of learned atoms have been used to obtain state-of-the-art results in several practical signal processing applications. Approximation methods are needed to process high-dimensional signals in this way because the problem to calculate optimal atoms for sparse coding is NP-hard. Here we study greedy algorithms for unsupervised learning of dictionaries of shift-invariant atoms and propose a new method where each atom is selected with the same probability on average, which corresponds to the homeostatic regulation of a recurrent convolutional neural network in order to approximate a signal processing algorithm for each individual state. We use high-dimensional, hyperparameterized representations of individual states. We analyze the order and magnitude of an individual state at various points in the process of selection. We analyze the number of times when there is a sufficiently large ensemble of atoms in the cluster with all the available states. We find a novel approach with a highly efficient convolutional neural network. We estimate that the total number of individual states is approximately 10.6 million. We calculate that the average state of a convolutional neural network was obtained in all of these states as of 1 h (Fig. 5). We also compute the average frequency of a convolutional neural network (Fig. 6), the rate of the recurrent convolutional neural network (Fig. 7). We calculate that the total number of states is approximately 2.8 million. As this approach is implemented, we find that the number of states is approximately 3.3 million. In practice the number of states in a convolutional neural network is a lot smaller than that of a single single state. The high-dimensional and hyperparameterized states of a convolutional neural network can be found in a few methods. As previously mentioned, this method is used in many more practical applications, such as signal processing, which may be applied in a number of applications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 28 Nov 2016 20:38:52 GMT  (164kb,D)", "http://arxiv.org/abs/1611.09333v1", "8 pages, 8 figures"]], "COMMENTS": "8 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["fredrik sandin", "sergio martin-del-campo"], "accepted": false, "id": "1611.09333"}, "pdf": {"name": "1611.09333.pdf", "metadata": {"source": "CRF", "title": "Dictionary Learning with Equiprobable Matching Pursuit", "authors": ["Fredrik Sandin", "Sergio Martin-del-Campo"], "emails": ["fredrik.sandin@ltu.se"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nSensors like cameras, microphones and accelerometers typically generate redundant information because the resulting data have an underlying structure. That is why most observed phenomena can be accurately approximated with mathematical relationships in the form of physical laws, and also why such data can be compressed using algorithms that identifies and approximates patterns. Redundancies in the data make the processes of communicating, analyzing and storing information inefficient, thus constraining the domain of feasible applications. The problem to identify the structure of signals and thereby derive succinct representations that are both compact and informative is a computational challenge, which limits the efficacy of sensor systems.\nSparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12]. In a sparse model, the signal is typically described as a linear combination of elementary functions called atoms, which can either be predefined (if an appropriate generative model for the signal class is known) or learned from a training\nsignal. The goal is to select or learn the atoms so that the model residual is minimum for a certain sparsity of the representation, or to maximize the sparsity for a certain tolerance on the model residual. Atoms typically have unit norm and the set of atoms defines a dictionary, which can be subject to further constraints like temporal or spatial translation invariance.\nFourier and wavelet analysis are two examples where predefined dictionaries are used. Models based on predefined dictionaries have enabled derivation of closed-form mathematical results and fast algorithms that are widely used, but such approaches are simplistic compared to the complexity of the underlying natural phenomena. The dictionary learning approach is based on the hypothesis that complex signals can be more accurately modeled by extracting environmentally matched atoms from the signal. However, the dictionary learning problem is NP-hard and it is also hard to find approximate solutions near the optimal sparsity level [13].\nThe development of dictionary learning methods was stimulated by results presented in the mid \u201990s by Olshausen and Field [14], [15], which demonstrate that atoms similar to the receptive fields of cells in visual cortex can be learned from natural images by imposing a few general optimization conditions, including sparsity and statistical independence of atoms. This demonstrates that some aspects of the low-level functions in the visual system can be explained by a few general computational principles, and that elementary structures of such complex signals can be automatically uncovered from examples. Since that time several probabilistic dictionary learning and sparse coding methods have been developed [2], [5], aiming for a dictionary that either maximizes the likelihood of the data, as for example in [16], or the posterior probability of the dictionary, as in [17]. Recent developments include extensions of dictionary learning methods to distributed systems [18] and low-power hardware [19].\nThere is a knowledge gap between receptive field models and the observed function of neural networks in biological sensory systems. For instance, neurons demonstrate a form of homeostatic adaptation of the functional properties of the network to the ongoing changes in the statistical structure of the sensory input [20], [21], [22], which may be related to optimal encoding [23] of the input by dynamic adaptation of the receptive fields. Furthermore, there is a notion that homeostatic mechanisms serve to maintain the dynamics of cortical networks at a critical point [24], [25] where the dynamic\nar X\niv :1\n61 1.\n09 33\n3v 1\n[ cs\n.L G\n] 2\n8 N\nov 2\n01 6\nrange and information processing capacity are optimal [26], [27]. Can we learn something new about dictionary learning by incorporating and studying such homeostatic regulation mechanisms?\nIn this paper we focus on greedy algorithms for dictionary learning, where the approximation error is reduced iteratively by sparse decomposition of the signal followed by gradient optimization of the atoms in the dictionary. Our aim is to study a basic implementation of homeostatic regulation where all atoms are enforced to occur with the same probability on average, which in terms of neurons and receptive fields mean that neurons fire with the same average probability.\nThe starting point is the dictionary learning method introduced by Smith and Lewicki [28], where the sparse code is generated with Matching Pursuit (MP) [29], [30] and the shiftinvariant dictionary is updated with probabilistic gradient ascent on the likelihood of the data [16]. In principle this model corresponds to a particular type of recurrent convolutional neural network with max pooling (cf. Fig. 5 in [15]), which we extend here with a homeostatic regularization mechanism. Typically a subset of the atoms are selected in the MP for one particular signal, while some atoms are rarely selected and do not take significant part in the gradient-based dictionary update process. We implement homeostatic regulation by enforcing equiprobable atom selection in the MP, which implies that each atom is equally likely to occur in the linear combination on average, see Figure 1. In addition to MP we consider dictionary learning with Orthogonal MP (OMP) [30], [31], which in the case of shift-invariant dictionaries is applicable to high-dimensional signals in the form of local OMP [32].\nWe study the effects of equiprobable selection on the learned dictionary and sparse representation of music and birdsong. With equiprobable selection all atoms adapt to the training signal, see Figure 2. Our main finding is that the entropy and reconstruction accuracy are higher for equiprobable sparse representations, and that the computational cost is reduced by the equiprobability constraint. Equiprobable selection can be generalized to other greedy dictionary learning methods and these results motivate further investigations of dictionary learning models incorporating more realistic homeostatic regulation mechanisms observed by neuroscientists."}, {"heading": "II. DICTIONARY LEARNING METHOD", "text": "The signal, x(t), is modelled as a linear superposition of waveforms, \u03d5i, with compact support and additive noise\nx(t) = M\u2211 i=1 Ni\u2211 j=1 ai,j\u03d5i(t\u2212 \u03c4i,j) + (t). (1)\nThe functions \u03d5i are shift-invariant atoms that represent elementary waveforms of the signal and M is the number of different atoms in the model. The variable (t) represents the model residual, including noise. The variable Ni refers to the number of instances of atom \u03d5i, and the temporal position and amplitude of the j-th instance of atom \u03d5i are denoted by \u03c4i,j and ai,j , respectively. The set of M atoms defines a dictionary\n\u03a6 = {\u03d51, \u00b7 \u00b7 \u00b7 , \u03d5M} , (2)\nwhich we want to adapt to the signal. In principle x(t) and \u03d5i can be multidimensional, but here we limit the numerical experiments to scalar signals.\nEq. (1) defines a sparse approximation of x(t) if the number of terms are few compared to the number of samples of x(t) and (t) is small compared to x(t), which requires that the dictionary \u03a6 is adapted to the signal. Thus, the dictionary learning problem is to calculate a set of \u03d5i that minimizes (t) under `0 regularization of the coefficient matrix. This problem is NP-hard [13] and cannot be solved explicitly. Instead, a greedy algorithm that reduces the approximation error in a\ntwo-step iteration process is used [33]: A) Encoding step; optimize the sparse representation of the signal x(t) with MP or OMP and a constant dictionary \u03a6. B) Learning step; update the dictionary \u03a6 using the sparse representation and model residual so that the approximation error is reduced.\nThe encoding step is by itself an iterative process that terminates at some predefined sparsity or tolerance on the model residual. Each iteration of the encoding process includes two steps: A1) Atom selection \u2013 Find the atom \u03d5i and offset \u03c4i,j that\nmaximizes the cross-correlation with the model residual, rk(t), and calculate the corresponding ai,j . A2) Residual update \u2013 Update the residual by subtracting the contribution from the selected atom, rk+1(t) = rk(t) \u2212 ai,j\u03d5i(t\u2212 \u03c4i,j) with r0(t) = x(t).\nThe accuracy of the resulting signal approximation depends on the complex interplay between the atom selection rule and the dictionary learning process.\nMP optimizes the parameters \u03c4i,j and ai,j of the most recently selected atom, while local OMP [32] re-optimizes all ai,j for selected atoms with overlapping support in each iteration (OMP compensates for the interference between atom instances). In the following we consider both MP and OMP when introducing homeostatic regulation. Our implementation of MP and OMP are based on efficient computational methods like those described in [32] and [34].\nThe atom selection rule defined above is optimal for sparse decomposition of a signal with a constant dictionary [30], but it does not imply optimal dictionary learning. For example, if some atoms are more frequently selected than others the entropy of the resulting sparse representation is expected to be suboptimal, potentially leading to information loss and lower reconstruction accuracy. Furthermore, atoms that are rarely selected mainly contribute to the model complexity. Therefore, we introduce a modified atom selection rule that regulates the average probability for each atom to be selected: A\u22171) Equiprobable atom selection \u2013 Find the atom \u03d5i and\noffset \u03c4i,j that maximizes the cross-correlation with the model residual, rk(t), under the constraint P (\u03d5i) < p and calculate the corresponding ai,j .\nThe atom selection constraint P (\u03d5i) < p can be applied to both MP and OMP for dictionary learning purposes. The constrained matching pursuit terminates when no atom is selected, at which point all atoms occur with the same probability p. We refer to the resulting \u201cequiprobable\u201d MP and OMP as E\u2013MP and E\u2013OMP, respectively. MP and OMP are defined by steps A1, A2 and B above, while E\u2013MP and E\u2013OMP are defined by A\u22171, A2 and B. The resulting four matching pursuits are summarized in Table I and Algorithm 1, which is a straightforward extension of the local OMP algorithm presented in [32] to equiprobable atom selection.\nThe function P (\u03d5i,\u03a6k) is the probability for each shiftinvariant atom to be selected, which is defined here by the relative number of occurrences of each atom in the subdictionary \u03a6k of selected atoms and offsets. For E\u2013MP and E\u2013OMP the\nAlgorithm 1 Matching pursuit 1: function match(\u03a6, x, p) 2: r0 = x 3: a0 = 0 4: \u03c40 = 0 5: \u03a60 = \u2205 6: for k = 1 to I do 7: {\u03d5k, tk} = argmax\u03d5i,\u03c4i,j \u3008rk\u22121,\u03a6\u3009\nsubject to constraint 8: \u03a6k = \u03a6k\u22121 \u222a \u03d5k 9: \u03a8k = neighborhood\n10: \u03c7k = (\u03a8 \u2217 k\u03a8k) \u22121\u03a8\u2217krk\u22121 11: ak = ak\u22121 + \u03c7k 12: \u03c4k = \u03c4k\u22121 + tk 13: rk = rk\u22121 \u2212\u03a8k\u03c7k 14: end for 15: return {aI , \u03c4I , rI} 16: end function\ntotal number of iterations, I , is defined so that P (\u03d5i,\u03a6I) \u2261 p for each shift-invariant atom, \u03d5i, which implies that each atom is included in the sparse approximation with the same probability, p, on average. For MP and OMP the total number of iterations, I , is defined to be equivalent to the number of E\u2013 MP and E\u2013OMP iterations. Thus, the resulting MP, OMP, E\u2013 MP and E\u2013OMP approximations are equally sparse and can be compared in terms of reconstruction error, denoising error etc. With MP and OMP the shift\u2013invariant atoms typically occur with different probabilities, resulting in a learned dictionary where only a subset of the atoms adapt to the training signal.\nThe dictionary learning problem is to infer the set of atomic waveforms, \u03d5i, in the dictionary, \u03a6, so that the matching pursuit results in a sparse representation with low residual. A computationally feasible formulation of this problem can be obtained by rewriting Eq. (1) in probabilistic form\np(x|\u03a6) = \u222b p(x|a,\u03a6)p(a)da (3)\n\u2248 p(x|a\u0302,\u03a6)p(a\u0302), (4)\nwhere a\u0302 is the maximum a posteriori (MAP) estimation of a,\na\u0302 = arg max a p(a|x,\u03a6) = arg max a p(x|a,\u03a6)p(a), (5)\nthat is generated by the matching pursuit [16], [28], [29]. Furthermore, we assume that the noise term, (t), in Eq. (1) is Gaussian. Thus, the data likelihood, p(x|a,\u03a6), is also\nGaussian and takes the form p(x|a,\u03a6) \u2248 exp ( \u2212\u2016x\u2212 a\u03a6\u2016 2\n2\u03c32\n) , (6)\nwhere\n\u2016x\u2212 a\u03a6\u20162 = \u2016x\u2212 M\u2211 i=1 Ni\u2211 j=1 ai,j\u03d5i(t\u2212 \u03c4i,j)\u20162, (7)\nand \u03c32 is the variance of the noise. Note that x, a and \u03a6 are matrices in these probabilistic expressions, and that the dictionary, \u03a6, includes all possible shifts of each atom \u03d5i.\nUnder these assumptions the atoms can be optimized by performing gradient ascent on the approximate log data probability [28]. It follows from Eq. (4) that\n\u2202 \u2202\u03d5i log(p(x|\u03a6)) = \u2202 \u2202\u03d5i [log(p(x|a\u0302,\u03a6)) + log(p(a\u0302))] . (8)\nBy taking the derivative and substituting the likelihood term with Eq. (6) this becomes\n\u2202\n\u2202\u03d5i log(p(x|\u03a6))\n= \u22121 2\u03c32 \u2202 \u2202\u03d5i \u2016x\u2212 M\u2211 i=1 Ni\u2211 j=1 ai,j\u03d5i(t\u2212 \u03c4i,j)\u20162 (9)\n= 1\n\u03c32 \u2211 j ai,j [rI ]\u03c4i,j . (10)\nThe factor [rI ]\u03c4i,j represents the model residual coinciding with atom \u03d5i at temporal position \u03c4i,j . In other words, the shape of each atom is adapted with a weighted average of the residual elements coinciding with the matches identified by the matching pursuit. This is a form of nonlinear Hebbian learning because the atoms are adapted to patterns in the signal that they correlate with.\nIn order to use the gradient for dictionary learning we introduce a relative steplength parameter, \u03b7, and define the gradient ascent update of atom \u03d5i as\n\u03d5i \u2192 \u03d5i + \u03b7\n\u03c32 \u2211 j ai,j [rI ]\u03c4i,j . (11)\nThis implies that the dictionary adaptation rate depends on the activation rate of atoms. We zero-pad all atoms with ten elements and grow each tail in length with ten additional elements if the RMS of the tail exceeds 0.1 of the atom RMS. The resulting dictionary learning method is summarized in Algorithm 2.\nThe function randdict(M) generates a random dictionary of M normalized atoms, where each atom includes fifty elements sampled from a Gaussian distribution with zero mean and two vanishing tails that are ten elements long. Thus, the M different atoms are seventy elements long initially. The extnorm(\u03d5) function extends the length of an atom \u03d5 with ten vanishing elements whenever the RMS of a tail exceeds the predefined threshold mentioned above, and it also normalizes the atom. Training data is fetched with the getdata() function, which allows block-based processing of\nAlgorithm 2 Dictionary learning 1: function dlearn(M,p, \u03b7) 2: n = 0 3: r0 = 0 4: \u03a6(0) = randdict(M) 5: while xn+1 = getdata(n, rn) is not empty do 6: n = n+ 1 7: {an, \u03c4n, rn} = match(\u03a6(n\u22121), xn, p) 8: for each \u03bei in an do 9: \u03b4i = \u03b7 \u03bei[rn]\u03c4n/var(rn) 10: \u03d5 (n) i = extnorm(\u03d5 (n\u22121) i + \u03b4i), \u03d5 (n) i \u2208 \u03a6(n) 11: end for 12: end while 13: return \u03a6(n) 14: end function\nsampled signals using overlapping window functions and part of the former residual, ri\u22121, of the matching pursuit."}, {"heading": "III. RESULTS", "text": "We investigate the effects of equiprobable atom selection on the learned dictionary and sparse approximation accuracy with numerical experiments using two different signals. One signal is a 155 seconds long 44.1 kHz rock music track with lyrics [35] and the second signal is a 26 seconds long 48 kHz recording of Zebra Finch song phrases [36]. The dictionary learning method is defined by Eq. (11) with steplength \u03b7 = 10\u22126. We do not observe significant improvements in the resulting model accuracy using other values of \u03b7 and our qualitative analysis does not depend on finetuning of this hyperparameter. The getdata() function is defined so that each block of data (xn in Algorithm 2) is five seconds long sampled from one random location in the data set. The progression of the dictionary learning protocol is thereby quantified in terms of time rather than epochs."}, {"heading": "A. Model accuracy and rate of convergence", "text": "In the first experiment we study the signal-to-noise ratio (SNR) of the sparse approximation of rock music with an average atom selection probability of p = 0.05, see Figure 3. The resulting dictionaries of 32 atoms are displayed in Figure 2. Equiprobable selection reduces the initial dictionary learning rate, but it also leads to an improved convergence time and accuracy of the sparse model. For example, the accuracy of the E\u2013MP-based approximation exceeds that of the OMPbased approximation after about 1000 seconds of learning. This is remarkable considering that the computational cost of E\u2013MP is lower than both MP and OMP (further details below). With 64 atoms a longer learning time is needed to reach a comparable SNR, but after about 2000 seconds of learning the SNR exceeds that of models with 32 atoms. Note that atoms are shift invariant. Thus, the number of possible sparse representations of a five-second long window is astronomical even if there are only a few atoms in the dictionary."}, {"heading": "B. Effect of varying atom selection probability", "text": "Next we study the sensitivity of the model to variations in the average atom selection probability, p. Using dictionaries learned from 1500 seconds of rock music we calculate the SNR of the sparse approximation of rock music for different values of p, see Figure 4. This result demonstrates that the models based on dictionaries learned with E\u2013MP and E\u2013OMP degrades gracefully when the sparsity of the model changes.\nThe learned dictionaries generalize to such varying conditions and are not \u201coverfitted\u201d to one particular value of p. Thus, equiprobable atom selection/activation can be feasible also with low-power computing substrates like neuromorphic chips where the average value of p cannot be precisely defined due to device mismatch and noise."}, {"heading": "C. Denoising", "text": "Sparse approximations of signals based on learned dictionaries are useful for solving denoising problems because atoms represent repeating additive structure, not noise-like components of the signal which mostly end up in the model residual, (t). We investigate the denoising capability of E\u2013 MP- and E\u2013OMP-based models by adding Gaussian noise to the rock music signal and comparing the SNR of the model for different noise levels with p = 0.05, see Figure 5. The noise level is quantified in terms of the ratio of the standard deviation of additive Gaussian noise, \u03c3n, to the standard deviation of the signal, \u03c3s. As expected the OMP-based model produces denoising results that are far better than MP, at least for moderate levels of noise, \u03c3n/\u03c3s \u2264 0.1. The denoising accuracy obtained with E\u2013MP and E\u2013OMP is comparable to that of OMP for moderate levels of noise, \u03c3n/\u03c3s \u2264 0.1, and it is superior to OMP for high levels of noise \u03c3n/\u03c3s > 0.1. This is another remarkable consequence of equiprobable atom selection, which indicates that the selection constraint prevents overfitting of noise-like signal components."}, {"heading": "D. Computational cost", "text": "Next we investigate the computational cost of the four different dictionary learning methods. The experiments are done using one 2.3 GHz Intel Core i7 processor core and a\nC++ implementation of the algorithms executed in Matlab. We calculate the average core time per signal sample and iteration of the matching pursuit, see Figure 6. This definition\nimplies that the displayed processing time should be multiplied with the number of samples squared times the average atom selection probability, p, to get the actual computing time. We find that equiprobable selection is beneficial in terms of computational cost. In particular, MP is more costly than E\u2013MP, and OMP is more costly than E\u2013OMP. Furthermore, with a window length of 200 kilosamples the processing time of E\u2013OMP is comparable to that of MP-based dictionary learning. Note that these results are obtained with the efficient implementations of MP and local OMP introduced above, which is the reason why the computational cost per sample decreases with the window length. This is not the case for straightforward implementations of the algorithms, which are significantly more costly."}, {"heading": "E. Entropy of sparse representation", "text": "Sparse representation with matching pursuit and a learned dictionary is a form of lossy compression where a signal is approximated using prior information encoded in the learned atoms. Next we study the information entropy of the sparse representations generated by the matching pursuits introduced above and the corresponding learned dictionaries. The number of selected atoms per second depends on the signal being processed and the method and dictionary used.\nThe top\u201325 most frequently MP/OMP-selected atoms obtained using the learned dictionaries are illustrated in Figure 7 for both rock music and birdsong. Illustrated in the figure are also the constant rate of atom selection events for E\u2013MP and E\u2013OMP with an average selection probability of p = 0.05. For MP and OMP some atoms are typically more likely to be selected than others, while some atoms may not be selected\nand learned at all. In contrast to that the average probability for selecting each atom is constant for E\u2013MP and E\u2013OMP. For example, for the equiprobable pursuits 0.05\u00d7 48000/32 = 75 events per second are expected for each atom on average. Thus, the entropy of the sequence of selected atom numbers is expected to be higher for E\u2013MP and E\u2013OMP than for the conventional matching pursuits, which is part of the motivation of this study outlined in the Introduction.\nWe calculate the Shannon entropy, \u2212 \u2211 i pilog2(pi), of the atom number sequence generated by the four matching pursuits and the two different signals, see Table II. We find numerically that OMP events have higher entropy than MP events for the signals considered here, and that E\u2013MP and E\u2013OMP have a maximum entropy of log2(M) as expected. For example, with a dictionary of 32 atoms the entropy is log2(32) = 5 bits per selected atom with E\u2013MP and E\u2013OMP. For comparison, the entropy is 3.5 (4.1) bits per selected atom for MP (OMP) in the case of rock music, and 3.5 (3.6) bits per selected atom for birdsong.\nThe distribution of atom coefficients, am,i, also depend on the method used and the signal being processed. Histograms of the atom coefficients calculated for rock music with 32 learned atoms are illustrated in Figure 8. An explicit calculation of the\nShannon entropy of the event coefficients is not meaningful due to the continuous (floating point) nature of these numbers. However, we calculate the entropies of multiple histograms with different bin counts (16, 32 and 64) and note a systematic difference between the four models, see Table III. Quantized OMP coefficients have the lowest entropy, followed by E\u2013 OMP, MP and E\u2013MP in order of increasing entropy. The maximum difference is 10% when comparing the coefficient entropies of E\u2013MP and OMP with 16 bins (about 4-bits of precision), which can be compared to the difference of about 20% in Table II. Thus, by adding the entropies of atom indices and coefficients we conclude that E\u2013MP has the highest event entropy followed by E\u2013OMP for the signals considered here.\nIn these numerical experiments the average atom selection probability is a constant with value 0.05, which implies that on average there is one selected atom in the sparse approximation for every 20 samples of the signal. This implies that the Shannon information of the sparse approximations generated by the different matching pursuits are directly proportional to the entropy per event. Therefore, for the signals considered here the sparse approximations calculated with equiprobable selection have higher Shannon information, in line with the higher reconstruction accuracy obtained with these methods in the former subsections."}, {"heading": "IV. DISCUSSION", "text": "We extend a well-known dictionary learning and sparse representation model [28] with a basic homeostatic regulation mechanism. The extension is motivated by the observation that the information entropy of such sparse representations is\nsub-optimal by construction, and partially also by the central role of homeostatic regulation in cortical networks and spiking neural network models of sensory areas, see for example [37], [38].\nThe sparse representations are generated with Matching Pursuit (MP) [30], Local Orthogonal MP (OMP) [32] and the homeostatic extensions Equiprobable MP (E\u2013MP) and Equiprobable OMP (E\u2013OMP) introduced here. Dictionaries of shift-invariant atoms are learned using probabilistic gradient ascent for two different signals (rock music and Zebra Finch song phrases). With dictionary learning based on E\u2013MP- and E\u2013OMP we obtain an improved rate of convergence and sparse representation SNR (Figure 3), improved denoising results (Figure 5), a lower computational cost (Figure 6) and higher information entropy of the sparse representation (Table II and Table III).\nNote that E\u2013MP and E\u2013OMP only make sense in a dictionary learning setting where the atoms are adapted to the signal, otherwise the regular OMP/MP methods should be used. Furthermore, dictionary learning with equiprobable selection is only sensible with complex signals when the number of shiftinvariant atoms in the dictionary is lower than the number of independent and uniqe signal components, which typically is the case in practical applications. Otherwise the preferred solution is to learn one atom for each independent component, which in general is not expected with equiprobable selection.\nThe low coefficient entropy of E\u2013OMP in combination with maximum atom selection entropy is an interesting property of E\u2013OMP-based dictionary learning that could improve the accuracy of dictionary learnimg implementations with quantized coefficients. Furthermore, with E\u2013MP and E\u2013OMP atoms are enforced to occur with the same probability on average and all atoms adapt to the training signal, which is not the case with MP/OMP. In principle the equiprobable selection mechanism resembles a dropout [39] mechanism where the probability of dropout dynamically depends on the selection rate of each atom. We study the sensitivity of the method to variations in the average selection probability, p, and find that the learned dictionaries are useful with other values of p and that the model degrades gracefully for lower values of p. Thus, low-power neuromorphic implementations of the proposed equiprobable selection and dictionary learning methods could be feasible regardless of uncertainties associated with for example device mismatch and noise.\nIn summary, our main finding is that the accuracy and learning rate of a well-known dictionary learning method are improved by the equiprobable atom selection constraint introduced here, and that the computational cost of the resulting E\u2013MP and E\u2013OMP methods are lower than of MP and local OMP, respectively. Equiprobable selection can be applied to other greedy methods for dictionary learning and there are also opportunities to further develop the basic homeostatic regulation mechanism outlined here in search for more efficient approaches to address this NP-hard problem."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was stimulated by discussions at the CapoCaccia Neuromorphic Engineering workshops in 2015 and 2016. In particular F.S. acknowledge the discussions with Prof. Christopher Kello about cortical criticality and the lectures by Prof. Yves Fre\u0301gnac on receptive fields where a knowledge gap between models and observed properties of cells in the visual cortex were highlighted. F.S. is funded by a Gunnar O\u0308quist Fellowship from the Kempe Foundations and S.M.C. is funded by the SKF\u2013LTU University Technology Center. We acknowledge travel support from the Swedish Foundation for International Cooperation in Research and Higher Education (STINT), grant number IG2011-2025."}], "references": [{"title": "A Wavelet Tour of Signal Processing: The Sparse Way, 3rd ed", "author": ["S. Mallat"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "From sparse solutions of systems of equations to sparse modeling of signals and images", "author": ["A. Bruckstein", "D. Donoho", "M. Elad"], "venue": "SIAM Review, vol. 51, no. 1, pp. 34\u201381, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse and redundant representations: from theory to applications in signal and image processing", "author": ["M. Elad"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "The cosparse analysis model and algorithms", "author": ["S. Nam", "M. Davies", "M. Elad", "R. Gribonval"], "venue": "Applied and Computational Harmonic Analysis, vol. 34, no. 1, pp. 30\u201356, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Dictionaries for sparse representation modeling", "author": ["R. Rubinstein", "A. Bruckstein", "M. Elad"], "venue": "Proceedings of the IEEE, vol. 98, no. 6, pp. 1045\u20131057, June 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Dictionary learning", "author": ["I. Tosic", "P. Frossard"], "venue": "Signal Processing Magazine, IEEE, vol. 28, no. 2, pp. 27\u201338, March 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Gradient pursuits", "author": ["T. Blumensath", "M. Davies"], "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 6, pp. 2370\u20132382, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Blind source separation by sparse decomposition in a signal dictionary", "author": ["M. Zibulevsky", "B.A. Pearlmutter"], "venue": "Neural Computation, vol. 13, no. 4, pp. 863\u2013882, 2016/02/04 2001. [Online]. Available: http://dx.doi.org/10.1162/089976601300014385", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Underdetermined blind source separation using sparse representations", "author": ["P. Bofill", "M. Zibulevsky"], "venue": "Signal Processing, vol. 81, no. 11, pp. 2353 \u2013 2362, 2001. [Online]. Available: http://www.sciencedirect.com/ science/article/pii/S0165168401001207", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "IEEE Trans. Inform. Theory, vol. 52, pp. 1289\u20131306, 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards a mathematical theory of super-resolution", "author": ["E.J. Cands", "C. Fernandez-Granda"], "venue": "Communications on Pure and Applied Mathematics, vol. 67, no. 6, pp. 906\u2013956, 2014. [Online]. Available: http://dx.doi.org/10.1002/cpa.21455", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Supervised dictionary learning", "author": ["J. Mairal", "J. Ponce", "G. Sapiro", "A. Zisserman", "F.R. Bach"], "venue": "Advances in Neural Information Processing Systems 21, D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, Eds. Curran Associates, Inc., 2009, pp. 1033\u20131040. [Online]. Available: http://papers.nips.cc/paper/3448-supervised-dictionary-learning.pdf", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "On the computational intractability of exact and approximate dictionary learning", "author": ["A.M. Tillmann"], "venue": "IEEE Signal Processing Letters, vol. 22, no. 1, pp. 45\u201349, Jan 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B. Olshausen", "D. Field"], "venue": "Nature, vol. 381, pp. 607\u2013609, 1996.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "Learning overcomplete representations", "author": ["M.S. Lewicki", "T.J. Sejnowski"], "venue": "Neural Computation, vol. 12, no. 2, pp. 337\u2013365, March 2000.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Dictionary learning algorithms for sparse representation", "author": ["K. Kreutz-Delgado", "J.F. Murray", "B.D. Rao", "K. Engan", "T.-W. Lee", "T.J. Sejnowski"], "venue": "Neural Computation, vol. 15, no. 2, pp. 349\u2013396, 2016/02/03 2003. [Online]. Available: http://dx.doi.org/10. 1162/089976603762552951", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "An online algorithm for distributed dictionary learning", "author": ["S. Chouvardas", "Y. Kopsinis", "S. Theodoridis"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, April 2015, pp. 3292\u20133296.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "On-chip sparse learning acceleration with cmos and resistive synaptic devices", "author": ["J. sun Seo", "B. Lin", "M. Kim", "P.-Y. Chen", "D. Kadetotad", "Z. Xu", "A. Mohanty", "S. Vrudhula", "S. Yu", "J. Ye", "Y. Cao"], "venue": "Nanotechnology, IEEE Transactions on, vol. 14, no. 6, pp. 969\u2013979, Nov 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptation of the simple or complex nature of v1 receptive fields to visual statistics", "author": ["J. Fournier", "C. Monier", "M. Pananceau", "Y. Fregnac"], "venue": "Nature Neuroscience, vol. 14, no. 8, pp. 1053\u20131060, Aug 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Spatiotemporal receptive fields of barrel cortex revealed by reverse correlation of synaptic input", "author": ["A. Ramirez", "E.A. Pnevmatikakis", "J. Merel", "L. Paninski", "K.D. Miller", "R.M. Bruno"], "venue": "Nature Neuroscience, vol. 17, no. 6, pp. 866\u2013875, Jun 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Hidden complexity of synaptic receptive fields in cat v1", "author": ["J. Fournier", "C. Monier", "M. Levy", "O. Marre", "K. Sri", "Z.F. Kisvrday", "Y. Frgnac"], "venue": "The Journal of Neuroscience, vol. 34, no. 16, pp. 5515\u20135528, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual adaptation as optimal information transmission", "author": ["M.J. Wainwright"], "venue": "Vision Research, vol. 39, no. 23, pp. 3960 \u2013 3974, 1999.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "The criticality hypothesis: how local cortical networks might optimize information processing", "author": ["J.M. Beggs"], "venue": "Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, vol. 366, no. 1864, pp. 329\u2013343, 2008. [Online]. Available: http://rsta.royalsocietypublishing.org/content/366/1864/329", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1864}, {"title": "Being critical of criticality in the brain", "author": ["J.M. Beggs", "N. Timme"], "venue": "Frontiers in Physiology, vol. 3, no. 163, 2012. [Online]. Available: http://www.frontiersin.org/fractal physiology/10.3389/fphys. 2012.00163/abstract", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "The functional benefits of criticality in the cortex", "author": ["W.L. Shew", "D. Plenz"], "venue": "The Neuroscientist, vol. 19, no. 1, pp. 88\u2013100, 2013. [Online]. Available: http://nro.sagepub.com/content/19/1/88.abstract", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Quasicritical brain dynamics on a nonequilibrium widom line", "author": ["R.V. Williams-Garc\u0131\u0301a", "M. Moore", "J.M. Beggs", "G. Ortiz"], "venue": "Phys. Rev. E, vol. 90, p. 062714, Dec 2014. [Online]. Available: http://link.aps.org/doi/10.1103/PhysRevE.90.062714", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient auditory coding", "author": ["E. Smith", "M.S. Lewicki"], "venue": "Nature, no. 7079, pp. 978\u2013982, 02.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 0}, {"title": "Efficient coding of time-relative structure using spikes", "author": ["E. Smith", "M. Lewicki"], "venue": "Neural Computation, vol. 17, no. 1, pp. 19\u201345, 2005.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Matching pursuits with time-frequency dictionaries", "author": ["S. Mallat", "Z. Zhang"], "venue": "IEEE T Signal Proces, 1993.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1993}, {"title": "Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition", "author": ["Y. Pati", "R. Rezaiifar", "P. Krishnaprasad"], "venue": "Signals, Systems and Computers, 1993. 1993 Conference Record of The Twenty-Seventh Asilomar Conference on, Nov 1993, pp. 40\u201344 vol.1.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1993}, {"title": "A low complexity orthogonal matching pursuit for sparse signal approximation with shift-invariant dictionaries", "author": ["B. Mailhe", "R. Gribonval", "F. Bimbot", "P. Vandergheynst"], "venue": "Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on, April 2009, pp. 3445\u20133448.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic framework for the adaptation and comparison of image codes", "author": ["M.S. Lewicki", "B.A. Olshausen"], "venue": "J. Opt. Soc. Am. A, vol. 16, no. 7, pp. 1587\u20131601, Jul 1999. [Online]. Available: http://josaa.osa.org/abstract.cfm?URI=josaa-16-7-1587", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1999}, {"title": "MPTK: Matching Pursuit made tractable", "author": ["S. Krstulovic", "R. Gribonval"], "venue": "Proc. Int. Conf. Acoust. Speech Signal Process. (ICASSP\u201906), vol. 3, May 2006, pp. III\u2013496 \u2013 III\u2013499.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "After the last", "author": ["The Red Thread"], "venue": "licensed under CC BY-NC 3.0. [Online]. Available: http://freemusicarchive.org", "citeRegEx": "35", "shortCiteRegEx": null, "year": 0}, {"title": "Song phrases of Zebra Finch", "author": ["M. Anderson"], "venue": "cat. nr. XC287103. [Online]. Available: www.xeno-canto.org/287103/download", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2871}, {"title": "Inhibitory interneurons decorrelate excitatory cells to drive sparse code formation in a spiking model of v1", "author": ["P.D. King", "J. Zylberberg", "M.R. DeWeese"], "venue": "Journal of Neuroscience, vol. 33, no. 13, pp. 5475\u20135485, 2013.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonlinear hebbian learning as a unifying principle in receptive field formation", "author": ["C.S.N. Brito", "W. Gerstner"], "venue": "2016. [Online]. Available: http://arxiv.org/abs/1601.00701", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, pp. 1929\u20131958, 2014.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1929}], "referenceMentions": [{"referenceID": 0, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 44, "endOffset": 47}, {"referenceID": 4, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 93, "endOffset": 96}, {"referenceID": 6, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 242, "endOffset": 245}, {"referenceID": 7, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 282, "endOffset": 285}, {"referenceID": 8, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 287, "endOffset": 290}, {"referenceID": 9, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 311, "endOffset": 315}, {"referenceID": 10, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 342, "endOffset": 346}, {"referenceID": 11, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 367, "endOffset": 371}, {"referenceID": 12, "context": "However, the dictionary learning problem is NP-hard and it is also hard to find approximate solutions near the optimal sparsity level [13].", "startOffset": 134, "endOffset": 138}, {"referenceID": 13, "context": "The development of dictionary learning methods was stimulated by results presented in the mid \u201990s by Olshausen and Field [14], [15], which demonstrate that atoms similar to the receptive fields of cells in visual cortex can be learned from natural images by imposing a few general optimization conditions, including sparsity and statistical independence of atoms.", "startOffset": 122, "endOffset": 126}, {"referenceID": 1, "context": "Since that time several probabilistic dictionary learning and sparse coding methods have been developed [2], [5], aiming for a dictionary that either maximizes the", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "Since that time several probabilistic dictionary learning and sparse coding methods have been developed [2], [5], aiming for a dictionary that either maximizes the", "startOffset": 109, "endOffset": 112}, {"referenceID": 14, "context": "likelihood of the data, as for example in [16], or the posterior probability of the dictionary, as in [17].", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "likelihood of the data, as for example in [16], or the posterior probability of the dictionary, as in [17].", "startOffset": 102, "endOffset": 106}, {"referenceID": 16, "context": "Recent developments include extensions of dictionary learning methods to distributed systems [18] and low-power hardware [19].", "startOffset": 93, "endOffset": 97}, {"referenceID": 17, "context": "Recent developments include extensions of dictionary learning methods to distributed systems [18] and low-power hardware [19].", "startOffset": 121, "endOffset": 125}, {"referenceID": 18, "context": "For instance, neurons demonstrate a form of homeostatic adaptation of the functional properties of the network to the ongoing changes in the statistical structure of the sensory input [20], [21], [22], which may be related to optimal encoding [23] of the input by dynamic adaptation of the receptive fields.", "startOffset": 184, "endOffset": 188}, {"referenceID": 19, "context": "For instance, neurons demonstrate a form of homeostatic adaptation of the functional properties of the network to the ongoing changes in the statistical structure of the sensory input [20], [21], [22], which may be related to optimal encoding [23] of the input by dynamic adaptation of the receptive fields.", "startOffset": 190, "endOffset": 194}, {"referenceID": 20, "context": "For instance, neurons demonstrate a form of homeostatic adaptation of the functional properties of the network to the ongoing changes in the statistical structure of the sensory input [20], [21], [22], which may be related to optimal encoding [23] of the input by dynamic adaptation of the receptive fields.", "startOffset": 196, "endOffset": 200}, {"referenceID": 21, "context": "For instance, neurons demonstrate a form of homeostatic adaptation of the functional properties of the network to the ongoing changes in the statistical structure of the sensory input [20], [21], [22], which may be related to optimal encoding [23] of the input by dynamic adaptation of the receptive fields.", "startOffset": 243, "endOffset": 247}, {"referenceID": 22, "context": "Furthermore, there is a notion that homeostatic mechanisms serve to maintain the dynamics of cortical networks at a critical point [24], [25] where the dynamic ar X iv :1 61 1.", "startOffset": 131, "endOffset": 135}, {"referenceID": 23, "context": "Furthermore, there is a notion that homeostatic mechanisms serve to maintain the dynamics of cortical networks at a critical point [24], [25] where the dynamic ar X iv :1 61 1.", "startOffset": 137, "endOffset": 141}, {"referenceID": 24, "context": "range and information processing capacity are optimal [26], [27].", "startOffset": 54, "endOffset": 58}, {"referenceID": 25, "context": "range and information processing capacity are optimal [26], [27].", "startOffset": 60, "endOffset": 64}, {"referenceID": 26, "context": "The starting point is the dictionary learning method introduced by Smith and Lewicki [28], where the sparse code is generated with Matching Pursuit (MP) [29], [30] and the shiftinvariant dictionary is updated with probabilistic gradient ascent on the likelihood of the data [16].", "startOffset": 85, "endOffset": 89}, {"referenceID": 27, "context": "The starting point is the dictionary learning method introduced by Smith and Lewicki [28], where the sparse code is generated with Matching Pursuit (MP) [29], [30] and the shiftinvariant dictionary is updated with probabilistic gradient ascent on the likelihood of the data [16].", "startOffset": 153, "endOffset": 157}, {"referenceID": 28, "context": "The starting point is the dictionary learning method introduced by Smith and Lewicki [28], where the sparse code is generated with Matching Pursuit (MP) [29], [30] and the shiftinvariant dictionary is updated with probabilistic gradient ascent on the likelihood of the data [16].", "startOffset": 159, "endOffset": 163}, {"referenceID": 14, "context": "The starting point is the dictionary learning method introduced by Smith and Lewicki [28], where the sparse code is generated with Matching Pursuit (MP) [29], [30] and the shiftinvariant dictionary is updated with probabilistic gradient ascent on the likelihood of the data [16].", "startOffset": 274, "endOffset": 278}, {"referenceID": 28, "context": "In addition to MP we consider dictionary learning with Orthogonal MP (OMP) [30], [31], which in the case of shift-invariant dictionaries is applicable to high-dimensional signals in the form of local OMP [32].", "startOffset": 75, "endOffset": 79}, {"referenceID": 29, "context": "In addition to MP we consider dictionary learning with Orthogonal MP (OMP) [30], [31], which in the case of shift-invariant dictionaries is applicable to high-dimensional signals in the form of local OMP [32].", "startOffset": 81, "endOffset": 85}, {"referenceID": 30, "context": "In addition to MP we consider dictionary learning with Orthogonal MP (OMP) [30], [31], which in the case of shift-invariant dictionaries is applicable to high-dimensional signals in the form of local OMP [32].", "startOffset": 204, "endOffset": 208}, {"referenceID": 12, "context": "This problem is NP-hard [13] and cannot be solved explicitly.", "startOffset": 24, "endOffset": 28}, {"referenceID": 31, "context": "two-step iteration process is used [33]: A) Encoding step; optimize the sparse representation of the signal x(t) with MP or OMP and a constant dictionary \u03a6.", "startOffset": 35, "endOffset": 39}, {"referenceID": 30, "context": "MP optimizes the parameters \u03c4i,j and ai,j of the most recently selected atom, while local OMP [32] re-optimizes all ai,j for selected atoms with overlapping support in each iteration (OMP compensates for the interference between atom instances).", "startOffset": 94, "endOffset": 98}, {"referenceID": 30, "context": "Our implementation of MP and OMP are based on efficient computational methods like those described in [32] and [34].", "startOffset": 102, "endOffset": 106}, {"referenceID": 32, "context": "Our implementation of MP and OMP are based on efficient computational methods like those described in [32] and [34].", "startOffset": 111, "endOffset": 115}, {"referenceID": 28, "context": "The atom selection rule defined above is optimal for sparse decomposition of a signal with a constant dictionary [30], but it does not imply optimal dictionary learning.", "startOffset": 113, "endOffset": 117}, {"referenceID": 30, "context": "The resulting four matching pursuits are summarized in Table I and Algorithm 1, which is a straightforward extension of the local OMP algorithm presented in [32] to equiprobable atom selection.", "startOffset": 157, "endOffset": 161}, {"referenceID": 14, "context": "that is generated by the matching pursuit [16], [28], [29].", "startOffset": 42, "endOffset": 46}, {"referenceID": 26, "context": "that is generated by the matching pursuit [16], [28], [29].", "startOffset": 48, "endOffset": 52}, {"referenceID": 27, "context": "that is generated by the matching pursuit [16], [28], [29].", "startOffset": 54, "endOffset": 58}, {"referenceID": 26, "context": "Under these assumptions the atoms can be optimized by performing gradient ascent on the approximate log data probability [28].", "startOffset": 121, "endOffset": 125}, {"referenceID": 33, "context": "1 kHz rock music track with lyrics [35] and the second signal is a 26 seconds long 48 kHz recording of Zebra Finch song phrases [36].", "startOffset": 35, "endOffset": 39}, {"referenceID": 34, "context": "1 kHz rock music track with lyrics [35] and the second signal is a 26 seconds long 48 kHz recording of Zebra Finch song phrases [36].", "startOffset": 128, "endOffset": 132}, {"referenceID": 26, "context": "We extend a well-known dictionary learning and sparse representation model [28] with a basic homeostatic regulation mechanism.", "startOffset": 75, "endOffset": 79}, {"referenceID": 35, "context": "that the information entropy of such sparse representations is sub-optimal by construction, and partially also by the central role of homeostatic regulation in cortical networks and spiking neural network models of sensory areas, see for example [37], [38].", "startOffset": 246, "endOffset": 250}, {"referenceID": 36, "context": "that the information entropy of such sparse representations is sub-optimal by construction, and partially also by the central role of homeostatic regulation in cortical networks and spiking neural network models of sensory areas, see for example [37], [38].", "startOffset": 252, "endOffset": 256}, {"referenceID": 28, "context": "The sparse representations are generated with Matching Pursuit (MP) [30], Local Orthogonal MP (OMP) [32] and the homeostatic extensions Equiprobable MP (E\u2013MP) and Equiprobable OMP (E\u2013OMP) introduced here.", "startOffset": 68, "endOffset": 72}, {"referenceID": 30, "context": "The sparse representations are generated with Matching Pursuit (MP) [30], Local Orthogonal MP (OMP) [32] and the homeostatic extensions Equiprobable MP (E\u2013MP) and Equiprobable OMP (E\u2013OMP) introduced here.", "startOffset": 100, "endOffset": 104}, {"referenceID": 37, "context": "In principle the equiprobable selection mechanism resembles a dropout [39] mechanism where the probability of dropout dynamically depends on the selection rate of each atom.", "startOffset": 70, "endOffset": 74}], "year": 2016, "abstractText": "Sparse signal representations based on linear combinations of learned atoms have been used to obtain state-ofthe-art results in several practical signal processing applications. Approximation methods are needed to process high-dimensional signals in this way because the problem to calculate optimal atoms for sparse coding is NP-hard. Here we study greedy algorithms for unsupervised learning of dictionaries of shiftinvariant atoms and propose a new method where each atom is selected with the same probability on average, which corresponds to the homeostatic regulation of a recurrent convolutional neural network. Equiprobable selection can be used with several greedy algorithms for dictionary learning to ensure that all atoms adapt during training and that no particular atom is more likely to take part in the linear combination on average. We demonstrate via simulation experiments that dictionary learning with equiprobable selection results in higher entropy of the sparse representation and lower reconstruction and denoising errors, both in the case of ordinary matching pursuit and orthogonal matching pursuit with shift-invariant dictionaries. Furthermore, we show that the computational costs of the matching pursuits are lower with equiprobable selection, leading to faster and more accurate dictionary learning algorithms.", "creator": "LaTeX with hyperref package"}}}