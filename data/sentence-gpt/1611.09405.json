{"id": "1611.09405", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "An End-to-End Architecture for Keyword Spotting and Voice Activity Detection", "abstract": "We propose a single neural network architecture for two tasks: on-line keyword spotting and voice activity detection. We develop novel inference algorithms for an end-to-end Recurrent Neural Network trained with the Connectionist Temporal Classification loss function which allow our model to achieve high accuracy on both keyword spotting and voice activity detection without retraining. In contrast to prior voice activity detection models, our architecture does not require aligned training data and uses the same parameters as the keyword spotting model to determine the frequency of a search. We describe the algorithms in this paper.", "histories": [["v1", "Mon, 28 Nov 2016 22:03:22 GMT  (42kb,D)", "http://arxiv.org/abs/1611.09405v1", "NIPS 2016 End-to-End Learning for Speech and Audio Processing Workshop"]], "COMMENTS": "NIPS 2016 End-to-End Learning for Speech and Audio Processing Workshop", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chris lengerich", "awni hannun"], "accepted": false, "id": "1611.09405"}, "pdf": {"name": "1611.09405.pdf", "metadata": {"source": "CRF", "title": "An End-to-End Architecture for Keyword Spotting and Voice Activity Detection", "authors": ["Chris Lengerich", "Awni Hannun"], "emails": ["chris@mindori.com", "awni@mindori.com"], "sections": [{"heading": "1 Introduction", "text": "Keyword spotting (KWS) is a speech task which requires detecting a specific word in an audio signal, commonly for use as the \u201cwake word\u201d of a large-vocabulary (LV) speech recognizer. Voice activity detection (VAD) requires detecting human speech in the signal, often for the purpose of endpointing in a large vocabulary speech recognition system. Both tasks are challenging due to computational constraints and noisy environments. To limit computational cost, VAD models have often leaned on hand-engineered features, requiring training separately from KWS models.\nWe propose instead a single end-to-end neural network architecture for both KWS and VAD. We develop novel inference algorithms which allow us to run KWS and VAD tasks without retraining. Our model outperforms both baselines, is trained only on unaligned character-level transcripts, and requires maintaining only a single architecture for training and deployment."}, {"heading": "2 Related Work", "text": "The model is based on work in end-to-end speech recognition which uses the Connectionist Temporal Classification loss function coupled with deep Recurrent Neural Networks [7, 8]. In this work we develop the model and inference procedure for the KWS and VAD tasks. A thorough treatment of the benefits of this model for LVCSR is given in [1]. A character-level CTC architecture was also recently adopted for keyword spotting [10], where it outperformed a DNN-HMM baseline, while a word-level CTC architecture was used for keyword spotting in [5].\nTraditional VAD architectures trade off accuracy for low computational cost, as they have historically been developed for very low-resource environments. Some simple and efficient techniques include a threshold on the energy of the audio signal, a threshold on the number of zero-crossings [11] or combinations of these features, however, these methods are typically not robust to nonstationary environments. We note that modern LV speech environments afford more computational\n\u2217Equal contribution\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 1.\n09 40\n5v 1\n[ cs\n.C L\n] 2\n8 N\nov 2\nresources than before, and the use of larger neural models is feasible, especially for LV ASR endpointing. Neural architectures have been proposed for VAD, notably the RNN architecture in [9], however that approach relied on frame-aligned labels."}, {"heading": "3 Model", "text": "For a general keyword spotter, we model p(k|x), where k is a keyword and x is a window of speech. For VAD we use the same distribution and simply set k to the empty string.\nWe use the Connectionist Temporal Classification [6] (CTC) objective function to train an RNN on a corpus of utterance and transcription pairs. The CTC objective gives us the probability of any label string for a given utterance. We do not need an alignment as CTC efficiently computes the score over all possible alignments. The objective function for an utterance x and corresponding transcription ` is given by\npCTC(`|x) = \u2211\ns\u2208align(`,T ) T\u220f t p(st|x). (1)\nThe align(\u00b7) function computes the set of possible alignments of the transcription ` over the T time-steps of the utterance under the CTC operator. The CTC operator allows for repetitions of any character and insertions of the blank character, , which signifies no output at a given time-step.\nFor the on-line KWS task we must determine with low latency if the keyword has been said. In order to use this model for KWS, we score a moving window of the audio stream x so that we can find the keyword soon after it occurs. The score is computed as pCTC(k|xt:t+w) where k is any keyword and xt:t+w is a window of speech w frames long. For the VAD task we first compute the probability of no speech by setting k to the empty string. From this we can find the probability of speech by taking one minus the probability of no speech."}, {"heading": "3.1 Network Architecture", "text": "The network accepts as input a spectrogram computed from the raw waveform sampled at 8kHz. The first layer is a 2-dimensional convolution with a stride of three [1]. For the next three layers of the network we use gated recurrent RNN layers [3][4]. The last layer is a single affine transformation followed by a softmax. The network outputs directly to characters in the alphabet including the blank and space characters."}, {"heading": "3.2 Inference", "text": "The optimal window size for KWS detection varies per utterance based on speech rate, noise and adjacent utterances. To alleviate the sensitivity of the algorithm to the window size parameter we propose a modification to the CTC scoring algorithm presented above. For a given keyword k instead of scoring k itself under the model we instead score the regular expression [\u02c6k0]*k[\u02c6kn\u22121]*, where k0 and kn\u22121 are the first and last characters of k, respectively. This is described in Algorithm 1.\nComputing the VAD score reduces to summing the log probabilities of the blank character over the window of speech frames:\nlog p(speech|xt:t+w) = 1\u2212 t+w\u2211 i=t log pi( |xt:t+w). (2)"}, {"heading": "4 Experiments", "text": "The model parameters are optimized with stochastic gradient descent for 50 epochs and a minibatch size of 256. We sort examples so that the minibatch consists of utterances of similar length for computational efficiency. The learning rate and momentum parameters are chosen to optimize speed of convergence. We anneal the learning rate by a factor of 0.9 every 5000 iterations.\nThe architecture of the network is as described in Section 3.1. The filters for the convolution layer are 11 by 32 over the time and frequency dimensions respectively. We use 32 filters in all models.\nAlgorithm 1 Compute the score of a keyword, k, given the CTC output probabilities, P . The parameter l is the keyword with inserted at the beginning, end and between every pair of characters of k .\nfunction SCOREKEYWORD(l, P ) S \u2190 size(l) T \u2190 numberOfColumns(P ) \u03b1\u2190 zeros(S, T) \u03b11,1 \u2190 1\u2212 Pl2,1 \u03b12,1 \u2190 Pl2,1 for t = 2 : T do\nfor s = 1 : S do if s = 1 then\np\u2190 1\u2212 Pl2,t else if s = S then\np\u2190 1\u2212 PlS\u22121,t else\np\u2190 Pls,t end if if s > 2 and ls 6= ls\u22122 and ls 6= then\n\u03b1s,t \u2190 p \u2217 (\u03b1s,t\u22121 + \u03b1s\u22121,t\u22121 + \u03b1s\u22122,t\u22121) else if s > 1 then\n\u03b1s,t \u2190 p \u2217 (\u03b1s,t\u22121 + \u03b1s\u22121,t\u22121) else\n\u03b1s,t \u2190 p \u2217 \u03b1s,t\u22121 end if\nend for end for return \u03b1S,T + \u03b1S\u22121,T\nend function"}, {"heading": "4.1 Data", "text": "The data used to train the model consists of two datasets. The first dataset is a corpus of 526K transcribed utterances collected on Android phones via an assistant-like application. The second corpus consists of 1544 spoken examples of the keyword, in this case, \u201cOlivia\u201d. The model is trained on both data-sets simultaneously. We do not need to pre-train on the large corpus prior to fine-tuning. We also use a collection of about a hundred hours of noise and music downloaded from the web to generate synthetic noisy examples of the keyword and empty noise clips. When training with the noisy data we replicate each keyword 10 times, each time with a random noise clip. We also use a corpus of 57K randomly sampled noise clips with a blank label as filler.\nThe KWS model is evaluated on a test set of 550 positive examples (e.g. containing the keyword \u201cOlivia\u201c) and 5000 negative examples held-out from the large speech corpus described above. During inference we evaluate the utterance with Algorithm 1 every 100 milliseconds over a window of 800 milliseconds in order to detect the presence of the keyword. We classify an example as positive if the score found from the output of Algorithm 1 over the utterance is ever above a preset threshold.\nWe evaluate the same models on the VAD task. The positive examples are the same 5000 examples of speech used as the negative examples for the KWS task. We collected about 10 hours of nonspeech audio from a variety of noise backgrounds. We sample 5000 random clips from the 10 hours of noise to construct the negative samples."}, {"heading": "4.2 Results", "text": "Our KWS baseline is a DNN keyword spotter from kitt.ai 2. Our VAD baseline is the WebRTC VAD codec3 with frame size 30ms. Our model of 3 layers of size 256 outperforms both baselines.\n2https://github.com/Kitt-AI/snowboy 3https://github.com/wiseman/py-webrtcvad\nFor a fixed false positive rate of 5% our model achieves a 98.1% true positive rate on keyword spotting, in comparison to the baseline 96.2%. For the VAD task, for the same false positive rate, our model achieves 99.8% true positive rate vs. 44.6% for the baseline. This large delta may be due to the substantial difference in representational power of a large-parameter neural model vs. the small-parameter GMM baseline, as well as differences in the type and volume of training data.\nFigures 1 and 2 show that our model consistently improves at detecting the keyword as we increase the number of layers and the size of the model. Increasing the model depth and size also improves VAD performance; however, when the layers are larger than 128 units or there are more than 2 layers, VAD performance saturates. Most of the large VAD models achieve near 99.9% true positive rate or higher at a fixed false positive rate of 5.0%.\nIn Figure 3 we see that adding noise to the keywords during training results in substantial improvements. At a false positive rate of 5% the model with noise has a true positive rate of 98.9% compared to 94.3% for the model without noise. Further using the random noise data on its own does not help much; in fact the results are slightly worse. On the VAD task we also notice an improvement in the ROC curve as we add noise.\nOur production model with 3 layers of 256 hidden units has \u223c1.5M trainable parameters, comparable to other neural network-based KWS approaches [2], and has been deployed to a modern smartphone."}, {"heading": "5 Conclusion", "text": "We have described a single neural network architecture which can be used for both keyword spotting and voice activity detection without retraining. The model is simple to train and does not require an alignment or frame-wise labels, in contrast to prior VAD models. We propose inference algorithms\nfor KWS and VAD modified from the basic CTC scoring algorithm which allow the model to perform both tasks. While our model is efficient, applying neural compression techniques might further increase performance and represents an interesting area for future work."}], "references": [{"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos", "E. Elsen", "J. Engel", "L. Fan", "C. Fougner", "T. Han", "A.Y. Hannun", "B. Jun", "P. LeGresley", "L. Lin", "S. Narang", "A.Y. Ng", "S. Ozair", "R. Prenger", "J. Raiman", "S. Satheesh", "D. Seetapun", "S. Sengupta", "Y. Wang", "Z. Wang", "C. Wang", "B. Xiao", "D. Yogatama", "J. Zhan", "Z. Zhu"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Small-footprint keyword spotting using deep neural networks", "author": ["G. Chen", "C. Parada", "G. Heigold"], "venue": "In ICASSP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "\u00c7. G\u00fcl\u00e7ehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": "In NIPS Deep Learning Workshop,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "An application of recurrent neural networks to discriminative keyword spotting", "author": ["S. Fern\u00e1ndez", "A. Graves", "J. Schmidhuber"], "venue": "In Proceedings of the 17th International Conference on Artificial Neural Networks,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Towards End-to-End Speech Recognition with Recurrent Neural Networks", "author": ["A. Graves", "N. Jaitly"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["A.Y. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates", "A.Y. Ng"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Recurrent neural networks for voice activity detection", "author": ["T. Hughes", "K. Mierle"], "venue": "In ICASSP,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Online keyword spotting with a character-level recurrent neural network", "author": ["K. Hwang", "M. Lee", "W. Sung"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A study of endpoint detection algorithms in adverse conditions: incidence on a dtw and hmm recognizer", "author": ["J.-C. Junqua", "B. Reaves", "B. Mak"], "venue": "In EUROSPEECH,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1991}], "referenceMentions": [{"referenceID": 6, "context": "The model is based on work in end-to-end speech recognition which uses the Connectionist Temporal Classification loss function coupled with deep Recurrent Neural Networks [7, 8].", "startOffset": 171, "endOffset": 177}, {"referenceID": 7, "context": "The model is based on work in end-to-end speech recognition which uses the Connectionist Temporal Classification loss function coupled with deep Recurrent Neural Networks [7, 8].", "startOffset": 171, "endOffset": 177}, {"referenceID": 0, "context": "A thorough treatment of the benefits of this model for LVCSR is given in [1].", "startOffset": 73, "endOffset": 76}, {"referenceID": 9, "context": "A character-level CTC architecture was also recently adopted for keyword spotting [10], where it outperformed a DNN-HMM baseline, while a word-level CTC architecture was used for keyword spotting in [5].", "startOffset": 82, "endOffset": 86}, {"referenceID": 4, "context": "A character-level CTC architecture was also recently adopted for keyword spotting [10], where it outperformed a DNN-HMM baseline, while a word-level CTC architecture was used for keyword spotting in [5].", "startOffset": 199, "endOffset": 202}, {"referenceID": 10, "context": "Some simple and efficient techniques include a threshold on the energy of the audio signal, a threshold on the number of zero-crossings [11] or combinations of these features, however, these methods are typically not robust to nonstationary environments.", "startOffset": 136, "endOffset": 140}, {"referenceID": 8, "context": "Neural architectures have been proposed for VAD, notably the RNN architecture in [9], however that approach relied on frame-aligned labels.", "startOffset": 81, "endOffset": 84}, {"referenceID": 5, "context": "We use the Connectionist Temporal Classification [6] (CTC) objective function to train an RNN on a corpus of utterance and transcription pairs.", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "The first layer is a 2-dimensional convolution with a stride of three [1].", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "For the next three layers of the network we use gated recurrent RNN layers [3][4].", "startOffset": 75, "endOffset": 78}, {"referenceID": 3, "context": "For the next three layers of the network we use gated recurrent RNN layers [3][4].", "startOffset": 78, "endOffset": 81}, {"referenceID": 1, "context": "5M trainable parameters, comparable to other neural network-based KWS approaches [2], and has been deployed to a modern smartphone.", "startOffset": 81, "endOffset": 84}], "year": 2016, "abstractText": "We propose a single neural network architecture for two tasks: on-line keyword spotting and voice activity detection. We develop novel inference algorithms for an end-to-end Recurrent Neural Network trained with the Connectionist Temporal Classification loss function which allow our model to achieve high accuracy on both keyword spotting and voice activity detection without retraining. In contrast to prior voice activity detection models, our architecture does not require aligned training data and uses the same parameters as the keyword spotting model. This allows us to deploy a high quality voice activity detector with no additional memory or maintenance requirements.", "creator": "LaTeX with hyperref package"}}}