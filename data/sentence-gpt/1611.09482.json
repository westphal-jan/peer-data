{"id": "1611.09482", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "Fast Wavenet Generation Algorithm", "abstract": "This paper presents an efficient implementation of the Wavenet generation process called Fast Wavenet. Compared to a naive implementation that has complexity O(2^L) (L denotes the number of layers in the network), our proposed approach removes redundant convolution operations by caching previous calculations, thereby reducing the complexity to O(L) time. Timing experiments show significant advantages of our fast implementation over a naive one. While this method is presented for Wavenet, the same scheme can be applied anytime one wants to perform autoregressive generation or online prediction using a model with dilated convolution layers. The code for our method is publicly available.\n\n\n\n\nThe paper is available in https://github.com/kylo.org/fastwavenet_learning\n[source: http://www.wavenet.org/projects/fastwavenet-learn-generation-with-vibrant-compression-generation-for-supervised-learning-with-vibrant-compression-gen.html ]\n[source: https://github.com/kylo.org/fastwavenet_learning/download-for-fastwavenet-learning-with-vibrant-compression-gen.html ]", "histories": [["v1", "Tue, 29 Nov 2016 04:16:44 GMT  (249kb,D)", "http://arxiv.org/abs/1611.09482v1", "Technical Report"]], "COMMENTS": "Technical Report", "reviews": [], "SUBJECTS": "cs.SD cs.DS cs.LG", "authors": ["tom le paine", "pooya khorrami", "shiyu chang", "yang zhang", "prajit ramachandran", "mark a hasegawa-johnson", "thomas s huang"], "accepted": false, "id": "1611.09482"}, "pdf": {"name": "1611.09482.pdf", "metadata": {"source": "CRF", "title": "FAST WAVENET GENERATION ALGORITHM", "authors": ["Tom Le Paine", "Pooya Khorrami", "Shiyu Chang", "Yang Zhang", "Prajit Ramachandran", "Mark A. Hasegawa-Johnson", "Thomas S. Huang"], "emails": ["t-huang1}@illinois.edu", "shiyu.chang@ibm.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Wavenet (Oord et al., 2016), a deep generative model of raw audio waveforms, has drawn a tremendous amount of attention since it was first released. It changed existing paradigms in audio generation by directly modeling the raw waveform of audio signals. This has led to state-of-the-art performance in text-to-speech and other general audio generation settings including music.\nWavenet models the conditional probability via a stack of dilated causal convolutional layers for next-sample audio generation given all of the previous samples. At training time, since the audio samples for all timestamps are known, the conditional predictions can be naturally made in parallel. However, when generating audio using a trained model, the predictions are sequential. Every time an output value is predicted, the prediction is then fed back to the input of the network to predict the next sample.\nIn Figure 1, we show a toy Wavenet network used to compute the value of a single output node (A dynamic visualization can be found at DeepMinds blog post1). The input nodes (blue) are the leaves of the tree, and the output is the root. The intermediate computations are the orange nodes. The edges of the graph correspond to matrix multiplications. Since the computation forms a binary tree, the overall computation time for a single output is O(2L), where L is the number of layers in the network. When L is large, this is extremely undesirable.\nThis work fills a missing piece of the original Wavenet paper by providing an efficient implementation for audio generation. The main ingredient of the proposed approach is that we store necessary intermediate computations.\nThe na\u0131\u0308ve implementation in Figure 1 recomputes many variables that have already been computed for previous samples. Note that, though we call the implementation in Figure 1 \u201cna\u0131\u0308ve\u201d, it is the implementation used in previous open source distributions 2. By caching previous computations, we can reduce the computational complexity of generating a single output to O(L). We call our efficient implementation: Fast Wavenet 3.\n1https://deepmind.com/blog/wavenet-generative-model-raw-audio 2https://github.com/ibab/tensorflow-wavenet 3https://github.com/tomlepaine/fast-wavenet\nar X\niv :1\n61 1.\n09 48\n2v 1\n[ cs\n.S D\n] 2\n9 N\nov 2\n01 6\nWhile we present this fast generation scheme for Wavenet, the same scheme can be applied anytime one wants to perform auto-regressive generation or online prediction using a model with dilated convolution layers. For example, the decoder in ByteNet (?) performs auto-regressive generation using dilated convolution layers, therefore our fast generation scheme can be applied."}, {"heading": "2 FAST WAVENET", "text": "The key insight to Fast Wavenet is the following: given a specific set of nodes in the graph, we will have sufficient information to compute the current output. We call these nodes the recurrent states in reference to recurrent neural networks (RNNs) (Graves, 2013). An efficient algorithm can be implemented by caching these recurrent states, instead of recomputing them from scratch every time a new sample is generated."}, {"heading": "2.1 A GRAPHICAL ILLUSTRATION", "text": "The graph displayed in Figure 2 illustrates the idea of the recurrent states. This graph, like the one in Figure 1, shows how a single output sample is generated except now it is in terms of the pre-computed (\u201drecurrent\u201d) states. In fact, upon closer inspection, the reader will notice that the graph shown in Figure 2 looks exactly like a single step of a multi-layer RNN. For some given time t, the incoming input sample (h0e) can be thought of as the \u201dembedding\u201d input and is given the subscript \u2019e\u2019. Similarly, the recurrent states are given subscript \u2019r\u2019. Since these recurrent nodes have already been computed, all we need to do is cache them using a queue. From Figure 2, we see by using cached values, the generation process now has complexity O(L).\nHowever, it should be noted that, due to the dilated convolutions, outputs at each layer will depend on the stored recurrent states computed several time steps back, not the immediate predecessors. Thus, we can use a first-in-first-out queue in each layer to cache the recurrent states that are yet to be used. The number of states cached at each layer is determined by the dilation value of the layer. We provide an example in Figure 3. For the first hidden layer, it has a dilation value of 1, therefore the queue below this layer, denoted (queue0) in the figure, only needs to keep track of 1 value. On the other hand, the output layer has a dilation value of 8, which means the queue housing the previous recurrent states below this layer, denoted as (queue3), is size 8."}, {"heading": "2.2 ALGORITHM", "text": "Our algorithm has two main components:\n\u2022 Generation Model \u2022 Convolution Queues\nThey are shown visually in Figure 4. As we described previously, the generation model resembles and behaves like a single step of a multi-layer RNN. Specifically, it takes in the current input along\nwith a list of recurrent states and produces the current output, along with the new recurrent states. The convolution queues store the recurrent states and are updated when the new recurrent states are computed.\nTo generate audio, we first initialize the generation model using the weights from a pre-trained Wavenet model. Next, we initialize the convolution queues by setting all of their recurrent states to zeros. Then, when generating each output sample, we perform the following steps:\n\u2022 Pop Phase\n\u2022 Push Phase\nDuring the pop phase, the first recurrent state is popped off of each convolution queue and fed into the corresponding location of the generation model. These values along with the current input are used to compute the current output and the new recurrent states. This process is illustrated in Figure 5. Once the new recurrent states have been computed, they are then pushed into their respective queues during the push phase, as shown in Figure 6."}, {"heading": "3 COMPLEXITY ANALYSIS", "text": "In this section, we demonstrate the advantage of our Fast Wavenet algorithm over a na\u0131\u0308ve implementation of the generation process both theoretically and experimentally."}, {"heading": "3.1 THEORETICAL ANALYSIS", "text": "Here we briefly summarize the complexity of both the na\u0131\u0308ve and proposed simplified implementations. In terms of computational complexity, the simplified implementation requires O(L), whereas a previous implementation of the algorithm in Figure 1 requires O(2L).\nIn terms of space complexity, the simplified implementation needs to maintain L queues, which altogether occupy O(2L) additional space. On the other hand, the na\u0131\u0308ve implementation needs to store intermediate hidden outputs. Assuming the intermediate results of the lower hidden layer will be erased after those of the higher layer are computed, the additional space required by the na\u0131\u0308ve implementation is also O(2L). In short, the proposed implementation saves computational complexity dramatically without compromising space complexity.\nIt is also worth mentioning that the proposed implementation scales well to more general architectures. For an architecture with filter width w, and convolution rate of the lth layer rl, assuming r \u2265 w,the proposed implementation requires O(wL) computation and O((w \u2212 1)rL) additional space to generate a new sample, while the na\u0131\u0308ve version requires O(wL) and O(wL\u22121) respectively. The computational complexity differs greatly, but the space complexity remains comparable, especially when r and w are close and small."}, {"heading": "3.2 EXPERIMENTAL ANALYSIS", "text": "We will now compare the speed of our proposed implementation with the na\u0131\u0308ve implementation. In Figure 7, we generated samples from a model containing 2 blocks of L layers each, using the previous implementation and ours. Results are averaged over 100 repeats. When L is small, the na\u0131\u0308ve implementation performs better than expected due to GPU parallelization of the convolution operations. However, when L is large, our efficient implementation starts to significantly outperform the na\u0131\u0308ve method."}, {"heading": "4 CONCLUSIONS", "text": "In this work, we presented Fast Wavenet, an implementation of the Wavenet generation process that greatly reduces computational complexity without sacrificing space complexity. The same fast\ngeneration scheme can be applied anytime one wants to perform auto-regressive generation or online prediction using a model with dilated convolution layers. The authors hope that readers will find the algorithm useful in their future research."}, {"heading": "ACKNOWLEDGMENTS", "text": "Authors would like to thank Wei Han and Yuchen Fan for their insightful discussions."}], "references": [{"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Wavenet: A generative model for raw audio", "author": ["Aaron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1609.03499,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Wavenet (Oord et al., 2016), a deep generative model of raw audio waveforms, has drawn a tremendous amount of attention since it was first released.", "startOffset": 8, "endOffset": 27}, {"referenceID": 0, "context": "We call these nodes the recurrent states in reference to recurrent neural networks (RNNs) (Graves, 2013).", "startOffset": 90, "endOffset": 104}], "year": 2016, "abstractText": "This paper presents an efficient implementation of the Wavenet generation process called Fast Wavenet. Compared to a na\u0131\u0308ve implementation that has complexity O(2) (L denotes the number of layers in the network), our proposed approach removes redundant convolution operations by caching previous calculations, thereby reducing the complexity to O(L) time. Timing experiments show significant advantages of our fast implementation over a na\u0131\u0308ve one. While this method is presented for Wavenet, the same scheme can be applied anytime one wants to perform autoregressive generation or online prediction using a model with dilated convolution layers. The code for our method is publicly available.", "creator": "LaTeX with hyperref package"}}}