{"id": "1611.09830", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "NewsQA: A Machine Comprehension Dataset", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning from top level experts, the general public, and the general public. We invite you to explore these questions in this dataset and submit your questions to our experts to provide them with the answers they require. These questions will include responses to questions from the general public, and responses that are asked by experts at CNN.com. For the first time, this dataset has been available for public viewing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 29 Nov 2016 20:38:07 GMT  (760kb,D)", "http://arxiv.org/abs/1611.09830v1", "Under review for ICLR 2016"], ["v2", "Thu, 22 Dec 2016 18:12:57 GMT  (760kb,D)", "http://arxiv.org/abs/1611.09830v2", "Under review for ICLR 2016"], ["v3", "Tue, 7 Feb 2017 16:27:59 GMT  (760kb,D)", "http://arxiv.org/abs/1611.09830v3", null]], "COMMENTS": "Under review for ICLR 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["adam trischler", "tong wang", "xingdi yuan", "justin harris", "alessandro sordoni", "philip bachman", "kaheer suleman"], "accepted": false, "id": "1611.09830"}, "pdf": {"name": "1611.09830.pdf", "metadata": {"source": "CRF", "title": "NEWSQA: A MACHINE COMPREHENSION DATASET", "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "emails": ["k.suleman}@maluuba.com"], "sections": [{"heading": null, "text": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a fourstage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA."}, {"heading": "1 INTRODUCTION", "text": "Almost all human knowledge is recorded in the language of text. As such, comprehension of written language by machines, at a near-human level, would enable a broad class of artificial intelligence applications. In human students we evaluate reading comprehension by posing questions based on a text passage and then assessing a student\u2019s answers. Such comprehension tests are appealing because they are objectively gradable and may measure a range of important abilities, from basic understanding to causal reasoning to inference (Richardson et al., 2013). To teach literacy to machines, the research community has taken a similar approach with machine comprehension (MC).\nRecent years have seen the release of a host of MC datasets. Generally, these consist of (document, question, answer) triples to be used in a supervised learning framework. Existing datasets vary in size, difficulty, and collection methodology; however, as pointed out by Rajpurkar et al. (2016), most suffer from one of two shortcomings: those that are designed explicitly to test comprehension (Richardson et al., 2013) are too small for training data-intensive deep learning models, while those that are sufficiently large for deep learning (Hermann et al., 2015; Hill et al., 2016; Bajgar et al., 2016) are generated synthetically, yielding questions that are not posed in natural language and that may not test comprehension directly (Chen et al., 2016). More recently, Rajpurkar et al. (2016) sought to overcome these deficiencies with their crowdsourced dataset, SQuAD.\nHere we present a challenging new largescale dataset for machine comprehension: NewsQA. NewsQA contains 119,633 natural language questions posed by crowdworkers on 12,744 news articles from CNN. Answers to these questions consist in spans of text within the corresponding article highlighted by a distinct set of crowdworkers. To build NewsQA we utilized a four-stage collection process designed to encourage exploratory, curiosity-based questions that reflect human information seeking. CNN articles were chosen as the source material because they have been used in the past (Hermann et al., 2015) and, in our view, machine comprehension systems are particularly suited to high-volume, rapidly changing information sources like news.\n\u2217These three authors contributed equally.\nar X\niv :1\n61 1.\n09 83\n0v 1\n[ cs\n.C L\n] 2\n9 N\nov 2\n01 6\nAs Trischler et al. (2016a), Chen et al. (2016), and others have argued, it is important for datasets to be sufficiently challenging to teach models the abilities we wish them to learn. Thus, in line with Richardson et al. (2013), our goal with NewsQA was to construct a corpus of questions that necessitates reasoning mechanisms, such as synthesis of information across different parts of an article. We designed our collection methodology explicitly to capture such questions.\nThe challenging characteristics of NewsQA that distinguish it from most previous comprehension tasks are as follows:\n1. Answers are spans of arbitrary length within an article, rather than single words or entities.\n2. Some questions have no answer in the corresponding article (the null span).\n3. There are no candidate answers from which to choose.\n4. Our collection process encourages lexical and syntactic divergence between questions and answers.\n5. A significant proportion of questions requires reasoning beyond simple word- and contextmatching (as shown in our analysis).\nIn this paper we describe the collection methodology for NewsQA, provide a variety of statistics to characterize it and contrast it with previous datasets, and assess its difficulty. In particular, we measure human performance and compare it to that of two strong neural-network baselines. Unsurprisingly, humans significantly outperform the models we designed and assessed, achieving an F1 score of 74.9% versus 49.6% for the best-performing machine. We hope that this corpus will spur further advances on the challenging task of machine comprehension."}, {"heading": "2 RELATED DATASETS", "text": "NewsQA follows in the tradition of several recent comprehension datasets. These vary in size, difficulty, and collection methodology, and each has its own distinguishing characteristics. We agree with Bajgar et al. (2016) who have said \u201cmodels could certainly benefit from as diverse a collection of datasets as possible.\u201d We discuss this collection below."}, {"heading": "2.1 MCTEST", "text": "MCTest (Richardson et al., 2013) is a crowdsourced collection of 660 elementary-level children\u2019s stories with associated questions and answers. The stories are fictional, to ensure that the answer must be found in the text itself, and carefully limited to what a young child can understand. Each question comes with a set of 4 candidate answers that range from single words to full explanatory sentences. The questions are designed to require rudimentary reasoning and synthesis of information across sentences, making the dataset quite challenging. This is compounded by the dataset\u2019s size, which limits the training of expressive statistical models. Nevertheless, recent comprehension models have performed well on MCTest (Sachan et al., 2015; Wang et al., 2015), including a highly structured neural model (Trischler et al., 2016a). These models all rely on access to the small set of candidate answers, a crutch that NewsQA does not provide."}, {"heading": "2.2 CNN/DAILY MAIL", "text": "The CNN/Daily Mail corpus (Hermann et al., 2015) consists of news articles scraped from those outlets with corresponding cloze-style questions. Cloze questions are constructed synthetically by deleting a single entity from abstractive summary points that accompany each article (written presumably by human authors). As such, determining the correct answer relies mostly on recognizing textual entailment between the article and the question. The named entities within an article are identified and anonymized in a preprocessing step and constitute the set of candidate answers; contrast this with NewsQA in which answers often include longer phrases and no candidates are given.\nBecause the cloze process is automatic, it is straightforward to collect a significant amount of data to support deep-learning approaches: CNN/Daily Mail contains about 1.4 million question-answer pairs. However, Chen et al. (2016) demonstrated that the task requires only limited reasoning and, in\nfact, performance of the strongest models (Kadlec et al., 2016; Trischler et al., 2016b; Sordoni et al., 2016) nearly matches that of humans."}, {"heading": "2.3 CHILDREN\u2019S BOOK TEST", "text": "The Children\u2019s Book Test (CBT) (Hill et al., 2016) was collected using a process similar to that of CNN/Daily Mail. Text passages are 20-sentence excerpts from children\u2019s books available through Project Gutenberg; questions are generated by deleting a single word in the next (i.e., 21st) sentence. Consequently, CBT evaluates word prediction based on context. It is a comprehension task insofar as comprehension is likely necessary for this prediction, but comprehension may be insufficient and other mechanisms may be more important."}, {"heading": "2.4 BOOKTEST", "text": "Bajgar et al. (2016) convincingly argue that, because existing datasets are not large enough, we have yet to reach the full capacity of existing comprehension models. As a remedy they present BookTest. This is an extension to the named-entity and common-noun strata of CBT that increases their size by over 60 times. Bajgar et al. (2016) demonstrate that training on the augmented dataset yields a model (Kadlec et al., 2016) that matches human performance on CBT. This is impressive and suggests that much is to be gained from more data, but we repeat our concerns about the relevance of story prediction as a comprehension task. We also wish to encourage more efficient learning from less data."}, {"heading": "2.5 SQUAD", "text": "The comprehension dataset most closely related to NewsQA is SQuAD (Rajpurkar et al., 2016). It consists of natural language questions posed by crowdworkers on paragraphs from high-PageRank Wikipedia articles. As in NewsQA, each answer consists of a span of text from the related paragraph and no candidates are provided. Despite the effort of manual labelling, SQuAD\u2019s size is significant and amenable to deep learning approaches: 107,785 question-answer pairs based on 536 articles.\nSQuAD is a challenging comprehension task in which humans far outperform machines. The authors measured human accuracy at 90.5% F1 (we measured human F1 at 82.0% using a different methodology), whereas the strongest published model to date achieves only 70.0% F1 (Wang & Jiang, 2016b)."}, {"heading": "3 COLLECTION METHODOLOGY", "text": "We collected NewsQA through a four-stage process: article curation, question sourcing, answer sourcing, and validation. We also applied a post-processing step with answer agreement consolidation and span merging to enhance the usability of the dataset."}, {"heading": "3.1 ARTICLE CURATION", "text": "We retrieve articles from CNN using the script created by Hermann et al. (2015) for CNN/Daily Mail. From the returned set of 90,266 articles, we select 12,744 uniformly at random. These cover a wide range of topics that includes politics, economics, and current events. Articles are partitioned at random into a training set (90%), a development set (5%), and a test set (5%)."}, {"heading": "3.2 QUESTION SOURCING", "text": "It was important to us to collect challenging questions that could not be answered using straightforward word- or context-matching. Like Richardson et al. (2013) we want to encourage reasoning in comprehension models. We are also interested in questions that, in some sense, model human curiosity and reflect actual human use-cases of information seeking. Along a similar line, we consider it an important (though as yet overlooked) capacity of a comprehension model to recognize when given information is inadequate, so we are also interested in questions that may not have sufficient evidence in the text. Our question sourcing stage was designed to solicit questions of this nature, and deliberately separated from the answer sourcing stage for the same reason.\nQuestioners (a distinct set of crowdworkers) see only a news article\u2019s headline and its summary points (also available from CNN); they do not see the full article itself. They are asked to formulate a question from this incomplete information. This encourages curiosity about the contents of the full article and prevents questions that are simple reformulations of sentences in the text. It also increases the likelihood of questions whose answers do not exist in the text. We reject questions that have significant word overlap with the summary points to ensure that crowdworkers do not treat the summaries as mini-articles, and further discouraged this in the instructions. During collection each Questioner is solicited for up to three questions about an article. They are provided with positive and negative examples to prompt and guide them (detailed instructions are shown in Figure 3)."}, {"heading": "3.3 ANSWER SOURCING", "text": "A second set of crowdworkers (Answerers) provide answers. Although this separation of question and answer increases the overall cognitive load, we hypothesized that unburdening Questioners in this way would encourage more complex questions. Answerers receive a full article along with a crowdsourced question and are tasked with determining the answer. They may also reject the question as nonsensical, or select the null answer if the article contains insufficient information. Answers are submitted by clicking on and highlighting words in the article while instructions encourage the set of answer words to consist in a single continuous span (again, we give an example prompt in the Appendix). For each question we solicit answers from multiple crowdworkers with the aim of achieving agreement between at least two Answerers."}, {"heading": "3.4 VALIDATION", "text": "Crowdsourcing is a powerful tool but it is not without peril (collection glitches; uninterested or malicious workers). To obtain a dataset of the highest possible quality we use a validation process that mitigates some of these issues. In validation, a third set of crowdworkers sees the full article, a question, and the set of unique answers to that question. We task these workers with choosing the best answer from the candidate set or rejecting all answers. Validation was used on those questions without answer-agreement after the previous stage, amounting to 43.2% of all questions."}, {"heading": "3.5 ANSWER MARKING AND CLEANUP", "text": "After validation, 86.0% of all questions in NewsQA have answers agreed upon by at least two separate crowdworkers\u2014either at the initial answer sourcing stage or in the top-answer selection. This improves the dataset\u2019s quality. We choose to include the questions without agreed answers in the corpus also, but they are specially marked. Such questions could be treated as having the null answer and used to train models that are aware of poorly posed questions.\nAs a final cleanup step we combine answer spans that are less than 3 words apart (punctuation is discounted). We find that 5.68% of answers consist in multiple spans, while 71.3% of multi-spans are within the 3-word threshold. Looking more closely at the data reveals that the multi-span answers often represent lists. These may present an interesting challenge for comprehension models moving forward."}, {"heading": "4 DATA ANALYSIS", "text": "We provide a thorough analysis of NewsQA to demonstrate its challenge and its usefulness as a machine comprehension benchmark. The analysis focuses on the types of answers that appear in the dataset and the various forms of reasoning required to solve it."}, {"heading": "4.1 ANSWER TYPES", "text": "Following Rajpurkar et al. (2016), we categorize answers based on their linguistic type (see Table 1). This categorization relies on Stanford CoreNLP to generate constituency parses, POS tags, and NER tags for answer spans (see Rajpurkar et al. (2016) for more details). From the table we see that the majority of answers (23.1%) are common noun phrases. Thereafter, answers are fairly evenly\nspread among the person (14.4%), clause phrase (14.0%), numeric (12.0%), and other (11.1%) types. Clearly, answers in NewsQA are linguistically diverse.\nThe proportions in Table 1 only account for cases when an answer span exists. The complement of this set comprises questions with an agreed null answer (5.8% of the full corpus) and answers without agreement after validation (4.5% of the full corpus)."}, {"heading": "4.2 REASONING TYPES", "text": "The forms of reasoning required to solve NewsQA directly influence the abilities that models will learn from the dataset. We stratified reasoning types using a variation on the taxonomy presented by Chen et al. (2016) in their analysis of the CNN/Daily Mail dataset. Types are as follows, in ascending order of difficulty:\n1. Word Matching: Important words in the question exactly match words in the answer span such that a keyword search algorithm could perform well on this subset.\n2. Paraphrasing: A single sentence in the article entails or paraphrases the question. Paraphrase recognition may require synonymy and word knowledge.\n3. Inference: The answer must be inferred from incomplete information in the article or by recognizing conceptual overlap. This typically draws on world knowledge.\n4. Synthesis: The answer can only be inferred by synthesizing information distributed across multiple sentences.\n5. Ambiguous/Insufficient: The question has no answer or no unique answer in the article.\nWe manually labelled 500 examples (drawn randomly from the development set) according to these types and compiled the results in Table 2. Some examples fall into more than one category, in which case we defaulted to the more challenging type. We can see from the table that word matching, the easiest type, makes up the largest subset of the data (31.6%). However, the more difficult reasoning forms collectively outnumber word matching by a significant margin: cumulatively, paraphrasing, synthesis, and inference make up 58.6% of the data."}, {"heading": "5 BASELINE MODELS", "text": "We test the performance of three comprehension systems on NewsQA: human data analysts and two neural models. The first neural model is the match-LSTM (mLSTM) system of Wang & Jiang (2016b). The second is a model of our own design that is computationally cheaper. We describe these models below but omit the personal details of our analysts. Implementation details of the models are described in Appendix A."}, {"heading": "5.1 MATCH-LSTM", "text": "There are three stages involved in the mLSTM model. First, LSTM networks encode the document and question (represented by GloVe word embeddings (Pennington et al., 2014)) as sequences of hidden states. Second, an mLSTM network (Wang & Jiang, 2016a) compares the document encodings with the question encodings. This network processes the document sequentially and at each token uses an attention mechanism to obtain a weighted vector representation of the question; the weighted combination is concatenated with the encoding of the current token and fed into a standard LSTM. Finally, a Pointer Network uses the hidden states of the mLSTM to select the boundaries of the answer span. We refer the reader to Wang & Jiang (2016a;b) for full details. At the time of writing, mLSTM is state-of-the-art on SQuAD (see Table 3) so it is natural to test it further on NewsQA."}, {"heading": "5.2 THE BILINEAR ANNOTATION RE-ENCODING BOUNDARY (BARB) MODEL", "text": "The match-LSTM is computationally intensive since it computes an attention over the entire question at each document token in the recurrence. To facilitate faster experimentation with NewsQA we developed a lighter-weight model (BARB) that achieves similar results on SQuAD. Our model consists in four stages:\nEncoding All words in the document and question are mapped to real-valued vectors using the GloVe embedding matrix W \u2208 R|V |\u00d7d. This yields d1, . . . ,dn \u2208 Rd and q1, . . . ,qm \u2208 Rd. A bidirectional GRU network (Bahdanau et al., 2015) takes in di and encodes contextual states hi \u2208 RD1 for the document. The same encoder is applied to qj to derive contextual states kj \u2208 RD1 for the question.1\nBilinear Annotation Next we compare the document and question encodings using a set of C bilinear transformations,\ngij = h T i T [1:C]kj , T c \u2208 RD1\u00d7D1 , gij \u2208 RC ,\nwhich we use to produce an (n\u00d7m\u00d7 C)-dimensional tensor of annotation scores, G = [gij ]. We take the maximum over the question-token (second) dimension and call the columns of the resulting matrix gi \u2208 RC . We use this matrix as an annotation over the document word dimension. Contrasting the multiplicative application of attention vectors, this annotation matrix is to be concatenated to the encoder RNN input in the re-encoding stage.\nRe-encoding For each document word, the input of the re-encoding RNN (another biGRU network) consists of three components: the document encodings hi, the annotation vectors gi, and a binary feature qi indicating whether the document word appears in the question. The resulting vectors\n1A bidirectional GRU concatenates the hidden states of two GRU networks running in opposite directions. Each of these has hidden size 1\n2 D1.\nfi = [hi;gi; qi] are fed into the re-encoding RNN to produce D2-dimensional encodings ei as input in the boundary-pointing stage.\nBoundary pointing Finally, we search for the boundaries of the answer span using a convolutional network (in a process similar to edge detection). Encodings ei are arranged in matrix E \u2208 RD2\u00d7n. E is convolved with a bank of nf filters, F`k \u2208 RD2\u00d7w, where w is the filter width, k indexes the different filters, and ` indexes the layer of the convolutional network. Each layer has the same number of filters of the same dimensions. We add a bias term and apply a nonlinearity (ReLU) following each convolution, with the result an (nf \u00d7 ns)-dimensional matrix B`. We use two convolutional layers in the boundary-pointing stage. Given B1 and B2, the answer span\u2019s start- and end-location probabilities are computed using p(s) \u221d exp ( vTs B1 + bs ) and p(e) \u221d\nexp ( vTe B2 + be ) , respectively. We also concatenate p(s) to the input of the second convolutional layer (along the nf -dimension) so as to condition the end-boundary pointing on the start-boundary. Vectors vs, ve \u2208 Rnf and scalars bs, be \u2208 R are trainable parameters. We also provide an intermediate level of \u201cguidance\u201d to the annotation mechanism by first reducing the feature dimension C in G with mean-pooling, then maximizing the softmax probabilities in the resulting (n-dimensional) vector corresponding to the answer word positions in each document. This auxiliary task is observed empirically to improve performance."}, {"heading": "6 EXPERIMENTS2", "text": ""}, {"heading": "6.1 HUMAN EVALUATION", "text": "We tested two near-native English speakers on 100 questions each from the NewsQA development set. As given in Table 3, they averaged 74.9% F1, which likely represents a ceiling for machine performance. Our students\u2019 exact match (EM) scores are relatively low at 55.0%. This is because in many cases there are multiple ways to select the same answer, e.g., \u201c1996\u201d versus \u201cin 1996\u201d. We also compared human performance on the answers that had agreement with and without validation, finding a difference of only 1.4 percentage points F1. This suggests our validation stage yields good-quality answers.\nThe original SQuAD evaluation of human performance compares separate answers given by crowdworkers; for a closer comparison with NewsQA, we replicated our human test using the same \u201cstudents\u201d. We measured their answers against the second group of crowdsourced responses in SQuAD\u2019s development set, as in Rajpurkar et al. (2016). Our students scored 82.0% F1."}, {"heading": "6.2 MODEL PERFORMANCE", "text": "Performance (measured by EM and F1 with the official evaluation script from SQuAD) of the baseline models and humans is listed in Table 3. For NewsQA we use the same hyperparameters that gave the best performance on SQuAD. The gap between human and machine performance on NewsQA is a striking 25.3 points F1 \u2014 much larger than the gap on SQuAD (11.1% or 19.6%, depending on the human evaluation method). The gaps suggest a large margin for improvement with automated methods.\nFigure 1 stratifies model performance according to answer type (left) and reasoning type (right) as defined in Sections 4.1 and 4.2, respectively. The answer-type stratification suggests that the model is better at pointing to named entities. The reasoning-type stratification, on the other hand, shows that questions requiring inference and synthesis are, not surprisingly, more difficult for the model."}, {"heading": "6.3 SENTENCE-LEVEL SCORING", "text": "We propose a simple sentence-level subtask to demonstrate quantitatively the relative difficulty of NewsQA. Given a document and a question, the goal is to find the sentence containing the answer\n2All experiments in this section use the subset of NewsQA dataset with answer agreements (92,487 samples for training, 5,103 for validation, and 5,251 for testing). We leave the challenge of identifying the unanswerable questions for future work.\nTable 3: Performance of several methods and humans on the SQuAD and NewsQA datasets.\nSQuAD Exact Match (%) F1 (%)\nModel Dev Test Dev Test\nRandom1 1.1 1.3 4.1 4.3 mLSTM2 59.1 59.5 70.0 70.3 BARB 59.1 - 70.9 -\nHuman1 80.3 77.0 90.5 86.8 Human (ours) 70.5 - 82.0 -\nNewsQA Exact Match (%) F1 (%)\nModel Dev Test Dev Test\nRandom 0.0 0.0 3.0 3.0 mLSTM 35.2 33.4 48.9 48.0 BARB 36.1 34.1 49.6 48.2\nHuman 55.0 - 74.9 -\nPerson\nNumeric\nDate/time\nLocation\nOther Other entity Common noun\nClause Prepositional\nAdjective\nVerb\n0 0.175 0.35 0.525 0.7\nF1 EM\nWord Matching\nParaphrasing\nInference\nSynthesis\nAmbiguous/ Insufficient\n0 0.2 0.4 0.6 0.8\nF1 EM\nFigure 1: Performance stratification by answer type (left, full development set) and reasoning type (right, 500 human-assessed development questions).\nspan. We hypothesize that techniques like word-matching are inadequate to this task owing to the more involved reasoning required by NewsQA.\nTo solve the sentence-level task we employ a technique that resembles inverse document frequency (idf ), which we call inverse sentence frequency (isf ). Given a document sentence Si and the corresponding questionQ, the isf score is given by the sum of the idf scores of the words common to Si and Q (each sentence is treated as a document for the idf computation). The sentence with the highest isf is taken as the answer sentence S\u2217, that is,\nS\u2217 = argmax i \u2211 w\u2208Si\u2229Q idf (w).\nThe isf method achieves an impressive 79.2% sentence-level accuracy on SQuAD\u2019s development set but only 35.4% accuracy on NewsQA\u2019s development set, highlighting the comparative difficulty of the latter. This likely owes partly to the length of documents in the respective corpora: 4.9 sentences on average for SQuAD versus 30.7 sentences on average for NewsQA."}, {"heading": "7 CONCLUSION", "text": "We have introduced a challenging new comprehension dataset: NewsQA. We collected the 100,000+ examples of NewsQA using teams of crowdworkers, who variously read CNN articles or highlights, posed questions about them, and determined answers. Our methodology yields diverse answer types and a significant proportion of questions that require some reasoning ability to solve. This makes the corpus challenging, as confirmed by the large performance gap between humans and deep neural models (25.3percentage points F1). By its size and complexity, NewsQA makes a significant extension to the existing body of comprehension datasets. We hope that our corpus will spur further advances in machine comprehension and guide the development of literate artificial intelligence."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank \u00c7ag\u0306lar G\u00fcl\u00e7ehre, Sandeep Subramanian and Saizheng Zhang for helpful discussions, and Pranav Subramani for the graphs."}, {"heading": "B DATA COLLECTION USER INTERFACE", "text": "Here we present the user interfaces used in question sourcing, answer sourcing, and question/answer validation."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Embracing data abundance: Booktest dataset for reading comprehension", "author": ["Ondrej Bajgar", "Rudolf Kadlec", "Jan Kleindienst"], "venue": "arXiv preprint arXiv:1610.00956,", "citeRegEx": "Bajgar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bajgar et al\\.", "year": 2016}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. WardeFarley", "Y. Bengio"], "venue": "Proc. of SciPy,", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "A thorough examination of the cnn / daily mail reading comprehension", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst"], "venue": "arXiv preprint arXiv:1603.01547,", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "ICML (3),", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "venue": "arXiv preprint arXiv:1606.05250,", "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher JC Burges", "Erin Renshaw"], "venue": "In EMNLP,", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Learning answerentailing structures for machine comprehension", "author": ["Mrinmaya Sachan", "Avinava Dubey", "Eric P Xing", "Matthew Richardson"], "venue": "In Proceedings of ACL,", "citeRegEx": "Sachan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sachan et al\\.", "year": 2015}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Iterative alternating neural attention for machine reading", "author": ["Alessandro Sordoni", "Philip Bachman", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1606.02245,", "citeRegEx": "Sordoni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2016}, {"title": "A parallelhierarchical model for machine comprehension on sparse data", "author": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Jing He", "Philip Bachman", "Kaheer Suleman"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Trischler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Natural language comprehension with the epireader", "author": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Kaheer Suleman"], "venue": "In EMNLP,", "citeRegEx": "Trischler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Machine comprehension with syntax, frames, and semantics", "author": ["Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David McAllester"], "venue": "In Proceedings of ACL, Volume 2: Short Papers,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Learning natural language inference with lstm", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "NAACL,", "citeRegEx": "Wang and Jiang.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "arXiv preprint arXiv:1608.07905,", "citeRegEx": "Wang and Jiang.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "Such comprehension tests are appealing because they are objectively gradable and may measure a range of important abilities, from basic understanding to causal reasoning to inference (Richardson et al., 2013).", "startOffset": 183, "endOffset": 208}, {"referenceID": 12, "context": "(2016), most suffer from one of two shortcomings: those that are designed explicitly to test comprehension (Richardson et al., 2013) are too small for training data-intensive deep learning models, while those that are sufficiently large for deep learning (Hermann et al.", "startOffset": 107, "endOffset": 132}, {"referenceID": 5, "context": ", 2013) are too small for training data-intensive deep learning models, while those that are sufficiently large for deep learning (Hermann et al., 2015; Hill et al., 2016; Bajgar et al., 2016) are generated synthetically, yielding questions that are not posed in natural language and that may not test comprehension directly (Chen et al.", "startOffset": 130, "endOffset": 192}, {"referenceID": 6, "context": ", 2013) are too small for training data-intensive deep learning models, while those that are sufficiently large for deep learning (Hermann et al., 2015; Hill et al., 2016; Bajgar et al., 2016) are generated synthetically, yielding questions that are not posed in natural language and that may not test comprehension directly (Chen et al.", "startOffset": 130, "endOffset": 192}, {"referenceID": 1, "context": ", 2013) are too small for training data-intensive deep learning models, while those that are sufficiently large for deep learning (Hermann et al., 2015; Hill et al., 2016; Bajgar et al., 2016) are generated synthetically, yielding questions that are not posed in natural language and that may not test comprehension directly (Chen et al.", "startOffset": 130, "endOffset": 192}, {"referenceID": 3, "context": ", 2016) are generated synthetically, yielding questions that are not posed in natural language and that may not test comprehension directly (Chen et al., 2016).", "startOffset": 140, "endOffset": 159}, {"referenceID": 5, "context": "CNN articles were chosen as the source material because they have been used in the past (Hermann et al., 2015) and, in our view, machine comprehension systems are particularly suited to high-volume, rapidly changing information sources like news.", "startOffset": 88, "endOffset": 110}, {"referenceID": 7, "context": "Existing datasets vary in size, difficulty, and collection methodology; however, as pointed out by Rajpurkar et al. (2016), most suffer from one of two shortcomings: those that are designed explicitly to test comprehension (Richardson et al.", "startOffset": 99, "endOffset": 123}, {"referenceID": 1, "context": ", 2016; Bajgar et al., 2016) are generated synthetically, yielding questions that are not posed in natural language and that may not test comprehension directly (Chen et al., 2016). More recently, Rajpurkar et al. (2016) sought to overcome these deficiencies with their crowdsourced dataset, SQuAD.", "startOffset": 8, "endOffset": 221}, {"referenceID": 14, "context": "As Trischler et al. (2016a), Chen et al.", "startOffset": 3, "endOffset": 28}, {"referenceID": 3, "context": "(2016a), Chen et al. (2016), and others have argued, it is important for datasets to be sufficiently challenging to teach models the abilities we wish them to learn.", "startOffset": 9, "endOffset": 28}, {"referenceID": 3, "context": "(2016a), Chen et al. (2016), and others have argued, it is important for datasets to be sufficiently challenging to teach models the abilities we wish them to learn. Thus, in line with Richardson et al. (2013), our goal with NewsQA was to construct a corpus of questions that necessitates reasoning mechanisms, such as synthesis of information across different parts of an article.", "startOffset": 9, "endOffset": 210}, {"referenceID": 1, "context": "We agree with Bajgar et al. (2016) who have said \u201cmodels could certainly benefit from as diverse a collection of datasets as possible.", "startOffset": 14, "endOffset": 35}, {"referenceID": 12, "context": "MCTest (Richardson et al., 2013) is a crowdsourced collection of 660 elementary-level children\u2019s stories with associated questions and answers.", "startOffset": 7, "endOffset": 32}, {"referenceID": 13, "context": "Nevertheless, recent comprehension models have performed well on MCTest (Sachan et al., 2015; Wang et al., 2015), including a highly structured neural model (Trischler et al.", "startOffset": 72, "endOffset": 112}, {"referenceID": 18, "context": "Nevertheless, recent comprehension models have performed well on MCTest (Sachan et al., 2015; Wang et al., 2015), including a highly structured neural model (Trischler et al.", "startOffset": 72, "endOffset": 112}, {"referenceID": 5, "context": "The CNN/Daily Mail corpus (Hermann et al., 2015) consists of news articles scraped from those outlets with corresponding cloze-style questions.", "startOffset": 26, "endOffset": 48}, {"referenceID": 3, "context": "However, Chen et al. (2016) demonstrated that the task requires only limited reasoning and, in", "startOffset": 9, "endOffset": 28}, {"referenceID": 7, "context": "fact, performance of the strongest models (Kadlec et al., 2016; Trischler et al., 2016b; Sordoni et al., 2016) nearly matches that of humans.", "startOffset": 42, "endOffset": 110}, {"referenceID": 15, "context": "fact, performance of the strongest models (Kadlec et al., 2016; Trischler et al., 2016b; Sordoni et al., 2016) nearly matches that of humans.", "startOffset": 42, "endOffset": 110}, {"referenceID": 6, "context": "The Children\u2019s Book Test (CBT) (Hill et al., 2016) was collected using a process similar to that of CNN/Daily Mail.", "startOffset": 31, "endOffset": 50}, {"referenceID": 7, "context": "(2016) demonstrate that training on the augmented dataset yields a model (Kadlec et al., 2016) that matches human performance on CBT.", "startOffset": 73, "endOffset": 94}, {"referenceID": 11, "context": "The comprehension dataset most closely related to NewsQA is SQuAD (Rajpurkar et al., 2016).", "startOffset": 66, "endOffset": 90}, {"referenceID": 5, "context": "We retrieve articles from CNN using the script created by Hermann et al. (2015) for CNN/Daily Mail.", "startOffset": 58, "endOffset": 80}, {"referenceID": 12, "context": "Like Richardson et al. (2013) we want to encourage reasoning in comprehension models.", "startOffset": 5, "endOffset": 30}, {"referenceID": 11, "context": "Following Rajpurkar et al. (2016), we categorize answers based on their linguistic type (see Table 1).", "startOffset": 10, "endOffset": 34}, {"referenceID": 11, "context": "Following Rajpurkar et al. (2016), we categorize answers based on their linguistic type (see Table 1). This categorization relies on Stanford CoreNLP to generate constituency parses, POS tags, and NER tags for answer spans (see Rajpurkar et al. (2016) for more details).", "startOffset": 10, "endOffset": 252}, {"referenceID": 3, "context": "We stratified reasoning types using a variation on the taxonomy presented by Chen et al. (2016) in their analysis of the CNN/Daily Mail dataset.", "startOffset": 77, "endOffset": 96}, {"referenceID": 10, "context": "First, LSTM networks encode the document and question (represented by GloVe word embeddings (Pennington et al., 2014)) as sequences of hidden states.", "startOffset": 92, "endOffset": 117}, {"referenceID": 0, "context": "A bidirectional GRU network (Bahdanau et al., 2015) takes in di and encodes contextual states hi \u2208 R1 for the document.", "startOffset": 28, "endOffset": 51}, {"referenceID": 11, "context": "We measured their answers against the second group of crowdsourced responses in SQuAD\u2019s development set, as in Rajpurkar et al. (2016). Our students scored 82.", "startOffset": 111, "endOffset": 135}], "year": 2016, "abstractText": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a fourstage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "creator": "LaTeX with hyperref package"}}}