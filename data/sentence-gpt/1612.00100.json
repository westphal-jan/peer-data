{"id": "1612.00100", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "Noise-Tolerant Life-Long Matrix Completion via Adaptive Sampling", "abstract": "We study the problem of recovering an incomplete $m\\times n$ matrix of rank $r$ with columns arriving online over time. This is known as the problem of life-long matrix completion, and is widely applied to recommendation system, computer vision, system identification, etc. The challenge is to design provable algorithms tolerant to a large amount of noises, with small sample complexity. In this work, we give algorithms achieving strong guarantee under two realistic noise models. In bounded deterministic noise, an adversary can add any bounded yet unstructured noise to each column. For this problem, we present an algorithm that returns a matrix of a small error, with sample complexity almost as small as the best prior results in the noiseless case. For sparse random noise, where the corrupted columns are sparse and drawn randomly, we give an algorithm that exactly recovers an $\\mu_0$-incoherent matrix by probability at least $1-\\delta$ with sample complexity as small as $O\\left(\\mu_0rn\\log (r/\\delta)\\right)$. This result advances the state-of-the-art work and matches the lower bound in a worst case. We also study the scenario where the hidden matrix lies on a mixture of subspaces and show that the sample complexity can be even smaller. Our proposed algorithms perform well experimentally in both synthetic and real-world datasets.\n\n\n\nThe goal of this paper is to explore two major assumptions about how a finite matrix is able to find optimal values of real-time noise, the value of the matrix, and a particular threshold of error, when it is able to distinguish among the best possible real-time noise sources on a large, data-driven matrix. This paper proposes that, with the help of data from a small dataset, the optimal value of the matrix can be divided into a given threshold: 1, and 2. The difference between the two approaches is as small as 2. The difference is due to a small threshold and a subset of the noise sources, which are more expensive, and the result is even more costly. The algorithm can use different threshold estimates to estimate the quality of noise from a single dataset, and by applying similar approaches, the goal becomes clear: one that is more expensive is the average quality of the noise sources.\nThe goal of this paper is to develop the best algorithm for all these factors. It is possible to achieve a robust algorithm that allows users to estimate the average noise. It is possible to obtain", "histories": [["v1", "Thu, 1 Dec 2016 01:10:07 GMT  (822kb,D)", "http://arxiv.org/abs/1612.00100v1", "24 pages, 5 figures in NIPS 2016"]], "COMMENTS": "24 pages, 5 figures in NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["maria-florina balcan", "hongyang zhang"], "accepted": true, "id": "1612.00100"}, "pdf": {"name": "1612.00100.pdf", "metadata": {"source": "CRF", "title": "Noise-Tolerant Life-Long Matrix Completion via Adaptive Sampling", "authors": ["Maria-Florina Balcan", "Hongyang Zhang"], "emails": ["ninamf@cs.cmu.edu", "hongyanz@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Life-long learning is an emerging object of study in machine learning, statistics, and many other domains [BBV15, CBK+10]. In machine learning, study of such a framework has led to significant advances in learning systems that continually learn many tasks over time and improve their ability to learn as they do so, like humans [GMK01]. A natural approach to achieve this goal is to exploit information from previouslylearned tasks under the belief that some commonalities exist across the tasks [BBV15, WK08]. The focus of this work is to apply this idea of life-long learning to the matrix completion problem. That is, given columns of a matrix that arrive online over time with missing entries, how to approximately/exactly recover the underlying matrix by exploiting the low-rank commonality across each column.\nOur study is motivated by several promising applications where life-long matrix completion is applicable. In recommendation systems, the column of the hidden matrix consists of ratings by multiple users to a specific movie/news; The news or movies are updated online over time but usually only a few ratings are submitted by those users. In computer vision, inferring camera motion from a sequence of online arriving images with missing pixels has received significant attention in recent years, known as the structure-from-motion problem; Recovering those missing pixels from those partial measurements is an important preprocessing\nar X\niv :1\n61 2.\n00 10\n0v 1\n[ cs\n.L G\n] 1\nD ec\nstep. Other examples where our technique is applicable include system identification, multi-class learning, global positioning of sensors, etc.\nDespite a large amount of applications of life-long matrix completion, many fundamental questions remain unresolved. One of the long-standing challenges is designing noise-tolerant, life-long algorithms that can recover the unknown target matrix with small error. In the absence of noise, this problem is not easy because the overall structure of the low rankness is unavailable in each round. This problem is even more challenging in the context of noise, where an adversary can add any bounded yet unstructured noise to those observations and the error propagates as the algorithm proceeds. This is known as bounded deterministic noise. Another type of noise model that receives great attention is sparse random noise, where the noise is sparse compared to the number of columns and is drawn i.i.d. from a non-degenerate distribution.\nOur Contributions: This paper tackles the problem of noise-tolerant, life-long matrix completion and advances the state-of-the-art results under the two realistic noise models.\n\u2022 Under bounded deterministic noise, we design and analyze an algorithm that is robust to noise, with only a small output error (See Figure 4). The sample complexity is almost as small as the best prior results in the noiseless case, provided that the noise level is small.\n\u2022 Under sparse random noise, we give sample complexity that guarantees an exact recovery of the hidden matrix with high probability. The sample complexity advances the state-of-the-art results (See Figure 4) and matches the lower bound in the worst case of this scenario.\n\u2022 We extend our result of sparse random noise to the setting where the columns of the hidden matrix lie on a mixture of subspaces, and show that smaller sample complexity suffices to exactly recover the hidden matrix in this more benign setting.\n\u2022 We also show that our proposed algorithms perform well experimentally in both synthetic and realworld datasets."}, {"heading": "2 Preliminaries", "text": "Before proceeding, we define some notations and clarify problem setup in this section.\nNotations: We will use bold capital letter to represent matrix, bold lower-case letter to represent vector, and lower-case letter to represent scalar. Specifically, we denote by M \u2208 Rm\u00d7n the noisy observation matrix in hindsight. We denote by L the underlying clean matrix, and by E the noise. We will frequently use M:t \u2208 Rm\u00d71 to indicate the t-th column of matrix M, and similarly Mt: \u2208 R1\u00d7n the t-th row. For any set of indices \u2126, M\u2126: \u2208 R|\u2126|\u00d7n represents subsampling the rows of M at coordinates \u2126. Without confusion, denote by U the column space spanned by the matrix L. Denote by U\u0303 the noisy version of U, i.e., the subspace corrupted by the noise, and by U\u0302 our estimated subspace. The superscript k of U\u0303k means that U\u0303k has k columns in the current round. PU is frequently used to represent the orthogonal projection operator onto subspace U. We use \u03b8(a,b) to denote the angle between vectors a and b. For a vector u and a subspace V, define \u03b8(u,V) = minv\u2208V \u03b8(u,v). We define the angle between two subspaces U and V as \u03b8(U,V) = maxu\u2208U \u03b8(u,V). For norms, denote by \u2016v\u20162 the vector `2 norm of v. For matrix, \u2016M\u20162F = \u2211 ij M 2 ij and \u2016M\u2016\u221e,2 = maxi \u2016Mi:\u20162, i.e., the maximum vector `2 norm across rows. The\noperator norm is induced by the matrix Frobenius norm, which is defined as \u2016P\u2016 = max\u2016M\u2016F\u22641 \u2016PM\u2016F . If P can be represented as a matrix, \u2016P\u2016 also denotes the maximum singular value."}, {"heading": "2.1 Problem Setup", "text": "In the setting of life-long matrix completion, we assume that each column of the underlying matrix L is normalized to have unit `2 norm, and arrives online over time. We are not allowed to get access to the next column until we perform the completion for the current one. This is in sharp contrast to the offline setting where all columns come at one time and so we are able to immediately exploit the low-rank structure to do the completion. In hindsight, we assume the underlying matrix is of rank r. This assumption enables us to represent L as L = US, where U is the dictionary (a.k.a. basis matrix) of size m \u00d7 r with each column representing a latent metafeature, and S is a matrix of size r\u00d7n containing the weights of linear combination for each column L:t. The overall subspace structure is captured by U and the finer grouping structure, e.g., the mixture of multiple subspaces, is captured by the sparsity of S. Our goal is to approximately/exactly recover the subspace U and the matrix L from a small fraction of the entries, possibly corrupted by noise, although these entries can be selected sequentially in a feedback-driven way.\nNoise Models: We study two types of realistic noise models, one of which is the deterministic noise. In this setting, we assume that the `2 norm of noise on each column is bounded by noise. Beyond that, no other assumptions are made on the nature of noise. The challenge under this noise model is to design an online algorithm limiting the possible error propagation during the completion procedure. Another noise model we study is the sparse random noise, where we assume that the noise vectors are drawn i.i.d. from any non-degenerate distribution. Additionally, we assume the noise is sparse, i.e., only a few columns of L are corrupted by noise. Our goal is to exactly recover the underlying matrix L with sample complexity as small as possible.\nIncoherence: Apart from the sample budget and noise level, another quantity governing the difficulty of the completion problem is the coherence parameter on the row/column space. Intuitively, the completion should perform better when the information spreads evenly throughout the matrix. To quantify this term, for subspace U of dimension r in Rm, we define\n\u00b5(U) = m\nr max i\u2208[m]\n\u2016PUei\u201622, (1)\nwhere ei is the i-th column of the identity matrix. Indeed, without (1) there is an identifiability issue in the matrix completion problem [CP10, CR09, ZLZC15]. As an extreme example, let L be a matrix with only one non-zero entry. Such a matrix cannot be exactly recovered unless we see the non-zero element. As in [KS14], to mitigate the issue, in this paper we assume incoherence \u00b50 = \u00b5(U) on the column space of the underlying matrix. This is in contrast to the classical results of Cand\u00e8s et al. [CP10, CR09], in which one requires incoherence \u00b50 = max{\u00b5(U), \u00b5(V)} on both the column and the row subspaces.\nSampling Model: Instead of sampling the entries passively by uniform distribution, our sampling oracle allows adaptively measuring entries in each round. Specifically, for any arriving column we are allowed to have two types of sampling phases: we can either uniformly take the samples of the entries, as the passive sampling oracle, or choose to request all entries of the column in an adaptive manner. This is a natural extension of the classical passive sampling scheme with wide applications. For example, in network tomography, a network operator is interested in inferring latencies between hosts while injecting few packets\ninto the network. The operator is in control of the network, thus can adaptively sample the matrix of pair-wise latencies. In particular, the operator can request full columns of the matrix by measuring one host to all others. In gene expression analysis, we are interested in recovering a matrix of expression levels for various genes across a number of conditions. The high-throughput microarrays provide expression levels of all genes of interest across operating conditions, corresponding to revealing entire columns of the matrix."}, {"heading": "3 Main Results", "text": "In this section, we formalize our life-long matrix completion algorithm, develop our main theoretical contributions, and compare our results with the prior work."}, {"heading": "3.1 Bounded Deterministic Noise", "text": "To proceed, our algorithm streams the columns of noisy M into memory and iteratively updates the estimate for the column space of L. In particular, the algorithm maintains an estimate U\u0302 of subspace U, and when processing an arriving column M:t, requests only a few entries of M:t and a few rows of U\u0302 to estimate the distance between L:t and U. If the value of the estimator is greater than a given threshold \u03b7k, the algorithm requests the remaining entries of M:t and adds the new direction M:t to the subspace estimate; Otherwise, finds a best approximation of M:t by a linear combination of columns of U\u0302. The pseudocode of the procedure is displayed in Algorithm 1. We note that our algorithm is similar to the algorithm of [KS14] for the problem of offline matrix completion without noise. However, our setting, with the presence of noise (which might conceivably propagate through the course of the algorithm), makes our analysis significantly more subtle.\nAlgorithm 1 Noise-Tolerant Life-Long Matrix Completion under Bounded Deterministic Noise Input: Columns of matrices arriving over time. Initialize: Let the basis matrix U\u03020 = \u2205. Randomly draw entries \u2126 \u2282 [m] of size d uniformly with replacement. 1: For t from 1 to n, do 2: (a) If \u2016M\u2126t \u2212 PU\u0302k\u2126:M\u2126t\u20162 > \u03b7k 3: i. Fully measure M:t and add it to the basis matrix U\u0302k. Orthogonalize U\u0302k. 4: ii. Randomly draw entries \u2126 \u2282 [m] of size d uniformly with replacement. 5: iii. k := k + 1. 6: (b) Otherwise M\u0302:t := U\u0302kU\u0302k\u2020\u2126:M\u2126t. 7: End For Output: Estimated range space U\u0302K and the underlying matrix M\u0302 with column M\u0302:t.\nThe key ingredient of the algorithm is to estimate the distance between the noiseless column L:t and the clean subspace Uk with only a few measurements with noise. To estimate this quantity, we downsample both M:t and U\u0302k to M\u2126t and U\u0302k\u2126:, respectively. We then project M\u2126t onto subspace U\u0302 k \u2126: and use the projection residual \u2016M\u2126t \u2212 PU\u0302k\u2126:M\u2126t\u20162 as our estimator. A subtle and critical aspect of the algorithm is the choice of the threshold \u03b7k for this estimator. In the noiseless setting, we can simply set \u03b7k = 0 if the sampling number |\u2126| is large enough \u2014 in the order of O(\u00b50r log2 r), because O(\u00b50r log2 r) noiseless measurements already contain enough information for testing whether a specific column lies in a given subspace [KS14].\nIn the noisy setting, however, the challenge is that both M:t and U\u0302k are corrupted by noise, and the error propagates as the algorithm proceeds. Thus instead of setting the threshold as 0 always, our theory suggests setting \u03b7k proportional to the noise level \u221a noise. Indeed, the threshold \u03b7k balances the trade-off between the estimation error and the sample complexity: a) if \u03b7k is too large, most of the columns are represented by the noisy dictionary and therefore the error propagates too quickly; b) In contrast, if \u03b7k is too small, we observe too many columns in full and so the sample complexity increases. Our goal in this paper is to capture this trade-off, providing a global upper bound on the estimation error of the life-long arriving columns while keeping the sample complexity as small as possible."}, {"heading": "3.1.1 Recovery Guarantee", "text": "Our analysis leads to the following guarantee on the performance of Algorithm 1.\nTheorem 1 (Robust Recovery under Deterministic Noise). Let r be the rank of the underlying matrix L with \u00b50-incoherent column space. Suppose that the `2 norm of noise in each column is upper bounded by noise. Set the parameters d \u2265 c(\u00b50r + mk noise) log2(2n/\u03b4)) and \u03b7k = C \u221a dk noise/m for global constants c and C. Then with probability at least 1\u2212 \u03b4, Algorithm 1 outputs U\u0302K with K \u2264 r and outputs M\u0302 with `2 error \u2016M\u0302:t \u2212 L:t\u20162 \u2264 O ( m d \u221a k noise ) 1 uniformly for all t, where k \u2264 r is the number of base vectors when processing the t-th column.\nProof of Theorem 1. We firstly show that our estimated subspace in each round is accurate. The key ingredient of our proof is a result pertaining the angle between the underlying subspace and the noisy one. Ideally, the column space spanned by the noisy dictionary cannot be too far to the underlying subspace if the noise level is small. This is true only if the angle between the newly added vector and the column space of the current dictionary is large, as shown by the following lemma.\nLemma 2. Let\nUk = span{u1,u2, ...,uk} and U\u0303k = span{u\u03031, u\u03032, ..., u\u0303k}\nbe two subspaces such that \u03b8(ui, u\u0303i) \u2264 noise for all i \u2208 [k]. Let \u03b3k = \u221a 20k noise and \u03b8(u\u0303i, U\u0303i\u22121) \u2265 \u03b3i\nfor i = 2, ..., k. Then \u03b8(Uk, U\u0303k) \u2264 \u03b3k/2.\nProof. The proof is basically by induction on k. Instead, we will prove a stronger result by showing that the conclusion holds on subspaces Uk = span{W,u1,u2, ...,uk} and U\u0303k = span{W, u\u03031, u\u03032, ..., u\u0303k} for arbitrary fixed subspace W. The base case k = 1 follows immediately from Lemma 3 .\nLemma 3 (Lemma 2. [BBV15]). Let W = span{w1,w2, ...,wk\u22121}, U = span{w1,w2, ...,wk\u22121,u}, and U\u0303 = span{w1,w2, ...,wk\u22121, u\u0303} be subspaces spanned by vectors in Rm. Then\n\u03b8 ( U, U\u0303 ) \u2264 \u03c0\n2\n\u03b8(u\u0303,u)\n\u03b8(u\u0303,W) .\n1By our proof, the constant factor is 9.\nNow suppose the conclusion holds for any index \u2264 k \u2212 1. Let Uk0 = span{Uk\u22121, u\u0303k}. Then for index k, we have\n\u03b8(Uk, U\u0303k) \u2264 \u03b8(Uk,Uk0) + \u03b8(Uk0, U\u0303k)\n\u2264 \u03c0 2 \u03b8(u\u0303k,uk) \u03b8(u\u0303k,Uk\u22121) + 10(k \u2212 1) noise \u03b3k\u22121 (By Lemma 3 and induction hypothesis) \u2264 \u03c0 2 noise \u03b8(u\u0303k, U\u0303k\u22121)\u2212 \u03b8(Uk\u22121, U\u0303k\u22121) + 10(k \u2212 1) noise \u03b3k\u22121 \u2264 \u03c0 2\nnoise \u03b3k \u2212 10(k \u2212 1) noise\u03b3k\u22121 + 10(k \u2212 1) noise \u03b3k\u22121 (By induction hypothesis)\n= noise \u03b3k\u22121\n( \u03c0\n2 \u03b32k\u22121 \u03b3k\u03b3k\u22121 \u2212 10(k \u2212 1) noise + 10(k \u2212 1) ) \u2264 noise\n\u03b3k\u22121 (\u03c0 + 10k \u2212 10)\n= noise \u03b3k \u03b3k \u03b3k\u22121 (\u03c0 + 10k \u2212 10)\n= noise \u03b3k\n\u221a k\nk \u2212 1 (\u03c0 + 10k \u2212 10)\n\u2264 noise \u03b3k 10k (k \u2265 2).\nWe then prove the correctness of our test in Step 2. Lemma 2 guarantees that the underlying subspace Uk and our estimated one U\u0303k cannot be too distinct. So by algorithm, projecting any vector on the subspace spanned by U\u0303k does not make too many mistakes, i.e., \u03b8(M:t, U\u0303k) \u2248 \u03b8(M:t,Uk). On the other hand, by standard concentration argument our test statistic \u2016M\u2126t \u2212 PU\u0303k\u2126:M\u2126t\u20162 is close to d m\u2016M:t \u2212 PU\u0303kM:t\u20162. Note that the latter term is determined by the angle of \u03b8(M:t, U\u0303k). Therefore, our test statistic in Step 2 is indeed an effective measure of \u03b8(M:t, U\u0303k), or \u03b8(L:t, U\u0303k) since L:t \u2248M:t, as proven by the following novel result. Lemma 4. Let k = 2\u03b3k, \u03b3k = \u221a\n20k noise, and k \u2264 r. Suppose that we observe a set of coordinates \u2126 \u2282 [m] of size d uniformly at random with replacement, where d \u2265 c0(\u00b50r + mk noise) log2(2/\u03b4). If \u03b8(L:t, U\u0303 k) \u2264 k, then with probability at least 1\u2212 4\u03b4, we have \u2016M\u2126t \u2212 PU\u0303k\u2126:M\u2126t\u20162 \u2264 C \u221a dk noise/m. Inversely, if \u03b8(L:t, U\u0303k) \u2265 c k, then with probability at least 1 \u2212 4\u03b4, we have \u2016M\u2126t \u2212 PU\u0303k\u2126:M\u2126t\u20162 \u2265 C \u221a dk noise/m, where c0, c and C are absolute constants.\nProof. The first part of the theorem follows from the upper bound of Lemma 15. Specifically, by plugging d into the lower bound of Lemma 15, we see that \u03b1 < 1/2 and \u03b3 < 1/3. Note that\u2225\u2225L:t \u2212 PU\u0303kL:t\u2225\u22252 = \u2016L:t\u20162 sin \u03b8 (L:t,PU\u0303kL:t)\n\u2264 \u03b8 ( L:t,PU\u0303kL:t ) = \u03b8(L:t, U\u0303 k) \u2264 k.\nTherefore, by Lemma 15,\u2225\u2225\u2225M\u2126t \u2212 PU\u0303k\u2126:M\u2126t\u2225\u2225\u22252 \u2264 O (\u221a d m \u2225\u2225M:t \u2212 PU\u0303kM:t\u2225\u22252 )\n\u2264 O\n(\u221a d\nm\n(\u2225\u2225L:t \u2212 PU\u0303kL:t\u2225\u22252 + \u2225\u2225PU\u0303k(L:t \u2212M:t)\u2225\u22252 + \u2016M:t \u2212 L:t\u20162) )\n\u2264 O\n(\u221a d\nm ( k + 2 noise)\n) \u2264 C \u221a dk noise m .\nWe now proceed the second part of the theorem. To this end, we first explore the relation between the incoherence of the noisy basis U\u0303k and the clean one Uk. Since we are able to control the error propagation in U\u0303k, intuitively, the incoherence of U\u0303k and Uk is not distinct too much. In particular, for any i \u2208 [m],\u2225\u2225P U\u0303k ei \u2225\u2225 2 \u2264 \u2016PUkei\u20162 +\n\u2225\u2225PUkei \u2212 PU\u0303kei\u2225\u22252 \u2264 \u2016PUkei\u20162 +\n\u2225\u2225PUk \u2212 PU\u0303k\u2225\u2225 \u2016ei\u20162 = \u2016PUkei\u20162 + \u2016ei\u20162 sin \u03b8 ( Uk, U\u0303k\n) \u2264 \u2016PUkei\u20162 + \u03b8 ( Uk, U\u0303k\n) \u2264 \u2016PUkei\u20162 +\n\u03b3k 2\n= \u2016PUkei\u20162 + 1\n4 k. Therefore, \u00b5 ( U\u0303k ) = mk maxi\u2208[m] \u2225\u2225P U\u0303k ei \u2225\u22252 2 \u2264 mk ( 2 \u2016PUkei\u2016 2 2 + 1 8 2 k ) \u2264 2\u00b5(Uk) + c\u2032\u2032m noise, for global constant c\u2032\u2032. Also, note that\u2225\u2225P U\u0303k M:t \u2212 L:t \u2225\u2225 2 \u2265 sin \u03b8 ( L:t,PU\u0303kM:t ) \u2016L:t\u20162 \u2265 1 2 \u03b8 ( L:t,PU\u0303kM:t ) \u2265 1 2 \u03b8 ( L:t, U\u0303 k ) \u2265 c k 2 . So we have\u2225\u2225\u2225M\u2126t \u2212 PU\u0303k\u2126M\u2126t\u2225\u2225\u22252 \u2265 \u221a\u221a\u221a\u221a 1 m ( d 2 \u2212 3k\u00b5(U\u0303 k)\u03b2 2\n)\u2225\u2225M:t \u2212 PU\u0303kM:t\u2225\u22252 \u2265 \u2126 (\u221a d\nm \u2212 3k\u00b5(U\nk)\nm log2(1/\u03b4)\u2212 c0k noise log2(1/\u03b4) \u2225\u2225M:t \u2212 PU\u0303kM:t\u2225\u22252 )\n\u2265 \u2126\n(\u221a d\nm \u2212 3\u00b50r m log2(1/\u03b4)\u2212 c0k noise log2(1/\u03b4) \u2225\u2225M:t \u2212 PU\u0303kM:t\u2225\u22252 ) (Since Uk \u2286 Ur)\n\u2265 \u2126\n(\u221a d\nm \u2212 c0k noise log2(1/\u03b4)\n(\u2225\u2225P U\u0303k M:t \u2212 L:t \u2225\u2225 2 \u2212 \u2016L:t \u2212M:t\u20162 )) ( Since d > 3\u00b50r log2(1/\u03b4)\n) > \u2126 (\u221a d\nm \u2212 c0k noise log2(1/\u03b4) (c k 2 \u2212 noise\n))\n> C \u221a dk noise m ( Since d > c0mk noise log2(1/\u03b4) ) .\nFinally, as both our dictionary and our statistic are accurate, the output error cannot be too large. In particular, we first show K \u2264 r. Notice that every time we add a new direction to the basis matrix if and only if Condition (a) in Algorithm 1 holds true. In that case by Lemma 4, if setting \u03b7k = C \u221a dk noise/m, then with probability at least 1\u2212 4\u03b4, we have that \u03b8(L:t, U\u0303k) \u2265 2\u03b3k, which implies \u03b8(M:t, U\u0303k) \u2265 \u03b8(L:t, U\u0303k)\u2212 \u03b8(M:t,L:t) \u2265 \u03b3k. So by Lemma 2, \u03b8(Uk, U\u0303k) \u2264 \u03b3k/2. Thus \u03b8(L:t,Uk) \u2265 \u03b8(L:t, U\u0303k) \u2212 \u03b8(Uk, U\u0303k) \u2265 3\u03b3k/2. Since rank(L) = r, we obtain that K \u2264 r.\nWe now proceed to prove the upper bound on the `2 error in Theorem 1. We discuss Case (a) and (b) respectively. If Condition (a) in Algorithm 1 holds true, then according to the algorithm, we fully observe M:t and use it as our estimate M\u0302:t. So \u2225\u2225\u2225M\u0302:t \u2212 L:t\u2225\u2225\u2225 2 \u2264 noise \u2264 \u0398(md \u221a k noise); On the other hand, if Case (b) in Algorithm 1 holds true, then we represent M\u0302:t by the basis subspace U\u0303k. So we have\u2225\u2225\u2225M\u0302:t \u2212 L:t\u2225\u2225\u2225 2 = \u2225\u2225\u2225U\u0303kU\u0303k\u2020\u2126:M\u2126t \u2212 L:t\u2225\u2225\u2225 2\n\u2264 \u2225\u2225\u2225U\u0303kU\u0303k\u2020L:t \u2212 L:t\u2225\u2225\u2225 2 + \u2225\u2225\u2225U\u0303kU\u0303k\u2020\u2126:L\u2126t \u2212 U\u0303kU\u0303k\u2020L:t\u2225\u2225\u2225 2 + \u2225\u2225\u2225U\u0303kU\u0303k\u2020\u2126:L\u2126t \u2212 U\u0303kU\u0303k\u2020\u2126:M\u2126t\u2225\u2225\u2225 2\n= sin \u03b8(L:t, U\u0303 k) + \u2225\u2225\u2225U\u0303kU\u0303k\u2020\u2126:L\u2126t \u2212 U\u0303kU\u0303k\u2020L:t\u2225\u2225\u2225 2 + \u2225\u2225\u2225U\u0303kU\u0303k\u2020\u2126:(L\u2126t \u2212M\u2126t)\u2225\u2225\u2225 2 .\nTo bound the second term, let L:t = U\u0303kv + e, where U\u0303kv = U\u0303kU\u0303k\u2020L:t and \u2016e\u20162 \u2264 k since \u2016e\u20162 = sin \u03b8(L:t, U\u0303 k) \u2264 k. So\nU\u0303kU\u0303k\u2020\u2126:L\u2126t \u2212 U\u0303 kU\u0303k\u2020L:t = U\u0303 k(U\u0303kT\u2126: U\u0303 k \u2126:) \u22121U\u0303kT\u2126: (U\u0303 k \u2126:v + e\u2126)\u2212 U\u0303k\u2126:v\n= U\u0303kU\u0303k\u2020\u2126:e\u2126.\nTherefore,\u2225\u2225\u2225M\u0302:t \u2212 L:t\u2225\u2225\u2225 2 \u2264 \u03b8(L:t, U\u0303k) + \u2225\u2225\u2225U\u0303kU\u0303k\u2020\u2126:L\u2126t \u2212 U\u0303kU\u0303k\u2020L:t\u2225\u2225\u2225 2 + \u2225\u2225\u2225U\u0303kU\u0303k\u2020\u2126:(L\u2126t \u2212M\u2126t)\u2225\u2225\u2225 2\n\u2264 \u03b8(L:t, U\u0303k) + \u2225\u2225\u2225U\u0303kU\u0303k\u2020\u2126:\u2225\u2225\u2225 \u2016e\u2126\u20162 + \u2225\u2225\u2225U\u0303kU\u0303k\u2020\u2126:\u2225\u2225\u2225 \u2016L\u2126t \u2212M\u2126t\u20162\n\u2264 k + \u0398 (m d k ) + \u0398 (m d noise ) = \u0398 (m d \u221a k noise ) ,\nwhere \u2225\u2225\u2225U\u0303kU\u0303k\u2126:\u2225\u2225\u2225 \u2264 \u03c31(U\u0303k)/\u03c3k(U\u0303k\u2126:) \u2264 \u0398(m/d) once d \u2265 \u2126(\u00b5(U\u0303k)k log(k/\u03b4)), due to Lemma 16. The final sample complexity follows from the union bound on the n columns.\nTheorem 1 implies a result in the noiseless setting when noise goes to zero. Indeed, with the sample size growing in the order of O(\u00b50nr log2 n), Algorithm 1 outputs a solution that is exact with probability at least 1\u2212 1\nn10 . To the best of our knowledge, this is the best sample complexity in the existing literature for noiseless matrix completion without additional side information [KS14, Rec11]. For the noisy setting, Algorithm 1 enjoys the same sample complexity O(\u00b50nr log2 n) as the noiseless case, if noise \u2264 \u0398(\u00b50r/(mk)). In addition, Algorithm 1 inherits the benefits of adaptive sampling scheme. The vast majority results in the passive sampling scenarios require both the row and column incoherence for exact/robust recovery [Rec11].\nIn contrast, via adaptive sampling we can relax the incoherence assumption on the row space of the underlying matrix and are therefore more applicable.\nWe compare our result with several related lines of research in the prior work. While lots of online matrix completion algorithms have been proposed recently, they either lack of solid theoretical guarantee [KTB14], or require strong assumptions for the streaming data [KS14, LV15, DGC14, KS13]. Specifically, Krishnamurthy et al. [KS13] proposed an algorithm that requires column subset selection in the noisy case, which might be impractical in the online setting as we cannot measure columns that do not arrive. Focusing on a similar online matrix completion problem, Lois et al. [LV15] assumed that a) there is a good initial estimate for the column space; b) the column space changes slowly; c) the base vectors of the column space are dense; d) the support of the measurements changes by at least a certain amount. In contrast, our assumptions are much simpler and more realistic.\nWe mention another related line of research \u2014 matched subspace detection. The goal of matched subspace detection is to decide whether an incomplete signal/vector lies within a given subspace [BRN10, BNR10]. It is highly related to the procedure of our algorithm in each round, where we aim at determining whether an arriving vector belongs to a given subspace based on partial and noisy observations. Prior work targeting on this problem formalizes the task as a hypothesis testing problem. So they assume a specific random distribution on the noise, e.g., Gaussian, and choose \u03b7k by fixing the probability of false alarm in the hypothesis testing [BRN10, SF94]. Compared with this, our result does not have any assumption on the noise structure/distribution."}, {"heading": "3.2 Sparse Random Noise", "text": "In this section, we discuss life-long matrix completion on a simpler noise model but with a stronger recovery guarantee. We assume that noise is sparse, meaning that the total number of noisy columns is small compared to the total number of columns n. The noisy columns may arrive at any time, and each noisy column is assumed to be drawn i.i.d. from a non-degenerate distribution. Our goal is to exactly recover the underlying matrix and identify the noise with high probability.\nWe use an algorithm similar to Algorithm 1 to attack the problem, with \u03b7k = 0. The challenge is that here we frequently add noise vectors to the dictionary and so we need to distinguish the noise from the clean column and remove them out of the dictionary at the end of the algorithm. To resolve the issue, we additionally record the support of the representation coefficients in each round when we represent the arriving vector by the linear combinations of the columns in the dictionary matrix. On one hand, the noise vectors in the dictionary fail to represent any column, because they are random. So if the representation coefficient corresponding to a column in the dictionary is 0 always, it is convincing to identify the column as a noise. On the other hand, to avoid recognizing a true base vector as a noise, we make a mild assumption that the underlying column space is identifiable. Typically, that means for each direction in the underlying subspace, there are at least two clean data points having non-zero projection on that direction. We argue that the assumption is indispensable, since without it there is an identifiability issue between the clean data and the noise. As an extreme example, we cannot identify the black point in Figures 1 as the clean data or as noise if we make no assumption on the underlying subspace. To mitigate the problem, we assume that for each i \u2208 [r] and a subspace Ur with orthonormal basis, there are at least two columns L:ai and L:bi of L such that [Ur]T:iL:ai 6= 0 and [Ur]T:iL:bi 6= 0. The detailed algorithm can be found in Algorithm 2.\nAlgorithm 2 Noise-Tolerant Life-Long Matrix Completion under Sparse Random Noise Input: Columns of matrices arriving over time. Initialize: Let the basis matrix B\u03020 = \u2205, the counter C = \u2205. Randomly draw entries \u2126 \u2282 [m] of size d uniformly without replacement. 1: For each column t of M, do 2: (a) If \u2016M\u2126t \u2212 PB\u0302k\u2126:M\u2126t\u20162 > 0 3: i. Fully measure M:t and add it to the basis matrix B\u0302k. 4: ii. C := [C, 0]. 5: iii. Randomly draw entries \u2126 \u2282 [m] of size d uniformly without replacement. 6: iv. k := k + 1. 7: (b) Otherwise 8: i. C := C + 1T\nsupp(B\u0302k\u2020\u2126:M\u2126t) . //Record supports of representation coefficient\n9: ii. M\u0302:t := B\u0302kB\u0302k\u2020\u2126:M\u2126t. 10: t := t+ 1. 11: End For Outlier Removal: Remove columns corresponding to entry 0 in vector C from B\u0302s0+r = [Es0 ,Ur]. Output: Estimated range space, identified outlier vectors, and recovered underlying matrix M\u0302 with column M\u0302:t.\n3.2.1 Upper Bound\nWe now provide upper and lower bound on the sample complexity of above algorithm for the exact recovery of underlying matrix. Our upper bound matches the lower bound up to a constant factor. We then analyze a more benign setting, namely, the data lie on a mixture of low-rank subspaces with dimensionality \u03c4 r. Our analysis leads to the following guarantee on the performance of above algorithm.\nTheorem 5 (Exact Recovery under Random Noise). Let r be the rank of the underlying matrix L with \u00b50-incoherent column space. Suppose that the noise Es0 of size m\u00d7 s0 are drawn from any non-degenerate distribution, and that the underlying subspace Ur is identifiable. Then our algorithm exactly recovers the underlying matrix L, the column space Ur, and the outlier Es0 with probability at least 1\u2212 \u03b4, provided that d \u2265 c\u00b50r log (r/\u03b4) and s0 \u2264 d\u2212 r \u2212 1. The total sample complexity is thus c\u00b50rn log (r/\u03b4), where c is a universal constant.\nProof of Theorem 5. We first prove a useful lemma which shows that the orthogonalization of a matrix does not change the rank of the matrix restricted on some rows/columns.\nLemma 6. Let X = U\u03a3VT be the skinny SVD of X, orthc(X) = U, and orthr(X) = V\nT . Then for any set of coordinates \u2126 and any matrix X \u2208 Rm\u00d7n, we have\nrank(X\u2126:) = rank([orthc(X)]\u2126:) and rank(X:\u2126) = rank([orthr(X)]:\u2126).\nProof. Let X = U\u03a3VT be the skinny SVD of matrix X, where U = orthc(X) and VT = orthr(X). On one hand,\nX\u2126: = I\u2126:X = I\u2126:U\u03a3V T = [orthc(X)]\u2126:\u03a3V T .\nSo rank(X\u2126:) \u2264 rank([orthc(X)]\u2126:). On the other hand, we have\nX\u2126:V\u03a3 \u22121 = [orthc(X)]\u2126:.\nThus rank([orthc(X)]\u2126:) \u2264 rank(X\u2126:). So rank(X\u2126:) = rank([orthc(X)]\u2126:).\nThe second part of the argument can be proved similarly. Indeed, X:\u2126 = U\u03a3VT I:\u2126 = U\u03a3[orthr(X)]:\u2126 and \u03a3\u22121UTX:\u2126 = [orthr(X)]:\u2126. So rank(X:\u2126) = rank([orthr(X)]:\u2126), as desired.\nWe then investigate the effect of sampling on the rank of a matrix.\nProposition 7. Let L \u2208 Rm\u00d7n be any rank-r matrix with skinny SVD U\u03a3VT . Denote by L:\u2126 the submatrix formed by subsampling the columns of L with i.i.d. Ber(d/n). If d \u2265 8\u00b5(V)r log(r/\u03b4), then with probability at least 1 \u2212 \u03b4, we have rank(L:\u2126) = r. Similarly, denote by L\u2126: the submatrix formed by subsampling the rows of L with i.i.d. Ber(d/m). If d \u2265 8\u00b5(U)r log(r/\u03b4), then with probability at least 1\u2212 \u03b4, we have rank(L\u2126:) = r.\nProof. We only prove the first part of the argument. For the second part, applying the first part to matrix LT gets the result. Denote by T the matrix VT = orthr(L) with orthonormal rows, and by X =\u2211n\ni=1 \u03b4iT:ie T i \u2208 Rr\u00d7n the sampling of columns from T with \u03b4i \u223c Ber(d/n). Let Xi = \u03b4iT:ieTi . Define\npositive semi-definite matrix\nY = XXT = n\u2211 i=1 \u03b4iT:iT T :i .\nObviously, \u03c32r (X) = \u03bbr(Y). To invoke the matrix Chernoff bound, we estimate the parameters L and \u00b5r in Lemma 16. Specifically, note that\nEY = n\u2211 i=1 E\u03b4iT:iTT:i = d n n\u2211 i=1 T:iT T :i = d n TTT .\nTherefore, \u00b5r = \u03bbr(EY) = d\u03c32r (T)/n > 0. Furthermore, we also have\n\u03bbmax(Xi) = \u2016\u03b4iT:i\u201622 \u2264 \u2016T\u201622,\u221e , L.\nBy the matrix Chernoff bound where we set = 1/2,\nPr [\u03c3r(X) > 0] = Pr [\u03bbr(Y) > 0] \u2265 Pr [ \u03bbr(Y) > 1\n2 \u00b5r ] = Pr [ \u03bbr(Y) > d\n2n \u03c32r (T) ] \u2265 1\u2212 r exp ( \u2212 d\u03c3 2 r (T)\n8n\u2016T\u201622,\u221e ) , 1\u2212 \u03b4.\nSo if\nd \u2265 8n\u2016T\u201622,\u221e \u03c32r (T) log r \u03b4 = 8n\u2016T\u201622,\u221e log (r \u03b4 ) ,\nthen Pr [\u03c3k+1(X) = 0] \u2264 \u03b4, where the last equality holds since \u03c3r(T) = \u03c3r(VT ) = 1. Note that\n\u2016T\u201622,\u221e \u2264 max i\u2208[n] \u2016VTei\u201622 \u2264\nr n \u00b5(V).\nSo if d \u2265 8\u00b5(V)r log(r/\u03b4) then with probability at least 1 \u2212 \u03b4, rank(T:\u2126) = r. Also, by Lemma 6, rank(T:\u2126) = rank([orthr(L)]:\u2126) = rank(L:\u2126). Therefore, rank(L:\u2126) = r with a high probability, as desired.\nWe now study the effectiveness of our representation step.\nLemma 8. Let Uk \u2208 Rm\u00d7k be a k-dimensional subspace of Ur. Suppose we get access to a set of coordinates \u2126 \u2282 [m] of size d uniformly at random without replacement. Let s \u2264 d\u2212 r\u2212 1 and d \u2265 c\u00b50r log(k/\u03b4) for a universal constant c.\n\u2022 If M:t \u2208 Ur but M:t 6\u2208 Uk then with probability at least 1\u2212 \u03b4, rank ( [Es\u2126:,U k \u2126:,M\u2126t] ) = s+ k+ 1.\n\u2022 If M:t \u2208 Uk, then rank ( [Es\u2126:,U k \u2126:,M\u2126t] ) = s+ k with probability 1, the representation coefficients\nof M:t corresponding to Es in the dictionary [Es,Uk] is 0 with probability 1, and [Es,Uk][Es\u2126:,U k \u2126:] \u2020M\u2126t = M:t with probability at least 1\u2212 \u03b4.\n\u2022 If M:t 6\u2208 Ur, i.e., M:t is an outlier drawn from a non-degenerate distribution, then rank ( [Es\u2126:,U k \u2126:,M\u2126t] ) =\ns+ k + 1 with probability 1\u2212 \u03b4.\nProof. For the first part of the lemma, note that rank([Uk,M:t]) = k + 1. So according to Proposition 7, with probability 1 \u2212 \u03b4 we have that rank([Uk,M:t]\u2126:) = k + 1 since d \u2265 c\u00b50r log((k + 1)/\u03b4) \u2265 8\u00b5([Uk,M:t])k log((k + 1)/\u03b4) (Because M:t \u2208 Ur). Recall Facts 3 and 4 of Lemma 13 which imply that rank([Es,Uk,M:t]\u2126:) = s+ k + 1 when s \u2264 d\u2212 r \u2212 1. This is what we desire.\nFor the middle part, the statement rank ( [Es\u2126:,U k \u2126:,M\u2126t] ) = s + k comes from the assumption that\nM:t \u2208 Uk, which implies that M\u2126t \u2208 Uk\u2126: with probability 1, and that rank ( [Es\u2126:,U k \u2126:] )\n= s+ k when s \u2264 d \u2212 r \u2212 1 (Facts 3 and 4 of Lemma 13). Now suppose that the representation coefficients of M:t corresponding to Es in the dictionary [Es,Uk] is NOT 0 and M:t \u2208 Uk. Then M:t \u2212Ukc \u2208 span(Es), where c is the representation coefficients of M:t corresponding to Uk in the dictionary [Es,Uk]. Also, note that M:t \u2212Ukc \u2208 Uk. So rank[Es,M:t \u2212Ukc] = s, which is contradictory with Fact 2 of Lemma 13. So the coefficient w.r.t. Es in the dictionary [Es,Uk] is 0, and we have that [Es,Uk][Es\u2126:,U k \u2126:] \u2020M\u2126t = UkUk\u2020\u2126:M\u2126t = U k(UkT\u2126: U k \u2126:) \u22121UkT\u2126: M\u2126t = U k(UkT\u2126: U k \u2126:) \u22121UkT\u2126: U k \u2126:v = U\nkv = M:t, where v is the representation coefficient of M:t w.r.t. Uk. (The (UkT\u2126: U k \u2126:) \u22121 exists because rank(Uk\u2126:) = k by Proposition 7)\nAs for the last part of the lemma, note that by Facts 2 and 4 of Lemma 13, rank([Es,M:t]\u2126:) = s+ 1. Then by Fact 3 of Lemma 13 and the fact that Uk\u2126: has rank k (Proposition 7), we have rank ( [Es\u2126:,U k \u2126:,M\u2126t] ) = s+ k + 1 when s \u2264 d\u2212 r \u2212 1, as desired.\n( ) ( )\nNow we are ready to prove Theorem 5. The proof of Theorem 5 is an immediate result of Lemma 8 by using the union bound on the samplings of \u2126. Although Lemma 8 states that, for a specific column M:t, the algorithm succeeds with probability at least 1\u2212 \u03b4, the probability of success that uniformly holds for all columns is 1 \u2212 (r + s0)\u03b4 rather than 1 \u2212 n\u03b4. This observation is from the proof of Lemma 8: [Es,Uk][Es\u2126:,U k \u2126:] \u2020M\u2126t = M:t holds so long as (UkT\u2126: U k \u2126:) \u22121 exists. Since in Algorithm 2 we resample \u2126 if and only if we add new vectors into the basis matrix, which happens at most r + s0 times, the conclusion follows from the union bound of the r + s0 events. Thus, to achieve a global probability of 1\u2212 \u03b4, the sample complexity for each upcoming column is \u0398(\u00b50r log(r+ s0/\u03b4)). Since we also require that s0 \u2264 d \u2212 r \u2212 1, the algorithm succeeds with probability 1 \u2212 \u03b4 once d \u2265 \u0398(\u00b50r log(d/\u03b4)). Solving for d, we obtain that d & \u00b50r log(\u00b520r\n2/\u03b42) \u00b50r log(r/\u03b4)2. The total sample complexity for Algorithm 2 is thus \u0398(\u00b50rn log(r/\u03b4)).\nFor the exact identifiability of the outliers, we have the following guarantee:\nLemma 9 (Outlier Removal). Let the underlying subspace Ur be identifiable, i.e., for each i \u2208 [r], there are at least two columns M:ai and M:bi of M such that [orthc(U\nr)]T:iM:ai 6= 0 and [orthc(Ur)]T:iM:bi 6= 0. Then the entries of C in Algorithm 2 corresponding to Ur cannot be 0\u2019s.\nProof. Without loss of generality, let Ur be orthonormal. Suppose that the lemma does not hold true. Then there must exist one column Ur:i of U\nr, say e.g., ei, such that eTi M:t = 0 for all t except when the index t corresponds exactly to the Ur:i. This is contradictory with the condition that the subspace U\nr is identifiable. The proof is completed.\nThus the proof of Theorem 5 is completed.\nTheorem 5 implies an immediate result in the noise-free setting as noise goes to zero. In particular, O (\u00b50nr log(r/\u03b4)) measurements are sufficient so that our algorithm outputs a solution that is exact with probability at least 1\u2212 \u03b4. This sample complexity improves over existing results of O ( \u00b50nr log 2(n/\u03b4) )\n[Rec11] and O ( \u00b50nr 3/2 log(r/\u03b4) ) [KS13], and over O ( \u00b50nr log 2(r/\u03b4) )\nof Theorem 1 when noise = 0. Indeed, our sample complexity O (\u00b50nr log(r/\u03b4)) matches the lower bound, as shown by Theorem 10 (See Table 1 for comparisons of sample complexity). We notice another paper of Gittens [Git11] which showed that Nsytro\u0308m method recovers a positive-semidefinite matrix of rank r from uniformly sampling O(\u00b50r log(r/\u03b4)) columns. While this result matches our sample complexity, the assumptions of positive-semidefiniteness and of subsampling the columns are impractical in the online setting. We compare Theorem 5 with prior methods on decomposing an incomplete matrix as the sum of a low-rank term and a column-sparse term. Probably one of the best known algorithms is Robust PCA via Outlier Pursuit [XCS12, ZLZG15, ZLZC15, ZLZ16].\n2We assume here that \u00b50 \u2264 poly(r/\u03b4).\nOutlier Pursuit converts this problem to a convex program:\nmin L,E \u2016L\u2016\u2217 + \u03bb\u2016E\u20162,1, s.t. P\u2126M = P\u2126(L + E), (2)\nwhere \u2016 \u00b7 \u2016\u2217 captures the low-rankness of the underlying subspace and \u2016 \u00b7 \u20162,1 captures the column-sparsity of the noise. Recent papers on Outlier Pursuit [ZLZ16] prove that the solution to (2) exactly recovers the underlying subspace, provided that d \u2265 c1\u00b520r2 log3 n and s0 \u2264 c2d4n/(\u00b550r5m3 log6 n) for constants c1 and c2. Our result definitely outperforms the existing result in term of the sample complexity d, while our dependence of s0 is not always better (although in some cases better) when n is large. Note that while Outlier Pursuit loads all columns simultaneously and so can exploit the global low-rank structure, our algorithm is online and therefore cannot tolerate too much noise."}, {"heading": "3.2.2 Lower Bound", "text": "We now establish a lower bound on the sample complexity. Our lower bound shows that in our adaptive sampling setting, one needs at least \u2126 (\u00b50rn log (r/\u03b4)) many samples in order to uniquely identify a certain matrix in the worst case. This lower bound matches our analysis of upper bound in Section 3.2.1.\nTheorem 10 (Lower Bound on Sample Complexity). Let 0 < \u03b4 < 1/2, and \u2126 \u223c Uniform(d) be the index of the row sampling \u2286 [m]. Suppose that Ur is \u00b50-incoherent. If the total sampling number dn < c\u00b50rn log (r/\u03b4) for a constant c, then with probability at least 1 \u2212 \u03b4, there is an example of M such that under the sampling model of Section 2.1 (i.e., when a column arrives the choices are either (a) randomly sample or (b) view the entire column), there exist infinitely many matrices L\u2032 of rank r obeying \u00b50-incoherent condition on column space such that L\u2032\u2126: = L\u2126:.\nProof. We prove the theorem by assuming that the underlying column space is known. Since we require additional samples to estimate the subspace, the proof under this assumption gives a lower bound. Let ` = \u230a m \u00b50r \u230b . Construct the underlying matrix L by\nL = r\u2211 k=1 bkuku T k ,\nwhere the known uk (Because the column space is known) is defined as\nuk =\n\u221a 1\n` \u2211 i\u2208Bk ei, Bk = {(k \u2212 1)`+ 1, (k \u2212 1)`+ 2, ..., k`}.\nSo the matrix L is a block diagonal matrix formulated as Figure 2. Further, construct the noisy matrix M by M = [L,E]. The matrix E \u2208 Rm\u00d7s0 corresponds to the outliers, and the matrix L corresponds to the underlying matrix.\nNotice that the information of bk\u2019s is only implied in the corresponding block of L. So overall, the lower bound is given by solving from the inequality\nPr{For all blocks, there must be at least one row being sampled} \u2265 1\u2212 \u03b4.\nWe highlight that the bk\u2019s can be chosen arbitrarily in that they do not change the coherence of the column space of L. Also, it is easy to check that the column space of L is \u00b50-incoherent. By construction, the underlying matrix L is block-diagonal with r blocks, each of which is of size `\u00d7`. According to our sampling scheme, we always sample the same positions of the arriving column after the column space is known to us. This corresponds to sample the row of the matrix in hindsight. To recover L, we argue that each block should have at least one row fully observed; Otherwise, there is no information to recover bk\u2019s. Let A be the event that for a fixed block, none of its rows is observed. The probability \u03c00 of this event A is therefore \u03c00 = (1\u2212 p)`, where p is the Bernoulli sampling parameter. Thus by independence, the probability of the event that there is at least one row being sampled holds true for all diagonal blocks is (1\u2212 \u03c00)r, which is \u2265 1\u2212 \u03b4 as we have argued. So\n\u2212r\u03c00 \u2265 r log(1\u2212 \u03c00) \u2265 log(1\u2212 \u03b4),\nwhere the first inequality is due to the fact that \u2212x \u2265 log(1 \u2212 x) for any x < 1. Since we have assumed \u03b4 < 1/2, which implies that log(1\u2212 \u03b4) \u2265 \u22122\u03b4, thus \u03c00 \u2264 2\u03b4/r. Note that \u03c00 = (1\u2212 p)`, and so\n\u2212 log(1\u2212 p) \u2265 1 `\nlog ( r\n2\u03b4\n) \u2265 \u00b50r m log ( r 2\u03b4 ) .\nThis is equivalent to mp \u2265 m ( 1\u2212 exp ( \u2212\u00b50r m log r 2\u03b4 )) . Note that 1\u2212 e\u2212x \u2265 x\u2212 x2/2 whenever x \u2265 0, we have\nmp \u2265 (1\u2212 /2)\u00b50r log ( r\n2\u03b4\n) ,\nwhere = \u00b50r log(r/2\u03b4) < 1. Finally, by the equivalence between the uniform and Bernoulli sampling models (i.e., d \u2248 mp, Lemma 14), the proof is completed.\nWe mention several lower bounds on the sample complexity for passive matrix completion. The first is the paper of Cand\u00e8s and Tao [CT10], that gives a lower bound of \u2126(\u00b50nr log(n/\u03b4)) if the matrix has both incoherent rows and columns. Taking a weaker assumption, Krishnamurthy and Singh [KS13, KS14] showed\nthat if the row space is coherent, any passive sampling scheme followed by any recovery algorithm must have \u2126(mn) measurements. In contrast, Theorem 10 demonstrates that in the absence of row-space incoherence, exact recovery of the matrix is possible with only \u2126(\u00b50nr log(r/\u03b4)) samples, if the sampling scheme is adaptive.\n3.2.3 Extension to Mixture of Subspaces\nTheorem 10 gives a lower bound on sample complexity in the worst case. In this section, we explore the possibility of further reducing the sample complexity with more complex common structure. We assume that the underlying subspace is a mixture of h independent subspaces3 [LZ14], each of which is of dimension at most \u03c4 r. Such an assumption naturally models settings in which there are really h different categories of movies/news while they share a certain commonality across categories. We can view this setting as a network with two layers: The first layer captures the overall subspace with r metafeatures; The second layer is an output layer, consisting of metafeatures each of which is a linear combination of only \u03c4 metafeatures in the first layer. See Figures 3 for visualization. Our argument shows that the sparse connections between the two layers significantly improve the sample complexity.\nAlgorithmically, given a new column, we uniformly sample O\u0303(\u03c4 log r) entries as our observations. We try to represent those elements by a sparse linear combination of only \u03c4 columns in the basis matrix, whose rows are truncated to those sampled indices; If we fail, we measure the column in full, add that column into the dictionary, and repeat the procedure for the next arriving column. The detailed algorithm can be found in Algorithm 3.\nRegarding computational considerations, learning a \u03c4 -sparse representation of a given vector w.r.t. a known dictionary can be done in polynomial time if the dictionary matrix satisfies the restricted isometry property [CRT06], or trivially if \u03c4 is a constant [BBV15]. This can be done by applying `1 minimization or brute-force algorithm, respectively. Indeed, many real datasets match the constant-\u03c4 assumption, e.g., face image [BJ03] (each person lies on a subspace of dimension \u03c4 = 9), 3D motion trajectory [CK98] (each object lies on a subspace of dimension \u03c4 = 4), handwritten digits [HS98] (each script lies on a subspace of dimension \u03c4 = 12), etc. So our algorithm is applicable for all these settings.\nTheoretically, the following theorem provides a strong guarantee for our algorithm.\nTheorem 11 (Mixture of Subspaces). Let r be the rank of the underlying matrix L. Suppose that the columns of L lie on a mixture of identifiable and independent subspaces, each of which is of dimension at most \u03c4 . Denote by \u00b5\u03c4 the maximal incoherence over all \u03c4 -combinations of L. Let the noise model be that of Theorem 5. Then our algorithm exactly recovers the underlying matrix L, the column space Ur, and the outlier Es0 with probability at least 1\u2212 \u03b4, provided that d \u2265 c\u00b5\u03c4\u03c42 log (r/\u03b4) for some global constant c and s0 \u2264 d\u2212 \u03c4 \u2212 1. The total sample complexity is thus c\u00b5\u03c4\u03c42n log (r/\u03b4).\n3h linear subspaces are independent if the dimensionality of their sum is equal to the sum of their dimensions.\nAlgorithm 3 Noise-Tolerant Life-Long Matrix Completion under Random Noise for Mixture of Subspaces Input: Columns of matrices arriving over time. Initialize: Let the basis matrix B\u03020 = \u2205, the counter C = \u2205. Randomly draw entries \u2126 \u2282 [m] of size d uniformly without replacement. 1: For each column t of M, do 2: (a) If there does not exist a \u03c4 -sparse linear combination of columns of B\u0302k\u2126: that represents M\u2126t exactly 3: i. Fully measure M:t and add it to the basis matrix B\u0302k. 4: ii. C := [C, 0]. 5: iii. Randomly draw entries \u2126 \u2282 [m] of size d uniformly without replacement. 6: iv. k := k + 1. 7: (b) Otherwise 8: i. C := C + 1T\nsupp(B\u0302k\u2020\u2126:M\u2126t) . //Record supports of representation coefficient\n9: ii. M\u0302:t := B\u0302kB\u0302k\u2020\u2126:M\u2126t. 10: t := t+ 1. 11: End For Outlier Removal: Remove columns corresponding to entry 0 in vector C from B\u0302s0+r = [Es0 ,Ur]. Output: Estimated range space, identified outlier vectors, and recovered underlying matrix M\u0302 with column M\u0302:t.\nAs a concrete example, if the incoherence parameter \u00b5\u03c4 is a global constant and the dimension \u03c4 of each subspace is far less than r, the sample complexity of O(\u00b5\u03c4n\u03c42 log(r/\u03b4)) is significantly better than the complexity of O(\u00b50nr log(r/\u03b4)) for the structure of a single subspace in Theorem 5. This argument shows that the sparse connections between the two layers improve the sample complexity.\nProof of Theorem 11. We first study the effectiveness of our representation step.\nLemma 12. Let [Es,Uk] be the current dictionary matrix consisting of a random noise matrix Es \u2208 Rm\u00d7s and a clean basis matrix Uk \u2208 Rm\u00d7k. Suppose we get access to a set of coordinates \u2126 \u2282 [m] of size d uniformly at random without replacement. Let s \u2264 d\u2212\u03c4\u22121 and d \u2265 8\u00b5\u03c4\u03c4 log(\u03c4/\u03b4). Denote by U\u03c4 \u2208 Rm\u00d7\u03c4 a submatrix of Uk with \u03c4 columns.\n\u2022 If M:t \u2208 Ur but it cannot be represented by a linear combination of \u03c4 vectors in the current dictionary, then with probability at least 1\u2212 \u03b4, M\u2126t does not belong to any fixed \u03c4 -combination of the truncated dictionary as well.\n\u2022 If M:t can be represented by a linear combination of \u03c4 vectors in the current basis, then M\u2126t can be represented as a linear combination of the same \u03c4 truncated vectors in the dictionary with probability 1, the representation coefficients of M:t corresponding to Es in the dictionary is 0 with probability 1, and [Es,Uk][Es\u2126:,U k \u2126:] \u2020M\u2126t = M:t with probability at least 1\u2212 \u03b4.\n\u2022 If M:t is an outlier drawn from a non-degenerate distribution, then M\u2126t cannot be represented by the dictionary with probability 1.\nProof. The proof is similar as that of Lemma 8. For completeness, we give a brief proof here. For the first part of the lemma, by Facts 2 and 4 of Lemma 13, the Es\u2126: cannot have a non-zero representation\ncoefficient of M\u2126t in any possible \u03c4 -combination of the current dictionary when s \u2264 d\u2212 \u03c4 \u2212 1, due to the randomness. Thus the problem of whether M\u2126t can be \u03c4 represented by the current dictionary [Es\u2126:,U k \u2126:] is totally determined by whether it can be \u03c4 represented by Uk\u2126:. Now suppose that M\u2126t can be written as a linear \u03c4 -combination of the current basis U\u03c4\u2126:. Then according to Proposition 7, since d \u2265 8\u00b5\u03c4\u03c4 log(\u03c4/\u03b4), we have that rank([U\u03c4 ,M:t]) = \u03c4 , which is contradictory with the assumption of Event 1.\nThe first argument in Event 2 is obvious. Now suppose that the representation coefficients of M:t corresponding to Es in the dictionary [Es,Uk] is NOT 0 and M:t \u2208 Uk. Then M:t \u2212Ukc \u2208 span(Es), where c is the representation coefficients of M:t corresponding to Uk in the dictionary [Es,Uk]. Also, note that M:t \u2212Ukc \u2208 Uk. So rank[Es,M:t \u2212Ukc] = s, which is contradictory with Fact 2 of Lemma 13. So the coefficient w.r.t. Es in the dictionary [Es,Uk] is 0. Since by assumption M:t can be represented by \u03c4 combination of columns in Uk, termed U\u03c4 , we have that [Es,Uk][Es\u2126:,U k \u2126:] \u2020M\u2126t = U\n\u03c4U\u03c4\u2020\u2126:M\u2126t = U\u03c4 (U\u03c4T\u2126: U \u03c4 \u2126:) \u22121U\u03c4T\u2126: M\u2126t = U \u03c4 (U\u03c4T\u2126: U \u03c4 \u2126:) \u22121U\u03c4T\u2126: U \u03c4 \u2126:v = U\n\u03c4v = M:t, where v is the representation coefficient of M:t w.r.t. U\u03c4 . (The (U\u03c4T\u2126: U \u03c4 \u2126:) \u22121 exists because rank(U\u03c4\u2126:) = \u03c4 by Proposition 7)\nEvent 3 is an immediate result of Lemma 13.\nNow we are ready to prove Theorem 11. In fact, Theorem 11 is a result of union bound of Lemma 12. For the event of type 1, the union bound is over ( r \u03c4 ) = O(r\u03c4 ) events. For the event of type 2, since we resample \u2126 at most r + s0 times by algorithm, the union bound is over r + s0 samplings. The event of type 3 is with probability 1. So overall, replacing \u03b4 with min{\u03b4/r\u03c4 , \u03b4/(r + s0)} in Lemma 12, the sample complexity we need is at least O(\u00b5\u03c4\u03c4 log(max{r\u03c4 , r + s0}/\u03b4)). Note that s0 \u2264 d\u2212 \u03c4 \u2212 1. So the sample complexity for each column is at least O(\u00b5\u03c4\u03c42 log(r/\u03b4)) and the total one is O(\u00b5\u03c4\u03c42n log(r/\u03b4)), as desired. The success of outlier removal step is guaranteed by Lemma 9."}, {"heading": "4 Experimental Results", "text": "Bounded Deterministic Noise: We verify the estimated error of our algorithm in Theorem 1 under bounded deterministic noise. Our synthetic data are generated as follows. We construct 5 base vectors {ui}5i=1 by sampling their entries from N (0, 1). The underlying matrix L is then generated by L = [ u11 T 200, \u22112 i=1 ui1 T 200, \u22113 i=1 ui1 T 200, \u22114 i=1 ui1 T 200, \u22115 i=1 ui1 T 1,200 ] \u2208 R100\u00d72,000, each column of which is normalized to the unit `2 norm. Finally, we add bounded yet unstructured noise to each column, with noise level noise = 0.6. We randomly pick 20% entries to be unobserved. The left figure in Figure 4 shows the comparison between our estimated error4 and the true error by our algorithm. The result demonstrates that empirically, our estimated error successfully predicts the trend of the true algorithmic error.\nSparse Random Noise: We then verify the exact recoverability of our algorithm under sparse random noise. The synthetic data are generated as follows. We construct the underlying matrix L = XY as a product of m \u00d7 r and r \u00d7 n i.i.d. N (0, 1) matrices. The sparse random noise is drawn from standard Gaussian distribution such that s0 \u2264 d\u2212 r \u2212 1. For each size of problem (50\u00d7 500 and 100\u00d7 1, 000), we test with different rank ratios r/m and measurement ratios d/m. The experiment is run by 10 times. We define that the algorithm succeeds if \u2016L\u0302\u2212 L\u2016F \u2264 10\u22126, rank(L\u0302) = r, and the recovered support of the noise is exact for at least one experiment. The right two figures in Figure 4 plots the fraction of correct recoveries: white\n4The estimated error is up to a constant factor.\ndenotes perfect recovery by nuclear norm minimization approach (2); white+gray represents perfect recovery by our algorithm; black indicates failure for both methods. It shows that the success region of our algorithm strictly contains that of the prior approach. Moreover, the phase transition of our algorithm is nearly a linear function w.r.t r and d. This is consistent with our prediction d = \u2126(\u00b50r log(r/\u03b4)) when \u03b4 is small, e.g., poly(1/n).\nMixture of Subspaces: To test the performance of our algorithm for the mixture of subspaces, we conduct an experiment on the Hopkins 155 dataset. The Hopkins 155 database is composed of 155 matrices/tasks, each of which consists of multiple data points drawn from two or three motion objects. The trajectory of each object lie in a subspace. We input the data matrix to our algorithm with varying sample sizes. Table 2 records the average relative error \u2016L\u0302\u2212 L\u2016F /\u2016L\u2016F of 10 trials for the first five tasks in the dataset. It shows that our algorithm is able to recover the target matrix with high accuracy.\nSingle Subspace v.s. Mixture of Subspaces: We compare the sample complexity of Algorithm 2 (Single Subspace) and Algorithm 3 (Mixture of Subspaces) for the exact recovery of the underlying matrix. The data are generated as follows. We construct 5 independent subspaces {Si}5i=1 whose bases {Ui}5i=1 are 100\u00d7 4 random matrices consisting of orthogonal columns (\u03c4 = 4 and r = 20). We then sample 20 data from each\nsubspace uniformly and obtain a 100 \u00d7 100 data matrix. The sample size d varies from 1 to 100, and we record the empirical probability of success over 200 times of experiments, where we define that an algorithm succeeds if \u2016L\u0302\u2212 L\u2016F \u2264 10\u22126 and rank(L\u0302) = r. As shown in Figure 5, we see that the sample complexity can indeed be smaller in the case of mixture of subspaces."}, {"heading": "5 Conclusions", "text": "In this paper, we study life-long matrix completion that aims at online recovering an m\u00d7 n matrix of rank r under two realistic noise models \u2014 bounded deterministic noise and sparse random noise. Our result advances the state-of-the-art work and matches the lower bound under sparse random noise. In a more benign setting where the columns of the underlying matrix lie on a mixture of subspaces, we show that a smaller sample complexity is possible to exactly recover the target matrix. It would be interesting to extend our results to other realistic noise models, including random classification noise or malicious noise previously studied in the context of supervised classification [ABL14, BF13]\nAcknowledgements. This work was supported in part by NSF grants NSF CCF-1422910, NSF CCF-1535967, NSF CCF-1451177, NSF IIS-1618714, a Sloan Research Fellowship, and a Microsoft Research Faculty Fellowship."}, {"heading": "A Facts on Subspace Spanned by Non-Degenerate Random Vectors", "text": "Lemma 13. Let Es \u2208 Rm\u00d7s be matrix consisting of corrupted vectors drawn from any non-degenerate distribution. Let Uk \u2208 Rm\u00d7k be any fixed matrix with rank k. Then with probability 1, we have\n\u2022 rank(Es) = s for any s \u2264 m;\n\u2022 rank([Es,x]) = s+ 1 holds for x \u2208 Uk \u2282 Rm uniformly and s \u2264 m\u2212 k, where x can even depend on Es;\n\u2022 rank([Es,Uk]) = s+ k, provided that s+ k \u2264 m;\n\u2022 The marginal of non-degenerate distribution is non-degenerate.\nProof. For simplicity, we only show the proof of Fact 1. The other facts can be proved similarly. Let Es = [Es\u22121, e]. Since e is drawn from a non-degenerate distribution, the conditional probability satisfies Pr[rank(Es\u22121, e) = s | Es\u22121] = 1 by the definition of non-degenerate distribution. So Pr[rank(Es\u22121, e) = s] = EEs\u22121 Pr[rank(Es\u22121, e) = s | Es\u22121] = 1."}, {"heading": "B Equivalence between Bernoulli and Uniform Models", "text": "Lemma 14. Let n be the number of Bernoulli trials and suppose that \u2126 \u223c Ber(d/n). Then with probability at least 1\u2212 \u03b4, |\u2126| = \u0398(d), provided that d \u2265 4 log(1/\u03b4).\nProof. Take a perturbation such that d/n = d0/n+ . By the scalar Chernoff bound which states that\nPr(|\u2126| \u2264 d0) \u2264 e\u2212 2n2/2d0 ,\nif taking d0 = d/2, = d/2n and d \u2265 4 log(1/\u03b4), we have\nPr(|\u2126| \u2264 d/2) \u2264 e\u2212d/4 \u2264 \u03b4. (3)\nIn the other direction, by the scalar Chernoff bound again which states that\nPr(|\u2126| \u2265 d0) \u2264 e\u2212 2n2/3d,\nif taking d0 = 2d, = \u2212d/n and d \u2265 4 log(1/\u03b4), we obtain\nPr(|\u2126| \u2265 2d) \u2264 e\u2212d/3 \u2264 \u03b4. (4)\nFinally, according to (3) and (4), we conclude that d/2 < |\u2126| < 2d with probability at least 1\u2212 \u03b4."}, {"heading": "C A Collection of Concentration Results", "text": "Lemma 15 (Theorem 6. [KS14]). Denote by U\u0303k a k-dimensional subspace in Rm. Let the sampling number d \u2265 max{83k\u00b5(U\u0303 k) log(2k\u03b4 ), 4\u00b5(PU\u0303k\u22a5y) log( 1 \u03b4 )}. Denote by \u2126 an index set of size d sampled uniformly at random with replacement from [m]. Then with probability at least 1\u2212 4\u03b4, for any y \u2208 Rm, we have\nd(1\u2212 \u03b1)\u2212 k\u00b5(U\u0303k) \u03b21\u2212\u03b6 m \u2225\u2225y \u2212 P U\u0303k y \u2225\u22252 2 \u2264 \u2225\u2225\u2225y\u2126 \u2212 PU\u0303k\u2126:y\u2126\u2225\u2225\u222522 \u2264 (1 + \u03b1) dm \u2225\u2225y \u2212 PU\u0303ky\u2225\u222522 ,\nwhere\u03b1 = \u221a 2 \u00b5(P U\u0303k\u22a5y)\nd log(1/\u03b4)+ 2\u00b5(P U\u0303k\u22a5y) 3d log(1/\u03b4), \u03b2 = (1+2 log(1/\u03b4)) 2, and \u03b6 =\n\u221a 8k\u00b5(U\u0303k)\n3d log(2r/\u03b4).\nLemma 16 (Matrix Chernoff Bound. [GT11]). Consider a finite sequence {Xk} \u2208 Rn\u00d7n of independent, random, Hermitian matrices. Assume that\n0 \u2264 \u03bbmin(Xk) \u2264 \u03bbmax(Xk) \u2264 L.\nDefine Y = \u2211\nk Xk, and \u00b5r as the r-th largest eigenvalue of the expectation EY, i.e., \u00b5r = \u03bbr(EY). Then\nPr {\u03bbr(Y) > (1\u2212 )\u00b5r} \u2265 1\u2212 r [\ne\u2212\n(1\u2212 )1\u2212\n]\u00b5r L\n\u2265 1\u2212 re\u2212 \u00b5r\n2\n2L for \u2208 [0, 1)."}], "references": [{"title": "The power of localization for efficiently learning linear separators with noise", "author": ["Pranjal Awasthi", "Maria Florina Balcan", "Philip M Long"], "venue": "In ACM Symposium on Theory of Computing,", "citeRegEx": "Awasthi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2014}, {"title": "Efficient representations for life-long learning and autoencoding", "author": ["Maria Florina Balcan", "Avrim Blum", "Santosh Vempala"], "venue": "In Annual Conference on Learning Theory,", "citeRegEx": "Balcan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2015}, {"title": "Statistical active learning algorithms", "author": ["Maria Florina Balcan", "Vitaly Feldman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Balcan and Feldman.,? \\Q2013\\E", "shortCiteRegEx": "Balcan and Feldman.", "year": 2013}, {"title": "Lambertian reflectance and linear subspaces", "author": ["Ronen Basri", "David W Jacobs"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Basri and Jacobs.,? \\Q2003\\E", "shortCiteRegEx": "Basri and Jacobs.", "year": 2003}, {"title": "Online identification and tracking of subspaces from highly incomplete information", "author": ["Laura Balzano", "Robert Nowak", "Benjamin Recht"], "venue": "In Annual Allerton Conference on Communication,", "citeRegEx": "Balzano et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Balzano et al\\.", "year": 2010}, {"title": "High-dimensional matched subspace detection when data are missing", "author": ["Laura Balzano", "Benjamin Recht", "Robert Nowak"], "venue": "In IEEE International Symposium on Information Theory,", "citeRegEx": "Balzano et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Balzano et al\\.", "year": 2010}, {"title": "Toward an architecture for never-ending language learning", "author": ["Andrew Carlson", "Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R. Hruschka Jr.", "Tom M. Mitchell"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Carlson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "A multibody factorization method for independently moving objects", "author": ["J. Costeira", "T. Kanade"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Costeira and Kanade.,? \\Q1998\\E", "shortCiteRegEx": "Costeira and Kanade.", "year": 1998}, {"title": "Matrix completion with noise", "author": ["Emmanuel J Cand\u00e8s", "Yaniv Plan"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Cand\u00e8s and Plan.,? \\Q2010\\E", "shortCiteRegEx": "Cand\u00e8s and Plan.", "year": 2010}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "author": ["Emmanuel J Cand\u00e8s", "Justin Romberg", "Terence Tao"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2006}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["E.J. Cand\u00e8s", "T. Tao"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cand\u00e8s and Tao.,? \\Q2010\\E", "shortCiteRegEx": "Cand\u00e8s and Tao.", "year": 2010}, {"title": "Online matrix completion through nuclear norm regularisation", "author": ["Charanpal Dhanjal", "Romaric Gaudel", "St\u00e9phan Cl\u00e9mencon"], "venue": "In SIAM International Conference on Data Mining,", "citeRegEx": "Dhanjal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dhanjal et al\\.", "year": 2014}, {"title": "The spectral norm error of the na\u00efve Nystr\u00f6m extension", "author": ["Alex Gittens"], "venue": "arXiv preprint arXiv:1110.5305,", "citeRegEx": "Gittens.,? \\Q2011\\E", "shortCiteRegEx": "Gittens.", "year": 2011}, {"title": "How babies think: the science of childhood", "author": ["Alison Gopnik", "Andrew N Meltzoff", "Patricia Katherine Kuhl"], "venue": null, "citeRegEx": "Gopnik et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Gopnik et al\\.", "year": 2001}, {"title": "Tail bounds for all eigenvalues of a sum of random matrices", "author": ["Alex Gittens", "Joel A Tropp"], "venue": "arXiv preprint:", "citeRegEx": "Gittens and Tropp.,? \\Q2011\\E", "shortCiteRegEx": "Gittens and Tropp.", "year": 2011}, {"title": "Metrics and models for handwritten character recognition", "author": ["Trevor Hastie", "Patrice Y Simard"], "venue": "Statistical Science,", "citeRegEx": "Hastie and Simard.,? \\Q1998\\E", "shortCiteRegEx": "Hastie and Simard.", "year": 1998}, {"title": "Low-rank matrix and tensor completion via adaptive sampling", "author": ["Akshay Krishnamurthy", "Aarti Singh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krishnamurthy and Singh.,? \\Q2013\\E", "shortCiteRegEx": "Krishnamurthy and Singh.", "year": 2013}, {"title": "On the power of adaptivity in matrix completion and approximation", "author": ["Akshay Krishnamurthy", "Aarti Singh"], "venue": "arXiv preprint arXiv:1407.3619,", "citeRegEx": "Krishnamurthy and Singh.,? \\Q2014\\E", "shortCiteRegEx": "Krishnamurthy and Singh.", "year": 2014}, {"title": "Online completion of ill-conditioned low-rank matrices", "author": ["Ryan Kennedy", "Camillo J Taylor", "Laura Balzano"], "venue": "In IEEE Global Conference on Signal and Information,", "citeRegEx": "Kennedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kennedy et al\\.", "year": 2014}, {"title": "Online matrix completion and online robust PCA", "author": ["Brian Lois", "Namrata Vaswani"], "venue": "In IEEE International Symposium on Information Theory,", "citeRegEx": "Lois and Vaswani.,? \\Q2015\\E", "shortCiteRegEx": "Lois and Vaswani.", "year": 2015}, {"title": "`p-recovery of the most significant subspace among multiple subspaces with outliers", "author": ["Gilad Lerman", "Teng Zhang"], "venue": "Constructive Approximation,", "citeRegEx": "Lerman and Zhang.,? \\Q2014\\E", "shortCiteRegEx": "Lerman and Zhang.", "year": 2014}, {"title": "A simpler approach to matrix completion", "author": ["Benjamin Recht"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Recht.,? \\Q2011\\E", "shortCiteRegEx": "Recht.", "year": 2011}, {"title": "Matched subspace detectors", "author": ["Louis L Scharf", "Benjamin Friedlander"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Scharf and Friedlander.,? \\Q1994\\E", "shortCiteRegEx": "Scharf and Friedlander.", "year": 1994}, {"title": "Randomized online pca algorithms with regret bounds that are logarithmic in the dimension", "author": ["Manfred K Warmuth", "Dima Kuzmin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Warmuth and Kuzmin.,? \\Q2008\\E", "shortCiteRegEx": "Warmuth and Kuzmin.", "year": 2008}, {"title": "Robust PCA via outlier pursuit", "author": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "IEEE Transaction on Information Theory,", "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}, {"title": "Completing low-rank matrices with corrupted samples from few coefficients in general basis", "author": ["Hongyang Zhang", "Zhouchen Lin", "Chao Zhang"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Exact recoverability of robust PCA via outlier pursuit with tight recovery bounds", "author": ["H. Zhang", "Z Lin", "C. Zhang", "E. Chang"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Relations among some low rank subspace recovery models", "author": ["Hongyang Zhang", "Zhouchen Lin", "Chao Zhang", "Junbin Gao"], "venue": "Neural Computation,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "We study the problem of recovering an incomplete m \u00d7 n matrix of rank r with columns arriving online over time. This is known as the problem of life-long matrix completion, and is widely applied to recommendation system, computer vision, system identification, etc. The challenge is to design provable algorithms tolerant to a large amount of noises, with small sample complexity. In this work, we give algorithms achieving strong guarantee under two realistic noise models. In bounded deterministic noise, an adversary can add any bounded yet unstructured noise to each column. For this problem, we present an algorithm that returns a matrix of a small error, with sample complexity almost as small as the best prior results in the noiseless case. For sparse random noise, where the corrupted columns are sparse and drawn randomly, we give an algorithm that exactly recovers an \u03bc0-incoherent matrix by probability at least 1\u2212 \u03b4 with sample complexity as small asO (\u03bc0rn log(r/\u03b4)). This result advances the state-of-the-art work and matches the lower bound in a worst case. We also study the scenario where the hidden matrix lies on a mixture of subspaces and show that the sample complexity can be even smaller. Our proposed algorithms perform well experimentally in both synthetic and real-world datasets.", "creator": "LaTeX with hyperref package"}}}