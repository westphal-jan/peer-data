{"id": "1612.00132", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "CDVAE: Co-embedding Deep Variational Auto Encoder for Conditional Variational Generation", "abstract": "Problems such as predicting an optical flow field (Y) for an image (X) are ambiguous: many very distinct solutions are good. Representing this ambiguity requires building a conditional model P(Y|X) of the prediction, conditioned on the image. It is hard because training data usually does not contain many different flow fields for the same image. As a result, we need different images to share data to produce good models. We demonstrate an improved method for building conditional models, the Co-Embedding Deep Variational Auto Encoder. Our CDVAE exploits multiple encoding and decoding layers for both X and Y. These are tied during training to produce a model of the joint distribution P(X, Y), which provides the necessary smoothing. Our tying procedure is designed to yield a conditional model easy at test time. We demonstrate our model on three example tasks using real data: image saturation adjustment, image relighting, and motion prediction. We describe quantitative evaluation metrics to evaluate ambiguous generation results. Our results quantitatively and qualitatively advance the state of the art. Our findings have been described as promising and validated in various disciplines.\n\n\n\nThe method used in our study describes a mathematical solution that incorporates an explicit specification of the type of optimization algorithm used in the design. The solution allows the modeling of a finite set of possible inputs. To evaluate the optimal output of the optimization algorithm, the model can specify input parameters by the appropriate values or parameter values to be used in the analysis. The model can optionally specify the optimal output of the optimization algorithm by the appropriate values or parameter values to be used in the analysis. The model can optionally specify the optimal output of the optimization algorithm by the appropriate values or parameter values to be used in the analysis. The model can optionally specify the optimal output of the optimization algorithm by the appropriate values or parameter values to be used in the analysis.\n\nThe model can optionally specify the optimal output of the optimization algorithm by the appropriate values or parameter values to be used in the analysis. The model can optionally specify the optimal output of the optimization algorithm by the appropriate values or parameter values to be used in the analysis. The model can optionally specify the optimal output of the optimization algorithm by the appropriate values or parameter values to be used in the analysis. The model can optionally specify the optimal output of the optimization algorithm by the appropriate values or parameter values to be used in the analysis. In our study, we found that performance improvement is achieved when the optimization algorithm is implemented in the model. In this model, performance", "histories": [["v1", "Thu, 1 Dec 2016 03:40:42 GMT  (8992kb,D)", "https://arxiv.org/abs/1612.00132v1", null], ["v2", "Tue, 28 Mar 2017 03:21:34 GMT  (7823kb,D)", "http://arxiv.org/abs/1612.00132v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.GR", "authors": ["jiajun lu", "aditya deshpande", "david forsyth"], "accepted": false, "id": "1612.00132"}, "pdf": {"name": "1612.00132.pdf", "metadata": {"source": "CRF", "title": "CDVAE: Co-embedding Deep Variational Auto Encoder for Conditional Variational Generation", "authors": ["Jiajun Lu", "Aditya Deshpande", "David Forsyth"], "emails": ["daf}@illinois.edu"], "sections": [{"heading": "1. Introduction", "text": "Many vision problems have ambiguous solutions. There are many motion fields consistent with an image [19, 25, 26]. Similarly, there are many shading fields consistent with the layout of an image (Figure 1); many good ways to colorize an image [3, 27, 29]; many possible ways to adjust the saturation of an image (Figure 1); many possible long term futures for an image frame [30]; and so on. For each of these problems, one must output a spatial field Y for an input image X; but Y is not uniquely determined by X . Worse, Y has complex spatial structure (for example, saturation at a pixel is typically similar to the saturation at the next pixel, except over boundaries). It is natural to build a generative model of Y , conditioned on X , and draw samples from that model. Towards this end, recent work has modified a strong generative model for images that uses latent variables, the\nvariational auto-encoder (VAE) of [7], to produce a conditional VAE (CVAE) [19, 25, 26]. The very high dimension and complex spatial covariances of Y are managed by the underlying latent variable model in the CVAE.\nHowever, building a good conditional model still poses some challenges. In most practical vision problems, the training dataset we can access is \u201cscattered\u201d, and the model is multimodal. Scattered training data consists of pairs of (yi, xi), but we never see multiple different values of yi for one particular xi. Practical vision models compute some intermediate representation c(x) (which we call the \u201ccode\u201d), and predict y from that intermediate representation and a random (or latent) variable u. Write y = F (u; c(x)). The hope is that different choices of the random variable will produce different values of y, and we can therefore predict the entire gamut of outputs (shading, motion field, saturation etc.). This implies that the method can represent a multimodal P (Y |X).\n1\nar X\niv :1\n61 2.\n00 13\n2v 2\n[ cs\n.C V\n] 2\n8 M\nar 2\n01 7\nThis setting presents a difficulty. To build a plausible conditional model, we must smooth. The model smooths by allowing examples with similar codes to \u201cshare\u201d y values. In turn, the choice of code is crucial, and a poor choice of code can result in a method that appears to work well, but does not. Figure 2 illustrates this point. The figure shows two possible choices of code for a particular dataset. In the good choice of code, F is forced to use the random variable to produce distinct values of y for similar x, because similar x result in similar codes. In the poor choice of code, similar x can have different codes (viz. different codes A and B for similar x in the right side of Figure 2) and vice versa. Then, F can largely ignore the random variable, but simulates a multimodal process by producing very different y for quite similar x using the different c(x). This means the network we have trained will (a) not be effective at making diverse predictions and (b) may change its prediction very significantly for a small change in x, in a manner that is uncontrolled by the code. This is not desirable behavior.\nWe call the effect \u201ccode collapse\u201d, because the network is encouraged to produce similar codes for different inputs. The result is a model with imperfect diversity and generalization, but good loss on scattered training data. The absence of variance, in what should be a diverse pool of output predictions, is a strong diagnostic indicator of code collapse. We exploit this indicator and show that our baselines, particularly the CVAE, generate low variance and therefore suffer from code collapse (Section 5.2, Figure 5).\nThe key problem, resulting in code collapse, is that the current training procedures have no term to force a good choice of codes. For example, VAE loss requires the code distribution to look like a standard normal distribution. This loss does not force it to preserve the similarity, dissimilar input images can be closer and similar inputs further apart in the code space. Recent work shows that better generative models are obtained by conditioning on text-\nembeddings [15] or pre-trained features from visual recognition network [12]. This suggests that using an embedding with some structure is better than conditioning on raw pixels (with high-capacity networks).\nIn our approach, instead of using a fixed embedding as input, we use raw pixels but guide the codes (or intermediate representation) with a metric constraint. Our metric term encourages codes c(x) for distinct x to be different and codes for similar x to be similar. This prevents code collapse. To ensure that F (u; c(x)) will vary for similar c(x), we use a Mixture Density Network (MDN) [2]. MDN explicitly models a multimodal conditional distribution. We call our model CDVAE (Co-Embedding Deep Variational Auto Encoder).\nWe apply CDVAE to two novel (from the point of view of automated editing) problems: (a) Photo Relighting, (b) Image Resaturation (Section 4). In relighting (or reshading), we decompose the image into shading and albedo, then produce a new shading field consistent with the albedo field. In resaturation, we produce a new saturation field and apply it to the image. In each case, the resulting image should look \u201cnatural\u201d \u2013 like a real image, but differently illuminated (reshading) or with differently color saturated objects (resaturation). In all cases, our model outperforms strong baselines (including the CVAE).\nContributions:\n\u2022 We describe a novel method to build conditional models for extremely demanding datasets (Section 3, Figure 3) and apply it to photo-editing tasks (Section 4).\n\u2022 We show how to regularize our model so that the latent representation is not distorted, and this helps us improve results (Section 3.2 and Figure 5).\n\u2022 Our method is compared to a variety of strong baselines, and produces predictions that (a) have high variance and (b) have high accuracy (Section 5.3 and Section 5.2). Our method clearly outperforms existing models.\n\u2022 Training previous conditional models is hard, these models tend to either go to code collapse or random prediction. Our methods can avoid code collapse and create multiple distinct plausible results (Section 5.4 and Figure 6)."}, {"heading": "2. Related Work", "text": "Generating a spatial field with complex spatial structure from an image is an established problem. Important application examples, where the prediction is naturally ambiguous, include colorization [4, 9, 29], style transfer [5], temporal transformations prediction [30], predicting motion fields [19, 25, 26], and predicting future frames [24].\nThis is usually treated as a regression problem; current state of the art methods use deep networks to learn features. However, predicting the expected value of a conditional distribution, through regression, works poorly, because the expected value of a multimodal distribution may have low probability. While one might temper the distribution (eg [29]), the ideal is to obtain multiple different solutions.\nOne strategy is to draw samples from a generative model. Generative models of images present problems of dimension; these can be addressed with latent variable models, leading to the variational autoencoder (VAE) of [7]. As the topic attracts jargon, we review the standard VAE briefly here. This approach learns an encoder E that maps data x into a continuous latent variable z = E(x) (the codes), and a decoder D that maps z to an image x\u0302 = D(z). Learning ensures that (a) x and D(E(x)) are close; (b) if \u03be is close to z, then D(\u03be) is close to D(z); and (c) z is distributed like a standard normal random variable. Images can then be generated by sampling a standard normal distribution to get \u03be, and forming D(\u03be). This is a latent variable model, modelling P (x) as P (x) = \u222b z P (x|z)P (z)dz. P (x|z) is represented by the decoder, we use an auxiliary distribution Q = P (z|x) for P (z), where P (z|x) is now the encoder. Learning is by maximizing a lower-bound on log-likelihood (Equation 1)\nVAE(\u03b8) = \u2211 data EQ[logP (x|z)]\u2212 D(Q||P (z)) (1)\nwhere \u03b8 are the parameters of encoder and decoder networks of the VAE.\nCurrent generative models are reliable in simple do-\nmains (handwritten digits [7, 18]; faces [7, 8, 16]; and CIFAR images [6]) but less so for general images. Improvements are available using multiple layers of latent variables (a DVAE) [16]. Models can be trained with adversarial loss [14]. However, these deep models are still hard to train. The ladder VAE imposes both top-down and bottom up variational distributions for more efficient training [20].\nThe generative model needs to be conditioned on an image, and needs to be multimodal. Tang et al. give a multimodal conditional model [21], but the conditioning variables are binary. A conditional variational autoencoder (CVAE) [19] conditions the decoder model on a continuous representation produced by a network applied to x. This approach has been demonstrated on motion prediction problems [25, 30].\nWe use the mixture density network (MDN) in our models to capture the underlying multimodal distribution [2]. MDN predicts the parameters of a mixture of gaussians from real-valued input features. MDNs have been successfully applied to articulatory-acoustic inversion mapping [17, 22] and speech synthesis [28]."}, {"heading": "3. Method", "text": "Our CDVAE consists of two deep variational auto encoders (DVAE) [16] and a mixture density network (MDN) [2]. An overview of the CDVAE model is shown in Figure 3.\nOur architecture is, we use two DVAEs to embed the conditioning image xc (or X from Section 1) and the generated image xg (or Y from Section 1) into two lowdimensional latent variables (or code spaces) zc and zg . The\ngenerated image corresponds to output spatial field viz. saturation or shading etc. and the conditioning image is the input image viz. intensity or albedo etc. Next, we regularize the latent variables zc with embedding guidance (or metric constraints) such that the similarity in input space is maintained (Section 3.2). Since our problem is ambiguous, the conditional distribution between zg and zc is multimodal. MDN allows us to fit a multimodal gaussian mixture model (GMM) between the conditioning code zc and the generated code zg (Section 3.1). At test time, we sample from this multimodal GMM (zg \u223c MDN ) and use the decoder of the generated image DVAE (Decoder(zg)) to predict different shading, saturation for a single input image.\nIn Figure 3, we simplify and show a single layer of latent variables. In practice, our DVAE utilizes multiple layers of gaussian latent variables. This hierarchical variational auto encoder architecture captures complex structure in the data, and provides good practical performance.\nWe jointly train the two DVAEs and the MDN, allowing them to adapt and mutually benefit each other. Joint training also enables CDVAE to model a joint probability distribution of xc and xg , instead of a conditional probability distribution (viz. like a CVAE). The joint probability model allows for more smoothing between data points. In CDVAE, we optimize the joint probability model P (xc, xg) during training (Refer Equation 25). At test time, we remove the decoder for the conditioning layer and the encoder for the generated layer. This converts the joint model of CDVAE into a conditional model P (xg|xc). We can then sample this conditional model to generate diverse outputs. Similar to VAE probability model, we write the joint probability by marginalizing over the joint distribution P (zg, zc)\nP (xc,xg)= \u222b zc \u222b zg P (xc|zc)P (xg|zg)P (zg,zc)dzgdzc (2) In Section 3.1, we derive the loss terms corresponding to\nthis joint probability model of CDVAE."}, {"heading": "3.1. CDVAE Loss", "text": "In CDVAE, we use two multi-layer variational auto encoders (DVAE), one for xc and one for xg . Additionally, we have an MDN that models the relationship between the embeddings zc and zg . The loss function LCDV AE corresponding to the CDVAE joint model P (xc, xg) is a combination of the two DVAE models and the conditional probability model of MDN. In Equations 1 and 25, assume it is possible to encode xc without seeing xg , then we can use the auxiliary sampling distribution Q = P (zc|xc)P (zg|xg) for our CDVAE. If P (zg, zc) = P (zg)P (zc) in Equation 25, we can separate out the product terms of joint probability model. Taking negative log-likelihood, we obtain separate additive loss terms for each DVAE. Write DVAE(\u03b8) for the\nloss function of a DVAE with weights (or parameters) \u03b8, which has the standard form of a VAE loss (Equation 1). Write the loss of the DVAE for the generated image xg as DVAE(\u03b8g) (similarly for xc). Write loss for the MDN as Lmdn. We can then derive the loss function for CDVAE (Equation 3),\nLCDV AE = DVAE(\u03b8c) + DVAE(\u03b8g) +Lmdn(\u03b8c, \u03b8g) (3)\nOur MDN estimates the conditional probability model P (zg|zc). For each input code zc, our MDN estimates the parameters of aK component gaussian mixture distribution with mixture coefficients \u03c0i, means \u00b5i and fixed diagonal covariance \u03c32i . The loss Lmdn is obtained by taking the negative log-likelihood of this conditional distribution,\nLmdn=\u2212EQ [ log\nK\u2211 k=1 \u03c0k(zc)N (zg|\u00b5k(zc),\u03c32k(zc))\n] (4)\nWe use an inference method [20] that co-relates the latent variables of the encoder and the decoder. This speeds up the training. Refer to the supplementary materials for the detailed derivations."}, {"heading": "3.2. Embedding Guidance: Preventing code collapse", "text": "Vanilla DVAEs have difficulty in learning a code space which encodes the complex spatial structure of the output. The code space learned by a DVAE appears to be underdetermined, especially for large and complex datasets. This is a common failure mode of VAEs. For our conditional models, it is desirable that codes for \u201csimilar\u201d inputs are nearby, and codes for \u201cvery different\u201d inputs are further apart. This discourages the method from grouping together very different inputs. It also prevents similar images from having\ndifferent codes, and therefore avoids incorrect smoothing of the model (See Figure 2). We guide the codes (at multiple layers) to be similar to a pre-computed embedding. Our pre-computed embedding is such that it preserves the similarity observed in input domain. Refer to Figure 4 and supplementary material for the details of our pre-computed embedding. Write p for the pre-computed embedding and zc for the gaussian latent variables of the network. We use the L2-norm between p and zc as a loss term\nLembed = \u2016zc \u2212 p\u201622 (5)\nWrite L for the final loss function with the additional regularization in the form of embedding guidance\nL = LCDV AE + \u03bbLembed (6)\nWe use a large value of \u03bb when training starts and gradually reduce it during the training process."}, {"heading": "3.3. Post Processing", "text": "Current deep generative models only handle small images, for our case 32\u00d732, and they generate results without high spatial frequencies. We post process generated images for viewing at high resolution (not used in any quantitative evaluation). Our post processing upsamples results to a resolution of 512 \u00d7 512, with more details. We aggressively upsample the generated fields with the approach in [10], which preserves edges during upsampling. In particular, the method represents a high resolution field by a weight and an orientation value at each sample point; these parameters control a basis function placed at each sample, and the field at any test point is a sum of the values of the basis functions at that point. The sum of basis functions includes only those basis functions in the same image segment. Write Interp(w, \u03b8;S(I)) for the high resolution field produced by interpolating a weight vector w and a vector of orientations \u03b8, assuming a segment mask S(I) obtained from the high resolution grey level image. Write Y (d) for a low resolution field produced by the decoder, and \u2193 G\u03c3\u2217 for a process that smooths and downsamples. We solve\n\u2193 G\u03c3 \u2217 (Interp(w, \u03b8;S(I))) = Y (d)\nfor w, \u03b8, regularizing with the magnitude of w. This produces a high resolution field that (a) is compatible with the high resolution edges of the grey level image (unlike the learned upsampling in common use) and (b) produces the decoder sample when smoothed.\nFor relighting and saturation adjustment tasks, we polish images for display by using detail maps to recover fine scale details. The detail map is calculated by taking the conditioning image Ic and subtracting the output produced by the conditioning image decoder with the code zc. This\ncaptures the high frequency details lost during the neural network process. We get our result I\u0302g as\nI\u0302g = CDVAE(zg) + (Ic \u2212 CDVAE(zc)) (7)"}, {"heading": "4. Applications", "text": "We apply our methods to two different ambiguous tasks, each of which admits both quantitative and qualitative evaluations.\nPhoto Relighting (or Reshading): In this application, we predict new shading fields for images which are consistent with a relit version of the scene. We decompose the image into albedo (the conditioning image xc), and shading (the generated image xg). In real images, shading is quite strongly conditioned by albedo because the albedo image contains semantic information such as the scene categories, object layout and potential light sources. A scene can be lighted in multiple ways, so relighting is a multimodal problem. We use the MS-COCO dataset for relighting (Section 5.1).\nImage Resaturation: Here, we predict new saturation fields for color images, i.e. we modify color saturation for some or all objects in the image. We transform the input RGB color image into HSV color space, and use the value channel as our conditioning image and the saturation channel as our generated image. We use CDVAE to generate new saturation fields consistent with the input value image. Combining the new saturation fields with H and V channels leads to new versions of an image, with edited saturation. See the grilled cheese on the broccoli in Figure 1 which demonstrates that we obtain natural edits. Again, we use the MS-COCO dataset for this application (Section 5.1)."}, {"heading": "5. Results", "text": "To evaluate the effectiveness of our method, we compare with recent strong methods (Section 5.3). We also evaluate different variants of our method. We perform quantitative and qualitative comparison on applications of photo relighting, image resaturation. Quantitative results (Section 5.2, Figure 5) are computed on the network output, without any post processing. Images shown for qualitative evaluation of resaturation and reshading (Section 5.4, Figures 1, 6, 8 and 7) are post processed using the steps described in Section 3.3. We downsample all the input images to 32 \u00d7 32 dimensions, and all neural network operations before post processing are performed on this image size. After our CDVAE model generates samples, our post processing upsamples it to a resolution of 512\u00d7 512."}, {"heading": "5.1. Datasets", "text": "MS-COCO: We use MS-COCO dataset for our both the tasks, photo relighting and image resaturation. It is a\nwild dataset (unlike the structured face data commonly used by generative models), and has complex spatial structure. Typically, such data is challenging for VAE-based methods. We use train2014 (80K images) for model training, and sample 6400 images from val2014 (40K images) for testing. For photo relighting, intrinsic image decomposition method from [1] is used to obtain albedo and shading images. For image resaturation, we transform the image from RGB space to HSV space."}, {"heading": "5.2. Quantitative Metrics and Evaluation", "text": "Error-of-Best to ground truth: We follow the motion prediction work [25] to use error-of-best to ground truth as an evaluation metric. We draw 100 samples for each conditional model and calculate the minimum per pixel error to ground truth fields. A better model will produce smaller values, because it should produce some samples closer to the ground truth.\nVariance: A key goal of our paper is to generate diverse solutions. However, no current evaluation regime can tell whether a pool of diverse solutions is right. We opt for the strictly weaker proxy of measuring variance in the pool, on the reasonable assumption that diverse predictions for our problems must have high variance. Thus, procedures that produce low variance predictions are clearly unacceptable. Clearly, it is not enough just to produce variance \u2013 we want the pool to contain appealing diverse predictions. To assess this, we rely on qualitative evaluations (Figures 6, 7, 8). The supplementary materials contain many additional qualitative results.\nTo compute variance, we obtain the values for 16 (4\u00d7 4) equally spaced grid (since distant pixels are de-correlated to some extent) of pixels in our 32\u00d7 32 prediction. We collect these values across 100 samples, and compute the variance at each grid point across samples. We report this average variance vs. minimum error (See Figure 5). In particular, a method with more diverse output predictions should result in higher variances and one of them should also be close to the ground-truth (therefore, low minimum error). Specifically, we need to be in the bottom-right part of Figure 5, which our CDVAE achieves.\nTherefore, our CDVAE model creates results with desirable properties: lower error-of-best to ground truth and large variance. Our CDVAE model produces better results with more gaussian kernels (CDVAE12 vs. CDVAE4) and performance drops (higher minimum error and low variance) when no embedding guidance is used (CDVAE4 noemb)."}, {"heading": "5.3. Baseline Methods", "text": "Nearest neighbor (NN): We perform top\u2212k nearest neighbor (NN) search in xc space, and return the k corresponding xg as multiple outputs. Gaussian smoothing is ap-\nplied to returned xg to remove inconsistent high frequency signal. Since our training data does not have explicit oneto-many correspondences, NN is a natural strong baseline. It is a non-parametric method to model multimodal distribution by borrowing output spatial fields (we also smooth these) from nearby input images.\nConditional variational autoencoder: We implement a CVAE similar to [25]. We cannot use [25] since their architecture is specific to prediction of coarse motion trajectories. Our decoder is modelled on the DCGAN architecture of Radford et al. [14] with 5 deconvolution layers, and we use codes of dimension 64 (to be consistent with CDVAE). Our image tower and encoder tower use 5 convolutional layers (mirror image of the decoder). We use the same strategy as [25], i.e. we spatially replicate code from encoder and multiply it to the output of image tower. The decoder takes as input the result of this. We train our CVAE with the standard pixel-wise L2 loss on output and KL-divergence loss on the code space. At test time, codes are randomly sampled from the normal distribution.\nConditional GAN: CGAN [11] is another conditional image generation model. It uses a regularized code (drawn from a uniform distribution) along with a fixed embedding of the conditioning image as input. We observe that CGAN achieves higher minimum error (or error-of-best) and lower variance as compared to CDVAE (See Figure 5). Therefore, we generate better (lower minimum error) and more diverse (higher variance) predictions that CGAN. These metrics are explained in detail in Section 5.2.\nConditional PixelCNN (CPixel): Conditional PixelCNN [23] uses masked and gated convolutions (sigmoid and tanh activations layers multiplied). The receptive field of masked convolutions mimics the causal dependency and gated convolutions approximate the behavior of LSTM gates in recurrent architectures. Therefore, PixelCNN (CPixel) feasibly approximates the compute intensive recurrent architectures [6] for image generation. However, their receptive field grows linearly and handling long-scale effects is difficult. Our results are qualitatively better than PixelCNN, we believe our DVAE with fully-connected layers is better at capturing the global structure given the coarse resolution used. Note, our CDVAE has lower minimum error than PixelCNN (Figure 5)."}, {"heading": "5.4. Qualitative Evaluation", "text": "In addition to outperforming baselines on quantitative results, our method generates better qualitative results. Samples from our jointly trained conditional model smooth information across \u201csimilar\u201d images, allowing us to produce aligned, semantically sensible and reasonable diverse predictions. Our qualitative comparisons with other methods for the two tasks is shown in Figure 6. In both examples, we generate plausible and diverse relighted scenes and re-\nsaturated image. Image Resaturation: More results for image resaturation with our CDVAE (12 gaussian kernels) and embedding guidance are shown in Figure 7. For each input image, we draw four samples for saturation fields from our conditional model. The diverse saturation adjustment results show that our model learns multimodal saturation distributions effectively. Our automatic saturation adjustment creates appealing photos. We demonstrate artistic stylization/editing by using our automated method.\nPhoto Relighting: In Figure 8, we show additional photo relighting results from our method. For each input image, we again draw four samples from the distribution. The photo relighting results show that our CDVAE model learns the light source distributions as well as important objects. Our model creates light fields coming from reasonable light sources and the lighting looks natural, especially on important objects."}, {"heading": "6. Discussion", "text": "Our CDVAE generates good results qualitatively and quantitatively. However, there are still some limitations. Some of them are due to the limitations of VAE based generative models. For example, variational auto-encoders and its variants oversmooth their outputs, which leads to loss of spatial structure. Our multilayer gaussian latent variable\nOriginal Sample 1 Sample 2 Sample 3 Sample 4\narchitecture can capture more complex structures, but we do miss out on the finer details compared to ground truth. Second, our model \u2013 like all current generative models \u2013 is applied to low resolution images, meaning that much of the structural and semantic information important to obtaining good results is lost. Last, our model has no spatial hierarchy. Coarse to fine multiscale hierarchy on both generative side and conditional side would likely enable us to produce results with more details."}, {"heading": "7. Conclusion", "text": "We described an approach that simplifies building conditional models by building a joint model. Our joint model yields a conditional model that can be easily sampled. This allows us to train on scattered data using a joint model. We have demonstrated our approach on the task of generating photo-realistic relighted and resaturated images. We propose a metric regularization of code space which prevents code collapse. In future, this regularization can be investigated in the context of other generative models."}, {"heading": "8. Appendix", "text": ""}, {"heading": "8.1. Architecture Details", "text": "Our CDVAE has a different architecture compared to a CVAE. The detailed architecture of our CDVAE is in Table 8.1. Write lin for the input layer and lout for the output layer, fc stands for a fully connected layer, mean is the mean of the gaussian distribution of the code space, and var is the variance of the gaussian distribution of the code space. Sample is the process of sampling the gaussian distribution with mean and var. lout is sampled from mean4 and var4. We use L2 regularization (or weight decay) for the parameters for MDN model. The learning rate is set to 5\u00d7 10\u22125 and we use the ADAM optimizer. We initially set the reconstruction cost high, LPP embedding guidance cost high, and MDN cost low. We keep this setting and train for 100 epochs. For the next 200 epochs, we gradually decrease the embedding cost, and increase the MDN cost. Finally, we keep the relative cost fixed and train another 200 epochs."}, {"heading": "8.2. DVAE", "text": "The difference between DVAE [16] and VAE [7] is multiple layers of gaussian latent variables. DVAE for xc (same for xg) consists of L layers of latent variables. To generate a sample from the model, we begin at the top-most layer (L) by drawing from a Gaussian distribution to get zc,L.\nP (zc,L) = N (zc,L|0, I) (8)\nThe mean and variance for the Gaussian distributions at any lower layer is formed by a non-linear transformation of the sample from above layer.\n\u00b5c,i = f\u00b5c,i(zc,i+1) (9)\n\u03c32c,i = f\u03c32c,i(zc,i+1) (10)\nwhere f represents multi-layer perceptrons. We descend through the hierarchy by one hot vector sample process.\nzc,i = \u00b5c,i + \u03bei\u03c3c,i (11)\nwhere \u03bei are mutually independent Gaussian variables. xc is generated by sampling from the Gaussian distribution at the lowest layer.\nP (xc|zc,1) = N (xc|\u00b5c,0, \u03c32c,0) (12)\nThe joint probability distribution P (xc, zc) of this model is formulated as\nP (xc, zc) = P (xc|zc,1)P (zc)\n= P (xc|zc,1)P (zc,L) L\u22121\u220f i=1 P (zc,i|zc,i+1) (13)\nwhere P (zc,i|zc,i+1) = N (zc,i|\u00b5c,i, \u03c32c,i). Other details of the DVAE model are similar to VAE."}, {"heading": "8.2.1 Inference", "text": "DVAE with several layers of dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models. LVAE [20] recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recent Ladder Network. It utilizes a deeper more distributed hierarchy of latent variables and captures more complex structures. We follow this work and for xc, write \u00b5c,p,i and \u03c32c,p,i for the mean and variance on the i\u2019s level of generative side, write \u00b5c,q,i and \u03c32c,q,i for the mean and variance on the i\u2019s level of inference side.\nThis changes the notation in the previous part on the generative side.\nPp(zc) = Pp(zc,L) L\u22121\u220f i=1 Pp(zc,i|zc,i+1) (14)\nPp(zc,L) = N (zc,L|0, I) (15)\nPp(zc,i|zc,i+1) = N (zc,i|\u00b5c,p,i, \u03c32c,p,i) (16)\nPp(xc|zc,1) = N (xc|\u00b5c,p,0, \u03c32c,p,0) (17)\nOn the inference side, the notation also changes.\nPq(zc|xc) = Pq(zc,1|xc) L\u220f i=2 Pq(zc,i|zc,i\u22121) (18)\nPq(zc,1|xc) = N (zc,1|\u00b5c,q,1, \u03c32c,q,1) (19)\nPq(zc,i|zc,i\u22121) = N (zc,i|\u00b5c,q,i, \u03c32c,q,i) (20)\nDuring inference, first a deterministic upward pass computes the approximate distribution \u00b5\u0302c,q,i and \u03c3\u03022c,q,i. This is followed by a stochastic downward pass recursively computing both the approximate posterior and generative distributions.\nPq(zc|xc) = Pq(zc,L|xc) L\u22121\u220f i=1 Pq(zc,i|zc,i+1) (21)\n\u03c3c,q,i = 1\n\u03c3\u0302\u22122c,q,i + \u03c3 \u22122 c,p,i\n(22)\n\u00b5c,q,i = \u00b5\u0302c,q,i\u03c3\u0302\n\u22122 c,q,i + \u00b5c,p,i\u03c3 \u22122 c,p,i\n\u03c3\u0302\u22122c,q,i + \u03c3 \u22122 c,p,i\n(23)\nPq(zc,i|\u00b7) = N (zc,i|\u00b5c,q,i, \u03c32c,q,i) (24)\nwhere \u00b5c,q,L = \u00b5\u0302c,q,L and \u03c32c,q,L = \u03c3\u0302 2 c,q,L."}, {"heading": "8.3. Joint Models", "text": "First, we prove that if the joint probability is independent, we will get two separate DVAEs. Then, we prove the derivations for joint model with non-independent joint probability."}, {"heading": "8.3.1 Separate DVAEs", "text": "From Section 3 in the paper, the joint probability P (xc, xg) in CDVAE model is\nP (xc, xg) = \u222b z P (xc|zc)P (xg|zg)P (zg, zc)dzgdzc (25)\nIf zc and zg are independent, so P (zg, zc) =\nP (zg)P (zc), and Equation 25 can be transformed\nP (xc, xg) = \u222b z P (xc|zc)P (xg|zg)P (zg)P (zc)dzgdzc\n= \u222b z (P (xc|zc)P (zc))dzc(P (xg|zg)P (zg))dzg\n= \u222b zc P (xc|zc)P (zc)dzc + \u222b zg P (xg|zg)P (zg)dzg\n= P (xc) + P (xg)\n(26)\nwhere P (xc) is DVAE model for xc and P (xg) is DVAE model for xg ."}, {"heading": "8.3.2 Joint Model Derivation", "text": "From Section 2 in the paper, we have objective function for VAE as\nVAE(\u03b8) = \u2211 data [EQ logP (x|z)\u2212 D(Q||P (z))] (27)\nwhere Q = P (z|x). Applying the same derivations, the objective function for our CDVAE model can be written as CDVAE(\u03b8c, \u03b8g) = \u2211 data[EQ logP (xc, xg|zc, zg)\u2212 D(Q||P (zc, zg))]\n(28) where Q = P (zc, zg|xc, xg). Assume it is possible to encode xc without seeing xg , then the variational distribution Q = P (zc|xc)P (zg|xg) applies. It is also possible to decode xc without seeing xg , so we have P (xc, xg|zc, zg) = P (xc|zc)P (xg|zg). With these formulas, Equation 28 can be transformed\nEQ logP (xc, xg|zc, zg) = EQ log(P (xc|zc)P (xg|zg)) = EQ1 logP (xc|zc) + EQ2 logP (xg|zg)\n(29)\nwhere Q1 = P (zc|xc) and Q2 = P (zg|xg). The joint distribution can be written as logP (zc, zg) = logP (zc) + logP (zg) + Fmdn(zc, zg), so we have the following equations for the second part.\nD(Q||P (zc, zg)) = D(P (zc|xc)P (zg|xg)||P (zc, zg)) = EQ(log(P (zc|xc)P (zg|xg))\u2212 logP (zc, zg)) = EQ1(logP (zc|xc)) + EQ2(logP (zg|xg)) \u2212 EQ1(logP (zc))\u2212 EQ2(logP (zg)) \u2212 EQ(Fmdn(zc, zg)) = D(Q1||P (zc)) + D(Q2||P (zg)) \u2212 EQ(Fmdn(zc, zg))\n(30)\nIn our CDVAE model, we have logP (zc) = \u2212 z T c zc 2 and logP (zg) = \u2212 zTg zg 2 because zc and zg are Gaussian distributions. Our CDVAE objective function turns into CDVAE(\u03b8c, \u03b8g) = DVAE(\u03b8c) + DVAE(\u03b8g) + \u2211 data EQ(Fmdn(zc, zg))\n(31)"}, {"heading": "8.4. Embedding Influence", "text": "We compare the results with embedding guidance and without embedding guidance. The comparisons for reshading can be found in Figures 9. The re-shading results without embedding guidance tend to have less variety, more flaws and artifacts. The comparisons for re-saturation can be found in Figures 10. The re-saturation results without embedding guidance tend to have limited variety and produce less vivid results."}, {"heading": "8.5. Quantitative Results", "text": "The detailed quantitative evaluation results for photo relighting are in Table 2 and image resaturation are in Table 3. The tables contain best error to ground-truth with different sample numbers. As the sample number increases, the error drops fast at beginning, and then becomes stable. Our CDVAEs are consistently better than other methods. The second parts of both tables are average variances across 100 samples. We only report the final variance, since it almost does not change with the sample number. The variance we report comes from 100 samples."}, {"heading": "8.6. Qualitative Results", "text": "We include more qualitative results and comparisons in this section. Photo relighting results and comparisons can be found in Figure 11, 12. Photo relighting results with CGAN tend to have less variety and be less reasonable; results with CPixel tend to be extreme and random, and they also have less spatial structures; results with CVAE suffers from mode collapsion and have limited variety. Image resaturation results and comparisons can be found in Figure 13, 14. Image re-saturation results with CGAN tend to ignore the image content and like random, and creates various of artifacts; results with CPixel tend to be extreme, and either like random or go into mode collapsion; results with CVAE have limited variety and creates more artifacts."}], "references": [{"title": "Intrinsic images in the wild", "author": ["S. Bell", "K. Bala", "N. Snavely"], "venue": "ACM Trans. Graph.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Mixture density networks", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Learning diverse image colorization", "author": ["A. Deshpande", "J. Lu", "M. Yeh", "D.A. Forsyth"], "venue": "CoRR, abs/1612.01958,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Learning large-scale automatic image colorization", "author": ["A. Deshpande", "J. Rock", "D. Forsyth"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Image style transfer using convolutional neural networks", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Jun 2016", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D.J. Rezende", "D. Wierstra"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Deep convolutional inverse graphics network", "author": ["T.D. Kulkarni", "W.F. Whitney", "P. Kohli", "J. Tenenbaum"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Learning representations for automatic colorization", "author": ["G. Larsson", "M. Maire", "G. Shakhnarovich"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Sparse depth super resolution", "author": ["J. Lu", "D. Forsyth"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Conditional generative adversarial nets", "author": ["M. Mirza", "S. Osindero"], "venue": "arXiv preprint arXiv:1411.1784,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Plug & play generative networks: Conditional iterative generation of images in latent space", "author": ["A. Nguyen", "J. Yosinski", "Y. Bengio", "A. Dosovitskiy", "J. Clune"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Locality preserving projections", "author": ["X. Niyogi"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Learning what and where to draw", "author": ["S.E. Reed", "Z. Akata", "S. Mohan", "S. Tenka", "B. Schiele", "H. Lee"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "ICML\u201914, pages II\u20131278\u2013II\u20131286. JMLR.org,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Trajectory mixture density networks with multiple mixtures for acoustic-articulatory inversion", "author": ["K. Richmond"], "venue": "In  International Conference on Nonlinear Speech Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Learning structured output representation using deep conditional generative models", "author": ["K. Sohn", "H. Lee", "X. Yan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Ladder variational autoencoders", "author": ["C.K. S\u00f8nderby", "T. Raiko", "L. Maal\u00f8e", "S.K. S\u00f8nderby", "O. Winther"], "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Learning stochastic feedforward neural networks", "author": ["Y. Tang", "R.R. Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Deep architectures for articulatory inversion", "author": ["B. Uria", "I. Murray", "S. Renals", "K. Richmond"], "venue": "In Proceedings of Interspeech,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["A. van den Oord", "N. Kalchbrenner", "O. Vinyals", "L. Espeholt", "A. Graves", "K. Kavukcuoglu"], "venue": "CoRR, abs/1606.05328,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Anticipating visual representations from unlabeled video", "author": ["C. Vondrick", "H. Pirsiavash", "A. Torralba"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "An uncertain future: Forecasting from static images using variational autoencoders", "author": ["J. Walker", "C. Doersch", "A. Gupta", "M. Hebert"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks", "author": ["T. Xue", "J. Wu", "K. Bouman", "B. Freeman"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Unsupervised diverse colorization via generative adversarial networks", "author": ["W.Z. Yun Cao", "Zhiming Zhou", "Y. Yu"], "venue": "CoRR, abs/1702.06674,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2017}, {"title": "Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis", "author": ["H. Zen", "A. Senior"], "venue": "In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Colorful image colorization", "author": ["R. Zhang", "P. Isola", "A.A. Efros"], "venue": "ECCV, 2016", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Learning temporal transformations from time-lapse videos", "author": ["Y. Zhou", "T.L. Berg"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "There are many motion fields consistent with an image [19, 25, 26].", "startOffset": 54, "endOffset": 66}, {"referenceID": 23, "context": "There are many motion fields consistent with an image [19, 25, 26].", "startOffset": 54, "endOffset": 66}, {"referenceID": 24, "context": "There are many motion fields consistent with an image [19, 25, 26].", "startOffset": 54, "endOffset": 66}, {"referenceID": 2, "context": "Similarly, there are many shading fields consistent with the layout of an image (Figure 1); many good ways to colorize an image [3, 27, 29]; many possible ways to adjust the saturation of an image (Figure 1); many possible long term futures for an image frame [30]; and so on.", "startOffset": 128, "endOffset": 139}, {"referenceID": 25, "context": "Similarly, there are many shading fields consistent with the layout of an image (Figure 1); many good ways to colorize an image [3, 27, 29]; many possible ways to adjust the saturation of an image (Figure 1); many possible long term futures for an image frame [30]; and so on.", "startOffset": 128, "endOffset": 139}, {"referenceID": 27, "context": "Similarly, there are many shading fields consistent with the layout of an image (Figure 1); many good ways to colorize an image [3, 27, 29]; many possible ways to adjust the saturation of an image (Figure 1); many possible long term futures for an image frame [30]; and so on.", "startOffset": 128, "endOffset": 139}, {"referenceID": 28, "context": "Similarly, there are many shading fields consistent with the layout of an image (Figure 1); many good ways to colorize an image [3, 27, 29]; many possible ways to adjust the saturation of an image (Figure 1); many possible long term futures for an image frame [30]; and so on.", "startOffset": 260, "endOffset": 264}, {"referenceID": 6, "context": "variational auto-encoder (VAE) of [7], to produce a conditional VAE (CVAE) [19, 25, 26].", "startOffset": 34, "endOffset": 37}, {"referenceID": 17, "context": "variational auto-encoder (VAE) of [7], to produce a conditional VAE (CVAE) [19, 25, 26].", "startOffset": 75, "endOffset": 87}, {"referenceID": 23, "context": "variational auto-encoder (VAE) of [7], to produce a conditional VAE (CVAE) [19, 25, 26].", "startOffset": 75, "endOffset": 87}, {"referenceID": 24, "context": "variational auto-encoder (VAE) of [7], to produce a conditional VAE (CVAE) [19, 25, 26].", "startOffset": 75, "endOffset": 87}, {"referenceID": 14, "context": "Recent work shows that better generative models are obtained by conditioning on textembeddings [15] or pre-trained features from visual recognition network [12].", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "Recent work shows that better generative models are obtained by conditioning on textembeddings [15] or pre-trained features from visual recognition network [12].", "startOffset": 156, "endOffset": 160}, {"referenceID": 1, "context": "To ensure that F (u; c(x)) will vary for similar c(x), we use a Mixture Density Network (MDN) [2].", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "Important application examples, where the prediction is naturally ambiguous, include colorization [4, 9, 29], style transfer [5], temporal transformations prediction [30], predicting motion fields [19, 25, 26], and predicting future frames [24].", "startOffset": 98, "endOffset": 108}, {"referenceID": 8, "context": "Important application examples, where the prediction is naturally ambiguous, include colorization [4, 9, 29], style transfer [5], temporal transformations prediction [30], predicting motion fields [19, 25, 26], and predicting future frames [24].", "startOffset": 98, "endOffset": 108}, {"referenceID": 27, "context": "Important application examples, where the prediction is naturally ambiguous, include colorization [4, 9, 29], style transfer [5], temporal transformations prediction [30], predicting motion fields [19, 25, 26], and predicting future frames [24].", "startOffset": 98, "endOffset": 108}, {"referenceID": 4, "context": "Important application examples, where the prediction is naturally ambiguous, include colorization [4, 9, 29], style transfer [5], temporal transformations prediction [30], predicting motion fields [19, 25, 26], and predicting future frames [24].", "startOffset": 125, "endOffset": 128}, {"referenceID": 28, "context": "Important application examples, where the prediction is naturally ambiguous, include colorization [4, 9, 29], style transfer [5], temporal transformations prediction [30], predicting motion fields [19, 25, 26], and predicting future frames [24].", "startOffset": 166, "endOffset": 170}, {"referenceID": 17, "context": "Important application examples, where the prediction is naturally ambiguous, include colorization [4, 9, 29], style transfer [5], temporal transformations prediction [30], predicting motion fields [19, 25, 26], and predicting future frames [24].", "startOffset": 197, "endOffset": 209}, {"referenceID": 23, "context": "Important application examples, where the prediction is naturally ambiguous, include colorization [4, 9, 29], style transfer [5], temporal transformations prediction [30], predicting motion fields [19, 25, 26], and predicting future frames [24].", "startOffset": 197, "endOffset": 209}, {"referenceID": 24, "context": "Important application examples, where the prediction is naturally ambiguous, include colorization [4, 9, 29], style transfer [5], temporal transformations prediction [30], predicting motion fields [19, 25, 26], and predicting future frames [24].", "startOffset": 197, "endOffset": 209}, {"referenceID": 22, "context": "Important application examples, where the prediction is naturally ambiguous, include colorization [4, 9, 29], style transfer [5], temporal transformations prediction [30], predicting motion fields [19, 25, 26], and predicting future frames [24].", "startOffset": 240, "endOffset": 244}, {"referenceID": 18, "context": "Each DVAE has two layers of latent gaussian variables and we use the ladder VAE architecture of [20].", "startOffset": 96, "endOffset": 100}, {"referenceID": 27, "context": "While one might temper the distribution (eg [29]), the ideal is to obtain multiple different solutions.", "startOffset": 44, "endOffset": 48}, {"referenceID": 6, "context": "Generative models of images present problems of dimension; these can be addressed with latent variable models, leading to the variational autoencoder (VAE) of [7].", "startOffset": 159, "endOffset": 162}, {"referenceID": 6, "context": "Current generative models are reliable in simple domains (handwritten digits [7, 18]; faces [7, 8, 16]; and CIFAR images [6]) but less so for general images.", "startOffset": 77, "endOffset": 84}, {"referenceID": 6, "context": "Current generative models are reliable in simple domains (handwritten digits [7, 18]; faces [7, 8, 16]; and CIFAR images [6]) but less so for general images.", "startOffset": 92, "endOffset": 102}, {"referenceID": 7, "context": "Current generative models are reliable in simple domains (handwritten digits [7, 18]; faces [7, 8, 16]; and CIFAR images [6]) but less so for general images.", "startOffset": 92, "endOffset": 102}, {"referenceID": 15, "context": "Current generative models are reliable in simple domains (handwritten digits [7, 18]; faces [7, 8, 16]; and CIFAR images [6]) but less so for general images.", "startOffset": 92, "endOffset": 102}, {"referenceID": 5, "context": "Current generative models are reliable in simple domains (handwritten digits [7, 18]; faces [7, 8, 16]; and CIFAR images [6]) but less so for general images.", "startOffset": 121, "endOffset": 124}, {"referenceID": 15, "context": "Improvements are available using multiple layers of latent variables (a DVAE) [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "Models can be trained with adversarial loss [14].", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "The ladder VAE imposes both top-down and bottom up variational distributions for more efficient training [20].", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": "give a multimodal conditional model [21], but the conditioning variables are binary.", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "A conditional variational autoencoder (CVAE) [19] conditions the decoder model on a continuous representation produced by a network applied to x.", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "This approach has been demonstrated on motion prediction problems [25, 30].", "startOffset": 66, "endOffset": 74}, {"referenceID": 28, "context": "This approach has been demonstrated on motion prediction problems [25, 30].", "startOffset": 66, "endOffset": 74}, {"referenceID": 1, "context": "We use the mixture density network (MDN) in our models to capture the underlying multimodal distribution [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 16, "context": "MDNs have been successfully applied to articulatory-acoustic inversion mapping [17, 22] and speech synthesis [28].", "startOffset": 79, "endOffset": 87}, {"referenceID": 20, "context": "MDNs have been successfully applied to articulatory-acoustic inversion mapping [17, 22] and speech synthesis [28].", "startOffset": 79, "endOffset": 87}, {"referenceID": 26, "context": "MDNs have been successfully applied to articulatory-acoustic inversion mapping [17, 22] and speech synthesis [28].", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": "Our CDVAE consists of two deep variational auto encoders (DVAE) [16] and a mixture density network (MDN) [2].", "startOffset": 64, "endOffset": 68}, {"referenceID": 1, "context": "Our CDVAE consists of two deep variational auto encoders (DVAE) [16] and a mixture density network (MDN) [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 12, "context": "We use Niyogi [13] to build an embedding which preserves metric relations between data points.", "startOffset": 14, "endOffset": 18}, {"referenceID": 12, "context": "Our input feature vector to [13] is constructed in three parts.", "startOffset": 28, "endOffset": 32}, {"referenceID": 18, "context": "We use an inference method [20] that co-relates the latent variables of the encoder and the decoder.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "We aggressively upsample the generated fields with the approach in [10], which preserves edges during upsampling.", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "For photo relighting, intrinsic image decomposition method from [1] is used to obtain albedo and shading images.", "startOffset": 64, "endOffset": 67}, {"referenceID": 23, "context": "Error-of-Best to ground truth: We follow the motion prediction work [25] to use error-of-best to ground truth as an evaluation metric.", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "Conditional variational autoencoder: We implement a CVAE similar to [25].", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "We cannot use [25] since their architecture is specific to prediction of coarse motion trajectories.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "[14] with 5 deconvolution layers, and we use codes of dimension 64 (to be consistent with CDVAE).", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "We use the same strategy as [25], i.", "startOffset": 28, "endOffset": 32}, {"referenceID": 10, "context": "Conditional GAN: CGAN [11] is another conditional image generation model.", "startOffset": 22, "endOffset": 26}, {"referenceID": 21, "context": "Conditional PixelCNN (CPixel): Conditional PixelCNN [23] uses masked and gated convolutions (sigmoid and tanh activations layers multiplied).", "startOffset": 52, "endOffset": 56}, {"referenceID": 5, "context": "Therefore, PixelCNN (CPixel) feasibly approximates the compute intensive recurrent architectures [6] for image generation.", "startOffset": 97, "endOffset": 100}, {"referenceID": 15, "context": "The difference between DVAE [16] and VAE [7] is multiple layers of gaussian latent variables.", "startOffset": 28, "endOffset": 32}, {"referenceID": 6, "context": "The difference between DVAE [16] and VAE [7] is multiple layers of gaussian latent variables.", "startOffset": 41, "endOffset": 44}, {"referenceID": 18, "context": "LVAE [20] recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recent Ladder Network.", "startOffset": 5, "endOffset": 9}], "year": 2017, "abstractText": "Problems such as predicting a new shading field (Y ) for an image (X) are ambiguous: many very distinct solutions are good. Representing this ambiguity requires building a conditional model P (Y |X) of the prediction, conditioned on the image. Such a model is difficult to train, because we do not usually have training data containing many different shadings for the same image. As a result, we need different training examples to share data to produce good models. This presents a danger we call \u201ccode space collapse\u201d \u2014 the training procedure produces a model that has a very good loss score, but which represents the conditional distribution poorly. We demonstrate an improved method for building conditional models by exploiting a metric constraint on training data that prevents code space collapse. We demonstrate our model on two example tasks using real data: image saturation adjustment, image relighting. We describe quantitative metrics to evaluate ambiguous generation results. Our results quantitatively and qualitatively outperform different strong baselines.", "creator": "LaTeX with hyperref package"}}}