{"id": "1612.00637", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "A General Framework for Density Based Time Series Clustering Exploiting a Novel Admissible Pruning Strategy", "abstract": "Time Series Clustering is an important subroutine in many higher-level data mining analyses, including data editing for classifiers, summarization, and outlier detection. It is well known that for similarity search the superiority of Dynamic Time Warping (DTW) over Euclidean distance gradually diminishes as we consider ever larger datasets. However, as we shall show, the same is not true for clustering of data. The DDTW is typically used to infer an univariate linear distribution of the mean distance between variables in the graph. To compute a distance between variables with a maximum depth of 3, the DDTW and the DDTW are plotted separately. The data are only then averaged separately with the mean distance in the graph. In contrast to the DDTW, we can make a partial comparison for both DDTW and DDTW at the same time in order to find a linear distribution of the mean distance between variables. In fact, it is now possible to infer a partial order without the addition of a non-linear linear distribution of the mean distance between variables in the graph. The DDTW is usually used in a linear regression. A similar approach can be applied to DDTW, where there is a parallel approach. For instance, the DDTW is shown on the graph, when the DDTW is shown on the graph, and when the DDTW is shown on the graph, and when the DDTW is shown on the graph, the DDTW is shown on the graph.\n\nThe DDTW is shown in a simplified mode (e.g., in a full screen mode, where it is displayed on the graph, a non-linear regression), where it is shown in a simplified mode (e.g., in a full screen mode, where it is displayed on the graph, a non-linear regression), where it is shown on the graph, and when the DDTW is shown on the graph, and when the DDTW is shown on the graph, and when the DDTW is shown on the graph, and when the DDTW is shown on the graph, and when the DDTW is shown on the graph, and when the DDTW is shown on the graph, and when the DDTW is shown on the graph, and when the DDTW is shown on the graph, and when the DDTW is shown on the graph, and when the DDTW is shown on the graph, and", "histories": [["v1", "Fri, 2 Dec 2016 11:27:44 GMT  (2519kb)", "http://arxiv.org/abs/1612.00637v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nurjahan begum", "liudmila ulanova", "hoang anh dau", "jun wang", "eamonn keogh"], "accepted": false, "id": "1612.00637"}, "pdf": {"name": "1612.00637.pdf", "metadata": {"source": "CRF", "title": "A General Framework for Density Based Time Series Clustering Exploiting a Novel Admissible Pruning Strategy", "authors": ["Nurjahan Begum", "Liudmila Ulanova", "Hoang Anh Dau", "Jun Wang", "Eamonn Keogh"], "emails": [], "sections": [{"heading": null, "text": "for classifiers, summarization, and outlier detection. It is well known that for similarity search the superiority of Dynamic Time Warping (DTW) over Euclidean distance gradually diminishes as we consider ever larger datasets. However, as we shall show, the same is not true for clustering. Clustering time series under DTW remains a computationally expensive operation. In this work, we address this issue in two ways. We propose a novel pruning strategy that exploits both the upper and lower bounds to prune off a very large fraction of the expensive distance calculations. This pruning strategy is admissible and gives us provably identical results to the brute force algorithm, but is at least an order of magnitude faster. For datasets where even this level of speedup is inadequate, we show that we can use a simple heuristic to order the unavoidable calculations in a most-useful-first ordering, thus casting the clustering into an anytime framework. We demonstrate the utility of our ideas with both single and multidimensional case studies in the domains of astronomy, speech physiology, medicine and entomology. In addition, we show the generality of our clustering framework to other domains by efficiently obtaining semantically significant clusters in protein sequences using the Edit Distance, the discrete data analogue of DTW.\nIndex Terms\u2014 Clustering, Anytime Algorithms, Time Series \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014"}, {"heading": "1 INTRODUCTION", "text": "ECAUSE of the prevalence of time series data in human endeavors, the research community has made substantial efforts to create efficient algorithms for classification, clustering, rule discovery, and anomaly detection for this data type [1][4][23][35][49]. In particular, time series clustering is very useful, both as an exploratory technique and as a sub-module for solving higher-level data mining problems. As a motivating example, consider Fig. 1, which illustrates a subset of a cluster we discovered in a social media dataset [59]. This clustering allows us to at least partly address two problems:\n Synonym Discovery: In this example, we have a time series containing the volume of the hashtag #Michael over time. It is not clear to whom this refers: Michael Phelps? Michael Caine? However, by noting that this cluster also contains #MichaelJackson, this ambiguity is resolved.\n Association Discovery: Here we see that #kanyewest and #taylorswift have highly similar time series representations, but are clearly not synonyms. If we test to see whether this relationship existed prior to the illustrated timeframe, we find it does not. This suggests the existence of an event that caused this tempo-\nrary association, and with a little work we can discover that the famous \u201cI'mma let you finish\u201d event at the 2009 Video Music Awards produced this relationship [62].\nIn this example, the knowledge gleaned is clearly trivial; however, similar ideas have been used to track the levels of disease activity and public concern during the recent influenza A H1N1 pandemic [48]. Note that while we discovered this example using Dynamic Time Warping (DTW), it might have been discovered more efficiently with the Euclidean distance (ED). However, in cases where there is a causal relationship (rather than just an association) between events, a local lag can result between peaks. It has been extensively shown in the literature that the 40-year old distance DTW is an ideal similarity measure to capture/be invariant to such out-of-sync relationships [47].\nWe argue that the problem we wish to solve, robustly clustering large time series datasets with invariance to irrele-\nxxxx-xxxx/0x/$xx.00 \u00a9 200x IEEE Published by the IEEE Computer Society\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014"}, {"heading": "1 Nurjahan Begum, Liudmila Ulanova, Hoang Anh Dau and Eamonn Keogh are affiliated with the University of California, Riverside, CA- 92521. E-mail: {nbegu001, lulan001, hdau001, eamonn}@cs.ucr.edu.", "text": ""}, {"heading": "2 Jun Wang is affiliated with the University of Texas at Dallas, Richardson,", "text": "TX - 75080. E-mail: wangjun@utdallas.edu.\nB\nvant data, has not been solved before. For most time series data mining algorithms, the quality of the output depends almost exclusively on the distance measure used [49]. In the last decade, a consensus has emerged that the DTW distance measure is the best measure in most domains, almost always outperforming the Euclidean Distance (ED) and other purported rivals [49]. As a concrete example, consider the two clusterings of three randomly chosen mammals shown in Fig. 2. The input data is the mitochondrial DNA after it was converted to a time series representation (converting DNA to time series is a commonly used operation [40][41]). Two types of DNA mutations, insertions and deletions, have the effect of \u201cwarping\u201d the time series. At least in this case, we can see that DTW is invariant to these mutations and correctly unites Bos taurus (cattle) and Hyperoodon ampullatus (bottlenose whale), with Talpa europaea (mole) as the out-group.\nWhile this example1 is on a small and somewhat specialized dataset in the domain of bioinformatics, in Section 6 we will show that the superiority of DTW extends to large datasets in many domains including discrete protein data."}, {"heading": "1.1 Why This Problem Is Hard", "text": "Because DTW is intrinsically slow due to its quadratic time complexity, there are two ideas that are commonly used to mitigate the problem of using such a sluggish distance measure [41]. We briefly discuss them here only to dismiss them as possible solutions.\n The convergence of DTW and Euclidean distance results for increasing data sizes. It has been noted that for many problems, including motif discovery [35] and classification [49], the results returned by DTW and Euclidean distance tend to become increasingly similar as the dataset sizes increase. This suggests that it is more efficient to use Euclidean distance to cluster large datasets.\n The increasing effectiveness of lower-bounding pruning for increasing data sizes. For some problems, notably similarity search, the lower-bounding pruning of unnecessary calculations is the main technique used to produce speedup. The effectiveness of this lowerbounding tends to improve dramatically as the datasets get larger [41]. Unfortunately, neither of these observations helps us for clustering under DTW. To demonstrate why the first\n1 We defer a discussion of our experimental philosophy until Section 5, but we note that all experiments in this work are made reproducible by our unrestricted sharing of code/data.\nobservation does not help, we performed a simple experiment in which we measured the leave-one-out training error of 1-NN classification using both DTW and ED, for various numbers (50 to 2000) of exemplars from the CBF dataset [25]. With 50 objects, the error rates differ by a factor of 4.6 (7% and 1.75%, respectively), but as shown in Fig. 3.top, by the time we consider the 2,000 object dataset, this difference is essentially zero.\nThis effect is well known for time series classification [44][49], and it might be imagined that this also applies to clustering. To show that this is not the case, we performed a parallel experiment in which we clustered the same objects and measured the performance using the Rand Index [43]. As shown in Fig. 3.bottom, DTW clustering maintains its superiority over Euclidean distance as the datasets get larger.\nSimilarly, the second observation above does not help significantly to prune DTW distance computation for clustering. It is true that lower bounds are increasingly effective for larger datasets when attempting a similarity search. This is because for larger datasets, we can expect to have a smaller best-so-far early on, which allows more effective pruning [41][44][49]. However, in clustering, we need to know the distance between all pairs [23], or at least all distances within a certain range, which renders the typical use of lower-bounding pruning ineffective."}, {"heading": "1.2 Why Existing Work Is Not the Answer", "text": "More generally, many clustering algorithms achieve scalability by exploiting a spatial access method. For example, the scalable version of the ubiquitous DBSCAN uses an R*tree [12]. However, because DTW is not a metric, it is very difficult to index, especially for long (i.e., high-dimensional) time series objects. Beyond the need to scalably support DTW, we note the need for a clustering algorithm that supports invariance to outliers. That is to say, unlike some clustering methods such as k-means, which attempt to explain all the data, we believe it is especially important to allow a time series clustering algorithm the freedom to ignore some data.\nConsider the example in Fig. 4. We took twelve objects from a heraldic shield dataset [63] and clustered these using k-means and DP, the algorithm we propose to augment (described in detailed in Section 4). Because we are using the (non-metric) DTW measure, which may\nSTRATEGY 3\nprevent k-means from converging, we used the variant in [21] which performs k-means clustering using the all-pair distance matrix. Note that for the ease of visualization, we used multidimensional scaling to cast high-dimensional time series objects to two dimensions. After we ran the algorithms, both of them gave a perfect Rand Index score of 1.0. We then inserted a single outlier object (object 12) from this dataset and reran the algorithms. As we can see from Fig. 4. bottom.left), k-means assigned objects 6-11 to the cluster of the outlier object. In addition, k-means falsely identified objects 1 and 2 as a separate cluster from the cluster of objects 3-5. In contrast, from Fig. 4.bottom.right) we can see that DP only clustered object 7 in the cluster of the outlier object, but did not change the cluster labels of the rest of the dataset.\nThis toy example is contrived and anecdotal, but conformed by more rigorous and wide-reaching experiments on real data [64].\nIn this work, we address all the considerations above. We adapt DP (Density Peaks), a relatively new clustering framework that is able to ignore outlying data points [45]. While DP is insensitive to outliers, it is relatively slow, as it requires O(N2) DTW calculations. We augment DP such that it can exploit both DTW upper and lower bounds to compute only the absolutely necessary DTW calculations, and do so in a best-first manner, giving our algorithm the desirable anytime algorithm behavior [3][61]."}, {"heading": "2 RELATED WORK", "text": "The field of clustering is vast, and even the subfield of clustering time series has an enormous literature [1][23][57][60]. Much of the works on time series clustering are concerned with clustering based on time series features [57], which are at best tangentially related to our goals. Here, we are only interested in clustering based on time series shapes. In the latter case, there are two important and interrelated choices that define most of the literature: the choice of distance measure and the choice of clustering algorithm.\nMost of the literature on time series shape-based clustering uses metric measures such as Euclidean distance [10][57]. The ubiquity of Euclidean distance seems to derive more from its familiarity and ease of indexing than any data-driven assessment of its effectiveness. As Fig. 3.top illustrates with a single representative example, the general superiority of DTW over ED is well understood in the community (cf. [49]), at least for classification. As 3.bottom suggests, and as we later empirically confirm on many diverse datasets, the dominance of DTW over ED for clustering is, if anything, much greater.\nThe plethora of shape-based clustering algorithms [23][42][60] can be divided at the highest level into those that insist on explaining (i.e., clustering) all the data [60] vs. those that have the representational power to leave some data unclustered (a small minority) [42]. We believe that this distinction is underappreciated and critical to the success of most efforts. For clarity, consider the following analogy: if we were clustering people, surely every person in our database would belong to some group, even if (due to the small size of our sample) the size of some groups were just one. In contrast, if we were clustering subsequences from a speech articulation database (see Section 6), we would hope that the subsequences would cluster into well-defined words or phrases. However, it is highly likely that we would have some examples of coughing, sneezing or harrumphing. Such sequences are likely to be very dissimilar to the rest of the database. Not only do we not want/need them to be clustered, but also we do not want them to affect the clustering of the clusterable words or phrases (recall Fig. 4). However k-means and its variants insist on explaining these instances and because of kmeans\u2019s sum of squares objective function, these highly dissimilar items have a huge effect on the quality of the overall clustering.\nOne of the basic questions in clustering problems is the notion of a \u2018cluster\u2019 itself. There exist partitional clustering algorithms such as k-means which typically assume data has balanced Gaussian \u201cball-shaped\u201d clusters. Whereas some other algorithms (e.g. DBSCAN [12]) take the density of objects into consideration regardless of the shape the clusters may have. The intuition behind DBSCAN algorithm is that, each object in a cluster must have at least some number of other objects (MinPts) in its vicinity (\u03b5). If two objects are density-reachable from each other, then they are assigned in the same cluster. However, DBSCAN is not deterministic in its assignment of the cluster border points, because the result specifically depends on the order of the objects considered. Besides, DBSCAN has two parameters, MinPts and \u03b5, and there is no concrete strategy for setting these parameters.\nThere exist works [33] in the literature that perform clustering on top of DBSCAN [12]. The problems with such approaches are the inheritance of the nondeterminism of DBSCAN and the use of only lower bounds to prune expensive distance calculations. A variant of DBSCAN called IncrementalDBSCAN [13] has been demonstrated to perform much better than the bruteforce re-clustering of newly added/removed objects which is mostly appropriate for datasets changing incre-\nmentally. DBCLASD [58] is a non-parametric grid-based clustering algorithm which considers the density of objects like DBSCAN to define clusters. It exploits a gridbased approach to find polygons of clusters. However, this algorithm cannot be applied to high-dimensional time series domains. DENCLUE [18] is an algorithm which combines both density-based and grid-based approaches to define cluster centers that has the local maximum of some density function. The cluster assignment of other objects in the datasets is done by a hill-climbing approach. There exist other types of clustering algorithms like hierarchical and model-based [17]. However, such algorithms are known to be quite slow and not appropriate for our context.\nA handful of research efforts [60] have attempted to mitigate the slow performance of DTW clustering by casting it to an anytime framework. Most such efforts reduce to the following: until there is a user interrupt, these frameworks keep replacing the (fast to compute) approximate DTW distances with true (slow to compute) DTW distances. If there is no user interrupt, such frameworks will calculate the full distance matrix (generally in some \u201cmost-likely-to-be-useful\u201d order) and return the exact clustering. We refer readers interested in time series clustering to the detailed surveys [26][30].\nOur proposed algorithm goes beyond this literature in several ways. Most importantly, we show that calculating the full distance matrix is unnecessary in the general case. By exploiting both upper and lower bounds to DTW, and, more critically, by exploiting the relationship between these bounds, we can compute the exact clustering while only calculating a tiny fraction of the full distance matrix."}, {"heading": "3 BACKGROUND", "text": "There has been significant research on clustering datasets that are too large to fit in main memory [5][6]. This problem setting typically assumes inexpensive distance measures, but costly disk accesses [6]. However, the problem we wish to solve exploits DTW, which itself is a very expensive distance measure. Therefore, in situations when even the data can be stored in main memory, the time needed to do the clustering may be on the order of days/weeks. The problem we are interested in is therefore, CPU constrained, not I/O constrained."}, {"heading": "3.1 Anytime Algorithms", "text": "For most clustering algorithms, it is well known that not all distance measurements contribute equally to the final clustering assignments. For example, a recent paper on hierarchical clustering demonstrates (under some mild assumptions) that it is possible to capture the true structure of the clustering with just carefully chosen O(n log2 n) distance computations [28]. The fact that some distance computations are more important than others, immediately suggests that we should use anytime algorithms, assuming only that we can find an efficient and (even somewhat) effective test to identify these influential distance computations.\nRecall that anytime algorithms are algorithms that can\nreturn a valid solution to a problem, even if interrupted before ending [60][61]. Starting with a negligibly small amount of setup time, these algorithms always have a best-so-far answer available and the quality of the answer improves with the increase of execution time. The desirable properties of anytime algorithms are interruptibility, monotonicity, measurable quality, diminishing returns, preemptibility, and low overhead [61]. Note that this is a very brief introduction to anytime algorithms; we refer the interested reader to [61], which contains an excellent survey. As Fig. 5 shows, anytime algorithms, in essence, optimize the tradeoff between execution time and quality of the solution.\nFor clarity, we reiterate that the anytime algorithm approach is just one of the two contributions of this work. We propose to make the clustering absolutely faster by admissible pruning. This is in addition to rearranging the order in which the non-prunable calculations are considered to produce the best possible diminishing returns anytime algorithm behavior."}, {"heading": "4 ALGORITHM", "text": "We begin by reviewing the basic algorithm that we will augment and specialize to handle DTW."}, {"heading": "4.1 Density Peaks Algorithm Overview", "text": "Our proposed solution is inspired by DP, the densitybased clustering algorithm recently proposed in [45]. We chose to augment the DP framework for solving large time series clustering problems because of the following:  Recent literature [42] and our own experience on real datasets (cf. Section 6) suggest that the successful clustering of time series requires the ability to ignore some data objects. The issue is not merely that anomalous objects themselves are unclusterable; but that the presence of these objects can affect the labels of objects that are clusterable in unpredictable ways. The DP algorithm has been shown to be able to ignore anomalous data points.  The DP algorithm is able to handle datasets whose clusters can form arbitrary shapes. This is in contrast to kmeans and related algorithms, which assume the clusters are \u201cballs\u201d in space. This observation is particularly important for DTW, which is not a metric. While we cannot exactly visualize DTW clusters in a metric space, it is clear that some classes of objects under DTW form complex manifolds in DTW \u201cspace.\u201d\nSTRATEGY 5\n Many clustering algorithms require the user to set many parameters. In contrast, the DP algorithm requires only two. These parameters are relatively intuitive and not particularly sensitive to user choice.  Finally, it happens to be the case that the DP algorithm is amiable to optimization and conversion to an anytime algorithm.\nTo make our argument more concrete, we will take the time to explain the clustering algorithm [45] we adapt and augment. The DP algorithm assumes that the cluster centers are surrounded by lower local density neighbors and are at a relatively higher distance from any point with a higher local density. Therefore, for each point i in the dataset, the DP algorithm computes two quantities:\n Local density (\u03c1i)  Distance from points with higher local density (\u03b4i). We can formally define these two quantities:\nDefinition 1 The Local Density \u03c1i of point i is the number of points that are closer to it than some cutoff distance dc. Definition 2 The Distance from Points of Higher Density is the minimum distance \u03b4i from point i to all the points of higher density. For the special case of the highest density point, this distance is the maximum of the distances of all the points from their higher density points. We give the algorithm to compute \u03c1i in TABLE 1 and \u03b4i in TABLE 2.\nTABLE 1\nLOCAL DENSITY CALCULATION ALGORITHM\nInput: D,all-pair distance matrix dc, cutoff distance Output: \u03c1,the local density vector for all n points in the\ndataset\n1 2 3\nfor i = 1:n\n\u03c1(i) = count(D(i,otherObjects)<dc) end\nGiven the all-pair distance matrix D and a cutoff distance dc, for each point i in the dataset, \u03c1i is calculated in lines 1-3 of TABLE 1. In TABLE 2, using the local densities \u03c1 from TABLE 1, for each point i, the list of the points with higher densities is calculated (line 2). In line 4, this list is sorted in descending order. From lines 5 \u2013 7, for each point in the sorted order, the distances from their higher density points are calculated. For the special case of the highest density point (which by definition has no higher density neighbor), this distance is calculated in line 8.\nGiven the \u03c1i and \u03b4i for each object i\u00b8 the DP algorithm calculates the cluster centers \u03c7, and performs the cluster assignments based on these centers.\nTABLE 2\nDISTANCE TO HIGHER DENSITY POINTS ALGORITHM\nInput: D,all-pair distance matrix \u03c1,the local density vector Output: \u03b4,NN distance vector of higher density points\n1 for i = 1:n\n2\n3\n4\n5\n6\n7\n8\n\u03b4_list(i) = findHigherDensityItems(i,\u03c1)\nend\n[sorted_\u03b4_list, sortIndex] = sort(\u03b4_list,\u2019descend\u2019)\nfor j = 2:n\n\u03b4(sortIndex(j)) = NNDist(sorted_\u03b4_list(j))\nend\n\u03b4(sortIndex(1)) = max(\u03b4(2:n))\nThe cluster centers are selected using a simple heuristic: points with higher values of (\u03c1i\u00d7\u03b4i) are more likely to be centers. We give the cluster center selection algorithm in TABLE 3.\nTABLE 3\nCLUSTER CENTER SELECTION ALGORITHM\nInput: \u03b4,NN distance vector of higher density points\n\u03c1,the local density vector k, number of clusters\nOutput: \u03c7, cluster centers\n1 \u03c7 = topK(sort(\u03c1*.\u03b4, \u2018descend\u2019),k)\nGiven the sorted values of (\u03c1i\u00d7\u03b4i) in descending order, the top k items are selected as cluster centers (line 1). The value of k can be specified by the user, or found automatically using a \u201cknee-finding\u201d type of algorithm [45].\nThe final step of the DP algorithm is the cluster assignment. Each data item gets the cluster label of its nearest neighbor (NN) from the list of points with higher local densities than it has. We give the cluster assignment algorithm in TABLE 4.\nTABLE 4\nCLUSTER ASSIGNMENT ALGORITHM\nInput: \u03c7, cluster centers\n\u03b4,NN distance vector of higher density points sortIndex, sorted index of items based on descending \u03c1\nOutput: C, clusters\n1\n2\n3\n4\n5\n6\n7\n8\nfor i = 1:size(\u03c7)\nC(\u03c7(i)) = i //assign cluster labels for centers\nend\nfor j = 1:n\nif C(sortIndex(j)) == empty //no cluster label yet\nC(sortIndex(j)) = C(NN(sortIndex(j)))\nend if\nend\nIn lines 1-3 the cluster labels of the centers are assigned. After this initialization, each of the points in the dataset (other than the centers themselves) gets the cluster label of its nearest neighbor from the higher density list in the descending order of local density (lines 4-8). It is important to note that this algorithm allows the clusters to have arbitrary, possibly non-convex shapes, unlike kmeans and its variants, which are restricted to a Voronoi partitioning of the input space. We are now in a position to describe our augmented version of the DP framework."}, {"heading": "4.2 TADPole: Our Proposed Algorithm", "text": "We call our algorithm, TADPole (Time-series Anytime DP). As stated in Section 1, in order for the original DP algorithm to cluster a dataset, we need to know the distances between all pairs. The time needed to compute these all-pair distances becomes untenable for a quadratic time distance measure such as DTW. In order to mitigate this undesirable time complexity, our thoughts naturally turn to attempts to speed up other (non-clustering) algorithms that need to compute DTW frequently. Most such algorithms exploit linear time lower bounds like LB_Keogh [24], LB_Kim, LB_Yi [57], etc. Moreover, some algorithms exploit the fact that ED is an upper bound to DTW, and can also be computed in O(n) time.\nIn our TADPole algorithm, we augment the DP clustering framework and exploit the upper and lower bounds of DTW to prune unnecessary distance computations, which results in at least an order of magnitude speedup. For datasets where even this level of speedup is inadequate, we show that we can use a simple heuristic to order the unavoidable calculations in a most-useful-first ordering. As a result, our algorithm can be cast to an anytime clustering framework, which quickly produces a good answer and rapidly refines it until it converges to the exact answer (for proof, c.f. Section 10).\nThe inputs to the TADPole algorithm are the lower bound and upper bound matrices for the true DTW distances of all the objects of the dataset. Note that the time needed to compute these is inconsequential (<1%) relative to the overall clustering time.\nThe only parameters we need are the cutoff distance (dc) and optionally, the number of clusters (k), if the user wishes to specify this value rather than use the kneefinding heuristic suggested in [45]. Because we use DTW as the underlying distance measure, the warping window size is another parameter for TADPole. We discuss the heuristic to set these parameters in Section 5.3. Note that our use of two additional upper bound and lower bound matrices increases the space complexity of the algorithm by 200%. However, this is not an issue because:\n The DP algorithm (especially when using DTW or another expensive measure) is CPU bound, not space bound.\n If really necessary, we could greatly mitigate this space overhead. The lower bound matrix will have many elements that are zeros, and thus would be amiable to be encoded as a sparse matrix.\n If needed, both the upper and lower bound matrix\u2019s could be computed in a just-in-time fashion [41]. This would greatly reduce the memory footprint, at the expense of a more complicated implementation.\nFor clarity of presentation, we present our contributions in two different sections, although the final algorithm incorporates both ideas. In Sections 4.2.1 to 4.2.4, we show how to accelerate the TADPole algorithm by admissibly pruning the distance computations during the calculation of local densities (\u03c1) and NN distances (\u03b4) from a higher density list for each item. In Section 4.2.5, we show how to reorder these computations to give us the diminishing returns property of anytime algorithms\n[3][61]."}, {"heading": "4.2.1 Pruning During Local Density Calculation", "text": "Consider the four cases shown in Fig. 6. In this step of the TADPole algorithm, the inputs are the fully computed lower (LBMatrix) and upper bound (UBMatrix) matrix. For each object pair (i,j), while calculating their local densities (lines 1 to 3 in TABLE 1), we prune their distance (Dij) computation according to the following four cases shown in Fig. 6:\nCase A: Objects i and j are identical The DTW distance of two identical objects, i and j, is equal to their ED distance. It is a simple lookup in the upper bound distance matrix, and requires no actual DTW distance computation. This case is logically possible but very rare (Fig. 6.A)).\nCase B: UBMatrix(i,j) < dc If the upper bound distance between objects i and j is less than the cutoff distance (dc), then i and j are definitely within dc distance to each other (Fig. 6.B)). Therefore, we can prune the DTW distance computation of these two objects.\nCase C: LBMatrix(i,j) > dc If the lower bound distance of i and j is greater than the cutoff distance, then these two objects are definitely not within dc distance to each other (Fig. 6.C)). We can therefore admissibly prune their DTW distance computation.\nCase D: LBMatrix(i,j) < dc and UBMatrix(i,j) > dc In this case, we cannot tell whether or not the actual DTW distance between i and j is within dc. Therefore, only in this case do we need to compute Dij (Fig. 6.D)).\nWith this intuition in mind, we specify the formal distance pruning algorithm during the local density calculation in TABLE 5.\nAs we can see from TABLE 5, in lines 5 - 21, for all the object pairs in the data, the TADPole algorithm checks which of the four cases applies in order to determine whether or not these objects are within the cutoff distance.\nSTRATEGY 7\nThe occurrence of case B tells us that the object pair in question is definitely within dc (lines 10 -11) without having to calculate the expensive true DTW distance. Cases A (lines 8 -9) and C (lines 12 -13) specify that the object pair is not within dc. It is only the occurrence of case D that forces the algorithm to calculate the true DTW distance of the object pair in question (lines 14 -19).\nAt the end of this section of TADPole, for each object i, we have all the local densities (\u03c1i) computed. Using lines 1 - 3 of TABLE 2, we can now find the \u03b4 list, the list of the points with higher densities. Next we will describe our pruning strategy for this step.\nTABLE 5\nPRUNING ALGORITHM DURING LOCAL DENSITY CALCULATION\nInput: LBMatrix, full computed lower bound matrix\nUBMatrix, full computed upper bound matrix Data, the dataset dc, cutoff distance\nOutput: \u03c1,local density vector for all points in dataset\nDSparse, partially filled distance matrix\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n23\n24\nDSparse = empty for i = 1:size(Data)\nobjectsWithin_dc = empty\nfor j = 1: size(Data)\nif i == j\ncontinue;\nelse\nif LBMatrix(i,j) == UBMatrix(i,j) //case A)\ncontinue\nelseif UBMatrix(i,j) < dc //case B)\nobjectsWithin_dc = [objectsWithin_dc j]\nelseif LBMatrix(i,j) > dc // case C)\ncontinue\n//case D)\nelseif LBMatrix(i,j) < dc and UBMatrix(i,j) > dc\nDSparse(i,j) = calculateDist(Data(i),Data(j)) if DSparse(i,j) < dc\nobjectsWithin_dc = [objectsWithin_dc j]\nend if\nend if\nend if\n\u03c1(i) = length(objectsWithin_dc)\nend for"}, {"heading": "4.2.2 Pruning During NN Distance Calculation from Higher Density List", "text": "Our pruning strategy for this step works in two phases. First, for each item we find an upper bound of the NN distance from its higher density list. In the second phase we perform the actual pruning based on these upper bounds. The distance computation of TADPole terminates when for all objects in the dataset, we are done finding their actual NN distance from their respective higher density lists.\nPhase 1: Upper bound calculation Given DSparse and \u03c1i for each item i, we initialize the\nupper bound of its NN distance from its higher density list, ubi, to inf. For each item j in the higher density list of i, we either have the actual DTW distance (Dij) computed already or have access to the upper bound (UBMatrix(i,j)) to this distance. We scan the higher density list of item i, and if the current ubi > Dij or ubi > UBMatrix(i,j), we update the current ubi to Dij (if available already), or to UBMatrix(i,j) otherwise. Therefore, we can guarantee that the NN distance from the higher density list for item i can be no larger than ubi. We give a visual illustration of this upper bound calculation in Fig. 7.\nIn Fig. 7, the elements on object i\u2019s higher density list are j1 \u2013 j4. Assume that we only know the DTW distances from object i to objects j1 and j3, (D1 and D3 respectively, shown in blue). Because we do not know D2 and D4, we have shown these distances in gray in Fig. 7. When Phase 1 starts, ubi is initialized to inf. Now our TADPole algorithm scans object j1 and updates ubi to D1. Because UBMatrix(i,j2) and D3 are both are greater than ubi, we do not need to update ubi. In the last step, given that UBMatrix(i,j4) < ubi, we update ubi to UBMatrix(i,j4). This ubi is an upper bound of the NN distance from object i\u2019s higher density list.\nWe give the upper bound calculation algorithm for the NN distance computation from a higher density list in TABLE 6. We initialize the upper bound vectors of NN distances of objects from their higher density list, ub to inf (line 1). Next, considering each of the item on the higher density list of an object i, \u03b4_listi(j),we check whether i\u2019s current upper bound can be tightened (lines 5 -13). In lines 5 - 8 we see if the actual distance between i and \u03b4_listi(j) has been computed already, then whether or not this distance can tighten ubi. If the distance has not been computed yet, then in lines 10 - 12 we check whether we can tighten ubi by replacing it with the upper bound distance between i and \u03b4_listi(j).\nAt the end of this phase of TADPole, we have ub, the upper bound vector of NN distances from higher density points, computed. We now describe exploiting ub to\nLBMatrix(i,j1)\nD1\nUBMatrix(i,j1)\nD2 UBMatrix(i,j2)\nD3\nUBMatrix(i,j3)\nA)\nB)\nC)\ni j1\ni\ni\nj2\nj3\nD4 UBMatrix(i,j4)\ni j4\nD)\nLBMatrix(i,j2)\nLBMatrix(i,j4)\nLBMatrix(i,j3)\nprune the distance calculations during the computation of the higher density list.\nTABLE 6\nUPPER BOUND CALCULATION ALGORITHM FOR NN DISTANCE COMPUTATION FROM HIGHER DENSITY LIST\nInput: UBMatrix, full computed upper bound matrix\nData, the dataset\nDSparse, partially filled distance matrix \u03b4_list, list of the points with higher densities\nOutput: ub, upper bound vector of NN distances from higher\ndensity points\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nub = inf(size(Data))\nfor i = 1:size(Data)\nfor j = 1:size(\u03b4_listi)\nhighDensityItem = \u03b4_listi(j) if DSparse(i, highDensityItem)\u2260 empty\nif ubi > DSparse(i, highDensityItem)\nubi = DSparse(i, highDensityItem)\nend if\nelse\nif ubi > UBMatrix(i,highDensityItem)\nubi = UBMatrix(i,highDensityItem)\nend if\nend if\nend for\nend for\nPhase 2: Pruning We give the pruning algorithm during the computation of NN distances from the higher density lists of all objects in TABLE 7.\nWe begin by scanning the higher density list of each of the objects again. In line 5 of TABLE 7, for an object i, we test whether LBMatrix(i, \u03b4_listi(j)) is greater than ubi we calculated in TABLE 6 . If this is the case, we prune the distance computation (line 6) for \u03b4_listi(j). Otherwise, if the true distance between i and \u03b4_listi(j) is already calculated, then we consider this distance as one of the potential NN distances from i\u2019s higher density list (line 9). If the true distance is not yet known, only then do we compute it (line 11-12). Finally, we compute the NN distance vector for all objects from their higher density lists (line 17).\nIn Fig. 7, we see that both LBMatrix(i,j2) and LBMatrix(i,j3) are greater than ubi. Therefore, we can prune D2 and D3. In this example, we assumed we know D1; therefore, after the pruning done by Phase 2, we only need to calculate D4.\nTABLE 7\nPRUNING ALGORITHM DURING THE COMPUTATION OF THE NN DISTANCES FROM THE HIGHER DENSITY LISTS OF ALL OBJECTS\nInput: LBMatrix, full computed lower bound matrix\nData, the dataset\nDSparse, partially filled distance matrix \u03b4_list, list of the points with higher densities\nub, upper bound vector of NN distances from higher\ndensity points\nOutput: \u03b4, NN distance vector of higher density points\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nfor i = 1:size(Data)\ntemp_\u03b4 = empty\nfor j = 1:size(\u03b4_listi)\nhighDensityItem = \u03b4_listi(j) if LBMatrix(i,highDensityItem)> ubi\ncontinue //prune distance computation\nelse\nif DSparse(i, highDensityItem)\u2260 empty\ntemp_\u03b4 = [temp_\u03b4 DSparse(i, highDensityItem]\nelse // calculate distance\nDSparse(i, highDensityItem) = calculateDist(Data(i),Data(highDensityItem))\ntemp_\u03b4 = [temp_\u03b4 DSparse(i, highDensityItem]\nend if\nend if\nend for\n\u03b4(i) = min(temp_\u03b4)\nend for\nAfter this phase of TADPole, for each item i we have access to the NN distance from points with higher local densities (\u03b4i). At this point, given \u03c1i and \u03b4i for each object i\u00b8 the TADPole algorithm calculates the cluster centers \u03c7 using the algorithm in TABLE 3, and performs the cluster assignments based on these centers according to the algorithm in TABLE 4."}, {"heading": "4.2.3 Multidimensional Time Series Clustering", "text": "While most of the research efforts on time series clustering have considered only the single-dimensional case [23][42], the increasing prevalence of medical sensors (c.f. Section 6.2) and wearable devices (c.f. Section 6) has given urgency to the need to support multidimensional clustering [50]. Fortunately, our extension of TADPole to the multidimensional case requires changing only a single line of code. For clarity, we highlight these changes for multidimensional clustering for TABLE 5 and TABLE 7 in TABLE 8 and TABLE 9, respectively.\nTABLE 8\nPRUNING ALGORITHM DURING LOCAL DENSITY CALCULATION FOR MULTIDIMENSIONAL DATA (SEE TABLE 5)\nInput: LBMatrix, full computed lower bound matrix\nalong d dimensions\nUBMatrix, full computed upper bound matrix\nalong d dimensions\nData, the dataset dc, cutoff distance\nOutput: \u03b4, NN distance vector of higher density points\n16\n...\nDSparse(i,j) =\n...\nSTRATEGY 9\nTABLE 9\nPRUNING ALGORITHM DURING NN DISTANCE COMPUTATION FROM HIGHER DENSITY LIST, MULTIDIMENSIONAL CASE (SEE TABLE 7)\nRecall that in TABLE 5, we gave the full lower bound and upper bound distance matrices as inputs to the algorithm. To perform multidimensional clustering, for each dimension we wish to consider, we calculate the corresponding lower/upper bound distance matrices independently along those dimensions. We take the sum of all lower bound matrices/upper bound matrices and give these cumulative matrices as inputs to our algorithm described in TABLE 5. In addition, when we actually calculate the distances (line 16 in TABLE 5 and lines 10-12 in TABLE 7), we take the summation of the distances along all the dimensions. All other components of TADPole will remain the unchanged. We explicitly evaluate the utility of TADPole clustering for multidimensional clustering in Section 6.4. More generally, as we shall show empirically in Section 5, by using the methods described above, TADPole can obtain at least an order of magnitude speedup over the original DP algorithm while producing identical results."}, {"heading": "4.2.4 How Effective Is Our Pruning?", "text": "Here we will demonstrate just the utility of our pruning strategy, before generalizing to allow anytime behavior in the next section. In order to intuitively calibrate the effectiveness of our pruning, we compare TADPole to the best and worst possible cases of DP:  In order to perform clustering, the DP algorithm needs the all-pair distance matrix computed [45]. Therefore, in terms of distance computation, the brute force DP algorithm itself is the obvious worst-case straw man.  The best possible variant of DP is the one that performs a distance computation only when it is necessary. Therefore, during density computation, this variant of DP considers only those distance computations that contribute to the actual density of an object. In addition to this, during the computation of the NN distance from the higher density list of an object, this variant considers only the actual NN distances. We call this algorithm the oracle variant of DP. Note that we obviously cannot compute this in real-time, but only by doing an expensive post-hoc study.\nWe compare the amount of distance pruning we achieve against these two variants of the DP algorithm. For this experiment, we consider the StarLightCurves dataset [25]. We vary the number of objects in the dataset we need to cluster (by random sampling) and record the number of true DTW distance computations. As we can see from Fig. 8.left), the number of distance computations increases quadratically using the brute force algorithm. In contrast, the oracle algorithm requires very few distance\ncomputations; moreover, we can see that our TADPole algorithm performs almost as well as the oracle algorithm.\nThis claim is reinforced in Fig. 8.right, in which we see that the percentage of distance computations TADPole requires is very close to the oracle algorithm. As we can see, as the datasets get larger, TADPole converges closer and closer to the oracle algorithm.\nAlso note that a similar performance is observed in all datasets we considered (archived in [64] for brevity). Moreover, we obtain similar results if we measure the CPU time instead. As we can see from Fig. 9, to cluster the StarLightCurves data, TADPole requires only 9 minutes, whereas the DP algorithm needs 9 hours.\nTo put these results into context we consider the results in a very recent research effort. In [33], the authors discuss the scalability of their clustering algorithm saying that \u201cFor a large time series dataset with 9,236 objects with the length of each object n = 8192, it costs only 2.1 seconds to transform the whole dataset and an hour for clustering with DBSCAN\u201d. Using this as a benchmark, we did the following experiment: we took 9,236 random walks each of length 8,192, and clustered this dataset using DP algorithm. DP took only 16 seconds to cluster this dataset. Of course the hardware settings of these two experiments may not be exactly commensurate, but it is clear we have lost nothing in terms of scalability by considering adopting the DP algorithm rather than the near ubiquitous DBSCAN [33]. As we will later show, we have also lost nothing in terms of accuracy.\nIn spite of these very promising results, which demonstrate a sixty-fold speedup, there exist datasets where even this amount of speedup is not adequate. In order to address similar scalability issues for other types of data mining analyses, including classification [52] and outlier\n10\n11\n12\nelse // calculate distance\nDSparse(i, highDensityItem) =\ndetection [3][4], researchers have attempted to create anytime versions of their algorithms [3][60]. One significant advantage of the DP algorithm (and our modifications to it) is that it is particularly amiable to casting as an anytime framework. In essence, the computations discussed in this section can be computed in any order. Thus far, we have simply computed them in a top-to-bottom, left-toright order. However, we should expect that not all such computations of the true DTW are equally significant in terms of their impact on the final clustering, and that if we could find even an approximate \u201cmost-significantfirst\u201d order, we could converge more quickly. In the next section, we describe such an ordering heuristic."}, {"heading": "4.2.5 Distance-Computation Ordering Heuristic", "text": "Recall from the above that the DP algorithm may be considered a two-step algorithm; calculating the local densities (TABLE 1) first, and then finding the NN distances from the higher density lists (TABLE 2) of the objects. Only the latter step is amiable to anytime ordering; the former step may be regarded as the setup time [3][52][60].\nBefore attempting to create an anytime ordering function, it will be instructive to consider two baselines: what is the best we could possibly do, and what would we have to do in order to claim we are beating the most obvious straw men?  The best ordering heuristic we consider is an oracle ordering. We can compute this by allowing the algorithm to cheat. In each step of the algorithm, this order chooses the object that maximizes the current Rand Index. The algorithm is cheating, because by definition, a clustering algorithm normally does not have access to class labels.  The most basic straw man is top-to-bottom, left-toright ordering, but that is brittle to \u201cluck\u201d. A random ordering is much less so, so we consider random ordering as the baseline we would like to improve upon.\nTo understand the performance of these two algorithms, we took the Insect dataset from [25] and measured the Rand Index as the two algorithms above kept refining the mixture of true DTW distances and upper bound distances that we have at the end of phase 1 (i.e., TABLE 6), into the set of all necessary DTW computations needed (i.e., TABLE 7). The results are shown in Fig. 10 (for the moment, ignore the blue line). For completeness, we also show the accuracy achieved using the Euclidean distance with the DP algorithm. If the Euclidean distance was competitive, it would be fruitless to waste time computing expensive DTW calculations. The results clearly show that in this dataset, DTW is needed.\nBeing initially worse, the random ordering linearly (in a stepwise fashion) converges on the true clustering. In contrast, the oracle algorithm achieves a perfect Rand Index after calculating the true DTW distances for the NN list of just five objects.\nAs impressive as the oracle\u2019s performance is, we can actually come very close to it, as shown by the blue curve in Fig. 10. The ordering heuristic TADPole exploits is the descending order of the local density (\u03c1) \u00d7 the upper bound distance (ub) from the higher density list of the\nobjects. With a little introspection, it is easy to see why our distance computation-ordering heuristic is as close as the oracle ordering. Recall from Section 4 that points with higher values of \u03c1\u00d7\u03b4 are more likely to become cluster centers. Until we calculate all the NN from each object\u2019s higher density list, we do not have access to their \u03b4. However, we can estimate \u03b4 by the tight upper bound ub to \u03b4. Our distance computation-order heuristic exploits ub to prioritize distance computations for items that are more likely to be centers. Because the centers are selected earlier, we achieve a higher Rand Index with very few actual distance computations.\nFrom Fig. 10.top) we see that in conjunction with all the pruning strategies described in Sections 4.2.1 and 4.2.2, TADPole achieves a perfect Rand Index after doing only ~6% of all possible distance computations. Of course, it does not \u201crealize\u201d this, and must compute ~10% of all possible distance computations before admissibly terminating."}, {"heading": "5 EXPERIMENTAL EVALUATION", "text": "All experiments in this paper (including the ones \u201cinline\u201d in the above text) are completely reproducible. We have archived all experimental code, parameter settings and data at [64]. The goal of our experiments is to show that our algorithm is more efficient and effective than current algorithms. We also show that our algorithm is not particularly sensitive to the few parameter choices we have to make. In addition to this, we demonstrate the utility of our approach on multiple real-world case studies.\nWe conducted our experiments on a Windows 8 machine with 3.5 GHz AMD A8-6500 APU with Radeon(tm) HD Graphics processor and 16 GB RAM. All our implementations are single threaded and were written in Matlab 7.9.0.529 (R2009b).\nSTRATEGY 11\n5.1 Comparison with State-of-the-Art Clustering Algorithms\nThe principle straw man we need to compare TADPole to is the brute force version of DP with DTW. This comparison is explicitly encoded in Fig. 10 and the similar figures below. In these experiments we also replace DTW with Euclidean distance to demonstrate that DTW is really necessary. In TABLE 10, we show a comparison of the clustering performance of TADPole to some well-known state-of-the-art clustering algorithms (which we carefully tuned) under DTW in five randomly chosen datasets from [25]. As we can see, the cluster quality returned by TADPole is usually better than the best-performing clustering algorithm. Note that we are not claiming DP is always superior, rather we chose DP because it is at least competitive with the state-of-the-art, and amiable to acceleration as we have demonstrated.\nThe greatly superior accuracy of TADPole makes the timing results somewhat irrelevant, but TADPole is at least an order of magnitude faster than the rival methods (exact numbers at [64]).\nIn addition to the comparisons mentioned above, we compare against a recent partitional clustering algorithm called k-Shape [37]. The k-Shape algorithm considers the shapes of time series by using a normalized version of the cross-correlation measure. Depending on the properties of the distance measure, this algorithm computes the cluster centroids, which are used to update the assignment of objects to cluster centers in an iterative fashion. In order to demonstrate the robustness of k-Shape, the authors compare the clustering time and quality against a large number of state-of-the-art clustering algorithms. According to these experiments, k-Shape outperforms its rivals in terms of both clustering quality and time.\nWe compare the cluster quality and time of TADPole against k-Shape. We exhaustively run experiments on all the datasets chosen by the k-Shape authors. To be fair, we use the same data folds for both these algorithms. For TADPole, we use a fixed 5% warping window for consistency. We show the cluster quality result in Fig. 11.\nAs we can see from Fig. 11, for the majority of the datasets, TADPole does much better than k-Shape. It is only in the ECGFiveDays and SonyAIBORobotSurface datasets, that k-Shape performs significantly better than TADPole.\nSince these two datasets are outliers, it is worth considering why they seem to favor k-Shape.\nIn the case of the SonyAIBORobotSurface dataset, the start and end time of the gaits are not aligned well in\nterms of time, which means that the first and last points of the two time series being compared may have very different values. In Fig. 12, we show two example time series from the two classes of this dataset.\nAs we can see in Fig. 12, the start points of these two time series are not aligned at all, even though the end points almost agree. Because of DTW\u2019s endpoint constraint [24][44], these are forced to match, resulting in large, near random contribution to the overall DTW distance calculation. This issue (and several solutions) has been known for decades (see Section 3 of \u201cEndpoint Variants of the DTW Algorithm\u201d of [39]). Rather than using one of the fixes in [39], we simply smoothed the data, which resulted in a Rand Index of 0.91 by TADPole, moving it from the \u201cin this region k-Shape is better\u201d firmly into the \u201cin this region TADPole is better\u201d camp.\nA similar observation explains results with the ECGFiveDays dataset, because the objects were extracted by an imperfect beat extractor, and therefore the data endpoints are not perfectly aligned (cf. Fig. 13.left). In addition to this, the disagreement of the start and last endpoints of the data is present (cf. Fig. 13.right) in this dataset. Once again, off-the-shelf smoothing (Matlab\u2019s smooth function with the default parameter) is enough to mitigate most of the problem. We ran TADPole on the smoothed data and the improved Rand Index for this dataset was 0.84, which is very close the k-Shape clustering\n(0.85).\nFinally, another recently published time series clustering technique called YADING is shown to \u201cprovide theoretical proof which... ...guarantees YADING\u2019s high performance\u201d [10]. However, these guarantees are only with respect to Euclidean distance. The only publicly available real dataset the authors of [10] test on is StarLightCurves, where they obtain a Normalized Mutual Information (NMI) score of 0.60. However, TADPole can achieve an NMI of 0.61, which is very slightly better. Likewise, in an expanded tech report that augments the paper [11], the method achieves an NMI of 0.61 on the CinC_ECG_torso dataset and 0.74 on MALLAT dataset, whereas TADPole achieves NMIs of 0.76 and 0.84, which are significantly better."}, {"heading": "5.2 Parameter Sensitivity Experiments", "text": "To demonstrate that TADPole is not sensitive to parameter choices, we took the Symbols and Insect dataset [25] and performed TADPole on it with k = 6 and k = 11 respectively. We then varied the cutoff distance (dc) parameter and measured the Rand Index obtained with the alternative settings. As we can see from Fig. 14. (a) and (b), there is a very wide range of choices for the values of dc, which leads to high-quality clustering. We did the same experiment for several other datasets, (details available in [64]) and the results confirm our claim that TADPole is not sensitive to this cutoff distance parameter.\nIn spite of this finding, it is clearly desirable to have some guidance in parameter setting. In the next section we introduce such a heuristic."}, {"heading": "5.3 Heuristic for Setting Parameters", "text": "Up to this point we have glossed over the issue of setting the values of the threshold and the warping window width, we will now repair that omission.\nFirst, we note that this problem is not unique to our setting, most unsupervised algorithms have this challenge. Our first proposed solution is therefore the default idea in the community. For any given problem, we can simply find the most similar dataset for which we do have labels, and hope that the best settings there (discovered by cross validation) will generalize to the current setting.\nHowever, beyond this we do have an idea which allows us to find good (not necessarily optimal) parameter settings in most cases. The idea is very simple, we use our unlabeled dataset at hand to build a new dataset for which we do have some labels, and use this labeled dataset to do cross validation, to set the parameters.\nThis idea leaves open the question of where we can find some class labeled data, our solution is to make it. Our basic idea is simple. Before we perform any clustering, we randomly sample objects from the dataset, which we call set R. For each object O in R, we create a copy of it that we denote as \u014c. We add some warping to \u014c, and place it into the dataset with the same pseudo-class label as O. The intuition is that because we know that object \u014c is just a minor variant of O, we can safely assume that had \u014c occurred naturally, it would have been in the same cluster as O, and our must-be-in-same cluster constraint was warranted. We denote the set containing all such warped version of objects in R, .\nAt this point, for all object \u014c in , we know a pseudo label. Given this, in addition to the must-be-in-samecluster constraint, we can say that if an object in R has a different label than a pseudo label of a new object \u014c, then for this pair, the must-not-be-in-same-cluster constraint can be warranted.\nThis idea seems to have a tautological paradox to it. It seems that if we add w amount of warping to the dataset, we will discover w warping in that dataset. However, this is not the case. A good setting for w depends not only on the intrinsic variability of the time axis and on the size of the dataset, but on the time series shapes themselves.\nHaving these two constraints in hand, we design a scoring function that measures the number of object pairs in <R, > satisfying these constraints for different warping windows over all possible pairs. Therefore, our score function is:\n(1)\nWe note the following:\n This idea, of generating new data, by randomly perturbing real data, is not novel [8][53] in general, but appears to be novel in the domain of time series.  We are not claiming that this idea is the final or best solution to this issue. We merely introduce it as an existence proof that it is possible to set the parameters to reasonable settings, even in the absence of labeled data.\nFor some randomly chosen datasets, we compare the Rand Index obtained by TADPole for different warping window sizes against the score from equation 1. Our intuition is, if the shape of the score curve agrees closely with the Rand Index curve, then this score function has good"}, {"heading": "20 60 100 140 20 60 100 1400 0", "text": "STRATEGY 13\ncorrelation with the Rand Index. Having set a reasonable value of dc , for warping window sizes where this score is relatively \u2018higher\u2019, we assume those warping window sizes as \u2018reasonable\u2019 parameter values. We illustrate our results in Fig. 15.\nFrom Fig. 15 we can see that the score obtained corresponding to different warping window widths matches closely with the associated Rand Index for a fixed dc. As long as the warping window width stays in relatively high scoring region, we can consider the parameter choice good.\nAt this point we are now able to address the following question: given a reasonably well set warping window width, how to estimate dc? We take the same approach to set dc as we did for setting the warping window size. First we set a reasonably well value of the warping window size for a fixed dc according to the scoring function described above. Now we fix the warping window width to any value in the \u2018good parameter zone\u2019 and change the value of dc. To quantify how well our selection of dc is, we consider the same scoring function we used for the warping window. Just like what we did for setting the warping window, we vary dc and record the score associated. The range of values resulting relatively high scores is the good parameter zone for dc. For the four datasets shown in Fig. 15, by keeping the warping window size fixed, we find the best dc values listed as in TABLE 11.\nTABLE 11\nBEST VALUE OF DC FOR REASONABLY GOOD WARPING WINDOW WIDTH\nDataset Warping Window\nWidth dc\nTrace 14% 1.7\nFaceAll 2% 3.25\nFaceFour 6% 4.8\nSwedishLeaf 2% 0.65"}, {"heading": "6 CASE STUDIES", "text": ""}, {"heading": "6.1 Electromagnetic Articulograph Dataset", "text": "The Electromagnetic Articulograph (EMA) is a device that is increasingly used for mouth movement studies [55]. The apparatus consists of a set of unobtrusive accelerometers that are attached to multiple positions on the tongue, lips, jaw, nose and forehead, and can record highresolution 3D movement/position data in real-time. Recent research has suggested that articulographs may eventually allow a \u201csilent speech\u201d interface that translates non-audio articulatory data to speech output, an idea that has significant potential for facilitating oral communication after a laryngectomy [55]. The most common use of articulographs is in speech therapy for a plethora of speech disorders. However, there is a significant obstacle to EMA use: setting up the system can take up to 30 minutes per session (this time is spent carefully gluing the sensors to the participant\u2019s face and tongue). Given this significant setup time, practitioners are anxious to get the most from each session, yet the goals of the session are not typically fixed, but rather change in reaction to the participant\u2019s progress and areas of difficulty. Thus, there is a need to cluster the utterances of speakers in an interactive fashion, so that sessions can be adapted on the fly. We consider a dataset of lower-lip accelerometer time series movement data of 18 words collected from multiple speakers, for a total of 414 objects [56]. The duration of utterance of each of the words is ~0.7 seconds. In Fig. 16.left), we show the data collection process for one of our subjects. In Fig. 16.right), we show two examples of the utterances of the word \u201cfate\u201d by two different individuals. These examples are clearly warped, suggesting this is an appropriate domain for TADPole.\nWe tested the power set of combinations of axes, confirming that axis Z with axis Y gives us the best clustering, with a Rand Index of 0.94 (the other results are archived at [64]). We can now ask how effective our pruning strategy is when performing this clustering. We show the result in Fig. 17.\nAs we can see from Fig. 17, TADPole achieves ~94% pruning, converging almost as quickly as the oracle algorithm. In wall clock terms, TADPole takes only 2.89 seconds, which is fast enough to provide interactive analysis and feedback to the patient."}, {"heading": "6.2 Pulsus Dataset", "text": "Pulsus Paradoxus is defined as a significant decline in the pulse with inspiration. It is a symptom of Cardiac Tamponade, a life-threatening condition where highpressure fluid fills the sac surrounding the heart, impairing cardiac filling and causing 20,000 deaths per year in the USA alone. Of the several ways to detect Pulsus, the least invasive and simplest uses the PPG (PhotoPlethysmoGram) shown in Fig. 18.top).\nFor this case study we consider a dataset of 500 PPGs from two sources: the MIMIC II Waveform Database [16][46] and our collaborators. The latter dataset has the advantage that our collaborators followed up on the patients (in some case, post-mortem); thus, we have access to unusually rich annotations and external knowledge to evaluate our result. As shown in Fig. 18.top), the raw PPG data is very complex, and following the suggestions of Dr. John Criley (UCLA School of Medicine), we converted the PPGs to amplitude spectrums (Fig. 18.middle) and clustered in that space. Dr. Criley\u2019s intuition is that for Tamponade patients, the fluid that fills the sac surrounding the heart will cause a \u201cshadow\u201d signal to show up during respiration.\nFig. 18. top) PPG and Power Spectral Density (PSD) signal of a patient with non-severe Pulsus (top.left)) and severe Pulsus (top.right)). bottom) Four PSDs of four patients forming four different clusters within the non-Pulsus objects. From these four clusters, we can see the objects are clearly warped.\nFor this dataset, TADPole produced a perfect clustering with a pruning rate of 88%, making it an order of magnitude faster than brute-force. In Fig. 19.left), we show the PPG measurement process. From Fig. 19.right), we can see that the Pulsus instances are within a compact cluster and the non-Pulsus instances seem to form a number of sub-clusters. Our medical collaborator suggests this reflects the fact that there is one way to have Tamponade, but multiple ways to have a healthy heartbeat."}, {"heading": "6.3 Person Re-Identification Dataset", "text": "Person re-identification is the task of recognizing individuals across spatially disjointed cameras [15], and is an important problem for understanding human behavior in areas covered by surveillance cameras. As shown in Fig. 20, we can extract color histograms from the video, thus treating the problem as a multidimensional time series problem. We considered the PRID dataset [20], randomly extracting 1,000 images of 12 different individuals. After we ran TADPole in this dataset to cluster the images of these 12 individuals, we achieved a Rand Index of 95.4% and distance pruning of 80% (anytime plot at [64]). In contrast, Euclidean distance only achieves a Rand Index of 89%."}, {"heading": "6.4 Clustering Multidimensional Data", "text": "In order to demonstrate TADPole\u2019s suitability for multidimensional data, we clustered two real-world datasets - Cricket and uWaveGesture."}, {"heading": "6.4.1 Cricket Dataset", "text": "Cricket is a very popular game around the globe. As part of the refereeing, an umpire uses a fixed set of gestures using his hands and (sometimes) legs to communicate his decisions to a distant scorekeeper. The interested reader may find a list of complete signals in [32].\nIn this case study, we analyze the utility of TADPole for gesture recognition applications. For this, we use the dataset in [7], where 4 different umpires perform the various signals. On average, each signal is performed 16 times. Two accelerometers were attached to the wrists of the umpires and the data was sampled at 184 Hz. Considering both the left and right hands, this data has 6 dimensions (each accelerometer has X, Y and Z components).\nFig. 21 shows what each of the seven gestures Last Hour, Leg Bye, No Ball, One Short, Out, Penalty Runs and Six look like.\n0 10 20 30 40 50 60 0 10 20 30 40 50 60 0 10 20 30 40 50 60 0 10 20 30 40 50 60\nPatient 639 Patient 523 Patient 618 Patient 2975918\n0 10 20 30 40 50 600 10 20 30 40 50 60\nNormalized Respiration Rate Normalized Heart Rate\nP o\nw er\nS p\nec tr\nal\nD en\nsi ty\nFrequency\nA) B)\nC) D) E) F)\n200 600 1000 1400 1800200 600 1000 1400 1800\nNon-Severe Pulsus Severe Pulsus\nP P\nG\nSTRATEGY 15\nIn this data setting, we picked the tri-axial right hand signals as the three dimensions we wish to cluster. The number of objects in this dataset is 258 with each of the time series being of length 1,155. We used warping window = 5%.\nIn this dataset, TADPole gives a Rand Index of 0.92 with ~82% pruning. In wall clock terms, TADPole takes only 52 seconds to cluster this 27 minutes long data. That is to say, we can cluster the data about 30 times faster than real time."}, {"heading": "6.4.2 uWaveGestureLibrary Dataset", "text": "For this experiment we consider the three dimensional uWaveGestureLibrary dataset [25]. This data was collected to support efficient personalized gesture recognition on a wide range of electronic and mobile devices. This dataset contains eight simple gestures identified by a Nokia research study. The gestures were collected from eight participants with the Wii remote-based prototype. Fig. 22 shows these gestures as the paths of hand movement [31][38].\nIn this experiment, as before, we are interested in the actionability of TADPole to cluster human gestures. We use all eight gestures as input to TADPole. The number of objects in this dataset is 896 with each of the time series being of length 315 and a sampling rate of 100 Hz. After we run TADPole on this data, we achieve a Rand Index of 0.93 with ~88% pruning. In wall clock terms, TADPole needs only 56 seconds to cluster this 47 minutes long data. As before, we can cluster the data much faster than real time."}, {"heading": "6.5 Generalizing TADPole to Cluster Discrete Data", "text": "Until now, we have shown that our TADPole framework can efficiently perform exact clustering of real-valued time series data. The interested reader might wonder whether this framework is general enough to be extended for handling other types of data. In this section we present a dis-\ncussion of the generalization of this framework to cluster very long discrete biological sequences. We note here that our experiments are not claiming a novel discovery of any biological significance. We are merely demonstrating a \u201cproof-of-concept,\u201d that the TADPole framework can be extended to perform fast clustering of long biological strings.\nMore generally, we believe that this example shows that the TADPole framework may be useful in scaling up clustering for any distance measure for which one can compute both upper and lower bounds. Beyond DTW, this would include most string similarity measures (as we show below), the Earth Movers Distance, Graph Edit Distance etc."}, {"heading": "6.5.1 Clustering of Protein Sequences with Edit Distance", "text": "The identification of biological sequences with similar functionality requires similarity search in large databases, and is a well-known problem in the Bioinformatics community [14][19]. As an example, there exist diseases such as Maple Syrup Urine, Propionic Acidemia, Hurler Syndrome [34], etc. which are caused by the lack of sections of protein sequences in an organism, and can be lifethreatening. In order to design drugs for such diseases, biologists produce sequences with the same functionality, and insert these sequences into the organisms to compensate the deficit.\nTypically these protein strings are very long, therefore similarity search in such a large sequence database can be computationally challenging [2]. For example, large genomes are usually expressed using Expressed Sequence Tag (EST) databases. In such databases, gene portions are represented by mature mRNA, which typically are 500- 800 nucleotides long. Therefore, similarity-based algorithms in such biological databases require algorithms that can handle large strings efficiently. We see TADPole as a potential general framework for efficiently clustering such biological data.\nIn order to measure the similarity between two sequences of biological strings, researchers often use Edit Distance (EdD), or one of its many variants [29]. Given two strings A and B, the EdD is defined as the minimum number of deletion(s), insertion(s), and substitution(s) needed to transform A into B. We list the notation of EdD in TABLE 12:\nGiven the notations above, a substitution operation S(a[i],b[j]) is defined as replacing a[i] in A by b[j] in B. An insertion operation I(i, b[j]) is inserting b[j] in the ith location of A. Deleting the symbol a[i] in A is denoted by D(a[i],-). Each of these edit operations is assigned a weight which represent the difference between two symbols or the penalty for insertion/deletion operation. A transformation from A to B, T(A,B) is the set of edit operations applied to A iteratively which transform A to B. The weight of this transformation, w(T(A,B)) is defined as the sum of the weights of the operations in T(A,B). Given these definitions, the edit distance between A and B, EdD(A,B) is defined as, EdD(A,B) = min(w(t)), \u2200 t\u2208T(A,B).\nAs an example, for two strings A = \u2018INDUSTRY\u2019 and B = \u2018INTEREST\u2019, the optimal set of editing operations for T(A,B) are:\nStarting string = INDUSTRY, Goal String = INTEREST S(A[3],B[3]) = INT(D)USTRY S(A[4],B[4]) = INTE(U)STRY I(5,B[5]) = INTERSTRY I(6,B[6]) = INTERESTRY D(A[9],-) = INTEREST-(R)Y D(A[9],-) = INTEREST-(Y) Therefore EdD(A,B) = 6. Using dynamic programming, EdD(A,B) can be solved in O(nm) time and O(n+m) space, where n and m are the lengths of A and B respectively. For very large biological sequences, the computation of edit distance can therefore be very time consuming. We propose to exploit our TADPole framework to accelerate edit distance based clustering of long biological strings. Both edit distance and DTW are elastic distance measures [9] and for both of these measures, tight lower and upper bounds are already defined. Therefore, our TADPole framework is a suitable fit to perform fast and exact clustering of long biological strings. For clarity however, we confine our discussion only to the clustering of protein sequences.\nWe use Edit Distance (EdD) to perform the clustering of long protein sequences with TADPole framework. According to the framework structure, the inputs are the fully computed lower and upper bound matrices of the edit distance of all the objects of the dataset. As the lower bound of the edit distance, we use a notion of the distance that maps the strings of the database onto a multidimensional integer space using a wavelet based method [22]. We use the difference of the length of the strings in the dataset as the upper bound of the edit distance. Tighter bounds are known, but we are only interested in the general principle here.\nIn order to demonstrate the utility of TADPole in accelerating the clustering of large protein sequences, we use a random chunk of\nin [27] of the UniProt dataset [54]. It includes 178 strings defined over an alphabet of 21 letters. The protein strings have variable lengths ranging from 1052 to 1597. We cluster this dataset using TADPole and obtain distance pruning of 28%. Although the speedup we achieve does not look impressive at the first sight, we believe that with the application of tighter upper and lower bound of edit distance, TADPole would\nfurther improve. Given the distance pruning we achieve, it is instructive to see the utility of the clusters returned by TADPole. Out of the eight clusters returned by TADPole, one represents the rpoB gene which encodes the beta subunit of bacterial RNA polymerase. According to the STRING database, the DNA-dependent RNA polymerase \u201c...catalyzes the transcription of DNA into RNA using the four ribonucleoside triphosphates as substrates.\u201d [51] Another cluster corresponded to cell division protein mukB, which \u201c...plays a central role in chromosome condensation, segregation and cell cycle progression .\u201d[51]\nConsidering the fact that the clusters returned by TADPole have some level of biological significance, the readers might wonder how fast the algorithm converges to the ground truth. We show the result in Fig. 23.\nFrom Fig. 23 we can see that even if the amount of pruning achieved by TADPole is not impressive at the very first sight, the quality of the clusters in the earlier stages of the algorithm is very impressive, and it is almost as good as the oracle ordering. Therefore, with very small amount of actual edit distance computation, TADPole can produce very high quality cluster results."}, {"heading": "7 CONCLUSIONS", "text": "By introducing novel pruning strategies that exploit both upper and lower bounds, we have produced a robust DTW clustering algorithm that is both absolutely much faster than the state-of-the-art algorithms, and able to compute the clustering in an anytime fashion. We have demonstrated the utility of our algorithms on diverse domains, including two in which our algorithm is currently actively deployed (EMA and Pulsus). We have made all our code and data publicly available to allow the community to exploit and extend our ideas."}, {"heading": "8 ACKNOWLEDGEMENTS", "text": "We gratefully acknowledge the financial support for our research provided by NSF IIS-1161997 II, NIH R03 DC013990, NIH R01 DC013547 and all the donors of the datasets. We further acknowledge Dr. John Criley (UCLA School of Medicine) for donating the PPG data and providing detailed explanations of the intricacies of Pulsus Paradoxus.\nSTRATEGY 17"}, {"heading": "9 REFERERENCES", "text": "[1] Aggarwal, C. C., & Reddy, C. K. (Eds.). Data Clustering: Algo-\nrithms and Applications. CRC Press, 2013.\n[2] Agrawal, R., Faloutsos, C., & Swami, A. Efficient Similarity\nSearch in Sequence Databases (pp. 69-84). Springer Berlin Heidelberg, 1993.\n[3] Assent, I., et al. Anyout: Anytime Outlier Detection on Streaming\nData. In Database Systems for Advanced Applications, pp. 228-242, 2012.\n[4] Begum, N. et al. Rare Time Series Motif Discovery from Un-\nbounded Streams. Proceedings of the VLDB Endowment, 8(2), 2014.\n[5] Bradley, P. S., Fayyad, U. M., & Reina, C. Scaling Clustering\nAlgorithms to Large Databases. ACM SIGKDD, pp. 9-15, 1998.\n[6] Cao, F., Ester, M., Qian, W., & Zhou, A. Density-Based Cluster-\ning over an Evolving Data Stream with Noise. SIAM SDM, 2006.\n[7] Chambers, G. S., Venkatesh, S., West, G. A., & Bui, H. H.\nSegmentation of Intentional Human Gestures for Sports Video Annotation. IEEE Multimedia Modelling Conference, 2004.\n[8] Dau, H. A., Begum, N., & Keogh, E. Semi-Supervision Dramati-\ncally Improves Time Series Clustering under Dynamic Time Warping. ACM CIKM, 2016.\n[9] Ding, H., Trajcevski, G., Scheuermann, P., Wang, X., &\nKeogh, E. Querying and Mining of Time Series Data: Experimental Comparison of Representations and Distance Measures. Proceedings of the VLDB Endowment,1(2), 2008.\n[10] Ding, R. et al. YADING: Fast Clustering of Large-Scale Time\nSeries Data Proceedings of the VLDB Endowment 8.5, 2015.\n[11] Ding, R., Wang, Q., Dang, Y., Fu, Q., Zhang, H., and Zhang,\nD. Evaluation on Real Datasets: YADING: Fast Clustering of Large-scale Time Series Data. Microsoft Tech Report , 2015.\n[12] Ester, M., Kriegel, H. P., Sander, J., & Xu, X. A Density-Based\nAlgorithm for Discovering Clusters in Large Spatial Databases with Noise. ACM SIGKDD, pp. 226-231, 1996.\n[13] Ester, M., Kriegel, H.-P., Sander, J., Wimmer, M., and Xu, X.\nIncremental Clustering for Mining in A Data Warehousing Environment. VLDB, vol. 98, pp. 323-333. 1998.\n[14] Farr\u00e9, D., Roset, R., Huerta, M., Adsuara, J. E., Rosell\u00f3, L.,\nAlb\u00e0, M. M., & Messeguer, X. Identification of Patterns in Biological Sequences at the ALGGEN server: PROMO and MALGEN. Nucleic acids research, 31(13), 3651-3653, 2003.\n[15] Gheissari, N. et al. Person Reidentification using Spatiotemporal\nAppearance. IEEE CVPR, vol. 2, pp. 1528-1535, 2006.\n[16] Goldberger, A. L. et al. Physiobank, Physiotoolkit, and Physionet\nComponents of A New Research Resource for Complex Physiologic Signals. Circulation, 101(23), e215-e220, 2000.\n[17] Guha, S., Rastogi, R., & Shim, K. CURE: an efficient clustering\nalgorithm for large databases. ACM SIGMOD Record, vol. 27, no. 2, pp. 73-84, 1998.\n[18] Hinneburg, A., and Gabriel, H.-H. Denclue 2.0: Fast Clustering\nBased on Kernel Density Estimation. Advances in Intelligent Data Analysis VII, pp. 70-80, 2007.\n[19] Hildebrand, J. D., Schaller, M. D., & Parsons, J. T. Identification\nof Sequences Required for the Efficient Localization of the Focal Ad-\nhesion Kinase, pp125FAK, to Cellular Focal Adhesions. The Journal of cell biology, 123(4), 993-1005, 1993.\n[20] Hirzer, Martin, et al. Person Re-identification by Descriptive and\nDiscriminative Classification. Image Analysis. Springer Berlin Heidelberg, pp. 91-102, 2011.\n[21] Jang, J. S. R. Machine Learning Toolbox, available at\nmirlab.org/jang/matlab/toolbox/machineLearning, (Dec 1, 2014).\n[22] Kahveci, T., & Singh, A. K. An Efficient Index Structure for\nString Databases. Proceedings of the VLDB Endowment, Vol. 1, pp. 351-360, 2001.\n[23] Keogh, E., & Lin, J. Clustering of Time Series Subsequences is\nMeaningless: Implications for Previous and Future Research. KAIS, 8(2), 154-177, 2005.\n[24] Keogh, E. & Ratanamahatana, C.A. Exact Indexing of Dynamic\nTime Warping. KAIS 7, no. 3, 358-386, 2005.\n[25] Keogh, E., et al. The UCR Time Series Classification Page\n[26] Keogh, E., & Kasetty, S. On the Need for Time Series Data Min-\ning Benchmarks: A Survey and Empirical Demonstration. DMKD, 7(4), 349-371, 2003.\n[27] Kotsifakos, A., Stefan, A., Athitsos, V., Das, G., & Papapetrou,\nP. DRESS: Dimensionality Reduction for Efficient Sequence Search. DMKD, 1-32, 2015.\n[28] Krishnamurthy, A., Balakrishnan, S., Xu, M., & Singh, A.\nEfficient Active Algorithms for Hierarchical Clustering. arXiv preprint arXiv:1206.4672, 2012.\n[29] Levenshtein, V. I. Binary Codes Capable of Correcting Deletions,\nInsertions, and Reversals. In Soviet physics doklady (Vol. 10, No. 8, pp. 707-710), 1966.\n[30] Liao, T. W. Clustering of Time Series Data\u2014A Survey. Pattern\nrecognition 38, no. 11, 2005.\n[31] Liu, J., Zhong, L., Wickramasuriya, J., & Vasudevan, V.\nuWave: Accelerometer-based Personalized Gesture Recognition and Its Applications. Pervasive and Mobile Computing, 2009.\n[32] Lord\u2019s, The Home of Cricket, www.lords.org/laws-and-\nspirit/lawsof-cricket/laws/,accessed on Sept 17, 2015.\n[33] Mai, Son T., et al. Efficient Anytime Density-Based Clustering.\nSDM, 2013.\n[34] Metabolic Diseases Encyclopedia,\nhttp://www.encyclopedia.com/topic/Metabolic_diseases.aspx\n[35] Mueen, A., Keogh, E. J., Zhu, Q., Cash, S., & Westover, M. B.\nExact Discovery of Time Series Motifs. SDM, pp. 473-484, 2009.\n[36] Ng, A. Y., Jordan, M. I., & Weiss, Y. On Spectral Clustering:\nAnalysis and An Algorithm. Advances in Neural Information Processing Systems, 2, pp. 849-856, 2002.\n[37] Paparrizos, J., & Gravano, L. k-Shape: Efficient and Accurate\nClustering of Time Series. ACM SIGMOD, 2015.\n[38] Rabiner, L. R., & Juang, B. H. An Introduction to Hidden Markov\nModels. ASSP Magazine, IEEE, 3(1), 1986.\n[39] Rabiner, L. R., & Levinson, S. E. Isolated and Connected Word\nRecognition--Theory and Selected Applications. Communications, IEEE Transactions on, 29(5), 621-659, 1981.\n[40] Rakthanmanon, T. et al. The UCR Suite: Fast Subsequence\nSearch (DNA) www.youtube.com/watch?v=c7xz9pVr05Q, 2012.\n[41] Rakthanmanon, T., et al. Addressing Big Data Time Series: Min-\ning Trillions of Time Series Subsequences Under Dynamic Time Warping. ACM TKDD, 7(3), 10, 2013.\n[42] Rakthanmanon, T., et al. Time Series Epenthesis: Clustering\nTime Series Streams Requires Ignoring Some Data. ICDM 2011.\n[43] Rand, W. M. Objective Criteria for the Evaluation of Clustering\nMethods. J. Am. Statist. Assoc. 66.336, pp. - 846-850, 1971.\n[44] Ratanamahatana, C. A., & Keogh, E. Everything You Know\nAbout Dynamic Time Warping is Wrong. In 3rd Workshop on Mining Temporal and Sequential Data, pp. 22-25, 2004.\n[45] Rodriguez, A., & Laio, A. Clustering by Fast Search and Find of\nDensity Peaks. Science, 344(6191), 1492-1496, 2014.\n[46] Saeed, M., et al. Multiparameter Intelligent Monitoring in Inten-\nsive Care II (MIMIC-II): A Public-access Intensive Care Unit Database. Critical care medicine, 39(5), 952, 2011.\n[47] Sakoe, H., & Chiba, S. Dynamic Programming Algorithm Opti-\nmization for Spoken Word Recognition. IEEE Transactions on Acoustics, Speech and Signal Processing, 26(1), 43-49, 1978.\n[48] Signorini, A., Segre, A. M., & Polgreen, P. M. The Use of Twit-\nter to Track Levels of Disease Activity and Public Concern in the US During the Influenza A H1N1 Pandemic. PloS one, 6(5), 2011.\n[49] Shieh, J., & Keogh, E. iSAX: Indexing and Mining Terabyte Sized\nTime Series. ACM SIGKDD, pp. 623 \u2013 631, 2008.\n[50] Shokoohi-Yekta, M., Hu, B., Jin, H., Wang, J., & Keogh, E.\nGeneralizing Dynamic Time Warping to the Multi-Dimensional Case Requires an Adaptive Approach. SDM 2015.\n[51] STRING database: http://string-db.org/\n[52] Ueno, K., Xi, X., Keogh, E., & Lee, D. J. Anytime Classification\nUsing the Nearest Neighbor Algorithm with Applications to Stream Mining. IEEE ICDM, pp. 623-632, 2006.\n[53] Ulanova, L., Hao, Y. and Keogh, E. Generating Synthetic Data\nto Allow Learning from a Single Exemplar Per Class. SISAP 2014.\n[54] UniProt dataset: http://www.ebi.ac.uk/uniprot/\n[55] Wang, J. et al. Preliminary Test of A Real-time, Interactive Silent\nSpeech Interface Based on Electromagnetic Articulograph, SLPAT, pp. - 38 - 45, 2014.\n[56] Wang, J., Balasubramanian, A., Mojica de la Vega, L., Green,\nJ. R., Samal, A., & Prabhakaran, B. Word Recognition from Continuous Articulatory Movement Time-series Data Using Symbolic Representations, ACL/ISCA Workshop on Speech and Language Processing for Assistive Technologies, 2013.\n[57] Wang, X., et al. Experimental Comparison of Representation\nMethods and Distance Measures for Time Series Data. DMKD, 26(2), 275-309, 2013.\n[58] Xu, X., Ester, M., Kriegel, H-P., and Sander, J. A Distribution-\nbased Clustering Algorithm for Mining in Large Spatial Databases. ICDE, 1998 .pp. 324-331.\n[59] Yang, J., & Leskovec, J. Patterns of Temporal Variation in Online\nMedia. ACM WSDM, pp. 177-186, 2011.\n[60] Zhu, Q. et al. A Novel Approximation to Dynamic Time Warping\nallows Anytime Clustering of Massive Time Series Datasets. SDM, 2012.\n[61] Zilberstein, S. Using Anytime Algorithms in Intelligent Sys-\ntems. AI magazine, 17(3), 73, 1996.\n[62] 2009 MTV Video Music Awards\nen.wikipedia.org/wiki/2009_MTV_Video_Music_Awards\n[63] Unknown Author. (15th cent., second half). Treatises on Herald-\nry. Bodleian Library collection, MS. Lat. misc. e.\n[64] Supporting Webpage:\nhttp://www.cs.ucr.edu/~nbegu001/TADPoleDMKD Password: DMKD2016"}, {"heading": "10 APPENDIX", "text": "Proof of Exactness of the TADPole Algorithm\nTheorem 1. The cluster result obtained by TADPole is exactly the same as the one obtained by DPDTW.\nProof. In order to prove this theorem, we must first consider the following two lemmas: Lemma 1. The pruned distance computations by TADPole\nduring the density calculation have no effect on the local densities of the objects in the dataset. Proof (by contradiction). We prove this lemma in the context of the three cases of distance computation pruning during the local density calculation. Case A: Objects i and j are identical When i and j are identical, then their actual distance Dij is zero, which means they are definitely within dc. Therefore, we can safely prune their distance computation. Case B: UBMatrix(i,j) < dc In this case our claim is, Dij <= dc and we can safely prune the distance computation. For the sake of the proof, let us assume that Dij > dc. By definition, LBMatrix(i,j) <= Dij <= UBMatrix(i,j) Therefore, because UBMatrix(i,j) < dc, and Dij <= UBMatrix(i,j), Dij \u226fdc. Therefore, our claim holds. Case C: LBMatrix(i,j) > dc In this case our claim is, Dij > dc and we can safely prune the distance computation. For the sake of the proof, let us assume that Dij <= dc. By definition, LBMatrix(i,j) <= Dij <= UBMatrix(i,j) Therefore, because LBMatrix(i,j) > dc, and Dij >= LBMatrix(i,j), Dij \u2270dc. Therefore, our claim holds. TADPole only calculate the distances for the case when LBMatrix(i,j) < dc and UBMatrix(i,j) > dc. Therefore, lemma 1 holds. Lemma 2. The pruned distance computations by TADPole do\nnot affect the NN distance of any object calculation from its higher density list. Proof (by contradiction). We prove this lemma in the context of the two phase pruning during NN distance calculation of an object from its higher density list. Phase 1: Upper bound calculation\nSTRATEGY 19\nFor this phase we need to prove that the NN distance from its higher density list of an object i, \u03b4i <= ubi, where ubi is the upper bound of this NN distance we intend to find.\nFor the sake of the proof, let us assume \u03b4i > ubi. Assume that the higher density list of i consists of j1...jk. Assume that the NN of i from its higher density list is jn, 1<= n <= k . Therefore, \u03b4i By definition, ubi = or , ubi = UBMatrix(i,jp) ,where 1 <= p <= k Therefore, \u03b4i = , when n = p or \u03b4i <= UBMatrix(i,jp), when n \u2260 k. From this, we can say that \u03b4i \u226f ubi. Therefore, our claim holds. Phase 2: Pruning For this phase we need to prove that \u2200 j\u2208\u03b4_listi(j)), if LBMatrix(i, j) > ubi , then j is not the NN of i from its higher density list.\nFor the sake of the proof let us assume j is the NN of i from its higher density list.\nFrom the proof in phase 1, Dij <= ubi. By definition, LBMatrix(i,j) <= Dij <= UBMatrix(i,j)\nBut if LBMatrix(i, j) > ubi, then Dij > ubi. Therefore, Dij > ubi. It means that j cannot be the NN of i. (Proved)\nTADPole only prunes the computation of all Dij where j cannot be the NN of i from its higher density list. Therefore, lemma 2 holds. \nThe proof of lemma 1 and 2 states the exactness of our pruning strategy. Therefore, theorem 1 holds. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNurjahan Begum obtained her PhD in Computer Science from University of California, Riverside in 2016 with focus on Data Mining, Time Series Analysis and Machine Learning. Nurjahan has over 10 publications in top data mining and machine learning venues including SIGKDD, VLDB, ICDM, SDM, CIKM, etc. Currently Nurjahan is a Software Engineer at Teradata Labs, where she is actively building new development\nenhancement features for the Teradata query optimization engine. Prior to joining Teradata, Nurjahan worked as a Research Intern in Bell Labs (2014) and Yahoo Research (2015).\nLiudmila Ulanova graduated with PhD in Computer Science from University of California, Riverside in Spring 2016. She was focusing on Time Series Data Mining. Liudmila coauthored over a dozen of publications including those in the top-tier data mining venues such as SIGKDD, ICDM, and SDM. After graduation Liudmila joined Analysis and Experimentation team at Microsoft Corporation as a Software Developer. She works on the\nExperimentation Platform which aims to accelerate software innovation through trustworthy experimentation. Before starting at Microsoft, Liudmila was a Summer Research Intern at NEC Laboratories America (Summer 2014) and Facebook Inc. (Summer 2015).\nHoang Anh Dau is a second-year PhD student in Computer Science at University of California, Riverside. Her research interests lie in data mining of time series. She is particularly interested in the practical applications of data mining on real world problems. She holds a Bachelor of Foreign Languages from Hanoi University, a Master of Arts in Communication and Media Studies from Monash University,\nand a Master of Science in Computing from RMIT University. Jun Wang is an Assistant Professor of Biomedical Engineering and Communication Disorders at the University of Texas at Dallas. His research areas include silent speech recognition from articulatory motion, motor speech disorders due to neurological diseases, and computational neuroscience for speech. Wang earned his PhD in Computer Science from the University of Nebraska-Lincoln in 2011. He received the American SpeechLanguage-Hearing Foundation New Century Scholar Award in 2015. He has also authored in more than 25 papers in premier speech recognition, communication disorders, and data mining conferences and journals\nEamonn Keogh is a full professor of computer science at the University of California Riverside. His research areas include data mining, machine learning and information retrieval, specializing in techniques for solving similarity and indexing problems in time-series datasets. He has authored more than 200 papers. He received the ACM SIGKDD 2012 best paper award, the IEEE ICDM 2007 best paper award,\nand the SIGMOD 2001 best paper award. He has given over twodozen well-received tutorials in the premier conferences in data mining and databases."}], "references": [{"title": "Eds.). Data Clustering: Algorithms and Applications", "author": ["C.C. Aggarwal", "C.K. Reddy"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Efficient Similarity Search in Sequence Databases (pp. 69-84)", "author": ["R. Agrawal", "C. Faloutsos", "A. Swami"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1993}, {"title": "Anyout: Anytime Outlier Detection on Streaming Data", "author": ["I Assent"], "venue": "In Database Systems for Advanced Applications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Rare Time Series Motif Discovery from Unbounded Streams", "author": ["N Begum"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Scaling Clustering Algorithms to Large Databases", "author": ["P.S. Bradley", "U.M. Fayyad", "C. Reina"], "venue": "ACM SIGKDD,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Density-Based Clustering over an Evolving Data Stream with Noise", "author": ["F. Cao", "M. Ester", "W. Qian", "A. Zhou"], "venue": "SIAM SDM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Segmentation of Intentional Human Gestures for Sports Video Annotation", "author": ["G.S. Chambers", "S. Venkatesh", "G.A. West", "H.H. Bui"], "venue": "IEEE Multimedia Modelling Conference,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Semi-Supervision Dramatically Improves Time Series Clustering under Dynamic Time Warping", "author": ["H.A. Dau", "N. Begum", "E. Keogh"], "venue": "ACM CIKM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Querying and Mining of Time Series Data: Experimental Comparison of Representations and Distance Measures", "author": ["H. Ding", "G. Trajcevski", "P. Scheuermann", "X. Wang", "E. Keogh"], "venue": "Proceedings of the VLDB Endowment,1(2),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "YADING: Fast Clustering of Large-Scale Time Series Data", "author": ["R Ding"], "venue": "Proceedings of the VLDB Endowment", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Evaluation on Real Datasets: YADING: Fast Clustering of Large-scale Time Series Data", "author": ["R. Ding", "Q. Wang", "Y. Dang", "Q. Fu", "H. Zhang", "D. Zhang"], "venue": "Microsoft Tech Report ,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise", "author": ["M. Ester", "H.P. Kriegel", "J. Sander", "X. Xu"], "venue": "ACM SIGKDD,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Incremental Clustering for Mining in A Data Warehousing Environment", "author": ["M. Ester", "Kriegel", "H.-P", "J. Sander", "M. Wimmer", "X. Xu"], "venue": "VLDB, vol", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Identification of Patterns in Biological Sequences at the ALGGEN server: PROMO and MALGEN", "author": ["D. Farr\u00e9", "R. Roset", "M. Huerta", "J.E. Adsuara", "L. Rosell\u00f3", "M.M. Alb\u00e0", "X. Messeguer"], "venue": "Nucleic acids research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Person Reidentification using Spatiotemporal Appearance", "author": ["N Gheissari"], "venue": "IEEE CVPR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Physiobank, Physiotoolkit, and Physionet Components of A New Research Resource for Complex Physiologic Signals", "author": ["Goldberger", "A. L"], "venue": "Circulation, 101(23),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "CURE: an efficient clustering algorithm for large databases", "author": ["S. Guha", "R. Rastogi", "K. Shim"], "venue": "ACM SIGMOD Record, vol. 27,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Denclue 2.0: Fast Clustering Based on Kernel Density Estimation", "author": ["A. Hinneburg", "Gabriel", "H.-H"], "venue": "Advances in Intelligent Data Analysis VII,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Identification of Sequences Required for the Efficient Localization of the Focal Ad-  hesion Kinase, pp125FAK, to Cellular Focal Adhesions", "author": ["J.D. Hildebrand", "M.D. Schaller", "J.T. Parsons"], "venue": "The Journal of cell biology,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1993}, {"title": "Person Re-identification by Descriptive and Discriminative Classification. Image Analysis", "author": ["Hirzer", "Martin"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Machine Learning Toolbox, available at mirlab.org/jang/matlab/toolbox/machineLearning, (Dec", "author": ["J.S.R. Jang"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "An Efficient Index Structure for String Databases", "author": ["T. Kahveci", "A.K. Singh"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2001}, {"title": "Clustering of Time Series Subsequences is Meaningless: Implications for Previous and Future Research", "author": ["E. Keogh", "J. Lin"], "venue": "KAIS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Exact Indexing of Dynamic Time Warping", "author": ["E. Keogh", "C.A. Ratanamahatana"], "venue": "KAIS 7,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "On the Need for Time Series Data Mining Benchmarks: A Survey and Empirical Demonstration", "author": ["E. Keogh", "S. Kasetty"], "venue": "DMKD,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2003}, {"title": "DRESS: Dimensionality Reduction for Efficient Sequence", "author": ["A. Kotsifakos", "A. Stefan", "V. Athitsos", "G. Das", "P. Papapetrou"], "venue": "Search. DMKD,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Efficient Active Algorithms for Hierarchical Clustering", "author": ["A. Krishnamurthy", "S. Balakrishnan", "M. Xu", "A. Singh"], "venue": "arXiv preprint arXiv:1206.4672,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Binary Codes Capable of Correcting Deletions, Insertions, and Reversals", "author": ["V.I. Levenshtein"], "venue": "In Soviet physics doklady (Vol. 10,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1966}, {"title": "Clustering of Time Series Data\u2014A Survey", "author": ["T.W. Liao"], "venue": "Pattern recognition", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2005}, {"title": "uWave: Accelerometer-based Personalized Gesture Recognition and Its Applications", "author": ["J. Liu", "L. Zhong", "J. Wickramasuriya", "V. Vasudevan"], "venue": "Pervasive and Mobile Computing,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Efficient Anytime Density-Based Clustering", "author": ["Mai", "Son T"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Exact Discovery of Time Series Motifs", "author": ["A. Mueen", "E.J. Keogh", "Q. Zhu", "S. Cash", "M.B. Westover"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "On Spectral Clustering: Analysis and An Algorithm", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2002}, {"title": "k-Shape: Efficient and Accurate Clustering of Time Series", "author": ["J. Paparrizos", "L. Gravano"], "venue": "ACM SIGMOD,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "An Introduction to Hidden Markov Models", "author": ["L.R. Rabiner", "B.H. Juang"], "venue": "ASSP Magazine, IEEE,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1986}, {"title": "Isolated and Connected Word Recognition--Theory and Selected Applications", "author": ["L.R. Rabiner", "S.E. Levinson"], "venue": "Communications, IEEE Transactions on,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1981}, {"title": "The UCR Suite: Fast Subsequence Search (DNA) www.youtube.com/watch?v=c7xz9pVr05Q", "author": ["T Rakthanmanon"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2012}, {"title": "Addressing Big Data Time Series: Mining Trillions of Time Series Subsequences Under Dynamic Time Warping", "author": ["T Rakthanmanon"], "venue": "ACM TKDD,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "Time Series Epenthesis: Clustering Time Series Streams Requires Ignoring Some Data", "author": ["T Rakthanmanon"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2011}, {"title": "Objective Criteria for the Evaluation of Clustering Methods", "author": ["W.M. Rand"], "venue": "J. Am. Statist. Assoc. 66.336,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1971}, {"title": "Everything You Know About Dynamic Time Warping is Wrong", "author": ["C.A. Ratanamahatana", "E. Keogh"], "venue": "In 3rd Workshop on Mining Temporal and Sequential Data,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2004}, {"title": "Clustering by Fast Search and Find of Density Peaks", "author": ["A. Rodriguez", "A. Laio"], "venue": "Science, 344(6191),", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Multiparameter Intelligent Monitoring in Intensive Care II (MIMIC-II): A Public-access Intensive Care Unit Database", "author": ["M Saeed"], "venue": "Critical care medicine,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2011}, {"title": "Dynamic Programming Algorithm Optimization for Spoken Word Recognition", "author": ["H. Sakoe", "S. Chiba"], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1978}, {"title": "The Use of Twitter to Track Levels of Disease Activity and Public Concern in the US During the Influenza A H1N1 Pandemic", "author": ["A. Signorini", "A.M. Segre", "P.M. Polgreen"], "venue": "PloS one,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2011}, {"title": "iSAX: Indexing and Mining Terabyte Sized Time Series", "author": ["J. Shieh", "E. Keogh"], "venue": "ACM SIGKDD, pp", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2008}, {"title": "Generalizing Dynamic Time Warping to the Multi-Dimensional Case Requires an Adaptive Approach", "author": ["M. Shokoohi-Yekta", "B. Hu", "H. Jin", "J. Wang", "E. Keogh"], "venue": "SDM", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "Anytime Classification Using the Nearest Neighbor Algorithm with Applications to Stream Mining", "author": ["K. Ueno", "X. Xi", "E. Keogh", "D.J. Lee"], "venue": "IEEE ICDM,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2006}, {"title": "Generating Synthetic Data to Allow Learning from a Single Exemplar Per Class", "author": ["L. Ulanova", "Y. Hao", "E. Keogh"], "venue": "SISAP", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2014}, {"title": "Preliminary Test of A Real-time, Interactive Silent Speech Interface Based on Electromagnetic Articulograph, SLPAT, pp", "author": ["J Wang"], "venue": null, "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2014}, {"title": "Word Recognition from Continuous Articulatory Movement Time-series Data Using Symbolic Representations, ACL/ISCA", "author": ["J. Wang", "A. Balasubramanian", "L. Mojica de la Vega", "J.R. Green", "A. Samal", "B. Prabhakaran"], "venue": "Workshop on Speech and Language Processing for Assistive Technologies,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2013}, {"title": "Experimental Comparison of Representation Methods and Distance Measures for Time", "author": ["X Wang"], "venue": "Series Data. DMKD,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2013}, {"title": "A Distributionbased Clustering Algorithm for Mining in Large Spatial Databases", "author": ["X. Xu", "M. Ester", "Kriegel", "H-P", "J. Sander"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 1998}, {"title": "Patterns of Temporal Variation in Online Media", "author": ["J. Yang", "J. Leskovec"], "venue": "ACM WSDM,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2011}, {"title": "A Novel Approximation to Dynamic Time Warping allows Anytime Clustering of Massive Time Series Datasets", "author": ["Q Zhu"], "venue": null, "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2012}, {"title": "Using Anytime Algorithms in Intelligent Systems", "author": ["S. Zilberstein"], "venue": "AI magazine,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "ECAUSE of the prevalence of time series data in human endeavors, the research community has made substantial efforts to create efficient algorithms for classification, clustering, rule discovery, and anomaly detection for this data type [1][4][23][35][49].", "startOffset": 237, "endOffset": 240}, {"referenceID": 3, "context": "ECAUSE of the prevalence of time series data in human endeavors, the research community has made substantial efforts to create efficient algorithms for classification, clustering, rule discovery, and anomaly detection for this data type [1][4][23][35][49].", "startOffset": 240, "endOffset": 243}, {"referenceID": 22, "context": "ECAUSE of the prevalence of time series data in human endeavors, the research community has made substantial efforts to create efficient algorithms for classification, clustering, rule discovery, and anomaly detection for this data type [1][4][23][35][49].", "startOffset": 243, "endOffset": 247}, {"referenceID": 31, "context": "ECAUSE of the prevalence of time series data in human endeavors, the research community has made substantial efforts to create efficient algorithms for classification, clustering, rule discovery, and anomaly detection for this data type [1][4][23][35][49].", "startOffset": 247, "endOffset": 251}, {"referenceID": 45, "context": "ECAUSE of the prevalence of time series data in human endeavors, the research community has made substantial efforts to create efficient algorithms for classification, clustering, rule discovery, and anomaly detection for this data type [1][4][23][35][49].", "startOffset": 251, "endOffset": 255}, {"referenceID": 53, "context": "1, which illustrates a subset of a cluster we discovered in a social media dataset [59].", "startOffset": 83, "endOffset": 87}, {"referenceID": 53, "context": "A cluster of four Twitter hashtag usage time series (normalized for volume) over ~6 days starting from June 12, 2009 [59].", "startOffset": 117, "endOffset": 121}, {"referenceID": 44, "context": "In this example, the knowledge gleaned is clearly trivial; however, similar ideas have been used to track the levels of disease activity and public concern during the recent influenza A H1N1 pandemic [48].", "startOffset": 200, "endOffset": 204}, {"referenceID": 43, "context": "It has been extensively shown in the literature that the 40-year old distance DTW is an ideal similarity measure to capture/be invariant to such out-of-sync relationships [47].", "startOffset": 171, "endOffset": 175}, {"referenceID": 45, "context": "For most time series data mining algorithms, the quality of the output depends almost exclusively on the distance measure used [49].", "startOffset": 127, "endOffset": 131}, {"referenceID": 45, "context": "In the last decade, a consensus has emerged that the DTW distance measure is the best measure in most domains, almost always outperforming the Euclidean Distance (ED) and other purported rivals [49].", "startOffset": 194, "endOffset": 198}, {"referenceID": 36, "context": "The input data is the mitochondrial DNA after it was converted to a time series representation (converting DNA to time series is a commonly used operation [40][41]).", "startOffset": 155, "endOffset": 159}, {"referenceID": 37, "context": "The input data is the mitochondrial DNA after it was converted to a time series representation (converting DNA to time series is a commonly used operation [40][41]).", "startOffset": 159, "endOffset": 163}, {"referenceID": 37, "context": "1 Why This Problem Is Hard Because DTW is intrinsically slow due to its quadratic time complexity, there are two ideas that are commonly used to mitigate the problem of using such a sluggish distance measure [41].", "startOffset": 208, "endOffset": 212}, {"referenceID": 31, "context": "It has been noted that for many problems, including motif discovery [35] and classification [49], the results returned by DTW and Euclidean distance tend to become increasingly similar as the dataset sizes increase.", "startOffset": 68, "endOffset": 72}, {"referenceID": 45, "context": "It has been noted that for many problems, including motif discovery [35] and classification [49], the results returned by DTW and Euclidean distance tend to become increasingly similar as the dataset sizes increase.", "startOffset": 92, "endOffset": 96}, {"referenceID": 37, "context": "The effectiveness of this lowerbounding tends to improve dramatically as the datasets get larger [41].", "startOffset": 97, "endOffset": 101}, {"referenceID": 40, "context": "This effect is well known for time series classification [44][49], and it might be imagined that this also applies to clustering.", "startOffset": 57, "endOffset": 61}, {"referenceID": 45, "context": "This effect is well known for time series classification [44][49], and it might be imagined that this also applies to clustering.", "startOffset": 61, "endOffset": 65}, {"referenceID": 39, "context": "To show that this is not the case, we performed a parallel experiment in which we clustered the same objects and measured the performance using the Rand Index [43].", "startOffset": 159, "endOffset": 163}, {"referenceID": 37, "context": "This is because for larger datasets, we can expect to have a smaller best-so-far early on, which allows more effective pruning [41][44][49].", "startOffset": 127, "endOffset": 131}, {"referenceID": 40, "context": "This is because for larger datasets, we can expect to have a smaller best-so-far early on, which allows more effective pruning [41][44][49].", "startOffset": 131, "endOffset": 135}, {"referenceID": 45, "context": "This is because for larger datasets, we can expect to have a smaller best-so-far early on, which allows more effective pruning [41][44][49].", "startOffset": 135, "endOffset": 139}, {"referenceID": 22, "context": "However, in clustering, we need to know the distance between all pairs [23], or at least all distances within a certain range, which renders the typical use of lower-bounding pruning ineffective.", "startOffset": 71, "endOffset": 75}, {"referenceID": 11, "context": "For example, the scalable version of the ubiquitous DBSCAN uses an R*tree [12].", "startOffset": 74, "endOffset": 78}, {"referenceID": 20, "context": "prevent k-means from converging, we used the variant in [21] which performs k-means clustering using the all-pair distance matrix.", "startOffset": 56, "endOffset": 60}, {"referenceID": 41, "context": "We adapt DP (Density Peaks), a relatively new clustering framework that is able to ignore outlying data points [45].", "startOffset": 111, "endOffset": 115}, {"referenceID": 2, "context": "We augment DP such that it can exploit both DTW upper and lower bounds to compute only the absolutely necessary DTW calculations, and do so in a best-first manner, giving our algorithm the desirable anytime algorithm behavior [3][61].", "startOffset": 226, "endOffset": 229}, {"referenceID": 55, "context": "We augment DP such that it can exploit both DTW upper and lower bounds to compute only the absolutely necessary DTW calculations, and do so in a best-first manner, giving our algorithm the desirable anytime algorithm behavior [3][61].", "startOffset": 229, "endOffset": 233}, {"referenceID": 0, "context": "The field of clustering is vast, and even the subfield of clustering time series has an enormous literature [1][23][57][60].", "startOffset": 108, "endOffset": 111}, {"referenceID": 22, "context": "The field of clustering is vast, and even the subfield of clustering time series has an enormous literature [1][23][57][60].", "startOffset": 111, "endOffset": 115}, {"referenceID": 51, "context": "The field of clustering is vast, and even the subfield of clustering time series has an enormous literature [1][23][57][60].", "startOffset": 115, "endOffset": 119}, {"referenceID": 54, "context": "The field of clustering is vast, and even the subfield of clustering time series has an enormous literature [1][23][57][60].", "startOffset": 119, "endOffset": 123}, {"referenceID": 51, "context": "Much of the works on time series clustering are concerned with clustering based on time series features [57], which are at best tangentially related to our goals.", "startOffset": 104, "endOffset": 108}, {"referenceID": 9, "context": "Most of the literature on time series shape-based clustering uses metric measures such as Euclidean distance [10][57].", "startOffset": 109, "endOffset": 113}, {"referenceID": 51, "context": "Most of the literature on time series shape-based clustering uses metric measures such as Euclidean distance [10][57].", "startOffset": 113, "endOffset": 117}, {"referenceID": 45, "context": "[49]), at least for classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "The plethora of shape-based clustering algorithms [23][42][60] can be divided at the highest level into those that insist on explaining (i.", "startOffset": 50, "endOffset": 54}, {"referenceID": 38, "context": "The plethora of shape-based clustering algorithms [23][42][60] can be divided at the highest level into those that insist on explaining (i.", "startOffset": 54, "endOffset": 58}, {"referenceID": 54, "context": "The plethora of shape-based clustering algorithms [23][42][60] can be divided at the highest level into those that insist on explaining (i.", "startOffset": 58, "endOffset": 62}, {"referenceID": 54, "context": ", clustering) all the data [60] vs.", "startOffset": 27, "endOffset": 31}, {"referenceID": 38, "context": "those that have the representational power to leave some data unclustered (a small minority) [42].", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "DBSCAN [12]) take the density of objects into consideration regardless of the shape the clusters may have.", "startOffset": 7, "endOffset": 11}, {"referenceID": 30, "context": "There exist works [33] in the literature that perform clustering on top of DBSCAN [12].", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "There exist works [33] in the literature that perform clustering on top of DBSCAN [12].", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "A variant of DBSCAN called IncrementalDBSCAN [13] has been demonstrated to perform much better than the bruteforce re-clustering of newly added/removed objects which is mostly appropriate for datasets changing incre1 2 3", "startOffset": 45, "endOffset": 49}, {"referenceID": 52, "context": "DBCLASD [58] is a non-parametric grid-based clustering algorithm which considers the density of objects like DBSCAN to define clusters.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "DENCLUE [18] is an algorithm which combines both density-based and grid-based approaches to define cluster centers that has the local maximum of some density function.", "startOffset": 8, "endOffset": 12}, {"referenceID": 16, "context": "There exist other types of clustering algorithms like hierarchical and model-based [17].", "startOffset": 83, "endOffset": 87}, {"referenceID": 54, "context": "A handful of research efforts [60] have attempted to mitigate the slow performance of DTW clustering by casting it to an anytime framework.", "startOffset": 30, "endOffset": 34}, {"referenceID": 24, "context": "We refer readers interested in time series clustering to the detailed surveys [26][30].", "startOffset": 78, "endOffset": 82}, {"referenceID": 28, "context": "We refer readers interested in time series clustering to the detailed surveys [26][30].", "startOffset": 82, "endOffset": 86}, {"referenceID": 4, "context": "There has been significant research on clustering datasets that are too large to fit in main memory [5][6].", "startOffset": 100, "endOffset": 103}, {"referenceID": 5, "context": "There has been significant research on clustering datasets that are too large to fit in main memory [5][6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 5, "context": "This problem setting typically assumes inexpensive distance measures, but costly disk accesses [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 26, "context": "For example, a recent paper on hierarchical clustering demonstrates (under some mild assumptions) that it is possible to capture the true structure of the clustering with just carefully chosen O(n log2 n) distance computations [28].", "startOffset": 227, "endOffset": 231}, {"referenceID": 54, "context": "Recall that anytime algorithms are algorithms that can return a valid solution to a problem, even if interrupted before ending [60][61].", "startOffset": 127, "endOffset": 131}, {"referenceID": 55, "context": "Recall that anytime algorithms are algorithms that can return a valid solution to a problem, even if interrupted before ending [60][61].", "startOffset": 131, "endOffset": 135}, {"referenceID": 55, "context": "The desirable properties of anytime algorithms are interruptibility, monotonicity, measurable quality, diminishing returns, preemptibility, and low overhead [61].", "startOffset": 157, "endOffset": 161}, {"referenceID": 55, "context": "Note that this is a very brief introduction to anytime algorithms; we refer the interested reader to [61], which contains an excellent survey.", "startOffset": 101, "endOffset": 105}, {"referenceID": 41, "context": "1 Density Peaks Algorithm Overview Our proposed solution is inspired by DP, the densitybased clustering algorithm recently proposed in [45].", "startOffset": 135, "endOffset": 139}, {"referenceID": 38, "context": "We chose to augment the DP framework for solving large time series clustering problems because of the following: \uf0b7 Recent literature [42] and our own experience on real datasets (cf.", "startOffset": 133, "endOffset": 137}, {"referenceID": 41, "context": "To make our argument more concrete, we will take the time to explain the clustering algorithm [45] we adapt and augment.", "startOffset": 94, "endOffset": 98}, {"referenceID": 41, "context": "The value of k can be specified by the user, or found automatically using a \u201cknee-finding\u201d type of algorithm [45].", "startOffset": 109, "endOffset": 113}, {"referenceID": 23, "context": "Most such algorithms exploit linear time lower bounds like LB_Keogh [24], LB_Kim, LB_Yi [57], etc.", "startOffset": 68, "endOffset": 72}, {"referenceID": 51, "context": "Most such algorithms exploit linear time lower bounds like LB_Keogh [24], LB_Kim, LB_Yi [57], etc.", "startOffset": 88, "endOffset": 92}, {"referenceID": 41, "context": "The only parameters we need are the cutoff distance (dc) and optionally, the number of clusters (k), if the user wishes to specify this value rather than use the kneefinding heuristic suggested in [45].", "startOffset": 197, "endOffset": 201}, {"referenceID": 37, "context": "\uf0b7 If needed, both the upper and lower bound matrix\u2019s could be computed in a just-in-time fashion [41].", "startOffset": 97, "endOffset": 101}, {"referenceID": 2, "context": "5, we show how to reorder these computations to give us the diminishing returns property of anytime algorithms [3][61].", "startOffset": 111, "endOffset": 114}, {"referenceID": 55, "context": "5, we show how to reorder these computations to give us the diminishing returns property of anytime algorithms [3][61].", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": "3 Multidimensional Time Series Clustering While most of the research efforts on time series clustering have considered only the single-dimensional case [23][42], the increasing prevalence of medical sensors (c.", "startOffset": 152, "endOffset": 156}, {"referenceID": 38, "context": "3 Multidimensional Time Series Clustering While most of the research efforts on time series clustering have considered only the single-dimensional case [23][42], the increasing prevalence of medical sensors (c.", "startOffset": 156, "endOffset": 160}, {"referenceID": 46, "context": "Section 6) has given urgency to the need to support multidimensional clustering [50].", "startOffset": 80, "endOffset": 84}, {"referenceID": 41, "context": "In order to intuitively calibrate the effectiveness of our pruning, we compare TADPole to the best and worst possible cases of DP: \uf0b7 In order to perform clustering, the DP algorithm needs the all-pair distance matrix computed [45].", "startOffset": 226, "endOffset": 230}, {"referenceID": 30, "context": "In [33], the authors discuss the scalability of their clustering algorithm saying that \u201cFor a large time series dataset with 9,236 objects with the length of each object n = 8192, it costs only 2.", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "Of course the hardware settings of these two experiments may not be exactly commensurate, but it is clear we have lost nothing in terms of scalability by considering adopting the DP algorithm rather than the near ubiquitous DBSCAN [33].", "startOffset": 231, "endOffset": 235}, {"referenceID": 47, "context": "In order to address similar scalability issues for other types of data mining analyses, including classification [52] and outlier 10", "startOffset": 113, "endOffset": 117}, {"referenceID": 2, "context": "detection [3][4], researchers have attempted to create anytime versions of their algorithms [3][60].", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "detection [3][4], researchers have attempted to create anytime versions of their algorithms [3][60].", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "detection [3][4], researchers have attempted to create anytime versions of their algorithms [3][60].", "startOffset": 92, "endOffset": 95}, {"referenceID": 54, "context": "detection [3][4], researchers have attempted to create anytime versions of their algorithms [3][60].", "startOffset": 95, "endOffset": 99}, {"referenceID": 2, "context": "Only the latter step is amiable to anytime ordering; the former step may be regarded as the setup time [3][52][60].", "startOffset": 103, "endOffset": 106}, {"referenceID": 47, "context": "Only the latter step is amiable to anytime ordering; the former step may be regarded as the setup time [3][52][60].", "startOffset": 106, "endOffset": 110}, {"referenceID": 54, "context": "Only the latter step is amiable to anytime ordering; the former step may be regarded as the setup time [3][52][60].", "startOffset": 110, "endOffset": 114}, {"referenceID": 20, "context": "Dataset TADPoleDTW (TADPoleED) kmeans[21]", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "DTWversion DBSCAN [12]", "startOffset": 18, "endOffset": 22}, {"referenceID": 32, "context": "DTWversion Spectral [36]", "startOffset": 20, "endOffset": 24}, {"referenceID": 33, "context": "In addition to the comparisons mentioned above, we compare against a recent partitional clustering algorithm called k-Shape [37].", "startOffset": 124, "endOffset": 128}, {"referenceID": 23, "context": "Because of DTW\u2019s endpoint constraint [24][44], these are forced to match, resulting in large, near random contribution to the overall DTW distance calculation.", "startOffset": 37, "endOffset": 41}, {"referenceID": 40, "context": "Because of DTW\u2019s endpoint constraint [24][44], these are forced to match, resulting in large, near random contribution to the overall DTW distance calculation.", "startOffset": 41, "endOffset": 45}, {"referenceID": 35, "context": "This issue (and several solutions) has been known for decades (see Section 3 of \u201cEndpoint Variants of the DTW Algorithm\u201d of [39]).", "startOffset": 124, "endOffset": 128}, {"referenceID": 35, "context": "Rather than using one of the fixes in [39], we simply smoothed the data, which resulted in a Rand Index of 0.", "startOffset": 38, "endOffset": 42}, {"referenceID": 9, "context": "guarantees YADING\u2019s high performance\u201d [10].", "startOffset": 38, "endOffset": 42}, {"referenceID": 9, "context": "The only publicly available real dataset the authors of [10] test on is StarLightCurves, where they obtain a Normalized Mutual Information (NMI) score of 0.", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "Likewise, in an expanded tech report that augments the paper [11], the method achieves an NMI of 0.", "startOffset": 61, "endOffset": 65}, {"referenceID": 7, "context": "We note the following: \uf0b7 This idea, of generating new data, by randomly perturbing real data, is not novel [8][53] in general, but appears to be novel in the domain of time series.", "startOffset": 107, "endOffset": 110}, {"referenceID": 48, "context": "We note the following: \uf0b7 This idea, of generating new data, by randomly perturbing real data, is not novel [8][53] in general, but appears to be novel in the domain of time series.", "startOffset": 110, "endOffset": 114}, {"referenceID": 49, "context": "1 Electromagnetic Articulograph Dataset The Electromagnetic Articulograph (EMA) is a device that is increasingly used for mouth movement studies [55].", "startOffset": 145, "endOffset": 149}, {"referenceID": 49, "context": "Recent research has suggested that articulographs may eventually allow a \u201csilent speech\u201d interface that translates non-audio articulatory data to speech output, an idea that has significant potential for facilitating oral communication after a laryngectomy [55].", "startOffset": 257, "endOffset": 261}, {"referenceID": 50, "context": "We consider a dataset of lower-lip accelerometer time series movement data of 18 words collected from multiple speakers, for a total of 414 objects [56].", "startOffset": 148, "endOffset": 152}, {"referenceID": 49, "context": "The amount of distance pruning achieved by TADPole is ~94% for the Articulographs [55].", "startOffset": 82, "endOffset": 86}, {"referenceID": 15, "context": "For this case study we consider a dataset of 500 PPGs from two sources: the MIMIC II Waveform Database [16][46] and our collaborators.", "startOffset": 103, "endOffset": 107}, {"referenceID": 42, "context": "For this case study we consider a dataset of 500 PPGs from two sources: the MIMIC II Waveform Database [16][46] and our collaborators.", "startOffset": 107, "endOffset": 111}, {"referenceID": 14, "context": "3 Person Re-Identification Dataset Person re-identification is the task of recognizing individuals across spatially disjointed cameras [15], and is an important problem for understanding human behavior in areas covered by surveillance cameras.", "startOffset": 135, "endOffset": 139}, {"referenceID": 19, "context": "We considered the PRID dataset [20], randomly extracting 1,000 images of 12 different individuals.", "startOffset": 31, "endOffset": 35}, {"referenceID": 19, "context": "Representative images from the dataset [20] with their corresponding color histograms.", "startOffset": 39, "endOffset": 43}, {"referenceID": 6, "context": "For this, we use the dataset in [7], where 4 different umpires perform the various signals.", "startOffset": 32, "endOffset": 35}, {"referenceID": 29, "context": "22 shows these gestures as the paths of hand movement [31][38].", "startOffset": 54, "endOffset": 58}, {"referenceID": 34, "context": "22 shows these gestures as the paths of hand movement [31][38].", "startOffset": 58, "endOffset": 62}, {"referenceID": 29, "context": "The gesture vocabulary adopted from [31][38].", "startOffset": 36, "endOffset": 40}, {"referenceID": 34, "context": "The gesture vocabulary adopted from [31][38].", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "1 Clustering of Protein Sequences with Edit Distance The identification of biological sequences with similar functionality requires similarity search in large databases, and is a well-known problem in the Bioinformatics community [14][19].", "startOffset": 230, "endOffset": 234}, {"referenceID": 18, "context": "1 Clustering of Protein Sequences with Edit Distance The identification of biological sequences with similar functionality requires similarity search in large databases, and is a well-known problem in the Bioinformatics community [14][19].", "startOffset": 234, "endOffset": 238}, {"referenceID": 1, "context": "Typically these protein strings are very long, therefore similarity search in such a large sequence database can be computationally challenging [2].", "startOffset": 144, "endOffset": 147}, {"referenceID": 27, "context": "In order to measure the similarity between two sequences of biological strings, researchers often use Edit Distance (EdD), or one of its many variants [29].", "startOffset": 151, "endOffset": 155}, {"referenceID": 2, "context": "As an example, for two strings A = \u2018INDUSTRY\u2019 and B = \u2018INTEREST\u2019, the optimal set of editing operations for T(A,B) are: Starting string = INDUSTRY, Goal String = INTEREST S(A[3],B[3]) = INT(D)USTRY S(A[4],B[4]) = INTE(U)STRY I(5,B[5]) = INTERSTRY I(6,B[6]) = INTERESTRY D(A[9],-) = INTEREST-(R)Y D(A[9],-) = INTEREST-(Y) Therefore EdD(A,B) = 6.", "startOffset": 174, "endOffset": 177}, {"referenceID": 2, "context": "As an example, for two strings A = \u2018INDUSTRY\u2019 and B = \u2018INTEREST\u2019, the optimal set of editing operations for T(A,B) are: Starting string = INDUSTRY, Goal String = INTEREST S(A[3],B[3]) = INT(D)USTRY S(A[4],B[4]) = INTE(U)STRY I(5,B[5]) = INTERSTRY I(6,B[6]) = INTERESTRY D(A[9],-) = INTEREST-(R)Y D(A[9],-) = INTEREST-(Y) Therefore EdD(A,B) = 6.", "startOffset": 179, "endOffset": 182}, {"referenceID": 3, "context": "As an example, for two strings A = \u2018INDUSTRY\u2019 and B = \u2018INTEREST\u2019, the optimal set of editing operations for T(A,B) are: Starting string = INDUSTRY, Goal String = INTEREST S(A[3],B[3]) = INT(D)USTRY S(A[4],B[4]) = INTE(U)STRY I(5,B[5]) = INTERSTRY I(6,B[6]) = INTERESTRY D(A[9],-) = INTEREST-(R)Y D(A[9],-) = INTEREST-(Y) Therefore EdD(A,B) = 6.", "startOffset": 201, "endOffset": 204}, {"referenceID": 3, "context": "As an example, for two strings A = \u2018INDUSTRY\u2019 and B = \u2018INTEREST\u2019, the optimal set of editing operations for T(A,B) are: Starting string = INDUSTRY, Goal String = INTEREST S(A[3],B[3]) = INT(D)USTRY S(A[4],B[4]) = INTE(U)STRY I(5,B[5]) = INTERSTRY I(6,B[6]) = INTERESTRY D(A[9],-) = INTEREST-(R)Y D(A[9],-) = INTEREST-(Y) Therefore EdD(A,B) = 6.", "startOffset": 206, "endOffset": 209}, {"referenceID": 4, "context": "As an example, for two strings A = \u2018INDUSTRY\u2019 and B = \u2018INTEREST\u2019, the optimal set of editing operations for T(A,B) are: Starting string = INDUSTRY, Goal String = INTEREST S(A[3],B[3]) = INT(D)USTRY S(A[4],B[4]) = INTE(U)STRY I(5,B[5]) = INTERSTRY I(6,B[6]) = INTERESTRY D(A[9],-) = INTEREST-(R)Y D(A[9],-) = INTEREST-(Y) Therefore EdD(A,B) = 6.", "startOffset": 230, "endOffset": 233}, {"referenceID": 5, "context": "As an example, for two strings A = \u2018INDUSTRY\u2019 and B = \u2018INTEREST\u2019, the optimal set of editing operations for T(A,B) are: Starting string = INDUSTRY, Goal String = INTEREST S(A[3],B[3]) = INT(D)USTRY S(A[4],B[4]) = INTE(U)STRY I(5,B[5]) = INTERSTRY I(6,B[6]) = INTERESTRY D(A[9],-) = INTEREST-(R)Y D(A[9],-) = INTEREST-(Y) Therefore EdD(A,B) = 6.", "startOffset": 252, "endOffset": 255}, {"referenceID": 8, "context": "As an example, for two strings A = \u2018INDUSTRY\u2019 and B = \u2018INTEREST\u2019, the optimal set of editing operations for T(A,B) are: Starting string = INDUSTRY, Goal String = INTEREST S(A[3],B[3]) = INT(D)USTRY S(A[4],B[4]) = INTE(U)STRY I(5,B[5]) = INTERSTRY I(6,B[6]) = INTERESTRY D(A[9],-) = INTEREST-(R)Y D(A[9],-) = INTEREST-(Y) Therefore EdD(A,B) = 6.", "startOffset": 273, "endOffset": 276}, {"referenceID": 8, "context": "As an example, for two strings A = \u2018INDUSTRY\u2019 and B = \u2018INTEREST\u2019, the optimal set of editing operations for T(A,B) are: Starting string = INDUSTRY, Goal String = INTEREST S(A[3],B[3]) = INT(D)USTRY S(A[4],B[4]) = INTE(U)STRY I(5,B[5]) = INTERSTRY I(6,B[6]) = INTERESTRY D(A[9],-) = INTEREST-(R)Y D(A[9],-) = INTEREST-(Y) Therefore EdD(A,B) = 6.", "startOffset": 299, "endOffset": 302}, {"referenceID": 8, "context": "Both edit distance and DTW are elastic distance measures [9] and for both of these measures, tight lower and upper bounds are already defined.", "startOffset": 57, "endOffset": 60}, {"referenceID": 21, "context": "As the lower bound of the edit distance, we use a notion of the distance that maps the strings of the database onto a multidimensional integer space using a wavelet based method [22].", "startOffset": 178, "endOffset": 182}, {"referenceID": 25, "context": "In order to demonstrate the utility of TADPole in accelerating the clustering of large protein sequences, we use a random chunk of in [27] of the UniProt dataset [54].", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "[1] Aggarwal, C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Agrawal, R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Assent, I.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Begum, N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Bradley, P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Cao, F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Chambers, G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Dau, H.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Ding, H.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Ding, R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Ding, R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Ester, M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Ester, M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Farr\u00e9, D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Gheissari, N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Goldberger, A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Guha, S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Hinneburg, A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Hildebrand, J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Hirzer, Martin, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Jang, J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Kahveci, T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Keogh, E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Keogh, E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] Keogh, E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] Kotsifakos, A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] Krishnamurthy, A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] Levenshtein, V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[30] Liao, T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] Liu, J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[33] Mai, Son T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[35] Mueen, A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[36] Ng, A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[37] Paparrizos, J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[38] Rabiner, L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[39] Rabiner, L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[40] Rakthanmanon, T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[41] Rakthanmanon, T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[42] Rakthanmanon, T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[43] Rand, W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[44] Ratanamahatana, C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[45] Rodriguez, A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[46] Saeed, M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[47] Sakoe, H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[48] Signorini, A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[49] Shieh, J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[50] Shokoohi-Yekta, M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[52] Ueno, K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "[53] Ulanova, L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[55] Wang, J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[56] Wang, J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[57] Wang, X.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[58] Xu, X.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "[59] Yang, J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "[60] Zhu, Q.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "[61] Zilberstein, S.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "Time Series Clustering is an important subroutine in many higher-level data mining analyses, including data editing for classifiers, summarization, and outlier detection. It is well known that for similarity search the superiority of Dynamic Time Warping (DTW) over Euclidean distance gradually diminishes as we consider ever larger datasets. However, as we shall show, the same is not true for clustering. Clustering time series under DTW remains a computationally expensive operation. In this work, we address this issue in two ways. We propose a novel pruning strategy that exploits both the upper and lower bounds to prune off a very large fraction of the expensive distance calculations. This pruning strategy is admissible and gives us provably identical results to the brute force algorithm, but is at least an order of magnitude faster. For datasets where even this level of speedup is inadequate, we show that we can use a simple heuristic to order the unavoidable calculations in a most-useful-first ordering, thus casting the clustering into an anytime framework. We demonstrate the utility of our ideas with both single and multidimensional case studies in the domains of astronomy, speech physiology, medicine and entomology. In addition, we show the generality of our clustering framework to other domains by efficiently obtaining semantically significant clusters in protein sequences using the Edit Distance, the discrete data analogue of DTW.", "creator": "Microsoft\u00ae Office Word 2007"}}}