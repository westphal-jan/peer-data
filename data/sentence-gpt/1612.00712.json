{"id": "1612.00712", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Probabilistic Neural Programs", "abstract": "We present probabilistic neural programs, a framework for program induction that permits flexible specification of both a computational model and inference algorithm while simultaneously enabling the use of deep neural networks. Probabilistic neural programs combine a computation graph for specifying a neural network with an operator for weighted nondeterministic choice. Thus, a program describes both a collection of decisions as well as the neural network architecture used to make each one. We evaluate our approach on a challenging diagram question answering task where probabilistic neural programs correctly execute nearly twice as many programs as a baseline model. Probabilistic neural programs provide a general framework for evaluating the degree to which there are some features in the model that support it. The probabilistic neural programs use features like the probabilistic data structures. An alternative, probabilistic neural program for assessing the degree to which probabilistic neural programs have the correct data structures are distributed.\n\n\n\n\n\nTo review, please refer to the paper, \"Neurosciences.com.\"", "histories": [["v1", "Fri, 2 Dec 2016 15:46:09 GMT  (763kb,D)", "http://arxiv.org/abs/1612.00712v1", "Appears in NAMPI workshop at NIPS 2016"]], "COMMENTS": "Appears in NAMPI workshop at NIPS 2016", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["kenton w murray", "jayant krishnamurthy"], "accepted": false, "id": "1612.00712"}, "pdf": {"name": "1612.00712.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Neural Programs", "authors": ["Kenton W. Murray", "Jayant Krishnamurthy"], "emails": ["kmurray4@nd.edu", "jayantk@allenai.org", "@NIPS"], "sections": [{"heading": "1 Introduction", "text": "In recent years, deep learning has produced tremendous accuracy improvements on a variety of tasks in computer vision and natural language processing. A natural next step for deep learning is to consider program induction, the problem of learning computer programs from (noisy) input/output examples. Compared to more traditional problems, such as object recognition that require making only a single decision, program induction is difficult because it requires making a sequence of decisions and possibly learning control flow concepts such as loops and if statements.\nPrior work on program induction has described two general classes of approaches. First, in the noise-free setting, program synthesis approaches pose program induction as completing a program \u201csketch,\u201d which is a program containing nondeterministic choices (\u201choles\u201d) to be filled by the learning algorithm [13]. Probabilistic programming languages generalize this approach to the noisy setting by permitting the sketch to specify a distribution over these choices as a function of prior parameters and further to condition this distribution on data, thereby training a Bayesian generative model to execute the sketch correctly [6]. Second, neural abstract machines define continuous analogues of Turing machines or other general-purpose computational models by \u201clifting\u201d their discrete state and computation rules into a continuous representation [9, 11, 7, 12]. Both of these approaches have demonstrated success at inducing simple programs from synthetic data but have yet to be applied to practical problems.\nWe observe that there are (at least) three dimensions along which we can characterize program induction approaches:\n1. Computational Model \u2013 what abstract model of computation does the model learn to control? (e.g., a Turing machine)\n2. Learning Mechanism \u2013 what kinds of machine learning models are supported? (e.g., neural networks, Bayesian generative models)\n\u2217Work done while on Internship at Allen Institute for Artificial Intelligence\n1st Workshop on Neural Abstract Machines & Program Induction (NAMPI), @NIPS 2016, Barcelona, Spain.\nar X\niv :1\n61 2.\n00 71\n2v 1\n[ cs\n.N E\n] 2\nD ec\n2 01\ndef mlp(v: Tensor): Pp[CgNode] = for {\nw1 <- param(\"w1\") b1 <- param(\"b1\") h1 = ((w1 * v) + b1).tanh w2 <- param(\"w2\") b2 <- param(\"b2\") out = (w2 * h1) + b2\n} yield { out }\nval dist: Pp[Int] = for { s <- mlp(new Tensor(...)) v <- choose(Array(0, 1), s) y <- choose(Array(2, 3), s) } yield { v + y } // tensor parameters initialized to 0 val params: NnParams println(dist.beamSearch(10, params)) // output: 2 (0.25), 3 (0.25), // 3 (0.25), 4 (0.25)\nNeural abstract machines conflate some of these dimensions: they naturally support deep learning, but commit to a particular computational model and approximate inference algorithm. These choices are suboptimal as (1) the bias/variance trade-off suggests that training a more expressive computational model will require more data than a less expressive one suited to the task at hand, and (2) recent work has suggested that discrete inference algorithms may outperform continuous approximations [5]. In contrast, probabilistic programming supports the specification of different (possibly taskspecific) computational models and inference algorithms, including discrete search and continuous approximations. However, these languages are restricted to generative models and cannot leverage the power of deep neural networks.\nWe present probabilistic neural programs, a framework for program induction that permits flexible specification of the computational model and inference algorithm while simultaneously enabling the use of deep neural networks. Our approach builds on computation graph frameworks [1, 3] for specifying neural networks by adding an operator for weighted nondeterministic choice that is used to specify the computational model. Thus, a program sketch describes both the decisions to be made and the architecture of the neural network used to score these decisions. Importantly, the computation graph interacts with nondeterminism: the scores produced by the neural network determine the weights of nondeterministic choices, while the choices determine the network\u2019s architecture. As with probabilistic programs, various inference algorithms can be applied to a sketch. Furthermore, a sketch\u2019s neural network parameters can be estimated using stochastic gradient descent from either input/output examples or full execution traces.\nWe evaluate our approach on a challenging diagram question answering task, which recent work has demonstrated can be formulated as learning to execute a certain class of probabilistic programs. On this task, we find that the enhanced modeling power of neural networks improves accuracy."}, {"heading": "2 Probabilistic Neural Programs", "text": "Probabilistic neural programs build on computation graph frameworks for specifying neural networks by adding an operator for nondeterministic choice. We have developed a Scala library for probabilistic neural programming that we use to illustrate the key concepts.\nFigure 1 (left) defines a multilayer perceptron as a probabilistic neural program. This definition resembles those of other computation graph frameworks. Network parameters and intermediate values are represented as computation graph nodes with tensor values. They can be manipulated with standard operations such as matrix-vector multiplication and hyperbolic tangent. Evaluating this function with a tensor yields a program sketch object that can be evaluated with a set of network parameters to produce the network\u2019s output.\nFigure 1 (right) shows how to use the choose function to create a nondeterministic choice. This function nondeterministically selects a value from a list of options. The score of each option is given by the value of a computation graph node that has the same number of elements as the list. Evaluating this function with a tensor yields a program sketch object that represents a function\nfrom neural network parameters to a probability distribution over values. The log probability of a value is proportional to the sum of the scores of the choices made in the execution that produced it. Performing (approximate) inference over this object \u2013 in this case, using beam search \u2013 produces an explicit representation of the distribution. Multiple nondeterministic choices can be combined to produce more complex sketches; this capability can be used to define complex computational models, including general-purpose models such as Turing machines. The library also has functions for conditioning on observations.\nAlthough various inference algorithms may be applied to a program sketch, in this work we use a simple beam search over executions. This approach accords with the recent trend in structured prediction to combine greedy inference or beam search with powerful non-factoring models [2, 10, 4]. The beam search maintains a queue of partial program executions, each of which is associated with a score. Each step of the search continues each execution until it encounters a call to choose, which adds zero or more executions to the queue for the next search step. The lowest scoring executions are discarded to maintain a fixed beam width. As an execution proceeds, it may generate new computation graph nodes; the search maintains a single computation graph shared by all executions to which these nodes are added. The search simultaneously performs the forward pass over these nodes as necessary to compute scores for future choices.\nThe neural network parameters are trained to maximize the loglikelihood of correct program executions using stochastic gradient descent. Each training example consists of a pair of program sketches, representing an unconditional and conditional distribution. The gradient computation is similar to that of a loglinear model with neural network factors. It first performs inference on both the conditional and unconditional distributions to estimate the expected counts associated with each nondeterministic choice. These counts are then backpropagated through the computation graph to update the network parameters."}, {"heading": "3 Diagram Question Answering with Probabilistic Neural Programs", "text": "We consider the problem of learning to execute program sketches in a food web computational model using visual information from a diagram. This problem is motivated by recent work [8], which has demonstrated that diagram question answering can be formulated as translating natural language questions to program sketches in this model, then learning to execute these sketches. Figure 2 shows some example questions from this work, along with the accompanying diagram that must be interpreted to determine the answers. The diagram (left) is a food web, which depicts a collection of organisms in an ecosystem with arrows to indicate what each organism eats. The right side of the figure shows questions pertaining to the diagram and their associated program sketches.\nThe possible executions of each program sketch are determined by a domain-specific computational model that is designed to reason about food webs. The nondeterministic choices in this model correspond to information that must be extracted from the diagram. Specifically, there are two functions that call choose to nondeterministically return a boolean value. The first function, organism(x), should return true if the text label x is an organism (as opposed to e.g., the image title). The second function, eat(x, y), should return true if organism x eats organism y. These functions do influence program control flow. The food web model also includes various other\nfunctions, e.g., for reasoning about population changes, that call organism and eat to extract information from the diagram. [8] has a more thorough description of the theory; our goal is to learn to make the choices in this theory.\nWe consider three models for learning to make the choices for both organism and eat: a non-neural (LOGLINEAR) model, as well as two probabilistic neural models (2-LAYER PNP and MAXPOOL PNP). All three learn models for both organism and eat using outputs from a computer vision system trained to detect organism, text, and arrow relations between them. [8] defines a set of hand-engineered features heuristically created from the outputs of this vision system. LOGLINEAR and 2-LAYER PNP use only these features, and the difference is simply in the greater expressivity of a two-layer neural network. However, one of the major strengths of neural models is their ability to learn latent feature representations automatically, and our third model also uses the direct outputs of the vision system not made into features. The architecture of MAXPOOL PNP reflects this and contains additional input layers that maxpool over detected relationships between objects and confidence scores. The expectation is that our neural network modeling of nondeterminism will learn better latent representations than the manually defined features."}, {"heading": "4 Experiments", "text": "We evaluate probabilistic neural programs on the FOODWEBS dataset introduced by [8]. This data set contains a training set of ~2,900 programs and a test set of ~1,000 programs. These programs are human annotated gold standard interpretations for the questions in the data set, which corresponds to assuming that the translation from questions to programs is perfect. We train our probabilistic neural programs using correct execution traces of each program, which are also provided in the data set.\nWe evaluate our models using two metrics. First, execution accuracy measures the fraction of programs in the test set that are executed completely correctly by the model. This metric is challenging because correctly executing a program requires correctly making a number of choose decisions. Our 1,000 test programs had over 35,000 decisions, implying that to completely execute a program correctly means getting on average 35 choose decisions correct without making any mistakes. Second, choose accuracy measures the accuracy of each decision independently, assuming all previous decisions were made correctly.\nTable 1 compares the accuracies of our three models on the FOODWEBS dataset. The improvement in accuracy between the baseline (LOGLINEAR) and the probabilistic neural program (2-LAYER PNP) is due to the neural network\u2019s enhanced modeling power. Though the choose accuracy does not improve by a large margin, the improvements translate into large gains in entire program correctness. Finally, as expected, the inclusion of lower level features (MAXPOOL PNP) not possible in LOGLINEAR significantly improved performance. Note that this task requires performing computer vision, and thus it is not expected that any model achieve 100% accuracy."}, {"heading": "5 Conclusion", "text": "We have presented probabilistic neural programs, a framework for program induction that permits flexible specification of computational models and inference algorithms while simultaneously enabling the use of deep learning. A program sketch describes a collection of nondeterministic decisions to be made during execution, along with the neural architecture to be used for scoring these decisions. The network parameters of a sketch can be trained from data using stochastic gradient descent. We demonstrate that probabilistic neural programs improve accuracy on a diagram question answering task which can be formulated as learning to execute program sketches in a domain-specific computational model."}, {"heading": "Acknowledgements", "text": "The authors would like to thank the reviewers for their comments as well as helpful discussions with Arturo Argueta and Oyvind Tafjord."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u00edn Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Gregory S. Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Ian J. Goodfellow", "Andrew Harp", "Geoffrey Irving", "Michael Isard", "Yangqing Jia", "Rafal J\u00f3zefowicz", "Lukasz Kaiser", "Manjunath Kudlur", "Josh Levenberg", "Dan Man\u00e9", "Rajat Monga", "Sherry Moore", "Derek Gordon Murray", "Chris Olah", "Mike Schuster", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul A. Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda B. Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Terpret: A probabilistic programming language for program induction", "author": ["Alexander L. Gaunt", "Marc Brockschmidt", "Rishabh Singh", "Nate Kushman", "Pushmeet Kohli", "Jonathan Taylor", "Daniel Tarlow"], "venue": "arXiv preprint arXiv:1608.04428,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Church: a language for generative models", "author": ["Noah D. Goodman", "Vikash K. Mansinghka", "Daniel M. Roy", "Keith Bonawitz", "Joshua B. Tenenbaum"], "venue": "In Proc. of Uncertainty in Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Semantic parsing to probabilistic programs for situated question answering", "author": ["Jayant Krishnamurthy", "Oyvind Tafjord", "Aniruddha Kembhavi"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever"], "venue": "CoRR, abs/1511.04834,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Maltparser: A language-independent system for data-driven dependency parsing", "author": ["Joakim Nivre", "Johan Hall", "Jens Nilsson", "Atanas Chanev", "G\u00fclsen Eryigit", "Sandra K\u00fcbler", "Svetoslav Marinov", "Erwin Marsi"], "venue": "Natural Language Engineering,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Programming with a differentiable forth interpreter", "author": ["Sebastian Riedel", "Matko Bo\u0161njak", "Tim Rockt\u00e4schel"], "venue": "arXiv preprint arXiv:1605.06640,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Combinatorial sketching for finite programs", "author": ["Armando Solar-Lezama", "Liviu Tancau", "Rastislav Bodik", "Sanjit Seshia", "Vijay Saraswat"], "venue": "In Proceedings of the 12th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS XII,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}], "referenceMentions": [{"referenceID": 10, "context": "First, in the noise-free setting, program synthesis approaches pose program induction as completing a program \u201csketch,\u201d which is a program containing nondeterministic choices (\u201choles\u201d) to be filled by the learning algorithm [13].", "startOffset": 224, "endOffset": 228}, {"referenceID": 4, "context": "Probabilistic programming languages generalize this approach to the noisy setting by permitting the sketch to specify a distribution over these choices as a function of prior parameters and further to condition this distribution on data, thereby training a Bayesian generative model to execute the sketch correctly [6].", "startOffset": 315, "endOffset": 318}, {"referenceID": 7, "context": "Second, neural abstract machines define continuous analogues of Turing machines or other general-purpose computational models by \u201clifting\u201d their discrete state and computation rules into a continuous representation [9, 11, 7, 12].", "startOffset": 215, "endOffset": 229}, {"referenceID": 5, "context": "Second, neural abstract machines define continuous analogues of Turing machines or other general-purpose computational models by \u201clifting\u201d their discrete state and computation rules into a continuous representation [9, 11, 7, 12].", "startOffset": 215, "endOffset": 229}, {"referenceID": 9, "context": "Second, neural abstract machines define continuous analogues of Turing machines or other general-purpose computational models by \u201clifting\u201d their discrete state and computation rules into a continuous representation [9, 11, 7, 12].", "startOffset": 215, "endOffset": 229}, {"referenceID": 3, "context": "These choices are suboptimal as (1) the bias/variance trade-off suggests that training a more expressive computational model will require more data than a less expressive one suited to the task at hand, and (2) recent work has suggested that discrete inference algorithms may outperform continuous approximations [5].", "startOffset": 313, "endOffset": 316}, {"referenceID": 0, "context": "Our approach builds on computation graph frameworks [1, 3] for specifying neural networks by adding an operator for weighted nondeterministic choice that is used to specify the computational model.", "startOffset": 52, "endOffset": 58}, {"referenceID": 1, "context": "This approach accords with the recent trend in structured prediction to combine greedy inference or beam search with powerful non-factoring models [2, 10, 4].", "startOffset": 147, "endOffset": 157}, {"referenceID": 8, "context": "This approach accords with the recent trend in structured prediction to combine greedy inference or beam search with powerful non-factoring models [2, 10, 4].", "startOffset": 147, "endOffset": 157}, {"referenceID": 2, "context": "This approach accords with the recent trend in structured prediction to combine greedy inference or beam search with powerful non-factoring models [2, 10, 4].", "startOffset": 147, "endOffset": 157}, {"referenceID": 6, "context": "This problem is motivated by recent work [8], which has demonstrated that diagram question answering can be formulated as translating natural language questions to program sketches in this model, then learning to execute these sketches.", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "[8] has a more thorough description of the theory; our goal is to learn to make the choices in this theory.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] defines a set of hand-engineered features heuristically created from the outputs of this vision system.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "We evaluate probabilistic neural programs on the FOODWEBS dataset introduced by [8].", "startOffset": 80, "endOffset": 83}], "year": 2016, "abstractText": "We present probabilistic neural programs, a framework for program induction that permits flexible specification of both a computational model and inference algorithm while simultaneously enabling the use of deep neural networks. Probabilistic neural programs combine a computation graph for specifying a neural network with an operator for weighted nondeterministic choice. Thus, a program describes both a collection of decisions as well as the neural network architecture used to make each one. We evaluate our approach on a challenging diagram question answering task where probabilistic neural programs correctly execute nearly twice as many programs as a baseline model.", "creator": "LaTeX with hyperref package"}}}