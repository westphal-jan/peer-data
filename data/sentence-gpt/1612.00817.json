{"id": "1612.00817", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Summary - TerpreT: A Probabilistic Programming Language for Program Induction", "abstract": "We study machine learning formulations of inductive program synthesis; that is, given input-output examples, synthesize source code that maps inputs to corresponding outputs. Our key contribution is TerpreT, a domain-specific language for expressing program synthesis problems. A TerpreT model is composed of a specification of a program representation and an interpreter that describes how programs map inputs to outputs. The inference task is to observe a set of input-output examples and infer the underlying program. From a TerpreT model we automatically perform inference using four different back-ends: gradient descent (thus each TerpreT model can be seen as defining a differentiable interpreter), linear program (LP) relaxations for graphical models, discrete satisfiability solving, and the Sketch program synthesis system. TerpreT has two main benefits. First, it enables rapid exploration of a range of domains, program representations, and interpreter models. Second, it separates the model specification from the inference algorithm, allowing proper comparisons between different approaches to inference. The model specification is not specific to the particular program, but rather specific to all three domains. The model specification is also not specific to the particular program. As with the inference algorithm, the inference task is not specific to the particular program, but rather to the input-output examples and implementation of the program. In general, it is important to consider the source code, such as the source code, the resulting code, and the output from the inference algorithm. In the second, the inference task is not specific to the specific program, but rather to the input-output examples and implementation of the program. The inference task is not specific to the particular program, but rather to the input-output examples and implementation of the program. In general, it is important to consider the source code, such as the source code, the resulting code, and the output from the inference algorithm. In general, it is important to consider the source code, such as the source code, the resulting code, and the output from the inference algorithm. In general, it is important to consider the source code, such as the source code, the resulting code, and the output from the inference algorithm. In general, it is important to consider the source code, such as the source code, the resulting code, and the output from the inference algorithm. In general, it is important to consider the source code, such as the source code, the result from the inference algorithm. In general, it is important to consider the source code, such as the source code, the", "histories": [["v1", "Fri, 2 Dec 2016 20:08:22 GMT  (6053kb,D)", "http://arxiv.org/abs/1612.00817v1", "7 pages, 2 figures, 4 tables in 1st Workshop on Neural Abstract Machines &amp; Program Induction (NAMPI), @NIPS 2016"]], "COMMENTS": "7 pages, 2 figures, 4 tables in 1st Workshop on Neural Abstract Machines &amp; Program Induction (NAMPI), @NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["alexander l gaunt", "marc brockschmidt", "rishabh singh", "nate kushman", "pushmeet kohli", "jonathan taylor", "daniel tarlow"], "accepted": false, "id": "1612.00817"}, "pdf": {"name": "1612.00817.pdf", "metadata": {"source": "CRF", "title": "TERPRET: A Probabilistic Programming Language for Program Induction", "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Rishabh Singh", "Nate Kushman", "Pushmeet Kohli", "Jonathan Taylor", "Daniel Tarlow"], "emails": ["dtartlow}@microsoft.com", "jtaylor@perceptiveio.com", "@NIPS"], "sections": [{"heading": "1 Introduction", "text": "Learning computer programs from input-output examples, or Inductive Program Synthesis (IPS), is a fundamental problem in computer science, dating back at least to Summers [1977] and Biermann [1978]. The field has produced many successes, with perhaps the most visible example being the FlashFill system in Microsoft Excel [Gulwani, 2011, Gulwani et al., 2012].\nThere has also been significant recent interest in neural network-based models with components that resemble computer programs [Giles et al., 1989, Joulin and Mikolov, 2015, Grefenstette et al., 2015, Graves et al., 2014, Weston et al., 2014, Kaiser and Sutskever, 2016, Reed and de Freitas, 2016, Neelakantan et al., 2016, Kurach et al., 2015, Andrychowicz and Kurach, 2016]. These models combine neural networks with external memory, external computational primitives, and/or built-in structure that reflects a desired algorithmic structure in their execution. However, none produce programs as output. Instead, the program is hidden inside \u201ccontrollers\u201d composed of neural networks that decide which operations to perform, and the learned program can only be understood in terms of the executions that it produces on specific inputs.\n\u2217Work done while author was at Microsoft Research.\n1st Workshop on Neural Abstract Machines & Program Induction (NAMPI), @NIPS 2016, Barcelona, Spain.\nar X\niv :1\n61 2.\n00 81\n7v 1\n[ cs\n.L G\n] 2\nD ec\nTechnique name Optimizer/Solver Description\nFMGD (Forward marginals, gradient descent) TensorFlow A gradient descent based approach which generalizes the approach used by Kurach et al. [2015].\n(I)LP ((Integer) linear programming) Gurobi A novel linear program relaxation that supports Gates [Minka and Winn, 2009].\nSMT Z3 Translation of the problem into a first-order logical formula with existential constraints.\nSKETCH [Solar-Lezama, 2008]\nSKETCH Cast the TERPRET model as a partial program (the interpreter) containing holes (the source code) to be inferred from a specification (input-output examples).\nTable 1: TERPRET back-end inference algorithms.\nIn this work we focus on models that represent programs as simple, natural source code [Hindle et al., 2012], i.e., the kind of source code that people write. There are two main advantages to representing programs as source code rather than weights of a neural network controller. First, source code is interpretable; the resulting models can be inspected by a human and debugged and modified. Second, programming languages are designed to make it easy to express the algorithms that people want to write. By using these languages as our model representation, we inherit inductive biases that can lead to strong generalization (i.e., generalizing successfully to test data that systematically differs from training). Of course, natural source code is likely not the best representation in all cases. For example, programming languages are not designed for writing programs that classify images, and there is no reason to expect that natural source code would impart a favorable inductive bias in this case.\nOptimization over program space is known to be a difficult problem. However, recent progress in neural networks showing that it is possible to learn models with differentiable computer architectures, along with the success of gradient descent-based optimization, raises the question of whether gradient descent could be a powerful new technique for searching over program space.\nThese issues motivate the main questions in this work. We ask (a) whether new models can be designed specifically to synthesize interpretable source code that may contain looping and branching structures, and (b) how searching over program space using gradient descent compares to the combinatorial search methods from traditional IPS.\nTo address the first question we develop models inspired by intermediate representations used in compilers like LLVM [Lattner and Adve, 2004] that can be trained by gradient descent. These models interact with external storage, handle non-trivial control flow with explicit if statements and loops, and, when appropriately discretized, a learned model can be expressed as interpretable source code. We note two concurrent works, Adaptive Neural Compilation [Bunel et al., 2016] and Differentiable Forth [Riedel et al., 2016], which implement similar models.\nTo address the second question, concerning the efficacy of gradient descent, we need a way of specifying many IPS problems such that the gradient based approach can be compared to alternative approaches in a like-for-like manner across a variety of domains. The alternatives originate from rich histories of IPS in the programming languages and inference in discrete graphical models. To our knowledge, no such comparison has previously been performed.\nBoth of the above benefit from being formulated in the context of TERPRET. TERPRET provides a means for describing an execution model (e.g., a Turing Machine, an assembly language, etc.) by defining a a program representation and an interpreter that maps inputs to outputs using the\nparametrized program. The TERPRET description is independent of any particular inference algorithm. The IPS task is to infer the execution model parameters (the program) given an execution model and pairs of inputs and outputs. An overview of the synthesis task appears in Fig. 1. To perform inference, TERPRET is automatically compiled into an intermediate representation which can be fed to a particular inference algorithm. Table 1 describes the inference algorithms currently supported by TERPRET. Interpretable source code can be obtained directly from the inferred model parameters. The driving design principle for TERPRET is to strike a subtle balance between the breadth of expression needed to precisely capture a range of execution models, and the restriction of expression needed to ensure that automatic compilation to a range of different back-ends is tractable.\n2 TERPRET Language\nDue to space, we give just a simple example of how TERPRET models are written. For a full grammar of the language and several longer examples, see the long version Gaunt et al. [2016b]. TERPRET obeys Python syntax, and we use the Python ast library to parse and compile TERPRET models. The model is composed of Param variables that define the program, Var variables that represent the intermediate state of computation, and Inputs and Outputs. Currently, all variables must be discrete with constant sizes. TERPRET supports loops with ranges defined by constants2, if statements, arrays, array indexing, and user-defined functions that map a discrete set of inputs to a discrete output. In total, the constraints imply that a model can be converted into a gated factor graph [Minka and Winn, 2009]. The details of how this works are in the longer version. A very simple TERPRET model appears in Fig. 2. In this model, there is a binary tape that the program writes to. The program is a rule table that specifies which state to write at the current position of a tape, conditional on the pattern that appeared on the two previous tape locations. The first two tape positions are initialized as Inputs, and the final tape position is the program output.\nWe have used TERPRET to build much more complicated models, including a Turing Machine, boolean circuits, a Basic Block model that is similar to the intermediate representation used by LLVM, and an assembly-like model.\n2These constants define maximum quantities, like the maximum number of timesteps that a program can execute for, or the maximum number of instructions that can be in a program. Variable-length behavior can be achieved by defining an absorbing end-state, by allowing a no-op instruction, etc."}, {"heading": "3 Experimental Results", "text": "Table 2 gives an overview of the benchmark programs which we attempt to synthesize. We created TERPRET descriptions for each execution model, we designed three synthesis tasks per model which are specified by up to 16 input-output examples. We measure the time taken by the inference techniques listed in Table 1 to synthesize a program for each task. Results are summarized in Table 3.\nWith the exception of the FMGD algorithm, we perform a single run with a timeout of 4 hours for each task. For the FMGD algorithm we run both a Vanilla form and an Optimized form with additional heuristics such as gradient clipping, gradient noise and an entropy bonus (Kurach et al. [2015]) to aid convergence. Even with these heuristics we observe that several random initializations of the FMGD algorithm stall in an uninterpretable local optimum rather than finding an interpretable discrete program in a global optimum. In the Vanilla case we report the fraction of 20 different random initializations which lead to any globally optimal solution consistent with the input-output specification and also the wall clock time for 1000 epochs of the gradient descent algorithm (the typical number required to reach convergence on a successful run). In the Optimized FMGD case, we randomly draw 120 sets of hyperparameters from a manually chosen distribution and run the learning with 20 different random initializations for each setting. We then report the success rate for the best hyperparameters found and also for the average across all runs in the random search.\nOur results show that traditional techniques employing constraint solvers (SMT and Sketch) outperform other methods (FMGD and ILP), with SKETCH being the only system able to solve all of these benchmarks before timeout3. Furthermore, Table 3 highlights that the precise formulation of the interpreter model can affect the speed of synthesis. Both the Basic Block and Assembly models are equally expressive, but the Assembly model is biased towards producing straight line code with minimal branching. In cases where synthesis was successful the Assembly representation is seen to outperform the Basic Block model in terms of synthesis time.\nIn addition, we performed a separate experiment to investigate the local optima arising in the FMGD formulation. Using TERPRET we describe the task of inferring the values of a bit string of length K from observation of the parity of neighboring variables. It is possible to show analytically that the number of local optima grow exponentially with K, and Table 4 provides empirical evidence that these minima are encountered in practice and they hinder the convergence of the FMGD algorithm.\n3Additional (pre)processing performed by SKETCH makes it slower than a raw SMT solver on the smaller tasks but allows it to succeed on the larger tasks"}, {"heading": "4 Conclusion", "text": "We presented TERPRET, a probabilistic programming language for specifying IPS problems. The flexibility of the TERPRET language in combination with the four inference backends allows likefor-like comparison between gradient-based and constraint-based techniques for inductive program synthesis. The primary take-away from the experiments is that constraint solvers outperform other approaches in all cases studied. However, our work is (intentionally) only measuring the ability to efficiently search over program space. We remain optimistic about extensions to the TERPRET framework which allow differentiable interpreters to handle problems involving perceptual data [Gaunt et al., 2016a], and about using machine learning techniques to guide search-based techniques [Balog et al., 2016]."}], "references": [{"title": "Learning efficient algorithms with hierarchical attentive memory", "author": ["Marcin Andrychowicz", "Karol Kurach"], "venue": "arXiv preprint arXiv:1602.03218,", "citeRegEx": "Andrychowicz and Kurach.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz and Kurach.", "year": 2016}, {"title": "Deepcoder: Learning to write programs", "author": ["Matej Balog", "Alexander L Gaunt", "Marc Brockschmidt", "Sebastian Nowozin", "Daniel Tarlow"], "venue": "arXiv preprint arXiv:1611.01989,", "citeRegEx": "Balog et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Balog et al\\.", "year": 2016}, {"title": "The inference of regular lisp programs from examples", "author": ["Alan W Biermann"], "venue": "IEEE transactions on Systems, Man, and Cybernetics,", "citeRegEx": "Biermann.,? \\Q1978\\E", "shortCiteRegEx": "Biermann.", "year": 1978}, {"title": "Adaptive neural compilation", "author": ["Rudy Bunel", "Alban Desmaison", "Pushmeet Kohli", "Philip H.S. Torr", "M. Pawan Kumar"], "venue": "CoRR, abs/1605.07969,", "citeRegEx": "Bunel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bunel et al\\.", "year": 2016}, {"title": "Lifelong perceptual programming by example", "author": ["Alexander L Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "venue": "arXiv preprint arXiv:1611.02109,", "citeRegEx": "Gaunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gaunt et al\\.", "year": 2016}, {"title": "Terpret: A probabilistic programming language for program induction", "author": ["Alexander L Gaunt", "Marc Brockschmidt", "Rishabh Singh", "Nate Kushman", "Pushmeet Kohli", "Jonathan Taylor", "Daniel Tarlow"], "venue": "arXiv preprint arXiv:1608.04428,", "citeRegEx": "Gaunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gaunt et al\\.", "year": 2016}, {"title": "Higher order recurrent networks and grammatical inference", "author": ["C. Lee Giles", "Guo-Zheng Sun", "Hsing-Hen Chen", "Yee-Chun Lee", "Dong Chen"], "venue": "In Advances in Neural Information Processing Systems 2, [NIPS Conference,", "citeRegEx": "Giles et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Giles et al\\.", "year": 1989}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Automating string processing in spreadsheets using input-output examples", "author": ["Sumit Gulwani"], "venue": "In ACM SIGPLAN Notices,", "citeRegEx": "Gulwani.,? \\Q2011\\E", "shortCiteRegEx": "Gulwani.", "year": 2011}, {"title": "Spreadsheet data manipulation using examples", "author": ["Sumit Gulwani", "William Harris", "Rishabh Singh"], "venue": "Communications of the ACM,", "citeRegEx": "Gulwani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gulwani et al\\.", "year": 2012}, {"title": "On the naturalness of software", "author": ["Abram Hindle", "Earl T Barr", "Zhendong Su", "Mark Gabel", "Premkumar Devanbu"], "venue": "In 2012 34th International Conference on Software Engineering (ICSE),", "citeRegEx": "Hindle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hindle et al\\.", "year": 2012}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Armand Joulin", "Tomas Mikolov"], "venue": "In Advances in Neural Information Processing Systems 2, [NIPS Conference,", "citeRegEx": "Joulin and Mikolov.,? \\Q1989\\E", "shortCiteRegEx": "Joulin and Mikolov.", "year": 1989}, {"title": "Neural gpus learn algorithms", "author": ["\u0141ukasz Kaiser", "Ilya Sutskever"], "venue": "In Proceedings of the 4th International Conference on Learning Representations.,", "citeRegEx": "Kaiser and Sutskever.,? \\Q2016\\E", "shortCiteRegEx": "Kaiser and Sutskever.", "year": 2016}, {"title": "Neural random-access machines", "author": ["Karol Kurach", "Marcin Andrychowicz", "Ilya Sutskever"], "venue": "In Proceedings of the 4th International Conference on Learning Representations 2016,", "citeRegEx": "Kurach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kurach et al\\.", "year": 2015}, {"title": "Llvm: A compilation framework for lifelong program analysis & transformation", "author": ["Chris Lattner", "Vikram Adve"], "venue": "In Code Generation and Optimization,", "citeRegEx": "Lattner and Adve.,? \\Q2004\\E", "shortCiteRegEx": "Lattner and Adve.", "year": 2004}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever"], "venue": "In Proceedings of the 4th International Conference on Learning Representations", "citeRegEx": "Neelakantan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "Neural programmer-interpreters", "author": ["Scott E. Reed", "Nando de Freitas"], "venue": "In Proceedings of the 4th International Conference on Learning Representations", "citeRegEx": "Reed and Freitas.,? \\Q2016\\E", "shortCiteRegEx": "Reed and Freitas.", "year": 2016}, {"title": "Programming with a differentiable forth interpreter", "author": ["Sebastian Riedel", "Matko Bosnjak", "Tim Rockt\u00e4schel"], "venue": "CoRR, abs/1605.06640,", "citeRegEx": "Riedel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2016}, {"title": "Program Synthesis By Sketching", "author": ["Armando Solar-Lezama"], "venue": "PhD thesis, EECS Dept., UC Berkeley,", "citeRegEx": "Solar.Lezama.,? \\Q2008\\E", "shortCiteRegEx": "Solar.Lezama.", "year": 2008}, {"title": "A methodology for lisp program construction from examples", "author": ["Phillip D Summers"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Summers.,? \\Q1977\\E", "shortCiteRegEx": "Summers.", "year": 1977}, {"title": "URL http://arxiv.org/ abs/1410.3916", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes"], "venue": "Memory networks. In Proceedings of the 3rd International Conference on Learning Representations", "citeRegEx": "Weston et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "Learning computer programs from input-output examples, or Inductive Program Synthesis (IPS), is a fundamental problem in computer science, dating back at least to Summers [1977] and Biermann [1978].", "startOffset": 163, "endOffset": 178}, {"referenceID": 1, "context": "Learning computer programs from input-output examples, or Inductive Program Synthesis (IPS), is a fundamental problem in computer science, dating back at least to Summers [1977] and Biermann [1978]. The field has produced many successes, with perhaps the most visible example being the FlashFill system in Microsoft Excel [Gulwani, 2011, Gulwani et al.", "startOffset": 182, "endOffset": 198}, {"referenceID": 13, "context": "Technique name Optimizer/Solver Description FMGD (Forward marginals, gradient descent) TensorFlow A gradient descent based approach which generalizes the approach used by Kurach et al. [2015].", "startOffset": 171, "endOffset": 192}, {"referenceID": 18, "context": "SKETCH [Solar-Lezama, 2008] SKETCH Cast the TERPRET model as a partial program (the interpreter) containing holes (the source code) to be inferred from a specification (input-output examples).", "startOffset": 7, "endOffset": 27}, {"referenceID": 10, "context": "In this work we focus on models that represent programs as simple, natural source code [Hindle et al., 2012], i.", "startOffset": 87, "endOffset": 108}, {"referenceID": 14, "context": "To address the first question we develop models inspired by intermediate representations used in compilers like LLVM [Lattner and Adve, 2004] that can be trained by gradient descent.", "startOffset": 117, "endOffset": 141}, {"referenceID": 3, "context": "We note two concurrent works, Adaptive Neural Compilation [Bunel et al., 2016] and Differentiable Forth [Riedel et al.", "startOffset": 58, "endOffset": 78}, {"referenceID": 17, "context": ", 2016] and Differentiable Forth [Riedel et al., 2016], which implement similar models.", "startOffset": 33, "endOffset": 54}, {"referenceID": 4, "context": "For a full grammar of the language and several longer examples, see the long version Gaunt et al. [2016b]. TERPRET obeys Python syntax, and we use the Python ast library to parse and compile TERPRET models.", "startOffset": 85, "endOffset": 106}, {"referenceID": 13, "context": "For the FMGD algorithm we run both a Vanilla form and an Optimized form with additional heuristics such as gradient clipping, gradient noise and an entropy bonus (Kurach et al. [2015]) to aid convergence.", "startOffset": 163, "endOffset": 184}], "year": 2016, "abstractText": "We study machine learning formulations of inductive program synthesis; that is, given input-output examples, synthesize source code that maps inputs to corresponding outputs. Our key contribution is TERPRET, a domain-specific language for expressing program synthesis problems. A TERPRET model is composed of a specification of a program representation and an interpreter that describes how programs map inputs to outputs. The inference task is to observe a set of inputoutput examples and infer the underlying program. From a TERPRET model we automatically perform inference using four different back-ends: gradient descent (thus each TERPRET model can be seen as defining a differentiable interpreter), linear program (LP) relaxations for graphical models, discrete satisfiability solving, and the SKETCH program synthesis system. TERPRET has two main benefits. First, it enables rapid exploration of a range of domains, program representations, and interpreter models. Second, it separates the model specification from the inference algorithm, allowing proper comparisons between different approaches to inference. We illustrate the value of TERPRET by developing several interpreter models and performing an extensive empirical comparison between alternative inference algorithms on a variety of program models. To our knowledge, this is the first work to compare gradient-based search over program space to traditional search-based alternatives. Our key empirical finding is that constraint solvers dominate the gradient descent and LP-based formulations.", "creator": "LaTeX with hyperref package"}}}