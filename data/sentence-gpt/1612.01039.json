{"id": "1612.01039", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2016", "title": "CER: Complementary Entity Recognition via Knowledge Expansion on Large Unlabeled Product Reviews", "abstract": "Product reviews contain a lot of useful information about product features and customer opinions. One important product feature is the complementary entity (products) that may potentially work together with the reviewed product. Knowing complementary entities of the reviewed product is very important because customers want to buy compatible products and avoid incompatible ones. In this paper, we address the problem of Complementary Entity Recognition (CER). Since no existing method can solve this problem, we first propose a novel unsupervised method to utilize syntactic dependency paths to recognize complementary entities. Then we expand category-level domain knowledge about complementary entities using only a few general seed verbs on a large amount of unlabeled reviews. The domain knowledge helps the unsupervised method to adapt to different products and greatly improves the precision of the CER task. The advantage of the proposed method is that it does not require any labeled data for training. We conducted experiments on 7 popular products with about 1200 reviews in total to demonstrate that the proposed approach is effective. To date, we have been able to demonstrate that the proposed approach is effective, and is not as effective as we hoped.", "histories": [["v1", "Sun, 4 Dec 2016 00:22:44 GMT  (378kb,D)", "http://arxiv.org/abs/1612.01039v1", "10 pages, 2 figures, IEEE BigData 2016"]], "COMMENTS": "10 pages, 2 figures, IEEE BigData 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hu xu", "sihong xie", "lei shu", "philip s yu"], "accepted": false, "id": "1612.01039"}, "pdf": {"name": "1612.01039.pdf", "metadata": {"source": "CRF", "title": "CER: Complementary Entity Recognition via Knowledge Expansion on Large Unlabeled Product Reviews", "authors": ["Hu Xu", "Sihong Xie", "Lei Shu", "Philip S. Yu"], "emails": ["hxu48@uic.edu,", "sxie@cse.lehigh.edu,", "lshu3@uic.edu,", "psyu@uic.edu"], "sections": [{"heading": null, "text": "Keywords-Entity Recognition; Relation Extraction; Product Relation; Complementary Entity; Complementary Product\nI. INTRODUCTION\nE-commerce websites (e.g., Amazon.com) contain a huge amount of products reviews and most existing works of sentiment analysis [1] (or opinion mining) on reviews focus on extracting opinion targets (aspects or features) of the reviewed product and the associated opinions [2]\u2013[4] (e.g., extract \u201cbattery\u201d and pos from \u201cIt has a good battery\u201d). Besides features about the reviewed product itself (e.g., \u201cbattery\u201d or \u201cscreen\u201d), one important feature is whether the reviewed product is compatible/incompatible with another product. We call the reviewed product target entity and the other product complementary entity. A pair of a target entity and its complementary entity forms a complementary relation. They may work together to fulfill some shared functionalities. So, they are usually co-purchased. For example, in Figure 1, we assume there are some reviews of several accessories (on the left) talking about compatibility issues. We consider these accessories as the target entities and they have some complementary entities (on the right side) mentioned in reviews. The target entities are one micro SD card, one tablet stand and one mouse; the\ncomplementary entities are one Nikon DSLR, one iPhone, one Samsung Galaxy S6 and one MS Surface Pro. An arrow pointing from a target entity to a complementary entity indicates that they have a complementary relation and shall work together. For example, the micro SD card can help the Samsung Galaxy S6 to expand its memory capacity. Knowing these complementary entities is important because compatible products are preferred over incompatible ones. Thus, recognizing complementary entities is an important task in text mining.\nProblem Statement: In this paper, we study the problem of Complementary Entity Recognition (CER) from reviews (e.g., extracting \u201cSamsung Galaxy S6\u201d from \u201cIt works with my Samsung Galaxy S6\u201d ). We observe that compatibility issues are more frequently discussed in reviews of electronics accessories, so we choose reviews of accessories for experiments. To the best of our knowledge, accessory reviews are not well studied before.\nPredicting complementary entities is pioneered by McAuley et al. [5] as a link prediction problem in social network. Their method mostly predicts category-level com-\nar X\niv :1\n61 2.\n01 03\n9v 1\n[ cs\n.C L\n] 4\nD ec\n2 01\npatible products based on the learned representations of the products. However, we observe that reviews contain many complementary entities based on firsthand user experiences, which provide practical fine-grained complementary entities. We detail the discussions of their method in Section II.\nThe proposed problem has a few challenges and also provides more research opportunities:\n\u2022 To the best of our knowledge, the linguistic patterns of complementary relations are not studied in computer science. There is no largely annotated dataset for supervised methods. We propose an unsupervised method, which does not require any labeled data to solve this problem (we only annotate a small amount of data for evaluation purposes). \u2022 Similar to the aspect (feature) extraction problem in reviews [4], CER is also a domain-specific problem. We leverage domain knowledge to help the unsupervised method to adapt to different products. This novel product domain knowledge is expanded using a few seed words on a large amount of unlabeled reviews under the same category as the target entity. The idea of using reviews under the same category as the target entity is that the number of reviews for one target entity is small. We observe that products (target entities) under the same category share similar complementary entities (i.e., two different micro SD cards may share complementary entities like phone or tablet). So the domain knowledge expanded on reviews from the same category is larger than that on reviews from a single target entity. Therefore, there is almost no labor-intensive effort to get domain knowledge. Our domain knowledge contains candidate complementary entities and domain-specific verbs. \u2022 Although the problem may be closely related to the well-known Named Entity Recognition (NER) problem on surface [6], recognizing a complementary entity requires more contexts. For example, given a review for a micro SD card, we should not treat \u201cSamsung Galaxy S6\u201d in \u201cSamsung Galaxy S6 is great\u201d as a complementary entity. However, we should consider the same entity in \u201cIt works with my Samsung Galaxy S6\u201d as a complementary entity. The domain knowledge contains domain-specific verbs, which greatly help to detect the contexts of complementary entities. \u2022 We further notice that some linguistic patterns of complementary relations are similar to other extraction patterns (e.g., patterns for aspect extraction). Candidate complementary entities in the domain knowledge can help to filter out non-complementary entities extracted by similar patterns.\nThe main contributions of this paper can be summarized as the following: we propose a novel problem called Complementary Entity Recognition (CER). Then we propose\na novel unsupervised method utilizing dependency paths to identify complementary relations and extract entities simultaneously. We further leverage domain knowledge to improve the precision of extraction. The domain knowledge is expanded on a large amount of unlabeled reviews from only a few seed words (general complementary verbs) via a novel set of dependency paths. The expanded domain knowledge can greatly improve the precision of the unsupervised method. We conduct thorough experiments and provide case studies to demonstrate that the proposed method is effective."}, {"heading": "II. RELATED WORKS", "text": "The proposed problem is closely related to product recommender systems that are able to separate substitues and complements [5], [7]. Zheng et al. [7] first propose to incorporate the concepts of substitutes and complements into recommendation systems by analyzing navigation logs. More specifically, predicting complementary relations is pioneered by McAuley et al. [5]. They utilize topic models and customer purchase information (e.g., the products in the \u201citems also viewed\u201d section and the \u201citems also bought\u201d section of a product page) to predict categorylevel substitutes and complements. However, we observe that purchase information generated by the unknown algorithm from Amazon.com tends to be noisy and inaccurate for complementary entities since co-purchased products may not be complementary to each other. We demonstrate that their predictions are non-complementary entities for the products that we use for experiments in Section VI. Also, categorylevel predictions are not good enough for specific pairs of products (i.e., DSLR lens and webcam are not complements). Furthermore, their predictions do not provide information about incompatible entities, which are valuable buying warnings for customers. Thus, fine-grained extraction of complementary entities from reviews that express firsthand user experience is important. To the best of our knowledge, the linguistic patterns of complementary relations are not studied in computer science.\nThe proposed problem is closely related to aspect extraction [2]\u2013[4], [8], which is to extract product features from reviews. More specifically, extracting comparable products (i.e, one type of substitutes, or products that can replace each other) from reviews is studied by Jindal and Liu [9]. Recently, dependency paths [10] are used for aspect extraction [8], [11]. Shu et al. [12] use unsupervised graph labeling method to identify entities from opinion targets. However, since aspects are mostly context independent and the same aspect may appear multiple times, aspect extraction in general does not need to extract each occurrence of an aspect (as long as the same aspect can be extracted at least once). In contrast, the CER problem is context dependent and many complementary entities are infrequent (i.e., Samsung Galaxy S6 is infrequent than the aspect price). We use dependency paths to accurately identify each\noccurrence of complementary entities. Since extracting each complementary entity can be inaccurate, we further utilize domain knowledge to improve the precision.\nCER is closely related to Named Entity Recognition (NER) [6] and relation extraction [13]. NER methods utilize annotated data to train a sequential tagger [14]\u2013[16]. However, our task is totally different from NER since we care about the context of a complementary entity and many complementary entities are not named entities (e.g., phone). CER is also different from relation extraction [13], [17]\u2013[19], which assumes that two entities are identified in advance. In reviews, the target entity is unfortunately missing in many cases (i.e., \u201cWorks with my phone\u201d). The proposed method only cares about the relation context of a complementary entity rather than a full relation."}, {"heading": "III. PRELIMINARIES", "text": "In this section, we first formally define our problem. Then we introduce basic ideas of the proposed method. Lastly, we describe dependency paths used in later sections.\nA. Problem Formalization\nOur problem is to recognize entities that functionally complement to the reviewed product. There are several definitions involved in this problem.\nDefinition 1 (Target Entity): We define target entity eT as the reviewed product.\nWe do not extract target entities from reviews but assume that the target entity can be retrieved from the meta data (product title) of reviews. This is because many mentions of the target entity are co-referenced or implicitly assumed in reviews. For example, if the reviewed product is a tablet stand, \u201cIt works with my Samsung Galaxy S6\u201d uses \u201cIt\u201d to refer to the target entity tablet stand; \u201cWorks well with Samsung Galaxy S6\u201d completely omits the target entity.\nDefinition 2 (Complementary Entity): Given a set of reviews RT of a target entity eT , a complementary entity eC is an entity mentioned in reviews that are functionally complementary to the target entity eT . A target entity has a set of complementary entities: eC \u2208 EC .\nA complementary entity can either be a single noun (e.g., iPhone) or a noun phrase (e.g., Samsung Galaxy S6). There are two types of complementary entities: a named entity or a general entity. A named entity is usually a specific product name containing a brand name and a model name (e.g., Samsung Galaxy S6 or Apple iPhone). A general entity (e.g., phone or tablet) represents a set of named entities. General entities are informative. For example, in a review of a tablet stand, \u201cphone\u201d in \u201cIt also works with my phone\u201d is a good assurance for phone owners who want to use this tablet stand as a phone stand.\nDefinition 3 (Complementary Relation): Each complementary entity eC \u2208 EC forms a complementary relation (eT , eC) with the target entity eT .\nDefinition 4 (Complementary Entity Recognition): Given a set of reviews RT for a target entity eT , the problem of Complementary Entity Recognition (CER) is to identify a set of complementary entities EC , where each eC \u2208 EC has a complementary relation (eT , eC) with the target entity eT .\nWe do not extract an entity without a complementary context (e.g., \u201cSamsung Galaxy S6\u201d in \u201cSamsung Galaxy S6 is great\u201d, even though Samsung Galaxy S6 may be a complementary entity).\nDefinition 5 (Domain): We assume that every target entity eT belongs to a pre-defined domain (or category) Dom(eT ) = d \u2208 D. A review corpora RDom(eT ) is all reviews under the same category as the target entity eT .\nDefinition 6 (Domain Knowledge): Each domain d has its own domain knowledge. We consider two types of domain knowledge: candidate complementary entity edC \u2208 EdC and domain-specific verb vd \u2208 V d. All target entities eT under the same domain share the same domain knowledge.\nB. Basic Ideas\nThe basic idea of the proposed method is to use dependency paths to identify complementary entities. Due to different linguistic patterns, these dependency paths may have different performance on extraction. Some dependency paths may have high precision but low recall and vice versa. To ensure the quality of extraction, high precision dependency paths are preferred. The idea of using domain knowledge is that high precision dependency paths can expand high quality (precision) domain knowledge on a large amount of unlabeled reviews, which in turn helps low precision but high recall dependency paths to improve their precisions. In the end, the domain knowledge serves as a filter to remove noises in low precision paths. This framework can potentially be generalized to any extraction task when a large amount of unlabeled data is accessible. We describe the proposed method in the following two parts: Basic Entity Recognition: We analyze the linguistic patterns and leverage multiple dependency paths to recognize complementary entities. The major goal of the basic entity recognition is to get high recall because each complementary entity can be infrequent and we care about each mention of a complementary entity. Due to similarity with other noisy patterns, these paths tend to have a low precision. Recognition via Domain Knowledge Expansion: We expand the domain knowledge on a large amount of unlabeled reviews using a set of high precision dependency paths to compensate for the low precision (noisy) dependency paths. First, we extract candidate complementary entities for each domain using only verbs fit and work. Then we use the extracted candidate complementary entities to induce domain-specific verbs (e.g., insert for micro SD card, or hold for tablet stand). Finally, we integrate these two types\nof domain knowledge into the dependency paths of basic entity recognition to improve the precision.\nC. Dependency Paths\nIn this subsection, we briefly review the concepts used by dependency paths. We further describe how to match a dependency path with a sentence.\nDefinition 7 (Dependency Relation): A dependency relation is a typed relation between two words in a sentence with the following format of attributes:\ntype(gov, govidx, govpos, dep, depidx, deppos),\nwhere type is the type of a dependency relation, gov is the governor word, govidx is the index (position) of the gov word in the sentence, govpos is the POS (Part-Of-Speech) tag of the gov word, dep is the dependent word, depidx is the index of the dep word in the sentence and deppos is the POS tag of the dep word. The direction of a dependency relation is from the gov word to the dep word.\nA sentence can be parsed into a set of dependency relations through dependency parsing1 [10], [20]. For example, \u201cIt works with my phone\u201d can be parsed into a set of dependency relations in Table I, which is further illustrated in Figure 2.\nDefinition 8 (Dependency Segment): A dependency segment is an abstract form of a dependency relation. A dependency segment has the following format of attributes, which is similar to a dependency relation:\n(src, srcpos) pathtype\u2212\u2212\u2212\u2212\u2192 (dst, dstpos),\nwhere src is the source word, srcpos is the POS tag of the source word, dst is the destination word, dstpos is the POS tag of the destination word and pathtype is the dependency type of the segment. Similarly, the direction of an segment is from the src word to the dst word.\nDefinition 9 (Dependency Segment Matching): A dependency segment can have a dependency segment matching with a dependency relation. To have such a match, we must ensure that attributes src, srcpos, dst, dstpos and pathtype in an segment match attributes gov, govpos, dep, deppos and type in a dependency relation respectively. So the direction\n1We utilize Stanford CoreNLP as the tool for dependency parsing.\nof a dependency segment also matches the direction of a dependency relation.\nTo allow a matching to cover more specific dependency relations, we further define a set of rules when matching the attributes, which are summarized in Table II. Please note that we finally want to extract the complementary entity covered by tag CETT. Other kinds of attributes are defined to make the dependency paths more compact.\nExample 1: The segment:\n(\u201cwork\u201d, V) nmod:cmprel\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 (CETT, N) (1)\ncan match the dependency relation 5 in Table I. This is because source word \u201cwork\u201d is the lemmatized governor word \u201cworks\u201d; V covers VBZ; N covers NN; and nmod:cmprel covers dependency type nmod:with. Since the tag CETT as the destination word in the segment covers the dependent word \u201cphone\u201d in dependency relation 5, this segment indicates \u201cphone\u201d is a possible complementary entity.\nDefinition 10 (Dependency Path): A dependency path is a finite sequence of dependency segments connected by a sequence of src/dst attributes.\nGiven different directions of 2 adjacent dependency segments, there are 4 possible types of a connection: \u2192\u2192, \u2192\u2190, \u2190\u2192 and \u2190\u2190.\nDefinition 11 (Dependency Path Matching): A procedure of dependency path matching is specified as the following: when matching a dependency path with a sentence, we first check whether there are at least one dependency relations for each segment. If so, we further check whether the two directions of dependency segments for each connection match the directions of two corresponding dependency relations and whether the connected governor/dependent words from two matched dependency relations have the same index (they are the same word in the original sentence).\nFinally, after we have a successful dependency path matching, we extract the gov/dep in dependency relations labeled as CETT by the dependency path.\nExample 2: The following path\n(*, V) nmod:with\u2212\u2212\u2212\u2212\u2212\u2192 (CETT, N) nmod:poss\u2212\u2212\u2212\u2212\u2212\u2192 (\u201cmy\u201d, PRP$) (2)\ncan match the sentence \u201cIt works with my phone\u201d since the two segments match dependency relation 5 and 4 respectively. Here wildcard * matches word \u201cworks\u201d. Further the dependent word \u201cphone\u201d of the dependency relation 5 have the same index (the 5th word described in Table I) as the governor word of the dependency relation 4."}, {"heading": "IV. BASIC ENTITY RECOGNITION", "text": "A. Syntactic Patterns of Complementary Relation\nThere are many ways to mention complementary relations in reviews. Complementary relations are usually expressed with or without a preposition. In the first case, the preposition is used to bring out the complementary entity and is usually associated with a verb, a noun, an adjective or a determiner; in the second case without a preposition, reviewers only use transitive verbs to bring out the complementary entities. The verbs used in both cases can either be general verbs such as \u201cfit\u201d or \u201cwork\u201d, or domain-specific verbs such as \u201cinsert\u201d for micro SD card or \u201chold\u201d for tablet stand. Complementary relations can also be expressed through nouns, adjectives or determiners. We discuss the syntactic patterns of complementary relations as the following: Verb+Prep: The majority of complementary relations are expressed through a verb followed by a preposition. For example, \u201cIt works with my phone\u201d falls into this pattern, where the verb \u201cworks\u201d and the preposition \u201cwith\u201d work together to relate the pronoun \u201cIt\u201d to \u201cphone\u201d. The target entity can appear in this pattern either as the subject or as the object of the verb. In the previous example, subject \u201cIt\u201d indicates the target entity. In \u201cI insert the card into my phone\u201d, \u201cthe card\u201d is the object of the verb \u201cinsert\u201d. The target entity can also be implicitly assumed as in \u201cWorks with my phone.\u201d\nNoun+Prep: Complementary relation can be expressed through nouns. Those nouns typically have opinions. For example, \u201cNo problem\u201d in \u201cNo problem with my phone\u201d has a positive opinion on \u201cphone\u201d. Adjective+Prep: Complementary relation can also be expressed through adjectives with prepositions. For example, the adjective \u201cuseful\u201d together with the preposition \u201cfor\u201d in \u201cIt is useful for my phone\u201d expresses a positive opinion on a complementary relation. Determiner+Prep: Determiner \u201cthis\u201d in \u201cI use this for my phone\u201d refers to the target entity. It is associated with the preposition \u201cfor\u201d in dependency parsing. Verb: Complementary relation can be expressed only through verbs without using any preposition. For example, in \u201cIt fits my phone\u201d, subject \u201cIt\u201d is related to the object \u201cphone\u201d via only the transitive verb \u201cfits\u201d. This pattern has low precision on extraction since almost every sentence has a subject, a verb and an object. We improve the precision of this pattern using the domain knowledge in Section V.\nB. Dependency Paths for Extraction\nAccording to the discussed patterns, we implement dependency paths, which are summarized in Table III. For patterns with a preposition (e.g., Verb+Prep, Noun+Prep, Adjective+Prep, Determiner+Prep), we use dependency type nmod:cmprel to encode all prepositions, because cmprel represents with, for, in, on, to, inside and into as described in Section III. Then type nmod:cmprel can relate verbs, nouns, adjectives or determiners to the complementary entities. As shown in Example 1 and 2, nmod:cmprel can match nmod:with and relates the verb \u201cworks\u201d to the complementary entity \u201cphone\u201d for dependency relation 5 in Table I. This path is defined as Path 1 in Table III.\nFor pattern Verb, we use dependency type dobj to relate a verb to the complementary entity. Since this pattern tends to have low precision, we further constrain the pattern by connecting a nsubj relation or a nmod:poss relation, as described in Path 5 or Path 6 respectively in Table III. For example, \u201cIt fits iPhone\u201d has the following two dependency relations: nsubj(\u201cfits\u201d, VBZ, 2, \u201cIt\u201d, PRP, 1) and dobj(\u201cfits\u201d,\nVBZ, 2, \u201ciPhone\u201d, NNP, 3). Path 6 can match these two dependency relations separately and then check the two \u201cfits\u201ds have the same index 2 in these two dependency relations. So \u201ciPhone\u201d tagged as CETT can be extracted.\nFinally, these paths may appear multiple times in a sentence. So multiple complementary entities in a sentence can be extracted. For example, \u201cIt works with my phone, laptop and tablet\u201d has 3 complementary entities. It has the following 3 dependency relations: nmod:with(\u201cworks\u201d, VBZ, 2, \u201cphone\u201d, NN, 5), nmod:with(\u201cworks\u201d, VBZ, 2, \u201claptop\u201d, NN, 7) and nmod:with(\u201cworks\u201d, VBZ, 2, \u201ctablet\u201d, NN, 9). So Path 1 can have 3 matches to extract \u201cphone\u201d, \u201claptop\u201d and \u201ctablet\u201d.\nPlease note that Table III does not list all possible dependency paths. For example, complementary entities can also serve as the subject of a sentence: \u201cMy phone likes this card\u201d. We simply demonstrate typical dependency paths and new dependency paths can be easily added into the system to improve the recall.\nC. Post-processing\nSince a dependency relation can only handle the relation between two individual words, a complementary entity (labeled by CETT) extracted from Subsection B can only contain a single word. In reality, many complementary entities are named entities that represent product names such as \u201cSamsung/NNP Galaxy/NNP S6/NNP\u201d. Dependency relations usually pick a single noun (e.g., \u201cS6\u201d) and relate it with other words in the phrase via other dependency relations (e.g., type compound). We use the regular expression pattern \u3008N\u3009\u3008N|CD\u3009* to chunk a single noun into a noun phrase2. This pattern means one noun (N) followed by 0 to many nouns or numbers. Nouns and numbers (model number) are typical POS tags of words in a product name."}, {"heading": "V. RECOGNITION VIA DOMAIN KNOWLEDGE EXPANSION", "text": "Using the paths defined in Section IV tends to have low precision (noisy) of extractions since syntactic patterns may not distinguish a complementary relation from other\n2We implement the noun phrase chunker via NLTK: http://www.nltk.org/\nrelations. For example, Path 6 can match any sentence with type dobj. A sentence like \u201cIt has fast speed\u201d uses type dobj to bring out \u201cspeed\u201d, which is a feature of the target entity itself. To improve the precision, we incorporate categorylevel domain knowledge (candidate complementary entities and domain-specific verbs) into the extraction process. Those knowledge can help to constrain possible choices of CETT and verb in dependency paths defined in Section IV.\nWe mine domain knowledge from a large amount of unlabeled reviews under the same category. We get those two types of domain knowledge by bootstrapping them only from general verb fit and work. We randomly select 6000 reviews for each domain (category) to accumulate enough knowledge (knowledge from reviews of a single target entity may not be sufficient). One important observation is that products under the same domain share similar complementary entities and use similar domain-specific verbs. For example, all micro SD cards have camera, camcorder, phone, tablet, etc. as their complementary entities and use verbs like insert to express complementary relations. But these complementary entities and domain-specific verbs do not make sense for category tablet stand. To ensure the quality of the domain knowledge, we utilize several high precision dependency paths. These paths have low recall, so applying them directly to the testing reviews of the target entity has poor performance. High precision paths can leverage big data to improve the precision of other paths in Section IV.\nA. Exploiting Candidate Complementary Entities\nKnowing category-level candidate complementary entities is important for extracting complementary entities for a target entity under that category. For example, the sentences \u201cIt works in iPhone\u201d, \u201cIt works in practice\u201d and \u201cIt works in 4G\u201d have similar dependency relations nmod:in(\u201cworks\u201d, VBZ, 2, \u201ciPhone\u201d/ \u201cpractice\u201d/ \u201c4G\u201d, NN, 4). But only the first sentence has a mention of a complementary entity; the second sentence has a common phrase \u201cin practice\u201d with a preposition \u201cin\u201d; the third sentence expresses an aspect of the target entity. The key idea is that if we know that iPhone is a potential complementary entity under the category of micro SD card and \u201cpractice\u201d and \u201c4G\u201d are not, we are confident\nto extract \u201ciPhone\u201d as a complementary entity. We use Path 7 to extract candidate complementary entities as described in Table IV. It has high precision because given a verb like \u201cfit\u201d or \u201cwork\u201d, a preposition that relates to another entity and the possessive pronoun \u201cmy\u201d, we are confident that the entity modified by \u201cmy\u201d is a complementary entity. Lastly, all extracted complementary entities are stored as domain knowledge for each category.\nB. Exploiting Domain-Specific Verbs\nSimilarly, knowing category level domain-specific verbs is also important. This is because each category of products may have its own domain verbs to describe a complementary relation. If we only use general verbs (e.g., fit and work), we may miss many complementary entities that are bring out via domain-specific verbs (e.g., insert for micro SD card or hold for tablet stand), and this leads to poor recall rate. In contrast, if we consider all verbs into the paths without distinguishing them as in Section IV, we may bring in lots of noisy false positives. For example, if the target entity is a tablet stand, \u201cIt holds my tablet\u201d and \u201cIt prevents my finger going numb\u201d have similar dependency relations ( dobj(\u201cholds\u201d/\u201cprevents\u201d, VB, 2, \u201ctablet\u201d/\u201cfinger\u201d, NN, 4) ). The former one has a complementary entity since \u201cholds\u201d indicates a functionality that a tablet stand can have. The latter one does not have one. So if we know hold (we lemmatize the verbs) is a domain-specific verb under the category of tablet stand and \u201cprevents\u201d is not, we are more confident to get rid of the latter one. Therefore, we design dependency paths to extract high quality domainspecific verbs. This time, candidate complementary entities can help to identify whether a verb has a semantic meaning of complement. So we leverage the domain knowledge extracted in Subsection A to extract domain-specific verbs. In the end, we get domain-specific verbs from general seed verbs fit and work.\nPath 8 and 9 in Table IV are used to get verbs in pattern Verb+Prep and Verb respectively. These paths also have high precision because given possessive modifier \u201cmy\u201d modifying a complementary entity or determiner \u201cthis\u201d indicating a target entity it is almost certain that the verb between them indicates a complementary relation. Then we keep the words tagged by verb more than once (to reduce the noise) and store them as domain knowledge. Please note that we do\nnot further expand domain knowledge to avoid reducing the quality of domain knowledge.\nC. Entity Extraction using Domain Knowledge\nWe use the same dependency paths in Section IV to perform extraction. But this time we utilize the knowledge of candidate complementary entities and domain-specific verbs under the same category as the target entity. During matching, we look up candidate complementary entities and domain-specific verbs for tags CETT and verb respectively. But there is an exception for CETT. Since a named entity as a complementary entity may rarely appear again in a large amount of reviews, we ignore such a check if the word covered by CETT can be expanded into a noun phrase (more than 1 word) during post-processing. Furthermore, we notice that knowledge about target entities is also useful. For example, \u201cI insert this card into my phone\u201d uses \u201cthis\u201d to bring out the target entities, which may indicate nearby entities are complementary entities. However, knowledge about a target entity may be expanded on reviews of that target entity (test data) rather than reviews under the same category because target entities are not the same under the same category."}, {"heading": "VI. EXPERIMENTAL RESULTS", "text": "A. Dataset\nWe select reviews of 7 products that have frequent mentions of complementary relations from the Amazon review datasets [5]. We choose accessories because compatibility issues are more frequently discussed in accessory reviews. The products are stylus, micro SD card, mouse, tablet stand,\nkeypad, notebook sleeve and compact flash. We select nearly 220 reviews for the first 4 products and 110 reviews for the last 3 products. We select 50% reviews of the first 4 products as the training data for Conditional Random Field (CRF) (one supervised baseline). The remaining reviews of the first 4 products and all reviews of the last 3 products are test data. We split the training/testing data for 5 times and average the results. We label complementary entities in each sentence. The whole datasets are labeled by 3 annotators independently. The initial agreement is 82%. Then disagreements are discussed and final agreements are reached. The statistics of the datasets3 can be found in Table V. We observe that more than half of the reviews have at least one mention of complementary entities and more than 10% sentences have at least one mention of complementary entities.\nWe also utilize the category information in the meta data of each review to group reviews under the same category together. Then we randomly select 1000 (1K), 3000 (3K), 6000 (6K) reviews from each category and use them for\n3The annotated dataset is available on the first author\u2019s website https://www.cs.uic.edu/\u223chxu/\nextracting domain knowledge. We choose different scales of reviews to see the performance of CER under the help of different sizes of domain reviews and the scalability of the running time of domain knowledge expansion.\nB. Compared Methods and Evaluation\nSince the proposed problem is novel, there are not so much existing baselines that can directly solve the problem. Except for CRF, we compare existing trained models or unsupervised methods with the proposed methods. NP Chunker: Since most product names are Noun Phrases (NP), we use the same noun phrase chunker (\u3008N\u3009\u3008N|CD\u3009*) as the proposed method to extract nouns or noun phrases and take them as names of complementary entity. This baseline is used to illustrate a close to random results. OpenNLP NP Chunker: We utilize the trained noun phrase chunking model from OpenNLP4 to tag noun phrases. We only consider chunks of words tagged as NP as predictions of complementary entities. UIUC NER: We use UIUC Named Entity Tagger [21] to perform Named Entity Recognition (NER) on product\n4https://opennlp.apache.org/\nreviews. It has 18 labels in total and we consider entities labeled as PRODUCT and ORG as complementary entities. We use this baseline to demonstrate the performance of a named entity tagger. CRF: We retrain a Conditional Random Field (CRF) model using 50% reviews of the first 4 products. We use BIO tags. For example, \u201cWorks with my Apple iPhone\u201d should be trained/predicted as \u201cWorks/O with/O my/O Apple/B iPhone/I\u201d. We use MALLET5 as the implementation of CRF. Sceptre: We also retrieve the top 25 complements for the same 7 products from Sceptre [5] and adapt their results for a comparison. Direct comparison is impossible since their task is a link prediction problem with different labeled ground truths. We label and compute the precision of the top 25 predictions and assume annotators have the same background knowledge for both datasets. We observe that the predicted products are mostly non-complementary products (e.g., network cables, mother board) and all 7 products have similar predictions. \u201cMy\u201d Entity: This baseline extracts complementary entities by finding all nouns/noun phrases modified by word \u201cmy\u201d via dependency type nmod:poss (e.g., \u201cIt works with my phone\u201d). The word \u201cmy\u201d usually indicates a product already purchased, so the modified nouns/noun phrases are highly possible complementary entities. We use path\n(CETT, N) nmod:poss\u2212\u2212\u2212\u2212\u2212\u2192 (\u201cmy\u201d, PRP$)\nto extract complementary entities and use the same postprocess step as CER/CER1K/3K/6K+. CER: This method uses all paths described in Section IV without using any domain knowledge. CER1K+, CER3K+, CER6K+: These methods incorporate domain knowledge extracted from 1000/3000/6000 domain reviews respectively, as described in both Section IV and V.\nWe perform our evaluation on each mention of complementary entities and compute precision and recall of extraction. We first count the true positive tp, the false positive fp and the false negative fn of each prediction. For each sentence, one extracted complementary entity that is contained in the annotated complementary entities from the sentence is considered as one count for tp; one extracted complementary entity that are not found contributes one count to fp; any annotated complementary entity that can not be extracted contributes one count to fn. We run the system on an i5 laptop with 4GB memory. The system is implemented using Python. All reviews are preprocessed via dependency parsing [20].\nC. Result Analysis\nTable VI demonstrates results of different methods. We can see that CER6K+ performs well on all products. It significantly outperforms CER for each product. This shows\n5http://mallet.cs.umass.edu/\nthat domain knowledge can successfully reduce the noise and improve the precision. More importantly, we notice that using just 3K reviews already gets good performance. This is important for categories with less than 6K reviews. We notice that the F1-scores of CER are close or worse than baselines such as CRF or \u201cMy\u201d Entity. The major reason of its low precisions is that Path 5 and Path 6 in Table III can introduce many false positives as we expected. Please note that removing Path 5 and 6 can increase the F1-score of CER. But to have a fair comparison with CER1K/3K/6K+ and demonstrate the room of improvement, we keep noisy Path 5 and 6 in CER. \u201cMy\u201d Entity has better precision but lower recall than those of CER baselines since not all complementary entities are modified by \u201cmy\u201d. CRF performs relatively good on these products. But the performance drops for the last 3 products because of the domain adaptation problem. In reality, it is impractical to have training data for each product. Sceptre performs poorly, we guess the reason is that products in \u201cItems also bought\u201d are noisy for training labels. The overall recall of UIUC NER is low because many complementary entities (e.g., general entities like tablet) are not named entities. Please note that the information of domain knowledge (or unlabeled data) may help other baselines, but all those baselines may not able to adopt domain knowledge easily. The running time of all testing is short (less than 1 seconds), so we omit the discussion here.\nNext, we demonstrate the running time of domain knowledge expansion and samples of domain knowledge in Table VII. We observe that expanding knowledge is pretty fast and scalable as the size of reviews grow. We can see that for each category most entities and verbs are reasonable based on our common sense. For example, for category Cat:Stylus, the system successfully detects capacitive screen devices as its candidate complementary entities and most drawing actions as domain-specific verbs.\nD. Case Studies\nWe notice that category-level domain knowledge is useful for extraction. Knowing candidate complementary entities can successfully remove many words that are not complementary entities or even entities. In the reviews of micro SD card, many features such as speed, data, etc. are mentioned; also, common phrases like \u201cin practice\u201d, \u201cin reality\u201d, \u201cin the long run\u201d are also mentioned. Handling these cases one-byone is impractical since identifying different types of false positive examples needs different techniques to identify. But knowing candidate complementary entities can easily remove those false positives.\nDomain-specific verbs such as draw, insert and hold are successfully mined for stylus, micro SD card and tablet stand respectively. Taking tablet stand for example, the significant improvement of the precision of CER1K/3K/6K+ comes from taking hold as a domain-specific verb. Reviewers are\nless likely to use general verbs such as fit or work for tablet stand. The reason could be that a tablet is loosely attached to a tablet stand. So people tend to use \u201cIt holds tablet well\u201d a lot. However, this sentence has a dobj relation that usually relates a verb to an object, which can appear in almost any sentence. Knowing hold is a domain-specific verb is important to improve the precision. The major errors come from parsing errors since reviews are informal texts."}, {"heading": "VII. CONCLUSION", "text": "In this paper, we propose the problem of CER. Then we propose an unsupervised method using dependency paths to solve this problem. It further incorporates domain knowledge mined from a large amount of unlabeled reviews to improve its precision. Applications of our work can be found in mining compatible/incompatible products, which is useful for customers, manufacturers and recommender systems. Future directions of our work are mining opinions on complementary relations."}, {"heading": "ACKNOWLEDGMENT", "text": "This work is supported in part by NSF through grants IIS1526499 and CNS-1626432. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X GPU used for this research."}], "references": [{"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["B. Pang", "L. Lee", "S. Vaithyanathan"], "venue": "Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10. Association for Computational Linguistics, 2002, pp. 79\u201386.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2004, pp. 168\u2013177.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Extracting product features and opinions from reviews", "author": ["A.-M. Popescu", "O. Etzioni"], "venue": "Natural language processing and text mining. Springer, 2007, pp. 9\u201328.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Sentiment Analysis: Mining Opinions, Sentiments, and Emotions", "author": ["B. Liu"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Inferring networks of substitutable and complementary products", "author": ["J. McAuley", "R. Pandey", "J. Leskovec"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2015, pp. 785\u2013794.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey of named entity recognition and classification", "author": ["D. Nadeau", "S. Sekine"], "venue": "Lingvisticae Investigationes, vol. 30, no. 1, pp. 3\u201326, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Substitutes or complements: another step forward in recommendations", "author": ["J. Zheng", "X. Wu", "J. Niu", "A. Bolivar"], "venue": "Proceedings of the 10th ACM conference on Electronic commerce. ACM, 2009, pp. 139\u2013146.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Opinion word expansion and target extraction through double propagation", "author": ["G. Qiu", "B. Liu", "J. Bu", "C. Chen"], "venue": "Computational linguistics, vol. 37, no. 1, pp. 9\u201327, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Mining comparative sentences and relations", "author": ["N. Jindal", "B. Liu"], "venue": "AAAI, vol. 22, 2006, pp. 1331\u20131336.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Dependency parsing", "author": ["S. K\u00fcbler", "R. McDonald", "J. Nivre"], "venue": "Synthesis Lectures on Human Language Technologies, vol. 1, no. 1, pp. 1\u2013127, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Automated rule selection for aspect extraction in opinion mining", "author": ["Q. Liu", "Z. Gao", "B. Liu", "Y. Zhang"], "venue": "International Joint Conference on Artificial Intelligence (IJCAI), 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Lifelong-rl: Lifelong relaxation labeling for separating entities and aspects in opinion targets", "author": ["L. Shu", "B. Liu", "H. Xu", "A. Kim"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "A review of relation extraction", "author": ["N. Bach", "S. Badaskar"], "venue": "Literature review for Language and Statistics II, 2007.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "An introduction to hidden markov models", "author": ["L.R. Rabiner", "B.-H. Juang"], "venue": "ASSP Magazine, IEEE, vol. 3, no. 1, pp. 4\u201316, 1986.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1986}, {"title": "Maximum entropy markov models for information extraction and segmentation.", "author": ["A. McCallum", "D. Freitag", "F.C. Pereira"], "venue": "in ICML, vol", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2000}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F.C. Pereira"], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning (ICML 2001), Williams College, Williamstown, MA, USA, June 28 - July 1, 2001, 2001, pp. 282\u2013289.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Dependency tree kernels for relation extraction", "author": ["A. Culotta", "J. Sorensen"], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, 2004, p. 423.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["M. Mintz", "S. Bills", "R. Snow", "D. Jurafsky"], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2- Volume 2. Association for Computational Linguistics, 2009, pp. 1003\u20131011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "A shortest path dependency kernel for relation extraction", "author": ["R.C. Bunescu", "R.J. Mooney"], "venue": "Proceedings of the conference on human language technology and empirical methods in natural language processing. Association for Computational Linguistics, 2005, pp. 724\u2013731.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Stanford typed dependencies manual", "author": ["M.-C. De Marneffe", "C.D. Manning"], "venue": "Technical report, Stanford University, Tech. Rep., 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["L. Ratinov", "D. Roth"], "venue": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics, 2009, pp. 147\u2013155.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "com) contain a huge amount of products reviews and most existing works of sentiment analysis [1] (or opinion mining) on reviews focus on extracting opinion targets (aspects or features) of the", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "reviewed product and the associated opinions [2]\u2013[4] (e.", "startOffset": 45, "endOffset": 48}, {"referenceID": 3, "context": "reviewed product and the associated opinions [2]\u2013[4] (e.", "startOffset": 49, "endOffset": 52}, {"referenceID": 4, "context": "[5] as a link prediction problem in social network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "\u2022 Similar to the aspect (feature) extraction problem in reviews [4], CER is also a domain-specific problem.", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "\u2022 Although the problem may be closely related to the well-known Named Entity Recognition (NER) problem on surface [6], recognizing a complementary entity requires more contexts.", "startOffset": 114, "endOffset": 117}, {"referenceID": 4, "context": "The proposed problem is closely related to product recommender systems that are able to separate substitues and complements [5], [7].", "startOffset": 124, "endOffset": 127}, {"referenceID": 6, "context": "The proposed problem is closely related to product recommender systems that are able to separate substitues and complements [5], [7].", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "[7] first propose to incorporate the concepts of substitutes and complements into recommendation systems by analyzing navigation logs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "The proposed problem is closely related to aspect extraction [2]\u2013[4], [8], which is to extract product features from reviews.", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "The proposed problem is closely related to aspect extraction [2]\u2013[4], [8], which is to extract product features from reviews.", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "The proposed problem is closely related to aspect extraction [2]\u2013[4], [8], which is to extract product features from reviews.", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "e, one type of substitutes, or products that can replace each other) from reviews is studied by Jindal and Liu [9].", "startOffset": 111, "endOffset": 114}, {"referenceID": 9, "context": "Recently, dependency paths [10] are used for aspect extraction [8], [11].", "startOffset": 27, "endOffset": 31}, {"referenceID": 7, "context": "Recently, dependency paths [10] are used for aspect extraction [8], [11].", "startOffset": 63, "endOffset": 66}, {"referenceID": 10, "context": "Recently, dependency paths [10] are used for aspect extraction [8], [11].", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "[12] use unsupervised graph labeling method to identify entities from opinion targets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "CER is closely related to Named Entity Recognition (NER) [6] and relation extraction [13].", "startOffset": 57, "endOffset": 60}, {"referenceID": 12, "context": "CER is closely related to Named Entity Recognition (NER) [6] and relation extraction [13].", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "NER methods utilize annotated data to train a sequential tagger [14]\u2013[16].", "startOffset": 64, "endOffset": 68}, {"referenceID": 15, "context": "NER methods utilize annotated data to train a sequential tagger [14]\u2013[16].", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "CER is also different from relation extraction [13], [17]\u2013[19], which assumes that two entities are identified in advance.", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "CER is also different from relation extraction [13], [17]\u2013[19], which assumes that two entities are identified in advance.", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "CER is also different from relation extraction [13], [17]\u2013[19], which assumes that two entities are identified in advance.", "startOffset": 58, "endOffset": 62}, {"referenceID": 9, "context": "A sentence can be parsed into a set of dependency relations through dependency parsing1 [10], [20].", "startOffset": 88, "endOffset": 92}, {"referenceID": 19, "context": "A sentence can be parsed into a set of dependency relations through dependency parsing1 [10], [20].", "startOffset": 94, "endOffset": 98}, {"referenceID": 4, "context": "We select reviews of 7 products that have frequent mentions of complementary relations from the Amazon review datasets [5].", "startOffset": 119, "endOffset": 122}, {"referenceID": 20, "context": "UIUC NER: We use UIUC Named Entity Tagger [21]", "startOffset": 42, "endOffset": 46}, {"referenceID": 4, "context": "Sceptre: We also retrieve the top 25 complements for the same 7 products from Sceptre [5] and adapt their results for a comparison.", "startOffset": 86, "endOffset": 89}, {"referenceID": 19, "context": "dependency parsing [20].", "startOffset": 19, "endOffset": 23}], "year": 2016, "abstractText": "Product reviews contain a lot of useful information about product features and customer opinions. One important product feature is the complementary entity (products) that may potentially work together with the reviewed product. Knowing complementary entities of the reviewed product is very important because customers want to buy compatible products and avoid incompatible ones. In this paper, we address the problem of Complementary Entity Recognition (CER). Since no existing method can solve this problem, we first propose a novel unsupervised method to utilize syntactic dependency paths to recognize complementary entities. Then we expand category-level domain knowledge about complementary entities using only a few general seed verbs on a large amount of unlabeled reviews. The domain knowledge helps the unsupervised method to adapt to different products and greatly improves the precision of the CER task. The advantage of the proposed method is that it does not require any labeled data for training. We conducted experiments on 7 popular products with about 1200 reviews in total to demonstrate that the proposed approach is effective. Keywords-Entity Recognition; Relation Extraction; Product Relation; Complementary Entity; Complementary Product", "creator": "LaTeX with hyperref package"}}}