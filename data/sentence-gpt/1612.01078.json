{"id": "1612.01078", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2016", "title": "Enhancing Use Case Points Estimation Method Using Soft Computing Techniques", "abstract": "Software estimation is a crucial task in software engineering. Software estimation encompasses cost, effort, schedule, and size. The importance of software estimation becomes critical in the early stages of the software life cycle when the details of software have not been revealed yet.\n\n\n\n\n\nThe following are some of the most important estimates that are required in software analysis. First, you must consider whether the information in the software analysis is correct. The most important questions that you have to answer to determine whether the information is correct are:\n\nWhat are the current levels of computing power and the speed of software analysis?\nHow much will computing power or speed of software analysis be required to perform the analysis?\nWhat are the number of programs that can be performed by computer analysis?\nHow many programs that can be performed by computer analysis?\nWhat is the current operating system software program type?\nWhere can computers be used to perform computing?\nWhat types of programs can be performed by computer analysis?\nWhat types of programs can be performed by computer analysis?\nWhat types of programs can be performed by computer analysis?\nWhat types of programs can be performed by computer analysis?\nWhat types of programs can be performed by computer analysis?\nWhat types of programs can be performed by computer analysis?\nWhat types of programs can be performed by computer analysis?\nWhat types of programs can be performed by computer analysis?\nHow many programs can be performed by computer analysis?\nWhat types of programs can be performed by computer analysis?\nWhat types of programs can be performed by computer analysis?\nWho does the software analysis come up with?\nWhat types of programs can be performed by computer analysis?\nHow many programs can be performed by computer analysis?\nHow many programs can be performed by computer analysis?\nWhat types of programs can be performed by computer analysis?\nHow many programs can be performed by computer analysis?\nWhat types of programs can be performed by computer analysis?\nHow many programs can be performed by computer analysis?\nWhat types of programs can be performed by computer analysis?\nHow many programs can be performed by computer analysis?\nWhat types of programs can be performed by computer analysis?\nHow many programs can be performed by computer analysis?\nHow many programs can be performed by computer analysis?\nHow many programs can be performed by computer analysis?\nHow many programs can be performed by computer analysis?\nWhat types of programs can be performed by computer analysis?\nHow many programs can be performed by computer analysis?\nHow", "histories": [["v1", "Sun, 4 Dec 2016 06:59:14 GMT  (799kb)", "http://arxiv.org/abs/1612.01078v1", null]], "reviews": [], "SUBJECTS": "cs.SE cs.AI cs.LG", "authors": ["ali bou nassif", "luiz fernando capretz", "danny ho"], "accepted": false, "id": "1612.01078"}, "pdf": {"name": "1612.01078.pdf", "metadata": {"source": "CRF", "title": "Enhancing Use Case Points Estimation Method Using Soft Computing Techniques", "authors": ["Ali Bou Nassif", "Luiz Fernando Capretz", "Danny Ho"], "emails": ["abounass@uwo.ca", "lcapretz@uwo.ca", "danny@nfa-estimation.com"], "sections": [{"heading": null, "text": "software estimation becomes critical in the early stages of the software life cycle when the details of software have not been revealed yet. Several commercial and non-commercial tools exist to estimate software in the early stages. Most software effort estimation methods require software size as one of the important metric inputs and consequently, software size estimation in the early stages becomes essential. One of the approaches that has been used for about two decades in the early size and effort estimation is called use case points. Use case points method relies on the use case diagram to estimate the size and effort of software projects. Although the use case points method has been widely used, it has some limitations that might adversely affect the accuracy of estimation. This paper presents some techniques using fuzzy logic and neural networks to improve the accuracy of the use case points method. Results showed that an improvement up to 22% can be obtained using the proposed approach.\nKeywords: Use Case Points, Early Software Size Estimation, Early Software Effort Estimation, Applied Soft Computing, Software Measurement"}, {"heading": "INTRODUCTION AND PROBLEM DEFINITION", "text": "As the role of software in the industry and the society becomes vital, it becomes crucial to develop high-quality and cost-effective software in a short period. To attain this goal, software development processes should be managed efficiently from the requirement phase to the implementation phase. One of the main tasks of project management is planning. Planning includes the cost and effort estimation of the project in the early stages of the software development life cycle. The earlier the estimation is, the better the project management will be. Even though early estimation is necessary, the accuracy of this estimation is very important. Software estimators are notorious with inaccurate estimation that leads to incomplete projects and consequently millions of dollars are wasted. The International Society of Parametric Analysis (ISPA) [1] and the Standish Group International [2] identified poor estimation as one of the main culprits behind software failure. Software cost and effort estimation mainly depend on the prediction of software size. This has led to the substantial increase in research in software engineering for estimating the size of software in the requirement stage. Function Points Analysis (FPA) is one of the earliest models that is used to predict the size of software in the early stages. The FPA model was proposed by Albrecht in 1979 [3] and it measures the size of software based on its functionalities. The main advantages of the FPA model are that it is independent of the technology and the programming language used in the implementation. On the other hand, the main issues with the FPA model are that function points cannot be computed automatically and the decisions made in counting function points are subjective [4]. Object-Oriented Modelling (OOM) has become dominant since the release of Unified Modeling Language (UML) version 1.1 in 1997 [5], but OOM has become more popular since the release of UML 2.0 in 2005 [6]. UML models include use case, sequence, component, activity and class diagrams. Recently, many software organizations use UML notation to convey the requirements and the design of their software projects. For instance, use case, sequence and component diagrams might be used to represent the requirements of the system while the class diagram might be used to represent the system design.\nOne of the size and effort estimation models that rely on the use case diagram is called Use Case Points (UCP). The UCP model was proposed by Gustav Karner in 1993 [7]. UCP is measured by counting the number of use cases and the number of actors, each multiplied by its complexity factors. Use cases and actors are classified into three categories. These include simple, average and complex. The determination of the use cases\u2019 complexity (simple, average or complex) is determined by the number of transactions per use case. For instance, a use case is classified as simple if number of transactions is between one and three, classified as average if the number of transactions is between four and seven, classified as complex if the number of transactions is greater than seven. The UCP presents some limitations that affect the accuracy of the estimation. The main drawback of this model is the absence of the graduation when classifying the complexity of the use cases. For example, if the number of the transactions in a use case is three, the use case is classified as simple, however, if the number of transactions is four, the use case is classified as average. According to the UCP, if project A contains ten use cases, each of three transactions and project B contains ten use cases, each of four transactions, then the size of project B will be double the size of project A. In practice, this approach is incorrect. Moreover, a use case of eight transactions has the same complexity factor as the use case of twenty transactions since this model does not distinguish between large, very large and super large use cases. This paper introduces a new approach to overcome the limitations of the UCP. First, rather than classifying a use case as simple, average, or complex, the use case will be classified as ux, such as x \u2208 [1,10] where x represents the number of transactions. This concludes that there will be ten degrees of complexity for use cases (u1, u2, u3, etc.). The proposed approach will be implemented in two independent stages. First, a fuzzy logic approach is applied to determine the complexity factor of ux. The second stage of the proposed approach is implemented through a neural network model. The neural network model is a black box that takes ux (10 vectors) as an input, in addition to three vectors which represent the three types of the actors (simple, average or complex). The output of the neural network will be the size of the software.\nThe rest of the paper is organized as follows: Section 2 presents the background and the related work for the proposed approach. Sections 3 and 4 propose the fuzzy logic and the neural network approaches respectively. Section 5 evaluates the proposed approaches. Section 6 presents general discussion about the paper. Section 7 highlights the threats to validity in this work. Finally, section 8 concludes the paper and proposes the future work.\nBACKGROUND AND RELATED WORK\nThis paper presents a new approach to improve the accuracy of the use case estimation model using fuzzy logic and neural network. This section presents the terms that are relevant to this work."}, {"heading": "Use Case Points", "text": "This method is based on mapping a use case diagram to a size metric called use-case points. When the size of software is known, the software development effort can be estimated. The use case model was first proposed by Jacobson et al. [8] A use case diagram shows how users interact with the system. A use case diagram is composed of use cases and actors. Use cases represent the functional requirements where an actor is a role played by a user. In the use case diagram, a use case can extend or include another use case. Figure 1 is an example of a use case diagram [9].\nThe use case points method mainly depends on four factors. These include the number and the complexity of the use cases, the number and the complexity of the actors, some non-functional requirements such as usability and portability, and some environmental factors where the software will be developed. The complexity of a use case is determined by the number of transactions of the use case scenario. A use case scenario is usually composed of several points. These include the actors involved in the scenario, the precondition of the system, the main success scenario, the extensions or exceptions and the post condition. The following example introduces the scenario of the use case \u201cStudent Enrolls in a Course\u201d in a University Online Registration System.\nUse Case Title: Student Enrolls in a Course Actors: Student, Admin Precondition: The student is not enrolled in a course"}, {"heading": "Main Success Scenario:", "text": "1. Check if the student has permission to register a course 2. Student chooses the course he or she wishes to enroll in\n3. System checks for the deadline of enrollment 4. System checks for the prerequisite of the course 5. System checks if the student has registered in another\ncourse which is scheduled at the same time\n6. System checks for the maximum number of courses the student can register 7. System checks if the course is full"}, {"heading": "Extensions", "text": "1a: The student does not have permission (e.g. the student has not paid the tuition)\n1a1: Notify the student to contact the administrator\n3a: The deadline has passed\n3a1: An Error message will be displayed 3a2: The student will be informed to contact the registrar\n4a: The prerequisite of the course is not fulfilled\n4a1: The student will be advised to contact the professor to obtain permission 4b1: If the student has permission from the professor, the student will be advised to contact the registrar to enroll him/her in the course\n5a: Two courses have the same schedule\n5a1: The student is advised to choose either one\n6a: The number of the enrolled courses has been exceeded\n6a1: An error message will be displayed\n7a: The course is full\n7a1: An error message will be displayed\nPost condition: The student has enrolled in a course\nWith respect to counting the transactions in the scenario, the transactions should be counted in the success scenario as well as in the extensions. For example, the number of transactions of the above scenario will be fifteen. This includes seven transactions in the success scenario and eight transactions in the extensions (1a1 + 3a1 + 3a2 + 4a1 + 4b1 + 5a1 + 6a1 + 7a1). For instance, counting the number of transactions can be subjective and one might count 3a1 and 3a2 as one transaction. We argue in section 6 that counting the transactions in the extensions the same way as counting the transactions in the success scenario might lead to overestimation. Thus, we believe that counting the transactions in the extension part should be performed in a different way.\nUnadjusted and Adjusted Use Case Points: To estimate the size of software using this method, several rules should be applied. These rules include [7]\n Identify the complexity of each use case: The complexity is said to be Simple if the number of\ntransactions within this use case is between one and three. The complexity is Average if the number of transactions is between four and seven. The complexity is Complex if the number of transactions is eight or more.\n Assign a weight factor for each level of complexity for use cases: This factor depends on the type of\nthe project. Usually, if the complexity level is Simple, the factor given is five. If the complexity level is Average, the factor given is ten. If the complexity level is Complex, the factor given is fifteen.\n Identify the complexity of each actor: An actor is defined as Simple if it is System Interface. An actor\nis defined as Average if it is Interactive or ProtocolDriven Interface. The actor is defined as Complex if it is a Graphical Interface.\n Assign a weight factor for each level of complexity for actors: This is similar to the weight factors\ngiven to use cases. The weight factor is one for Simple, two for Average and three for Complex.\n Calculate the total use case weight factor (UseCase_WeightFactor): This is the sum of all\nSimple use cases multiplied by their weighting factor + the sum of all Average use cases multiplied by their weighting factor + the sum of all Complex use cases multiplied by their weighting factor.\n Calculate the total actor weight factor (Actor_WeightFactor): Apply the same rule as\nabove to calculate the total actor weight factor.\n Calculate the Unadjusted Use Case Points (UUCP): UUCP = UseCase_WeightFactor +\nActor_WeightFactor. The Unadjusted Use Case Points can be expressed as:\n\ud835\udc48\ud835\udc48\ud835\udc36\ud835\udc43 = \u2211 \ud835\udc5b\ud835\udc56 \u2217 \ud835\udc4a\ud835\udc56\n6\n\ud835\udc56=1\n(1)\nwhere ni is the number of items of variety i and Wi is the complexity weight. At this point, the UUCP is calculated. Some cost estimation methods such as SEER-SEM takes the UUCP as an input of software size to calculate the cost and effort of software development. Karner [7] proposed an effort estimation method based on the Adjusted Use Case Points (UCP). The UCP is calculated by multiplying the UUCP by the technical and environmental factors. The technical factors contribute to the complexity of the system where the environmental factors contribute to the efficiency of the system. Depending on the technical and environmental factors, the UCP can be same as, smaller or larger than the UUCP. At most, the UCP can be larger or smaller than the UUCP by 30%. The technical and environmental factors can be classified in Table 1 and Table 2 respectively."}, {"heading": "Fi Factors Contributing to Complexity Wi", "text": ""}, {"heading": "Fi Factors contributing to efficiency Wi", "text": ""}, {"heading": "F1 Familiar with Objectory 1.5", "text": "The Adjustment Use Case points (UCP) can be expressed as:\nUCP = UUCP \u2217 TF \u2217 EF (2)\nwhere TF is the Technical Factor and the EF is the environmental factor. TF is calculated as:\n\ud835\udc47\ud835\udc39 = \ud835\udc361 + \ud835\udc362 \u2211 \ud835\udc39\ud835\udc56 \u2217 \ud835\udc4a\ud835\udc56\n13\n\ud835\udc56=1\n(3)\nwhere \ud835\udc361 = 0.6, \ud835\udc362 = 0.01 and \ud835\udc39\ud835\udc56 is a factor that takes values 0 or 1 or 2 or 3 or 4 or 5. The value 0 means irrelevant while the value 5 means essential. The value 3 means that the factor is not very essential, neither irrelevant. For instance, if all the factors have the value of 3, the TF will be 1. On the other hand, the environmental factor EF is calculated as:\n\ud835\udc38\ud835\udc39 = \ud835\udc361 + \ud835\udc362 \u2211 \ud835\udc39\ud835\udc56 \u2217 \ud835\udc4a\ud835\udc56\n8\n\ud835\udc56=1\n(4)\nwhere \ud835\udc361 = 1.4, \ud835\udc362 = \u22120.03 and \ud835\udc39\ud835\udc56 is a factor which is equivalent to the \ud835\udc39\ud835\udc56 of the technical factor (i.e between 0 and 5). If all the factors have the value of 3, then the EF will be 1. After the size of software is calculated in UCP, the effort to develop this software can be estimated. According to Karner, the effort required to complete one UCP is twenty person hours."}, {"heading": "Fuzzy Logic", "text": "Fuzzy logic is derived from the fuzzy set theory that was proposed by Lotfi Zadeh in 1965 [10]. As a contrary to the conventional binary (bivalent) logic that can only handle two values True or False (1 or 0), fuzzy logic can have a truth value which is ranged between 0 and 1. This means that in the binary logic, a member is completely belonged or not belonged to a certain set, however in the fuzzy logic, a member can partially belong to a certain set. Mathematically, a fuzzy set A is represented by a membership function as follows:\n\ud835\udc39\ud835\udc67[\ud835\udc65 \u2208 \ud835\udc34] = \ud835\udf07\ud835\udc34(\ud835\udc65): \u211d \u2192 [0, 1] (5)\nWhere \ud835\udf07\ud835\udc34 is the degree of the membership of element x in the fuzzy set A. A fuzzy set is represented by a membership function. Each element will have a grade of membership that represents the degree to which a specific element belongs to the set. Membership functions include Triangular, Trapezoidal and S-Shaped. In fuzzy logic, linguistic variables are used to express a rule or fact. For example, \u201cthe temperature is thirty degrees\u201d is expressed in fuzzy logic by \u201cthe temperature is low\u201d or \u201cthe temperature is high\u201d where the words low and high are linguistic variables. In fuzzy logic, the knowledge based is represented by if-then rules. For example, if the temperature is high, then turn on the fan. The fuzzy system is mainly composed of three parts. These include Fuzzification, Fuzzy Rule Application and Defuzzification. Fuzzification means applying fuzzy membership functions to inputs. Fuzzy Rule Application is to make inferences and associations among members in different groups. The third step in the fuzzy system is to defuzzify the inferences and associations and make a decision and provide an output that can be understood. In this paper, fuzzy logic will be used to calibrate the complexity weight of use cases.\nArtificial Neural Network\nArtificial Neural Network (ANN) is a network composed of artificial neurons or nodes which emulate the biological neurons [11]. ANN can be trained to be used to approximate a non-linear function, to map an input to an output or to classify outputs. There are several algorithms available to train a neural network but this depends on the type and topology of the neural network. The most prominent topology of ANN is the feed-forward networks. In a feedforward network, the information always flows in one direction (from input to output) and never goes backwards. An ANN is composed of nodes organized into layers and connected through weight elements. At each node, the weighted inputs are aggregated, thresholded and inputted to an activation function to generate an output of that node. Mathematically, this can be represented by:\n\ud835\udc66(\ud835\udc61) = \ud835\udc53 [ \u2211 \ud835\udc64\ud835\udc56\n\ud835\udc5b\n\ud835\udc56=1\n\ud835\udc65\ud835\udc56 \u2212 \ud835\udc640 ] (6)\nWhere \ud835\udc65\ud835\udc56 are neuron inputs, \ud835\udc64\ud835\udc56 are the weights and \ud835\udc53[. ] is the activation function. Feed-forward ANN layers are usually represented as input, hidden and output layers. If the hidden layer does not exist, then this type of the ANN is called perceptron. The perceptron is a linear classifier that maps an input to an output provided that the output falls under two categories. The perceptron can map an input to an output if the relationship between the input and output is linear. If the relationship between the input and output is not linear, multi-layer perceptron (MLP) can be used. A MLP contains at least one hidden layer. MLPs can be trained using the backpropagation algorithm. In this paper, a MLP is used and trained using the backpropagation algorithm."}, {"heading": "Evaluation Criteria", "text": "Several methods exist to compare cost estimation models. Each method has its advantages and disadvantages. In this work, three methods will be used. These include the Mean of the Magnitude of Relative Error (MMRE), the Mean of Magnitude of error Relative to the Estimate (MMER) and the Mean Error with Standard Deviation.\nMMRE: This is a very common criterion used to evaluate software cost estimation models [12]. The Magnitude of Relative Error (MRE) for each observation i can be obtained as:\n\ud835\udc40\ud835\udc45\ud835\udc38\ud835\udc56 = | \ud835\udc34\ud835\udc50\ud835\udc61\ud835\udc62\ud835\udc4e\ud835\udc59 \ud835\udc38\ud835\udc53\ud835\udc53\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc56 \u2212 \ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc61\ud835\udc52\ud835\udc51 \ud835\udc38\ud835\udc53\ud835\udc53\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc56 |\n\ud835\udc34\ud835\udc50\ud835\udc61\ud835\udc62\ud835\udc4e\ud835\udc59 \ud835\udc38\ud835\udc53\ud835\udc53\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc56 (7)\nMMRE can be achieved through the summation of MRE over N observations:\n\ud835\udc40\ud835\udc40\ud835\udc45\ud835\udc38 = 1\n\ud835\udc41 \u2211 \ud835\udc40\ud835\udc45\ud835\udc38\ud835\udc56\n\ud835\udc41\n1\n(8)\nMMER: MMER is another method for cost estimation models evaluation [13]. MER is similar to MRE with a difference that the denominator is the predicted effort instead of the actual effort. Consequently, the equations for MER and MMER are:\n\ud835\udc40\ud835\udc38\ud835\udc45\ud835\udc56 = | \ud835\udc34\ud835\udc50\ud835\udc61\ud835\udc62\ud835\udc4e\ud835\udc59 \ud835\udc38\ud835\udc53\ud835\udc53\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc56 \u2212 \ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc61\ud835\udc52\ud835\udc51 \ud835\udc38\ud835\udc53\ud835\udc53\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc56 |\n\ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc61\ud835\udc52\ud835\udc51 \ud835\udc38\ud835\udc53\ud835\udc53\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc56 (9)\n\ud835\udc40\ud835\udc40\ud835\udc38\ud835\udc45 = 1\n\ud835\udc41 \u2211 \ud835\udc40\ud835\udc38\ud835\udc45\ud835\udc56\n\ud835\udc41\n1\n(10)\nWhen using the MMRE and the MMER in evaluation, good results are implied by lower values of MMRE and MMER.\nMean Error with Standard Deviation: Although MMRE and MMER have been used for a long time, both methods might lack accuracy. If the actual effort was small, MMRE would be high. On the other hand, if the predicted effort was low, MMER would also be high. Foss et al. argued that MMRE should not be used when comparing cost estimation models and using the standard deviation would be better [14]. The standard deviation method was first proposed by Karl Pearson in 1894 [15]. The equation for the mean error for each observation i and total number of observations N is:\n\ud835\u0305\udc65 = 1\n\ud835\udc41 \u2211 \ud835\udc65\ud835\udc56\n\ud835\udc41 \ud835\udc56=1 (11)\nWhere \ud835\udc65\ud835\udc56 = (\ud835\udc34\ud835\udc50\ud835\udc61\ud835\udc62\ud835\udc4e\ud835\udc59 \ud835\udc38\ud835\udc53\ud835\udc53\ud835\udc5c\ud835\udc5f\ud835\udc61 \ud835\udc56 \u2212 \ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc61\ud835\udc52\ud835\udc51 \ud835\udc38\ud835\udc53\ud835\udc53\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc56 )\nThe equation of the standard deviation can be seen as:\n\ud835\udc46\ud835\udc37 = \u221a 1\n\ud835\udc41 \u2212 1 \u2211( \ud835\udc65\ud835\udc56 \u2212 \ud835\u0305\udc65 )\n2\n\ud835\udc41\n\ud835\udc56=1\n(12)\nThe mean error with standard deviation can be represented as:\n\ud835\u0305\udc65 \ud835\udc46\ud835\udc37 (13)\u2212 +\nRelated Work\nLittle work has been done to improve the use case points model, however soft computing techniques such as fuzzy logic and neuro-fuzzy have been widely implemented to improve cost estimation models such as COCOMO II, Function Points Analysis and SEER-SEM. This section presents the work relevant to applying soft computing techniques on cost estimation models. These include the following: Fetcke et al. [16] mapped UML use case diagrams to the software size metric Function Points. This method is based on four main steps. In the first step, Fetcke et al. define boundary concepts. This is similar to the boundary definition in FPA IFPUG. The authors suggest that actors are mapped into users and external applications, but the relationship is not always one-to-one. In the second step, the identification of items within the boundary is defined. In FPA, there are 2 types of items, transactional functions and files (data functions). Use cases are mapped in transactional functions. In order to count transactional functions, use cases must be described in further detail (use case scenarios). The concept of a file in Object Oriented is the object. The authors distinguish between typed objects and untyped objects. Each is treated in a specific way. Aggregation (PartOf) and Inheritance (IS-A) relationships are also taken into consideration. In the third step, the identification of item types is defined. Transactional functions are treated as external outputs, external inquiries and external inputs. Files are treated as internal logical files and external interface files. The counting rules for transactional functions and files are the same as reported in the IFPUG Counting Practices Manual [17]. Finally, weight factors are applied. In this step, transactions and files are weighed based on IFPUG Counting Practices Manual. Issa et al. [18], used the use case diagram of software to determine the effort of the software based on three steps. First, the effort estimation can be roughly calculated based on the number of use cases multiplied by 0.67 personmonths. Secondly, estimation can be done using the use case patterns catalogue estimation method. Finally, object points can be extracted using the use case model method. Mittal et al. [19], used fuzzy logic to tune the parameters of COCOMO cost estimation model. After that, a comparison between the proposed model and other models was conducted. Huang et al. [20], proposed a new model using neuro-fuzzy technique to improve the estimation of the COCOMO model. This model can be easily trained and evaluated by experts. A learning algorithm for this model was also put forward."}, {"heading": "PROPOSED MODEL USING FUZZY LOGIC", "text": ""}, {"heading": "APPROACH", "text": "As explained in section 1, the main problem of the use case points model is that there is no graduation when classifying the complexity factors of use cases. In this section of the work, fuzzy logic with triangular membership was used to\nsolve this issue. The input and output memberships are displayed in Figure 2 and Figure 3 respectively.\nFuzzy Logic Rules: If Input = 2 transactions then output = 5 If Input = 6 transactions then output = 10 If input = 10 transactions then output = 15\nRather than classifying the use cases into three classes (simple, average and complex) as in Karners\u2019s work, the use cases will be classified into ten categories according to the number of transactions per use case. Since the main goal of our approach is to enhance the current model proposed by Karner and not to completely modifying it, we assume that the largest use case contains ten transactions. We also assume that the complexity factor of the largest use case is fifteen. Table 3 presents a comparison between the original work (Karner\u2019s method) and the proposed fuzzy logic approach. The table shows that in the proposed approach, the weights of the use cases are gradually increasing as opposed to the abrupt increase in Karner\u2019s method."}, {"heading": "PROPOSED MODEL USING NEURAL NETWORK APPROACH", "text": "In this stage, a neural network approach is used to map the input vectors (use cases and actors) to an output vector (UUCP) as shown in Figure 4. Since the nature of the problem is non linear, Multi Layer Perceptron with one hidden layer was used to simulate the problem. There are thirteen input vectors. These include ten vectors that represent the use cases and three vectors that represent the actors.\nThe training algorithm used was Levenberg-Marquardt backpropagation (trainlm). Several experiments were conducted to determine the number of neurons in the hidden layer. As a rule of thumb, the number of neurons in the hidden layer must be greater than the number of neurons of the input layer. However, there are no standard rules to determine the number of neurons in the hidden layer other than trial and error [21]. Twelve experiments were performed. The number of neurons was set between fourteen and twenty five. The best results were obtained when the number of neurons in the hidden layer was twenty. Seven projects were used in training the neural network and thirteen projects were used for testing and validation. The next section demonstrates the results of applying the neural network approach."}, {"heading": "EVALUATION", "text": "The evaluation of this work was conducted on twenty different projects. There is no standard and known conversion between the size in UCP and the size in function points or SLOC. Since some information about the complexity of the projects and the team experience is known about each project, the Technical Factor (TF) and the Environmental Factor (EF) were calculated. Karner suggested that the effort required to develop one UCP is twenty person hours. This method had been criticized by many researchers. Schneider et al. [22] refined Karner\u2019s method in determining the effort from UCP. Schneider suggested counting the number of factor ratings of F1-F6 in Table 2 (Technical Factors) that are below three and the number of factor ratings of F7-F8 that are above three. If the total is three or less, then twenty person hours per UCP should be used. If the total is three or four, twenty eight person hours per UCP should be used. If the total is five or more, then the project team should be reconstructed so that the numbers fall at least below five. A value of five indicates that this project is at significant risk of failure with this team. In this paper, Schneider\u2019s method has been used to calculate the size of the projects in UCP from the effort. Equation 2 is used to determine the size of each project in UUCP. To distinguish between the results in the proposed fuzzy logic and neural network approaches, the evaluation of each approach was done separately. Furthermore, to determine the effect of the extension part of the use case scenario on size, two different experiments were conducted."}, {"heading": "Evaluation of the Fuzzy Logic Approach", "text": "Karner ignored the \u201cextend\u201d and \u201cinclude\u201d use cases when applying the UCP model, however we believe that the \u201cextend\u201d and \u201cinclude\u201d use cases of the use case model should be considered when estimating the size of software. The evaluation of the fuzzy logic approach was conducted in three different stages. First, the evaluation was done on seven projects. The use case models of these projects contain no or very few \u201cextend\u201d and \u201cinclude\u201d use cases. In the second stage, the evaluation was done on five projects. The use case models of these projects contain a fair number of \u201cextend\u201d and \u201cinclude\u201d use cases. In this stage, the number of \u201cextend\u201d and \u201cinclude\u201d use cases in each project is between 15% to 25% of the number of total use cases. Finally, in the third stage, eight projects were chosen for evaluation. In these projects, the number of the \u201cextend\u201d and \u201cinclude\u201d is more than 25% of the number of total use cases. In each stage, the error (MER, and MRE) of each project was calculated between the original size in UUCP and each of Karner\u2019s method and the proposed fuzzy logic approach. At the end of each stage, the error was presented as MMRE, MMER and mean error with standard deviation. Table 4 shows a comparison between the Karner\u2019s model and the proposed fuzzy logic approach.\nIn the first stage, there is 22% improvement in MMRE and 9% improvement in MMER by applying the proposed fuzzy logic approach. According to equation 13, the mean error with standard deviation of Karner\u2019s method can be expressed as 40.34 25.33\u2212 + . However, for the fuzzy logic approach, the mean error with standard deviation is 23.77 17\u2212 + . In the second stage, there is slim improvement in the proposed method. The MMRE is enhanced by 4% and the MMER is only enhanced by 2%. In the third stage, the new approach has a negative impact and Karner\u2019s estimation provided better results. Section 6 will address this change in the results."}, {"heading": "Evaluation of the Neural Network Approach", "text": "Seven random projects were selected to train the neural network presented in section 4. The neural model was tested and evaluated over thirteen projects. Good results were obtained in the training process. The mean error was 0.0215, and the standard deviation was 0.0616. Table 5 presents the results of the neural network approach.\nThe results show that an improvement of 20% in the MMER was obtained. Table 5 also shows that the neural network approach had adverse results in the MMRE and in the mean error with standard deviation. Section 6 will discuss the results of the neural network approach."}, {"heading": "Effect of the Extension Part in the Use Case Scenario on Size Estimation", "text": ""}, {"heading": "According to Karner\u2019s model, the transactions in the", "text": "extensions are counted the same way as in the main scenario. Two experiments were performed on two projects (project 3 and project 4) to learn the effect of the extension part on size estimation. There were two reasons for choosing these projects. First, the number of \u201cextend\u201d and \u201cinclude\u201d use cases in these two projects is about 5% of the number of total use cases in the use case diagram. This is important to put the problem of counting the \u201cextend\u201d and \u201cinclude\u201d use cases aside while working with extensions. Secondly, we are very familiar with these projects. Surprisingly, the MMRE and the MMER of both Karner and the fuzzy logic approach had improved when the extension part of the scenario was ignored. This concluded that in the first stage of projects (project 1 to project 7) where the number of \u201cextend\u201d and \u201cinclude\u201d use cases is very low, one of the reasons behind the overestimation in both Karner\u2019s and the fuzzy logic approach was counting the transactions in the extension part the same way as in the success scenario. For instance, in the projects where the number of transactions in the extensions is approximate to the number of transactions in the success scenario (like the scenario proposed in section 2.1), counting the transactions of the extensions in the same way as in the success scenario might lead to overestimation in the software effort by 30% to 50%."}, {"heading": "DISCUSSION", "text": "Upon conducting experiments in this paper, some important points are noted. These include:\n In about 80% of the projects, the average size of the projects using the fuzzy logic approach was less\nthan the average size of the projects using Karner\u2019s approach. This is because the fuzzy logic approach provided a gradual and smooth increase of the complexity weights of the use cases as opposed to the abrupt change in Karner\u2019s model.\n Karner did not consider the \u201cinclude\u201d and the \u201cextend\u201d use cases when counting the transactions\nin each use case, however the number of \u201cextend\u201d and \u201cinclude\u201d use cases has an impact on estimation and should be considered. However, more research is required to compare the effort needed to develop the \u201cextend\u201d and \u201cinclude\u201d use cases with the effort needed to develop the main\nuse cases. In a nut shell, counting the \u201cextend\u201d and \u201cinclude\u201d use cases might differ from counting the main use cases. Furthermore, the experiments show that Karner\u2019s model leads to overestimation when there are no \u201cextend\u201d or \u201cinclude\u201d use cases. On the other hand, Karner\u2019s method gives better results when there is a fair number of \u201cextend\u201d and \u201cinclude\u201d use cases. It might be concluded that Karner made a rough estimation when he assigned the complexity weights by indirectly including the \u201cextend\u201d and \u201cinclude\u201d use cases.\n Regarding the extensions in the use case scenario, the transactions in the extension part should be\nconsidered, but they should be counted in a different way than in the success scenario. For instance, in the scenario proposed in section 2.1, the number of transactions in the extension part is larger than the number of transactions in the success scenario. Nonetheless, the effort required to develop the extension part might be about 30% of the effort required to develop the success scenario.\n According to Karner, the actor that interacts with five use cases has the same value as if it interacts\nwith one use case. In practice, this might be incorrect. However, since the weight of actors is very low in comparison with use cases, the error is negligible, especially in large projects.\n The results of the fuzzy logic approach were better than the Karner\u2019s model in the first two stages (see\nTable 4). However, the fuzzy logic approach could not beat Karner\u2019s model in stage three. The main reason is that the average size of these projects is large and an assumption was made in Section 3 to set the complexity weight of the largest use case to fifteen as Karner proposed. Had the complexity weight of the largest use case been greater than fifteen, the fuzzy logic approach would have given better results.\n The results of the neural network were good in the MMER and not as favourable in the MMRE. This\nis because more projects are required for training and testing. Moreover, in some situations, the MMRE and the MMER work against each other. This means that improving the MMRE might worsen the MMER and vice versa. This is because the denominator in the MRE is the actual value, however the denominator in the MER is the estimated value.\nTHREATS TO VALIDITY\nIn these experiments, threats to validity can be summarized as follows:\n In the neural network approach, promising results were obtained in the training phase, however this\nmodel was not effective in the testing phase when using the MMER criteria. The main reason of this is the lack of projects. The industrial projects that are available for evaluation are scarce. This is because industrial firms are not ready to divulge the UML diagrams of their projects.\n Most of the projects used were educational projects. Some students may not follow the steps of the\nsoftware development life cycle effectively. Moreover, the quality of some projects might be poor and if the same projects are developed in industry, the actual size might be much more than the size obtained by students.\n There were difficulties in calculating the actual size in UCP or UUCP. Since there is no conversion\nmetrics between UCP and other size metrics, the size in UCP was obtained from the effort, and then equation 2 was used to obtain the size in UUCP. Although Schneider\u2019s method (Karner\u2019s refined method) was used to calculate the size in UCP, this method might not be as accurate as other sophisticated cost estimation models such as SEER-SEM.\n The use case points model mainly depends on the use case diagram. If the use case diagram was not\nproperly designed, a huge error could be incurred."}, {"heading": "CONCLUSION AND FUTURE WORK", "text": "The use case points model is one of the cost estimation models that has been widely used because it is simple, fast, accurate to a certain degree and can be automated. The use case points model is based on the number and the complexity of the use cases as well as the actors. The original model suggested three degrees of complexity to the use cases and there is no graduation among the complexity weights of the use cases. This paper presented the disadvantages of the current model and proposed an enhancement to this model using fuzzy logic and neural network. The fuzzy logic approach presents ten degrees of complexity of the use cases. Moreover, this approach provides graduation among the complexity weight. The neural network approach was used as a black box to map the input vectors of the use case model to software size. The results showed that the UCP software estimation can be improved up to 22% in some projects.\nFuture work will focus on revamping the use case model. First, the largest use case should contain at least twenty transactions as opposed to eight transactions as in Karner\u2019s model. Secondly, the complexity weight of the use cases will be calibrated using the neuro-fuzzy approach. Thirdly, \u201cextend\u201d and \u201cinclude\u201d use cases should be considered when estimating the software size. Finally, the future work will focus on how the \u201cextend\u201d and \u201cinclude\u201d use cases as well as the transactions in the extension part should be counted."}], "references": [{"title": "Parametric estimating handbook", "author": ["D. Eck", "B. Brundick", "T. Fettig", "J. Dechoretz", "J. Ugljesa"], "venue": "The International Society of Parametric Analysis (ISPA), Fourth Edition. 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Oct.). Chaos manifesto", "author": ["J. Lynch"], "venue": "The Standish Group. Boston. [Online]. Available: http://www.standishgroup.com/newsroom/chaos_manifesto. php", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Measuring application development productivity", "author": ["A. Albrecht"], "venue": "IBM Application Development Symp. 1979, pp. 83-92.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1979}, {"title": "Function Point Analysis: Difficulties and Improvements", "author": ["C.R. Symons"], "venue": "IEEE Trans. Software Eng., vol. 14, pp. 2- 11, 1988.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1988}, {"title": "UML specification version 1.1", "author": ["G. Booch"], "venue": "OMG, Aug. 1997.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "UML version 2.0", "author": ["G. Booch", "I. Jacobson", "J. Rumbaugh"], "venue": "Jul. 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Resource Estimation for Objectory Projects", "author": ["G. Karner"], "venue": "Objective Systems, 1993.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1993}, {"title": "Object-Oriented Software Engineering: A use Case Driven Approach", "author": ["I. Jacobson", "M. Christerson", "P. Jonsson", "G. Overgaard"], "venue": "Addison Wesley,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1992}, {"title": "Use cases", "author": ["J. Rumbaugh", "I. Jacobson", "G. Booch"], "venue": "UML Distilled, 3rd ed., M. Fowler, Ed. Pearson Higher Education, 2004, pp. 103.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Fuzzy sets", "author": ["L.A. Zadeh"], "venue": "Information and Control, vol. 8, pp. 338-353, 6, 1965.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1965}, {"title": "An Introduction to Computing with Neural Nets", "author": ["R.P. Lippman"], "venue": "IEEE ASSP Magasine, vol. 3, pp. 4-22, 1987.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1987}, {"title": "An assessment and comparison of common software cost estimation modeling techniques", "author": ["L.C. Briand", "K.E. Emam", "D. Surmann", "I. Wieczorek", "K.D. Maxwell"], "venue": "ICSE'99, vol. 0, pp. 313-322, 1999.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "What Accuracy Statistics Really Measure", "author": ["B.A. Kitchenham", "L.M. Pickard", "S.G. MacDonell", "M.J. Shepperd"], "venue": "IEE Proc. -Softw, vol. 148, pp. 81-85, 2001.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "A Simulation Study of the Model Evaluation Criterion MMRE", "author": ["T. Foss", "E. Stensrud", "B. Kitchenham", "I. Myrtveit"], "venue": "IEEE Transactions on Software Engineering, vol. 29, pp. 985-995, 2003.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "On the dissection of asymmetrical frequency curves", "author": ["P. Karl"], "venue": "Philosophical Transaction of Royal Society, vol. 185, pp. 71, 1894.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1894}, {"title": "Mapping the OO-jacobson approach into function point analysis", "author": ["T. Fetcke", "A. Abran", "N. Tho-Hau"], "venue": "Technology of Object-Oriented Languages and Systems, 1997. TOOLS 23. Proceedings, 1997, pp. 192-202.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "Software cost estimation using use-case models: A critical evaluation", "author": ["A. Issa", "M. Odeh", "D. Coward"], "venue": "Information and Communication Technologies, 2006. 2006, pp. 2766-2771.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Software cost estimation using fuzzy logic", "author": ["A. Mittal", "K. Parkash", "H. Mittal"], "venue": "SIGSOFT Softw. Eng. Notes, vol. 35, pp. 1-7, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Improving the COCOMO model using a neuro-fuzzy approach", "author": ["X. Huang", "D. Ho", "J. Ren", "L.F. Capretz"], "venue": "Appl. Soft Comput., vol. 7, pp. 29-40, 2007.  Ali Bou Nassif et al, Journal of Global Research in Computer Science, 1 (4), November 2010, 12-21", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Neural network studies. 1. Comparison of overfitting and overtraining", "author": ["I.V. Tetko", "D.J. Livingstone", "A.I. Luik"], "venue": "J. Chem. Inf. Comput. Sci., vol. 35, pp. 826- 833, 09/01, 1995.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": "The International Society of Parametric Analysis (ISPA) [1] and the Standish Group International [2] identified poor estimation as one of the main culprits behind software failure.", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "The International Society of Parametric Analysis (ISPA) [1] and the Standish Group International [2] identified poor estimation as one of the main culprits behind software failure.", "startOffset": 97, "endOffset": 100}, {"referenceID": 2, "context": "The FPA model was proposed by Albrecht in 1979 [3] and it measures the size of software based on its functionalities.", "startOffset": 47, "endOffset": 50}, {"referenceID": 3, "context": "cannot be computed automatically and the decisions made in counting function points are subjective [4].", "startOffset": 99, "endOffset": 102}, {"referenceID": 4, "context": "1 in 1997 [5], but OOM has become more popular since the release of UML 2.", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "0 in 2005 [6].", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "The UCP model was proposed by Gustav Karner in 1993 [7].", "startOffset": 52, "endOffset": 55}, {"referenceID": 0, "context": "First, rather than classifying a use case as simple, average, or complex, the use case will be classified as ux, such as x \u2208 [1,10] where x represents the number of transactions.", "startOffset": 125, "endOffset": 131}, {"referenceID": 9, "context": "First, rather than classifying a use case as simple, average, or complex, the use case will be classified as ux, such as x \u2208 [1,10] where x represents the number of transactions.", "startOffset": 125, "endOffset": 131}, {"referenceID": 7, "context": "[8] A use case diagram shows how users interact with the system.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Figure 1 is an example of a use case diagram [9].", "startOffset": 45, "endOffset": 48}, {"referenceID": 8, "context": "Figure 1: Use Case Diagram [9]", "startOffset": 27, "endOffset": 30}, {"referenceID": 6, "context": "These rules include [7]", "startOffset": 20, "endOffset": 23}, {"referenceID": 6, "context": "Karner [7] proposed an effort estimation method based on the Adjusted Use Case Points (UCP).", "startOffset": 7, "endOffset": 10}, {"referenceID": 6, "context": "Table 1: Technical Factors [7]", "startOffset": 27, "endOffset": 30}, {"referenceID": 6, "context": "F12 Provide direct access for third parties 1 F13 Special user training facilities 1 Table 2: Environmental Factors [7]", "startOffset": 116, "endOffset": 119}, {"referenceID": 9, "context": "Fuzzy logic is derived from the fuzzy set theory that was proposed by Lotfi Zadeh in 1965 [10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 0, "context": "Fz[x \u2208 A] = \u03bcA(x): R \u2192 [0, 1] (5)", "startOffset": 23, "endOffset": 29}, {"referenceID": 10, "context": "Artificial Neural Network (ANN) is a network composed of artificial neurons or nodes which emulate the biological neurons [11].", "startOffset": 122, "endOffset": 126}, {"referenceID": 11, "context": "MMRE: This is a very common criterion used to evaluate software cost estimation models [12].", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "MMER: MMER is another method for cost estimation models evaluation [13].", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "argued that MMRE should not be used when comparing cost estimation models and using the standard deviation would be better [14].", "startOffset": 123, "endOffset": 127}, {"referenceID": 14, "context": "The standard deviation method was first proposed by Karl Pearson in 1894 [15].", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "[16] mapped UML use case diagrams to the software size metric Function Points.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18], used the use case diagram of software to determine the effort of the software based on three steps.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19], used fuzzy logic to tune the parameters of COCOMO cost estimation model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20], proposed a new model using neuro-fuzzy technique to improve the estimation of the COCOMO model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "than trial and error [21].", "startOffset": 21, "endOffset": 25}], "year": 2016, "abstractText": "Software estimation is a crucial task in software engineering. Software estimation encompasses cost, effort, schedule, and size. The importance of software estimation becomes critical in the early stages of the software life cycle when the details of software have not been revealed yet. Several commercial and non-commercial tools exist to estimate software in the early stages. Most software effort estimation methods require software size as one of the important metric inputs and consequently, software size estimation in the early stages becomes essential. One of the approaches that has been used for about two decades in the early size and effort estimation is called use case points. Use case points method relies on the use case diagram to estimate the size and effort of software projects. Although the use case points method has been widely used, it has some limitations that might adversely affect the accuracy of estimation. This paper presents some techniques using fuzzy logic and neural networks to improve the accuracy of the use case points method. Results showed that an improvement up to 22% can be obtained using the proposed approach.", "creator": "Microsoft\u00ae Word 2010"}}}