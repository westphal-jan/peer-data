{"id": "1612.01120", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2016", "title": "The Complexity of Bayesian Networks Specified by Propositional and Relational Languages", "abstract": "We examine the complexity of inference in Bayesian networks specified by logical languages. We consider representations that range from fragments of propositional logic to function-free first-order logic with equality; in doing so we cover a variety of plate models and of probabilistic relational models. We study the complexity of inferences when network, query and domain are the input (the inferential and the combined complexity), when the network is fixed and query and domain are the input (the query/data complexity), and when the network and query are fixed and the domain is the input (the domain complexity). We draw connections with probabilistic databases and liftability results, and obtain complexity classes that range from polynomial to exponential levels. We also explore the use of Bayesian networks for optimization of our language and to consider approaches that address the need for functional inference of the language.", "histories": [["v1", "Sun, 4 Dec 2016 13:51:55 GMT  (93kb)", "http://arxiv.org/abs/1612.01120v1", null], ["v2", "Tue, 6 Dec 2016 02:00:14 GMT  (93kb)", "http://arxiv.org/abs/1612.01120v2", null], ["v3", "Fri, 6 Jan 2017 13:07:30 GMT  (93kb)", "http://arxiv.org/abs/1612.01120v3", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["fabio gagliardi cozman", "denis deratani mau\\'a"], "accepted": false, "id": "1612.01120"}, "pdf": {"name": "1612.01120.pdf", "metadata": {"source": "CRF", "title": "The Complexity of Bayesian Networks Specified by Propositional and Relational Languages", "authors": ["Fabio G. Cozman"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n61 2.\n01 12\n0v 1\n[ cs\n.A I]\n4 D"}, {"heading": "1 Introduction", "text": "A Bayesian network can represent any distribution over a given set of random variables [36, 71], and this flexibility has been used to great effect in many applications [109]. Indeed, Bayesian networks are routinely used to carry both deterministic and probabilistic assertions in a variety of knowledge representation tasks. Many of these tasks contain complex decision problems, with repetitive patterns of entities and relationships. Thus it is not surprising that practical concerns have led to modeling languages where Bayesian networks are specified using relations, logical variables, and quantifiers [49, 111]. Some of these languages enlarge Bayesian networks with plates [50, 85], while others resort to elements of database schema [47, 60]; some others mix probabilities with logic programming [106, 118] and even with functional programming [87, 89, 102]. The spectrum of tools that specify Bayesian networks by moving beyond propositional sentences is vast, and their applications are remarkable.\nYet most of the existing analysis on the complexity of inference with Bayesian networks focuses on a simplified setting where nodes of a network are associated with categorial variables and distributions are specified by flat tables containing probability values [115, 75]. This is certainly unsatisfying: as a point of comparison, consider the topic of logical inference, where much is known about the impact of specific constructs on computational complexity \u2014 suffice to mention the beautiful and detailed study of satisfiability in description logics [3].\nIn this paper we explore the complexity of inferences as dependent on the language that is used to specify the network. We adopt a simple specification strategy inspired by probabilistic programming [107] and by structural equation models [101]: A Bayesian network over binary variables is specified by a set of logical equivalences and a set of independent random variables. Using this simple scheme, we can parameterize computational complexity by the formal language that is allowed in the logical equivalences; we can move from sub-Boolean languages to relational ones, in the way producing languages that are similar in power to plate models [50] and to probabilistic relational models [74]. Note\nthat we follow a proven strategy adopted in logical formalisms: we focus on minimal sets of constructs (Boolean operators, quantifiers) that capture the essential connections between expressivity and complexity, and that can shed light on this connection for more sophisticated languages if needed. Our overall hope is to help with the design of knowledge representation formalims, and in that setting it is important to understand the complexity introduced by language features, however costly those may be.\nTo illustrate the sort of specification we contemplate, consider a simple example that will be elaborated later. Suppose we have a population of students, and we denote by fan(x ) the fact that student x is a fan of say a particular band. And we write friends(x , y) to indicate that x is a friend of y . Now consider a Bayesian network with a node fan(x ) per student, and a node friends(x , y) per pair of students (see Figure 1). Suppose each node fan(x ) is associated with the assessment P(fan(x ) = true) = 0.2. And finally suppose that a person is always a friend of herself, and two people are friends if they are fans of the band; that is, for each pair of students, friends(x , y) is associated with the formula\nfriends(x , y) \u2194 (x = y) \u2228 (fan(x ) \u2227 fan(y)). (1)\nNow if we have data on some students, we may ask for the probability that some two students are friends, or the probability that a student is a fan. We may wish to consider more sophisticated formulas specifying friendship: how would the complexity of our inferences change, say, if we allowed quantifiers in our formula? Or if we allowed relations of arity higher than two? Such questions are the object of our discussion.\nIn this study, we distinguish a few concepts. Inferential complexity is the complexity when the network, the query and the domain are given as input. When the specification vocabulary is fixed, inference complexity is akin to combined complexity as employed in database theory. Query complexity is the complexity when the network is fixed and the input consists of query and domain. Query complexity has often been defined, in the contex of probabilistic databases, as data complexity [123]. Finally, domain complexity is the complexity when network and query are fixed, and only the domain is given as input. Query and domain complexity are directly related respectively to dqe-liftability and domain liftability, concepts that have been used in lifted inference [9, 66]. We make connections with lifted inference and probabilistic databases whenever possible, and benefit from deep results originated from those topics. One of the contributions of this paper is a framework that can unify these varied research efforts with respect to the analysis of Bayesian networks. We show that many non-trivial complexity classes characterize the cost of inference as induced by various languages, and we make an effort to relate our investigation to various knowledge representation formalisms, from probabilistic description logics to plates to probabilistic relational models.\nThe paper is organized as follows. Section 2 reviews a few concepts concerning Bayesian networks and computational complexity. Our contributions start in Section 3, where we focus on propositional languages. In Section 4 we extend our framework to relational languages, and review relevant literature on probabilistic databases and lifted inference. In Sections 5 and 6 we study a variety of relational Bayesian network specifications. In Section 7 we connect these specifications to other schemes proposed in the literature. And in Section 8 we relate our results, mostly presented for decision problems, to\nValiant\u2019s counting classes and their extensions. Section 9 summarizes our findings and proposes future work.\nAll proofs are collected in A."}, {"heading": "2 A bit of notation and terminology", "text": "We denote by P(A) the probability of event A. In this paper, every random variable X is a function from a finite sample space (usually a space with finitely many truth assignments or interpretations) to real numbers (usually to {0, 1}). We refer to an event {X = x} as an assignment. Say that {X = 1} is a positive assignment, and {X = 0} is a negative assignment.\nA graph consists of a set of nodes and a set of edges (an edge is a pair of nodes), and we focus on graphs that are directed and acyclic [71]. The parents of a node X , for a given graph, are denoted pa(X). Suppose we have a directed acyclic graph G such that each node is a random variable, and we also have a joint probability distribution P over these random variables. Say that G and P satisfy the Markov condition iff each random variable X is independent of its nondescendants (in the graph) given its parents (in the graph).\nA Bayesian network is a pair consisting of a directed acyclic graph G whose nodes are random variables and a joint probability distribution P over all variables in the graph, such that G and P satisfy the Markov condition [92]. For a collection of measurable sets A1, . . . , An, we then have\nP(X1 \u2208 A1, . . . , Xn \u2208 An) = n\u220f\ni=1\nP\n(\nXi \u2208 Ai|pa(Xi) \u2208 \u22c2\nj:Xj\u2208pa(Xi) Aj\n)\nwhenever the conditional probabilities exist. If all random variables are discrete, then one can specify \u201clocal\u201d conditional probabilities P(Xi = xi|pa(Xi) = \u03c0i), and the joint probability distribution is necessarily the product of these local probabilities:\nP(X1 = x1, . . . , Xn = xn) =\nn\u220f\ni=1\nP(Xi = xi|pa(Xi) = \u03c0i) , (2)\nwhere \u03c0i is the projection of {x1, . . . , xn} on pa(Xi), with the understanding that P(Xi = xi|pa(Xi) = \u03c0i) stands for P(Xi = xi) whenever Xi has not parents.\nIn this paper we only deal with finite objects, so we can assume that a Bayesian network is fully specified by a finite graph and a local conditional probability distribution per random variable (the local distribution associated with random variable X specifies the probability of X given the parents of X). Often probability values are given in tables (referred to as conditional probability tables). Depending on how these tables are encoded, the directed acyclic graph may be redundant; that is, all the information to reconstruct the graph and the joint distribution is already in the tables. In fact we rarely mention the graph G in our results; however graphs are visually useful and we often resort to drawing them in our examples.\nA basic computational problem for Bayesian networks is: Given a Bayesian network B, a set of assignments Q and a set of assignments E, determine whether P(Q|E) > \u03b3 for some rational number \u03b3. We assume that every probability value is specified as a rational number. Thus, P(Q|E) = P(Q,E) /P(E) is a rational number, as P(Q,E) and P(E) are computed by summing through products given by Expression (2).\nWe adopt basic terminology and notation from computational complexity [98]. A language is a set of strings. A language defines a decision problem; that is, the problem of deciding whether an input string is in the language. A complexity class is a set of languages; we use well-known complexity classes P, NP, PSPACE, EXP, ETIME, NETIME. The complexity class PP consists of those languages L that satisfy the following property: there is a polynomial time nondeterministic Turing machine M such that \u2113 \u2208 L iff more than half of the computations of M on input \u2113 end up accepting. Analogously, we\nhave PEXP, consisting of those languages L with the following property: there is an exponential time nondeterministic Turing machine M such that \u2113 \u2208 L iff half of the computations of M on input \u2113 end up accepting [15].\nTo proceed, we need to define oracles and related complexity classes. An oracle Turing machine ML, where L is a language, is a Turing machine with additional tapes, such that it can write a string \u2113 to a tape and obtain from the oracle, in unit time, the decision as to whether \u2113 \u2208 L or not. If a class of languages/functions A is defined by a set of Turing machines M (that is, the languages/functions are decided/computed by these machines), then define AL to be the set of languages/functions that are decided/computed by {ML : M \u2208 M}. For a function f , an oracle Turing machine Mf can be similarly defined, and for any class A we have Af . If A and B are classes of languages/functions, AB = \u222ax\u2208BAx. For instance, the polynomial hierarchy consists of classes \u03a3Pi = NP \u03a3Pi\u22121 and \u03a0Pi = co\u03a3 P i , with \u03a3P0 = P (and PH is the union \u222ai\u03a0 P i = \u222ai\u03a3 P i ).\nWe examine Valiant\u2019s approach to counting problems in Section 8; for now suffice to say that #P is the class of functions such that f \u2208 #P iff f(\u2113) is the number of computation paths that accept \u2113 for some polynomial time nondeterministic Turing machine [130]. It is as if we had a special machine, called by Valiant a counting Turing machine, that on input \u2113 prints on a special tape the number of computations that accept \u2113.\nWe will also use the class PP1, defined as the set of languages in PP that have a single symbol as input vocabulary. We can take this symbol to be 1, so the input is just a sequence of 1s (one can interpret this input as a non-negative integer written in unary notation). This is the counterpart of Valiant\u2019s class #P1 that consists of the functions in #P that have a single symbol as input vocabulary [131].\nWe focus on many-one reductions: such a reduction from L to L\u2032 is a polynomial time algorithm that takes the input to decision problem L and transforms it into the input to decision problem L\u2032 such that L\u2032 has the same output as L. A Turing reduction from L to L\u2032 is an polynomial time algorithm that decides L using L\u2032 as an oracle. For a complexity class C, a decision problem L is C-hard with respect to many-one reductions if each decision problem in C can be reduced to L with many-one reductions. A decision problem is then C-complete with respect to many-one reductions if it is in C and it is C-hard with respect to many-one reductions. Similar definitions of hardness and completeness are obtained when \u201cmany-one reductions\u201d are replaced by \u201cTuring reductions\u201d.\nAn important PP-complete (with respect to many-one reductions) decision problem is MAJSAT: the input is a propositional sentence \u03c6 and the decision is whether or not the majority of assignments to the propositions in \u03c6 make \u03c6 true [51]. Another PP-complete problem (with respect to many-one reductions) is deciding whether the number of satisfying assignments for \u03c6 is larger than an input integer k [119]; in fact this problem is still PP-complete with respect to many-one reductions even if \u03c6 is monotone [54]. Recall that a sentence is monotone if it has no negation.\nA formula is in kCNF iff it is in Conjunctive Normal Form with k literals per clause (if there is no restriction on k, we just write CNF). MAJSAT is PP-complete with respect to many-one reductions even if the input is restricted to be in CNF; however, it is not known whether MAJSAT is still PPcomplete with respect to many-one reductions if the sentence \u03c6 is in 3CNF. Hence we will resort in proofs to a slightly different decision problem, following results by Bailey et al. [7]. The problem #3SAT(>) gets as input a propositional sentence \u03c6 in 3CNF and an integer k, and the decision is whether #\u03c6 > k; we use, here and later in proofs, #\u03c6 to denote the number of satisfying assignments for a formula \u03c6. We will also use, in the proof of Theorem 2, the following decision problem. Say that an assignment to the propositions in a sentence in CNF repects the 1-in-3 rule if at most one literal per clause is assigned true. Denote by #(1-in-3)\u03c6 the number of satisfying assignments for \u03c6 that also respects the 1-in-3 rule. The decision problem #(1-in-3)SAT(>) gets as input a propositional sentence \u03c6 in 3CNF and an integer k, and decides whether #(1-in-3)\u03c6 > k. We have:\nProposition 1. Both #3SAT(>) and #(1-in-3)SAT(>) are PP-complete with respect to many-one reductions."}, {"heading": "3 Propositional languages: Inferential and query complexity", "text": "In this section we focus on propositional languages, so as to present our proposed framework in the most accessible manner. Recall that we wish to parameterize the complexity of inferences by the language used in specifying local distributions."}, {"heading": "3.1 A specification framework", "text": "We are interested in specifying Bayesian networks over binary variablesX1, . . . , Xn, where each random variable Xi is the indicator function of a proposition Ai. That is, consider the space \u2126 consisting of all truth assignments for these variables (there are 2n such truth assignments); then Xi yields 1 for a truth assignment that satisfies Ai, and Xi yields 0 for a truth assignment that does not satisfy Ai.\nWe will often use the same letter to refer to a proposition and the random variable that is the indicator function of the proposition.\nWe adopt a specification strategy that moves away from tables of probability values, and that is inspired by probabilistic programming [103, 118] and by structural models [100]. A Bayesian network specification associates with each proposition Xi either\n\u2022 a logical equivalence Xi \u2194 \u2113i, or\n\u2022 a probabilistic assessment P(Xi = 1) = \u03b1,\nwhere \u2113i is a formula in a propositional language L, such that the only extralogical symbols in \u2113i are propositions in {X1, . . . , Xn}, and \u03b1 is a rational number in the interval [0, 1].\nWe refer to each logical equivalence Xi \u2194 \u2113i as a definition axiom, borrowing terminology from description logics [3]. We refer to \u2113i as the body of the definition axiom. In order to avoid confusion between the leftmost symbol \u2194 and possible logical equivalences within \u2113i, we write a definition axiom as in description logics:\nXi \u2261\u2261 \u2113i,\nand we emphasize that \u2261\u2261 is just syntactic sugar for logical equivalence \u2194. A Bayesian network specification induces a directed graph where the nodes are the random variables X1, . . . , Xn, and Xj is a parent of Xi if and only if the definition axiom for Xi contains Xj . If this graph is acyclic, as we assume in this paper, then the Bayesian network specification does define a Bayesian network.\nFigure 2 depicts a Bayesian network specified this way. Note that we avoid direct assessments of conditional probability, because one can essentially create negation through P(X = 1|Y = 1) = P(X = 0|Y = 0) = 0. In our framework, the use of negation is a decision about the language. We will see that negation does make a difference when complexity is analyzed.\nAny distribution over binary variables given by a Bayesian network can be equivalently defined using definition axioms, as long as definitions are allowed to contain negation and conjunction (and then disjunction is syntactic sugar). To see that, consider a conditional distribution for X given Y1\nand Y2; we can specify this distribution using the definition axiom\nX \u2261\u2261 (\u00acY1 \u2227 \u00acY2 \u2227 Z00) \u2228 (\u00acY1 \u2227 Y2 \u2227 Z01) \u2228\n(Y1 \u2227 \u00acY2 \u2227 Z10) \u2228 (Y1 \u2227 Y2 \u2227 Z11) ,\nwhere Zab are fresh binary variables (that do not appear anywhere else), associated with assessments P(Zab = 1) = P(X = 1|Y1 = a, Y2 = b). This sort of encoding can be extended to any set Y1, . . . , Ym of parents, demanding the same space as the corresponding conditional probability table.\nExample 1. Consider a simple Bayesian network with random variables X and Y , where Y is the sole parent of X , and where:\nP(Y = 1) = 1/3, P(X = 1|Y = 0) = 1/5, P(X = 1|Y = 1) = 7/10.\nThen Figure 2 presents an equivalent specification for this network, in the sense that both specifications have the same marginal distribution over (X,Y ).\nNote that definition axioms can exploit structures that conditional probability tables cannot; for instance, to create a Noisy-Or gate [99], we simply say that X \u2261\u2261 (Y1 \u2227W1) \u2228 (Y2 \u2227W2), where W1 and W2 are inhibitor variables."}, {"heading": "3.2 The complexity of propositional languages", "text": "Now consider a language INF[L] that consists of the strings (B,Q,E, \u03b3) for which P(Q|E) > \u03b3, where\n\u2022 P is the distribution encoded by a Bayesian network specification B with definition axioms whose bodies are formulas in L,\n\u2022 Q and E are sets of assignments (the query),\n\u2022 and \u03b3 is a rational number in [0, 1].\nFor instance, denote by Prop(\u2227,\u00ac) the language of propositional formulas containing conjunction and negation. Then INF[Prop(\u2227,\u00ac)] is the language that decides the probability of a query for networks specified with definition axioms containing conjunction and negation. As every Bayesian network over binary variables can be specified with such definition axioms, INF[Prop(\u2227,\u00ac)] is in fact a PP-complete language [36, Theorems 11.3 and 11.5].\nThere is obvious interest in finding simple languages L such that deciding INF[L] is a tractable problem, so as to facilitate elicitation, decision-making and learning [34, 40, 64, 108, 116]. And there are indeed propositional languages that generate tractable Bayesian networks: for instance, it is well known that Noisy-Or networks display polynomial inference when the query consists of negative assignments [59]. Recall that a Noisy-Or network has a bipartite graph with edges pointing from nodes in one set to nodes in the other set, and the latter nodes are associated with Noisy-Or gates.\nOne might think that tractability can only be attained by imposing some structural conditions on graphs, given results that connect complexity and graph properties [76]. However, it is possible to attain tractability without restrictions on graph topology. Consider the following result, where we use Prop(\u03bd) to indicate a propositional language with operators restricted to the ones in the list \u03bd:\nTheorem 1. INF[Prop(\u2227)] is in to P when the query (Q,E) contains only positive assignments, and INF[Prop(\u2228)] is in to P when the query contains only negative assignments.\nAs the proof of this result shows (in A), only polynomial effort is needed to compute probabilities for positive queries in networks specified with Prop(\u2227), even if one allows root nodes to be negated (that is, the variables that appear in probabilistic assessments can appear negated in the body of definition axioms).\nAlas, even small movements away from the conditions in Theorem 1 takes us to PP-completeness:\nTheorem 2. INF[Prop(\u2227)] and INF[Prop(\u2228)] are PP-complete with respect to many-one reductions.\nProofs for these results are somewhat delicate due to the restriction to many-one reductions. In A we show that much simpler proofs for PP-completeness of INF(Prop(\u2227)) and INF(Prop(\u2228)) are possible if one uses Turing reductions. A Turing reduction gives some valuable information: if a problem is PP-complete with Turing reductions, then it is unlikely to be polynomial (for if it were polynomial, then PPP would equal P, a highly unlikely result given current assumptions in complexity theory [127]). However, Turing reductions tend to blur some significant distinctions. For instance, for Turing reductions it does not matter whether Q is a singleton or not: one can ask for P(Q1|E1), P(Q2|E2), and so on, and then obtain P(Q1, Q2, . . . |E) as the product of the intermediate computations. However, it may be the case that for some languages such a distinction concerning Q matters. Hence many-one reductions yield stronger results, so we emphasize them throughout this papper.\nOne might try to concoct additional languages by using specific logical forms in the literature [37]. We leave this to future work; instead of pursuing various possible sub-Boolean languages, we wish to quickly examine the query complexity of Bayesian networks, and then move to relational languages in Section 4."}, {"heading": "3.3 Query complexity", "text": "We have so far considered that the input is a string encoding a Bayesian network specification B, a query (Q,E), and a rational number \u03b3. However in practice one may face a situation where the Bayesian network is fixed, and the input is a string consisting of the pair (Q,E) and a rational number \u03b3; the goal is to determine whether P(Q|E) > \u03b3 with respect to the fixed Bayesian network.\nDenote by QINF[B], where B is a Bayesian network specification, the language consisting of each string (Q,E, \u03b3) for which P(Q|E) > \u03b3 with respect to B. And denote by QINF[L] the set of languages QINF[B] where B is a Bayesian network specification with definition axioms whose bodies are formulas in L.\nDefinition 1. Let L be a propositional language and C be a complexity class. The query complexity of L is C if and only if every language in QINF[L] is in C.\nThe fact that query complexity may differ from inferential complexity was initially raised by Darwiche and Provan [34], and has led to a number of techniques emphasizing compilation of a fixed Bayesian network [23, 35]. Indeed the expression \u201cquery complexity\u201d seems to have been coined by Darwiche [36, Section 6.9], without the formal definition presented here.\nThe original work by Darwiche and Provan [34] shows how to transform a fixed Bayesian network into a Query-DAG such that P(Q|E) > \u03b3 can be decided in linear time. That is:\nTheorem 3 (Darwiche and Provan [34]). QINF[Prop(\u2227,\u00ac)] is in P.\nResults on query complexity become more interesting when we move to relational languages."}, {"heading": "4 Relational Languages: Inferential, query, and domain com-", "text": "plexity\nIn this section we extend our specification framework so as to analyze the complexity of relational languages. Such languages have been used in a variety of applications with repetitive entities and relationships [49, 111]."}, {"heading": "4.1 Relational Bayesian network specifications", "text": "We start by blending some terminology and notation by Poole [105] and by Milch et al. [90].\nA parameterized random variable, abbreviated parvariable, is a function that yields, for each combination of its input parameters, a random variable. For instance, parvariableX yields a random variable X(x ) for each x . In what follows, parvariables and their parameters will correspond to relations and their logical variables.\nWe use a vocabulary consisting of names of relations. Every relation X is associated with a nonnegative integer called its arity. We also use logical variables; a logical variable is referred to as a logvar. A vector of logvars [x1, . . . , xk] is denoted ~x ; then X(~x ) is an atom. A domain is a set; in this paper every domain is finite. When the logvars in an atom are replaced by elements of the domain, we obtain X(a1, . . . , ak), a ground atom, often referred to as a grounding of relation X . An interpretation I is a function that assigns to each relation X of arity k a relation on Dk. An interpretation can be viewed as a function that assigns true or false to each grounding X(~a), where ~a is a tuple of elements of the domain. Typically in logical languages there is a distinction between constants and elements of a domain, but we avoid constants altogether in our discussion (as argued by Bacchus, if constants are used within a probabilistic logic, some sort of additional rigidity assumption must be used [4]).\nGiven a domain D, we can associate with each grounding X(~a) a random variable X\u0302(~a) over the set of all possible interpretations, such that X\u0302(~a)(I) = 1 if interpretation I assigns true to X(~a), and X\u0302(~a)(I) = 0 otherwise. Similarly, we can associate with a relation X a parvariable X\u0302 that yields, once a domain is given, a random variable X\u0302(~a) for each grounding X(~a). To simplify matters, we use the same symbol for a grounding X(~a) and its associated random variable X\u0302(~a), much as we did with propositions and their associated random variables. Similarly, we use the same symbol for a relation X and its associated parvariable X\u0302 . We can then write down logical formulas over relations/parvariables, and we can assess probabilities for relations/parvariables. The next example clarifies the dual use of symbols for relations/parvariables.\nExample 2. Consider a model of friendship built on top of the example in Section 1. Two people are friends if they are both fans of the same band, or if they are linked in some other unmodeled way, and a person is always a friend of herself. Take relations friends, fan, and linked. Given a domain, say D = {a, b}, we have the grounding friends(a, b), whose intended interpretation is that a and b are friends; we take friendship to be asymmetric so friends(a, b) may hold while friends(b, a) may not hold. We also have groundings fan(a), linked(b, a), and so on. Each one of these groundings corresponds to a random variable that yields 1 or 0 when the grounding is respectively true or false is an interpretation.\nThe stated facts about friendship might be encoded by an extended version of Formula (1), written here with the symbol \u2261\u2261 standing for logical equivalence:\nfriends(x , y) \u2261\u2261 (x = y) \u2228 (fan(x ) \u2227 fan(y)) \u2228 linked(x , y). (3)\nWe can draw a directed graph indicating the dependence of friends on the other relations, as in Figure 3. Suppose we believe 0.2 is the probability that an element of the domain is a fan, and 0.1 is the probability that two people are linked for some other reason. To express these assesssments we might write\nP(fan(x ) = 1) = 0.2 and P ( linked(x , y) = 1 ) = 0.1, (4)\nwith implicit outer universal quantification.\nGiven a formula and a domain, we can produce all groundings of the formula by replacing its logvars by elements of the domain in every possible way (as usual when grounding first-order formulas). We can similarly ground probabilistic assessments by grounding the affected relations.\nExample 3. In Example 2, we can produce the following groundings from domain D = {a, b} and Formula (3):\nfriends(a, a) \u2261\u2261 (a = a) \u2228 (fan(a) \u2227 fan(a)) \u2228 linked(a, a),\nfriends(a, b) \u2261\u2261 (a = b) \u2228 (fan(a) \u2227 fan(b)) \u2228 linked(a, b),\nfriends(b, a) \u2261\u2261 (b = a) \u2228 (fan(b) \u2227 fan(a)) \u2228 linked(b, a),\nfriends(b, b) \u2261\u2261 (b = b) \u2228 (fan(b) \u2227 fan(b)) \u2228 linked(b, b),\nSimilarly, we obtain:\nP(fan(a) = 1) = 0.2, P(fan(b) = 1) = 0.2, P(linked(a, a) = 1) = 0.1, P(linked(a, b) = 1) = 0.1, P(linked(b, a) = 1) = 0.1, P(linked(b, b) = 1) = 0.1,\nby grounding assessments in Expression (4).\nIn short: we wish to extend our propositional framework by specifying Bayesian networks using both parameterized probabilistic assessments and first-order definitions. So, suppose we have a finite set of parvariables, each one of them corresponding to a relation in a vocabulary. A relational Bayesian network specification associates, with each parvariable Xi, either\n\u2022 a definition axiom Xi(~x ) \u2261\u2261 \u2113i(~x , Y1, . . . , Ym), or\n\u2022 a probabilistic assessment P(X(~x ) = 1) = \u03b1,\nwhere\n\u2022 \u2113i is a well-formed formula in a language L, containing relations Y1, . . . , Ym and free logvars ~x (and possibly additional logvars bound to quantifiers),\n\u2022 and \u03b1 is a rational number in [0, 1].\nThe formula \u2113i is the body of the corresponding definition axiom. The parvariables that appear in \u2113i are the parents of parvariable Xi, and are denoted by pa(Xi). Clearly the definition axioms induce a directed graph where the nodes are the parvariables and the parents of a parvariable (in the graph) are exactly pa(Xi). This is the parvariable graph of the relational Bayesian network specification (this sort of graph is called a template dependency graph by Koller and Friedman [71, Definition 6.13]). For instance, Figure 3 depicts the parvariable graph for Example 2.\nWhen the parvariable graph of a relational Bayesian network specification is acyclic, we say the specification itself is acyclic. In this paper we assume that relational Bayesian network specifications are acyclic, and we do not even mention this anymore.\nThe grounding of a relational Bayesian network specification S on a domain D is defined as follows. First, produce all groundings of all definition axioms. Then, for each parameterized probabilistic assessment P(X(~x ) = 1) = \u03b1, produce its ground probabilistic assessments\nP(X( ~a1) = 1) = \u03b1, P(X( ~a2) = 1) = \u03b1, and so on,\nfor all appropriate tuples ~aj built from the domain. The grounded relations, definitions and assessments specify a propositional Bayesian network that is then the semantics of S with respect to domain D.\nExample 4. Consider Example 2. For a domain {a, b}, the relational Bayesian network specification given by Expressions (3) and (4) is grounded into the sentences and assessments in Example 3. By repeating this process for a larger domain {a, b, c}, we obtain a larger Bayesian network whose graph is depicted in Figure 4.\nNote that logical inference might be used to simplify grounded definitions; for instance, in the previous example, one might note that friends(a, a) is simply true. Note also that the grounding of an formula with quantifiers turns, as usual, an existential quantifier into a disjunction, and a universal quantifier into a conjunction. Consider the example:\nExample 5. Take the following relational Bayesian network specification (with no particular meaning, just to illustrate a few possibilities):\nP(X1(x ) = 1) = 2/3, P(X2(x ) = 1) = 1/10, P(X3(x ) = 1) = 4/5, P ( X4(x , y) = 1 ) = 1/2,\nX5(x ) \u2261\u2261 \u2203y : \u2200z : \u00acX1(x ) \u2228X2(y) \u2228X3(z), X6(x ) \u2261\u2261 X5(x ) \u2227 \u2203y : X4(x , y) \u2227X1(y),\nTake a domain D = {1, 2}; the grounded definition of X5(1) is\nX5(1) \u2261\u2261 ((\u00acX1(1) \u2228X2(1) \u2228X3(1)) \u2227 (\u00acX1(1) \u2228X2(1) \u2228X3(2))) \u2228\n((\u00acX1(1) \u2228X2(2) \u2228X3(1)) \u2227 (\u00acX1(1) \u2228X2(2) \u2228X3(2))) .\nFigure 5 depicts the parvariable graph and the grounding of this relational Bayesian network specification.\nIn order to study complexity questions we must decide how to encode any given domain. Note that there is no need to find special names for the elements of the domain, so we take that the domain is always the set of numbers {1, 2, . . . , N}. Now if this list is explicitly given as input, then the size of the input is of order N . However, if only the number N is given as input, then the size of the input is either of order N when N is encoded in unary notation, or of order logN when N is encoded in binary notation. The distinction between unary and binary notation for input numbers is often used in description logics [3].\nThe conceptual difference between unary and binary encodings of domain size can be captured by the following analogy. Suppose we are interested in the inhabitants of a city: the probabilities that they study, that they marry, that they vote, and so on. Suppose the behavior of these inhabitants is modeled by a relational Bayesian network specification, and we observe evidence on a few people. If we then take our input N to be in unary notation, we are implicitly assuming that we have a directory, say a mailing list, with the names of all inhabitants; even if we do not care about their specific names, each one of them exists concretely in our modeled reality. But if we take our input N to be in binary notation, we are just focusing on the impact of city size on probabilities, without any regard for the actual inhabitants; we may say that N is a thousand, or maybe fifty million (and perhaps neither of these numbers is remotely accurate)."}, {"heading": "4.2 Inferential, combined, query and domain complexity", "text": "To repeat, we are interested in the relationship between the language L that is employed in the body of definition axioms and the complexity of inferences. While in the propositional setting we distinguished between inferential and query complexity, here we have an additional distinction to make. Consider the following definitions, where S is a relational Bayesian network specification, N is the domain size, Q and E are sets of assignments for ground atoms, \u03b3 is a rational number in [0, 1], and C is a complexity class:\nDefinition 2. Denote by INF[L] the language consisting of strings (S,N,Q,E, \u03b3) for which P(Q|E) > \u03b3 with respect to the grounding of S on domain of size N , where S contains definition axioms whose bodies are formulas in L. The inferential complexity of L is C iff INF[L] is in C; moreover, the inferential complexity is C-hard with respect to a reduction iff INF[L] is C-hard with respect to the reduction, and it is C-complete with respect to a reduction iff it is in C and it is C-hard with respect to the reduction.\nDefinition 3. Denote by QINF[S] the language consisting of strings (N,Q,E, \u03b3) for which P(Q|E) > \u03b3 with respect to the grounding of S on domain of size N . Denote by QINF[L] the set of languages QINF[S] for S where the bodies of definition axioms in S are formulas in L. The query complexity of L is in C iff every language in QINF[L] is in C; moreover, the query complexity is C-hard with respect to a reduction iff some language in QINF[L] is C-hard with respect to the reduction, and it is C-complete with respect to a redution iff it is in C and it is C-hard with respect to the reduction.\nDefinition 4. Denote by DINF[S,Q,E] the language consisting of strings (N, \u03b3) for which P(Q|E) > \u03b3 with respect to the grounding of S on domain of size N . Denote by DINF[L] the set of languages DINF[S,Q,E] for S where the bodies of definition axioms in S are formulas in L, and where Q and E are sets of assignments. The domain complexity of L is in C iff every language in DINF[L] is in C; moreover, the domain complexity is C-hard with respect to a reduction iff some language in DINF[L] is C-hard with respect to the reduction, and it is C-complete with respect to a redution iff it is in C and it is C-hard with respect to the reduction.\nWe conclude this section with a number of observations.\nCombined complexity The definition of inferential complexity imposes no restriction on the vocabulary; later we will impose bounds on relation arity. We might instead assume that the vocabulary is fixed; in this case we might use the term combined complexity, as this is the term employed in finite model theory and database theory to refer to the complexity of model checking when both the formula and the model are given as input, but the vocabulary is fixed [80].\nLifted inference We note that query and domain complexities are related respectively to dqeliftability and domain-liftability, as defined in the study of lifted inference [66, 65].\nThe term \u201clifted inference\u201d is usually attached to algorithms that try to compute inferences involving parvariables without actually producing groundings [68, 90, 105]. A formal definition of lifted inference has been proposed by Van den Broeck [133]: an algorithm is domain lifted iff inference runs in polynomial time with respect to N , for fixed model and query. This definition assumes that N is given in unary notation; if N is given in binary notation, the input is of size logN , and a domain lifted algorithm may take exponential time. Domain liftability has been extended to dqe-liftability, where the inference must run in polynomial time with respect to N and the query, for fixed model [66].\nIn short, dqe-liftability means that query complexity is polynomial, while domain-liftability means that domain complexity is polynomial. Deep results have been obtained both on the limits of liftability [66, 65], and on algorithms that attain liftability [9, 135, 67, 94, 124]. We will use several of these results in our later proofs.\nWe feel that dqe-liftability and domain-liftability are important concepts but they focus only on a binary choice (polynomial versus non-polynomial); our goal here is to map languages and complexities in more detail. As we have mentioned in Section 1, our main goal is to grasp the complexity, however high, of language features.\nProbabilistic databases Highly relevant material has been produced in the study of probabilistic databases; that is, databases where data may be associated with probabilities. There exist several probabilistic database systems [31, 70, 120, 137, 139]; for instance, the Trio system lets the user indicate that Amy drives an Acura with probability 0.8 [10]. As another example, the NELL system scans text from the web and builds a database of facts, each associated with a number between zero and one [91].\nTo provide some focus to this overview, we adopt the framework described by Suciu et al. [123]. Consider a set of relations, each implemented as a table. Each tuple in a table may be associated with a probability. These probabilistic tuples are assumed independent (as dependent tuples can be modeled from independent ones [123, Section 2.7.1]). A probabilistic database management system receives a logical formula \u03c6(~x ) and must determine, using data and probabilities in the tables, the probability P(\u03c6(~a)) for tuples ~a. The logical formula \u03c6(~x ) is referred to as the query; for example, \u03c6 may be a Union of Conjunctive Queries (a first-order formula with equality, conjunction, disjunction and existential quantification). Note that the word \u201cquery\u201d is not used with the meaning usually adopted in the context of Bayesian networks; in probabilistic databases, a query is a formula whose probability is to be computed.\nSuppose that all tuples in the table for relation X(~x ) are associated with identical probability value \u03b1. This table can be viewed as the grounding of a parvariable X(~x ) that is associated with the assessment P(X(~x ) = 1) = \u03b1. Beame et al. say that a probabilistic database is symmetric iff each table in the database can be thus associated with a parvariable and a single probabilistic assessment [9].\nNow suppose we have a symmetric probabilistic database and a query \u03c6. Because the query is itself a logical formula, results on the complexity of computing its probability can be directly mapped to our study of relational Bayesian network specifications. This is pleasant because several deep results have been derived on the complexity probabilistic databases. We later transfer some of those results to obtain the combined and query complexity of specifications based on first-order logic and on fragments of first-order logic with bounded number of logvars. In Section 5 we also comment on safe queries and associated dichotomy theorems from the literature on probabilistic databases.\nA distinguishing characteristic of research on probabilistic databases is the intricate search for languages that lead to tractable inferences. Some of the main differences between our goals and the goals of research on probabilistic databases are already captured by Suciu et al. when they compare probabilistic databases and probabilistic graphical models [123, Section 1.2.7]: while probabilistic databases deal with simple probabilistic modeling and possibly large volumes of data, probabilistic graphical models encode complex probability models whose purpose is to yield conditional probabilities. As we have already indicated in our previous discussion of liftability, our main goal is to understand the connection between features of a knowledge representation formalism and the resulting complexity.\nWe are not so focused on finding tractable cases, even though we are obviously looking for them; in fact, later we present tractability results for the DLLitenf language, results that we take to be are one of the main contributions of this paper.\nQuery or data complexity? The definition of query complexity (Definition 3) reminds one of data complexity as adopted in finite model theory and in database theory [80]. It is thus not surprising that research on probabilistic databases has used the term \u201cdata complexity\u201d to mean the complexity when the database is the only input [123].\nIn the context of Bayesian networks, usually a \u201cquery\u201d is a pair (Q,E) of assignments. We have adopted such a terminology in this paper. Now if Q and E contain all available data, there is no real difference between \u201cquery\u201d and \u201cdata\u201d. Thus we might have adopted the term \u201cdata complexity\u201d in this paper as we only discuss queries that contain all available data.1 However we feel that there are situations where the \u201cquery\u201d is not equal to the \u201cdata\u201d. For instance, in probabilistic relational models one often uses auxiliary grounded relations to indicate which groundings are parents of a given grounding (we return to this in Section 7). And in probabilistic logic programming one can use probabilistic facts to associate probabilities with specific groundings [44, 103, 118]. In these cases there is a distinction between the \u201cquery\u201d (Q,E) and the \u201cdata\u201d that regulate parts of the grounded Bayesian network.\nConsider another possible difference between \u201cquery\u201d and \u201cdata\u201d. Suppose we have a relational Bayesian network specification, a formula \u03c6 whose probability P(\u03c6) is to be computed, and a table with the probabilities for various groundings. Here \u03c6 is the \u201cquery\u201d and the table is the \u201cdata\u201d (this sort of arrangement has been used in description logics [20]). One might then either fix the specification and vary the query and the data (\u201cquery\u201d complexity), or fix the specification and the query and vary the data (\u201cdata\u201d complexity).\nIt is possible that such distinctions between \u201cquery\u201d and \u201cdata\u201d are not found to be of practical value in future work. For now we prefer to keep open the possibility of a fine-grained analysis of complexity, so we use the term \u201cquery complexity\u201d even though our queries are simply sets of assignments containing all available data."}, {"heading": "5 The complexity of relational Bayesian network specifications", "text": "We start with function-free first-order logic with equality, a language we denote by FFFO. One might guess that such a powerful language leads to exponentially hard inference problems. Indeed:\nTheorem 4. INF[FFFO] is PEXP-complete with respect to many-one reductions, regardless of whether the domain is specified in unary or binary notation.\nWe note that Grove, Halpern and Koller have already argued that counting the number of suitably defined distinct interpretations of monadic first-order logic is hard for the class of languages decided by exponential-time counting Turing machines [57, Theorem 4.14]. As they do not present a proof of their counting result (and no similar proof seems to be available in the literature), and as we need some of the reasoning to address query complexity later, we present a detailed proof of Theorem 4 in A.\nWe emphasize that when the domain is specified in binary notation the proof of Theorem 4 only requires relations of arity one. One might hope to find lower complexity classes for fragments of FFFO that go beyond monadic logic but restrict quantification. For instance, the popular description logic ALC restricts quantification to obtain PSPACE-completeness of satisfiability [3]. Inspired by this result, we might consider the following specification language:\nDefinition 5. The language ALC consists of all formulas recursively defined so that X(x ) is a formula where X is a unary relation, \u00ac\u03c6 is a formula when \u03c6 is a formula, \u03c6\u2227\u03d5 is a formula when both \u03c6 and\n1In fact we have used the term data complexity in previous work [26].\n\u03d5 are formulas, and \u2203y : X(x , y) \u2227 Y (y) is a formula when X is a binary relation and Y is a unary relation.\nHowever, ALC does not move us below PEXP when domain size is given in binary notation:\nTheorem 5. INF[ALC] is PEXP-complete with respect to many-one reductions, when domain size is given in binary notation.\nNow returning to full FFFO, consider its query complexity. We divide the analysis in two parts, as the related proofs are quite different:2\nTheorem 6. QINF[FFFO] is PEXP-complete with respect to many-one reductions, when the domain is specified in binary notation.\nTheorem 7. QINF[FFFO] is PP-complete with respect to many-one reductions when the domain is specified in unary notation.\nAs far as domain complexity is concerned, it seems very hard to establish a completeness result for FFFO when domain size is given in binary notation.3 We simply rephrase an ingenious argument by Jaeger [65] to establish:\nTheorem 8. Suppose NETIME 6= ETIME. Then DINF[FFFO] is not solved in deterministic exponential time, when the domain size is given in binary notation.\nAnd for domain size in unary notation:\nTheorem 9. DINF[FFFO] is PP1-complete with respect to many-one reductions, when the domain is given in unary notation.\nTheorem 9 is in essence implied by a major result by Beame et al. [9, Lemma 3.9]: they show that counting the number of interpretations for formulas in the three-variable fragment FFFO3 is #P1complete. The fragment FFFOk consists of the formulas in FFFO that employ at most k logvars (note that logvar symbols may be reused within a formula, but there is a bounded supply of such symbols) [80, Chapter 111]. The proof by Beame et al. is rather involved because they are restricted to three logvars; in A we show that a relatively simple proof of Theorem 9 is possible when there is no bound on the number of logvars, a small contribution that may be useful to researchers.\nIt is apparent from Theorems 4, 5, 6 and 8 that we are bound to obtain exponential complexity when domain size is given in binary notation. Hence, from now on we work with domain sizes in unary notation, unless explicitly indicated.\nOf course, a domain size in unary notation cannot by itself avoid exponential behavior, as an exponentially large number of groundings can be simulated by increasing arity. For instance, a domain with two individuals leads to 2k groundings for a relation with arity k. Hence, we often assume that our relations have bounded arity. We might instead assume that the vocabulary is fixed, as done in finite model theory when studying combined complexity. We prefer the more general strategy where we bound arity; clearly a fixed vocabulary implies a fixed maximum arity.\nWith such additional assumptions, we obtain PSPACE-completeness of inferential complexity. With a few differences, this result is implied by results by Beame et al. in their important paper [9, Theorem 4.1]: they show that counting interpretations with a fixed vocabulary is PSPACE-complete (that is, they focus on combined complexity and avoid conditioning assignments). We present a short proof of Theorem 10 within our framework in A.\n2The query complexity of monadic FFFO seems to be open, both for domain in binary and in unary notation; proofs of Theorems 6 and 7 need relations of arity two.\n3One might think that, when domain size is given in binary notation, some small change in the proof of Theorem 9 would show that DINF[FFFO] is complete for a suitable subset of PEXP. Alas, it does not seem easy to define a complexity class that can convey the complexity of DINF[FFFO] when domain size is in binary notation. Finding the precise complexity class of DINF[FFFO] is an open problem.\nTheorem 10. INF[FFFO] is PSPACE-complete with respect to many-one reductions, when relations have bounded arity and the domain size is given in unary notation.\nNote that the proof of Theorem 7 is already restricted to arity 2, hence QINF[FFFO] is PP-complete with respect to many-one reductions when relations have bounded arity (larger than one) and the domain is given in unary notation.\nWe now turn to FFFOk. As we have already noted, this sort of language has been studied already, again by Beame et al., who have derived their domain and combined complexity [9]. In A we present a short proof of the next result, to emphasize that it follows by a simple adaptation of the proof of Theorem 7:\nTheorem 11. INF[FFFOk] is PP-complete with respect to many-one reductions, for all k \u2265 0, when the domain size is given in unary notation.\nQuery complexity also follows directly from arguments in the proofs of previous results, as is clear from the proof of the next theorem in A:4\nTheorem 12. QINF[FFFOk] is PP-complete with respect to many-one reductions, for all k \u2265 2, when domain size is given in unary notation.\nNow consider domain complexity for the bounded variable fragment; previous results in the literature establish this complexity [9, 133, 135]. In fact, the case k > 2 is based on a result by Beame et al. that we have already alluded to; in A we present a simplified argument for this result.\nTheorem 13. DINF[FFFOk] is PP1-complete with respect to many-one reductions, for k > 2, and is in P for k \u2264 2, when the domain size is given in unary notation.\nThere are important knowledge representation formalisms within bounded-variable fragments of FFFO. An example is the description logic ALC that we have discussed before: every sentence in this description logic can be translated to a formula in FFFO2 [3]. Hence we obtain:\nTheorem 14. Suppose the domain size is specified in unary notation. Then INF[ALC] and QINF[ALC] are PP-complete, and DINF[ALC] is in P.\nAs a different exercise, we now consider the quantifier-free fragment of FFFO. In such a language, every logvar in the body of a definition axiom must appear in the defined relation, as no logvar is bound to any quantifier. Denote this language by QF; in Section 7 we show the close connection between QF and plate models. We have:\nTheorem 15. Suppose relations have bounded arity. INF[QF] and QINF[QF] are PP-complete with respect to many-one reductions, and DINF[QF] requires constant computational effort. These results hold even if domain size is given in binary notation.\nAs we have discussed at the end of Section 4, the literature on lifted inference and on probabilistic databases has produced deep results on query and domain complexity. One example is the definition of safe queries, a large class of formulas with tractable query complexity [32]. Similar classes of formulas have been studied for symmetric probabilistic databases [56]. Based on such results in the literature, one might define the language SAFE consisting of safe queries, or look for similar languages with favorable query complexity. We prefer to move to description logics in the next section, leaving safe queries and related languages to future work; we prefer to focus on languages whose complexity can be determined directly from their constructs (note that a sentence can be decided to be safe in polynomial time, but such a decision requires computational support).\n4The case k = 1 seems to be open; when k = 1, query complexity is polynomial when inference is solely on unary relations [134, 135]. When k = 0 we obtain propositional networks and then query complexity is polynomial by Theorem 3.\n6 Specifications based on description logics: the DLLite language\nThe term \u201cdescription logic\u201d encompasses a rich family of formal languages with the ability to encode terminologies and assertions about individuals. Those languages are now fundamental knowledge representation tools, as they have solid semantics and computational guarantees concerning reasoning tasks [3]. Given the favorable properties of description logics, much effort has been spent in mixing them with probabilities [83].\nIn this section we examine relational Bayesian network specifications based on description logics. Such specifications can benefit from well tested tools and offer a natural path to encode probabilistic ontologies. Recall that we have already examined the description logic ALC in the previous section.\nTypically a description logic deals with individuals, concepts, and roles. An individual like John corresponds to a constant in first-order logic; a concept like researcher corresponds to a unary relation in first-order logic; and a role like buysFrom corresponds to a binary relation in first-order logic. A vocabulary contains a set of individuals plus some primitve concepts and some primitive roles. From these primitive concepts and roles one can define other concepts and roles using a set of operators. For instance, one may allow for concept intersection: then C \u2293D is the intersection of concepts C and D. Likewise, C \u2294D is the union of C and D, and \u00acC is the complement of C. For a role r and a concept C, a common construct is \u2200r.C, called a value restriction. Another common construct is \u2203r.C, an existential restriction. Description logics often define composition of roles, inverses of roles, and even intersection/union/complement of roles. For instance, usually r\u2212 denotes the inverse of role r.\nThe semantics of description logics typically resorts to domains and interpretations. A domain D is a set. An interpretation I maps each individual to an element of the domain, each primitive concept to a subset of the domain, and each role to a set of pairs of elements of the domain. And then the semantics of C \u2293D is fixed by I(C \u2293D) = I(C) \u2229 I(D). Similarly, I(C \u2294D) = I(C) \u222a I(D) and I(\u00acC) = D\\I(C). And for the restricted quantifiers, we have I(\u2200r.C) = {x \u2208 D : \u2200y : (x, y) \u2208 I(r) \u2192 y \u2208 I(C)} and I(\u2203r.C) = {x \u2208 D : \u2203y : (x, y) \u2208 I(r)\u2227y \u2208 I(C)}. The semantics of the inverse role r\u2212 is, unsurprisingly, given by I(r\u2212) = {(x, y) \u2208 D \u00d7D : (y, x) \u2208 I(r)}.\nWe can translate this syntax and semantics to their counterparts in first-order logic. Thus C \u2293D can be read as C(x)\u2227D(x), C \u2294D as C(x) \u2228D(x), and \u00acC as \u00acC(x). Moreover, \u2200r.C translates into \u2200y : r(x, y) \u2192 C(y) and \u2203r.C translates into \u2203y : r(x, y) \u2227 C(y).\nDefinition 5 introduced the language ALC by adopting intersection, complement, and existential restriction (union and value restrictions are then obtained from the other constructs). We can go much further than ALC in expressivity and still be within the two-variable fragment of FFFO; for instance, we can allow for role composition, role inverses, and Boolean operations on roles. The complexity of such languages is obtained from results discussed in the previous section.\nClearly we can also contemplate description logics that are less expressive than ALC in an attempt to obtain tractability. Indeed, some description logics combine selected Boolean operators with restricted quantification to obtain polynomial complexity of logical inferences. Two notable such description logics are EL and DL-Lite; due to their favorable balance between expressivity and complexity, they are the basis of existing standards for knowledge representation.5\nConsider first the description logic EL, where the only allowed operators are intersection and existential restrictions, and where the top concept is available, interpreted as the whole domain [2]. Note that we can translate every sentence of EL into the negation-free fragment of ALC, and we can simulate the top concept with the assessment P(\u22a4 = 1) = 1. Thus we take the language EL as the negation-free fragment of ALC. Because EL contains conjunction, we easily have that INF[EL] is PPhard by Theorem 2. And domain complexity is polynomial as implied by DINF[ALC]. Query complexity requires some additional work as discussed in A; altogether, we have:6\n5Both EL and DL-Lite define standard profiles of the OWL knowledge representation language, as explained at http://www.w3.org/TR/owl2-profiles/.\n6The proof of Theorem 16 uses queries with negative assignments; both the inferential/query complexity of EL are open when the query is restricted to positive assignments.\nTheorem 16. Suppose the domain size is specified in unary notation. Then INF[EL] and QINF[EL] are PP-complete with respect to many-one reductions, even if the query contains only positive assignments, and DINF[EL] is in P.\nWe can present more substantial results when we focus on the negation-free fragment of the popular description logic DL-Lite [19]. DL-Lite is particularly interesting because it captures central features of ER or UML diagrams, and yet common inference services have polynomial complexity [1].\nThe simplicity and computational efficiency of the DL-Lite language have led many researchers to mix them with probabilities. For instance, D\u2019Amato et al. [33] propose a variant of DL-Lite where the interpretation of each sentence is conditional on a context that is specified by a Bayesian network. A similar approach was taken by Ceylan and Pe\u00f1alosa [22], with minor semantic differences. A different approach is to extend the syntax of DL-Lite sentences with probabilistic subsumption connectives, as in the Probabilistic DL-Lite [113]. Differently from our focus here, none of those proposals employ DL-Lite to specify Bayesian networks.\nIn DL-Lite one has primitive concepts as before, and also basic concepts: a basic concept is either a primitive concept, or \u2203r for a role r, or \u2203r\u2212 for a role r. Again, r\u2212 denotes the inverse of r. And then a concept in DL-Lite is either a basic concept, or \u00acC when C is a basic concept, or C \u2293D when C and D are concepts. The semantics of r\u2212, \u00acC and C \u2293D are as before, and the semantics of \u2203r is, unsurprisingly, given by I(\u2203r) = {x \u2208 D : \u2203y : (x, y) \u2208 I(r)}.\nWe focus on the negation-free fragment of DL-Lite; that is, we consider:\nDefinition 6. The language DLLitenf consists of all formulas recursively defined so that X(x ) are formulas when X is a unary relation, \u03c6 \u2227 \u03d5 is a formula when both \u03c6 and \u03d5 are formulas, and \u2203y : X(x , y) and \u2203y : X(y , x ) are formulas when X is a binary relation.\nExample 6. The following definition axioms express a few facts about families:\nfemale(x ) \u2261\u2261 \u00acmale(x ), father(x ) \u2261\u2261 male(x ) \u2227 \u2203y : parentOf(x , y),\nmother(x ) \u2261\u2261 female(x ) \u2227 \u2203y : parentOf(x , y), son(x ) \u2261\u2261 male(x ) \u2227 \u2203y : parentOf(y , x ),\ndaughter(x ) \u2261\u2261 female(x ) \u2227 \u2203y : parentOf(y , x ).\nFor domain D = {1, 2}, this relational Bayesian network is grounded into the Bayesian network in Figure 6.\nWe again have that INF[DLLitenf ] is PP-hard by Theorem 2. However, inferential complexity becomes polynomial when the query is positive:\nTheorem 17. Suppose the domain size is specified in unary notation. Then DINF[DLLitenf ] is in P; also, INF[DLLitenf ] and QINF[DLLitenf ] are in P when the query (Q,E) contains only positive assignments.\nIn proving this result (in A) we show that an inference with a positive query can be reduced to a particular tractable model counting problem. The analysis of this model counting problem is a result of independent interest.\nUsing the model counting techniques we just mentioned, we can also show that a related problem, namely finding the most probable explanation, is polynomial for relational Bayesian network specifications based on DLLitenf . To understand this, consider a relational Bayesian network S based on DLLitenf , a set of assignments E for ground atoms, and a domain size N . Denote by X the set of random variables that correspond to groundings of relations in S. Now there is at least one set of assignments M such that: (i) M contains assignments to all random variables in X that are not mentioned in E; and (ii) P(M,E) is maximum over all such sets of assignments. Denote by MLE(S,E, N) the function problem that consists in generating such a set of assignments M.\nTheorem 18. Given a relational Bayesian network S based on DLLitenf , a set of positive assignments to grounded relations E, and a domain size N in unary notation, MLE(S,E, N) can be solved in polynomial time.\nThese results on DLLitenf can be directly extended in some other important ways. For example, suppose we allow negative groundings of roles in the query. Then most of the proof of Theorem 17 follows (the difference is that the intersection graphs used in the proof do not satisfy the same symmetries); we can then resort to approximations for weighted edge cover counting [82], so as to develop a fully polynomial-time approximation scheme (FPTAS) for inference. Moreover, the MLE(S,E, N) problem remains polynomial. Similarly, we could allow for different groundings of the same relation to be associated with different probabilities; the proofs given in A can be modified to develop a FPTAS for inference.\nWe have so far presented results for a number of languages. Table 1 summarizes most of our findings; as noted previously, most of the results on FFFO with bound on relation arity and on FFFOk have been in essence derived by Beame et al. [9]."}, {"heading": "7 Plates, probabilistic relational models, and related specifica-", "text": "tion languages\nIn this paper we have followed a strategy that has long been cherished in the study of formal languages; that is, we have focused on languages that are based on minimal sets of constructs borrowed from logic. Clearly this plan succeeds only to the extent that results can be transferred to practical specification languages. In this section we examine some important cases where our strategy pays off.\nConsider, for instance, plate models, a rather popular specification formalims. Plate models have been extensively used in statistical practice [84] since they were introduced in the BUGS project [50, 85]. In machine learning, they have been used to convey several models since their first appearance [17].\nThere seems to be no standard formalization for plate models, so we adapt some of our previous concepts as needed. A plate model consists of a set of parvariables, a directed acyclic graph where each node is a parvariable, and a set of template conditional probability distributions. Parvariables are typed: each parameter of a parvariable is associated with a set, the domain of the parameter. All parvariables that share a domain are said to belong to a plate. The central constraint on \u201cstandard\u201d plate models is that the domains that appear in the parents of a parvariable must appear in the parvariable. For a given parvariable X , its corresponding template conditional probability distribution associates a probability value to each value of X given each configuration of parents of X .\nTo make things simple, here we focus on parvariables that correspond to relations, thus every random variable has values true and false (plate models in the literature often specify discrete and even continuous random variables [84, 122]). Our next complexity results do not really change if one allows parvariables to have a finite number of values.\nWe can use the same semantics as before to interpret plate models, with a small change: now the groundings of a relation are produced by running only over the domains of its associated logvars.\nExample 7. Suppose we are interested in a \u201cUniversityWorld\u201d containing a population of students and a population of courses [47]. Parvariable Failed?(x , y) yields the final status of student y in course x ; Difficult?(x ) is a parvariable indicating the difficulty of a course x , and Committed?(y) is a parvariable indicating the commitment of student y .\nA plate model is drawn in Figure 7, where plates are rectangles. Each parvariable is associated with a template conditional probability distribution:\nP(Difficult?(x ) = 1) = 0.3, P ( Committed?(y) = 1 ) = 0.7,\nP\n(\nFailed?(x , y) = 1 \u2223 \u2223 \u2223 \u2223 Difficult?(x ) = d, Committed?(y) = c ) =\n \n 0.4 if d = 0, c = 0; 0.2 if d = 0, c = 1; 0.9 if d = 1, c = 0; 0.8 if d = 1, c = 1.\nNote that plate models can always be specified using definition axioms in the quantifier-free fragment of FFFO, given that the logvars of a relation appear in its parent relations. For instance, the\ntable in Example 7 can be encoded as follows:\nFailed?(x , y) \u2261\u2261\n\n  \n( \u00acDifficult?(x ) \u2227 \u00acCommitted?(y) \u2227 A1(x , y) ) \u2228\n( \u00acDifficult?(x ) \u2227 Committed?(y) \u2227 A2(x , y) ) \u2228 ( Difficult?(x ) \u2227 \u00acCommitted?(y) \u2227 A3(x , y) ) \u2228\n( Difficult?(x ) \u2227 Committed?(y) \u2227 A4(x , y) )\n\n   , (5)\nwhere we introduced four auxiliary parvariables with associated assessments P ( A1(x , y) = 1 ) = 0.4, P ( A2(x , y) = 1 ) = 0.2, P ( A3(x , y) = 1 ) = 0.9, and P ( A4(x , y) = 1 ) = 0.8.\nDenote by INF[PLATE] the language consisting of inference problems as in Definition 2, where relational Bayesian network specifications are restricted to satisfy the constraints of plate models. Adopt QINF[PLATE] and DINF[PLATE similarly. We can reuse arguments in the proof of Theorem 15 to show that:\nTheorem 19. INF[PLATE] and QINF[PLATE] are PP-complete with respect to many-one reductions, and DINF[PLATE] requires constant computational effort. These results hold even if the domain size is given in binary notation.\nOne can find extended versions of plate models in the literature, where a node can have children in other plates (for instance the smoothed Latent Dirichlet Allocation (sLDA) model [12] depicted in Figure 8). In such extended plates a template conditional probability distribution can refer to logvars from plates that are not enclosing the parvariable; if definition axioms are then allowed to specify template distributions, one obtains as before INF[FFFO], QINF[FFFO], etc; that is, results obtained in previous sections apply.\nBesides plates, several other languages can encode repetitive Bayesian networks. Early proposals resorted to object orientation [73, 86], to frames [74], and to rule-based statements [5, 52, 58], all inspired by knowledge-based model construction [61, 53, 138]. Some of these proposals coalesced into a family of models loosely grouped under the name of Probabilistic Relational Models (PRMs) [46]. We adopt PRMs as defined by Getoor et al. [47]; again, to simplify matters, we focus on parvariables that correspond to relations.\nSimilarly to a plate model, a PRM contains typed parvariables and domains. A domain is now called a class; each class appears as a box containing parvariables. For instance, Figure 9 depicts a PRM for the University World: edges between parvariables indicate probabilistic dependence, and dashed edges between classes indicate associations between elements of the classes. In Figure 9 we have classes Course, Student, and Registration, with associations between them. Consider association studentOf: the idea is that studentOf(x , z) holds when x is the student in registration z. Following terminology by Koller and Friedman [71], we say that relations that encode classes and associations, such as Course and studentOf , are guard parvariables.\nA relational skeleton for a PRM is an explicit specification of elements in each class, plus the explicit specification of pairs of objects that are associated. That is, the relational skeleton specifies the groundings of the guard parvariables.\nEach parvariable X in a PRM is then associated with a template probability distribution that specifies probabilities for the parvariable X given a selected set of other parvariables. The latter are the parents of X , again denoted by pa(X). In the University World of Figure 9, we must associate with Failed? the template probabilities for P ( Failed?(z)|Difficult?(x ),Committed?(y) ) . But differently from plate models, here the parents of a particular grounding are determined by going through associations: for instance, to find the parents of Failed?(r), we must find the course c and the student s such that courseOf(c, r) and studentOf(s, r) hold, and then we have parents Difficult?(c) and Committed?(s).\nAll of these types and associations can, of course, be encoded using first-order logic, as long as all parvariables correspond to relations. For instance, here is a definition axiom that captures the PRM for the University World:\nFailed?(z) \u2261\u2261 \u2200x : \u2200y :\n( Course(x ) \u2227 Student(y)\u2227\ncourseOf(x , z) \u2227 studentOf(y , z)\n)\n\u2192\n\n  \n( \u00acDifficult?(x ) \u2227 \u00acCommitted?(y) \u2227 A1(x , y) ) \u2228\n( \u00acDifficult?(x ) \u2227 Committed?(y) \u2227 A2(x , y) ) \u2228 ( Difficult?(x ) \u2227 \u00acCommitted?(y) \u2227 A3(x , y) ) \u2228\n( Difficult?(x ) \u2227 Committed?(y) \u2227 A4(x , y) )\n\n   ,\nusing the same auxiliary parvariables employed in Expression (5). The parvariable graph for the resulting specification is depicted in Figure 10.\nThus we can take a PRM and translate it into a relational Bayesian network specification S. As long as the parvariable graph is acyclic, results in the previous sections apply. To see this, note that a skeleton is simply an assignment for all groundings of the guard parvariables. Thus a skeleton can be encoded into a set of assignments S, and our inferences should focus on deciding P(Q|E,S) > \u03b3 with respect to S and a domain that is the union of all classes of the PRM.\nFor instance, suppose we have a fixed PRM and we receive as input a skeleton and a query (Q,E), and we wish to decide whether P(Q|E) > \u03b3. If the template probability distributions are specified with FFFO, and the parvariable graph is acyclic, then this decision problem is a PP-complete problem. We can replay our previous results on inferential and query complexity this way. The concept of domain complexity seems less meaningful when PRMs are considered: the larger the domain, the more data on guard parvariables are needed, so we cannot really fix the domain in isolation.\nWe conclude this section with a few observations.\nCyclic parvariable graphs Our results assume acyclicity of parvariable graphs, but this is not a condition that is typically imposed on PRMs. A cyclic parvariable graph may still produce an acyclic grounding, depending on the given skeleton. For instance, one might want to model blood-type inheritance, where a Person inherits a genetic predisposition from another Person. This creates a loop around the class Person, even though we do not expect a cycle in any valid grounding of the PRM. The literature has proposed languages that allow cycles [47, 60]; one example is shown in Figure 11. The challenge then is to guarantee that a given skeleton will lead to an acyclic grounded Bayesian network; future work on cyclic parvariable graphs must deal with such a consistency problem.\nOther specification languages There are several other languages that specify PRMs and related formalisms; such languages can be subjected to the same analysis we have explored in this paper. A notable formalism is the Probabilistic Relational Language (PRL) [48], where logic program are used to specify PRMs; the specification is divided into logical background (that is, guard parvariables), probabilistic background, and probabilistic dependencies. Two other examples of textual formalisms that can be used to encode PRMs are Logical Bayesian Networks (LBNs) [43, 42] and Bayesian Logic Programs (BLPs) [69, 112]. Both distinguish between logical predicates that constrain groundings (that is, guard parvariables), and probabilistic or Bayesian predicates that encode probabilistic assessments [93].\nA more visual language, based on Entity-Relationship Diagrams, is DAPER [60]. Figure 12 shows a DAPER diagram for the University World and a DAPER diagram for the blood-type model. Another diagrammatic language is given by Multi-Entity Bayesian Networks (MEBNs), a graphical representation for arbitrary first-order sentences [78]. Several other graphical languages mix probabilities with description logics [21, 24, 39, 72], as we have mentioned in Section 6.\nThere are other formalisms in the literature that are somewhat removed from our framework. For instance, Jaeger\u2019s Relational Bayesian Networks [62, 63] offer a solid representation language where the user can directly specify and manipulate probability values, for instance specifying that a probability value is the average of other probability values. We have examined the complexity of Relational Bayesian Networks elsewhere [88]; some results and proofs, but not all of them, are similar to the ones presented here. There are also languages that encode repetitive Bayesian networks using functional programming [87, 89, 125, 102] or logic programming [25, 44, 103, 104, 114, 117], We have examined the complexity of the latter formalisms elsewhere [27, 28, 29]; again, some results and proofs, but not all of them, are similar to the ones presented here."}, {"heading": "8 A detour into Valiant\u2019s counting hierarchy", "text": "We have so far focused on inferences that compare a conditional probability with a given rational number. However one might argue that the real purpose of a probabilistic inference is to compute a\nprobability value. We can look at the complexity of calculating such numbers using Valiant\u2019s counting classes and their extensions. Indeed, most work on probabilistic databases and lifted inference has used Valiant\u2019s classes. In this section we justify our focus on decision problems, and adapt our results to Valiant\u2019s approach.\nValiant defines, for complexity class A, the class #A to be \u222aL\u2208A(#P)L, where (#P)L is the class of functions counting the accepting paths of nondeterministic polynomial time Turing machines with L as oracle [130]. Valiant declares function f to be #P-hard when #P \u2286 FPf ; that is, f is #P-hard if any function f \u2032 in #P can be reduced to f by the analogue of a polynomial time Turing reduction (recall that FP is the class of functions that can be computed in polynomial time by a deterministic Turing machine). Valiant\u2019s is a very loose notion of hardness; as shown by Toda and Watanabe [128], any function in #PH can be reduced to a function in #P via a one-Turing reduction (where #PH is a counting class with the whole polynomial hierarchy as oracle). Thus a Turing reduction is too weak to distinguish classes of functions within #PH. For this reason, other reductions have been considered for counting problems [8, 41].\nA somewhat stringent strategy is to say that f is #P-hard if any function f \u2032 in #P can be produced from f by a parsimonious reduction; that is, f \u2032(\u2113) is computed by applying a polynomial time function g to x and then computing f(g(\u2113)) [119]. However, such a strategy is inadequate for our purposes: counting classes such as #P produce integers, and we cannot produce integers by computing probabilities.\nA sensible strategy is to adopt a reduction that allows for multiplication by a polynomial function. This has been done both in the context of probabilistic inference with \u201creductions modulo normalization\u201d [75] and in the context of probabilistic databases [123]. We adopt the reductions proposed by Bulatov et al. in their study of weighted constraint satisfaction problems [16]. They define weighted reductions as follows (Bulatov et al. consider functions into the algebraic numbers, but for our purposes we can restrict the weighted reductions to rational numbers):\nDefinition 7. Consider functions f1 and f2 from an input language L to rational numbers Q. A weighted reduction from f1 to f2 a pair of polynomial time functions g1 : L \u2192 Q and g2 : L \u2192 L such that f1(\u2113) = g1(\u2113)f2(g2(\u2113)) for all \u2113.\nWe say a function f is #P-hard with respect to weighted reductions if any function in #P can be reduced to f via a weighted reduction.\nHaving decided how to define hardness, we must look at membership. As counting problems generate integers, we cannot really say that probabilistic inference problems belong to any class in Valiant\u2019s counting hierarchy. In fact, in his seminal work on the complexity of Bayesian networks, Roth\nnotes that \u201cstrictly speaking the problem of computing the degree of belief is not in #P, but easily seem equivalent to a problem in this class\u201d [115]. The challenge is to formalize such an equivalence.\nGrove, Halpern, and Koller quantify the complexity of probabilistic inference by allowing polynomial computations to occur after counting [57, Definition 4.12]. Their strategy is to say that f is #P-easy if there exists f \u2032 \u2208 #P and f \u2032\u2032 \u2208 FP such that for all \u2113 we have f(\u2113) = f \u2032\u2032(f \u2032(\u2113)). Similarly, Campos, Stamoulis and Weyland take f to be #P[1]-equivalent if f is #P-hard (in Valiant\u2019s sense) and belongs to FP#P[1]. Here the superscript #P[1] means that the oracle #P can be called only once. It is certainly a good idea to resort to a new term (\u201cequivalence\u201d) in this context; however one must feel that membership to FP#P[1] is too weak a requirement given Toda and Watanabe\u2019s theorem [128]: any function in #PH can be produced within FP#P[1]. We adopt a stronger notion of equivalence: a function f is #P-equivalent if it is #P-hard with respect to weighted reductions and g \u00b7 f is in #P for some polynomial-time function g from the input language to rational numbers.\nAlso, we need to define a class of functions that corresponds to the complexity class PEXP. We might extend Valiant\u2019s definitions and take #EXP to be \u222aL\u2208EXPFP\nL. However functions in such a class produce numbers whose size is at most polynomial on the size of the input, as the number of accepting paths of a nondeterministic Turing machine on input \u2113 is bounded by 2p(|\u2113|) where p is polynomial and |\u2113| is the length of \u2113. This is not appropriate for our purposes, as even simple specifications may lead to exponentially long output (for instance, take P(X(x ) = 1) = 1/2 and compute P(\u2203x : X(x )): we must be able to write the answer 1\u2212 1/2N using N bits, and this number of bits is exponential on the input if the domain size is given in binary notation). For this reason, we take #EXP to be the class of functions that can be computed by counting machines of exponential time complexity [97]. We say that a function f is #EXP-equivalent if f is #EXP-hard with respect to reductions that follow exactly Definition 7, except for the fact that g1 may be an exponential time function, and gf is in #EXP for some exponential time function g from the input language to the rational numbers.\nNow consider polynomial bounds on space. We will use the class \u266ePSPACE class defined by Ladner [77], consisting of those functions that can be computed by counting Turing machines with a polynomial space bound and a polynomial bound on the number of nondeterministic moves. This class is actually equal to FPSPACE[poly], the class of functions computable in polynomial space whose outputs are strings encoding numbers in binary notation, and bounded in length by a polynomial [77, Theorem 2]. We say that a function is \u266ePSPACE-equivalent if f is \u266ePSPACE-hard with respect to weighted reductions (as in Definition 7), and g \u00b7 f is in #PSPACE for some polynomial space function g from the input language to the rational numbers. Of course we might have used \u201cFPSPACE[poly]-equivalent\u201d instead, but we have decided to follow Ladner\u2019s original notation.\nThere is one more word of caution when it comes to adopting Valiant\u2019s counting Turing machines. It is actually likely that functions that are proportional to conditional probabilities P(Q|E) cannot be produced by counting Turing machines, as classes in Valiant\u2019s counting hierarchy are not likely to be closed under division even by polynomial-time computable functions [95]. Thus we must focus on inferences of the form P(Q); indeed this is the sort of computation that is analyzed in probabilistic databases [123].\nThe drawback of Valiant\u2019s hierarchy is, therefore, that a significant amount of adaptation is needed before that hierarchy can be applied to probabilistic inference. But after all this preliminary work, we can convert our previous results accordingly. For instance, we have:\nTheorem 20. Consider the class of functions that gets as input a relational Bayesian network specification based on FFFO, a domain size N (in binary or unary notation), and a set of assignments Q, and returns P(Q). This class of functions is #EXP-equivalent.\nTheorem 21. Consider the class of functions that gets as input a relational Bayesian network specification based on FFFO with relations of bounded arity, a domain size N in unary notation, and a set of assignments Q, and returns P(Q). This class of functions is \u266ePSPACE-equivalent.\nTheorem 22. Consider the class of functions that gets as input a relational Bayesian network specification based on FFFOk for k \u2265 2, a domain size N in unary notation, and a set of assignments Q, and returns P(Q). This class of functions is #P-equivalent.\nTheorem 23. Consider the class of functions that get as input a plate model based on FFFO, a domain size N (either in binary or unary notation), and a set of assignments Q, and returns P(Q). This class of functions is #P-equivalent."}, {"heading": "9 Conclusion", "text": "We have presented a framework for specification and analysis of Bayesian networks, particularly networks containing repetitive patterns that can be captured using function-free first-order logic. Our specification framework is based on previous work on probabilistic programming and structural equation models; our analysis is based on notions of complexity (inferential, combined, query, data, domain) that are similar to concepts used in lifted inference and in probabilistic databases.\nOur emphasis was on knowledge representation; in particular we wanted to understand how features of the specification language affect the complexity of inferences. Thus we devoted some effort to connect probabilistic modeling with knowledge representation formalisms, particularly description logics. We hope that we have produced here a sensible framework that unifies several disparate efforts, a contribution that may lead to further insight into probabilistic modeling.\nAnother contribution of this work is a collection of results on complexity of inferences, as summarized by Table 1 and related commentary. We have also introduced relational Bayesian network specifications based on the DLLitenf logic, a language that can be used to specify probabilistic ontologies and a sizeable class of probabilistic entity-relationship diagrams. In proving results about DLLitenf , we have identified a class of model counting problems with tractability guarantees. Finally, we have shown how to transfer our results into plate models and PRMs, and in doing so we have presented a much needed analysis of these popular specification formalisms.\nThere are several avenues open for future work. Ultimately, we must reach an understanding of the relationship between expressivity and complexity of Bayesian networks specifications that is as rich as the understanding we now have about the expressivity and complexity of logical languages. We must consider Bayesian networks specified by operators from various description and modal logics, or look at languages that allow variables to have more than two values. In a different direction, we must look at parameterized counting classes [45], so as to refine the analysis even further. There are also several problems that go beyond the inferences discussed in this paper: for example, the computation of Maximum a Posteriori (MAP) configurations, and the verification that a possibly cyclic PRM is consistent for every possible skeleton. There are also models that encode structural uncertainty, say about the presence of edges [71], and novel techniques must be developed to investigate the complexity of such models."}, {"heading": "Acknowledgements", "text": "The first author is partially supported by CNPq (grant 308433/2014-9) and the second author is supported by FAPESP (grant 2013/23197-4). We thank Cassio Polpo de Campos for valuable insights concerning complexity proofs, and Johan Kwisthout for discussion concerning the MAJSAT problem."}, {"heading": "A Proofs", "text": "Proposition 1. Both #3SAT(>) and #(1-in-3)SAT(>) are PP-complete with respect to many-one reductions.\nProof. Consider first #3SAT(>). It belongs to PP because deciding #\u03c6 > k, for propositional sentence \u03c6, is in PP [119, Theorem 4.4]. And it is PP-hard because it is already PP-complete when the input is k = 2n/2 \u2212 1 [7, Proposition 1].\nNow consider #(1-in-3)SAT(>). Suppose the input is a propositional sentence \u03c6 in 3CNF with propositions A1, . . . , An and m clauses. Turn \u03c6 into another sentence \u03d5 in 3CNF by turning each clause L1 \u2228 L2 \u2228 L3 in \u03c6 into a set of four clauses:\n\u00acL1 \u2228B1 \u2228B2, L2 \u2228B2 \u2228B3, \u00acL3 \u2228B3 \u2228B4, B1 \u2228B3 \u2228B5,\nwhere the Bj are fresh propositions not in \u03c6. We claim that #\u03d5 = #(1-in-3)\u03c6; that is, #(1-in-3)\u03c6 > k is equivalent to #\u03c6 > k, proving the desired hardness.\nTo prove this claim in the previous sentence, reason as follows. Define \u03b8(L1, L2, L3) = (L1 \u2227\u00acL2 \u2227 \u00acL3) \u2228 (\u00acL1 \u2227 L2 \u2227 \u00acL3) \u2228 (\u00acL1 \u2227 \u00acL2 \u2227 L3); that is, \u03b8(L1, L2, L3) holds if exactly one of its input literals is true. And for a clause \u03c1 = (L1 \u2228 L2 \u2228 L3), define\n\u03bd(\u03c1) = \u03b8(\u00acL1, B1, B2) \u2227 \u03b8(L2, B2, B3) \u2227 \u03b8(\u00acL3, B3, B4) \u2227 \u03b8(B1, B3, B5),\nwhere each Bj is a fresh proposition not in \u03c6. Note that for each assignment to (L1, L2, L3) that satisfies \u03c1 there is only one assignment to (B1, B2, B3, B4, B5) that satisfies \u03bd(\u03c1). To prove this, Table 2 presents the set of all assignments that satisfy \u03bd(C). Consequently, #\u03c1 = #\u03bd(\u03c1). By repeating this argument for each clause in \u03c6, we obtain our claim.\nTheorem 1. INF[Prop(\u2227)] is in to P when the query (Q,E) contains only positive assignments, and INF[Prop(\u2228)] is in to P when the query contains only negative assignments.\nProof. Consider first INF[Prop(\u2227)]. To run inference with positive assignments (Q,E), just run dseparation to collect the set of root variables that must be true given the assignments (note that as soon as a node is set to true, its parents must be true, and so on recursively). Then the probability of the conjunction of assignments in Q and in E is just the product of probabilities for these latter atomic propositions to be true, and these probabilities are given in the network specification. Thus we obtain P(Q,E). Now repeat the same polynomial computation only using assignments in E, to obtain P(E), and determine whether P(Q,E) /P(E) > \u03b3 or not.\nNow consider INF[Prop(\u2228)]. For any input network specification, we can easily build a network specification in INF[Prop(\u2227)] by turning every variable X into a new variable X \u2032 such that X = \u00acX \u2032. Then the root node associated with assessment P(X = 1) = \u03b1 is turned into a root node associated with P(X \u2032 = 1) = 1 \u2212 \u03b1, and a definition axiom X \u2261\u2261 \u2228iYi is turned into a definition axiom X \u2032 \u2261\u2261 \u2227iY \u2032i . Any negative evidence is then turned into positive evidence, and the reasoning in the previous paragraph applies.\nTheorem 2. INF[Prop(\u2227)] and INF[Prop(\u2228)] are PP-complete with respect to many-one reductions.\nProof. Membership follows from the fact that INF[Prop(\u2227,\u00ac)] \u2208 PP. We therefore focus on hardness. Consider first INF[Prop(\u2227)]. We present a parsimonious reduction from#(1-in-3)SAT(>) to INF[Prop(\u2227)], following a strategy by Mau\u00e1 et al. [38]. Take a sentence \u03c6 in 3CNF with propositions A1, . . . , An and m clauses. If there is a clause with a repeated literal (for instance, (A1 \u2228 A1 \u2228 A2) or (\u00acA1 \u2228 A2 \u2228 \u00acA1)), then there is no assignment respecting the 1-in-3 rule, so the count can be immediately assigned zero. So we assume that no clause contains a repeated literal in the remainder of this proof.\nFor each literal in \u03c6, introduce a random variable Xij , where i refers to the ith clause, and j refers to the jth literal (note: j \u2208 {1, 2, 3}). The set of all such random variables is L.\nFor instance, suppose we have the sentence (A1 \u2228A2 \u2228 A3) \u2227 (A4 \u2228 \u00acA1 \u2228 A3). We then make the correspondences: X11 A1, X12 A2, X13 A3, X21 A4, X22 \u00acA1, X23 A3.\nNote that {Xij = 1} indicates an assignment of true to the corresponding literal. Say that a configuration of L is gratifying if Xi1 +Xi2 +Xi3 \u2265 1 for every clause (without necessarily respecting the 1-in-3 rule). Say that a configuration is respectful if is respects the 1-in-3 rule; that is, if Xi1+Xi2+ Xi3 \u2264 1 for every clause. And say that a configuration is sensible if two variables that correspond to the same literal have the same value, and two variables that correspond to a literal and its negation have distinct values (in the example in the last paragraph, both {X11 = 1, X22 = 1} and {X13 = 1, X23 = 0} fail to produce a sensible configuration).\nFor each random variable Xij , introduce the assessment P(Xij = 1) = 1\u2212 \u03b5, where \u03b5 is a rational number determined later. Our strategy is to introduce definition axioms so that only the gratifyingrespectful-sensible configurations of L get high probability, while the remaining configurations have low probability. The main challenge is to do so without negation.\nLet Q be an initially empty set of assignments. We first eliminate the configurations that do not respect the 1-in-3 rule. To do so, for i = 1, . . . ,m include definition axioms\nYi12 \u2261\u2261 Xi1 \u2227Xi2, Yi13 \u2261\u2261 Xi1 \u2227Xi3, Yi32 \u2261\u2261 Xi2 \u2227Xi3, (6)\nand add {Yi12 = 0, Yi13 = 0, Yi23 = 0} to Q. This guarantees that configurations of L that fail to be respectful are incompatible with Q.\nWe now eliminate gratifying-respectful configurations that are not sensible. We focus on gratifying and respectful configurations because, as we show later, ungratifying configurations compatible with Q are assigned low probability.\n\u2022 Suppose first that we have clause where the same literal appears twice. For instance, suppose we have (Ai \u2228 \u00acAi \u2228 L), where L is a literal. Assume the literals of this clause correspond to variables Xi1, Xi2, and Xi3). Then impose {Xi3 = 0}. All other cases where a clause contains a literal and its negation must be treated similarly.\n\u2022 Now suppose we have two clauses (A\u2228Li2 \u2228Li3) and (\u00acA\u2228Lj2 \u2228Lj3), where A is a proposition and the Luv are literals (possibly referring more than once to the same propositions). Suppose the six literals in these two clauses correspond to variables (Xi1, Xi2, Xi3) and (Xj1, Xj2, Xj3), in this order. We must have Xi1 = 1 \u2212 Xj1. To encode this relationship, we take two steps. First, introduce the definition axiom\nYi1j1 \u2261\u2261 Xi1 \u2227Xj1,\nand add {Yi1j1 = 0} to Q: at most one of Xi1 and Xj1 is equal to 1, but there may still be gratifying-respectful configurations where Xi1 = Xj1 = 0. Thus the second step is enforce the sentence \u03b8 = \u00ac(Li2\u2228Li3)\u2228\u00ac(Lj2 \u2228Lj3), as this forbids Xi1 = Xj1 = 0. Note that \u03b8 is equivalent to \u00ac(Li2 \u2227 Lj2) \u2227 \u00ac(Li2 \u2227 Lj3) \u2227 \u00ac(Li3 \u2227 Lj2) \u2227 \u00ac(Li3 \u2227 Lj3), so introduce the definition axiom\nYiujv \u2261\u2261 Xiu \u2227Xjv\nand add {Yiujv = 0} to Q, for each u \u2208 {2, 3} and v \u2208 {2, 3}. Proceed similarly if the literals of interest appear in other positions in the clauses.\n\u2022 We must now deal with cases where the same literal appears in different positions; recall that no clause contains a repeated literal. So we focus on two clauses that share a literal. Say we have (A\u2228Li2\u2228Li3) and (A\u2228Lj2\u2228Lj3) where the symbols are as in the previous bullet, and where the literals are again paired with variables (Xi1, Xi2, Xi3) and (Xj1, Xj2, Xj3). If Xi1 = 1, then we must have Xj1 = 1, and to guarantee this in a gratifying-respectful configuration we introduce\nYi1j2 \u2261\u2261 Xi1 \u2227Xj2, Yi1j3 \u2261\u2261 Xi1 \u2227Xj3,\nand add {Yi1j2 = 0, Yi1j3 = 0} to Q. Similarly, if Xj1 = 1, we must have Xi1 = 1, so introduce\nYi2j1 \u2261\u2261 Xi2 \u2227Xj1, Yi3j1 \u2261\u2261 Xi3 \u2227Xj1,\nand add {Yi2j1 = 0, Yi3j1 = 0} to Q. Again, proceed similarly if the literals of interest appear in other positions in the clauses.\nConsider a configuration x11, . . . , xm3 of L. If this is a gratifying-respectful-sensible configuration, we have that\nP(X11 = x11, . . . , Xm3 = xm3) = (1\u2212 \u03b5) m\u03b52m = \u03b1 .\nIf the configuration is respectful but not gratifying, then\nP(X11 = x11, . . . , Xm3 = xm3) \u2264 (1\u2212 \u03b5) m\u22121\u03b52m+1 = \u03b2 .\nThe number of respectful configurations is at most 4m, since for each i there are 4 ways to assign values to (Xi1, Xi2, Xi3) such that Xi1 +Xi2 +Xi3 \u2264 1.\nThe whole reasoning is illustrated in the decision tree in Figure 13. If the number of solutions to the original problem is strictly greater than k then P(Q) \u2265 (k + 1)\u03b1. And if the number of solutions is smaller or equal than k then P(Q) \u2264 k\u03b1+4m\u03b2. Now we must choose \u03b5 so that (k + 1)\u03b1 > k\u03b1 + 4m\u03b2, so that we can differentiate between the two cases. We do so by choosing \u03b5 < 1/(1 + 4m). Then (\u03d5, k) is in the language #(1-in-3)SAT(>) iff P(Q) > k\u03b1.\nThe whole construction is polynomial: the number of definition axioms is at most quadratic in the number of literals of \u03d5, and \u03b5 can be encoded with O(m+ n) bits.\nBecause the construction just described is somewhat complicated, we present an example. Consider again the sentence (A1 \u2228 A2 \u2228 A3) \u2227 (A4 \u2228 \u00acA1 \u2228 A3) and the related variables Xij . We introduce definitions enforcing the 1-in-3 rule:\nY112 \u2261\u2261 X11 \u2227X12 Y113 \u2261\u2261 X11 \u2227X13 Y123 \u2261\u2261 X12 \u2227X13 ,\nY212 \u2261\u2261 X21 \u2227X22 Y213 \u2261\u2261 X21 \u2227X23 Y223 \u2261\u2261 X22 \u2227X23 .\nand appropriate assignments in Q. We then guarantee that at most one of A1 and \u00acA1 is true, by introducing Y1122 \u2261\u2261 X11 \u2227X22, and by adding {Y1122 = 0} to Q. Note that are configurations that\nare not sensible but that satisfy the previous constraints: for instance, {L13 = L23 = 1, L11 = L12 = L21 = L22 = 0} is not sensible and has probability \u03b1 = (1 \u2212 \u03b5)2\u03b54. To remove those configurations that are not sensible but that have \u201chigh\u201d probability, we introduce:\nY1221 \u2261\u2261 X12 \u2227X21 , Y1223 \u2261\u2261 X12 \u2227X23 ,\nY1321 \u2261\u2261 X13 \u2227X21 , Y1323 \u2261\u2261 X13 \u2227X23 ,\nY1321 \u2261\u2261 X13 \u2227X21 , Y1322 \u2261\u2261 X13 \u2227X22 ,\nY1123 \u2261\u2261 X11 \u2227X23 , Y1223 \u2261\u2261 X12 \u2227X23 ,\nand we add {E1221 = 0, E1223 = 0, E1321 = 0, E1323 = 0, E1321 = 0, E1322 = 0, E1123 = 0, E1223 = 0} to Q. There are 26 = 64 configurations of X11, . . . , X23, and 15 of them have Xi1 = Xi2 = Xi3 = 0 for i = 1 or i = 2 (or both). Among these ungratifying configurations, 8 do not respect the 1-in-3 rule; the remaining 7 that respect the 1-in-3 rule are assigned at most probability \u03b2. Among the 49 gratifying configurations (i.e., those that assign Xij = 1 for some j for i = 1, 2), 40 do not respect the 1-in-3 rule. Of the remaining 9 configurations, 7 are not sensible. The last 2 configurations are assigned probability \u03b1. We thus have that\nP(Q) = \u2211\nx11,...,x23\nP(X11 = x11, . . . , X23 = x23,Q) \u2264 2\u03b1+ 7\u03b2,\nwhich implies that (\u03d5, 3) is not in#(1-in-3)SAT(>); indeed, there are 2 < 3 assignments toA1, A2, A3, A4 that satisfy \u03d5 and respect the 1-in-3 rule.\nThis concludes our discussion of INF[Prop(\u2227)], so we move to INF[Prop(\u2228)]. To prove its PPcompleteness, we must do almost exactly the same construction described before, with a few changes that we enumerate.\nFirst, we associate each literal with a random variable Xij as before, but now Xij stands for a negated literal. That is, if the literal corresponding to Xij is A and A is true, then {Xij = 0}. Thus we must associate each Xij with the assessment P(Xij = 1) = \u03b5. Definitions must change accordingly: a configuration is now gratifying if Xi1 +Xi2 +Xi3 < 3.\nSecond, the previous construction used a number of definition axioms of the form\nY \u2261\u2261 X \u2227X \u2032,\nwith associated assignment {Y = 0}. We must replace each such pair by a definition axiom\nY \u2261\u2261 X \u2228X \u2032\nand an assignment {Y = 1}; recall that X is just the negation of the variable used in the previous construction, so the overall effect of the constraints is the same.\nAll other arguments carry, so we obtain the desired hardness.\nIt is instructive to look at a proof of Theorem 2 that uses Turing reductions, as it is much shorter than the previous proof:\nProof. To prove hardness of INF[Prop(\u2228)], we use the fact that the function #MON2SAT is #Pcomplete with respect to Turing reductions [131, Theorem 1]. Recall that #MON2SAT is the function that counts the number of satisfying assignments of a monotone sentence in 2CNF.\nSo, we can take any MAJSAT problem where the input is sentence \u03c6 and produce (with polynomial effort) another sentence \u03c6\u2032 in 2CNF such that #\u03c6 is obtained from #\u03c6\u2032 (again with polynomial effort). And we can compute #\u03c6\u2032 using INF[Prop(\u2228)], as follows. Write \u03c6\u2032 as\n\u2227m i=1(Ai1 \u2228Ai2), where each Aij\nis a proposition in A1, . . . , An. Introduce fresh propositions/variables Ci and definition axioms Ci \u2261\u2261 Ai1 \u2228 Ai2 . Also introduce P(Ai = 1) = 1/2 for each Ai, and consider the query Q = {C1, . . . , Cm}. Note that P(Q) > \u03b3 if and only if #\u03c6\u2032 = 2nP(Q) > 2n\u03b3, so we can bracket #\u03c6\u2032. From #\u03c6\u2032 we obtain #\u03c6 and we can decide whether #\u03c6 > 2n\u22121, thus solving the original MAJSAT problem.\nTo prove hardness of INF[Prop(\u2227)], note that the number of satisfying assignments of \u03c6\u2032 in 2CNF is equal to the number of satisfying assignments of\n\u2227m i=1(\u00acAi1 \u2228 \u00acAi2), because one can take each\nsatisfying assignment for the latter sentence and create a satisfying assignment for \u03c6\u2032 by interchanging true and false, and likewise for the unsatisfying assignments. Introduce fresh propositions/variables Ci and definition axioms Ci \u2261\u2261 Ai1 \u2227 Ai2 . Also introduce P(Ai = 1) = 1/2 for each Ai, and consider the query where Q = {\u00acC1, . . . ,\u00acCm}. Again we can bracket the number of assignments that satisfy \u03c6\u2032, and thus we can solve any MAJSAT problem by using INF[Prop(\u2227)] and appropriate auxiliary polynomial computations.\nTheorem 4. INF[FFFO] is PEXP-complete with respect to many-one reductions, regardless of whether the domain is specified in unary or binary notation.\nProof. To prove membership, note that a relational Bayesian network specification based on FFFO can be grounded into an exponentially large Bayesian network, and inference can be carried out in that network using a counting Turing machine with an exponential bound on time. This is true even if we have unbounded arity of relations: even if we have domain size 2N and maximum arity k, grounding each relation generates up to 2kN nodes, still an exponential quantity in the input.\nTo prove hardness, we focus on binary domain size N as this simplifies the notation. Clearly if N is given in unary, then an exponential number of groundings can be produced by increasing the arity of relations (even if the domain is of size 2, an arity k leads to 2k groundings).\nGiven the lack of PEXP-complete problems in the literature, we have to work directly from Turing machines. Start by taking any language L such that \u2113 \u2208 L if and only \u2113 is accepted by more than half of the computation paths of a nondeterministic Turing machine M within time 2p(|\u2113|) where p is a polynomial and |\u2113| denotes the size of \u2113. To simplify matters, denote p(|\u2113|) by n. The Turing machine is defined by its alphabet, its states, and its transition function.\nDenote by \u03c3 a symbol in M\u2019s alphabet, and by q a state of M. A configuration of M can be described by a string \u03c30\u03c31 . . . \u03c3i\u22121(q\u03c3i)\u03c3i+1 . . . \u03c32\nn\u22121, where each \u03c3j is a symbol in the tape, (q\u03c3i) indicates both the state q and the position of the head at cell i with symbol \u03c3i. The initial configuration is (q0\u03c3 0)\u03c31\u2217 . . . \u03c3 m\u22121 \u2217 followed by 2\nn \u2212m blanks, where q0 is the initial state. There are also states qa and qr that respectively indicate acceptance or rejection of the input string \u03c3 0 \u2217 . . . \u03c3 m\u22121 \u2217 . We assume that if qa or qr appear in some configuration, then the configuration is not modified anymore (that is, the transition moves from this configuration to itself). This is necessary to guarantee that the number of accepting computations is equal to the number of ways in which we can fill in a matrix of computation. For instance, a particular accepting computation could be depicted as a 2n \u00d7 2n matrix as in Figure 14, where xy denotes the blank, and where we complete the rows of the matrix after the acceptance by repeating the accepting row.\nThe transition function \u03b4 of M takes a pair (q, \u03c3) consisting of a state and a symbol in the machine\u2019s tape, and returns a triple (q\u2032, \u03c3\u2032,m): the next state q\u2032, the symbol \u03c3\u2032 to be written in the tape (we assume that a blank is never written by the machine), and an integer m in {\u22121, 0, 1}. Here \u22121 means that the head is to move left, 0 means that the head is to stay in the current cell, and 1 means that the head is to move right.\nWe now encode this Turing machine using monadic logic, mixing some ideas by Lewis [79] and by Tobies [126].\nTake a domain of size 22n. The idea is that each x is a cell in the computation matrix. From now on, a \u201cpoint\u201d is a cell in that matrix. Introduce parvariables X0(x ), . . . , Xn\u22121(x ) and Y0(x ), . . . , Yn\u22121(x ) to encode the index of the column and the row of point x . Impose, for 0 \u2264 i \u2264 n\u2212 1, the assessments P(Xi(x ) = 1) = P(Yi(x ) = 1) = 1/2.\nWe need to specify the concept of adjacent points in the computation matrix. To this end we introduce two macros, EAST(x , y) and NORTH(x , y) (note that we do not actually need binary relations here; these expressions are just syntactic sugar). The meaning of EAST(x , y) is that for point x there is a point y that is immediately to the right of x . And the meaning of NORTH(x , y) is that for point x there is a point y that is immediately on top of x .\nEAST(x , y) :=\nn\u22121\u2227\nk=0\n(\u2227k\u22121j=0Xj(x )) \u2192 (Xk(x ) \u2194 \u00acXk(y))\n\u2227 n\u22121\u2227\nk=0\n(\u2228k\u22121j=0\u00acXj(x )) \u2192 (Xk(x ) \u2194 Xk(y))\n\u2227 n\u22121\u2227\nk=0\n(Yk(x ) \u2194 Yk(y)).\nNORTH(x , y) := n\u22121\u2227\nk=0\n(\u2227k\u22121j=0Yj(x )) \u2192 (Yk(x ) \u2194 \u00acYk(y))\n\u2227 n\u22121\u2227\nk=0\n(\u2228k\u22121j=0\u00acYj(x )) \u2192 (Yk(x ) \u2194 Yk(y))\n\u2227 n\u22121\u2227\nk=0\n(Xk(x ) \u2194 Xk(y)).\nNow introduce Z1 \u2261\u2261 ( \u2200x : \u2203y : EAST(x , y) ) \u2227 ( \u2200x : \u2203y : NORTH(x , y) ) ,\nZ2 \u2261\u2261 \u2203x : n\u22121\u2227\nk=0\n(\u00acXk(x ) \u2227 \u00acYk(x )).\nNow if Z1 \u2227 Z2 is true, we \u201cbuild\u201d a square \u201cboard\u201d of size 2n \u00d7 2n (in fact this is a torus as the top row is followed by the bottom row, and the rightmost column is followed by the leftmost column).\nIntroduce a relation Cj for each triplet (\u03b1, \u03b2, \u03b3) where each element of the triplet is either a symbol \u03c3 or a symbol of the form (q\u03c3) for our machine M, and with an additional condition: if (\u03b1, \u03b2, \u03b3) has \u03b2 equal to a blank, then \u03b3 is a blank. Furthermore, introduce a relation Cj for each triple (\u22c4, \u03b2, \u03b3), where \u03b2 and \u03b3 are as before, and \u22c4 is a new special symbol (these relations are needed later to encode the \u201cleft border\u201d of the board). We refer to each Cj as a tile, as we are in effect encoding a domino system [79]. For each tile, impose P(Cj(x ) = 1) = 1/2.\nNow each point must have one and only one tile:\nZ3 \u2261\u2261 \u2200x :\n\n\nc\u22121\u2228\nj=0\nCj(x )\n\n \u2227\n  \u2227\n0\u2264j\u2264c\u22121,0\u2264k\u2264c\u22121,j 6=k\n\u00ac(Cj(x ) \u2227 Ck(x ))\n\n .\nHaving defined the tiles, we now define a pair of relations encoding the \u201chorizontal\u201d and \u201cvertical\u201d constraints on tiles, so as to encode the transition function of the Turing machine. Denote by H\nthe relation consisting of pairs of tiles that satisfy the horizontal constraints and by V the relation consisting of pairs of tiles that satisfy the vertical constraints.\nThe horizontal constraints must enforce the fact that, in a fixed row, a tile (\u03b1, \u03b2, \u03b3) at column i for 0 \u2264 i \u2264 2n \u2212 1 overlaps the tile (\u03b1\u2032, \u03b2\u2032, \u03b3\u2032) at column i+ 1 by satisfying\n((\u03b1, \u03b2, \u03b3), (\u03b1\u2032, \u03b2\u2032, \u03b3\u2032)) : \u03b2 = \u03b1\u2032, \u03b3 = \u03b2\u2032.\nThe vertical constraints must encode the possible computations. To do so, consider a tile t = (\u03b1, \u03b2, \u03b3) at row j, for 0 \u2264 j \u2264 2n \u2212 1, and tile t\u2032 = (\u03b1\u2032, \u03b2\u2032, \u03b3\u2032) at row j + 1, both at the same column. The pair (t, t\u2032) is in V if and only if (a) t\u2032 can be reached from t given the states in the Turing machine; and (b) if t = (\u22c4, \u03b2, \u03b3), then t\u2032 = (\u22c4, \u03b2\u2032, \u03b3\u2032) for \u03b2\u2032 and \u03b3\u2032 that follow from the behavior of M.\nWe distinguish the last row and the last column, as the transition function does not apply to them:\nDX(x ) \u2261\u2261 n\u22121\u2227\nk=0\nXk(x ), DY (x ) \u2261\u2261 n\u22121\u2227\nk=0\nYk(x ).\nWe can now encode the transition function:\nZ4 \u2261\u2261 \u2200x : \u00acDX(x ) \u2192\n\n\nc\u22121\u2227\nj=0\nCj(x ) \u2192 (\u2200y : EAST(x , y) \u2192 \u2228k:(j,k)\u2208HCk(y))\n\n ,\nZ5 \u2261\u2261 \u2200x : \u00acDY (x ) \u2192\n\n\nc\u22121\u2227\nj=0\nCj(x )\u2192(\u2200y : NORTH(x , y) \u2192 \u2228k:(j,k)\u2208V Ck(y))\n\n.\nWe create a parvariable that signals the accepting state:\nZ6 \u2261\u2261 \u2203x : \u2228\nj:Cjcontains qa\nCj(x ).\nFinally, we must also impose the initial conditions. Take the tiles in the first row so that symbols in the input of M are encoded as m tiles, with the first tile t0 = (\u22c4, (q0\u03c30\u2217), \u03c3 1 \u2217) and the following ones tj = (\u03c3j\u22121\u2217 , \u03c3 j \u2217, \u03c3 j+1 \u2217 ) up to t m\u22121 = (\u03c3m\u22122\u2217 , \u03c3 m\u22121 \u2217 , xy). So the next tile will be (\u03c3 m\u22121 \u2217 , xy, xy), and after that all tiles in the first row will contain only blanks. Now take individuals ai for i \u2208 {0, . . . ,m\u22121} and create an assignment {C0i (ai) = 1} for each ai, where C 0 i is the ith tile encoding the initial conditions. Denote by E the set of such assignments.\nNow P ( Z6|E \u2227 \u22276 i=1 Zi ) > 1/2 if and only if the number of correct arrangments of tiles that contain the accepting state is larger than the total number of possible valid arrangements. Hence an inference with the constructed relational Bayesian network specification decides the language L we started with, as desired.\nTheorem 5. INF[ALC] is PEXP-complete with respect to many-one reductions, when domain size is given in binary notation.\nProof. Membership follows from Theorem 4. To prove hardness, consider that by imposing an assessment P ( X(x , y) ) = 1, we transform \u2203y : X(x , y) \u2227 Y (y) into \u2203y : Y (y). This is all we need (together with Boolean operators) to build the proof of PEXP-completeness in Theorem 4. (The inferential complexity of ALC has been derived, with a different proof, by Cozman and Polastro [30].)\nTheorem 6. QINF[FFFO] is PEXP-complete with respect to many-one reductions, when the domain is specified in binary notation.\nProof. Membership is obvious as the inferential complexity is already in PEXP. To show hardness, take a Turing machine M that solves some PEXP-complete problem within 2n steps. That is, there is a PEXP-complete language L such that \u2113 \u2208 L if and only if the input string \u2113 is accepted by more than half of the computation paths of M within time 2n.\nSuch a Turing machineM has alphabet, states and transitions as in the proof of Theorem 4. Assume that M repeats its configuration as soon as it enters into the accepting or the rejecting state, as in the proof of Theorem 4.\nTo encode M we resort to a construction by Gradel [55] where relations of arity two are used. We use: (a) for each state q of M, a unary relation Xq; (b) for each symbol \u03c3 in the alphabet of M, a binary relation Y\u03c3; (c) a binary relation Z. The idea is that Xq(x ) means that M is in state q at computation step x , while Y\u03c3(x , y) means that \u03c3 is the symbol at the yth position in the tape at computation step x , and Z(x , y) means that the machine head is at the yth position in the tape at computation step x . Impose P(Xq(x ) = 1) = P ( Y\u03c3(x , y) = 1 ) = P ( Z(x , y) = 1 ) = 1/2.\nWe use a distinguished relation <, assumed not to be in the vocabulary. This relation is to be a linear order on the domain; to obtain this behavior, just introduce P ( <(x , y) = 1 ) = 1/2 and\nZ1 \u2261\u2261 (\u2200x : \u00ac(x < x )) \u2227\n(\u2200x : \u2200y : \u2200z : (x < y \u2227 y < z) \u2192 x < z) \u2227\n(\u2200x : \u2200y : (x < y) \u2228 (y < x ) \u2228 (x = y)).\nWe will later set evidence on Z1 to force < to be a linear order. The important point is that we can assume that a domain of size 2n is given and all elements of this domain are ordered according to <.\nClearly we can define a successor relation using <:\nsuccessor(x , y) \u2261\u2261 (x < y) \u2227 ( \u00ac\u2203z : (x < z) \u2227 (z < y) ) .\nAlso, we can define a relation that signals the \u201cfirst\u201d individual:\nfirst(x ) \u2261\u2261 \u00ac\u2203y : y < x .\nWe must guarantee that at any given step the machine is in a single state, each cell of the tape has a single symbol, and the head is at a a single position of the tape:\nZ2 \u2261\u2261 \u2200x : \u2228\nq\n Xq(x ) \u2227 \u2227\nq\u2032 6=q\n\u00acXq\u2032(x )\n\n ,\nZ3 \u2261\u2261 \u2200x : \u2200y : \u2228\n\u03c3\n Y\u03c3(x , y) \u2227 \u2227\n\u03c3\u2032 6=\u03c3\n\u00acY\u03c3\u2032 (x , y)\n\n ,\nZ4 \u2261\u2261 \u2200x : (\u2203y : Z(x , y) \u2227 \u2200z : (z 6= y) \u2192 \u00acZ(x , z)).\nWe also have to guarantee that computations do not change the content of a cell that is not visited by the head:\nZ5 \u2261\u2261 \u2200x : \u2200y : \u2200z : \u2227\n\u03c3\n( \u00acZ(x , y) \u2227 Y\u03c3(x , y) \u2227 successor(x , z) ) \u2192 Y\u03c3(z, y).\nWe must encode the changes made by the transition function:\nZ6 \u2261\u2261 \u2200x : \u2200y : \u2200z : \u2227\nq,\u03c3\n( Z(x , y) \u2227 Y\u03c3(x , y) \u2227Xq(x ) \u2227 successor(x , z) ) \u2192\n\u2228\n(q\u2032\u03c3\u2032,1)\u2208\u03b4(q,\u03c3)\n( Xq\u2032(z) \u2227 Y\u03c3\u2032(z, y) \u2227 (\u2200w : successor(y ,w) \u2192 Z(z,w)) )\n\u2228 \u2228\n(q\u2032\u03c3\u2032,0)\u2208\u03b4(q,\u03c3)\n( Xq\u2032(z) \u2227 Y\u03c3\u2032(z, y) \u2227 Z(z, y) )\n\u2228 \u2228\n(q\u2032\u03c3\u2032,1)\u2208\u03b4(q,\u03c3)\n( Xq\u2032(z) \u2227 Y\u03c3\u2032(z, y) \u2227 (\u2200w : successor(w , y) \u2192 Z(z,w)) ) .\nWe must also guarantee that all cells to the right of a blank cell are also blank:\nZ7 \u2261\u2261 \u2200x : \u2200y : \u2200z : Yxy(x , y) \u2227 successor(y , z) \u2192 Yxy(x , z).\nFinally, we must signal the accepting state:\nZ8 \u2261\u2261 \u2203x : Xqa(x ).\nWe have thus created a set of formulas that encode the behavior of the Turing machine. Now take the input string \u2113, equal to \u03c30\u2217 , \u03c3 1 \u2217 , . . . , \u03c3 m\u22121 \u2217 , and encode it as a query as follows. Start by \u201ccreating\u201d the first individual in the ordering by taking the assignment {first(a0) = 1}. Then introduce {Z(a0, a0) = 1} to initialize the head. Introduce {Y\u03c30\n\u2217\n(a0, a0) = 1} to impose the initial condition on the first cell,\nand for each subsequent initial condition \u03c3i\u2217 we set {Y\u03c3i\u2217(a0, ai) = 1} and {successor(ai\u22121, ai) = 1} where ai is a fresh individual. Finally, set {Yxy(a0, am) = 1} and {successor(am\u22121, am) = 1} and {Xq0(a0) = 1}. These assignments are denoted by E.\nNow P ( Z8|E \u2227 \u22278 i=1 Zi ) > 1/2 for a domain of size 2n if and only if the number of interpretations reaching the accepting state is larger than the total number of possible interpretations encoding computation paths.\nTheorem 7. QINF[FFFO] is PP-complete with respect to many-one reductions when the domain is specified in unary notation.\nProof. To prove hardness, take a MAJSAT problem where \u03c6 is in CNF with m clauses and propositions A1, . . . , An. Make sure each clause has at most n literals by removing repeated literals, or by removing clauses with a proposition and its negation). Make sure m = n: if m < n, then add trivial clauses such as A1 \u2228 \u00acA1; if instead n < m, then add fresh propositions An+1, . . . , Am. These changes do not change the output of MAJSAT. Introduce unary relations sat(x ) and impose P(sat(x )) = 1/2. Take a domain {1, . . . , n}; the elements of the domain serve a dual purpose, indexing both propositions and clauses. Introduce relations sat(x ), positiveLit(x , y) and negativeLit(x , y), assessments P(sat(x ) = 1) = P ( positiveLit(x , y) = 1 ) = P ( negativeLit(x , y) = 1 ) = 1/2, and definition axioms:\nclause(x ) \u2261\u2261 \u2203y : (positiveLit(x , y) \u2227 sat(y)) \u2228 (negativeLit(x , y) \u2227 \u00acsat(y)),\nquery \u2261\u2261 \u2200x : clause(x ).\nTake evidence E as follows. For each clause, run over the literals. Consider the ith clause, and its nonnegated literal Aj : set positiveLit(i, j) to true. And consider negated literal \u00acAj : set negativeLit(i, j) to true. Set all other groundings of positiveLit and negativeLit to false. Note that P(E) = 2\u22122n 2\n> 0. Now decide whether P(query = 1|E) > 1/2. If YES, the MAJSAT problem is accepted, if NO, it is not accepted. Hence we have the desired polynomial reduction (the query is quadratic on domain size; all other elements are linear on domain size).\nTo prove membership in PP, we describe a Turing machine M that decides whether P(Q|E) > \u03b3. The machine guesses a truth assignment for each one of the polynomially-many grounded root nodes\n(and writes the guess in the working tape). Note that each grounded root node X is associated with an assessment P(X = 1) = c/d, where c and d are the smallest such integers. Then the machine replicates its computation paths out of the guess on X : there are c paths with identical behavior for guess {X = 1}, and d\u2212 c paths with identical behavior for guess {X = 0}.\nNow the machine verifies whether the set of guessed truth assignment satisfies E; if not, move to state q1. If yes, then verify whether the guessed truth assignment fails to satisfy Q; if not, move to state q2. And if yes, then move to state q3. The key point is that there is a logarithmic space, hence polynomial time, algorithm that can verifiy whether a set of assignments holds once the root nodes are set [80, Section 6.2].\nSuppose that out of N computation paths that M can take, N1 of them reach q1, N2 reach q2, and N3 reach q3. By construction,\nN1/N = 1\u2212 P(E) , N2/N = P(\u00acQ,E) , N3/N = P(Q,E) , (7)\nwhere we abuse notation by taking \u00acQ to mean that some assignment in Q is not true. Note that up to this point we do not have any rejecting nor accepting path, so the specification of M is not complete.\nThe remainder of this proof just reproduces a construction by Park in his proof of PP-completeness for propositional Bayesian networks [36]. Park\u2019s construction adds rejecting/accepting computation paths emanating from q1, q2 and q3. It uses numbers\na = { 1 if \u03b3 < 1/2, 1/(2\u03b3) otherwise,\nb = { (1\u2212 2\u03b3)/(2\u2212 2\u03b3) if \u03b3 < 1/2, 0 otherwise.\nand the smallest integers a1, a2, b1, b2 such that a = a1/a2 and b = b1/b2. Now, out of q1 branch into a2b2 computation paths that immediately stop at the accepting state, and a2b2 computation paths that immediately stop at the rejecting state.7 Out of q2 branch into 2a2b1 paths that immediately stop at the accepting state, and 2(b2 \u2212 b1)a2 paths that immediately stop at the rejecting state. Out of q3 branch into 2a1b2 paths that immediately stop at the accepting state, and 2(a2\u2212a1)b2 paths that immediately stop at the rejecting state. For the whole machine M, the number of computation paths that end up at the accepting state is a2b2N1 + 2a2b1N2 + 2a1b2N3, and the total number of computation paths is a2b2N1+a2b2N1+2b1a2N2+2(b2\u2212b1)a2N2+2a1b2N3+2(a2\u2212a1)b2N3 = 2a2b2N . Hence the number of accepting paths divided by the total number of paths is (N1(1/2)+(b1/b2)N2+(a1/a2)N3)/N . This ends Park\u2019s construction. By combining this construction with Expression (7), we obtain\nN1 2 + b1N2 b2 + a1N3a2 N > 1/2 \u21d4 1\u2212 P(E) 2 + bP(\u00acQ,E) + aP(Q,E) > 1/2\n\u21d4 aP(Q,E) + bP(\u00acQ,E) > P(E) /2 \u21d4 aP(Q|E) + bP(\u00acQ|E) > 1/2,\nas we can assume that P(E) > 0 (otherwise the number of accepting paths is equal to the number of rejecting paths), and then\n{ if \u03b3 < 1/2 : P(Q|E) + 1\u22122\u03b32\u22122\u03b3 (1\u2212 P(Q|E)) > 1/2 \u21d4 P(Q|E) > \u03b3;\nif \u03b3 \u2265 1/2 : (1/(2\u03b3))P(Q|E) > 1/2 \u21d4 P(Q|E) > \u03b3.\nHence the number of accepting computation paths of M is larger than half the total number of computation paths if and only if P(Q|E) > \u03b3. This completes the proof of membership.\nTheorem 8. Suppose NETIME 6= ETIME. Then DINF[FFFO] is not solved in deterministic exponential time, when the domain size is given in binary notation.\n7The number of created paths may be exponential in the numbers a2 and b2; however it is always possible to construct a polynomial sequence of steps that encodes an exponential number of paths (say the number of paths has B bits; then build B distinct branches, each one of them multiplying alternatives so as to simulate an exponential). This sort of branching scheme is also assumed whenever needed.\nProof. Jaeger describes results implying, in case NETIME 6= ETIME, that there is a sentence \u03c6 \u2208 FFFO such that the spectrum of \u03c6 cannot be recognized in deterministic exponential time [65]. Recall: the spectrum of a sentence is a set containing each integer N , in binary notation, such that \u03c6 has a model whose domain size is N [55]. So take N in binary notation, the relational Bayesian network specification A \u2261\u2261 \u03c6, and decide whether P(A) > 0 for domain size N ; if yes, then N is in the spectrum of \u03c6.\nTheorem 9. DINF[FFFO] is PP1-complete with respect to many-one reductions, when the domain is given in unary notation.\nProof. To prove membership, just consider the Turing machine used in the proof of Theorem 7, now with a fixed query. This is a polynomial-time nondeterministic Turing machine that gets the domain size in unary (that is, as a sequence of 1s) and produces the desired output.\nTo prove hardness, take a Turing machine with input alphabet consisting of symbol 1, and that solves a PP1-complete problem in N\nm steps for input consisting of N symbols 1. Take the probabilistic assessment and the definition axioms for successor, first, and Z1 as in the proof of Theorem 6. Now introduce relationsXq, Y\u03c3 and Z as in that proof, with the difference that x is substituted form logvars xi, and likewise y is substituted for m logvars xj . For instance, we now have Z(x1, . . . , xm, y1, . . . , ym). Repeat definition axioms for Z2, . . . , Z8 as presented in the proof of Theorem 6, with appropriate changes in the arity of relations. In doing so we have an encoding for the Turing machine where the computation steps are indexed by a vector [x1, . . . , xm], and the tape is indexed by a vector [y1, . . . , ym]. The remaining problem is to insert the input. To do so, introduce:\nZ9 \u2261\u2261 \u2200x : \u2200y1 : . . .\u2200ym : first(x ) \u2192 \n \u2227\ni\u2208{2,...,m}\nfirst(yi) \u2192 Y1( m logvars \ufe37 \ufe38\ufe38 \ufe37 x , . . . , x , y1, . . . , ym)\n\n\n\u2227\n \u00ac \u2227\ni\u2208{2,...,m}\nfirst(yi) \u2192 Yxy( m logvars \ufe37 \ufe38\ufe38 \ufe37 x , . . . , x , y1, . . . , ym)\n\n .\nNow P ( Z8| \u22279 i=1 Zi ) > 1/2 for a domain of size N if and only if the number of interpretations that set an accepting state to true is larger than half the total number of interpretations encoding computation paths.\nTheorem 10. INF[FFFO] is PSPACE-complete with respect to many-one reductions, when relations have bounded arity and the domain size is given in unary notation.\nProof. To prove membership, construct a Turing machine that goes over the truth assignments for all of the polynomially-many grounded root nodes. The machine generates an assignment, writes it using polynomial space, and verifies whether E can be satisfied: there is a polynomial space algorithm to do this, as we basically need to do model checking in first-order logic [55, Section 3.1.4]. While cycling through truth assignments, keep adding the probabilities of the truth assignments that satisfy E. If the resulting probability for E is zero, reject; otherwise, again go through every truth assignment of the root nodes, now keeping track of how many of them satisfy {Q,E}, and adding the probabilities for these assignments. Then divide the probability of {Q,E} by the probability of E, and compare the result with the rational number \u03b3.\nTo show hardness, consider the definition axiom Y \u2261\u2261 Q1x1 : . . .Qnxn : \u03c6(x1, . . . , xn), where each Qi is a quantifier (either \u2200 or \u2203) and \u03c6 is a quantifier-free formula containing only Boolean operators, a unary relation X , and logvars x1, . . . , xn. The relation X is associated with assessment P(X(x ) = 1) = 1/2. Take domain D = {0, 1} and evidence E = {X(0) = 0, X(1) = 1}. Then P(Y = 1|E) > 1/2 if and only if Q1x1 : . . . Qnxn : \u03c6(x1, . . . , xn) is satisfiable. Deciding the latter satisfiability question is in fact equivalent to deciding the satisfiability of a Quantified Boolean Formula, a PSPACE-complete problem [80, Section 6.5].\nNow consider the bounded variable fragment FFFOk. It is important to notice that if the body of every definition axiom belongs to FFFOk for an integer k, then all definition axioms together are equivalent to a single formula in FFFOk. Hence results on logical inference for FFFOk can be used to derive inferential, query and domain complexities.\nTheorem 11. INF[FFFOk] is PP-complete with respect to many-one reductions, for all k \u2265 0, when the domain size is given in unary notation.\nProof. Hardness is trivial: even Prop(\u2227,\u00ac) is PP-hard. To prove membership, use the Turing machine described in the proof of membership in Theorem 7, with a small difference: when it is necessary to check whether E (or Q \u222a E) holds given a guessed assignment for root nodes, use the appropriate model checking algorithm [136], as this verification can be done in polynomial time.\nTheorem 12. QINF[FFFOk] is PP-complete with respect to many-one reductions, for all k \u2265 2, when domain size is given in unary notation.\nProof. To prove membership, note that QINF[FFFO] is in PP by Theorem 7. To prove hardness, note that the proof of hardness in Theorem 7 uses only FFFO2.\nTheorem 13. DINF[FFFOk] is PP1-complete with respect to many-one reductions, for k > 2, and is in P for k \u2264 2, when the domain size is given in unary notation.\nProof. For k \u2264 2, results in the literature show how to count the number of satisfying models of a formula in polynomial time [133, 135].\nFor k > 2, membership obtains as in the proof of Theorem 9. Hardness has been in essence proved by Beame et al. [9, Lemmas 3.8, 3.9]. We adapt their arguments, simplifying them by removing the need to enumerate the counting Turing machines. Take a Turing machine M that solves a #P1-complete problem in Nm steps for an input consisting of N ones. By padding the input, we can always guarantee that M runs in time linear in the input. To show this, consider that for the input sequence with N ones, we can generate another sequence S(N) consisting of f(N) = (2N + 1)2m\u2308log2 N\u2309 ones. Because (21+log2 N )m \u2265 2m\u2308log2 N\u2309, we have (2N + 1)2mNm > f(N), and consequently S(N) can be generated in polynomial time. Modify M so that the new machine: (a) receives S(N); (b) in linear time produces the binary representation of S(N), using an auxiliary tape;8 (c) then discards the trailing zeroes to obtain 2N + 1; (d) obtains N ; (e) writes N ones in its tape; (f) then runs the original computation in M. Because 2m\u2308log2 N\u2309 \u2265 Nm, we have f(N) > Nm, and consequently the new machine runs in time that is overall linear in the input size f(N), and in space within f(N). Suppose, to be concrete, that the new machine runs in time that is smaller than Mf(N) for some integer M . We just have to encode this machine in FFFO3, by reproducing a clever construction due to Beame et al. [9].\nWe use the Turing machine encoding described in the proof of Theorem 8, but instead of using a single relation Z(x , y) to indicate the head position at step x , we use\nZ1,1(x , y), . . . , ZM,1(x , y), Z1,2(x , y), . . . , ZM,2(x , y),\nwith the understanding that for a fixed x we have that Zi,j(x , y) yields the position y of the head in step x and sub-step i, either in the main tape (tape 1) or in the auxiliary tape (tape 2). So, Z1,j is followed by Z2,j and so on until ZM,j for a fixed step x . Similarly, we use Xtq(x ), Y t,1 \u03c3 (x , y) and Y t,2\u03c3 (x , y) for t \u2208 {1, . . . ,M}. Definition axioms must be changed accordingly; for instance, we have\nZ2 \u2261\u2261 \u2200x : \u2227\nt\n\u2228\nq\n Xtq(x ) \u2227 \u2227\nq\u2032 6=q\n\u00acXtq\u2032(x )\n\n ,\n8For instance: go from left to right replacing pairs 11 by new symbols \u2663\u2665; if a blank is reached in the middle of such a pair, then add a 1 at the first blank in the auxiliary tape, and if a blank is reached after such a pair, then add a 0 at the first blank in the auxiliary tape; then mark the current end of the auxiliary tape with a symbol \u2660 and return from the end of the main tape, erasing it and adding a 1 to the end of the auxiliary tape for each \u2665 in the main tape; now copy the 1s after \u2660 from the auxiliary tape to the main tape (and remove these 1s from the auxiliary tape), and repeat. Each iteration has cost smaller than (U + U + logU)c for some constant c, where U is the number of ones in the main tape; thus the total cost from input of size f(N) is smaller than 3c(f(N) + f(N)/2 + f(N)/4 + . . . ) \u2264 6cf(N).\nand\nZ3 \u2261\u2261 \u2200x : \u2227\nt\n\u2200y : \u2227\nj\u2208{1,2}\n\u2228\n\u03c3\n Y t,j\u03c3 (x , y) \u2227 \u2227\n\u03c3\u2032 6=\u03c3\n\u00acY t,j\u03c3\u2032 (x , y)\n\n .\nAs another example, we can change Z4 as follows. First, introduce auxiliary definition axioms:\nW t1(x ) \u2261\u2261 \u2203y : Z t,1(x , y) \u2227 (\u2200z : (z 6= y) \u2192 \u00acZt,1(x , z)) \u2227 (\u2200z : \u00acZt,2(x , z)),\nW t2(x ) \u2261\u2261 \u2203y : Z t,2(x , y) \u2227 (\u2200z : (z 6= y) \u2192 \u00acZt,2(x , z)) \u2227 (\u2200z : \u00acZt,1(x , z)),\nand then write: Z4 \u2261\u2261 \u2200x : \u2227\nt\nW t1(x ) \u2227W t 2(x ).\nSimilar changes must be made to Z7 and Z8:\nZ7 \u2261\u2261 \u2200x : \u2227\nt\n\u2200y : \u2227\nj\u2208{1,2}\n\u2200z : Y t,j xy (x , y) \u2227 successor(y , z) \u2192 Y t,j xy (x , z),\nZ8 \u2261\u2261 \u2203x : \u2228\nt\nXtqa(x ).\nThe changes to Z5 and Z6 are similar, but require more tedious repetition; we omit the complete expressions but explain the procedure. Basically, Z5 and Z6 encode the transitions of the Turing machine. So, instead of just taking the successor of a computation step x , we must operate in substeps: the successor of step x substep t is x substep t+ 1, unless t = M (in which case we must move to the successor of x , substep 1). We can also capture the behavior of the Turing machine with two transition functions, one per tape, and it is necessary to encode each one of them appropriately. It is enough to have M different versions of Z5 and 2M different versions of Z6, each one of them responsible for one particular substep transition.\nTo finish, we must encode the initial conditions. Introduce:\nlast(x ) \u2261\u2261 \u00ac\u2203y : x < y\nand\nZ9 \u2261\u2261 ( \u2200x : \u2200y : (first(x ) \u2227 \u00aclast(y)) \u2192 Y 1,11 (x , y) )\n\u2227 ( \u2200x : \u2200y : (first(x ) \u2227 last(y)) \u2192 Y 1,1\nxy (x , y)\n)\n\u2227 ( \u2200x : \u2200y : first(x ) \u2192 Y 1,2\nxy (x , y)\n) .\nNow P ( Z8| \u22279 i=1 Zi ) > 1/2 for a domain of size f(N) + 1 if and only if the number of interpretations that set an accepting state to true is larger than half the total number of interpretations encoding computation paths.\nTheorem 15. Suppose relations have bounded arity. INF[QF] and QINF[QF] are PP-complete with respect to many-one reductions, and DINF[QF] requires constant computational effort. These results hold even if domain size is given in binary notation.\nProof. Consider first INF[Q]. To prove membership, take a relational Bayesian network specification S with relations X1, . . . , Xn, all with arity no larger than k. Suppose we ground this specification on a domain of size N . To compute P(Q|E), the only relevant groundings are the ones that are ancestors of each of the ground atoms in Q \u222a E. Our strategy will be to bound the number of such relevant groundings. To do that, take a grounding Xi(a1, . . . , aki) in Q\u222aE, and suppose that Xi is not a root node in the parvariable graph. Each parent Xj of Xi in the parvariable graph may appear in several\ndifferent forms in the definition axiom related to Xi; that is, we may have Xj(x2, x3), Xj(x9, x1), . . . , and each one of these combinations leads to a distinct grounding. There are in fact at most kkii ways to select individuals from the grounding Xi(a1, . . . , aki) so as to form groundings of Xj . So for each parent of Xi in the parvariable graph there will be at most k\nk relevant groundings. And each parent of these parents will again have at most kk relevant groundings; hence there are at most (n \u2212 1)kk relevant groundings that are ancestors of Xi(a1, . . . , aki). We can take the union of all groundings that are ancestors of groundings of Q \u222a E, and the number of such groundings is still polynomial in the size of the input. Thus in polynomial time we can build a polynomially-large Bayesian network that is a fragment of the grounded Bayesian network. Then we can run a Bayesian network inference in this smaller network (an effort within PP); note that domain size is actually not important so it can be specified either in unary or binary notation. To prove hardness, note that INF[Prop(\u2227,\u00ac)] is PP-hard, and a propositional specification can be reproduced within QF.\nNow consider QINF[QF]. To prove membership, note that even INF[QF] is in PP. To prove hardness, take an instance of#3SAT(>) consisting of a sentence \u03c6 in 3CNF, with propositions A1, . . . , An, and an integer k. Consider the relational Bayesian network specification consisting of eight definition axioms:\nclause0(x , y , z) \u2261\u2261 \u00acsat(x ) \u2228 \u00acsat(y) \u2228 \u00acsat(z),\nclause1(x , y , z) \u2261\u2261 \u00acsat(x ) \u2228 \u00acsat(y) \u2228 sat(z),\nclause2(x , y , z) \u2261\u2261 \u00acsat(x ) \u2228 sat(y) \u2228 \u00acsat(z),\n... ...\n...\nclause7(x , y , z) \u2261\u2261 sat(x ) \u2228 sat(y) \u2228 sat(z),\nand P(sat(x ) = 1) = 1/2. Now the query is just a set of assignments Q (E is empty) containing an assignment per clause. If a clause is \u00acA2 \u2228 A3 \u2228 \u00acA1, then take the corresponding assignment {clause2(a2, a3, a1) = 1}, and so on. The #3SAT(>) problem is solved by deciding whether P(Q) > k/2n with domain of size n; hence the desired hardness is proved.\nAnd DINF[QF] requires constant effort: in fact, domain size is not relevant to a fixed inference, as can be seen from the proof of inferential complexity above.\nTheorem 16. Suppose the domain size is specified in unary notation. Then INF[EL] and QINF[EL] are PP-complete with respect to many-one reductions, even if the query contains only positive assignments, and DINF[EL] is in P.\nProof. INF[EL] belongs to PP by Theorem 11 as EL belongs to FFFO2. Hardness is obtained from hardness of query complexity.\nSo, consider QINF[EL]. Membership follows from membership of INF[EL], so we focus on hardness. Our strategy is to reduce INF[Prop(\u2228)] to QINF[EL], using most of the construction in the proof of Theorem 2. So take a sentence \u03c6 in 3CNF with propositions A1, . . . , An and m clauses, and an integer k. The goal is to decide whether #(1-in-3)\u03c6 > k. We can assume that no clause contains a repeated literal.\nWe start by adapting several steps in the proof of Theorem 2. First, associate each literal with a random variable Xij (where Xij stands for a negated literal). In the present proof we use a parvariable X(x ); the idea is that x is the integer 3(i\u2212 1) + j for some i \u2208 {1, . . . , n} and j \u2208 {1, 2, 3} (clearly we can obtain (i, j) from x and vice-versa). Then associate X with the assessment\nP(X(x ) = 1) = \u03b5,\nwhere \u03b5 is exactly as in the proof for INF[Prop(\u2228)]. The next step in the proof of Theorem 2 is to introduce a number of definition axioms of the form Yiuv \u2261\u2261 Xiu \u2228 Xiv, together with assignments {Yiuv = 1}. There are 3m such axioms. Then additional axioms are added to guarantee that configurations are sensible. Note that we can compute in polynomial time the total number of definition axioms that are to be created. We denote this number\nby N , as we will use it as the size of the domain. In any case, we can easily bound N : first, each clause produces 3 definition axioms as in Expression 6; second, to guarantee that configurations are sensible, every time a literal is identical to another literal, or identical to the negation of another literal, four definition axioms are inserted (there are 3m literals, and for each one there may be 2 identical/negated literals in the other m\u2212 1 clauses). Thus we have that N \u2264 3m+ 4\u00d7 3m\u00d7 2(m\u2212 1) = 24m2 \u2212 21m. Suppose we order these definition axioms from 1 to N by some appropriate scheme.\nTo encode these N definition axioms, we introduce two other parvariables Y (x ) and Z(x , y), with definition axiom\nY (x ) \u2261\u2261 \u2203y : Z(x , y) \u2227X(y)\nand assessment P ( Z(x , y) = 1 ) = \u03b7,\nfor some \u03b7 to be determined later. The idea is this. We take a domain with size N , and for each x from 1 to N , we set Z(x , y) to 0 if X(x ) does not appear in the definition axiom indexed by x , and we set Z(x , y) to 1 if X(x ) appears in the definition axiom indexed by x . We collect all these assignments in a set E. Note that E in effect \u201ccreates\u201d all the desired definition axioms by selecting two instances of X per instance of Y .\nNote that if we enforce {Y (x ) = 1} for all x , we obtain the same construction used in the proof of Theorem 2, we one difference: in that proof we had 3m variables Xij , while here we have N variables X(x ) (note that N \u2265 3m, and N > 3m for m > 1).\nConsider grounding this relational Bayesian network specification and computing\nP(X(1) = x1, . . . , X(N) = xN , Y (1) = y1, . . . , Y (N) = yN |E) .\nThis distribution is encoded by a Bayesian network consisting of nodes X(1), . . . , X(N) and nodes Y (1), . . . , Y (N), where all nodes Z(x , y) are removed as they are set by E; also, each node Y (x ) has two parents, and all nodes X(3m+ 1), . . . , X(N) have no children. Denote by L a generic configuration of X(1), . . . , X(3m), and by Q a configuration of Y (1), . . . , Y (N) where all variables are assigned value 1. As in the proof of Theorem 2, we have P(L) = \u03b1 if L is gratifying-sensible-respectful, and P(L) \u2264 \u03b2 if L is respectful but not gratifying. If #(1-in-3)\u03c6 > k, then P(Q|E) = \u2211\nL P(L,Q) \u2265 (k + 1)\u03b1. And\nif #(1-in-3)\u03c6 \u2264 k, then P(Q|E) \u2264 k\u03b1 + 4m\u03b2. Define \u03b41 = (k + 1)\u03b1 and \u03b42 = k\u03b1 + 4m\u03b2 and choose \u03b5 < 1/(1 + 4m) to guarantee that \u03b41 > \u03b42, so that we can differentiate between the two cases with an inference.\nWe have thus solved our original problem using a fixed Bayesian network specification plus a query (Q,E). Hence PP-hardness of QINF[EL] obtains. However, note that Q contains only positive assignments, but E contains both positive and negative assignments. We now constrain ourselves to positive assignments.\nDenote by E1 the assignments of the form {Z(x , y) = 1} in E, and denote by E0 the assignments of the form {Z(x , y) = 0} in E. Consider:\nP(Q|E1) = P(Q|E0,E1)P(E0|E1) + P(Q|E c 0,E1)P(E c 0|E1) ,\nwhere Ec0 is the event consisting of configurations of those variables that appear in E0 such that at least one of these variables is assigned 1 (of course, such variables are assigned 0 in E0).\nWe have that P(Q|E0,E1) = P(Q|E) by definition. And variables in E0 and E1 are independent, hence P(E0|E1) = P(E0) = (1 \u2212 \u03b7)M where M is the number of variables in E0 (so M \u2264 N2). Consequently, P(Ec0|E1) = 1\u2212 (1 \u2212 \u03b7) M . Thus we obtain:\nP(Q|E1) = (1\u2212 \u03b7) MP(Q|E) + (1\u2212 (1\u2212 \u03b7)M )P(Q|Ec0,E1) .\nNow reason as follows. If #(1-in-3)\u03c6 > k, then P(Q|E1) \u2265 (1\u2212\u03b7)M\u03b41. And if #(1-in-3)\u03c6 \u2264 k, then P(Q|E1) \u2264 (1\u2212 (1\u2212 \u03b7)M ) + (1\u2212 \u03b7)M\u03b42. To guarantee that (1\u2212 \u03b7)M\u03b41 > (1\u2212 (1\u2212 \u03b7)M )+ (1\u2212 \u03b7)M\u03b42, we must have (1 \u2212 \u03b7)M > 1/(1 + \u03b41 \u2212 \u03b42). We do so by selecting \u03b7 appropriately. Note first that\n1/(1 + \u03b41 \u2212 \u03b42) \u2208 (0, 1) by our choice of \u03b5; note also that 1 + (x \u2212 1)/M > x1/M for any x \u2208 (0, 1), so select\n1\u2212 \u03b7 > 1 +\n( 1\n1 + \u03b41 \u2212 \u03b42 \u2212 1\n)\n/M ;\nthat is, \u03b7 < (1\u2212 1/(1 + \u03b41 \u2212 \u03b42))/M . By doing so, we can differentiate between the two cases with an inference, so the desired hardness is proved.\nDomain complexity is polynomial because EL is in FFFO2 [133, 135].\nTheorem 17. Suppose the domain size is specified in unary notation. Then DINF[DLLitenf ] is in P; also, INF[DLLitenf ] and QINF[DLLitenf ] are in P when the query (Q,E) contains only positive assignments.\nProof. We prove the polynomial complexity of INF[DLLite] with positive queries by a quadratic-time reduction to multiple problems of counting weighted edge covers with uniform weights in a particular class of graphs. Then we use the fact that the latter problem can be solved in quadratic time (hence the total time is quadratic).\nFrom now on we simply use Q to refer to a set of assignments whose probability is of interest. We first transform the relational Bayesian network specification into an equal-probability model. Collapse each role r and its inverse r\u2212 into a single node r. For each (collapsed) role r, insert variables er \u2261 \u2203r and e\u2212r \u2261 \u2203r\n\u2212; replace each appearance of the formula \u2203r by the variable er, and each appearance of \u2203r\u2212 by e\u2212r . This transformation does not change the probability of Q, and it allows us to easily refer to groundings of formulas \u2203r and \u2203r\u2212 as groundings of er and e\u2212r , respectively.\nObserve that only the nodes with assignments in Q and their ancestors are relevant for the computation of P(Q), as every other node in the Bayesian network is barren [36]. Hence, we can assume without loss of generality that Q contains only leaves of the network. If Q contains only root nodes, then P(Q) can be computed trivially as the product of marginal probabilities which are readily available from the specification. Thus assume that Q assigns a positive value to at least one leaf grounding s(a), where a is some individual in the domain. Then by construction s(a) is associated with a logical sentenceX1\u2227\u00b7 \u00b7 \u00b7\u2227Xk, where eachXi is either a grounding of non-primitive unary relation in individual a, a grounding of a primitive unary relation in a, or the negation of a grounding of a primitive unary relation in a. It follows that P(Q) = P(s(a) = 1|X1 = 1, . . . , Xk = 1)P(Q\u2032) = P(Q\u2032), where Q\u2032 is Q after removing the assignment s(a) = 1 and adding the assignments {X1 = 1, . . . , Xk = 1}. Now it might be that Q\u2032 contains both the assignments {Xi = 1} and {Xi = 0}. Then P(Q) = 0 (this can be verified efficiently). So assume there are no such inconsistencies. The problem of computing P(Q) boils down to computing P(Q\u2032); in the latter problem the node s(a) is discarded for being barren. Moreover, we can replace any assignment {\u00acr(a) = 1} in Q\u2032 for some primitive concept r with the equivalent assignment {r(a) = 0}. By repeating this procedure for all internal nodes which are not groundings of er or e \u2212 r , we end up with a set A containing positive assignments of groundings of roles and of concepts er and e \u2212 r , and (not necessarily positive) assignments of groundings of primitive concepts. Each grounding of a primitive concept or role is (a root node hence) marginally independent from all other groundings in A; hence P(A) = P(B|C) \u220f\ni P(Ai), where each Ai is an assignment to a root node, B are (positive) assignments to groundings of concepts er and e \u2212 r for relations r, and C \u2286 {A1, A2, . . . } are groundings of roles (if C is empty then assume it expresses a tautology). Since the marginal probabilities P(Ai) are available from the specification the joint \u220f\ni P(Ai) can be computed in linear time in the input. We thus focus on computing P(B|C) as defined (if B is empty, we are done). To recap, B is a set of assignments er(a) = 1 and e \u2212 r (b) = 1 and C is a set of assignments r(c, d) = 1 for arbitrary roles r and individuals a, b, c and d. For a role r, let Dr be the set of individuals a \u2208 D such that er(a) = 1 is in B, and let D\u2212r be the set of individuals a \u2208 D such that B contains e\u2212r (a) = 1. Let gr(r) be the set of all groundings of relation r, and let r1, . . . , rk be the roles in the (relational) network. By the factorization property of Bayesian networks it follows that\nP(B|C) = \u2211\ngr(r1)\n\u00b7 \u00b7 \u00b7 \u2211\ngr(rk)\nk\u220f\ni=1\n\u220f\na\u2208Dri\nP(eri(a) = 1|pa(eri(a)),C)\u00d7\n\u220f\na\u2208D\u2212ri\nP ( e\u2212ri (a) = 1|pa(e \u2212 ri (a)),C ) P(gr(rk)|C) ,\nwhich by distributing the products over sums is equal to\nk\u220f\ni=1\n\u2211\ngr(ri)\n\u220f\na\u2208Dr\nP(er(a)=1|pa(er(a)),C)\u00d7\n\u220f\na\u2208D\u2212r\nP ( e\u2212r (a)=1|pa(e \u2212 r (a)),C ) P(gr(rk)|C) .\nConsider an assignment r(a, b) = 1 in C. By construction, the children of the grounding r(a, b) are er(a) and e\u2212r (b). Moreover, the assignment r(a, b) = 1 implies that P(er(a) = 1|pa(er(a)),C) = 1 (for any assignment to the other parents) and P(e\u2212r (b) = 1|pa(er(a)),C) = 1 (for any assignment to the other parents). This is equivalent in the factorization above to removing r(a, b) from C (as it is independent of all other groundings), and removing individuals a from Dr and b from D \u2212 r . So repeat this procedure for every grounding in C until this set is empty (this can be done in polynomial time). The inference problem becomes one of computing\n\u03b3(r) = \u2211\ngr(ri)\n\u220f\na\u2208Dr\nP(er(a) = 1|pa(er(a))) \u220f\na\u2208D\u2212r\nP ( e\u2212r (a) = 1|pa(e \u2212 r (a)) ) P(gr(rk))\nfor every relation ri, i = 1, . . . , k. We will show that this problem can be reduced to a tractable instance of counting weighted edge covers.\nTo this end, consider the graph G whose node set V can be partitioned into sets V1 = {e \u2212 r (a) : a \u2208\nD \\ D\u2212r }, V2 = {er(a) : a \u2208 Dr}, V3 = {e \u2212 r (a) : a \u2208 D \u2212 r }, V4 = {er(a) : a \u2208 D \\ Dr}, and for i = 1, 2, 3 the graph obtained by considering nodes Vi \u222aVi+1 is bipartite complete. An edge with endpoints er(a) and e\u2212r (b) represents the grounding r(a, b); we identify every edge with its corresponding grounding. We call this graph the intersection graph of B with respect to r and D. The parents of a node in the graph correspond exactly to the parents of the node in the Bayesian network. For example, the graph in Figure 15 represents the assignments B = {er(a) = 1, er(b) = 1, er(d) = 1, e\u2212r (b) = 1, e \u2212 r (c) = 1}, with respect to domain D = {a, b, c, d, e}. The black nodes (resp., white nodes) represent groundings in (resp., not in) B. For clarity\u2019s sake, we label only a few edges.\nBefore showing the equivalence between the inference problem and counting edges covers, we need to introduce some graph-theoretic notions and notation. Consider a (simple, undirected) graph G = (V,E). Denote by EG(u) the set of edges incident on a node u \u2208 V , and by NG(u) the open neighborhood of u. For U \u2286 V , we say that C \u2286 E is a U -cover if for each node u \u2208 U there is an edge e \u2208 C incident in u (i.e., e \u2208 EG(u)). For any fixed real \u03bb, we say that \u03bb|C| is the weight of cover C. The partition function of G is Z(G,U, \u03bb) = \u2211\nC\u2208EC(G,U) \u03bb |C|, where U \u2286 V , EC(G,U) is the set\nof U -covers of G and \u03bb is a positive real. If \u03bb = 1 and U = V , the partition function is the number of edge covers. The following result connects counting edge covers to marginal inference in DL-Lite Bayesian networks.\nLemma 1. Let G = (V1, V2, V3, V4, E) be the intersection graph of B with respect to a relation r and domain D. Then \u03b3(r) = Z(G, V2 \u222a V3, \u03b1/(1\u2212 \u03b1))/(1 \u2212 \u03b1)|E|, where \u03b1 = P ( r(x , y) ) .\nProof of Lemma 1. Let B = V2 \u222a V3, and consider a B-cover C. The assignment that sets to true all groundings r(a, b) corresponding to edges in C, and sets to false the remaining groundings of r makes P(er(a) = 1|pa(er(a))) = P(e\u2212r (b) = 1|pa(e \u2212 r (b))) = 1 for every a \u2208 Dr and b \u2208 D \u2212 r ; it makes P(gr(r)) = P(r) |C|\n(1 \u2212 P(r))|E|\u2212|C| = (1 \u2212 \u03b1)|E|\u03b1|C|/(1 \u2212 \u03b1)|C|, which is the weight of the cover C scaled by (1\u2212\u03b1)|E|. Now consider a set of edges C which is not a B-cover and obtains an assignment to groundings gr(r) as before. There is at least one node in B that does not contain any incident edges in C. Assume that node is e(a); then all parents of e(a) are assigned false, which implies that P(er(a) = 1|pa(er(a))) = 0. The same is true if the node not covered is a grounding e\u2212(a). Hence, for each B-cover C the probability of the corresponding assignment equals its weight up to the factor (1\u2212\u03b1)|E|. And for each edge set C which is not a B-cover its corresponding assignment has probability zero.\nWe have thus established that, if a particular class of edge cover counting problems is polynomial, then marginal inference in DL-Lite Bayesian networks is also polynomial. Because the former is shown to be true in B, this concludes the proof of Theorem 17.\nTheorem 18. Given a relational Bayesian network S based on DLLitenf , a set of positive assignments to grounded relations E, and a domain size N in unary notation, MLE(S,E, N) can be solved in polynomial time.\nProof. In this theorem we are interested in finding an assignment X to all groundings that maximizes P(X \u2227E), where E is a set of positive assignments. Perform the substitution of formulas \u2203r and \u2203r\u2212 by logically equivalent concepts er and e\u2212r as before. Consider a non-root grounding s(a) in E which is not the grounding of er or e \u2212 r ; by construction, s(a) is logically equivalent to a conjunction X1\u2227\u00b7 \u00b7 \u00b7\u2227Xk, where X1, . . . , Xk are unary groundings. Because s(a) is assigned to true, any assignment X with nonzero probability assigns X1, . . . , Xk to true. Moreover, since s(a) is an internal node, its corresponding probability is one. Hence, if we include all the assignmentsXi = 1 to its parents in E, the MPE value does not change. As in the computation of inference, we might generate an inconsistency when setting the values of parents; in this case halt and return zero (and an arbitrary assignment). So assume we repeated this procedure until E contains all ancestors of the original groundings which are groundings of unary relations, and that no inconsistency was found. Note that at this point we only need to assign values to nodes which are either not ancestors of any node in the original set E, and to groundings of (collapsed) roles r.\nConsider the groundings of primitive concepts r which are not ancestors of any grounding in E. Setting its value to maximize its marginal probability does not introduce any inconsistency with respect to E. Moreover, for any assignment to these groundings, we can find a consistent assignment to the remaining groundings (which are internal nodes and not ancestors of E), that is, an assignment which assigns positive probability. Since this is the maximum probability we can obtain for these groundings, this is a partial optimum assignment.\nWe are thus only left with the problem of assigning values to the groundings of relations r which are ancestors of E. Consider a relation r such that P(r) \u2265 1/2. Then assigning all groundings of r to true maximizes their marginal probability and satisfies the logical equivalences of all groundings in E. Hence, this is a maximum assignment (and its value can be computed efficiently). So assume there is a relation r with P(r) < 1/2 such that a grounding of er or e \u2212 r appear in E. In this case, the greedy assignment sets every grounding of r; however, such an assignment is inconsistent with the logical equivalence of er and e \u2212 r , hence obtains probability zero. Now consider an assignment that assigns exactly one grounding r(a, b) to true and all the other to false. This assignment is consistent\nwith er(a) and er(b), and maximizes the probability; any assignment that sets more groundings to true has a lower probability since it replaces a term 1 \u2212 P(r) \u2265 1/2 with a term P(r) < 1/2 in the joint probability. More generally, to maximize the joint probability we need to assign to true as few groundings r(a, b) which are ancestors of E as possible. This is equivalent to a minimum cardinality edge covering problem as follows.\nFor every relation r in the relational network, construct the bipartite complete graphGr = (V1, V2, E) such that V1 is the set of groundings er(a) that appears and have no parent r(a, b) in E, and V2 is the set of groundings e\u2212r (a) that appears and have no parents in E. We identify an edge connecting er(a) and e\u2212r (b) with the grounding r(a, b). For any set C \u2286 E, construct an assignment by attaching true to the groundings r(a, b) in C and false to every other grounding r(a, b). This assignment is consistent with E if and only if C is an edge cover; hence the minimum cardinality edge cover maximizes the joint probability (it is consistent with E and attaches true to the least number of groundings of r). This concludes the proof of Theorem 18.\nTheorem 19. INF[PLATE] and QINF[PLATE] are PP-complete with respect to many-one reductions, and DINF[PLATE] requires constant computational effort. These results hold even if the domain size is given in binary notation.\nProof. Consider first INF[PLATES]. To prove membership, take a plate model with relationsX1, . . . , Xn. Suppose we ground this specification on a domain of size N . To compute P(Q|E), the only relevant groundings are the ones that are ancestors of each of the ground atoms in Q \u222a E. Our strategy will be to bound the number of such relevant groundings. To do that, take a grounding Xi(a1, . . . , aki) in Q \u222a E, and suppose that Xi is not a root node. Each parent Xj of Xi may appear once in the definition axiom related to Xi. And each parent of these parents will again have a limited number of parent groundings; in the end there are at most (n \u2212 1) relevant groundings that are ancestors of Xi(a1, . . . , aki). We can take the union of all groundings that are ancestors of groundings of Q \u222a E, and the number of such groundings is still polynomial in the size of the input. Thus in polynomial time we can build a polynomially-large Bayesian network that is a fragment of the grounded Bayesian network. Then we can run a Bayesian network inference in this smaller network (an effort within PP); note that domain size is actually not important so it can be specified either in unary or binary notation. To prove hardness, note that INF[Prop(\u2227,\u00ac)] is PP-hard, and a propositional specification can be reproduced within PLATES.\nNow consider QINF[PLATES]. First, to prove membership, note that even INF[PLATES] is in PP. To prove hardness, reproduce the proof of Theorem 15 by encoding a #3SAT(>) problem, specified by sentence \u03c6 and integer k, with the definition axioms:\nclause0(x , y , z) \u2261\u2261 \u00acleft(x ) \u2228 \u00acmiddle(y) \u2228 \u00acright(z),\nclause1(x , y , z) \u2261\u2261 \u00acleft(x ) \u2228 \u00acmiddle(y) \u2228 right(z),\nclause2(x , y , z) \u2261\u2261 \u00acleft(x ) \u2228middle(y) \u2228 \u00acright(z),\n... ...\n...\nclause7(x , y , z) \u2261\u2261 left(x ) \u2228middle(y) \u2228 right(z),\nequal(x , y , z) \u2261\u2261 left(x ) \u2194 middle(y) \u2194 right(z),\nand P(left(x ) = 1) = P(middle(x ) = 1) = P(right(x ) = 1) 1/2. The resulting plate model is depicted in Figure 16. The query is again just a set of assignments Q (E is empty) containing an assignment per clause. If a clause is \u00acA2 \u2228 A3 \u2228 \u00acA1, then take the corresponding assignment {clause2(a2, a3, a1) = 1}, and so on. Moreover, add the assignments {equal(ai, ai, ai) = 1} for each i \u2208 {1, . . . , n}, to guarantee that left, middle and right have identical truth assignments for all elements of the domain. The #3SAT(>) is solved by deciding whether P(Q) > k/2n with domain of size n; hence the desired hardness is proved.\nAnd DINF[PLATES] requires constant effort: in fact, domain size is not relevant to a fixed inference, as can be seen from the proof of inferential complexity above.\nTheorem 20. Consider the class of functions that gets as input a relational Bayesian network specification based on FFFO, a domain size N (in binary or unary notation), and a set of assignments Q, and returns P(Q). This class of functions is #EXP-equivalent.\nProof. Build a relational Bayesian network specification as in the proof of Theorem 4. Note that the p = P ( E \u2227 \u22276\ni=1 Zi\n)\nis the probability that a tiling is built satisfying all horizontal and vertical\nrestrictions and the initial condition, and moreover containing the accepting state qa. If we can recover the number of tilings of the torus from this probability, we obtain the number of accepting computations of the exponentially-bounded Turing machine we started with. Assume we have p. There are 22n elements in our domain; if the plate model is grounded, there are 22n(2n+ c) grounded root random variables, hence there are 22 2n(2n+c) interpretations. Hence p\u00d722\n2n(2n+c) is the number of truth assignments that build the board satisfying all horizontal and vertical constraints and the initial conditions. However, this number is not equal to the number of tilings of the board. To see this, consider the grounded Bayesian network where each a in the domain is associated with a \u201cslice\u201d containing groundings Xi(a), Yi(a), Cj(a) and so on. If a particular configuration of these indicator variables corresponds to a tiling, then we can produce the same tiling by permuting all elements of the domain with respect to the slices of the network. Intuitively, we can fix a tiling and imagine that we are labelling each point of the torus with an element of the domain; clearly every permutation of these labels produces the same tiling (this intuition is appropriate because each a corresponds to a different point in the torus). So, in order to produce the number of tilings of the torus, we must compute p \u00d7 22 2n(2n+c)/(22n!), where we divide the number of satisfying truth assignments by the number of repeated tilings.\nTheorem 21. Consider the class of functions that gets as input a relational Bayesian network specification based on FFFO with relations with bounded arity, a domain size N in unary notation, and a set of assignments Q, and returns P(Q). This class of functions is \u266ePSPACE-equivalent.\nProof. First we describe a counting Turing machine that produces a count proportional to P(Q) using a polynomial number of nondeterministic guesses. This nondeterministic machine guesses a truth assignment for each one of the polynomially-many grounded root nodes (and writes the guess in the working tape). Note that each grounded root node X is associated with an assessment P(X = 1) = c/d, where c and d are integers. The machine must replicate its computation paths to handle such rational assessments exactly as in the proof of Theorem 7. The machine then verifies, in each computation path, whether the guessed truth assignment satisfies Q; if it does, then accept; if not, then reject. Denote by R the number of grounded root nodes and by #A the number of accepting paths of this machine; then P(Q) = #A/2R.\nNow we show that Q is \u266ePSPACE-hard with respect to weighted reductions. Define \u03d5(x1, . . . , xm)\nto be a quantified Boolean formula with free logvars x1, . . . , xm:\n\u2200y1 : Q2y2 : . . .QMxM : \u03c6(x1, . . . , xm),\nwhere each logvar can only be true or false, each Qj is a quantifier (either \u2200 or \u2203). And define #\u03d5 to be the number of instances of x1, . . . , xm such that \u03d5(x1, . . . , xm) is true. Denote by \u266eQBF the function that gets a formula \u03d5(x1, . . . , xm) and returns #\u03d5; Ladner shows that \u266eQBF is \u266ePSPACE-complete [77, Theorem 5(2)]. So, adapt the hardness proof of Theorem 10: introduce the definition axiom\nY \u2261\u2261 \u2200y1 : . . . Qmym : \u03c6 \u2032(X1, . . . , Xm),\nwhere \u03c6\u2032 has the same structure of \u03c6 but logvars are replaced as follows. First, each xj is replaced by a relation Xj of arity zero (that is, a proposition). Second, each logvar yj is replaced by the atom X(yj) where X is a fresh unary relation. These relations are associated with assessments P(Xj = 1) = 1/2 and P(X(x ) = 1) = 1/2. This completes the relational Bayesian network specification. Now for domain {0, 1}, first compute P(Q) for Q = {Y = 1, X(0) = 0, X(1) = 1} and then compute 2m(P(Q) /(1/4)). The latter number is the desired value of \u266eQBF; note that P(Q) /(1/4) = P(Y = 1|X(0) = 0, X(1) = 1).\nTheorem 22. Consider the class of functions that gets as input a relational Bayesian network specification based on FFFOk for k \u2265 2, a domain size N in unary notation, and a set of assignments Q, and returns P(Q). This class of functions is #P-equivalent.\nProof. Hardness is trivial: even Prop(\u2227,\u00ac) is #P-equivalent, as Prop(\u2227,\u00ac) suffice to specify any propositional Bayesian network, and equivalence then obtains [115]. To prove membership, use the Turing machine described in the proof of membership in Theorem 11 without assignments E (that is, the machine only processes Q) and without Park\u2019s construction. At the end the machine produces the number #A of computation paths that satisfy Q; then return #A/2R, where R is the number of grounded root nodes.\nTheorem 23. Consider the class of functions that get as input a plate model based on FFFO, a domain size N in unary notation, and a set of assignments Q, and returns P(Q). This class of functions is #P-equivalent.\nProof. Hardness is trivial: a propositional Bayesian network can be encoded with a plate model. To prove membership, build the same fragment of the grounded Bayesian network as described in the proof of Theorem 19: inference with the plate model is then reduced to inference with this polynomially large Bayesian network."}, {"heading": "B A tractable class of model counting problems", "text": "\u201cModel counting\u201d usually refers to the problem of counting the number of satisfying truth-value assignments of a given Boolean formula. Many problems in artificial intelligence and combinatorial optimization can be either specialized to or generalized from model counting. For instance, propositional satisfiability (i.e., the problem of deciding whether a satisfying truth-value assignment exists) is a special case of model counting; probabilistic reasoning in graphical models such as Bayesian networks can be reduced to a weighted variant of model counting [6, 36]; validity of conformal plans can be formulated as model counting [96]. Thus, characterizing the theoretical complexity of the problem is both of practical and theoretical interest.\nIn unrestricted form, the problem is complete for the class #P (with respect to various reductions). Even very restrictive versions of the problem are complete for #P. For example, the problem is #Pcomplete even when the formulas are in conjunctive normal form with two variables per clause, there is no negation, and the variables can be partitioned into two sets such that no clause contains two variables in the same block [110]. The problem is also #P-complete when the formula is monotone and each\nvariable appears at most twice, or when the formula is monotone, the clauses contain two variables and each variables appears at most k times for any k \u2265 5 [129]. A few tractable classes have been found: for example, Roth [115] developed an algorithm for counting the number of satisfying assignments of formulas in conjunctive normal form with two variables per clause, each variable appearing in at most two clauses. Relaxing the constraint on the number of variables per clauses takes us back to intractability: model counting restricted to formulas in conjunctive normal form with variables appearing in at most two clauses is #P-complete [14].\nResearchers have also investigated the complexity with respect to the graphical representation of formulas. Computing the number of satisfying assingments for monotone formulas in conjunctive normal form, with at most two variables per clause, with each variable appearing at most four times is #P-complete even when the primal graph (where nodes are variables and an edge connects variables that coappear in a clause) is bipartite and planar [129]. The problem is also#P-complete for monotone conjunctive normal form formulas whose primal graph is 3-regular, bipartite and planar. In fact, even deciding whether the number of satisfying assignments is even (i.e., counting modulo two) in conjunctive normal form formulas where each variable appears at most twice, each clause has at most three variables, and the incidence graph (where nodes are variables and clauses, and edges connect variables appearing in clauses) of the formula is planar is known to be NP-hard by a randomized reduction [140]. Interestingly, counting the number of satisfying assignments modulo seven (!) of that same class of formulas is polynomial-time computable [132].\nIn this appendix, we present another class of tractable model counting problems defined by its graphical representation. In particular, we develop a polynomial-time algorithm for formulas in monotone conjunctive normal form whose clauses can be partitioned into two sets such that (i) any two clauses in the same set have the same number of variables which are not shared between them, and (ii) any two clauses in different sets share exactly one variable. These formulas lead to intersection graphs (where nodes are clauses, and edges connect clauses which share variables) which are bipartite complete. We state our result in the language of edge coverings; the use of a graph problem makes communication easier with no loss of generality.\nThe basics of model counting and the particular class of problems we consider are presented in B.1. We then examine the problem of counting edge covers in black-and-white graphs in B.2, and describe a polynomial-time algorithm for counting edge covers of a certain class of black-and-white graphs in B.3. Restrictions are removed in B.4, and we comment on possible extensions of the algorithms in B.5.\nB.1 Model counting: some needed concepts\nSay that two clauses do not intersect if the variables in one clause do not appear in the other. If X is the largest set of variables that appear in two clauses, we say that the clauses intersect (at X). For instance, the clauses X1 \u2228X2 \u2228X3 and \u00acX2 \u2228\u00acX4 intersect at {X2}. A clause containing k variables is called a k-clause, and k is called the size of the clause. The degree of a variable in a CNF formula is the number of clauses in which either the variable or its negation appears. A CNF formula where every variable has degree at most two is said read-twice. If any two clauses intersect in at most one variable, the formula is said linear. The formula (X1 \u2228X2)\u2227 (\u00acX1 \u2228\u00acX3) is a linear read-twice 2CNF containing two 2-clauses that intersect at X1. The degree of X1 is two, while the degree of either X2 or X3 is one. To recap, a formula is monotone if no variable appears negated, such as in X1 \u2228X2.\nWe can graphically represent the dependencies between variables and clauses in a CNF formula in many ways. The incidence graph of a CNF formula is the bipartite graph with variable-nodes and clause-nodes. The variable-nodes correspond to variables of the formula, while the clause-nodes correspond to clauses. An edge is drawn between a variable-node and a clause-node if and only if the respective variable appears in the respective clause. The primal graph of a CNF formula is a graph whose nodes are variables and edges connect variables that co-appear in some clause. The primal graph can be obtained from the incidence graph by deleting clause-nodes (along with their edges) and pairwise connecting their neighbors. The intersection graph of a CNF formula is the graph whose nodes correspond to clauses, and an edge connects two nodes if and only if the corresponding clauses\nintersect. The intersection graph can be obtained from the incidence graph by deleting variable-nodes and pairwise connecting their neighbors. Figure 17 shows examples of graphical illustrations of a Boolean formula. We represent clauses as rectangles and variables as circles.\nA CNF formula \u03c6 is satisfied by an assignment \u03c3 (written \u03c3 |= \u03c6) if each clause contains either a nonnegated variable Xi such that \u03c3(Xi) = 1 or a negated variable Xj such that \u03c3(Xj) = 0. In this case, we say that \u03c3 is a model of \u03c6. For monotone CNF formulas, this condition simplifies to the existence of a variable Xi in each clause for which \u03c3(Xi) = 1. Hence, monotone formulas are always satisfiable (by the trivial model that assigns every variable the value one). The model count of a formula \u03c6 is the number Z(\u03c6) = |{\u03c3 : \u03c3 \u03c6}| of models of the formula. The model counting problem is to compute the model count of a given CNF formula \u03c6.\nIn this appendix, we consider linear monotone CNF formulas whose intersection graph is bipartite complete, and such that all clauses in the same part have the same size. These assumptions imply that each variable appears in at most two clauses (hence the formula is read-twice). We call CNF formulas satisfying all of these assumptions linear monotone clause-bipartite complete (LinMonCBPC) formulas. Under these assumptions, we show that model counting can be performed in quadratic time in the size of the input. It is our hope that in future work some of these assumptions can be relaxed. However, due to the results mentioned previously, we do not expect that much can be relaxed without moving to #P-completeness.\nThe set of model counting problems generated by LinMonCBPC formulas is equivalent to the following problem. Take integers m,n,M,N such that N > n > 0 and M > m > 0, and compute how many {0, 1}-valued matrices of size M -by-N exist such that (i) each of the first m rows has at least one cell with value one, and (ii) each of the first n columns has at least one cell with value one. Call Aij the value of the ith row, jth column. The problem is equivalent to computing the number of matrices AM\u00d7N with \u2211N j=1 Aij > 0, for i = 1, . . . ,m, and \u2211M i=1 Aij > 0, for j = 1, . . . , n. This problem can be\nencoded as the model count of the CNF formula whose clauses are\nA11 \u2228 A12 \u2228 \u00b7 \u00b7 \u00b7 \u2228 A1n \u2228 \u00b7 \u00b7 \u00b7 \u2228 A1N ,\nA21 \u2228 A22 \u2228 \u00b7 \u00b7 \u00b7 \u2228 A2n \u2228 \u00b7 \u00b7 \u00b7 \u2228 A2N ,\n...\nAm1 \u2228 An2 \u2228 \u00b7 \u00b7 \u00b7 \u2228 Amn \u2228 \u00b7 \u00b7 \u00b7 \u2228A1N ,\nA11 \u2228 A21 \u2228 \u00b7 \u00b7 \u00b7 \u2228Am1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 AM1,\n...\nA1n \u2228 A2n \u2228 \u00b7 \u00b7 \u00b7 \u2228 Amn \u2228 \u00b7 \u00b7 \u00b7 \u2228 AMN .\nThe first m clauses are the row constraints, while the last n clauses are the columns constraints. The row constraints have size n, and the column constraints have size m. The ith row constraint intersects with the jth column constraint at the variable Aij . For example, given integers m = 3, n = 2,M = 5, N = 6, the equivalent model counting problem has clauses\n\u03c61 : A11 \u2228 A12 \u2228 A13 \u2228 A14 \u2228A15 \u2228 A16,\n\u03c62 : A21 \u2228 A22 \u2228 A23 \u2228 A24 \u2228A25 \u2228 A26,\n\u03c63 : A31 \u2228 A32 \u2228 A33 \u2228 A34 \u2228A35 \u2228 A36,\n\u03c64 : A11 \u2228 A21 \u2228 A31 \u2228 A41 \u2228A51,\n\u03c65 : A12 \u2228 A22 \u2228 A32 \u2228 A42 \u2228A52.\nThe intersection graph of that formula is show in Figure 18. Note that for the complexity of both problems be equivalent we must have the integers in the matrix problem be given in unary notation (otherwise building the equivalent formula takes time exponential in the input).\nB.2 Counting edge covers and its connection to model counting\nA black-and-white graph (bw-graph) is a triple G = (V,E, \u03c7) where (V,E) is a simple undirected graph and \u03c7 : V \u2192 {0, 1} is binary valued function on the node set (assume 0 means white and 1 means black).9 We denote by EG(u) the set of edges incident in a node u, and NG(u) the open neighborhood of u (i.e., not including u). Let G = (V,E, \u03c7) be a bw-graph. An edge e = (u, v) \u2208 E can be classified into one of three categories:10\n\u2022 free edge: if \u03c7(u) = \u03c7(v) = 0;\n\u2022 dangling edge: if \u03c7(u) 6= \u03c7(v); or\n9In [81] and [82], graphs are uncolored, but edges might contain empty endpoints. These are analogous to white node endpoints in our terminology. We prefer defining coloured graphs and allow only simple edges to make our framework close to standard graph theory terminology.\n10The classifications of edges given here are analogous to those defined in [81, 82], but not fully equivalent. Regular edges are analogous to the normal edges defined in [81, 82].\n\u2022 regular edge: if \u03c7(u) = \u03c7(v) = 1.\nIn the graph in Figure 15(b), the edge (f, g) is a dangling edge while the edge (g, j) is a free edge. The edge (f, g) in the graph in Figure 15(a) is a regular edge.\nAn edge cover of a bw-graph G is a set C \u2286 E such that for each node v \u2208 V with \u03c7(v) = 1 there is at least one edge e \u2208 C incident in it. An edge cover for the graph in Figure 15(a) is {(a, d), (d, g), (e, g), (f, g), (h, j)}. We denote by Z(G) the number of edge covers of a bw-color graph G. Computing Z(G) is #P-complete [18], and admits an FPTAS [81, 82].\nConsider a LinMonCBPC formula and let (L,R,ELR) be its intersection graph, where L and R are the two partitions. Call sL and sR the sizes of a clause in L and R, respectively (by construction, all clauses in the same part have the same size), and let kL = sL \u2212 |R| and kR = sR \u2212 |L|. The value of kL+kR is the number of variables that appear in a single clause. Since the graph is bipartite complete, kL, kR \u2265 0. Obtain a bw-graph G = (V1 \u222a V2 \u222a V3 \u222a V4, E, \u03c7) such that\n1. V1 = {1, . . . , kL}, V2 = L, V3 = R and V4 = {1, . . . , kR};\n2. All nodes in V1 \u222a V4 are white, and all nodes in V2 \u222a V3 are black;\n3. There is an edge connecting (u, v) in E for every u \u2208 V1 and v \u2208 V2, for every (u, v) \u2208 ELR, and for every u \u2208 V3 and v \u2208 V4.\nWe call B the family of graphs that can obtained by the procedure above. Figure 15(a) depicts an example of a graph in B obtained by applying the procedure to the formula represented in the Figure 18. By construction, for any two nodes u, v \u2208 Vi, i = 1, . . . , 4, it follows that NG(u) = NG(v) and (u, v) 6\u2208 E. The following result shows the equivalence between edge covers and model counting.\nProposition 2. Consider a LinMonCBPC formula \u03c6 and suppose that G = (V1, V2, V3, V4, E, \u03c7) is a corresponding bw-graph in B. Then number of edge covers of G equals the model counting of \u03c6, that is, Z(G) = Z(\u03c6).\nProof. Let ui denote the node in G corresponding to a clause \u03c6i in \u03c6. Label each edge (ui, vj) for \u03c6i \u2208 L and \u03c6j \u2208 R with the variable corresponding to the intersection of the two clauses. For each \u03c6i \u2208 L, label each dangling edge (u, ui) incident in ui with a different variable that appears only at \u03c6i. Similarly, label each dangling edge (uj , u) with a different variable that appears only at \u03c6j \u2208 R. Note that the labeling function is bijective, as every variable in \u03c6 labels exactly one edge of G.\nNow consider a satisfying assignment \u03c3 of \u03c6 and let C be set of edges labeled with the variables Xi such that \u03c3(Xi) = 1. Then C is an edge cover since every clause (node in G) has at least one variable (incident edge) with \u03c3(Xi) = 1 and the corresponding edge is in C. To show the converse holds, consider an edge cover C for G, and construct an assignment such that \u03c3(Xi) = 1 if the edge labeled by Xi is in C and \u03c3(Xi) = 0 otherwise. Then \u03c3 satisfies \u03c6, since for every clause \u03c6i (node ui) there is a variable in \u03c6i with \u03c3(Xi) (incident edge in ui in C). Since there are as many edges as variables, the correspondence between edge covers and satisfying assignment is one-to-one.\nB.3 A dynamic programming approach to counting edge covers\nIn this section we derive an algorithm for computing the number of edge covers of a graph in B. Let e be an edge and u be a node in G = (V,E, \u03c7). We define the following operations and notation:\n\u2022 edge removal: G\u2212 e = (V,E \\ {e}, \u03c7).\n\u2022 node whitening: G\u2212 u = (V,E, \u03c7\u2032), where \u03c7\u2032(u) = 0 and \u03c7\u2032(v) = \u03c7(v) for v 6= u.\nNote that these operations do not alter the node set, and that they are associative (e.g., G\u2212 e\u2212 f = G\u2212 f \u2212 e, G\u2212 u\u2212 v = G\u2212 v \u2212 u, and G\u2212 e\u2212 u = G\u2212 u\u2212 e). Hence, if E = {e1, \u00b7 \u00b7 \u00b7 , ed} is a set of edges, we can write G\u2212E to denote G\u2212 e1\u2212 \u00b7 \u00b7 \u00b7\u2212 ed applied in any arbitrary order. The same is true\nfor node whitening and for any combination of node whitening and edge removal. These operations are illustrated in the examples in Figure 15.\nThe following result shows that the number of edge covers can be computed recursively on smaller graphs:\nProposition 3. Let e = (u, v) be a dangling edge with u colored black. Then:\nZ(G) = 2Z(G\u2212 e\u2212 u)\u2212 Z(G\u2212 EG(u)\u2212 u) .\nProof. There are Z(G\u2212 e\u2212 u) edge covers of G that contain e and Z(G\u2212 e) edge covers that do not contain e. Hence, Z(G) = Z(G\u2212e\u2212u)+Z(G\u2212e). Now, consider the graph G\u2032 = G\u2212e\u2212u. There are Z(G\u2212 e) edge covers of G\u2032 that contain at least one edge of EG\u2032(u) and Z(G\u2212EG(u)\u2212u) edge covers that contain no edge of EG\u2032(u). Thus Z(G \u2212 e \u2212 u) = Z(G \u2212 e) + Z(G \u2212 EG(u) \u2212 u). Substituting Z(G\u2212 e) in the first identity gives us the desired result.\nFree edges and isolated white nodes can be removed by adjusting the edge count correspondingly:\nProposition 4. We have:\n1. Let e = (u, v) be a free edge of G. Then Z(G) = 2Z(G\u2212 e).\n2. If u is an isolated white node (i.e., NG(u) = \u2205) then Z(G) = Z(G\u2212 u).\nProof. (1) If C is an edge cover of G \u2212 e then both C and C \u222a {e} are edge covers of G. Hence, the number of edge covers containing e equals the number Z(G \u2212 e) of edge covers not containing e. (2) Every edge cover of G is also an edge cover of G\u2212 u and vice-versa.\nWe can use the formulas in Propositions 3 and 4 to compute the edge cover count of a graph recursively. Each recursion computes the count as a function of the counts of two graphs obtained by the removal of edges and whitening of nodes. Such a naive approach requires an exponential number of recursions (in the number of edges or nodes of the initial graph) and finishes after exponential time. We can transform such an approach into a polynomial-time algorithm by exploiting the symmetries of the graphs produced during the recursions. In particular, we take advantage of the invariance of edge cover count to isomorphisms of a graph, as we discuss next.\nWe say that two bw-graphs G = (V,E, \u03c7) and G\u2032 = (V \u2032, E\u2032, \u03c7\u2032) are isomorphic if there is a bijection \u03b3 from V to V \u2032 (or vice-versa) such that (i) \u03c7(v) = \u03c7\u2032(\u03b3(v)) for all v \u2208 V , and (ii) (u, v) \u2208 E if and only if (\u03b3(u), \u03b3(v)) \u2208 E\u2032. In other words, two bw-graphs are isomorphic if there is a color-preserving renaming of nodes that preserves the binary relation induced by E. The function \u03b3 is called an isomorphism from V to V \u2032. The graphs in Figures 15(b) and 15(c) are isomorphic by an isomorphism that maps g in h and maps any other node into itself. If C is an edge cover of G and \u03b3 is an isomorphism between G and G\u2032, then C\u2032 = {(\u03b3(u), \u03b3(v)) : (u, v) \u2208 C} is an edge cover for G\u2032 and vice-versa. Hence, Z(G) = Z(G\u2032). The following result shows how to obtain isomorphic graphs with a combination of node whitenings and edge removals.\nProposition 5. Consider a bw-graph G with nodes v1, . . . , vn, where NG(v1) = \u00b7 \u00b7 \u00b7 = NG(vn) 6= \u2205 and \u03c7G(v1) = \u00b7 \u00b7 \u00b7 = \u03c7G(vn). For any node w \u2208 NG(v1), mapping \u03b3 : {v1, . . . , vn} \u2192 {v1, . . . , vn}, and nonnegative integers k1 and k2 such that k1 + k2 \u2264 n the graphs G\u2032 = G \u2212 EG(v1)\u2212 \u00b7 \u00b7 \u00b7 \u2212 EG(vk1) \u2212 (w, vk1+1) \u2212 \u00b7 \u00b7 \u00b7 \u2212 (w, vk1+k2) \u2212 v1 \u2212 \u00b7 \u00b7 \u00b7 \u2212 vk1+k2 and G\n\u2032\u2032 = G \u2212 EG(\u03b3(v1)) \u2212 \u00b7 \u00b7 \u00b7 \u2212 EG(\u03b3(vk1)) \u2212 (w, \u03b3(vk1+1))\u2212 \u00b7 \u00b7 \u00b7 \u2212 (w, \u03b3(vk1+k2))\u2212 \u03b3(v1)\u2212 \u00b7 \u00b7 \u00b7 \u2212 \u03b3(vk1+k2) are isomorphic.\nProof. Let \u03b3\u2032 be the bijection on the nodes of G that extends \u03b3, that is, \u03b3\u2032(u) = u for u 6\u2208 {v1, . . . , vn} and \u03b3\u2032(u) = \u03b3(vi), for i = 1, . . . , n. We will show that \u03b3\n\u2032 is an isomorphism from G\u2032 to G\u2032\u2032. First note that \u03c7G(u) = \u03c7G(\u03b3(u)) for every node u. The only nodes that have their color (possibly) changed in G\u2032 with respect to G are the nodes v1, . . . , vk1+k+2, and these are white nodes in G\n\u2032. Likewise, the only nodes that would (possibly) changed color in G\u2032\u2032 were \u03b3(v1), . . . , \u03b3(vk1+k2) and these are white in G\u2032\u2032. Hence, \u03c7G\u2032(u) = \u03c7G\u2032\u2032(\u03b3(u)) for every node u.\nNow let us look at the edges. First note that since NG(vi) is constant through i = 1, . . . , n, G \u2032 and G\u2032\u2032 have the same number of edges. Hence, it suffices to show that for each edge (u, v) in G\u2032 the edge (\u03b3\u2032(u), \u03b3\u2032(v)) is in G\u2032\u2032. The only edges modified in obtaining G\u2032 and G\u2032\u2032 are, respectively, those incident in v1, . . . , vk1+k2 and in \u03b3(v1), . . . , \u03b3(vk1+k2). Consider an edge (u, v) where u, v 6\u2208 {v1, . . . , vn} (hence not in EG(vi) for any i). If (u, v) = (\u03b3\n\u2032(u), \u03b3\u2032(v)) is in G\u2032 then it is also in G\u2032\u2032. Now consider an edge (u, vi) in G where u 6\u2208 {w, vk1+1, . . . , vn} and k1 < i \u2264 k1 + k2. Then (u, vi) is in G\n\u2032 and (\u03b3\u2032(u), \u03b3\u2032(vi)) is in G\u2032\u2032. Note that u could be in NG(vi) for k1 + k2 < i \u2264 n.\nAccording to the proposition above, the graphs in Figures 15(b) and 15(c) are isomorphic by a mapping from g to h (and with w = i). Hence, the number of edge covers in either graph is the same.\nThe algorithms RightRecursion and LeftRecursion described in Figures 20 and 21, respectively, exploit the isomorphisms described in Proposition 5 in order to achieve polynomial-time behavior when using the recursions in Propositions 3 and 4. Either algorithm requires a base white node w and integers k1 and k2 specifying the recursion level (with the same meaning as in Proposition 5). Unless k1 + k2 equals the number of neighbors of w in the original graph, a call to either algorithm generates two more calls to the same algorithm: one with the graph obtained by removing edge (w, vh) and whitening vh, and another by removing edges E(vh) and whitening vh. Assume that |V2| \u2265 |V3| (if |V3| > |V2| we can simply manipulate node sets to obtain an isomorphic graph satisfying the assumption). The RightRecursion algorithm first checks whether the value for the current recursion level has been already computed; if yes, then it simply returns the cached value; otherwise it uses the formula in Proposition 3 (and possibly the isomorphism in Proposition 5) and generates two calls of the same algorithm on smaller graphs (i.e. with fewer edges) to compute the edge cover counting for the current graph and stores the result in memory. The recursion continues until the recursion levels equates with the number of nodes in V3, in which case it checks for free edges and isolated nodes, removes them and computes the correction factor 2k, where k is the number of free edges, and calls the algorithm LeftRecursion to start a new recursion. At this point the graph in the input is bipartite complete and contains only nodes in V1 and V2. The latter algorithm behaves very similarly to the former except at the termination step. When all neighbors vh of w have been whitened the graph no longer contains black nodes, and the corresponding edge cover count can be directly computed using the formulas in Proposition 4. Note that a different cache function must be used when we call LeftRecursion from RightRecursion (this can be done by instantiating an object at that point and passing it as argument; we avoid stating the algorithm is this way to avoid cluttering).\nNote that the algorithms do not use the color of nodes, which hence does not need to be stored or manipulated. In fact the node whitening operations (\u2212vh or \u2212uh) performed when calling the recursion are redundant and can be neglected without altering the soundness of the procedure (we decided to leave these operations as they make the connection with Proposition 3 more clear).\nFigure 22 shows the recursion diagram of a run of RightRecursion. Each box in the figure represents a call of the algorithm with the graph drawn as input. The left child of each box is the call RightRecursion(G\u2212(vh, w)\u2212vh, w, k1, k2+1), and the right child is the call RightRecursion(G\u2212EG(vh)\u2212 vh, w, k1+1, k2). For instance, the topmost box represents RightRecursion(G0, w, 0, 0), which computes"}, {"heading": "8: end if", "text": ""}, {"heading": "7: end if", "text": "Z(G0) as the sum of 2Z(G1) and \u2212Z(G24), which are obtained, respectively, from the calls corresponding to its left and right children. The number of the graph in each box corresponds to the order in which each call was generated. Solid arcs represent non cached calls, while dotted arcs indicate cached calls. For instance, by the time RightRecursion(G24, w, 1, 0) is called, RightRecursion(G13, w, 0, 0) has already been computed so the value of Z(G13) is simply read from memory and returned. When called in the graph in the top, with to rightmost node w, and integers k1 = k2 = 0, the algorithm computes the partition function Z(G0) as the sum of 2Z(G1) and \u2212Z(G24), where G1 is obtained from the removal of edge (v1, w) and whitening of v1, while G24 is obtained by removing edges EG1(v1) and whitening of v1. The recursion continues until all incident edges on w have been removed, at which point it removes free edges and isolated nodes and calls LeftRecursion. The recursion diagram for the call of LeftRecursion(G4, w, 0, 0) where w is the top leftmost node of G4 in the figure is shown in Figure 23. The semantics of the diagram is analogous. Note that the recursion of LeftRecursion eventually reaches a graph with no black nodes, for which the edge cover count can be directly computed (in closed-form).\nIn these diagrams, it is possible to see how the isomorphisms stated in Proposition 5 are used by the algorithms and lead to polynomial-time behavior. For instance, in the run in Figure 22, the graph G13 is not the graph obtained from G24 by removing edge (v2, w) and whitening v2 but instead is isomorphic to it. Note that both G13 and its isomorphic graph obtained as the left child of G24 were obtained by one operation of edge removal \u2212(w, vi) and one operation of neighborhood removal \u2212E(vi), plus node whitenings of v1 and v2. Hence, Proposition 5 guarantees their isomorphism.\nThe polynomial-time behavior of the algorithms strongly depends on caching the calls (dotted arcs) and exploiting known isomorphisms. For instance, in the run in Figure 22, the graph G13 is not the graph obtained from G24 by removing edge (v2, w) and whitening v2 but instead is isomorphic to it. Note that both G13 and its isomorphic graph obtained as the left child of G24 were obtained by one operation of edge removal (w, vi) and one operation of neighborhood removal E(vi), plus node whitenings of v1 and v2. Hence, Proposition 5 guarantees their isomorphism.\nWithout the caching of computations, the algorithm would perform exponentially many recursive calls (and its corresponding diagram would be a binary tree with exponentially many nodes). The use of caching allows us to compute only one call of RightRecursion for each configuration of k1, k2 such that k1+k2 \u2264 n, resulting in at most \u2211n i=0(i+1) = (n+1)(n+2)/2 = O(n\n2) calls for RightRecursion, where n = |V3|. Similarly, each call of LeftRecursion requires at most \u2211m i=0(i+1) = (m+1)(m+2)/2 = O(m\n2) recursive calls for LeftRecursion, wherem = |V2|. Each call to RightRecursion with k1+k2 = n generates a call to LeftRecursion (there are n+ 1 such configurations). Hence, the overall number of recursions (i.e., call to either function) is\n(n+ 1)(n+ 2)\n2 + (n+ 1)\n(m+ 1)(m+ 2)\n2 = O(n2 + n \u00b7m2) .\nThis leads us to the following result.\nTheorem 24. Let G be a graph in B with w \u2208 V4 6= \u2205. Then the call RightRecursion(G,w, 0, 0) outputs Z(G) in time and memory at most cubic in the number of nodes of G.\nProof. Except when k1 + k2 = n, RightRecursion calls the recursion given in Proposition 3 with the isomorphisms in Proposition 5 (any graph obtained fromG by k1 operations\u2212EG(vi) and k2 operations \u2212(w, vi) are isomorpohic). For k1 + k2, any edge left connecting a node in V3 and a node in V4 must be a free edge (since all nodes in V4 have been whitened), hence they can be removed according to Proposition 4 with the appropriate correction of the count. By the same result, any isolated node can be removed. When the remaining nodes in V3 are transfered to V1, the resulting graph is bipartite complete (with white nodes in one part and black nodes in the other). Hence, we can call LeftRecursion, which is guaranteed to compute the correct count by the same arguments.\nThe cubic time and space behavior is due to RightRecursion and LeftRecursion being called at most O(n2) and O(nm2), respectively, and by the fact that each call consists of local operations (edge removals and node whitenings) which take at most linear time in the number of nodes and edges of the graph.\nB.4 Graphs with no dangling edges\nThe algorithm RightRecursion requires the existence of a dangling edge. Now it might be that the graph contains no white nodes (hence no dangling edges), that is, that G is bipartite complete graph for V2 \u222a V3. The next result shows how to decompose the problem of counting edge covers in smaller graphs that either contain dangling edges, or are also bipartite complete.\nProposition 6. Let G be a bipartite complete bw-graph with all nodes colored black and e = (u, v) be some edge. Then Z(G) = 2Z(G\u2212 e\u2212u\u2212 v)\u2212Z(G\u2212EG(v)\u2212 v)\u2212Z(G\u2212EG(u)\u2212u)\u2212Z(G\u2212EG(u)\u2212 EG(v)\u2212 u\u2212 v).\nProof. The edge covers of G can be partitioned according to whether they contain the edge e. The number of edge covers that contain e is not altered if we color both u and v white. Thus, Z(G) = Z(G \u2212 e \u2212 u \u2212 v) + Z(G \u2212 e). Let e1, . . . , en be the edges incident in u other than e, and f1, . . . , fm be the edges incident in v other than v. We have that Z(G\u2212 e\u2212 u\u2212 v) = Z(G\u2212 e\u2212 u\u2212 v) + Z(G\u2212 EG(u)\u2212 u)+Z(G\u2212 e)+Z(G\u2212EG(u)\u2212EG(v)\u2212 u\u2212 v). Substituting Z(G\u2212 e) into the first equation obtains the result.\nIn the result above, the graphs G \u2212 e \u2212 u \u2212 v, G \u2212 EG(v) \u2212 v and G \u2212 EG(u) \u2212 u are in B and contain dangling edges, while the graph G\u2212 EG(u)\u2212 EG(v) \u2212 u\u2212 v is bipartite complete. Note that Proposition 5 can be applied to show that altering the edges on which the operations are applied lead to isomorphic graphs. A very similar algorithm to LeftRecursion, implementing the recursion in the result above in polynomial-time can be easily derived.\nB.5 Extensions\nPrevious results can be used beyond the class of graphs B. For instance, the algorithms can compute the edge cover count for any graph that can be obtained from a graph G in B by certain sequences of edge removals and node whitenings, which includes graphs not in B. Graphs that satisfy the properties of the class B except that every node in V2 (or V4 or both) are pairwise connected can also have their edge cover count computed by the algorithm (as this satisfies the conditions in Proposition 5). Another possibility is to consider graphs which can be decomposed in graphs B by polynomially many applications of Proposition 3.\nWe can also consider more general forms of counting problems. A simple mechanism for randomly generating edge covers is to implement a Markov Chain with starts with some trivial edge cover (e.g. one containing all edges) and moves from an edge cover Xt to an edge cover Xt+1 by the following Glauber Dynamics-type move: (1) Select an edge e uniformly at random; (2a) if e 6\u2208 Xt, make Xt+1 = Xt \u222a {e} with probability \u03bb/(1 + \u03bb); (2b) if e \u2208 Xt and if Xt \\ {e} is an edge cover, make Xt+1 = Xt \\ {e} with probability 1/(1 + \u03bb); (2c) else make Xt+1 = Xt. The above Markov chain can be shown to be ergodic and to converge to a stationary distribution which samples an edge cover C with probability \u03bb|C| [13, 11]. When \u03bb = 1, the algorithm performs uniform sampling of edge covers. A related problem is to compute the total probability mass that such an algorithm will assign to sets of edge covers given a bw-graph G, the so-called partition function: Z(G, \u03bb) = \u2211\nC\u2208EC(G) \u03bb |C|, defined\nfor any real \u03bb > 0, where EC(G) is the set of edge covers of G. For \u03bb = 1 the problem is equivalent to counting edge covers. This is also equivalent to weighted model counting of LinMonCBPC formulas with uniform weight \u03bb.\nThe following results are analogous to Propositions 3 and 4 for computing the partition function:\nProposition 7. The following assertions are true:\n1. Let e = (u, v) be a free edge of G. Then Z(G) = (1 + \u03bb)Z(G \u2212 e).\n2. If u is an isolated white node (i.e., NG(u) = \u2205) then Z(G) = Z(G\u2212 u).\n3. Let e = (u, v) be a dangling edge with u colored black. Then Z(G) = (1+\u03bb)Z(G\u2212e\u2212u)\u2212Z(G\u2212 EG(u)\u2212 u).\nHence, by modifying the weights by which the the recursive calls are multiplied, we easily modify algorithms RightRecursion and LeftRecursion () so as to compute the partition function of graphs in B (or equivalently, the partition function of LinMonCBPC formulas)."}], "references": [{"title": "The DL-Lite family and relations", "author": ["A. Artale", "D. Calvanese", "R. Kontchakov", "M. Zakharyashev"], "venue": "Journal of Artificial Intelligence Research 36 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Terminological cycles in a description logic with existential restrictions", "author": ["F. Baader"], "venue": "in: IJCAI", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Basic description logics", "author": ["F. Baader", "W. Nutt"], "venue": "in: Description Logic Handbook, Cambridge University Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Representing and Reasoning with Probabilistic Knowledge: A Logical Approach", "author": ["F. Bacchus"], "venue": "MIT Press, Cambridge", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "Using first-order probability logic for the construction of Bayesian networks", "author": ["F. Bacchus"], "venue": "in: Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1993}, {"title": "Solving #SAT and Bayesian inference with backtracking search", "author": ["F. Bacchus", "S. Dalmao", "T. Pitassi"], "venue": "Journal of Artificial Intelligence Research 34 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Phase transitions of pp-complete satisfiability problems", "author": ["D.D. Bailey", "V. Dalmau", "P.G. Kolaitis"], "venue": "Discrete Applied Mathematics 155 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "The complexity of problems for quantified constraints", "author": ["M. Bauland", "E. Bohler", "N. Creignou", "S. Reith", "H. Schnoor", "H. Vollmer"], "venue": "Theory of Computing Systems 47 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "G", "author": ["P. Beame"], "venue": "Van den Broeck, E. Gribkoff, D. Suciu, Symmetric weighted first-order model counting, in: ACM Symposium on Principles of Database Systems (PODS)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Databases with uncertainty and lineage", "author": ["O. Benjelloun", "A.D. Sarma", "A. Halevy", "M. Theobald", "J. Widom"], "venue": "The International Journal on Very Large Data Bases 17 (2) ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Sampling edge covers in 3-regular graphs", "author": ["I. Bez\u00e1kov\u00e1", "W. Rummler"], "venue": "in: International Symposium on Mathematical Foundations of Computer Science", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Latent Dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research 3 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Path coupling using stopping times", "author": ["M. Bordewich", "M. Dyer", "M. Karpinski"], "venue": "in: International Symposium Fundamentals of Computation Theory", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Graph orientations with no sink and an approximation for a hard case of #SAT", "author": ["R. Bubley", "M. Dyer"], "venue": "in: ACM-SIAM Symposium on Discrete Algorithms", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Nonrelativizing separations", "author": ["H. Buhrman", "L. Fortnow", "T. Thierauf"], "venue": "in: Proceedings of IEEE Complexity", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "The complexity of weighted and unweighted #CSP", "author": ["A. Bulatov", "M. Dyer", "L.A. Goldberg", "M. Jalsenius", "M. Jerrum", "D. Richerby"], "venue": "Journal of Computer and System Sciences 78 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Operations for learning with graphical models", "author": ["W.L. Buntine"], "venue": "Journal of Artificial Intelligence Research 2 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1994}, {"title": "Holographic reduction", "author": ["J.-Y. Cai", "P. Lu", "M. Xia"], "venue": "interpolation and hardness, Computational Complexity 21 (4) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "DL-Lite: Tractable description logics for ontologies", "author": ["D. Calvanese", "G.D. Giacomo", "D. Lembo", "M. Lenzerini", "R. Rosati"], "venue": "in: AAAI", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Data complexity of query answering in description logics", "author": ["D. Calvanese", "G.D. Giacomo", "D. Lembo", "M. Lenzerini", "R. Rosati"], "venue": "in: Knowledge Representation", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "K", "author": ["R.N. Carvalho"], "venue": "B. Laskey, , P. C. Costa, PR-OWL 2.0 \u2014 bridging the gap to OWL semantics, in: URSW 2008-2010/UniDL 2010, LNAI 7123", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "The Bayesian description logic BEL", "author": ["I.I. Ceylan", "R. Pe\u00f1aloza"], "venue": "in: International Joint Conference on Automated Reasoning", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "On probabilistic inference by weighted model counting", "author": ["M. Chavira", "A. Darwiche"], "venue": "Artificial Intelligence 172 (6-7) ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "PR-OWL: A framework for probabilistic ontologies", "author": ["P.C.G. Costa", "K.B. Laskey"], "venue": "in: Conference on Formal Ontology in Information Systems", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "CLP(BN ): Constraint logic programming for probabilistic knowledge", "author": ["V.S. Costa", "D. Page", "M. Qazi", "J. Cussens"], "venue": "in: U. Kjaerulff, C. Meek (eds.), Conference on Uncertainty in Artificial Intelligence, Morgan-Kaufmann, San Francisco, California", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Bayesian networks specified using propositional and relational constructs: Combined", "author": ["F.G. Cozman", "D.D. Mau\u00e1"], "venue": "data, and domain complexity, in: AAAI Conference on Artificial Intelligence", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic graphical models specified by probabilistic logic programs: Semantics and complexity", "author": ["F.G. Cozman", "D.D. Mau\u00e1"], "venue": "in: Conference on Probabilistic Graphical Models \u2014 JMLR Workshop and Conference Proceedings, vol. 52", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "The structure and complexity of credal semantics", "author": ["F.G. Cozman", "D.D. Mau\u00e1"], "venue": "in: Workshop on Probabilistic Logic Programming", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "The well-founded semantics of cyclic probabilistic logic programs: meaning and complexity", "author": ["F.G. Cozman", "D.D. Mau\u00e1"], "venue": "in: Encontro Nacional de Intelig\u00eancia Artificial e Computacional", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Complexity analysis and variational inference for interpretationbased probabilistic description logics", "author": ["F.G. Cozman", "R.B. Polastro"], "venue": "in: Proceedings of the Twenty-Fifth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-09), AUAI Press, Corvallis, Oregon", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "D", "author": ["N. Dalvi"], "venue": "Suciu, Efficient query evaluation on probabilistic databases 16 ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Tractable reasoning with Bayesian description logics", "author": ["C. d\u2019Amato", "N. Fanizzi", "T. Lukasiewicz"], "venue": "in: International Conference on Scalable Uncertainty Management,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "Query DAGSs: A practical paradigm for implementing belief-network inference", "author": ["A. Darviche", "G. Provan"], "venue": "in: E. Horvitz, F. Jensen (eds.), Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann, San Francisco, California, United States", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1996}, {"title": "A differential approach to inference in Bayesian networks", "author": ["A. Darwiche"], "venue": "Journal of the ACM 50 (3) ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2003}, {"title": "Modeling and Reasoning with Bayesian Networks", "author": ["A. Darwiche"], "venue": "Cambridge University Press", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "A knowledge compilation map", "author": ["A. Darwiche", "P. Marquis"], "venue": "Journal of Artificial Intelligence Research 17 ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2002}, {"title": "The complexity of MAP inference in Bayesian networks specified through logical languages", "author": ["F.G.C. Denis Deratani Maua", "Cassio Polpo de Campos"], "venue": "in: International Joint Conference on Artificial Intelligence,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "BayesOWL: Uncertainty modeling in semantic web ontologies", "author": ["Z. Ding", "Y. Peng", "R. Pan"], "venue": "in: Soft Computing in Ontologies and Semantic Web, vol. 204 of Studies in Fuzziness and Soft Computing, Springer, Berlin/Heidelberg", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "A tractable first-order probabilistic logic", "author": ["P. Domingos", "W.A. Webb"], "venue": "in: AAAI", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Subtractive reductions and complete problems for counting complexity classes", "author": ["A. Durand", "M. Hermann", "P.G. Kolaitis"], "venue": "Theoretical Computer Science 340 (3) ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2005}, {"title": "Logical Bayesian networks and their relation to other probabilistic logical models", "author": ["D. Fierens", "H. Blockeel", "M. Bruynooghe", "J. Ramon"], "venue": "in: Int. Conference on Inductive Logic Pogramming", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2005}, {"title": "Logical Bayesian networks", "author": ["D. Fierens", "H. Blockeel", "J. Ramon", "M. Bruynooghe"], "venue": "in: Workshop on Multi-Relational Data Mining", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2004}, {"title": "G", "author": ["D. Fierens"], "venue": "Van den Broeck, J. Renkens, D. Shrerionov, B. Gutmann, G. Janssens, L. de Raedt, Inference and learning in probabilistic logic programs using weighted Boolean formulas, Theory and Practice of Logic Programming 15 (3) ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "The parameterized complexity of counting problems", "author": ["J. Flum", "M. Grohe"], "venue": "SIAM Journal of Computing 33 (4) ", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning probabilistic relational models", "author": ["N. Friedman", "L. Getoor", "D. Koller", "A. Pfeffer"], "venue": "in: International Joint Conference on Artificial Intelligence", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1999}, {"title": "Probabilistic relational models", "author": ["L. Getoor", "N. Friedman", "D. Koller", "A. Pfeffer", "B. Taskar"], "venue": "in: Introduction to Statistical Relational Learning", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2007}, {"title": "PRL: A probabilistic relational language", "author": ["L. Getoor", "J. Grant"], "venue": "Machine Learning 62 ", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2006}, {"title": "Introduction to Statistical Relational Learning", "author": ["L. Getoor", "B. Taskar"], "venue": "MIT Press", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2007}, {"title": "A language and program for complex Bayesian modelling", "author": ["W. Gilks", "A. Thomas", "D. Spiegelhalter"], "venue": "The Statistician 43 ", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1993}, {"title": "Computational complexity of probabilistic Tusing machines", "author": ["J. Gill"], "venue": "SIAM Journal on Computing 6 (4) ", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1977}, {"title": "Constructing flexible dynamic belief networks from first-order probabilistic knowledge bases", "author": ["S. Glesner", "D. Koller"], "venue": "in: Symbolic and Quantitative Approaches to Reasoning with Uncertainty", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1995}, {"title": "Dynamic construction of belief networks", "author": ["R.P. Goldman", "E. Charniak"], "venue": "in: Conference of Uncertainty in Artificial Intelligence", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1990}, {"title": "Complexity of DNF minimization and isomorphism testing for monotone formulas", "author": ["J. Goldsmith", "M. Hagen", "M. Mundhenk"], "venue": "Information and Computation 206 (6) ", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2008}, {"title": "Finite model theory and descriptive complexity", "author": ["E. Gr\u00e4del"], "venue": "in: Finite Model Theory and its Applications, Springer", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2007}, {"title": "G", "author": ["E. Gribkoff"], "venue": "Van den Broeck, D. Suciu, Understanding the complexity of lifted inference and asymmetric weighted model counting, in: Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2014}, {"title": "Asymptotic conditional probabilities: the unary case", "author": ["A. Grove", "J. Halpern", "D. Koller"], "venue": "SIAM Journal on Computing 25 (1) ", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1996}, {"title": "Generating Bayesian networks from probability logic knowledge", "author": ["P. Haddawy"], "venue": "in: Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1994}, {"title": "An empirical comparison of three inference methods", "author": ["D. Heckerman"], "venue": "in: R. D. Shachter, L. N. Kanal, J. F. Lemmer (eds.), Uncertainty in Artificial Intelligence 4, Elsevier Science Publishers, North-Holland", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1990}, {"title": "Probabilistic entity-relationship models", "author": ["D. Heckerman", "C. Meek", "D. Koller"], "venue": "PRMs, and plate models, in: L. Getoor, B. Taskar (eds.), Introduction to Statistical Relational Learning, MIT Press", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2007}, {"title": "A dynamic approach to probabilistic inference using Bayesian networks", "author": ["M.C. Horsch", "D. Poole"], "venue": "in: Conference of Uncertainty in Artificial Intelligence", "citeRegEx": "61", "shortCiteRegEx": null, "year": 1990}, {"title": "Relational Bayesian networks", "author": ["M. Jaeger"], "venue": "in: D. Geiger, P. P. Shenoy (eds.), Conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann, San Francisco, California", "citeRegEx": "62", "shortCiteRegEx": null, "year": 1997}, {"title": "Complex probabilistic modeling with recursive relational Bayesian networks", "author": ["M. Jaeger"], "venue": "Annals of Mathematics and Artificial Intelligence 32 ", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2001}, {"title": "Probabilistic role models and the guarded fragment", "author": ["M. Jaeger"], "venue": "in: Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU)", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2004}, {"title": "Lower complexity bounds for lifted inference", "author": ["M. Jaeger"], "venue": "Theory and Practice of Logic Programming 15 (2) ", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2014}, {"title": "G", "author": ["M. Jaeger"], "venue": "Van Den Broeck, Liftability of probabilistic inference: Upper and lower bounds, in: 2nd Statistical Relational AI (StaRAI-12) Workshop", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2012}, {"title": "G", "author": ["S.M. Kazemi", "A. Kimmig"], "venue": "Van den Broeck, D. Poole, New liftable classes for first-order probabilistic inference, in: NIPS", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2016}, {"title": "Lifted probabilistic inference", "author": ["K. Kersting"], "venue": "in: L. D. Raedt, C. Bessiere, D. Dubois, P. Doherty, P. Frasconi, F. Heintz, P. Lucas ", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2012}, {"title": "Interpreting Bayesian logic programs", "author": ["K. Kersting", "L.D. Raedt", "S. Kramer"], "venue": "in: AAAI-2000 Workshop on Learning Statistical Models from Relational Data", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2000}, {"title": "MayBMS: A system for managing large uncertain and probabilistic databases", "author": ["C. Koch"], "venue": "in: C. C. Aggarwal (ed.), Managing and Mining Uncertain Data, Springer-Verlag", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT Press", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2009}, {"title": "P-CLASSIC: A tractable probablistic description logic", "author": ["D. Koller", "A.Y. Levy", "A. Pfeffer"], "venue": "in: AAAI", "citeRegEx": "72", "shortCiteRegEx": null, "year": 1997}, {"title": "Object-oriented Bayesian networks", "author": ["D. Koller", "A. Pfeffer"], "venue": "in: Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "73", "shortCiteRegEx": null, "year": 1997}, {"title": "Probabilistic frame-based systems", "author": ["D. Koller", "A. Pfeffer"], "venue": "in: National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "74", "shortCiteRegEx": null, "year": 1998}, {"title": "The computational complexity of probabilistic inference", "author": ["J. Kwisthout"], "venue": "Tech. Rep. ICIS\u2013R11003, Radboud University Nijmegen, The Netherlands ", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2011}, {"title": "H", "author": ["J.H.P. Kwisthout"], "venue": "L. Bodlaender, L., V. der Gaag, The necessity of bounded treewidth for efficient inference in Bayesian networks, in: European Conference on Artificial Intelligence", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2010}, {"title": "Polynomial space counting problems", "author": ["R.E. Ladner"], "venue": "SIAM Journal of Computing 18 (6) ", "citeRegEx": "77", "shortCiteRegEx": null, "year": 1989}, {"title": "MEBN: A language for first-order Bayesian knowledge bases", "author": ["K.B. Laskey"], "venue": "Artificial Intelligence 172 (2-3) ", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2008}, {"title": "Complexity results for classes of quantificational formulas", "author": ["H.R. Lewis"], "venue": "Journal of Computer and System Sciences 21 ", "citeRegEx": "79", "shortCiteRegEx": null, "year": 1980}, {"title": "Elements of Finite Model Theory", "author": ["L. Libkin"], "venue": "Springer", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2004}, {"title": "A simple FPTAS for counting edge covers", "author": ["C. Lin", "J. Liu", "P. Lu"], "venue": "in: ACM-SIAM Symposium on Discrete Algorithms", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2014}, {"title": "FPTAS for counting weighted edge covers", "author": ["J. Liu", "P. Lu", "C. Zhang"], "venue": "in: European Symposium on Algorithms", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2014}, {"title": "Managing uncertainty and vagueness in description logics for the semantic web", "author": ["T. Lukasiewicz", "U. Straccia"], "venue": "Journal of Web Semantics 6 ", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2008}, {"title": "The BUGS Book: A Practical Introduction to Bayesian Analysis", "author": ["D. Lunn", "C. Jackson", "N. Best", "A. Thomas", "D. Spiegelhalter"], "venue": "CRC Press/Chapman and Hall", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2012}, {"title": "The BUGS project: Evolution", "author": ["D. Lunn", "D. Spiegelhalter", "A. Thomas", "N. Best"], "venue": "critique and future directions, Statistics in Medicine 28 ", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2009}, {"title": "Network engineering for complex belief networks", "author": ["S. Mahoney", "K.B. Laskey"], "venue": "in: Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "86", "shortCiteRegEx": null, "year": 1996}, {"title": "CoreVenture: a highlevel", "author": ["V. Mansinghka", "A. Radul"], "venue": "reflective machine language for probabilistic programming, in: NIPS Workshop on Probabilistic Programming", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2014}, {"title": "The effect of combination functions on the complexity of relational Bayesian networks", "author": ["D.D. Mau\u00e1", "F.G. Cozman"], "venue": "in: Conference on Probabilistic Graphical Models \u2014 JMLR Workshop and Conference Proceedings, vol. 52", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2016}, {"title": "BLOG: Probabilistic models with unknown objects", "author": ["B. Milch", "B. Marthi", "D. Sontag", "S. Russell", "D.L. Ong", "A. Kolobov"], "venue": "in: IJCAI", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2005}, {"title": "Lifted probabilistic inference with counting formulas", "author": ["B. Milch", "L.S. Zettlemoyer", "K. Kersting", "M. Haimes", "L.P. Kaelbling"], "venue": "in: AAAI", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2008}, {"title": "Never-ending learning", "author": ["T. Mitchell", "W. Cohen", "E. Hruschka", "P. Talukdar", "J. Betteridge", "A. Carlson", "B. Dalvi", "M. Gardner", "B. Kisiel", "J. Krishnamurthy", "N. Lao", "K. Mazaitis", "T. Mohamed", "N. Nakashole", "E. Platanios", "A. Ritter", "M. Samadi", "B. Settles", "R. Wang", "D. Wijaya", "A. Gupta", "X. Chen", "A. Saparov", "M. Greaves", "J. Welling"], "venue": "in: Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15)", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning Bayesian Networks", "author": ["R.E. Neapolitan"], "venue": "Prentice Hall", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2003}, {"title": "Answering queries from context-sensitive probabilistic knowledge bases", "author": ["L. Ngo", "P. Haddawy"], "venue": "Theoretical Computer Science 171 (1\u20132) ", "citeRegEx": "93", "shortCiteRegEx": null, "year": 1997}, {"title": "G", "author": ["M. Niepert"], "venue": "Van den Broeck, Tractability through exchangeability: a new perspective on efficient probabilistic inference, in: AAAI Conference on Artificial Intelligence", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2014}, {"title": "A complexity theory for feasible closure properties", "author": ["M. Ogiwara"], "venue": "Journal of Computer and System Sciences 46 ", "citeRegEx": "95", "shortCiteRegEx": null, "year": 1993}, {"title": "Pruning conformant plans by counting models on compiled d-DNNF representations", "author": ["H. Palacios", "B. Bonet", "A. Darwiche", "H. Geffner"], "venue": "in: International Conference on Automated Planning and Scheduling", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2005}, {"title": "A note on succinct representations of graphs", "author": ["C.H. Papadimitriou"], "venue": "Information and Control 71 ", "citeRegEx": "97", "shortCiteRegEx": null, "year": 1986}, {"title": "Computational Complexity", "author": ["C.H. Papadimitriou"], "venue": "Addison-Wesley Publishing", "citeRegEx": "98", "shortCiteRegEx": null, "year": 1994}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": "Morgan Kaufmann, San Mateo, California", "citeRegEx": "99", "shortCiteRegEx": null, "year": 1988}, {"title": "Causality: models", "author": ["J. Pearl"], "venue": "reasoning, and inference, Cambridge University Press, Cambridge, United Kingdom", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2000}, {"title": "Causality: models", "author": ["J. Pearl"], "venue": "reasoning, and inference (2nd edition), Cambridge University Press, Cambridge, United Kingdom", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2009}, {"title": "IBAL: a probabilistic rational programming language", "author": ["A. Pfeffer"], "venue": "in: International Joint Conference on Artificial Intelligence", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2001}, {"title": "Probabilistic Horn abduction and Bayesian networks", "author": ["D. Poole"], "venue": "Artificial Intelligence 64 ", "citeRegEx": "103", "shortCiteRegEx": null, "year": 1993}, {"title": "The independent choice logic for modelling multiple agents under uncertainty", "author": ["D. Poole"], "venue": "Artificial Intelligence 94 (1/2) ", "citeRegEx": "104", "shortCiteRegEx": null, "year": 1997}, {"title": "First-order probabilistic inference", "author": ["D. Poole"], "venue": "in: International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2003}, {"title": "The Independent Choice Logic and beyond", "author": ["D. Poole"], "venue": "in: L. D. Raedt, P. Frasconi, K. Kersting, S. Muggleton (eds.), Probabilistic Inductive Logic Programming, vol. 4911 of Lecture Notes in Computer Science, Springer", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2008}, {"title": "Probabilistic programming languages: Independent choices and deterministic systems", "author": ["D. Poole"], "venue": "in: R. Dechter, H. Geffner, J. Y. Halpern (eds.), Heuristics, Probability and Causality \u2014 A Tribute to Judea Pearl, College Publications", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2010}, {"title": "Sum-product networks: a new deep architecture", "author": ["H. Poon", "P. Domingos"], "venue": "in: Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2011}, {"title": "Bayesian Networks \u2014 A Practical Guide to Applications", "author": ["O. Pourret", "P. Naim", "B. Marcot"], "venue": "Wiley", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2008}, {"title": "The complexity of counting cuts and of computing the probability that a graph is connected", "author": ["J.S. Provan", "M.O. Ball"], "venue": "SIAM Journal on Computing 12 (4) ", "citeRegEx": "110", "shortCiteRegEx": null, "year": 1983}, {"title": "Logical and Relational Learning", "author": ["L.D. Raedt"], "venue": "Springer", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2008}, {"title": "Probabilistic inductive logic programming", "author": ["L.D. Raedt", "K. Kersting"], "venue": "in: International Conference on Algorithmic Learning Theory", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2004}, {"title": "Probabilistic reasoning in DL-Lite", "author": ["R. Ramachandran", "G. Qi", "K. Wang", "J. Wang", "J. Thornton"], "venue": "in: Pacific Rim International Conference on Trends in Artificial Intelligence", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2012}, {"title": "The distribution semantics is well-defined for all normal programs", "author": ["F. Riguzzi"], "venue": "in: F. Riguzzi, J. Vennekens (eds.), International Workshop on Probabilistic Logic Programming, vol. 1413 of CEUR Workshop Proceedings", "citeRegEx": "114", "shortCiteRegEx": null, "year": 2015}, {"title": "On the hardness of approximate reasoning", "author": ["D. Roth"], "venue": "Artificial Intelligence 82 (1-2) ", "citeRegEx": "115", "shortCiteRegEx": null, "year": 1996}, {"title": "Affine algebraic decision diagrams (AADDs) and their application to structured probabilistic inference", "author": ["S. Sanner", "D. McAllester"], "venue": "in: International Joint Conference on Artificial Intelligence", "citeRegEx": "116", "shortCiteRegEx": null, "year": 2005}, {"title": "A statistical learning method for logic programs with distribution semantics", "author": ["T. Sato"], "venue": "in: Int. Conference on Logic Programming", "citeRegEx": "117", "shortCiteRegEx": null, "year": 1995}, {"title": "Parameter learning of logic programs for symbolic-statistical modeling", "author": ["T. Sato", "Y. Kameya"], "venue": "Journal of Artificial Intelligence Research 15 ", "citeRegEx": "118", "shortCiteRegEx": null, "year": 2001}, {"title": "On some central problems in computational complexity", "author": ["J. Simon"], "venue": "Tech. Rep. TR75-224, Department of Computer Science, Cornell University ", "citeRegEx": "119", "shortCiteRegEx": null, "year": 1975}, {"title": "R", "author": ["S. Singh", "C. Mayfield", "S. Mittal", "S. Prabhakar", "S. Hambrusch"], "venue": "Shah, Orion 2.0: Native support for uncertain data, in: SIGMOD", "citeRegEx": "120", "shortCiteRegEx": null, "year": 2008}, {"title": "A probabilistic relational model for security risk analysis", "author": ["T. Sommestad", "M. Ekstedt", "P. Johnson"], "venue": "Computers and Security 29 ", "citeRegEx": "121", "shortCiteRegEx": null, "year": 2010}, {"title": "Complexity of inference in latent Dirichlet allocation", "author": ["D. Sontag", "D. Roy"], "venue": "in: Advances in Neural Information Processing Systems, vol. 24", "citeRegEx": "122", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic Databases", "author": ["D. Suciu", "D. Oiteanu", "C. R\u00e9", "C. Koch"], "venue": "Morgan & Claypool Publishers", "citeRegEx": "123", "shortCiteRegEx": null, "year": 2011}, {"title": "G", "author": ["N. Taghipour", "D. Fierens"], "venue": "Van den Broeck, J. Davis, H. Blockeel, Completeness results for lifted variable elimination, in: Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), Scottsdale, USA", "citeRegEx": "124", "shortCiteRegEx": null, "year": 2013}, {"title": "KnowRob: A knowledge processing infrastructure for cognition-enabled robots", "author": ["M. Tenorth", "M. Beetz"], "venue": "The International Journal of Robotics Research 32 (5) ", "citeRegEx": "125", "shortCiteRegEx": null, "year": 2013}, {"title": "The complexity of reasoning with cardinality restrictions and nominals in expressive description logics", "author": ["S. Tobies"], "venue": "Journal of Artificial Intelligence Research 12 ", "citeRegEx": "126", "shortCiteRegEx": null, "year": 2000}, {"title": "PP is as hard as the polynomial-time hierarchy", "author": ["S. Toda"], "venue": "SIAM Journal of Computing 20 (5) ", "citeRegEx": "127", "shortCiteRegEx": null, "year": 1991}, {"title": "Polynomial-time 1-Turing reductions from #PH to #P", "author": ["S. Toda", "O. Watanabe"], "venue": "Theoretical Computer Science 100 ", "citeRegEx": "128", "shortCiteRegEx": null, "year": 1992}, {"title": "The complexity of counting in sparse", "author": ["S.P. Vadhan"], "venue": "regular and planar graphs, SIAM Journal of Computing 31 (2) ", "citeRegEx": "129", "shortCiteRegEx": null, "year": 2001}, {"title": "The complexity of computing the permanent", "author": ["L.G. Valiant"], "venue": "Theoretical Computer Science 8 ", "citeRegEx": "130", "shortCiteRegEx": null, "year": 1979}, {"title": "The complexity of enumeration and reliability problems", "author": ["L.G. Valiant"], "venue": "SIAM Journal of Computing 8 (3) ", "citeRegEx": "131", "shortCiteRegEx": null, "year": 1979}, {"title": "Accidental algorithms", "author": ["L.G. Valiant"], "venue": "in: Annual IEEE Symposium on Foundations of Computer Science", "citeRegEx": "132", "shortCiteRegEx": null, "year": 2006}, {"title": "On the completeness of first-order knowledge compilation for lifted probabilistic inference", "author": ["G. Van den Broeck"], "venue": "in: Neural Processing Information Systems,", "citeRegEx": "133", "shortCiteRegEx": "133", "year": 2011}, {"title": "Conditioning in first-order knowledge compilation and lifted probabilistic inference", "author": ["G. Van den Broeck", "J. Davis"], "venue": "in: AAAI Conference on Artificial Intelligence,", "citeRegEx": "134", "shortCiteRegEx": "134", "year": 2012}, {"title": "Skolemization for weighted first-order model counting", "author": ["G. Van den Broeck", "M. Wannes", "A. Darwiche"], "venue": "in: International Conference on Principles of Knowledge Representation and Reasoning,", "citeRegEx": "135", "shortCiteRegEx": "135", "year": 2014}, {"title": "The complexity of relational query languages", "author": ["M.Y. Vardi"], "venue": "in: Annual ACM Symposium on Theory of Computing", "citeRegEx": "136", "shortCiteRegEx": null, "year": 1982}, {"title": "BayesStore: Managing large", "author": ["D.Z. Wang", "E. Michelaks", "M. Garofalakis", "J.M. Hellerstein"], "venue": "uncertain data repositories with probabilistic graphical models, in: VLDB Endowment, vol. 1", "citeRegEx": "137", "shortCiteRegEx": null, "year": 2008}, {"title": "From knowledge bases to decision models", "author": ["M.P. Wellman", "J.S. Breese", "R.P. Goldman"], "venue": "Knowledge Engineering Review 7 (1) ", "citeRegEx": "138", "shortCiteRegEx": null, "year": 1992}, {"title": "Trio: A system for data", "author": ["J. Widom"], "venue": "uncertainty, and lineage, in: C. C. Aggarwal (ed.), Managing and Mining Uncertain Data, Springer-Verlag", "citeRegEx": "139", "shortCiteRegEx": null, "year": 2009}, {"title": "3-regular bipartite planar vertex cover is #P-complete", "author": ["M. Xia", "W. Zhao"], "venue": "in: Theory and Applications of Models of Computation, vol. 3959", "citeRegEx": "140", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 34, "context": "A Bayesian network can represent any distribution over a given set of random variables [36, 71], and this flexibility has been used to great effect in many applications [109].", "startOffset": 87, "endOffset": 95}, {"referenceID": 69, "context": "A Bayesian network can represent any distribution over a given set of random variables [36, 71], and this flexibility has been used to great effect in many applications [109].", "startOffset": 87, "endOffset": 95}, {"referenceID": 107, "context": "A Bayesian network can represent any distribution over a given set of random variables [36, 71], and this flexibility has been used to great effect in many applications [109].", "startOffset": 169, "endOffset": 174}, {"referenceID": 47, "context": "Thus it is not surprising that practical concerns have led to modeling languages where Bayesian networks are specified using relations, logical variables, and quantifiers [49, 111].", "startOffset": 171, "endOffset": 180}, {"referenceID": 109, "context": "Thus it is not surprising that practical concerns have led to modeling languages where Bayesian networks are specified using relations, logical variables, and quantifiers [49, 111].", "startOffset": 171, "endOffset": 180}, {"referenceID": 48, "context": "Some of these languages enlarge Bayesian networks with plates [50, 85], while others resort to elements of database schema [47, 60]; some others mix probabilities with logic programming [106, 118] and even with functional programming [87, 89, 102].", "startOffset": 62, "endOffset": 70}, {"referenceID": 83, "context": "Some of these languages enlarge Bayesian networks with plates [50, 85], while others resort to elements of database schema [47, 60]; some others mix probabilities with logic programming [106, 118] and even with functional programming [87, 89, 102].", "startOffset": 62, "endOffset": 70}, {"referenceID": 45, "context": "Some of these languages enlarge Bayesian networks with plates [50, 85], while others resort to elements of database schema [47, 60]; some others mix probabilities with logic programming [106, 118] and even with functional programming [87, 89, 102].", "startOffset": 123, "endOffset": 131}, {"referenceID": 58, "context": "Some of these languages enlarge Bayesian networks with plates [50, 85], while others resort to elements of database schema [47, 60]; some others mix probabilities with logic programming [106, 118] and even with functional programming [87, 89, 102].", "startOffset": 123, "endOffset": 131}, {"referenceID": 104, "context": "Some of these languages enlarge Bayesian networks with plates [50, 85], while others resort to elements of database schema [47, 60]; some others mix probabilities with logic programming [106, 118] and even with functional programming [87, 89, 102].", "startOffset": 186, "endOffset": 196}, {"referenceID": 116, "context": "Some of these languages enlarge Bayesian networks with plates [50, 85], while others resort to elements of database schema [47, 60]; some others mix probabilities with logic programming [106, 118] and even with functional programming [87, 89, 102].", "startOffset": 186, "endOffset": 196}, {"referenceID": 85, "context": "Some of these languages enlarge Bayesian networks with plates [50, 85], while others resort to elements of database schema [47, 60]; some others mix probabilities with logic programming [106, 118] and even with functional programming [87, 89, 102].", "startOffset": 234, "endOffset": 247}, {"referenceID": 87, "context": "Some of these languages enlarge Bayesian networks with plates [50, 85], while others resort to elements of database schema [47, 60]; some others mix probabilities with logic programming [106, 118] and even with functional programming [87, 89, 102].", "startOffset": 234, "endOffset": 247}, {"referenceID": 100, "context": "Some of these languages enlarge Bayesian networks with plates [50, 85], while others resort to elements of database schema [47, 60]; some others mix probabilities with logic programming [106, 118] and even with functional programming [87, 89, 102].", "startOffset": 234, "endOffset": 247}, {"referenceID": 113, "context": "Yet most of the existing analysis on the complexity of inference with Bayesian networks focuses on a simplified setting where nodes of a network are associated with categorial variables and distributions are specified by flat tables containing probability values [115, 75].", "startOffset": 263, "endOffset": 272}, {"referenceID": 73, "context": "Yet most of the existing analysis on the complexity of inference with Bayesian networks focuses on a simplified setting where nodes of a network are associated with categorial variables and distributions are specified by flat tables containing probability values [115, 75].", "startOffset": 263, "endOffset": 272}, {"referenceID": 2, "context": "This is certainly unsatisfying: as a point of comparison, consider the topic of logical inference, where much is known about the impact of specific constructs on computational complexity \u2014 suffice to mention the beautiful and detailed study of satisfiability in description logics [3].", "startOffset": 281, "endOffset": 284}, {"referenceID": 105, "context": "We adopt a simple specification strategy inspired by probabilistic programming [107] and by structural equation models [101]: A Bayesian network over binary variables is specified by a set of logical equivalences and a set of independent random variables.", "startOffset": 79, "endOffset": 84}, {"referenceID": 99, "context": "We adopt a simple specification strategy inspired by probabilistic programming [107] and by structural equation models [101]: A Bayesian network over binary variables is specified by a set of logical equivalences and a set of independent random variables.", "startOffset": 119, "endOffset": 124}, {"referenceID": 48, "context": "Using this simple scheme, we can parameterize computational complexity by the formal language that is allowed in the logical equivalences; we can move from sub-Boolean languages to relational ones, in the way producing languages that are similar in power to plate models [50] and to probabilistic relational models [74].", "startOffset": 271, "endOffset": 275}, {"referenceID": 72, "context": "Using this simple scheme, we can parameterize computational complexity by the formal language that is allowed in the logical equivalences; we can move from sub-Boolean languages to relational ones, in the way producing languages that are similar in power to plate models [50] and to probabilistic relational models [74].", "startOffset": 315, "endOffset": 319}, {"referenceID": 121, "context": "Query complexity has often been defined, in the contex of probabilistic databases, as data complexity [123].", "startOffset": 102, "endOffset": 107}, {"referenceID": 8, "context": "Query and domain complexity are directly related respectively to dqe-liftability and domain liftability, concepts that have been used in lifted inference [9, 66].", "startOffset": 154, "endOffset": 161}, {"referenceID": 64, "context": "Query and domain complexity are directly related respectively to dqe-liftability and domain liftability, concepts that have been used in lifted inference [9, 66].", "startOffset": 154, "endOffset": 161}, {"referenceID": 69, "context": "A graph consists of a set of nodes and a set of edges (an edge is a pair of nodes), and we focus on graphs that are directed and acyclic [71].", "startOffset": 137, "endOffset": 141}, {"referenceID": 90, "context": "A Bayesian network is a pair consisting of a directed acyclic graph G whose nodes are random variables and a joint probability distribution P over all variables in the graph, such that G and P satisfy the Markov condition [92].", "startOffset": 222, "endOffset": 226}, {"referenceID": 96, "context": "We adopt basic terminology and notation from computational complexity [98].", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "have PEXP, consisting of those languages L with the following property: there is an exponential time nondeterministic Turing machine M such that l \u2208 L iff half of the computations of M on input l end up accepting [15].", "startOffset": 213, "endOffset": 217}, {"referenceID": 128, "context": "We examine Valiant\u2019s approach to counting problems in Section 8; for now suffice to say that #P is the class of functions such that f \u2208 #P iff f(l) is the number of computation paths that accept l for some polynomial time nondeterministic Turing machine [130].", "startOffset": 254, "endOffset": 259}, {"referenceID": 129, "context": "This is the counterpart of Valiant\u2019s class #P1 that consists of the functions in #P that have a single symbol as input vocabulary [131].", "startOffset": 130, "endOffset": 135}, {"referenceID": 49, "context": "An important PP-complete (with respect to many-one reductions) decision problem is MAJSAT: the input is a propositional sentence \u03c6 and the decision is whether or not the majority of assignments to the propositions in \u03c6 make \u03c6 true [51].", "startOffset": 231, "endOffset": 235}, {"referenceID": 117, "context": "Another PP-complete problem (with respect to many-one reductions) is deciding whether the number of satisfying assignments for \u03c6 is larger than an input integer k [119]; in fact this problem is still PP-complete with respect to many-one reductions even if \u03c6 is monotone [54].", "startOffset": 163, "endOffset": 168}, {"referenceID": 52, "context": "Another PP-complete problem (with respect to many-one reductions) is deciding whether the number of satisfying assignments for \u03c6 is larger than an input integer k [119]; in fact this problem is still PP-complete with respect to many-one reductions even if \u03c6 is monotone [54].", "startOffset": 270, "endOffset": 274}, {"referenceID": 6, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 101, "context": "We adopt a specification strategy that moves away from tables of probability values, and that is inspired by probabilistic programming [103, 118] and by structural models [100].", "startOffset": 135, "endOffset": 145}, {"referenceID": 116, "context": "We adopt a specification strategy that moves away from tables of probability values, and that is inspired by probabilistic programming [103, 118] and by structural models [100].", "startOffset": 135, "endOffset": 145}, {"referenceID": 98, "context": "We adopt a specification strategy that moves away from tables of probability values, and that is inspired by probabilistic programming [103, 118] and by structural models [100].", "startOffset": 171, "endOffset": 176}, {"referenceID": 0, "context": ", Xn}, and \u03b1 is a rational number in the interval [0, 1].", "startOffset": 50, "endOffset": 56}, {"referenceID": 2, "context": "We refer to each logical equivalence Xi \u2194 li as a definition axiom, borrowing terminology from description logics [3].", "startOffset": 114, "endOffset": 117}, {"referenceID": 97, "context": "Note that definition axioms can exploit structures that conditional probability tables cannot; for instance, to create a Noisy-Or gate [99], we simply say that X \u2261\u2261 (Y1 \u2227W1) \u2228 (Y2 \u2227W2), where W1 and W2 are inhibitor variables.", "startOffset": 135, "endOffset": 139}, {"referenceID": 0, "context": "2 The complexity of propositional languages Now consider a language INF[L] that consists of the strings (B,Q,E, \u03b3) for which P(Q|E) > \u03b3, where \u2022 P is the distribution encoded by a Bayesian network specification B with definition axioms whose bodies are formulas in L, \u2022 Q and E are sets of assignments (the query), \u2022 and \u03b3 is a rational number in [0, 1].", "startOffset": 347, "endOffset": 353}, {"referenceID": 32, "context": "There is obvious interest in finding simple languages L such that deciding INF[L] is a tractable problem, so as to facilitate elicitation, decision-making and learning [34, 40, 64, 108, 116].", "startOffset": 168, "endOffset": 190}, {"referenceID": 38, "context": "There is obvious interest in finding simple languages L such that deciding INF[L] is a tractable problem, so as to facilitate elicitation, decision-making and learning [34, 40, 64, 108, 116].", "startOffset": 168, "endOffset": 190}, {"referenceID": 62, "context": "There is obvious interest in finding simple languages L such that deciding INF[L] is a tractable problem, so as to facilitate elicitation, decision-making and learning [34, 40, 64, 108, 116].", "startOffset": 168, "endOffset": 190}, {"referenceID": 106, "context": "There is obvious interest in finding simple languages L such that deciding INF[L] is a tractable problem, so as to facilitate elicitation, decision-making and learning [34, 40, 64, 108, 116].", "startOffset": 168, "endOffset": 190}, {"referenceID": 114, "context": "There is obvious interest in finding simple languages L such that deciding INF[L] is a tractable problem, so as to facilitate elicitation, decision-making and learning [34, 40, 64, 108, 116].", "startOffset": 168, "endOffset": 190}, {"referenceID": 57, "context": "And there are indeed propositional languages that generate tractable Bayesian networks: for instance, it is well known that Noisy-Or networks display polynomial inference when the query consists of negative assignments [59].", "startOffset": 219, "endOffset": 223}, {"referenceID": 74, "context": "One might think that tractability can only be attained by imposing some structural conditions on graphs, given results that connect complexity and graph properties [76].", "startOffset": 164, "endOffset": 168}, {"referenceID": 125, "context": "A Turing reduction gives some valuable information: if a problem is PP-complete with Turing reductions, then it is unlikely to be polynomial (for if it were polynomial, then P would equal P, a highly unlikely result given current assumptions in complexity theory [127]).", "startOffset": 263, "endOffset": 268}, {"referenceID": 35, "context": "One might try to concoct additional languages by using specific logical forms in the literature [37].", "startOffset": 96, "endOffset": 100}, {"referenceID": 32, "context": "The fact that query complexity may differ from inferential complexity was initially raised by Darwiche and Provan [34], and has led to a number of techniques emphasizing compilation of a fixed Bayesian network [23, 35].", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": "The fact that query complexity may differ from inferential complexity was initially raised by Darwiche and Provan [34], and has led to a number of techniques emphasizing compilation of a fixed Bayesian network [23, 35].", "startOffset": 210, "endOffset": 218}, {"referenceID": 33, "context": "The fact that query complexity may differ from inferential complexity was initially raised by Darwiche and Provan [34], and has led to a number of techniques emphasizing compilation of a fixed Bayesian network [23, 35].", "startOffset": 210, "endOffset": 218}, {"referenceID": 32, "context": "The original work by Darwiche and Provan [34] shows how to transform a fixed Bayesian network into a Query-DAG such that P(Q|E) > \u03b3 can be decided in linear time.", "startOffset": 41, "endOffset": 45}, {"referenceID": 32, "context": "Theorem 3 (Darwiche and Provan [34]).", "startOffset": 31, "endOffset": 35}, {"referenceID": 47, "context": "Such languages have been used in a variety of applications with repetitive entities and relationships [49, 111].", "startOffset": 102, "endOffset": 111}, {"referenceID": 109, "context": "Such languages have been used in a variety of applications with repetitive entities and relationships [49, 111].", "startOffset": 102, "endOffset": 111}, {"referenceID": 103, "context": "1 Relational Bayesian network specifications We start by blending some terminology and notation by Poole [105] and by Milch et al.", "startOffset": 105, "endOffset": 110}, {"referenceID": 88, "context": "[90].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Typically in logical languages there is a distinction between constants and elements of a domain, but we avoid constants altogether in our discussion (as argued by Bacchus, if constants are used within a probabilistic logic, some sort of additional rigidity assumption must be used [4]).", "startOffset": 282, "endOffset": 285}, {"referenceID": 0, "context": "\u2022 and \u03b1 is a rational number in [0, 1].", "startOffset": 32, "endOffset": 38}, {"referenceID": 2, "context": "The distinction between unary and binary notation for input numbers is often used in description logics [3].", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "Consider the following definitions, where S is a relational Bayesian network specification, N is the domain size, Q and E are sets of assignments for ground atoms, \u03b3 is a rational number in [0, 1], and C is a complexity class:", "startOffset": 190, "endOffset": 196}, {"referenceID": 78, "context": "We might instead assume that the vocabulary is fixed; in this case we might use the term combined complexity, as this is the term employed in finite model theory and database theory to refer to the complexity of model checking when both the formula and the model are given as input, but the vocabulary is fixed [80].", "startOffset": 311, "endOffset": 315}, {"referenceID": 64, "context": "Lifted inference We note that query and domain complexities are related respectively to dqeliftability and domain-liftability, as defined in the study of lifted inference [66, 65].", "startOffset": 171, "endOffset": 179}, {"referenceID": 63, "context": "Lifted inference We note that query and domain complexities are related respectively to dqeliftability and domain-liftability, as defined in the study of lifted inference [66, 65].", "startOffset": 171, "endOffset": 179}, {"referenceID": 66, "context": "The term \u201clifted inference\u201d is usually attached to algorithms that try to compute inferences involving parvariables without actually producing groundings [68, 90, 105].", "startOffset": 154, "endOffset": 167}, {"referenceID": 88, "context": "The term \u201clifted inference\u201d is usually attached to algorithms that try to compute inferences involving parvariables without actually producing groundings [68, 90, 105].", "startOffset": 154, "endOffset": 167}, {"referenceID": 103, "context": "The term \u201clifted inference\u201d is usually attached to algorithms that try to compute inferences involving parvariables without actually producing groundings [68, 90, 105].", "startOffset": 154, "endOffset": 167}, {"referenceID": 131, "context": "A formal definition of lifted inference has been proposed by Van den Broeck [133]: an algorithm is domain lifted iff inference runs in polynomial time with respect to N , for fixed model and query.", "startOffset": 76, "endOffset": 81}, {"referenceID": 64, "context": "Domain liftability has been extended to dqe-liftability, where the inference must run in polynomial time with respect to N and the query, for fixed model [66].", "startOffset": 154, "endOffset": 158}, {"referenceID": 64, "context": "Deep results have been obtained both on the limits of liftability [66, 65], and on algorithms that attain liftability [9, 135, 67, 94, 124].", "startOffset": 66, "endOffset": 74}, {"referenceID": 63, "context": "Deep results have been obtained both on the limits of liftability [66, 65], and on algorithms that attain liftability [9, 135, 67, 94, 124].", "startOffset": 66, "endOffset": 74}, {"referenceID": 8, "context": "Deep results have been obtained both on the limits of liftability [66, 65], and on algorithms that attain liftability [9, 135, 67, 94, 124].", "startOffset": 118, "endOffset": 139}, {"referenceID": 133, "context": "Deep results have been obtained both on the limits of liftability [66, 65], and on algorithms that attain liftability [9, 135, 67, 94, 124].", "startOffset": 118, "endOffset": 139}, {"referenceID": 65, "context": "Deep results have been obtained both on the limits of liftability [66, 65], and on algorithms that attain liftability [9, 135, 67, 94, 124].", "startOffset": 118, "endOffset": 139}, {"referenceID": 92, "context": "Deep results have been obtained both on the limits of liftability [66, 65], and on algorithms that attain liftability [9, 135, 67, 94, 124].", "startOffset": 118, "endOffset": 139}, {"referenceID": 122, "context": "Deep results have been obtained both on the limits of liftability [66, 65], and on algorithms that attain liftability [9, 135, 67, 94, 124].", "startOffset": 118, "endOffset": 139}, {"referenceID": 30, "context": "There exist several probabilistic database systems [31, 70, 120, 137, 139]; for instance, the Trio system lets the user indicate that Amy drives an Acura with probability 0.", "startOffset": 51, "endOffset": 74}, {"referenceID": 68, "context": "There exist several probabilistic database systems [31, 70, 120, 137, 139]; for instance, the Trio system lets the user indicate that Amy drives an Acura with probability 0.", "startOffset": 51, "endOffset": 74}, {"referenceID": 118, "context": "There exist several probabilistic database systems [31, 70, 120, 137, 139]; for instance, the Trio system lets the user indicate that Amy drives an Acura with probability 0.", "startOffset": 51, "endOffset": 74}, {"referenceID": 135, "context": "There exist several probabilistic database systems [31, 70, 120, 137, 139]; for instance, the Trio system lets the user indicate that Amy drives an Acura with probability 0.", "startOffset": 51, "endOffset": 74}, {"referenceID": 137, "context": "There exist several probabilistic database systems [31, 70, 120, 137, 139]; for instance, the Trio system lets the user indicate that Amy drives an Acura with probability 0.", "startOffset": 51, "endOffset": 74}, {"referenceID": 9, "context": "8 [10].", "startOffset": 2, "endOffset": 6}, {"referenceID": 89, "context": "As another example, the NELL system scans text from the web and builds a database of facts, each associated with a number between zero and one [91].", "startOffset": 143, "endOffset": 147}, {"referenceID": 121, "context": "[123].", "startOffset": 0, "endOffset": 5}, {"referenceID": 8, "context": "say that a probabilistic database is symmetric iff each table in the database can be thus associated with a parvariable and a single probabilistic assessment [9].", "startOffset": 158, "endOffset": 161}, {"referenceID": 78, "context": "Query or data complexity? The definition of query complexity (Definition 3) reminds one of data complexity as adopted in finite model theory and in database theory [80].", "startOffset": 164, "endOffset": 168}, {"referenceID": 121, "context": "It is thus not surprising that research on probabilistic databases has used the term \u201cdata complexity\u201d to mean the complexity when the database is the only input [123].", "startOffset": 162, "endOffset": 167}, {"referenceID": 42, "context": "And in probabilistic logic programming one can use probabilistic facts to associate probabilities with specific groundings [44, 103, 118].", "startOffset": 123, "endOffset": 137}, {"referenceID": 101, "context": "And in probabilistic logic programming one can use probabilistic facts to associate probabilities with specific groundings [44, 103, 118].", "startOffset": 123, "endOffset": 137}, {"referenceID": 116, "context": "And in probabilistic logic programming one can use probabilistic facts to associate probabilities with specific groundings [44, 103, 118].", "startOffset": 123, "endOffset": 137}, {"referenceID": 19, "context": "Here \u03c6 is the \u201cquery\u201d and the table is the \u201cdata\u201d (this sort of arrangement has been used in description logics [20]).", "startOffset": 112, "endOffset": 116}, {"referenceID": 2, "context": "For instance, the popular description logic ALC restricts quantification to obtain PSPACE-completeness of satisfiability [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 25, "context": "The language ALC consists of all formulas recursively defined so that X(x ) is a formula where X is a unary relation, \u00ac\u03c6 is a formula when \u03c6 is a formula, \u03c6\u2227\u03c6 is a formula when both \u03c6 and In fact we have used the term data complexity in previous work [26].", "startOffset": 251, "endOffset": 255}, {"referenceID": 63, "context": "We simply rephrase an ingenious argument by Jaeger [65] to establish:", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": ", who have derived their domain and combined complexity [9].", "startOffset": 56, "endOffset": 59}, {"referenceID": 8, "context": "Now consider domain complexity for the bounded variable fragment; previous results in the literature establish this complexity [9, 133, 135].", "startOffset": 127, "endOffset": 140}, {"referenceID": 131, "context": "Now consider domain complexity for the bounded variable fragment; previous results in the literature establish this complexity [9, 133, 135].", "startOffset": 127, "endOffset": 140}, {"referenceID": 133, "context": "Now consider domain complexity for the bounded variable fragment; previous results in the literature establish this complexity [9, 133, 135].", "startOffset": 127, "endOffset": 140}, {"referenceID": 2, "context": "An example is the description logic ALC that we have discussed before: every sentence in this description logic can be translated to a formula in FFFO [3].", "startOffset": 151, "endOffset": 154}, {"referenceID": 54, "context": "Similar classes of formulas have been studied for symmetric probabilistic databases [56].", "startOffset": 84, "endOffset": 88}, {"referenceID": 132, "context": "The case k = 1 seems to be open; when k = 1, query complexity is polynomial when inference is solely on unary relations [134, 135].", "startOffset": 120, "endOffset": 130}, {"referenceID": 133, "context": "The case k = 1 seems to be open; when k = 1, query complexity is polynomial when inference is solely on unary relations [134, 135].", "startOffset": 120, "endOffset": 130}, {"referenceID": 2, "context": "Those languages are now fundamental knowledge representation tools, as they have solid semantics and computational guarantees concerning reasoning tasks [3].", "startOffset": 153, "endOffset": 156}, {"referenceID": 81, "context": "Given the favorable properties of description logics, much effort has been spent in mixing them with probabilities [83].", "startOffset": 115, "endOffset": 119}, {"referenceID": 1, "context": "Consider first the description logic EL, where the only allowed operators are intersection and existential restrictions, and where the top concept is available, interpreted as the whole domain [2].", "startOffset": 193, "endOffset": 196}, {"referenceID": 18, "context": "We can present more substantial results when we focus on the negation-free fragment of the popular description logic DL-Lite [19].", "startOffset": 125, "endOffset": 129}, {"referenceID": 0, "context": "DL-Lite is particularly interesting because it captures central features of ER or UML diagrams, and yet common inference services have polynomial complexity [1].", "startOffset": 157, "endOffset": 160}, {"referenceID": 31, "context": "[33] propose a variant of DL-Lite where the interpretation of each sentence is conditional on a context that is specified by a Bayesian network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "A similar approach was taken by Ceylan and Pe\u00f1alosa [22], with minor semantic differences.", "startOffset": 52, "endOffset": 56}, {"referenceID": 111, "context": "A different approach is to extend the syntax of DL-Lite sentences with probabilistic subsumption connectives, as in the Probabilistic DL-Lite [113].", "startOffset": 142, "endOffset": 147}, {"referenceID": 80, "context": "Then most of the proof of Theorem 17 follows (the difference is that the intersection graphs used in the proof do not satisfy the same symmetries); we can then resort to approximations for weighted edge cover counting [82], so as to develop a fully polynomial-time approximation scheme (FPTAS) for inference.", "startOffset": 218, "endOffset": 222}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 82, "context": "Plate models have been extensively used in statistical practice [84] since they were introduced in the BUGS project [50, 85].", "startOffset": 64, "endOffset": 68}, {"referenceID": 48, "context": "Plate models have been extensively used in statistical practice [84] since they were introduced in the BUGS project [50, 85].", "startOffset": 116, "endOffset": 124}, {"referenceID": 83, "context": "Plate models have been extensively used in statistical practice [84] since they were introduced in the BUGS project [50, 85].", "startOffset": 116, "endOffset": 124}, {"referenceID": 16, "context": "In machine learning, they have been used to convey several models since their first appearance [17].", "startOffset": 95, "endOffset": 99}, {"referenceID": 82, "context": "To make things simple, here we focus on parvariables that correspond to relations, thus every random variable has values true and false (plate models in the literature often specify discrete and even continuous random variables [84, 122]).", "startOffset": 228, "endOffset": 237}, {"referenceID": 120, "context": "To make things simple, here we focus on parvariables that correspond to relations, thus every random variable has values true and false (plate models in the literature often specify discrete and even continuous random variables [84, 122]).", "startOffset": 228, "endOffset": 237}, {"referenceID": 45, "context": "Suppose we are interested in a \u201cUniversityWorld\u201d containing a population of students and a population of courses [47].", "startOffset": 113, "endOffset": 117}, {"referenceID": 11, "context": "One can find extended versions of plate models in the literature, where a node can have children in other plates (for instance the smoothed Latent Dirichlet Allocation (sLDA) model [12] depicted in Figure 8).", "startOffset": 181, "endOffset": 185}, {"referenceID": 71, "context": "Early proposals resorted to object orientation [73, 86], to frames [74], and to rule-based statements [5, 52, 58], all inspired by knowledge-based model construction [61, 53, 138].", "startOffset": 47, "endOffset": 55}, {"referenceID": 84, "context": "Early proposals resorted to object orientation [73, 86], to frames [74], and to rule-based statements [5, 52, 58], all inspired by knowledge-based model construction [61, 53, 138].", "startOffset": 47, "endOffset": 55}, {"referenceID": 72, "context": "Early proposals resorted to object orientation [73, 86], to frames [74], and to rule-based statements [5, 52, 58], all inspired by knowledge-based model construction [61, 53, 138].", "startOffset": 67, "endOffset": 71}, {"referenceID": 4, "context": "Early proposals resorted to object orientation [73, 86], to frames [74], and to rule-based statements [5, 52, 58], all inspired by knowledge-based model construction [61, 53, 138].", "startOffset": 102, "endOffset": 113}, {"referenceID": 50, "context": "Early proposals resorted to object orientation [73, 86], to frames [74], and to rule-based statements [5, 52, 58], all inspired by knowledge-based model construction [61, 53, 138].", "startOffset": 102, "endOffset": 113}, {"referenceID": 56, "context": "Early proposals resorted to object orientation [73, 86], to frames [74], and to rule-based statements [5, 52, 58], all inspired by knowledge-based model construction [61, 53, 138].", "startOffset": 102, "endOffset": 113}, {"referenceID": 59, "context": "Early proposals resorted to object orientation [73, 86], to frames [74], and to rule-based statements [5, 52, 58], all inspired by knowledge-based model construction [61, 53, 138].", "startOffset": 166, "endOffset": 179}, {"referenceID": 51, "context": "Early proposals resorted to object orientation [73, 86], to frames [74], and to rule-based statements [5, 52, 58], all inspired by knowledge-based model construction [61, 53, 138].", "startOffset": 166, "endOffset": 179}, {"referenceID": 136, "context": "Early proposals resorted to object orientation [73, 86], to frames [74], and to rule-based statements [5, 52, 58], all inspired by knowledge-based model construction [61, 53, 138].", "startOffset": 166, "endOffset": 179}, {"referenceID": 44, "context": "Some of these proposals coalesced into a family of models loosely grouped under the name of Probabilistic Relational Models (PRMs) [46].", "startOffset": 131, "endOffset": 135}, {"referenceID": 45, "context": "[47]; again, to simplify matters, we focus on parvariables that correspond to relations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 69, "context": "Following terminology by Koller and Friedman [71], we say that relations that encode classes and associations, such as Course and studentOf , are guard parvariables.", "startOffset": 45, "endOffset": 49}, {"referenceID": 45, "context": "Associations appear as dashed edges [47, 121].", "startOffset": 36, "endOffset": 45}, {"referenceID": 119, "context": "Associations appear as dashed edges [47, 121].", "startOffset": 36, "endOffset": 45}, {"referenceID": 45, "context": "[47].", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "The literature has proposed languages that allow cycles [47, 60]; one example is shown in Figure 11.", "startOffset": 56, "endOffset": 64}, {"referenceID": 58, "context": "The literature has proposed languages that allow cycles [47, 60]; one example is shown in Figure 11.", "startOffset": 56, "endOffset": 64}, {"referenceID": 46, "context": "A notable formalism is the Probabilistic Relational Language (PRL) [48], where logic program are used to specify PRMs; the specification is divided into logical background (that is, guard parvariables), probabilistic background, and probabilistic dependencies.", "startOffset": 67, "endOffset": 71}, {"referenceID": 41, "context": "Two other examples of textual formalisms that can be used to encode PRMs are Logical Bayesian Networks (LBNs) [43, 42] and Bayesian Logic Programs (BLPs) [69, 112].", "startOffset": 110, "endOffset": 118}, {"referenceID": 40, "context": "Two other examples of textual formalisms that can be used to encode PRMs are Logical Bayesian Networks (LBNs) [43, 42] and Bayesian Logic Programs (BLPs) [69, 112].", "startOffset": 110, "endOffset": 118}, {"referenceID": 67, "context": "Two other examples of textual formalisms that can be used to encode PRMs are Logical Bayesian Networks (LBNs) [43, 42] and Bayesian Logic Programs (BLPs) [69, 112].", "startOffset": 154, "endOffset": 163}, {"referenceID": 110, "context": "Two other examples of textual formalisms that can be used to encode PRMs are Logical Bayesian Networks (LBNs) [43, 42] and Bayesian Logic Programs (BLPs) [69, 112].", "startOffset": 154, "endOffset": 163}, {"referenceID": 91, "context": "Both distinguish between logical predicates that constrain groundings (that is, guard parvariables), and probabilistic or Bayesian predicates that encode probabilistic assessments [93].", "startOffset": 180, "endOffset": 184}, {"referenceID": 58, "context": "A more visual language, based on Entity-Relationship Diagrams, is DAPER [60].", "startOffset": 72, "endOffset": 76}, {"referenceID": 76, "context": "Another diagrammatic language is given by Multi-Entity Bayesian Networks (MEBNs), a graphical representation for arbitrary first-order sentences [78].", "startOffset": 145, "endOffset": 149}, {"referenceID": 20, "context": "Several other graphical languages mix probabilities with description logics [21, 24, 39, 72], as we have mentioned in Section 6.", "startOffset": 76, "endOffset": 92}, {"referenceID": 23, "context": "Several other graphical languages mix probabilities with description logics [21, 24, 39, 72], as we have mentioned in Section 6.", "startOffset": 76, "endOffset": 92}, {"referenceID": 37, "context": "Several other graphical languages mix probabilities with description logics [21, 24, 39, 72], as we have mentioned in Section 6.", "startOffset": 76, "endOffset": 92}, {"referenceID": 70, "context": "Several other graphical languages mix probabilities with description logics [21, 24, 39, 72], as we have mentioned in Section 6.", "startOffset": 76, "endOffset": 92}, {"referenceID": 60, "context": "For instance, Jaeger\u2019s Relational Bayesian Networks [62, 63] offer a solid representation language where the user can directly specify and manipulate probability values, for instance specifying that a probability value is the average of other probability values.", "startOffset": 52, "endOffset": 60}, {"referenceID": 61, "context": "For instance, Jaeger\u2019s Relational Bayesian Networks [62, 63] offer a solid representation language where the user can directly specify and manipulate probability values, for instance specifying that a probability value is the average of other probability values.", "startOffset": 52, "endOffset": 60}, {"referenceID": 86, "context": "We have examined the complexity of Relational Bayesian Networks elsewhere [88]; some results and proofs, but not all of them, are similar to the ones presented here.", "startOffset": 74, "endOffset": 78}, {"referenceID": 85, "context": "There are also languages that encode repetitive Bayesian networks using functional programming [87, 89, 125, 102] or logic programming [25, 44, 103, 104, 114, 117], We have examined the complexity of the latter formalisms elsewhere [27, 28, 29]; again, some results and proofs, but not all of them, are similar to the ones presented here.", "startOffset": 95, "endOffset": 113}, {"referenceID": 87, "context": "There are also languages that encode repetitive Bayesian networks using functional programming [87, 89, 125, 102] or logic programming [25, 44, 103, 104, 114, 117], We have examined the complexity of the latter formalisms elsewhere [27, 28, 29]; again, some results and proofs, but not all of them, are similar to the ones presented here.", "startOffset": 95, "endOffset": 113}, {"referenceID": 123, "context": "There are also languages that encode repetitive Bayesian networks using functional programming [87, 89, 125, 102] or logic programming [25, 44, 103, 104, 114, 117], We have examined the complexity of the latter formalisms elsewhere [27, 28, 29]; again, some results and proofs, but not all of them, are similar to the ones presented here.", "startOffset": 95, "endOffset": 113}, {"referenceID": 100, "context": "There are also languages that encode repetitive Bayesian networks using functional programming [87, 89, 125, 102] or logic programming [25, 44, 103, 104, 114, 117], We have examined the complexity of the latter formalisms elsewhere [27, 28, 29]; again, some results and proofs, but not all of them, are similar to the ones presented here.", "startOffset": 95, "endOffset": 113}, {"referenceID": 24, "context": "There are also languages that encode repetitive Bayesian networks using functional programming [87, 89, 125, 102] or logic programming [25, 44, 103, 104, 114, 117], We have examined the complexity of the latter formalisms elsewhere [27, 28, 29]; again, some results and proofs, but not all of them, are similar to the ones presented here.", "startOffset": 135, "endOffset": 163}, {"referenceID": 42, "context": "There are also languages that encode repetitive Bayesian networks using functional programming [87, 89, 125, 102] or logic programming [25, 44, 103, 104, 114, 117], We have examined the complexity of the latter formalisms elsewhere [27, 28, 29]; again, some results and proofs, but not all of them, are similar to the ones presented here.", "startOffset": 135, "endOffset": 163}, {"referenceID": 101, "context": "There are also languages that encode repetitive Bayesian networks using functional programming [87, 89, 125, 102] or logic programming [25, 44, 103, 104, 114, 117], We have examined the complexity of the latter formalisms elsewhere [27, 28, 29]; again, some results and proofs, but not all of them, are similar to the ones presented here.", "startOffset": 135, "endOffset": 163}, {"referenceID": 102, "context": "There are also languages that encode repetitive Bayesian networks using functional programming [87, 89, 125, 102] or logic programming [25, 44, 103, 104, 114, 117], We have examined the complexity of the latter formalisms elsewhere [27, 28, 29]; again, some results and proofs, but not all of them, are similar to the ones presented here.", "startOffset": 135, "endOffset": 163}, {"referenceID": 112, "context": "There are also languages that encode repetitive Bayesian networks using functional programming [87, 89, 125, 102] or logic programming [25, 44, 103, 104, 114, 117], We have examined the complexity of the latter formalisms elsewhere [27, 28, 29]; again, some results and proofs, but not all of them, are similar to the ones presented here.", "startOffset": 135, "endOffset": 163}, {"referenceID": 115, "context": "There are also languages that encode repetitive Bayesian networks using functional programming [87, 89, 125, 102] or logic programming [25, 44, 103, 104, 114, 117], We have examined the complexity of the latter formalisms elsewhere [27, 28, 29]; again, some results and proofs, but not all of them, are similar to the ones presented here.", "startOffset": 135, "endOffset": 163}, {"referenceID": 26, "context": "There are also languages that encode repetitive Bayesian networks using functional programming [87, 89, 125, 102] or logic programming [25, 44, 103, 104, 114, 117], We have examined the complexity of the latter formalisms elsewhere [27, 28, 29]; again, some results and proofs, but not all of them, are similar to the ones presented here.", "startOffset": 232, "endOffset": 244}, {"referenceID": 27, "context": "There are also languages that encode repetitive Bayesian networks using functional programming [87, 89, 125, 102] or logic programming [25, 44, 103, 104, 114, 117], We have examined the complexity of the latter formalisms elsewhere [27, 28, 29]; again, some results and proofs, but not all of them, are similar to the ones presented here.", "startOffset": 232, "endOffset": 244}, {"referenceID": 28, "context": "There are also languages that encode repetitive Bayesian networks using functional programming [87, 89, 125, 102] or logic programming [25, 44, 103, 104, 114, 117], We have examined the complexity of the latter formalisms elsewhere [27, 28, 29]; again, some results and proofs, but not all of them, are similar to the ones presented here.", "startOffset": 232, "endOffset": 244}, {"referenceID": 58, "context": "[60]; note the constraint 2dag, meaning that each child of the node has at most two parents and cannot be his or her own ancestor.", "startOffset": 0, "endOffset": 4}, {"referenceID": 128, "context": "Valiant defines, for complexity class A, the class #A to be \u222aL\u2208A(#P), where (#P) is the class of functions counting the accepting paths of nondeterministic polynomial time Turing machines with L as oracle [130].", "startOffset": 205, "endOffset": 210}, {"referenceID": 126, "context": "Valiant\u2019s is a very loose notion of hardness; as shown by Toda and Watanabe [128], any function in #PH can be reduced to a function in #P via a one-Turing reduction (where #PH is a counting class with the whole polynomial hierarchy as oracle).", "startOffset": 76, "endOffset": 81}, {"referenceID": 7, "context": "For this reason, other reductions have been considered for counting problems [8, 41].", "startOffset": 77, "endOffset": 84}, {"referenceID": 39, "context": "For this reason, other reductions have been considered for counting problems [8, 41].", "startOffset": 77, "endOffset": 84}, {"referenceID": 117, "context": "A somewhat stringent strategy is to say that f is #P-hard if any function f \u2032 in #P can be produced from f by a parsimonious reduction; that is, f (l) is computed by applying a polynomial time function g to x and then computing f(g(l)) [119].", "startOffset": 236, "endOffset": 241}, {"referenceID": 73, "context": "This has been done both in the context of probabilistic inference with \u201creductions modulo normalization\u201d [75] and in the context of probabilistic databases [123].", "startOffset": 105, "endOffset": 109}, {"referenceID": 121, "context": "This has been done both in the context of probabilistic inference with \u201creductions modulo normalization\u201d [75] and in the context of probabilistic databases [123].", "startOffset": 156, "endOffset": 161}, {"referenceID": 15, "context": "in their study of weighted constraint satisfaction problems [16].", "startOffset": 60, "endOffset": 64}, {"referenceID": 113, "context": "notes that \u201cstrictly speaking the problem of computing the degree of belief is not in #P, but easily seem equivalent to a problem in this class\u201d [115].", "startOffset": 145, "endOffset": 150}, {"referenceID": 0, "context": "Similarly, Campos, Stamoulis and Weyland take f to be #P[1]-equivalent if f is #P-hard (in Valiant\u2019s sense) and belongs to FP.", "startOffset": 56, "endOffset": 59}, {"referenceID": 0, "context": "Here the superscript #P[1] means that the oracle #P can be called only once.", "startOffset": 23, "endOffset": 26}, {"referenceID": 126, "context": "It is certainly a good idea to resort to a new term (\u201cequivalence\u201d) in this context; however one must feel that membership to FP is too weak a requirement given Toda and Watanabe\u2019s theorem [128]: any function in #PH can be produced within FP.", "startOffset": 189, "endOffset": 194}, {"referenceID": 95, "context": "For this reason, we take #EXP to be the class of functions that can be computed by counting machines of exponential time complexity [97].", "startOffset": 132, "endOffset": 136}, {"referenceID": 75, "context": "We will use the class \u266ePSPACE class defined by Ladner [77], consisting of those functions that can be computed by counting Turing machines with a polynomial space bound and a polynomial bound on the number of nondeterministic moves.", "startOffset": 54, "endOffset": 58}, {"referenceID": 93, "context": "It is actually likely that functions that are proportional to conditional probabilities P(Q|E) cannot be produced by counting Turing machines, as classes in Valiant\u2019s counting hierarchy are not likely to be closed under division even by polynomial-time computable functions [95].", "startOffset": 274, "endOffset": 278}, {"referenceID": 121, "context": "Thus we must focus on inferences of the form P(Q); indeed this is the sort of computation that is analyzed in probabilistic databases [123].", "startOffset": 134, "endOffset": 139}, {"referenceID": 43, "context": "In a different direction, we must look at parameterized counting classes [45], so as to refine the analysis even further.", "startOffset": 73, "endOffset": 77}, {"referenceID": 69, "context": "There are also models that encode structural uncertainty, say about the presence of edges [71], and novel techniques must be developed to investigate the complexity of such models.", "startOffset": 90, "endOffset": 94}, {"referenceID": 36, "context": "[38].", "startOffset": 0, "endOffset": 4}, {"referenceID": 77, "context": "We now encode this Turing machine using monadic logic, mixing some ideas by Lewis [79] and by Tobies [126].", "startOffset": 82, "endOffset": 86}, {"referenceID": 124, "context": "We now encode this Turing machine using monadic logic, mixing some ideas by Lewis [79] and by Tobies [126].", "startOffset": 101, "endOffset": 106}, {"referenceID": 77, "context": "We refer to each Cj as a tile, as we are in effect encoding a domino system [79].", "startOffset": 76, "endOffset": 80}, {"referenceID": 29, "context": "(The inferential complexity of ALC has been derived, with a different proof, by Cozman and Polastro [30].", "startOffset": 100, "endOffset": 104}, {"referenceID": 53, "context": "To encode M we resort to a construction by Gradel [55] where relations of arity two are used.", "startOffset": 50, "endOffset": 54}, {"referenceID": 34, "context": "The remainder of this proof just reproduces a construction by Park in his proof of PP-completeness for propositional Bayesian networks [36].", "startOffset": 135, "endOffset": 139}, {"referenceID": 63, "context": "Jaeger describes results implying, in case NETIME 6= ETIME, that there is a sentence \u03c6 \u2208 FFFO such that the spectrum of \u03c6 cannot be recognized in deterministic exponential time [65].", "startOffset": 177, "endOffset": 181}, {"referenceID": 53, "context": "Recall: the spectrum of a sentence is a set containing each integer N , in binary notation, such that \u03c6 has a model whose domain size is N [55].", "startOffset": 139, "endOffset": 143}, {"referenceID": 134, "context": "To prove membership, use the Turing machine described in the proof of membership in Theorem 7, with a small difference: when it is necessary to check whether E (or Q \u222a E) holds given a guessed assignment for root nodes, use the appropriate model checking algorithm [136], as this verification can be done in polynomial time.", "startOffset": 265, "endOffset": 270}, {"referenceID": 131, "context": "For k \u2264 2, results in the literature show how to count the number of satisfying models of a formula in polynomial time [133, 135].", "startOffset": 119, "endOffset": 129}, {"referenceID": 133, "context": "For k \u2264 2, results in the literature show how to count the number of satisfying models of a formula in polynomial time [133, 135].", "startOffset": 119, "endOffset": 129}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 131, "context": "Domain complexity is polynomial because EL is in FFFO [133, 135].", "startOffset": 54, "endOffset": 64}, {"referenceID": 133, "context": "Domain complexity is polynomial because EL is in FFFO [133, 135].", "startOffset": 54, "endOffset": 64}, {"referenceID": 34, "context": "Observe that only the nodes with assignments in Q and their ancestors are relevant for the computation of P(Q), as every other node in the Bayesian network is barren [36].", "startOffset": 166, "endOffset": 170}, {"referenceID": 113, "context": "Hardness is trivial: even Prop(\u2227,\u00ac) is #P-equivalent, as Prop(\u2227,\u00ac) suffice to specify any propositional Bayesian network, and equivalence then obtains [115].", "startOffset": 151, "endOffset": 156}, {"referenceID": 5, "context": ", the problem of deciding whether a satisfying truth-value assignment exists) is a special case of model counting; probabilistic reasoning in graphical models such as Bayesian networks can be reduced to a weighted variant of model counting [6, 36]; validity of conformal plans can be formulated as model counting [96].", "startOffset": 240, "endOffset": 247}, {"referenceID": 34, "context": ", the problem of deciding whether a satisfying truth-value assignment exists) is a special case of model counting; probabilistic reasoning in graphical models such as Bayesian networks can be reduced to a weighted variant of model counting [6, 36]; validity of conformal plans can be formulated as model counting [96].", "startOffset": 240, "endOffset": 247}, {"referenceID": 94, "context": ", the problem of deciding whether a satisfying truth-value assignment exists) is a special case of model counting; probabilistic reasoning in graphical models such as Bayesian networks can be reduced to a weighted variant of model counting [6, 36]; validity of conformal plans can be formulated as model counting [96].", "startOffset": 313, "endOffset": 317}, {"referenceID": 108, "context": "For example, the problem is #Pcomplete even when the formulas are in conjunctive normal form with two variables per clause, there is no negation, and the variables can be partitioned into two sets such that no clause contains two variables in the same block [110].", "startOffset": 258, "endOffset": 263}, {"referenceID": 127, "context": "variable appears at most twice, or when the formula is monotone, the clauses contain two variables and each variables appears at most k times for any k \u2265 5 [129].", "startOffset": 156, "endOffset": 161}, {"referenceID": 113, "context": "A few tractable classes have been found: for example, Roth [115] developed an algorithm for counting the number of satisfying assignments of formulas in conjunctive normal form with two variables per clause, each variable appearing in at most two clauses.", "startOffset": 59, "endOffset": 64}, {"referenceID": 13, "context": "Relaxing the constraint on the number of variables per clauses takes us back to intractability: model counting restricted to formulas in conjunctive normal form with variables appearing in at most two clauses is #P-complete [14].", "startOffset": 224, "endOffset": 228}, {"referenceID": 127, "context": "Computing the number of satisfying assingments for monotone formulas in conjunctive normal form, with at most two variables per clause, with each variable appearing at most four times is #P-complete even when the primal graph (where nodes are variables and an edge connects variables that coappear in a clause) is bipartite and planar [129].", "startOffset": 335, "endOffset": 340}, {"referenceID": 138, "context": ", counting modulo two) in conjunctive normal form formulas where each variable appears at most twice, each clause has at most three variables, and the incidence graph (where nodes are variables and clauses, and edges connect variables appearing in clauses) of the formula is planar is known to be NP-hard by a randomized reduction [140].", "startOffset": 331, "endOffset": 336}, {"referenceID": 130, "context": "Interestingly, counting the number of satisfying assignments modulo seven (!) of that same class of formulas is polynomial-time computable [132].", "startOffset": 139, "endOffset": 144}, {"referenceID": 79, "context": "An edge e = (u, v) \u2208 E can be classified into one of three categories: \u2022 free edge: if \u03c7(u) = \u03c7(v) = 0; \u2022 dangling edge: if \u03c7(u) 6= \u03c7(v); or In [81] and [82], graphs are uncolored, but edges might contain empty endpoints.", "startOffset": 144, "endOffset": 148}, {"referenceID": 80, "context": "An edge e = (u, v) \u2208 E can be classified into one of three categories: \u2022 free edge: if \u03c7(u) = \u03c7(v) = 0; \u2022 dangling edge: if \u03c7(u) 6= \u03c7(v); or In [81] and [82], graphs are uncolored, but edges might contain empty endpoints.", "startOffset": 153, "endOffset": 157}, {"referenceID": 79, "context": "The classifications of edges given here are analogous to those defined in [81, 82], but not fully equivalent.", "startOffset": 74, "endOffset": 82}, {"referenceID": 80, "context": "The classifications of edges given here are analogous to those defined in [81, 82], but not fully equivalent.", "startOffset": 74, "endOffset": 82}, {"referenceID": 79, "context": "Regular edges are analogous to the normal edges defined in [81, 82].", "startOffset": 59, "endOffset": 67}, {"referenceID": 80, "context": "Regular edges are analogous to the normal edges defined in [81, 82].", "startOffset": 59, "endOffset": 67}, {"referenceID": 17, "context": "Computing Z(G) is #P-complete [18], and admits an FPTAS [81, 82].", "startOffset": 30, "endOffset": 34}, {"referenceID": 79, "context": "Computing Z(G) is #P-complete [18], and admits an FPTAS [81, 82].", "startOffset": 56, "endOffset": 64}, {"referenceID": 80, "context": "Computing Z(G) is #P-complete [18], and admits an FPTAS [81, 82].", "startOffset": 56, "endOffset": 64}, {"referenceID": 12, "context": "The above Markov chain can be shown to be ergodic and to converge to a stationary distribution which samples an edge cover C with probability \u03bb [13, 11].", "startOffset": 144, "endOffset": 152}, {"referenceID": 10, "context": "The above Markov chain can be shown to be ergodic and to converge to a stationary distribution which samples an edge cover C with probability \u03bb [13, 11].", "startOffset": 144, "endOffset": 152}], "year": 2016, "abstractText": "We examine the complexity of inference in Bayesian networks specified by logical languages. We consider representations that range from fragments of propositional logic to function-free first-order logic with equality; in doing so we cover a variety of plate models and of probabilistic relational models. We study the complexity of inferences when network, query and domain are the input (the inferential and the combined complexity), when the network is fixed and query and domain are the input (the query/data complexity), and when the network and query are fixed and the domain is the input (the domain complexity). We draw connections with probabilistic databases and liftability results, and obtain complexity classes that range from polynomial to exponential levels.", "creator": "LaTeX with hyperref package"}}}