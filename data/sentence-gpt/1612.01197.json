{"id": "1612.01197", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2016", "title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision (Short Version)", "abstract": "Extending the success of deep neural networks to natural language understanding and symbolic reasoning requires complex operations and external memory. Recent neural program induction approaches have attempted to address this problem, but are typically limited to differentiable memory, and consequently cannot scale beyond small synthetic tasks. In this work, we propose the Manager-Programmer-Computer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface. Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural \"programmer\", and a non-differentiable \"computer\" that is a Lisp interpreter with code assist. To successfully apply REINFORCE for training, we augment it with approximate gold programs found by an iterative maximum likelihood training process. NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WebQuestionsSP, a challenging semantic parsing dataset, with weak supervision. Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge. The ML programs are not specifically geared for language processing or training, so our ML programs are used to perform some of the same functions. The program's basic state-of-the-art performance is consistent with previous approaches, but has been criticized for performance degradation (1) because of its high level of complexity.\n\n\n\n\nIntroduction\nMachine learning (machine learning) is an extremely expensive and expensive task that requires enormous amounts of computational resources. Learning the full language of machine learning is expensive, and requires lots of CPU and GPU power. Machine learning requires considerable amounts of computation and training to create a fully synthetic-programmable program. It is also possible to generate large, highly flexible programs that support multiple, but not large-scale, and therefore require little or no computational resources. However, there is a common point in machine learning that requires significant amounts of computational resources. A system that relies upon this type of machine learning has several benefits: the general approach is to build a program that is highly scalable with a large variety of components, such as a neural network, which has several features. The general approach is to build a program that is highly scalable with a large variety of components, such as a neural network, which has several features. The general approach is to build a program that is highly scalable with a large variety of components, such as a neural network, which has several features. The", "histories": [["v1", "Sun, 4 Dec 2016 22:29:32 GMT  (74kb,D)", "http://arxiv.org/abs/1612.01197v1", "Published in NAMPI workshop at NIPS 2016. Short version ofarXiv:1611.00020"]], "COMMENTS": "Published in NAMPI workshop at NIPS 2016. Short version ofarXiv:1611.00020", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["chen liang", "jonathan berant", "quoc le", "kenneth d forbus", "ni lao"], "accepted": true, "id": "1612.01197"}, "pdf": {"name": "1612.01197.pdf", "metadata": {"source": "CRF", "title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision (Short Version)", "authors": ["Chen Liang", "Jonathan Berant", "Quoc Le", "Kenneth Forbus", "Ni Lao"], "emails": ["chenliang2013@u.northwestern.edu", "forbus@u.northwestern.edu", "joberant@cs.tau.ac.il", "qvl@google.com", "nlao@google.com", "@NIPS"], "sections": [{"heading": "1 Introduction", "text": "Deep neural networks have achieved impressive performance in classification and structured prediction tasks with full supervision such as speech recognition [9] and machine translation [14, 2, 16]. Extending the success to natural language understanding and symbolic reasoning requires the ability to perform complex operations and make use of an external memory. There were several recent attempts to address this problem in neural program induction [7, 12, 13, 11, 19, 8, 1], which learn programs by using a neural sequence model to control a computation component. However, the memories in these models are either low-level (such as in Neural Turing machines[19]), or differentiable so that they can be trained by backpropagation. This makes it difficult to utilize efficient discrete memory in a traditional computer, and limits their application to small synthetic tasks. \u2217The long version with more details can be found at https://arxiv.org/abs/1611.00020. \u2020Work done while the author was interning at Google \u2021Work done while the author was a visiting scholar at Google\n1st Workshop on Neural Abstract Machines & Program Induction (NAMPI), @NIPS 2016, Barcelona, Spain.\nar X\niv :1\n61 2.\n01 19\n7v 1\n[ cs\n.C L\nTo better utilize efficient memory and operations, we propose a Manager-Programmer-Computer (MPC) framework for neural program induction, which integrates three components:\n1. A \"manager\" that provides weak supervision through input and a reward signal indicating how well a task is performed. Unlike full supervision, this weak supervision is much easier to obtain at large scale (see an example task in Section 3).\n2. A \"programmer\" that takes natural language as input and generates a program that is a sequence of tokens. The programmer learns from the reward signal and must overcome the hard search problem of finding good programs. (Section 2.2).\n3. A \"computer\" that executes the program. It can use all the operations that can be implemented as a function in a high level programming language like Lisp. The non-differentiable memory enables abstract, scalable and precise operations, but it requires reinforcement learning. It also provides a friendly neural computer interface to help the \"programmer\" reduce the search space by detecting and eliminating invalid choices (Section 2.1).\nWithin the MPC framework, we introduce the Neural Symbolic Machine (NSM) and apply it to semantic parsing. NSM contains a sequence-to-sequence neural network model (\"programmer\") augmented with a key-variable memory to save and reuse intermediate results for compositionality, and a non-differentiable Lisp interpreter (\"computer\") that executes programs against a large knowledge base. As code assist, the \"computer\" also helps reduce the search space by checking for syntax and semantic errors. Compared to existing neural program induction approaches, the efficient memory and friendly interface of the \"computer\" greatly reduce the burden of the \"programmer\" and enable the model to perform competitively on real applications. On the challenging semantic parsing dataset WEBQUESTIONSSP [18], NSM achieves new state-of-the-art results with weak supervision. Compared to previous work, it is end-to-end, therefore does not require any feature engineering or domain-specific knowledge."}, {"heading": "2 Neural Symbolic Machines", "text": "Now we describe in details a Neural Symbolic Machine that falls into the MPC framework, and how it is applied to learn semantic parsing from weak supervision.\nSemantic parsing is defined as follows: given a knowledge base (KB) K, and a question q = (w1, w2, ..., wk), produce a program or logical form z that when executed against K generates the right answer y. Let E denote a set of entities (e.g., ABELINCOLN)4, and let P denote a set of properties (or relations, e.g., PLACEOFBIRTHOF). A knowledge base K is a set of assertions (e1, p, e2) \u2208 E \u00d7 P \u00d7 E , such as (HODGENVILLE, PLACEOFBIRTHOF, ABELINCOLN))."}, {"heading": "2.1 \"Computer\": Lisp interpreter with code assist", "text": "Operations learned by current neural network models with differentiable memory, such as addition or sorting, do not generalize perfectly to inputs that are larger than previously observed ones [7, 13]. In contrast, operations implemented in ordinary programming language are abstract, scalable, and precise, because no matter how large the input is or whether it has been seen or not, they will be processed precisely. Based on this observation, we implement all the operations necessary for semantic parsing with ordinary non-differentiable memory, and allow the \"programmer\" to use them with a high level general purpose programming language.\nWe adopt a Lisp interpreter with predefined functions listed in 1 as the \"computer\". The programs that can be executed by it are equivalent to the limited subset of \u03bb-calculus in [17], but easier for a sequence-to-sequence model to generate given Lisp\u2019s simple syntax. Because Lisp is a generalpurpose and high level language, it is easy to extend the model with more operations, which can be implemented as new functions, and complex constructs like control flows and loops.\nA program C is a list of expressions (c1...cN ). Each expression is either a special token \"RETURN\" indicating the end of the program, or a list of tokens enclosed by parentheses \"( F A0 ... AK )\". F is one of the functions in Table 1, which take as input a list of arguments of specific types, and, when executed, returns the denotation of this expression in K, which is typically a list of entities, and saves\n4We also consider numbers (e.g., \u201c1.33\u201d) and date-times (e.g., \u201c1999-1-1\u201d) as entities.\nit in a new variable. Ak is F \u2019s kth argument, which can be either a relation p \u2208 P or a variable v. The variables hold the results from previous computations, which can be either a list of entities from executing an expression or an entity resolved from the natural language input.\nTo create a better neural computer interface, the interpreter provides code assist by producing a list of valid tokens for the \"programmer\" to pick from at each step. First, a valid token should not cause a syntax error, which is usually checked by modern compilers. For example, if the previous token is \"(\", the next token must be a function, and if the previous token is \"Hop\", the next token must be a variable. More importantly, a valid token should not cause a semantic error or run-time error, which can be detected by the interpreter using the value or denotation of previous expressions. For example, given that the previously generated tokens are \"(\", \"Hop\", \"v\", the next available token is restricted to the set of relations that are reachable from entities in v. By providing this neural computer interface, the interpreter reduces the \"programmer\"\u2019s search space by orders of magnitude, and enables weakly supervised learning on a large knowledge base."}, {"heading": "2.2 \"Programmer\": key-variable memory augmented Seq2Seq model", "text": "The \"computer\" implements the operations (functions) and stores the values (intermediate results) in variables, which simplifies the task for the \"programmer\". The \"programmer\" only needs to map natural language into a program, which is a sequence of tokens that references operations and values in the \"computer\". We use a standard sequence-to-sequence model with attention and augment it with a key-variable memory to reference the values.\nA typical sequence-to-sequence model consists of two RNNs, an encoder and a decoder. We used a 1-layer GRU [5], which is a simplified variant of LSTM, for both the encoder and the decoder. Given a sequence of words w1, w2...wm, each word wt is mapped to a multi-dimensional embedding qt (see details about the embeddings in Section 3). Then, the encoder reads in these embeddings and updates its hidden state step by step using: ht+1 = GRU(ht, qt, \u03b8Encoder), where \u03b8Encoder are the GRU parameters. The decoder updates its hidden states ut by ut+1 = GRU(ut, ct\u22121, \u03b8Decoder), where ct\u22121 is the embedding of last step\u2019s output token at\u22121 (see details about the embeddings in Section 3), and \u03b8Decoder are the GRU parameters. The last hidden state of the encoder hT is used as the decoder\u2019s initial state. We adopt a dot-product attention similar to that of [6]. The tokens of the program a1, a2...an are generated one by one using a softmax over the vocabulary of valid tokens for each step (Section 2.1).\nTo achieve compositionality, we augment the model with a key-variable memory (Figure 1). Each entry in the key-variable memory has two components: a continuous multi-dimensional embedding\nkey vi, and a corresponding variable Ri that references certain result in the \"computer\". During encoding if a token (\"US\") is the last token of a resolved entity (by an entity resolver), then the resolved entity id (m.USA) is saved in a new variable in the \"computer\", and the key embedding for this variable is the average GRU output of the tokens spanned by this entity. During decoding if an expression is completely finished (the decoder reads in \")\"), it gets executed, and the result is stored as the value of a new variable in the \"computer\". This variable is keyed by the GRU output of that step. Every time a new variable is pushed into the memory, the variable token is added to the vocabulary of the decoder."}, {"heading": "2.3 Training NSM with Weak Supervision", "text": "To efficiently train NSM from weak supervision, we apply the REINFORCE algorithm [15, 19]. However, the REINFORCE objective is known to be very hard to optimize starting from scratch. Therefore, we augment it with approximate gold programs found by an iterative maximum likelihood training process. During training, the model always puts a reasonable amount of probability on the best programs found so far, and anchoring the model to these high-reward programs greatly speeds up the training and helps to avoid local optimum. More details of the training procedure can be found in the long version."}, {"heading": "3 Experiments and analysis", "text": "Modern semantic parsers [4], which map natural language utterances to executable logical forms, have been successfully trained over large knowledge bases from weak supervision[17], but require substantial feature engineering. Recent attempts to train an end-to-end neural network for semantic parsing [6, 10] have either used strong supervision (full logical forms), or have employed synthetic datasets.\nWe apply NSM to learn a semantic parser with weak supervision and no manual engineering. we used the challenging semantic parsing dataset WEBQUESTIONSSP [18], which consists of 3,098 question-answer pairs for training and 1,639 for testing. These questions were collected using Google Suggest API and the answers were originally obtained [3] using Amazon Mechanical Turk and updated by annotators who are familiar with the design of Freebase [18]. We further separate out 620 questions in the training set as validation set. For query pre-prosessing we used an in-house named entity linking system to find the entities in a question. The quality of the entity resolution is similar to that of [17] with about 94% of the gold root entities being included in the resolution results. Similar to [6], we also replaced named entity tokens with a special token \"ENT\". For example, the question \"who plays meg in family guy\" is changed to \"who plays ENT in ENT ENT\".\nFollowing [17] we use the last public available snapshot of Freebase KB. Since NSM training requires random access to Freebase during decoding, we preprocess Freebase by removing predicates that are not related to world knowledge (starting with \"/common/\", \"/type/\", \"/freebase/\")5, and removing all text valued predicates, which are rarely the answer. This results in a graph with 23K relations, 82M nodes, and 417M edges.\nWe evaluate performance using the offical measures for WEBQUESTIONSSP. Because the answer to a question can contain multiple entities or values, precision, recall and F1 are computed based on the output for each individual question. The average F1 score is reported as the main evaluation metric. The accuracy@1 measures the percentage of questions that are answered exactly. The comparison with previous state-of-the-art [18, 17] is shown in Table 2. Besides the better performance, our model does not rely on domain-specific rules or feature engineering. More ablation studies and analysis are included in the long version.\n5Except that we kept \u201c/common/topic/notable_types\u201d."}], "references": [{"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang"], "venue": "In EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Learning phrase representations using rnn encoder\u2013 decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Language to logical form with neural attention", "author": ["Li Dong", "Mirella Lapata"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Hybrid computing using a neural network", "author": ["Alex Graves", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka Grabska- Barwinska", "Sergio G. Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou", "Adri\u00c3 P. Badia", "Karl M. Hermann", "Yori Zwols", "Georg Ostrovski", "Adam Cain", "Helen King", "Christopher Summerfield", "Phil Blunsom", "Koray Kavukcuoglu", "Demis Hassabis"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Data recombination for neural semantic parsing", "author": ["Robin Jia", "Percy Liang"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Neural gpus learn algorithms", "author": ["\u0141ukasz Kaiser", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1511.08228,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever"], "venue": "CoRR, abs/1511.04834,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Neural programmer-interpreters", "author": ["Scott Reed", "Nando de Freitas"], "venue": "In ICLR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "In Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V. Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey", "Jeff Klingner", "Apurva Shah", "Melvin Johnson", "Xiaobing Liu", "Lukasz Kaiser", "Stephan Gouws", "Yoshikiyo Kato", "Taku Kudo", "Hideto Kazawa", "Keith Stevens", "George Kurian", "Nishant Patil", "Wei Wang", "Cliff Young", "Jason Smith", "Jason Riesa", "Alex Rudnick", "Oriol Vinyals", "Greg Corrado", "Macduff Hughes", "Jeffrey Dean"], "venue": "translation. CoRR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "The value of semantic parse labeling for knowledge base question answering", "author": ["Wen-tau Yih", "Matthew Richardson", "Chris Meek", "Ming-Wei Chang", "Jina Suh"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Reinforcement learning neural turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Deep neural networks have achieved impressive performance in classification and structured prediction tasks with full supervision such as speech recognition [9] and machine translation [14, 2, 16].", "startOffset": 157, "endOffset": 160}, {"referenceID": 12, "context": "Deep neural networks have achieved impressive performance in classification and structured prediction tasks with full supervision such as speech recognition [9] and machine translation [14, 2, 16].", "startOffset": 185, "endOffset": 196}, {"referenceID": 1, "context": "Deep neural networks have achieved impressive performance in classification and structured prediction tasks with full supervision such as speech recognition [9] and machine translation [14, 2, 16].", "startOffset": 185, "endOffset": 196}, {"referenceID": 14, "context": "Deep neural networks have achieved impressive performance in classification and structured prediction tasks with full supervision such as speech recognition [9] and machine translation [14, 2, 16].", "startOffset": 185, "endOffset": 196}, {"referenceID": 5, "context": "There were several recent attempts to address this problem in neural program induction [7, 12, 13, 11, 19, 8, 1], which learn programs by using a neural sequence model to control a computation component.", "startOffset": 87, "endOffset": 112}, {"referenceID": 10, "context": "There were several recent attempts to address this problem in neural program induction [7, 12, 13, 11, 19, 8, 1], which learn programs by using a neural sequence model to control a computation component.", "startOffset": 87, "endOffset": 112}, {"referenceID": 11, "context": "There were several recent attempts to address this problem in neural program induction [7, 12, 13, 11, 19, 8, 1], which learn programs by using a neural sequence model to control a computation component.", "startOffset": 87, "endOffset": 112}, {"referenceID": 9, "context": "There were several recent attempts to address this problem in neural program induction [7, 12, 13, 11, 19, 8, 1], which learn programs by using a neural sequence model to control a computation component.", "startOffset": 87, "endOffset": 112}, {"referenceID": 17, "context": "There were several recent attempts to address this problem in neural program induction [7, 12, 13, 11, 19, 8, 1], which learn programs by using a neural sequence model to control a computation component.", "startOffset": 87, "endOffset": 112}, {"referenceID": 6, "context": "There were several recent attempts to address this problem in neural program induction [7, 12, 13, 11, 19, 8, 1], which learn programs by using a neural sequence model to control a computation component.", "startOffset": 87, "endOffset": 112}, {"referenceID": 0, "context": "There were several recent attempts to address this problem in neural program induction [7, 12, 13, 11, 19, 8, 1], which learn programs by using a neural sequence model to control a computation component.", "startOffset": 87, "endOffset": 112}, {"referenceID": 17, "context": "However, the memories in these models are either low-level (such as in Neural Turing machines[19]), or differentiable so that they can be trained by backpropagation.", "startOffset": 93, "endOffset": 97}, {"referenceID": 16, "context": "On the challenging semantic parsing dataset WEBQUESTIONSSP [18], NSM achieves new state-of-the-art results with weak supervision.", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "Operations learned by current neural network models with differentiable memory, such as addition or sorting, do not generalize perfectly to inputs that are larger than previously observed ones [7, 13].", "startOffset": 193, "endOffset": 200}, {"referenceID": 11, "context": "Operations learned by current neural network models with differentiable memory, such as addition or sorting, do not generalize perfectly to inputs that are larger than previously observed ones [7, 13].", "startOffset": 193, "endOffset": 200}, {"referenceID": 15, "context": "The programs that can be executed by it are equivalent to the limited subset of \u03bb-calculus in [17], but easier for a sequence-to-sequence model to generate given Lisp\u2019s simple syntax.", "startOffset": 94, "endOffset": 98}, {"referenceID": 3, "context": "We used a 1-layer GRU [5], which is a simplified variant of LSTM, for both the encoder and the decoder.", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "We adopt a dot-product attention similar to that of [6].", "startOffset": 52, "endOffset": 55}, {"referenceID": 13, "context": "To efficiently train NSM from weak supervision, we apply the REINFORCE algorithm [15, 19].", "startOffset": 81, "endOffset": 89}, {"referenceID": 17, "context": "To efficiently train NSM from weak supervision, we apply the REINFORCE algorithm [15, 19].", "startOffset": 81, "endOffset": 89}, {"referenceID": 15, "context": "Modern semantic parsers [4], which map natural language utterances to executable logical forms, have been successfully trained over large knowledge bases from weak supervision[17], but require substantial feature engineering.", "startOffset": 175, "endOffset": 179}, {"referenceID": 4, "context": "Recent attempts to train an end-to-end neural network for semantic parsing [6, 10] have either used strong supervision (full logical forms), or have employed synthetic datasets.", "startOffset": 75, "endOffset": 82}, {"referenceID": 8, "context": "Recent attempts to train an end-to-end neural network for semantic parsing [6, 10] have either used strong supervision (full logical forms), or have employed synthetic datasets.", "startOffset": 75, "endOffset": 82}, {"referenceID": 16, "context": "we used the challenging semantic parsing dataset WEBQUESTIONSSP [18], which consists of 3,098 question-answer pairs for training and 1,639 for testing.", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "These questions were collected using Google Suggest API and the answers were originally obtained [3] using Amazon Mechanical Turk and updated by annotators who are familiar with the design of Freebase [18].", "startOffset": 97, "endOffset": 100}, {"referenceID": 16, "context": "These questions were collected using Google Suggest API and the answers were originally obtained [3] using Amazon Mechanical Turk and updated by annotators who are familiar with the design of Freebase [18].", "startOffset": 201, "endOffset": 205}, {"referenceID": 15, "context": "The quality of the entity resolution is similar to that of [17] with about 94% of the gold root entities being included in the resolution results.", "startOffset": 59, "endOffset": 63}, {"referenceID": 4, "context": "Similar to [6], we also replaced named entity tokens with a special token \"ENT\".", "startOffset": 11, "endOffset": 14}, {"referenceID": 15, "context": "Following [17] we use the last public available snapshot of Freebase KB.", "startOffset": 10, "endOffset": 14}, {"referenceID": 16, "context": "The comparison with previous state-of-the-art [18, 17] is shown in Table 2.", "startOffset": 46, "endOffset": 54}, {"referenceID": 15, "context": "The comparison with previous state-of-the-art [18, 17] is shown in Table 2.", "startOffset": 46, "endOffset": 54}], "year": 2016, "abstractText": "Extending the success of deep neural networks to natural language understanding and symbolic reasoning requires complex operations and external memory. Recent neural program induction approaches have attempted to address this problem, but are typically limited to differentiable memory, and consequently cannot scale beyond small synthetic tasks. In this work, we propose the Manager-ProgrammerComputer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface. Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural \"programmer\", and a nondifferentiable \"computer\" that is a Lisp interpreter with code assist. To successfully apply REINFORCE for training, we augment it with approximate gold programs found by an iterative maximum likelihood training process. NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WEBQUESTIONSSP, a challenging semantic parsing dataset. Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge.", "creator": "LaTeX with hyperref package"}}}