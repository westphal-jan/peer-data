{"id": "1612.01543", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Dec-2016", "title": "Towards the Limit of Network Quantization", "abstract": "Network quantization is one of network compression techniques employed to reduce the redundancy of deep neural networks. It compresses the size of the storage for a large number of network parameters in a neural network by quantizing them and encoding the quantized values into binary codewords of smaller sizes. In this paper, we aim to design network quantization schemes that minimize the expected loss due to quantization while maximizing the compression ratio. To this end, we analyze the quantitative relation of quantization errors to the loss function of a neural network and identify that the Hessian-weighted distortion measure is locally the right objective function that we need to optimize for minimizing the loss due to quantization. As a result, Hessian-weighted k-means clustering is proposed for clustering network parameters to quantize when fixed-length binary encoding follows. When optimal variable-length binary codes, e.g., Huffman codes, are employed for further compression of quantized values after clustering, we derive that the network quantization problem can be related to the entropy-constrained scalar quantization (ECSQ) problem in information theory and consequently propose two solutions of ECSQ for network quantization, i.e., uniform quantization and an iterative algorithm similar to Lloyd's algorithm for k-means clustering. Finally, using the simple uniform quantization followed by Huffman coding, our experiment results show that the compression ratios of 51.25, 22.17 and 40.65 are achievable (i.e., the sizes of the compressed models are 1.95%, 4.51% and 2.46% of the original model sizes) for LeNet, ResNet and AlexNet, respectively, at no or marginal performance loss.", "histories": [["v1", "Mon, 5 Dec 2016 21:04:17 GMT  (400kb)", "http://arxiv.org/abs/1612.01543v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["yoojin choi", "mostafa el-khamy", "jungwon lee"], "accepted": true, "id": "1612.01543"}, "pdf": {"name": "1612.01543.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yoojin Choi"], "emails": ["yoojin.c@samsung.com", "mostafa.e@samsung.com", "jungwon2.lee@samsung.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 2.\n01 54\n3v 1\n[ cs\n.C V\n] 5\nD ec\n2 01\n6 Under review as a conference paper at ICLR 2017"}, {"heading": "1 INTRODUCTION", "text": "Deep neural networks have emerged to be the state-of-the-art in the field of machine learning for image classification, object detection, speech recognition, natural language processing, and machine translation (LeCun et al., 2015). The substantial progress of neural networks however comes with additional cost of computations and resources resulting from an increasing number of parameters. For example, Krizhevsky et al. (2012) came up with a deep convolutional neural network consisting of 61 million parameters and won the ImageNet competition in 2012. It is followed by deeper neural networks with even larger numbers of parameters, e.g., Simonyan & Zisserman (2014).\nThe large sizes of deep neural networks make it difficult to deploy them on resource-limited devices, e.g., mobile or embedded devices. However, it has been shown that deep neural networks typically include many redundant parameters (Denil et al., 2013) and it is of great interest in recent years to reduce the redundancy of deep neural networks.\nThe whole procedure to reduce the redundancy of a neural network is called as network compression. The benefit of compression is twofold: computational cost reduction and hardware resource saving. In this paper, our interest is mainly on the size reduction of the storage (memory) for a large number of network parameters (weights and biases). In particular, we focus on the network size compression by quantizing network parameters. The most related work to our investigation can be found in Gong et al. (2014); Han et al. (2015a), where a conventional quantization method\nusing k-means clustering followed by binary encoding is employed for network quantization. In this paper, we identify the suboptimality of this conventional method and aim to newly design quantization schemes that maximize compression while minimizing the loss due to quantization for neural networks.\nNetwork pruning (Mozer & Smolensky, 1989; LeCun et al., 1989; Hassibi & Stork, 1993; Han et al., 2015b) and low-precision fixed- or floating-point implementation of neural networks (Vanhoucke et al., 2011; Hwang & Sung, 2014; Courbariaux et al., 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression. Some extremes of low-precision neural networks consisting of binary or ternary parameters were presented in Courbariaux et al. (2015); Lin et al. (2015b); Rastegari et al. (2016). Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015). We note that these are different types of network compression techniques, which can be employed on top of each other.\nA network compression framework consisting of network pruning, network quantization and finetuning is presented in Han et al. (2015a). Network pruning is employed to remove some of network parameters completely from a neural network by setting their values to be always zero. For a pruned network, one only needs to keep unpruned network parameters and their respective locations (indexes) in the original model. After network pruning, unpruned network parameters are fined-tuned to recover the loss due to pruning. The values for pruned parameters remain to be zero during this fine-tuning stage. Then, network quantization follows. Network quantization reduces the number of bits needed for representing (unpruned) network parameters by quantizing them and encoding their quantized values into binary codewords with smaller bit sizes. The quantized values can be retrieved from the binary codewords stored instead of actual values by using a lookup table of a small size. Finally, the quantized values in the lookup table can be further fined-tuned to reduce the loss due to quantization. We note that the fine-tuning stages are optional but it is preferred to have them in order to recover the loss due to compression after aggressive pruning and/or quantization.\nFor network quantization. we consider a neural network that is already trained, pruned if employed and fine-tuned before quantization. If no network pruning is employed, all parameters in a network are subject to quantization. For pruned networks, our focus is on quantization of unpruned parameters. We note that pruning and quantization methods can be designed jointly but it makes the problem more complicated. In particular, if fine-tuning of unpruned network parameters is assumed to be employed after aggressive pruning, it is difficult to predict and to consider the impact of fine-tuning in network quantization since unpruned parameters can change substantially from their original values after fine-tuning due to the fact that a large number of pruned parameters are fixed to be zero. Hence, in this paper, we treat them as two separable procedures for network compression and focus on network quantization.\nIn Gong et al. (2014); Han et al. (2015a), it is proposed to utilize the conventional k-means clustering method for quantizing network parameters. However, we identify that this conventional method has the following two issues when it is used for network quantization. First, k-means clustering treats all network parameters with equal importance in quantization while their quantization impact on the performance loss can be different. Second, k-means clustering does not consider the impact of the binary encoding scheme, followed after clustering for lossless compression of quantized values, on the compression ratio although it matters in particular when variable-length binary encoding is used, where the compression ratio depends not only on the number of clusters but also on the sizes of clusters and the lengths of the binary codewords assigned for them.\nIn order to address the issues that the conventional k-means clustering method cannot handle properly, this paper proposes (1) utilizing the second-order partial derivatives, i.e., the diagonal of Hessian matrix, of the loss function with respect to network parameters as a measure of the importance of network parameters and (2) solving the network quantization problem under a constraint of the actual compression ratio resulted in by the specific binary encoding scheme employed. The main contribution of the paper can be summarized as follows:\n\u2022 It is derived that the performance loss due to network quantization can be minimized locally by minimizing the Hessian-weighted distortion due to quantization. Furthermore, from this result, Hessian-weighted k-means clustering is proposed for network quantization when fixed-length binary encoding is employed.\n\u2022 It is identified that the optimization problem for network quantization under a constraint on the compression ratio can be reduced to an entropy-constrained scalar quantization (ECSQ) problem when entropy coding (i.e., optimal variable-length coding whose average codeword length for a given source approaches to the entropy of the source) is employed. Two efficient heuristic solutions for ECSQ are proposed for network quantization, i.e., (a) uniform quantization and (b) an iterative algorithm similar to Lloyd\u2019s algorithm.\n\u2022 As an alternative of Hessian, it is proposed to utilize some function (e.g., square root) of the second moments of gradients when the Adam (Kingma & Ba, 2014) stochastic gradient decent (SGD) optimizer is employed. Similar metrics can be obtained for other SGD optimizers, e.g., AdaGrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), and RMSProp (Tieleman & Hinton, 2012). The advantage of using these metrics instead of Hessian is that they are computed while training and can be obtained at the end of training at no additional cost.\n\u2022 It is shown how the proposed network quantization schemes can be applied for quantizing network parameters of all layers together at once, rather than layer-by-layer network quantization in Gong et al. (2014); Han et al. (2015a). This follows from our investigation that Hessian-weighting can handle the different impact of quantization errors properly not only within layers but also across layers.\nThe rest of the paper is organized as follows. Section 2 describes a general neural network model. In Section 3, we define the network quantization problem and review the conventional method using k-means clustering. Section 4 discusses network quantization using Hessian-weight. Our entropyconstrained network quantization schemes follow in Section 5. Experiment results and conclusion can be found in Section 6 and Section 7, respectively."}, {"heading": "2 NETWORK MODEL", "text": "We consider a general non-linear neural network that yields output y from input x according to\ny = f(x;w),\nwhere the function f is determined by the structure of the neural network while w = [w1 \u00b7 \u00b7 \u00b7 wN ]T is the vector consisting of all trainable network parameters in the network; N is the total number of trainable parameters in the network.\nA loss function loss(y, y\u0302) is defined as the objective function that we aim to minimize in average by training: loss(y, y\u0302) = loss(f(x;w), y\u0302(x)). Observe that y = f(x;w) is the predicted output from the network for input x and y\u0302 = y\u0302(x) is the expected (ground-truth) output for input x. Cross entropy or mean square error are typical examples of a loss function. We define the average loss function for any input data set X as follows:\nL(X ;w) = 1\n|X |\n\u2211\nx\u2208X\nloss(f(x;w), y\u0302(x)).\nGiven a training data set Xtrain, we optimize network parameters by solving the following problem, e.g., approximately by using stochastic gradient decent with mini-batches:\nw\u0302 = argmin w L(Xtrain;w) = argmin w\n\u2211\nx\u2208Xtrain\nloss(f(x;w), y\u0302(x))."}, {"heading": "3 NETWORK QUANTIZATION", "text": "The goal of network quantization is to quantize (unpruned) network parameters in order to reduce the size of the storage for them while minimizing the performance degradation due to quantization. Figure 2(a) illustrates an example of network quantization. For network quantization, network parameters are grouped into clusters. Parameters in the same cluster share their quantized value, which is the representative value (i.e., cluster center) of the cluster they belong to. After clustering, lossless binary encoding follows to encode quantized parameters into binary codewords to store instead of actual parameter values. Either fixed-length binary encoding or variable-length binary encoding, e.g., Huffman coding, can be employed to this end. Note that one also needs to keep a lookup table for decoding quantized values from their binary encoded codewords as shown in Figure 2(b)."}, {"heading": "3.1 COMPRESSION RATIO", "text": "Before quantization, each network parameter is assumed to be of b bits, implying that we need Nb bits for all N network parameters. Suppose that we partition the network parameters into k clusters. For 1 \u2264 i \u2264 k, let Ci be the set of network parameters in cluster i and let bi be the number of bits of the codeword assigned for the network parameters in cluster i. Then, we need\n\u2211k i=1 |Ci|bi bits in\ntotal for network parameters after quantization instead of Nb bits. For a lookup table that stores k binary codewords (bi bits for 1 \u2264 i \u2264 k) and corresponding quantized values (b bits for each), we additionally need\n\u2211k i=1 bi + kb bits. The compression ratio is then given by\nCompression ratio = Nb\n\u2211k i=1(|Ci|+ 1)bi + kb\n. (1)\nObserve in (1) that the compression ratio depends not only on the number of clusters but also on the sizes of the clusters and the lengths of the binary codewords assigned for them, in particular, when a\nvariable-length code is used for encoding quantized values. However, for fixed-length codes, where all codewords are of the same length, i.e., bi = \u2308log2 k\u2309 for all 1 \u2264 i \u2264 k, it reduces to\nCompression ratio = Nb\nN\u2308log2 k\u2309+ kb , (2)\nwhich is only a function of the number of clusters, i.e., k, assuming that N and b are given; here, we note that it is not necessary to store k binary codewords in a lookup table for fixed-length codes since they can be implicitly known, e.g., if quantized values are encoded into binary numbers ranging from 0 to k \u2212 1 in increasing order and are stored in a lookup table in the same order."}, {"heading": "3.2 K-MEANS CLUSTERING", "text": "Provided network parameters {wi}Ni=1 to quantize, k-means clustering partitions them into k disjoint sets (clusters), denoted by C1, C2, . . . , Ck, while minimizing the following distortion measure:\nargmin C1,C2,...,Ck\nk\u2211\ni=1\n\u2211\nw\u2208Ci\n|w \u2212 ci| 2, (3)\nwhere ci is the mean of network parameters in Ci, i.e.,\nci = 1\n|Ci|\n\u2211\nw\u2208Ci\nw.\nAfter k-means clustering, network parameters in cluster i are quantized to the same value, i.e., their cluster center ci. The k-means clustering problem is known to be NP-hard but there are efficient heuristic algorithms that can be employed in practice to find a local optimum. The most well-known heuristic algorithm is Lloyd\u2019s algorithm (Lloyd, 1982).\nThe distortion measure in (3) is the same as the mean square quantization error (MSQE) when normalized, implying that k-means clustering minimizes the MSQE for network quantization. However, we observe two issues with employing k-means clustering for network quantization.\n\u2022 First, although k-means clustering minimizes the MSQE, it does not imply that k-means clustering minimizes the performance loss due to quantization as well in neural networks. K-means clustering treats quantization errors from all network parameters with equal importance. However, quantization errors from some network parameters may impact significantly on the performance while quantization errors from some other network parameters may not be important and the impact on the performance could be negligible. Therefore, for minimizing the loss due to quantization in neural networks, one needs to take this dissimilarity into account in designing clustering algorithms.\n\u2022 Second, k-means clustering does not consider the impact of the binary encoding scheme on the compression ratio. Note that k-means clustering minimizes the MSQE for a given number of clusters, i.e., for k clusters. Assuming that a fixed-length binary code is employed, the constraint on the number of clusters can be replaced with an equivalent constraint on the compression ratio, since the compression ratio is a function of the number of clusters only in this case. However, if variable-length binary coding is assumed, then the compression ratio depends not only on the number of clusters but also on the sizes of the clusters and assigned codeword lengths for them, which are determined by the binary code employed. Thus, in this case, one can optimize network quantization further by minimizing the loss due to quantization under a constraint on the actual compression ratio resulted in by the specific binary encoding scheme employed.\nIn this paper, we propose network quantization schemes that address these two issues that the conventional quantization method using k-means clustering cannot handle properly."}, {"heading": "4 NETWORK QUANTIZATION USING HESSIAN-WEIGHT", "text": "In this section, we analyze the impact of quantization errors on the loss function of a neural network and derive that Hessian-weight can be used to quantify the importance of network parameters in\nquantization. The square matrix consisting of second-order partial derivatives is called as Hessian matrix or Hessian. In LeCun et al. (1989); Hassibi & Stork (1993), Hessian is utilized in selecting network parameters to prune. In this section, we identify that Hessian, in particular, the diagonal of Hessian, can be employed in network quantization as well in order to give different weights to the quantization errors of different network parameters."}, {"heading": "4.1 HESSIAN-WEIGHTED QUANTIZATION ERROR", "text": "The average loss function L(X ;w) can be expanded by Taylor series with respect to w as follows:\n\u03b4L(X ;w) = g(w)T \u03b4w + 1\n2 \u03b4wTH(w)\u03b4w +O(\u2016\u03b4w\u20163), (4)\nwhere w = [w1 \u00b7 \u00b7 \u00b7 wN ]T and\ng(w) = \u2202L(X ;w)\n\u2202w , H(w) =\n\u22022L(X ;w)\n\u2202w2 ;\nthe square matrix H(w) consisting of the second-order partial derivatives is called as Hessian matrix or Hessian. Assume that network parameters have been optimized and the loss function has reached to one of its local minima, at w = w\u0302, after training and/or fine-tuning. At local minima, gradients are all zero, i.e., g(w\u0302) = 0, and thus the first term in the right-hand side of (4) can be neglected. The third term in right-hand side of (4) is also ignored under the assumption that the average loss function is approximately quadratic at the local minimum w = w\u0302. Finally, for simplicity, we approximate the Hessian matrix as a diagonal matrix by setting its off-diagonal terms to be zero. Then, it follows from (4) that\n\u03b4L(X ; w\u0302) \u2248 1\n2\nN\u2211\ni=1\nhii(w\u0302)|\u03b4w\u0302i| 2, (5)\nwhere hii(w\u0302) is the second-order partial derivative of the average loss function with respect to wi evaluated at w = w\u0302, which is the i-th diagonal element of the Hessian matrix H(w\u0302).\nNow, we connect (5) with the problem of network quantization by treating \u03b4w\u0302i as the quantization error of network parameter wi at its local optimum wi = w\u0302i, i.e.,\n\u03b4w\u0302i = w\u0304i \u2212 w\u0302i, (6)\nwhere w\u0304i is a quantized value of w\u0302i. Finally, combining (5) and (6), we derive that the local impact of quantization on the average loss function at w = w\u0302 can be quantified approximately as follows:\n\u03b4L(X ; w\u0302) \u2248 1\n2\nN\u2211\ni=1\nhii(w\u0302)|w\u0302i \u2212 w\u0304i| 2. (7)\nAt a local minimum, the diagonal elements of Hessian, i.e., hii(w\u0302)\u2019s, are all non-negative and thus the summation in (7) is always additive, implying that the average loss function either increases or stays the same.\nWe note that we do not consider the interactions between retraining and quantization in our formulation. In this paper, we analyze the expected loss due to quantization of all network parameters assuming no further retraining and focus on finding optimal network quantization schemes that minimize the performance loss while maximizing the compression ratio. After quantization, however, in our experiments, we fine-tune the quantized values (cluster centers) so that we can recover the loss due to quantization and improve the performance further."}, {"heading": "4.2 HESSIAN-WEIGHTED K-MEANS CLUSTERING", "text": "In the previous subsection, we identify the local relation of the Hessian-weighted quantization errors to the average loss function of a neural network. In this subsection, we utilize this relation in (7) to design network quantization schemes that minimize the quantization loss. For notational simplicity, we use wi \u2261 w\u0302i and hii \u2261 hii(w\u0302) from now on. From (7), the optimal clustering that minimizes the Hessian-weighted distortion measure is given by\nargmin C1,C2,...,Ck\nk\u2211\nj=1\n\u2211\nwi\u2208Cj\nhii|wi \u2212 cj | 2, (8)\nwhere cj is the Hessian-weighted mean of network parameters in Cj , i.e.,\ncj =\n\u2211\nwi\u2208Cj hiiwi\n\u2211\nwi\u2208Cj hii\n.\nWe call this as Hessian-weighted k-means clustering. Similar to the conventional k-means clustering, solving this optimization is not easy, but Lloyd\u2019s algorithm is still applicable to find a local optimum for this problem by using Hessian-weighted means as cluster centers instead of non-weighted regular means.\nObserve in (8) that we give a larger penalty in the distortion measure of Hessian-weighted k-means clustering when the second-order partial derivative for a network parameter is larger in order to avoid a large deviation from its original value, since the impact on the loss function due to quantization is expected to be larger for that network parameter. Hessian-weighted k-means clustering is locally optimal in minimizing the quantization loss when a fixed-length binary code is employed for encoding quantized values, where the compression ratio solely depends on the number of clusters as shown in Section 3.1."}, {"heading": "4.3 HESSIAN COMPUTATION", "text": "For obtaining Hessian-weights, one needs to evaluate the second-order partial derivative of the average loss function with respect to each of network parameters, i.e., we need to calculate\nhii(w\u0302) = \u22022L(X ;w)\n\u2202w2i\n\u2223 \u2223 \u2223 \u2223 w=w\u0302 = 1 |X | \u22022 \u2202w2i \u2211\nx\u2208X\nloss(f(x;w), y\u0302(x)) \u2223 \u2223 \u2223 \u2223 w=w\u0302 . (9)\nRecall that we are interested in only the diagonal of Hessian. An efficient way of computing the diagonal of Hessian is presented in Le Cun (1987); Becker & Le Cun (1988) and it is based on the back propagation method that is similar to the back propagation algorithm used for computing first-order partial derivatives (gradients). That is, computing the diagonal of Hessian is of the same order of complexity as computing gradients. Hessian computation and our network quantization are performed after completing network training and/or fine-tuning. In particular, we note that Hessian can be obtained during the validation stage of a trained model.\nFor the data set X that we utilize to compute Hessian in (9), we can either reuse a training data set or use some other data set, e.g., validation data set. We observed from our experiments that even using a small subset of the training or validation data set is sufficient to yield good approximation of Hessian for network quantization."}, {"heading": "4.4 ALTERNATIVE OF HESSIAN", "text": "Although there is an efficient way to obtain the diagonal of Hessian as discussed in the previous subsection, Hessian-weight computation is not free. In this subsection, we present an alternative metric that can be used instead of Hessian-weight. In particular, we consider neural networks trained with the Adam SGD optimizer (Kingma & Ba, 2014) and propose to use some function (e.g., square root) of the second moment estimates of gradients as an alternative of Hessian.\nThe Adam algorithm computes individual adaptive learning rates for different network parameters from the estimates of the first and second moments of gradients. The parameter update in the Adam algorithm follows\nwi(t) = wi(t\u2212 1)\u2212 \u03b1 m\u0302i(t) \u221a\nv\u0302i(t) + \u01eb , (10)\nwhere wi(t) is a parameter updated at iteration t, m\u0302i(t) is the calculated estimate of the momentum (first moment) at iteration t, and v\u0302i(t) is the calculated estimate of the second moment at iteration t. The first and second moments are calculated as a function of gradient gi(t) at iteration t by m\u0302i(t) = mi(t)/(1\u2212\u03b2t1) and v\u0302i(t) = vi(t)/(1\u2212\u03b2 t 2), respectively, where mi(t) = \u03b21mi(t\u22121)+(1\u2212\u03b21)gi(t) and vi(t) = \u03b22vi(t\u2212 1) + (1\u2212 \u03b22)g2i (t).\nWe compare the Adam method with Newton\u2019s optimization method using Hessian. Newton\u2019s optimization method is given by\nw(t) = w(t\u2212 1)\u2212H\u22121(t)g(t), (11)\nwhere w(t) is the vector of parameters updated at iteration t, H\u22121(t) is the inverse of the Hessian matrix computed at iteration t, and g(t) is the gradient at iteration t. From (10) and (11), we notice that the denominator \u221a\nv\u0302i(t) + \u01eb in (10) acts like the Hessian H(t) in (11) while the numerator m\u0302i(t) corresponds to the gradient g(t). This observation leads us to use some function (e.g., square root) of the second moment estimates of gradients instead of Hessian as an alternative. Further discussion on the relation between the second moments of gradients and second-order derivatives can be found in Appendix A.1.\nThe advantage of using the second moment estimates from the Adam method is that they are computed while training and we can obtain them at the end of training at no additional cost. This makes Hessian-weighting more feasible for deep neural networks, which have millions of parameters. We note that similar quantities can be found and used for other SGD optimization methods using adaptive learning rates, e.g., AdaGrad (Duchi et al., 2011), Adadelta (Zeiler, 2012) and RMSProp (Tieleman & Hinton, 2012)."}, {"heading": "4.5 QUANTIZATION OF ALL LAYERS", "text": "We propose quantizing the network parameters of all layers in a neural network together at once by taking Hessian-weight into account. Layer-by-layer quantization was examined in the previous work (Gong et al., 2014; Han et al., 2015a). However, e.g., in Han et al. (2015a), a larger number of bits (a larger number of clusters) are assigned for convolutional layers than fully-connected layers, which implies that they heuristically treat convolutional layers more importantly. It follows from the fact that the impact of quantization errors on the performance varies significantly across layers; some layers, e.g., convolutional layers, may be more important than the others. This concern is exactly what we want to address by using Hessian-weight, and so we propose performing quantization all layers together with our quantization schemes using Hessian-weight.\nWe note that quantization of all layers is proposed under the assumption that all of binary encoded quantized parameters in a network are simply stored in one single array. Under this assumption, if layer-by-layer quantization is employed, then we need to assign some (additional) bits to each binary codeword for layer information (layer index), and it hurts the compression ratio. If we quantize all parameters of a network together, then we can avoid such additional overhead for layer indication when storing binary encoded quantized parameters. Thus, in this case, quantizing all layers together is beneficial and Hessian-weighting can be used to address the different impact of the quantization errors across layers.\nFor layer-by-layer quantization, it is advantageous to use separate arrays and separate lookup tables for different layers since layer information can be excluded in each of binary codewords for network parameters. Hessian-weighting can still provide gain even in this case for layer-by-layer quantization since it can address the different impact of the quantization errors of network parameters within each layer as well.\nFinally, note that recent neural networks are getting deeper, e.g., see Szegedy et al. (2015a;b); He et al. (2015). In such deep neural networks, quantizing network parameters of all layers together is more efficient since optimizing layer-by-layer clustering jointly across all layers requires exponential time complexity with respect to the number of layers."}, {"heading": "5 ENTROPY-CONSTRAINED NETWORK QUANTIZATION", "text": "In this section, we investigate how to solve the network quantization problem under a constraint on the compression ratio. In designing network quantization schemes, we not only want to minimize the performance loss but also want to maximize the compression ratio. In Section 4, we explore how to quantify and minimize the loss due to quantization. In this section, we investigate how to take the compression ratio into account properly in the optimization of network quantization."}, {"heading": "5.1 ENTROPY CODING", "text": "After clustering network parameters, lossless data compression with a variable-length binary code can be followed for compressing quantized values by assigning short binary codewords to the most\nfrequent symbols (i.e., quantized values) and necessarily longer binary codewords to the less frequent symbols. There is a set of optimal codes that achieve the minimum average codeword length for a given source. Entropy is the theoretical limit of the average codeword length per symbol that we can achieve by lossless data compression, proved by Shannon (see, e.g., Cover & Thomas (2012, Section 5.3)). It is known that optimal codes achieve this limit with some overhead less than 1 bit when only integer-length codewords are allowed. So optimal coding is also called as entropy coding. Huffman coding is one of optimal coding schemes commonly used when the distribution of a source is provided (see, e.g., Cover & Thomas (2012, Section 5.6))."}, {"heading": "5.2 ENTROPY-CONSTRAINED SCALAR QUANTIZATION (ECSQ)", "text": "Considering the impact of variable-length binary encoding employed for lossless data compression of quantized network parameters, we need to solve the optimization problem in (3) or the problem with Hessian-weight in (8) under a constraint on the compression ratio given by\nCompression ratio = b\nb\u0304+ ( \u2211k i=1 bi + kb)/N > C, (12)\nwhich follows from (1), where b\u0304 is the average codeword length, i.e.,\nb\u0304 = 1\nN\nk\u2211\ni=1\n|Ci|bi.\nSolving this optimization with a constraint on the compression ratio for any arbitrary variable-length binary code is too complex in general since the average codeword length can be arbitrary depending on the clustering output. However, we identify that it can be simplified if optimal codes, e.g., Huffman codes, are assumed to be employed after clustering. In particular, since optimal coding closely achieves the lower limit of the average codeword length of a source, i.e., entropy, we approximately have\nb\u0304 \u2248 H = \u2212 k\u2211\ni=1\npi log2 pi, (13)\nwhere H is the entropy of quantized network parameters after clustering (i.e., source), given that pi = |Ci|/N is the ratio of the number of network parameters in cluster Ci to the number of all network parameters (i.e., source distribution). Furthermore, assuming that N \u226b k, we have\n1\nN\n( k\u2211\ni=1\nbi + kb\n)\n\u2248 0, (14)\nin (12). From (13) and (14), the constraint in (12) can be altered to an entropy constraint given by\nH = \u2212 k\u2211\ni=1\npi log2 pi < R.\nwhere R \u2248 b/C. In summary, assuming that optimal coding is employed after clustering, one can approximately replace a constraint on the compression ratio with a constraint on the entropy of the clustering output. Then, the network quantization problem is translated into a quantization problem with an entropy constraint, which is called as entropy-constrained scalar quantization (ECSQ) in information theory. Two efficient heuristic solutions for ECSQ are proposed for network quantization in the following subsections, i.e., uniform quantization and an iterative algorithm similar to Lloyd\u2019s algorithm for k-means clustering."}, {"heading": "5.3 UNIFORM QUANTIZATION", "text": "It is shown in Gish & Pierce (1968) that the uniform quantizer is the optimal high-resolution entropyconstrained scalar quantizer regardless of the source distribution for the mean square error criterion, implying that it is asymptotically optimal in minimizing the mean square quantization error for any random source with a reasonably smooth density function as the resolution becomes infinite, i.e., the number of clusters k \u2192 \u221e. This asymptotic result leads us to come up with a very simple network quantization scheme as follows:\n1. We first set uniformly spaced thresholds and divide network parameters into clusters.\n2. After determining clusters, their quantized values (cluster centers) are obtained by taking the mean of network parameters in each cluster.\n3. Then, optimal binary coding, e.g., Huffman coding, follows to encode quantized values into variable-length binary codewords.\nNote that one can use Hessian-weighted mean instead of non-weighted mean in computing cluster centers in the second step above in order to take the benefit of Hessian-weight. A performance comparison of uniform quantization with non-weighted mean and uniform quantization with Hessianweighted mean can be found in Appendix A.3. Another thing to note here is that some clusters could be empty after uniform quantization since the source is discrete in our case. Although uniform quantization is known to be asymptotically optimal for continuous random sources, we observed from our experiments that it still yields good performance when employed for network quantization where the source is discrete and the resolution is finite."}, {"heading": "5.4 ITERATIVE ALGORITHM TO SOLVE ECSQ", "text": "Another scheme proposed to solve the ECSQ problem for network quantization is an iterative algorithm, which is similar to Lloyd\u2019s algorithm for k-means clustering. Although this iterative algorithm is more complicated than uniform quantization in Section 5.3, it finds a local optimum for a given discrete source. The iterative algorithm for solving a general ECSQ problem is provided in Chou et al. (1989) and it follows from the method of Lagrangian multipliers (Boyd & Vandenberghe, 2004, Section 5.1). In order to employ the same technique for our network quantization problem, we first define a Lagrangian cost function given by\nJ\u03bb(C1, C2, . . . , Ck) = D + \u03bbH, (15)\nwhere\nD = 1\nN\nk\u2211\nj=1\n\u2211\nwi\u2208Cj\nhii|wi \u2212 cj | 2, H = \u2212\nk\u2211\nj=1\npj log2 pj . (16)\nNote that the distortion measure D in (16) is normalized so that the Lagrangian cost function in (15) can be represented by the average of individual Lagrangian costs, denoted by d\u03bb(i, j), for network parameters as follows:\nJ\u03bb(C1, C2, . . . , Ck) = 1\nN\nk\u2211\nj=1\n\u2211\nwi\u2208Cj\n(hii|wi \u2212 cj | 2 \u2212 \u03bb log2 pj) \ufe38 \ufe37\ufe37 \ufe38\n=d\u03bb(i,j)\n, (17)\nwhich stems from\nH = \u2212 k\u2211\nj=1\npj log2 pj = \u2212 1\nN\nk\u2211\nj=1\n|Cj | log2 pj = \u2212 1\nN\nk\u2211\nj=1\n\u2211\nwi\u2208Cj\nlog2 pj .\nThe network quantization problem is now reduced to find k partitions (clusters) C1, C2, . . . , Ck that minimize the Lagrangian cost function as follows:\nargmin C1,C2,...,Ck J\u03bb(C1, C2, . . . , Ck). (18)\nIn this optimization problem, note that we have two optional parameters to choose, i.e., the Lagrangian multiplier \u03bb and the number of (initial) clusters k.\n\u2022 The Lagrangian multiplier \u03bb controls the entropy constraint and solving this optimization with different values of \u03bb results in the optimal quantization under different entropy constraints. For example, using a larger value of \u03bb in optimization, we effectively give more penalty for entropy H , and consequently it leads us to minimize the distortion under a smaller entropy constraint. Recall that the entropy constraint is related to the compression ratio while the distortion determines the performance loss. Hence, solving this optimization problem for different values of \u03bb, we can obtain a trade-off curve between compression ratio and performance, which shows the optimal compression ratio for a given performance limit or the performance achievable for a given compression ratio.\nAlgorithm 1 Iterative solution for entropy-constrained network quantization Initialization: n \u2190 0\nInitialization of k cluster centers: c(0)1 , c (0) 2 , . . . , c (0) k Initial assignment of each network parameters to its closest cluster: C(0)1 , C (0) 2 , . . . , C (0) k Compute initial distribution of network parameters over k clusters: p(0)1 , p (0) 2 , . . . , p (0) k\nrepeat Assignment:\nfor all clusters j = 1 \u2192 k do C (n+1) j \u2190 \u2205 end for for all network parameters i = 1 \u2192 N do\nAssign wi to the cluster j that minimizes the following individual Lagrangian cost:\nC (n+1) l \u2190 C (n+1) l \u222a {wi} for l = argmin\nj\n{\nhii|wi \u2212 c (n) j | 2 \u2212 \u03bb log2 p (n) j\n}\nend for Update:\nfor all clusters j = 1 \u2192 k do Update the cluster center and the distribution for cluster j:\nc (n+1) j \u2190\n\u2211\nwi\u2208C (n+1) j hiiwi \u2211\nwi\u2208C (n+1) j\nhii and p(n+1)j \u2190\n|C (n+1) j |\nN\nend for n \u2190 n+ 1\nuntil Lagrangian cost function J\u03bb decreases less than some threshold\n\u2022 The number of clusters k in the definition of the problem (18) is not equal to the number of remaining clusters after optimization since some clusters may end up to be empty due to the entropy constraint. We note that as long as k is big enough, the optimization output is not impacted much by k since solving this problem with an entropy constraint automatically optimizes the number of non-empty clusters as well.\nIn practice, we choose one value for k that is big enough and solve the optimization for different values of \u03bb, which will provide a curve that shows the maximum performance achievable for different compression ratios. From this curve, we can choose the point that satisfies our target performance and/or compression ratio.\nGiven \u03bb and k, a heuristic iterative algorithm to solve this ECSQ problem for network quantization is presented in Algorithm 1. Note that it is similar to Lloyd\u2019s algorithm for k-means clustering; the only major difference is how to partition network parameters at the assignment step. In Lloyd\u2019s algorithm, the Euclidean distance (quantization error) is minimized. For ECSQ, the individual Lagrangian cost function, i.e., d\u03bb(i, j) in (17), is minimized instead, which includes both quantization error and expected codeword length after optimal encoding. We use Hessian-weighted quantization errors as shown in (16) in defining the cost function (15). However, one can utilize ordinary quantization errors without Hessian-weights if Hessian-weights are not available or if one does not want to use them. More description for Algorithm 1 can be found in Appendix A.2."}, {"heading": "6 EXPERIMENTS", "text": "This section presents our experiment results of the proposed network quantization schemes in three exemplary convolutional neural networks: (a) LeNet (LeCun et al., 1998) for the MNIST data set, (b) ResNet (He et al., 2015) for the CIFAR-10 data set, and (c) AlexNet (Krizhevsky et al., 2012) for the ImageNet data set. Our experiments can be summarized as follows:\n\u2022 We employ the proposed network quantization methods to quantize all of network parameters in a network together at once, as discussed in 4.5. In particular, we include 32-layer ResNet (He et al., 2015) in our experiments in order to see the gain of our methods using Hessian-weight for very deep convolution neural networks.\n\u2022 We evaluate the performance of the proposed network quantization methods in the cases with and without network pruning. In the case with network pruning, we choose network parameters to prune by thresholding their absolute values. After pruning, unpruned network parameters are fine-tuned so that the original accuracy is recovered. For a pruned model, we not only need to store the values of unpruned network parameters but also need to store their respective indexes (locations) in the original model. For the index information, we compute index differences between unpruned network parameters in the original model and further compress them by Huffman coding as in Han et al. (2015a).\n\u2022 We experiment our network quantization methods with fixed-length coding as well as with Huffman coding. It is straightforward that Huffman coding yields more compression than fixed-length coding since Huffman coding is optimal for a given source distribution. However, we evaluate both scenarios since fixed-length coding could be advantageous in practice due to its simplicity in some applications or for some devices.\n\u2022 We also compare the quantization results before and after fine-tuning of quantized values (cluster centers) in order to show the impact of fine-tuning after quantization.\n\u2022 Finally, we evaluate the performance of our network quantization schemes using Hessianweight when its alternative is used instead, as discussed in Section 4.4. To this end, we retrain the considered neural networks with the Adam SGD optimizer and obtain the second moment estimates of gradients at the end of training. Then, we use the square roots of the second moment estimates instead of Hessian-weights and evaluate the performance."}, {"heading": "6.1 EXPERIMENT MODELS", "text": "First, we evaluate our network quantization schemes for the MNIST data set with a simplified version of LeNet5 (LeCun et al., 1998), consisting of two convolutional layers and two fully-connected layers followed by a soft-max layer. It has total 431,080 parameters and achieves 99.25% accuracy. For a pruned model, we keep only 8.55% (36,860 parameters) of the original network parameters and prune the rest. For Hessian computation, 50,000 samples of the training set are reused. We also evaluate the performance when Hessian is computed with 1,000 samples only.\nSecond, we experiment our network quantization schemes for the CIFAR-10 data set (Krizhevsky, 2009) with a pre-trained 32-layer ResNet (He et al., 2015). The 32-layer ResNet consists of 464,154 parameters in total and achieves 92.58% accuracy. For a pruned model, we keep only 20% (92,830 parameters) of the original network parameters and prune the rest. Similar to LeNet5, for Hessian computation, we reuse 50,000 training images and also evaluate the performance when Hessian is computed with only 1,000 training images.\nThird, we evaluate our network quantization schemes for the ImageNet ILSVRC-2012 data set (Russakovsky et al., 2015) with AlexNet (Krizhevsky et al., 2012). The AlexNet consists of five convolutional layers and three fully-connected layers followed by a soft-max layer, and the total number of parameters is 61 million. We obtain a pre-trained AlexNet Caffe model, which achieves a top-1 accuracy of 57.16%. For a pruned model, we prune 88.86% parameters and fine-tune the unpruned 6.8 million parameters. In fine-tuning, the Adam SGD optimizer is used in order to avoid the computation of Hessian by utilizing its alternative (see Section 4.4). However, we note that the pruned model does not recover the original accuracy after fine-tuning with the Adam method; the top-1 accuracy recovered after pruning is 56.00%."}, {"heading": "6.2 EXPERIMENT RESULTS", "text": "We present the quantization results for unpruned models first. In Figure 3, the accuracy of LeNet5 is plotted against the average codeword length per network parameter after quantization. When fixedlength coding is employed, the proposed Hessian-weighted k-means clustering method performs the best, as expected. Observe that Hessian-weighted k-means clustering provides better accuracy than others even after fine-tuning. On the other hand, when Huffman coding is employed, uniform\nquantization and the iterative algorithm for ECSQ outperform Hessian-weighted k-means clustering and k-means clustering. However, those two ECSQ solutions underperform Hessian-weighted kmeans clustering and even k-means clustering when fixed-length coding is employed since they are optimized for optimal variable-length coding.\nSimilar results can be observed in Figure 4 for 32-layer ResNet. The performance differences among different quantization schemes are more significant in this figure for ResNet, which is a very deep convolutional neural network. Recall that we employ our network quantization schemes in order to quantize network parameters in all layers together and evaluate the performance. The impact of quantization errors across layers could vary more substantially than within layers. Thus, our Hessian-weighted k-means clustering method can have more benefit for deeper neural networks, as presented in Figure 4. Two ECSQ solutions still show better performance than the other algorithms based on k-means clustering when Huffman coding is employed. Moreover, we can now observe the performance difference between uniform quantization and the iterative algorithm for ECSQ. Both methods are heuristic, and so it is difficult to say which one is expected to be better. However, the iterative algorithm can outperform uniform quantization as shown in Figure 4 since it converges to a local optimum for a discrete source, which matches with the situation that we have in network quantization, while uniform quantization is asymptotically optimal as k \u2192 \u221e for continuous sources.\nFigure 5 shows the performance of Hessian-weighted k-means clustering when Hessian is computed with a small number of samples (1,000 samples). Observe that even using a small number of samples in computing Hessian yields almost the same performance. Furthermore, we show the performance of Hessian-weighted k-means clustering when an alternative of Hessian is used instead of Hessian as explained in Section 4.4. In particular, the square roots of the second moments of gradients are used instead of Hessian-weight, and using this alternative provides similar performance to using Hessian.\nNow we present the performance evaluation results of our network quantization schemes for pruned models. The general trend of the quantization results for pruned models is similar to the one for unpruned models. Thus, for pruned models, we summarize the maximum compression ratios that we can achieve at no or marginal performance loss for different network quantization methods in\nTable 1; the original network parameters are 32-bit float numbers. Using the simple uniform quantization followed by Huffman encoding, we achieve the compression ratios of 51.25, 22.17 and 40.65 (i.e., the sizes of the compressed models are 1.95%, 4.51% and 2.46% of the original model sizes) for LeNet, ResNet and AlexNet, respectively, at no or marginal performance loss; note that the loss in the compressed AlexNet is mainly due to pruning. The iterative algorithm for ECSQ provides slightly worse results than uniform quantization although they are shown to perform similarly for unpruned models as shown in Figure 3 and Figure 4. Hessian-weighted k-means clustering still outperforms k-means clustering, but both are not as effective as uniform quantization when Huffman coding follows.\nFinally, we note that layer-by-layer quantization was evaluated in Han et al. (2015a) under the assumption that the layer information (layer index) is not needed to be encoded as additional bits for each network parameter (e.g., by using separate arrays for different layers), which is different from our assumption in this paper that all of binary encoded quantized parameters in a network are simply stored in one array (see Section 4.5)."}, {"heading": "7 CONCLUSION", "text": "This paper investigates the quantization problem of network parameters in deep neural networks. Network quantization compresses the size of the storage for a large number of network parameters by quantizing them and encoding the quantized values into binary codewords of smaller bit sizes. We identify the suboptimality of the conventional method using k-means clustering and newly design network quantization schemes so that they can maximize the compression ratio while minimizing the loss due to quantization errors. In particular, we analytically show that Hessian-weight can be employed as a measure of the importance of network parameters and propose to minimize Hessianweighted quantization errors in average for clustering network parameters to quantize. Hessianweighting is beneficial in quantizing all of the network parameters together at once since it can handle the different impact of quantization errors properly not only within layers but also across layers; thus, using Hessian-weighting, we can avoid layer-by-layer compression rate optimization. Furthermore, we make connection from the network quantization problem to the entropy-constrained data compression problem in information theory and push the compression ratio to the limit that information theory provides. Two efficient heuristic solutions to this end are proposed: uniform quantization and an iterative solution for ECSQ. Uniform quantization is shown to be not only simple but also effective for network quantization when entropy coding, e.g., Huffman coding, follows. From our experiments on exemplary convolutional neural networks, we show that the proposed network quantization schemes provide considerable gain over the conventional method using k-means clustering. In particular, using the simple uniform quantization followed by Huffman coding, we achieve the compression ratios of 51.25, 22.17 and 40.65 for LeNet, ResNet and AlexNet, respectively, at no or marginal performance loss."}, {"heading": "A APPENDIX", "text": "A.1 FURTHER DISCUSSION ON THE ALTERNATIVE OF HESSIAN\nThe relation between the second moments of gradients and second-order derivatives can be derived from the following. As SGD optimization converges to one of local minima, the step size decreases and consequently the second-order derivative hi(t) at iteration t can be obtained approximately from\nhi(t) \u2248 gi(t)\u2212 gi(t\u2212 1)\n\u03b8i(t)\u2212 \u03b8i(t\u2212 1) . (19)\nDue to the stochastic nature of SGD, it is likely to swing around a local minimum, i.e., it repeats converging to or diverging from a local minimum.Then, using the fact that the gradient at a local minimum is zero, (19) reduces to\n|hi(t)| \u2248 \u2223 \u2223 \u2223 \u2223\ngi(t\u2212 1)\n\u03b8i(t)\u2212 \u03b8i(t\u2212 1)\n\u2223 \u2223 \u2223 \u2223 or |hi(t)| \u2248 \u2223 \u2223 \u2223 \u2223\ngi(t)\n\u03b8i(t)\u2212 \u03b8i(t\u2212 1)\n\u2223 \u2223 \u2223 \u2223 . (20)\nNow assume that effective step sizes selected by the Adam optimizer for different network parameters converge to small but similar values at a local minimum, i.e.,\n|\u03b8i(t)\u2212 \u03b8i(t\u2212 1)| \u2248 \u03b4, (21)\nfor all i, which is actually what the Adam method aims to achieve by its adaptive learning rates. Combining (20) and (21) yields\n|hi(t)| \u2248 \u2223 \u2223 \u2223 \u2223 gi(t\u2212 1)\n\u03b4\n\u2223 \u2223 \u2223 \u2223 = 1\n\u03b4\n\u221a\ng2i (t\u2212 1) or |hi(t)| \u2248 \u2223 \u2223 \u2223 \u2223 gi(t)\n\u03b4\n\u2223 \u2223 \u2223 \u2223 = 1\n\u03b4\n\u221a\ng2i (t\u2212 1),\nrespectively, which shows the approximate relation of the square root of the second moment of a gradient and the corresponding second-order derivative at a local minimum.\nA.2 FURTHER DISCUSSION ON ALGORITHM 1\nIn Algorithm 1, we first initialize k cluster centers and partition network parameters into k clusters. The initial distribution of network parameters over k clusters are also computed. In the assignment step, we compute individual Lagrangian costs for each network parameter and find the cluster that minimizes the Lagrangian cost for each network parameter. The Lagrangian cost takes into account not only the (Hessian-weighted) quantization error but also the expected codeword length after optimal coding. In particular, we allow a non-integer codeword length and use log2 p (n) i as the expected codeword length after optimal coding for network parameter wi. This makes the average codeword length becomes the exact entropy but it can cause some loss when the binary code actually employed consists of integer-length codewords, e.g., Huffman codes. Moreover, note that once a cluster becomes empty, i.e., p(n)i = 0, then the individual cost function for the empty cluster becomes infinite and thus it remains to be empty until the end of iterations, implying that some clusters are excluded from iterative clustering once they become empty. Next we update the cluster center of each cluster to be the mean or Hessian-weighted mean of network parameters in the cluster. We also update the distribution of network parameters for k clusters. The assignment and update steps are iteratively performed until the Lagrangian cost function does not decrease more than a predetermined threshold.\nA.3 FURTHER EXPERIMENT RESULTS FOR UNIFORM QUANTIZATION\nWe compare uniform quantization with non-weighted mean and uniform quantization with Hessianweighted mean in Figure 6, which shows that uniform quantization with Hessign-weighted mean slightly outperforms uniform quantization with non-weighted mean. After fine-tuning quantized values (cluster centers), the performance difference between uniform quantization with non-weighted mean and uniform quantization with Hessian-weighted mean becomes more marginal as shown in Figure 6(b), which is due to the fact that cluster centers are fined-tuned and likely converge to the same local optimum.\nA.4 EXTRA INFORMATION OF NETWORK QUANTIZATION RESULTS FOR PRUNED MODELS\nIn Table 2, we present extra information of the network quantization results in Table 1 for pruned models."}], "references": [{"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["Sajid Anwar", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Anwar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anwar et al\\.", "year": 2015}, {"title": "Improving the convergence of back-propagation learning with second order methods", "author": ["Sue Becker", "Yann Le Cun"], "venue": "In Proceedings of the Connectionist Models Summer School,", "citeRegEx": "Becker and Cun.,? \\Q1988\\E", "shortCiteRegEx": "Becker and Cun.", "year": 1988}, {"title": "Entropy-constrained vector quantization", "author": ["Philip A Chou", "Tom Lookabaugh", "Robert M Gray"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Chou et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Chou et al\\.", "year": 1989}, {"title": "Training deep neural networks with low precision multiplications", "author": ["Matthieu Courbariaux", "Jean-Pierre David", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Elements of information theory", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "Cover and Thomas.,? \\Q2012\\E", "shortCiteRegEx": "Cover and Thomas.", "year": 2012}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Nando de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Asymptotically efficient quantizing", "author": ["Herbert Gish", "John Pierce"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Gish and Pierce.,? \\Q1968\\E", "shortCiteRegEx": "Gish and Pierce.", "year": 1968}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Deep learning with limited numerical precision", "author": ["Suyog Gupta", "Ankur Agrawal", "Kailash Gopalakrishnan", "Pritish Narayanan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["Babak Hassibi", "David G Stork"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hassibi and Stork.,? \\Q1993\\E", "shortCiteRegEx": "Hassibi and Stork.", "year": 1993}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Fixed-point feedforward deep neural network design using weights +1, 0, and -1", "author": ["Kyuyeon Hwang", "Wonyong Sung"], "venue": "In IEEE Workshop on Signal Processing Systems,", "citeRegEx": "Hwang and Sung.,? \\Q2014\\E", "shortCiteRegEx": "Hwang and Sung.", "year": 2014}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "In Proceedings of the British Machine Vision Conference,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Yong-Deok Kim", "Eunhyeok Park", "Sungjoo Yoo", "Taelim Choi", "Lu Yang", "Dongjun Shin"], "venue": "arXiv preprint arXiv:1511.06530,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": null, "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Mod\u00e8les connexionnistes de l\u2019apprentissage", "author": ["Yann Le Cun"], "venue": "PhD thesis, Paris", "citeRegEx": "Cun.,? \\Q1987\\E", "shortCiteRegEx": "Cun.", "year": 1987}, {"title": "Speeding-up convolutional neural networks using fine-tuned CP-decomposition", "author": ["Vadim Lebedev", "Yaroslav Ganin", "Maksim Rakhuba", "Ivan Oseledets", "Victor Lempitsky"], "venue": "arXiv preprint arXiv:1412.6553,", "citeRegEx": "Lebedev et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lebedev et al\\.", "year": 2014}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S Denker", "Sara A Solla", "Richard E Howard", "Lawrence D Jackel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Fixed point quantization of deep convolutional networks", "author": ["Darryl D Lin", "Sachin S Talathi", "V Sreekanth Annapureddy"], "venue": "arXiv preprint arXiv:1511.06393,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Neural networks with few multiplications", "author": ["Zhouhan Lin", "Matthieu Courbariaux", "Roland Memisevic", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1510.03009,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Least squares quantization in pcm", "author": ["Stuart Lloyd"], "venue": "IEEE transactions on information theory,", "citeRegEx": "Lloyd.,? \\Q1982\\E", "shortCiteRegEx": "Lloyd.", "year": 1982}, {"title": "Skeletonization: A technique for trimming the fat from a network via relevance assessment", "author": ["Michael C Mozer", "Paul Smolensky"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mozer and Smolensky.,? \\Q1989\\E", "shortCiteRegEx": "Mozer and Smolensky.", "year": 1989}, {"title": "XNOR-Net: Imagenet classification using binary convolutional neural networks", "author": ["Mohammad Rastegari", "Vicente Ordonez", "Joseph Redmon", "Ali Farhadi"], "venue": "arXiv preprint arXiv:1603.05279,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Lowrank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["Tara N Sainath", "Brian Kingsbury", "Vikas Sindhwani", "Ebru Arisoy", "Bhuvana Ramabhadran"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Sainath et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "venue": "arXiv preprint arXiv:1512.00567,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Convolutional neural networks with low-rank regularization", "author": ["Cheng Tai", "Tong Xiao", "Xiaogang Wang"], "venue": "arXiv preprint arXiv:1511.06067,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Improving the speed of neural networks on CPUs", "author": ["Vincent Vanhoucke", "Andrew Senior", "Mark Z Mao"], "venue": "In Deep Learning and Unsupervised Feature Learning Workshop,", "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["Jian Xue", "Jinyu Li", "Yifan Gong"], "venue": "In INTERSPEECH,", "citeRegEx": "Xue et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2013}, {"title": "Deep fried convnets", "author": ["Zichao Yang", "Marcin Moczulski", "Misha Denil", "Nando de Freitas", "Alex Smola", "Le Song", "Ziyu Wang"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "However, it has been shown that deep neural networks typically include many redundant parameters (Denil et al., 2013) and it is of great interest in recent years to reduce the redundancy of deep neural networks.", "startOffset": 97, "endOffset": 117}, {"referenceID": 15, "context": "For example, Krizhevsky et al. (2012) came up with a deep convolutional neural network consisting of 61 million parameters and won the ImageNet competition in 2012.", "startOffset": 13, "endOffset": 38}, {"referenceID": 15, "context": "For example, Krizhevsky et al. (2012) came up with a deep convolutional neural network consisting of 61 million parameters and won the ImageNet competition in 2012. It is followed by deeper neural networks with even larger numbers of parameters, e.g., Simonyan & Zisserman (2014). The large sizes of deep neural networks make it difficult to deploy them on resource-limited devices, e.", "startOffset": 13, "endOffset": 280}, {"referenceID": 6, "context": "However, it has been shown that deep neural networks typically include many redundant parameters (Denil et al., 2013) and it is of great interest in recent years to reduce the redundancy of deep neural networks. The whole procedure to reduce the redundancy of a neural network is called as network compression. The benefit of compression is twofold: computational cost reduction and hardware resource saving. In this paper, our interest is mainly on the size reduction of the storage (memory) for a large number of network parameters (weights and biases). In particular, we focus on the network size compression by quantizing network parameters. The most related work to our investigation can be found in Gong et al. (2014); Han et al.", "startOffset": 98, "endOffset": 724}, {"referenceID": 6, "context": "However, it has been shown that deep neural networks typically include many redundant parameters (Denil et al., 2013) and it is of great interest in recent years to reduce the redundancy of deep neural networks. The whole procedure to reduce the redundancy of a neural network is called as network compression. The benefit of compression is twofold: computational cost reduction and hardware resource saving. In this paper, our interest is mainly on the size reduction of the storage (memory) for a large number of network parameters (weights and biases). In particular, we focus on the network size compression by quantizing network parameters. The most related work to our investigation can be found in Gong et al. (2014); Han et al. (2015a), where a conventional quantization method", "startOffset": 98, "endOffset": 744}, {"referenceID": 23, "context": "Network pruning (Mozer & Smolensky, 1989; LeCun et al., 1989; Hassibi & Stork, 1993; Han et al., 2015b) and low-precision fixed- or floating-point implementation of neural networks (Vanhoucke et al.", "startOffset": 16, "endOffset": 103}, {"referenceID": 37, "context": ", 2015b) and low-precision fixed- or floating-point implementation of neural networks (Vanhoucke et al., 2011; Hwang & Sung, 2014; Courbariaux et al., 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression.", "startOffset": 86, "endOffset": 215}, {"referenceID": 3, "context": ", 2015b) and low-precision fixed- or floating-point implementation of neural networks (Vanhoucke et al., 2011; Hwang & Sung, 2014; Courbariaux et al., 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression.", "startOffset": 86, "endOffset": 215}, {"referenceID": 0, "context": ", 2015b) and low-precision fixed- or floating-point implementation of neural networks (Vanhoucke et al., 2011; Hwang & Sung, 2014; Courbariaux et al., 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression.", "startOffset": 86, "endOffset": 215}, {"referenceID": 10, "context": ", 2015b) and low-precision fixed- or floating-point implementation of neural networks (Vanhoucke et al., 2011; Hwang & Sung, 2014; Courbariaux et al., 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression.", "startOffset": 86, "endOffset": 215}, {"referenceID": 31, "context": "Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015).", "startOffset": 83, "endOffset": 224}, {"referenceID": 38, "context": "Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015).", "startOffset": 83, "endOffset": 224}, {"referenceID": 16, "context": "Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015).", "startOffset": 83, "endOffset": 224}, {"referenceID": 22, "context": "Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015).", "startOffset": 83, "endOffset": 224}, {"referenceID": 39, "context": "Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015).", "startOffset": 83, "endOffset": 224}, {"referenceID": 17, "context": "Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015).", "startOffset": 83, "endOffset": 224}, {"referenceID": 35, "context": "Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015).", "startOffset": 83, "endOffset": 224}, {"referenceID": 0, "context": ", 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression. Some extremes of low-precision neural networks consisting of binary or ternary parameters were presented in Courbariaux et al. (2015); Lin et al.", "startOffset": 8, "endOffset": 254}, {"referenceID": 0, "context": ", 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression. Some extremes of low-precision neural networks consisting of binary or ternary parameters were presented in Courbariaux et al. (2015); Lin et al. (2015b); Rastegari et al.", "startOffset": 8, "endOffset": 274}, {"referenceID": 0, "context": ", 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression. Some extremes of low-precision neural networks consisting of binary or ternary parameters were presented in Courbariaux et al. (2015); Lin et al. (2015b); Rastegari et al. (2016). Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al.", "startOffset": 8, "endOffset": 299}, {"referenceID": 0, "context": ", 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression. Some extremes of low-precision neural networks consisting of binary or ternary parameters were presented in Courbariaux et al. (2015); Lin et al. (2015b); Rastegari et al. (2016). Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015). We note that these are different types of network compression techniques, which can be employed on top of each other. A network compression framework consisting of network pruning, network quantization and finetuning is presented in Han et al. (2015a). Network pruning is employed to remove some of network parameters completely from a neural network by setting their values to be always zero.", "startOffset": 8, "endOffset": 778}, {"referenceID": 0, "context": ", 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression. Some extremes of low-precision neural networks consisting of binary or ternary parameters were presented in Courbariaux et al. (2015); Lin et al. (2015b); Rastegari et al. (2016). Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015). We note that these are different types of network compression techniques, which can be employed on top of each other. A network compression framework consisting of network pruning, network quantization and finetuning is presented in Han et al. (2015a). Network pruning is employed to remove some of network parameters completely from a neural network by setting their values to be always zero. For a pruned network, one only needs to keep unpruned network parameters and their respective locations (indexes) in the original model. After network pruning, unpruned network parameters are fined-tuned to recover the loss due to pruning. The values for pruned parameters remain to be zero during this fine-tuning stage. Then, network quantization follows. Network quantization reduces the number of bits needed for representing (unpruned) network parameters by quantizing them and encoding their quantized values into binary codewords with smaller bit sizes. The quantized values can be retrieved from the binary codewords stored instead of actual values by using a lookup table of a small size. Finally, the quantized values in the lookup table can be further fined-tuned to reduce the loss due to quantization. We note that the fine-tuning stages are optional but it is preferred to have them in order to recover the loss due to compression after aggressive pruning and/or quantization. For network quantization. we consider a neural network that is already trained, pruned if employed and fine-tuned before quantization. If no network pruning is employed, all parameters in a network are subject to quantization. For pruned networks, our focus is on quantization of unpruned parameters. We note that pruning and quantization methods can be designed jointly but it makes the problem more complicated. In particular, if fine-tuning of unpruned network parameters is assumed to be employed after aggressive pruning, it is difficult to predict and to consider the impact of fine-tuning in network quantization since unpruned parameters can change substantially from their original values after fine-tuning due to the fact that a large number of pruned parameters are fixed to be zero. Hence, in this paper, we treat them as two separable procedures for network compression and focus on network quantization. In Gong et al. (2014); Han et al.", "startOffset": 8, "endOffset": 2851}, {"referenceID": 0, "context": ", 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression. Some extremes of low-precision neural networks consisting of binary or ternary parameters were presented in Courbariaux et al. (2015); Lin et al. (2015b); Rastegari et al. (2016). Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015). We note that these are different types of network compression techniques, which can be employed on top of each other. A network compression framework consisting of network pruning, network quantization and finetuning is presented in Han et al. (2015a). Network pruning is employed to remove some of network parameters completely from a neural network by setting their values to be always zero. For a pruned network, one only needs to keep unpruned network parameters and their respective locations (indexes) in the original model. After network pruning, unpruned network parameters are fined-tuned to recover the loss due to pruning. The values for pruned parameters remain to be zero during this fine-tuning stage. Then, network quantization follows. Network quantization reduces the number of bits needed for representing (unpruned) network parameters by quantizing them and encoding their quantized values into binary codewords with smaller bit sizes. The quantized values can be retrieved from the binary codewords stored instead of actual values by using a lookup table of a small size. Finally, the quantized values in the lookup table can be further fined-tuned to reduce the loss due to quantization. We note that the fine-tuning stages are optional but it is preferred to have them in order to recover the loss due to compression after aggressive pruning and/or quantization. For network quantization. we consider a neural network that is already trained, pruned if employed and fine-tuned before quantization. If no network pruning is employed, all parameters in a network are subject to quantization. For pruned networks, our focus is on quantization of unpruned parameters. We note that pruning and quantization methods can be designed jointly but it makes the problem more complicated. In particular, if fine-tuning of unpruned network parameters is assumed to be employed after aggressive pruning, it is difficult to predict and to consider the impact of fine-tuning in network quantization since unpruned parameters can change substantially from their original values after fine-tuning due to the fact that a large number of pruned parameters are fixed to be zero. Hence, in this paper, we treat them as two separable procedures for network compression and focus on network quantization. In Gong et al. (2014); Han et al. (2015a), it is proposed to utilize the conventional k-means clustering method for quantizing network parameters.", "startOffset": 8, "endOffset": 2871}, {"referenceID": 7, "context": ", AdaGrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), and RMSProp (Tieleman & Hinton, 2012).", "startOffset": 10, "endOffset": 30}, {"referenceID": 40, "context": ", 2011), Adadelta (Zeiler, 2012), and RMSProp (Tieleman & Hinton, 2012).", "startOffset": 18, "endOffset": 32}, {"referenceID": 7, "context": ", AdaGrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), and RMSProp (Tieleman & Hinton, 2012). The advantage of using these metrics instead of Hessian is that they are computed while training and can be obtained at the end of training at no additional cost. \u2022 It is shown how the proposed network quantization schemes can be applied for quantizing network parameters of all layers together at once, rather than layer-by-layer network quantization in Gong et al. (2014); Han et al.", "startOffset": 11, "endOffset": 470}, {"referenceID": 7, "context": ", AdaGrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), and RMSProp (Tieleman & Hinton, 2012). The advantage of using these metrics instead of Hessian is that they are computed while training and can be obtained at the end of training at no additional cost. \u2022 It is shown how the proposed network quantization schemes can be applied for quantizing network parameters of all layers together at once, rather than layer-by-layer network quantization in Gong et al. (2014); Han et al. (2015a). This follows from our investigation that Hessian-weighting can handle the different impact of quantization errors properly not only within layers but also across layers.", "startOffset": 11, "endOffset": 490}, {"referenceID": 27, "context": "The most well-known heuristic algorithm is Lloyd\u2019s algorithm (Lloyd, 1982).", "startOffset": 61, "endOffset": 74}, {"referenceID": 21, "context": "In LeCun et al. (1989); Hassibi & Stork (1993), Hessian is utilized in selecting network parameters to prune.", "startOffset": 5, "endOffset": 23}, {"referenceID": 21, "context": "In LeCun et al. (1989); Hassibi & Stork (1993), Hessian is utilized in selecting network parameters to prune.", "startOffset": 5, "endOffset": 47}, {"referenceID": 21, "context": "An efficient way of computing the diagonal of Hessian is presented in Le Cun (1987); Becker & Le Cun (1988) and it is based on the back propagation method that is similar to the back propagation algorithm used for computing first-order partial derivatives (gradients).", "startOffset": 73, "endOffset": 84}, {"referenceID": 21, "context": "An efficient way of computing the diagonal of Hessian is presented in Le Cun (1987); Becker & Le Cun (1988) and it is based on the back propagation method that is similar to the back propagation algorithm used for computing first-order partial derivatives (gradients).", "startOffset": 73, "endOffset": 108}, {"referenceID": 7, "context": ", AdaGrad (Duchi et al., 2011), Adadelta (Zeiler, 2012) and RMSProp (Tieleman & Hinton, 2012).", "startOffset": 10, "endOffset": 30}, {"referenceID": 40, "context": ", 2011), Adadelta (Zeiler, 2012) and RMSProp (Tieleman & Hinton, 2012).", "startOffset": 18, "endOffset": 32}, {"referenceID": 9, "context": "Layer-by-layer quantization was examined in the previous work (Gong et al., 2014; Han et al., 2015a).", "startOffset": 62, "endOffset": 100}, {"referenceID": 9, "context": "Layer-by-layer quantization was examined in the previous work (Gong et al., 2014; Han et al., 2015a). However, e.g., in Han et al. (2015a), a larger number of bits (a larger number of clusters) are assigned for convolutional layers than fully-connected layers, which implies that they heuristically treat convolutional layers more importantly.", "startOffset": 63, "endOffset": 139}, {"referenceID": 9, "context": "Layer-by-layer quantization was examined in the previous work (Gong et al., 2014; Han et al., 2015a). However, e.g., in Han et al. (2015a), a larger number of bits (a larger number of clusters) are assigned for convolutional layers than fully-connected layers, which implies that they heuristically treat convolutional layers more importantly. It follows from the fact that the impact of quantization errors on the performance varies significantly across layers; some layers, e.g., convolutional layers, may be more important than the others. This concern is exactly what we want to address by using Hessian-weight, and so we propose performing quantization all layers together with our quantization schemes using Hessian-weight. We note that quantization of all layers is proposed under the assumption that all of binary encoded quantized parameters in a network are simply stored in one single array. Under this assumption, if layer-by-layer quantization is employed, then we need to assign some (additional) bits to each binary codeword for layer information (layer index), and it hurts the compression ratio. If we quantize all parameters of a network together, then we can avoid such additional overhead for layer indication when storing binary encoded quantized parameters. Thus, in this case, quantizing all layers together is beneficial and Hessian-weighting can be used to address the different impact of the quantization errors across layers. For layer-by-layer quantization, it is advantageous to use separate arrays and separate lookup tables for different layers since layer information can be excluded in each of binary codewords for network parameters. Hessian-weighting can still provide gain even in this case for layer-by-layer quantization since it can address the different impact of the quantization errors of network parameters within each layer as well. Finally, note that recent neural networks are getting deeper, e.g., see Szegedy et al. (2015a;b); He et al. (2015). In such deep neural networks, quantizing network parameters of all layers together is more efficient since optimizing layer-by-layer clustering jointly across all layers requires exponential time complexity with respect to the number of layers.", "startOffset": 63, "endOffset": 1992}, {"referenceID": 2, "context": "The iterative algorithm for solving a general ECSQ problem is provided in Chou et al. (1989) and it follows from the method of Lagrangian multipliers (Boyd & Vandenberghe, 2004, Section 5.", "startOffset": 74, "endOffset": 93}, {"referenceID": 24, "context": "This section presents our experiment results of the proposed network quantization schemes in three exemplary convolutional neural networks: (a) LeNet (LeCun et al., 1998) for the MNIST data set, (b) ResNet (He et al.", "startOffset": 150, "endOffset": 170}, {"referenceID": 14, "context": ", 1998) for the MNIST data set, (b) ResNet (He et al., 2015) for the CIFAR-10 data set, and (c) AlexNet (Krizhevsky et al.", "startOffset": 43, "endOffset": 60}, {"referenceID": 20, "context": ", 2015) for the CIFAR-10 data set, and (c) AlexNet (Krizhevsky et al., 2012) for the ImageNet data set.", "startOffset": 51, "endOffset": 76}, {"referenceID": 14, "context": "In particular, we include 32-layer ResNet (He et al., 2015) in our experiments in order to see the gain of our methods using Hessian-weight for very deep convolution neural networks.", "startOffset": 42, "endOffset": 59}, {"referenceID": 11, "context": "For the index information, we compute index differences between unpruned network parameters in the original model and further compress them by Huffman coding as in Han et al. (2015a). \u2022 We experiment our network quantization methods with fixed-length coding as well as with Huffman coding.", "startOffset": 164, "endOffset": 183}, {"referenceID": 24, "context": "First, we evaluate our network quantization schemes for the MNIST data set with a simplified version of LeNet5 (LeCun et al., 1998), consisting of two convolutional layers and two fully-connected layers followed by a soft-max layer.", "startOffset": 111, "endOffset": 131}, {"referenceID": 19, "context": "Second, we experiment our network quantization schemes for the CIFAR-10 data set (Krizhevsky, 2009) with a pre-trained 32-layer ResNet (He et al.", "startOffset": 81, "endOffset": 99}, {"referenceID": 14, "context": "Second, we experiment our network quantization schemes for the CIFAR-10 data set (Krizhevsky, 2009) with a pre-trained 32-layer ResNet (He et al., 2015).", "startOffset": 135, "endOffset": 152}, {"referenceID": 30, "context": "Third, we evaluate our network quantization schemes for the ImageNet ILSVRC-2012 data set (Russakovsky et al., 2015) with AlexNet (Krizhevsky et al.", "startOffset": 90, "endOffset": 116}, {"referenceID": 20, "context": ", 2015) with AlexNet (Krizhevsky et al., 2012).", "startOffset": 21, "endOffset": 46}, {"referenceID": 11, "context": "Finally, we note that layer-by-layer quantization was evaluated in Han et al. (2015a) under the assumption that the layer information (layer index) is not needed to be encoded as additional bits for each network parameter (e.", "startOffset": 67, "endOffset": 86}], "year": 2016, "abstractText": "Network quantization is one of network compression techniques employed to reduce the redundancy of deep neural networks. It compresses the size of the storage for a large number of network parameters in a neural network by quantizing them and encoding the quantized values into binary codewords of smaller sizes. In this paper, we aim to design network quantization schemes that minimize the expected loss due to quantization while maximizing the compression ratio. To this end, we analyze the quantitative relation of quantization errors to the loss function of a neural network and identify that the Hessian-weighted distortion measure is locally the right objective function that we need to optimize for minimizing the loss due to quantization. As a result, Hessian-weighted k-means clustering is proposed for clustering network parameters to quantize when fixed-length binary encoding follows. Hessian-weighting properly handles the different impact of quantization errors not only within layers but also across layers and thus it can be employed for quantizing all layers of a network together at once; it is beneficial since one can avoid layer-by-layer compression rate optimization. When optimal variablelength binary codes, e.g., Huffman codes, are employed for further compression of quantized values after clustering, we derive that the network quantization problem can be related to the entropy-constrained scalar quantization (ECSQ) problem in information theory and consequently propose two solutions of ECSQ for network quantization, i.e., uniform quantization and an iterative algorithm similar to Lloyd\u2019s algorithm for k-means clustering. Finally, using the simple uniform quantization followed by Huffman coding, our experiment results show that the compression ratios of 51.25, 22.17 and 40.65 are achievable (i.e., the sizes of the compressed models are 1.95%, 4.51% and 2.46% of the original model sizes) for LeNet, ResNet and AlexNet, respectively, at no or marginal performance loss.", "creator": "LaTeX with hyperref package"}}}