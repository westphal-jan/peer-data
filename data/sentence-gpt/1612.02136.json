{"id": "1612.02136", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2016", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 7 Dec 2016 07:45:38 GMT  (1133kb,D)", "http://arxiv.org/abs/1612.02136v1", "Under review as a conference paper at ICLR 2017"], ["v2", "Fri, 9 Dec 2016 06:08:37 GMT  (1133kb,D)", "http://arxiv.org/abs/1612.02136v2", "Under review as a conference paper at ICLR 2017"], ["v3", "Sun, 18 Dec 2016 05:55:22 GMT  (1006kb,D)", "http://arxiv.org/abs/1612.02136v3", "Under review as a conference paper at ICLR 2017"], ["v4", "Mon, 20 Feb 2017 05:01:27 GMT  (1337kb,D)", "http://arxiv.org/abs/1612.02136v4", "Published as a conference paper at ICLR 2017"], ["v5", "Thu, 2 Mar 2017 06:28:13 GMT  (1337kb,D)", "http://arxiv.org/abs/1612.02136v5", "Published as a conference paper at ICLR 2017"]], "COMMENTS": "Under review as a conference paper at ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.NE", "authors": ["tong che", "yanran li", "athul paul jacob", "yoshua bengio", "wenjie li"], "accepted": true, "id": "1612.02136"}, "pdf": {"name": "1612.02136.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Paul Jacob", "\u2020Yoshua Bengio", "\u2021Wenjie Li"], "emails": ["tong.che@umontreal.ca", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "csyli@comp.polyu.edu.hk", "cswjli@comp.polyu.edu.hk"], "sections": [{"heading": "1 INTRODUCTION", "text": "Generative adversarial networks (GAN) (Goodfellow et al., 2014) have demonstrated their potential on various tasks, such as image generation, image super-resolution, 3D object generation, and video prediction (Radford et al., 2015; Ledig et al., 2016; S\u00f8nderby et al., 2016; Nguyen et al., 2016; Wu et al., 2016; Mathieu et al., 2015). The objective is to train a parametrized function (the generator) which maps noise samples (e.g., uniform or Gaussian) to samples whose distribution is close to that of the data generating distribution. The basic scheme of the GAN training procedure is to train a discriminator which assigns higher probabilities to real data samples and lower probabilities to generated data samples, while simultaneously trying to move the generated samples towards the real data manifold using the gradient information provided by the discriminator. In a typical setting, the generator and the discriminator are represented by deep neural networks.\nDespite their success, GANs are generally considered as very hard to train due to training instability and sensitivity to hyper-parameters. On the other hand, a common failure pattern observed while training GANs is the collapsing of large volumes of probability mass onto a few modes. Namely, although the generators produce meaningful samples, these samples are often from just a few modes (small regions of high probability under the data distribution). Behind this phenomenon is the missing modes problem, which is widely conceived as a major problem for training GANs: many modes of the data generating distribution are not at all represented in the generated samples, yielding a much lower entropy distribution, with less variety than the data generating distribution.\nThis issue has been the subject of several recent papers proposing several tricks and new architectures to stabilize GAN\u2019s training and encourage its samples\u2019 diversity. However, we argue that a general cause behind these problems is the lack of control on the discriminator during GAN training. We would like to encourage the manifold of the samples produced by the generator to move towards that of real data, using the discriminator as a metric. However, even if we train the discriminator to distinguish between these two manifolds, we have no control over the shape of the discriminator function in between these manifolds. In fact, the shape of the discriminator function in the data\n\u2217Authors contributed equally.\nar X\niv :1\n61 2.\n02 13\n6v 1\n[ cs\n.L G\n] 7\nD ec\n2 01\nspace can be very non-linear with bad plateaus and wrong maxima and this can therefore hurt the training of GANs (Figure 1).\npose a family of additional regularizers for the GAN objective. We then design a set of metrics to evaluate the generated samples in terms of both the diversity of modes and the distribution fairness of the probability mass. These metrics are shown to be more robust in judging complex generative models, including those which are well-trained and collapsed ones.\nRegularizers usually bring a trade-off between model variance and bias. Our results have shown that, when correctly applied, our regularizers can dramatically reduce model variance, stabilize the training, and fix the missing mode problem all at once, with positive or at the least no negative effects on the generate samples. We also discuss a variant of the regularized GAN algorithm, which can even improve sample quality as compared to the DCGAN baseline."}, {"heading": "2 RELATED WORK", "text": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks.\nIn Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN\u2019s representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information. Motivated from this, several conditional variants of GAN has been applied to a wide range of tasks, including image prediction from a normal map Wang & Gupta (2016), image synthesis from text Reed et al. (2016) and edge map Isola et al. (2016), real-time image manipulation Zhu et al. (2016), temporal image generation Zhou & Berg (2016); Saito & Matsumoto (2016); Vondrick et al. (2016), texture synthesis, style transfer, and video stylization Li & Wand (2016).\nResearchers also aim at stretching GAN\u2019s limit to generate higher-resolution, photo-realistic images. Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution. At each level of their LAPGAN, both the generator and the discriminator are convolutional networks. As an alternative to LAPGAN, Radford et al. (2015) successfully designs a class of deep convolutional generative adversarial networks which has led to significant improvements on unsupervised image representation learning. Another line of work aimed at improving GANs are through feature learning, including features from the latent space and image space. The motivation is that features from different spaces are complementary for generating perceptual and natural-looking images. With this perspective, some researchers use distances between learned features as losses for training objectives for generative models. Larsen et al. (2015) combine a variational autoencoder objective with a GAN and utilize the learned features from the discriminator in the GANs for better image similarity metrics. It is shown that the learned distance from the discriminator is of great help for the sample visual fidelity. Recent literature have also shown impressive results on image super-resolution to infer photo-realistic natural images for 4x upscaling factors Ledig et al. (2016); S\u00f8nderby et al. (2016); Nguyen et al. (2016).\nDespite these promising successes, GANs are notably hard to train. Although Radford et al. (2015) provide a class of empirical architectural choices that are critical to stabilize GAN\u2019s training, it would be even better to train GANs more robustly and systematically. Salimans et al. (2016) propose feature matching technique to stabilize GAN\u2019s training. The generator is required to match the statistics of intermediate features of the discriminator. Similar idea is adopted by Zhao et al. (2016).\nIn addition to feature distances, Dosovitskiy & Brox (2016) found that the counterpart loss in image space further improves GAN\u2019s training stability. Furthermore, some researchers make use of information in both spaces in a unified learning procedure (Dumoulin et al., 2016; Donahue et al., 2016). In Dumoulin et al. (2016), one trains not just a generator but also an encoder, and the discriminator is trained to distinguish between two joint distributions over image and latent spaces produced either by the application of the encoder on the training data or by the application of the generator (decoder) to the latent prior. This is in contrast with the regular GAN training, in which the discriminator only attempts to separate the distributions in the image space. Parallelly, Metz et al. (2016) stabilize GANs by unrolling the optimization of discriminator, which can be considered as an orthogonal work with ours.\nOur work is related to VAEGAN (Larsen et al., 2015) in terms of training an autoencoder or VAE jointly with the GAN model. However, the variational autoencoder (VAE) in VAEGAN is used to generate samples whereas our autoencoder based losses serves as a regularizer to penalize missing modes and thus improving GAN\u2019s training stability and sample qualities."}, {"heading": "3 MODE REGULARIZERS FOR GANS", "text": "The GAN training procedure can be viewed as a non-cooperative two player game, in which the discriminator D tries to distinguish real and generated examples, while the generator G tries to fool the discriminator by pushing the generated samples towards the direction of higher discrimination values. Training the discriminator D can be viewed as training an evaluation metric on the sample space. Then the generator G has to take advantage of the local gradient\u2207 logD(G) provided by the discriminator to improve itself, namely to move towards the data manifold.\nWe now take a closer look at the root cause of the instabilities while training GANs. The discriminator is trained on both generated and real examples. As pointed out by Goodfellow et al. (2014); Denton et al. (2015); Radford et al. (2015), when the data manifold and the generation manifold are disjoint (which is true in almost all practical situations), it is equivalent to training a characteristic function to be very close to 1 on the data manifold, and 0 on the generation manifold. In order to pass good gradient information to the generator, it is important that the trained discriminator produces stable and smooth gradients. However, since the discriminator objective does not directly depend on the behavior of the discriminator in other parts of the space, training can easily fail if the shape of the discriminator function is not as expected. As an example,Denton et al. (2015) noted a common failure pattern for training GANs which is the vanishing gradient problem, in which the discriminator D perfectly classifies real and fake examples, such that around the fake examples, D is nearly zero. In such cases, the generator will receive no gradient to improve itself.1\nAnother important problem while training GANs is mode missing. In theory, if the generated data and the real data come from the same low dimensional manifold, the discriminator can help the generator distribute its probability mass, because the missing modes will not have near-0 probability under the generator and so the samples in these areas can be appropriately concentrated towards regions where D is closer to 1. However, in practice since the two manifolds are disjoint, D tends to be near 1 on all the real data samples, so large modes usually have a much higher chance of attracting the gradient of discriminator. For a typical GAN model, since all modes have similar D values, there is no reason why the generator cannot collapse to just a few major modes. In other words, since the discriminator\u2019s output is nearly 0 and 1 on fake and real data respectively, the generator is not penalized for missing modes."}, {"heading": "3.1 GEOMETRIC METRICS REGULARIZER", "text": "Compared with the objective for the GAN generator, the optimization targets for supervised learning are more stable from an optimization point of view. The difference is clear: the optimization target for the GAN generator is a learned discriminator. While in supervised models, the optimization targets are distance functions with nice geometric properties. The latter usually provides much easier training gradients than the former, especially at the early stages of training.\n1This problem exists even when we use logD(G(z)) as target for the generator, as noted by Denton et al. (2015) and our experiments.\nInspired by this observation, we propose to incorporate a supervised training signal as a regularizer on top of the discriminator target. Assume the generator G(z) : Z \u2192 X generates samples by sampling first from a fixed prior distribution in space Z followed by a deterministic trainable transformation G into the sample space X . Together with G, we also jointly train an encoder E(x) : X \u2192 Z. Assume d is some similarity metric in the data space, we add Ex\u223cpd [d(x,G\u25e6E(x))] as a regularizer, where pd is the data generating distribution. The encoder itself is trained by minimizing the same reconstruction error.\nIn practice, there are many options for the distance measure d. For instance, the pixel-wise L2 distance, or the distance of learned features by the discriminator (Dumoulin et al., 2016) or by other networks, such as a VGG classifier. (Ledig et al., 2016)\nThe geometric intuition for this regularizer is straight-forward. We are trying to move the generated manifold to the real data manifold using gradient descent. In addition to the gradient provided by the discriminator, we can also try to match the two manifolds by other geometric distances, say, Ls metric. The idea of adding an encoder is equivalent to first training a point to point mapping G(E(x)) between the two manifolds and then trying to minimize the expected distance between the points on these two manifolds."}, {"heading": "3.2 MODE REGULARIZER", "text": "In addition to the metric regularizer, we propose a mode regularizer to further penalize missing modes. In traditional GANs, the optimization target for the generator is the empirical sum\u2211 i\u2207\u03b8 logD(G\u03b8(zi)). The missing mode problem is caused by the conjunction of two facts: (1) the areas near missing modes are rarely visited by the generator, by definition, thus providing very few examples to improve the generator around those areas, and (2) both missing modes and nonmissing modes tend to correspond to a high value of D, because the generator is not perfect so that the discriminator can take strong decisions locally and obtain a high value of D even near non-missing modes.\nto move towards a nearby mode of the data generating distribution. In this way, we can achieve fair probability mass distribution across different modes.\nIn short, our regularized optimization target for the generator and the encoder becomes:\nTG = Ez[logD(G(z))] + Ex\u223cpd [\u03bb1d(x,G \u25e6 E(x)) + \u03bb2 logD(G \u25e6 E(x))] (1) TE = Ex\u223cpd [\u03bb1d(x,G \u25e6 E(x)) + \u03bb2 logD(G \u25e6 E(x))] (2)"}, {"heading": "3.3 MANIFOLD-DIFFUSION TRAINING FOR REGULARIZED GANS", "text": "On some large scale datasets, CelebA for example, the regularizers we have discussed do improve the diversity of generated samples, but the quality of samples may not be as good without carefully tuning the hyperparameters. Here we propose a new algorithm for training metric-regularized GANs, which is very stable and much easier to tune for producing good samples.\nThe proposed algorithm divides the training procedure of GANs into two steps: a manifold step and a diffusion step. In the manifold step, we try to match the generation manifold and the real data manifold with the help of an encoder and the geometric metric loss. In the diffusion step, we try to distribute the probability mass on the generation manifold fairly according to the real data distribution.\nAn example of manifold-diffusion training of GAN (MDGAN for short) is as follows: we train a discriminator D1 which separates between the samples x and G \u25e6E(x), for x from the data, and we optimizeGwith respect to the regularized GAN loss E[logD1(G\u25e6E(x))+\u03bbd(x,G\u25e6E(x))] in order to match the two manifolds. In the diffusion step we train a discriminator D2 between distributions G(z) and G \u25e6 E(x), and we train G to maximize logD2(G(z)). Since these two distributions are now nearly on the same low dimensional manifold, the discriminator D2 provides much smoother and more stable gradients. The detailed training procedure is given in Appendix A. See Figure 6 for the quality of generated samples."}, {"heading": "3.4 EVALUATION METRICS FOR MODE MISSING", "text": "In order to estimate both the missing modes and the sample qualities in our experiments, we used several different metrics for different experiments instead of human annotators.\nThe inception score (Salimans et al., 2016) was considered as a good assessment for sample quality from a labelled dataset:\nexp (ExKL(p(y|x)||p\u2217(y))) (3)\nWhere x denotes one sample, p(y|x) is the softmax output of a trained classifier of the labels, and p\u2217(y) is the overall label distribution of generated samples. The intuition behind this score is that a strong classifier usually has a high confidence for good samples. However, the inception score is sometimes not a good metric for our purpose. Assume a generative model that collapse to a very bad image. Although the model is very bad, it can have a perfect inception score, because p(y|x) can have a high entropy and p\u2217(y) can have a low entropy. So instead, for labelled datasets, we propose another assessment for both visual quality and variety of samples, the MODE score:\nexp (ExKL(p(y|x)||p(y))\u2212KL(p\u2217(y)||p(y))) (4)\nwhere p(y) is the distribution of labels in the training data. According to our human evaluation experiences, the MODE score successfully measures two important aspects of generative models, i.e., variety and visual quality, in one metric.\nHowever, in datasets without labels (LSUN) or where the labels are not sufficient to characterize every data mode (CelebA), the above metric does not work well. We instead train a third party discriminator between the real data and the generated data from the model. It is similar to the GAN discriminator but is not used to train the generator. We can view the output of the discriminator as an estimator for the quantity (See (Goodfellow et al., 2014) for proof):\nD\u2217(s) \u2248 pg(s) pg(s) + pd(s)\n(5)\nWhere pg is the probability density of the generator and pd is the density of the data distribution. To prevent D\u2217 from learning a perfect 0-1 separation of pg and pd, we inject a zero-mean Gaussian noise to the inputs when training D\u2217. After training, we test D\u2217 on the test set T of the real dataset. If for any test sample t \u2208 T , the discrimination value D(t) is close to 1, we can conclude that the mode corresponding to t is missing. In this way, although we cannot measure exactly the number of modes that are missing, we have a good estimator of the total probability mass of all the missing modes.\n4 EXPERIMENTS\n4.1 MNIST\nTable 1: Grid Search for Hyperparameters.\nnLayerG [2,3,4] nLayerD [2,3,4] sizeG [400,800,1600,3200] sizeD [256, 512, 1024] dropoutD [True,False] optimG [SGD,Adam] optimD [SGD,Adam] lr [1e-2,1e-3,1e-4]\nWe perform two classes of experiments on MNIST. For the MNIST dataset, we can assume that the data generating distribution can be approximated with ten dominant modes, if we define the term \u201cmode\u201d here as a connected component of the data manifold."}, {"heading": "4.1.1 GRID SEARCH FOR MNIST GAN MODELS", "text": "In order to systemically explore the effect of our proposed regularizers on GAN models in terms of improving stability and sample quality, we use a large scale grid search of different GAN hyper-parameters on the MNIST dataset. The grid search is based on a pair of randomly selected loss weights: \u03bb1 = 0.2 and \u03bb2 = 0.4. We use the same hyper-parameter settings for both GAN and Regularized GAN, and list the search ranges in Table 1. Our grid search is similar to those proposed in Zhao et al. (2016). Please refer to it for detailed explanations regarding these hyper-parameters.\nFor evaluation, we first train a 4-layer CNN classifier on the MNIST digits, and then apply it to compute the MODE scores for the generated samples from all these models. The resulting distribution of MODE score is shown in Figure 3. Clearly, our proposed regularizer significantly improves the MODE scores and thus demonstrates its benefits on stabilizing GANs and improving sample qualities.\nTo illustrate the effect of regularizers with different coefficients, we randomly pick an architecture and train it with different \u03bb1 = \u03bb2. The results are shown in Figure 4."}, {"heading": "4.1.2 COMPOSITIONAL MNIST DATA WITH 1000 MODES", "text": "In order to quantitatively study the effect of our regularizers on the missing modes, we concatenate three MNIST digits to a number in [0,999] in a single 64x64 image, and then train DCGAN as a baseline model on the 1000 modes dataset. The digits on the image are sampled with different\nprobabilities, in order to test the model\u2019s capability to preserve small modes in generation. We again use a pre-trained classifier for MNIST instead of a human to evaluate the models.\nThe performances on the compositional experiment are measured by two metrics. #Miss represents the classifier-reported number of missing modes, which is the size of the set of numbers that the model never generates. KL stands for the KL divergence between the classifier-reported distribution of generated numbers and the distribution of numbers in the training data (as for the Inception score). The results are shown in Table 2. With the help of our proposed regularizer, both the number of missing modes and KL divergence drop dramatically among all the sets of the compositional MNIST dataset, which again proves the effectiveness of our regularizer for preventing the missing modes problem."}, {"heading": "4.2 CELEBA", "text": "To test the effectiveness of our proposal on harder problems, we implement an encoder for the DCGAN algorithm and train our model with different hyper-parameters together with the DCGAN baseline on the CelebA dataset. We provide the detailed architecture of our regularized DCGAN in Appendix B."}, {"heading": "4.2.1 MISSING MODES ESTIMATION ON CELEBA", "text": "We also employ a third party discriminator trained with injected noise as a metric for missing mode estimation. To implement this, we add noise in the input layer in the discriminator network. For each GAN model to be estimated, we independently train this noisy discriminator, as mode estimator, with the same architecture and hyper-parameters on the generated data and the training data. We then apply the mode estimator to the test data. The images which have high mode estimator outputs can be viewed as on the missing modes.\nThe comparison result is shown in Table 3. Both our proposed Regularized-GAN and MDGAN outperform baseline DCGAN models on all settings. Especially, MDGAN suppresses other models, showing its superiority on modes preserving. We also find that, although sharing the same architecture, the DCGAN with 200-dimensional noise performs quite worse than that with 100-dimensional noise as input. On the contrary, our regularized GAN performs more consistently.\nTo get a better understanding of the models\u2019 performance, we want to figure out when and where these models miss the modes. Visualizing the test images associated with missed modes is instructive. In Figure 5, the left three images are missed by all models. It is rare to see in the training data the cap in the second image and the type of background in the third, which thus can be viewed as small modes under this situation. These three images should be considered as the hardest test data\nfor GAN to learn. Nonetheless, our best model, MDGAN still capture certain small modes. The seven images on the right in Figure 5 are only missed by DCGAN. The sideface, paleface, black, and the berets are special attributes among these images, but our proposed MDGAN performs well on all of them."}, {"heading": "4.2.2 QUALITATIVE EVALUATION OF GENERATED SAMPLES", "text": "After quantitative evaluation, we manually examine the generated samples by our regularized GAN to see whether the proposed regularizer has side-effects on sample quality. We compare our model with ALI (Dumoulin et al., 2016), VAEGAN (Larsen et al., 2015), and DCGAN (Radford et al., 2015) in terms of sample visual quality and mode diversity. Samples generated from these models are shown in Figure 6.\nBoth MDDGAN and Regularized-GAN generate clear and natural-looking face images. Although ALI\u2019s samples are plausible, they are sightly deformed in comparison with those from MDGAN. The samples from VAEGAN and DCGAN are unsatisfactory due to insufficient sharpness.\nAs to sample quality, it is worth noting that the samples from MDGAN enjoy fewer distortions. With all four other models, the majority of generated samples suffer from some sort of distortion. However, for the samples generated by MDGAN, the level of distortion is lower compared with the other four GAN-based models. We attribute it to the help of the autoencoder as the regularizer to alter the generation manifolds. In this way, the generator is able to learn fine-grained details such as face edges. As a result, MDGAN is able to avoid distortions.\nIn terms of missing modes problem, we instructed five individuals to conduct human evaluation on the generated samples. They achieve consensus that MDGAN wins in terms of mode diversities. Two people pointed out that MDGAN generates a larger amount of samples with side faces than other models. We select several of these side face samples in Figure 7. Clearly, our samples maintain acceptable visual fidelity meanwhile share diverse modes. Combined with the above quantitative\nresults, it is convincing that our regularizers bring benefits for both training stability and mode variety without the loss of sample quality."}, {"heading": "5 CONCLUSIONS", "text": "Although GANs achieve state-of-the-art results on a large variety of unsupervised learning tasks, training them is considered highly unstable, very difficult and sensitive to hyper-parameters, all the while, missing modes from the data distribution or even collapsing large amounts of probability mass on some modes. Successful GAN training usually requires large amounts of human and computing efforts to fine tune the hyper-parameters, in order to stabilize training and avoid collapsing. Researchers usually rely on their own experience and published tricks and hyper-parameters instead of systematic methods for training GANs.\nWe provide systematic ways to measure and avoid the missing modes problem and stabilize training with the proposed autoencoder-based regularizers. The key idea is that some geometric metrics can provide more stable gradients than trained discriminators, and when combined with the encoder, they can be used as regularizers for training. These regularizers can also penalize missing modes and encourage a fair distribution of probability mass on the generation manifold."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We thank Naiyan Wang, Jianbo Ye, Yuchen Ding, Saboya Yang for their GPU support. We also want to thank Huiling Zhen for helpful discussions, as well as Junbo Zhao for providing the details of grid search experiments on his EBGAN model."}, {"heading": "A APPENDIX: PSEUDO CODE FOR MDGAN", "text": "In this Appendix, we give the detailed training procedure of an MDGAN example we discuss in Section 3.3."}, {"heading": "B APPENDIX: ARCHITECTURE FOR EXPERIMENTS", "text": "We use similar architectures for Compositional MNIST and CelebA experiments. The architecture is based on that found in DCGAN Radford et al. (2015). Apart from the discriminator and generator which are the same as DCGAN, we add an encoder which is the \u201dinverse\u201d of the generator, by reversing the order of layers and replacing the de-convolutional layers with convolutional layers.\nOne has to pay particular attention to batch normalization layers. In DCGAN, there are batch normalization layers both in the generator and the discriminator. However, two classes of data go through the batch normalization layers in the generator. One come from sampled noise z, the other one come from the encoder. In our implementation, we separate the batch statistics for these two classes of data in the generator, while keeping the parameters of BN layer to be shared. In this way, the batch statistics of these two kinds of batches cannot interfere with each other."}, {"heading": "C APPENDIX: ADDITIONAL SYNTHESIZED EXPERIMENTS", "text": "To demonstrate the effectiveness of mode-regularized GANs proposed in this paper, we train a very simple GAN architecture on synthesized 2D dataset, following Metz et al. (2016).\nThe data is sampled from a mixture of 6 Gaussians, with standard derivation of 0.1. The means of the Gaussians are placed around a circle with radius 5. The generator network has two ReLU hidden layers with 128 neurons. It generates 2D output samples from 3D uniform noise from [0,1]. The discriminator consists of only one fully connected layer of ReLU neurons, mapping the 2D input to\na real 1D number. Both networks are optimized with the Adam optimizer with the learning rate of 1e-4.\nIn the regularized version, we choose \u03bb1 = \u03bb2 = 0.005. The comparison between the generator distribution from standard GAN and our proposed regularized GAN are shown in Figure 9."}], "references": [{"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily L Denton", "Soumith Chintala", "Rob Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Adversarial feature learning", "author": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1605.09782,", "citeRegEx": "Donahue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2016}, {"title": "Generating images with perceptual similarity metrics based on deep networks", "author": ["Alexey Dosovitskiy", "Thomas Brox"], "venue": "arXiv preprint arXiv:1602.02644,", "citeRegEx": "Dosovitskiy and Brox.,? \\Q2016\\E", "shortCiteRegEx": "Dosovitskiy and Brox.", "year": 2016}, {"title": "Adversarially learned inference", "author": ["Vincent Dumoulin", "Ishmael Belghazi", "Ben Poole", "Alex Lamb", "Martin Arjovsky", "Olivier Mastropietro", "Aaron Courville"], "venue": "arXiv preprint arXiv:1606.00704,", "citeRegEx": "Dumoulin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dumoulin et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "A Efros. Image-to-image translation with conditional adversarial networks", "author": ["Phillip Isola", "Jun-Yan Zhu", "Tinghui Zhou", "Alexei"], "venue": "arxiv,", "citeRegEx": "Isola et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Isola et al\\.", "year": 2016}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["Anders Boesen Lindbo Larsen", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "venue": "arXiv preprint arXiv:1512.09300,", "citeRegEx": "Larsen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 2015}, {"title": "Photo-realistic single image super-resolution using a generative adversarial network", "author": ["Christian Ledig", "Lucas Theis", "Ferenc Husz\u00e1r", "Jose Caballero", "Andrew Aitken", "Alykhan Tejani", "Johannes Totz", "Zehan Wang", "Wenzhe Shi"], "venue": "arXiv preprint arXiv:1609.04802,", "citeRegEx": "Ledig et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ledig et al\\.", "year": 2016}, {"title": "Precomputed real-time texture synthesis with markovian generative adversarial networks", "author": ["Chuan Li", "Michael Wand"], "venue": "arXiv preprint arXiv:1604.04382,", "citeRegEx": "Li and Wand.,? \\Q2016\\E", "shortCiteRegEx": "Li and Wand.", "year": 2016}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["Michael Mathieu", "Camille Couprie", "Yann LeCun"], "venue": "arXiv preprint arXiv:1511.05440,", "citeRegEx": "Mathieu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2015}, {"title": "Unrolled generative adversarial networks", "author": ["Luke Metz", "Ben Poole", "David Pfau", "Jascha Sohl-Dickstein"], "venue": "arXiv preprint arXiv:1611.02163,", "citeRegEx": "Metz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Metz et al\\.", "year": 2016}, {"title": "Conditional generative adversarial nets", "author": ["Mehdi Mirza", "Simon Osindero"], "venue": "arXiv preprint arXiv:1411.1784,", "citeRegEx": "Mirza and Osindero.,? \\Q2014\\E", "shortCiteRegEx": "Mirza and Osindero.", "year": 2014}, {"title": "Plug & play generative networks: Conditional iterative generation of images in latent space", "author": ["Anh Nguyen", "Jason Yosinski", "Yoshua Bengio", "Alexey Dosovitskiy", "Jeff Clune"], "venue": "arXiv preprint arXiv:1612.00005,", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Generative adversarial text to image synthesis", "author": ["Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee"], "venue": "arXiv preprint arXiv:1605.05396,", "citeRegEx": "Reed et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2016}, {"title": "Temporal generative adversarial nets", "author": ["Masaki Saito", "Eiichi Matsumoto"], "venue": "arXiv preprint arXiv:1611.06624,", "citeRegEx": "Saito and Matsumoto.,? \\Q2016\\E", "shortCiteRegEx": "Saito and Matsumoto.", "year": 2016}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "arXiv preprint arXiv:1606.03498,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Amortised map inference for image super-resolution", "author": ["Casper Kaae S\u00f8nderby", "Jose Caballero", "Lucas Theis", "Wenzhe Shi", "Ferenc Husz\u00e1r"], "venue": "arXiv preprint arXiv:1610.04490,", "citeRegEx": "S\u00f8nderby et al\\.,? \\Q2016\\E", "shortCiteRegEx": "S\u00f8nderby et al\\.", "year": 2016}, {"title": "Generating videos with scene dynamics", "author": ["Carl Vondrick", "Hamed Pirsiavash", "Antonio Torralba"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Vondrick et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vondrick et al\\.", "year": 2016}, {"title": "Generative image modeling using style and structure adversarial networks", "author": ["Xiaolong Wang", "Abhinav Gupta"], "venue": "In ECCV,", "citeRegEx": "Wang and Gupta.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Gupta.", "year": 2016}, {"title": "Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling", "author": ["Jiajun Wu", "Chengkai Zhang", "Tianfan Xue", "William T Freeman", "Joshua B Tenenbaum"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Energy-based generative adversarial network", "author": ["Junbo Zhao", "Michael Mathieu", "Yann LeCun"], "venue": "arXiv preprint arXiv:1609.03126,", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}, {"title": "Learning temporal transformations from time-lapse videos", "author": ["Yipin Zhou", "Tamara L Berg"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Zhou and Berg.,? \\Q2016\\E", "shortCiteRegEx": "Zhou and Berg.", "year": 2016}, {"title": "Generative visual manipulation on the natural image manifold", "author": ["Jun-Yan Zhu", "Philipp Kr\u00e4henb\u00fchl", "Eli Shechtman", "Alexei A. Efros"], "venue": "In Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}, {"title": "The data is sampled from a mixture of 6 Gaussians, with standard derivation of 0.1. The means of the Gaussians are placed around a circle with radius 5. The generator network has two ReLU hidden layers with 128 neurons. It generates 2D output samples from 3D uniform noise from [0,1]. The discriminator consists of only one fully connected layer of ReLU neurons, mapping the 2D input", "author": ["Metz"], "venue": "GAN architecture on synthesized 2D dataset,", "citeRegEx": "Metz,? \\Q2016\\E", "shortCiteRegEx": "Metz", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "Generative adversarial networks (GAN) (Goodfellow et al., 2014) have demonstrated their potential on various tasks, such as image generation, image super-resolution, 3D object generation, and video prediction (Radford et al.", "startOffset": 38, "endOffset": 63}, {"referenceID": 13, "context": ", 2014) have demonstrated their potential on various tasks, such as image generation, image super-resolution, 3D object generation, and video prediction (Radford et al., 2015; Ledig et al., 2016; S\u00f8nderby et al., 2016; Nguyen et al., 2016; Wu et al., 2016; Mathieu et al., 2015).", "startOffset": 153, "endOffset": 278}, {"referenceID": 7, "context": ", 2014) have demonstrated their potential on various tasks, such as image generation, image super-resolution, 3D object generation, and video prediction (Radford et al., 2015; Ledig et al., 2016; S\u00f8nderby et al., 2016; Nguyen et al., 2016; Wu et al., 2016; Mathieu et al., 2015).", "startOffset": 153, "endOffset": 278}, {"referenceID": 17, "context": ", 2014) have demonstrated their potential on various tasks, such as image generation, image super-resolution, 3D object generation, and video prediction (Radford et al., 2015; Ledig et al., 2016; S\u00f8nderby et al., 2016; Nguyen et al., 2016; Wu et al., 2016; Mathieu et al., 2015).", "startOffset": 153, "endOffset": 278}, {"referenceID": 12, "context": ", 2014) have demonstrated their potential on various tasks, such as image generation, image super-resolution, 3D object generation, and video prediction (Radford et al., 2015; Ledig et al., 2016; S\u00f8nderby et al., 2016; Nguyen et al., 2016; Wu et al., 2016; Mathieu et al., 2015).", "startOffset": 153, "endOffset": 278}, {"referenceID": 20, "context": ", 2014) have demonstrated their potential on various tasks, such as image generation, image super-resolution, 3D object generation, and video prediction (Radford et al., 2015; Ledig et al., 2016; S\u00f8nderby et al., 2016; Nguyen et al., 2016; Wu et al., 2016; Mathieu et al., 2015).", "startOffset": 153, "endOffset": 278}, {"referenceID": 9, "context": ", 2014) have demonstrated their potential on various tasks, such as image generation, image super-resolution, 3D object generation, and video prediction (Radford et al., 2015; Ledig et al., 2016; S\u00f8nderby et al., 2016; Nguyen et al., 2016; Wu et al., 2016; Mathieu et al., 2015).", "startOffset": 153, "endOffset": 278}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks.", "startOffset": 43, "endOffset": 68}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets.", "startOffset": 43, "endOffset": 180}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN\u2019s representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information.", "startOffset": 43, "endOffset": 313}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN\u2019s representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information. Motivated from this, several conditional variants of GAN has been applied to a wide range of tasks, including image prediction from a normal map Wang & Gupta (2016), image synthesis from text Reed et al.", "startOffset": 43, "endOffset": 635}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN\u2019s representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information. Motivated from this, several conditional variants of GAN has been applied to a wide range of tasks, including image prediction from a normal map Wang & Gupta (2016), image synthesis from text Reed et al. (2016) and edge map Isola et al.", "startOffset": 43, "endOffset": 681}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN\u2019s representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information. Motivated from this, several conditional variants of GAN has been applied to a wide range of tasks, including image prediction from a normal map Wang & Gupta (2016), image synthesis from text Reed et al. (2016) and edge map Isola et al. (2016), real-time image manipulation Zhu et al.", "startOffset": 43, "endOffset": 714}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN\u2019s representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information. Motivated from this, several conditional variants of GAN has been applied to a wide range of tasks, including image prediction from a normal map Wang & Gupta (2016), image synthesis from text Reed et al. (2016) and edge map Isola et al. (2016), real-time image manipulation Zhu et al. (2016), temporal image generation Zhou & Berg (2016); Saito & Matsumoto (2016); Vondrick et al.", "startOffset": 43, "endOffset": 762}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN\u2019s representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information. Motivated from this, several conditional variants of GAN has been applied to a wide range of tasks, including image prediction from a normal map Wang & Gupta (2016), image synthesis from text Reed et al. (2016) and edge map Isola et al. (2016), real-time image manipulation Zhu et al. (2016), temporal image generation Zhou & Berg (2016); Saito & Matsumoto (2016); Vondrick et al.", "startOffset": 43, "endOffset": 808}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN\u2019s representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information. Motivated from this, several conditional variants of GAN has been applied to a wide range of tasks, including image prediction from a normal map Wang & Gupta (2016), image synthesis from text Reed et al. (2016) and edge map Isola et al. (2016), real-time image manipulation Zhu et al. (2016), temporal image generation Zhou & Berg (2016); Saito & Matsumoto (2016); Vondrick et al.", "startOffset": 43, "endOffset": 834}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN\u2019s representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information. Motivated from this, several conditional variants of GAN has been applied to a wide range of tasks, including image prediction from a normal map Wang & Gupta (2016), image synthesis from text Reed et al. (2016) and edge map Isola et al. (2016), real-time image manipulation Zhu et al. (2016), temporal image generation Zhou & Berg (2016); Saito & Matsumoto (2016); Vondrick et al. (2016), texture synthesis, style transfer, and video stylization Li & Wand (2016).", "startOffset": 43, "endOffset": 858}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN\u2019s representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information. Motivated from this, several conditional variants of GAN has been applied to a wide range of tasks, including image prediction from a normal map Wang & Gupta (2016), image synthesis from text Reed et al. (2016) and edge map Isola et al. (2016), real-time image manipulation Zhu et al. (2016), temporal image generation Zhou & Berg (2016); Saito & Matsumoto (2016); Vondrick et al. (2016), texture synthesis, style transfer, and video stylization Li & Wand (2016). Researchers also aim at stretching GAN\u2019s limit to generate higher-resolution, photo-realistic images.", "startOffset": 43, "endOffset": 933}, {"referenceID": 0, "context": "Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution. At each level of their LAPGAN, both the generator and the discriminator are convolutional networks. As an alternative to LAPGAN, Radford et al. (2015) successfully designs a class of deep convolutional generative adversarial networks which has led to significant improvements on unsupervised image representation learning.", "startOffset": 0, "endOffset": 264}, {"referenceID": 0, "context": "Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution. At each level of their LAPGAN, both the generator and the discriminator are convolutional networks. As an alternative to LAPGAN, Radford et al. (2015) successfully designs a class of deep convolutional generative adversarial networks which has led to significant improvements on unsupervised image representation learning. Another line of work aimed at improving GANs are through feature learning, including features from the latent space and image space. The motivation is that features from different spaces are complementary for generating perceptual and natural-looking images. With this perspective, some researchers use distances between learned features as losses for training objectives for generative models. Larsen et al. (2015) combine a variational autoencoder objective with a GAN and utilize the learned features from the discriminator in the GANs for better image similarity metrics.", "startOffset": 0, "endOffset": 852}, {"referenceID": 0, "context": "Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution. At each level of their LAPGAN, both the generator and the discriminator are convolutional networks. As an alternative to LAPGAN, Radford et al. (2015) successfully designs a class of deep convolutional generative adversarial networks which has led to significant improvements on unsupervised image representation learning. Another line of work aimed at improving GANs are through feature learning, including features from the latent space and image space. The motivation is that features from different spaces are complementary for generating perceptual and natural-looking images. With this perspective, some researchers use distances between learned features as losses for training objectives for generative models. Larsen et al. (2015) combine a variational autoencoder objective with a GAN and utilize the learned features from the discriminator in the GANs for better image similarity metrics. It is shown that the learned distance from the discriminator is of great help for the sample visual fidelity. Recent literature have also shown impressive results on image super-resolution to infer photo-realistic natural images for 4x upscaling factors Ledig et al. (2016); S\u00f8nderby et al.", "startOffset": 0, "endOffset": 1286}, {"referenceID": 0, "context": "Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution. At each level of their LAPGAN, both the generator and the discriminator are convolutional networks. As an alternative to LAPGAN, Radford et al. (2015) successfully designs a class of deep convolutional generative adversarial networks which has led to significant improvements on unsupervised image representation learning. Another line of work aimed at improving GANs are through feature learning, including features from the latent space and image space. The motivation is that features from different spaces are complementary for generating perceptual and natural-looking images. With this perspective, some researchers use distances between learned features as losses for training objectives for generative models. Larsen et al. (2015) combine a variational autoencoder objective with a GAN and utilize the learned features from the discriminator in the GANs for better image similarity metrics. It is shown that the learned distance from the discriminator is of great help for the sample visual fidelity. Recent literature have also shown impressive results on image super-resolution to infer photo-realistic natural images for 4x upscaling factors Ledig et al. (2016); S\u00f8nderby et al. (2016); Nguyen et al.", "startOffset": 0, "endOffset": 1310}, {"referenceID": 0, "context": "Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution. At each level of their LAPGAN, both the generator and the discriminator are convolutional networks. As an alternative to LAPGAN, Radford et al. (2015) successfully designs a class of deep convolutional generative adversarial networks which has led to significant improvements on unsupervised image representation learning. Another line of work aimed at improving GANs are through feature learning, including features from the latent space and image space. The motivation is that features from different spaces are complementary for generating perceptual and natural-looking images. With this perspective, some researchers use distances between learned features as losses for training objectives for generative models. Larsen et al. (2015) combine a variational autoencoder objective with a GAN and utilize the learned features from the discriminator in the GANs for better image similarity metrics. It is shown that the learned distance from the discriminator is of great help for the sample visual fidelity. Recent literature have also shown impressive results on image super-resolution to infer photo-realistic natural images for 4x upscaling factors Ledig et al. (2016); S\u00f8nderby et al. (2016); Nguyen et al. (2016). Despite these promising successes, GANs are notably hard to train.", "startOffset": 0, "endOffset": 1332}, {"referenceID": 0, "context": "Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution. At each level of their LAPGAN, both the generator and the discriminator are convolutional networks. As an alternative to LAPGAN, Radford et al. (2015) successfully designs a class of deep convolutional generative adversarial networks which has led to significant improvements on unsupervised image representation learning. Another line of work aimed at improving GANs are through feature learning, including features from the latent space and image space. The motivation is that features from different spaces are complementary for generating perceptual and natural-looking images. With this perspective, some researchers use distances between learned features as losses for training objectives for generative models. Larsen et al. (2015) combine a variational autoencoder objective with a GAN and utilize the learned features from the discriminator in the GANs for better image similarity metrics. It is shown that the learned distance from the discriminator is of great help for the sample visual fidelity. Recent literature have also shown impressive results on image super-resolution to infer photo-realistic natural images for 4x upscaling factors Ledig et al. (2016); S\u00f8nderby et al. (2016); Nguyen et al. (2016). Despite these promising successes, GANs are notably hard to train. Although Radford et al. (2015) provide a class of empirical architectural choices that are critical to stabilize GAN\u2019s training, it would be even better to train GANs more robustly and systematically.", "startOffset": 0, "endOffset": 1431}, {"referenceID": 0, "context": "Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution. At each level of their LAPGAN, both the generator and the discriminator are convolutional networks. As an alternative to LAPGAN, Radford et al. (2015) successfully designs a class of deep convolutional generative adversarial networks which has led to significant improvements on unsupervised image representation learning. Another line of work aimed at improving GANs are through feature learning, including features from the latent space and image space. The motivation is that features from different spaces are complementary for generating perceptual and natural-looking images. With this perspective, some researchers use distances between learned features as losses for training objectives for generative models. Larsen et al. (2015) combine a variational autoencoder objective with a GAN and utilize the learned features from the discriminator in the GANs for better image similarity metrics. It is shown that the learned distance from the discriminator is of great help for the sample visual fidelity. Recent literature have also shown impressive results on image super-resolution to infer photo-realistic natural images for 4x upscaling factors Ledig et al. (2016); S\u00f8nderby et al. (2016); Nguyen et al. (2016). Despite these promising successes, GANs are notably hard to train. Although Radford et al. (2015) provide a class of empirical architectural choices that are critical to stabilize GAN\u2019s training, it would be even better to train GANs more robustly and systematically. Salimans et al. (2016) propose feature matching technique to stabilize GAN\u2019s training.", "startOffset": 0, "endOffset": 1624}, {"referenceID": 0, "context": "Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution. At each level of their LAPGAN, both the generator and the discriminator are convolutional networks. As an alternative to LAPGAN, Radford et al. (2015) successfully designs a class of deep convolutional generative adversarial networks which has led to significant improvements on unsupervised image representation learning. Another line of work aimed at improving GANs are through feature learning, including features from the latent space and image space. The motivation is that features from different spaces are complementary for generating perceptual and natural-looking images. With this perspective, some researchers use distances between learned features as losses for training objectives for generative models. Larsen et al. (2015) combine a variational autoencoder objective with a GAN and utilize the learned features from the discriminator in the GANs for better image similarity metrics. It is shown that the learned distance from the discriminator is of great help for the sample visual fidelity. Recent literature have also shown impressive results on image super-resolution to infer photo-realistic natural images for 4x upscaling factors Ledig et al. (2016); S\u00f8nderby et al. (2016); Nguyen et al. (2016). Despite these promising successes, GANs are notably hard to train. Although Radford et al. (2015) provide a class of empirical architectural choices that are critical to stabilize GAN\u2019s training, it would be even better to train GANs more robustly and systematically. Salimans et al. (2016) propose feature matching technique to stabilize GAN\u2019s training. The generator is required to match the statistics of intermediate features of the discriminator. Similar idea is adopted by Zhao et al. (2016).", "startOffset": 0, "endOffset": 1831}, {"referenceID": 3, "context": "Furthermore, some researchers make use of information in both spaces in a unified learning procedure (Dumoulin et al., 2016; Donahue et al., 2016).", "startOffset": 101, "endOffset": 146}, {"referenceID": 1, "context": "Furthermore, some researchers make use of information in both spaces in a unified learning procedure (Dumoulin et al., 2016; Donahue et al., 2016).", "startOffset": 101, "endOffset": 146}, {"referenceID": 6, "context": "Our work is related to VAEGAN (Larsen et al., 2015) in terms of training an autoencoder or VAE jointly with the GAN model.", "startOffset": 30, "endOffset": 51}, {"referenceID": 1, "context": ", 2016; Donahue et al., 2016). In Dumoulin et al. (2016), one trains not just a generator but also an encoder, and the discriminator is trained to distinguish between two joint distributions over image and latent spaces produced either by the application of the encoder on the training data or by the application of the generator (decoder) to the latent prior.", "startOffset": 8, "endOffset": 57}, {"referenceID": 1, "context": ", 2016; Donahue et al., 2016). In Dumoulin et al. (2016), one trains not just a generator but also an encoder, and the discriminator is trained to distinguish between two joint distributions over image and latent spaces produced either by the application of the encoder on the training data or by the application of the generator (decoder) to the latent prior. This is in contrast with the regular GAN training, in which the discriminator only attempts to separate the distributions in the image space. Parallelly, Metz et al. (2016) stabilize GANs by unrolling the optimization of discriminator, which can be considered as an orthogonal work with ours.", "startOffset": 8, "endOffset": 534}, {"referenceID": 3, "context": "As pointed out by Goodfellow et al. (2014); Denton et al.", "startOffset": 18, "endOffset": 43}, {"referenceID": 0, "context": "(2014); Denton et al. (2015); Radford et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "(2014); Denton et al. (2015); Radford et al. (2015), when the data manifold and the generation manifold are disjoint (which is true in almost all practical situations), it is equivalent to training a characteristic function to be very close to 1 on the data manifold, and 0 on the generation manifold.", "startOffset": 8, "endOffset": 52}, {"referenceID": 0, "context": "(2014); Denton et al. (2015); Radford et al. (2015), when the data manifold and the generation manifold are disjoint (which is true in almost all practical situations), it is equivalent to training a characteristic function to be very close to 1 on the data manifold, and 0 on the generation manifold. In order to pass good gradient information to the generator, it is important that the trained discriminator produces stable and smooth gradients. However, since the discriminator objective does not directly depend on the behavior of the discriminator in other parts of the space, training can easily fail if the shape of the discriminator function is not as expected. As an example,Denton et al. (2015) noted a common failure pattern for training GANs which is the vanishing gradient problem, in which the discriminator D perfectly classifies real and fake examples, such that around the fake examples, D is nearly zero.", "startOffset": 8, "endOffset": 705}, {"referenceID": 0, "context": "This problem exists even when we use logD(G(z)) as target for the generator, as noted by Denton et al. (2015) and our experiments.", "startOffset": 89, "endOffset": 110}, {"referenceID": 3, "context": "For instance, the pixel-wise L distance, or the distance of learned features by the discriminator (Dumoulin et al., 2016) or by other networks, such as a VGG classifier.", "startOffset": 98, "endOffset": 121}, {"referenceID": 7, "context": "(Ledig et al., 2016) The geometric intuition for this regularizer is straight-forward.", "startOffset": 0, "endOffset": 20}, {"referenceID": 16, "context": "The inception score (Salimans et al., 2016) was considered as a good assessment for sample quality from a labelled dataset:", "startOffset": 20, "endOffset": 43}, {"referenceID": 4, "context": "We can view the output of the discriminator as an estimator for the quantity (See (Goodfellow et al., 2014) for proof):", "startOffset": 82, "endOffset": 107}, {"referenceID": 21, "context": "Our grid search is similar to those proposed in Zhao et al. (2016). Please refer to it for detailed explanations regarding these hyper-parameters.", "startOffset": 48, "endOffset": 67}, {"referenceID": 3, "context": "We compare our model with ALI (Dumoulin et al., 2016), VAEGAN (Larsen et al.", "startOffset": 30, "endOffset": 53}, {"referenceID": 6, "context": ", 2016), VAEGAN (Larsen et al., 2015), and DCGAN (Radford et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 13, "context": ", 2015), and DCGAN (Radford et al., 2015) in terms of sample visual quality and mode diversity.", "startOffset": 19, "endOffset": 41}], "year": 2017, "abstractText": "Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.", "creator": "LaTeX with hyperref package"}}}