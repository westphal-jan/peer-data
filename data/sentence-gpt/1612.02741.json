{"id": "1612.02741", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2016", "title": "Coupling Distributed and Symbolic Execution for Natural Language Queries", "abstract": "Building neural networks to query a knowledge base (a table) with natural language is an emerging research topic in NLP. The neural enquirer typically necessitates multiple steps of execution because of the compositionality of queries. In previous studies, researchers have developed either distributed enquirers or symbolic ones for table querying information, and thus used the natural language system in its computational models to model the language's underlying language complexity. The data generated was then stored in a relational database, and the resulting system was trained using the same machine learning tools used to model language complexity.\n\n\n\nThe computational model for enquirers is typically an advanced learning method, that is used to generate a set of nested enquirers. In a later study, the neural enquirers were trained on the task of selecting a representation of a number of variables. The process was run as follows:\n\n\n(A) Randomizing (R) Randomizing (R) Randomization (R)\n(C) Randomizing (C)\n(D) Randomizing (D) Randomizing (D)\n(E) Randomizing (D)\nThe neural enquirer is known to work in many fields of human-machine learning, including neural networks, but only in some fields of computational neuroscience. Anecdotally, we have not seen large-scale enquirers in many disciplines, with some authors focusing only on neural networks. But the literature on neural enquirers in the field has not yet been published.", "histories": [["v1", "Thu, 8 Dec 2016 17:45:16 GMT  (534kb,D)", "http://arxiv.org/abs/1612.02741v1", null], ["v2", "Thu, 16 Feb 2017 11:37:44 GMT  (1587kb,D)", "http://arxiv.org/abs/1612.02741v2", null], ["v3", "Tue, 25 Apr 2017 20:39:57 GMT  (4331kb,D)", "http://arxiv.org/abs/1612.02741v3", "ICLR-17 Workshop"], ["v4", "Fri, 16 Jun 2017 14:33:31 GMT  (1066kb,D)", "http://arxiv.org/abs/1612.02741v4", "Accepted by ICML-17; also presented at ICLR-17 Workshop"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL cs.NE cs.SE", "authors": ["lili mou", "zhengdong lu", "hang li", "zhi jin"], "accepted": true, "id": "1612.02741"}, "pdf": {"name": "1612.02741.pdf", "metadata": {"source": "CRF", "title": "Coupling Distributed and Symbolic Execution for Natural Language Queries", "authors": ["Lili Mou", "Zhengdong Lu", "Hang Li", "Zhi Jin"], "emails": ["doublepower.mou@gmail.com,", "luz@DeeplyCurious.ai", "HangLi.HL@huawei.com,", "zhijin@sei.pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Using natural language to query a knowledge base is an important task in NLP and has wide applications in question answering (QA) [Yin et al., 2016a], human-computer conversation [Wen et al., 2016], etc. Table 1 illustrates an example of a knowledge base (a table) and a query \u201cHow long is the game with the largest host country size?\u201d To answer the question, we shall first find a row with the largest value in the column Area, and then select the value of the chosen row with the column being Duration.\nA typical approach is to convert a natural language sentence to an \u201cexecutable\u201d logic forms for table/knowledge base querying, known as semantic parsing [Long et al., 2016; Pasupat and Liang, 2016]. Such approaches require extensive human engineering. With the fast development of modern neural networks, Dong and Lapata [2016] and Xiao et al. [2016] apply sequenceto-sequence (seq2seq) neural models to generate a logic form conditioned on an input sentence. However, seq2seq models need strong supervision of groundtruth logic forms, which are\n\u2217Work done during internship at Noah\u2019s Ark Lab, Huawei Technologies. The preprint version of this paper is formatted in a page-saving and thus environment-friendly style.\nQuery: How long is the game with the largest host country size? Knowledge base (table): Year City \u00b7 \u00b7 \u00b7 Area \u00b7 \u00b7 \u00b7 Duration\nlabor-intensive to obtain and ad hoc to a specific dataset; they cannot be trained in an end-to-end fashion with denotations1 only. An emerging research topic in neural semantic parsing is to query a knowledge base directly by neural networks [Yin et al., 2016b; Neelakantan et al., 2016; Liang et al., 2016]. Because queries can be composited in a highly complicated manner, neural enquirers necessitate multiple steps of execution. The difficulty then lies in the lack of step-by-step supervision. In other words, we only assume groundtruth denotations are available in realistic settings, and that we do not know the execution sequence or intermediate execution results.\nThere have seen several studies addressing the problem. Yin et al. [2016b] propose a fully distributed neural enquirer, comprising several neuralized execution layers of field attention, row annotation, etc. While the model is not efficient in execution because of intensive matrix/vector operation during neural information processing and lacks explicit interpretation of execution, it can be trained in an end-to-end fashion because all components in the neural enquirer are differentiable.\nNeelakantan et al. [2016], on the other hand, propose a neural programmer by defining a set of symbolic operations (e.g., argmax, greater than); at each step, all possible execution results are fused by a softmax layer, which predicts the probability of each operator at the current step. The step-bystep fusion is accomplished by weighted sum and the model is trained with mean square error. Hence, such approaches work with numeric tables, but may not be suited for other operations like string matching; it also suffers from the problem of \u201cexponential numbers of combinatorial states.\u201d Liang et al. [2016] train a symbolic executor by REINFORCE policy gradient, but it is known that the REINFORCE algorithm is sensitive to the\n1A denotation refers to the execution result.\nar X\niv :1\n61 2.\n02 74\n1v 1\n[ cs\n.L G\n] 8\nD ec\n2 01\n6\nNeural ExecutorIn\npu t Neural\nExecutor In te rm\ned ia te\n\u00a0R es ul ts Neural Executor\nIn te rm\ned ia te\n\u00a0R es ul ts\nO ut pu\nt\nOperator\u00a01 In pu t\nIn te rm\ned ia te\n\u00a0R es ul ts\nIn te rm\ned ia te\n\u00a0R es ul ts\nO ut pu\nt\nOperator\u00a02\nOperator\u00a0n\n. . .\nOperator\u00a01\nOperator\u00a02\nOperator\u00a0n\n. . .\nOperator\u00a01\nOperator\u00a02\nOperator\u00a0n\n. . .\nOperator/Argument\u00a0 Predictor Operator/Argument\u00a0 Predictor Operator/Argument\u00a0 Predictor\n(a)\n(b)\nDifferentiable\nImperfect\u00a0 Stepbystep supervision\nNon differentiable\nFigure 1: An overview of the coupled distributed and symbolic neural enquirers.\ninitial policy. It could be very difficult to get started with a random initial policy; thus it only works with simple cases.\nIn this paper, we propose coupled distributed and symbolic execution for natural language queries. Our intuition rises from the observation that a fully distributed/neuralized executor also exhibits some (imperfect) symbolic interpretation. For example, the field attention gadget in Yin et al. [2016b] generally aligns with column selection. We therefore use the distributed model\u2019s intermediate execution results as supervision signals to pretrain a symbolic executor. Guided by such imperfect step-by-step supervision, the symbolic executor learns a fairly meaningful initial policy, which largely alleviates the cold start problem of the REINFORCE algorithm.\nWe evaluate the proposed approach on the QA dataset in Yin et al. [2016b]. In our experiment, the REINFORCE algorithm alone takes long to get started; even if it does, it is stuck in a poor local optimum. Once pretrained by imperfect supervision signals, the symbolic executor can recover execution sequences with an accuracy of 83.8%. It also outperforms the distributed enquirer in terms of denotation accuracy. It should be mentioned that, in our experiment, neither the distributed enquirer nor the symbolic one is aware of groundtruth execution sequences, and that the entire model is trained with weak supervision of denotations.\nTo the best of our knowledge, we are the first to couple distributed and symbolic execution for semantic parsing. Our study also sheds light on neural sequence prediction in general."}, {"heading": "2 Approach", "text": "In this section, we first introduce the fully distributed (neuralized) enquirer proposed in Yin et al. [2016b]. Then we design a set of operators that are complete to the task at hand. At each execution step, a neural network predicts a particular operator and possibly arguments for symbolic execution.\nSubsection 2.3 provides a unified view of distributed and symbolic execution. (See also Figure 1). We explain how the symbolic executor is pretrained by the distributed model\u2019s intermediate execution results, and then further trained with the REINFORCE algorithm."}, {"heading": "2.1 Distributed Enquirer", "text": "Yin et al. [2016b] propose a fully distributed neural enquirer to query a table. By \u201cdistributed,\u201d we mean that all semantic units (including words in the query, entries in the table, and execution results) are represented as distributed, real-valued vectors and\nprocessed by neural networks. One of the most notable studies of distributed semantics is word embeddings, which map discrete words to vectors as anonymous meaning representations [Mikolov et al., 2013].\nThe distributed neural enquirer consists of the following main components \u2022 Query encoder. Words are mapped to word embeddings\nand a bi-directional recurrent neural network (RNN) aggregates information over the sentence. RNNs\u2019 last states in both directions are concatenated as the query representation (denoated as q). \u2022 Table encoder. All table cells are also represented as embeddings. For a cell c (e.g., Beijing in Table 1) with column/field name being f (e.g., City), the cell vector is the concatenation of the embeddings of c and f , further processed by a feed-forward neural network (also known as multi-layer perceptron). We denote the representation of a cell as c. \u2022 Executor. As shown in Figure 1a, the neural enquirer comprises several layers of execution, which is further illustrated in Figure 2. In each execution step, the neural network selects a field and then one or a few rows based on the previous execution results. More specifically, the execution result is a distribution pf over columns and a scalar r in (0, 1) for each row.2 Finally, a softmax layer is applied to the entire table to select a cell as the answer. We further describe the executor as follows. Let p(t\u22121)f and r (t\u22121) be the previous step\u2019s execution results. We represent the selected cell in each row as the sum of all cells in that row, weighted by soft field selection p(t\u22121)f . Formally, for row i,\nc (t\u22121) select [i] = \u2211 j p (t\u22121) fj cij (1)\nThe row selection r(t\u22121) serves as a gate, indicating how much information cselect flows to subsequent processing. The i-th row\u2019s information is thus\nr (t\u22121) i = r (t\u22121) i \u00b7 c (t\u22121) select [i] (2)\nThen we summarize the global information of execution (denoted as g(t\u22121)) by max-pooling c(t\u22121)select over all rows:\ng(t\u22121) = MaxPooli { c (t\u22121) select [i] } (3)\nIn the current step, field selection p(t)f is based on the query q, the previous global information g(t\u22121), and the field name embeddings f .\np (t) fj\n= softmax ( MLP ( [q;fj ; g (t\u22121)] ))\n(4)\nwhere [\u00b7; \u00b7; \u00b7 \u00b7 \u00b7 ] denotes vector concatenation; MLP refers to a multi-layer perceptron.\n2Here, we do not strictly follow the original neural enquirer, where the row vector representation is computed directly a feed-forward neural network. In the current version, a neural layer predicts a gate for each row; then the row vector is computed by Equation 2. But this is not the main point of this paper.\nOperator Explanation select row Choose a row whose value of a particular column is mentioned in the query argmin Choose the row from previously selected candidate rows with the minimum value in a particular column argmin Choose the row from previously selected candidate rows with the maximum value in a particular column greater than Choose rows whose value in a particular column is greater than a previously selected row less than Choose rows whose value in a particular column is less than a previously selected row select value Choose the value of a particular column and of the previously selected row EOE Terminate, indicating the end of execution\nTable 2: Primitive operators for symbolic execution.\nNow we select one or a few rows based on the query q, previous global execution information g(t\u22121), previous row information r(t\u22121)i , and current cell selection c (t) i . In particular, we maintain a scalar for each row ri \u2208 (0, 1) as\nr (t) i = sigmoid\n( MLP ( [q, g(t\u22121), r(t\u22121), c (t) i ] )) (5)\nIntuitively, if ri is close to one, the row is selected; if ri is close to zero, the row is turned off. Because some executions (e.g., greater than) may select multiple rows, we use the sigmoid function instead of softmax for row selection.\nAs said, a softmax layer over all cells is applied at the end of execution to select the answer. Please refer to Yin et al. [2016b] for details of the distributed neural enquirer."}, {"heading": "2.2 Symbolic Executor", "text": "The methodology of designing a symbolic executor is to define a set of primitive operators (Subsection 2.2.1) and then to use a machine learning model to predict the operator and its arguments (Subsection 2.2.2)."}, {"heading": "2.2.1 Primitive Operators", "text": "We design six operators for symbolic execution, which are complete as they cover all types of queries in our scenario. Similar to the distributed neural enquirer, the result of one-step symbolic execution is a 0-1 boolean scalar for each row; it takes previous execution results as input, with a column/field as the argument. Table 2 summarizes our symbolic operator set.\nIn Table 1, for example, the first step of execution is argmax over the column Area, with previous (initial) row selection being all ones. This step yields a single row \u30082008,Beijing, \u00b7 \u00b7 \u00b7 , 350, \u00b7 \u00b7 \u00b7 , 25\u3009. The second execution operator is select value, which an argument column=Duration, yielding the result 25. Then the executor terminates (EOE).\nStacked with multiple steps of primitive operators, the executor can answer faily complicated questions like \u201cHow long is the last game which has smaller country size than the game whose host country GDP is 250?\u201d The execution sequence is\n1. select row: select the row where the column is GDP and the value is mentioned in the query. 2. less than: select rows whose country size is less than that of the previously selected row. 3. argmax: select the row whose year is the largest among previously selected rows. 4. select value: choose the value of the previously selected row with the column being Duration.\nThen the execution terminates. In our scenario, the execution is limited to four steps (EOE excluded) as such queries are touching the logic depth of common human intuition."}, {"heading": "2.2.2 Operator and Argument Predictors", "text": "We also leverage neural models, in particular recurrent neural networks (RNNs), to predict the operator and its argument (a selected field/column).\nLet h(t\u22121) be the previous state\u2019s hidden vectors. The current vector is\nh(t\u22121)op = sigmoid(Wrech (t\u22121) op ) (6)\nwhere Wrec is weight parameters and a bias term is omitted in the equation for simplicity. The initial hidden state is the query embedding, i.e., h(0)op = q.\nThe predicted probability of an operator i is given by\np(t)opi = softmax { w>i h (t\u22121) op } (7)\nThe operator with the largest predicted probability is selected for execution. Our RNN here does not have input, because the execution sequence is not dependent on the result of the previous execution step. Such architecture is known as a Jordan-type RNN [Jordan, 1997; Mesnil et al., 2013].\nLikewise, we use another Jordan-type RNN to predict the field selection. The only difference lies in the weight of the output softmax, i.e., wi in Equation 8 is substituted with the embedding of a field/column name f , given by\np (t) fj = softmax { f>j h (t\u22121) field } (8)\nTraining a symbolic executor without step-by-step supervision signals is non-trivial. A typical training method is reinforcement learning in a trial-and-error fashion. However, for a random initial policy, the probability of recovering an accurate execution sequence is extremely low. For a 10 \u00d7 10 table, for example, the probability is 1/(64 \u00b7 104) \u2248 7.7 \u00d7 10\u22128; the probability of obtaining an accurate denotation is 1%, which is also very low. Therefore, symbolic executors are not efficient in learning."}, {"heading": "2.3 A Unified View", "text": "We now have two worlds of execution: \u2022 The distributed enquirer is end-to-end learnable, but it\nis of low execution efficiency because of intensive matrix/vector multiplication during neural information processing. The neural execution also lacks explicit interpretation. \u2022 The symbolic enquirer has high execution efficiency and explicit interpretation. However, it cannot be trained in an end-to-end manner, and suffers from the cold start problem of reinforcement learning.\nWe propose to combine the two worlds by using the distributed enquirer\u2019s intermediate execution results to pretrain the\nDenotation Execution Query type SEMPRE Distributed Our Method Distributed Our Method SelectWhere 93.8 96.2 100.0 \u2013 100.0 Superlative 97.8 98.9 98.8 \u2013 98.7 WhereSuperlative 34.8 80.4 82.1 \u2013 69.1 NestQuery 34.4 60.5 84.4 \u2013 67.5 Overall 65.2 84.0 91.3 \u2013 83.8\nTable 3: Accuracies (in percentage) of Sempre tookit (reported in Yin et al. [2016b]), the distributed neural enquirer, and our proposed coupled approach.\nsymbolic enquirer for an initial policy. We then use the REINFORCE algorithm to improve the policy for symbolic execution. As a result, we manage to make use of the advantages in both worlds: high learning efficiency, high execution efficiency, and high interpretability; we also obtain better performance in terms of accuracy.\nConcretely, we observe that the field attention in Equation 4 generally aligns with column selection in Equation 7. We therefore pretrain the column selector in the symbolic enquirer with labels predicted by a fully neuralized enquirer. Such pretraining can obtain up to 70% accurate field selection and largely reduce the search space during reinforcement learning.\nAfter obtaining a meaningful, albeit imperfect, initial policy, we apply REINFORCE [Sutton and Barto, 1998] to improve the policy. Formally, the operator predictor and argument predictor in each execution step are the actions (denoted as a) in reinforcement learning terminologies.\nWe define a binary reward R indicating whether the final result of symbolic execution matches with the groundtruth denotation. The loss function of a policy is the expected reward where actions are sampled from the current predicted probabilities\nJ = Ea1,a2,\u00b7\u00b7\u00b7 ,an\u223c\u03b8[R(a1, a2, \u00b7 \u00b7 \u00b7 , an)] (9)\nThe partial derivative for a particular sampled action is\n\u2202J \u2202oi = R\u0303 \u00b7 (pi \u2212 1ai) (10)\nwhere pi is the predicted probability of all possible actions at the time step i, 1ai is a onehot representation of a sampled action ai, and oi is the input (also known as logit) of softmax. R\u0303 is the adjusted reward, which will be described shortly.\nTo help the training of REINFORCE, we have two tricks:\n\u2022 We balance exploration and exploitation with a small probability . In other words, we sample an action from the predicted action distribution with probability 1 \u2212 and from a uniform distribution over all possible actions with probability . The small fraction of uniform sampling helps the model to escape from poor local optima, as it continues to explore the entire action space during training. \u2022 We adjust the reward by subtracting the mean reward, av-\neraged over action samples for a certain data point. This is a common practice for REINFORCE [Ranzato et al., 2016]. We also truncate negative rewards as zero to prevent gradient from being messed up by incorrect executions."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Dataset", "text": "We evaluated our approach on a QA dataset in Yin et al. [2016b].3 The dataset comprises 25k different tables and queries. The queries can be divided into four types: SelectWhere, Superlative, WhereSuperlative, and NestQuery, each requiring 2\u20134 execution steps (EOE excluded).\nWe have both groundtruth denotation and execution actions (including operators and fields), as the dataset is synthesized by complicated rules and templates for research purposes. However, only denotations are used as labels during training, which is a realistic setting. Execution sequences are used only during testing to better understand our approach; this is also a strong motivation of using synthesis datasets for research. For the sake of simplicity, we presume the number of execution steps is known a priori during training (but not during testing). Although we have such (little) knowledge of execution, it is not a limitation of our approach and out of scope of the focus of this paper. One can easily design a dummy operator to fill an unnecessary step or one can also train a discriminative sentence model to predict the number of execution steps if a small quantity of labels are available."}, {"heading": "3.2 Settings", "text": "All settings of distributed neural enquirers were derived from Yin et al. [2016b] so that we can have a fair comparison. The dimensions of all layers were in the range of 20\u201350 and the learning algorithm was AdaDelta with default hyperparameters.\nFor the pretraining of the symbolic enquirer, we applied maximum likelihood estimation for 40 epochs to column selection with labels predicted by the distributed enquirers. We then used the REINFORCE algorithm to improve the policy, where we generated 10 action samples for each data point with the exploration probability being 0.1."}, {"heading": "3.3 Results", "text": "Table 3 presents the experimental results of our coupled distributed and symbolic enquirers as well as the SUMPRE baseline.\nAs we see, both the fully distributed and our proposed approach outperform the SEMPRE system.4 The coupled distributed and symbolic enquirer also significant outperforms the fully distributed one in terms of almost all query types. Regarding execution accuracy, we find that our approach can recover 83.4% correct execution sequences even without intermediate\n3The dataset will be made available soon. 4http://nlp.stanford.edu/software/sempre/\nstep-by-step training signals. By contrast, a fully distributed neural enquirer does not have explicit interpretations.\nFigure 3 further plots the validation learning curves. We see that, if the symbolic enquirer is trained by REINFORCE alone, it takes 200 epochs to get started to escape from the poor initial plateau. Even if it achieves \u223c50% execution accuracy (for simple query types) and \u223c75% denotation accuracy, it is stuck in the poor local optima.\nRed lines plot the learning curves of the symbolic executor pretrained with intermediate field attention of the distributed enquirer. Because the operator predictors in the symbolic executor are still hanging after pretraining, the denotation accuracy is near 0 before reinforcement learning. However, after only a few epochs of REINFORCE training, the performance increases sharply until it reaches a plateau of \u223c50% execution accuracy and \u223c75% denotation accuracy. The model eventually escapes from the plateau at about 200-th epoch and achieves high accuracy gradually. The results show that our coupled distributed and symbolic execution has much higher learning efficiency than a purely symbolic executor."}, {"heading": "4 Conclusion", "text": "In this paper, we proposed a coupled view of distributed and symbolic execution for natural language queries. By pretraining with intermediate execution results of a distributed enquirer, we managed to accelerate the training of the symbolic model to a large extent. Our proposed approach takes advantage of both distributed and symbolic worlds, and achieves high accuracy, high execution efficiency, high learning efficiency, as well as high interpretability.\nIn future work, we would like to design interpretable operator selection in the distributed model to better couple the two worlds and to further ease the training with REINFORCE. We would also like to feed back the symbolic enquirer\u2019s execution results to better train the distributed one. Such step-by-step supervision may also be iterated in a co-training fashion."}, {"heading": "Acknowledgments", "text": "We thank Pengcheng Yin, Jiatao Gu, and Hao Zhou for helpful discussion."}], "references": [{"title": "Language to logical form with neural attention", "author": ["Li Dong", "Mirella Lapata"], "venue": "In ACL,", "citeRegEx": "Dong and Lapata.,? \\Q2016\\E", "shortCiteRegEx": "Dong and Lapata.", "year": 2016}, {"title": "Serial order: A parallel distributed processing approach", "author": ["Michael I Jordan"], "venue": "Advances in psychology,", "citeRegEx": "Jordan.,? \\Q1997\\E", "shortCiteRegEx": "Jordan.", "year": 1997}, {"title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision", "author": ["Chen Liang", "Jonathan Berant", "Quoc Le", "Kenneth D Forbus", "Ni Lao"], "venue": "arXiv preprint arXiv:1611.00020,", "citeRegEx": "Liang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2016}, {"title": "Simpler context-dependent logical forms via model projections", "author": ["Reginald Long", "Panupong Pasupat", "Percy Liang"], "venue": "In ACL,", "citeRegEx": "Long et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Long et al\\.", "year": 2016}, {"title": "Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding", "author": ["Gr\u00e9goire Mesnil", "Xiaodong He", "Li Deng", "Yoshua Bengio"], "venue": "In INTERSPEECH,", "citeRegEx": "Mesnil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V Le", "Ilya Sutskever"], "venue": "In ICLR,", "citeRegEx": "Neelakantan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "Inferring logical forms from denotations", "author": ["Panupong Pasupat", "Percy Liang"], "venue": "In ACL,", "citeRegEx": "Pasupat and Liang.,? \\Q2016\\E", "shortCiteRegEx": "Pasupat and Liang.", "year": 2016}, {"title": "Sequence level training with recurrent neural networks", "author": ["MarcAurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "In ICLR,", "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "Reinforcement Learning: An Introduction, volume 1", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "A network-based end-to-end trainable taskoriented dialogue system", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M RojasBarahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young"], "venue": "arXiv preprint arXiv:1604.04562,", "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Sequence-based structured prediction for semantic parsing", "author": ["Chunyang Xiao", "Marc Dymetman", "Claire Gardent"], "venue": "In ACL,", "citeRegEx": "Xiao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2016}, {"title": "Neural generative question answering", "author": ["Jun Yin", "Xin Jiang", "Zhengdong Lu", "Lifeng Shang", "Hang Li", "Xiaoming Li"], "venue": "In IJCAI,", "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "Neural enquirer: Learning to query tables with natural language", "author": ["Pengcheng Yin", "Zhengdong Lu", "Hang Li", "Ben Kao"], "venue": "In IJCAI,", "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": ", 2016a], human-computer conversation [Wen et al., 2016], etc.", "startOffset": 38, "endOffset": 56}, {"referenceID": 3, "context": "A typical approach is to convert a natural language sentence to an \u201cexecutable\u201d logic forms for table/knowledge base querying, known as semantic parsing [Long et al., 2016; Pasupat and Liang, 2016].", "startOffset": 153, "endOffset": 197}, {"referenceID": 7, "context": "A typical approach is to convert a natural language sentence to an \u201cexecutable\u201d logic forms for table/knowledge base querying, known as semantic parsing [Long et al., 2016; Pasupat and Liang, 2016].", "startOffset": 153, "endOffset": 197}, {"referenceID": 0, "context": "With the fast development of modern neural networks, Dong and Lapata [2016] and Xiao et al.", "startOffset": 53, "endOffset": 76}, {"referenceID": 0, "context": "With the fast development of modern neural networks, Dong and Lapata [2016] and Xiao et al. [2016] apply sequenceto-sequence (seq2seq) neural models to generate a logic form conditioned on an input sentence.", "startOffset": 53, "endOffset": 99}, {"referenceID": 6, "context": "An emerging research topic in neural semantic parsing is to query a knowledge base directly by neural networks [Yin et al., 2016b; Neelakantan et al., 2016; Liang et al., 2016].", "startOffset": 111, "endOffset": 176}, {"referenceID": 2, "context": "An emerging research topic in neural semantic parsing is to query a knowledge base directly by neural networks [Yin et al., 2016b; Neelakantan et al., 2016; Liang et al., 2016].", "startOffset": 111, "endOffset": 176}, {"referenceID": 2, "context": ", 2016; Liang et al., 2016]. Because queries can be composited in a highly complicated manner, neural enquirers necessitate multiple steps of execution. The difficulty then lies in the lack of step-by-step supervision. In other words, we only assume groundtruth denotations are available in realistic settings, and that we do not know the execution sequence or intermediate execution results. There have seen several studies addressing the problem. Yin et al. [2016b] propose a fully distributed neural enquirer, comprising several neuralized execution layers of field attention, row annotation, etc.", "startOffset": 8, "endOffset": 468}, {"referenceID": 2, "context": ", 2016; Liang et al., 2016]. Because queries can be composited in a highly complicated manner, neural enquirers necessitate multiple steps of execution. The difficulty then lies in the lack of step-by-step supervision. In other words, we only assume groundtruth denotations are available in realistic settings, and that we do not know the execution sequence or intermediate execution results. There have seen several studies addressing the problem. Yin et al. [2016b] propose a fully distributed neural enquirer, comprising several neuralized execution layers of field attention, row annotation, etc. While the model is not efficient in execution because of intensive matrix/vector operation during neural information processing and lacks explicit interpretation of execution, it can be trained in an end-to-end fashion because all components in the neural enquirer are differentiable. Neelakantan et al. [2016], on the other hand, propose a neural programmer by defining a set of symbolic operations (e.", "startOffset": 8, "endOffset": 912}, {"referenceID": 2, "context": ", 2016; Liang et al., 2016]. Because queries can be composited in a highly complicated manner, neural enquirers necessitate multiple steps of execution. The difficulty then lies in the lack of step-by-step supervision. In other words, we only assume groundtruth denotations are available in realistic settings, and that we do not know the execution sequence or intermediate execution results. There have seen several studies addressing the problem. Yin et al. [2016b] propose a fully distributed neural enquirer, comprising several neuralized execution layers of field attention, row annotation, etc. While the model is not efficient in execution because of intensive matrix/vector operation during neural information processing and lacks explicit interpretation of execution, it can be trained in an end-to-end fashion because all components in the neural enquirer are differentiable. Neelakantan et al. [2016], on the other hand, propose a neural programmer by defining a set of symbolic operations (e.g., argmax, greater than); at each step, all possible execution results are fused by a softmax layer, which predicts the probability of each operator at the current step. The step-bystep fusion is accomplished by weighted sum and the model is trained with mean square error. Hence, such approaches work with numeric tables, but may not be suited for other operations like string matching; it also suffers from the problem of \u201cexponential numbers of combinatorial states.\u201d Liang et al. [2016] train a symbolic executor by REINFORCE policy gradient, but it is known that the REINFORCE algorithm is sensitive to the", "startOffset": 8, "endOffset": 1496}, {"referenceID": 12, "context": "For example, the field attention gadget in Yin et al. [2016b] generally aligns with column selection.", "startOffset": 43, "endOffset": 62}, {"referenceID": 12, "context": "For example, the field attention gadget in Yin et al. [2016b] generally aligns with column selection. We therefore use the distributed model\u2019s intermediate execution results as supervision signals to pretrain a symbolic executor. Guided by such imperfect step-by-step supervision, the symbolic executor learns a fairly meaningful initial policy, which largely alleviates the cold start problem of the REINFORCE algorithm. We evaluate the proposed approach on the QA dataset in Yin et al. [2016b]. In our experiment, the REINFORCE algorithm alone takes long to get started; even if it does, it is stuck in a poor local optimum.", "startOffset": 43, "endOffset": 496}, {"referenceID": 12, "context": "In this section, we first introduce the fully distributed (neuralized) enquirer proposed in Yin et al. [2016b]. Then we design a set of operators that are complete to the task at hand.", "startOffset": 92, "endOffset": 111}, {"referenceID": 5, "context": "One of the most notable studies of distributed semantics is word embeddings, which map discrete words to vectors as anonymous meaning representations [Mikolov et al., 2013].", "startOffset": 150, "endOffset": 172}, {"referenceID": 12, "context": "Please refer to Yin et al. [2016b] for details of the distributed neural enquirer.", "startOffset": 16, "endOffset": 35}, {"referenceID": 1, "context": "Such architecture is known as a Jordan-type RNN [Jordan, 1997; Mesnil et al., 2013].", "startOffset": 48, "endOffset": 83}, {"referenceID": 4, "context": "Such architecture is known as a Jordan-type RNN [Jordan, 1997; Mesnil et al., 2013].", "startOffset": 48, "endOffset": 83}, {"referenceID": 12, "context": "Table 3: Accuracies (in percentage) of Sempre tookit (reported in Yin et al. [2016b]), the distributed neural enquirer, and our proposed coupled approach.", "startOffset": 66, "endOffset": 85}, {"referenceID": 9, "context": "After obtaining a meaningful, albeit imperfect, initial policy, we apply REINFORCE [Sutton and Barto, 1998] to improve the policy.", "startOffset": 83, "endOffset": 107}, {"referenceID": 8, "context": "This is a common practice for REINFORCE [Ranzato et al., 2016].", "startOffset": 40, "endOffset": 62}, {"referenceID": 12, "context": "We evaluated our approach on a QA dataset in Yin et al. [2016b].3 The dataset comprises 25k different tables and queries.", "startOffset": 45, "endOffset": 64}, {"referenceID": 12, "context": "All settings of distributed neural enquirers were derived from Yin et al. [2016b] so that we can have a fair comparison.", "startOffset": 63, "endOffset": 82}], "year": 2016, "abstractText": "Building neural networks to query a knowledge base (a table) with natural language is an emerging research topic in NLP. The neural enquirer typically necessitates multiple steps of execution because of the compositionality of queries. In previous studies, researchers have developed either distributed enquirers or symbolic ones for table querying. The distributed enquirer is end-to-end learnable, but is weak in terms of execution efficiency and explicit interpretability. The symbolic enqurier, on the contrary, is efficient during execution; but it is very difficult to train especially at initial stages. In this paper, we propose to couple distributed and symbolic execution for natural language queries. The observation is that a fully distributed executor also exhibits meaningful, albeit imperfect, interpretation. We can thus pretrain the symbolic executor with the distributed one\u2019s intermediate execution results in a step-by-step fashion. Experiments show that our approach significantly outperforms either the distributed or symbolic executor; moreover, we have recovered more than 80% execution sequences with only groundtruth denotations during training. In summary, the coupled neural enquirer takes advantages of both distributed and symbolic executors, and has high performance, high learning efficiency, high execution efficiency, and high interpretability.", "creator": "LaTeX with hyperref package"}}}