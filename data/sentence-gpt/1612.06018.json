{"id": "1612.06018", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2016", "title": "Self-Correcting Models for Model-Based Reinforcement Learning", "abstract": "When an agent cannot represent a perfectly accurate model of its environment's dynamics, model-based reinforcement learning (MBRL) can fail catastrophically. Planning involves composing the predictions of the model; when flawed predictions are composed, even minor errors can compound and render the model useless for planning. Hallucinated Replay (Talvitie 2014) trains the model to \"correct\" itself when it produces errors, substantially improving MBRL with flawed models.\n\n\n\nIn the future, the best way to improve MBRL has been to \"fix\" it. There are lots of ways of doing this\u2014such as to improve MBRL performance by improving learning rates, improving the quality of training and improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by improving performance by", "histories": [["v1", "Mon, 19 Dec 2016 01:09:23 GMT  (235kb,D)", "https://arxiv.org/abs/1612.06018v1", "To appear in Proceedings of the 31st AAAI Conference on Artificial Intelligence, 2017. This version incorporates the appendix into document (rather than as supplementary material)"], ["v2", "Wed, 26 Jul 2017 18:53:51 GMT  (236kb,D)", "http://arxiv.org/abs/1612.06018v2", "Original paper appeared in Proceedings of the 31st AAAI Conference on Artificial Intelligence, 2017. This version incorporates the appendix into document (rather than as supplementary material), corrects a minor error in Lemma 1, and fixes some type-os"]], "COMMENTS": "To appear in Proceedings of the 31st AAAI Conference on Artificial Intelligence, 2017. This version incorporates the appendix into document (rather than as supplementary material)", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["erik talvitie"], "accepted": true, "id": "1612.06018"}, "pdf": {"name": "1612.06018.pdf", "metadata": {"source": "CRF", "title": "Self-Correcting Models for Model-Based Reinforcement Learning", "authors": ["Erik Talvitie"], "emails": ["erik.talvitie@fandm.edu"], "sections": [{"heading": "1 Introduction", "text": "In model-based reinforcement learning (MBRL) the agent learns a predictive model of its environment and uses it to make decisions. The overall MBRL approach is intuitively appealing and there are many anticipated benefits to learning a model, most notably sample efficiency (Szita and Szepesva\u0301ri 2010). Despite this, with few exceptions (e.g. Abbeel et al. 2006), model-free methods have been far more successful in large-scale problems. Even as modellearning methods demonstrate increasing prediction accuracy in high-dimensional domains (e.g. Bellemare et al. 2014, Oh et al. 2015) this rarely corresponds to improvements in control performance.\nOne key reason for this disparity is that model-free methods are generally robust to representational limitations that prevent convergence to optimal behavior. In contrast, when the model representation is insufficient to perfectly capture the environment\u2019s dynamics (even in seemingly innocuous ways), or when the planner produces suboptimal plans, MBRL methods can fail catastrophically. If the benefits of MBRL are to be gained in large-scale problems, it is vital to understand how MBRL can be effective even when the model and planner are fundamentally flawed.\nRecently there has been growing awareness that the standard measure of model quality, one-step prediction accu-\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nracy, is an inadequate proxy for MBRL performance. For instance, Sorg et al. (2010) and Joseph et al. (2013) both pointed out that the most accurate model by this measure is not necessarily the best for planning. Both proposed optimizing model parameters for control performance using policy gradient methods. Though appealing in its directness, this approach arguably discards some of the benefits of learning a model in the first place.\nTalvitie (2014) pointed out that one-step prediction accuracy does not account for how the model behaves when composed with itself and introduced the Hallucinated Replay meta-algorithm to address this. As illustrated in Figure 1, this approach rolls out the model and environment in parallel, training the model to predict the correct environment state (s4) even when its input is an incorrect sampled state (z3). This effectively causes the model to \u201cself-correct\u201d its rollouts. Hallucinated Replay was shown to enable meaningful planning with flawed models in examples where the standard approach failed. However, it offers no theoretical guarantees. Venkatraman et al. (2015) and Oh et al. (2015) used similar approaches to improve models\u2019 long-range predictions, though not in the MBRL setting.\nThis paper presents novel error bounds that reveal the theoretical principles that underlie the empirical success of Hallucinated Replay. It presents negative results that identify settings where hallucinated training would be ineffec-\nar X\niv :1\n61 2.\n06 01\n8v 2\n[ cs\n.L G\n] 2\n6 Ju\nl 2 01\n7\ntive (Section 2.3) and identifies a case where it yields a tighter performance bound than standard training (Section 2.4). This result allows the derivation of a novel MBRL algorithm with theoretical performance guarantees that are robust to model class limitations (Section 3). The analysis also highlights a previously underexplored practical concern with this approach, which is examined empirically (Section 4.1)."}, {"heading": "1.1 Notation and background", "text": "We focus on Markov decision processes (MDP). The environment\u2019s initial state s1 is drawn from a distribution \u00b5. At each step t the environment is in a state st. The agent selects an action at which causes the environment to transition to a new state sampled from the transition distribution: st+1 \u223c P atst . The environment also emits a reward, R(st, at). For simplicity, assume that the reward function is known and is bounded within [0,M ].\nA policy \u03c0 specifies a way to behave in the MDP. Let \u03c0(a | s) = \u03c0s(a) be the probability that \u03c0 chooses action a in state s. For a sequence of actions a1:t let P (s\u2032 | s, a1:t) = P a1:ts (s\n\u2032) be the probability of reaching s\u2032 by starting in s and taking the actions in the sequence. For any state s, action a, and policy \u03c0, let Dts,a,\u03c0 be the state-action distribution obtained after t steps, starting with state s and action a and thereafter following policy \u03c0. For a state action distribution \u03be, let Dt\u03be,\u03c0 = E(s,a)\u223c\u03beD t s,a,\u03c0 . For a state distribution \u00b5 let Dt\u00b5,\u03c0 = Es\u223c\u00b5,a\u223c\u03c0s D t s,a,\u03c0 . For some discount factor \u03b3 \u2208\n[0, 1), let D\u00b5,\u03c0 = (1 \u2212 \u03b3) \u2211\u221e t=1 \u03b3\nt\u22121Dt\u00b5,\u03c0 be the infinitehorizon discounted state-action distribution under policy \u03c0.\nThe T -step state-action value of a policy, Q\u03c0T (s, a) represents the expected discounted sum of rewards obtained by taking action a in state s and executing \u03c0 for an additional T \u2212 1 steps: Q\u03c0T (s, a) =\u2211T t=1 \u03b3 t\u22121 E(s\u2032,a\u2032)\u223cDts,a,\u03c0 R(s \u2032, a\u2032). Let the T -step state value V \u03c0T (s) = Ea\u223c\u03c0s [Q \u03c0 T (s, a)]. For infinite horizons we write Q\u03c0 = Q\u03c0\u221e, and V \u03c0 = V \u03c0\u221e. The agent\u2019s goal will be to learn a policy \u03c0 that maximizes Es\u223c\u00b5[V \u03c0(s)]. The MBRL approach is to learn a model P\u0302 , approximating P , and then to use the model to produce a policy via a planning algorithm. We let D\u0302, Q\u0302, and V\u0302 represent the corresponding quantities using the learned model. Let C represent the model class, the set of models the learning algorithm could possibly produce. Critically, in this paper, it is not assumed that C contains a perfectly accurate model."}, {"heading": "2 Bounding value error", "text": "We consider an MBRL architecture that uses the simple oneply Monte Carlo planning algorithm (one-ply MC), which has its roots in the \u201crollout algorithm\u201d (Tesauro and Galperin 1996). For every state-action pair (s, a), the planner executes N T -step \u201crollouts\u201d in P\u0302 , starting at s, taking action a, and then following a rollout policy \u03c1. Let Q\u0304(s, a) be the average discounted return of the rollouts. For largeN , Q\u0304will closely approximate Q\u0302\u03c1T (Kakade 2003). The agent will select its actions greedily with respect to Q\u0304. Talvitie (2015) bounds the performance of one-ply MC in terms of model quality.\nFor a policy \u03c0 and state-action distribution \u03be, let \u03be,\u03c0,Tval = E(s,a)\u223c\u03be [ |Q\u03c0T (s, a) \u2212 Q\u0302\u03c0T (s, a)| ] be the error in the T -step state-action values the model assigns to the policy under the given distribution. Then the following result can be straightforwardly adapted from one provided by Talvitie (2015). Lemma 1. Let Q\u0304 be the state-action value function returned by applying one-ply Monte Carlo to the model P\u0302 with rollout policy \u03c1 and rollout depth T . Let \u03c0\u0302 be greedy w.r.t. Q\u0304. For any policy \u03c0 and state-distribution \u00b5,\nE s\u223c\u00b5\n[ V \u03c0(s)\u2212 V \u03c0\u0302(s) ] \u2264 4\n1\u2212 \u03b3 \u03be,\u03c1,Tval + mc,\nwhere we let \u03be(s, a) = 12D\u00b5,\u03c0\u0302(s, a) + 1 4D\u00b5,\u03c0(s, a) + 1 4 ( (1\u2212 \u03b3)\u00b5(s)\u03c0\u0302s(a) + \u03b3 \u2211 z,bD\u00b5,\u03c0(z, b)P b z (s)\u03c0\u0302s(a) ) and mc = 41\u2212\u03b3 \u2016Q\u0304 \u2212 Q\u0302 \u03c1 T \u2016\u221e + 2 1\u2212\u03b3 \u2016BV \u03c1 T \u2212 V \u03c1 T \u2016\u221e (here"}, {"heading": "B is the Bellman operator).", "text": "The mc term represents error due to limitations of the planning algorithm: error due to the sample average Q\u0304 and the sub-optimality of the T -step value function with respect to \u03c1. The \u03be,\u03c1,Tval term represents error due to the model parameters. The key factor in the model\u2019s usefulness for planning is the accuracy of the value it assigns to the rollout policy in state-actions visited by \u03c0 and \u03c0\u0302. Our goal in the next sections is to bound \u03be,\u03c1,Tval in terms of measures of model accuracy, ultimately deriving insight into how to train models that will be effective for MBRL. Proofs may be found in the appendix."}, {"heading": "2.1 One-step prediction error", "text": "Intuitively, the value of a policy should be accurate if the model is accurate in states that the policy would visit. We can adapt a bound from Ross and Bagnell (2012). Lemma 2. For any policy \u03c0 and state-action distribution \u03be,\n\u03be,\u03c0,Tval \u2264 M\n1\u2212 \u03b3 T\u22121\u2211 t=1 (\u03b3t \u2212 \u03b3T ) E (s,a)\u223cDt\u03be,\u03c0 [ \u2016P as \u2212 P\u0302 as \u20161 ] .\nCombining Lemmas 1 and 2 yields an overall bound on control performance in terms of the model\u2019s prediction error. This result matches common MBRL practice; it recommends minimizing the model\u2019s one-step prediction error. It acknowledges that the model may be imperfect by allowing it to have one-step error in unimportant (i.e. unvisited) states. However, if limitations of the model class prevent the model from achieving low error in important states, this bound can be quite loose, as the following example illustrates.\nConsider the \u201cShooter\u201d domain introduced by Talvitie (2015), pictured in Figure 2a. The agent moves a spaceship left and right at the bottom of the screen. It can fire bullets upward, but each one has a cost (-1 reward). If a bullet hits one of the three targets, the agent receives 10 reward. Each target has a \u201cbullseye\u201d (the white dots). If a bullet hits the same column as a bullseye, the agent receives an additional 10 reward. Though the control problem is simple, the state/observation space is high-dimensional due to the many possible configurations of objects on the screen.\nIn the original Shooter the bullseyes remained still but here they move back and forth across the targets. As such, the problem is second-order Markov; when the bullseye is in the center, one cannot predict its next position without knowing its previous position. The agent, however, will use a factored Markov model, predicting each pixel conditioned on the current image. It cannot accurately predict the bullseyes\u2019 movement, though it can predict everything else perfectly.\nOne might imagine that this limitation would be fairly minor; the agent can still obtain reward even if it cannot reliably hit the bullseyes. However, consider the sample rollout pictured in Figure 2b. Here each image is sampled from a model\u2019s one-step predictions, and is then given as input for the next predictions. This model has the lowest possible onestep prediction error. Still, as anticipated, it does not correctly predict the movement of the bullseyes in the second image. Because of the resulting errors, the sampled image is unlike any the environment would generate, and therefore unlike any the model has trained on. The model\u2019s uninformed predictions based on this unfamiliar image cause more errors in the third image, and so on. Ultimately this model assigns low probability to a target persisting more than a few steps, making it essentially useless for planning.\nNote, however, that there are models within this model class that are useful for planning. Consider the sample rollout pictured in Figure 2c. The model that generated this rollout makes the same one-step errors as the previous model when given an environment state. However, when it encounters an unreasonable sampled state it still makes reasonable predictions, effectively \u201cself-correcting.\u201d Talvitie (2014) presents several similar examples involving various model deficiencies. These examples illustrate the inadequacy of Lemma 2 when the model class is limited. Models with similar one-step prediction error can vary wildly in their usefulness for planning. The true distinguisher is the accuracy of predictions far into the future."}, {"heading": "2.2 Multi-step prediction error", "text": "Since Q\u03c0T (s, a) = \u2211T t=1 \u03b3 t\u22121 E(s\u2032,a\u2032)\u223cDts,a,\u03c0 R(s \u2032, a\u2032), it is straightforward to bound \u03be,\u03c0,Tval in terms of multi-step error.\nLemma 3. For any policy \u03c0 and state-action distribution \u03be,\n\u03be,\u03c0,Tval \u2264M T\u2211 t=1 \u03b3t\u22121 E (s,a)\u223c\u03be [ \u2016Dts,a,\u03c0 \u2212 D\u0302ts,a,\u03c0\u20161 ] .\nThe bound in Lemma 2 has dependence on 11\u2212\u03b3 because it effectively assumes the worst possible loss in value if the model samples an \u201cincorrect\u201d state. In contrast, Lemma 3 accounts for the model\u2019s ability to recover after an error, only penalizing it for individual incorrect transitions. Unfortunately, it is difficult to directly optimize for multi-step prediction accuracy. Nevertheless, this bound suggests that algorithms that account for a model\u2019s multi-step error will yield more robust MBRL performance."}, {"heading": "2.3 Hallucinated one-step prediction error", "text": "We now seek to formally analyze the practice of hallucinated training, described in Section 1. Venkatraman et al. (2015) provide some analysis but in the uncontrolled time series prediction setting. Here we focus on its impact on control performance in MBRL. As a first step, we derive a bound based on a model\u2019s ability to predict the next environment state, given a state sampled from the model\u2019s own predictions, i.e. to self-correct. For a policy \u03c0 and state-action distribution \u03be let J t\u03be,\u03c0 represent the joint distribution over environment and model state-action pairs if \u03c0 is executed in both simultaneously. Specifically, let\nJ t\u03be,\u03c0(s, a, z, b) = E(s\u2032,a\u2032)\u223c\u03be[D t s\u2032,a\u2032,\u03c0(s, a)D\u0302 t s\u2032,a\u2032,\u03c0(z, b)].\nLemma 4. For any policy \u03c0 and state-action distribution \u03be,\n\u03be,\u03c0,Tval \u2264M T\u22121\u2211 t=1 \u03b3t E (s,a,z,b)\u223cJt\u03be,\u03c0 [ \u2016P as \u2212 P\u0302 bz \u20161 ] .\nInspired by \u201cHallucinated Replay\u201d (Talvitie 2014), we call the quantity on the right the hallucinated one-step error. Hallucinated one-step error is intended as a proxy for multi-step error, but having formalized it we may now see that in some cases it is a poor proxy. Note that, regardless of the policy, the multi-step and one-step error of a perfect model is 0. This is not always so for hallucinated error. Proposition 5. The hallucinated one-step error of a perfect model may be non-zero.\nProof. Consider a simple MDP with three states {s0, sh, st} and a single action a. In the initial state s0, a fair coin is flipped, transitioning to sh or st with equal probability, where it stays forever. Consider a perfect model P\u0302 = P . Then J1s0,a(sh, a, st, a) = P a s0(sh)P a s0(st) = 0.25. However, |P ash(sh) \u2212 P a st(sh)| = 1 \u2212 0 = 1. Thus, the hallucinated one-step error of a perfect model is non-zero.\nHere the environment samples heads and the model samples tails. Given its own state, the model rightly predicts tails, but incurs error nevertheless since the environment\u2019s next state is heads. Because the model and environment dynamics are uncoupled, one cannot distinguish between model error and legitimately different stochastic outcomes.\nAs such, the hallucinated error is misleading when the true dynamics are stochastic. This corroborates the conjecture that Hallucinated Replay may be problematic in stochastic environments (Talvitie 2014). Note that this observation applies not just to hallucinated training, but to any method that attempts to improve multi-step predictions by comparing sample rollouts from the model and the environment.\nWhile it may seem limiting to restrict our attention to deterministic environments, this is still a large, rich class of problems. For instance, Oh et al. (2015) learned models of Atari 2600 games, which are fully deterministic (Hausknecht et al. 2014); human players often perceive them as stochastic due to their complexity. Similarly, in synthetic RL domains stochasticity is often added to simulate complex, deterministic phenomena (e.g. robot wheels slipping on debris), not necessarily to capture inherently stochastic effects in the world. As in these examples, we shall assume that the environment is deterministic but complex, so a limited agent will learn an imperfect, stochastic model.\nThat said, even specialized to deterministic environments, the bound in Lemma 4 is loose for arbitrary policies.\nProposition 6. The hallucinated one-step error of a perfect model may be non-zero, even in a deterministic MDP.\nProof. Alter the coin MDP, giving the agent two actions which fully determine the coin\u2019s orientation. The original dynamics can be recovered via a stochastic policy that randomly selects sh or st and then leaves the coin alone.\nOh et al. (2015) tied action selection to the environment state only (rather than independently selecting actions in the environment and model). This prevents stochastic decoupling but may fail to train the model on state-action pairs that the policy would reach under the model\u2019s dynamics."}, {"heading": "2.4 A Tighter Bound", "text": "In the remainder of the paper we assume that the environment is deterministic. Let \u03c3a1:ts be the unique state that results from starting in state s and taking the action sequence a1:t. The agent\u2019s model will still be stochastic.\nRecall that our goal is to bound the value error under the one-ply MC rollout policy. Proposition 6 shows that hallucinated error gives a loose bound under arbitrary policies. We now focus on blind policies (Bowling et al. 2006). A blind policy depends only on the action history, i.e. \u03c0(at | st, a1:t\u22121) = \u03c0(at | a1:t\u22121). This class of policies ranges from stateless policies to open-loop action sequences. It includes the uniform random policy, a common rollout policy.\nFor any blind policy \u03c0 and state-action distribution \u03be, let Ht\u03be,\u03c0 be the distribution over environment state, model state, and action if a single action sequence is sampled from \u03c0 and then executed in both the model and the environment. So,\nH1\u03be,\u03c0(s1, z1, a1) = \u03be(s1, a1) when z1 = s1 (0 otherwise);\nH2\u03be,\u03c0(s2, z2, a2) = E(s1,a1)\u223c\u03be[\u03c0(a2 | a1)P a1 s1 (s2)P\u0302 a1 s1 (zs)];\nand for t > 2, Ht\u03be,\u03c0(st, zt, at) = E(s1,a1)\u223c\u03be [\u2211 a2:t\u22121 \u03c0(a2:t | a1)P a1:t\u22121s1 (st)P\u0302 a1:t\u22121 s1 (zt) ] .\nLemma 7. If P is deterministic, then for any blind policy \u03c0 and any state-action distribution \u03be,\n\u03be,\u03c0,Tval \u2264 2M T\u22121\u2211 t=1 \u03b3t E (s,z,a)\u223cHt\u03be,\u03c0 [ 1\u2212 P\u0302 az (\u03c3as ) ] .\nWe can also show that, in the deterministic setting, Lemma 7 gives an upper bound for multi-step error (Lemma 3) and a lower bound for one-step error (Lemma 2). Theorem 8. If P is deterministic, then for any blind policy \u03c0 and any state-action distribution \u03be,\n\u03be,\u03c0,Tval \u2264 M T\u2211 t=1 \u03b3t\u22121 E (s,a)\u223c\u03be [ \u2016Dts,a,\u03c0 \u2212 D\u0302ts,a,\u03c0\u20161 ] \u2264 2M\nT\u22121\u2211 t=1 \u03b3t E (s,z,a)\u223cHt\u03be,\u03c0 [ 1\u2212 P\u0302 az (\u03c3as ) ] \u2264 2M\n1\u2212 \u03b3 T\u22121\u2211 t=1 (\u03b3t \u2212 \u03b3T ) E (s,a)\u223cDt\u03be,\u03c0 [ 1\u2212 P\u0302 as (\u03c3as ) ] .\nThus, with a deterministic environment and a blind rollout policy, the hallucinated one-step error of the model is more tightly related to MBRL performance than the standard onestep error. This is the theoretical reason for the empirical success of Hallucinated Replay (Talvitie 2014), which trains the model to predict the next environment state, given its own samples as input. We now exploit this fact to develop a novel MBRL algorithm that similarly uses hallucinated training to mitigate the impact of model class limitations and that offers strong theoretical guarantees."}, {"heading": "3 Hallucinated DAgger-MC", "text": "The \u201cData Aggregator\u201d (DAgger) algorithm (Ross and Bagnell 2012) was the first practically implementable MBRL algorithm with performance guarantees agnostic to the model class. It did, however, require that the planner be near optimal. DAgger-MC (Talvitie 2015) relaxed this assumption, accounting for the limitations of the planner that uses the model (one-ply MC). This section augments DAgger-MC to use hallucinated training, resulting in the Hallucinated DAgger-MC algorithm1, or H-DAgger-MC (Algorithm 1).\nIn addition to assuming a particular form for the planner (one-ply MC with a blind rollout policy), H-DAgger-MC assumes that the model will be \u201cunrolled\u201d (similar to, e.g. Abbeel et al. 2006). Rather than learning a single model P\u0302 , H-DAgger-MC learns a set of models {P\u0302 1, . . . , P\u0302T\u22121} \u2286 C, where model P\u0302 i is responsible for predicting the outcome of step i of a rollout, given the state sampled from P\u0302 i\u22121 as input. The importance of learning an unrolled model will be discussed more deeply in Section 4.1. 1\u201cIs this a dagger which I see before me, The handle toward my hand? Come, let me clutch thee. I have thee not, and yet I see thee still. Art thou not, fatal vision, sensible To feeling as to sight? Or art thou but A dagger of the mind, a false creation, Proceeding from the heat-oppress\u2019d brain?\u201d [Macbeth 2.1.33\u201339]\nAlgorithm 1 Hallucinated DAgger-MC Require: Exploration distribution \u03bd, ONLINE-LEARNER,\nMC-PLANNER (blind rollout policy \u03c1, rollout depth T ), num. iterations N , num. rollouts per iteration K\n1: Get initial datasets D1:T\u221211 (maybe using \u03bd) 2: Initialize P\u0302 1:T\u221211 \u2190 ONLINE-LEARNER(D 1:T\u22121 1 ). 3: Initialize \u03c0\u03021 \u2190 MC-PLANNER(P\u0302 1:T\u221211 ). 4: for n\u2190 2 . . . N do 5: for k \u2190 1 . . .K do 6: With probability... . Sample (x, b) \u223c \u03ben... 7: 1/2: Sample (x, b) \u223c D\u03c0\u0302n\u22121\u00b5 8: 1/4: Reset to (x, b) \u223c \u03bd. 9: (1\u2212\u03b3)/4: Sample x \u223c \u00b5, b \u223c \u03c0\u0302n\u22121(\u00b7 | x). 10: \u03b3/4: 11: Reset to (y, c) \u223c \u03bd 12: Sample x \u223c P (\u00b7 | y, c), b \u223c \u03c0\u0302n\u22121(\u00b7 | x) 13: Let s\u2190 x, z \u2190 x, a\u2190 b. 14: for t\u2190 1 . . . T \u2212 1 do . Sample from Htn... 15: Sample s\u2032 \u223c P (\u00b7 | s, a). 16: Add \u3008z, a, s\u2032\u3009 to Dtn. . Hallucinated training . (DAgger-MC adds \u3008s, a, s\u2032\u3009 instead). 17: Sample z\u2032 \u223c P\u0302 tn\u22121(\u00b7 | z, a). 18: Let s\u2190 s\u2032, z \u2190 z\u2032, and sample a \u223c \u03c1. 19: P\u0302 1:T\u22121n \u2190 ONLINE-LEARNER(P\u0302 1:T\u22121n\u22121 , D1:T\u22121n ) 20: \u03c0\u0302n \u2190 MC-PLANNER(P\u0302 1:T\u22121n ). 21: return the sequence \u03c0\u03021:N\nMuch of the H-DAgger-MC algorithm is identical to DAgger-MC. The main difference lies in lines 14-18, in which \u03c1 is executed in both the environment and the model to generate hallucinated examples. This trains the model to self-correct during rollouts. Like DAgger and DAgger-MC, H-DAgger-MC requires the ability to reset to the initial state distribution \u00b5 and also the ability to reset to an \u201cexploration distribution\u201d \u03bd. The exploration distribution ideally ensures that the agent will encounter states that would be visited by a good policy, otherwise no agent could promise good performance. The performance bound for H-DAgger-MC will depend in part on the quality of the selected \u03bd.\nWe now analyze H-DAgger-MC, adapting Ross and Bagnell (2012)\u2019s DAgger analysis. Let Htn be the distribution from which H-DAgger-MC samples a training triple at depth t (lines 6-13 to pick an initial state-action pair, lines 14-18 to roll out). Define the error of the model at depth t to be \u0304tprd = 1 N \u2211N n=1 E(s,z,a)\u223cHtn [1\u2212 P\u0302 t n(\u03c3 a s | z, a)].\nFor a policy \u03c0, let c\u03c0\u03bd = sups,a D\u00b5,\u03c0(s,a) \u03bd(s,a) represent the mismatch between the discounted state-action distribution under \u03c0 and the exploration distribution \u03bd. Now, consider the sequence of policies \u03c0\u03021:N generated by H-DAgger-MC. Let \u03c0\u0304 be the uniform mixture over all policies in the sequence. Let \u0304mc = 1N 4 1\u2212\u03b3 \u2211N n=1(\u2016Q\u0304n \u2212 Q\u0302 \u03c1 T,n\u2016\u221e + 2 1\u2212\u03b3 \u2016BV \u03c1 T \u2212 V \u03c1T \u2016\u221e be the error induced by the choice of planning algorithm, averaged over all iterations.\nLemma 9. In H-DAgger-MC, the policies \u03c0\u03021:N are such\nthat for any policy \u03c0,\nE s\u223c\u00b5\n[ V \u03c0(s)\u2212 V \u03c0\u0304(s) ] \u2264 8M\n1\u2212 \u03b3 c\u03c0\u03bd T\u22121\u2211 t=1 \u0304tprd + \u0304mc.\nNote that this result holds for any comparison policy \u03c0. Thus, if \u0304mc is small and the learned models have low hallucinated one-step prediction error, then if \u03bd is similar to the state-action distribution under some good policy, \u03c0\u0304 will compare favorably to it. Like the original DAgger and DAgger-MC results, Lemma 9 has limitations. It uses the L1 loss, which is not always a practical learning objective. It also assumes that the expected loss at each iteration can be computed exactly (i.e. that there are infinitely many samples per iteration). It also applies to the average policy \u03c0\u0304, rather than the last policy in the sequence. Ross and Bagnell (2012) discuss extensions that address more practical loss functions, finite sample bounds, and results for \u03c0\u0302N .\nThe next question is, of course, when will the learned models be accurate? Following Ross and Bagnell (2012) note that \u0304tprd can be interpreted as the average loss of an online learner on the problem defined by the aggregated datasets at each iteration. In that case, for each horizon depth t let \u0304tmdl be the error of the best model in C under the training distribution at that depth, in retrospect. Specifically, \u0304tmdl = infP \u2032\u2208C 1 N \u2211N n=1 E(s,z,a)\u223cHtn [1 \u2212 P\n\u2032(\u03c3as | z, a)]. Then the average regret for the model at depth t is \u0304trgt = \u0304 t prd \u2212 \u0304tmdl. For a no-regret online learning algorithm, \u0304trgt \u2192 0 as N \u2192\u221e. This gives the following bound on H-DAgger-MC\u2019s performance in terms of model regret. Theorem 10. In H-DAgger-MC, the policies \u03c0\u03021:N are such that for any policy \u03c0,\nE s\u223c\u00b5\n[ V \u03c0(s)\u2212 V \u03c0\u0304(s) ] \u2264 8M\n1\u2212 \u03b3 c\u03c0\u03bd T\u22121\u2211 t=1 (\u0304tmdl + \u0304 t rgt) + \u0304mc,\nand if the model learning algorithm is no-regret then \u0304trgt \u2192 0 as N \u2192\u221e for each 1 \u2264 t \u2264 T \u2212 1.\nTheorem 10 says that if C contains a low-error model for each rollout depth then low error models will be learned. Then, as discussed above, if \u0304mc is small and \u03bd visits important states, the resulting policy will yield good performance. Notably, even with hallucinated training, if C contains a perfect model, H-DAgger-MC will learn a perfect model.\nIt is important to note that this result does not promise that H-DAgger-MC will eventually achieve the performance of the best performing set of models in the class. The model at each rollout depth is trained to minimize prediction error given the input distribution provided by the shallower models. Note, however, that changing the parameters of a model at one depth alters the training distribution for deeper models. It is possible that better overall error could be achieved by increasing the prediction error at one depth in exchange for a favorable state distribution for deeper models. This effect is not taken into account by H-DAgger-MC."}, {"heading": "4 Empirical Illustration", "text": "In this section we illustrate the practical impact of optimizing hallucinated error by comparing DAgger, DAgger-MC,\nand H-DAgger-MC in the Shooter example described in Section 2.12. The experimental setup matches that of Talvitie (2015) for comparison\u2019s sake, though the qualitative comparison presented here is robust to the parameter settings.\nIn all cases one-ply MC was used with 50 uniformly random rollouts of depth 15 at every step. The exploration distribution was generated by following the optimal policy with (1\u2212\u03b3) probability of termination at each step. The model for each pixel was learned using Context Tree Switching (Veness et al. 2012), similar to the FAC-CTW algorithm (Veness et al. 2011), and used a 7\u00d7 7 neighborhood around the pixel in the previous timestep as input. Data was shared across all positions. The discount factor was \u03b3 = 0.9. In each iteration 500 training rollouts were generated and the resulting policy was evaluated in an episode of length 30. The discounted return obtained by the policy in each iteration is reported, averaged over 50 trials.\nThe results can be seen in Figure 3a and 3b. The shaded regions represent 95% confidence intervals for the mean performance. The benchmark lines labeled \u201cRandom\u201d and \u201cPerfect Model\u201d represent the average performance of the uniform random policy and one-ply Monte Carlo using a perfect model, respectively. In Figure 3a the bullseyes move, simulating the typical practical reality that C does not contain a perfect model. In Figure 3b the bullseyes have fixed positions, so C does contain a perfect model.\nAs observed by Talvitie (2015), DAgger performs poorly in both versions, due to the suboptimal planner. DAgger-MC is able to perform well with fixed bullseyes (Figure 3b), but with moving bullseyes the model suffers from compounding errors and is not useful for planning (Figure 3a). This holds for a single model and for an \u201cunrolled\u201d model.\nIn these experiments one practically-minded alteration was made to the H-DAgger-MC algorithm. In early training the model is highly inaccurate, and thus deep rollouts produce incoherent samples. Training with these samples is counter-productive (also, the large number of distinct, nonsensical contexts renders CTS impractical). For these experiments, training rollouts in iteration i were truncated at depth bi/10c. Planning rollouts in these early iterations use the models that have been trained so far and then repeatedly apply the deepest model in order to complete the rollout. Talvitie (2014), Venkatraman et al. (2015), and Oh et al. (2015) all similarly discarded noisy examples early in training. This transient modification does not impact H-DAggerMC\u2019s asymptotic guarantees.\nIn Figure 3a it is clear that H-DAgger-MC obtains a good policy despite the limitations of the model class. Hallucinated training has made MBRL possible with both a flawed model and a flawed planner while the standard approach has failed entirely. In the case that C contains a perfect model (rare in problems of genuine interest) H-DAgger-MC is outperformed by DAgger-MC. Despite the adjustment to training, deep models still receive noisy inputs. Theoretically the model should become perfectly accurate in the limit, though in practice it may do so very slowly.\n2Source code for these experiments may be found at github. com/etalvitie/hdaggermc."}, {"heading": "4.1 Impact of the unrolled model", "text": "Recall that the H-DAgger-MC algorithm assumes the model will be \u201cunrolled,\u201d with a separate model responsible for sampling each step in a rollout. This has clear practical disadvantages, but it is important theoretically. When one model is used across all time-steps, convergence to a perfect model cannot be guaranteed, even if one exists in C.\nIn Figure 3c, H-DAgger-MC has been trained using a single model in Shooter with fixed bullseyes. The temporary truncation schedule described above is employed, but the training rollouts have been permanently limited to various depths. First consider the learning curve marked \u201cDepth 15\u201d, where training rollouts are permitted to reach the maximum depth. While the rollouts are temporarily truncated the model does well, but performance degrades as longer rollouts are permitted even though C contains a perfect model!\nRecall from Section 3 that changing the model parameters impacts both prediction error and the future training distribution. Furthermore, training examples generated by deep rollouts may contain highly flawed samples as inputs. Sometimes attempting to \u201ccorrect\u201d a large error (i.e. reduce prediction error) causes additional, even worse errors in the next iteration (i.e. harms the training distribution). For instance consider a hallucinated training example with the 4th screen from Figure 2b as input and the 5th screen from Figure 2a as the target. The model would effectively learn that targets can appear out of nowhere, an error that would be even harder to correct in future iterations. With a single model across timesteps, a feedback loop can emerge: the model parameters change to attempt to correct large errors, thereby causing larger errors, and so on. This feedback loop causes the observed performance crash. With an unrolled model the parameters of each sub-model cannot impact that sub-model\u2019s own training distribution, ensuring stability.\nNote that none of Talvitie (2014), Venkatraman et al. (2015), or Oh et al. (2015) used an unrolled model. As such, all of their approaches are subject to this concern. Notably, all three limited the depth of training rollouts, presumably to prevent overly noisy samples. Figure 3c shows that in this experiment, the shorter the training rollouts, the better the performance. These results show that it may be possible in practice to avoid unrolling the model by truncating training rollouts, though for now there is no performance guarantee or principled choice of rollout depth."}, {"heading": "5 Conclusions and future work", "text": "The primary contribution of this work is a deeper theoretical understanding of how to perform effective MBRL in the face of model class limitations. Specifically we have examined a novel measure of model quality that, under some assumptions, is more tightly related to MBRL performance than standard one-step prediction error. Using this insight, we have also analyzed a MBRL algorithm that achieves good control performance despite flaws in the model and planner and provides strong theoretical performance guarantees.\nWe have also seen negative results indicating that hallucinated one-step error may not be an effective optimization criterion in the most general setting. This poses the\nopen challenge of relaxing the assumptions of deterministic dynamics and blind policies, or of developing alternative approaches for improving multi-step error in more general settings. We have further observed that hallucinated training can cause stability issues, since model parameters affect both prediction error and the training distribution itself. It would be valuable to develop techniques that account for both of these effects when adapting model parameters.\nSpecializing to the one-ply MC planning algorithm may seem restrictive, but then again, the choice of planning algorithm cannot make up for a poor model. When the model class is limited, H-DAgger-MC is likely still a good choice over DAgger, even with a more sophisticated planner. Still, it would be valuable to investigate whether these principles can be applied to more sophisticated planning algorithms.\nThough this work has assumed that the reward function is known, the results presented here can be straightforwardly extended to account for reward error. However, this also raises the interesting point that sampling an \u201cincorrect\u201d state has little negative impact if the sampled state\u2019s rewards and transitions are similar to the \u201ccorrect\u201d state. It may be possible to exploit this to obtain still tighter bounds, and more effective guidance for model learning in MBRL architectures."}, {"heading": "Acknowledgements", "text": "This work was supported in part by NSF grant IIS-1552533. Many thanks to Marc Bellemare whose feedback has positively influenced the work, both in substance and presentation. Thanks to Drew Bagnell and Arun Venkatraman for their valuable insights. Thanks also to Joel Veness for his freely available FAC-CTW and CTS implementations (http://jveness.info/software/)."}, {"heading": "Appendix: Proofs", "text": "Lemma 2. For any policy \u03c0 and state-action distribution \u03be,\n\u03be,\u03c0,Tval \u2264 M\n1\u2212 \u03b3 T\u22121\u2211 t=1 (\u03b3t \u2212 \u03b3T ) E (s,a)\u223cDt\u03be,\u03c0 [ \u2016P as \u2212 P\u0302 as \u20161 ] .\nProof. First note that\n\u03be,\u03c0,Tval = E (s1,a1)\u223c\u03be\n[ |Q\u0302\u03c0T (s1, a1)\u2212Q\u03c0T (s1, a1)| ] = \u03b3 E\n(s1,a1)\u223c\u03be [ | E s\u223cP\u0302a1s1 [V\u0302 \u03c0T\u22121(s)]\u2212 E s\u223cPa1s1 [V \u03c0T\u22121(s)]| ]\n= \u03b3 E (s1,a1)\u223c\u03be [\u2223\u2223 E s\u223cP\u0302a1s1 [V\u0302 \u03c0T\u22121(s)]\u2212 E s\u223cPa1s1 [V\u0302 \u03c0T\u22121(s)]\n+ E s\u223cPa1s1 [V\u0302 \u03c0T\u22121(s)]\u2212 E s\u223cPa1s1\n[V \u03c0T\u22121(s)] \u2223\u2223]\n\u2264 \u03b3 E (s1,a1)\u223c\u03be [\u2223\u2223 E s\u223cP\u0302a1s1 a\u223c\u03c0s [Q\u0302\u03c0T\u22121(s, a)]\u2212 E s\u223cPa1s1 a\u223c\u03c0s [Q\u0302\u03c0T\u22121(s, a)] \u2223\u2223]\n+ \u03b3 E (s,a)\u223cD2\u03be,\u03c0\n[ |Q\u0302\u03c0T\u22121(s, a)\u2212Q\u03c0T\u22121(s, a)])| ] .\nRolling out the recurrence gives\n\u03be,\u03c0,Tval = E (s1,a1)\u223c\u03be\n[ |Q\u0302\u03c0T (s1, a1)\u2212Q\u03c0T (s1, a1)| ] \u2264 T\u22121\u2211 t=1 \u03b3t E (s,a)\u223cDt\u03be,\u03c0 [\u2223\u2223 E s\u2032\u223cP\u0302as a\u2032\u223c\u03c0s\u2032 [Q\u0302\u03c0T\u2212t(s \u2032, a\u2032)]\n\u2212 E s\u2032\u223cPas a\u2032\u223c\u03c0s\u2032 [Q\u0302\u03c0T\u2212t(s \u2032, a\u2032)]\n\u2223\u2223]\n\u2264 T\u22121\u2211 t=1 \u03b3t \u2211 s\u2032,a\u2032 \u03c0s\u2032(a \u2032)Q\u0302\u03c0T\u2212t(s \u2032, a\u2032)\nE (s,a)\u223cDt\u03be,\u03c0\n[ |P\u0302 as (s\u2032)\u2212 P as (s\u2032)| ] \u2264 T\u22121\u2211 t=1 \u03b3t M(1\u2212 \u03b3T\u2212t) 1\u2212 \u03b3 E (s,a)\u223cDt\u03be,\u03c0 [ \u2016P\u0302 as \u2212 P as \u20161\n] = M\n1\u2212 \u03b3 T\u22121\u2211 t=1 (\u03b3t \u2212 \u03b3T ) E (s,a)\u223cDt\u03be,\u03c0 [ \u2016P\u0302 as \u2212 P as \u20161 ] .\nLemma 3. For any policy \u03c0 and state-action distribution \u03be,\n\u03be,\u03c0,Tval \u2264M T\u2211 t=1 \u03b3t\u22121 E (s,a)\u223c\u03be [ \u2016Dts,a,\u03c0 \u2212 D\u0302ts,a,\u03c0\u20161 ] .\nProof. This follows straightforwardly from the definition.\n\u03be,\u03c0,Tval = E (s,a)\u223c\u03be\n[ |Q\u03c0T (s, a)\u2212 Q\u0302\u03c0T (s, a)| ] = E\n(s,a)\u223c\u03be [\u2223\u2223\u2223\u2223 T\u2211 t=1 \u03b3t\u22121 \u2211 (s\u2032,a\u2032) (Dts,a,\u03c0(s \u2032, a\u2032)\n\u2212 D\u0302ts,a,\u03c0(s\u2032, a\u2032))R(s\u2032, a\u2032) \u2223\u2223\u2223\u2223 ]\n\u2264 T\u2211 t=1 \u03b3t\u22121 E (s,a)\u223c\u03be [ \u2211 (s\u2032,a\u2032) \u2223\u2223(Dts,a,\u03c0(s\u2032, a\u2032) \u2212 D\u0302ts,a,\u03c0(s\u2032, a\u2032))R(s\u2032, a\u2032)\n\u2223\u2223] \u2264M\nT\u2211 t=1 \u03b3t\u22121 E (s,a)\u223c\u03be [ \u2211 (s\u2032,a\u2032) \u2223\u2223Dts,a,\u03c0(s\u2032, a\u2032) \u2212 D\u0302ts,a,\u03c0(s\u2032, a\u2032)\n\u2223\u2223] = M\nT\u2211 t=1 \u03b3t\u22121 E (s,a)\u223c\u03be [ \u2016Dts,a,\u03c0 \u2212 D\u0302ts,a,\u03c0\u20161 ] .\nLemma 4. For any policy \u03c0 and state-action distribution \u03be,\n\u03be,\u03c0,Tval \u2264M T\u22121\u2211 t=1 \u03b3t E (s,a,z,b)\u223cJt\u03be,\u03c0 [ \u2016P as \u2212 P\u0302 bz \u20161 ] .\nProof. First note that for any s1, a1, s\u2032, a\u2032 and for t > 1,\nDts1,a1,\u03c0(s \u2032, a\u2032)\u2212 D\u0302ts1,a1,\u03c0(s \u2032, a\u2032) = \u2211 s,a Dt\u22121s1,a1,\u03c0(s, a)P a s (s \u2032)\u03c0s\u2032(a \u2032)\n\u2212 \u2211 z,b D\u0302t\u22121s1,a1,\u03c0(z, b)P\u0302 b z (s \u2032)\u03c0s\u2032(a \u2032)\n= \u03c0s\u2032(a \u2032) (\u2211 s,a Dt\u22121s1,a1,\u03c0(s, a)P a s (s \u2032) \u2211 z,b D\u0302t\u22121s1,a1,\u03c0(z, b)\n\u2212 \u2211 z,b D\u0302t\u22121s1,a1,\u03c0(z, b)P\u0302 b z (s \u2032) \u2211 s,a Dt\u22121s1,a1,\u03c0(s, a)\n)\n= \u03c0s\u2032(a \u2032) ( \u2211 s,a,z,b Dt\u22121s1,a1,\u03c0(s, a)D\u0302 t\u22121 s1,a1,\u03c0(z, b)\n(P as (s \u2032)\u2212 P\u0302 bz (s\u2032))\n) .\nFurther note that for any s, a, \u03c0, D1s,a,\u03c0 = D\u0302 1 s,a,\u03c0 . Com-\nbining these facts with Lemma 3 we see that\n\u03be,\u03c0,Tval = E (s,a)\u223c\u03be\n[ |Q\u03c0T (s, a)\u2212 Q\u0302\u03c0T (s, a)| ] \u2264M\nT\u2211 t=1 \u03b3t\u22121 E (s,a)\u223c\u03be [ \u2016Dts,a,\u03c0 \u2212 D\u0302ts,a,\u03c0\u20161 ] = M\nT\u2211 t=2 \u03b3t\u22121 E (s1,a1)\u223c\u03be [\u2211 s\u2032,a\u2032 \u2223\u2223\u2223Dts1,a1,\u03c0(s\u2032, a\u2032) \u2212 D\u0302ts1,a1,\u03c0(s \u2032, a\u2032) \u2223\u2223\u2223]\n= M T\u22121\u2211 t=1 \u03b3t E (s1,a1)\u223c\u03be [\u2211 s\u2032,a\u2032 \u2223\u2223\u2223Dt+1s1,a1,\u03c0(s\u2032, a\u2032) \u2212 D\u0302t+1s1,a1,\u03c0(s \u2032, a\u2032) \u2223\u2223\u2223]\n\u2264M T\u22121\u2211 t=1 \u03b3t E (s1,a1)\u223c\u03be [ \u2211 s,a,z,b Dts1,a1,\u03c0(s, a)D\u0302 t s1,a1,\u03c0(z, b)\n\u2211 s\u2032 \u2223\u2223\u2223P as (s\u2032)\u2212 P\u0302 bz (s\u2032)\u2223\u2223\u2223]\n= M T\u22121\u2211 t=1 \u03b3t \u2211 s,a,z,b \u2016P as \u2212 P\u0302 bz \u20161\nE (s1,a1)\u223c\u03be\n[ Dts1,a1,\u03c0(s, a)D\u0302 t s1,a1,\u03c0(z, b) ] = M\nT\u22121\u2211 t=1 \u03b3t E (s,a,z,b)\u223cJt\u03be,\u03c0 [ \u2016P as \u2212 P\u0302 bz \u20161 ] .\nLemma 7. If P is deterministic, then for any blind policy \u03c0 and any state-action distribution \u03be,\n\u03be,\u03c0,Tval \u2264 2M T\u22121\u2211 t=1 \u03b3t E (s,z,a)\u223cHt\u03be,\u03c0 [ 1\u2212 P\u0302 az (\u03c3as ) ] .\nProof. As in the proof of Lemma 4 note that for any s1, a1, \u03c0,D1s1,a1,\u03c0 = D\u0302 1 s1,a1,\u03c0 . Further note that under these assumptions, for any s1, a1, \u2016D2s1,a1,\u03c0 \u2212 D\u0302 2 s1,a1,\u03c0 \u2225\u2225 1\n= \u2211 s,a \u03c0(a) \u2223\u2223P a1s1 (s)\u2212 P\u0302 a1s1 (s)\u2223\u2223 = \u2211 s \u2223\u2223P a1s1 (s)\u2212 P\u0302 a1s1 (s)\u2223\u2223. For t > 2, \u2016Dts1,a1,\u03c0 \u2212 D\u0302 t s1,a1,\u03c0\u20161\n= \u2211 st \u2223\u2223\u2223\u2223\u2211 a2:t \u03c0(a2:t | a1) (\u2211 s P a1:t\u22122s1 (s)P at\u22121 s (st)\n\u2212 \u2211 z P\u0302 a1:t\u22122s1 (z)P\u0302 at\u22121 z (st) )\u2223\u2223\u2223\u2223 = \u2211 st \u2223\u2223\u2223\u2223 \u2211 a2:t\u22121\n\u03c0(a2:t\u22121 | a1)(\u2211 s P a1:t\u22122s1 (s)P at\u22121 s (st) \u2211 z P\u0302 a1:t\u22122s1 (z)\n\u2212 \u2211 z P\u0302 a1:t\u22122s1 (z)P\u0302 at\u22121 z (st) \u2211 s P a1:t\u22122s1 (s) )\u2223\u2223\u2223\u2223 = \u2211 st \u2223\u2223\u2223\u2223 \u2211 a2:t\u22121 \u03c0(a2:t\u22121 | a1) \u2211 s,z\nP a1:t\u22122s1 (s)P\u0302 a1:t\u22122 s1 (z)(\nP at\u22121s (st)\u2212 P\u0302 at\u22121z (st) )\u2223\u2223\u2223\u2223\n= \u2211 st \u2223\u2223\u2223\u2223 \u2211 a2:t\u22121 \u03c0(a2:t\u22121 | a1) \u2211 s,z\nP a1:t\u22122s1 (s)P\u0302 a1:t\u22122 s1 (z)(\nP at\u22121s (st)\u2212 P\u0302 at\u22121z (st) )\u2223\u2223\u2223\u2223\n\u2264 \u2211 a2:t\u22121 \u03c0(a2:t\u22121) \u2211 s,z\nP a1:t\u22122s1 (s)P\u0302 a1:t\u22122 s1 (z)\u2211\nst\n\u2223\u2223\u2223P at\u22121s (st)\u2212 P\u0302 at\u22121z (st)\u2223\u2223\u2223.\nNow, for any s, z, a, \u2211 st \u2223\u2223\u2223P as (st)\u2212 P\u0302 az (st)\u2223\u2223\u2223 = ( 1\u2212 P\u0302 az (\u03c3as )\n) + \u2211 st 6=\u03c3as \u2223\u2223\u2212 P\u0302 az (st)\u2223\u2223 = 2 ( 1\u2212 P\u0302 az (\u03c3as ) ) .\nCombining these facts with Lemma 3,\n\u03be,\u03c0,Tval = E (s,a)\u223c\u03be\n[ |Q\u03c0T (s, a)\u2212 Q\u0302\u03c0T (s, a)| ] \u2264M\nT\u2211 t=1 \u03b3t\u22121 E (s,a)\u223c\u03be [ \u2016Dts,a,\u03c0 \u2212 D\u0302ts,a,\u03c0\u20161 ] = M\nT\u2211 t=2 \u03b3t\u22121 E (s1,a1)\u223c\u03be [ \u2016Dts1,a1,\u03c0 \u2212 D\u0302 t s1,a1,\u03c0\u20161 ] = M\u03b3 E\n(s1,a1)\u223c\u03be\n[ \u2016D2s1,a1,\u03c0 \u2212 D\u0302 2 s1,a1,\u03c0\u20161 ] +M\nT\u2211 t=3 \u03b3t\u22121 E (s1,a1)\u223c\u03be [ \u2016Dts1,a1,\u03c0 \u2212 D\u0302 t s1,a1,\u03c0\u20161 ] = M\u03b3 E\n(s1,a1)\u223c\u03be\n[ \u2016D2s1,a1,\u03c0 \u2212 D\u0302 2 s1,a1,\u03c0\u20161 ] +M\nT\u22121\u2211 t=2 \u03b3t E (s1,a1)\u223c\u03be [\u2211 s\u2032,a\u2032 \u2016Dt+1s1,a1,\u03c0 \u2212 D\u0302 t+1 s1,a1,\u03c0\u20161 ] \u2264 2M\u03b3 E\n(s1,a1)\u223c\u03be\n[ 1\u2212 P\u0302 a1s1 (\u03c3 a1 s1 ) ]\n+ 2M T\u22121\u2211 t=2 \u03b3t E (s1,a1)\u223c\u03be [\u2211 a2:t \u03c0(a2:t | a1)\n\u2211 s,z P a1:t\u22121s1 (s)P\u0302 a1:t\u22121 s1 (z) ( 1\u2212 P\u0302 atz (\u03c3ats ) )] = 2M\u03b3 E\n(s1,a1)\u223c\u03be\n[ 1\u2212 P\u0302 a1s1 (\u03c3 a1 s1 ) ]\n+ 2M\u03b32 \u2211\ns2,z2,a2\n( 1\u2212 P\u0302 a2z2 (\u03c3 a2 s2 ) )\nE (s1,a1)\u223c\u03be\n[ \u03c0(a2 | a1)P a1s1 (s2)P\u0302 a1 s1 (z2) ] + 2M\nT\u22121\u2211 t=3 \u03b3t \u2211 st,zt,at ( 1\u2212 P\u0302 atzt (\u03c3 at st ) )\nE (s1,a1)\u223c\u03be [ \u2211 a2:t\u22121 \u03c0(a2:t | a1)P a1:t\u22121s1 (st)P\u0302 a1:t\u22121 s1 (zt) ] = 2M\u03b3 E\n(s,z,a)\u223cH1\u03be,\u03c0\n[ 1\u2212 P\u0302 az (\u03c3as ) ] + 2M\u03b32 E\n(s,z,a)\u223cH2\u03be,\u03c0\n[ 1\u2212 P\u0302 az (\u03c3as ) ] + 2M\nT\u22121\u2211 t=3 \u03b3t E (s,z,a)\u223cHt\u03be,\u03c0 [ 1\u2212 P\u0302 az (\u03c3as ) ]\n= 2M T\u22121\u2211 t=1 \u03b3t E (s,z,a)\u223cHt\u03be,\u03c0 [ 1\u2212 P\u0302 (\u03c3as | z, a) ] .\nTheorem 8. If P is deterministic, then for any blind policy \u03c0 and any state-action distribution \u03be,\n\u03be,\u03c0,Tval \u2264 M T\u2211 t=1 \u03b3t\u22121 E (s,a)\u223c\u03be [ \u2016Dts,a,\u03c0 \u2212 D\u0302ts,a,\u03c0\u20161 ] \u2264 2M\nT\u22121\u2211 t=1 \u03b3t E (s,z,a)\u223cHt\u03be,\u03c0 [ 1\u2212 P\u0302 az (\u03c3as ) ] \u2264 2M\n1\u2212 \u03b3 T\u22121\u2211 t=1 (\u03b3t \u2212 \u03b3T ) E (s,a)\u223cDt\u03be,\u03c0 [ 1\u2212 P\u0302 as (\u03c3as ) ] .\nProof. The first inequality was proven in Lemma 3. The proof of Lemma 7 also proves the second inequality. Thus, we shall focus on the third inequality. First note that\nT\u22121\u2211 t=1 \u03b3t E (s,z,a)\u223cHt\u03be,\u03c0 [ 1\u2212 P\u0302 az (\u03c3as ) ] = \u2211 s,a T\u22121\u2211 t=1 \u03b3tHt\u03be,\u03c0(s, s, a) [ 1\u2212 P\u0302 as (\u03c3as )\n] +\n\u2211 s,z 6=s,a T\u22121\u2211 t=1 \u03b3tHt\u03be,\u03c0(s, z, a) [ 1\u2212 P\u0302 az (\u03c3as ) ] .\nThis expression has two terms; the first captures prediction error when the model and environment are in the same state, the second when the environment and world state differ. Consider the first term.\u2211 s,a T\u22121\u2211 t=1 \u03b3tHt\u03be,\u03c0(s, s, a) [ 1\u2212 P\u0302 as (\u03c3as )\n] = \u2211 s,a ( 1\u2212 P\u0302 as (\u03c3as )\n) ( \u03b3\u03be(s, a) + \u03b32 E\n(s1,a1)\u223c\u03be \u03c0(a | a1)P a1s1 (s)P\u0302 a1 s1 (s) ) +\nT\u22121\u2211 t=3 \u03b3t \u2211 st,at ( 1\u2212 P\u0302 atst (\u03c3 at st ) )\nE (s1,a1)\u223c\u03be [ \u2211 a2:t\u22121 \u03c0(a2:t | a1)P a1:t\u22121s1 (st)P\u0302 a1:t\u22121 s1 (st) ] = \u2211 s,a ( 1\u2212 P\u0302 as (\u03c3as )\n) ( \u03b3\u03be(s, a) + \u03b32 E\n(s1,a1)\u223c\u03be \u03c0(a | a1)P a1s1 (s) ) +\nT\u22121\u2211 t=3 \u03b3t \u2211 st,at ( 1\u2212 P\u0302 atst (\u03c3 at st ) )\nE (s1,a1)\u223c\u03be [ \u2211 a2:t\u22121 \u03c0(a2:t | a1)P a1:t\u22121s1 (st) ]\n= T\u22121\u2211 t=1 \u03b3t E (s,a)\u223cDt\u03be,\u03c0 [ 1\u2212 P\u0302 as (\u03c3as ) ] .\nNow consider the second term. Note that for any a and s 6= z, H1\u03be,\u03c0(s, z, a) = 0. So,\u2211 s,z 6=s,a T\u22121\u2211 t=1 \u03b3tHt\u03be,\u03c0(s, z, a) [ 1\u2212 P\u0302 az (\u03c3as )\n] =\n\u2211 s,z 6=s,a T\u22121\u2211 t=2 \u03b3tHt\u03be,\u03c0(s, z, a) [ 1\u2212 P\u0302 az (\u03c3as ) ] =\n\u2211 s,z 6=s,a T\u22122\u2211 t=1 \u03b3t+1Ht+1\u03be,\u03c0 (s, z, a) [ 1\u2212 P\u0302 az (\u03c3as ) ] \u2264\n\u2211 s,z 6=s,a T\u22122\u2211 t=1 \u03b3t+1Ht+1\u03be,\u03c0 (s, z, a)\n= \u03b3 \u2211\ns\u2032,z\u2032,a\u2032 ( \u2211 s,z 6=s P a \u2032 s\u2032 (s)P\u0302 a\u2032 z\u2032 (z) ) T\u22122\u2211 t=1 \u03b3tHt\u03be,\u03c0(s \u2032, z\u2032, a\u2032)\n= \u03b3 \u2211\ns\u2032,z\u2032,a\u2032\n( 1\u2212 P\u0302 a \u2032 z\u2032 (\u03c3 a\u2032 s\u2032 ) ) T\u22122\u2211 t=1 \u03b3tHt\u03be,\u03c0(s \u2032, z\u2032, a\u2032)\n= \u03b3 T\u22122\u2211 t=1 \u03b3t E (s,z,a)\u223cHt\u03be,\u03c0 [ 1\u2212 P\u0302 az (\u03c3as ) ] .\nCombining the two reveals a recurrence relation. T\u22121\u2211 t=1 \u03b3t E (s,z,a)\u223cHt\u03be,\u03c0 [ 1\u2212 P\u0302 az (\u03c3as ) ] \u2264 T\u22121\u2211 t=1 \u03b3t E (s,a)\u223cDt\u03be,\u03c0 [ 1\u2212 P\u0302 as (\u03c3as )\n] + \u03b3\nT\u22122\u2211 t=1 \u03b3t E (s,z,a)\u223cHt\u03be,\u03c0 [ 1\u2212 P\u0302 az (\u03c3as ) ] .\nUnrolling the recurrence, we see that T\u22121\u2211 t=1 \u03b3t E (s,z,a)\u223cHt\u03be,\u03c0 [ 1\u2212 P\u0302 az (\u03c3as ) ] \u2264 T\u22121\u2211 j=1 \u03b3j\u22121 T\u2212j\u2211 t=1 \u03b3t E (s,a)\u223cDt\u03be,\u03c0 [ 1\u2212 P\u0302 as (\u03c3as ) ] .\nNow note that T\u22121\u2211 j=1 \u03b3j\u22121 T\u2212j\u2211 t=1 \u03b3t E (s,a)\u223cDt\u03be,\u03c0 [ 1\u2212 P\u0302 as (\u03c3as ) ] =\nT\u22121\u2211 t=1 T\u2212t\u2211 j=1 \u03b3j\u22121\u03b3t E (s,a)\u223cDt\u03be,\u03c0 [ 1\u2212 P\u0302 as (\u03c3as ) ] =\nT\u22121\u2211 t=1 \u03b3t E (s,a)\u223cDt\u03be,\u03c0 [ 1\u2212 P\u0302 as (\u03c3as ) ] T\u2212t\u2211 j=1 \u03b3j\u22121\n= T\u22121\u2211 t=1 \u03b3t E (s,a)\u223cDt\u03be,\u03c0 [ 1\u2212 P\u0302 as (\u03c3as ) ]1\u2212 \u03b3T\u2212t 1\u2212 \u03b3\n= 1\n1\u2212 \u03b3 T\u22121\u2211 t=1 (\u03b3t \u2212 \u03b3T ) E (s,a)\u223cDt\u03be,\u03c0 [ 1\u2212 P\u0302 as (\u03c3as )) ] .\nLemma 9. In H-DAgger-MC, the policies \u03c0\u03021:N are such that for any policy \u03c0,\nE s\u223c\u00b5\n[ V \u03c0(s)\u2212 V \u03c0\u0304(s) ] \u2264 8M\n1\u2212 \u03b3 c\u03c0\u03bd T\u22121\u2211 t=1 \u0304tprd + \u0304mc.\nProof. Recall that\nE s\u223c\u00b5\n[ V \u03c0(s)\u2212 V \u03c0\u0304(s) ] = 1\nN N\u2211 n=1 E s\u223c\u00b5 [ V \u03c0(s)\u2212 V \u03c0\u0302n(s) ] .\nand by Lemma 1 for any n \u2265 1,\nE s\u223c\u00b5\n[ V \u03c0(s)\u2212 V \u03c0\u0302n(s) ] \u2264\n4\n1\u2212 \u03b3 E (s,a)\u223c\u03be\u03c0,\u03c0\u0302n\u00b5 [|Q\u0302\u03c1T,n(s, a)\u2212Q \u03c1 T (s, a)|] + \u0304mc,\nwhere\n\u03be\u03c0,\u03c0\u0302n\u00b5 (s, a) = 1\n2 D\u00b5,\u03c0\u0302n(s, a) +\n1 4 D\u00b5,\u03c0(s, a)\n+ 1\n4\n( (1\u2212 \u03b3)\u00b5(s)\u03c0\u0302n(a | s)\n+ \u03b3 \u2211 z,b D\u00b5,\u03c0(z, b)P b z (s)\u03c0\u0302n(a | s) ) .\nCombining this with Lemma 7,\n1\nN N\u2211 n=1 4 1\u2212 \u03b3 E (s,a)\u223c\u03be\u03c0,\u03c0\u0302n\u00b5 [|Q\u0302\u03c1T,n(s, a)\u2212Q \u03c1 T (s, a)|] + \u0304mc\n\u2264 1 N N\u2211 n=1 8M 1\u2212 \u03b3 T\u22121\u2211 t=1 \u03b3t\nE (s,z,a)\u223cHt,n\n\u03be \u03c0,\u03c0\u0302n \u00b5 ,\u03c1\n[ 1\u2212 P\u0302 tn(\u03c3as | z, a) ] + \u0304mc\n\u2264 8M 1\u2212 \u03b3 1 N N\u2211 n=1 T\u22121\u2211 t=1\nE (s,z,a)\u223cHt,n\n\u03be \u03c0,\u03c0\u0302n \u00b5 ,\u03c1\n[ 1\u2212 P\u0302 tn(\u03c3as | z, a) ] + \u0304mc.\nNow note that for any t and any n,\nE (s,z,a)\u223cHt,n\n\u03be \u03c0,\u03c0\u0302n \u00b5 ,\u03c1\n[ 1\u2212 P\u0302 tn(\u03c3as | z, a) ] = 1\n2 \u2211 s\u2032,a\u2032 D\u00b5,\u03c0\u0302n(s \u2032, a\u2032) E (s,z,a)\u223cHt,n s\u2032,a\u2032,\u03c1 [ 1\u2212 P\u0302 tn(\u03c3as | z, a) ] + 1\n4 \u2211 s\u2032,a\u2032 D\u00b5,\u03c0(s \u2032, a\u2032) E (s,z,a)\u223cHt,n s\u2032,a\u2032,\u03c1 [ 1\u2212 P\u0302 tn(\u03c3as | z, a) ] + \u03b3\n4 \u2211 s\u2032,a\u2032 \u2211 s\u2032\u2032,a\u2032\u2032 D\u00b5,\u03c0(s \u2032\u2032, a\u2032\u2032)P a \u2032\u2032 s\u2032\u2032 (s \u2032)\u03c0\u0302n(a \u2032 | s\u2032)\nE (s,z,a)\u223cHt,n\ns\u2032,a\u2032,\u03c1\n[ 1\u2212 P\u0302 tn(\u03c3as | z, a) ] +\n1\u2212 \u03b3 4 \u2211 s\u2032,a\u2032 \u00b5(s\u2032)\u03c0\u0302n(a \u2032 | s\u2032)\nE (s,z,a)\u223cHt,n\ns\u2032,a\u2032,\u03c1\n[ 1\u2212 P\u0302 tn(\u03c3as | z, a) ] \u2264 1\n2 \u2211 s\u2032,a\u2032 D\u00b5,\u03c0\u0302n(s \u2032, a\u2032) E (s,z,a)\u223cHt,n s\u2032,a\u2032,\u03c1 [ 1\u2212 P\u0302 tn(\u03c3as | z, a) ] + 1\n4 c\u03c0\u03bd \u2211 s\u2032,a\u2032 \u03bd(s\u2032, a\u2032) E (s,z,a)\u223cHt,n s\u2032,a\u2032,\u03c1 [ 1\u2212 P\u0302 tn(\u03c3as | z, a) ] + \u03b3\n4 c\u03c0\u03bd \u2211 s\u2032,a\u2032 \u2211 s\u2032\u2032,a\u2032\u2032 \u03bd(s\u2032\u2032, a\u2032\u2032)P a \u2032\u2032 s\u2032\u2032 (s \u2032)\u03c0\u0302n(a \u2032 | s\u2032)\nE (s,z,a)\u223cHt,n\ns\u2032,a\u2032,\u03c1\n[ 1\u2212 P\u0302 tn(\u03c3as | z, a) ] +\n1\u2212 \u03b3 4 \u2211 s\u2032,a\u2032 \u00b5(s\u2032)\u03c0\u0302n(a \u2032 | s\u2032)\nE (s,z,a)\u223cHt,n\ns\u2032,a\u2032,\u03c1\n[ 1\u2212 P\u0302 tn(\u03c3as | z, a) ] \u2264 c\u03c0\u03bd ( 1\n2 \u2211 s\u2032,a\u2032 D\u00b5,\u03c0\u0302n(s \u2032, a\u2032)\nE (s,z,a)\u223cHt,n\ns\u2032,a\u2032,\u03c1\n[ 1\u2212 P\u0302 tn(\u03c3as | z, a) ] + 1\n4 \u2211 s\u2032,a\u2032 \u03bd(s\u2032, a\u2032) E (s,z,a)\u223cHt,n s\u2032,a\u2032,\u03c1 [ 1\u2212 P\u0302 tn(\u03c3as | z, a) ] + \u03b3\n4 \u2211 s\u2032,a\u2032 \u2211 s\u2032\u2032,a\u2032\u2032 \u03bd(s\u2032\u2032, a\u2032\u2032)P a \u2032\u2032 s\u2032\u2032 (s \u2032)\u03c0\u0302n(a \u2032 | s\u2032)\nE (s,z,a)\u223cHt,n\ns\u2032,a\u2032,\u03c1\n[ 1\u2212 P\u0302 tn(\u03c3as | z, a) ] +\n1\u2212 \u03b3 4 \u2211 s\u2032,a\u2032 \u00b5(s\u2032)\u03c0\u0302n(a \u2032 | s\u2032)\nE (s,z,a)\u223cHt,n\ns\u2032,a\u2032,\u03c1\n[ 1\u2212 P\u0302 tn(\u03c3as | z, a) ]) = c\u03c0\u03bd E\n(s,z,a)\u223cHt,n\u03ben,\u03c1\n[ 1\u2212 P\u0302 tn(\u03c3as | z, a) ] .\nWhen t = 1, E(s,z,a)\u223cHt,n\u03ben,\u03c1 [ 1 \u2212 P\u0302 tn(\u03c3as | z, a) ] =\nE(s,a)\u223c\u03ben(s,a) [ 1\u2212 P\u0302 tn(\u03c3as | s, a) ] . When t > 1,\nE (s,z,a)\u223cHt,n\u03ben,\u03c1\n[ 1\u2212 P\u0302 tn(\u03c3as | z, a) ] =\n\u2211 st,zt,at E (s1,a1)\u223c\u03ben [ \u2211 a1:t\u22121 \u03c1(a2:t | a1)\nP a0:t\u22121s1 (st | s1, a0:t\u22121)P\u0302 1:t\u22121 n (zt | s1, a0:t\u22121) ] ( 1\u2212 P\u0302 tn(\u03c3atst | zt, at)\n) = E\n(s,z,a)\u223cHtn\n[ 1\u2212 P\u0302 tn(\u03c3as | z, a) ] .\nThus, putting it all together, we have shown that\nE s\u223c\u00b5\n[ V \u03c0(s)\u2212 V \u03c0\u0304(s) ] \u2264 8M\n1\u2212 \u03b3 c\u03c0\u03bd\n1\nN N\u2211 n=1 T\u22121\u2211 t=1 E (s,z,a)\u223cHtn [ 1\u2212 P\u0302 tn(\u03c3as | z, a) ] + \u0304mc\n= 8M\n1\u2212 \u03b3 c\u03c0\u03bd T\u22121\u2211 t=1 \u0304tprd + \u0304mc."}], "references": [{"title": "Using inaccurate models in reinforcement learning", "author": ["P. Abbeel", "M. Quigley", "A.Y. Ng"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "Abbeel et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2006}, {"title": "Skip context tree switching", "author": ["M.G. Bellemare", "J. Veness", "E. Talvitie"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Bellemare et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2014}, {"title": "A neuroevolution approach to general atari game playing", "author": ["M. Hausknecht", "J. Lehman", "R. Miikkulainen", "P. Stone"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games,", "citeRegEx": "Hausknecht et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hausknecht et al\\.", "year": 2014}, {"title": "Reinforcement learning with misspecified model classes", "author": ["J. Joseph", "A. Geramifard", "J.W. Roberts", "J.P. How", "N. Roy"], "venue": "IEEE International Conference on Robotics and Automation,", "citeRegEx": "Joseph et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Joseph et al\\.", "year": 2013}, {"title": "On the sample complexity of reinforcement learning", "author": ["S.M. Kakade"], "venue": "PhD thesis, University of London,", "citeRegEx": "Kakade.,? \\Q2003\\E", "shortCiteRegEx": "Kakade.", "year": 2003}, {"title": "Actionconditional video prediction using deep networks in atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S. Singh"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Agnostic system identification for modelbased reinforcement learning", "author": ["S. Ross", "D. Bagnell"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Ross and Bagnell.,? \\Q2012\\E", "shortCiteRegEx": "Ross and Bagnell.", "year": 2012}, {"title": "Reward design via online gradient ascent", "author": ["J. Sorg", "R.L. Lewis", "S. Singh"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sorg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sorg et al\\.", "year": 2010}, {"title": "Model-based reinforcement learning with nearly tight exploration complexity bounds", "author": ["I. Szita", "C. Szepesv\u00e1ri"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Szita and Szepesv\u00e1ri.,? \\Q2010\\E", "shortCiteRegEx": "Szita and Szepesv\u00e1ri.", "year": 2010}, {"title": "Model regularization for stable sample rollouts", "author": ["E. Talvitie"], "venue": "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Talvitie.,? \\Q2014\\E", "shortCiteRegEx": "Talvitie.", "year": 2014}, {"title": "Agnostic system identification for monte carlo planning", "author": ["E. Talvitie"], "venue": "In Proceedings of the 29th AAAI Conference on Artificial Intelligence,", "citeRegEx": "Talvitie.,? \\Q2015\\E", "shortCiteRegEx": "Talvitie.", "year": 2015}, {"title": "On-line policy improvement using monte-carlo search", "author": ["G. Tesauro", "G.R. Galperin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Tesauro and Galperin.,? \\Q1996\\E", "shortCiteRegEx": "Tesauro and Galperin.", "year": 1996}, {"title": "A Monte-Carlo AIXI Approximation", "author": ["J. Veness", "K.S. Ng", "M. Hutter", "W.T.B. Uther", "D. Silver"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Veness et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Veness et al\\.", "year": 2011}, {"title": "Context tree switching", "author": ["J. Veness", "K.S. Ng", "M. Hutter", "M. Bowling"], "venue": "In Proceedings of the 2012 Data Compression Conference,", "citeRegEx": "Veness et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Veness et al\\.", "year": 2012}, {"title": "Improving multistep prediction of learned time series models", "author": ["A. Venkatraman", "M. Hebert", "J.A. Bagnell"], "venue": "In Proceedings of the 29th AAAI Conference on Artificial Intelligence,", "citeRegEx": "Venkatraman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venkatraman et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "For instance, Sorg et al. (2010) and Joseph et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 3, "context": "(2010) and Joseph et al. (2013) both pointed out that the most accurate model by this measure is not necessarily the best for planning.", "startOffset": 11, "endOffset": 32}, {"referenceID": 3, "context": "(2010) and Joseph et al. (2013) both pointed out that the most accurate model by this measure is not necessarily the best for planning. Both proposed optimizing model parameters for control performance using policy gradient methods. Though appealing in its directness, this approach arguably discards some of the benefits of learning a model in the first place. Talvitie (2014) pointed out that one-step prediction accuracy does not account for how the model behaves when composed with itself and introduced the Hallucinated Replay meta-algorithm to address this.", "startOffset": 11, "endOffset": 378}, {"referenceID": 3, "context": "(2010) and Joseph et al. (2013) both pointed out that the most accurate model by this measure is not necessarily the best for planning. Both proposed optimizing model parameters for control performance using policy gradient methods. Though appealing in its directness, this approach arguably discards some of the benefits of learning a model in the first place. Talvitie (2014) pointed out that one-step prediction accuracy does not account for how the model behaves when composed with itself and introduced the Hallucinated Replay meta-algorithm to address this. As illustrated in Figure 1, this approach rolls out the model and environment in parallel, training the model to predict the correct environment state (s4) even when its input is an incorrect sampled state (z3). This effectively causes the model to \u201cself-correct\u201d its rollouts. Hallucinated Replay was shown to enable meaningful planning with flawed models in examples where the standard approach failed. However, it offers no theoretical guarantees. Venkatraman et al. (2015) and Oh et al.", "startOffset": 11, "endOffset": 1041}, {"referenceID": 3, "context": "(2010) and Joseph et al. (2013) both pointed out that the most accurate model by this measure is not necessarily the best for planning. Both proposed optimizing model parameters for control performance using policy gradient methods. Though appealing in its directness, this approach arguably discards some of the benefits of learning a model in the first place. Talvitie (2014) pointed out that one-step prediction accuracy does not account for how the model behaves when composed with itself and introduced the Hallucinated Replay meta-algorithm to address this. As illustrated in Figure 1, this approach rolls out the model and environment in parallel, training the model to predict the correct environment state (s4) even when its input is an incorrect sampled state (z3). This effectively causes the model to \u201cself-correct\u201d its rollouts. Hallucinated Replay was shown to enable meaningful planning with flawed models in examples where the standard approach failed. However, it offers no theoretical guarantees. Venkatraman et al. (2015) and Oh et al. (2015) used similar approaches to improve models\u2019 long-range predictions, though not in the MBRL setting.", "startOffset": 11, "endOffset": 1062}, {"referenceID": 4, "context": "For largeN , Q\u0304will closely approximate Q\u0302\u03c1T (Kakade 2003). The agent will select its actions greedily with respect to Q\u0304. Talvitie (2015) bounds the performance of one-ply MC in terms of model quality.", "startOffset": 46, "endOffset": 139}, {"referenceID": 4, "context": "For largeN , Q\u0304will closely approximate Q\u0302\u03c1T (Kakade 2003). The agent will select its actions greedily with respect to Q\u0304. Talvitie (2015) bounds the performance of one-ply MC in terms of model quality. For a policy \u03c0 and state-action distribution \u03be, let \u03be,\u03c0,T val = E(s,a)\u223c\u03be [ |QT (s, a) \u2212 Q\u0302T (s, a)| ] be the error in the T -step state-action values the model assigns to the policy under the given distribution. Then the following result can be straightforwardly adapted from one provided by Talvitie (2015). Lemma 1.", "startOffset": 46, "endOffset": 511}, {"referenceID": 4, "context": "For largeN , Q\u0304will closely approximate Q\u0302\u03c1T (Kakade 2003). The agent will select its actions greedily with respect to Q\u0304. Talvitie (2015) bounds the performance of one-ply MC in terms of model quality. For a policy \u03c0 and state-action distribution \u03be, let \u03be,\u03c0,T val = E(s,a)\u223c\u03be [ |QT (s, a) \u2212 Q\u0302T (s, a)| ] be the error in the T -step state-action values the model assigns to the policy under the given distribution. Then the following result can be straightforwardly adapted from one provided by Talvitie (2015). Lemma 1. Let Q\u0304 be the state-action value function returned by applying one-ply Monte Carlo to the model P\u0302 with rollout policy \u03c1 and rollout depth T . Let \u03c0\u0302 be greedy w.r.t. Q\u0304. For any policy \u03c0 and state-distribution \u03bc, E s\u223c\u03bc [ V (s)\u2212 V (s) ] \u2264 4 1\u2212 \u03b3 \u03be,\u03c1,T val + mc, where we let \u03be(s, a) = 1 2D\u03bc,\u03c0\u0302(s, a) + 1 4D\u03bc,\u03c0(s, a) + 1 4 ( (1\u2212 \u03b3)\u03bc(s)\u03c0\u0302s(a) + \u03b3 \u2211 z,bD\u03bc,\u03c0(z, b)P b z (s)\u03c0\u0302s(a) ) and mc = 4 1\u2212\u03b3 \u2016Q\u0304 \u2212 Q\u0302 \u03c1 T \u2016\u221e + 2 1\u2212\u03b3 \u2016BV \u03c1 T \u2212 V \u03c1 T \u2016\u221e (here B is the Bellman operator). The mc term represents error due to limitations of the planning algorithm: error due to the sample average Q\u0304 and the sub-optimality of the T -step value function with respect to \u03c1. The \u03be,\u03c1,T val term represents error due to the model parameters. The key factor in the model\u2019s usefulness for planning is the accuracy of the value it assigns to the rollout policy in state-actions visited by \u03c0 and \u03c0\u0302. Our goal in the next sections is to bound \u03be,\u03c1,T val in terms of measures of model accuracy, ultimately deriving insight into how to train models that will be effective for MBRL. Proofs may be found in the appendix. 2.1 One-step prediction error Intuitively, the value of a policy should be accurate if the model is accurate in states that the policy would visit. We can adapt a bound from Ross and Bagnell (2012). Lemma 2.", "startOffset": 46, "endOffset": 1805}, {"referenceID": 9, "context": "Consider the \u201cShooter\u201d domain introduced by Talvitie (2015), pictured in Figure 2a.", "startOffset": 44, "endOffset": 60}, {"referenceID": 9, "context": "\u201d Talvitie (2014) presents several similar examples involving various model deficiencies.", "startOffset": 2, "endOffset": 18}, {"referenceID": 14, "context": "Venkatraman et al. (2015) provide some analysis but in the uncontrolled time series prediction setting.", "startOffset": 0, "endOffset": 26}, {"referenceID": 2, "context": "(2015) learned models of Atari 2600 games, which are fully deterministic (Hausknecht et al. 2014); human players often perceive them as stochastic due to their complexity.", "startOffset": 73, "endOffset": 97}, {"referenceID": 4, "context": "For instance, Oh et al. (2015) learned models of Atari 2600 games, which are fully deterministic (Hausknecht et al.", "startOffset": 14, "endOffset": 31}, {"referenceID": 2, "context": "(2015) learned models of Atari 2600 games, which are fully deterministic (Hausknecht et al. 2014); human players often perceive them as stochastic due to their complexity. Similarly, in synthetic RL domains stochasticity is often added to simulate complex, deterministic phenomena (e.g. robot wheels slipping on debris), not necessarily to capture inherently stochastic effects in the world. As in these examples, we shall assume that the environment is deterministic but complex, so a limited agent will learn an imperfect, stochastic model. That said, even specialized to deterministic environments, the bound in Lemma 4 is loose for arbitrary policies. Proposition 6. The hallucinated one-step error of a perfect model may be non-zero, even in a deterministic MDP. Proof. Alter the coin MDP, giving the agent two actions which fully determine the coin\u2019s orientation. The original dynamics can be recovered via a stochastic policy that randomly selects sh or st and then leaves the coin alone. Oh et al. (2015) tied action selection to the environment state only (rather than independently selecting actions in the environment and model).", "startOffset": 74, "endOffset": 1013}, {"referenceID": 6, "context": "We now analyze H-DAgger-MC, adapting Ross and Bagnell (2012)\u2019s DAgger analysis.", "startOffset": 37, "endOffset": 61}, {"referenceID": 6, "context": "Ross and Bagnell (2012) discuss extensions that address more practical loss functions, finite sample bounds, and results for \u03c0\u0302N .", "startOffset": 0, "endOffset": 24}, {"referenceID": 6, "context": "Ross and Bagnell (2012) discuss extensions that address more practical loss functions, finite sample bounds, and results for \u03c0\u0302N . The next question is, of course, when will the learned models be accurate? Following Ross and Bagnell (2012) note that \u0304prd can be interpreted as the average loss of an online learner on the problem defined by the aggregated datasets at each iteration.", "startOffset": 0, "endOffset": 240}, {"referenceID": 13, "context": "The model for each pixel was learned using Context Tree Switching (Veness et al. 2012), similar to the FAC-CTW algorithm (Veness et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 12, "context": "2012), similar to the FAC-CTW algorithm (Veness et al. 2011), and used a 7\u00d7 7 neighborhood around the pixel in the previous timestep as input.", "startOffset": 40, "endOffset": 60}, {"referenceID": 8, "context": "The experimental setup matches that of Talvitie (2015) for comparison\u2019s sake, though the qualitative comparison presented here is robust to the parameter settings.", "startOffset": 39, "endOffset": 55}, {"referenceID": 8, "context": "The experimental setup matches that of Talvitie (2015) for comparison\u2019s sake, though the qualitative comparison presented here is robust to the parameter settings. In all cases one-ply MC was used with 50 uniformly random rollouts of depth 15 at every step. The exploration distribution was generated by following the optimal policy with (1\u2212\u03b3) probability of termination at each step. The model for each pixel was learned using Context Tree Switching (Veness et al. 2012), similar to the FAC-CTW algorithm (Veness et al. 2011), and used a 7\u00d7 7 neighborhood around the pixel in the previous timestep as input. Data was shared across all positions. The discount factor was \u03b3 = 0.9. In each iteration 500 training rollouts were generated and the resulting policy was evaluated in an episode of length 30. The discounted return obtained by the policy in each iteration is reported, averaged over 50 trials. The results can be seen in Figure 3a and 3b. The shaded regions represent 95% confidence intervals for the mean performance. The benchmark lines labeled \u201cRandom\u201d and \u201cPerfect Model\u201d represent the average performance of the uniform random policy and one-ply Monte Carlo using a perfect model, respectively. In Figure 3a the bullseyes move, simulating the typical practical reality that C does not contain a perfect model. In Figure 3b the bullseyes have fixed positions, so C does contain a perfect model. As observed by Talvitie (2015), DAgger performs poorly in both versions, due to the suboptimal planner.", "startOffset": 39, "endOffset": 1439}, {"referenceID": 8, "context": "The experimental setup matches that of Talvitie (2015) for comparison\u2019s sake, though the qualitative comparison presented here is robust to the parameter settings. In all cases one-ply MC was used with 50 uniformly random rollouts of depth 15 at every step. The exploration distribution was generated by following the optimal policy with (1\u2212\u03b3) probability of termination at each step. The model for each pixel was learned using Context Tree Switching (Veness et al. 2012), similar to the FAC-CTW algorithm (Veness et al. 2011), and used a 7\u00d7 7 neighborhood around the pixel in the previous timestep as input. Data was shared across all positions. The discount factor was \u03b3 = 0.9. In each iteration 500 training rollouts were generated and the resulting policy was evaluated in an episode of length 30. The discounted return obtained by the policy in each iteration is reported, averaged over 50 trials. The results can be seen in Figure 3a and 3b. The shaded regions represent 95% confidence intervals for the mean performance. The benchmark lines labeled \u201cRandom\u201d and \u201cPerfect Model\u201d represent the average performance of the uniform random policy and one-ply Monte Carlo using a perfect model, respectively. In Figure 3a the bullseyes move, simulating the typical practical reality that C does not contain a perfect model. In Figure 3b the bullseyes have fixed positions, so C does contain a perfect model. As observed by Talvitie (2015), DAgger performs poorly in both versions, due to the suboptimal planner. DAgger-MC is able to perform well with fixed bullseyes (Figure 3b), but with moving bullseyes the model suffers from compounding errors and is not useful for planning (Figure 3a). This holds for a single model and for an \u201cunrolled\u201d model. In these experiments one practically-minded alteration was made to the H-DAgger-MC algorithm. In early training the model is highly inaccurate, and thus deep rollouts produce incoherent samples. Training with these samples is counter-productive (also, the large number of distinct, nonsensical contexts renders CTS impractical). For these experiments, training rollouts in iteration i were truncated at depth bi/10c. Planning rollouts in these early iterations use the models that have been trained so far and then repeatedly apply the deepest model in order to complete the rollout. Talvitie (2014), Venkatraman et al.", "startOffset": 39, "endOffset": 2351}, {"referenceID": 8, "context": "The experimental setup matches that of Talvitie (2015) for comparison\u2019s sake, though the qualitative comparison presented here is robust to the parameter settings. In all cases one-ply MC was used with 50 uniformly random rollouts of depth 15 at every step. The exploration distribution was generated by following the optimal policy with (1\u2212\u03b3) probability of termination at each step. The model for each pixel was learned using Context Tree Switching (Veness et al. 2012), similar to the FAC-CTW algorithm (Veness et al. 2011), and used a 7\u00d7 7 neighborhood around the pixel in the previous timestep as input. Data was shared across all positions. The discount factor was \u03b3 = 0.9. In each iteration 500 training rollouts were generated and the resulting policy was evaluated in an episode of length 30. The discounted return obtained by the policy in each iteration is reported, averaged over 50 trials. The results can be seen in Figure 3a and 3b. The shaded regions represent 95% confidence intervals for the mean performance. The benchmark lines labeled \u201cRandom\u201d and \u201cPerfect Model\u201d represent the average performance of the uniform random policy and one-ply Monte Carlo using a perfect model, respectively. In Figure 3a the bullseyes move, simulating the typical practical reality that C does not contain a perfect model. In Figure 3b the bullseyes have fixed positions, so C does contain a perfect model. As observed by Talvitie (2015), DAgger performs poorly in both versions, due to the suboptimal planner. DAgger-MC is able to perform well with fixed bullseyes (Figure 3b), but with moving bullseyes the model suffers from compounding errors and is not useful for planning (Figure 3a). This holds for a single model and for an \u201cunrolled\u201d model. In these experiments one practically-minded alteration was made to the H-DAgger-MC algorithm. In early training the model is highly inaccurate, and thus deep rollouts produce incoherent samples. Training with these samples is counter-productive (also, the large number of distinct, nonsensical contexts renders CTS impractical). For these experiments, training rollouts in iteration i were truncated at depth bi/10c. Planning rollouts in these early iterations use the models that have been trained so far and then repeatedly apply the deepest model in order to complete the rollout. Talvitie (2014), Venkatraman et al. (2015), and Oh et al.", "startOffset": 39, "endOffset": 2378}, {"referenceID": 5, "context": "(2015), and Oh et al. (2015) all similarly discarded noisy examples early in training.", "startOffset": 12, "endOffset": 29}, {"referenceID": 5, "context": "(2015), and Oh et al. (2015) all similarly discarded noisy examples early in training. This transient modification does not impact H-DAggerMC\u2019s asymptotic guarantees. In Figure 3a it is clear that H-DAgger-MC obtains a good policy despite the limitations of the model class. Hallucinated training has made MBRL possible with both a flawed model and a flawed planner while the standard approach has failed entirely. In the case that C contains a perfect model (rare in problems of genuine interest) H-DAgger-MC is outperformed by DAgger-MC. Despite the adjustment to training, deep models still receive noisy inputs. Theoretically the model should become perfectly accurate in the limit, though in practice it may do so very slowly. Source code for these experiments may be found at github. com/etalvitie/hdaggermc. 4.1 Impact of the unrolled model Recall that the H-DAgger-MC algorithm assumes the model will be \u201cunrolled,\u201d with a separate model responsible for sampling each step in a rollout. This has clear practical disadvantages, but it is important theoretically. When one model is used across all time-steps, convergence to a perfect model cannot be guaranteed, even if one exists in C. In Figure 3c, H-DAgger-MC has been trained using a single model in Shooter with fixed bullseyes. The temporary truncation schedule described above is employed, but the training rollouts have been permanently limited to various depths. First consider the learning curve marked \u201cDepth 15\u201d, where training rollouts are permitted to reach the maximum depth. While the rollouts are temporarily truncated the model does well, but performance degrades as longer rollouts are permitted even though C contains a perfect model! Recall from Section 3 that changing the model parameters impacts both prediction error and the future training distribution. Furthermore, training examples generated by deep rollouts may contain highly flawed samples as inputs. Sometimes attempting to \u201ccorrect\u201d a large error (i.e. reduce prediction error) causes additional, even worse errors in the next iteration (i.e. harms the training distribution). For instance consider a hallucinated training example with the 4th screen from Figure 2b as input and the 5th screen from Figure 2a as the target. The model would effectively learn that targets can appear out of nowhere, an error that would be even harder to correct in future iterations. With a single model across timesteps, a feedback loop can emerge: the model parameters change to attempt to correct large errors, thereby causing larger errors, and so on. This feedback loop causes the observed performance crash. With an unrolled model the parameters of each sub-model cannot impact that sub-model\u2019s own training distribution, ensuring stability. Note that none of Talvitie (2014), Venkatraman et al.", "startOffset": 12, "endOffset": 2805}, {"referenceID": 5, "context": "(2015), and Oh et al. (2015) all similarly discarded noisy examples early in training. This transient modification does not impact H-DAggerMC\u2019s asymptotic guarantees. In Figure 3a it is clear that H-DAgger-MC obtains a good policy despite the limitations of the model class. Hallucinated training has made MBRL possible with both a flawed model and a flawed planner while the standard approach has failed entirely. In the case that C contains a perfect model (rare in problems of genuine interest) H-DAgger-MC is outperformed by DAgger-MC. Despite the adjustment to training, deep models still receive noisy inputs. Theoretically the model should become perfectly accurate in the limit, though in practice it may do so very slowly. Source code for these experiments may be found at github. com/etalvitie/hdaggermc. 4.1 Impact of the unrolled model Recall that the H-DAgger-MC algorithm assumes the model will be \u201cunrolled,\u201d with a separate model responsible for sampling each step in a rollout. This has clear practical disadvantages, but it is important theoretically. When one model is used across all time-steps, convergence to a perfect model cannot be guaranteed, even if one exists in C. In Figure 3c, H-DAgger-MC has been trained using a single model in Shooter with fixed bullseyes. The temporary truncation schedule described above is employed, but the training rollouts have been permanently limited to various depths. First consider the learning curve marked \u201cDepth 15\u201d, where training rollouts are permitted to reach the maximum depth. While the rollouts are temporarily truncated the model does well, but performance degrades as longer rollouts are permitted even though C contains a perfect model! Recall from Section 3 that changing the model parameters impacts both prediction error and the future training distribution. Furthermore, training examples generated by deep rollouts may contain highly flawed samples as inputs. Sometimes attempting to \u201ccorrect\u201d a large error (i.e. reduce prediction error) causes additional, even worse errors in the next iteration (i.e. harms the training distribution). For instance consider a hallucinated training example with the 4th screen from Figure 2b as input and the 5th screen from Figure 2a as the target. The model would effectively learn that targets can appear out of nowhere, an error that would be even harder to correct in future iterations. With a single model across timesteps, a feedback loop can emerge: the model parameters change to attempt to correct large errors, thereby causing larger errors, and so on. This feedback loop causes the observed performance crash. With an unrolled model the parameters of each sub-model cannot impact that sub-model\u2019s own training distribution, ensuring stability. Note that none of Talvitie (2014), Venkatraman et al. (2015), or Oh et al.", "startOffset": 12, "endOffset": 2832}, {"referenceID": 5, "context": "(2015), and Oh et al. (2015) all similarly discarded noisy examples early in training. This transient modification does not impact H-DAggerMC\u2019s asymptotic guarantees. In Figure 3a it is clear that H-DAgger-MC obtains a good policy despite the limitations of the model class. Hallucinated training has made MBRL possible with both a flawed model and a flawed planner while the standard approach has failed entirely. In the case that C contains a perfect model (rare in problems of genuine interest) H-DAgger-MC is outperformed by DAgger-MC. Despite the adjustment to training, deep models still receive noisy inputs. Theoretically the model should become perfectly accurate in the limit, though in practice it may do so very slowly. Source code for these experiments may be found at github. com/etalvitie/hdaggermc. 4.1 Impact of the unrolled model Recall that the H-DAgger-MC algorithm assumes the model will be \u201cunrolled,\u201d with a separate model responsible for sampling each step in a rollout. This has clear practical disadvantages, but it is important theoretically. When one model is used across all time-steps, convergence to a perfect model cannot be guaranteed, even if one exists in C. In Figure 3c, H-DAgger-MC has been trained using a single model in Shooter with fixed bullseyes. The temporary truncation schedule described above is employed, but the training rollouts have been permanently limited to various depths. First consider the learning curve marked \u201cDepth 15\u201d, where training rollouts are permitted to reach the maximum depth. While the rollouts are temporarily truncated the model does well, but performance degrades as longer rollouts are permitted even though C contains a perfect model! Recall from Section 3 that changing the model parameters impacts both prediction error and the future training distribution. Furthermore, training examples generated by deep rollouts may contain highly flawed samples as inputs. Sometimes attempting to \u201ccorrect\u201d a large error (i.e. reduce prediction error) causes additional, even worse errors in the next iteration (i.e. harms the training distribution). For instance consider a hallucinated training example with the 4th screen from Figure 2b as input and the 5th screen from Figure 2a as the target. The model would effectively learn that targets can appear out of nowhere, an error that would be even harder to correct in future iterations. With a single model across timesteps, a feedback loop can emerge: the model parameters change to attempt to correct large errors, thereby causing larger errors, and so on. This feedback loop causes the observed performance crash. With an unrolled model the parameters of each sub-model cannot impact that sub-model\u2019s own training distribution, ensuring stability. Note that none of Talvitie (2014), Venkatraman et al. (2015), or Oh et al. (2015) used an unrolled model.", "startOffset": 12, "endOffset": 2853}], "year": 2017, "abstractText": "When an agent cannot represent a perfectly accurate model of its environment\u2019s dynamics, model-based reinforcement learning (MBRL) can fail catastrophically. Planning involves composing the predictions of the model; when flawed predictions are composed, even minor errors can compound and render the model useless for planning. Hallucinated Replay (Talvitie 2014) trains the model to \u201ccorrect\u201d itself when it produces errors, substantially improving MBRL with flawed models. This paper theoretically analyzes this approach, illuminates settings in which it is likely to be effective or ineffective, and presents a novel error bound, showing that a model\u2019s ability to self-correct is more tightly related to MBRL performance than one-step prediction error. These results inspire an MBRL algorithm for deterministic MDPs with performance guarantees that are robust to model class limitations.", "creator": "LaTeX with hyperref package"}}}