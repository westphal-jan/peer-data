{"id": "1612.07130", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2016", "title": "Sparse Coding of Neural Word Embeddings for Multilingual Sequence Labeling", "abstract": "In this paper we propose and carefully evaluate a sequence labeling framework which solely utilizes sparse indicator features derived from dense distributed word representations. The proposed model obtains (near) state-of-the art performance for both part-of-speech tagging and named entity recognition for a variety of languages. Our model relies only on a few thousand sparse coding-derived features, without applying any modification of the word representations employed for the different tasks. The proposed model has favorable generalization properties as it retains over 89.8% of its average POS tagging accuracy when trained at 1.2% of the total available training data, i.e.~150 sentences per language. While the majority of the language processing language learning literature is described in a recent study, our data may be highly informative. The fact that the language processing task is able to learn and recognize language specific to every language in a relatively short amount of time is not unique to many languages.\n\n\nThe computational capabilities of the language processing system are not limited to the language processing capabilities of any language, but also the processing power of the language processing system, which is currently being developed in the context of complex social systems. The language processing system provides a flexible and robust, scalable, and distributed algorithm, capable of using sparse and sparse indicators to classify and classify the various language parts of the world. In addition, the language processing system allows the language processing system to perform the tasks needed to classify and classify all of the other language parts of the world without the need for any special language processing. Our model provides an even more versatile, efficient and dynamic programming language which can be adapted to other languages with limited, less flexible, and more efficient computational capabilities.\nIn order to use our model efficiently, we need to design and maintain the language system as well as to develop a high-level representation of a linguistic language, with minimal or no changes to the language processing system.\nMethods\nThe computer program has been implemented as a single machine and is used to generate and classify speech and recognition, and to model and classify other language parts of the world with minimal or no changes to the language processing system. The machine will be connected to the network of computers and can be connected to the network of computers and computer processing systems, which are distributed among the languages of the world. The operating system supports a number of functions:\n- The system is based on a simple computer program that interprets the text of the spoken language. The language is used to determine the pronunciation of the words and to measure the number of", "histories": [["v1", "Wed, 21 Dec 2016 14:17:53 GMT  (150kb,D)", "http://arxiv.org/abs/1612.07130v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["g\\'abor berend"], "accepted": true, "id": "1612.07130"}, "pdf": {"name": "1612.07130.pdf", "metadata": {"source": "CRF", "title": "Sparse Coding of Neural Word Embeddings for Multilingual Sequence Labeling", "authors": ["G\u00e1bor Berend"], "emails": ["berendg@inf.u-szeged.hu"], "sections": [{"heading": "1 Introduction", "text": "Determining the linguistic structure of natural language texts based on rich hand-crafted features has a long-going history in natural language processing. The focus of traditional approaches has mostly been on building linguistic analyzers for a particular kind of analysis, which often leads to the incorporation of extensive linguistic and/or domain knowledge for defining the feature space. Consequently, traditional models easily become language and/or task specific resulting in improper generalization properties.\nA new research direction has emerged recently, which aims at building more general models that require far less feature engineering or none at all. These advancements of natural language processing, pioneered by Bengio et al. (2003), followed by Collobert and Weston (2008), Collobert et al. (2011),\nMikolov et al. (2013a) among others, employ a different philosophy. The objective of these works is to find representations for linguistic phenomena in an unsupervised manner by relying on large amounts of text.\nNatural language phenomena are extremely sparse by their nature, whereas continuous word embeddings employ dense representations of words. In our paper we empirically verify via rigorous experiments that turning these dense representations into a much sparser (yet denser than one-hot encoding) form can help in keeping the most salient parts of word representations that are highly suitable for sequence models.\nFurthermore, our experiments reveal that our proposed model performs substantially better than traditional feature-rich models in the absence of abundant training data. Our proposed model also has the advantage of performing well on multiple sequence labeling tasks without any modification in the applied word representations thanks to the sparse features derived from continuous word representations.\nOur work aims at introducing a novel sequence labeling model solely utilizing features derived from the sparse coding of continuous word embeddings. Even though sparse coding has been utilized in NLP prior to us (Faruqui et al., 2015; Chen et al., 2016), to the best of our knowledge, we are the first to propose a sequence labeling framework incorporating it with the following contributions:\n\u2022 we show that the proposed sparse representation is general as sequence labeling models trained on them achieve (near) state-of-the-art performances for both POS tagging and NER,\nar X\niv :1\n61 2.\n07 13\n0v 1\n[ cs\n.C L\n] 2\n1 D\nec 2\n\u2022 we show that the representation is general in the other sense, that it produces reasonable results for more than 40 treebanks for POS tagging,\n\u2022 we rigorously compare different sparse coding approaches in conjunction with differently trained continuous word embeddings,\n\u2022 we highlight the favorable generalization properties of our model in settings when access to a very limited training corpus is assumed,\n\u2022 we release the sparse word representations determined for our experiments at https:// begab.github.io/sparse_embeds to ensure the replicability of our results and to foster further multilingual NLP research."}, {"heading": "2 Related work", "text": "The line of research introduced in this paper relies on distributed word representations (Al-Rfou et al., 2013) and dictionary learning for sparse coding (Mairal et al., 2010) and also shows close resemblance to (Faruqui et al., 2015)."}, {"heading": "2.1 Distributed word representations", "text": "Distributed word representations assign some relatively low-dimensional, dense vectors to each word in a corpus such that words with similar context and meaning tend to have similar representations. From an algebraic point of view, the embedding of word i having index idxi in a vocabulary V can be thought of as the result of a matrix-vector multiplication W1i, where the ith column of matrix W \u2208 Rk\u00d7|V | contains the k-dimensional (k |V |) embedding for word i and vector 1i \u2208 R|V| is the one-hot representation of word i. The one-hot representation of word i is such a vector, which contains zeros for all of its entries except for index idxi where it stores a one. Depending on how the columns of W (i.e. the word embeddings) get determined, we could distinguish a plethora of approaches (Bengio et al., 2003; Lebret and Collobert, 2013; Mnih and Kavukcuoglu, 2013; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014).\nPrediction-based distributed word embedding approaches such as word2vec (Mikolov et al., 2013a) have been conjectured to have superior performance over count-based word representations\n(Baroni et al., 2014). However, as Lebret and Collobert (2014), Levy et al. (2015) and Qu et al. (2015) point out count-based distributional models can perform on par with prediction-based distributed word embedding models. Levy et al. (2015) illustrate that the effectiveness of neural word embeddings largely depend on the selection of model hyperparameters and other design choices.\nAccording to these findings, in order to avoid any hassles of tuning the hyperparameters of the word embedding model employed, we rather primarily use the publicly available pre-trained polyglot word embeddings of (Al-Rfou et al., 2013) without any task specific modification for our experiments. A key thing to note is that polyglot word embeddings are not tailored for any specific language analysis task such as POS tagging or NER. These word embeddings are instead trained in a manner favoring the word analogy task introduced by Mikolov et al. (2013c). The polyglot project distributes word embeddings for more than 100 languages. AlRfou et al. (2013) also report results on POS tagging, however, word representations they apply for these experiments are different from the task-agnostic representations they made publicly available.\nThere have been further previous research conducted on training neural networks for learning distributed word representations for various specific language analysis tasks. Collobert et al. (2011) propose neural network architectures to four natural language processing tasks, i.e. POS tagging, named entity recognition, semantic role labeling and chunking. Collobert et al. (2011) train word representations on large amounts of unannotated texts from Wikipedia, then update the pre-trained word representations for the individual tasks. Our approach is different in that we do not update our word representations for the different tasks and most importantly that we use successfully the features derived from sparse coding in a log-linear model instead of a neural network architecture. A final difference to (Collobert et al., 2011) is that we experiment with a much wider range of languages while they report results for English only.\nQu et al. (2015) evaluate the impacts of choosing different embedding methods on four sequence labeling tasks, i.e. POS tagging, NER, syntactic chunking and multiword expression identification.\nThe hand-crafted features they employ for POS tagging and NER are the same as in Collobert et al. (2011) and Turian et al. (2010)."}, {"heading": "2.2 Sparse coding", "text": "The general goal of sparse coding is to express signals in the form of sparse linear combination of basis vectors and the task of finding an appropriate set of basis vectors is referred to as the dictionary learning problem (Mairal et al., 2010). Generally, given a data matrix X \u2208 Rk\u00d7n with its ith column xi representing the ith k-dimensional signal, the task is to findD \u2208 Rk\u00d7m and \u03b1 \u2208 Rm\u00d7n, such thatX \u2248 D\u03b1. This can be formalized into an `1-regularized linear least-squares minimization problem having the form\nmin D\u2208C,\u03b1\n1\n2n n\u2211 i=1 ( \u2016xi \u2212D\u03b1i\u201622 + \u03bb\u2016\u03b1i\u20161 ) , (1)\nwith C being the convex set of matrices that comprise of column vectors having an `2 norm at most one, matrix D acts as the shared dictionary across the signals, and the columns of the sparse matrix \u03b1 contains the coefficients for the linear combinations of each of the n observed signals.\nPerforming sparse coding of word embeddings has recently been proposed by Faruqui et al. (2015), however, the objective function they optimize differs from (1). In Section 4, we compare the effects of employing different sparse coding paradigms including the ones in (Faruqui et al., 2015).\nIn their work, Yogatama et al. (2015) proposed an efficient learning algorithm for determining hierarchically organized sparse word representations using stochastic proximal methods. Most recently, Sun et al. (2016) have proposed an online learning algorithm using regularized dual averaging to directly obtain `1 regularized continuous bag of words (CBOW) representations (Mikolov et al., 2013a) without the need to determine dense CBOW representations first."}, {"heading": "3 Sequence labeling framework", "text": "This section introduces the sequence labeling framework we use for both POS tagging and NER. Since our goal is to measure the effectiveness of sparse word embeddings alone, we do not apply any features based on gazetters, capitalization patterns or character suffixes.\nAs described previously, word embedding methods turn high-dimensional (i.e. as many dimensional as many words are in the vocabulary) and extremely sparse (i.e. containing only one non-zero element at the vocabulary index of the word it represents) onehot encoded representation of words into a dense embedding of much lower dimensionality k.\nIn our work, instead of taking the low dimensional dense word embeddings, we use a dictionary learning approach to obtain sparse codings for the embedded word representations. Formally, given the lookup matrix W \u2208 Rk\u00d7|V | which contains the embedding vectors, we learned D \u2208 Rk\u00d7m being the dictionary matrix shared across all the embedding vectors and \u03b1 \u2208 Rm\u00d7|V | containing sparse linear combination coefficients for each of the word embeddings in such a way that \u2016W \u2212D\u03b1\u20162F + \u03bb\u2016\u03b1\u20161 is minimized.\nOnce the dictionary matrix D is learned, the sparse linear combination coefficients \u03b1i can easily be determined for a word embedding vector wi by solving an `1-regularized linear least-squares minimization problem (Mairal et al., 2010). We define features based on vector \u03b1i by taking the signs and indices of its non-zero coefficients, that is\nf(wi) = {sign(\u03b1i[j])j | \u03b1i[j] 6= 0}, (2)\n\u03b1i[j] denoting the jth coefficient in the sparse vector \u03b1i. The intuition behind this feature is that words with similar meaning are expected to use an overlapping set of basis vectors from dictionary D. Incorporating the signs of coefficients into the feature function can help to distinguish cases when a basis vector takes part in the reconstruction of a word representation \u2019destructively\u2019 or \u2019constructively\u2019.\nWhen assigning features to a target word at some position within a sentence, we determine the same set of feature functions for the target word itself and its neighboring words of window size 1. Experiments with window size 2 were also performed, however, these results get omitted for brevity as they do not substantially differ from the ones obtained when a window size of 1 is applied.\nWe then use the previously described set of features in a linear chain CRF (Lafferty et al., 2001) using CRFsuite (Okazaki, 2007). As our goal is not to tweak the proposed framework by extreme hyperparameter tuning, we simply use the default settings\nof CRFsuite to learn model parameters. That is, the coefficients for `1 and `2 regularization is set to 1.0 and 0.001, respectively."}, {"heading": "4 Experiments", "text": "We primarily rely on the SPArse Modeling Software1 (SPAMS) (Mairal et al., 2010) for performing sparse coding of distributed word representations. For dictionary learning as formulated in Equation 1 one should choose m and \u03bb, controlling the number of the basis vectors and the regularization coefficient affecting the sparsity of \u03b1, respectively. Starting with m = 256 and doubling it at each iteration, our preliminary investigations showed a steady growth in the usefulness of sparse word representations as a function of m, plateauing at 1024, for which reason we set m to that value for further experiments."}, {"heading": "4.1 Baseline methods", "text": "Brown clustering Various studies have identified Brown clustering (Brown et al., 1992) as a useful source of feature generation for sequence labeling tasks (Ratinov and Roth, 2009; Turian et al., 2010; Owoputi et al., 2013; Stratos and Collins, 2015; Derczynski et al., 2015). We should note that sparse coding can also be viewed as a kind of clustering which \u2013 unlike Brown clustering \u2013 has the capability of assigning word forms to multiple clusters at a time (corresponding the non-zero coefficients in \u03b1).\nWe thus define a linear chain CRF relying on features from the Brown cluster identifier of words as one of our baseline approach. Since Brown clustering defines a hierarchical clustering over words, cluster supersets can easily function as features. We employ the frequently used approach of generating features from the length-p (p \u2208 {4, 6, 10, 20}) prefixes of Brown cluster identifiers similar to Ratinov and Roth (2009) and Turian et al. (2010).\nIn our experiments we use the implementation by Liang (2005) for performing Brown clustering2. We provide the very same Wikipedia articles as input text for determining Brown clusters that are used for training the polyglot3 word embeddings. We\n1http://spams-devel.gforge.inria.fr/ 2https://github.com/percyliang/\nbrown-cluster 3https://sites.google.com/site/rmyeid/ projects/polyglot\nalso set the number of Brown clusters to be identified to 1024, that is the number of basis vectors applied during sparse coding (cf. D \u2208 R64\u00d71024).\nFeature-rich representation We report results relying on linear chain CRFs that assign standard state-of-the-art feature-rich representation to sequences. We apply the very same features and feature templates included in the POS tagging model of CRFSuite4. We summarize these features in Table 1, where \u2295 denotes the binary operator which defines features as a combination of word forms at different (not necessarily contiguous) positions of a sentence.\nWe use the same pool of features described in Table 1 for both POS tagging and NER. The reason why we do not adjust the feature-rich representation employed as our baseline for the different tasks is that we do not alter our representation in any way when using our sparse coding-based model either.\nNote that features numbered up to 5 in Table 1 operate at the character-level, whereas our proposed framework solely uses features derived from the sparse coding of word forms. We thus distinguish two feature-rich baselines, i.e. FRw+c including both word and character-level features and FRw which treats word forms as atomic units to define features.\nUsing dense word representations As our ultimate goal is to demonstrate the usefulness of sparse\n4http://github.com/chokkan/crfsuite/ blob/master/example/pos.py\nfeatures derived from dense word representations, it is important to address the question whether sparse word representations are more beneficial for sequence labeling tasks compared to their dense counterparts. For this end, we came up with a similar model to the one proposed in Section 3 except for the fact that we used the original dense word representations for inducing features.\nAccording to this modification, we made the following change in our feature function: instead of calculating Equation (2) for some word i, the modified feature function we use for this baseline is\nf(wi) = {j : wi[j] | \u2200j \u2208 {1, . . . , k}}.\nThat is, instead of relying on the nonzero values in \u03b1i, each word is characterized by its k real-valued coordinates in the embedding space. In order to notationally distinguish sparse and dense representations, we add subscript SC when we refer to a sparse coded version of some word embedding (e.g. SGSC)."}, {"heading": "4.2 POS tagging experiments", "text": "Even though it is reasonable to assume that languages share a common coarse set of linguistic categories, linguistic resources used to have their own notations for part-of-speech tags. The first notable attempt trying to canonize the multiple tag sets existing is the Google universal part-of-speech tags introduced by Petrov et al. (2012) arguing that the POS tags of various tagging schemes can be mapped to 12 language-independent part-of-speech tags.\nThere is a recent initiative of universal dependencies (UD) (Nivre, 2015), which aims at providing a unified notation for multiple linguistic phenomena, including part-of-speech tags as well. The POS tag set proposed for UD has 17 partially overlapping categories to the ones defined by Petrov et al. (2012)."}, {"heading": "4.2.1 Experiments using CoNLL 2006/07 data", "text": "We use 12 treebanks in the CoNLL-X format from the CoNLL-2006/07 (Buchholz and Marsi, 2006; Nivre et al., 2007) shared tasks. The complete list of the treebanks included in our experiments is presented in Table 2.\nWe rely on the official scripts released by Petrov et al. (2012)5 for mapping the treebank specific\n5https://github.com/slavpetrov/ universal-pos-tags\nPOS tags to the Google universal POS tags in order to obtain results comparable across languages. For our experiments we used the original CoNLL-X train/test splits of the treebanks.\nA key factor for the efficiency of our proposed model resides in the coverage of word embeddings, i.e. the proportion of tokens/word forms with distributed representation determined for. Figure 1 depicts these coverage scores calculated over the merged training and test sets for the different languages. Figure 1 reveals that a substantial amount of tokens has distributed representation defined for (around 90% for the majority of languages, except for Turkish where it is 5 point less). Token coverages of the word embeddings are most likely affected by the morphological richness of the languages and the elaborateness of the corresponding Wikipedia used for training word embeddings.\nComparing word embeddings Our motivation for choosing polyglot word embeddings as input to sparse coding is that they are publicly available for a variety of languages. However, distributed word representations trained in any other reasonable manner can serve as input to our approach. In order to investigate if some of the popular word embedding techniques seem favorable for our algorithm, we conduct experiments using alternatively trained embeddings, i.e. skip-gram (SG), continuous bagof-words (CBOW) and Glove.\nIn order the utility of different word embeddings not to conflate with other factors, we train them on the same Wikipedia dumps used for training the polyglot word vectors. We choose further hyperparameters identically to polyglot, i.e. we\ntrain 64 dimensional dense word representations using a symmetric context window of size 2 for both SG/CBOW6 and Glove7.\nFigure 2 includes POS tagging accuracies over the 12 treebanks from the CoNLL 2006/07 shared tasks evaluated against Google Universal POS tags. Instead of reporting results as a function of \u03bb, we rather present accuracies as a function of the different sparsity levels induced by different \u03bb values. Figure 2 demonstrates that POS tagging performance is quite insensitive to the choice of \u03bb unless it yields some extreme sparsity level (>99.5%).\nFigure 2 also reveals that the usage of 6https://code.google.com/archive/p/ word2vec/ 7http://nlp.stanford.edu/projects/glove/\npolyglotSC word representations tend to produce superior results over all alternative representations we experiment with. Furthermore, models using polyglotSC consistently outperform the FRw and Brown clustering-based baselines.\nModels relying on SGSC and CBOWSC representations have an average tagging accuracy of 93.74 and 93.63, respectively, and they typically perform better than the baseline using Brown clustering with an average tagging performance of 93.27. Although utilizing Glove embeddings produce the lowest scores (91.92 on average), its scores still surpass those of the FRw baseline for all languages except for Turkish.\nThe average tagging performance over the 12 languages when relying on features based on polyglotSC is only 1.3 points below that of FRw+c (i.e. 94.4 versus 95.7). Recall that FRw+c uses a feature-rich representation, whereas our proposed model uses only O(m) features, i.e. it is tied to the number of the basis vectors employed for sparse coding. Furthermore, neither does our model employs word identity features nor it relies on character-level features of words.\nAnalyzing the effects of window size Hyperparameters for training word representations can largely impact their quality as also concluded by Levy et al. (2015). We thus investigate if providing a larger context window size during the training of CBOW, SG and Glove embeddings can improve their utility of being employed in our model.\nAccording to Figure 3 applying context window sizes of 2 for training the word embeddings tend to\nproduce better overall POS tagging accuracies than applying a larger window size of 10. Differences are the most pronounced in case of skip-gram representation, confirming the findings of Lin et al. (2015), i.e. embedding models that model short-range context are more effective for POS tagging.\nComparing dense and sparse representations In accordance to Figure 2, we set the value of \u03bb to 0.1 for our upcoming experiments unless stated otherwise. Table 3 demonstrates that performances obtained by models using dense word representations as features are consistently inferior to those models which enjoy the benefit of features derived from sparse word representations.\nIn Table 3b, we can see that polyglot embeddings perform the best for dense representations as well. When using dense features, the CBOW representation-based model tends to produce results better by a 1.4 points margin on average compared to SG embeddings. This performance gap between the two word2vec variants vanishes, however, when dense word representations are replaced\nby their sparse counterparts. Table 3 also reveals that sparse word representations improve average POS tagging accuracies by 3.3, 5.4, 6.7 and 10.4 points for polylgot, CBOW, SG and Glove word representations, respectively.\nComparing the effects of training corpus size We also investigate the generalization characteristics of the proposed representation by training models which have access to substantially different amounts of training data per language. We distinguish three scenarios, i.e. when using only the first 150, the first 1,500 and all the available training sentences from each corpora. Figure 4 illustrates the average POS tagging accuracy over the 12 CoNLL-X datasets for different amounts of training data and models.\nTable 4 includes performances in more details revealing that the average performance of polyglotSC is 14.55 and 3.76 points better compared to the FRw and FRw+c baselines when using only 1.2% of all the available training data, i.e. 150 sentences per each language. By discarding 98.8% of the training data polyglotSC obtains 89.8% of its average performance compared to the scenario when it has access to all the training sentences. However, under the same scenario the FRw+c and FRw models only manage to preserve 85% and 77%\nof their original performance, respectively. Our model performs on par with FRw+c and has a 6.85 points advantage over FRw with a training corpus of 1,500 sentences. FRw+c has an average of 1.3 points advantage over polyglotSC when we provide access to all training data during training, nevertheless FRw still underperforms polyglotSC in that setting by 3.67 points.\nComparing sparse coding techniques Next, we compare different sparse coding approaches on the\npre-trained polyglot word representations. The recent work of Faruqui et al. (2015) formulated alternative approaches to determine sparse word representations. One of the objective functions Faruqui et al. (2015) apply is\nmin D,\u03b1\n1\n2n n\u2211 i=1 \u2016xi\u2212D\u03b1i\u201622+\u03bb\u2016\u03b1i\u20161+ \u03c4\u2016D\u201622. (3)\nThe main difference in Eq. 1 and 3 is that the latter does not explicitly constrain D to be a member of the convex set of matrices comprising of column vectors having a pre-defined upper bound on their norm. In order to implicitly control for the norms of the basis vectors Faruqui et al. (2015) apply an additional regularization term affected by an extra parameter \u03c4 in their objective function.\nFaruqui et al. (2015) also formulated a constrained objective function of the form\nmin D\u2208Rk\u00d7m\u22650 \u03b1\u2208Rk\u00d7|V |\u22650\n1\n2n n\u2211 i=1 \u2016xi\u2212D\u03b1i\u201622+\u03bb\u2016\u03b1i\u20161+ \u03c4\u2016D\u201622, (4)\nfor which a non-negativity constraint on the elements of \u03b1 (but no constraint on D) is imposed. When using the objective functions introduced by Faruqui et al. (2015), we use the default \u03c4 = 10\u22125 value. Notationally, we distinguish the sparse coding approaches based on the equation they use as their objective function, i.e. SC-i, i \u2208 {1, 3, 4}.\nWe applied \u03bb = 0.05 for SC-1 and \u03bb = 0.5 for SC-3 and SC-4 in order to obtain word representations of comparable average sparsity levels across the 12 languages, i.e. 95.3%, 94.5% and 95.2%, respectively (cf. the left of Figure 5). The right of Figure 5 further illustrates the spread of POS tagging accuracies over the 12 CoNLL-X treebanks when using models that rely on different sparse coding strategies with comparable sparsity levels.\nAlthough Murphy et al. (2012) mentions nonnegativity as a desired property of word representations for cognitive plausibility, Figure 5 reveals that our sequence labeling model cannot benefit from it as the average POS tagging accuracy for SC-4 is 0.7 points below that of SC-3 approach. The average performances when applying SC-1 and SC-3 are nearly identical with a 0.18 point difference between the two.\nIt is instructive to analyze the patterns different sparse coding approaches exhibit. Even though the objective functions used by the different approaches are similar, decompositions obtained by them convey rather different sparsity structures.\nFigure 6a illustrates that there exist substantial variation in the length of the basis vectors obtained by SC-3 and SC-4 both within and across languages. On the contrary, SC-1 produces practically no variation in the length of the basis vectors comprising D due to the constraint present in the objective function it employs. Figure 6b shows similar differences about the relative frequency of basis vectors taking part in the reconstruction of word embeddings.\nA further characteristic depicted in Figure 7 is that strong correlation can be observed between the `2 norm of basis vectors and the number of times they are assigned a non-zero coefficient in \u03b1 for SC-3 and SC-4 but not for SC-1.\nIt can be further noted from Figure 7 that the norm of the basis vectors determined by SC-3 and SC-4 are often orders of magnitude larger than those determined by SC-1. This effect, however, can be naturally mitigated by increasing \u03c4 .\nOverall, the different approaches convey comparable POS tagging accuracies but different decompositions due to the differences in the objective functions they employ. Upcoming experiments are conducted using the objective function in Eq. 1."}, {"heading": "4.2.2 Experiments using UD treebanks", "text": "For POS tagging we also experiment with UD v1.2 (Nivre et al., 2015) treebanks. We used the default train-test splits of the treebanks not utilizing the development sets for fine tuning performance on any of the languages during our experiments. We omitted the Japanese treebank as words in it are stripped off due to licensing issues. Also there is no polyglot vector released for Old Church Slavonic and Gothic. Even though polyglotword representations are released for Arabic, it was of no practical use as it contained unvocalized surface forms of tokens in contrast to the vocalized forms in UD v.1.2. For this reason, we discarded the Arabic treebank as less than 30% of its tokens could be associated with a representation. By omitting these 4 languages from our experiments we are finally left\nwith 33 treebanks for 29 languages. We note that for Ancient Greek treebanks (grc*) we use word embeddings trained on Modern Greek.\nWe should add that there are 4 languages (related to 6 treebanks) for which polyglot word vectors are accessible, however, the Wikipedia dumps used for training them are not distributed. For this reason, Brown clustering-based baselines are missing for the affected treebanks.\nWe report our results on UD v1.2 in Table 5. Recall that the default behavior of our sparse codingbased models (SC in Table 5) is that they do not handle word identity as an explicit feature. We now investigate how much contribution word identity features convey on their own and also when used in conjunction with sparse coding-derived features. For this end we introduce a simple linear chain CRF model generating features solely on the identity of the current word and the ones surrounding it (WI in Table 5). Likewise, we define a model that relies on WI and SC features simultaneously (WI+SC). Table 5 reveals that SC outperforms WI by a large margin and that combining the two feature sets together yields some further improvements over SC scores.\nWe also present in Table 5 the state-of-the-art results of the bidirectional LSTM models by Plank et al. (2016) for comparative purposes. Note that the authors reported results only on a subset of UD v1.2 (i.e. treebanks with at least 60k tokens), for which reason we can include their results on 21 treebanks. Out of these 21 UD v1.2 treebanks there are 15 and 20 cases, respectively, for which SC and WI+SC produces better results than bi-LSTMw. Only FRw+c and bi-LSTMw+c \u2013 models which enjoy the additional benefit of employing character-level features besides word-level ones \u2013 are capable of outperforming SC and WI+SC."}, {"heading": "4.3 Named entity recognition experiments", "text": "Besides the POS tagging experiments, we investigated if the very same features as the ones applied for POS tagging can be utilized in a different sequence labeling task, namely named entity recognition. In order to evaluate our approach, we obtained the English, Spanish and Dutch datasets from the 2002 and 2003 CoNLL shared tasks on multilingual named entity recognition (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003).\nWe use the train-test splits provided by the organizers and report our NER results using the F1 scores based on the official evaluation script of the CoNLL shared task. Similar to Collobert et al. (2011) we also apply the 17-tag IOBES tagging scheme during training and inference. The best F1 scores reported for English by Collobert et al. (2011) without employing additional unlabeled texts to enhance their language model is 81.47. When pre-training their neural language model on large amounts of Wikipedia texts they report an F1 score of 87.58. Figure 8 includes our NER results obtained when relying on different word embedding representations as input for sparse coding and different levels of sparsity. Similar to our POS tagging experiments, using polyglotSC vectors tend to perform best for NER also. A substantial difference compared\nto the POS tagging results, however, is that NER performances do not degrade for extreme levels of sparsity neither and that sparse coding-based models perform much better when compared to the stronger FRw+c baseline.\nIn Table 6, we compare the effectiveness of models relying on sparse and dense word representations for NER. In order not to fine-tune hyperparameters for a particular experiment, similarly to our previous choices m and \u03bb are set to 1024 and 0.1, respectively. Results in Table 6 are in line with those reported in Table 3 for POS tagging."}, {"heading": "5 Conclusion", "text": "In this paper we showed that it is possible to train sequence models that perform (near) state-of-the-art on a variety of languages for both POS tagging and NER. Our approach does not require word identity features to perform reliably, furthermore, it is ca-\npable of achieving comparable results to traditional feature-rich models. We also illustrated the advantageous generalization property of our model as it retained 89.8% of its original average POS tagging accuracy when trained only on 1.2% of the total accessible training sentences.\nAs Mikolov et al. (2013b) pointed out the similarities of continuous word embeddings across languages, we think that our proposed model could be employed in not just multi-lingual, but cross-lingual language analysis settings. In fact, we consider investigating its feasibility as our future work. Finally, we make the sparse coded word embedding vectors publicly available in order to facilitate the reproducibility of our results and to foster multilingual and cross-lingual research."}, {"heading": "Acknowledgement", "text": "The author would like to thank the TACL editors and the anonymous reviewers for their valuable feedbacks and suggestions."}], "references": [{"title": "Floresta sint\u00e1(c)tica\u201d: a treebank for Portuguese", "author": ["Susana Afonso", "Eckhard Bick", "Renato Haber", "Diana Santos."], "venue": "Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC), pages 1698\u20131703, Las Palmas, Spain.", "citeRegEx": "Afonso et al\\.,? 2002", "shortCiteRegEx": "Afonso et al\\.", "year": 2002}, {"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 183\u2013192, Sofia, Bulgaria, August. Association", "citeRegEx": "Al.Rfou et al\\.,? 2013", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "The annotation process in the Turkish treebank", "author": ["Nart B. Atalay", "Kemal Oflazer", "Bilge Say."], "venue": "In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora (LINC), Budapest, Hungary. Association for Computational Linguistics.", "citeRegEx": "Atalay et al\\.,? 2003", "shortCiteRegEx": "Atalay et al\\.", "year": 2003}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski."], "venue": "context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "J. Mach. Learn. Res., 3:1137\u20131155, March.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "The TIGER treebank", "author": ["Sabine Brants", "Stefanie Dipper", "Silvia Hansen", "Wolfgang Lezius", "George Smith."], "venue": "Proceedings of the Workshop on Treebanks and Linguistic Theories, Sozopol.", "citeRegEx": "Brants et al\\.,? 2002", "shortCiteRegEx": "Brants et al\\.", "year": 2002}, {"title": "Classbased n-gram models of natural language", "author": ["Peter F. Brown", "Peter V. deSouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai."], "venue": "Comput. Linguist., 18(4):467\u2013479, December.", "citeRegEx": "Brown et al\\.,? 1992", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Conll-x shared task on multilingual dependency parsing", "author": ["Sabine Buchholz", "Erwin Marsi."], "venue": "Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X \u201906, pages 149\u2013 164, Stroudsburg, PA, USA. Association for Compu-", "citeRegEx": "Buchholz and Marsi.,? 2006", "shortCiteRegEx": "Buchholz and Marsi.", "year": 2006}, {"title": "Compressing neural language models by sparse word representations", "author": ["Yunchuan Chen", "Lili Mou", "Yan Xu", "Ge Li", "Zhi Jin."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 226\u2013235,", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th International Conference on Machine Learning, ICML \u201908, pages 160\u2013167, New York, NY,", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "J. Mach. Learn. Res., 12:2493\u20132537, November.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "The Szeged treebank", "author": ["D\u00f3ra Csendes", "J\u00e1nos Csirik", "Tibor Gyim\u00f3thy", "Andr\u00e1s Kocsor."], "venue": "TSD, pages 123\u2013131.", "citeRegEx": "Csendes et al\\.,? 2005", "shortCiteRegEx": "Csendes et al\\.", "year": 2005}, {"title": "Tune your brown clustering, please", "author": ["Leon Derczynski", "Sean Chester", "Kenneth B\u00f8gh."], "venue": "Pro-", "citeRegEx": "Derczynski et al\\.,? 2015", "shortCiteRegEx": "Derczynski et al\\.", "year": 2015}, {"title": "Towards a Slovene dependency treebank", "author": ["Sa\u0161o D\u017eeroski", "Toma\u017e Erjavec", "Nina Ledinek", "Petr Pajas", "Zden\u011bk \u017dabokrtsk\u00fd", "Andreja \u017dele."], "venue": "Proceedings of the Fifth International Language Resources and Evaluation Conference, LREC 2006, pages 1388\u20131391, Gen-", "citeRegEx": "D\u017eeroski et al\\.,? 2006", "shortCiteRegEx": "D\u017eeroski et al\\.", "year": 2006}, {"title": "Sparse overcomplete word vector representations", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Dani Yogatama", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Danish dependency treebank", "author": ["Matthias T. Kromann", "Line Mikkelsen", "Stine Kern Lynge"], "venue": null, "citeRegEx": "Kromann et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kromann et al\\.", "year": 2004}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, ICML \u201901, pages", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Word emdeddings through hellinger PCA", "author": ["R\u00e9mi Lebret", "Ronan Collobert."], "venue": "CoRR, abs/1312.5542.", "citeRegEx": "Lebret and Collobert.,? 2013", "shortCiteRegEx": "Lebret and Collobert.", "year": 2013}, {"title": "Rehabilitation of count-based models for word vector representations", "author": ["R\u00e9mi Lebret", "Ronan Collobert."], "venue": "CoRR, abs/1412.4930.", "citeRegEx": "Lebret and Collobert.,? 2014", "shortCiteRegEx": "Lebret and Collobert.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "TACL, 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Semi-supervised learning for natural language", "author": ["P. Liang."], "venue": "Master\u2019s thesis, Massachusetts Institute of Technology.", "citeRegEx": "Liang.,? 2005", "shortCiteRegEx": "Liang.", "year": 2005}, {"title": "Unsupervised pos induction with word embeddings", "author": ["Chu-Cheng Lin", "Waleed Ammar", "Chris Dyer", "Lori Levin."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["Julien Mairal", "Francis Bach", "Jean Ponce", "Guillermo Sapiro."], "venue": "J. Mach. Learn. Res., 11:19\u201360, March.", "citeRegEx": "Mairal et al\\.,? 2010", "shortCiteRegEx": "Mairal et al\\.", "year": 2010}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR, abs/1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Tomas Mikolov", "Quoc V. Le", "Ilya Sutskever."], "venue": "CoRR, abs/1309.4168.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR, abs/1310.4546.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Andriy Mnih", "Koray Kavukcuoglu."], "venue": "Advances in Neural Information Processing Systems (NIPS 2013).", "citeRegEx": "Mnih and Kavukcuoglu.,? 2013", "shortCiteRegEx": "Mnih and Kavukcuoglu.", "year": 2013}, {"title": "Building the Italian syntactic-semantic treebank", "author": ["zotto", "Nadia Mana", "Fabio Pianesi", "Rodolfo Delmonte"], "venue": "In Anne Abeill, editor, Building and using Parsed Corpora, Language and Speech series,", "citeRegEx": "zotto et al\\.,? \\Q2003\\E", "shortCiteRegEx": "zotto et al\\.", "year": 2003}, {"title": "Learning effective and interpretable semantic models using non-negative sparse embedding", "author": ["Brian Murphy", "Partha Talukdar", "Tom Mitchell."], "venue": "Proceedings of COLING 2012, pages 1933\u20131950, Mumbai, India, December. The COLING 2012 Organizing Commit-", "citeRegEx": "Murphy et al\\.,? 2012", "shortCiteRegEx": "Murphy et al\\.", "year": 2012}, {"title": "Talbanken05: A Swedish treebank with phrase structure and dependency annotation", "author": ["Joakim Nivre", "Jens Nilsson", "Johan Hall."], "venue": "Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC 2006), Genova, Italy.", "citeRegEx": "Nivre et al\\.,? 2006", "shortCiteRegEx": "Nivre et al\\.", "year": 2006}, {"title": "The CoNLL 2007 shared task on dependency parsing", "author": ["Joakim Nivre", "Johan Hall", "Sandra K\u00fcbler", "Ryan McDonald", "Jens Nilsson", "Sebastian Riedel", "Deniz Yuret."], "venue": "Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 915\u2013932,", "citeRegEx": "Nivre et al\\.,? 2007", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "Universal dependencies 1.2", "author": ["Aaron Smith", "Jan \u0160t\u011bp\u00e1nek", "Alane Suhr", "Zsolt Sz\u00e1nt\u00f3", "Takaaki Tanaka", "Reut Tsarfaty", "Sumire Uematsu", "Larraitz Uria", "Viktor Varga", "Veronika Vincze", "Zden\u011bk \u017dabokrtsk\u00fd", "Daniel Zeman", "Hanzhi Zhu"], "venue": null, "citeRegEx": "Smith et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2015}, {"title": "Towards a universal grammar for natural language processing", "author": ["Joakim Nivre."], "venue": "Computational Linguistics and Intelligent Text Processing - 16th International Conference, CICLing 2015, Cairo, Egypt, April 14-20, 2015, Proceedings, Part I, pages 3\u201316.", "citeRegEx": "Nivre.,? 2015", "shortCiteRegEx": "Nivre.", "year": 2015}, {"title": "CRFsuite: a fast implementation of Conditional Random Fields (CRFs)", "author": ["Naoaki Okazaki"], "venue": null, "citeRegEx": "Okazaki.,? \\Q2007\\E", "shortCiteRegEx": "Okazaki.", "year": 2007}, {"title": "Improved part-of-speech tagging for online conversational text with word clusters", "author": ["Olutobi Owoputi", "Brendan O\u2019Connor", "Chris Dyer", "Kevin Gimpel", "Nathan Schneider", "Noah A. Smith"], "venue": "In Proceedings of the 2013 Conference of the North American Chap-", "citeRegEx": "Owoputi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Owoputi et al\\.", "year": 2013}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A universal part-of-speech tagset", "author": ["Slav Petrov", "Dipanjan Das", "Ryan McDonald."], "venue": "Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios", "citeRegEx": "Petrov et al\\.,? 2012", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of the 54th Annual Meet-", "citeRegEx": "Plank et al\\.,? 2016", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Big data small data, in domain out-of domain, known word unknown word: The impact of word representations on sequence labelling tasks", "author": ["Lizhen Qu", "Gabriela Ferraro", "Liyuan Zhou", "Weiwei Hou", "Nathan Schneider", "Timothy Baldwin."], "venue": "Proceedings", "citeRegEx": "Qu et al\\.,? 2015", "shortCiteRegEx": "Qu et al\\.", "year": 2015}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Lev Ratinov", "Dan Roth."], "venue": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning, CoNLL \u201909, pages 147\u2013155, Stroudsburg, PA, USA. Association", "citeRegEx": "Ratinov and Roth.,? 2009", "shortCiteRegEx": "Ratinov and Roth.", "year": 2009}, {"title": "Extending the annotation of BulTreeBank: Phase 2", "author": ["Kiril Simov", "Petya Osenova."], "venue": "The Fourth Workshop on Treebanks and Linguistic Theories (TLT 2005), pages 173\u2013184, Barcelona, December.", "citeRegEx": "Simov and Osenova.,? 2005", "shortCiteRegEx": "Simov and Osenova.", "year": 2005}, {"title": "Simple semisupervised pos tagging", "author": ["Karl Stratos", "Michael Collins."], "venue": "Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing, pages 79\u201387, Denver, Colorado, June. Association for Computational Linguistics.", "citeRegEx": "Stratos and Collins.,? 2015", "shortCiteRegEx": "Stratos and Collins.", "year": 2015}, {"title": "Sparse word embeddings using `1 regularized online learning", "author": ["Fei Sun", "Jiafeng Guo", "Yanyan Lan", "Jun Xu", "Xueqi Cheng."], "venue": "Proceedings of the TwentyFifth International Joint Conference on Artificial Intelligence, pages 2915\u20132921.", "citeRegEx": "Sun et al\\.,? 2016", "shortCiteRegEx": "Sun et al\\.", "year": 2016}, {"title": "AnCora: Multilevel annotated corpora for Catalan and Spanish", "author": ["Mariona Taul\u00e9", "Maria Ant\u00f2nia Mart\u0131", "Marta Recasens"], "venue": null, "citeRegEx": "Taul\u00e9 et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Taul\u00e9 et al\\.", "year": 2008}, {"title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "author": ["Erik F. Tjong Kim Sang", "Fien De Meulder."], "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 - Volume 4, CONLL", "citeRegEx": "Sang and Meulder.,? 2003", "shortCiteRegEx": "Sang and Meulder.", "year": 2003}, {"title": "Introduction to the conll2002 shared task: Language-independent named entity recognition", "author": ["Erik F. Tjong Kim Sang."], "venue": "Proceedings of the 6th Conference on Natural Language Learning - Volume 20, COLING02, pages 1\u20134, Stroudsburg, PA, USA. Association for", "citeRegEx": "Sang.,? 2002", "shortCiteRegEx": "Sang.", "year": 2002}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL \u201910, pages 384\u2013394, Strouds-", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Chapter 5", "author": ["Leonoor van der Beek", "Gosse Bouma", "Jan Daciuk", "Tanja Gaustad", "Robert Malouf", "Gertjan van Noord", "Robbert Prins", "Begoa Villada."], "venue": "the Alpino dependency treebank. In Algorithms for Linguistic Processing NWO PIONIER Progress Report, Gronin-", "citeRegEx": "Beek et al\\.,? 2002", "shortCiteRegEx": "Beek et al\\.", "year": 2002}, {"title": "Learning word representations with hierarchical sparse coding", "author": ["Dani Yogatama", "Manaal Faruqui", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of ICML.", "citeRegEx": "Yogatama et al\\.,? 2015", "shortCiteRegEx": "Yogatama et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "Even though sparse coding has been utilized in NLP prior to us (Faruqui et al., 2015; Chen et al., 2016), to the best of our knowledge, we are the first to propose a sequence labeling framework incorporating it with the following contributions:", "startOffset": 63, "endOffset": 104}, {"referenceID": 8, "context": "Even though sparse coding has been utilized in NLP prior to us (Faruqui et al., 2015; Chen et al., 2016), to the best of our knowledge, we are the first to propose a sequence labeling framework incorporating it with the following contributions:", "startOffset": 63, "endOffset": 104}, {"referenceID": 4, "context": "These advancements of natural language processing, pioneered by Bengio et al. (2003), followed by Collobert and Weston (2008), Collobert et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 4, "context": "These advancements of natural language processing, pioneered by Bengio et al. (2003), followed by Collobert and Weston (2008), Collobert et al.", "startOffset": 64, "endOffset": 126}, {"referenceID": 4, "context": "These advancements of natural language processing, pioneered by Bengio et al. (2003), followed by Collobert and Weston (2008), Collobert et al. (2011), Mikolov et al.", "startOffset": 64, "endOffset": 151}, {"referenceID": 4, "context": "These advancements of natural language processing, pioneered by Bengio et al. (2003), followed by Collobert and Weston (2008), Collobert et al. (2011), Mikolov et al. (2013a) among others, employ a different philosophy.", "startOffset": 64, "endOffset": 175}, {"referenceID": 1, "context": "The line of research introduced in this paper relies on distributed word representations (Al-Rfou et al., 2013) and dictionary learning for sparse coding (Mairal et al.", "startOffset": 89, "endOffset": 111}, {"referenceID": 22, "context": ", 2013) and dictionary learning for sparse coding (Mairal et al., 2010) and also shows close resemblance to (Faruqui et al.", "startOffset": 50, "endOffset": 71}, {"referenceID": 14, "context": ", 2010) and also shows close resemblance to (Faruqui et al., 2015).", "startOffset": 44, "endOffset": 66}, {"referenceID": 4, "context": "the word embeddings) get determined, we could distinguish a plethora of approaches (Bengio et al., 2003; Lebret and Collobert, 2013; Mnih and Kavukcuoglu, 2013; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 83, "endOffset": 236}, {"referenceID": 17, "context": "the word embeddings) get determined, we could distinguish a plethora of approaches (Bengio et al., 2003; Lebret and Collobert, 2013; Mnih and Kavukcuoglu, 2013; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 83, "endOffset": 236}, {"referenceID": 27, "context": "the word embeddings) get determined, we could distinguish a plethora of approaches (Bengio et al., 2003; Lebret and Collobert, 2013; Mnih and Kavukcuoglu, 2013; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 83, "endOffset": 236}, {"referenceID": 9, "context": "the word embeddings) get determined, we could distinguish a plethora of approaches (Bengio et al., 2003; Lebret and Collobert, 2013; Mnih and Kavukcuoglu, 2013; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 83, "endOffset": 236}, {"referenceID": 24, "context": "the word embeddings) get determined, we could distinguish a plethora of approaches (Bengio et al., 2003; Lebret and Collobert, 2013; Mnih and Kavukcuoglu, 2013; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 83, "endOffset": 236}, {"referenceID": 36, "context": "the word embeddings) get determined, we could distinguish a plethora of approaches (Bengio et al., 2003; Lebret and Collobert, 2013; Mnih and Kavukcuoglu, 2013; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 83, "endOffset": 236}, {"referenceID": 24, "context": "Prediction-based distributed word embedding approaches such as word2vec (Mikolov et al., 2013a) have been conjectured to have superior performance over count-based word representations (Baroni et al.", "startOffset": 72, "endOffset": 95}, {"referenceID": 3, "context": ", 2013a) have been conjectured to have superior performance over count-based word representations (Baroni et al., 2014).", "startOffset": 98, "endOffset": 119}, {"referenceID": 3, "context": ", 2013a) have been conjectured to have superior performance over count-based word representations (Baroni et al., 2014). However, as Lebret and Collobert (2014), Levy et al.", "startOffset": 99, "endOffset": 161}, {"referenceID": 3, "context": ", 2013a) have been conjectured to have superior performance over count-based word representations (Baroni et al., 2014). However, as Lebret and Collobert (2014), Levy et al. (2015) and Qu et al.", "startOffset": 99, "endOffset": 181}, {"referenceID": 3, "context": ", 2013a) have been conjectured to have superior performance over count-based word representations (Baroni et al., 2014). However, as Lebret and Collobert (2014), Levy et al. (2015) and Qu et al. (2015) point out count-based distributional models can perform on par with prediction-based distributed word embedding models.", "startOffset": 99, "endOffset": 202}, {"referenceID": 3, "context": ", 2013a) have been conjectured to have superior performance over count-based word representations (Baroni et al., 2014). However, as Lebret and Collobert (2014), Levy et al. (2015) and Qu et al. (2015) point out count-based distributional models can perform on par with prediction-based distributed word embedding models. Levy et al. (2015) illustrate that the effectiveness of neural word embeddings largely depend on the selection of model hyperparameters and other design choices.", "startOffset": 99, "endOffset": 341}, {"referenceID": 1, "context": "According to these findings, in order to avoid any hassles of tuning the hyperparameters of the word embedding model employed, we rather primarily use the publicly available pre-trained polyglot word embeddings of (Al-Rfou et al., 2013) without any task specific modification for our experiments.", "startOffset": 214, "endOffset": 236}, {"referenceID": 1, "context": "According to these findings, in order to avoid any hassles of tuning the hyperparameters of the word embedding model employed, we rather primarily use the publicly available pre-trained polyglot word embeddings of (Al-Rfou et al., 2013) without any task specific modification for our experiments. A key thing to note is that polyglot word embeddings are not tailored for any specific language analysis task such as POS tagging or NER. These word embeddings are instead trained in a manner favoring the word analogy task introduced by Mikolov et al. (2013c). The polyglot project distributes word embeddings for more than 100 languages.", "startOffset": 215, "endOffset": 557}, {"referenceID": 1, "context": "According to these findings, in order to avoid any hassles of tuning the hyperparameters of the word embedding model employed, we rather primarily use the publicly available pre-trained polyglot word embeddings of (Al-Rfou et al., 2013) without any task specific modification for our experiments. A key thing to note is that polyglot word embeddings are not tailored for any specific language analysis task such as POS tagging or NER. These word embeddings are instead trained in a manner favoring the word analogy task introduced by Mikolov et al. (2013c). The polyglot project distributes word embeddings for more than 100 languages. AlRfou et al. (2013) also report results on POS tagging, however, word representations they apply for these experiments are different from the task-agnostic representations they made publicly available.", "startOffset": 215, "endOffset": 657}, {"referenceID": 10, "context": "A final difference to (Collobert et al., 2011) is that we experiment with a much wider range of languages while they report results for English only.", "startOffset": 22, "endOffset": 46}, {"referenceID": 10, "context": "Collobert et al. (2011) propose neural network architectures to four natural language processing tasks, i.", "startOffset": 0, "endOffset": 24}, {"referenceID": 10, "context": "Collobert et al. (2011) propose neural network architectures to four natural language processing tasks, i.e. POS tagging, named entity recognition, semantic role labeling and chunking. Collobert et al. (2011) train word representations on large amounts of unannotated texts from Wikipedia, then update the pre-trained word representations for the individual tasks.", "startOffset": 0, "endOffset": 209}, {"referenceID": 10, "context": "The hand-crafted features they employ for POS tagging and NER are the same as in Collobert et al. (2011) and Turian et al.", "startOffset": 81, "endOffset": 105}, {"referenceID": 10, "context": "The hand-crafted features they employ for POS tagging and NER are the same as in Collobert et al. (2011) and Turian et al. (2010).", "startOffset": 81, "endOffset": 130}, {"referenceID": 22, "context": "The general goal of sparse coding is to express signals in the form of sparse linear combination of basis vectors and the task of finding an appropriate set of basis vectors is referred to as the dictionary learning problem (Mairal et al., 2010).", "startOffset": 224, "endOffset": 245}, {"referenceID": 14, "context": "In Section 4, we compare the effects of employing different sparse coding paradigms including the ones in (Faruqui et al., 2015).", "startOffset": 106, "endOffset": 128}, {"referenceID": 24, "context": "(2016) have proposed an online learning algorithm using regularized dual averaging to directly obtain `1 regularized continuous bag of words (CBOW) representations (Mikolov et al., 2013a) without the need to determine dense CBOW representations first.", "startOffset": 164, "endOffset": 187}, {"referenceID": 14, "context": "Performing sparse coding of word embeddings has recently been proposed by Faruqui et al. (2015), however, the objective function they optimize differs from (1).", "startOffset": 74, "endOffset": 96}, {"referenceID": 14, "context": "Performing sparse coding of word embeddings has recently been proposed by Faruqui et al. (2015), however, the objective function they optimize differs from (1). In Section 4, we compare the effects of employing different sparse coding paradigms including the ones in (Faruqui et al., 2015). In their work, Yogatama et al. (2015) proposed an efficient learning algorithm for determining hierarchically organized sparse word representations using stochastic proximal methods.", "startOffset": 74, "endOffset": 329}, {"referenceID": 14, "context": "Performing sparse coding of word embeddings has recently been proposed by Faruqui et al. (2015), however, the objective function they optimize differs from (1). In Section 4, we compare the effects of employing different sparse coding paradigms including the ones in (Faruqui et al., 2015). In their work, Yogatama et al. (2015) proposed an efficient learning algorithm for determining hierarchically organized sparse word representations using stochastic proximal methods. Most recently, Sun et al. (2016) have proposed an online learning algorithm using regularized dual averaging to directly obtain `1 regularized continuous bag of words (CBOW) representations (Mikolov et al.", "startOffset": 74, "endOffset": 507}, {"referenceID": 22, "context": "Once the dictionary matrix D is learned, the sparse linear combination coefficients \u03b1i can easily be determined for a word embedding vector wi by solving an `1-regularized linear least-squares minimization problem (Mairal et al., 2010).", "startOffset": 214, "endOffset": 235}, {"referenceID": 16, "context": "We then use the previously described set of features in a linear chain CRF (Lafferty et al., 2001) using CRFsuite (Okazaki, 2007).", "startOffset": 75, "endOffset": 98}, {"referenceID": 34, "context": ", 2001) using CRFsuite (Okazaki, 2007).", "startOffset": 23, "endOffset": 38}, {"referenceID": 22, "context": "We primarily rely on the SPArse Modeling Software1 (SPAMS) (Mairal et al., 2010) for performing sparse coding of distributed word representations.", "startOffset": 59, "endOffset": 80}, {"referenceID": 6, "context": "Brown clustering Various studies have identified Brown clustering (Brown et al., 1992) as a useful source of feature generation for sequence labeling tasks (Ratinov and Roth, 2009; Turian et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 40, "context": ", 1992) as a useful source of feature generation for sequence labeling tasks (Ratinov and Roth, 2009; Turian et al., 2010; Owoputi et al., 2013; Stratos and Collins, 2015; Derczynski et al., 2015).", "startOffset": 77, "endOffset": 196}, {"referenceID": 47, "context": ", 1992) as a useful source of feature generation for sequence labeling tasks (Ratinov and Roth, 2009; Turian et al., 2010; Owoputi et al., 2013; Stratos and Collins, 2015; Derczynski et al., 2015).", "startOffset": 77, "endOffset": 196}, {"referenceID": 35, "context": ", 1992) as a useful source of feature generation for sequence labeling tasks (Ratinov and Roth, 2009; Turian et al., 2010; Owoputi et al., 2013; Stratos and Collins, 2015; Derczynski et al., 2015).", "startOffset": 77, "endOffset": 196}, {"referenceID": 42, "context": ", 1992) as a useful source of feature generation for sequence labeling tasks (Ratinov and Roth, 2009; Turian et al., 2010; Owoputi et al., 2013; Stratos and Collins, 2015; Derczynski et al., 2015).", "startOffset": 77, "endOffset": 196}, {"referenceID": 12, "context": ", 1992) as a useful source of feature generation for sequence labeling tasks (Ratinov and Roth, 2009; Turian et al., 2010; Owoputi et al., 2013; Stratos and Collins, 2015; Derczynski et al., 2015).", "startOffset": 77, "endOffset": 196}, {"referenceID": 40, "context": "We employ the frequently used approach of generating features from the length-p (p \u2208 {4, 6, 10, 20}) prefixes of Brown cluster identifiers similar to Ratinov and Roth (2009) and Turian et al.", "startOffset": 150, "endOffset": 174}, {"referenceID": 40, "context": "We employ the frequently used approach of generating features from the length-p (p \u2208 {4, 6, 10, 20}) prefixes of Brown cluster identifiers similar to Ratinov and Roth (2009) and Turian et al. (2010).", "startOffset": 150, "endOffset": 199}, {"referenceID": 20, "context": "In our experiments we use the implementation by Liang (2005) for performing Brown clustering2.", "startOffset": 48, "endOffset": 61}, {"referenceID": 33, "context": "There is a recent initiative of universal dependencies (UD) (Nivre, 2015), which aims at providing a unified notation for multiple linguistic phenomena, including part-of-speech tags as well.", "startOffset": 60, "endOffset": 73}, {"referenceID": 36, "context": "The first notable attempt trying to canonize the multiple tag sets existing is the Google universal part-of-speech tags introduced by Petrov et al. (2012) arguing that the POS tags of various tagging schemes can be mapped to 12 language-independent part-of-speech tags.", "startOffset": 134, "endOffset": 155}, {"referenceID": 33, "context": "There is a recent initiative of universal dependencies (UD) (Nivre, 2015), which aims at providing a unified notation for multiple linguistic phenomena, including part-of-speech tags as well. The POS tag set proposed for UD has 17 partially overlapping categories to the ones defined by Petrov et al. (2012).", "startOffset": 61, "endOffset": 308}, {"referenceID": 7, "context": "We use 12 treebanks in the CoNLL-X format from the CoNLL-2006/07 (Buchholz and Marsi, 2006; Nivre et al., 2007) shared tasks.", "startOffset": 65, "endOffset": 111}, {"referenceID": 31, "context": "We use 12 treebanks in the CoNLL-X format from the CoNLL-2006/07 (Buchholz and Marsi, 2006; Nivre et al., 2007) shared tasks.", "startOffset": 65, "endOffset": 111}, {"referenceID": 7, "context": "We use 12 treebanks in the CoNLL-X format from the CoNLL-2006/07 (Buchholz and Marsi, 2006; Nivre et al., 2007) shared tasks. The complete list of the treebanks included in our experiments is presented in Table 2. We rely on the official scripts released by Petrov et al. (2012)5 for mapping the treebank specific", "startOffset": 66, "endOffset": 279}, {"referenceID": 19, "context": "Analyzing the effects of window size Hyperparameters for training word representations can largely impact their quality as also concluded by Levy et al. (2015). We thus investigate if providing a larger context window size during the training of CBOW, SG and Glove embeddings can improve their utility of being employed in our model.", "startOffset": 141, "endOffset": 160}, {"referenceID": 21, "context": "Differences are the most pronounced in case of skip-gram representation, confirming the findings of Lin et al. (2015), i.", "startOffset": 100, "endOffset": 118}, {"referenceID": 14, "context": "The recent work of Faruqui et al. (2015) formulated alternative approaches to determine sparse word representations.", "startOffset": 19, "endOffset": 41}, {"referenceID": 14, "context": "The recent work of Faruqui et al. (2015) formulated alternative approaches to determine sparse word representations. One of the objective functions Faruqui et al. (2015) apply is", "startOffset": 19, "endOffset": 170}, {"referenceID": 14, "context": "In order to implicitly control for the norms of the basis vectors Faruqui et al. (2015) apply an additional regularization term affected by an extra parameter \u03c4 in their objective function.", "startOffset": 66, "endOffset": 88}, {"referenceID": 14, "context": "In order to implicitly control for the norms of the basis vectors Faruqui et al. (2015) apply an additional regularization term affected by an extra parameter \u03c4 in their objective function. Faruqui et al. (2015) also formulated a constrained objective function of the form", "startOffset": 66, "endOffset": 212}, {"referenceID": 14, "context": "When using the objective functions introduced by Faruqui et al. (2015), we use the default \u03c4 = 10\u22125 value.", "startOffset": 49, "endOffset": 71}, {"referenceID": 14, "context": "When using the objective functions introduced by Faruqui et al. (2015), we use the default \u03c4 = 10\u22125 value. Notationally, we distinguish the sparse coding approaches based on the equation they use as their objective function, i.e. SC-i, i \u2208 {1, 3, 4}. We applied \u03bb = 0.05 for SC-1 and \u03bb = 0.5 for SC-3 and SC-4 in order to obtain word representations of comparable average sparsity levels across the 12 languages, i.e. 95.3%, 94.5% and 95.2%, respectively (cf. the left of Figure 5). The right of Figure 5 further illustrates the spread of POS tagging accuracies over the 12 CoNLL-X treebanks when using models that rely on different sparse coding strategies with comparable sparsity levels. Although Murphy et al. (2012) mentions nonnegativity as a desired property of word representations for cognitive plausibility, Figure 5 reveals that our sequence labeling model cannot benefit from it as the average POS tagging accuracy for SC-4 is 0.", "startOffset": 49, "endOffset": 721}, {"referenceID": 38, "context": "We also present in Table 5 the state-of-the-art results of the bidirectional LSTM models by Plank et al. (2016) for comparative purposes.", "startOffset": 92, "endOffset": 112}, {"referenceID": 38, "context": "The bi-LSTM results are from Plank et al. (2016).", "startOffset": 29, "endOffset": 49}, {"referenceID": 10, "context": "Similar to Collobert et al. (2011) we also apply the 17-tag IOBES tagging scheme during training and inference.", "startOffset": 11, "endOffset": 35}, {"referenceID": 10, "context": "Similar to Collobert et al. (2011) we also apply the 17-tag IOBES tagging scheme during training and inference. The best F1 scores reported for English by Collobert et al. (2011) without employing additional unlabeled texts to enhance their language model is 81.", "startOffset": 11, "endOffset": 179}, {"referenceID": 24, "context": "As Mikolov et al. (2013b) pointed out the similarities of continuous word embeddings across languages, we think that our proposed model could be employed in not just multi-lingual, but cross-lingual language analysis settings.", "startOffset": 3, "endOffset": 26}], "year": 2016, "abstractText": "In this paper we propose and carefully evaluate a sequence labeling framework which solely utilizes sparse indicator features derived from dense distributed word representations. The proposed model obtains (near) state-of-the art performance for both part-ofspeech tagging and named entity recognition for a variety of languages. Our model relies only on a few thousand sparse coding-derived features, without applying any modification of the word representations employed for the different tasks. The proposed model has favorable generalization properties as it retains over 89.8% of its average POS tagging accuracy when trained at 1.2% of the total available training data, i.e. 150 sentences per language.", "creator": "LaTeX with hyperref package"}}}