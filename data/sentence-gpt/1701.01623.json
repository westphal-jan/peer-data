{"id": "1701.01623", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2017", "title": "Cross-Lingual Dependency Parsing with Late Decoding for Truly Low-Resource Languages", "abstract": "In cross-lingual dependency annotation projection, information is often lost during transfer because of early decoding. We present an end-to-end graph-based neural network dependency parser that can be trained to reproduce matrices of edge scores, which can be directly projected across word alignments. We show that our approach to cross-lingual dependency parsing is not only simpler, but also achieves an absolute improvement of 2.9% of the learning time, as well.\n\n\n\nThe main reason the approach was chosen is that we now know that an array of word alignments, when combined with word alignments, are important in the learning time. These changes can then be applied as vectors of the word alignments, and this also allows us to use all the necessary information (numerically, numerically, numerically) in the prediction of a prediction.\nThe general approach is to combine a number of word alignments, and one step at a time. An array of word alignments in an array of word alignments (including word alignments) can then be used for a prediction of the result. The results can be generated in a vector of the word alignments. We then use a single vector of word alignments to estimate the expected result of the prediction.\nThe first step in this approach is to integrate a number of word alignments into the model, where the distribution is defined by a function of the number of word alignments. The function of the number of word alignments is defined by a function of the number of word alignments. It is not a single function; it is a common function.\nWe now have an algorithm that uses vector learning to obtain information about the distribution of word alignments, as well as a data model, called tectal-linear regression. We also integrate the same algorithm as the model in Tectal-Linear Bayes.\nWe also integrate the same algorithm as the model in Tectal-Linear Bayes. We also integrate the same algorithm as the model in Tectal-Linear Bayes. We then use the same algorithm to generate the correct result, as in Tectal-Linear Bayes.", "histories": [["v1", "Fri, 6 Jan 2017 12:54:48 GMT  (98kb,D)", "http://arxiv.org/abs/1701.01623v1", "To be published at EACL 2017"]], "COMMENTS": "To be published at EACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["michael sejr schlichtkrull", "anders s{\\o}gaard"], "accepted": false, "id": "1701.01623"}, "pdf": {"name": "1701.01623.pdf", "metadata": {"source": "CRF", "title": "Cross-Lingual Dependency Parsing with Late Decoding for Truly Low-Resource Languages", "authors": ["Michael Sejr Schlichtkrull", "Anders S\u00f8gaard"], "emails": ["m.s.schlichtkrull@uva.nl", "soegaard@di.ku.dk"], "sections": [{"heading": "1 Introduction", "text": "Dependency parsing is an integral part of many natural language processing systems. However, most research into dependency parsing has focused on learning from treebanks, i.e. collections of manually annotated, well-formed syntactic trees. In this paper, we develop and evaluate a graph-based parser which does not require the training data to be well-formed trees. We show that such a parser has an important application in cross-lingual learning.\nAnnotation projection is a method for developing parsers for low-resource languages, relying on aligned translations from resource-rich source languages into the target language, rather than linguistic resources such as treebanks or dictionaries. The Bible has been translated completely into 542 languages, and partially translated into a further 2344 languages. As such, the assumption that we have access to parallel Bible data, is much less constraining than the assumption of access to linguistic resources. Furthermore, for truly lowresource languages, relying upon the Bible scales\n\u2217Work done while at the University of Copenhagen.\nbetter than relying on less biased data such as the EuroParl corpus.\nIn Agic\u0301 et al. (2016), a projection scheme is proposed wherein labels are collected from many sources, projected into a target language, and then averaged. Crucially, the paper demonstrates how projecting and averaging edge scores from a graph-based parser before decoding improves performance. Even so, decoding is still a requirement between projecting labels and retraining from the projected data, since their parser (TurboParser) requires well-formed input trees. This introduces a potential source of noise and loss of information that may be important for finding the best target sentence parse.\nOur approach circumvents the need for decoding prior to training, thereby surpassing a stateof-the-art dependency parser trained on decoded multi-source annotation projections as done by Agic\u0301 et al. We first evaluate the model across several languages, demonstrating results comparable to the state of the art on the Universal Dependencies (McDonald et al., 2013) dataset. Then, we evaluate the same model by inducing labels from cross-lingual multi-source annotation projection, comparing the performance of a model with early decoding to a model with late decoding.\nContributions We present a novel end-to-end neural graph-based dependency parser and apply it in a cross-lingual setting where the task is to induce models for truly low-resource languages, assuming only parallel Bible text. Our parser is more flexible than similar parsers, and accepts any weighted or non-weighted graph over a token sequence as input. In our setting, the input is a dense weighted graph, and we show that our parser is superior to previous best approaches to cross-lingual parsing. The code is made available on GitHub.1\n1https://github.com/MichSchli/Tensor-LSTM\nar X\niv :1\n70 1.\n01 62\n3v 1\n[ cs\n.C L\n] 6\nJ an\n2 01\n7"}, {"heading": "2 Model", "text": "The goal of this section is to construct a first-order graph-based dependency parser capable of learning directly from potentially incomplete matrices of edge scores produced by another first-order graph-based parser. Our approach is to treat the encoding stage of the parser as a tensor transformation problem, wherein tensors of edge features are mapped to matrices of edge scores. This allows our model to approximate sets of scoring matrices generated by another parser directly through non-linear regression. The core component of the model is a layered sequence of recurrent neural network transformations applied to the axes of an input tensor.\nMore formally, any digraph G = (V,E) can be expressed as a binary |V | \u00d7 |V |-matrix M , where Mij = 1 if and only if (j, i) \u2208 E \u2013 that is, if i has an ingoing edge from j. If G is a tree rooted at v0, v0 has no ingoing edges. Hence, it suffices to use a (|V |\u22121)\u00d7|V |-matrix. In dependency parsing, every sentence is expressed as a matrix S \u2208 Rw\u00d7f , where w is the number of words in the sentence and f is the width of a feature vector corresponding to each word. The goal is to learn a function P : Rw\u00d7f \u2192 Zw\u00d7(w+1)2 , such that P (S) corresponds to the matrix representation of the correct parse tree for that sentence \u2013 see Figure 1 for an example.\nIn the arc-factored (first-order), graph-based model, P is a composite function P = D \u25e6 E where the encoder E : Rw\u00d7f \u2192 Rw\u00d7(w+1) is a real-valued scoring function and the decoder D : Rw\u00d7(w+1) \u2192 Zw\u00d7(w+1)2 is a minimum spanning tree algorithm (McDonald et al., 2005). Commonly, the encoder includes only local information \u2013 that is, Eij is only dependent on Si and\nSj , where Si and Sj are feature vectors corresponding to dependent and head. Our contribution is the introduction of an LSTM-based global encoder where the entirety of S is represented in the calculation of Eij .\nWe begin by extending S to a (w+1)\u00d7(f+1)matrix S\u2217 with an additional row corresponding to the root node and a single binary feature denoting whether a node is the root.We now compute a 3-tensor F = S S\u2217 of dimension w \u00d7 (w + 1) \u00d7 (2f + 1) consisting of concatenations of all combinations of rows in S and S\u2217. This tensor effectively contains a featurization of every edge (u, v) in the complete digraph over the sentence, consisting of the features of the parent word u and child word v. These edge-wise feature vectors are organized in the tensor exactly as the dependency arcs in a parse matrix such as the one shown in the example in Figure 1.\nThe edges represented by elements Fij can as such easily be interpreted in the context of related edges represented by the row i and the column j in which that edge occurs. The classical arc-factored parsing algorithm of McDonald et al. (2005) corresponds to applying a function O : R2f+1 \u2192 R pointwise to S S\u2217, then decoding the resulting w \u00d7 (w + 1)-matrix. Our model diverges by applying an LSTM-based transformation Q : Rw\u00d7(w+1)\u00d7(2f+1) \u2192 Rw\u00d7(w+1)\u00d7d to S S\u2217 before applying an analogous transformation O : Rd \u2192 R.\nThe Long Short-Term Memory (LSTM) unit is a function LSTM(x, ht\u22121, ct\u22121) = (ht, ct) defined through the use of several intermediary steps, following Hochreiter et al. (2001). A concatenated input vector I = x \u2295 hprev is constructed, where \u2295 represents vector concatenation. Then, functions corresponding to input, forget, and output gates are defined following the form ginput = \u03c3(WinputI+binput). Finally, the internal cell state ct and the output vector ht at time t are defined using the Hadamard (pointwise) product \u2022:\nct = gforget \u2022 cprev + ginput \u2022 tanh(WcellI + bcell) ht = goutput \u2022 tanh(ct)\nWe define a function Matrix-LSTM inductively, that applies an LSTM to the rows of a matrix X . Formally, Matrix-LSTM is a function M : Ra\u00d7b \u2192 Ra\u00d7c such that (h1, c1) = LSTM(X1, 0, 0), \u22001 < i \u2264 n (hi, ci) = LSTM(Xi, hi\u22121, ci\u22121), andM(X)i = hi.\nAn effective extension is the bidirectional LSTM, wherein the LSTM-function is applied to the sequence both in the forward and in the backward direction, and the results are concatenated. In the matrix formulation, reversing a sequence corresponds to inverting the order of the rows. This is most naturally accomplished through leftmultiplication with an exchange matrix Jm \u2208 Rm\u00d7m such that:\nJm = 0 \u00b7 \u00b7 \u00b7 1... ... ... 1 \u00b7 \u00b7 \u00b7 0  Bidirectional Matrix-LSTM is therefore defined as a functionM2d : Ra\u00d7b \u2192 Ra\u00d72c such that:\nM2d(S) =M(S)\u22952 JaM(JaS)\nWhere \u22952 refers to concatenation along the second axis of the matrix.\nKeeping in mind the goal of constructing a tensor transformationQ capable of propagating information in an LSTM-like manner between any two elements of the input tensor, we are interested in constructing an equivalent of the Matrix-LSTMmodel operating on 3-tensors rather than matrices. This construct, when applied to the edge tensor F = S S\u2217, can then provide a means of interpreting edges in the context of related edges.\nA very simple variant of such an LSTMfunction operating on 3-tensors can be constructed by applying a bidirectional Matrix-LSTM to every matrix along the first axis of the tensor. This forms\nthe center of our approach. Formally, bidirectional Tensor-LSTM is a function T2d : Ra\u00d7b\u00d7c \u2192 Ra\u00d7b\u00d72h such that:\nT2d(T )i =M2d(Ti)\nThis definition allows information to flow within the matrices of the first axis of the tensor, but not between them \u2013 corresponding in Figure 2 to horizontal connection along the rows, but no vertical connections along the columns. To fully cover the tensor structure, we must extend this model to include connections along columns.\nThis is accomplished through tensor transposition. Formally, tensor transposition is an operator T T\u03c3 where \u03c3 is a permutation on the set {1, ..., rank(T )}. The last axis of the tensor contains the feature representations, which we are not interested in scrambling. For the Matrix-LSTM, this leaves only one option \u2013 MT (1,2). When the LSTM is operating on a 3-tensor, we have two options \u2013 T T (2,1,3) and T T (1,2,3). This leads to the following definition of four-directional TensorLSTM as a function T4d : Ra\u00d7b\u00d7c \u2192 Ra\u00d7b\u00d74h analogous to bi-directional Sequence-LSTMs:\nT4d(T ) = T2d(T )\u22953 T2d(T T (2,1,3))T (2,1,3)\nCalculating the LSTM-function on T T (1,2,3) and T T (2,1,3) can be thought of as constructing the recurrent links either \u201dside-wards\u201d or \u201ddownwards\u201d in the tensor \u2013 or, equivalently, constructing recurrent links either between the outgoing or between the in-going edges of every vertex in\nthe dependency graph. In Figure 2, we illustrate the two directions respectively with full or dotted edges in the hidden layer.\nThe output of Tensor-LSTM is itself a tensor. In our experiments, we use a multi-layered variation implemented by stacking layers of models: T4d,stack(T ) = T4d(T4d(...T4d(T )...)). We do not share parameters between stacked layers. Training the model is done by minimizing the value E(G,O(Q(S S\u2217))) of some loss function E for each sentence S with gold tensor G. We experiment with two loss functions.\nIn our monolingual set-up, we exploit the fact that parse matrices by virtue of depicting trees are right stochastic matrices. Following this observation, we constrain each row of O(Q(S S\u2217)) under a softmax-function and use as loss the rowwise cross entropy. In our cross-lingual set-up, we use mean squared error. In both cases, predictiontime decoding is done with Chu-Liu-Edmonds algorithm (Edmonds, 1968) following McDonald et al. (2005)."}, {"heading": "3 Cross-lingual parsing", "text": "Hwa et al. (2005) is a seminal paper for crosslingual dependency parsing, but they use very detailed heuristics to ensure that the projected syntactic structures are well-formed. Agic\u0301 et al. (2016) is the latest continuation of their work, presenting a new approach to cross-lingual projection, projecting edge scores rather than subtrees. Agic\u0301 et al. (2016) construct target-language treebanks by aggregating scores from multiple source languages, before decoding. Averaging before decoding is especially beneficial when the parallel data is of low quality, as the decoder introduces errors, when edge scores are missing. Despite averaging, there will still be scores missing from the input weight matrices, especially when the source and target languages are very distant. Below we show that we can circumvent error-inducing early decoding by training directly on the projected edge scores.\nWe assume source language datasets L1, ...,Ln, parsed by monolingual arc-factored parsers, In our case, this data comes from the Bible. We assume access to a set of sentence alignment functions As : Ls \u00d7 Lt \u2192 R0,1 where As(Ss, St) is the confidence that St is the translation of Ss. Similarly, we have access to a set of word alignment functions WLs,Ss,St : Ss \u00d7 St \u2192 R0,1 such that\nSs \u2208 Ls, St \u2208 Lt, and W (ws, wt) represents the confidence that ws aligns to wt given that St is the translation of Ss\nFor each source language Ls with a scoring function scoreLs , we define a local edge-wise voting function voteSs((us, vs), (ut, vt)) operating on a source language edge (us, vs) \u2208 Ss and a target language edge (ut, ut) \u2208 St. Intuitively, every source language edge votes for every target language edge with a score proportional to the confidence of the edges aligning and the score given in the source language. For every target language edge (ut, vt) \u2208 St:\nvoteSs((us, vs), (ut, vt)) =WLs,Ss,St(us, ut)\n\u00b7WLs,Ss,St(vs, vt) \u00b7 scoreLs(us, vs)\nFollowing Agic\u0301 et al. (2016), a sentence-wise voting function is then constructed as the highest contribution from a source-language edge:\nvoteSs(ut, vt) = max us,vs\u2208Ss voteSs((us, vs), (ut, vt))\nThe final contribution of each source language datasetLs to a target language edge (ut, vt) is then calculated as the sum for all sentences Ss \u2208 Ls over voteSs(ut, vt) multiplied by the confidence that the source language sentence aligns with the target language sentence. For an edge (ut, vt) in a target language sentence St \u2208 Lt:\nvoteLs(ut, vt) = \u2211 Ss\u2208Ls As(Ss, St) voteSs(ut, vt)\nFinally, we can compute a target language scoring function by summing over the votes for every source language:\nscore(ut, vt) =\nn\u2211 i=1 voteLi(ut, vt)\nZSt\nHere, ZSt is a normalization constant ensuring that the target-language scores are proportional to those created by the source-language scoring functions. As such, ZSt should consist of the sum over the weights for each sentence contributing to the scoring function. We can compute this as:\nZSt = n\u2211 i=1 \u2211 Ss\u2208Li As(Ss, St)\nThe sentence alignment function is not a probability distribution; it may be the case that no sourcelanguage sentences contribute to a target language sentence, causing the sum of the weights and the sum of the votes to approach zero. In this case, we define score(ut, vt) = 0. Before projection, the source language scores are all standardized to have 0 as the mean and 1 as the standard deviation. Hence, this corresponds to assuming neither positive nor negative evidence concerning the edge.\nWe experiment with two methods of learning from the projected data \u2013 decoding with Chu-LiuEdmonds algorithm and then training as proposed in Agic\u0301 et al. (2016), or directly learning to reproduce the matrices of edge scores. For alignment, we use the sentence-level hunalign algorithm introduced in Varga et al. (2005) and the token-level model presented in O\u0308stling (2015)."}, {"heading": "4 Experiments", "text": "We conduct two sets of experiments. First, we evaluate the Tensor-LSTM-parser in the monolingual setting. We compare Tensor-LSTM to the TurboParser (Martins et al., 2010) on several languages from the Universal Dependencies dataset. In the second experiment, we evaluate TensorLSTM in the cross-lingual setting. We include as baselines the delexicalized parser of McDonald et al. (2011), and the approach of Agic\u0301 et al. (2016) using TurboParser. To demonstrate the effectiveness of circumventing the decoding step, we conduct the cross-lingual evaluation of Tensor-LSTM using cross entropy loss with early decoding, and using mean squared loss with late decoding."}, {"heading": "4.1 Model selection and training", "text": "Our features consist of 500-dimensional word embeddings trained on translations of the Bible. The word embeddings were trained using skipgram with negative sampling on a word-by-sentence PMI matrix induced from the Edinburgh Bible Corpus, following (Levy et al., 2017). Our embeddings are not trainable, but fixed representations throughout the learning process. Unknown tokens were represented by zero-vectors.\nWe combined the word embeddings with onehot-encodings of POS-tags, projected across word alignments following the method of Agic\u0301 et al. (2016). To verify the value of the POS-features, we conducted preliminary experiments on English development data. When including POS-\ntags, we found small, non-significant improvements for monolingual parsing, but significant improvements for cross-lingual parsing.\nThe weights were initialized using the normalized values suggested in Glorot and Bengio (2010). Following Jozefowicz et al. (2015), we add 1 to the initial forget gate bias. We trained the network using RMSprop (Tieleman and Hinton, 2012) with hyperparameters \u03b1 = 0.1 and \u03b3 = 0.9, using minibatches of 64 sentences. Following Neelakantan et al. (2015), we added a noise factor n \u223c N (0, 1\n(1+t)0.55 ) to the gradient in each\nupdate. We applied dropouts after each LSTMlayer with a dropout probability p = 0.5, and between the input layer and the first LSTM-layer with a dropout probability of p = 0.2 (Bluche et al., 2015). As proposed in Pascanu et al. (2012), we employed a gradient clipping factor of 15. In the monolingual setting, we used early stopping on the development set.\nWe experimented with 10, 50, 100, and 200 hidden units per layer, and with up to 6 layers. Using greedy search on monolingual parsing and evaluating on the English development data, we determined the optimal network shape to contain 100 units per direction per hidden layer, and a total of 4 layers.\nFor the cross-lingual setting, we used two additional hyper-parameters. We used the development data from one of our target languages (German) to determine the optimal number of epochs before stopping. Furthermore, we trained only on a subset of the projected sentences, choosing the size of the subset using the development data.\nWe experimented with either 5000 or 10000 randomly sampled sentences. There are two motivating factors behind this subsampling. First, while the Bible in general consists of about 30000 sentences, for many low-resource languages we do not have access to annotation projections for the full Bible, because parts were never translated, and because of varying projection quality. Second, subsampling speeds up the training, which was necessary to make our experiments practical: At 10000 sentences and on a single GPU, each epoch takes approximately 2.5 hours. As such, training for a single language could be completed in less than a day. We plot the results in Figure 3. We see that the best performance is achieved at 10000 sentences, and with respectively 6 and 5 epochs for cross entropy and mean squared loss."}, {"heading": "4.2 Results", "text": "In the monolingual setting, we compare our parser to TurboParser (Martins et al., 2010) \u2013 a fast, capable graph-based parser used as a component in many larger systems. TurboParser is also the system of choice for the cross-lingual pipeline of Agic\u0301 et al. (2016). It is therefore interesting to make a direct comparison between the two. The results can be seen in Table 1.\nNote that in order for a parser to be directly applicable to the annotation projection setup explored in the secondary experiment, it must be a first-order graph-based parser. In the monolingual setting, the best results reported so far (84.74, on average) for the above selection of treebanks were by the Parsito system (Straka et al., 2015), a transition-based parser using a dynamic oracle.\nFor the cross-lingual annotation projection experiments, we use the delexicalized system suggested by McDonald et al. (2011) as a baseline. We also compare against the annotation projection scheme using TurboParser suggested in Agic\u0301 et al. (2016), representing the previous state of the art for truly low-resource cross-lingual dependency parsing. Note that while our results for the TurboParser-based system use the same training data, test data, and model as in Agic\u0301 et al., our results differ due to the use of the Bible corpus rather than a Watchtower publications corpus as parallel data. The authors made results available using the Edinburgh Bible Corpus for unlabeled data. The two tested conditions of Tensor-LSTM are the mean squared loss model without intermediary decoding, and the cross entropy model with intermediary decoding. The results of the crosslingual experiment can be seen in Table 2."}, {"heading": "5 Discussion", "text": "As is evident from Table 2, the variation in performance across different languages is large for all systems. This is to be expected, as the quality of the projected label sets vary widely due to linguistic differences. On average, Tensor-LSTM with mean squared loss outperforms all other systems. In Section 1, we hypothesized that incomplete projected scorings would have a larger impact upon systems reliant on an intermediary decoding step. To investigate this claim, we plot in Figure 4 the performance difference with mean squared loss and cross entropy loss for each language versus the percentage of missing edge scores.\nFor languages outside the Germanic and Latin families, our claim holds \u2013 the performance of the cross entropy loss system decreases faster with the percentage of missing labels than the performance of the mean squared loss system. To an extent, this confirms our hypothesis, as we for the average language observe an improvement by circumventing the decoding step. French and Spanish, however, do not follow the same trend, with cross entropy loss outperforming mean squared loss despite the high number of missing labels.\nIn Table 2, performance on French and Spanish for both systems can be seen to be very high. It may be the case that indo-european target languages are not as affected by missing labels as most of the source languages are themselves indoeuropean. Another explanation could be that some feature of the cross entropy loss function makes it especially well suited for Latin languages \u2013 as seen in Table 1, French and Spanish are also two of the languages for which Tensor-LSTM yields the highest performance improvement.\nTo compare the effect of missing edge scores upon performance without influence from linguistic factors such as language similarity, we repeat the cross-lingual experiment on one language with respectively 10%, 20%, 30%, and 40% of the projected and averaged edge scores artificially set to 0, simulating missing data. We choose the English data for this experiment, as the English projected data has the lowest percentage of missing labels\nacross any of the languages. In Figure 5, we plot the performance for each of the two systems versus the percentage of deleted values.\nAs can be clearly seen, performance drops faster with the percentage of deleted labels for the cross entropy model. This confirms our intuition that the initially lower performance using mean squared loss compared to cross entropy loss is mitigated by a greater robustness towards missing labels, gained by circumventing the decoding step in the training process. In Table 2, this is reflected as dramatic performance increases using mean squared error for Finnish, Persian, Hindi, and Hebrew \u2013 the four languages furthest removed\nfrom the predominantly indoeuropean source languages and therefore the four languages with the poorest projected label quality.\nSeveral possible avenues for future work on this project are available. In this paper, we used an extremely simple feature function. More complex feature functions is one potential source of improvement. Another interesting direction for future work would be to include POS-tagging directly as a component of Tensor-LSTM prior to the construction of S S\u2217 in a multi-task learning framework. Similarly, incorporating semantic tasks on top of dependency parsing could lead to interesting results. Finally, extensions of the Tensor-LSTM function to deeper models, wider models, or more connected models as seen in e.g. Kalchbrenner et al. (2015) may yield further performance gains."}, {"heading": "6 Related Work", "text": "Experiments with neural networks for dependency parsing have focused mostly on learning higherorder scoring functions and creating efficient feature representations, with the notable exception of Fonseca et al. (2015). In their paper, a convolutional neural network is used to evaluate local edge scores based on global information. In Zhang and Zhao (2015) and Pei et al. (2015), neural networks are used to simultaneously evaluate first-order and higher-order scores for graph-based parsing, demonstrating good results. Bidirectional LSTM-models have been successfully applied to feature generation (Kiperwasser and Goldberg, 2016). Such LSTM-based features could in future work be employed and trained in conjunction with Tensor-LSTM, incorporating global information both in parsing and in featurization.\nAn extension of LSTM to tensor-structured data has been explored in Graves et al. (2007), and further improved upon in Kalchbrenner et al. (2015) in the form of GridLSTM. Our approach is similar, but simpler and computationally more efficient as no within-layer connections between the first and the second axes of the tensor are required.\nAnnotation projection for dependency parsing has been explored in a number of papers, starting with Hwa et al. (2005). In Tiedemann (2014) and Tiedemann (2015) the process in extended and evaluated across many languages. Li et al. (2014) follows the method of Hwa et al. (2005) and adds a probabilistic target-language classifier to deter-\nmine and filter out high-uncertainty trees. In Ma and Xia (2014), performance on projected data is used as an additional objective for unsupervised learning through a combined loss function.\nA common thread in these papers is the use of high-quality parallel data such as the EuroParl corpus. For truly low-resource target languages, this setting is unrealistic as parallel resources may be restricted to biased data such as the Bible. In Agic\u0301 et al. (2016) this problem is addressed, and a parser is constructed which utilizes averaging over edge posteriors for many source languages to compensate for low-quality projected data. Our work builds upon their contribution by constructing a more flexible parser which can bypass a source of bias in their projected labels, and we therefore compared our results directly to theirs.\nAnnotation projection procedures for crosslingual dependency parsing has been the focus of several other recent papers (Guo et al., 2015; Zhang and Barzilay, 2015; Duong et al., 2015; Rasooli and Collins, 2015). In Guo et al. (2015), distributed, language-independent feature representations are used to train shared parsers. Zhang and Barzilay (2015) introduce a tensor-based feature representation capable of incorporating prior knowledge about feature interactions learned from source languages. In Duong et al. (2015), a neural network parser is built wherein higher-level layers are shared between languages.\nFinally, Rasooli and Collins (2015) leverage dense information in high-quality sentence translations to improve performance. Their work can be seen as opposite to ours \u2013 whereas Rasooli and Collins leverage high-quality translations to improve performance when such are available, we focus on improving performance in the absence of high-quality translations."}, {"heading": "7 Conclusion", "text": "We have introduced a novel algorithm for graphbased dependency parsing based on an extension of sequence-LSTM to the more general TensorLSTM. We have shown how the parser with a cross entropy loss function performs comparably to state of the art for monolingual parsing. Furthermore, we have demonstrated that the flexibility of our parser enables learning from non wellformed data and from the output of other parsers. Using this property, we have applied our parser to a cross-lingual annotation projection problem\nfor truly low-resource languages, demonstrating an average target-language unlabeled attachment score of 48.54, which to the best of our knowledge are the best results yet for the task."}, {"heading": "Acknowledgments", "text": "The second author was supported by ERC Starting Grant No. 313695."}], "references": [{"title": "Multilingual projection for parsing truly low-resource languages", "author": ["\u017deljko Agi\u0107", "Anders Johannsen", "Barbara Plank", "H\u00e9ctor Mart\u0131\u0301nez Alonso", "Natalie Schluter", "Anders S\u00f8gaard"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Agi\u0107 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agi\u0107 et al\\.", "year": 2016}, {"title": "Where to apply dropout in recurrent neural networks for handwriting recognition", "author": ["Theodore Bluche", "Christopher Kermorvant", "Jerome Louradour"], "venue": "In Document Analysis and Recognition (ICDAR),", "citeRegEx": "Bluche et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bluche et al\\.", "year": 2015}, {"title": "Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser", "author": ["Long Duong", "Trevor Cohn", "Steven Bird", "Paul Cook."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Duong et al\\.,? 2015", "shortCiteRegEx": "Duong et al\\.", "year": 2015}, {"title": "Optimum branchings", "author": ["Jack Edmonds."], "venue": "Mathematics and the Decision Sciences, Part 1, pages 335\u2013345. American Mathematical Society.", "citeRegEx": "Edmonds.,? 1968", "shortCiteRegEx": "Edmonds.", "year": 1968}, {"title": "A deep architecture for non-projective dependency parsing", "author": ["Erick R Fonseca", "Avenida Trabalhador S\u00e3o-carlense", "Sandra M Alu\u0131\u0301sio"], "venue": "In Proceedings of the 2015 NAACL-HLT Workshop on Vector Space Modeling for NLP,", "citeRegEx": "Fonseca et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fonseca et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "Proceedings of the 2010 International conference on Artificial Intelligence and Statistics, pages 249\u2013256. Society for Artificial Intelligence", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Multi-dimensional recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "J\u00fcrgen Schmidhuber."], "venue": "arXiv preprint arXiv:0705.2011.", "citeRegEx": "Graves et al\\.,? 2007", "shortCiteRegEx": "Graves et al\\.", "year": 2007}, {"title": "Cross-lingual dependency parsing based on distributed representations", "author": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Guo et al\\.,? 2015", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi", "J\u00fcrgen Schmidhuber."], "venue": "A Field Guide to Dynamic Recurrent Neural Networks. IEEE press.", "citeRegEx": "Hochreiter et al\\.,? 2001", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Bootstrapping parsers via syntactic projection across parallel texts", "author": ["Rebecca Hwa", "Philip Resnik", "Amy Weinberg", "Clara Cabezas", "Okan Kolak."], "venue": "Natural language engineering, 11(03):311\u2013325.", "citeRegEx": "Hwa et al\\.,? 2005", "shortCiteRegEx": "Hwa et al\\.", "year": 2005}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings of the 32nd International Conference on Machine Learning, pages 2342\u20132350. International Machine Learn-", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves."], "venue": "arXiv preprint arXiv:1507.01526.", "citeRegEx": "Kalchbrenner et al\\.,? 2015", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "arXiv preprint arXiv:1603.04351.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "A strong baseline for learning cross-lingual word representations from sentence alignments", "author": ["Omer Levy", "Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "EACL.", "citeRegEx": "Levy et al\\.,? 2017", "shortCiteRegEx": "Levy et al\\.", "year": 2017}, {"title": "Soft cross-lingual syntax projection for dependency parsing", "author": ["Zhenghua Li", "Min Zhang", "Wenliang Chen."], "venue": "Proceedings of the 25th International Conference on Computational Linguistics, pages 783\u2013793. Association for Computational Linguis-", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Unsupervised dependency parsing with transferring distribution via parallel guidance and entropy regularization", "author": ["Xuezhe Ma", "Fei Xia."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1337\u2013", "citeRegEx": "Ma and Xia.,? 2014", "shortCiteRegEx": "Ma and Xia.", "year": 2014}, {"title": "Turbo parsers: Dependency parsing by approximate variational inference", "author": ["Andr\u00e9 FT Martins", "Noah A Smith", "Eric P Xing", "Pedro MQ Aguiar", "M\u00e1rio AT Figueiredo."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural", "citeRegEx": "Martins et al\\.,? 2010", "shortCiteRegEx": "Martins et al\\.", "year": 2010}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["Ryan McDonald", "Fernando Pereira", "Kiril Ribarov", "Jan Haji\u010d."], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Pro-", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Multi-source transfer of delexicalized dependency parsers", "author": ["Ryan McDonald", "Slav Petrov", "Keith Hall."], "venue": "Proceedings of the 2011 Conference on", "citeRegEx": "McDonald et al\\.,? 2011", "shortCiteRegEx": "McDonald et al\\.", "year": 2011}, {"title": "Universal dependency annotation for multilingual parsing", "author": ["Ryan T McDonald", "Joakim Nivre", "Yvonne QuirmbachBrundage", "Yoav Goldberg", "Dipanjan Das", "Kuzman Ganchev", "Keith B Hall", "Slav Petrov", "Hao Zhang", "Oscar T\u00e4ckstr\u00f6m"], "venue": null, "citeRegEx": "McDonald et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2013}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V Le", "Ilya Sutskever", "Lukasz Kaiser", "Karol Kurach", "James Martens."], "venue": "arXiv preprint arXiv:1511.06807.", "citeRegEx": "Neelakantan et al\\.,? 2015", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Bayesian models for multilingual word alignment", "author": ["Robert \u00d6stling."], "venue": "Ph.D. thesis, Department of Linguistics, Stockholm University.", "citeRegEx": "\u00d6stling.,? 2015", "shortCiteRegEx": "\u00d6stling.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1211.5063.", "citeRegEx": "Pascanu et al\\.,? 2012", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "An effective neural network model for graph-based dependency parsing", "author": ["Wenzhe Pei", "Tao Ge", "Baobao Chang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Confer-", "citeRegEx": "Pei et al\\.,? 2015", "shortCiteRegEx": "Pei et al\\.", "year": 2015}, {"title": "Density-driven cross-lingual transfer of dependency parsers", "author": ["Mohammad Sadegh Rasooli", "Michael Collins."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 328\u2013338. Association for Com-", "citeRegEx": "Rasooli and Collins.,? 2015", "shortCiteRegEx": "Rasooli and Collins.", "year": 2015}, {"title": "Parsing universal dependency treebanks using neural networks and search-based oracle", "author": ["Milan Straka", "Jan Haji\u010d", "Jana Strakov\u00e1", "Jan Haji\u010d jr."], "venue": "Proceedings of the 14th International Workshop on Treebanks and Linguistic Theories, pages 208\u2013220.", "citeRegEx": "Straka et al\\.,? 2015", "shortCiteRegEx": "Straka et al\\.", "year": 2015}, {"title": "Rediscovering annotation projection for cross-lingual parser induction", "author": ["J\u00f6rg Tiedemann."], "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1854\u20131864. Association for Com-", "citeRegEx": "Tiedemann.,? 2014", "shortCiteRegEx": "Tiedemann.", "year": 2014}, {"title": "Cross-lingual dependency parsing with universal dependencies and predicted pos labels", "author": ["J\u00f6rg Tiedemann."], "venue": "Proceedings of the Third International Conference on Dependency Linguistics, pages 340\u2013 349.", "citeRegEx": "Tiedemann.,? 2015", "shortCiteRegEx": "Tiedemann.", "year": 2015}, {"title": "Parallel corpora for medium density languages", "author": ["D\u00e1niel Varga", "P\u00e9ter Hal\u00e1csy", "Andr\u00e1s Kornai", "Viktor Nagy", "L\u00e1szl\u00f3 N\u00e9meth", "Viktor Tr\u00f3n."], "venue": "Proceedings of the 2005 Conference on Recent Advances in Natural Language Processing. Associa-", "citeRegEx": "Varga et al\\.,? 2005", "shortCiteRegEx": "Varga et al\\.", "year": 2005}, {"title": "Hierarchical low-rank tensors for multilingual transfer parsing", "author": ["Yuan Zhang", "Regina Barzilay."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1857\u20131867. Association for Computational Linguis-", "citeRegEx": "Zhang and Barzilay.,? 2015", "shortCiteRegEx": "Zhang and Barzilay.", "year": 2015}, {"title": "High-order graph-based neural dependency parsing", "author": ["Zhisong Zhang", "Hai Zhao."], "venue": "Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation, pages 114\u2013 123. Association for Computational Linguistics.", "citeRegEx": "Zhang and Zhao.,? 2015", "shortCiteRegEx": "Zhang and Zhao.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In Agi\u0107 et al. (2016), a projection scheme is proposed wherein labels are collected from many sources, projected into a target language, and", "startOffset": 3, "endOffset": 22}, {"referenceID": 19, "context": "eral languages, demonstrating results comparable to the state of the art on the Universal Dependencies (McDonald et al., 2013) dataset.", "startOffset": 103, "endOffset": 126}, {"referenceID": 17, "context": "In the arc-factored (first-order), graph-based model, P is a composite function P = D \u25e6 E where the encoder E : Rw\u00d7f \u2192 Rw\u00d7(w+1) is a real-valued scoring function and the decoder D : Rw\u00d7(w+1) \u2192 Z 2 is a minimum spanning tree algorithm (McDonald et al., 2005).", "startOffset": 234, "endOffset": 257}, {"referenceID": 16, "context": "The classical arc-factored parsing algorithm of McDonald et al. (2005) corresponds to applying a function O : R2f+1 \u2192 R pointwise to S S\u2217, then decoding the resulting w \u00d7 (w + 1)-matrix.", "startOffset": 48, "endOffset": 71}, {"referenceID": 8, "context": "The Long Short-Term Memory (LSTM) unit is a function LSTM(x, ht\u22121, ct\u22121) = (ht, ct) defined through the use of several intermediary steps, following Hochreiter et al. (2001). A concatenated input vector I = x \u2295 hprev is constructed, where \u2295 represents vector concatenation.", "startOffset": 149, "endOffset": 174}, {"referenceID": 3, "context": "In both cases, predictiontime decoding is done with Chu-Liu-Edmonds algorithm (Edmonds, 1968) following McDonald et al.", "startOffset": 78, "endOffset": 93}, {"referenceID": 3, "context": "In both cases, predictiontime decoding is done with Chu-Liu-Edmonds algorithm (Edmonds, 1968) following McDonald et al. (2005).", "startOffset": 60, "endOffset": 127}, {"referenceID": 0, "context": "Agi\u0107 et al. (2016) is the latest continuation of their work, pre-", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "Agi\u0107 et al. (2016) construct target-language treebanks by aggregating scores from multiple source languages, before decoding.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "Following Agi\u0107 et al. (2016), a sentence-wise voting function is then constructed as the highest contribution from a source-language edge:", "startOffset": 10, "endOffset": 29}, {"referenceID": 0, "context": "We experiment with two methods of learning from the projected data \u2013 decoding with Chu-LiuEdmonds algorithm and then training as proposed in Agi\u0107 et al. (2016), or directly learning to reproduce the matrices of edge scores.", "startOffset": 141, "endOffset": 160}, {"referenceID": 27, "context": "troduced in Varga et al. (2005) and the token-level model presented in \u00d6stling (2015).", "startOffset": 12, "endOffset": 32}, {"referenceID": 21, "context": "(2005) and the token-level model presented in \u00d6stling (2015).", "startOffset": 46, "endOffset": 61}, {"referenceID": 16, "context": "We compare Tensor-LSTM to the TurboParser (Martins et al., 2010) on several languages from the Universal Dependencies dataset.", "startOffset": 42, "endOffset": 64}, {"referenceID": 0, "context": "(2011), and the approach of Agi\u0107 et al. (2016) using TurboParser.", "startOffset": 28, "endOffset": 47}, {"referenceID": 13, "context": "The word embeddings were trained using skipgram with negative sampling on a word-by-sentence PMI matrix induced from the Edinburgh Bible Corpus, following (Levy et al., 2017).", "startOffset": 155, "endOffset": 174}, {"referenceID": 0, "context": "We combined the word embeddings with onehot-encodings of POS-tags, projected across word alignments following the method of Agi\u0107 et al. (2016). To verify the value of the POS-features, we conducted preliminary experiments on English development data.", "startOffset": 124, "endOffset": 143}, {"referenceID": 10, "context": "Following Jozefowicz et al. (2015), we add 1 to the initial forget gate bias.", "startOffset": 10, "endOffset": 35}, {"referenceID": 20, "context": "lowing Neelakantan et al. (2015), we added a noise factor n \u223c N (0, 1 (1+t)0.", "startOffset": 7, "endOffset": 33}, {"referenceID": 1, "context": "2 (Bluche et al., 2015).", "startOffset": 2, "endOffset": 23}, {"referenceID": 1, "context": "2 (Bluche et al., 2015). As proposed in Pascanu et al. (2012), we employed a gradient clipping factor of 15.", "startOffset": 3, "endOffset": 62}, {"referenceID": 16, "context": "In the monolingual setting, we compare our parser to TurboParser (Martins et al., 2010) \u2013 a fast, ca-", "startOffset": 65, "endOffset": 87}, {"referenceID": 0, "context": "TurboParser is also the system of choice for the cross-lingual pipeline of Agi\u0107 et al. (2016). It is therefore interesting to make a direct comparison between the two.", "startOffset": 75, "endOffset": 94}, {"referenceID": 25, "context": "74, on average) for the above selection of treebanks were by the Parsito system (Straka et al., 2015), a transition-based parser using a dynamic oracle.", "startOffset": 80, "endOffset": 101}, {"referenceID": 16, "context": "For the cross-lingual annotation projection experiments, we use the delexicalized system suggested by McDonald et al. (2011) as a baseline.", "startOffset": 102, "endOffset": 125}, {"referenceID": 0, "context": "We also compare against the annotation projection scheme using TurboParser suggested in Agi\u0107 et al. (2016), representing the previous state of the art for truly low-resource cross-lingual dependency parsing.", "startOffset": 88, "endOffset": 107}, {"referenceID": 16, "context": "We include the results of two baselines \u2013 the delexicalized system of McDonald et al. (2011) and the Turbo-based projection scheme of Agi\u0107 et al.", "startOffset": 70, "endOffset": 93}, {"referenceID": 0, "context": "(2011) and the Turbo-based projection scheme of Agi\u0107 et al. (2016). English and German development data was used for hyperparameter tuning (marked *).", "startOffset": 48, "endOffset": 67}, {"referenceID": 11, "context": "Kalchbrenner et al. (2015) may yield further per-", "startOffset": 0, "endOffset": 27}, {"referenceID": 4, "context": "ture representations, with the notable exception of Fonseca et al. (2015). In their paper, a convolutional neural network is used to evaluate local edge scores based on global information.", "startOffset": 52, "endOffset": 74}, {"referenceID": 4, "context": "ture representations, with the notable exception of Fonseca et al. (2015). In their paper, a convolutional neural network is used to evaluate local edge scores based on global information. In Zhang and Zhao (2015) and Pei et al.", "startOffset": 52, "endOffset": 214}, {"referenceID": 4, "context": "ture representations, with the notable exception of Fonseca et al. (2015). In their paper, a convolutional neural network is used to evaluate local edge scores based on global information. In Zhang and Zhao (2015) and Pei et al. (2015), neu-", "startOffset": 52, "endOffset": 236}, {"referenceID": 12, "context": "Bidirectional LSTM-models have been successfully applied to feature generation (Kiperwasser and Goldberg, 2016).", "startOffset": 79, "endOffset": 111}, {"referenceID": 6, "context": "An extension of LSTM to tensor-structured data has been explored in Graves et al. (2007), and further improved upon in Kalchbrenner et al.", "startOffset": 68, "endOffset": 89}, {"referenceID": 6, "context": "An extension of LSTM to tensor-structured data has been explored in Graves et al. (2007), and further improved upon in Kalchbrenner et al. (2015) in the form of GridLSTM.", "startOffset": 68, "endOffset": 146}, {"referenceID": 9, "context": "Annotation projection for dependency parsing has been explored in a number of papers, starting with Hwa et al. (2005). In Tiedemann (2014) and Tiedemann (2015) the process in extended and evaluated across many languages.", "startOffset": 100, "endOffset": 118}, {"referenceID": 9, "context": "Annotation projection for dependency parsing has been explored in a number of papers, starting with Hwa et al. (2005). In Tiedemann (2014) and Tiedemann (2015) the process in extended and evaluated across many languages.", "startOffset": 100, "endOffset": 139}, {"referenceID": 9, "context": "Annotation projection for dependency parsing has been explored in a number of papers, starting with Hwa et al. (2005). In Tiedemann (2014) and Tiedemann (2015) the process in extended and evaluated across many languages.", "startOffset": 100, "endOffset": 160}, {"referenceID": 9, "context": "Annotation projection for dependency parsing has been explored in a number of papers, starting with Hwa et al. (2005). In Tiedemann (2014) and Tiedemann (2015) the process in extended and evaluated across many languages. Li et al. (2014) follows the method of Hwa et al.", "startOffset": 100, "endOffset": 238}, {"referenceID": 9, "context": "Annotation projection for dependency parsing has been explored in a number of papers, starting with Hwa et al. (2005). In Tiedemann (2014) and Tiedemann (2015) the process in extended and evaluated across many languages. Li et al. (2014) follows the method of Hwa et al. (2005) and adds a probabilistic target-language classifier to determine and filter out high-uncertainty trees.", "startOffset": 100, "endOffset": 278}, {"referenceID": 9, "context": "Annotation projection for dependency parsing has been explored in a number of papers, starting with Hwa et al. (2005). In Tiedemann (2014) and Tiedemann (2015) the process in extended and evaluated across many languages. Li et al. (2014) follows the method of Hwa et al. (2005) and adds a probabilistic target-language classifier to determine and filter out high-uncertainty trees. In Ma and Xia (2014), performance on projected data is used as an additional objective for unsupervised learning through a combined loss function.", "startOffset": 100, "endOffset": 403}, {"referenceID": 0, "context": "In Agi\u0107 et al. (2016) this problem is addressed, and a parser is constructed which utilizes averaging over edge posteriors for many source languages to compensate for low-quality projected data.", "startOffset": 3, "endOffset": 22}, {"referenceID": 7, "context": "lingual dependency parsing has been the focus of several other recent papers (Guo et al., 2015; Zhang and Barzilay, 2015; Duong et al., 2015; Rasooli and Collins, 2015).", "startOffset": 77, "endOffset": 168}, {"referenceID": 29, "context": "lingual dependency parsing has been the focus of several other recent papers (Guo et al., 2015; Zhang and Barzilay, 2015; Duong et al., 2015; Rasooli and Collins, 2015).", "startOffset": 77, "endOffset": 168}, {"referenceID": 2, "context": "lingual dependency parsing has been the focus of several other recent papers (Guo et al., 2015; Zhang and Barzilay, 2015; Duong et al., 2015; Rasooli and Collins, 2015).", "startOffset": 77, "endOffset": 168}, {"referenceID": 24, "context": "lingual dependency parsing has been the focus of several other recent papers (Guo et al., 2015; Zhang and Barzilay, 2015; Duong et al., 2015; Rasooli and Collins, 2015).", "startOffset": 77, "endOffset": 168}, {"referenceID": 2, "context": ", 2015; Zhang and Barzilay, 2015; Duong et al., 2015; Rasooli and Collins, 2015). In Guo et al. (2015), distributed, language-independent feature represen-", "startOffset": 34, "endOffset": 103}, {"referenceID": 28, "context": "Zhang and Barzilay (2015) introduce a tensor-based feature representation capable of incorporating prior knowledge about feature interactions learned from source languages.", "startOffset": 0, "endOffset": 26}, {"referenceID": 2, "context": "In Duong et al. (2015), a neural network parser is built wherein higher-level layers", "startOffset": 3, "endOffset": 23}, {"referenceID": 24, "context": "Finally, Rasooli and Collins (2015) leverage dense information in high-quality sentence translations to improve performance.", "startOffset": 9, "endOffset": 36}], "year": 2017, "abstractText": "In cross-lingual dependency annotation projection, information is often lost during transfer because of early decoding. We present an end-to-end graph-based neural network dependency parser that can be trained to reproduce matrices of edge scores, which can be directly projected across word alignments. We show that our approach to cross-lingual dependency parsing is not only simpler, but also achieves an absolute improvement of 2.25% averaged across 10 languages compared to the previous state of the art.", "creator": "TeX"}}}