{"id": "1701.01887", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jan-2017", "title": "Deep Learning for Time-Series Analysis", "abstract": "In many real-world application, e.g., speech recognition or sleep stage classification, data are captured over the course of time, constituting a Time-Series or Time-Series. This makes for an application that has a very large amount of processing power.\n\n\n\nThe ability to use data from the app to perform tasks, such as reading an article, checking for new messages, writing a report, and writing a report is a useful tool when performing tasks or having a particular type of task, which is not necessary.\nThe amount of processing power, which is required for tasks, depends on the amount of time that an application has spent processing the data. For example, processing the first item in the browser and using the last item in the app, as well as the last item in the data, can take as much as three minutes.\nA number of tools have been built for this use in general.\nSee the table below for details on the number of processing power available for some applications and the number of task times it takes to complete a task.\nNote that the number of tasks being performed is only limited by the number of processing power being added to the application.\nSee also: Creating and using JavaScript in JavaScript\nNote that the number of tasks being used in the App Engine provides a number of different performance constraints.\nThe user can only use the number of processing power being added to the application.", "histories": [["v1", "Sat, 7 Jan 2017 21:44:04 GMT  (63kb,D)", "http://arxiv.org/abs/1701.01887v1", "Written as part of the Seminar on Collaborative Intelligence in the TU Kaiserslautern. January 2016"]], "COMMENTS": "Written as part of the Seminar on Collaborative Intelligence in the TU Kaiserslautern. January 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["john cristian borges gamboa"], "accepted": false, "id": "1701.01887"}, "pdf": {"name": "1701.01887.pdf", "metadata": {"source": "CRF", "title": "Deep Learning for Time-Series Analysis", "authors": ["John Gamboa"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Artificial Neural Networks, Deep Learning, Time-Series"}, {"heading": "1 Introduction", "text": "Artificial Neural Networks (ANN), since their origin in 1943 [46], have been used to solve a large range of problems as diverse as robotic processing [32], object recognition [56], speech and handwriting recognition [19], and even real time sign-language translation [2]. Despite the intuition that deeper architectures would yield better results than the then more commonly used shallow ones, empirical tests with deep networks had found similar or even worse results when compared to networks with only one or two layers [57] (for more details, see [6]). Additionally, training was found to be difficult and often inefficient [6]. La\u0308ngkvist [34] argues that this scenario started to change with the proposal of greedy layer-wise unsupervised learning [22], which allowed for the fast learning of Deep Belief Networks, while also solving the vanishing gradients problem [7]. Latest deep architectures use several modules that are trained separately and stacked together so that the output of the first one is the input of the next one.\nFrom stock market prices to the spread of an epidemic, and from the recording of an audio signal to sleep monitoring, it is common for real world data to be registered taking into account some notion of time. When collected together, the measurements compose what is known as a Time-Series. For different fields, suitable applications vary depending on the nature and purpose of the data:\nar X\niv :1\n70 1.\n01 88\n7v 1\n[ cs\n.L G\n] 7\nJ an\n2 01\n7\nwhile doctors can be interested in searching for anomalies in the sleep patterns of a patient, economists may be more interested in forecasting the next prices some stocks of interest will assume. These kinds of problems are addressed in the literature by a range of different approches (for a recent review of the main techniques applied to perform tasks such as Classification, Segmentation, Anomaly Detection and Prediction, see [14]).\nThis paper reviews some of the recently presented approaches to performing tasks related to Time-Series using Deep Learning architectures. It is important, therefore, to have a formal definition of Time-Series. Malhotra et al. [44] defined Time-Series as a vector X = {x(1),x(2), . . . ,x(n)}, where each element x(t) \u2208 Rm pertaining to X is an array of m values such that {x(t)1 , x (t) 2 , . . . , x (t) m }. Each one of the m values correspond to the input variables measured in the time-series. The rest of this paper is structured as follows: Section 1.1 introduces basic types of Neural Network (NN) modules that are often used to build deep neural structures. Section 2 describes how the present paper relates to other works in the literature. Sections 3, 4 and 5 describe some approaches using Deep Learning to perform Modeling, Classification and Anomaly Detection in Time-Series data, respectively. Finally, Section 6 concludes the paper."}, {"heading": "1.1 Artificial Neural Network", "text": "This section explains the basic concepts related to ANN. The types of networks described here are by no means the only kinds of ANN architectures found in the literature. The reader is referred to [49] for a thorough description of architectural alternatives such as Restricted Boltzmann Machines (RBM), Hopfield Networks and Auto-Encoders, as well as for a detailed explanation of the Backpropagation algorithm. Additionally, we refer the reader to [19] for applications of RNN as well as more details on the implementation of a LSTM, and to [54] for details on CNN.\nAn ANN is basically a network of computing units linked by directed connections. Each computing unit performs some calculation and outputs a value that is then spread through all its outgoing connections as input into other units. Connections normally have weights that correspond to how strong two units are linked. Typically, the computation performed by a unit is separated into two stages: the aggregation and the activation functions. Applying the aggregation function commonly corresponds to calculating the sum of the inputs received by the unit through all its incoming connections. The resulting value is then fed into the activation function. It commonly varies in different network architectures, although popular choices are the logistic sigmoid (\u03c3(x) = 11+e\u2212x ) and the hyperbolic tangent (tanh(x) = 21+e\u22122x \u2212 1) functions. Recently, rectified linear units employing a ramp function (R(x) = max(0, x)) have become increasingly popular.\nThe input of the network is given in a set of input computing units which compose an input layer. Conversely, the output of the network are the values output by the units composing the output layer. All other units are called hidden and are often also organized in layers (see Figure 1b for an example network).\nThe focus of learning algorithms is frequently on deciding what weights would cause the network to output, given some input, the expected values. A popular learning algorithm is the Backpropagation algorithm [51], whereby the gradient of an error function is calculated and the weights are iteratively set so as to minimize the error.\nConvolutional Neural Network (CNN) A network that is too big and with layers that are fully connected can become infeasible to train. Also trained with the Backpropagation algorithm, CNNs [36] are common for image processing tasks and reduce the number of parameters to be learned by limiting the number of connections of the neurons in the hidden layer to only some of the input neurons (i.e., a local area of the input image). A hidden layer (in this case, also called a convolutional layer \u2013 see Figure 1a) is composed by several groups of neurons. The weights of all neurons in a group are shared. Each group is generally composed by as many neurons as needed to cover the entire image. This way, it is as if each group of neurons in the hidden layer calculated a convolution of the image with their weights, resulting in a \u201cprocessed\u201d version of the image. We call this convolution a feature.\nCommonly, pooling is applied to the resulting filtered images. The tecnique allows for achieving some translation invariance of the learned features. The groups (containing a newly processed version of the input image) are divided in chunks (e.g., 2 \u00d7 2) and their maximum value is taken. This results in yet another version of the input image, now smaller than the original size (in the example, 1/4 of the size of the group).\nThese steps can be repeatedly applied as many times as desired: a new convolutional layer can be applied on the pooled layer, followed by another pooling layer, and so forth. Finally, when the layers become small enough, it is common to have fully connected layers before the output layer.\nTiled Convolutional Neural Network The usage of shared weights in a group allow for the translation invariance of the learned features. However, \u201cit prevents the pooling units from capturing more complex invariances, such as scale and rotation invariance\u201d [48]. To solve this problem, Tiled CNNs allow for a group to be divided into subgroups called tiles, each of which can have separate weights. A parameter k defines how many tiles each group has: neurons that are exactly k steps away from each other share the same weights.\nFully Convolutional Networks (FCN) While the pooling operation performed by CNNs makes sense for object recognition tasks, because it has the advantage of achieving some robustness to small shifts of the learned features, it is not suited for tasks like Semantic Segmentation, where the goal is to segment the pixels of the image according to the objects that they refer to. FCNs [42] allow for the input and output layers to have the same dimensions by introducing \u201ca decoder stage that is consisted of upsampling, convolution, and rectified linear units layers, to the CNN architecture\u201d [47].\nRecurrent Neural Network (RNN) When the network has loops, it is called a RNN. It is possible to adapt the Backpropagation algorithm to train a recurrent network, by \u201cunfolding\u201d the network through time and constraining some of the connections to always hold the same weights [51].\nLong Short-Term Memory (LSTM) One problem that arises from the unfolding of an RNN is that the gradient of some of the weights starts to become too small or too large if the network is unfolded for too many time steps. This is called the vanishing gradients problem [7]. A type of network architecture that solves this problem is the LSTM [23]. In a typical implementation, the hidden layer is replaced by a complex block (see Figure 1c) of computing units composed by gates that trap the error in the block, forming a so-called \u201cerror carrousel\u201d."}, {"heading": "2 Literature Review", "text": "Independently of Deep Learning, analysis of Time-Series data have been a popular subject of interest in other fields such as Economics, Engineering and Medicine. Traditional techniques on manipulating such data can be found in [21], and the application of traditional ANN techniques on this kind of data is described in [5].\nMost work using ANN to manipulate Time-Series data focuses on modeling and forecasting. As an early attempt on using ANN for such tasks, [9] modelled flour prices over the range of 8 years. Still in the 90\u2019s, [29] delineated eight steps on \u201cdesigning a neural network forecast model using economic time series data\u201d. More recent approaches include usage of Elman RNNs to predict chaotic TimeSeries [10], employing ANN ensemble methods for forecasting Internet traffic [12], using simple Multilayer Perceptrons for modeling the amount of littering in the North Sea [53], and implementing in FPGA a prediction algorithm using Echo State Networks for \u201cexploiting the inherent parallelism of these systems\u201d [3].\nHybrid approaches to Time-Series analysis utilizing ANN are not uncommon. [31] presents a model for Time-Series forecasting using ANN and ARIMA models, and [15] applies the same kinds of models to water quality time series prediction. In still other examples of the same ideas, [30] compares the performance of ARIMA models and ANNs to make short-term predictions on photovoltaic power generators, while [38] compares both models with the performance of Multivariate Adaptive Regression Splines. [13] performs Time-Series forecasting by using a hybrid fuzzy model: while the Fuzzy C-means method is utilized for fuzzification, ANN are employed for defuzzification. Finally, [24] forecasts the speed of the wind using a hybrid of Support Vector Machines, Ensemble Empirical Mode Decomposition and Partial Autocorrelation Function.\nDespite being relatively new, the field of Deep Learning has attracted a lot of interest in the past few years. A very thorough review of the entire history of developments that led the field to its current state can be found in [52], while a higher focus on the novelties from the last decade is given in [4]. We proceed to a review of the applications of Deep Learning to Time-Series data.\nClassification The task of Classification of any type of data has benefited by the advent of CNNs. Previously existing methods for classification generally relied on the usage of domain specific features normally crafted manually by human experts. Finding the best features was the subject of a lot of research and the performance of the classifier was heavily dependent on their quality. The advantage of CNNs is that they can learn such features by themselves, reducing the need for human experts [34].\nAn example of the application of such unsupervised feature learning for the classification of audio signals is presented in [37]. In [1], the features learned by the CNN are used as input to a Hidden Markov Model, achieving a drop at the error rate of over 10%. The application of CNNs in these works presuppose the constraint that the Time-Series is composed of only one channel. An architecture that solves this constraint is presented in [60].\nIn [18] the performance of CNNs is compared with that of LSTM for the classification of Visual and Haptic Data in a robotics setting, and in [28] the signals produced by wearable sensors are transformed into images so that Deep CNNs can be used for classification.\nRelevant to Tiled CNNs was the development of Independent Component Analysis (ICA) [27]. Several alternative methods for calculating independent components can be found in the literature (e.g., [17], [55] or [25]). Tiled CNNs are normally trained with a variation of such technique that looses the assumption that each component is statistically independent and tries to find a topographic order between them: the Topographic ICA [26].\nForecasting Several different Deep Learning approaches can be found in the literature for performing Forecasting tasks. For example, Deep Belief Networks are used in the work of [33] along with RBM. [58] also compares the performance of Deep Belief Networks with that of Stacked Denoising Autoencoders. This last type of network is also employed by [50] to predict the temperature of an indoor environment. Another application of Time-Series forecasting can be found in [43], which uses Stacked Autoencoders to predict the flow of traffic from a Big Data dataset.\nA popular application to the task of Time-Series prediction is on Weather Forecasting. In [41], some preliminary predictions on weather data provided by The Hong Kong Observatory are made through the usage of Stacked Autoencoders. In a follow up work, the authors use similar ideas to perform predictions on Big Data [40]. Instead of Autoencoders, [20] uses Deep Belief Networks for constructing a hybrid model in which the ANN models the joint distribution between the weather predictors variables.\nAnomaly Detection Work applying Deep Learning techniques to Anomaly Detection detection of Time-Series data is not very abundant in the literature. It is still difficult to find works such as [16], that uses Stacked Denoising Autoencoders to perform Anomaly Detection of trajectories obtained from low level tracking algorithms.\nHowever, there are many similarities between Anomaly Detection and the previous two tasks. For example, identifying an anomaly could be transformed into a Classification task, as was done in [35]. Alternatively, detecting an anomaly could be considered the same as finding regions in the Time-Series for which the forecasted values are too different from the actual ones."}, {"heading": "3 Deep Learning for Time-Series Modeling", "text": "In this section the work presented in [47] is reviewed. As discussed above, FCNs are a modification of the CNN architecture that, as required by some TimeSeries related problems, allows for the input and output signals to have the same dimensions.\nMittelman [47] argues that the architecture of the FCN resembles the application of a wavelet transform, and that for this reason, it can present strong variations when the input signal is subject to small translations. To solve this problem, and inspired by the undecimeated wavelet transform, which is translation invariant, they propose the Undecimated Fully Convolutional Neural Network (UFCNN), also translation invariant.\nThe only difference between an FCN and an UFCNN is that the UFCNN removes both the upsampling and pooling operators from the FCN architecture. Instead, the \u201cfilters at the lth resolution level are upsampled by a factor of 2l\u22121 along the time dimension\u201d. See Figure 2b for a graphical representation of the proposed architecture.\nThe performance of the UFCNN is tested in three different experiments. In the first experiment, \u201c2000 training sequences, 50 validation sequences and 50 testing sequences, each of length 5000 time-steps\u201d are automatically generated by a probabilistic algorithm. The values of the Time-Series represent the position of a target object moving inside a bounded square. The performance of the UFCNN in estimating the position of the object at each time-step is compared to that of a FCN, a LSTM, and a RNN, and the UFCNN does perform better in most cases.\nIn a second experiment, the \u201cMUSE\u201d and \u201cNOTTINGHAM\u201d datasets1 area used. The goal is to predict the values of the Time-Series in the next timestep. In both cases, the UFCNN outperforms the competing networks: a RNN, a Hessian-Free optimization-RNN [45], and a LSTM.\nFinally, the third experiment uses a trading dataset2, where the goal is, given only information about the past, to predict the set of actions that would \u201cmaximize the profit\u201d. In a comparison to a RNN, the UFCNN again yielded the best results.\n1 available at http://www-etud.iro.umontreal.ca/ boulanni/icml2012 2 available at http://www.circulumvite.com/home/trading-competition"}, {"heading": "4 Deep Learning for Time-Series Classification", "text": "Wang and Oates [59] presented an approach for Time-Series Classification using CNN-like networks. In order to benefit from the high accuracy that CNNs have achieved in the past few years on image classification tasks, the authors propose the idea of transforming a Time-Series into an image.\nTwo approaches are presented. The first one generates a Gramian Angular Field (GAF), while the second generates a Markov Transition Field (MTF). In both cases, the generation of the images increases the size of the Time-Series, making the images potentially prohibitively large. The authors therefore propose strategies to reduce their size without loosing too much information. Finally, the two types of images are combined in a two-channel image that is then used to produce better results than those achieved when using each image separately. In the next sections, GAF and MTF are described.\nIn the equations below, we suppose that m = 1. The Time-Series is therefore composed by only real-valued observations, such that referring to x(i) \u2208 X is the same as referring to x(i) \u2208 X."}, {"heading": "4.1 Gramian Angular Field", "text": "The first step on generating a GAF is to rescale the entire Time-Series into values between [\u22121, 1]. In the equation 1, max(X) and min(X) represent the maximum and minimum real-values present in the Time-Series X:\nx\u0303(i) = (x(i) \u2212max(X)) + (x(i) \u2212max(X))\nmax(X)\u2212min(X) (1)\nThe next step is to recode the newly created Time-Series X\u0303 into polar coordinates. The angle is encoded by x(i) and the radius is encoded by the the time stamp i.\nNotice that, because the values x(i) were rescaled, no information is lost by the usage of arccos(x\u0303(i)) in 2.{\n\u03c6 = arccos(x\u0303(i)), \u22121 \u2264 x\u0303(i) \u2264 1, x\u0303(i) \u2208 X\u0303 r = iN , i \u2208 N\n(2)\nFinally, the GAF is defined as follows:\nG =  cos(\u03c61 + \u03c61) \u00b7 \u00b7 \u00b7 cos(\u03c61 + \u03c6n) cos(\u03c62 + \u03c61) \u00b7 \u00b7 \u00b7 cos(\u03c62 + \u03c6n) ... . . .\n... cos(\u03c6n + \u03c61) \u00b7 \u00b7 \u00b7 cos(\u03c6n + \u03c6n)  (3) Here, some information is lost by the fact that \u03c6 no more belongs to the interval [0, \u03c0]. When trying to recover the Time-Series from the image, there may be some errors introduced."}, {"heading": "4.2 Markov Transition Field", "text": "The creation of the Markov Transition Field is based on the ideas proposed in [8] for the definition of the so-called Markov Transition Matrix (MTM).\nFor a Time-Series X, the first step is defining Q quantile bins. Each x(i) is then assigned to the corresponding bin qj . The Markov Transition Matrix is the matrix W composed by elements wij such that \u2211 j wij = 1 and wij corresponds to the normalized \u201cfrequency with which a point in the quantile qj is followed by a point in the quantile qi.\u201d This is a Q\u00d7Q matrix.\nThe MTF is the n\u00d7n matrix M . Each pixel of M contains a value from W . The value in the pixel ij is the likelihood (as calculated when constructing W ) of going from the bin in which the pixel i is to the bin in which the pixel j is:\nM =  wij|x1\u2208qi,x1\u2208qj \u00b7 \u00b7 \u00b7 wij|x1\u2208qi,xn\u2208qj wij|x2\u2208qi,x1\u2208qj \u00b7 \u00b7 \u00b7 wij|x2\u2208qi,xn\u2208qj ... . . .\n... wij|xn\u2208qi,x1\u2208qj \u00b7 \u00b7 \u00b7 wij|xn\u2208qi,xn\u2208qj\n (4)"}, {"heading": "4.3 Performing Classification with the Generated Images", "text": "The authors use Tiled CNNs to perform classifications using the images. In the reported experiments, both methods are assessed separately in 12 \u201chard\u201d datasets \u201con which the classification error rate is above 0.1 with the state-of-theart SAX-BoP approach\u201d [39], which are 50Words, Adiac, Beef, Coffee, ECG200, Face (all), Lightning-2, Lightning-7, OliveOil, OSU Leaf, Swedish Leaf and Yoga [11]. The authors then suggest the usage of both methods as \u201ccolors\u201d of the images. The performance of the resulting classifier is competitive against many of the state-of-the-art classifiers, which are also reported by the authors."}, {"heading": "5 Deep Learning for Time-Series Anomaly Detection", "text": "Anomaly Detection can be easily transformed into a task where the goal is to model the Time-Series and, given this model, find regions where the predicted values are too different from the actual ones (or, in other words, where the probability of the observed region is too low). This is the idea implemented by the paper reviewed in this section [44].\nThrough the learned model, not all m input variables need to be predicted. The learned model predicts, at any given time-step, l vectors with d input variables, where 1 \u2264 d \u2264 m.\nThe modeling of the Time-Series is done through the application of a Stacked LSTM architecture. The network has m input neurons (one for each input variable) and d \u00d7 l output neurons (one neuron for each one of the d predicted variables of the l vectors that are predicted at a time-step). The hidden layers are composed by LSTM units that are \u201cfully connected through recurrent connections\u201d. Additionally, any hidden unit is fully connected to all units in the hidden layer above it. Figure 2a sketches the proposed architecture.\nFor each one of the predictions and each one of the d predicted variables, a prediction error is calculated. The errors are then used to fit a multivariate Guassian distribution, and a probability p(t) is assigned to each observation. Any observation whose probability p(t) < \u03c4 is treated as an anomaly.\nThe approach was tested in four real-world datasets. One of them, called Multi-sensor engine data is not publicly available. The other three datasets (Electrocardiograms (ECGs), Space Suttle Marotta valve time series, and Power demand dataset) are available for download3. The results demonstrated a significant improvement in capturing long-term dependencies when compared to simpler RNN-based implementations."}, {"heading": "6 Conclusion", "text": "When applying Deep Learning, one seeks to stack several independent neural network layers that, working together, produce better results than the already existing shallow structures. In this paper, we have reviewed some of these modules, as well the recent work that has been done by using them, found in the literature. Additionally, we have discussed some of the main tasks normally performed when manipulating Time-Series data using deep neural network structures.\nFinally, a more specific focus was given on one work performing each one of these tasks. Employing Deep Learning to Time-Series analysis has yielded results in these cases that are better than the previously existing techniques, which is an evidence that this is a promising field for improvement.\nAcknowledgments. I would like to thank Ahmed Sheraz and Mohsin Munir for their guidance and contribution to this paper."}], "references": [{"title": "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition", "author": ["O. Abdel-Hamid", "Mohamed", "A.r.", "H. Jiang", "G. Penn"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. pp. 4277\u20134280. IEEE", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Real-time malaysian sign language translation using colour segmentation and neural network", "author": ["R. Akmeliawati", "M.P.L. Ooi", "Y.C. Kuang"], "venue": "Instrumentation and Measurement Technology Conference Proceedings, 2007. IMTC 2007. IEEE. pp. 1\u20136. IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Fpgabased stochastic echo state networks for time-series forecasting", "author": ["M.L. Alomar", "V. Canals", "N. Perez-Mora", "V. Mart\u0301\u0131nez-Moll", "J.L. Rossell\u00f3"], "venue": "Computational Intelligence and Neuroscience 501, 537267", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep machine learning-a new frontier in artificial intelligence research [research frontier", "author": ["I. Arel", "D.C. Rose", "T.P. Karnowski"], "venue": "Computational Intelligence Magazine, IEEE 5(4), 13\u201318", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Neural network time series forecasting of financial markets", "author": ["E.M. Azoff"], "venue": "John Wiley & Sons, Inc.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H Larochelle"], "venue": "Advances in neural information processing systems 19, 153", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Neural Networks, IEEE Transactions on 5(2), 157\u2013166", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1994}, {"title": "Duality between time series and networks", "author": ["A. Campanharo", "M.I. Sirer", "R.D. Malmgren", "F.M. Ramos", "L.A. Amaral"], "venue": "PloS one 6(8), e23378", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Forecasting the behavior of multivariate time series using neural networks", "author": ["K. Chakraborty", "K. Mehrotra", "C.K. Mohan", "S. Ranka"], "venue": "Neural networks 5(6), 961\u2013970", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Cooperative coevolution of elman recurrent neural networks for chaotic time series prediction", "author": ["R. Chandra", "M. Zhang"], "venue": "Neurocomputing 86, 116\u2013123", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "The ucr time series classification archive (July 2015), www.cs.ucr.edu/~eamonn/time_ series_data", "author": ["Y. Chen", "E. Keogh", "B. Hu", "N. Begum", "A. Bagnall", "A. Mueen", "G. Batista"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Multi-scale internet traffic forecasting using neural networks and time series methods", "author": ["P. Cortez", "M. Rio", "M. Rocha", "P. Sousa"], "venue": "Expert Systems 29(2), 143\u2013155", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Fuzzy time series forecasting with a novel hybrid approach combining fuzzy c-means and neural networks", "author": ["E. Egrioglu", "C.H. Aladag", "U. Yolcu"], "venue": "Expert Systems with Applications 40(3), 854\u2013857", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Time-series data mining", "author": ["P. Esling", "C. Agon"], "venue": "ACM Computing Surveys (CSUR) 45(1), 12", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "A hybrid neural network and arima model for water quality time series prediction", "author": ["D.\u00d6. Faruk"], "venue": "Engineering Applications of Artificial Intelligence 23(4), 586\u2013594", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "A novel approach for trajectory feature representation and anomalous trajectory detection", "author": ["W. Feng", "C. Han"], "venue": "Information Fusion (Fusion), 2015 18th International Conference on. pp. 1093\u20131099. IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Non-linear independent component analysis using series reversion and weierstrass network", "author": ["P. Gao", "W. Woo", "S. Dlay"], "venue": "IEE Proceedings-Vision, Image and Signal Processing 153(2), 115\u2013131", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep learning for tactile understanding from visual and haptic data", "author": ["Y. Gao", "L.A. Hendricks", "K.J. Kuchenbecker", "T. Darrell"], "venue": "arXiv preprint arXiv:1511.06065", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised sequence labelling with recurrent neural networks, vol", "author": ["A Graves"], "venue": "385. Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "A deep hybrid model for weather forecasting", "author": ["A. Grover", "A. Kapoor", "E. Horvitz"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp. 379\u2013386. ACM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Time series analysis, vol", "author": ["J.D. Hamilton"], "venue": "2. Princeton university press Princeton", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1994}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation 18(7), 1527\u20131554", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8), 1735\u20131780", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}, {"title": "A hybrid forecasting approach applied to wind speed time series", "author": ["J. Hu", "J. Wang", "G. Zeng"], "venue": "Renewable Energy 60, 185\u2013194", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast and robust fixed-point algorithms for independent component analysis", "author": ["A. Hyv\u00e4rinen"], "venue": "Neural Networks, IEEE Transactions on 10(3), 626\u2013634", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1999}, {"title": "Topographic independent component analysis", "author": ["A. Hyv\u00e4rinen", "P. Hoyer", "M. Inki"], "venue": "Neural computation 13(7), 1527\u20131558", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Independent component analysis: algorithms and applications", "author": ["A. Hyv\u00e4rinen", "E. Oja"], "venue": "Neural networks 13(4), 411\u2013430", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}, {"title": "Human activity recognition using wearable sensors by deep convolutional neural networks", "author": ["W. Jiang", "Z. Yin"], "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia Conference. pp. 1307\u20131310. ACM", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Designing a neural network for forecasting financial and economic time series", "author": ["I. Kaastra", "M. Boyd"], "venue": "Neurocomputing 10(3), 215\u2013236", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1996}, {"title": "Application of time series and artificial neural network models in short-term forecasting of pv power generation", "author": ["E.G. Kardakos", "M.C. Alexiadis", "S. Vagropoulos", "C.K. Simoglou", "P.N. Biskas", "Bakirtzis", "A.G"], "venue": "Power Engineering Conference (UPEC), 2013 48th International Universities\u2019. pp. 1\u20136. IEEE", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "A novel hybridization of artificial neural networks and arima models for time series forecasting", "author": ["M. Khashei", "M. Bijari"], "venue": "Applied Soft Computing 11(2), 2664\u2013 2675", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Neural network architectures for robotic applications", "author": ["S.Y. King", "J.N. Hwang"], "venue": "Robotics and Automation, IEEE Transactions on 5(5), 641\u2013657", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1989}, {"title": "Time series forecasting using a deep belief network with restricted boltzmann machines", "author": ["T. Kuremoto", "S. Kimura", "K. Kobayashi", "M. Obayashi"], "venue": "Neurocomputing 137, 47\u201356", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling time-series with deep networks", "author": ["M. L\u00e4ngkvist"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Sleep stage classification using unsupervised feature learning", "author": ["M. L\u00e4ngkvist", "L. Karlsson", "A. Loutfi"], "venue": "Advances in Artificial Neural Systems 2012, 5", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11), 2278\u20132324", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1998}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["H. Lee", "P. Pham", "Y. Largman", "A.Y. Ng"], "venue": "Advances in neural information processing systems. pp. 1096\u20131104", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "Forecasting tourism demand using time series, artificial neural networks and multivariate adaptive regression splines: evidence from taiwan", "author": ["C.J. Lin", "H.F. Chen", "T.S. Lee"], "venue": "International Journal of Business Administration 2(2), p14", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Rotation-invariant similarity in time series using bag-ofpatterns representation", "author": ["J. Lin", "R. Khade", "Y. Li"], "venue": "Journal of Intelligent Information Systems 39(2), 287\u2013315", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural network modeling for big data weather forecasting", "author": ["J.N. Liu", "Y. Hu", "Y. He", "P.W. Chan", "L. Lai"], "venue": "Information Granularity, Big Data, and Computational Intelligence, pp. 389\u2013408. Springer", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural network based feature representation for weather forecasting", "author": ["J.N. Liu", "Y. Hu", "J.J. You", "P.W. Chan"], "venue": "Proceedings on the International Conference on Artificial Intelligence (ICAI). p. 1. The Steering Committee of The World Congress in Computer Science, Computer Engineering and Applied Computing (WorldComp)", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "arXiv preprint arXiv:1411.4038", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Traffic flow prediction with big data: a deep learning approach", "author": ["Y. Lv", "Y. Duan", "W. Kang", "Z. Li", "F.Y. Wang"], "venue": "Intelligent Transportation Systems, IEEE Transactions on 16(2), 865\u2013873", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11). pp. 1033\u20131040", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "A logical calculus of the ideas immanent in nervous activity", "author": ["W.S. McCulloch", "W. Pitts"], "venue": "The bulletin of mathematical biophysics 5(4), 115\u2013133", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1943}, {"title": "Time-series modeling with undecimated fully convolutional neural networks", "author": ["R. Mittelman"], "venue": "arXiv preprint arXiv:1508.00317", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Tiled convolutional neural networks", "author": ["J. Ngiam", "Z. Chen", "D. Chia", "P.W. Koh", "Q.V. Le", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems. pp. 1279\u2013 1287", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Neural networks: a systematic introduction", "author": ["R. Rojas"], "venue": "Springer Science & Business Media", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}, {"title": "Time-series forecasting of indoor temperature using pre-trained deep neural networks", "author": ["P. Romeu", "F. Zamora-Mart\u0301\u0131nez", "P. Botella-Rocamora", "J. Pardo"], "venue": "Artificial Neural Networks and Machine Learning\u2013ICANN 2013, pp. 451\u2013458. Springer", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning representations by backpropagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Cognitive modeling 5, 3", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1988}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks 61, 85\u2013117", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Artificial neural networks for modeling time series of beach litter in the southern north sea", "author": ["M. Schulz", "M. Matthies"], "venue": "Marine environmental research 98, 14\u201320", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding convolutional neural networks", "author": ["D. Stutz"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2014}, {"title": "Fast and precise independent component analysis for high field fmri time series tailored using prior information on spatiotemporal structure", "author": ["K. Suzuki", "T. Kiryu", "T. Nakada"], "venue": "Human brain mapping 15(1), 54\u201366", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2002}, {"title": "Deep neural networks for object detection", "author": ["C. Szegedy", "A. Toshev", "D. Erhan"], "venue": "Advances in Neural Information Processing Systems. pp. 2553\u20132561", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}, {"title": "Practical issues in temporal difference learning", "author": ["G. Tesauro"], "venue": "Springer", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1992}, {"title": "Time series analysis using deep feed forward neural networks", "author": ["J.T. Turner"], "venue": "Ph.D. thesis, University of Maryland, Baltimore County", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2014}, {"title": "Encoding time series as images for visual inspection and classification using tiled convolutional neural networks", "author": ["Z. Wang", "T. Oates"], "venue": "Workshops at the TwentyNinth AAAI Conference on Artificial Intelligence", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2015}, {"title": "Time series classification using multi-channels deep convolutional neural networks", "author": ["Y. Zheng", "Q. Liu", "E. Chen", "Y. Ge", "J.L. Zhao"], "venue": "Web-Age Information Management, pp. 298\u2013310. Springer", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 44, "context": "Artificial Neural Networks (ANN), since their origin in 1943 [46], have been used to solve a large range of problems as diverse as robotic processing [32], object recognition [56], speech and handwriting recognition [19], and even real time sign-language translation [2].", "startOffset": 61, "endOffset": 65}, {"referenceID": 31, "context": "Artificial Neural Networks (ANN), since their origin in 1943 [46], have been used to solve a large range of problems as diverse as robotic processing [32], object recognition [56], speech and handwriting recognition [19], and even real time sign-language translation [2].", "startOffset": 150, "endOffset": 154}, {"referenceID": 54, "context": "Artificial Neural Networks (ANN), since their origin in 1943 [46], have been used to solve a large range of problems as diverse as robotic processing [32], object recognition [56], speech and handwriting recognition [19], and even real time sign-language translation [2].", "startOffset": 175, "endOffset": 179}, {"referenceID": 18, "context": "Artificial Neural Networks (ANN), since their origin in 1943 [46], have been used to solve a large range of problems as diverse as robotic processing [32], object recognition [56], speech and handwriting recognition [19], and even real time sign-language translation [2].", "startOffset": 216, "endOffset": 220}, {"referenceID": 1, "context": "Artificial Neural Networks (ANN), since their origin in 1943 [46], have been used to solve a large range of problems as diverse as robotic processing [32], object recognition [56], speech and handwriting recognition [19], and even real time sign-language translation [2].", "startOffset": 267, "endOffset": 270}, {"referenceID": 55, "context": "Despite the intuition that deeper architectures would yield better results than the then more commonly used shallow ones, empirical tests with deep networks had found similar or even worse results when compared to networks with only one or two layers [57] (for more details, see [6]).", "startOffset": 251, "endOffset": 255}, {"referenceID": 5, "context": "Despite the intuition that deeper architectures would yield better results than the then more commonly used shallow ones, empirical tests with deep networks had found similar or even worse results when compared to networks with only one or two layers [57] (for more details, see [6]).", "startOffset": 279, "endOffset": 282}, {"referenceID": 5, "context": "Additionally, training was found to be difficult and often inefficient [6].", "startOffset": 71, "endOffset": 74}, {"referenceID": 33, "context": "L\u00e4ngkvist [34] argues that this scenario started to change with the proposal of greedy layer-wise unsupervised learning [22], which allowed for the fast learning of Deep Belief Networks, while also solving the vanishing gradients problem [7].", "startOffset": 10, "endOffset": 14}, {"referenceID": 21, "context": "L\u00e4ngkvist [34] argues that this scenario started to change with the proposal of greedy layer-wise unsupervised learning [22], which allowed for the fast learning of Deep Belief Networks, while also solving the vanishing gradients problem [7].", "startOffset": 120, "endOffset": 124}, {"referenceID": 6, "context": "L\u00e4ngkvist [34] argues that this scenario started to change with the proposal of greedy layer-wise unsupervised learning [22], which allowed for the fast learning of Deep Belief Networks, while also solving the vanishing gradients problem [7].", "startOffset": 238, "endOffset": 241}, {"referenceID": 13, "context": "These kinds of problems are addressed in the literature by a range of different approches (for a recent review of the main techniques applied to perform tasks such as Classification, Segmentation, Anomaly Detection and Prediction, see [14]).", "startOffset": 235, "endOffset": 239}, {"referenceID": 47, "context": "The reader is referred to [49] for a thorough description of architectural alternatives such as Restricted Boltzmann Machines (RBM), Hopfield Networks and Auto-Encoders, as well as for a detailed explanation of the Backpropagation algorithm.", "startOffset": 26, "endOffset": 30}, {"referenceID": 18, "context": "Additionally, we refer the reader to [19] for applications of RNN as well as more details on the implementation of a LSTM, and to [54] for details on CNN.", "startOffset": 37, "endOffset": 41}, {"referenceID": 52, "context": "Additionally, we refer the reader to [19] for applications of RNN as well as more details on the implementation of a LSTM, and to [54] for details on CNN.", "startOffset": 130, "endOffset": 134}, {"referenceID": 18, "context": "Each node of a layer is connected to all nodes of the next layer; (c) A LSTM block (adapted from [19]).", "startOffset": 97, "endOffset": 101}, {"referenceID": 45, "context": "2: (a) The proposed \u201cStacked Architecture\u201d for performing Anomaly Detection (adapted from [44]); (b) The architecture of a UFCNN (adapted from [47]).", "startOffset": 143, "endOffset": 147}, {"referenceID": 49, "context": "A popular learning algorithm is the Backpropagation algorithm [51], whereby the gradient of an error function is calculated and the weights are iteratively set so as to minimize the error.", "startOffset": 62, "endOffset": 66}, {"referenceID": 35, "context": "Also trained with the Backpropagation algorithm, CNNs [36] are common for image processing tasks and reduce the number of parameters to be learned by limiting the number of connections of the neurons in the hidden layer to only some of the input neurons (i.", "startOffset": 54, "endOffset": 58}, {"referenceID": 46, "context": "However, \u201cit prevents the pooling units from capturing more complex invariances, such as scale and rotation invariance\u201d [48].", "startOffset": 120, "endOffset": 124}, {"referenceID": 41, "context": "FCNs [42] allow for the input and output layers to have the same dimensions by introducing \u201ca decoder stage that is consisted of upsampling, convolution, and rectified linear units layers, to the CNN architecture\u201d [47].", "startOffset": 5, "endOffset": 9}, {"referenceID": 45, "context": "FCNs [42] allow for the input and output layers to have the same dimensions by introducing \u201ca decoder stage that is consisted of upsampling, convolution, and rectified linear units layers, to the CNN architecture\u201d [47].", "startOffset": 214, "endOffset": 218}, {"referenceID": 49, "context": "It is possible to adapt the Backpropagation algorithm to train a recurrent network, by \u201cunfolding\u201d the network through time and constraining some of the connections to always hold the same weights [51].", "startOffset": 197, "endOffset": 201}, {"referenceID": 6, "context": "This is called the vanishing gradients problem [7].", "startOffset": 47, "endOffset": 50}, {"referenceID": 22, "context": "A type of network architecture that solves this problem is the LSTM [23].", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "Traditional techniques on manipulating such data can be found in [21], and the application of traditional ANN techniques on this kind of data is described in [5].", "startOffset": 65, "endOffset": 69}, {"referenceID": 4, "context": "Traditional techniques on manipulating such data can be found in [21], and the application of traditional ANN techniques on this kind of data is described in [5].", "startOffset": 158, "endOffset": 161}, {"referenceID": 8, "context": "As an early attempt on using ANN for such tasks, [9] modelled flour prices over the range of 8 years.", "startOffset": 49, "endOffset": 52}, {"referenceID": 28, "context": "Still in the 90\u2019s, [29] delineated eight steps on \u201cdesigning a neural network forecast model using economic time series data\u201d.", "startOffset": 19, "endOffset": 23}, {"referenceID": 9, "context": "More recent approaches include usage of Elman RNNs to predict chaotic TimeSeries [10], employing ANN ensemble methods for forecasting Internet traffic [12], using simple Multilayer Perceptrons for modeling the amount of littering in the North Sea [53], and implementing in FPGA a prediction algorithm using Echo State Networks for \u201cexploiting the inherent parallelism of these systems\u201d [3].", "startOffset": 81, "endOffset": 85}, {"referenceID": 11, "context": "More recent approaches include usage of Elman RNNs to predict chaotic TimeSeries [10], employing ANN ensemble methods for forecasting Internet traffic [12], using simple Multilayer Perceptrons for modeling the amount of littering in the North Sea [53], and implementing in FPGA a prediction algorithm using Echo State Networks for \u201cexploiting the inherent parallelism of these systems\u201d [3].", "startOffset": 151, "endOffset": 155}, {"referenceID": 51, "context": "More recent approaches include usage of Elman RNNs to predict chaotic TimeSeries [10], employing ANN ensemble methods for forecasting Internet traffic [12], using simple Multilayer Perceptrons for modeling the amount of littering in the North Sea [53], and implementing in FPGA a prediction algorithm using Echo State Networks for \u201cexploiting the inherent parallelism of these systems\u201d [3].", "startOffset": 247, "endOffset": 251}, {"referenceID": 2, "context": "More recent approaches include usage of Elman RNNs to predict chaotic TimeSeries [10], employing ANN ensemble methods for forecasting Internet traffic [12], using simple Multilayer Perceptrons for modeling the amount of littering in the North Sea [53], and implementing in FPGA a prediction algorithm using Echo State Networks for \u201cexploiting the inherent parallelism of these systems\u201d [3].", "startOffset": 386, "endOffset": 389}, {"referenceID": 30, "context": "[31] presents a model for Time-Series forecasting using ANN and ARIMA models, and [15] applies the same kinds of models to water quality time series prediction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[31] presents a model for Time-Series forecasting using ANN and ARIMA models, and [15] applies the same kinds of models to water quality time series prediction.", "startOffset": 82, "endOffset": 86}, {"referenceID": 29, "context": "In still other examples of the same ideas, [30] compares the performance of ARIMA models and ANNs to make short-term predictions on photovoltaic power generators, while [38] compares both models with the performance of Multivariate Adaptive Regression Splines.", "startOffset": 43, "endOffset": 47}, {"referenceID": 37, "context": "In still other examples of the same ideas, [30] compares the performance of ARIMA models and ANNs to make short-term predictions on photovoltaic power generators, while [38] compares both models with the performance of Multivariate Adaptive Regression Splines.", "startOffset": 169, "endOffset": 173}, {"referenceID": 12, "context": "[13] performs Time-Series forecasting by using a hybrid fuzzy model: while the Fuzzy C-means method is utilized for fuzzification, ANN are employed for defuzzification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Finally, [24] forecasts the speed of the wind using a hybrid of Support Vector Machines, Ensemble Empirical Mode Decomposition and Partial Autocorrelation Function.", "startOffset": 9, "endOffset": 13}, {"referenceID": 50, "context": "A very thorough review of the entire history of developments that led the field to its current state can be found in [52], while a higher focus on the novelties from the last decade is given in [4].", "startOffset": 117, "endOffset": 121}, {"referenceID": 3, "context": "A very thorough review of the entire history of developments that led the field to its current state can be found in [52], while a higher focus on the novelties from the last decade is given in [4].", "startOffset": 194, "endOffset": 197}, {"referenceID": 33, "context": "The advantage of CNNs is that they can learn such features by themselves, reducing the need for human experts [34].", "startOffset": 110, "endOffset": 114}, {"referenceID": 36, "context": "An example of the application of such unsupervised feature learning for the classification of audio signals is presented in [37].", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "In [1], the features learned by the CNN are used as input to a Hidden Markov Model, achieving a drop at the error rate of over 10%.", "startOffset": 3, "endOffset": 6}, {"referenceID": 58, "context": "An architecture that solves this constraint is presented in [60].", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "In [18] the performance of CNNs is compared with that of LSTM for the classification of Visual and Haptic Data in a robotics setting, and in [28] the signals produced by wearable sensors are transformed into images so that Deep CNNs can be used for classification.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "In [18] the performance of CNNs is compared with that of LSTM for the classification of Visual and Haptic Data in a robotics setting, and in [28] the signals produced by wearable sensors are transformed into images so that Deep CNNs can be used for classification.", "startOffset": 141, "endOffset": 145}, {"referenceID": 26, "context": "Relevant to Tiled CNNs was the development of Independent Component Analysis (ICA) [27].", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": ", [17], [55] or [25]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 53, "context": ", [17], [55] or [25]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": ", [17], [55] or [25]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "Tiled CNNs are normally trained with a variation of such technique that looses the assumption that each component is statistically independent and tries to find a topographic order between them: the Topographic ICA [26].", "startOffset": 215, "endOffset": 219}, {"referenceID": 32, "context": "For example, Deep Belief Networks are used in the work of [33] along with RBM.", "startOffset": 58, "endOffset": 62}, {"referenceID": 56, "context": "[58] also compares the performance of Deep Belief Networks with that of Stacked Denoising Autoencoders.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "This last type of network is also employed by [50] to predict the temperature of an indoor environment.", "startOffset": 46, "endOffset": 50}, {"referenceID": 42, "context": "Another application of Time-Series forecasting can be found in [43], which uses Stacked Autoencoders to predict the flow of traffic from a Big Data dataset.", "startOffset": 63, "endOffset": 67}, {"referenceID": 40, "context": "In [41], some preliminary predictions on weather data provided by The Hong Kong Observatory are made through the usage of Stacked Autoencoders.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "In a follow up work, the authors use similar ideas to perform predictions on Big Data [40].", "startOffset": 86, "endOffset": 90}, {"referenceID": 19, "context": "Instead of Autoencoders, [20] uses Deep Belief Networks for constructing a hybrid model in which the ANN models the joint distribution between the weather predictors variables.", "startOffset": 25, "endOffset": 29}, {"referenceID": 15, "context": "It is still difficult to find works such as [16], that uses Stacked Denoising Autoencoders to perform Anomaly Detection of trajectories obtained from low level tracking algorithms.", "startOffset": 44, "endOffset": 48}, {"referenceID": 34, "context": "For example, identifying an anomaly could be transformed into a Classification task, as was done in [35].", "startOffset": 100, "endOffset": 104}, {"referenceID": 45, "context": "In this section the work presented in [47] is reviewed.", "startOffset": 38, "endOffset": 42}, {"referenceID": 45, "context": "Mittelman [47] argues that the architecture of the FCN resembles the application of a wavelet transform, and that for this reason, it can present strong variations when the input signal is subject to small translations.", "startOffset": 10, "endOffset": 14}, {"referenceID": 43, "context": "In both cases, the UFCNN outperforms the competing networks: a RNN, a Hessian-Free optimization-RNN [45], and a LSTM.", "startOffset": 100, "endOffset": 104}, {"referenceID": 57, "context": "Wang and Oates [59] presented an approach for Time-Series Classification using CNN-like networks.", "startOffset": 15, "endOffset": 19}, {"referenceID": 7, "context": "The creation of the Markov Transition Field is based on the ideas proposed in [8] for the definition of the so-called Markov Transition Matrix (MTM).", "startOffset": 78, "endOffset": 81}, {"referenceID": 38, "context": "1 with the state-of-theart SAX-BoP approach\u201d [39], which are 50Words, Adiac, Beef, Coffee, ECG200, Face (all), Lightning-2, Lightning-7, OliveOil, OSU Leaf, Swedish Leaf and Yoga [11].", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "1 with the state-of-theart SAX-BoP approach\u201d [39], which are 50Words, Adiac, Beef, Coffee, ECG200, Face (all), Lightning-2, Lightning-7, OliveOil, OSU Leaf, Swedish Leaf and Yoga [11].", "startOffset": 179, "endOffset": 183}], "year": 2017, "abstractText": "In many real-world application, e.g., speech recognition or sleep stage classification, data are captured over the course of time, constituting a Time-Series. Time-Series often contain temporal dependencies that cause two otherwise identical points of time to belong to different classes or predict different behavior. This characteristic generally increases the difficulty of analysing them. Existing techniques often depended on hand-crafted features that were expensive to create and required expert knowledge of the field. With the advent of Deep Learning new models of unsupervised learning of features for Time-series analysis and forecast have been developed. Such new developments are the topic of this paper: a review of the main Deep Learning techniques is presented, and some applications on Time-Series analysis are summaried. The results make it clear that Deep Learning has a lot to contribute to the field.", "creator": "LaTeX with hyperref package"}}}