{"id": "1701.02149", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jan-2017", "title": "Task-Specific Attentive Pooling of Phrase Alignments Contributes to Sentence Matching", "abstract": "This work studies comparatively two typical sentence matching tasks: textual entailment (TE) and answer selection (AS), observing that weaker phrase alignments are more critical in TE, while stronger phrase alignments deserve more attention in AS. The key to reach this observation lies in phrase detection, phrase representation, phrase alignment, and more importantly how to connect those aligned phrases of different matching degrees with the final classifier. Prior work (i) has limitations in phrase generation and representation, or (ii) conducts alignment at word and phrase levels by handcrafted features or (iii) utilizes a single framework of alignment without considering the characteristics of specific tasks, which limits the framework's effectiveness across tasks.\n\n\n\n\n\n\nFor a summary of the basic concepts discussed, see The Effect of Word and Words.\n\nIn this work, we present the theoretical framework for determining the concept of word and words to be used for word and words in sentence search, and are trying to answer questions using different approaches. However, in many cases, it is not possible to understand the structure of word and word use in word search, because it is only possible for each word to be interpreted in order to find the following meaning:\n\"Word is not an effective word that is not available. Word is not an effective word that is not available. So, this will be the first to discuss whether to use words as a word or word in language search in a particular order. The most powerful approach for a single topic to describe it in a single sentence search, and it is one of the most powerful approaches we have seen to describe word use in sentence search, but it is difficult to distinguish the meanings of word and word. When word was used as a word, it may have different meanings, but it is not possible to distinguish the meanings of words. For example, the word \"s\" is not a word that has the same name (or title) as the name of the word, but a word with the same name. Thus, words like \"S\" or \"F\" are often used in sentence search, but they are not useful in sentence search. These are common phrases for both sentence search and sentence search, and the basic concept of word and word are the most powerful. The first example to determine the meaning of word and word use is the idea that a word is not available. A word like \"B\" may be used in a sentence search, but it has no exact meaning. It is likely that some of the meanings of words are important.", "histories": [["v1", "Mon, 9 Jan 2017 12:03:11 GMT  (250kb,D)", "http://arxiv.org/abs/1701.02149v1", "EACL'2017 long paper. arXiv admin note: substantial text overlap witharXiv:1604.06896"]], "COMMENTS": "EACL'2017 long paper. arXiv admin note: substantial text overlap witharXiv:1604.06896", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenpeng yin", "hinrich sch\\\"utze"], "accepted": false, "id": "1701.02149"}, "pdf": {"name": "1701.02149.pdf", "metadata": {"source": "CRF", "title": "Task-Specific Attentive Pooling of Phrase Alignments Contributes to Sentence Matching\u2217", "authors": ["Wenpeng Yin", "Hinrich Sch\u00fctze"], "emails": ["wenpeng@cis.lmu.de"], "sections": [{"heading": null, "text": "Prior work (i) has limitations in phrase generation and representation, or (ii) conducts alignment at word and phrase levels by handcrafted features or (iii) utilizes a single framework of alignment without considering the characteristics of specific tasks, which limits the framework\u2019s effectiveness across tasks.\nWe propose an architecture based on Gated Recurrent Unit that supports (i) representation learning of phrases of arbitrary granularity and (ii) task-specific attentive pooling of phrase alignments between two sentences. Experimental results on TE and AS match our observation and show the effectiveness of our approach."}, {"heading": "1 Introduction", "text": "How to model a pair of sentences is a critical issue in many NLP tasks, including textual entailment (Marelli et al., 2014a; Bowman et al., 2015a; Yin et al., 2016a) and answer selection (Yu et al., 2014; Yang et al., 2015; Santos et al., 2016). A key challenge common to these tasks is the lack of explicit alignment annotation between the sentences of the pair. Thus, inferring and assessing the semantic relations between words and phrases in the two sentences is a core issue.\n\u2217EACL\u20192017 long paper\nFigure 1 shows examples of human annotated phrase alignments. In the TE example, we try to figure out Q entails C+ (positive) or C\u2212 (negative). As human beings, we discover the relationship of two sentences by studying the alignments between linguistic units. We see that some phrases are kept: \u201care playing outdoors\u201d (between Q and C+), \u201care playing \u201d (between Q and C\u2212). Some phrases are changed into related semantics on purpose: \u201cthe young boys\u201d (Q)\u2192 \u201cthe kids\u201d (C+ & C\u2212), \u201cthe man is smiling nearby\u201d (Q) \u2192 \u201cnear a man with a smile\u201d (C+) or \u2192 \u201can old man is standing in the background\u201d (C\u2212) . We can see that the kept parts have stronger alignments (green color), and changed parts have weaker alignments (blue color). Here, by \u201cstrong\u201d / \u201cweak\u201d we mean how semantically close the two aligned phrases are. To successfully identify the relationships of (Q, C+) or (Q, C\u2212), studying the changed parts is crucial. Hence, we argue that TE should pay more attention to weaker alignments. ar X iv :1 70 1.\n02 14\n9v 1\n[ cs\n.C L\n] 9\nJ an\n2 01\n7\nIn AS, we try to figure out: does sentence C+ or sentence C\u2212 answer question Q? Roughly, the content in candidatesC+ andC\u2212 can be classified into aligned part (e.g., repeated or relevant parts) and negligible part. This differs from TE, in which it is hard to claim that some parts are negligible or play a minor role, as TE requires to make clear that each part can entail or be entailed. Hence, TE is considerably sensitive to those \u201cunseen\u201d parts. In contrast, AS is more tolerant of negligible parts and less related parts. From the AS example in Figure 1, we see that \u201cAuburndale Florida\u201d (Q) can find related part \u201cthe city\u201d (C+), and \u201cAuburndale\u201d, \u201ca city\u201d (C\u2212) ; \u201chow big\u201d (Q) also matches \u201chad a population of 12,381\u201d (C+) very well. And some unaligned parts exist, denoted by red color. Hence, we argue that stronger alignments in AS deserve more attention.\nThe above analysis suggests that: (i) alignments connecting two sentences can happen between phrases of arbitrary granularity; (ii) phrase alignments can have different intensities; (iii) tasks of different properties require paying different attention to alignments of different intensities.\nAlignments at word level (Yih et al., 2013) or phrase level (Yao et al., 2013) both have been studied before. For example, Yih et al. (2013) make use of WordNet (Miller, 1995) and Probase (Wu et al., 2012) for identifying hyper- and hyponymy. Yao et al. (2013) use POS tags, WordNet and paraphrase database for alignment identification. Their approaches rely on manual feature design and linguistic resources. We develop a deep neural network (DNN) to learn representations of phrases of arbitrary lengths. As a result, alignments can be searched in a more automatic and exhaustive way.\nDNNs have been intensively investigated in sentence pair classifications (Blacoe and Lapata, 2012; Socher et al., 2011; Yin and Schu\u0308tze, 2015b), and attention mechanisms are also applied to individual tasks (Santos et al., 2016; Rockta\u0308schel et al., 2016; Wang and Jiang, 2016); however, most attention-based DNNs have implicit assumption that stronger alignments deserve more attention (Yin et al., 2016a; Santos et al., 2016; Yin et al., 2016b). Our examples in Figure 1, instead, show that this assumption does not hold invariably. Weaker alignments in certain tasks such as TE can be the indicator of the final decision. Our inspiration comes from the analysis of some prior work. For TE, Yin et al. (2016a)\nshow that considering the pairs in which overlapping tokens are removed can give a boost. This simple trick matches our motivation that weaker alignment should be given more attention in TE. However, Yin et al. (2016a) remove overlapping tokens completely, potentially obscuring complex alignment configurations. In addition, Yin et al. (2016a) use the same attention mechanism for TE and AS, which is less optimal based on our observations.\nThis motivates us in this work to introduce DNNs with a flexible attention mechanism that is adaptable for specific tasks. For TE, it can make our system pay more attention to weaker alignments; for AS, it enables our system to focus on stronger alignments. We can treat the pre-processing in (Yin et al., 2016a) as a hard way, and ours as a soft way, as our phrases have more flexible lengths and the existence of overlapping phrases decreases the risk of losing important alignments. In experiments, we will show that this attention scheme is very effective for different tasks.\nWe make the following contributions. (i) We use GRU (Gated Recurrent Unit (Cho et al., 2014)) to learn representations for phrases of arbitrary granularity. Based on phrase representations, we can detect phrase alignments of different intensities. (ii) We propose attentive pooling to achieve flexible choice among alignments, depending on the characteristics of the task. (iii) We achieve state-of-the-art on TE task."}, {"heading": "2 Related Work", "text": "Non-DNN for sentence pair modeling. Heilman and Smith (2010) describe tree edit models that generalize tree edit distance by allowing operations that better account for complex reordering phenomena and by learning from data how different edits should affect the model\u2019s decisions about sentence relations. Wang and Manning (2010) cope with the alignment between a sentence pair by using a probabilistic model that models tree-edit operations on dependency parse trees. Their model treats alignments as structured latent variables, and offers a principled framework for incorporating complex linguistic features. Guo and Diab (2012) identify the degree of sentence similarity by modeling the missing words (words that are not in the sentence) so as to relieve the sparseness issue of sentence modeling. Yih et\nal. (2013) try to improve the shallow semantic component, lexical semantics, by formulating sentence pair as a semantic matching problem with a latent word-alignment structure as in (Chang et al., 2010). More fine-grained word overlap and alignment between two sentences are explored in (Lai and Hockenmaier, 2014), in which negation, hypernym/hyponym, synonym and antonym relations are used. Yao et al. (2013) extend word-toword alignment to phrase-to-phrase alignment by a semi-Markov CRF. Such approaches often require more computational resources. In addition, using syntactic/semantic parsing during run-time to find the best matching between structured representation of sentences is not trivial.\nDNN for sentence pair classification. There recently has been great interest in using DNNs for classifying sentence pairs as they can reduce the burden of feature engineering.\nFor TE, Bowman et al. (2015b) employ recursive DNN to encode entailment on SICK (Marelli et al., 2014b). Rockta\u0308schel et al. (2016) present an attention-based LSTM (long short-term memory, Hochreiter and Schmidhuber (1997)) for the SNLI corpus (Bowman et al., 2015a).\nFor AS, Yu et al. (2014) present a bigram CNN (convolutional neural network (LeCun et al., 1998)) to model question and answer candidates. Yang et al. (2015) extend this method and get state-of-the-art performance on the WikiQA dataset. Feng et al. (2015) test various setups of a bi-CNN architecture on an insurance domain QA dataset. Tan et al. (2015) explore bidirectional LSTM on the same dataset. Other sentence matching tasks such as paraphrase identification (Socher et al., 2011; Yin and Schu\u0308tze, 2015a), question \u2013 Freebase fact matching (Yin et al., 2016b) etc. are also investigated.\nSome prior work aims to solve a general sentence matching problem. Hu et al. (2014) present two CNN architectures for paraphrasing, sentence completion (SC), tweet-response matching tasks. Yin and Schu\u0308tze (2015b) propose the MultiGranCNN architecture to model general sentence matching based on phrase matching on multiple levels of granularity. Wan et al. (2016) try to match two sentences in AS and SC by multiple sentence representations, each coming from the local representations of two LSTMs.\nAttention-based DNN for alignment. DNNs have been successfully developed to detect align-\nments, e.g., in machine translation (Bahdanau et al., 2015; Luong et al., 2015) and text reconstruction (Li et al., 2015; Rush et al., 2015). In addition, attention-based alignment is also applied in natural language inference (e.g., Rockta\u0308schel et al. (2016),Wang and Jiang (2016)). However, most of this work aligns word-by-word. As Figure 1 shows, many sentence relations can be better identified through phrase level alignments. This is one motivation of our work."}, {"heading": "3 Model", "text": "This section first gives a brief introduction of GRU and how it performs phrase representation learning, then describes the different attentive poolings for phrase alignments w.r.t TE and AS tasks."}, {"heading": "3.1 GRU Introduction", "text": "GRU is a simplified version of LSTM. Both are found effective in sequence modeling, as they are order-sensitive and can capture long-range context. The tradeoffs between GRU and its competitor LSTM have not been fully explored yet. According to empirical evaluations in (Chung et al., 2014; Jozefowicz et al., 2015), there is not a clear winner. In many tasks both architectures yield comparable performance and tuning hyperparameters like layer size is probably more important than picking the ideal architecture. GRU have fewer parameters and thus may train a bit faster or need less data to generalize. Hence, we use GRU, as shown in Figure 2, to model text:\nz = \u03c3(xtU z + st\u22121W z) (1) r = \u03c3(xtU r + st\u22121W r) (2) ht = tanh(xtU h + (st\u22121 \u25e6 r)Wh) (3)\nst = (1\u2212 z) \u25e6 ht + z \u25e6 st\u22121 (4)\nx is the input sentence with token xt \u2208 Rd at position t, st \u2208 Rh is the hidden state at t, supposed to\nencode the history x1, \u00b7 \u00b7 \u00b7 , xt\u22121. z and r are two gates. All U \u2208 Rd\u00d7h,W \u2208 Rh\u00d7h are parameters in GRU."}, {"heading": "3.2 Representation Learning for Phrases", "text": "For a general sentence s with five consecutive words: ABCDE, with each word represented by a word embedding of dimensionality d, we first create four fake sentences, s1: \u201cBCDEA\u201d, s2: \u201cCDEAB\u201d, s3: \u201cDEABC\u201d and s4: \u201cEABCD\u201d, then put them in a matrix (Figure 3, left).\nWe run GRUs on each row of this matrix in parallel. As GRU is able to encode the whole sequence up to current position, this step generates representations for any consecutive phrases in original sentence s. For example, the GRU hidden state at position \u201cE\u201d at coordinates (1,5) (i.e., 1st row, 5th column) denotes the representation of the phrase \u201cABCDE\u201d which in fact is s itself, the hidden state at \u201cE\u201d (2,4) denotes the representation of phrase \u201cBCDE\u201d, . . . , the hidden state of \u201cE\u201d (5,1) denotes phrase representation of \u201cE\u201d itself. Hence, for each token, we can learn the representations for all phrases ending with this token. Finally, all phrases of any lengths in s can get a representation vector. GRUs in those rows are set to share weights so that all phrase representations are comparable in the same space.\nNow, we reformat sentence \u201cABCDE\u201d into s\u2217 = \u201c(A) (B) (AB) (C) (BC) (ABC) (D) (CD) (BCD) (ABCD) (E) (DE) (CDE) (BCDE) (ABCDE)\u201d, as shown by arrows in Figure 3 (right), the arrow direction means phrase order. Each sequence in parentheses is a phrase (we use parentheses just for making the phrase boundaries clear). Randomly taking a phrase \u201cCDE\u201d as an example, its representation comes from the hidden state at \u201cE\u201d (3,3) in Figure 3 (left). Shaded parts are discarded. The main advantage of reformatting sentence \u201cABCDE\u201d into the new sentence s\u2217 is to cre-\nate phrase-level semantic units, but at the same time we maintain the order information.\nHence, the sentence \u201chow big is Auburndale Florida\u201d in Figure 1 will be reformatted into \u201c(how) (big) (how big) (is) (big is) (how big is) (Auburndale) (is Auburndale) (big is Auburndale) (how big is Auburndale) (Florida) (Auburndale Florida) (is Auburndale Florida) (big is Auburndale Florida) (how big is Auburndale Florida)\u201d. We can see that phrases are exhaustively detected and represented.\nIn the experiments of this work, we explore the phrases of maximal length 7 instead of arbitrary lengths."}, {"heading": "3.3 Attentive Pooling", "text": "As each sentence s\u2217 consists of a sequence of phrases, and each phrase is denoted by a representation vector generated by GRU, we can compute an alignment matrix A between two sentences s\u22171 and s\u22172, by comparing each two phrases, one from s\u22171 and one from s \u2217 2. Let s \u2217 1 and s \u2217 2 also denote lengths respectively, thus A \u2208 Rs\u22171\u00d7s\u22172 . While there are many ways of computing the entries of A, we found that cosine works well in our setting.\nThe first step then is to detect the best alignment for each phrase by leveraging A. To be concrete, for sentence s\u22171, we do row-wise max-pooling over A as attention vector a1:\na1,i = max(A[i, :]) (5)\nIn a1, the entry a1,i denotes the best alignment for ith phrase in sentence s\u22171. Similarly, we can do column-wise max-pooling to generate attention vector a2 for sentence s\u22172.\nNow, the problem is that we need to pay most attention to the phrases aligned very well or phrases aligned badly. According to the analysis of the two examples in Figure 1, we need to pay more attention to weaker (resp. stronger) alignments in TE (resp. AS). To this end, we adopt different second step over attention vector ai (i = 1, 2) for TE and AS.\nFor TE, in which weaker alignments are supposed to contribute more, we do k-min-pooling over ai, i.e., we only keep the k phrases which are aligned worst. For the (Q, C+) pair in TE example of Figure 1, we expect this step is able to put most of our attention to the phrases \u201cthe kids\u201d, \u201cthe young boys\u201d, \u201cnear a man with a smile\u201d and \u201cand the man is smiling nearby\u201d as they have rela-\ntively weaker alignments while their relations are the indicator of the final decision.\nFor AS, in which stronger alignments are supposed to contribute more, we do k-max-pooling over ai, i.e., we only keep the k phrases which are aligned best. For the (Q, C+) pair in AS example of Figure 1, we expect this k-max-pooling is able to put most of our attention to the phrases \u201chow big\u201d \u201cAuburndale Florida\u201d, \u201cthe city\u201d and \u201chad a population of 12,381\u201d as they have relatively stronger alignments and their relations are the indicator of the final decision. We keep the original order of extracted phrases after k-min/maxpooling.\nIn summary, for TE, we first do row-wise maxpooling over alignment matrix, then do k-minpooling over generated alignment vector; we use k-min-max-pooling to denote the whole process. In contrast, we use k-max-max-pooling for AS. We refer to this method of using two successive min or max pooling steps as attentive pooling."}, {"heading": "3.4 The Whole Architecture", "text": "Now, we present the whole system in Figure 4. We take sentences s1 \u201cABC\u201d and s2 \u201cDEFG\u201d as illustration. Each token, i.e., A to F, in the figure is denoted by an embedding vector, hence each sentence is represented as an order-3 tensor as input (they are depicted as rectangles just for simplicity). Based on tensor-style sentence input, we have described the phrase representation learning by GRU1 in Section 3.2 and attentive pooling in Section 3.3.\nAttentive pooling generates a new feature map for each sentence, as shown in Figure 4 (the third layer from the bottom), and each column representation in the feature map denotes a key phrase in this sentence that, based on our modeling assumptions, should be a good basis for the correct final decision. For instance, we expect such a feature map to contain representations of \u201cthe young boys\u201d, \u201coutdoors\u201d and \u201cand the man is smiling nearby\u201d for the sentence Q in the TE example of Figure 1.\nNow, we do another GRU2 step for: 1) the new\nfeature map of each sentence mentioned above, to encode all the key phrases as the sentence representation; 2) a concatenated feature map of the two new sentence feature maps, to encode all the key phrases in the two sentences sequentially as the representation of the sentence pair. As GRU generates a hidden state at each position, we always choose the last hidden state as the representation of the sentence or sentence pair. In Figure 4 (the fourth layer), these final GRU-generated representations for sentence s1, s2 and the sentence pair are depicted as green columns: s1, s2 and sp respectively.\nAs for the input of the final classifier, it can be flexible, such as representation vectors (rep), similarity scores between s1 and s2 (simi), and extra linguistic features (extra). This can vary based on the specific tasks. We give details in Section 4."}, {"heading": "4 Experiments", "text": "We test the proposed architectures on TE and AS benchmark datasets."}, {"heading": "4.1 Common Setup", "text": "For both TE and AS, words are initialized by 300- dimensional GloVe embeddings1 (Pennington et al., 2014) and not changed during training. A single randomly initialized embedding is created for all unknown words by uniform sampling from [\u2212.01, .01]. We use ADAM (Kingma and Ba, 2015), with a first momentum coefficient of 0.9 and a second momentum coefficient of 0.999,2 L2 regularization and Diversity Regularization (Xie et al., 2015). Table 1 shows the values of the hyperparameters, tuned on dev.\nClassifier. Following Yin et al. (2016a), we use three classifiers \u2013 logistic regression in DNN, logistic regression and linear SVM with default parameters3 directly on the feature vector \u2013 and report performance of the best.\n1nlp.stanford.edu/projects/glove/ 2Standard configuration recommended by Kingma and Ba 3http://scikit-learn.org/stable/ for both.\nCommon Baselines. (i) Addition. We sum up word embeddings element-wise to form sentence representation, then concatenate two sentence representation vectors (s01, s 0 2) as classifier input. (ii) A-LSTM. The pioneering attention based LSTM system for a specific sentence pair classification task \u201cnatural language inference\u201d (Rockta\u0308schel et al., 2016). A-LSTM has the same dimensionality as our GRU system in terms of initialized word representations and the hidden states. (iii) ABCNN (Yin et al., 2016a). The state-of-the-art system in both TE and AS.\nBased on the motivation in Section 1, the main hypothesis to be tested in experiments is: k-minmax-pooling is superior for TE and k-max-maxpooling is superior for AS. In addition, we would like to determine whether the second pooling step in attention pooling, i.e., the k-min/max-pooling, is more effective than a \u201cfull-pooling\u201d in which all the generated phrases are forwarded into the next layer."}, {"heading": "4.2 Textual Entailment", "text": "SemEval 2014 Task 1 (Marelli et al., 2014a) evaluates system predictions of textual entailment (TE) relations on sentence pairs from the SICK dataset (Marelli et al., 2014b). The three classes are entailment, contradiction and neutral. The sizes of SICK train, dev and test sets are 4439, 495 and 4906 pairs, respectively. We choose SICK benchmark dataset so that our result is directly comparable with that of (Yin et al., 2016a), in which nonoverlapping text are utilized explicitly to boost the performance. That trick inspires this work.\nFollowing Lai and Hockenmaier (2014), we train our final system (after fixing of hyperparameters) on train and dev (4,934 pairs). Our evaluation measure is accuracy."}, {"heading": "4.2.1 Feature Vector", "text": "The final feature vector as input of classifier contains three parts: rep, simi, extra.\nRep. Totally five vectors, three are the top sentence representation s1, s2 and the top sentence pair representation sp (shown in green in Figure 4), two are s01, s 0 2 from Addition baseline.\nSimi. Four similarity scores, cosine similarity and euclidean distance between s1 and s2, cosine similarity and euclidean distance between s01 and s02. Euclidean distance \u2016 \u00b7 \u2016 is transformed into 1/(1+ \u2016 \u00b7 \u2016).\nExtra. We include the same 22 linguistic features as Yin et al. (2016a). They cover 15 machine translation metrics between the two sentences; whether or not the two sentences contain negation tokens like \u201cno\u201d, \u201cnot\u201d etc; whether or not they contain synonyms, hypernyms or antonyms; two sentence lengths. See Yin et al. (2016a) for details."}, {"heading": "4.2.2 Results", "text": "Table 2 shows that GRU with k-min-max-pooling gets state-of-the-art performance on SICK and significantly outperforms k-max-max-pooling and full-pooling. Full-pooling has more phrase input than the combination of k-max-max-pooling and k-min-max-pooling, this might bring two problems: (i) noisy alignments increase; (ii) sentence pair representation sp is no longer discriminative \u2013 sp does not know its semantics comes from phrases of s1 or s2: as different sentences have different lengths, the boundary location separating two sentences varies across pairs. However, this is crucial to determine whether s1 entails s2.\nABCNN (Yin et al., 2016a) is based on assumptions similar to k-max-max-pooling: words/phrases with higher matching values should contribute more in this task. However, ABCNN gets the optimal performance by combining a reformatted SICK version in which\noverlapping tokens in two sentences are removed. This instead hints that non-overlapping units can do a big favor for this task, which is indeed the superiority of our \u201ck-min-max-pooling\u201d."}, {"heading": "4.3 Answer Selection", "text": "We use WikiQA4 subtask that assumes there is at least one correct answer for a question. This dataset consists of 20,360, 1130 and 2352 question-candidate pairs in train, dev and test, respectively. Following Yang et al. (2015), we truncate answers to 40 tokens and report mean average precision (MAP) and mean reciprocal rank (MRR).\nApart from the common baselines Addition, ALSTM and ABCNN, we compare further with: (i) CNN-Cnt (Yang et al., 2015): combine CNN with two linguistic features \u201cWordCnt\u201d (the number of non-stopwords in the question that also occur in the answer) and \u201cWgtWordCnt\u201d (reweight the counts by the IDF values of the question words); (ii) AP-CNN (Santos et al., 2016)."}, {"heading": "4.3.1 Feature Vector", "text": "The final feature vector in AS has the same (rep, simi, extra) structure as TE, except that simi consists of only two cosine similarity scores, and extra consists of four entries: two sentence lengths, WordCnt and WgtWordCnt.\n4http://aka.ms/WikiQA (Yang et al., 2015)\nth e ki ds\nth e\nki ds ar e\nki ds\na re th e. ..a re pl ay in g\nar e\npl ay\nin g\nki ds\n... pl ay\nin g\nth e.\n..p la yi ng ou td oo rs\npl ay\nin g\nou td\noo rs\nar e.\n..o ut\ndo or\ns\nki ds\n... ou\ntd oo\nrs\nth e.\n..o ut\ndo or s ne ar\nou td\noo rs\nn ea\nr\npl ay\nin g.\n..n ea\nr ar e. ..n ea r ki ds ... ne ar th e. ..n ea r a ne ar a\nou td\noo rs\n... a pl ay in g. ..a ar e. ..a ki ds ... a th e. ..a m an a m an ne ar ... m an\nou td\noo rs\n... m\nan\npl ay\nin g.\n..m an\nar e.\n..m an\nki ds\n... m\nan th e. ..m an w ith m an w ith a. ..w ith ne ar ... w ith\nou td\noo rs\n... w ith pl ay in g. ..w ith ar e. ..w ith ki ds ... w ith th e. ..w ith a w ith a m an ... a a. ..a ne ar ... a ou td oo rs ... a pl ay in g. ..a ar e. ..a ki ds ... a th e. ..a sm ile a sm ile w ith ... sm ile m an ... sm ile a. ..s m ile ne ar ... sm ile\nou td\noo rs\n... sm\nile\npl ay\nin g.\n..s m\nile ar e. ..s m ile ki ds ... sm ile th e. ..s m ile\na tt\ne n\nti o\nn v\na lu\ne s\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n(a) Attention distribution for phrases in \u201cQ\u201d of TE example in Figure 1\nth e ki ds\nth e\nki ds ar e\nki ds\na re th e. ..a re pl ay in g\nar e\npl ay\nin g\nki ds\n... pl ay\nin g\nth e.\n..p la yi ng ou td oo rs\npl ay\nin g\nou td\noo rs\nar e.\n..o ut\ndo or\ns\nki ds\n... ou\ntd oo\nrs\nth e.\n..o ut\ndo or s ne ar\nou td\noo rs\nn ea\nr\npl ay\nin g.\n..n ea\nr ar e. ..n ea r ki ds ... ne ar th e. ..n ea r a ne ar a\nou td\noo rs\n... a pl ay in g. ..a ar e. ..a ki ds ... a th e. ..a m an a m an ne ar ... m an\nou td\noo rs\n... m\nan\npl ay\nin g.\n..m an\nar e.\n..m an\nki ds\n... m\nan th e. ..m an w ith m an w ith a. ..w ith ne ar ... w ith\nou td\noo rs\n... w ith pl ay in g. ..w ith ar e. ..w ith ki ds ... w ith th e. ..w ith a w ith a m an ... a a. ..a ne ar ... a ou td oo rs ... a pl ay in g. ..a ar e. ..a ki ds ... a th e. ..a sm ile a sm ile w ith ... sm ile m an ... sm ile a. ..s m ile ne ar ... sm ile\nou td\noo rs\n... sm\nile\npl ay\nin g.\n..s m\nile ar e. ..s m ile ki ds ... sm ile th e. ..s m ile\na tt\ne n\nti o\nn v\na lu\ne s\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n(b) Attention distribution for phrases in \u201cC+\u201d of TE example in Figure 1\nFigure 5: Attention Visualization"}, {"heading": "4.3.2 Results", "text": "Table 3 shows that GRU with k-max-max-pooling is significantly better than its k-min-max-pooling and full-pooling versions. GRU with k-max-maxpooling has similar assumption with ABCNN (Yin et al., 2016a) and AP-CNN (Santos et al., 2016): units with higher matching scores are supposed to contribute more in this task. Our improvement\ncan be due to that: i) our linguistic units cover more exhaustive phrases, it enables alignments in a wider range; ii) we have two max-pooling steps in our attention pooling, especially the second one is able to remove some noisily aligned phrases. Both ABCNN and AP-CNN are based on convolutional layers, the phrase detection is constrained by filter sizes. Even though ABCNN tries a second\nCNN layer to detect bigger-granular phrases, their phrases in different CNN layers cannot be aligned directly as they are in different spaces. GRU in this work uses the same weights to learn representations of arbitrary-granular phrases, hence, all phrases can share the representations in the same space and can be compared directly."}, {"heading": "4.4 Visual Analysis", "text": "In this subsection, we visualize the attention distributions over phrases, i.e., ai in Equation 5, of example sentences in Figure 1 (for space limit, we only show this for TE example). Figures 5(a)5(b) respectively show the attention values of each phrase in (Q, C+) pair in TE example in Figure 1. We can find that k-min-pooling over this distributions can indeed detect some key phrases that are supposed to determine the pair relations. Taking Figure 5(a) as an example, phrases \u201cyoung boys\u201d, phrases ending with \u201cand\u201d, phrases \u201csmiling\u201d, \u201cis smiling\u201d, \u201cnearby\u201d and a couple of phrases ending with \u201cnearby\u201d have lowest attention values. According to our k-min-pooling step, these phrases will be detected as key phrases. Considering further the Figure 5(b), phrases \u201ckids\u201d, phrases ending with \u201cnear\u201d, and a couple of phrases ending with \u201csmile\u201d are detected as key phrases.\nIf we look at the key phrases in both sentences, we can find that the discovering of those key phrases matches our analysis in Section 1 for TE example: \u201ckids\u201d corresponds to \u201cyoung boys\u201d, \u201csmiling nearby\u201d corresponds to \u201cnear...smile\u201d.\nAnother interesting phenomenon is that, taking Figure 5(b) as example, even though \u201care playing outdoors\u201d can be well aligned as it appears in both sentences, nevertheless the visualization figures show that the attention values of \u201care playing outdoors and\u201d in Q and \u201care playing outdoors near\u201d drop dramatically. This hints that our model can get rid of some surface matching, as the key token \u201cand\u201d or \u201cnear\u201d makes the semantics of \u201care playing outdoors and\u201d and \u201care playing outdoors near\u201d be pretty different with their sub-phrase \u201care playing outdoors\u201d. This is important as \u201cand\u201d or \u201cnear\u201d is crucial unit to connect the following key phrases \u201csmiling nearby\u201d in Q or \u201ca smile\u201d in C+. If we connect those key phrases sequentially as a new fake sentence, as we did in attentive pooling layer of Figure 4, we can see that the fake sentence roughly \u201creconstructs\u201d the meaning of the original sentence while it is composed of phrase-level se-\nmantic units now."}, {"heading": "4.5 Effects of Pooling Size k", "text": "The key idea of the proposed method is achieved by the k-min/max pooling. We show how the hyperparameter k influences the results by tuning on the dev sets.\nIn Figure 6, we can see the performance trends of changing k value between 1 and 10 in the two tasks. Roughly k > 4 can give competitive results, but larger values bring performance drop."}, {"heading": "5 Conclusion", "text": "In this work, we investigate the contribution of different intensities of phrase alignments for different tasks. We argue that it is not true that stronger alignments always matter more. We found TE task prefers weaker alignments while AS task prefers stronger alignments. We proposed flexible attentive poolings in GRU system to satisfy the different requirements of different tasks. Experimental results show the soundness of our argument and the effectiveness of our attention pooling based GRU systems.\nAs future work, we plan to investigate phrase representation learning in context and how to conduct the attentive pooling automatically regardless of the categories of the tasks."}, {"heading": "Acknowledgments", "text": "We gratefully acknowledge the support of Deutsche Forschungsgemeinschaft for this work (SCHU 2246/8-2)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["William Blacoe", "Mirella Lapata."], "venue": "Proceedings of EMNLP-CoNLL, pages 546\u2013556.", "citeRegEx": "Blacoe and Lapata.,? 2012", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning."], "venue": "Proceedings of EMNLP, pages 632\u2013642.", "citeRegEx": "Bowman et al\\.,? 2015a", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Recursive neural networks can learn logical semantics", "author": ["Samuel R Bowman", "Christopher Potts", "Christopher D Manning."], "venue": "Proceedings of CVSC workshop, pages 12\u201321.", "citeRegEx": "Bowman et al\\.,? 2015b", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Discriminative learning over constrained latent representations", "author": ["Ming-Wei Chang", "Dan Goldwasser", "Dan Roth", "Vivek Srikumar."], "venue": "Proceedings of NAACL-HLT, pages 429\u2013437.", "citeRegEx": "Chang et al\\.,? 2010", "shortCiteRegEx": "Chang et al\\.", "year": 2010}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Applying deep learning to answer selection: A study and an open task", "author": ["Minwei Feng", "Bing Xiang", "Michael R Glass", "Lidan Wang", "Bowen Zhou."], "venue": "Proceedings of IEEE ASRU Workshop.", "citeRegEx": "Feng et al\\.,? 2015", "shortCiteRegEx": "Feng et al\\.", "year": 2015}, {"title": "Modeling sentences in the latent space", "author": ["Weiwei Guo", "Mona Diab."], "venue": "Proceedings of ACL, pages 864\u2013872.", "citeRegEx": "Guo and Diab.,? 2012", "shortCiteRegEx": "Guo and Diab.", "year": 2012}, {"title": "Tree edit models for recognizing textual entailments, paraphrases, and answers to questions", "author": ["Michael Heilman", "Noah A Smith."], "venue": "Proceedings of NAACL-HLT, pages 1011\u20131019.", "citeRegEx": "Heilman and Smith.,? 2010", "shortCiteRegEx": "Heilman and Smith.", "year": 2010}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen."], "venue": "Proceedings of NIPS, pages 2042\u20132050.", "citeRegEx": "Hu et al\\.,? 2014", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Unal-nlp: Combining soft cardinality features for semantic textual similarity, relatedness and entailment", "author": ["Sergio Jimenez", "George Duenas", "Julia Baquero", "Alexander Gelbukh", "Av Juan Dios B\u00e1tiz", "Av Mendiz\u00e1bal."], "venue": "SemEval, pages 732\u2013", "citeRegEx": "Jimenez et al\\.,? 2014", "shortCiteRegEx": "Jimenez et al\\.", "year": 2014}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings of ICML, pages 2342\u20132350.", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings of ICLR.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Illinois-lh: A denotational and distributional approach to semantics", "author": ["Alice Lai", "Julia Hockenmaier."], "venue": "SemEval, pages 329\u2013334.", "citeRegEx": "Lai and Hockenmaier.,? 2014", "shortCiteRegEx": "Lai and Hockenmaier.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner."], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky."], "venue": "Proceedings of ACL, pages 1106\u20131115.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "Proceedings of EMNLP, pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and tex", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "A sick cure for the evaluation of compositional distributional semantic models", "author": ["Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli."], "venue": "Proceedings of LREC, pages 216\u2013223.", "citeRegEx": "Marelli et al\\.,? 2014b", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Wordnet: A lexical database for english", "author": ["George A. Miller."], "venue": "Commun. ACM, 38(11):39\u201341.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of EMNLP, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom."], "venue": "Proceedings of ICLR.", "citeRegEx": "Rockt\u00e4schel et al\\.,? 2016", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2016}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "Proceedings of EMNLP, pages 379\u2013389.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Attentive pooling networks", "author": ["Cicero dos Santos", "Ming Tan", "Bing Xiang", "Bowen Zhou."], "venue": "arXiv preprint arXiv:1602.03609.", "citeRegEx": "Santos et al\\.,? 2016", "shortCiteRegEx": "Santos et al\\.", "year": 2016}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of NIPS, pages 801\u2013809.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Lstmbased deep learning models for non-factoid answer selection", "author": ["Ming Tan", "Bing Xiang", "Bowen Zhou."], "venue": "arXiv preprint arXiv:1511.04108.", "citeRegEx": "Tan et al\\.,? 2015", "shortCiteRegEx": "Tan et al\\.", "year": 2015}, {"title": "Convolutional neural networks vs", "author": ["Kateryna Tymoshenko", "Daniele Bonadiman", "Alessandro Moschitti."], "venue": "convolution kernels: Feature engineering for answer sentence reranking. In Proceedings of NAACL-HLT, pages 1268\u20131278.", "citeRegEx": "Tymoshenko et al\\.,? 2016", "shortCiteRegEx": "Tymoshenko et al\\.", "year": 2016}, {"title": "A deep architecture for semantic matching with multiple positional sentence representations", "author": ["Shengxian Wan", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Liang Pang", "Xueqi Cheng."], "venue": "Proceedings of AAAI, pages 2835\u20132841.", "citeRegEx": "Wan et al\\.,? 2016", "shortCiteRegEx": "Wan et al\\.", "year": 2016}, {"title": "Learning natural language inference with LSTM", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "Proceedings of NAACL, pages 1442\u20131451.", "citeRegEx": "Wang and Jiang.,? 2016", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Probabilistic tree-edit models with structured latent variables for textual entailment and question answering", "author": ["Mengqiu Wang", "Christopher D Manning."], "venue": "Proceedings of Coling, pages 1164\u20131172.", "citeRegEx": "Wang and Manning.,? 2010", "shortCiteRegEx": "Wang and Manning.", "year": 2010}, {"title": "Probase: A probabilistic taxonomy for text understanding", "author": ["Wentao Wu", "Hongsong Li", "Haixun Wang", "Kenny Q Zhu."], "venue": "Proceedings of SIGMOD, pages 481\u2013492.", "citeRegEx": "Wu et al\\.,? 2012", "shortCiteRegEx": "Wu et al\\.", "year": 2012}, {"title": "On the generalization error bounds of neural networks under diversity-inducing mutual angular regularization", "author": ["Pengtao Xie", "Yuntian Deng", "Eric Xing."], "venue": "arXiv preprint arXiv:1511.07110.", "citeRegEx": "Xie et al\\.,? 2015", "shortCiteRegEx": "Xie et al\\.", "year": 2015}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yi Yang", "Wen-tau Yih", "Christopher Meek."], "venue": "Proceedings of EMNLP, pages 2013\u20132018.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Semi-markov phrasebased monolingual alignment", "author": ["Xuchen Yao", "Benjamin Van Durme", "Chris CallisonBurch", "Peter Clark."], "venue": "Proceedings of EMNLP, pages 590\u2013600.", "citeRegEx": "Yao et al\\.,? 2013", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Question answering using enhanced lexical semantic models", "author": ["Wen-tau Yih", "Ming-Wei Chang", "Christopher Meek", "Andrzej Pastusiak."], "venue": "Proceedings of ACL, pages 1744\u20131753.", "citeRegEx": "Yih et al\\.,? 2013", "shortCiteRegEx": "Yih et al\\.", "year": 2013}, {"title": "Convolutional neural network for paraphrase identification", "author": ["Wenpeng Yin", "Hinrich Sch\u00fctze."], "venue": "Proceedings of NAACL, pages 901\u2013911, May\u2013 June.", "citeRegEx": "Yin and Sch\u00fctze.,? 2015a", "shortCiteRegEx": "Yin and Sch\u00fctze.", "year": 2015}, {"title": "Multigrancnn: An architecture for general matching of text chunks on multiple levels of granularity", "author": ["Wenpeng Yin", "Hinrich Sch\u00fctze."], "venue": "Proceedings of ACL-IJCNLP, pages 63\u201373.", "citeRegEx": "Yin and Sch\u00fctze.,? 2015b", "shortCiteRegEx": "Yin and Sch\u00fctze.", "year": 2015}, {"title": "ABCNN: Attention-based convolutional neural network for modeling sentence pairs", "author": ["Wenpeng Yin", "Hinrich Sch\u00fctze", "Bing Xiang", "Bowen Zhou."], "venue": "TACL, 4:259\u2013272.", "citeRegEx": "Yin et al\\.,? 2016a", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "Simple question answering by attentive convolutional neural network", "author": ["Wenpeng Yin", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Hinrich Sch\u00fctze."], "venue": "Proceedings of COLING, pages 1746\u20131756.", "citeRegEx": "Yin et al\\.,? 2016b", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "Deep learning for answer sentence selection", "author": ["Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman."], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "Yu et al\\.,? 2014", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment", "author": ["Jiang Zhao", "Tian Tian Zhu", "Man Lan."], "venue": "SemEval, pages 271\u2013277.", "citeRegEx": "Zhao et al\\.,? 2014", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "How to model a pair of sentences is a critical issue in many NLP tasks, including textual entailment (Marelli et al., 2014a; Bowman et al., 2015a; Yin et al., 2016a) and answer selection (Yu et al.", "startOffset": 101, "endOffset": 165}, {"referenceID": 39, "context": "How to model a pair of sentences is a critical issue in many NLP tasks, including textual entailment (Marelli et al., 2014a; Bowman et al., 2015a; Yin et al., 2016a) and answer selection (Yu et al.", "startOffset": 101, "endOffset": 165}, {"referenceID": 41, "context": ", 2016a) and answer selection (Yu et al., 2014; Yang et al., 2015; Santos et al., 2016).", "startOffset": 30, "endOffset": 87}, {"referenceID": 34, "context": ", 2016a) and answer selection (Yu et al., 2014; Yang et al., 2015; Santos et al., 2016).", "startOffset": 30, "endOffset": 87}, {"referenceID": 25, "context": ", 2016a) and answer selection (Yu et al., 2014; Yang et al., 2015; Santos et al., 2016).", "startOffset": 30, "endOffset": 87}, {"referenceID": 36, "context": "Alignments at word level (Yih et al., 2013) or phrase level (Yao et al.", "startOffset": 25, "endOffset": 43}, {"referenceID": 35, "context": ", 2013) or phrase level (Yao et al., 2013) both have been studied before.", "startOffset": 24, "endOffset": 42}, {"referenceID": 21, "context": "(2013) make use of WordNet (Miller, 1995) and Probase (Wu et al.", "startOffset": 27, "endOffset": 41}, {"referenceID": 32, "context": "(2013) make use of WordNet (Miller, 1995) and Probase (Wu et al., 2012) for identifying hyper- and hyponymy.", "startOffset": 54, "endOffset": 71}, {"referenceID": 33, "context": ", 2013) or phrase level (Yao et al., 2013) both have been studied before. For example, Yih et al. (2013) make use of WordNet (Miller, 1995) and Probase (Wu et al.", "startOffset": 25, "endOffset": 105}, {"referenceID": 21, "context": "(2013) make use of WordNet (Miller, 1995) and Probase (Wu et al., 2012) for identifying hyper- and hyponymy. Yao et al. (2013) use POS tags, WordNet and paraphrase database for alignment identification.", "startOffset": 28, "endOffset": 127}, {"referenceID": 1, "context": "DNNs have been intensively investigated in sentence pair classifications (Blacoe and Lapata, 2012; Socher et al., 2011; Yin and Sch\u00fctze, 2015b), and attention mechanisms are also applied to individual tasks (Santos et al.", "startOffset": 73, "endOffset": 143}, {"referenceID": 26, "context": "DNNs have been intensively investigated in sentence pair classifications (Blacoe and Lapata, 2012; Socher et al., 2011; Yin and Sch\u00fctze, 2015b), and attention mechanisms are also applied to individual tasks (Santos et al.", "startOffset": 73, "endOffset": 143}, {"referenceID": 38, "context": "DNNs have been intensively investigated in sentence pair classifications (Blacoe and Lapata, 2012; Socher et al., 2011; Yin and Sch\u00fctze, 2015b), and attention mechanisms are also applied to individual tasks (Santos et al.", "startOffset": 73, "endOffset": 143}, {"referenceID": 25, "context": ", 2011; Yin and Sch\u00fctze, 2015b), and attention mechanisms are also applied to individual tasks (Santos et al., 2016; Rockt\u00e4schel et al., 2016; Wang and Jiang, 2016); however, most attention-based DNNs have implicit assumption that stronger alignments deserve more attention (Yin et al.", "startOffset": 95, "endOffset": 164}, {"referenceID": 23, "context": ", 2011; Yin and Sch\u00fctze, 2015b), and attention mechanisms are also applied to individual tasks (Santos et al., 2016; Rockt\u00e4schel et al., 2016; Wang and Jiang, 2016); however, most attention-based DNNs have implicit assumption that stronger alignments deserve more attention (Yin et al.", "startOffset": 95, "endOffset": 164}, {"referenceID": 30, "context": ", 2011; Yin and Sch\u00fctze, 2015b), and attention mechanisms are also applied to individual tasks (Santos et al., 2016; Rockt\u00e4schel et al., 2016; Wang and Jiang, 2016); however, most attention-based DNNs have implicit assumption that stronger alignments deserve more attention (Yin et al.", "startOffset": 95, "endOffset": 164}, {"referenceID": 39, "context": ", 2016; Wang and Jiang, 2016); however, most attention-based DNNs have implicit assumption that stronger alignments deserve more attention (Yin et al., 2016a; Santos et al., 2016; Yin et al., 2016b).", "startOffset": 139, "endOffset": 198}, {"referenceID": 25, "context": ", 2016; Wang and Jiang, 2016); however, most attention-based DNNs have implicit assumption that stronger alignments deserve more attention (Yin et al., 2016a; Santos et al., 2016; Yin et al., 2016b).", "startOffset": 139, "endOffset": 198}, {"referenceID": 40, "context": ", 2016; Wang and Jiang, 2016); however, most attention-based DNNs have implicit assumption that stronger alignments deserve more attention (Yin et al., 2016a; Santos et al., 2016; Yin et al., 2016b).", "startOffset": 139, "endOffset": 198}, {"referenceID": 39, "context": "We can treat the pre-processing in (Yin et al., 2016a) as a hard way, and ours as a soft way, as our phrases have more flexible lengths and the existence of overlapping phrases decreases the risk of losing important alignments.", "startOffset": 35, "endOffset": 54}, {"referenceID": 5, "context": "(i) We use GRU (Gated Recurrent Unit (Cho et al., 2014)) to learn representations for phrases of arbitrary granularity.", "startOffset": 37, "endOffset": 55}, {"referenceID": 1, "context": "DNNs have been intensively investigated in sentence pair classifications (Blacoe and Lapata, 2012; Socher et al., 2011; Yin and Sch\u00fctze, 2015b), and attention mechanisms are also applied to individual tasks (Santos et al., 2016; Rockt\u00e4schel et al., 2016; Wang and Jiang, 2016); however, most attention-based DNNs have implicit assumption that stronger alignments deserve more attention (Yin et al., 2016a; Santos et al., 2016; Yin et al., 2016b). Our examples in Figure 1, instead, show that this assumption does not hold invariably. Weaker alignments in certain tasks such as TE can be the indicator of the final decision. Our inspiration comes from the analysis of some prior work. For TE, Yin et al. (2016a) show that considering the pairs in which overlapping tokens are removed can give a boost.", "startOffset": 74, "endOffset": 711}, {"referenceID": 1, "context": "DNNs have been intensively investigated in sentence pair classifications (Blacoe and Lapata, 2012; Socher et al., 2011; Yin and Sch\u00fctze, 2015b), and attention mechanisms are also applied to individual tasks (Santos et al., 2016; Rockt\u00e4schel et al., 2016; Wang and Jiang, 2016); however, most attention-based DNNs have implicit assumption that stronger alignments deserve more attention (Yin et al., 2016a; Santos et al., 2016; Yin et al., 2016b). Our examples in Figure 1, instead, show that this assumption does not hold invariably. Weaker alignments in certain tasks such as TE can be the indicator of the final decision. Our inspiration comes from the analysis of some prior work. For TE, Yin et al. (2016a) show that considering the pairs in which overlapping tokens are removed can give a boost. This simple trick matches our motivation that weaker alignment should be given more attention in TE. However, Yin et al. (2016a) remove overlapping tokens completely, potentially obscuring complex alignment configurations.", "startOffset": 74, "endOffset": 930}, {"referenceID": 1, "context": "DNNs have been intensively investigated in sentence pair classifications (Blacoe and Lapata, 2012; Socher et al., 2011; Yin and Sch\u00fctze, 2015b), and attention mechanisms are also applied to individual tasks (Santos et al., 2016; Rockt\u00e4schel et al., 2016; Wang and Jiang, 2016); however, most attention-based DNNs have implicit assumption that stronger alignments deserve more attention (Yin et al., 2016a; Santos et al., 2016; Yin et al., 2016b). Our examples in Figure 1, instead, show that this assumption does not hold invariably. Weaker alignments in certain tasks such as TE can be the indicator of the final decision. Our inspiration comes from the analysis of some prior work. For TE, Yin et al. (2016a) show that considering the pairs in which overlapping tokens are removed can give a boost. This simple trick matches our motivation that weaker alignment should be given more attention in TE. However, Yin et al. (2016a) remove overlapping tokens completely, potentially obscuring complex alignment configurations. In addition, Yin et al. (2016a) use the same attention mechanism for TE and AS, which is less optimal based on our observations.", "startOffset": 74, "endOffset": 1056}, {"referenceID": 8, "context": "Heilman and Smith (2010) describe tree edit models that generalize tree edit distance by allowing operations that better account for complex reordering phenomena and by learning from data how different edits should affect the model\u2019s decisions about sentence relations.", "startOffset": 0, "endOffset": 25}, {"referenceID": 8, "context": "Heilman and Smith (2010) describe tree edit models that generalize tree edit distance by allowing operations that better account for complex reordering phenomena and by learning from data how different edits should affect the model\u2019s decisions about sentence relations. Wang and Manning (2010) cope with the alignment between a sentence pair by using a probabilistic model that models tree-edit operations on dependency parse trees.", "startOffset": 0, "endOffset": 294}, {"referenceID": 8, "context": "Guo and Diab (2012) identify the degree of sentence similarity by modeling the missing words (words that are not in the sentence) so as to relieve the sparseness issue of sentence modeling.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "(2013) try to improve the shallow semantic component, lexical semantics, by formulating sentence pair as a semantic matching problem with a latent word-alignment structure as in (Chang et al., 2010).", "startOffset": 178, "endOffset": 198}, {"referenceID": 15, "context": "More fine-grained word overlap and alignment between two sentences are explored in (Lai and Hockenmaier, 2014), in which negation, hypernym/hyponym, synonym and antonym relations are used.", "startOffset": 83, "endOffset": 110}, {"referenceID": 20, "context": "(2015b) employ recursive DNN to encode entailment on SICK (Marelli et al., 2014b).", "startOffset": 58, "endOffset": 81}, {"referenceID": 2, "context": "(2016) present an attention-based LSTM (long short-term memory, Hochreiter and Schmidhuber (1997)) for the SNLI corpus (Bowman et al., 2015a).", "startOffset": 119, "endOffset": 141}, {"referenceID": 2, "context": "(2013) try to improve the shallow semantic component, lexical semantics, by formulating sentence pair as a semantic matching problem with a latent word-alignment structure as in (Chang et al., 2010). More fine-grained word overlap and alignment between two sentences are explored in (Lai and Hockenmaier, 2014), in which negation, hypernym/hyponym, synonym and antonym relations are used. Yao et al. (2013) extend word-toword alignment to phrase-to-phrase alignment by a semi-Markov CRF.", "startOffset": 179, "endOffset": 407}, {"referenceID": 2, "context": "For TE, Bowman et al. (2015b) employ recursive DNN to encode entailment on SICK (Marelli et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 2, "context": "For TE, Bowman et al. (2015b) employ recursive DNN to encode entailment on SICK (Marelli et al., 2014b). Rockt\u00e4schel et al. (2016) present an attention-based LSTM (long short-term memory, Hochreiter and Schmidhuber (1997)) for the SNLI corpus (Bowman et al.", "startOffset": 8, "endOffset": 131}, {"referenceID": 2, "context": "For TE, Bowman et al. (2015b) employ recursive DNN to encode entailment on SICK (Marelli et al., 2014b). Rockt\u00e4schel et al. (2016) present an attention-based LSTM (long short-term memory, Hochreiter and Schmidhuber (1997)) for the SNLI corpus (Bowman et al.", "startOffset": 8, "endOffset": 222}, {"referenceID": 16, "context": "(2014) present a bigram CNN (convolutional neural network (LeCun et al., 1998)) to model question and answer candidates.", "startOffset": 58, "endOffset": 78}, {"referenceID": 26, "context": "Other sentence matching tasks such as paraphrase identification (Socher et al., 2011; Yin and Sch\u00fctze, 2015a), question \u2013 Freebase fact matching (Yin et al.", "startOffset": 64, "endOffset": 109}, {"referenceID": 37, "context": "Other sentence matching tasks such as paraphrase identification (Socher et al., 2011; Yin and Sch\u00fctze, 2015a), question \u2013 Freebase fact matching (Yin et al.", "startOffset": 64, "endOffset": 109}, {"referenceID": 40, "context": ", 2011; Yin and Sch\u00fctze, 2015a), question \u2013 Freebase fact matching (Yin et al., 2016b) etc.", "startOffset": 67, "endOffset": 86}, {"referenceID": 30, "context": "For AS, Yu et al. (2014) present a bigram CNN (convolutional neural network (LeCun et al.", "startOffset": 8, "endOffset": 25}, {"referenceID": 14, "context": "(2014) present a bigram CNN (convolutional neural network (LeCun et al., 1998)) to model question and answer candidates. Yang et al. (2015) extend this method and get state-of-the-art performance on the WikiQA dataset.", "startOffset": 59, "endOffset": 140}, {"referenceID": 7, "context": "Feng et al. (2015) test various setups of a bi-CNN architecture on an insurance domain QA dataset.", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "Feng et al. (2015) test various setups of a bi-CNN architecture on an insurance domain QA dataset. Tan et al. (2015) explore bidirectional LSTM on the same dataset.", "startOffset": 0, "endOffset": 117}, {"referenceID": 7, "context": "Feng et al. (2015) test various setups of a bi-CNN architecture on an insurance domain QA dataset. Tan et al. (2015) explore bidirectional LSTM on the same dataset. Other sentence matching tasks such as paraphrase identification (Socher et al., 2011; Yin and Sch\u00fctze, 2015a), question \u2013 Freebase fact matching (Yin et al., 2016b) etc. are also investigated. Some prior work aims to solve a general sentence matching problem. Hu et al. (2014) present two CNN architectures for paraphrasing, sentence completion (SC), tweet-response matching tasks.", "startOffset": 0, "endOffset": 442}, {"referenceID": 7, "context": "Feng et al. (2015) test various setups of a bi-CNN architecture on an insurance domain QA dataset. Tan et al. (2015) explore bidirectional LSTM on the same dataset. Other sentence matching tasks such as paraphrase identification (Socher et al., 2011; Yin and Sch\u00fctze, 2015a), question \u2013 Freebase fact matching (Yin et al., 2016b) etc. are also investigated. Some prior work aims to solve a general sentence matching problem. Hu et al. (2014) present two CNN architectures for paraphrasing, sentence completion (SC), tweet-response matching tasks. Yin and Sch\u00fctze (2015b) propose the MultiGranCNN architecture to model general sentence matching based on phrase matching on multiple levels of granularity.", "startOffset": 0, "endOffset": 571}, {"referenceID": 7, "context": "Feng et al. (2015) test various setups of a bi-CNN architecture on an insurance domain QA dataset. Tan et al. (2015) explore bidirectional LSTM on the same dataset. Other sentence matching tasks such as paraphrase identification (Socher et al., 2011; Yin and Sch\u00fctze, 2015a), question \u2013 Freebase fact matching (Yin et al., 2016b) etc. are also investigated. Some prior work aims to solve a general sentence matching problem. Hu et al. (2014) present two CNN architectures for paraphrasing, sentence completion (SC), tweet-response matching tasks. Yin and Sch\u00fctze (2015b) propose the MultiGranCNN architecture to model general sentence matching based on phrase matching on multiple levels of granularity. Wan et al. (2016) try to match two sentences in AS and SC by multiple sentence representations, each coming from the local representations of two LSTMs.", "startOffset": 0, "endOffset": 722}, {"referenceID": 0, "context": ", in machine translation (Bahdanau et al., 2015; Luong et al., 2015) and text reconstruction (Li et al.", "startOffset": 25, "endOffset": 68}, {"referenceID": 18, "context": ", in machine translation (Bahdanau et al., 2015; Luong et al., 2015) and text reconstruction (Li et al.", "startOffset": 25, "endOffset": 68}, {"referenceID": 17, "context": ", 2015) and text reconstruction (Li et al., 2015; Rush et al., 2015).", "startOffset": 32, "endOffset": 68}, {"referenceID": 24, "context": ", 2015) and text reconstruction (Li et al., 2015; Rush et al., 2015).", "startOffset": 32, "endOffset": 68}, {"referenceID": 0, "context": ", in machine translation (Bahdanau et al., 2015; Luong et al., 2015) and text reconstruction (Li et al., 2015; Rush et al., 2015). In addition, attention-based alignment is also applied in natural language inference (e.g., Rockt\u00e4schel et al. (2016),Wang and Jiang (2016)).", "startOffset": 26, "endOffset": 249}, {"referenceID": 0, "context": ", in machine translation (Bahdanau et al., 2015; Luong et al., 2015) and text reconstruction (Li et al., 2015; Rush et al., 2015). In addition, attention-based alignment is also applied in natural language inference (e.g., Rockt\u00e4schel et al. (2016),Wang and Jiang (2016)).", "startOffset": 26, "endOffset": 271}, {"referenceID": 6, "context": "According to empirical evaluations in (Chung et al., 2014; Jozefowicz et al., 2015), there is not a clear winner.", "startOffset": 38, "endOffset": 83}, {"referenceID": 13, "context": "According to empirical evaluations in (Chung et al., 2014; Jozefowicz et al., 2015), there is not a clear winner.", "startOffset": 38, "endOffset": 83}, {"referenceID": 22, "context": "For both TE and AS, words are initialized by 300dimensional GloVe embeddings1 (Pennington et al., 2014) and not changed during training.", "startOffset": 78, "endOffset": 103}, {"referenceID": 14, "context": "We use ADAM (Kingma and Ba, 2015), with a first momentum coefficient of 0.", "startOffset": 12, "endOffset": 33}, {"referenceID": 33, "context": "999,2 L2 regularization and Diversity Regularization (Xie et al., 2015).", "startOffset": 53, "endOffset": 71}, {"referenceID": 14, "context": "We use ADAM (Kingma and Ba, 2015), with a first momentum coefficient of 0.9 and a second momentum coefficient of 0.999,2 L2 regularization and Diversity Regularization (Xie et al., 2015). Table 1 shows the values of the hyperparameters, tuned on dev. Classifier. Following Yin et al. (2016a), we use three classifiers \u2013 logistic regression in DNN, logistic regression and linear SVM with default parameters3 directly on the feature vector \u2013 and report performance of the best.", "startOffset": 13, "endOffset": 292}, {"referenceID": 23, "context": "The pioneering attention based LSTM system for a specific sentence pair classification task \u201cnatural language inference\u201d (Rockt\u00e4schel et al., 2016).", "startOffset": 121, "endOffset": 147}, {"referenceID": 39, "context": "(iii) ABCNN (Yin et al., 2016a).", "startOffset": 12, "endOffset": 31}, {"referenceID": 20, "context": ", 2014a) evaluates system predictions of textual entailment (TE) relations on sentence pairs from the SICK dataset (Marelli et al., 2014b).", "startOffset": 115, "endOffset": 138}, {"referenceID": 39, "context": "We choose SICK benchmark dataset so that our result is directly comparable with that of (Yin et al., 2016a), in which nonoverlapping text are utilized explicitly to boost the performance.", "startOffset": 88, "endOffset": 107}, {"referenceID": 15, "context": "Following Lai and Hockenmaier (2014), we train our final system (after fixing of hyperparameters) on train and dev (4,934 pairs).", "startOffset": 10, "endOffset": 37}, {"referenceID": 12, "context": "To p3 (Jimenez et al., 2014) 83.", "startOffset": 6, "endOffset": 28}, {"referenceID": 42, "context": "1 (Zhao et al., 2014) 83.", "startOffset": 2, "endOffset": 21}, {"referenceID": 15, "context": "6 (Lai and Hockenmaier, 2014) 84.", "startOffset": 2, "endOffset": 29}, {"referenceID": 3, "context": "6 TrRNTN (Bowman et al., 2015b) 76.", "startOffset": 9, "endOffset": 31}, {"referenceID": 39, "context": "7 ABCNN (Yin et al., 2016a) 86.", "startOffset": 8, "endOffset": 27}, {"referenceID": 39, "context": "We include the same 22 linguistic features as Yin et al. (2016a). They cover 15 machine translation metrics between the two sentences; whether or not the two sentences contain negation tokens like \u201cno\u201d, \u201cnot\u201d etc; whether or not they contain synonyms, hypernyms or antonyms; two sentence lengths.", "startOffset": 46, "endOffset": 65}, {"referenceID": 39, "context": "We include the same 22 linguistic features as Yin et al. (2016a). They cover 15 machine translation metrics between the two sentences; whether or not the two sentences contain negation tokens like \u201cno\u201d, \u201cnot\u201d etc; whether or not they contain synonyms, hypernyms or antonyms; two sentence lengths. See Yin et al. (2016a) for details.", "startOffset": 46, "endOffset": 320}, {"referenceID": 39, "context": "ABCNN (Yin et al., 2016a) is based on assumptions similar to k-max-max-pooling: words/phrases with higher matching values should contribute more in this task.", "startOffset": 6, "endOffset": 25}, {"referenceID": 28, "context": "88 (MRR) in (Tymoshenko et al., 2016)", "startOffset": 12, "endOffset": 37}, {"referenceID": 34, "context": "Apart from the common baselines Addition, ALSTM and ABCNN, we compare further with: (i) CNN-Cnt (Yang et al., 2015): combine CNN with two linguistic features \u201cWordCnt\u201d (the number of non-stopwords in the question that also occur in the answer) and \u201cWgtWordCnt\u201d (reweight the counts by the IDF values of the question words); (ii) AP-CNN (Santos et al.", "startOffset": 96, "endOffset": 115}, {"referenceID": 25, "context": ", 2015): combine CNN with two linguistic features \u201cWordCnt\u201d (the number of non-stopwords in the question that also occur in the answer) and \u201cWgtWordCnt\u201d (reweight the counts by the IDF values of the question words); (ii) AP-CNN (Santos et al., 2016).", "startOffset": 228, "endOffset": 249}, {"referenceID": 33, "context": "Following Yang et al. (2015), we truncate answers to 40 tokens and report mean average precision (MAP) and mean reciprocal rank (MRR).", "startOffset": 10, "endOffset": 29}, {"referenceID": 34, "context": "ms/WikiQA (Yang et al., 2015)", "startOffset": 10, "endOffset": 29}, {"referenceID": 39, "context": "GRU with k-max-maxpooling has similar assumption with ABCNN (Yin et al., 2016a) and AP-CNN (Santos et al.", "startOffset": 60, "endOffset": 79}, {"referenceID": 25, "context": ", 2016a) and AP-CNN (Santos et al., 2016): units with higher matching scores are supposed to contribute more in this task.", "startOffset": 20, "endOffset": 41}], "year": 2017, "abstractText": "This work studies comparatively two typical sentence matching tasks: textual entailment (TE) and answer selection (AS), observing that weaker phrase alignments are more critical in TE, while stronger phrase alignments deserve more attention in AS. The key to reach this observation lies in phrase detection, phrase representation, phrase alignment, and more importantly how to connect those aligned phrases of different matching degrees with the final classifier. Prior work (i) has limitations in phrase generation and representation, or (ii) conducts alignment at word and phrase levels by handcrafted features or (iii) utilizes a single framework of alignment without considering the characteristics of specific tasks, which limits the framework\u2019s effectiveness across tasks. We propose an architecture based on Gated Recurrent Unit that supports (i) representation learning of phrases of arbitrary granularity and (ii) task-specific attentive pooling of phrase alignments between two sentences. Experimental results on TE and AS match our observation and show the effectiveness of our approach.", "creator": "TeX"}}}