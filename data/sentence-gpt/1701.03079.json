{"id": "1701.03079", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2017", "title": "RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain Dialog Systems", "abstract": "Open-domain human-computer conversation has been attracting increasing attention over the past few years. However, there does not exist a standard automatic evaluation metric for open-domain dialog systems; researchers usually resort to human annotation for model evaluation, which is time- and labor-intensive. In this paper, we propose RUBER, a Referenced metric and Unreferenced metric Blended Evaluation Routine, which evaluates a reply by taking into consideration both a groundtruth reply and a query (previous user utterance). Our metric is learnable, but its training does not require labels of human satisfaction. Hence, RUBER is flexible and extensible to different datasets and languages. Experiments on both retrieval and generative dialog systems show that RUBER has high correlation with human annotation.", "histories": [["v1", "Wed, 11 Jan 2017 17:43:57 GMT  (508kb,D)", "http://arxiv.org/abs/1701.03079v1", null], ["v2", "Sun, 16 Jul 2017 16:43:08 GMT  (4482kb,D)", "http://arxiv.org/abs/1701.03079v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.HC cs.IR", "authors": ["chongyang tao", "lili mou", "dongyan zhao", "rui yan"], "accepted": false, "id": "1701.03079"}, "pdf": {"name": "1701.03079.pdf", "metadata": {"source": "CRF", "title": "RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain Dialog Systems", "authors": ["Chongyang Tao", "Lili Mou", "Dongyan Zhao", "Rui Yan"], "emails": ["chongyangtao@pku.edu.cn", "zhaody@pku.edu.cn", "ruiyan@pku.edu.cn", "doublepower.mou@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Automatic evaluation is crucial to the research of open-domain human-computer conversation systems. Nowadays, open-domain conversation is attracting increasing attention because of its wide applications (Bickmore and Picard, 2005; Bessho et al., 2012; Shang et al., 2015; Yan et al., 2016). In these studies, however, researchers typically resort to manual annotation to evaluate their models, which is expensive and time-consuming. Therefore, automatic evaluation metrics are particularly in need, so as to ease the burden of model comparison and to promote further research on this topic.\nIn early years, traditional vertical-domain dialog systems use automatic metrics like slot-filling accuracy and goal-completion rate (Walker et al.,\n1997, 2001; Schatzmann et al., 2005). Unfortunately, such evaluation hardly applies to the open domain due to the diversity and uncertainty of utterances: \u201caccurancy\u201d and \u201ccompletion,\u201d for example, make little sense in open-domain conversation.\nPrevious studies in several language generation tasks have developed successful automatic evaluation metrics, e.g., BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) for machine translation, and ROUGE (Lin, 2004) for summarization. For conversation systems, researchers occassionally adopt these metrics for evaluation (Ritter et al., 2011; Li et al., 2015; Song et al., 2016). However, Liu et al. (2016) conduct extensive empirical experiments and show weak correlation between existing metrics and human annotation.\nVery recently, Lowe et al. (2017) propose a neural network-based metric for conversation systems; it learns to predict a score of a reply given its query (previous user-issued utterance) and a groundtruth reply. But such approach requires massive human-annotated scores to train the network, and thus is less flexible and extensible.\nIn this paper, we propose RUBER, a Referenced metric and Unreferenced metric Blended Evaluation Routine for open-domain dialog systems. RUBER has the following distinct features: \u2022 An embedding-based scorer measures the\nsimilarity between a generated reply and the groundtruth. We call this a referenced metric, because it uses the groundtruth as a reference, akin to existing evaluation metrics. Instead of using word-overlapping information (e.g., in BLEU and ROUGE), we measure the similarity by pooling of word embeddings (Forgues et al., 2014); it is more suited to dialog systems due to the diversity of replies. \u2022 A neural network-based scorer measures the\nrelatedness between the generated reply and\nar X\niv :1\n70 1.\n03 07\n9v 1\n[ cs\n.C L\n] 1\n1 Ja\nn 20\n17\n\ud835\udc40 Bi-GRU RNN Query Reply\n\ud835\udc53(\ud835\udc5e, \ud835\udc5f) Word Embedding C N N + M a xP o o li n g\nC N\nN + M a xP o o li n g\nEmbedding Extrema\nQuery\nGenerated Reply\nGroundtruth Reply\nNN Scorer\nBlending\nUnreferenced\nRUBER Metric\nReferenced \ud835\udc5f\n\ud835\udc5e\n\ud835\udc5f\nReferenced Scorer\nQuery\nGenerated Reply\nGroundtruth Reply\nBlending\nRUBER Metric\n\ud835\udc5f\n\ud835\udc5e\n\ud835\udc5f Unreferenced\nScorer\nWe evaluated RUBER on prevailing dialog systems, including both retrieval and generative ones. Experiments show that RUBER significantly outperforms existing automatic metrics in terms of the Pearson and Spearman correlation against human judgments."}, {"heading": "2 Methodology", "text": "Figure 1 shows the overall design methodology of our RUBER metric. We introduce the referenced and unreferenced metrics in Subsections 2.1 and 2.2, respectively. Subsection 2.3 discusses how they are combined."}, {"heading": "2.1 Referenced Metric", "text": "We measure the similarity between a generated reply r\u0302 and a groundtruth r as a referenced metric. Traditional referenced metrics typically use wordoverlapping information including both precision (e.g., BLEU) and recall (e.g., ROUGE). As said, they may not be appropriate for open-domain dialog systems.\nWe adopt the vector pooling approach that summarizes sentence information by choosing the maximum and minimum values in each dimension; the closeness of a sentence pair is measured\n\ud835\udc40\nBi-GRU RNN\nQuery\nReply\n\ud835\udc60\ud835\udc48(\ud835\udc5e, \ud835\udc5f)\nWord Embedding\nReferenced Scorer\nQuery\nGenerated Reply\nGroundtruth Reply\nBlending\nRUBER Metric\n\ud835\udc5f\n\ud835\udc5e\n\ud835\udc5f Unreferenced\nScorer\nArxiv\nFigure 2: The neural network predicting the unreferenced score.\nby the cosine score. We use such heuristic matching because we assume no groundtruth scores, making it unfeasible to train a parametric model.\nFormally, let w1,w2, \u00b7 \u00b7 \u00b7 ,wn be the embeddings of words in a sentence, max pooling summarizes the maximum value as\nvmax[i] = max { w1[i],w2[i], \u00b7 \u00b7 \u00b7 ,wn[i] } (1)\nwhere [\u00b7] indexes a dimension of a vector. Likewise, min pooling yields a vector vmin. Because an embedding feature is symmetric in terms of its sign, we concatenate both max and min pooling vectors as v = [vmax;vmin].\nLet vr\u0302 be the generated reply\u2019s sentence vector and vr be that of the groundtruth reply, both obtained by max and min pooling. The referenced metric sR measures the similarity between r and r\u0302 by\nsR(r, r\u0302) = cos(vr, v\u0302r\u0302) = v>r v\u0302r\u0302\n\u2016vr\u2016 \u00b7 \u2016v\u0302r\u0302\u2016 (2)\nNotice that the pooling approach tends to address uncommon words more than common ones. During unsupervised word embedding learning, common words\u2019 vectors are pulled to the origin (near zero) because these words appear frequently in different contexts. Choosing the maximum and minimum values in each dimension extracts more information about uncommon words. Also, such treatment is more robust than vector extrema (Forgues et al., 2014); it chooses either the largest positive or smallest negative value, where features are more vulnerable to signs."}, {"heading": "2.2 Unreferenced Metric", "text": "We then measure the relatedness between the generated reply r\u0302 and its query q. This metric is unreferenced and denoted as sU (q, r\u0302), because it does not refer to a groundtruth reply.\nDifferent from the r-r\u0302 metric, which mainly measures two utterances\u2019 similarity, the q-r\u0302 metric in this part involves more semantics. Hence,\nwe empirically design a neural network (Figure 2) to predict the appropriateness of a reply with respect to a query.\nConcretely, each word in a query q and a reply r is mapped to an embedding; a bidirectional recurrent neural network with gated recurrent units (BiGRU RNN) captures information along the word sequence. The forward RNN takes the form\nrt = \u03c3(Wrxt + Urh \u2192 t\u22121 + br) (3) zt = \u03c3(Wzxt + Uzh \u2192 t\u22121 + bz) (4)\nh\u0303t = tanh ( Whxt + Uh(rt \u25e6 h\u2192t\u22121) + bh ) (5) h\u2192t = (1\u2212 zt) \u25e6 h\u2192t\u22121 + zt \u25e6 h\u0303t (6)\nwhere xt is the current input, and h\u2192t is the hidden state. Likewise, the backward RNN gives hidden states h\u2190t . The last states of both directions are concatenated as the sentence embedding (q for a query and r for a reply).\nWe further concatenate q and r to match the two utterances. Besides, we also include a \u201cquadratic feature\u201d as q>Mr, where M is a parameter matrix.\nFinally, a multi-layer perceptron (MLP) predicts a scalar score as our unreferenced metric sU . The hidden layer of MLP uses tanh as the activation function, whereas the last (scalar) unit uses sigmoid because we hope the score is bounded.\nThe above empirical structure is mainly inspired by several previous studies (Severyn and Moschitti, 2015; Yan et al., 2016). We may also apply other variants for utterance matching (Wang and Jiang, 2016; Mou et al., 2016); details are beyond the focus of this paper.\nTo train the neural network, we adopt negative sampling, which does not require human-labeled data. That is, given a groundtruth query reply pair, we randomly choose another reply, r\u2212, in the training set as a negative sample. We would like the score of a positive sample to be larger than that of a negative sample by at least a margin \u2206. The training objective is to minimize\nJ = max { 0,\u2206\u2212 sU (q, r) + sU (q, r\u2212) } (7)\nAll parameters are trained by Adam with backpropagation.\nIn previous work, researchers adopt negative sampling for utterance matching (Yan et al., 2016). Our study further verifies that negative sampling is useful for the evaluation task, which eases the burden of human annotation compared with fully\nsupervised approaches that requirer manual labels for training their metrics (Lowe et al., 2017)."}, {"heading": "2.3 Hybrid Evaluation", "text": "We combine the above two metrics by simple heuristics, resulting in a hybrid method RUBER for evaluating open-domain dialog systems.\nWe first normalize each metric to the range (0, 1), so that they are generally of the same scale. In particular, the normalization is given by\ns\u0303 = s\u2212min(s\u2032)\nmax(s\u2032)\u2212min(s\u2032) (8)\nwhere min(s\u2032) and max(s\u2032) refer to the maximum and minimum values, respectively, of a particular metric.\nThen we combine s\u0303R and s\u0303U as our ultimate RUBER metric by heuristics including min, max, geometric averaging, and arithmetic averaging. As we shall see in Section 3.2, different strategies yield similar results, consistently outperforming baselines."}, {"heading": "3 Experiments", "text": "In this section, we evaluate the correlation between our RUBER metric and human annotation, which is the ultimate goal of automatic metrics. The experiment was conducted on a Chinese corpus because of culture background, as human aspects are strongly involved in this paper. We nevertheless believe our evaluation routine could be applied to different languages."}, {"heading": "3.1 Setup", "text": "We crawled massive data from an online Chinese forum Douban.1 The training set contains 1,449,218 samples. We performed Chinese word segmentation, and obtained Chinese terms as primitive tokens.\nThe RUBER metric (along with baselines) is evaluated on two dialog systems. One is a featurebased retrieval-and-reranking system in Song et al. (2016), which first retrieves a coarse-grained candidate set by keyword matching and then reranks the candidates by human-engineered features; the top-ranked results are selected for evaluation. The other is a sequence-to-sequence neural network (Bahdanau et al., 2015) that encodes a query as a vector with an RNN and decodes the vector\n1http://www.douban.com\nto a reply with another RNN; the attention mechanism is also applied to enhance query-reply interaction.\nWe had 9 volunteers to express their human satisfaction of a reply (either retrieved or generated) to a query by rating an integer score among 0, 1, and 2. A score of 2 indicates a \u201cgood\u201d reply, 0 a bad reply, and 1 borderline."}, {"heading": "3.2 Results", "text": "Table 1 shows the Pearson and Spearman correlation between the proposed RUBER metric and human scores; also included are various baselines. Pearson and Spearman correlation measures are widely used in other research of automatic metrics such as machine translation (Stanojevic\u0301 et al., 2015).\nWe find that the reference metric sR based on mebeddings is more correlated with human annotation than existing metrics including both BLEU and ROUGE, which are based on word overlapping information. This implies the groundtruth alone is useful for evaluating a candidate reply. But exact word overlapping is too strict in the dialog setting; embedding-based methods measure sentence closeness in a \u201csoft\u201d way.\nThe referenced metric sU also achieves high correlation, showing that the query alone is also informative2 and that negative sampling is useful for training evaluation metrics, although it\n2Technically speaking, a dialog generator is also aware of the query. However, a discriminative model (scoring a queryreply pair) is more easy to train than a generative model (synthesizing a reply based on a query). There could also be possibilities of generative adversarial training.\ndoes not requirer human annotation as labels. Our neural network scorer also outperforms the embedding-based cosine measure. This is because cosine mainly captures similarity, but the rich semantic relationship between queries and replies necessitates more complicated mechanisms like neural networks.\nWe combine the referenced and unreferenced metrics as the ultimate RUBER approach. Experiments show that choosing the larger value of sR and sU (denoted as max) is too lenient, and is slightly worse than other strategies. Choosing the smaller value (min) are averaging (either geometric or arithmetic mean) yield similar results. While the peak performance is not consistent in two experiments, they significantly outperforms both single metrics, showing the rationale of using a hybrid metric for open-domain dialog systems.\nWe further notice that our RUBER metric has near-human correlation. More importantly, all components in RUBER are heuristic or unsupervised. Thus, RUBER does not requirer human labels; it is more flexible than the existing supervised metric (Lowe et al., 2017), and can be easily adapted to different datasets."}, {"heading": "4 Conclusion", "text": "In this paper, we proposed an evaluation methodology for open-domain dialog systems. Our metric is called RUBER (a Referenced metric and Unreferenced metric Blended Evaluation Routine), as it considers both groundtruth and its query. Experiments show that, although unsupervised, RUBER has strong correlation with human annotation."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Representation Learning.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments", "author": ["Satanjeev Banerjee", "Alon Lavie."], "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine", "citeRegEx": "Banerjee and Lavie.,? 2005", "shortCiteRegEx": "Banerjee and Lavie.", "year": 2005}, {"title": "Dialog system using real-time crowdsourcing and twitter large-scale corpus", "author": ["Fumihiro Bessho", "Tatsuya Harada", "Yasuo Kuniyoshi."], "venue": "Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue.", "citeRegEx": "Bessho et al\\.,? 2012", "shortCiteRegEx": "Bessho et al\\.", "year": 2012}, {"title": "Establishing and maintaining long-term humancomputer relationships", "author": ["Timothy W Bickmore", "Rosalind W Picard."], "venue": "ACM Transactions on Computer-Human Interaction 12(2):293\u2013327.", "citeRegEx": "Bickmore and Picard.,? 2005", "shortCiteRegEx": "Bickmore and Picard.", "year": 2005}, {"title": "Bootstrapping dialog systems with word embeddings", "author": ["Gabriel Forgues", "Joelle Pineau", "Jean-Marie Larchev\u00eaque", "R\u00e9al Tremblay."], "venue": "NIPS ML-NLP Workshop.", "citeRegEx": "Forgues et al\\.,? 2014", "shortCiteRegEx": "Forgues et al\\.", "year": 2014}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "ROUGE: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Text Summarization Branches out: Proceedings of the ACL-04 Workshop. pages 74\u201381. http://aclweb.org/anthology/W04-1013.", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian Serban", "Mike Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "Proceedings", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Towards an automatic turing test: Learning to evaluate dialogue responses", "author": ["Ryan Lowe", "Michael Noseworthy", "Iulian V. Serban", "Nicolas Angelard-Gontier", "Yoshua Bengio", "Joelle Pineau."], "venue": "https://openreview.net/pdf?id=HJ5PIaseg.", "citeRegEx": "Lowe et al\\.,? 2017", "shortCiteRegEx": "Lowe et al\\.", "year": 2017}, {"title": "Natural language inference by tree-based convolution and heuristic", "author": ["Lili Mou", "Rui Men", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin"], "venue": null, "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. pages 583\u2013593. http://aclweb.org/anthology/D11-1054.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Quantitative evaluation of user simulation techniques for spoken dialogue systems", "author": ["Jost Schatzmann", "Kallirroi Georgila", "Steve Young."], "venue": "Proceedings of the 6th Sigdial Workshop on Discourse an Dialogue. pages 45\u201354.", "citeRegEx": "Schatzmann et al\\.,? 2005", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2005}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["Aliaksei Severyn", "Alessandro Moschitti."], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. pages", "citeRegEx": "Severyn and Moschitti.,? 2015", "shortCiteRegEx": "Severyn and Moschitti.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Two are better than one: An ensemble of retrieval-and generation-based dialog systems", "author": ["Yiping Song", "Rui Yan", "Xiang Li", "Dongyan Zhao", "Ming Zhang."], "venue": "arXiv preprint arXiv:1610.07149 .", "citeRegEx": "Song et al\\.,? 2016", "shortCiteRegEx": "Song et al\\.", "year": 2016}, {"title": "Results of the wmt15 metrics shared task", "author": ["Milo\u0161 Stanojevi\u0107", "Amir Kamran", "Philipp Koehn", "Ond\u0159ej Bojar."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation. pages 256\u2013273. http://aclweb.org/anthology/W15-3031.", "citeRegEx": "Stanojevi\u0107 et al\\.,? 2015", "shortCiteRegEx": "Stanojevi\u0107 et al\\.", "year": 2015}, {"title": "PARADISE: A framework for evaluating spoken dialogue agents", "author": ["Marilyn A Walker", "Diane J Litman", "Candace A Kamm", "Alicia Abella."], "venue": "Proceedings of the 8th Conference on European Chapter of the Association", "citeRegEx": "Walker et al\\.,? 1997", "shortCiteRegEx": "Walker et al\\.", "year": 1997}, {"title": "Quantitative and qualitative evaluation of darpa communicator spoken dialogue systems", "author": ["Marilyn A Walker", "Rebecca Passonneau", "Julie E Boland."], "venue": "Proceedings of the 39th Annual Meeting on Association for Computational Linguistics. pages", "citeRegEx": "Walker et al\\.,? 2001", "shortCiteRegEx": "Walker et al\\.", "year": 2001}, {"title": "Learning natural language inference with lstm", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pages 1442\u20131451.", "citeRegEx": "Wang and Jiang.,? 2016", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Learning to rrespond with deep neural networks for retrievalbased human-computer conversation system", "author": ["Rui Yan", "Yiping Song", "Hua Wu."], "venue": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Infor-", "citeRegEx": "Yan et al\\.,? 2016", "shortCiteRegEx": "Yan et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "Nowadays, open-domain conversation is attracting increasing attention because of its wide applications (Bickmore and Picard, 2005; Bessho et al., 2012; Shang et al., 2015; Yan et al., 2016).", "startOffset": 103, "endOffset": 189}, {"referenceID": 2, "context": "Nowadays, open-domain conversation is attracting increasing attention because of its wide applications (Bickmore and Picard, 2005; Bessho et al., 2012; Shang et al., 2015; Yan et al., 2016).", "startOffset": 103, "endOffset": 189}, {"referenceID": 14, "context": "Nowadays, open-domain conversation is attracting increasing attention because of its wide applications (Bickmore and Picard, 2005; Bessho et al., 2012; Shang et al., 2015; Yan et al., 2016).", "startOffset": 103, "endOffset": 189}, {"referenceID": 20, "context": "Nowadays, open-domain conversation is attracting increasing attention because of its wide applications (Bickmore and Picard, 2005; Bessho et al., 2012; Shang et al., 2015; Yan et al., 2016).", "startOffset": 103, "endOffset": 189}, {"referenceID": 12, "context": "In early years, traditional vertical-domain dialog systems use automatic metrics like slot-filling accuracy and goal-completion rate (Walker et al., 1997, 2001; Schatzmann et al., 2005).", "startOffset": 133, "endOffset": 185}, {"referenceID": 10, "context": ", BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) for machine translation, and ROUGE (Lin, 2004) for summarization.", "startOffset": 7, "endOffset": 30}, {"referenceID": 1, "context": ", 2002) and METEOR (Banerjee and Lavie, 2005) for machine translation, and ROUGE (Lin, 2004) for summarization.", "startOffset": 19, "endOffset": 45}, {"referenceID": 6, "context": ", 2002) and METEOR (Banerjee and Lavie, 2005) for machine translation, and ROUGE (Lin, 2004) for summarization.", "startOffset": 81, "endOffset": 92}, {"referenceID": 11, "context": "For conversation systems, researchers occassionally adopt these metrics for evaluation (Ritter et al., 2011; Li et al., 2015; Song et al., 2016).", "startOffset": 87, "endOffset": 144}, {"referenceID": 5, "context": "For conversation systems, researchers occassionally adopt these metrics for evaluation (Ritter et al., 2011; Li et al., 2015; Song et al., 2016).", "startOffset": 87, "endOffset": 144}, {"referenceID": 15, "context": "For conversation systems, researchers occassionally adopt these metrics for evaluation (Ritter et al., 2011; Li et al., 2015; Song et al., 2016).", "startOffset": 87, "endOffset": 144}, {"referenceID": 4, "context": ", in BLEU and ROUGE), we measure the similarity by pooling of word embeddings (Forgues et al., 2014); it is more suited to dialog systems due to the diversity of replies.", "startOffset": 78, "endOffset": 100}, {"referenceID": 1, "context": ", 2002) and METEOR (Banerjee and Lavie, 2005) for machine translation, and ROUGE (Lin, 2004) for summarization. For conversation systems, researchers occassionally adopt these metrics for evaluation (Ritter et al., 2011; Li et al., 2015; Song et al., 2016). However, Liu et al. (2016) conduct extensive empirical experiments and show weak correlation between existing metrics and human annotation.", "startOffset": 20, "endOffset": 285}, {"referenceID": 1, "context": ", 2002) and METEOR (Banerjee and Lavie, 2005) for machine translation, and ROUGE (Lin, 2004) for summarization. For conversation systems, researchers occassionally adopt these metrics for evaluation (Ritter et al., 2011; Li et al., 2015; Song et al., 2016). However, Liu et al. (2016) conduct extensive empirical experiments and show weak correlation between existing metrics and human annotation. Very recently, Lowe et al. (2017) propose a neural network-based metric for conversation systems; it learns to predict a score of a reply given its query (previous user-issued utterance) and a groundtruth reply.", "startOffset": 20, "endOffset": 432}, {"referenceID": 6, "context": "We apply negative sampling to train the network. Our approach requires no manual annotation labels, and hence is more extensible than Lowe et al. (2017). \u2022 We propose to combine the referenced and unreferenced metrics to better make use both worlds.", "startOffset": 22, "endOffset": 153}, {"referenceID": 4, "context": "Also, such treatment is more robust than vector extrema (Forgues et al., 2014); it chooses either the largest positive or smallest negative value, where features are more vulnerable to signs.", "startOffset": 56, "endOffset": 78}, {"referenceID": 13, "context": "The above empirical structure is mainly inspired by several previous studies (Severyn and Moschitti, 2015; Yan et al., 2016).", "startOffset": 77, "endOffset": 124}, {"referenceID": 20, "context": "The above empirical structure is mainly inspired by several previous studies (Severyn and Moschitti, 2015; Yan et al., 2016).", "startOffset": 77, "endOffset": 124}, {"referenceID": 19, "context": "We may also apply other variants for utterance matching (Wang and Jiang, 2016; Mou et al., 2016); details are beyond the focus of this paper.", "startOffset": 56, "endOffset": 96}, {"referenceID": 9, "context": "We may also apply other variants for utterance matching (Wang and Jiang, 2016; Mou et al., 2016); details are beyond the focus of this paper.", "startOffset": 56, "endOffset": 96}, {"referenceID": 20, "context": "In previous work, researchers adopt negative sampling for utterance matching (Yan et al., 2016).", "startOffset": 77, "endOffset": 95}, {"referenceID": 8, "context": "Our study further verifies that negative sampling is useful for the evaluation task, which eases the burden of human annotation compared with fully supervised approaches that requirer manual labels for training their metrics (Lowe et al., 2017).", "startOffset": 225, "endOffset": 244}, {"referenceID": 0, "context": "The other is a sequence-to-sequence neural network (Bahdanau et al., 2015) that encodes a query as a vector with an RNN and decodes the vector", "startOffset": 51, "endOffset": 74}, {"referenceID": 5, "context": "The RUBER metric (along with baselines) is evaluated on two dialog systems. One is a featurebased retrieval-and-reranking system in Song et al. (2016), which first retrieves a coarse-grained candidate set by keyword matching and then reranks the candidates by human-engineered features; the top-ranked results are selected for evaluation.", "startOffset": 33, "endOffset": 151}, {"referenceID": 16, "context": "Pearson and Spearman correlation measures are widely used in other research of automatic metrics such as machine translation (Stanojevi\u0107 et al., 2015).", "startOffset": 125, "endOffset": 150}, {"referenceID": 8, "context": "Thus, RUBER does not requirer human labels; it is more flexible than the existing supervised metric (Lowe et al., 2017), and can be easily adapted to different datasets.", "startOffset": 100, "endOffset": 119}], "year": 2017, "abstractText": "Open-domain human-computer conversation has been attracting increasing attention over the past few years. However, there does not exist a standard automatic evaluation metric for open-domain dialog systems; researchers usually resort to human annotation for model evaluation, which is timeand labor-intensive. In this paper, we propose RUBER, a Referenced metric and Unreferenced metric Blended Evaluation Routine, which evaluates a reply by taking into consideration both a groundtruth reply and a query (previous user utterance). Our metric is learnable, but its training does not require labels of human satisfaction. Hence, RUBER is flexible and extensible to different datasets and languages. Experiments on both retrieval and generative dialog systems show that RUBER has high correlation with human annotation.", "creator": "LaTeX with hyperref package"}}}