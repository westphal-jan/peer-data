{"id": "1701.03578", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2017", "title": "Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network", "abstract": "In this paper, we propose an efficient transfer leaning methods for training a personalized language model using a recurrent neural network with long short-term memory architecture. With our proposed fast transfer learning schemes, a general language model is updated to a personalized language model with a small amount of user data and a limited computing resource. These methods are especially useful for a mobile device environment while the data is prevented from transferring out of the device for privacy purposes.\n\n\n\n\nThis paper demonstrates the ability to generate and model a general language model through a recurrent neural network, which can store and use neural networks without the need to generate a large amount of data.", "histories": [["v1", "Fri, 13 Jan 2017 07:26:00 GMT  (36kb)", "http://arxiv.org/abs/1701.03578v1", "AAAI workshop on Crowdsourcing, Deep Learning and Artificial Intelligence Agents, Feb 2017, San Francisco CA, USA"]], "COMMENTS": "AAAI workshop on Crowdsourcing, Deep Learning and Artificial Intelligence Agents, Feb 2017, San Francisco CA, USA", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["seunghyun yoon", "hyeongu yun", "yuna kim", "gyu-tae park", "kyomin jung"], "accepted": false, "id": "1701.03578"}, "pdf": {"name": "1701.03578.pdf", "metadata": {"source": "CRF", "title": "Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network", "authors": ["Seunghyun Yoon", "Hyeongu Yun", "Yuna Kim", "Gyu-tae Park", "Kyomin Jung"], "emails": ["kjung}@snu.ac.kr,", "hyeongu.yun.1989@gmail.com,", "gyutae.park}@samsung.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 1.\n03 57\n8v 1\n[ cs\n.C L\n] 1\n3 Ja\nn 20"}, {"heading": "Introduction", "text": "Recently there has been a considerable interest in language modeling due to various academic and commercial demands. Academically, many studies have investigated this domain such as machine translation, chat-bot, message generation, image tagging and other language-related areas. Commercially, it can be used as a core technology for providing a new application on consumer products or services. For instance, an automatic message-reply prediction service can be launched in mobile devices, thus helping a user to send a reply message when he/she is not provided with a proper input interface.\nTo model the language of human dialogue, a recurrent neural network (RNN) structure is known to show the state of the arts performance with its ability to learn a sequential pattern of the data (Sutskever, Vinyals, and Le 2014). Among the RNN structures, a Long Short-Term Memory RNN (LSTM-RNN) and its variants are successfully used for language modeling tasks (Hochreiter and Schmidhuber 1997; Cho et al. 2014). However, as a kind of deep learning technique, the LSTM-RNN and the RNN structure requires both a large number of data and huge computing power to train the model properly. Hence any attempts for applying the RNN structure to personalized language modeling are mainly constrained by the following two limitations. First, personal mobile devices contain private message data among close acquaintances, so users seldom agree to transfer their log out of the devices. This causes a limitation of gathering\nthe whole user data to common computing spaces, where high-performance machines are available. Second, in relatively small computing machines, i.e., smart phone, it is not always-guaranteed to have enough resources to train a deep model within the devices.\nTo resolve these limitations, we propose fast transfer learning schemes. It trains a base model with a large dataset and copies its first n-many layers to the first n-many layers of a target model. Then the target model is fine-tuned with relatively small target data. Several learning schemes such as freezing a certain layer or adding a surplus layer are proposed for achieving the result. In experiments, we trained a general language model with huge corpus such as an Workshop on Statistical Machine Translation (WMT) data1 and a movie script data by using powerful computing machines, and then transferred the model to target environment for updating to be a personalized language model. With this approach, the final model can mimic target user\u2019s language style with proper syntax.\nIn the experiments, we trained the general language model with literary-style data and applied the transfer learning with spoken-style data. Then we evaluated the model output for sentence completion task in a qualitative and a quantitative manner. The test result showed that the model learned the style of the target language properly. Another test was conducted by training the general language model with the script of the drama, \u201cFriends,\u201d and by applying transfer learning with main character corpora from the script to generate the personalized language model. The message-reply prediction task was evaluated with this model. The test result shows higher similarity between the output of the personalized language model and the same user dialogue than the one between the output of the personalized language model and other users\u2019 dialogues.\nThe contributions of this paper are as follows. First, we propose efficient transfer learning schemes for personalized language modeling, which is the first research on transfer learning for RNN based language models with privacy preserving. Second, we show the applicability of our research to the target scenario in the short message reply application by training the model in the similar environment to that of\n1Available from \u201chttp://www.statmt.org/wmt14/translationtask.html#download/\u201d\nthe mobile device, and highlight its test results."}, {"heading": "Architecture for Personalized Language Model", "text": "As we are focusing on a personalized language modeling with the preservation of user data, we generate two types of language models. First is a sentence completion language model, which can complete sentences with a given n-many sequence of words. Second is a message-reply prediction language model, which can generate a response sentence for a given message. The output of both models implies user characteristics such as preferable vocabulary, sentence length, and other language-related patterns.\nTo achieve this result, we trained the language model with a large amount of general data in powerful computing environments, and then applied the transfer learning in relatively small computing environments. We assume that this method would be applied to mobile devices. As we are taking the preservation of privacy into consideration, the transferred model is retrained within the local environments such as mobile devices, and no personal data is sent out of the devices. This could have been accomplished using the proposed transfer learning schemes in RNN-LSTM architecture."}, {"heading": "Sentence Completion Language Model", "text": "A sentence completion model completes a sentence with the given word sequence X = {x1, x2, . . . , xT }, where xN is a word (N = 1, 2, . . . , T ). The model can predict the next word xN+1 with given word sequence x1:N . By repeating the prediction until the output word reaches the end-ofsentence signal, \u201c< eos >,\u201d the whole sentence can be generated.\nThe model is similar to that of (Graves 2013), and we put the 1,000-dimension word-embedding layer right after the input layer. Then 3 deep LSTM layers with 100 LSTM cells each and without peephole connection are used for learning the sequential pattern of the sentences.\nThe output probability to the input sequence X and the training objective are\np(Y |X) = T\u220f\nt=1\np(yt|x1:t\u22121)\nL = \u2212 1\n|T |\nT\u2211\nt=1\nxt+1 log p(yt|x1:t\u22121),\n(1)\nwhere X is a word sequence in the sentence, Y is a model output sequence Y = {y1, y2, . . . , yT }"}, {"heading": "Message-Reply Prediction Language Model", "text": "A message-reply prediction model generates a response sentence for a given message. It is similar to the sentence completion language model except that the message sentence is encoded and used as a context information when the model generates a response word sequence. Our approach is inspired by the sequence-to-sequence learning research (Sutskever, Vinyals, and Le 2014) that is successfully applied to a machine translation task. The message\nword sequence X = {x1, x2, . . . , xT } is fed into the model, and the last hidden state is used as context information cT . With this context information, the next sequence word is predicted similarly to that in the sentence completion language model case. During implementation, we used 1,000- dimension word embedding and 3-deep LSTM layers with 100 LSTM cells in each layer. The output probability and the training objective are\np(Y |X) =\nT \u2032\u220f\nt=1\np(yt|cT , y1:t\u22121)\nL = \u2212 1\n|T \u2032| |\nT \u2032\u2211\nt=1\nzt log p(yt|cT , y1:t\u22121),\n(2)\nwhere X is a word sequence in the message sentence, Z is a target word sequence in the response sentence Z = {z1, z2, . . . , zT \u2032}, Y is a model output sequence Y = {y1, y2, . . . , yT \u2032}, cT is the encoding vector for the message sentence."}, {"heading": "Fast Transfer Learning Schemes", "text": "To generate a personalized language model with a small amount of user data and limited computing resources, transfer learning is essential. In the private data preservation scenario, we investigate three fast transfer learning schemes. Each scheme is described below:\n\u2022 Scheme 1, relearn the whole layer: As a baseline, we retrain the whole model with private data only and compare the result with the two other schemes below. Because of the retraining of the LSTM layers in their entirety, this scheme requires more computing power than the other two schemes.\n\u2022 Scheme 2, surplus layer: After the training of the model with general data, a surplus layer is inserted between the output layer and the last of the deep LSTM layers. Then, with private data, we update only the parameters of the surplus layer in the transfer learning phase. We assume that a user\u2019s parlance could be modeled by learning additional features in the user\u2019s private data.\n\u2022 Scheme 3, fixed first n layers: After training the model with general data, we fix the parameters in the first n LSTM layers (layer 1 and layer 2 in our experiments) and train remaining parameters in the transfer learning phase. We assume that the user\u2019s parlance is a subset of the general pattern and the last layer plays the key role in determining this pattern."}, {"heading": "Measures", "text": "The perplexity is one of the popular measures for a language model. It measures how well the language model predicts a sample. However, it is not good at measuring how well the output of the language model matches a target language style. Another measure, the BLEU score algorithm (Papineni et al. 2002), has been widely used for the automatic evaluation of the model output. However, it cannot be applied directly to measuring a quality of the personalized\nmodel output because it considers the similarity between one language and the target language. Other research was conducted on proving authorship and fraud in literature, for instance, Jane Austen\u2019s left-over novel with partially completed (Morton 1978). This research counted the occurrence of several words in the literature, compared their relative frequencies with those of the words in the target literature, and concluded that the target literature was a forgery. This approach could be applied to a text evaluation where a large amount of data is available and certain words are used more frequently. In spoken language, such as in the message-reply case, however, whole word distribution must be considered instead of considering the occurrence of several words, because the data is usually not enough than the literature case. So, we use a simple and efficient metric to measure the similarity between the user style and the output of the personalized model.\nAn output of a personalized language model can be measured by calculating the cross entropy between the word distribution of the model output and that of the target data. Word distribution can be acquired by normalizing a word histogram which is calculated based on word counts in the target corpus. Equation (3) shows the metric formulation.\nY1 = g(fLM (Mi)), Y2 = g(Ti) measure = Cross Entropy(Y1, Y2), (3)\nwhere Mi is a message \u2208 Dtest, Ti is a corpus \u2208 Dtarget, fLM is a language model, g(\u00b7) calculates word distribution with given corpus, CrossEntropy(p, q) is \u2212 \u2211\nx p(x) log q(x). The characteristics of a user speech can mainly be distinguished by the word dictionary. Thus, this metric tries to measure the differences of the word dictionary among the comparing set. Table 1 shows the quantitative measure results from the dialogue set of the main characters in drama data from \u201cFriends,\u201d a famous American television sitcom. In the figures, \u201ccharacter 1\u201d to \u201ccharacter 6\u201d are the main characters of the drama (Chandler, Joey, Monica, Phoebe, Rachel, and Ross, respectively). The dialogues were measured against one another by using the cross entropy metric. As shown in the table, the lower cross entropy value among the same character\u2019s dialogue was calculated, and the higher value was calculated among the different character\u2019s dialogues as expected. This result demonstrates that the cross entropy metric can be used to measure the similarities among the members of the set."}, {"heading": "Datasets", "text": "\u2022 WMT14 ENG Corpus: The WMT\u201914 dataset includes\nseveral corpora. We only use an English part of the 109 French-English corpus. The dataset was crawled data from the bilingual web pages of the international organizations (Callison-Burch et al. 2011). Thus, it contains high quality formal written language data. It consists of 21,000,000 sentences.\n\u2022 English Bible Corpus: The English bible corpus is another type of written language data. It is useful data that differs from the WMT\u201914 dataset not only in the frequent vocabulary type but also in the average sentence length. It consists of 31,102 sentences.\n\u2022 Drama Corpus: To collect spoken language data, we use drama data from \u201cFriends\u201d from opensubtitles2. We extracted 69,000 sentences from dialogues, which we used to train a sentence completion language model. For the message-reply prediction language model, pairwise data is required. Among the extracted data, two consecutive sentences of different characters are linked into a single sentence to generate pairwise data.\n\u2022 Main Character Corpora: From the drama corpus, we extract main character corpora to model personal users. For example, the Chanlder (one of the main characters in \u201cFriends\u201d) corpus consisted of 8,406 lines and the Rachel (another major character in \u201cFriends\u201d) corpus consisted of 9,194 lines. The former data could represent a male adult, and the latter data could represent a female adult. We assume that those amounts of data could be gathered in a user device for the personalizing language model."}, {"heading": "Experiments", "text": "We mainly conduct two types of experiments. The first one is a sentence completion experiment, and the other one is a message-reply prediction experiment. In the former case, we train a general language model with literary-style data and apply a proposed transfer learning scheme with spokenstyle data to achieve a personalized language model. With this setting, the difference between general and personalized language models can be measured in a quantitative and a qualitative manner. For the latter case, we use dialogue-style data such as drama scripts to train a general language model. From the drama scripts, some characters\u2019 data are taken\n2Available from \u201chttp://www.opensubtitles.org/\u201d\napart and are used to train the personalized language model. With this setting, the output of the personalized model is compared to the original dialogue of the same character."}, {"heading": "Literary-Style to Spoken-Style Sentence Completion", "text": "We train a general language model of literary-style with the WMT\u201914 corpus. We then apply a transfer learning scheme with \u201cFriends\u201d drama data for the model to learn the spokenstyle language. Training the general language model took about 10 days then we spent another 4 hours training the personalized language model in each scheme. A \u201ctitan-X GPU\u201d and a \u201cGeForce GT 730 GPU\u201d were used for these experiments. The latter GPU is one of the low-end GPU series of which computing power was similar to that of latest mobile GPUs such as \u201cQualcomm Adreno 530\u201d in \u201cSamsung Galaxy S7\u201d or \u201cNVIDIA Tegra K1\u201d in \u201cGoogle Nexus 9\u201d. For a vocabulary setting, we construct our dictionary as 50,002 words, including \u201c< eos >\u201d to mark ends of sentence and \u201c**unknown**\u201d to replace unconsidered vocabulary in the data. The out-of-vocabulary rate is about 3.5%.\nThe \u201cgeneral language model\u201d in Table 2 shows the sample output of the general language model trained with document-style data, and the \u201cpersonal language model 1\u201d in Table 2 shows the sample output of the personalized language model trained with human-dialogue-style data. Scheme 1 to scheme 3 are relearn-whole, surplus layer, and fixed-n layer, respectively. Given input word sequence for the test was, \u201cIt is possible, however.\u201d As can be seen in the table, both outputs differ in length and style. The sentence completed using the general language model tends to be longer than that of obtained using the personalized language model. This result indicates that the personalized language model is properly trained with the spoken language characteristics because human dialogue is usually briefer than the language in official documents.\nWe also apply the transfer learning schemes with some of the English bible data. The same general language model, which involved previously training with the WMT\u201914 corpus for 10 days, is used. English bible data is added and em-\nployed in training for another 4 hours using proposed transfer learning schemes.\nThe \u201cpersonalized language model 2\u201d in Table 2 shows the sample output of the personalized language model trained with another style of document data, English bible data. As shown in Table 2, the output of the personalized language model contains more bible-like vocabulary and sentence styles."}, {"heading": "General-Style to Personal-Style Message-Reply Prediction", "text": "We simulate the message-reply prediction scenario using the drama corpus. The script of the drama, \u201cFriends,\u201d is used to train a general language model, and two main character corpora are used to generate a personalized language model. For this message-reply prediction experiment, we use a vocabulary size of 18,107, and the out-of-vocabulary rate is about 3.5%. In the message-reply prediction case, pairwise data is generated by extracting the drama corpus of each character and concatenating two consecutive sentences of different characters to form one single message-reply sentence data. We insert the word \u201c< eos >\u201d between the message and reply to mark the border separating them. This pairwise data is used for the training, and only the message part of the pairwise data is used for the message-reply prediction. During implementation, it took about a day to train the general language model with the \u201cFriends\u201d corpus and another 4 hours to train the personalized language model with two main character corpora. The \u201ctitan-X GPU\u201d and the \u201cGeForce GT 730 GPU\u201d was used for these experiments. Validation messages-reply sentences of 1,281 are randomly sampled from the \u201cFriends\u201d corpus for tracking validation curve and another 753 test messages are prepared for predicting the responses. These data remained unseen from training phase. The word distributions of the model output from the test messages and the target corpus data are calculated to measure their similarity.\nFigure 1 shows the validation curve while training. Perplexity values from various model output are plotted. The perplexity of baseline model, \u201cscheme 1\u201d, decreases until\naround epoch 10, and then it starts to increase because model is over-fitted to training data. The proposed \u201cscheme 2\u201d and \u201cscheme 3\u201d, however, show continuous decreasing tendency and reach lower perplexity values compared to that of the baseline model. It is interesting that proposed methods achieve lower perplexity than baseline while saving computing power with reduced parameters.\nTable 3 shows the performances of various models measured with the same validation dataset used in Figure 1. An unpruned n-gram language models using modified Kneser-Ney smoothing are used for performance comparisons (Chen and Goodman 1996). The n-gram models were trained by using KenLM software package (Heafield et al. 2013). The chandler n-gram model was trained with \u201cChandler\u201d corpus and the friends n-gram model was trained with \u201cFriends\u201d corpus. The proposed scheme 1 to scheme 3 were trained with \u201cChandler\u201d corpus from \u201cFriends\u201d general language model. We see that our proposed schemes outperform the n-gram models (n=3 and 5).\nTo check the influence of training data size (number of sentences) in personalized language model, we trained the general language model (trained with \u201cFriends\u201d corpus, message-reply prediction model) with different sizes of personal (\u201cchandler\u201d and \u201crachel\u201d) dataset. The proposed scheme 2 method was used for this test. Table 4 shows evaluation results of the trained models. Dataset \u20190\u2019 means the model is not trained with personal dataset. The perplexity shows lower value as we use more dataset in training, and it outperforms \u201cfriends 5-gram\u201d model from the 2,000 dataset cases.\nTable 5 indicates the cross entropy measure between the output of \u201cscheme 1\u201d to \u201cscheme 3\u201d model and that of the target corpus, the \u201cfriends\u201d drama corpus, the \u201cchandler\u201d corpus, and the \u201cbible\u201d corpus. It shows the similarity between the personalized model output and the target corpus as the number of training epoch increasing. The general model was pre-trained with the \u201cFriends\u201d corpus and the \u201cChandler\u201d corpus was used training personalized model. Each Model is selected from various training epoch (0, 10, 20\nand 40) and schemes, and test messages of 753 are used for the reply generation with the selected model used. As the table shows, the cross entropy measure has the highest value when the target corpus is the \u201cbible\u201d as expected because it is written in different style than dialogues in drama script. For the drama script case, the cross entropy measured with the \u201cchandler\u201d corpus shows the lowest value among schemes. This result reveals that the personalized language model is trained properly from the general language model. Thus it is more similar in style to the target data corpus than the general language model. The \u201cepoch 0\u201d case means the initial model state trained from general language corpus, \u201cfriends\u201d corpus. Thus cross entropy with \u201cfriends\u201d target corpus shows lower value than that of \u201cchandler\u201d and \u201cbible\u201d target corpus cases."}, {"heading": "Related Work", "text": "Researchers have proposed language models using RNN, which learns the probability of next sequence data at the character or word level (Sutskever, Martens, and Hinton 2011; Graves 2013). The proposed language models were tested on web corpora (i.e. Wikipedia, news articles) and qualitative examples showed their applicability. (Sutskever, Vinyals, and Le 2014) proposed a sequence-to-sequence learning algorithm with RNN and long short-term memory (LSTM) architecture (Hochreiter and Schmidhuber 1997), and (Cho et al. 2014) proposed RNN encoder-decoder architecture. Those studies were applied to the machine translation problem.\nRecently, the RNN machine translation approach was extended to the short message generation problem (Sordoni et al. 2015). Considering the message and response as a trans-\nlation problem, the Neural Responding Machine achieved 40% accuracy for both contextually and syntactically proper response generations with twitter-like micro-blogging data (Shang, Lu, and Li 2015). Those studies were similar to our research in the sense that both target message-reply prediction language model using RNN. Our research, however, differs in that it updates a general language model to a personalized language model with user data separately, whereas the previous research trained a language model with the data, as a whole, in same place.\nIn the commercial sphere, Google recently released a smart-reply service that could generate a response to a given email by using a sequence-to-sequence learning model (Google 2015). There was another trial on the generation of responses in technical troubleshooting discourses (Vinyals and Le 2015). This research also required complete data in one place and did not provide a personalized model.\nMoreover, many researchers have conducted studies on transfer learning. (Bengio et al. 2011; Bengio 2012) suggested that a base-trained model with general data could be transferred to another domain. Recently, (Yosinski et al. 2014) showed, through experiments, that the lower layers tended to have general features whereas the higher layer tended to have specific features. However, none of this research was applied to an RNN language model.\nTo adapt a neural network model to an embedded system with limited resources, (Kim et al. 2015) (Han et al. 2015) reduced the size of the model by pruning the unnecessary connections within it. It repeatedly tried to reduce the model size without accuracy degradation. This research inspired us to a considerable extent. It applied a neural model to mobile devices. However, the research focused on reducing the model size using a powerful machine and releasing the final model to an embedded system, whereas ours investigated how to train a model within mobile devices so that private user data could be kept."}, {"heading": "Conclusion", "text": "We propose an efficient method for training a personalized model using the LSTM-RNN model. To preserve users\u2019 pri-\nvacy, we suggest various transfer learning schemes so that the personalized language model can be generated within the user\u2019s local environment. The proposed schemes \u201csurplus layer\u2019 and \u201cfixed-n layer\u2019 shows higher generalization performance whereas it trains only reduced number of parameters than baseline model. The quantitative and qualitative test result indicate that the output of the model is similar to that of the user\u2019s style.\nIt is certain that our proposed method reveals the applicability of the RNN-based language model in a user device with the preservation of privacy. Furthermore, with our method the personalized language model can be generated with a smaller amount of user data than the huge amount of training data that is usually required in the traditional deep neural network discipline. In the future work, we aim to visualize the deep neural network and to investigate the specific relationship among users\u2019 language styles and the LSTM cells in the network. This approach seems likely to uncover enhanced learning schemes that require less data than was previously necessary."}], "references": [{"title": "Deep learners benefit more from out-of-distribution", "author": ["Y. Bengio", "F. Bastien", "A. Bergeron", "N. BoulangerLewandowski", "T.M. Breuel", "Y. Chherawala", "M. Cisse", "M. C\u00f4t\u00e9", "D. Erhan", "J Eustache"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2011}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Y. Bengio"], "venue": "Unsupervised and Transfer Learning Challenges in Machine Learning 7:19.", "citeRegEx": "Bengio,? 2012", "shortCiteRegEx": "Bengio", "year": 2012}, {"title": "Findings of the 2011 workshop on statistical machine translation", "author": ["C. Callison-Burch", "P. Koehn", "C. Monz", "O.F. Zaidan"], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation, 22\u201364. Association for Computational Linguistics.", "citeRegEx": "Callison.Burch et al\\.,? 2011", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2011}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "Proceedings of the 34th annual meeting on Association for Computa-", "citeRegEx": "Chen and Goodman,? 1996", "shortCiteRegEx": "Chen and Goodman", "year": 1996}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850.", "citeRegEx": "Graves,? 2013", "shortCiteRegEx": "Graves", "year": 2013}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Advances in Neural Information Processing Systems, 1135\u2013 1143.", "citeRegEx": "Han et al\\.,? 2015", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Scalable modified kneser-ney language model estimation", "author": ["K. Heafield", "I. Pouzyrevsky", "J.H. Clark", "P. Koehn"], "venue": "ACL (2), 690\u2013696.", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Y.-D. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin"], "venue": "arXiv preprint arXiv:1511.06530.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Literary detection: How to prove authorship and fraud in literature and documents", "author": ["A.Q. Morton"], "venue": "Scribner.", "citeRegEx": "Morton,? 1978", "shortCiteRegEx": "Morton", "year": 1978}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, 311\u2013318. Association for Computational Linguistics.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Neural responding machine for short-text conversation", "author": ["L. Shang", "Z. Lu", "H. Li"], "venue": "ACL.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "J.-Y. Nie", "J. Gao", "B. Dolan"], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G.E. Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 1017\u20131024.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS, 3104\u2013 3112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model", "author": ["O. Vinyals", "Q. Le"], "venue": "arXiv preprint arXiv:1506.05869.", "citeRegEx": "Vinyals and Le,? 2015", "shortCiteRegEx": "Vinyals and Le", "year": 2015}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "Among the RNN structures, a Long Short-Term Memory RNN (LSTM-RNN) and its variants are successfully used for language modeling tasks (Hochreiter and Schmidhuber 1997; Cho et al. 2014).", "startOffset": 133, "endOffset": 183}, {"referenceID": 4, "context": "Among the RNN structures, a Long Short-Term Memory RNN (LSTM-RNN) and its variants are successfully used for language modeling tasks (Hochreiter and Schmidhuber 1997; Cho et al. 2014).", "startOffset": 133, "endOffset": 183}, {"referenceID": 5, "context": "The model is similar to that of (Graves 2013), and we put the 1,000-dimension word-embedding layer right after the input layer.", "startOffset": 32, "endOffset": 45}, {"referenceID": 11, "context": "Another measure, the BLEU score algorithm (Papineni et al. 2002), has been widely used for the automatic evaluation of the model output.", "startOffset": 42, "endOffset": 64}, {"referenceID": 10, "context": "Other research was conducted on proving authorship and fraud in literature, for instance, Jane Austen\u2019s left-over novel with partially completed (Morton 1978).", "startOffset": 145, "endOffset": 158}, {"referenceID": 2, "context": "The dataset was crawled data from the bilingual web pages of the international organizations (Callison-Burch et al. 2011).", "startOffset": 93, "endOffset": 121}, {"referenceID": 3, "context": "An unpruned n-gram language models using modified Kneser-Ney smoothing are used for performance comparisons (Chen and Goodman 1996).", "startOffset": 108, "endOffset": 131}, {"referenceID": 7, "context": "The n-gram models were trained by using KenLM software package (Heafield et al. 2013).", "startOffset": 63, "endOffset": 85}, {"referenceID": 5, "context": "Researchers have proposed language models using RNN, which learns the probability of next sequence data at the character or word level (Sutskever, Martens, and Hinton 2011; Graves 2013).", "startOffset": 135, "endOffset": 185}, {"referenceID": 8, "context": "(Sutskever, Vinyals, and Le 2014) proposed a sequence-to-sequence learning algorithm with RNN and long short-term memory (LSTM) architecture (Hochreiter and Schmidhuber 1997), and (Cho et al.", "startOffset": 141, "endOffset": 174}, {"referenceID": 4, "context": "(Sutskever, Vinyals, and Le 2014) proposed a sequence-to-sequence learning algorithm with RNN and long short-term memory (LSTM) architecture (Hochreiter and Schmidhuber 1997), and (Cho et al. 2014) proposed RNN encoder-decoder architecture.", "startOffset": 180, "endOffset": 197}, {"referenceID": 13, "context": "Recently, the RNN machine translation approach was extended to the short message generation problem (Sordoni et al. 2015).", "startOffset": 100, "endOffset": 121}, {"referenceID": 16, "context": "There was another trial on the generation of responses in technical troubleshooting discourses (Vinyals and Le 2015).", "startOffset": 95, "endOffset": 116}, {"referenceID": 0, "context": "(Bengio et al. 2011; Bengio 2012) suggested that a base-trained model with general data could be transferred to another domain.", "startOffset": 0, "endOffset": 33}, {"referenceID": 1, "context": "(Bengio et al. 2011; Bengio 2012) suggested that a base-trained model with general data could be transferred to another domain.", "startOffset": 0, "endOffset": 33}, {"referenceID": 17, "context": "Recently, (Yosinski et al. 2014) showed, through experiments, that the lower layers tended to have general features whereas the higher layer tended to have specific features.", "startOffset": 10, "endOffset": 32}, {"referenceID": 9, "context": "To adapt a neural network model to an embedded system with limited resources, (Kim et al. 2015) (Han et al.", "startOffset": 78, "endOffset": 95}, {"referenceID": 6, "context": "2015) (Han et al. 2015) reduced the size of the model by pruning the unnecessary connections within it.", "startOffset": 6, "endOffset": 23}], "year": 2017, "abstractText": "In this paper, we propose an efficient transfer leaning methods for training a personalized language model using a recurrent neural network with long short-term memory architecture. With our proposed fast transfer learning schemes, a general language model is updated to a personalized language model with a small amount of user data and a limited computing resource. These methods are especially useful for a mobile device environment while the data is prevented from transferring out of the device for privacy purposes. Through experiments on dialogue data in a drama, it is verified that our transfer learning methods have successfully generated the personalized language model, whose output is more similar to the personal language style in both qualitative and quantitative aspects.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}