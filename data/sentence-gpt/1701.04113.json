{"id": "1701.04113", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2017", "title": "Near Optimal Behavior via Approximate State Abstraction", "abstract": "The combinatorial explosion that plagues planning and reinforcement learning (RL) algorithms can be moderated using state abstraction. Prohibitively large task representations can be condensed such that essential information is preserved, and consequently, solutions are tractably computable. However, exact abstractions, which treat only fully-identical situations as equivalent, fail to present opportunities for abstraction in environments where no two situations are exactly alike (see Materials and Methods). The key to an abstract approach to the problem, particularly in large-scale computational projects, is not to confuse the formalized problem, in which the problem of generative representations of a problem are more straightforward. The idea of such generative representations as vectors is very well-defined, but does not fully inform the idea of what a problem is (in particular if it's not a generative representation of the problem in a way that makes one very abstract). The goal is to define an abstract concept that is, in a particular context, and can be applied to the problem: an abstract concept that could be used for this task in the context of the problem.\n\n\n\nIn the following example, the application is to create a group of objects that can be trained for this task. The group is to represent the class of objects. The class can be assigned to a task (i.e. a class of objects). For example, a class of objects can be trained for this task. The class can be assigned to a class of objects.\n\nThe application can be directed by a particular task (i.e. a task or a task). For example, a class of objects can be trained for this task. The class can be assigned to a class of objects. For example, a class of objects can be trained for this task. The class can be assigned to a class of objects. For example, a class of objects can be trained for this task.\n\nA class of objects can be trained for this task. For example, a class of objects can be trained for this task. The class can be assigned to a class of objects. For example, a class of objects can be trained for this task.\nThe application can be directed by a particular task. The class can be assigned to a class of objects. For example, a class of objects can be trained for this task. The class can be assigned to a class of objects. For example, a class of objects can be trained for this task.\nThe application can be directed by a particular task. The", "histories": [["v1", "Sun, 15 Jan 2017 21:24:45 GMT  (3873kb,D)", "http://arxiv.org/abs/1701.04113v1", "Earlier version published at ICML 2016"]], "COMMENTS": "Earlier version published at ICML 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["david abel", "d ellis hershkowitz", "michael l littman"], "accepted": true, "id": "1701.04113"}, "pdf": {"name": "1701.04113.pdf", "metadata": {"source": "META", "title": "Near Optimal Behavior via Approximate State Abstraction", "authors": ["David Abel", "D. Ellis Hershkowitz", "Michael L. Littman"], "emails": ["abel@brown.edu", "dhershko@cs.cmu.edu", "mlittman@cs.brown.edu"], "sections": [{"heading": "1 Introduction", "text": "Abstraction plays a fundamental role in learning. Through abstraction, intelligent agents may reason about only the salient features of their environment while ignoring what is irrelevant. Consequently, agents are able to solve considerably more complex problems than they would be able to without the use of abstraction. However, exact abstractions, which treat only fully-identical situations as equivalent, require complete knowledge that is computationally intractable to obtain. Furthermore, often no two situations are identical, so exact abstractions are often ineffective. To overcome these issues, we investigate approximate abstractions that enable agents to treat sufficiently similar situations as identical. This work characterizes the impact of equating \u201csufficiently similar\u201d states in the context of planning and RL in Markov Decision Processes (MDPs). The remainder of our introduction contextualizes these intuitions in MDPs.\nRL\nSolution to Abstracted Problem, MA\nBounded Error\n(MG)\nTrue Problem, MG Abstracted Problem, MA\nAbstraction,\nSt ar\nt G\noa l\nG oa l St ar t\nFigure 1: We investigate families of approximate state abstraction functions that induce abstract MDP\u2019s whose optimal policies have bounded value in the original MDP.\nA previous version of this paper was published in the Proceedings of the 33rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s). \u2020The first two authors contributed equally.\nar X\niv :1\n70 1.\n04 11\n3v 1\n[ cs\n.L G\n] 1\n5 Ja\nn 20\n17\nSolving for optimal behavior in MDPs in a planning setting is known to be P-Complete in the size of the state space [28, 25]. Similarly, many RL algorithms for solving MDPs are known to require a number of samples polynomial in the size of the state space [31]. Although polynomial runtime or sample complexity may seem like a reasonable constraint, the size of the state space of an MDP grows super-polynomially with the number of variables that characterize the domain - a result of Bellman\u2019s curse of dimensionality. Thus, solutions polynomial in state space size are often ineffective for sufficiently complex tasks. For instance, a robot involved in a pick-and-place task might be able to employ planning algorithms to solve for how to manipulate some objects into a desired configuration in time polynomial in the number of states, but the number of states it must consider grows exponentially with the number of objects with which it is working [1].\nThus, a key research agenda for planning and RL is leveraging abstraction to reduce large state spaces [2, 21, 10, 12, 6]. This agenda has given rise to methods that reduce ground MDPs with large state spaces to abstract MDPs with smaller state spaces by aggregating states according to some notion of equality or similarity. In the context of MDPs, we understand exact abstractions as those that aggregate states with equal values of particular quantities, for example, optimal Q-values. Existing work has characterized how exact abstractions can fully maintain optimality in MDPs [24, 8].\nThe thesis of this work is that performing approximate abstraction in MDPs by relaxing the state aggregation criteria from equality to similarity achieves polynomially bounded error in the resulting behavior while offering three benefits. First, approximate abstractions employ the sort of knowledge that we expect a planning or learning algorithm to compute without fully solving the MDP. In contrast, exact abstractions often require solving for optimal behavior, thereby defeating the purpose of abstraction. Second, because of their relaxed criteria, approximate abstractions can achieve greater degrees of compression than exact abstractions. This difference is particularly important in environments where no two states are identical. Third, because the state aggregation criteria are relaxed to near equality, approximate abstractions are able to tune the aggressiveness of abstraction by adjusting what they consider sufficiently similar states.\nWe support this thesis by describing four different types of approximate abstraction functions that preserve near-optimal behavior by aggregating states on different criteria: \u03c6\u0303Q\u2217,\u03b5, on similar optimal Q-values, \u03c6\u0303model,\u03b5, on similarity of rewards and transitions, \u03c6\u0303bolt,\u03b5, on similarity of a Boltzmann distribution over optimal Q-values, and \u03c6\u0303mult,\u03b5, on similarity of a multinomial distribution over optimal Q-values. Furthermore, we empirically demonstrate the relationship between the degree of compression and error incurred on a variety of MDPs.\nThis paper is organized as follows. In the next section, we introduce the necessary terminology and background of MDPs and state abstraction. Section 3 surveys existing work on state abstraction applied to sequential decision making. Section 5 introduces our primary result; bounds on the error guaranteed by four classes of approximate state abstraction. The following two sections introduce simulated domains used in experiments (Section 6), and a discussion of experiments in which we apply one class of approximate abstraction to a variety of different tasks to empirically illustrate the relationship between degree of compression and error incurred (Section 7)."}, {"heading": "2 MDPs and Sequential Decision Making", "text": "An MDP is a problem representation for sequential decision making agents, represented by a five-tuple: \u3008S,A, T ,R, \u03b3\u3009. Here, S is a finite state space; A is a finite set of actions available to the agent; T denotes T (s, a, s\u2032), the probability of an agent transitioning to state s\u2032 \u2208 S after applying action a \u2208 A in state s \u2208 S; R(s, a) denotes the reward received by the agent for executing action a in state s; \u03b3 \u2208 [0, 1] is a discount factor that determines how much the agent prefers future rewards over immediate rewards. We assume without loss of generality that the range of all reward functions is normalized to [0, 1]. The solution to an MDP is called a policy, denoted \u03c0 : S 7\u2192 A.\nThe objective of an agent is to solve for the policy that maximizes its expected discounted reward from any state, denoted \u03c0\u2217. We denote the expected discounted reward for following policy \u03c0 from state s as the value of the state under that policy, V \u03c0(s). We similarly denote the expected discounted reward for taking\naction a \u2208 A and then following policy \u03c0 from state s forever after as Q\u03c0(s, a), defined by the Bellman Equation as:\nQ\u03c0(s, a) = R(s, a) + \u03b3 \u2211 s\u2032 T (s, a, s\u2032)Q\u03c0(s\u2032, \u03c0(s\u2032)). (1)\nWe let RMax denote the maximum reward (which is 1), and QMax denote the maximum Q value, which is RMax1\u2212\u03b3 . The value function, V , defined under a given policy, denoted V \u03c0(s), is defined as:\nV \u03c0(s) = Q\u03c0(s, \u03c0(s)). (2)\nLastly, we denote the value and Q functions under the optimal policy as V \u2217 or V \u03c0 \u2217 and Q\u2217 or Q\u03c0 \u2217 , respectively. For further background, see Kaelbling et al. [22]."}, {"heading": "3 Related Work", "text": "Several other projects have addressed similar topics."}, {"heading": "3.1 Approximate State Abstraction", "text": "Dean et al. [9] leverage the notion of bisimulation to investigate partitioning an MDP\u2019s state space into clusters of states whose transition model and reward function are within \u03b5 of each other. They develop an algorithm called Interval Value Iteration (IVI) that converges to the correct bounds on a family of abstract MDPs called Bounded MDPs.\nSeveral approaches build on Dean et al. [9]. Ferns et al. [14, 15] investigated state similarity metrics for MDPs; they bounded the value difference of ground states and abstract states for several bisimulation metrics that induce an abstract MDP. This differs from our work which develops a theory of abstraction that bounds the suboptimality of applying the optimal policy of an abstract MDP to its ground MDP, covering four types of state abstraction, one of which closely parallels bisimulation. Even-Dar and Mansour [13] analyzed different distance metrics used in identifying state space partitions subject to \u03b5-similarity, also providing value bounds (their Lemma 4) for \u03b5-homogeneity subject to the L\u221e norm, which parallels our Claim 2. Ortner [27] developed an algorithm for learning partitions in an online setting by taking advantage of the confidence bounds for T and R provided by UCRL [3].\nHutter [18, 17] investigates state aggregation beyond the MDP setting. Hutter presents a variety of results for aggregation functions in reinforcement learning. Most relevant to our investigation is Hutter\u2019s Theorem 8, which illustrates properties of aggregating states based on similar Q values. Hutter\u2019s Theorem part (a) parallels our Claim: both bound the value difference between ground and abstraction states, and part (b) is analogous to our Lemma 1: both bound the value difference of applying the optimal abstraction policy in the ground, and part (c) is a repetition of the comment given by Li et al. [24] that Q\u2217 abstractions preserve the optimal value function. For Lemma 1, our proof strategies differ from Hutter\u2019s, but the result is the same.\nApproximate state abstraction has also been applied to the planning problem, in which the agent is given a model of its environment and must compute a plan that satisfies some goal. Hostetler et al. [16] apply state abstraction to Monte Carlo Tree Search and expectimax search, giving value bounds of applying the optimal abstract action in the ground tree(s), similarly to our setting. Dearden and Boutilier [10] also formalize stateabstraction for planning, focusing on abstractions that are quickly computed and offer bounded value. Their primary analysis is on abstractions that remove negligible literals from the planning domain description, yielding value bounds for these abstractions and a means of incrementally improving abstract solutions to planning problems. Jiang et al. [20] analyze a similar setting, applying abstractions to the Upper Confidence Bound applied to Trees algorithm adapted for planning, introduced by Kocsis and Szepesva\u0301ri [23].\nMandel et al. [26] advance Bayesian aggregation in RL to define Thompson Clustering for Reinforcement Learning (TCRL), an extension of which achieves near-optimal Bayesian regret bounds. Jiang [19] analyze the problem of choosing between two candidate abstractions. They develop an algorithm based on statistical\ntests that trades of the approximation error with the estimation error of the two abstractions, yielding a loss bound on the quality of the chosen policy."}, {"heading": "3.2 Specific Abstraction Algorithms", "text": "Many previous works have targeted the creation of algorithms that enable state abstraction for MDPs. Andre and Russell [2] investigated a method for state abstraction in hierarchical reinforcement learning leveraging a programming language called ALISP that promotes the notion of safe state abstraction. Agents programmed using ALISP can ignore irrelevant parts of the state, achieving abstractions that maintain optimality. Dietterich [12] developed MAXQ, a framework for composing tasks into an abstracted hierarchy where state aggregation can be applied. Bakker and Schmidhuber [4] also target hierarchical abstraction, focusing on subgoal discovery. Jong and Stone [21] introduced a method called policy-irrelevance in which agents identify (online) which state variables may be safely abstracted away in a factored-state MDP. Dayan and Hinton [7] develop \u201cFeudal Reinforcement Learning\u201d which presents an early form of hierarchical RL that restructures Q-Learning to manage the decomposition of a task into subtasks. For a more complete survey of algorithms that leverage state abstraction in past reinforcement-learning papers, see Li et al. [24], and for a survey of early works on hierarchical reinforcement learning, see Barto and Mahadevan [5]."}, {"heading": "3.3 Exact Abstraction Framework", "text": "Li et al. [24] developed a framework for exact state abstraction in MDPs. In particular, the authors defined five types of state aggregation functions, inspired by existing methods for state aggregation in MDPs. We generalize two of these five types, \u03c6Q\u2217 and \u03c6model, to the approximate abstraction case. Our generalizations are equivalent to theirs when exact criteria are used (i.e. \u03b5 = 0). Additionally, when exact criteria are used our bounds indicate that no value is lost, which is one of core results of Li et al. [24]. Walsh et al. [34] build on the framework they previously developed by showing empirically how to transfer abstractions between structurally related MDPs."}, {"heading": "4 Abstraction Notation", "text": "We build upon the notation used by Li et al. [24], who introduced a unifying theoretical framework for state abstraction in MDPs.\nDefinition 1 (MG, MA): We understand an abstraction as a mapping from the state space of a ground MDP, MG, to that of an abstract MDP, MA, using a state aggregation scheme. Consequently, this mapping induces an abstract MDP. Let MG = \u3008SG,A, TG,RG, \u03b3\u3009 and MA = \u3008SA,A, TA,RA, \u03b3\u3009.\nDefinition 2 (SA, \u03c6): The states in the abstract MDP are constructed by applying a state aggregation function, \u03c6, to the states in the ground MDP, SA. More specifically, \u03c6 maps a state in the ground MDP to a state in the abstract MDP:\nSA = {\u03c6(s) | s \u2208 SG}. (3)\nDefinition 3 (G): Given a \u03c6, each ground state has associated with it the ground states with which it is aggregated. Similarly, each abstract state has its constituent ground states. We let G be the function that retrieves these states:\nG(s) = { {g \u2208 SG | \u03c6(g) = \u03c6(s)}, if s \u2208 SG, {g \u2208 SG | \u03c6(g) = s}, if s \u2208 SA.\n(4)\nThe abstract reward function and abstract transition dynamics for each abstract state are a weighted combination of the rewards and transitions for each ground state in the abstract state.\nDefinition 4 (\u03c9(s)): We refer to the weight associated with a ground state, s \u2208 SG by \u03c9(s). The only restriction placed on the weighting scheme is that it induces a probability distribution on the ground states of each abstract state:\n\u2200s \u2208 SG  \u2211 s\u2208G(s) \u03c9(s)  = 1 AND \u03c9(s) \u2208 [0, 1]. (5) Definition 5 (RA): The abstract reward function RA : SA \u00d7A 7\u2192 [0, 1] is a weighted sum of the rewards of each of the ground states that map to the same abstract state:\nRA(s, a) = \u2211\ng\u2208G(s)\nRG(g, a)\u03c9(g). (6)\nDefinition 6 (TA): The abstract transition function TA : SA \u00d7 A \u00d7 SA 7\u2192 [0, 1] is a weighted sum of the transitions of each of the ground states that map to the same abstract state:\nTA(s, a, s\u2032) = \u2211\ng\u2208G(s) \u2211 g\u2032\u2208G(s\u2032) TG(g, a, g\u2032)\u03c9(g). (7)"}, {"heading": "5 Approximate State Abstraction", "text": "Here, we introduce our formal analysis of approximate state abstraction, including results bounding the error associated with these abstraction methods. In particular, we demonstrate that abstractions based on approximate Q\u2217 similarity (5.2), approximate model similarity (5.3), and approximate similarity between distributions over Q\u2217, for both Boltzmann (5.4) and multinomial (5.5) distributions induce abstract MDPs for which the optimal policy has bounded error in the ground MDP.\nWe first introduce some additional notation.\nDefinition 7 (\u03c0\u2217A, \u03c0 \u2217 G): We let \u03c0 \u2217 A : SA \u2192 A and \u03c0\u2217G : SG \u2192 A stand for the optimal policies in the abstract and ground MDPs, respectively.\nWe are interested in how the optimal policy in the abstract MDP performs in the ground MDP. As such, we formally define the policy in the ground MDP derived from optimal behavior in the abstract MDP:\nDefinition 8 (\u03c0GA): Given a state s \u2208 SG and a state aggregation function, \u03c6,\n\u03c0GA(s) = \u03c0 \u2217 A(\u03c6(s)). (8)\nWe now define types of abstraction based on functions of state\u2013action pairs.\nDefinition 9 (\u03c6\u0303f,\u03b5): Given a function f : SG \u00d7A \u2192 R and a fixed non-negative \u03b5 \u2208 R, we define \u03c6\u0303f,\u03b5 as a type of approximate state aggregation function that satisfies the following for any two ground states s1, s2:\n\u03c6\u0303f,\u03b5(s1) = \u03c6\u0303f,\u03b5(s2)\u2192 \u2200a |f(s1, a)\u2212 f(s2, a)| \u2264 \u03b5. (9)\nThat is, when \u03c6\u0303f,\u03b5 aggregates states, all aggregated states have values of f within \u03b5 of each other for all actions.\nFinally, we estliabsh notation to distinguish between the ground and abstract value (V ) and action value (Q) functions. Definition 10 (QG, VG): Let QG = Q \u03c0\u2217G : SG \u00d7A \u2192 R and VG = V \u03c0 \u2217 G : SG \u2192 R denote the optimal Q and optimal value functions in the ground MDP. Definition 11 (QA, VA): Let QA = Q \u03c0\u2217A : SA \u00d7A \u2192 R and VA = V \u03c0 \u2217 A : SA \u2192 R stand for the optimal Q and optimal value functions in the abstract MDP."}, {"heading": "5.1 Main Result", "text": "We now introduce the main result of the paper.\nTheorem 1. There exist at least four types of approximate state aggregation functions, \u03c6\u0303Q\u2217,\u03b5, \u03c6\u0303model,\u03b5, \u03c6\u0303bolt,\u03b5 and \u03c6\u0303mult,\u03b5, for which the optimal policy in the abstract MDP, applied to the ground MDP, has suboptimality bounded polynomially in \u03b5:\n\u2200s\u2208SGV \u03c0\u2217G G (s)\u2212 V \u03c0GA G (s) \u2264 2\u03b5\u03b7f (10)\nWhere \u03b7f differs between abstraction function families:\n\u03b7Q\u2217 = 1\n(1\u2212 \u03b3)2\n\u03b7model = 1 + \u03b3 (|SG| \u2212 1)\n(1\u2212 \u03b3)3\n\u03b7bolt =\n( |A| 1\u2212\u03b3 + \u03b5kbolt + kbolt ) (1\u2212 \u03b3)2\n\u03b7mult =\n( |A| 1\u2212\u03b3 + kmult ) (1\u2212 \u03b3)2\nFor \u03b7bolt and \u03b7mult, we also assume that the difference in the normalizing terms of each distribution is bounded by some non-negative constant, kmult, kbolt \u2208 R, of \u03b5:\u2223\u2223\u2223\u2223\u2223\u2223 \u2211 i QG(s1, ai)\u2212 \u2211 j QG(s2, aj) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 kmult \u00d7 \u03b5\u2223\u2223\u2223\u2223\u2223\u2223 \u2211 i eQG(s1,ai) \u2212 \u2211 j eQG(s2,aj)\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2264 kbolt \u00d7 \u03b5 Naturally, the value bound of Equation 10 is meaningless for 2\u03b5\u03b7f \u2265 RMax1\u2212\u03b3 = 1 1\u2212\u03b3 , since this is the maximum possible value in any MDP (and we assumed the range of R is [0, 1]). In light of this, observe that for \u03b5 = 0, all of the above bounds are exactly 0. Any value of \u03b5 interpolated between these two points achieves different degrees of abstraction, with different degrees of bounded loss.\nWe now introduce each approximate aggregation family and prove the theorem by proving the specific value bound for each function type."}, {"heading": "5.2 Optimal Q Function: \u03c6\u0303Q\u2217,\u03b5", "text": "We consider an approximate version of Li et al. [24]\u2019s \u03c6Q\u2217 . In our abstraction, states are aggregated together when their optimal Q-values are within \u03b5.\nDefinition 12 (\u03c6\u0303Q\u2217,\u03b5): An approximate Q function abstraction has the same form as Equation 9:\n\u03c6\u0303Q\u2217,\u03b5(s1) = \u03c6\u0303Q\u2217,\u03b5(s2)\u2192 \u2200a |QG(s1, a)\u2212QG(s2, a)| \u2264 \u03b5. (11)\nLemma 1. When a \u03c6\u0303Q\u2217,\u03b5 type abstraction is used to create the abstract MDP:\n\u2200s\u2208SGV \u03c0\u2217G G (s)\u2212 V \u03c0GA G (s) \u2264\n2\u03b5\n(1\u2212 \u03b3)2 . (12)\nProof of Lemma 1: We first demonstrate that Q-values in the abstract MDP are close to Q-values in the ground MDP (Claim 1). We next leverage Claim 1 to demonstrate that the optimal action in the abstract MDP is nearly optimal in the ground MDP (Claim 2). Lastly, we use Claim 2 to conclude Lemma 1 (Claim 3).\nClaim 1. Optimal Q-values in the abstract MDP closely resemble optimal Q-values in the ground MDP:\n\u2200sG\u2208SG,a|QG(sG, a)\u2212QA(\u03c6\u0303Q\u2217,\u03b5(sG), a)| \u2264 \u03b5\n1\u2212 \u03b3 . (13)\nConsider a non-Markovian decision process of the same form as an MDP, MT = \u3008ST ,AG,RT , TT , \u03b3\u3009, parameterized by integer an T , such that for the first T time steps the reward function, transition dynamics and state space are those of the abstract MDP, MA, and after T time steps the reward function, transition dynamics and state spaces are those of MG. Thus,\nST = { SG if T = 0 SA o/w\nRT (s, a) = { RG(s, a) if T = 0 RA(s, a) o/w\nTT (s, a, s\u2032) =  TG(s, a, s\u2032) if T = 0\u2211 g\u2208G(s) [TG(g, a, s\u2032)\u03c9(g)] if T = 1\nTA(s, a, s\u2032) o/w\nThe Q-value of state s in ST for action a is:\nQT (s, a) =  QG(s, a) if T = 0\u2211 g\u2208G(s) [QG(g, a)\u03c9(g)] if T = 1\nRA(s, a) + \u03c3T\u22121(s, a) o/w\n(14)\nwhere: \u03c3T\u22121(s, a) = \u03b3 \u2211 sA\u2032\u2208SA TA(s, a, sA\u2032) max a\u2032 QT\u22121(sA \u2032, a\u2032).\nWe proceed by induction on T to show that:\n\u2200T,sG\u2208SG,a|QT (sT , a)\u2212QG(sG, a)| \u2264 T\u22121\u2211 t=0 \u03b5\u03b3t, (15)\nwhere sT = sG if T = 0 and sT = \u03c6\u0303Q\u2217,\u03b5(sG) otherwise.\nBase Case: T = 0\nWhen T = 0, QT = QG, so this base case trivially follows.\nBase Case: T = 1\nBy definition of QT , we have that Q1 is Q1(s, a) = \u2211\ng\u2208G(s)\n[QG(g, a)\u03c9(g)] .\nSince all co-aggregated states have Q-values within \u03b5 of one another and \u03c9(g) induces a convex combination,\nQ1(sT , a) \u2264 \u03b5\u03b3t + \u03b5+QG(sG, a)\n\u2234 |Q1(sT , a)\u2212QG(sG, a)| \u2264 1\u2211 t=0 \u03b5\u03b3t.\nInductive Case: T > 1\nWe assume as our inductive hypothesis that:\n\u2200sG\u2208SG,a|QT\u22121(sT , a)\u2212QG(sG, a)| \u2264 T\u22122\u2211 t=0 \u03b5\u03b3t.\nConsider a fixed but arbitrary state, sG \u2208 SG, and fixed but arbitrary action a. Since T > 1, sT is \u03c6\u0303Q\u2217,\u03b5(sG). By definition of QT (sT , a), RA, TA:\nQT (sT , a) = \u2211\ng\u2208G(sT )\n\u03c9(g) \u00d7 RG(g, a) + \u03b3 \u2211 g\u2032\u2208SG TG(g, a, g\u2032) max a\u2032 QT\u22121(g \u2032, a\u2032)  . Applying our inductive hypothesis yields:\nQT (sT , a) \u2264 \u2211\ng\u2208G(sT )\n\u03c9(g)\u00d7 [ RG(g, a) + \u03b3 \u2211 g\u2032\u2208SG TG(g, a, g \u2032) max a\u2032 (QG(g \u2032, a\u2032) + T\u22122\u2211 t=0 \u03b5\u03b3t) ] .\nSince all aggregated states have Q-values within \u03b5 of one another:\nQT (sT , a) \u2264 \u03b3 T\u22122\u2211 t=0 \u03b5\u03b3t + \u03b5+QG(sG, a).\nSince sG is arbitrary we conclude Equation 15. As T \u2192\u221e, \u2211T\u22121 t=0 \u03b5\u03b3\nt \u2192 \u03b51\u2212\u03b3 by the sum of infinite geometric series and QT \u2192 QA. Thus, Equation 15 yields Claim 1.\nClaim 2. Consider a fixed but arbitrary state, sG \u2208 SG and its corresponding abstract state sA = \u03c6\u0303Q\u2217,\u03b5(sG). Let a\u2217G stand for the optimal action in sG, and a \u2217 A stand for the optimal action in sA:\na\u2217G = arg max a QG(sG, a), a \u2217 A = arg max a QA(sA, a).\nThe optimal action in the abstract MDP has a Q-value in the ground MDP that is nearly optimal:\nVG(sG) \u2264 QG(sG, a\u2217A) + 2\u03b5\n1\u2212 \u03b3 . (16)\nBy Claim 1,\nVG(sG) = QG(sG, a \u2217 G) \u2264 QA(sA, a\u2217G) +\n\u03b5\n1\u2212 \u03b3 . (17)\nBy the definition of a\u2217A, we know that\nQA(sA, a \u2217 G) +\n\u03b5\n1\u2212 \u03b3 \u2264 QA(sA, a\u2217A) +\n\u03b5\n1\u2212 \u03b3 . (18)\nLastly, again by Claim 1, we know\nQA(sA, a \u2217 A) +\n\u03b5\n1\u2212 \u03b3 \u2264 QG(sg, a\u2217A) +\n2\u03b5\n1\u2212 \u03b3 . (19)\nTherefore, Equation 16 follows.\nClaim 3. Lemma 1 follows from Claim 2.\nConsider the policy for MG of following the optimal abstract policy \u03c0 \u2217 A for t steps and then following the optimal ground policy \u03c0\u2217G in MG:\n\u03c0A,t(s) =\n{ \u03c0\u2217G(s) if t = 0\n\u03c0GA(s) if t > 0 (20)\nFor t > 0, the value of this policy for sG \u2208 SG in the ground MDP is:\nV \u03c0A,t G (sG) = RG(s, \u03c0A,t(sG)) + \u03b3 \u2211 sG\u2032\u2208SG TG(sG, a, sG\u2032)V \u03c0A,t\u22121 G (sG \u2032).\nFor t = 0, V \u03c0A,t G (sG) is simply VG(sG). We now show by induction on t that\n\u2200t,sG\u2208SgVG(sG) \u2264 V \u03c0A,t G (sG) + t\u2211 i=0 \u03b3i 2\u03b5 1\u2212 \u03b3 . (21)\nBase Case: t = 0\nBy definition, when t = 0, V \u03c0A,t G = VG, so our bound trivially holds in this case.\nInductive Case: t > 0\nConsider a fixed but arbitrary state sG \u2208 SG. We assume for our inductive hypothesis that\nVG(sG) \u2264 V \u03c0A,t\u22121 G (sG) + t\u22121\u2211 i=0 \u03b3i 2\u03b5 1\u2212 \u03b3 . (22)\nBy definition,\nV \u03c0A,t G (sG) = RG(s, \u03c0A,t(sG)) + \u03b3 \u2211 g\u2032 TG(sG, a, sG\u2032)V \u03c0A,t\u22121 G (sG \u2032).\nApplying our inductive hypothesis yields:\nV \u03c0A,t G (sG) \u2265 RG(sG, \u03c0A,t(sG)) + \u03b3 \u2211 sG\u2032 TG(sG, \u03c0A,t(sG), sG\u2032)\n( VG(sG\n\u2032)\u2212 t\u22121\u2211 i=0 \u03b3i 2\u03b5 1\u2212 \u03b3\n) .\nTherefore,\nV \u03c0A,t G (sG) \u2265 \u2212\u03b3 t\u22121\u2211 i=0 \u03b3i 2\u03b5 1\u2212 \u03b3 +QG(sG, \u03c0A,t(sG)).\nApplying Claim 2 yields:\nV \u03c0A,t G (sG) \u2265 \u2212\u03b3 t\u22121\u2211 i=0 \u03b3i 2\u03b5 1\u2212 \u03b3 \u2212 2\u03b5 1\u2212 \u03b3 + VG(sG)\n\u2234 VG(sG) \u2264 V \u03c0A,t G (sG) + t\u2211 i=0 \u03b3i 2\u03b5 1\u2212 \u03b3 .\nSince sG was arbitrary, we conclude that our bound holds for all states in SG for the inductive case. Thus, from our base case and induction, we conclude that\n\u2200t,sG\u2208SgV \u03c0\u2217G G (sG) \u2264 V \u03c0A,t G (sG) + t\u2211 i=0 \u03b3i 2\u03b5 1\u2212 \u03b3 . (23)\nNote that as t\u2192\u221e, \u2211t i=0 \u03b3 i 2\u03b5 1\u2212\u03b3 \u2192 2\u03b5 (1\u2212\u03b3)2 by the sum of infinite geometric series and \u03c0A,t(s)\u2192 \u03c0GA. Thus, we conclude Lemma 1."}, {"heading": "5.3 Model Similarity: \u03c6\u0303model,\u03b5", "text": "Now, consider an approximate version of Li et al. [24]\u2019s \u03c6model, where states are aggregated together when their rewards and transitions are within \u03b5.\nDefinition 13 (\u03c6\u0303model,\u03b5): We let \u03c6\u0303model,\u03b5 define a type of abstraction that, for fixed \u03b5, satisfies:\n\u03c6\u0303model,\u03b5(s1) = \u03c6\u0303model,\u03b5(s2)\u2192\n\u2200a |RG(s1, a)\u2212RG(s2, a)| \u2264 \u03b5 AND \u2200sA\u2208SA \u2223\u2223\u2223\u2223\u2223\u2223 \u2211\nsG\u2032\u2208G(sA)\n[TG(s1, a, sG\u2032)\u2212 TG(s2, a, sG\u2032)] \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u03b5. (24) Lemma 2. When SA is created using a \u03c6\u0303model,\u03b5 type:\n\u2200s\u2208SGV \u03c0\u2217G G (s)\u2212 V \u03c0GA G (s) \u2264 2\u03b5+ 2\u03b3\u03b5 (|SG| \u2212 1) (1\u2212 \u03b3)3 . (25)\nProof of Lemma 2:\nLet B be the maximum Q-value difference between any pair of ground states in the same abstract state for \u03c6\u0303model,\u03b5:\nB = max s1,s2,a\n|QG(s1, a)\u2212QG(s2, a)|,\nwhere s1, s2 \u2208 G(sA). First, we expand:\nB = max s1,s2,a \u2223\u2223\u2223\u2223RG(s1, a)\u2212RG(s2, a) + \u03b3 \u2211 sG\u2032\u2208SG [ (TG(s1, a, sG\u2032)\u2212 TG(s2, a, sG\u2032)) max a\u2032 QG(sG \u2032, a\u2032) ]\u2223\u2223\u2223\u2223 (26) Since difference of rewards is bounded by \u03b5:\nB \u2264 \u03b5+ \u03b3 \u2211\nsA\u2208SA \u2211 sG\u2032\u2208G(sA) [ (TG(s1, a, sG \u2032) \u2212 TG(s2, a, sG\u2032)) max a\u2032 QG(sG \u2032, a\u2032) ] . (27)\nBy similarity of transitions under \u03c6\u0303model,\u03b5: B \u2264 \u03b5+ \u03b3QMax \u2211\nsA\u2208SA\n\u03b5 \u2264 \u03b5+ \u03b3|SG|\u03b5QMax.\nRecall that QMax = RMax1\u2212\u03b3 , and we defined RMax = 1:\nB \u2264 \u03b5+ \u03b3(|SG| \u2212 1)\u03b5 1\u2212 \u03b3 .\nSince the Q-values of ground states grouped under \u03c6\u0303model,\u03b5 are strictly less than B, we can understand \u03c6\u0303model,\u03b5 as a type of \u03c6\u0303Q\u2217,B . Applying Lemma 1 yields Lemma 2."}, {"heading": "5.4 Boltzmann over Optimal Q: \u03c6\u0303bolt,\u03b5", "text": "Here, we introduce \u03c6\u0303bolt,\u03b5, which aggregates states with similar Boltzmann distributions on Q-values. This type of abstractions is appealing as Boltzmman distributions balance exploration and exploitation [32]. We\nfind this type particularly interesting for abstraction purposes as, unlike \u03c6\u0303Q\u2217,\u03b5, it allows for aggregation when Q-value ratios are similar but their magnitudes are different.\nDefinition 14 (\u03c6\u0303bolt,\u03b5): We let \u03c6\u0303bolt,\u03b5 define a type of abstractions that, for fixed \u03b5, satisfies:\n\u03c6\u0303bolt,\u03b5(s1) = \u03c6\u0303bolt,\u03b5(s2)\u2192 \u2200a \u2223\u2223\u2223\u2223 eQG(s1,a)\u2211\nb e QG(s1,b) \u2212 e QG(s2,a)\u2211 b e QG(s2,b) \u2223\u2223\u2223\u2223 \u2264 \u03b5. (28) We also assume that the difference in normalizing terms is bounded by some non-negative constant, kbolt \u2208 R, of \u03b5: \u2223\u2223\u2223\u2223\u2223\u2211\nb eQG(s1,b) \u2212 \u2211 b eQG(s2,b) \u2223\u2223\u2223\u2223\u2223 \u2264 kbolt \u00d7 \u03b5. (29) Lemma 3. When SA is created using a function of the \u03c6\u0303bolt,\u03b5 type, for some non-negative constant k \u2208 R:\n\u2200s\u2208SGV \u03c0\u2217G G (s)\u2212 V \u03c0GA G (s) \u2264\n2\u03b5 ( |A| 1\u2212\u03b3 + \u03b5kbolt + kbolt ) (1\u2212 \u03b3)2 . (30)\nWe use the approximation for ex, with \u03b4 error:\nex = 1 + x+ \u03b4 \u2248 1 + x. (31)\nWe let \u03b41 denote the error in approximating e QG(s1,a) and \u03b42 denote the error in approximating e QG(s2,a).\nProof of Lemma 3:\nBy the approximation in Equation 31 and the assumption in Equation 29:\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 1 +QG(s1, a) + \u03b41\u2211 j e QG(s1,aj) \u2212 1 +QG(s2, a) + \u03b42\u2211 j e QG(s1,aj)\u00b1k\u03b5\ufe38\ufe37\ufe37\ufe38\na \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u03b5 (32) Either term a is positive or negative. First suppose the former. It follows by algebra that:\n\u2212 \u03b5 \u2264 1 +QG(s1, a) + \u03b41\u2211 j e QG(s1,aj) \u2212 1 +QG(s2, a) + \u03b42\u2211 j e QG(s1,aj) + \u03b5kbolt \u2264 \u03b5 (33)\nMoving terms:\n\u2212 \u03b5 k\u03b5+\u2211 j eQG(s1,aj) \u2212 \u03b41 + \u03b42 \u2264 \u03b5kbolt ( 1 +QG(s1, a) + \u03b41\u2211\nj e QG(s1,aj)\n) +QG(s1, a)\u2212QG(s2, a) \u2264\n\u03b5 \u03b5kbolt +\u2211 j eQG(s1,aj) \u2212 \u03b41 + \u03b42 (34) When a is the negative case, it follows that:\n\u2212 \u03b5 \u2264 1 +QG(s1, a) + \u03b41\u2211 j e QG(s1,aj) \u2212 1 +QG(s2, a) + \u03b42\u2211 j e QG(s1,aj) \u2212 \u03b5kbolt \u2264 \u03b5 (35)\nBy similar algebra that yielded Equation 34:\n\u2212 \u03b5 \u2212\u03b5kbolt +\u2211 j eQG(s1,aj) \u2212 \u03b41 + \u03b42 \u2264 \u2212 k\u03b5 ( 1 +QG(s1, a) + \u03b41\u2211\nj e QG(s1,aj)\n) +QG(s1, a)\u2212QG(s2, a) \u2264\n\u03b5 \u03b5kbolt +\u2211 j eQG(s1,aj) \u2212 \u03b41 + \u03b42 (36) Combining Equation 34 and Equation 36 results in:\n|QG(s1, a)\u2212QG(s2, a)| \u2264 \u03b5 ( |A|\n1\u2212 \u03b3 + \u03b5kbolt + kbolt\n) . (37)\nConsequently, we can consider \u03c6\u0303bolt,\u03b5 as a special case of the \u03c6\u0303Q\u2217,B type, where B = \u03b5 ( |A| 1\u2212\u03b3 + \u03b5kbolt + kbolt ) . Lemma 3 then follows from Lemma 1."}, {"heading": "5.5 Multinomial over Optimal Q: \u03c6\u0303mult,\u03b5", "text": "We consider approximate abstractions derived from a multinomial distribution over Q\u2217 for similar reasons to the Boltzmann distribution. Additionally, the multinomial distribution is appealing for its simplicity.\nDefinition 15 (\u03c6\u0303mult,\u03b5): We let \u03c6\u0303mult,\u03b5 define a type of abstraction that, for fixed \u03b5, satisfies \u03c6\u0303mult,\u03b5(s1) = \u03c6\u0303mult,\u03b5(s2)\u2192 \u2200a \u2223\u2223\u2223\u2223 QG(s1, a)\u2211\nbQG(s1, b) \u2212 QG(s1, a)\u2211 bQG(s1, b) \u2223\u2223\u2223\u2223 \u2264 \u03b5. (38) We also assume that the difference in normalizing terms is bounded by some non-negative constant, kmult \u2208 R, of \u03b5: \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 i QG(s1, ai)\u2212 \u2211 j QG(s2, aj)\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2264 kmult \u00d7 \u03b5. (39) Lemma 4. When SA is created using a function of the \u03c6\u0303mult,\u03b5 type, for some non-negative constant kmult \u2208 R:\n\u2200s\u2208SMV \u03c0\u2217G G (s)\u2212 V \u03c0GA G (s) \u2264\n2\u03b5 ( |A| 1\u2212\u03b3 + kmult ) (1\u2212 \u03b3)2\n(40) Proof of Lemma 4\nThe proof follows an identical strategy to that of Lemma 3, but without the approximation ex \u2248 1 + x."}, {"heading": "6 Example Domains", "text": "We apply approximate abstraction to five example domains\u2014NChain, Upworld, Taxi, Minefield and Random. These domains were selected for their diversity\u2014NChain is relatively simple, Upworld is particularly illustrative of the power of abstraction, Taxi is goal-based and hierarchical in nature, Minefield is stochastic, and Random MDP has many near-optimal policies.\nOur code base1 provides implementations for abstracting arbitrary MDPs as well as visualizing and evaluating the resulting abstract MDPs. We use the graph-visualization library GraphStream [29] and the planning and RL library, BURLAP2. For all experiments, we set \u03b3 to 0.95.\n1https://github.com/david-abel/state_abstraction 2http://burlap.cs.brown.edu/"}, {"heading": "6.1 Visualizations", "text": "We provide visuals of both the ground MDP and resulting abstract MDP for each domain. A grey circle indicates a state and colored arrows indicate transitions. The thickness of the arrow indicates how much reward is associated with that transition. In the ground MDPs, states are labeled with a number. In the abstract MDPs, we indicate which ground states were collapsed to each abstract state by labelling the abstract states with their ground states."}, {"heading": "6.1.1 NChain", "text": "NChain is a simple MDP investigated in the Bayesian RL literature due to the interesting exploration problem it poses [11]. In our implementation, we set N = 10, normalized rewards between 0 and 1, and used a slip probability of 0.2. An NChain instance (N = 10) and its abstraction are visualized in Figure6.1.2.\nIn all states, the agent has two actions available: advance down the chain, or return to state 0. The agent receives .2 reward for returning to state 0, and no reward for advancing down the chain. The exception is that when the agent transitions to the last state in the chain, it receives 1.0 reward. Transitions also have small slip probability \u03c1, such that the applied action results in the opposite dynamics. In our implementation, we set N = 10 and \u03c1 = 0.2."}, {"heading": "6.1.2 Upworld", "text": "The Upworld task is an N \u00d7M grid in which the agent starts in the lower left corner. The agent may move left, right, and up. The agent receives positive reward for transitioning to any state at the top of the grid, where moving up in the top cells self transitions. the agent receives 0 reward for all other transitions. Consequently, moving up is always the optimal action, since moving left and right does not change the agent\u2019s manhattan distance to positive reward. During experimentation, we set N = 10, M = 4. An Upworld instance (N = 10, M = 4) and its abstraction are visualized in Figure6.1.2.\nUpworld illustrates a compelling property with respect to state abstraction: the optimal exact Q\u2217 abstraction function (when \u03b5 = 0) can always construct an abstract MDP with |SA| = N , the height of the grid, with no change in the value of the optimal policy. Consequently, letting M be arbitrarily large, Upworld offers an arbitrary reduction in the size of the MDP through abstraction, at no cost to the value of the optimal policy. This is a result of the property that all states in the same row have the same Q values:\nRemark: The optimal exact abstraction, \u03c6Q\u2217,0, induces an abstract MDP with an optimal policy of equal value to the true optimal policy, and reduces the size of the state space from N\u00d7M (ground) to N (abstract)."}, {"heading": "6.1.3 Taxi", "text": "Taxi has long been studied by the hierarchical RL literature [12]. The agent, operating in a Grid World style domain [30], may move left, right, up, and down, as well as pick up a passenger and drop off a passenger. The goal is achieved when the agent has taken all passengers to their destinations.\nWe visualize the compression on a simple 626 Taxi instance in Figure 3. As stated above, we visualize the original Taxi problem into a graph representation so that we may visualize both the ground MDP and abstract MDP in the same format, despite the unnatural appearance."}, {"heading": "6.1.4 Minefield", "text": "Minefield is a test problem we are introducing that uses the Grid World dynamics of Russell and Norvig [30] with slip probability of x. The reward function is such that moving up in the top row of the grid receives 1.0 reward; all other transitions receive 0.2 reward, except for transitions to a random set of \u03ba mine-states (which may include the top row) that receive 0 reward. We set N = 10,M = 4, \u03b5 = 0.5, \u03ba = 5, x = 0.01."}, {"heading": "6.1.5 Random MDP", "text": "In the Random MDP domain we consider, there are 100 states and 3 actions. For each state, each action transitions to one of two randomly selected states with probability 0.5. The Random MDP and its compression are visualized in Figure 4."}, {"heading": "7 Empirical Results", "text": "We ran experiments on the \u03c6\u0303Q\u2217,\u03b5 type aggregation functions. We provide results for only \u03c6\u0303Q\u2217,\u03b5 because, as our proofs in Section 5 demonstrate, the other three functions are reducible to particular \u03c6\u0303Q\u2217,\u03b5 functions. For the purpose of illustrating what kinds of approximations are possible we built each abstraction by first solving the MDP, then greedily aggregating ground states into abstract states that satisfied the \u03c6\u0303Q\u2217,\u03b5 criteria. Since this approach represents an order-dependent approximation to the maximum amount of abstraction possible, we randomized the order in which states were considered across trials. Every ground state is equally weighted in its abstract state.\nFor each domain, we report two quantities as a function of epsilon with 95% confidence bars. First, we compare the number of states in the abstract MDP for different values of \u03b5, shown in the left column of Figure 5 and Figure 6. The smaller the number of abstract states, the smaller the state space of the MDP that the agent must plan over. Second, we report the value under the abstract policy of the initial ground state, also shown in the right column of Figure 5 and Figure 6. In the Taxi and Random domains, 200 trials were run for each data point, whereas 20 trials were sufficient in Upworld, Minefield, and NChain.\nOur empirical results corroborate our thesis\u2014approximate state abstractions can decrease state space size while retaining bounded error. In both NChain and Minefield, we observe that, as \u03b5 increases from 0, the number of states that must be planned over is reduced, and optimal behavior is either fully maintained (NChain) or very nearly maintained (Minefield). Similarly for Taxi, when \u03b5 is between .02 and .025, we observe a reduction in the number of states in the abstract MDP while value is fully maintained. After .025, increased reduction in state space size comes at a cost of value. Lastly, as \u03b5 is increased in the Random domain, there is a smooth reduction in the number of abstract states with a corresponding cost in the value of the derived policy. When \u03b5 = 0, there is no reduction in state space size whatsoever (the ground MDP has 100 states), because no two states have identical optimal Q-values.\nOur experimental results also highlight a noteworthy characteristic of approximate state abstraction in goal-based MDPs. Taxi exhibits relative stability in state space size and behavior for \u03b5 up to .02, at which point both fall off dramatically. We attribute the sudden fall off of these quantities to the goal-based nature of the domain; once information critical for achieving optimal behavior is lost in the state aggregation,\nsolving the goal\u2014and so acquiring any reward\u2014is impossible. Conversely, in the Random domain, a great deal of near optimal policies are available to the agent. Thus, even as the information for optimal behavior is lost, there are many near optimal policies available to the agent that remain available."}, {"heading": "8 Conclusion", "text": "Approximate abstraction in MDPs offers considerable advantages over exact abstraction. First, approximate abstraction relies on criteria that we imagine a planning or learning algorithm to be able to learn without solving the full MDP. Second, approximate abstractions can achieve greater degrees of compression due to their relaxed criteria of equality. Third, methods that employ approximate aggregation techniques are able to tune the aggressiveness of abstraction all the while incurring bounded error. In this work, we proved bounds for the value lost when behaving according to the optimal policy of the abstract MDP, and empirically demonstrate that approximate abstractions can reduce state space size with minor loss in the quality of the behavior. We provide a code base that provides implementations to abstract, visualize, and evaluate an arbitrary MDP to promote further investigation into approximate abstraction.\nThere are many directions for future work. First, we are interested in extending the approach of Ortner [27] by learning the approximate abstraction functions introduced in this paper online in the planning or RL setting, particularly when the agent must solve a collection of related MDPs. Additionally, while\nour work presents several sufficient conditions for achieving bounded error of learned behavior with approximate abstractions, we hope to investigate what conditions are strictly necessary for an approximate abstraction to achieve bounded error. Further, we are interested in characterizing the relationship between temporal abstractions, such as options [33] and approximate state abstractions. Lastly, we are interested in understanding the relationship between various approximate abstractions and the information theoretical limitations on the degree of abstraction achievable in MDPs."}], "references": [{"title": "Goal-based action priors", "author": ["David Abel", "David Ellis Hershkowitz", "Gabriel Barth-Maron", "Stephen Brawner", "Kevin O\u2019Farrell", "James MacGlashan", "Stefanie Tellex"], "venue": "In ICAPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "State abstraction for programmable reinforcement learning agents", "author": ["David Andre", "Stuart J Russell"], "venue": "In AAAI/IAAI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Peter Auer", "Thomas Jaksch", "Ronald Ortner"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization", "author": ["Bram Bakker", "J\u00fcrgen Schmidhuber"], "venue": "In Proc. of the 8-th Conf. on Intelligent Autonomous Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["Andrew G Barto", "Sridhar Mahadevan"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Dynamic programming aggregation", "author": ["James C Bean", "John R Birge", "Robert L Smith"], "venue": "Operations Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Feudal Reinforcement Learning", "author": ["Peter Dayan", "Geoffrey Hinton"], "venue": "Advances in neural information processing systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "Model minimization in markov decision processes", "author": ["Thomas Dean", "Robert Givan"], "venue": "In AAAI/IAAI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Model reduction techniques for computing approximately optimal solutions for markov decision processes", "author": ["Thomas Dean", "Robert Givan", "Sonia Leach"], "venue": "In Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Abstraction and approximate decision-theoretic planning", "author": ["Richard Dearden", "Craig Boutilier"], "venue": "Artificial Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Bayesian Q-learning", "author": ["Richard Dearden", "Nir Friedman", "Stuart Russell"], "venue": "In AAAI/IAAI,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Hierarchical reinforcement learning with the MAXQ value function decomposition", "author": ["Thomas G Dietterich"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Approximate equivalence of Markov decision processes", "author": ["Eyal Even-Dar", "Yishay Mansour"], "venue": "In Learning Theory and Kernel Machines,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Metrics for finite markov decision processes", "author": ["Norm Ferns", "Prakash Panangaden", "Doina Precup"], "venue": "In Proceedings of the 20th conference on Uncertainty in artificial intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Methods for computing state similarity in markov decision processes", "author": ["Norman Ferns", "Pablo Samuel Castro", "Doina Precup", "Prakash Panangaden"], "venue": "Proceedings of the 22nd conference on Uncertainty in artificial intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "State Aggregation in Monte Carlo Tree Search", "author": ["Jesse Hostetler", "Alan Fern", "Tom Dietterich"], "venue": "Aaai 2014,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Extreme state aggregation beyond mdps", "author": ["Marcus Hutter"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Extreme state aggregation beyond markov decision processes", "author": ["Marcus Hutter"], "venue": "Theoretical Computer Science,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Abstraction Selection in Model-Based Reinforcement Learning", "author": ["Nan Jiang"], "venue": "icml,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Improving uct planning via approximate homomorphisms", "author": ["Nan Jiang", "Satinder Singh", "Richard Lewis"], "venue": "In Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "State abstraction discovery from irrelevant state variables", "author": ["Nicholas K Jong", "Peter Stone"], "venue": "In IJCAI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Reinforcement learning: A survey", "author": ["Leslie Pack Kaelbling", "Michael L Littman", "Andrew W Moore"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1996}, {"title": "Bandit based monte-carlo planning", "author": ["Levente Kocsis", "Csaba Szepesv\u00e1ri"], "venue": "In European conference on machine learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Towards a unified theory of state abstraction for mdps", "author": ["Lihong Li", "Thomas J Walsh", "Michael L Littman"], "venue": "In ISAIM,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "On the complexity of solving Markov decision problems", "author": ["Michael L Littman", "Thomas L Dean", "Leslie Pack Kaelbling"], "venue": "In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1995}, {"title": "Efficient bayesian clustering for reinforcement learning", "author": ["Travis Mandel", "Yun-En Liu", "Emma Brunskill", "Zoran Popovic"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Adaptive aggregation for reinforcement learning in average reward Markov decision processes", "author": ["Ronald Ortner"], "venue": "Annals of Operations Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "The complexity of Markov decision processes", "author": ["Christos H Papadimitriou", "John N Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1987}, {"title": "Graphstream: A tool for bridging the gap between complex systems and dynamic graphs", "author": ["Yoann Pign\u00e9", "Antoine Dutot", "Fr\u00e9d\u00e9ric Guinand", "Damien Olivier"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Artificial Intelligence A Modern Approach", "author": ["Stuart Russell", "Peter Norvig"], "venue": "Prentice-Hall, Englewood Cliffs,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1995}, {"title": "Reinforcement Learning in Finite MDPs : PAC Analysis", "author": ["Alexander L. Strehl", "Lihong Li", "Michael L. Littman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1998}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1999}, {"title": "Transferring state abstractions between mdps", "author": ["Thomas J Walsh", "Lihong Li", "Michael L Littman"], "venue": "In ICML Workshop on Structural Knowledge Transfer for Machine Learning,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}], "referenceMentions": [{"referenceID": 27, "context": "Solving for optimal behavior in MDPs in a planning setting is known to be P-Complete in the size of the state space [28, 25].", "startOffset": 116, "endOffset": 124}, {"referenceID": 24, "context": "Solving for optimal behavior in MDPs in a planning setting is known to be P-Complete in the size of the state space [28, 25].", "startOffset": 116, "endOffset": 124}, {"referenceID": 30, "context": "Similarly, many RL algorithms for solving MDPs are known to require a number of samples polynomial in the size of the state space [31].", "startOffset": 130, "endOffset": 134}, {"referenceID": 0, "context": "For instance, a robot involved in a pick-and-place task might be able to employ planning algorithms to solve for how to manipulate some objects into a desired configuration in time polynomial in the number of states, but the number of states it must consider grows exponentially with the number of objects with which it is working [1].", "startOffset": 331, "endOffset": 334}, {"referenceID": 1, "context": "Thus, a key research agenda for planning and RL is leveraging abstraction to reduce large state spaces [2, 21, 10, 12, 6].", "startOffset": 103, "endOffset": 121}, {"referenceID": 20, "context": "Thus, a key research agenda for planning and RL is leveraging abstraction to reduce large state spaces [2, 21, 10, 12, 6].", "startOffset": 103, "endOffset": 121}, {"referenceID": 9, "context": "Thus, a key research agenda for planning and RL is leveraging abstraction to reduce large state spaces [2, 21, 10, 12, 6].", "startOffset": 103, "endOffset": 121}, {"referenceID": 11, "context": "Thus, a key research agenda for planning and RL is leveraging abstraction to reduce large state spaces [2, 21, 10, 12, 6].", "startOffset": 103, "endOffset": 121}, {"referenceID": 5, "context": "Thus, a key research agenda for planning and RL is leveraging abstraction to reduce large state spaces [2, 21, 10, 12, 6].", "startOffset": 103, "endOffset": 121}, {"referenceID": 23, "context": "Existing work has characterized how exact abstractions can fully maintain optimality in MDPs [24, 8].", "startOffset": 93, "endOffset": 100}, {"referenceID": 7, "context": "Existing work has characterized how exact abstractions can fully maintain optimality in MDPs [24, 8].", "startOffset": 93, "endOffset": 100}, {"referenceID": 0, "context": "Here, S is a finite state space; A is a finite set of actions available to the agent; T denotes T (s, a, s\u2032), the probability of an agent transitioning to state s\u2032 \u2208 S after applying action a \u2208 A in state s \u2208 S; R(s, a) denotes the reward received by the agent for executing action a in state s; \u03b3 \u2208 [0, 1] is a discount factor that determines how much the agent prefers future rewards over immediate rewards.", "startOffset": 300, "endOffset": 306}, {"referenceID": 0, "context": "We assume without loss of generality that the range of all reward functions is normalized to [0, 1].", "startOffset": 93, "endOffset": 99}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] leverage the notion of bisimulation to investigate partitioning an MDP\u2019s state space into clusters of states whose transition model and reward function are within \u03b5 of each other.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14, 15] investigated state similarity metrics for MDPs; they bounded the value difference of ground states and abstract states for several bisimulation metrics that induce an abstract MDP.", "startOffset": 0, "endOffset": 8}, {"referenceID": 14, "context": "[14, 15] investigated state similarity metrics for MDPs; they bounded the value difference of ground states and abstract states for several bisimulation metrics that induce an abstract MDP.", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "Even-Dar and Mansour [13] analyzed different distance metrics used in identifying state space partitions subject to \u03b5-similarity, also providing value bounds (their Lemma 4) for \u03b5-homogeneity subject to the L\u221e norm, which parallels our Claim 2.", "startOffset": 21, "endOffset": 25}, {"referenceID": 26, "context": "Ortner [27] developed an algorithm for learning partitions in an online setting by taking advantage of the confidence bounds for T and R provided by UCRL [3].", "startOffset": 7, "endOffset": 11}, {"referenceID": 2, "context": "Ortner [27] developed an algorithm for learning partitions in an online setting by taking advantage of the confidence bounds for T and R provided by UCRL [3].", "startOffset": 154, "endOffset": 157}, {"referenceID": 17, "context": "Hutter [18, 17] investigates state aggregation beyond the MDP setting.", "startOffset": 7, "endOffset": 15}, {"referenceID": 16, "context": "Hutter [18, 17] investigates state aggregation beyond the MDP setting.", "startOffset": 7, "endOffset": 15}, {"referenceID": 23, "context": "[24] that Q\u2217 abstractions preserve the optimal value function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] apply state abstraction to Monte Carlo Tree Search and expectimax search, giving value bounds of applying the optimal abstract action in the ground tree(s), similarly to our setting.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Dearden and Boutilier [10] also formalize stateabstraction for planning, focusing on abstractions that are quickly computed and offer bounded value.", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "[20] analyze a similar setting, applying abstractions to the Upper Confidence Bound applied to Trees algorithm adapted for planning, introduced by Kocsis and Szepesv\u00e1ri [23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[20] analyze a similar setting, applying abstractions to the Upper Confidence Bound applied to Trees algorithm adapted for planning, introduced by Kocsis and Szepesv\u00e1ri [23].", "startOffset": 169, "endOffset": 173}, {"referenceID": 25, "context": "[26] advance Bayesian aggregation in RL to define Thompson Clustering for Reinforcement Learning (TCRL), an extension of which achieves near-optimal Bayesian regret bounds.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Jiang [19] analyze the problem of choosing between two candidate abstractions.", "startOffset": 6, "endOffset": 10}, {"referenceID": 1, "context": "Andre and Russell [2] investigated a method for state abstraction in hierarchical reinforcement learning leveraging a programming language called ALISP that promotes the notion of safe state abstraction.", "startOffset": 18, "endOffset": 21}, {"referenceID": 11, "context": "Dietterich [12] developed MAXQ, a framework for composing tasks into an abstracted hierarchy where state aggregation can be applied.", "startOffset": 11, "endOffset": 15}, {"referenceID": 3, "context": "Bakker and Schmidhuber [4] also target hierarchical abstraction, focusing on subgoal discovery.", "startOffset": 23, "endOffset": 26}, {"referenceID": 20, "context": "Jong and Stone [21] introduced a method called policy-irrelevance in which agents identify (online) which state variables may be safely abstracted away in a factored-state MDP.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "Dayan and Hinton [7] develop \u201cFeudal Reinforcement Learning\u201d which presents an early form of hierarchical RL that restructures Q-Learning to manage the decomposition of a task into subtasks.", "startOffset": 17, "endOffset": 20}, {"referenceID": 23, "context": "[24], and for a survey of early works on hierarchical reinforcement learning, see Barto and Mahadevan [5].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[24], and for a survey of early works on hierarchical reinforcement learning, see Barto and Mahadevan [5].", "startOffset": 102, "endOffset": 105}, {"referenceID": 23, "context": "[24] developed a framework for exact state abstraction in MDPs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] build on the framework they previously developed by showing empirically how to transfer abstractions between structurally related MDPs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24], who introduced a unifying theoretical framework for state abstraction in MDPs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "The only restriction placed on the weighting scheme is that it induces a probability distribution on the ground states of each abstract state: \u2200s \u2208 SG \uf8eb\uf8ed \u2211 s\u2208G(s) \u03c9(s) \uf8f6\uf8f8 = 1 AND \u03c9(s) \u2208 [0, 1].", "startOffset": 186, "endOffset": 192}, {"referenceID": 0, "context": "Definition 5 (RA): The abstract reward function RA : SA \u00d7A 7\u2192 [0, 1] is a weighted sum of the rewards of each of the ground states that map to the same abstract state: RA(s, a) = \u2211 g\u2208G(s) RG(g, a)\u03c9(g).", "startOffset": 62, "endOffset": 68}, {"referenceID": 0, "context": "Definition 6 (TA): The abstract transition function TA : SA \u00d7 A \u00d7 SA 7\u2192 [0, 1] is a weighted sum of the transitions of each of the ground states that map to the same abstract state: TA(s, a, s\u2032) = \u2211 g\u2208G(s) \u2211 g\u2032\u2208G(s\u2032) TG(g, a, g\u2032)\u03c9(g).", "startOffset": 72, "endOffset": 78}, {"referenceID": 0, "context": "j eG2j \u2223\u2223\u2223\u2223\u2223 \u2264 kbolt \u00d7 \u03b5 Naturally, the value bound of Equation 10 is meaningless for 2\u03b5\u03b7f \u2265 RMax 1\u2212\u03b3 = 1 1\u2212\u03b3 , since this is the maximum possible value in any MDP (and we assumed the range of R is [0, 1]).", "startOffset": 198, "endOffset": 204}, {"referenceID": 23, "context": "[24]\u2019s \u03c6Q\u2217 .", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24]\u2019s \u03c6model, where states are aggregated together when their rewards and transitions are within \u03b5.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "This type of abstractions is appealing as Boltzmman distributions balance exploration and exploitation [32].", "startOffset": 103, "endOffset": 107}, {"referenceID": 28, "context": "We use the graph-visualization library GraphStream [29] and the planning and RL library, BURLAP.", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "1 NChain NChain is a simple MDP investigated in the Bayesian RL literature due to the interesting exploration problem it poses [11].", "startOffset": 127, "endOffset": 131}, {"referenceID": 11, "context": "3 Taxi Taxi has long been studied by the hierarchical RL literature [12].", "startOffset": 68, "endOffset": 72}, {"referenceID": 29, "context": "The agent, operating in a Grid World style domain [30], may move left, right, up, and down, as well as pick up a passenger and drop off a passenger.", "startOffset": 50, "endOffset": 54}, {"referenceID": 29, "context": "4 Minefield Minefield is a test problem we are introducing that uses the Grid World dynamics of Russell and Norvig [30] with slip probability of x.", "startOffset": 115, "endOffset": 119}, {"referenceID": 26, "context": "First, we are interested in extending the approach of Ortner [27] by learning the approximate abstraction functions introduced in this paper online in the planning or RL setting, particularly when the agent must solve a collection of related MDPs.", "startOffset": 61, "endOffset": 65}], "year": 2017, "abstractText": "The combinatorial explosion that plagues planning and reinforcement learning (RL) algorithms can be moderated using state abstraction. Prohibitively large task representations can be condensed such that essential information is preserved, and consequently, solutions are tractably computable. However, exact abstractions, which treat only fully-identical situations as equivalent, fail to present opportunities for abstraction in environments where no two situations are exactly alike. In this work, we investigate approximate state abstractions, which treat nearly-identical situations as equivalent. We present theoretical guarantees of the quality of behaviors derived from four types of approximate abstractions. Additionally, we empirically demonstrate that approximate abstractions lead to reduction in task complexity and bounded loss of optimality of behavior in a variety of environments.", "creator": "LaTeX with hyperref package"}}}