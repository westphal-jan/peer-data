{"id": "1701.04508", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jan-2017", "title": "Online Learning with Regularized Kernel for One-class Classification", "abstract": "This paper presents an online learning with regularized kernel based one-class extreme learning machine (ELM) classifier and is referred as online RK-OC-ELM. The baseline kernel hyperplane model considers whole data in a single chunk with regularized ELM approach for offline learning in case of one-class classification (OCC). Further, the basic hyper plane model is adapted in an online fashion from stream of training samples in this paper. The average log scale of the test is:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 17 Jan 2017 01:40:07 GMT  (756kb,D)", "http://arxiv.org/abs/1701.04508v1", "Paper has been submitted to special issue of IEEE Transactions on Systems, Man and Cybernetics: Systems with Manuscript ID: SMCA-16-09-1033"]], "COMMENTS": "Paper has been submitted to special issue of IEEE Transactions on Systems, Man and Cybernetics: Systems with Manuscript ID: SMCA-16-09-1033", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chandan gautam", "aruna tiwari", "sundaram suresh", "kapil ahuja"], "accepted": false, "id": "1701.04508"}, "pdf": {"name": "1701.04508.pdf", "metadata": {"source": "CRF", "title": "Online Learning with Regularized Kernel for One-Class Classification", "authors": ["Chandan Gautam", "Aruna Tiwari", "Sundaram Suresh", "Kapil Ahuja"], "emails": ["chandangautam31@gmail.com).", "artiwari@iiti.ac.in).", "ssundaram@ntu.edu.sg).", "kahuja@iiti.ac.in"], "sections": [{"heading": null, "text": "Index Terms\u2014One-Class Classification, Kernel, Regularization, Online Sequential, Extreme Learning Machine (ELM), Outliers Detection.\nI. INTRODUCTION\nTHE term \u2018One-Class Classification \u2019(OCC) was coined byMoya et al. [1]. The OCC has been generally employed to solve the problem of novelty, outlier, intrusion or fault detection [2]. These problems are also solved by multi-class classification when samples of both classes, normal and outlier class, are available [3] [4] [5] [6]. However, the OCC has been applied when either data of only one class is available or the data belonging to other classes is very rare. Oneclass classifier simply describes the data, therefore it is also called as a data descriptor. Most of the research in OCC has been based upon Support Vector Machine (SVM) and\n*Corresponding Author Chandan Gautam is with the Indian Institute of Technology Indore, (e-mail: chandangautam31@gmail.com). Aruna Tiwari is with the Indian Institute of Technology Indore, (e-mail: artiwari@iiti.ac.in). Sundaram Suresh is with the Nanyang Technological University, 639798 Singapore, (e-mail: ssundaram@ntu.edu.sg). Kapil Ahuja is with the Indian Institute of Technology Indore, (e-mail: kahuja@iiti.ac.in ).\nneural network. Two types of SVM based one-class classifier have been proposed, namely, One-Class SVM (OCSVM) [7] and Support Vector Domain Description (SVDD) [8]. After surveying various papers, it has been observed that SVM based one-class classifier has been explored more compared to other one class methods [2]. SVM based one-class classifier has also been developed for incremental and online learning [9]. For various one-class classifiers like SVDD, autoencoder etc., a toolbox is available in [10]. Based on different nature of data used during learning hyperplane of OCC, it can be classified as [11] (i) with positive examples only, (ii) with the positive and small amount of negative examples only, and (iii) with positive and unlabeled data. Online learning has attracted researchers in recent years due to its capability to handle high volume of streaming data. This is because it is computationally expensive to handle as well as costly to store large volumes of data.\nNeural network with backpropagation has been explored in past for OCC task [12] [13]. The learning algorithm in these one-class classifiers uses iterative and computationally intensive gradient descent based approach. This issue has been addressed by single layer feed-forward network in past decades. Single layer feed-forward network got quite attention by researchers for multi-class classification and regression due to its fast training capability as it does not adjust the weights by back propagation. Huang et al. [14] [15] also developed a single layer feed-forward network, called as Extreme Learning Machine (ELM). Online version of ELM has also been developed for multi-class classification and regression tasks with random [16] and kernel feature mapping [17] [18] [19]. In past decades, ELM has been well expanded in both dimensions: theory [20] [21] [22] [23] and application [24] [18] [25] [26]. A detailed survey on ELM and its application can be found in [27]. Recently, Leng et al. [28] developed ELM for OCC due to its fast learning speed, which supports only offline learning. Leng et al. [28] tested their model with only one type of threshold deciding criteria, i.e. rejection of few percentages of most deviant training samples after completion of training. Later, Iosifidis [29] improvised ELM based one-class classifier based on geometric information of class for offline learning. Most recently, Gautam et al. [30]1 [31] explored further ELM based one-class classifier for mainly offline learning and tested it with three different threshold criteria. Gautam et al. [30] have shown particularly two aspects: (i) random feature mapping based offline methods have been outperformed by kernel feature mapping based offline methods (ii) the possibility of\n1you can download the submitted version of accepted paper form the link: http://goo.gl/XhkVpE . We are providing this url because paper is accepted but not published yet.\nar X\niv :1\n70 1.\n04 50\n8v 1\n[ cs\n.L G\n] 1\n7 Ja\nn 20\n17\ndevelopment of online learning with ELM based one-class classifier. However, developed online one-class classifiers produce very inferior performance compared to offline one-class classifiers. Hence, there is a need to develop an online oneclass classifier with fast learning approach, which is required to provide performance similar to batch learning algorithm and handle stream of data. Therefore, regularized kernel based online-sequential one-class classifiers have been proposed in this paper, which can perform similar to offline one-class classifiers. Proposed one-class classifiers are developed for two types of framework: (i) boundary and (ii) reconstruction. Boundary framework based proposed method is developed as a fast online single output node architecture with kernel feature mapping and reconstruction based approach is developed as a fast online autoencoder with kernel feature mapping. Boundary framework based proposed method is explored with one type of threshold and reconstruction framework based proposed method is explored with two type of threshold criteria. Out of two threshold criteria, one rejects few samples from target data to decide threshold point, however, another threshold criteria doesn\u2019t reject any sample from target data to decide threshold point. Data description capability of the proposed methods is tested on synthetic or artificial datasets and compared the outcomes with random feature mapping based online oneclass classifier. Results on six benchmark data exhibit that performance of the proposed online one-class classifiers is either similar or better compared to traditional offline autoencoder and incremetal SVDD. Among two types of framework, reconstruction framework based proposed methods perform better than boundary framework based proposed methods.\nThe rest of the paper is organized as follows: Section II discussed about the proposed work. Performance evaluation has been discussed in Section III. Section IV contains the conclusion of the work."}, {"heading": "II. ONLINE LEARNING WITH REGULARIZED KERNEL FOR", "text": "OCC\nIn this section, ELM based online sequential one-class classifier is modeled by taking both factors viz., regularization and kernelization, into account. This is called as online learning with regularized kernel for one-class ELM (Online RK-OCELM) in this paper. Regularization factor helps classifier to achieve a better generalization capability for noisy data. Tikhonov regularization [32] has been employed due to its capability to handle ill-posed and singular problem, which generally emerges in solving the inverse problem. Following subsections discusses for two types of framework viz., boundary and reconstruction:"}, {"heading": "A. Online RK-OC-ELM: Boundary Framework Based Approach", "text": "In Boundary framework based Online RK-OC-ELM, model is trained by only target data X and endeavored to approximate all data to any real number. Fig. 1 shows online OCC with single output node architecture. In Fig. 1, given a stream of training data X , {(x1, c1), (x2, c2), ..., (xt, ct), ...}, where xt = [x1t , x2t , ..., xnt ] \u2208 <n is n-dimensional input of\nthe tth sample and ct is the class label of the target class, which is same for all the training data. Input layer takes data for tth input sample is coded as (xt, Rt) because model has to approximate all data to any real number R. Target output vector R is represented as [R1, R2....Rt, ...], however, value of Rt will be same for all samples. Here, value of Rt is considered as 1 for all the experiments. Further, kernel feature mapping has been employed between input and hidden layer \u03c6. Different symbol has been used for kernel matrix to avoid confusion between kernel \u03c6 and random feature mapping H . During training, hidden layer output or kernel matrix \u03c6 will be a square symmetric matrix of size [t \u00d7 t]. Output weight \u03b2 for any t samples in sequence of data is represented as [\u03b211, \u03b221....\u03b2(t\u22121)1, \u03b2tt]. R\u0302 = [R\u03021, R\u03022...R\u0302t, ...] is the predicted output vector and R\u0302t is the predicted output for tth sample. c\u0302 = [c\u03021, c\u03022...c\u0302t, ...] is the predicted class vector, where, c\u0302t is the predicted class for tth sample. Online learning algorithm of Online RK-OC-ELM for boundary framework based approach is as discussed below. Result of breast cancer and ecoli dataset is provided jointly in the table as both have well separated classes. Similarly, result of diabetes and liver datasets have provided jointly because both have overlapped classes, and, sonar and spectf datasets are also provided jointly as they are high dimensional datasets.\nFor initial chunk of N0 dataset {X0,R0} , objective is to minimize the output weight \u03b20 as well as error E0 between expected (R0) and predicted value (\u03c6(X0)\u03b20). For regularization, \u03bb is used as a regularization parameter with minimization problem. Hence, minimization problem can be written as follows:\nMinimize : L = 1\n2 \u2016\u03b20\u20162 + \u03bb\n1 2 \u2016E0\u20162\nSubject to : \u03c6(X0)\u03b20 = R0 \u2212E0 (1)\nAfter solving above minimization problem using KarushKuhn-Tucker (KKT) theorem [33], it yields the following solution:\n\u03b20 = P0R0\nP0 = \u03c6 \u22121 0\n\u03c60 = \u03c6(X0)\n(2)\nHere, \u03c60 is a kernel matrix for initial N0 of size N0 \u00d7 N0. Kernel matrix \u03c60 is defined based on Mercer\u2019s condition. Hence, any kernel method which satisfies Mercer\u2019s condition can be adopted as the kernel for the proposed classifier. For initial N0 sample, kernel matrix will be defined as:\n\u03c6 = \u03c60 =     \u212611 \u212612 ... \u21261N0 \u212621 \u212622 ... \u21262N0 ... ... . . . ...\n\u2126N01 \u2126N02 .... \u2126N0N0\n + I \u03bb   (3)\nIn\nP = P0 = \u03c6 \u22121 0 =     \u212611 \u212612 ... \u21261N0 \u212621 \u212622 ... \u21262N0 ... ... . . . ...\n\u2126N01 \u2126N02 .... \u2126N0N0\n + I \u03bb   \u22121\n(4) Initially, \u03c6 and P will be equal to \u03c60 and P0 respectively. Here, \u03c6 represents kernel matrix and P represents inverse of this kernel matrix for all the arrived samples till now for training. Here, \u03c6 and P will be updated continuously for any upcoming new samples Xv as per Equation (5), where, Xv= {(xv1, c1), (xv2, c2), ..., (xvs , cs)} and Xv \u2282X . Xv is just next chunk of the data. Current value of \u03c6 is represented as \u03c6u. Here, u and v simply denote old and new values respectively. Now, calculate \u03c6 after arrival of new sample as follows:\n\u03c6 =\n[ \u03c6u \u03c6u,v\n(\u03c6u,v) T \u03c6v\n] (5)\nHere, \u03c6 is combination of four block matrices. \u03c6u is the old value of \u03c6. Block matrix \u03c6u,v and \u03c6v in Equation (5) are calculated as per Equation (6), which is discussed below.\nLet the number of samples processed till now be b and number of samples in the current chunk be s. b is initially equal to N0. Update b and s each time when calculation starts for new samples. The block matrices \u03c6v and \u03c6u,v can be defined as follows:\n\u03c6v =     K(xv1, x v 1) ... K(x v 1, x v s) ... . . .\n... K(xvs , x v 1) .... K(x v s ,x v s)\n + I\n\u03bb\n  (6)\n\u03c6u,v =   K(xu1 , x v 1) ... K(x u 1 ,x v s) ... . . .\n... K(xub , x v 1) .... K(x u b , x v s)\n  (7)\nNow, value of P will be inverse of \u03c6 in Equation (5) as follows:\nP = \u03c6\u22121 = [\n\u03c6u \u03c6u,v (\u03c6u,v) T \u03c6v\n]\u22121 (8)\nFurther, compute the inverse in Equation (8) using block matrix inverse formula [34].\nS = D\u22121 = [ D11 D12 D21 D22 ]\u22121 = [ S11 S12 S21 S22 ] (9)\nwhere,\nS11 = (D11 \u2212D12D\u2212122 D21)\u22121 S12 = \u2212D\u2212111 D12(D22 \u2212D21D\u2212111 D12)\u22121 S21 = \u2212D\u2212122 D21(D11 \u2212D12D\u2212122 D21)\u22121 S22 = (D22 \u2212D21D\u2212111 D12)\u22121\nHence, Equation (8) will be rewritten as follows:\nP = \u03c6\u22121 = [ P11 P12 P21 P22 ] (10)\nP11, P12, P21 and P22 in Equation (10) can be written as follows:\nP11 = (\u03c6u \u2212 \u03c6u,v\u03c6\u22121v \u03c6Tu,v)\u22121 P12 = \u2212\u03c6\u22121u \u03c6u,vP22 P21 = \u2212\u03c6\u22121v \u03c6Tu,vP11 P22 = (\u03c6v \u2212 \u03c6Tu,v\u03c6\u22121u \u03c6u,v)\u22121\n(11)\nP11 can be expanded by employing Woodbury formula [34] as:\nP11 = (\u03c6u \u2212 \u03c6u,v\u03c6\u22121v \u03c6Tu,v)\u22121 = \u03c6\u22121u \u2212 \u03c6\u22121u \u03c6u,v(\u03c6Tu,v\u03c6\u22121u \u03c6u,v + \u03c6\u22121v ) \u22121\u03c6Tu,v\u03c6 \u22121 u\n(12)\nIn a similar fashion, Equation (11) can be explored for P12, P21 and P22. After processing the training dataset X , output function can be written for any set of k samples Xk = {x1, x2, ..., xk} as follows:\nf(Xk) =   K(X,x1) . . .\nK(X,xk)\n  T PR (13)\nwhere, K(X,xi) denotes kernel vector for ith sample xi. Further, perform the following steps to decide whether any sample is outlier or not:\n(i) Calculate distance (d) between the predicted value of the tth training sample and R as follows:\nd(xt) = |f(xt)\u2212Rt| = \u2223\u2223\u2223R\u0302t \u2212Rt \u2223\u2223\u2223 (14)\n(ii) After calculating distances d as per above Equation (14), sort the differences in decreasing order. Further, reject few percent of training samples based on the deviation. More deviated sample will be rejected first because they are most probably far from distribution of the target data. The threshold will be decided based on those deviations as follows:\n\u03b81 = d(b\u03b7 \u2217Nc) (15) Where 0 < \u03b7 \u2264 1, N is the number of training samples and \u03b7 is the fraction of rejection of training samples for deciding threshold value. We have considered 10% of rejection, i.e. \u03b7 = 0.1. Now, generate a decision function to decide whether any new sample z belongs to the target or outlier, where z = [z1, z2, ..., zn] as per following equation:\nSign(\u03b81 \u2212 d(z)) = {\n1, z is classified as target \u22121, z is classified as outlier\n(16) The above proposed approach is summarized as pseudo code\nin Algorithm 1."}, {"heading": "B. Online RK-OC-ELM: Reconstruction Based Approach", "text": "In reconstruction framework based Online RK-OC-ELM, model is trained by only target data X and endeavored to approximate all data to itself. Fig. 2 shows the architecture for reconstruction framework based Online RK-OC-ELM. In Fig. 2, given a stream of training data X is defined same as discussed above in the boundary framework based one-class classifier. Input layer takes data for tth input sample as (xt, xt) because the target is identical to the input layer. Further, kernel feature mapping is employed between input and hidden layer \u03c6. During training, hidden layer output or kernel matrix \u03c6 will be a square symmetric matrix of size [t \u00d7 t]. Output weight \u03b2 till tth samples is represented as [\u03b21, \u03b22....\u03b2(t\u22121), \u03b2t]. X\u0302 = (x\u03021, x\u03022...x\u0302t, ...) is the predicted output vectors, where x\u0302t = [x\u03021t , x\u0302 2 t , ..., x\u0302 n t ] is the\nAlgorithm 1 Online RK-OC-ELM:Boundary Based Approach Input: Training set X= (x1, c1), (x2, c2), ..., (xN0 , cN0), ...,\n(xt, ct), ... Output: Whether Target or Outlier corresponding to each\nsample 1: Pass initial set of samples X0,R0 to the classifier as: {(x1, R1), (x2, R2), ..., (xN0 , RN0)} // For first chunk of N0 samples, following steps are required 2: Employ kernel feature mapping: \u03c60 = \u03c6(X0). 3: Output Weight \u03b20 for (X0,R0):\n\u03b20\u2190 P0R0 P0\u2190 \u03c6\u221210 b\u2190 N0\n// For second chunk onwards, following steps are required \u03c6\u2190 \u03c60 P \u2190 P0 4: for i = 1 to last chunk or block of data in X do 5: Size of chunk at the current stage = s 6: Calculate \u03c6v for ith block by Equation (6) 7: Calculate \u03c6u,v for ith block by Equation (7) 8: Update the final kernel matrix \u03c6 by using Equation (5) 9: Inverse of final kernel matrix \u03c6, i.e. calculate P by\nusing Equation (8) 10: Solve Equation (8) by Equation (9)-(12) 11: b = b+ s 12: end for 13: Output Weight \u03b2 = PR 14: Compute the predicted value by using output function\nf(Xk), which is defined in Equation (13) 15: Calculate distances(d) between predicted value of training\nsample and R as per Equation (14) 16: Sort the distances in decreasing order 17: Compute \u03b81 by Equation (15) 18: Use Equation (16) to decide whether a new sample z\nbelongs to target or not\npredicted output vector for the tth sample. c\u0302 = [c\u03021, c\u03022...c\u0302t, ...] is the predicted class vector, where, c\u0302t is the predicted class for the tth sample. Formulation of Online RK-OC-ELM for reconstruction framework based approach is as discussed next:\nFor initial chunk of N0 dataset {X0,X0} , objective is to minimize the output weight \u03b20 as well as error E0 between expected (X0) and predicted values (\u03c6(X0)\u03b20). Minimization problem can be written as follows:\nMinimize : L = 1\n2 \u2016\u03b20\u20162 + \u03bb\n1 2 \u2016E0\u20162\nSubject to : \u03c6(X0)\u03b20 = X0 \u2212E0, (17)\nAfter solving the above minimization problem using Karush-Kuhn-Tucker (KKT) theorem [33], it yields following solution:\n\u03b20 = P0X0\nP0 = \u03c6 \u22121 0\n\u03c60 = \u03c6(X0)\n(18)\nHere, \u03c60 is a kernel matrix for initial N0 of size N0\u00d7N0. Initially, \u03c6 and P will be equal to \u03c60 and P0, respectively. Here, \u03c6 represents kernel matrix and P represents inverse of this kernel matrix for all the arrived samples till now for training. Both of these will be updated continuously for any upcoming new samples Xv as per Equation (5)-(12). After processing the training dataset X , output function for any set of k samples Xk = {x1, x2....xk} can be written as follows:\nf(Xk) =   K(X,xk) . . .\nK(X,x1)\n  T PX (19)\nwhere, K(X,xi) denotes kernel vector for ith sample xi. Afterwards, calculate error and decide whether any sample belongs to target or outlier as per pseudo code discussed in Algorithm 2. Algorithm 2 introduces one extra threshold decision criterion (\u03b82) compared to boundary framework based approach. Since, \u03b82 is based on only reconstruction error of input data, therefore, it is not applicable for boundary framework based approach."}, {"heading": "C. Parameter Selection for ELM based One-Class Classifier", "text": "Parameter selection is always a crucial task in the case of one-class classification. Various model selection criteria have been proposed in literature for handling parameters of the kernel, however, most of them are developed especially for Gaussian kernel only [35] [36]. Among various methods, consistency based model selection [37] is only suitable for any\ntype of Kernels. Hence, consistency based model selection has been applied for both the frameworks based OCC. There are two parameters for kernel feature mapping with a Gaussian kernel i.e. regularization parameter (\u03bb) and kernel parameter (\u03c3). Consistency based model selection [37] also employs K-fold cross validation within it (see Equation (22) below). 5-fold cross validation is used with all one-class classifiers. This model selection works mainly based on 2-sigma bound of classification model. It determines a threshold error (Ethr) by using Equation (22), and executes the proposed classifier with different parameters of the classifier till it yields less error than Ethr.\nEthr = (M \u2217 \u03b7 + \u03c3thr \u2217 \u221a\n(\u03b7 \u2217 (1\u2212 \u03b7) \u2217M))/M = \u03b7 + \u03c3thr \u2217 \u221a (\u03b7 \u2217 (1\u2212 \u03b7)/M)\n(22)\nwhere, M is equal to (N/f), f is number of folds, \u03c3thr is threshold require for determining decision boundary during model selection, M denotes number of samples in the validation set, \u03b7 denotes the fraction of the samples rejected from the dataset which lies between 0 and 1, (M \u2217 \u03b7) is the expected number of rejected samples, \u03b7 \u2217 (1 \u2212 \u03b7) \u2217M is the variance and after square root of this is standard deviation and (M \u2217 \u03b7 + \u03c3thr \u2217 \u221a (\u03b7 \u2217 (1\u2212 \u03b7) \u2217M)) is the maximum allowed number of rejected target objects.\nThe above discussed model selection selects the optimal value of the parameter. If the classifier has more than one parameters then this will search for the optimal value of every parameter on all possible combinations of range of the value of the parameters. Since, proposed classifiers have two parameters \u03c3 and \u03bb and the value of these parameters are decided by this model selection criteria, which performs a linear search on the range of all possible combinations of \u03c3 and \u03bb. When the optimal consistent boundary is obtained then further search will be stopped. Here, among regularization parameter (\u03bb) and kernel parameter (\u03c3), \u03c3 is given a higher\nAlgorithm 2 Online RK-OC-ELM:Reconstruction Based Approach Input: Training set X= {(x1, c1), (x2, c2), ..., (xN0 , cN0), ..., (xt, ct), ...} Output: Whether Target or Outlier corresponding to each sample\n1: Pass initial set of samples X0 to the classifier as: {(x1, x1), (x2, x2), ..., (xN0 , xN0)} // For first chunk of N0 samples, following steps are required 2: Employ kernel feature mapping: \u03c60 = \u03c6(X0). 3: Output Weight \u03b20 for (X0,X0):\n\u03b20\u2190 P0X0 P0\u2190 \u03c6\u221210 b\u2190 N0\n// For second chunk onwards, following steps are required \u03c6\u2190 \u03c60 P \u2190 P0\n4: for i = 1 to last chunk or block of data in X do 5: Size of chunk at the current stage = s 6: Calculate \u03c6v for ith block by Equation (6) 7: Calculate \u03c6u,v for ith block by Equation (7) 8: Update the final kernel matrix \u03c6 by using Equation (5) 9: Inverse of final kernel matrix \u03c6, i.e. calculate P by using Equation (8)\n10: Solve Equation (8) by Equation (9)-(12) 11: b = b+ s 12: end for 13: Output Weight \u03b2 = PX 14: Compute the predicted value by using output function f(Xk), which is defined in Equation (19) 15: if Threshold deciding Criteria is \u03b81 then 16: Calculate sum of square error (d) between predicted value (x\u0302t) of training sample and actual value (xt) of tth sample\nas follows:\nd(xt) = n\u2211\nj=1\n(xjt \u2212 x\u0302jt )2 (20)\n17: Sort the distances in decreasing order 18: Compute \u03b81 by Equation (15) 19: Use Equation (16) to decide whether a new sample z belongs to target or not 20: else if Threshold deciding Criteria is \u03b82 then 21: Calculate relative error (d) between predicted (x\u0302jt ) and actual value (x j t ) for j th attribute of tth sample as follows:\nd(xjt ) = abs ( xjt \u2212 x\u0302jt xjt + x\u0302 j t ) \u2217 100 (21)\n22: For any tth sample with n attributes, Number of properly reconstructed attributes (prop reconst)=0 23: for j=1 to n do 24: if d(xjt ) \u2264 50% then 25: prop reconst\u2190 prop reconst+ 1 26: end if 27: if prop reconst \u2265 b0.9 \u2217 nc then 28: tth sample is target 29: else 30: tth sample is Outlier 31: end if 32: end for 33: end if\npriority compared to \u03bb. Suppose the consistent boundary is obtained at two different parameter combinations, (\u03c31, \u03bb1) and (\u03c32, \u03bb2) then smaller \u03c3 is preferred over larger \u03bb as changing in small amount of sigma impacts more compared to small amount of change in regularization parameter. Every possible combination of \u03c3 and \u03bb have been employed to obtain most complex classifier as long as classifier is consistent. Range of \u03bb taken is [10\u22128, 10\u22127, ..., 107, 108] and range of \u03c3 is taken as twenty values between minimum and maximum pairwise distance among training samples of the dataset. Further, just for showing the impact of appropriate parameter selection, two boundaries are exhibited in Fig. 3 on bananashaped dataset for incremetal SVDD. Dashed line shows the boundary without optimal parameter selection and complete line boundary shows the boundary using the optimal parameter obtained by consistensy based model selection."}, {"heading": "III. PERFORMANCE EVALUATION", "text": "In this section, first discuss the performance of the proposed classifiers on two artificial datasets and then discuss performance on six benchmark datasets. All experiments for this paper have been executed on MATLAB 2011a in Windows 7 (64 bit) environment with 4 GB RAM, 3.10 GHz processor and Intel i5 processor. For implementing the existing classifiers, two toolboxes is used, which is proposed by Tax [10] and Gautam [30]."}, {"heading": "A. Performance Comparison on Artificial Datasets", "text": "In this section, the impact of online regularized kernel on boundary creation for two artificial datasets is discussed. Artificial datasets are created with the help of PRToolBox [38] and each dataset is of size hundred. Boundary is created around\nartificial dataset with and without kernel feature mapping. Without kernel feature mapping indicates that random feature mapping is employed instead of kernel feature mapping. Just for sake of notation we have given name to the one-class classifier without kernel feature mapping as online regularized one-class ELM (Online R-OC-ELM). Online R-OC-ELM is discussed for the both frameworks in the supplementary material provided with this paper. Further, impact of threshold on boundary creation for artificial dataset is discussed. Classifier name with employed threshold criteria is denoted as classifier threshold in this paper. For e.g., online RKOC-ELM \u03b81 and online RK-OC-ELM \u03b82 denote that online RK-OC-ELM employed \u03b81 and \u03b82 respectively as threshold deciding criteria. Same naming convention for classifiers\u2019 name have been used during whole discussion now onwards in Section III and Section IV. Hence, notation of existing ELM based classifier is also changed from their paper based on which threshold criteria is employed as follows:\n(a) ELM based offline one-class classifiers with kernel feature mapping: OCKELM \u03b81 is boundary framework based approach. AAKELM \u03b81 and AAKELM \u03b82 are reconstruction framework based approach (b) ELM based online one-class classifiers with random feature mapping: OS-OCELM \u03b81 is is boundary framework based approach. OS-AAELM \u03b81 and OS-AAELM \u03b82 are reconstruction framework based approach.\n1) Impact of Threshold on Boundary Creation for Artificial Datasets: Threshold deciding criterion is a very crucial factor in one-class classification. We employ one threshold deciding criterion \u03b81 with boundary framework based one-class classifier and two threshold deciding criteria viz., \u03b81 and \u03b82, with reconstruction framework based one-class classifier. Any one threshold deciding criterion is employed after the output obtained at the last layer of the one-class classifiers. The ability of the proposed methods on boundary creation has been tested on two artificial datasets viz., Banana-shaped and Ring-shaped datasets. These are used to test the ability of the proposed one-class classifiers for convex and internal boundary creation. Convexity of the proposed methods is evaluated based on the boundary creation ability around Banana shaped dataset. Fig. 4 exhibits the behavior of the proposed one-class classifiers on creation of convex decision boundary around banana-shaped dataset. One more thing can be noticed in Fig. 4e and 4f that they don\u2019t reject any sample during boundary creation due to threshold deciding criteria (\u03b82). However, classifiers with threshold \u03b81 reject 10% samples(can be seen from Fig. 4a4d).\nSecond artificial dataset is Ring-shaped dataset. Proposed classifiers are tested on this dataset to test the ability of classifiers to create an internal boundary of the ring because internal boundary differentiates it from the only circular boundary. If parameters are not chosen appropriately, then classifier may create only outer circular boundary of the ring as it covers all samples within it but it also covers extra void space within it. It is experienced that the performance of all methods over Ring-shaped dataset is similar to Banana dataset except online R-OC-ELM \u03b82 (same can be verified\nin Fig. 5). Online R-OC-ELM \u03b82 classifier is not able to create boundary around ring dataset as you can see in Fig. (5e). However, in contrast, when RK-OC-ELM is employed with \u03b82 (online RK-OC-ELM \u03b82) then performance was just opposite, it creates boundary perfectly without rejecting any amount of sample outside of its boundary. This classifier is tested online RK-OC-ELM with various other artificial datasets to check its boundary creation ability and found the same performance. By above discussion, it is clear that \u03b82 is more suitable for reconstruction framework based approach.\n2) Impact of Online Regularized Kernel on Boundary Creation for Artificial Datasets: In Fig. 4 and 5, with and without online kernel feature mapping based methods (i.e. online RK-OC-ELM and online R-OC-ELM, respectively) are kept side be side for better visualization. As you can see in Fig. 4, just adding regularization factor without kernel feature mapping (i.e. online R-OC-ELM) does not help much in boundary creation as they are not able to create smooth boundary for Banana dataset as well as they cover some extra void space. It means that data is not described properly by oneclass classifiers. However, when kernelized feature mapping is embedded instead of random feature mapping and tested the proposed classifier online RK-OC-ELM on Banana dataset then performance is significantly improved. online RK-OCELM based one-class classifiers create proper and smooth boundaries around Banana dataset. Overall two points are note here: (i) The proposed classifiers achieve similar boundary cre-\nation ability as offline kernel feature mapping based classifiers presented by Gautam et al. [30] (ii) Performance of kernel feature mapping is superior over random feature mapping, therefore, it was necessary to implement it with online ELM based one-class classifiers."}, {"heading": "B. Performance Comparison on Benchmark Datasets", "text": "The performance of the proposed methods have been tested on six benchmark datasets as mentioned in Table I. Benchmark datasets are available at UCI Machine Learning Repository [39] in original format and available in one-class format on the website of the TU Delft [40]. The datasets are prepared for OCC in the same way as prepared by Leng et al. [28] and Gautam et al. [30]. Dataset is divided according to their classes and assume normal samples as target class and remaining of the classes as outlier class. And same procedure is followed for each dataset. Further, randomly select 50% of target data for training and the remaining 50% is combined with the outlier class for testing. It is to be noted that optimal parameter is calculated in the first run only. Hence, it speeds up the execution and saves a lot of time. Data is normalized between 0 and 1 using max-min normalization. Classifiers are experimented over twenty runs and then calculate average of accuracy (ACC), area under curve (AUC = 1/2(Sensitivity + Specificity)) and F-measure (F1) over all runs as final output. All three performance measures are presented in the result tables (Table II-VII) but, AUC is considered as a performance\nmeasure criterion for further discussion as AUC provides better insight of classifiers\u2019 performance compared to ACC and F1. Standard deviation of F1, ACC and AUC over twenty runs is also presented to get a proper understanding of possible deviation in the performance over twenty runs.\nThe proposed and existing ELM based online classifiers have four parameters if use RBF kernel viz., the number of initial training data used in the initial phase of the online sequential one-class classifier, size of blocks of data learned by online sequential one-class classifier in each iteration, regularization parameter (\u03bb) and kernel parameter (\u03c3). Out of four parameters, values of two parameters are fixed and values for the rest of the two parameters (\u03bb and \u03c3) have been decided by consistency based model selection. The value of these two fixed parameters are taken as 10 and 5 for number of initial training data used in the initial phase and size of blocks of data learned by the classifier in each iteration, respectively.\nIn this section, first discuss performance comparison of boundary framework based one-class classifiers and then provide discussion on reconstruction framework based one-class classifiers.\n1) Performance Comparison of Boundary Framework Based Methods on Benchmark Datasets: Threshold cri-\nterion \u03b81 has been employed in all existing and proposed approaches presented in the the Table II, IV and VI. We compare the proposed classifiers with one of the most popular traditional methods, incremental SVDD and recently proposed ELM based online and offline one-class classifiers. Proposed classifier online RK-OC-ELM \u03b81 is the online version of the existing OCKELM \u03b81 [28] classifier. It is expected that online RK-OC-ELM \u03b81 will yield similar results as OCKELM \u03b81. As you can see from the table, the proposed classifier online RK-OC-ELM \u03b81 outperforms all existing online as well as offline classifier for all six datasets in term of AUC. It is observed that proposed kernel feature mapping based method online RK-OC-ELM \u03b81 performed better than existing random feature mapping based online method viz., OS-OCELM \u03b81 [30]. Superiority of kernel over random feature mapping was expected because Leng et al. [28] and [30] had also exhibited the same trend in their work for offline classifiers.\n2) Performance Comparison of Reconstruction Framework Based Methods on Benchmark Datasets: Proposed classifier online RK-OC-ELM has been employed with two threshold criteria viz., \u03b81 and \u03b82 (i.e. online RK-OCELM \u03b81 and online RK-OC-ELM \u03b82). These are the online version of the existing offline classifiers AAKELM \u03b81 and\nAAKELM \u03b82, respectively. Therefore, it is expected that both online and their corresponding offline versions will exhibit similar performance and these facts can be verified by the results of the tables (Table III, V and VII). It can also be easily inferred from these tables that \u03b82 performs better than \u03b81 since \u03b82 does not decide threshold by rejecting any amount of samples. Except for Diabetes and Liver dataset, the proposed classifiers outperform all existing online oneclass classifiers presented in the table significantly in term of AUC. Even, there is no significant difference in the value of AUC between OS-AAELM \u03b83 and online RK-OC-ELM \u03b82. Therefore, it can be stated that kernel feature mapping based classifiers have again exhibited the superiority over random feature mapping based classifiers. It is observed that \u03b82 based method online RK-OC-ELM \u03b82 has been performed better than \u03b81 based method online RK-OC-ELM \u03b81 for most of the datasets, hence, online RK-OC-ELM \u03b82 should be preferred over online RK-OC-ELM \u03b81 for reconstruction framework based approaches. But, threshold criteria \u03b82 exhibits strange behaviour for random feature mapping based classifier as it can be seen in Table VII. Since, the proposed works are based on kernel feature mapping, hence we didn\u2019t encounter that issue but, it is not suggested to use \u03b82 with random feature mapping based classifier."}, {"heading": "C. Discussion", "text": "In the previous section, the performance of the proposed classifiers has been discussed for boundary and reconstruction frameworks based approaches separately. This section will provide the general discussion on both frameworks together. Table II-VII present all results obtained on six benchmark datasets. It can be seen in the tables that proposed online oneclass classifiers achieve simiular performance as ELM based offline kernelized one-class classifiers (i.e. OCKELM \u03b81, AAKELM \u03b81, AAKELM \u03b82). However, it is not necessary that proposed online classifiers will achieve exactly the same output as offline classifier. It is due to two reasons: First, due to selection of training and testing data randomly in each run for any classifier. Second, method of inverse calculation is different in online and offline classifiers.\nThe proposed classifiers have been tested with various benchmark datasets and it is found that reconstruction based one-class classifiers perform better than boundary framework\nbased methods for 4 out of 6 datasets, only outperformed for Breast Cancer and Ecoli datasets. Here also, there is no significant difference between the performance of reconstruction framework based and boundary framework based classifiers for the Breast Cancer dataset. Two types of threshold viz., \u03b81 and \u03b82 have been employed with reconstruction framework based approach but only one type of threshold \u03b81 for boundary framework based approach. \u03b82 is not employed for boundary framework based classifier as it works based on reconstruction of data at output layer."}, {"heading": "IV. CONCLUSION", "text": "This paper has presented online learning with regularized kernel for one-class classification. Here, regularization parameter with kernel helps the classifier to achieve good generalization capability. Three methods have been discussed hence far in the paper, which are based on boundary and reconstruction frameworks. online RK-OC-ELM \u03b81 has been developed for boundary and reconstruction frameworks both, however, online RK-OC-ELM \u03b82 has been developed only for reconstruction framework based as \u03b82 cannot be employed for boundary framework based classifier.\nOur proposed classifier can handle data in an online manner and performance evaluation of these classifiers have exhibited that these online classifiers are equally capable as offline classifiers. As the proposed classifiers are online, hence, there is no need to train the model from scratch for any new arrival of training data. Simply, pass newly arrived data with the proposed classifiers and this will take care of the rest. However, proposed classifiers have been tested on only small size of benchmark datasets but, since these are an online classifier, hence, they are capable of handling large size dataset also. These online classifiers either outperformed existing online one-class classifier for most of the dataset or yielded similar results for some of the datasets. Overall, reconstruction framework based one-class classifiers have performed better compared to boundary framework based one-class classifiers.\nConsistency based model selection has been employed as a model selection criteria for all the existing and proposed oneclass classifiers. This model selection is efficient in the case where only two free parameters are there in the classifiers. However, it would be very time consuming if there would be more than two parameters as it searches the appropriate\nparameter by linear search on all possible combination of the parameters. Any evolutionary optimization technique can be employed to overcome this issue."}, {"heading": "ACKNOWLEDGMENT", "text": "This research was supported by Department of Electronics and Information Technology (DeITY, Govt. of India) under Visvesvaraya PHD scheme for electronics & IT."}], "references": [{"title": "One-class classifier networks for target recognition applications", "author": ["M.M. Moya", "M.W. Koch", "L.D. Hostetler"], "venue": "Technical report, Sandia National Labs., Albuquerque, NM (United States)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1993}, {"title": "A review of novelty detection", "author": ["M.A. Pimentel", "D.A. Clifton", "L. Clifton", "L. Tarassenko"], "venue": "Signal Processing, 99:215\u2013249", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Design and analysis of multimodel-based anomaly intrusion detection systems in industrial process automation", "author": ["C. Zhou", "S. Huang", "N. Xiong", "S.-H. Yang", "H. Li", "Y. Qin", "X. Li"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems, 45(10):1345\u20131360", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Gramophone noise detection and reconstruction using time delay artificial neural networks", "author": ["C.F. Stallmann", "A.P. Engelbrecht"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems, PP(99):1\u201313", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Sliding window-based fault detection from high-dimensional data streams", "author": ["L. Zhang", "J. Lin", "R. Karim"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems, PP(99):1\u201315", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Animal-vehicle collision mitigation system for automated vehicles", "author": ["A. Mammeri", "D. Zhou", "A. Boukerche"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems, 46(9):1287\u20131299", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Support vector method for novelty detection", "author": ["B. Sch\u00f6lkopf", "R.C. Williamson", "A.J. Smola", "J. Shawe-Taylor", "J.C. Platt"], "venue": "NIPS, volume 12, pages 582\u2013588", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Support vector domain description", "author": ["D.M.J. Tax", "R.P.W. Duin"], "venue": "Pattern recognition letters, 20(11):1191\u20131199", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "Online SVM learning: from classification to data description and back", "author": ["D.M.J. Tax", "P. Laskov"], "venue": "IEEE 13th Workshop on Neural Networks for Signal Processing, 2003 (NNSP\u201903), pages 499\u2013508. IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "DDtools, the data description toolbox for MATLAB, version 2.1.2", "author": ["D.M.J. Tax"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A survey of recent trends in one class classification", "author": ["S.S. Khan", "M.G. Madden"], "venue": "Irish conference on Artificial Intelligence and Cognitive Science, pages 188\u2013197. Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Concept-learning in the absence of counter-examples: An autoassociation-based approach to classification", "author": ["N. Japkowicz"], "venue": "PhD thesis, Rutgers, The State University of New Jersey", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "One-class document classification via neural networks", "author": ["L. Manevitz", "M. Yousef"], "venue": "Neurocomputing, 70(7):1466\u20131481", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Extreme learning machine: theory and applications", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": "Neurocomputing, 70(1):489\u2013501", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Extreme learning machine for regression and multiclass classification", "author": ["G.-B. Huang", "H. Zhou", "X. Ding", "R. Zhang"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 42(2):513\u2013529", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast and accurate online sequential learning algorithm for feedforward networks", "author": ["N.-Y. Liang", "G.-B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "IEEE Transactions on Neural Networks, 17(6):1411\u20131423", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Online sequential extreme learning machine with kernels", "author": ["S. Scardapane", "D. Comminiello", "M. Scarpiniti", "A. Uncini"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, 26(9):2214\u20132220", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Online sequential extreme learning machine with kernels for nonstationary time series prediction", "author": ["X. Wang", "M. Han"], "venue": "Neurocomputing, 145:90\u201397", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Online regularized and kernelized extreme learning machines with forgetting mechanism", "author": ["X. Zhou", "Z. Liu", "C. Zhu"], "venue": "Mathematical Problems in Engineering, 2014:1\u201311", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse extreme learning machine for classification", "author": ["Z. Bai", "G.-B. Huang", "D. Wang", "H. Wang", "M.B. Westover"], "venue": "IEEE Transactions on Cybernetics, 44(10):1858\u20131870", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Stacked extreme learning machines", "author": ["H. Zhou", "G.-B. Huang", "Z. Lin", "H. Wang", "Y.C. Soh"], "venue": "IEEE Transactions on Cybernetics, 45(9):2013\u2013 2025", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Graph embedded extreme learning machine", "author": ["A. Iosifidis", "A. Tefas", "I. Pitas"], "venue": "IEEE Transactions on Cybernetics, 46(1):311\u2013324", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "A meta-cognitive learning algorithm for an extreme learning machine classifier", "author": ["R. Savitha", "S. Suresh", "H.J. Kim"], "venue": "Cognitive Computation, 6(2):253\u2013263", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Domain adaptation extreme learning machines for drift compensation in e-nose systems", "author": ["L. Zhang", "D. Zhang"], "venue": "IEEE Transactions on Instrumentation and Measurement, 64(7):1790\u20131801", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Performance enhancement of extreme learning machine for multi-category sparse data classification problems", "author": ["S. Suresh", "S. Saraswathi", "N. Sundararajan"], "venue": "Engineering Applications of Artificial Intelligence, 23(7):1149\u20131157", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "No-reference image quality assessment using modified extreme learning machine classifier", "author": ["S. Suresh", "R. Venkatesh Babu", "H.J. Kim"], "venue": "Applied Soft Computing, 9(2):541\u2013552", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Trends in extreme learning machines: a review", "author": ["G. Huang", "G.-B. Huang", "S. Song", "K. You"], "venue": "Neural Networks, 61:32\u201348", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "One-class classification with extreme learning machine", "author": ["Q. Leng", "H. Qi", "J. Miao", "W. Zhu", "G. Su"], "venue": "Mathematical Problems in Engineering, pages 1\u201311", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "One-class classification based on extreme learning and geometric class information", "author": ["A. Iosifidis", "V. Mygdalis", "A. Tefas", "I. Pitas"], "venue": "Neural Processing Letters, pages 1\u201316", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "On the construction of extreme learning machine for online and offline one class classifier - An expanded toolbox", "author": ["C. Gautam", "A. Tiwari", "Q. Leng"], "venue": "Neurocomputing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "On the construction of extreme learning machine for one class classifier", "author": ["C. Gautam", "A. Tiwari"], "venue": "Proceedings of ELM-2015 Volume 1, pages 447\u2013461. Springer", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Matrix computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "volume 3. JHU Press", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast and efficient strategies for model selection of Gaussian support vector machine", "author": ["Z. Xu", "M. Dai", "D. Meng"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 39(5):1292\u20131307", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Parameter selection of Gaussian kernel for one-class SVM", "author": ["Y. Xiao", "H. Wang", "W. Xu"], "venue": "IEEE Transactions on Cybernetics, 45(5):941\u2013953", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "A consistency-based model selection for one-class classification", "author": ["D.M.J. Tax", "K.-R. M\u00fcller"], "venue": "Proceedings of the 17th International Conference on Pattern Recognition ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2004}, {"title": "PRTools", "author": ["R.P.W. Duin", "P. Juszczak", "D. Ridder", "P. Paclik", "E. Pekalska", "D.M.J. Tax"], "venue": "a MATLAB toolbox for pattern recognition", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "The OCC has been generally employed to solve the problem of novelty, outlier, intrusion or fault detection [2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "These problems are also solved by multi-class classification when samples of both classes, normal and outlier class, are available [3] [4] [5] [6].", "startOffset": 131, "endOffset": 134}, {"referenceID": 3, "context": "These problems are also solved by multi-class classification when samples of both classes, normal and outlier class, are available [3] [4] [5] [6].", "startOffset": 135, "endOffset": 138}, {"referenceID": 4, "context": "These problems are also solved by multi-class classification when samples of both classes, normal and outlier class, are available [3] [4] [5] [6].", "startOffset": 139, "endOffset": 142}, {"referenceID": 5, "context": "These problems are also solved by multi-class classification when samples of both classes, normal and outlier class, are available [3] [4] [5] [6].", "startOffset": 143, "endOffset": 146}, {"referenceID": 6, "context": "Two types of SVM based one-class classifier have been proposed, namely, One-Class SVM (OCSVM) [7] and Support Vector Domain Description (SVDD) [8].", "startOffset": 94, "endOffset": 97}, {"referenceID": 7, "context": "Two types of SVM based one-class classifier have been proposed, namely, One-Class SVM (OCSVM) [7] and Support Vector Domain Description (SVDD) [8].", "startOffset": 143, "endOffset": 146}, {"referenceID": 1, "context": "After surveying various papers, it has been observed that SVM based one-class classifier has been explored more compared to other one class methods [2].", "startOffset": 148, "endOffset": 151}, {"referenceID": 8, "context": "SVM based one-class classifier has also been developed for incremental and online learning [9].", "startOffset": 91, "endOffset": 94}, {"referenceID": 9, "context": ", a toolbox is available in [10].", "startOffset": 28, "endOffset": 32}, {"referenceID": 10, "context": "Based on different nature of data used during learning hyperplane of OCC, it can be classified as [11] (i) with positive examples only, (ii) with the positive and small amount of negative examples only, and (iii) with positive and unlabeled data.", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "Neural network with backpropagation has been explored in past for OCC task [12] [13].", "startOffset": 75, "endOffset": 79}, {"referenceID": 12, "context": "Neural network with backpropagation has been explored in past for OCC task [12] [13].", "startOffset": 80, "endOffset": 84}, {"referenceID": 13, "context": "[14] [15] also developed a single layer feed-forward network, called as Extreme Learning Machine (ELM).", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[14] [15] also developed a single layer feed-forward network, called as Extreme Learning Machine (ELM).", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "Online version of ELM has also been developed for multi-class classification and regression tasks with random [16] and kernel feature mapping [17] [18] [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "Online version of ELM has also been developed for multi-class classification and regression tasks with random [16] and kernel feature mapping [17] [18] [19].", "startOffset": 142, "endOffset": 146}, {"referenceID": 17, "context": "Online version of ELM has also been developed for multi-class classification and regression tasks with random [16] and kernel feature mapping [17] [18] [19].", "startOffset": 147, "endOffset": 151}, {"referenceID": 18, "context": "Online version of ELM has also been developed for multi-class classification and regression tasks with random [16] and kernel feature mapping [17] [18] [19].", "startOffset": 152, "endOffset": 156}, {"referenceID": 19, "context": "In past decades, ELM has been well expanded in both dimensions: theory [20] [21] [22] [23] and application [24] [18] [25] [26].", "startOffset": 71, "endOffset": 75}, {"referenceID": 20, "context": "In past decades, ELM has been well expanded in both dimensions: theory [20] [21] [22] [23] and application [24] [18] [25] [26].", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": "In past decades, ELM has been well expanded in both dimensions: theory [20] [21] [22] [23] and application [24] [18] [25] [26].", "startOffset": 81, "endOffset": 85}, {"referenceID": 22, "context": "In past decades, ELM has been well expanded in both dimensions: theory [20] [21] [22] [23] and application [24] [18] [25] [26].", "startOffset": 86, "endOffset": 90}, {"referenceID": 23, "context": "In past decades, ELM has been well expanded in both dimensions: theory [20] [21] [22] [23] and application [24] [18] [25] [26].", "startOffset": 107, "endOffset": 111}, {"referenceID": 17, "context": "In past decades, ELM has been well expanded in both dimensions: theory [20] [21] [22] [23] and application [24] [18] [25] [26].", "startOffset": 112, "endOffset": 116}, {"referenceID": 24, "context": "In past decades, ELM has been well expanded in both dimensions: theory [20] [21] [22] [23] and application [24] [18] [25] [26].", "startOffset": 117, "endOffset": 121}, {"referenceID": 25, "context": "In past decades, ELM has been well expanded in both dimensions: theory [20] [21] [22] [23] and application [24] [18] [25] [26].", "startOffset": 122, "endOffset": 126}, {"referenceID": 26, "context": "A detailed survey on ELM and its application can be found in [27].", "startOffset": 61, "endOffset": 65}, {"referenceID": 27, "context": "[28] developed ELM for OCC due to its fast learning speed, which supports only offline learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] tested their model with only one type of threshold deciding criteria, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Later, Iosifidis [29] improvised ELM based one-class classifier based on geometric information of class for offline learning.", "startOffset": 17, "endOffset": 21}, {"referenceID": 29, "context": "[30]1 [31] explored further ELM based one-class classifier for mainly offline learning and tested it with three different threshold criteria.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[30]1 [31] explored further ELM based one-class classifier for mainly offline learning and tested it with three different threshold criteria.", "startOffset": 6, "endOffset": 10}, {"referenceID": 29, "context": "[30] have shown particularly two aspects: (i) random feature mapping based offline methods have been outperformed by kernel feature mapping based offline methods (ii) the possibility of", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "Further, compute the inverse in Equation (8) using block matrix inverse formula [34].", "startOffset": 80, "endOffset": 84}, {"referenceID": 31, "context": "P11 can be expanded by employing Woodbury formula [34] as: P11 = (\u03c6u \u2212 \u03c6u,v\u03c6\u22121 v \u03c6u,v) = \u03c6\u22121 u \u2212 \u03c6\u22121 u \u03c6u,v(\u03c6u,v\u03c6 u \u03c6u,v + \u03c6\u22121 v ) \u03c6u,v\u03c6 \u22121 u (12)", "startOffset": 50, "endOffset": 54}, {"referenceID": 32, "context": "Various model selection criteria have been proposed in literature for handling parameters of the kernel, however, most of them are developed especially for Gaussian kernel only [35] [36].", "startOffset": 177, "endOffset": 181}, {"referenceID": 33, "context": "Various model selection criteria have been proposed in literature for handling parameters of the kernel, however, most of them are developed especially for Gaussian kernel only [35] [36].", "startOffset": 182, "endOffset": 186}, {"referenceID": 34, "context": "Among various methods, consistency based model selection [37] is only suitable for any type of Kernels.", "startOffset": 57, "endOffset": 61}, {"referenceID": 34, "context": "Consistency based model selection [37] also employs K-fold cross validation within it (see Equation (22) below).", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "For implementing the existing classifiers, two toolboxes is used, which is proposed by Tax [10] and Gautam [30].", "startOffset": 91, "endOffset": 95}, {"referenceID": 29, "context": "For implementing the existing classifiers, two toolboxes is used, which is proposed by Tax [10] and Gautam [30].", "startOffset": 107, "endOffset": 111}, {"referenceID": 35, "context": "Artificial datasets are created with the help of PRToolBox [38] and each dataset is of size hundred.", "startOffset": 59, "endOffset": 63}, {"referenceID": 29, "context": "[30] (ii) Performance of kernel feature mapping is superior over random feature mapping, therefore, it was necessary to implement it with online ELM based one-class classifiers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] and Gautam et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "incsvdd [6] 94.", "startOffset": 8, "endOffset": 11}, {"referenceID": 23, "context": "OCKELM \u03b81 [24] 93.", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "13) OS-OCELM \u03b81 [26] 86.", "startOffset": 16, "endOffset": 20}, {"referenceID": 5, "context": "autoenc dd [6] 92.", "startOffset": 11, "endOffset": 14}, {"referenceID": 25, "context": "AAKELM \u03b81 [26] 93.", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "5) AAKELM \u03b82 [26] 94.", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "5) OS-AAELM \u03b81 [26] 90.", "startOffset": 15, "endOffset": 19}, {"referenceID": 25, "context": "37) OS-AAELM \u03b82 [26] 89.", "startOffset": 16, "endOffset": 20}, {"referenceID": 5, "context": "incsvdd [6] 66.", "startOffset": 8, "endOffset": 11}, {"referenceID": 23, "context": "OCKELM \u03b81 [24] 68.", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "03) OS-OCELM \u03b81 [26] 61.", "startOffset": 16, "endOffset": 20}, {"referenceID": 5, "context": "autoenc dd [6] 66.", "startOffset": 11, "endOffset": 14}, {"referenceID": 25, "context": "AAKELM \u03b81 [26] 67.", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "AAKELM \u03b82 [26] 59.", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "62) OS-AAELM \u03b81 [26] 63.", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "32) OS-AAELM \u03b82 [26] 60.", "startOffset": 16, "endOffset": 20}, {"referenceID": 5, "context": "incsvdd [6] 57.", "startOffset": 8, "endOffset": 11}, {"referenceID": 23, "context": "OCKELM \u03b81 [24] 56.", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "OS-OCELM \u03b81 [26] 54.", "startOffset": 12, "endOffset": 16}, {"referenceID": 27, "context": "Proposed classifier online RK-OC-ELM \u03b81 is the online version of the existing OCKELM \u03b81 [28] classifier.", "startOffset": 88, "endOffset": 92}, {"referenceID": 29, "context": ", OS-OCELM \u03b81 [30].", "startOffset": 14, "endOffset": 18}, {"referenceID": 27, "context": "[28] and [30] had also exhibited the same trend in their work for offline classifiers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[28] and [30] had also exhibited the same trend in their work for offline classifiers.", "startOffset": 9, "endOffset": 13}, {"referenceID": 5, "context": "autoenc dd [6] 48.", "startOffset": 11, "endOffset": 14}, {"referenceID": 25, "context": "AAKELM \u03b81 [26] 49.", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "21) AAKELM \u03b82 [26] 62.", "startOffset": 14, "endOffset": 18}, {"referenceID": 25, "context": "OS-AAELM \u03b81 [26] 56.", "startOffset": 12, "endOffset": 16}, {"referenceID": 25, "context": "OS-AAELM \u03b82 [26] 39.", "startOffset": 12, "endOffset": 16}], "year": 2017, "abstractText": "This paper presents an online learning with regularized kernel based one-class extreme learning machine (ELM) classifier and is referred as \u201conline RK-OC-ELM\u201d. The baseline kernel hyperplane model considers whole data in a single chunk with regularized ELM approach for offline learning in case of one-class classification (OCC). Further, the basic hyper plane model is adapted in an online fashion from stream of training samples in this paper. Two frameworks viz., boundary and reconstruction are presented to detect the target class in online RKOC-ELM. Boundary framework based one-class classifier consists of single node output architecture and classifier endeavors to approximate all data to any real number. However, one-class classifier based on reconstruction framework is an autoencoder architecture, where output nodes are identical to input nodes and classifier endeavor to reconstruct input layer at the output layer. Both these frameworks employ regularized kernel ELM based online learning and consistency based model selection has been employed to select learning algorithm parameters. The performance of online RK-OC-ELM has been evaluated on standard benchmark datasets as well as on artificial datasets and the results are compared with existing state-of-the art oneclass classifiers. The results indicate that the online learning oneclass classifier is slightly better or same as batch learning based approaches. As, base classifier used for the proposed classifiers are based on the ELM, hence, proposed classifiers would also inherit the benefit of the base classifier i.e. it will perform faster computation compared to traditional autoencoder based one-class classifier.", "creator": "LaTeX with hyperref package"}}}