{"id": "1701.04600", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jan-2017", "title": "Faster K-Means Cluster Estimation", "abstract": "There has been considerable work on improving popular clustering algorithm `K-means' in terms of mean squared error (MSE) and speed, both. However, most of the k-means variants tend to compute distance of each data point to each cluster centroid for every iteration. We propose a fast heuristic to overcome this bottleneck with only marginal increase in MSE. We observe that across all iterations of K-means, a data point changes its membership only among a small subset of clusters. Our heuristic predicts such clusters for each data point by looking at nearby clusters after the first iteration of k-means. We augment well known variants of k-means with our heuristic to demonstrate effectiveness of our heuristic. For various synthetic and real-world datasets, our heuristic achieves speed-up of up-to 3 times when compared to efficient variants of k-means.\n\n\n\nThis paper was presented as an important contribution to the understanding of K-means. This paper was presented as an important contribution to the understanding of K-means. This paper was presented as an important contribution to the understanding of K-means. This paper was presented as an important contribution to the understanding of K-means. This paper was presented as an important contribution to the understanding of K-means. This paper was presented as an important contribution to the understanding of K-means. This paper was presented as an important contribution to the understanding of K-means. This paper was presented as an important contribution to the understanding of K-means. In other words, this paper has now been extensively used by other experts in the field.\nThe next paper describes a novel approach to this topic which aims to make the analysis of K-means more effective. The most advanced and important use of the technique is to make the analysis of K-means more effective. This is a first step in the analysis of K-means. This approach to this topic has now been extensively used by other experts in the field. The most advanced and important use of the technique is to make the analysis of K-means more effective. This is a first step in the analysis of K-means. This approach to this topic has now been extensively used by other experts in the field. The best use of the technique is to make the analysis of K-means more effective. This is a first step in the analysis of K-means. This approach to this topic has now", "histories": [["v1", "Tue, 17 Jan 2017 10:00:51 GMT  (20kb)", "http://arxiv.org/abs/1701.04600v1", "6 pages, Accepted at ECIR 2017"]], "COMMENTS": "6 pages, Accepted at ECIR 2017", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["siddhesh khandelwal", "amit awekar"], "accepted": false, "id": "1701.04600"}, "pdf": {"name": "1701.04600.pdf", "metadata": {"source": "CRF", "title": "Faster K-Means Cluster Estimation", "authors": ["Siddhesh Khandelwal", "Amit Awekar"], "emails": ["siddhesh166@gmail.com,", "awekar@iitg.ernet.in"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 1.\n04 60\n0v 1\n[ cs\n.L G\n] 1\n7 Ja\nn 20\n17\nKeywords: K-means, Clustering, Heuristic"}, {"heading": "1 Introduction", "text": "K-means is a popular clustering technique that is used in diverse fields such as humanities, bio-informatics, and astronomy. Given a dataset D with n data points in Rd space, K-means partitions D into k clusters with the objective to minimize the mean squared error (MSE). MSE is defined as the sum of the squared distance of each point from its corresponding centroid. The K-means problem is NP-hard. Polynomial time heuristics are commonly applied to obtain a local minimum.\nOne such popular heuristic is the Lloyd\u2019s algorithm[6] that selects certain initial centroids (also referred as seeds) at random from the dataset. Each data point is assigned to the cluster corresponding to the closest centroid. Each centroid is then recomputed as mean of the points assigned to that cluster. This procedure is repeated until convergence. Each iteration involves n \u2217 k distance computations. Our contribution is to reduce this cost to n\u2217k\u2032, (k\u2032 << k) by generating candidate cluster list (CCL) of size k\u2032 for each data point. The heuristic is based on the observation that across all iterations of K-means, a data point changes its membership only among a small subset of clusters. Our heuristic considers only a subset of nearby cluster as candidates for deciding membership for a data point. This heuristic has advantage of speeding up K-means clustering with marginal increase in MSE. We show effectiveness of our heuristic by extensive experimentation using various synthetic and real-world datasets."}, {"heading": "2 Our Work: Candidate cluster list for each data point", "text": "Our main contribution is in defining a heuristic that can be used as augmentation to current variants of k-means for faster cluster estimation. Let algorithm V be a variant of k-means and algorithm V \u2032 be the same variant augmented with our heuristic. Let T be the time required for V to converge to MSE value of E. Similarly, T \u2032 is the time required for V \u2032 to converge to MSE value of E\u2032. We should satisfy following two conditions when we compare V with V \u2032:\n\u2013 Condition 1: T \u2032 is lower than T , and\n\u2013 Condition 2: E\u2032 is either lower or only marginally higher than E.\nIn short, these conditions state that a K-means variant augmented with our heuristic should converge faster without significant increase in final MSE.\nMajor bottleneck of K-means clustering is the computation of data point to cluster centroid distance in each iteration of K-means. For a dataset with n data points and k clusters, each iteration of K-means performs n \u2217 k such distance computations. To overcome this bottleneck, we maintain a CCL of size k\u2032 for each data point. We assume that k\u2032 is significantly smaller than k. We discuss the effect of various choices for the size of CCL in Section 4. We build CCL based on top k\u2032 nearest clusters to the data point after first iteration of K-means. Now each iteration of K-means will perform only n \u2217 k\u2032 distance computations.\nConsider a data point p1 and cluster centroids represented as c1, c2..., ck. Initially all centroids are chosen randomly or using one of the seed selection algorithms mentioned in Section 3. Let us assume that k\u2032= 4, and k\u2032 << k. After first iteration of K-means c8, c5, c6, and c1 are the top four closest centroids to p1 in the increasing order of distance. This is the candidate cluster list for p1. If we run K-means for second iteration, p1 will compute distance to all k centroids. After second iteration, top four closest centroid list might change in two ways:\n1. Members of the list do not change but only ranking changes among the members. For example, top four closest centroid list for p1 might change to c1, c6, c8, and c5 in the increasing order of distance. 2. Some of the centroids in the previous list are replaced with other centroids which were not in the list. For example, top four closest list for p1 might change to c5, c2, c9, and c8 in the increasing order of distance\nFor many synthetic and real world datasets we observe that the later case rarely happens. That is, the set of top few closest centroids for a data point remains almost unchanged even though order among them might change. Therefore, CCL is a good enough estimate for the closest cluster when K-means converges [1]. For each data point, our heuristic involves computation overhead of O(k.log(k)) for creating CCL and memory overhead of O(k\u2032) to maintain CCL. For a sample dataset consisting 100,000 points in 54 dimensions and the value of k = 100 and k\u2032 = 40, this overhead is approximately 30MB."}, {"heading": "3 Related Work", "text": "In last three decades, there has been significant work on improving Lloyd\u2019s algorithm [6] both in terms of reducing MSE and running time. The follow up work on Lloyd\u2019s algorithm can be broadly divided into three categories: Better seed selection[2,5], Selecting ideal value for number of clusters[8], and Bounds on data point to cluster centroid distance[3,7,4]. Arthur et. al.[2] provided a better method for seed selection based on a probability distribution over closest cluster centroid distances for each data point. Likas et. al.[5] proposed the Global k-means method for selecting one seed at a time to reduce final mean squared error. Pham et. al.[8] designed a novel function to evaluate goodness of clustering for various potential values of number of clusters. Elkan[3] use triangle inequality to avoid redundant computations of distance between data points and cluster centroids. Pelleg and Moore[7] and Kanungo et al.[4] proposed similar algorithms that use k-d trees. Both these algorithms construct a k-d tree over the dataset to be clustered. Though these approaches have shown good results, k-d trees perform poorly for datasets in higher dimensions.\nSeed selection based K-means variants differ from Lloyd\u2019s algorithm only in the method of seed selection. Our heuristic can be directly used in such algorithms. K-means variants that find appropriate number of clusters in data, evaluate the goodness of clustering for various potential values of number of clusters. Such algorithms can use our heuristic while performing clustering for each potential value of k. K-means variants in third category compute exact distances only to few centroids for each data point. However, they have to compute bounds on distances to rest of the centroids for each data point. Our heuristic can help such K-means variants to further reduce distance and bound calculations."}, {"heading": "4 Experimental Results", "text": "Our heuristic can be augmented to multiple variants of K-means mentioned in Section 3. When augmented to Lloyd\u2019s algorithm, our heuristic provides a speedup of around 30 times with the error within 0.2% of that of Lloyd\u2019s algorithm. However to show the effectiveness of our heuristic, we present results of augmenting it to faster variants of K-means such as K-means with triangle inequality (KMT)[3]. Due to lack of space, we present results of augmenting our heuristic with only this variant. Augmenting KMT with our heuristic is referred\nas algorithm HT. Code and datasets used for our experiments are available for download [1].\nDuring each iteration of KMT, a data point computes distance to the centroid of its current cluster. KMT uses triangle inequality to compute efficient lower bounds on distances to all other centroids. A data point will compute exact distance to any other centroid only when the lower bound on such distance is smaller than the distance to the centroid of its current cluster. During each iteration of HT, a data point will also compute distance to the centroid of its current cluster. However, HT will compute lower bounds on distances to centroids only in its CCL. A data point will compute exact distance to any other centroid in the candidate cluster list only when the lower bound on such distance is smaller than the distance to the centroid of its current cluster.\nExperimental results are presented on five datasets, four of which were used by Elkan et. al.[3] to demonstrate the effectiveness of KMT and one is a synthetically generated dataset by us. These datasets vary in dimensionality from 2 to 784, indicating applicability of our heuristic for low as well as high dimensional data (please refer to Table 1). Our evaluation metrics are chosen based on two conditions mentioned in Section 2: Speedup to satisfy Condition 1 and Percentage Increase in MSE (PIM) to satisfy Condition 2. Speedup is calculated as T/T \u2032. PIM is calculated as (100 \u2217 (E\u2032 \u2212E))/E. We tried two different methods for initial seed selection: random [6]and K-means++ [2]. Both seed selection methods gave similar trends in results. To ensure fair comparison, the same initial seeds are used for both KMT and HT. For some experiments, HT achieves smaller MSE than KMT (E\u2032 \u2264 E). This happens because our heuristic jumps the local minima by not computing distance to every cluster centroid. Only in such cases, HT requires more iterations to converge and runs slower than KMT.\nEffect of Varying k\u2032: Please refer to Table 2. The value of the total number of clusters k is set to 100 for all datasets. Running time and MSE of KMT is independent of value of k\u2032. Speed up of HT over KMT increases with reduction\nin value of k\u2032. This is expected as for small value of k\u2032, HT can avoid many redundant distance computations using small CCL. Speed up of HT over KMT is not same as the ratio k/k\u2032. Reason for reduced speed up is that KMT also avoids some distance computations using its own filtering criteria of triangle inequality. Our heuristic achieves ideal speed of k/k\u2032 when compared against basic K-means algorithm [1]. E\u2032 increases with reduction in value of k\u2032. However, E\u2032 is only marginally higher than E as PIM value never exceeds 1.5.\nEffect of Varying k: Please refer to Table 3. Here, we report results for value of k\u2032 set to 0.4\u2217k. With increasing value of k, HT achieves better speed up over KMT and difference between MSE of HT and MSE of KMT reduces. With increasing value of k, most of the centroid to data point distance calculations become redundant as data-point is assigned only to the closest centroid. In such scenario, our heuristic avoids distance computations with reduced PIM. This shows that our heuristic can be used for datasets having only few as well as large number of clusters.\nEffect of Seeding: Please refer to Table 2 and Table 3. For each value of k\u2032 in Table 2 and k in Table 3, we used two different initial seedings - random (RND) and Kmeans++ [2]. If we compare the results, we observe that better seeding (KMeans++) generally gives better results in terms of PIM. Randomly selected seeds are not necessarily well distributed across the dataset. In such cases, successive iterations of K-means causes significant changes in cluster centroids. Improved seeding methods such as KMeans++ ensure that the initial centroids are spread out more uniformly. Thus centroids shift is less significant in successive iterations. In such scenario, CCL computed after first iteration is a better estimate for final cluster membership. Thus our heuristic is expected to perform better with newer variants of K-means that provide improved seeding.\nEffect of Cluster Well-Separateness: We also performed experiments on synthetic datasets in two dimensions. These datasets were generated using a mixture of Gaussians. The Gaussian centers are placed at equal angles on a circle\nof radius r (angle = 2\u03c0 k ), and each center is assigned equal number of points (n k ). The experiment was done on synthetic datasets of 100000 points generated using the method described above with variance set to 0.25. The value of k is set to 100 and the value of k\u2032 is set to 40. We generated nine datasets by varying the radius from zero to forty in steps of five units. We ran KMT and HT over these nine datasets to check how our heuristic performs with change in well separateness of clusters. We observed that when clusters are close, both the algorithms converge quickly as initial seeds happen to be close to actual cluster centroids. With higher radius, initial seeds might be far off from the actual cluster centroids and KMT takes longer to converge. However, HT performs significantly better for higher values of radius as HT can quickly discard far away clusters. HT achieves a speedup of around 2.31 for higher radius values. For all experiments over these synthetic datasets, we observed that PIM value never exceeds 0.01[1]. This indicates that our heuristic remains relevant even with variation in degree of separation among the clusters."}, {"heading": "5 Conclusion", "text": "We presented a heuristic to attack the bottleneck of redundant distance computations in K-means. Our heuristic limits distance computations for each data point to CCL. Our heuristic can be augmented with diverse variants of K-means to converge faster without any significant increase in MSE. With extensive experiments on real-world and synthetic datasets, we showed that our heuristic performs well with variations in dataset dimensionality, CCL size, number of clusters, and degree of separation among clusters. This work can be further improved by making the CCL dynamic to achieve better speed up while reducing the PIM value."}], "references": [{"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "In ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Using the triangle inequality to accelerate k-means", "author": ["C. Elkan"], "venue": "In International Conference om Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "An efficient k-means clustering algorithm: Analysis and implementation", "author": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Trans. on,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "The global k-means clustering algorithm", "author": ["A. Likas", "N. Vlassis", "J.J. Verbeek"], "venue": "Pattern recognition,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Least squares quantization in pcm", "author": ["S.P. Lloyd"], "venue": "Information Theory, IEEE Trans. on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1982}, {"title": "Accelerating exact k-means algorithms with geometric reasoning", "author": ["D. Pelleg", "A. Moore"], "venue": "In ACM SIGKDD,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Selection of k in k-means clustering", "author": ["D.T. Pham", "S.S. Dimov", "C. Nguyen"], "venue": "Journal of Mechanical Engineering Science,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}], "referenceMentions": [{"referenceID": 4, "context": "One such popular heuristic is the Lloyd\u2019s algorithm[6] that selects certain initial centroids (also referred as seeds) at random from the dataset.", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "In last three decades, there has been significant work on improving Lloyd\u2019s algorithm [6] both in terms of reducing MSE and running time.", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": "The follow up work on Lloyd\u2019s algorithm can be broadly divided into three categories: Better seed selection[2,5], Selecting ideal value for number of clusters[8], and Bounds on data point to cluster centroid distance[3,7,4].", "startOffset": 107, "endOffset": 112}, {"referenceID": 3, "context": "The follow up work on Lloyd\u2019s algorithm can be broadly divided into three categories: Better seed selection[2,5], Selecting ideal value for number of clusters[8], and Bounds on data point to cluster centroid distance[3,7,4].", "startOffset": 107, "endOffset": 112}, {"referenceID": 6, "context": "The follow up work on Lloyd\u2019s algorithm can be broadly divided into three categories: Better seed selection[2,5], Selecting ideal value for number of clusters[8], and Bounds on data point to cluster centroid distance[3,7,4].", "startOffset": 158, "endOffset": 161}, {"referenceID": 1, "context": "The follow up work on Lloyd\u2019s algorithm can be broadly divided into three categories: Better seed selection[2,5], Selecting ideal value for number of clusters[8], and Bounds on data point to cluster centroid distance[3,7,4].", "startOffset": 216, "endOffset": 223}, {"referenceID": 5, "context": "The follow up work on Lloyd\u2019s algorithm can be broadly divided into three categories: Better seed selection[2,5], Selecting ideal value for number of clusters[8], and Bounds on data point to cluster centroid distance[3,7,4].", "startOffset": 216, "endOffset": 223}, {"referenceID": 2, "context": "The follow up work on Lloyd\u2019s algorithm can be broadly divided into three categories: Better seed selection[2,5], Selecting ideal value for number of clusters[8], and Bounds on data point to cluster centroid distance[3,7,4].", "startOffset": 216, "endOffset": 223}, {"referenceID": 0, "context": "[2] provided a better method for seed selection based on a probability distribution over closest cluster centroid distances for each data point.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] proposed the Global k-means method for selecting one seed at a time to reduce final mean squared error.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] designed a novel function to evaluate goodness of clustering for various potential values of number of clusters.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Elkan[3] use triangle inequality to avoid redundant computations of distance between data points and cluster centroids.", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "Pelleg and Moore[7] and Kanungo et al.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "[4] proposed similar algorithms that use k-d trees.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "However to show the effectiveness of our heuristic, we present results of augmenting it to faster variants of K-means such as K-means with triangle inequality (KMT)[3].", "startOffset": 164, "endOffset": 167}, {"referenceID": 0, "context": "RND = Random initialization ; KPP = Initialization using Kmeans++[2]", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "[3] to demonstrate the effectiveness of KMT and one is a synthetically generated dataset by us.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "We tried two different methods for initial seed selection: random [6]and K-means++ [2].", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "We tried two different methods for initial seed selection: random [6]and K-means++ [2].", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "RND = Random initialization ; KPP = Initialization using Kmeans++[2]", "startOffset": 65, "endOffset": 68}, {"referenceID": 0, "context": "For each value of k in Table 2 and k in Table 3, we used two different initial seedings - random (RND) and Kmeans++ [2].", "startOffset": 116, "endOffset": 119}], "year": 2017, "abstractText": "There has been considerable work on improving popular clustering algorithm \u2018K-means\u2019 in terms of mean squared error (MSE) and speed, both. However, most of the k-means variants tend to compute distance of each data point to each cluster centroid for every iteration. We propose a fast heuristic to overcome this bottleneck with only marginal increase in MSE. We observe that across all iterations of K-means, a data point changes its membership only among a small subset of clusters. Our heuristic predicts such clusters for each data point by looking at nearby clusters after the first iteration of k-means. We augment well known variants of k-means with our heuristic to demonstrate effectiveness of our heuristic. For various synthetic and real-world datasets, our heuristic achieves speed-up of up-to 3 times when compared to efficient variants of k-means.", "creator": "LaTeX with hyperref package"}}}