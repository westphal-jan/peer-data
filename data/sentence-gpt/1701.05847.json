{"id": "1701.05847", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jan-2017", "title": "End-To-End Visual Speech Recognition With LSTMs", "abstract": "Traditional visual speech recognition systems consist of two stages, feature extraction and classification. Recently, several deep learning approaches have been presented which automatically extract features from the mouth images and aim to replace the feature extraction stage. However, research on joint learning of features and classification is very limited. In this work, we present an end-to-end visual speech recognition system based on Long-Short Memory (LSTM) networks. To the best of our knowledge, this is the first model which simultaneously learns to extract features directly from the pixels and perform classification and also achieves state-of-the-art performance in visual speech classification. The model consists of two streams which extract features directly from the mouth and difference images, respectively. The temporal dynamics in each stream are modelled by an LSTM and the fusion of the two streams takes place via a Bidirectional LSTM (BLSTM). An absolute improvement of 9.7% over the base line is reported on the OuluVS2 database, and 1.5% on the CUAVE database when compared with other methods which use a similar visual front-end. In order to provide a good comparison, the study was performed with an LSTM, as well as a non-invasive LSTM, which is used to generate a detailed visual picture for the entire time. The results are presented in a paper presented below. We are confident that this system can be used for the processing of visual information on a wide range of perceptual domains, such as general information processing and general information processing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 20 Jan 2017 16:36:09 GMT  (691kb)", "http://arxiv.org/abs/1701.05847v1", "Accepted for publication, ICASSP 2017"]], "COMMENTS": "Accepted for publication, ICASSP 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["stavros petridis", "zuwei li", "maja pantic"], "accepted": false, "id": "1701.05847"}, "pdf": {"name": "1701.05847.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["sp104@imperial.ac.uk", "zl4615@imperial.ac.uk", "m.pantic@imperial.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 1.\n05 84\n7v 1\n[ cs\n.C V\n] 2\n0 Ja\nn 20\nIndex Terms\u2014 Visual Speech Recognition, Lipreading, End-to-End Training, Long-Short Term Recurrent Neural Networks, Deep Networks"}, {"heading": "1. INTRODUCTION", "text": "Speech is an audiovisual signal which consists of the audio vocalisation and the corresponding mouth configuration. Although most of the information is carried by the audio signal, the visual signal also carries complementary and redundant information. This visual information, which is not affected by acoustic noise, can significantly improve the performance of speech recognition in noisy environments.\nTraditionally, visual speech recognition systems consist of two stages, feature extraction from the mouth region of interest (ROI) and classification [1, 2, 3]. The most common feature extraction approach is the use of a dimensionality reduction/compression method, with the most popular being the Discrete Cosine Transform (DCT), which results in a compact representation of the mouth ROI. In the second stage,\na dynamic classifier, like Hidden Markov Models (HMMs) or Long-Short Term Memory (LSTM) recurrent neural networks, is used to model the temporal evolution of the features.\nRecently, several deep learning approaches for visual speech recognition have been presented. The vast majority also follow a two stage approach where deep bottleneck architectures are used for feature extraction. First, high dimensional features are extracted from the mouth ROI which are compressed to a low dimensional representation at the bottleneck layer of a deep network and then fed to a classifier. Ngiam et al. [4] applied principal component analysis (PCA) to the mouth ROI and trained a deep autoencoder to extract bottleneck features. The features from the entire utterance were fed to a support vector machine ignoring the temporal dynamics of the speech. Ninomiya et al. [5] also applied PCA to the mouth ROIs and used a deep autoencoder to extract bottleneck features but an HMM was used in order to take into account the temporal dynamics. Sui et al. [6] extracted local binary patterns from the mouth ROI and used a deep autoencoder to reduce their dimensionality. Then, the bottleneck features were concatenated with DCT features and fed to an HMM. A similar approach has also been followed in audiovisual speech recognition [7, 8, 9] where a shared representation of the input audio and visual features is extracted from the bottleneck layer.\nFew works have also been presented which extract bottleneck features directly from the pixels. Li [10] used a convolutional neural network (CNN) in order to extract bottleneck features from dynamic representations of images, which are fed to an HMM for classification. In our previous work [11], we extracted bottleneck features directly from the raw mouth ROI using a deep feedforward network and then trained an LSTM for classification. Noda et al. [12] used a CNN to predict the phoneme that corresponds to an input mouth ROI, and then an HMM is used together with audio features in order to classify an utterance.\nDespite the success of deep learning methods in feature extraction, work on end-to-end visual speech recognition has been very limited. To the best of our knowledge, only Wand et al. [13] developed an end-to-end system for lipreading. The system consists of one feedforward layer followed by\ntwo LSTM layers and trained to perform lipreading directly from raw mouth ROIs. The system was tested on a subjectdependent experiment on the GRID corpus[14] and although it outperformed other baseline systems it failed to outperform the state-of-the-art results [15].\nIn this paper, we present an end-to-end visual speech recognition system which jointly learns the feature extraction and classification stages. To the best of our knowledge, this is the first end-to-end model which performs visual speech recognition from raw mouth ROIs and achieves state-of-theart performance. The system consists of two streams, one which encodes static information and one which encodes local temporal dynamics. The former operates on the raw mouth ROIs and the latter on the difference (diff) images. An LSTM models the temporal dynamics in each stream and the fusion of both streams occurs through a BLSTM.\nWe perform subject independent experiments on two different datasets, OuluVS2 and CUAVE. An absolute improvement of 9.7% over the baseline is reported on the OuluVS2\ndatabase, and 1.5% on the CUAVE database when compared with other methods which use a similar visual front-end."}, {"heading": "2. DATABASES", "text": "The databases used in this study are the OuluVS2 [16] and CUAVE [17]. The OuluVS2 contains 52 speakers saying 10 utterances, 3 times each, so in total there are 156 examples per utterance. The utterances are the following: \u201cExcuse me\u201d, \u201cGoodbye\u201d, \u201cHello\u201d, \u201cHow are you\u201d, \u201cNice to meet you\u201d, \u201cSee you\u201d, \u201cI am sorry\u201d, \u201cThank you\u201d, \u201cHave a good time\u201d, \u201cYou are welcome\u201d. The mouth ROIs are provided and they are downscaled to 26 by 44 in order to keep the aspect ratio constant.\nThe CUAVE dataset contains 36 subjects speaking digits 0 to 9, 5 times each, so in total there are 180 examples per digit. The normal portion of the database is used where the subjects are in frontal position. Sixty eight points are tracked on the face using the tracker proposed in [18]. The faces are first aligned using a neutral reference frame in order to normalise them for rotation and size differences. This is done using an affine transform using 5 stable points, two eyes corners in each eye and the tip of the nose. Then the center of the mouth is located based on the tracked points and a bounding box with size 90 by 150 is used to extract the mouth ROI as shown in Fig. 2. Finally, the mouth ROIs are downscaled to 30 by 50."}, {"heading": "3. END-TO-END VISUAL SPEECH RECOGNITION", "text": "The proposed deep learning system for visual speech recognition is shown in Fig. 1. It consists of two independent streams which extract features directly from the raw input. The first stream mainly encodes static information by extracting features directly from the raw mouth ROI. The second stream encodes the local temporal dynamics by extracting features from the diff mouth ROI, which is computed by taking the difference between two consecutive frames.\nBoth streams follow a bottleneck architecture in order to compress the high dimensional input image to a low dimensional representation at the bottleneck layer. The same architecture as in [19] is used, where 3 sigmoid hidden layers are used with sizes of 2000, 1000 and 500, respectively, followed by a linear bottleneck layer. These encoding layers are\nAccepted for publication, ICASSP 2017\npre-trained in a greedy layer-wise manner using Restricted Boltzmann Machines (RBMs) [20]. The \u2206 (first derivatives) and \u2206\u2206 (second derivatives) [21] features are also computed, based on the bottleneck features, and they are appended to the bottleneck layer. In this way, during training we force the encoding layers to learn representations which produce good \u2206 and \u2206\u2206 features.\nFinally, an LSTM layer is added on top of the encoding layers in order to model the temporal dynamics of the features in each stream. The LSTM outputs of each stream are concatenated and fed to a BLSTM in order to fuse the information from both streams. The output layer is a softmax layer which provides a label for each input frame. The entire system is trained end-to-end which enables the joint learning of features and classifier. In other words, the encoding layers learn to extract features from raw images which are useful for classification using LSTMs."}, {"heading": "4. EXPERIMENTAL SETUP", "text": ""}, {"heading": "4.1. Evaluation Protocol", "text": "We first partition the data into training and test sets. The protocol suggested by the creators of the OuluVS2 database is used [22] where 40 subjects are used for training and validation and 12 for testing. We randomly divided the 40 subjects into 30 and 10 subjects for training and validation purposes, respectively. This means that there are 900 training utterances, 300 validation utterances and 360 test utterances.\nThe evaluation protocol suggested in [4] was used for experiments on the CUAVE database. The odd-numbered subjects (18 in total) are used for testing and the even-numbered subjects are used for training. We further divided the latter ones into 12 subjects for training and 6 for validation. This means that there are 600, 300 and 900 training, validation and test utterances, respectively."}, {"heading": "4.2. Preprocessing", "text": "Since all the experiments are subject independent we first need to reduce the impact of subject dependent characteristics. This is done by subtracting the mean image, computed over the entire utterance, from each frame.\nThe next step is the normalisation of data. As recommended in [20] the data should be z-normalised, i.e. the mean and standard deviation should be equal to 0 and 1 respectively, before training an RBM with linear input units. Hence, each image is z-normalised before pre-training the encoding layers."}, {"heading": "4.3. Training", "text": "RBM Training: A Gaussian-Bernoulli RBM [20] is used for the first layer since the input (pixels) is real-valued, followed by two Bernoulli-Bernoulli RBMs and one BernoulliGaussian RBM for the linear bottleneck layer. Each RBM is trained for 20 epochs with a mini-batch size of 100 and L2 regularisation coefficient of 0.0002 using contrastive divergence. The learning rate is fixed to 0.1 for the BernoulliBernoulli RBMs and to 0.001 when one layer (input or bottleneck) is real-valued as suggested in [20]. End-to-End Training: The AdaDelta algorithm [27], which automatically computes the learning rate in each epoch, was used for training with a mini-batch size of 20 utterances. Early stopping with a delay of 5 epochs was also used in order to avoid overfitting. Gradient clipping was applied to the LSTM layers. The label of the last frame in each utterance was used in order to label the entire utterance."}, {"heading": "5. RESULTS", "text": "Results for the OuluVS2 database are shown in Table 1. Since this database has been released recently only the baseline results provided by the creators are available. The best provided baseline result, 74.8%, is achieved by HMMs in combination with DCT features. We first test each stream of the endto-end model individually, i.e., just the encoding layers and the LSTM layer are considered. It is interesting to note that both streams outperform the baseline performance. The best overall result is achieved by the end-to-end 2-stream model, shown in Fig. 1, with a classification accuracy of 84.5%. We should also emphasise that the baseline performance is evaluated using a leave-one-subject-out cross validation approach which means there are 51 subjects for training and validation and only one subject for testing in each iteration. On the other hand, we use much fewer subjects for training and validation, 40, and many more subjects for testing, 12, which makes the problem more challenging. Even in this case, the end-to-end system results in a significant improvement over the baseline performance.\nResults for the CUAVE database are shown in Table 2. There is not a standard evaluation protocol for this database which makes comparison between different works difficult. Only [4] and [23] use the same evaluation protocol as in this study. We see that the single-stream end-to-end model based on raw mouth ROIs outperforms both previous works. The 2-stream end-to-end model outperforms all approaches that use a similar visual front-end. This includes [24] where a 6- fold cross validation was used with 30 subjects for training and validation and 6 for testing, and [25] where 28 subjects were used for training and validation and 10 for testing. In this study, we use much fewer subjects, 18, for training and validation and many more subjects for testing, 18. Only [26] achieves a higher performance than our end-to-end system, but a much more complicated visual front-end is used, with a cascade of active appearance models (AAM), and the model is evaluated using a 6-fold cross-validation.\nFigures 3 and 4 show the confusion matrices for both\ndatasets. In the OuluVS2 dataset, the most confusions were between phrases 3 (Hello) and 8 (Thank you) and between phrases 6 (See you) and 9 (Have a good time). In the CUAVE dataset, number pairs zero and two, six and nine were most frequently confused. Zero and two share similar viseme sequences near the end of the utterance while six and nine share similar viseme sequences at the start of the utterance which explains the more frequently occurring confusions for these number pairs.\nFinally, we should also mention that we experimented with convolutional neural networks for the encoding layers but this led to worse performance than the proposed system. This is also reported in [13] and it is likely due to the small training sets. We also used data augmentation which improved the performance but did not exceed the performance of the proposed system."}, {"heading": "6. CONCLUSION", "text": "In this work, we present an end-to-end visual speech recognition system which jointly learns to extract features directly from the pixels and perform classification using LSTM networks. Results on subject independent experiments demonstrate that the proposed model achieves state-of-the-art performance on the OuluVS2 and CUAVE databases when compared with models which use a similar visual front end. The model can be easily extended to multiple streams so we are planning to add an audio stream in order to evaluate its performance on audiovisual speech recognition tasks."}, {"heading": "7. ACKNOWLEDGEMENTS", "text": "This work has been funded by the European Community Horizon 2020 under grant agreement no. 645094 (SEWA) and no. 688835 (DE- ENIGMA).\nAccepted for publication, ICASSP 2017"}, {"heading": "8. REFERENCES", "text": "[1] G. Potamianos, C. Neti, G. Gravier, A. Garg, and A. W. Senior, \u201cRecent advances in the automatic recognition of audiovisual speech,\u201d Proceedings of the IEEE, vol. 91, no. 9, pp. 1306\u20131326, Sept 2003.\n[2] S. Dupont and J. Luettin, \u201cAudio-visual speech modeling for continuous speech recognition,\u201d IEEE Trans. on Multimedia, vol. 2, no. 3, pp. 141\u2013151, Sep 2000.\n[3] Z. Zhou, G. Zhao, and M. Pietikinen, \u201cTowards a practical lipreading system,\u201d in IEEE CVPR, 2011, pp. 137\u2013 144.\n[4] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y Ng, \u201cMultimodal deep learning,\u201d in Proc. of ICML, 2011, pp. 689\u2013696.\n[5] H. Ninomiya, N. Kitaoka, S. Tamura, Y. Iribe, and K. Takeda, \u201cIntegration of deep bottleneck features for audio-visual speech recognition,\u201d in Conf. of the International Speech Communication Association, 2015.\n[6] C. Sui, R. Togneri, and M. Bennamoun, \u201cExtracting deep bottleneck features for visual speech recognition,\u201d in ICASSP, 2015, pp. 1518\u20131522.\n[7] J. Huang and B. Kingsbury, \u201cAudio-visual deep learning for noise robust speech recognition,\u201d in IEEE ICASSP, 2013, pp. 7596\u20137599.\n[8] Y. Mroueh, E. Marcheret, and V. Goel, \u201cDeep multimodal learning for audio-visual speech recognition,\u201d in IEEE ICASSP, 2015, pp. 2130\u20132134.\n[9] C. Sui, M. Bennamoun, and R. Togneri, \u201cListening with your eyes: Towards a practical visual speech recognition system using deep boltzmann machines,\u201d in IEEE ICCV, 2015, pp. 154\u2013162.\n[10] Y. Li, Y. Takashima, T. Takiguchi, and Y. Ariki, \u201cLip reading using a dynamic feature of lip images and convolutional neural networks,\u201d in IEEE/ACIS Intl. Conf. on Computer and Information Science, 2016, pp. 1\u20136.\n[11] S. Petridis and M. Pantic, \u201cDeep complementary bottleneck features for visual speech recognition,\u201d in IEEE ICASSP, 2016, pp. 2304\u20132308.\n[12] K. Noda, Y. Yamaguchi, K. Nakadai, H. G. Okuno, and T. Ogata, \u201cAudio-visual speech recognition using deep learning,\u201d Applied Intelligence, vol. 42, no. 4, pp. 722\u2013 737, 2015.\n[13] M. Wand, J. Koutn, and J. Schmidhuber, \u201cLipreading with long short-term memory,\u201d in IEEE ICASSP, 2016, pp. 6115\u20136119.\n[14] M. Cooke, J. Barker, S. Cunningham, and X. Shao, \u201cAn audio-visual corpus for speech perception and automatic speech recognition,\u201d The Journal of the Acoustical Society of America, vol. 120, no. 5, pp. 2421\u20132424, 2006.\n[15] A. H. Abdelaziz, S. Zeiler, and D. Kolossa, \u201cLearning dynamic stream weights for coupled-hmm-based audiovisual speech recognition,\u201d IEEE Trans. on Audio, Sp., and Lang. Proc., vol. 23, no. 5, pp. 863\u2013876, May 2015.\n[16] I. Anina, Z. Zhou, G. Zhao, and M. Pietikinen, \u201cOuluvs2: A multi-view audiovisual database for nonrigid mouth motion analysis,\u201d in IEEE FG, 2015.\n[17] E. Patterson, S. Gurbuz, Z. Tufekci, and J. Gowdy, \u201cMoving-talker, speaker-independent feature study, and baseline results using the cuave multimodal speech corpus,\u201d EURASIP J. Appl. Signal Process., vol. 2002, no. 1, pp. 1189\u20131201, Jan. 2002.\n[18] V. Kazemi and J. Sullivan, \u201cOne millisecond face alignment with an ensemble of regression trees,\u201d in IEEE CVPR, 2014, pp. 1867\u20131874.\n[19] G. Hinton and Salakhutdinov R., \u201cReducing the dimensionality of data with neural networks,\u201d Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.\n[20] G. Hinton, \u201cA practical guide to training restricted boltzmann machines,\u201d in Neural Networks: Tricks of the Trade, pp. 599\u2013619. Springer, 2012.\n[21] S. Young, G. Evermann, M. Gales, T. Hain, D. Kershaw, X. Liu, G. Moore, J. Odell, D. Ollason, D. Povey, et al., \u201cThe htk book,\u201d vol. 3, pp. 175, 2002.\n[22] ,\u201d http://ouluvs2.cse.oulu.fi.\n[23] N. Srivastava and R. Salakhutdinov, \u201cMultimodal learning with deep boltzmann machines,\u201d J. Mach. Learn. Res., vol. 15, no. 1, pp. 2949\u20132980, Jan. 2014.\n[24] G. Papandreou et al., \u201cMultimodal fusion and learning with uncertain features applied to audiovisual speech recognition,\u201d in Workshop on Multimedia Signal Processing, 2007, pp. 264\u2013267.\n[25] P. Lucey and S. Sridharan, \u201cPatch-based representation of visual speech,\u201d in Proc. of the HCSNet Workshop on Use of Vision in HCI, 2006, pp. 79\u201385.\n[26] G. Papandreou, A. Katsamanis, V. Pitsikalis, and P. Maragos, \u201cAdaptive multimodal fusion by uncertainty compensation with application to audiovisual speech recognition,\u201d IEEE Trans. on Audio, Speech, and Language Processing, vol. 17, no. 3, pp. 423\u2013435, 2009.\n[27] M. D. Zeiler, \u201cADADELTA: an adaptive learning rate method,\u201d vol. http://arxiv.org/abs/1212.5701, 2012."}], "references": [{"title": "Recent advances in the automatic recognition of audiovisual speech", "author": ["G. Potamianos", "C. Neti", "G. Gravier", "A. Garg", "A.W. Senior"], "venue": "Proceedings of the IEEE, vol. 91, no. 9, pp. 1306\u20131326, Sept 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Audio-visual speech modeling for continuous speech recognition", "author": ["S. Dupont", "J. Luettin"], "venue": "IEEE Trans. on Multimedia, vol. 2, no. 3, pp. 141\u2013151, Sep 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Towards a practical lipreading system", "author": ["Z. Zhou", "G. Zhao", "M. Pietikinen"], "venue": "IEEE CVPR, 2011, pp. 137\u2013 144.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A. Y Ng"], "venue": "Proc. of ICML, 2011, pp. 689\u2013696.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Integration of deep bottleneck features for audio-visual speech recognition", "author": ["H. Ninomiya", "N. Kitaoka", "S. Tamura", "Y. Iribe", "K. Takeda"], "venue": "Conf. of the International Speech Communication Association, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Extracting deep bottleneck features for visual speech recognition", "author": ["C. Sui", "R. Togneri", "M. Bennamoun"], "venue": "ICASSP, 2015, pp. 1518\u20131522.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Audio-visual deep learning for noise robust speech recognition", "author": ["J. Huang", "B. Kingsbury"], "venue": "IEEE ICASSP, 2013, pp. 7596\u20137599.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep multimodal learning for audio-visual speech recognition", "author": ["Y. Mroueh", "E. Marcheret", "V. Goel"], "venue": "IEEE ICASSP, 2015, pp. 2130\u20132134.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Listening with your eyes: Towards a practical visual speech recognition system using deep boltzmann machines", "author": ["C. Sui", "M. Bennamoun", "R. Togneri"], "venue": "IEEE ICCV, 2015, pp. 154\u2013162.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Lip reading using a dynamic feature of lip images and convolutional neural networks", "author": ["Y. Li", "Y. Takashima", "T. Takiguchi", "Y. Ariki"], "venue": "IEEE/ACIS Intl. Conf. on Computer and Information Science, 2016, pp. 1\u20136.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep complementary bottleneck features for visual speech recognition", "author": ["S. Petridis", "M. Pantic"], "venue": "IEEE ICASSP, 2016, pp. 2304\u20132308.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Audio-visual speech recognition using deep learning", "author": ["K. Noda", "Y. Yamaguchi", "K. Nakadai", "H.G. Okuno", "T. Ogata"], "venue": "Applied Intelligence, vol. 42, no. 4, pp. 722\u2013 737, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Lipreading with long short-term memory", "author": ["M. Wand", "J. Koutn", "J. Schmidhuber"], "venue": "IEEE ICASSP, 2016, pp. 6115\u20136119.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "An audio-visual corpus for speech perception and automatic speech recognition", "author": ["M. Cooke", "J. Barker", "S. Cunningham", "X. Shao"], "venue": "The Journal of the Acoustical Society of America, vol. 120, no. 5, pp. 2421\u20132424, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning dynamic stream weights for coupled-hmm-based audiovisual speech recognition", "author": ["A.H. Abdelaziz", "S. Zeiler", "D. Kolossa"], "venue": "IEEE Trans. on Audio, Sp., and Lang. Proc., vol. 23, no. 5, pp. 863\u2013876, May 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Ouluvs2: A multi-view audiovisual database for nonrigid mouth motion analysis", "author": ["I. Anina", "Z. Zhou", "G. Zhao", "M. Pietikinen"], "venue": "IEEE FG, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Moving-talker, speaker-independent feature study, and baseline results using the cuave multimodal speech corpus", "author": ["E. Patterson", "S. Gurbuz", "Z. Tufekci", "J. Gowdy"], "venue": "EURASIP J. Appl. Signal Process., vol. 2002, no. 1, pp. 1189\u20131201, Jan. 2002.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "One millisecond face alignment with an ensemble of regression trees", "author": ["V. Kazemi", "J. Sullivan"], "venue": "IEEE CVPR, 2014, pp. 1867\u20131874.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "Salakhutdinov R."], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["G. Hinton"], "venue": "Neural Networks: Tricks of the Trade, pp. 599\u2013619. Springer, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "The htk book", "author": ["S. Young", "G. Evermann", "M. Gales", "T. Hain", "D. Kershaw", "X. Liu", "G. Moore", "J. Odell", "D. Ollason", "D. Povey"], "venue": "vol. 3, pp. 175, 2002.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "J. Mach. Learn. Res., vol. 15, no. 1, pp. 2949\u20132980, Jan. 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal fusion and learning with uncertain features applied to audiovisual speech recognition", "author": ["G. Papandreou"], "venue": "Workshop on Multimedia Signal Processing, 2007, pp. 264\u2013267.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Patch-based representation of visual speech", "author": ["P. Lucey", "S. Sridharan"], "venue": "Proc. of the HCSNet Workshop on Use of Vision in HCI, 2006, pp. 79\u201385.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Adaptive multimodal fusion by uncertainty compensation with application to audiovisual speech recognition", "author": ["G. Papandreou", "A. Katsamanis", "V. Pitsikalis", "P. Maragos"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing, vol. 17, no. 3, pp. 423\u2013435, 2009.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "vol. http://arxiv.org/abs/1212.5701, 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Traditionally, visual speech recognition systems consist of two stages, feature extraction from the mouth region of interest (ROI) and classification [1, 2, 3].", "startOffset": 150, "endOffset": 159}, {"referenceID": 1, "context": "Traditionally, visual speech recognition systems consist of two stages, feature extraction from the mouth region of interest (ROI) and classification [1, 2, 3].", "startOffset": 150, "endOffset": 159}, {"referenceID": 2, "context": "Traditionally, visual speech recognition systems consist of two stages, feature extraction from the mouth region of interest (ROI) and classification [1, 2, 3].", "startOffset": 150, "endOffset": 159}, {"referenceID": 3, "context": "[4] applied principal component analysis (PCA) to the mouth ROI and trained a deep autoencoder to extract bottleneck features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] also applied PCA to the mouth ROIs and used a deep autoencoder to extract bottleneck features but an HMM was used in order to take into account the temporal dynamics.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] extracted local binary patterns from the mouth ROI and used a deep autoencoder to reduce their dimensionality.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "A similar approach has also been followed in audiovisual speech recognition [7, 8, 9] where a shared representation of the input audio and visual features is extracted from the bottleneck layer.", "startOffset": 76, "endOffset": 85}, {"referenceID": 7, "context": "A similar approach has also been followed in audiovisual speech recognition [7, 8, 9] where a shared representation of the input audio and visual features is extracted from the bottleneck layer.", "startOffset": 76, "endOffset": 85}, {"referenceID": 8, "context": "A similar approach has also been followed in audiovisual speech recognition [7, 8, 9] where a shared representation of the input audio and visual features is extracted from the bottleneck layer.", "startOffset": 76, "endOffset": 85}, {"referenceID": 9, "context": "Li [10] used a convolutional neural network (CNN) in order to extract bottleneck features from dynamic representations of images, which are fed to an HMM for classification.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "In our previous work [11], we extracted bottleneck features directly from the raw mouth ROI using a deep feedforward network and then trained an LSTM for classification.", "startOffset": 21, "endOffset": 25}, {"referenceID": 11, "context": "[12] used a CNN to predict the phoneme that corresponds to an input mouth ROI, and then an HMM is used together with audio features in order to classify an utterance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] developed an end-to-end system for lipreading.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "The system was tested on a subjectdependent experiment on the GRID corpus[14] and although it outperformed other baseline systems it failed to outperform the state-of-the-art results [15].", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": "The system was tested on a subjectdependent experiment on the GRID corpus[14] and although it outperformed other baseline systems it failed to outperform the state-of-the-art results [15].", "startOffset": 183, "endOffset": 187}, {"referenceID": 15, "context": "The databases used in this study are the OuluVS2 [16] and CUAVE [17].", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "The databases used in this study are the OuluVS2 [16] and CUAVE [17].", "startOffset": 64, "endOffset": 68}, {"referenceID": 17, "context": "Sixty eight points are tracked on the face using the tracker proposed in [18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 18, "context": "The same architecture as in [19] is used, where 3 sigmoid hidden layers are used with sizes of 2000, 1000 and 500, respectively, followed by a linear bottleneck layer.", "startOffset": 28, "endOffset": 32}, {"referenceID": 19, "context": "pre-trained in a greedy layer-wise manner using Restricted Boltzmann Machines (RBMs) [20].", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "The \u2206 (first derivatives) and \u2206\u2206 (second derivatives) [21] features are also computed, based on the bottleneck features, and they are appended to the bottleneck layer.", "startOffset": 54, "endOffset": 58}, {"referenceID": 3, "context": "The evaluation protocol suggested in [4] was used for experiments on the CUAVE database.", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "The end-to-end model is trained using the same protocol as [4, 23] where 18 subjects are used for training and validation and 18 for testing.", "startOffset": 59, "endOffset": 66}, {"referenceID": 21, "context": "The end-to-end model is trained using the same protocol as [4, 23] where 18 subjects are used for training and validation and 18 for testing.", "startOffset": 59, "endOffset": 66}, {"referenceID": 3, "context": "Deep Autoencoder + SVM [4] 68.", "startOffset": 23, "endOffset": 26}, {"referenceID": 21, "context": "Deep Boltzmann Machines + SVM [23] 69.", "startOffset": 30, "endOffset": 34}, {"referenceID": 22, "context": "AAM +HMM [24] \u2020 75.", "startOffset": 9, "endOffset": 13}, {"referenceID": 23, "context": "Patch-based Features + HMM [25] \u2217 77.", "startOffset": 27, "endOffset": 31}, {"referenceID": 24, "context": "Visemic AAM + HMM [26] \u2020 \u2021 83.", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "As recommended in [20] the data should be z-normalised, i.", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "RBM Training: A Gaussian-Bernoulli RBM [20] is used for the first layer since the input (pixels) is real-valued, followed by two Bernoulli-Bernoulli RBMs and one BernoulliGaussian RBM for the linear bottleneck layer.", "startOffset": 39, "endOffset": 43}, {"referenceID": 19, "context": "001 when one layer (input or bottleneck) is real-valued as suggested in [20].", "startOffset": 72, "endOffset": 76}, {"referenceID": 25, "context": "End-to-End Training: The AdaDelta algorithm [27], which automatically computes the learning rate in each epoch, was used for training with a mini-batch size of 20 utterances.", "startOffset": 44, "endOffset": 48}, {"referenceID": 3, "context": "Only [4] and [23] use the same evaluation protocol as in this study.", "startOffset": 5, "endOffset": 8}, {"referenceID": 21, "context": "Only [4] and [23] use the same evaluation protocol as in this study.", "startOffset": 13, "endOffset": 17}, {"referenceID": 22, "context": "This includes [24] where a 6fold cross validation was used with 30 subjects for training and validation and 6 for testing, and [25] where 28 subjects were used for training and validation and 10 for testing.", "startOffset": 14, "endOffset": 18}, {"referenceID": 23, "context": "This includes [24] where a 6fold cross validation was used with 30 subjects for training and validation and 6 for testing, and [25] where 28 subjects were used for training and validation and 10 for testing.", "startOffset": 127, "endOffset": 131}, {"referenceID": 24, "context": "Only [26] achieves a higher performance than our end-to-end system, but a much more complicated visual front-end is used, with a cascade of active appearance models (AAM), and the model is evaluated using a 6-fold cross-validation.", "startOffset": 5, "endOffset": 9}, {"referenceID": 12, "context": "This is also reported in [13] and it is likely due to the small training sets.", "startOffset": 25, "endOffset": 29}], "year": 2017, "abstractText": "Traditional visual speech recognition systems consist of two stages, feature extraction and classification. Recently, several deep learning approaches have been presented which automatically extract features from the mouth images and aim to replace the feature extraction stage. However, research on joint learning of features and classification is very limited. In this work, we present an end-to-end visual speech recognition system based on Long-Short Memory (LSTM) networks. To the best of our knowledge, this is the first model which simultaneously learns to extract features directly from the pixels and perform classification and also achieves state-of-the-art performance in visual speech classification. The model consists of two streams which extract features directly from the mouth and difference images, respectively. The temporal dynamics in each stream are modelled by an LSTM and the fusion of the two streams takes place via a Bidirectional LSTM (BLSTM). An absolute improvement of 9.7% over the base line is reported on the OuluVS2 database, and 1.5% on the CUAVE database when compared with other methods which use a similar visual front-end.", "creator": "LaTeX with hyperref package"}}}