{"id": "1701.06075", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jan-2017", "title": "Label Propagation on K-partite Graphs with Heterophily", "abstract": "In this paper, for the first time, we study label propagation in heterogeneous graphs under heterophily assumption. Homophily label propagation (i.e., two connected nodes share similar labels) in homogeneous graph (with same types of vertices and relations) has been extensively studied before. Unfortunately, real-life networks are heterogeneous, they contain different types of vertices (e.g., users, images, texts) and relations (e.g., friendships, co-tagging) and allow for each node to propagate both the same and opposite copy of labels to its neighbors. We propose a $\\mathcal{K}$-partite label propagation model to handle the mystifying combination of heterogeneous nodes/relations and heterophily propagation. With this model, we develop a novel label inference algorithm framework with update rules in near-linear time complexity. Since real networks change over time, we devise an incremental approach, which supports fast updates for both new data and evidence (e.g., ground truth labels) with guaranteed efficiency. We further provide a utility function to automatically determine whether an incremental or a re-modeling approach is favored. Extensive experiments on real datasets have verified the effectiveness and efficiency of our approach, and its superiority over the state-of-the-art label propagation methods. Using this model, we develop a model for quantified state-of-the-art labels in the context of distributed distributed graphs.\n\n\n\n\nThe next step will be to develop a new proof-of-concept model based on the principles that led to the emergence of distributed graphs. We propose a network of graphs with uniform data-type distribution. Each of these graphs represents an efficient version of the distributed network. For this task, each graph represents a state of the tree, with a distributed distribution that is distributed according to a fixed set of rules. This model uses a standard network. Each graph represents a set of two nodes (for a single point). It contains a set of two nodes, one node, and one node. Each node has one point. These nodes are separated by a single node in its distributed distribution. Each node receives a pair of nodes. Each node has one point. These nodes are separated by a single node in its distributed distribution. Each node has two point. Each node is separated by a single node in its distributed distribution. Each node has two point. Each node has two point. Each node has two point. Each node has two point. Each node is separated by a", "histories": [["v1", "Sat, 21 Jan 2017 19:47:38 GMT  (228kb,D)", "http://arxiv.org/abs/1701.06075v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SI", "authors": ["dingxiong deng", "fan bai", "yiqi tang", "shuigeng zhou", "cyrus shahabi", "linhong zhu"], "accepted": false, "id": "1701.06075"}, "pdf": {"name": "1701.06075.pdf", "metadata": {"source": "CRF", "title": "Label Propagation on K-partite Graphs with Heterophily", "authors": ["Dingxiong Deng", "Fan Bai", "Yiqi Tang", "Shuigeng Zhou", "Cyrus Shahabi", "Linhong Zhu"], "emails": ["dingxiod@usc.edu", "sgzhou@fudan.edu.cn", "shahabi@usc.edu", "linhong@isi.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "Label propagation [44] is one of the classic algorithms to learn the label information for each vertex in a network (or graph). It is a process that each vertex receives labels from neighbors in parallel, then updates its labels and finally sends new labels back to its neighbors. Recently, label propagation has received renewed interests from both academia and industry due to its various applications in many domains such as in spam detection [1], fraud detection [10], sentiment analysis [15], and graph partitioning [35]. Differ-\nent algorithms [11, 19, 33, 39, 41, 44] have been proposed to perform label propagation on trees or arbitrary graphs. All of these traditional algorithms simply assume that all vertices are of the same type and restricted to only a single pairwise similarity matrix (graph).\nUnfortunately, many real networks such as social networks are heterogeneous systems [3, 32, 38] that contain objects of multiple types and are interlinked via various relations. Considering the heterogeneity of data, it is tremendously challenging to analyze and understand such networks through label propagation. Applying traditional label propagation approaches directly to heterogeneous graphs is not feasible due to the following reasons. First, traditional approaches neither support label propagation among different types of vertices, nor distinguish propagation strengths among various types of relations. Consider the example of sentiment labeling shown in Fig. 1, the label of each tweet is estimated using the labels of words and users. In addition, the label information of users is much more reliable than that of words in terms of deciding the labels of tweets [42], and thus the user vertices should have stronger propagation strengths than word vertices. Second, traditional approaches do not support heterophily propagation. As illustrated in Fig. 1, in sentiment label propagation, traditional approaches assume that if a word has a positive label, then its connected tweets also have positive labels. However, it\nar X\niv :1\n70 1.\n06 07\n5v 1\n[ cs\n.L G\n] 2\n1 Ja\nn 20\nis reasonable that a tweet connected to a positive word is negative due to sarcasm.There are few notable exceptions [9, 13, 20] (see more in related works) that support either heterogeneous types of vertices or heterophily propagation, but not both. Last but not least, all of the current approaches simply assume that the propagation type (e.g., a homophily or heterophily propagation) is given as an input, though actually this is difficult to obtain from observation.\nIn this paper, we study the problem of labeling nodes with categories (e.g., topics, communities, sentiment classes) in a partially labeled heterogeneous network. To this end, we first propose a K-partite graph model as the knowledge representation for heterogeneous data. Many real-life data examples, naturally form K-partite with different types of nodes and relations. To provide a few examples, as shown in Fig. 2, in folksonomy system, the triplet relations among users, items and tags can be represented as a tripartite graph; document-word-author relation can also be modeled as another tripartite graph with three types of vertices. A K-partite graph model nicely captures the combination of vertex-level heterogeneity and heterogeneous relations, which motivates us to focus on label propagation on K-partite graphs. That is, given an observed K-partite and a set of very few seed vertices that have ground-truth labels, our goal is to learn the label information of the large number of the remaining vertices. Even though K-partite graphs are ubiquitous and the problem of label propagation on K-partite graphs has significant impact, this area is much less studied as compared to traditional label propagation in homogeneous graphs and thus various modeling and algorithmic challenges remain unsolved.\nTo address the modeling challenges, we develop a unified K-partite label propagation model with both vertex-level heterogeneity and propagation-level heterogeneity. Consider the sentiment example in Fig. 1, our model allows a tweet vertex to receive labels from both words and users, but automatically gives higher weights to the latter through our propagation matrix. The propagation-level heterogeneity, which is reflected by supporting homophily, heterophily and mixed propagation, are automatically learned in our model. To infer our model, we first propose a framework that supports both multiplicative [25] and addictive rules [18] (i.e., projected gradient descent) under the vertex-centric manner with near linear-time complexity. Because in practice graph and labels can continuously changing, we then study how and when we should apply label propagation to handle the changing scenarios. We thus devise a fast increment algorithm (i.e., assigning labels for new day, or updating labels upon feedbacks), that performs partial updates instead of re-running the label inference from scratch. We not only can control the trade-off between efficiency and accuracy via\nthe confidence level parameter for speed up, but also can determine whether we should apply our incremental algorithm through a utility function.\nThe contributions of this work are as follows:\n1. Problem formulation: we show how real-life data analytic tasks can be formulated as the problem of label propagation on K-partite graphs.\n2. Heterogeneity and heterophily: we propose a generic label propagation model that supports both heterogeneity and heterophily propagation.\n3. Fast inference algorithms: we develop a unified label propagation framework on K-partite graphs that supports both multiplicative and addictive rules with near-linear time complexity. We then propose an incremental framework, which supports much faster updates upon new data and labels than re-computing from scratch. In order to strike a balance between efficiency and effectiveness, we introduce a confidence level parameter for speed up. We further develop a utility function is designed to automatically choose between an incremental or a re-computing approach.\n4. Practical applications: we demonstrate with three typical application examples that various classification tasks in real scenarios can be solved by our label propagation framework with K-partite graphs."}, {"heading": "2. RELATED WORKS", "text": "In this section, we first provide an extensive (but not exhaustive) review about the state-of-the-art label propagation approaches, and then discuss related works on heterogeneous graph representation using K-partite graphs."}, {"heading": "2.1 Label Propagation", "text": "We summarize a set of representative label propagation algorithms in Table 1. In the following, we provide more details about each approach, with emphasis on their advantages and disadvantages. First we consider how these methods support propagating labels in heterogeneous networks. Various types of algorithms have been proposed to perform label propagation on trees or arbitrary graphs such\nas belief propagation [11] [39], loopy belief propagation [19], Gaussian Random Field (GRF) [44], MP [33], MAD [34], and local consistency [41]. All of these algorithms simply assume that all vertices are of the same type and restricted to only a single graph.\nRecently with the tremendously increasing of heterogeneous data, label propagation approaches on heterogeneous networks have been developed. Jacob et al. [20] focused on learning the unified latent space representation through supervised learning for heterogeneous networks. Ding et al. [9] proposed a cross propagation algorithm for K-partite graphs, which distinguishes vertices of different types, and propagates label information from vertices of one type to vertices of another type. However, they still make the homophily propagation assumption, that is, two connected nodes have similar labels (representations), even though they are from different types. On another hand, Gatterbauer et al. [13] proposed a heterophily belief propagation algorithm for homogeneous graphs with the same type of vertices (e.g., a user graph). Yamaguchi et al. [37] also proposed a heterophily propagation algorithm that connects to random walk and Gaussian Random Field. In both approaches, they assumed that even a user node is labeled as fraud, it can not simply propagate the fraud label to all its neighbors. However, their approach does not support vertex-level heterogeneity and use the same type of propagation over the entire network. In addition, the propagation matrices (e.g., diagonal matrix for homophily, off-diagonal matrix for heterophily) are required to be predefined based on observation, rather than to be automatically learned. In this work, we propose a unified label inference framework, which supports both vertex-level heterogeneity, and propagation-level heterophily (i.e., different types of propagation across heterogeneous relations). Furthermore, our framework is able to automatically learn the propagation matrices from the observed networks.\nCurrent researches have been focused on addressing algorithmic issues in the problem of label propagation such as incrementally and jointly learning for label inference. Gatterbauer et al. [13] proposed a heuristic incremental belief propagation algorithm with new data. However, more research is required in finding provable performance guarantee for the incremental algorithm. Chakrabarti et al. [7] proposed a framework with joint inference of label types such as hometown, current city, and employers, for users connected in a social network. To advance existing work, we propose an incremental framework which supports adaptive update upon both new data and labels. In addition, our algorithm is guaranteed to achieve speedup compared to re-computing algorithms with a certain confidence. It also supports multi-class label joint inference and provides a better understanding about the latent factors that cause link formations in the observed heterogeneous networks.\n2.2 K-partite Graphs K-partite graph analysis has wide applications in many domains such as topic modeling [27], community detection [29], and sentiment analysis [42]. Most of these works use tripartite graph modeling as a unified knowledge representation for heterogeneous data, and then formulate the real data analytic tasks as the corresponding tripartite graph clustering problem. For example, Long et al. [27] proposed a general model, the relation summary network, to find the hidden\nstructures (the local cluster structures and the global community structures) from a K-partite graph; Zhu et al. [42] addressed both static tripartite graph clustering and online tripartite graph clustering with matrices co-factorization. There are other works which study theoretical issues such as competition numbers of tripartite graphs [22].\nTo summarize, K-partite graph modeling and analysis has been studied from different perspectives due to its potential in various important applications. Yet studies on learning with K-partite graph modeling are limited. In this work, we formulate a set of traditional classification tasks such as sentiment classification, topic categorization, and rating prediction as the label propagation problem on K-partite graphs. With the observed K-partite graphs, our label inference approach is able to obtain decent accuracy with very few labels."}, {"heading": "3. PROBLEM FORMULATION", "text": "Let t, t\u2032 be a specific vertex type. A K-partite graph G = < \u222aKt=1Vt, \u222aKt=1 \u222aKt\u2032=1 Ett\u2032 > has K types of vertices, and contains at most K(K \u2212 1)/2 two-way relations. We also use notation G to denote the adjacency matrix representation of a K-partite graph and use Gtt\u2032 to denote the sub graph/matrix induced by the set of t-type vertices Vt and the set of t\u2032-type vertices Vt\u2032 . For ease of presentation, Table 2 lists the notations we use throughout this paper.\nIntuitively, the purpose of label propagation on K-partite graph is to utilize observed labels of seed vertices (denoted as V L) to infer label information for remaining unlabeled vertices (denoted as V U ). For each vertex v, we use a column vector Y (v) \u2208 Rk\u00d71 to denote the probabilistic label assignment to vertex v and a matrix Y \u2208 Rn\u00d7k to denote the probabilistic label assignment for all the vertices, where n is number of nodes and k is number of labels. Therefore, our objective is to infer the Y matrix so that each unlabeled vertex v \u2208 V U obtains an accurate estimation of its ground truth label.\nWe first build intuition about our model using a running example shown in Fig. 3(a). Suppose we only observe the links and very few labels of vertices as shown in Fig. 3(b), the purpose of label propagation on tripartite graphs is then to utilize observed links to infer label information. We introduce a matrix B \u2208 Rk\u00d7k to represent the correlation between link formation and associated labels. Each entry B(li, lj) denotes the likelihood that a node labeled as li is connected with another node labeled as lj . Subsequently, in the label propagation process, each entry B(li, lj) also denotes a proportional propagation that indicates the relative influence of nodes with label li on another nodes with label lj . To keep consistent with the label propagation process, here we interpret B as the propagation matrix.\nNote that the B matrix is an arbitrary non-negative matrix without any restrictions. If the B matrix is diagonal (e.g., Bac from Va to Vc in Fig. 3(d)), the link formation is consistent with homophily assumption; while if the B matrix is off-diagonal, the link formation is more likely due to heterophily assumption. In addition, as shown in Fig. 3(d), the propagation matrix between a\u2212 and b\u2212 type vertices is different from that between a\u2212 and c\u2212 type vertices. We thus let Btt\u2032 denote the propagation matrix between t-type vertices and t\u2032-type vertices.\nGenerally speaking, if we have prior knowledge about propagation matrix B, then each unlabeled vertex could receive\nproportional propagation from very few labeled vertices according to B. We propose to infer the propagation matrix and label assignment via embedding. We embed each vertex into a unified space (for all types of vertices), where each dimension of the latent space denotes a specific label and the link probability of two vertices are correlated to their labels (i.e., distance in the latent space). Fig. 3(c) shows a simplified two-dimension space. With the label embedding, the label assignment and the correlation can be automatically learned.\nWith these intuitions, we focus on the following problem:\nProblem 1 (K-partite Graph Label Inference) Given a K-partite graph, a set of labels L, a set of seed labeled vertices V L with ground truth labels Y \u2217 \u2208 RnL\u00d7k (nL = |V L|), the goal is to infer the label assignment matrix Y and the propagation matrix B such that\narg min Y,B\u22650 { \u2211 t \u2211 t\u2032 6=t \u2016Gtt\u2032 \u2212 YtBtt\u2032Y Tt\u2032 \u20162F\n+ \u03b2 \u2211 u\u2208V L \u2016Y (u)\u2212 Y \u2217(u)\u20162F + \u03bbregularizer(G,Y )} (1)\nwhere t denotes the type of vertex, Yt denote a sub matrix of Y , which gives the label assignment for the set of t-type vertices Vt, \u03b2 and \u03bb are parameters that control the contribution of different terms, and regularizer(G,Y ) denotes a regularization approach such as graph regularization [5], sparsity [14], diversity [40], and complexity regularization.\nIn our objective function, the first term evaluates how well each B matrix represents the correlation between link formation G and associated labels Y , via computing the error between estimated link formation probability and the observed graph. The second term \u2211 u \u2016Y (u)\u2212Y\n\u2217(u)\u20162F gives penalty to seed vertices if their learned labels are far away from the ground truths. Note that the regularization term provides an add-on property that utilizes additional domainspecific knowledge to improve learning accuracy.\nIn conclusion, our problem definition has well addressed all the mentioned modeling challenges. Unfortunately, besides modeling challenges, there are several computational\nchallenges remaining to be solved. First, the NP-hardness of Problem 1 (the sub problem of nonnegative matrix factorization is NP-hard [36]) requires efficient solutions for largescale real problems. Second, the rapid growth of data and feedbacks requires fast incremental update. In the following, we address those computational challenges by developing efficient algorithms that are highly scalable and achieve high quality in terms of classification accuracy for real-life tasks."}, {"heading": "4. LABEL INFERENCE ALGORITHMS", "text": "In this section, we discuss the algorithmic issues in label inference process. Specifically, our label inference problem leads to an optimization problem, which searches for the optimum weighted propagation matrix B and label assignment matrix Y to minimize the objective function in Eq. (1). Considering these nonnegative constraints, different approaches [18, 26, 21] have been proposed to solve this non-convex optimization problem. Among them, the multiplicative update [25] and additive update rules [26] are two most popular approaches because of their effectiveness. The multiplicative update approach batch updates each entry of the matrix by multiplying a positive coefficient at each iteration, while the additive update approach is a project gradient descent method. To the best of our knowledge, there is no existing work that combines different update rules in a unified framework.\nIn this paper, we propose to combine these two update rules in a unified framework. We observe that the typical multiplicative update rule that batch updates each entry of the matrix, can be transferred to a vertex-centric rule that corresponds to update each row of the matrix per iteration. This transformation allows us to unify both rules under the same vertex-centric label propagation framework because many addictive rules are updating each row per iterations [26, 21]. We notice that multiplicative and addictive update rules share some common computations. Consequently by pre-computing these common terms, our framework can be independent to various different update rules, while remains as efficient as possible. The proposed framework enjoys three important by-products: (1) supporting fair comparison between different update rules, (2) one unified incremental algorithm in Section 5 naturally support various update rules, (3) easy to parallel because the updates of each vertex can be performed at the same time.\nAlgorithm 1 presents the framework of our vertex-centric label inference algorithm, which consists of three steps: initialization, update for the propagation matrix B, and up-\nAlgorithm 1 The unified label inference framework Input: Graph G, a few ground truths Y \u2217 Output: Label Matrix Y 01: Initialize Y and B (see Section 4.1) 02: repeat 03: update Btt\u2032 (see Section 4.3) 04: update common terms At (see Eq. (6)) /\u2217 Vertex-centric search (block coordinate search)\u2217/ 05: for each vertex u \u2208 V 06: update and/or normalize Y (u) (see Section 4.2) 07: Ys = Y ; 08: Y = arg minY ||Ys \u2212 Y ||2F + \u03bbregularizer(G, Y ) 09: until converges 10: return Y\ndate for the label assignment of each vertex Y (u). We first provide a non-negative initialization for both Y and B in Section 4.1. We next iteratively update B (Section 4.3) and Y (Section 4.2) until the solution converges (Lines 2\u201310) with both multiplicative and addictive rules. Because the computational cost is dominated by updating the label assignments for all the vertices (Lines 5\u20136). To reduce the computational cost, when updating label assignment matrix Y in Section 4.2, we design an efficient cache technique that pre-computes and reuses common terms shared by the same type vertices (i.e., At for each t-type vertex) for both update rules. We show that by pre-computing At, the computational time for each Yt decreases, consequently the computational cost per iteration is much reduced.\nIn the following we first present the details of three components in our unified algorithm framework from Section 4.1\u2013 Section 4.3 , we then show the equivalence between elementwise multiplicative and vertex-centric multiplicative, and analyze the theoretical properties of different update rules in Section 4.4."}, {"heading": "4.1 Initialization", "text": "To achieve a better local optimum, a label inference algorithm should start from one or more relatively good initial guesses. In this work, we focus on graph proximity based initialization for label assignment matrix Y ; while B matrix is initialized using observed label propagation information among labeled seed vertices.\nInitializing Y Given the set of labeled vertices V L with ground truth label Y \u2217 \u2208 Rnl\u00d7k, we utilize the graph proximity to initialize the unlabeled vertices with similar labeled and the same-type vertices. Specifically, the label assignment matrix Y 0 can be initialized as follows:\nY 0(u) = Y \u2217(u) if u \u2208 V L avg\nv\u2208Vt(u) sim(u, v,G)Y (v) otherwise (2)\nwhere sim(u, v) evaluates the graph proximity between vertices u and v. In our experiments, we define sim(u, v,G) as the normalized Admic-Adar score [2]. In order to evaluate the similarity between two vertices, we sum the number of neighbors the two vertices have in common. Neighbors that are unique to a few vertices are weighted more than commonly occurring neighbors. That is,\nsim(u, v,G) = \u2211\nw\u2208(N(u)\u2229N(v))\n1\nlog d(w) (3)\nWe first compute the sim(u, v,G) with Eq. (3), and then normalize all the scores into the range [0, 1]. Here d(u) is the degree of vertex u, and N(u) is the set of neighbors of vertex u.\nInitializing B For each Btt\u2032 , we initialize it based on label class information and vertex type information of the observed labeled vertices. Specifically, we first initialize each Btt\u2032 as an identity matrix, where we assume that if type-t vertices are in li class, then all their connected type-t\n\u2032 vertices will receive corresponding li class label. We then increment Btt\u2032(li, lj) by one whenever we observe a type-t vertex labeled li is connected to another type-t\n\u2032 vertex labeled lj . Finally, we normalize each Btt\u2032 using L1 norm."}, {"heading": "4.2 Update Y", "text": "As introduced earlier, we perform vertex-centric update for Y . That is, in each iteration, we focus on minimizing the following sub objective:\nJ(Y (u)) = \u2211\nv\u2208N(u) (G(u, v)\u2212 Y (u)TBt(u)t(v)Y (v))2\n+ \u2211\nv 6\u2208N(u),t(v)6=t(u) (Y (u)TBt(u)t(v)Y (v)) 2\n+ 1V L (u)\u03b2\u2016Y (u)\u2212 Y \u2217(u)\u20162F\n(4)\nwhere 1A(x) is the indicator function, which is one if x \u2208 A and zero otherwise, N(u) is the neighbors of vertex u.\nIn the following, we adopt two representative update rules, multiplicative rule and additive rule, to derive the optimal solution for Eq. (17).\nLemma 1 ( Multiplicative rule for Y) Y can be approximated via the following multiplicative update rule:\nY (u) = Y (u)\u25e6\u221a\u2211 v\u2208N(u) G(u, v)Bt(u)t(v)Y (v) + \u03b21V L(u)Y \u2217(u)\nAt(u)Y (u) + \u03b21V L(u)Y (u) +\n(5)\nwhere > 0 is a very small positive value (e.g., 1\u22129), and At is defined as follows:\nAt = \u2211 v 6\u2208Vt Btt(v)Y (v)Y (v) TBTtt(v) (6)\nProof : The proof can be derived in spirit of the classic multiplicative algorithm [25] for Non-negative matrix factorization with the KKT condition [23]. Details are presented in Appendix 7.1.\nLemma 2 ( Additive rule for Y) An alternative approximate solution to Y can be derived via the following additive rule:\nY (u)r+1 = max( , Y (u)r + 2\u03b7( \u2211\nv\u2208N(u)\nG(u, v)Bt(u)t(v)Y (v)\n\u2212AtY (u) + \u03b21V L(u)(Y \u2217(u)\u2212 Y (u)r)))\n(7) where \u03b7 is the step size, and At is defined in Eq. (6).\nProof : It can be easily derived by replacing the deviation of J(Y (u)) into the standard gradient descent update rule.\nStep size for additive rule. We use Nesterov\u2019s method [17],[28] and [43] to compute the step size \u03b7, which can be estimated using the Lipschitz constant L for \u2207J(Y (u)), see Appendix 7.2.\nTable 3: Time complexity of basic operators, where n is number of nodes, m is number of edges, and k is number of label classes.\nB Y At Multi O(n+m)k O(n+m)k\n\u2211\nt\u2032 6=t nt\u2032k 2\nAddti O(n+m)k O(n+m)k \u2211 t\u2032 6=t nt\u2032k 2"}, {"heading": "4.3 Update B", "text": "In the following, we present the detailed update rules for propagation B.\nLemma 3 ( Multiplicative rule for B) B can be derived via the following update rule:\nBtt\u2032 = Btt\u2032 \u25e6\n\u221a Y Tt Gtt\u2032Yt\u2032\nY Tt YtBtt\u2032Y T t\u2032 Yt\u2032 +\n(8)\nProof : Proof of this Lemma is similar to Lemma 1, as shown in Appendix 7.3.\nLemma 4 ( Additive rule for B) An alternative approximate solution to B can be derived via the following additive rule:\nBtt\u2032 = max( , Btt\u2032 + 2\u03b7b(Y T t Gtt\u2032Yt\u2032 \u2212 Y Tt YtBtt\u2032Y Tt\u2032 Yt\u2032))\n(9) where \u03b7b again denotes the step size.\nProof : It can be easily derived by replacing the deviation of J(Btt\u2032) into the standard gradient descent update rule.\nSimilar to the computation of \u03b7 with Nesterov\u2019s gradient method, \u03b7b can be computed with Lipschitz constant Lb= 2\u2016Y Tt\u2032 Yt\u2032Y Tt Yt\u2016F for \u2207J(Btt\u2032), see Appendix 7.4."}, {"heading": "4.4 Comparison and Analysis", "text": "We first show that the solution Y returned by the proposed multiplicative rule is identical to that by the traditional multiplicative rule proved in the following lemma.\nLemma 5 Updating label assignment Y vertex by vertex using Lemma 1 is identical to the following traditional multiplicative rule [42]:\nYt = Yt \u25e6\n\u221a \u2211 t\u2032 6=tGtt\u2032YtB\nT tt\u2032 + \u03b2StY0\u2211\nt\u2032 6=t YtBtt\u2032Y T t\u2032 Yt\u2032B T tt\u2032 + \u03b2StYt\nwhere S \u2208 Rn\u00d7n is the label indicator matrix, of which Suu = 1 if u \u2208 V L and zero for all the other entries, and St is the sub matrix of S for t-type vertices.\nProof : The detailed proof is shown in Appendix 7.5. We then analyze the time complexity of computing each basic operator in a single iteration. As outlined in Table 3, each basic operator can be computed efficiently in real sparse networks. In addition, because of our pre-computation of At, multiplicative and additive update rules have the same near-linear time computational cost in a single iteration, which is much smaller than many traditional multiplicative rules for Y . For example, if we apply the multiplicative rule proposed by Zhu et. al. [42], it leads to O(nanbk+ nanck+ nbnck) computation complexity per iteration. Convergence. Let us first examine the general convergence of our label inference framework. Based on Corollary 1,\n2 and 3 [21], any limited point of the sequence generated by Algorithm 1 reaches the stationary point if the update rules remain non-zero and achieve optimum. Therefore, in general, if any update rule in Algorithm 1 is optimum for both sub objectives, it leads to the stationary point.\nWe next analyze the convergence properties when using both multiplicative update rules and additive update rules. Although both the multiplicative updating rules and additive rules used in this work are not optimum for subproblems, they still have very nice convergence properties. As proved on Theorem 2.4 [6], the proposed additive rules still converge into a stationary point. For the multiplicative rule, we conclude that using the proposed multiplicative updating rules guarantees that the value of objective function is non-increasing, and thus the algorithm converges into a local optima. This is because the proposed multiplicative rules are identical to the traditional multiplicative rules as proved in Lemma 6, and Zhu et al. [42] have proved that the value of objective function is non-increasing with the traditional multiplicative rules."}, {"heading": "5. INCREMENTAL LABEL INFERENCE", "text": "In this section, we present our solution to the fundamental research question with practical importance: How can we support fast incremental updates upon graph updates such as new labels and/or new node/edges? This is because in practice graphs and labels are continuously changing. In the following, we first develop an incremental algorithm that adaptively updates label assignment upon new data, where we can control the trade-off between efficiency and accuracy. We then further explore another interesting question: on which condition it is faster to perform incremental update than recomputing from scratch? To address this issue, we propose a utility function that examines the reward and cost of both update operations. With the utility function, our framework is able to automatically determine the \u201cbest\u201d strategy based on different levels of changes."}, {"heading": "5.1 Incremental Update Algorithm", "text": "Our incremental update algorithm supports both graph changes and label changes. The first scenario includes vertex/edge insertion and deletion; while the label changes include receiving additional labels, or correction of noise labels. With the proliferation of crowdsourcing, whenever we get additional explicit labels from users; or we identify noise labels based on user feedback, we can update label assignment for each vertex.\nA simple approach to deal with graph/label updates is to re-run our label inference algorithm for the entire graph. However, this can be computationally expensive. We thus propose the incremental algorithm, where we perform partial updates instead of full updates for each vertex. Our incremental algorithm is built upon the intuition that even with the graph and label changes, the majority of vertices tend to keep the label assignments or will not change much. Therefore, we perform a \u201dlazy\u201d adjustment, i.e., utilizing the old label assignment and updating a small candidate set of change vertices. Unfortunately, it is very challenging to design an efficient and effective incremental label propagation algorithm because in label propagation the adjustment of existing vertices will affect its neighbors, and even vertices that are far away. This requires us to propose an effective\nAlgorithm 2 Incremental label Inference algorithm with changes Input: Graph G, old label matrix Y , a few ground truths Y \u2217, confidence level \u03b8 \u2208 [0, 1), and changes \u2206V Output: New label matrix Yn 01: cand=\u2206V 02: for each u \u2208 G 03: Yn(u) = Y (u) 04: wtt\u2032 = avg(u,v)\u2208Ett\u2032 Y (u)TBtt\u2032Y (v),\n\u03c3tt\u2032 = std(u,v)\u2208Ett\u2032Y (u) TBtt\u2032Y (v)\n05: repeat 06: for each vertex u \u2208 cand 07: update Yn(u) (see Section 4.2) 08: for each v \u2208 N(u), v 6\u2208 cand 09: if |Y Tn (u)Bt(u)t(v)Yn(v)\u2212 wt(u)t(v)| \u2265 \u221a 1 1\u2212\u03b8\u03c3t(u)t(v) 10: cand = cand \u222a {v} 11: until Yn converges 12: return Yn\nstrategy to choose which subset of vertices to be adjusted to guarantee the performance gain of the incremental algorithm. Let us use the following example to illustrate more about the challenge.\nExample 1 Fig. 4 shows an example of initializing the candidate set of vertices to be adjusted when graph receive updates. Two vertices 9 and 10 are inserted with new formed edges into the tripartite graph shown in Fig. 3 (a), which consequently leads to vertices 5, 6, 8 receive new links from vertices 9 and 10. Therefore, the subset of vertices {5, 6, 8, 9, 10} (i.e., the subgraph bounded by the red box) receive graph changes and their label embedding require to be updated. However, updating the position of vertex 6 in the latent space might cause the change of that of vertex 2, or even all of the remaining vertices. It is unclear to what extent we should prorogate those changes: Too aggressive leads to an entire update (equivalent to the recomputing) while too conservative leads to great loss in accuracy.\nOverview of the incremental algorithm. We develop an incremental algorithm based on the mean field theory, which assumes that the effect of all the other individuals on any given individual is approximated by a single averaged effect. Hence we choose the candidate set of vertices based on how much they differ from the averaged effect. The overall process is outlined in Algorithm 2. We first identify a small portion of changes \u2206V as our intial candidate set of vertices cand (Line 3), where \u2206V denote the set of changed vertices (e.g., {5, 6, 8, 9, 10} in Fig. 4), including new and deleted vertices, vertices that have new or deleted edges, and vertices\nwhich receive updated ground truth labels. Next, we iteratively perform a conditioned label assignment update for each vertex in the candidate set (Lines 6\u20137), as well as an update for candidate vertices set cand (Lines 8\u201310). When updating the candidate vertices, we include one neighbor of an existing candidate vertex into cand only if it satisfies the conditions (Line 9) that are based on the pre-computed value of w and \u03b4 (Lines 4\u20135). The w exactly denotes the averaged effect of any given individual and \u03b4 denotes the standard deviation of effects of any given individual, and \u03b8 is a confidence level parameter for the trade-off between efficiency and accuracy. We here make an assumption that the propagation matrix B is inherent property and can be accurately estimated by the sampled old data (i.e., B is not changing with new data). The details of candidate vertex update are presented as follows.\nUpdate of candidate vertices set. The set of candidate vertices (cand) denotes a subset of vertices, where the label assignment requires update due to graph/label changes. Basically, for each current vertex u in cand, we update its label assignment and examine its propagation behavior over its neighbor vertex v. Intuitively, the modification of one vertex can cause the relation to its neighbors adjusted, whereas the general behavior between the corresponding two types of vertices should not change much. More specifically, let the wtt\u2032 = avg(u,v)\u2208Ett\u2032 Y (u)TBtt\u2032Y (v) denote the averaged effect of any individual between t\u2212 and t\u2032\u2212 type, and let the \u03c3tt\u2032 = std(u,v)\u2208Ett\u2032Y (u) TBtt\u2032Y (v) denote the standard deviation of effects between t\u2212 and t\u2032\u2212 type individuals. If the estimated effect between u and v after adjustment, significantly differs from the averaged effect (i.e., wt(u)t(v)) within the same types, we add vertex v into cand. The significance is evaluated using a threshold that consists of the confidence parameter \u03b8 and \u03c3tt\u2032 .\nExample 2 Consider again the example shown in Fig. 4, the graph changes activate the changes of embedded positions of vertices {5, 6, 8, 9, 10} (i.e., vertices in the box). The movement of changed vertices in the label embedding space, consequently causes their neighbor vertices to leave old embedded positions. For example, if vertex 6 is moved further to the red axis, its neighbor vertex 2 might be required to move away from the green axis too. Similarly, in Fig. 5 when vertex 7 receives a new label, vertex 7 are pushed closer to the green axis, which further influences the movement of both vertex 1 and vertex 4. Meanwhile, the new label also strengthens the green label propagation to vertex 1.\nConfidence level parameter \u03b8 for speed up. We now discuss the effect of parameter \u03b8 that controls the percentage of neighbors that avoid label update in each iteration (Line\n9). A larger value of \u03b8 indicates that more neighbors are filtered out for update, thus leading to a higher confidence that the incremental algorithm is more efficient than a recomputing approach. One nice property of Algorithm 2 is that it can bound the number of candidate vertices requiring label assignment update in each iteration. In particular, we present the following theorem that provides the efficiency guarantee for our incremental algorithm.\nTheorem 1 In each iteration, the probability Pc that a neighbor of any candidate vertex requires label assignment update is lower bounded by 1\u2212 \u03b8. That is : Pc \u2264 1\u2212 \u03b8.\nProof (sketch): Let X denote a random variable, which represents the value of Y (u)TBt(u)t(v)Y (v) for any pair of linked vertices (u, v) between t-type vertices and t\u2032-type vertices. Then wtt\u2032 is the average value of X, and \u03c3 2 tt\u2032 is the variance of X. Based on Chebyshev\u2019s inequality, ifX is a random variable with finite expected value u and finite non-zero variance \u03c32. Then for any real number q > 0, we have:\nPr(|X \u2212 u| \u2265 q\u03c3) \u2264 1\nq2 (10) Let us replace u by wtt\u2032 , \u03c3 by \u03c3tt\u2032 , q by \u221a\n1/(1\u2212 \u03b8), we have:\nPr(|X \u2212 wtt\u2032 | \u2265 \u221a 1/(1\u2212 \u03b8)\u03c3tt\u2032 ) \u2264 (1\u2212 \u03b8) (11)\nEq. (11) exactly examines the maximum bound of the probability of reaching Line 9 in Algorithm 2. Since the probability of reaching Line 9 in Algorithm 2, is identical to the probability that a neighbor of any candidate vertex requires label assignment update, we complete the proof.\nIn conclusion, the parameter \u03b8 roughly provides a utility that controls the estimated speed up of an incremental algorithm toward a re-computing approach. A larger value of \u03b8 indicates that more neighbors are filtered out for update, thus leading to a higher confidence that the incremental algorithm is more efficient than a re-computing approach. On another hand, a larger value of \u03b8 leads to fewer updates of neighbors and subsequently lower accuracy. From this perspective, the parameter \u03b8 allows us to choose a good tradeoff between computational cost and solution accuracy."}, {"heading": "5.2 To Re-compute or Not?", "text": "In the online label inference, we continually receive new data and/or new evidence (labels). Conditioning on the new data and new evidence, we have two choices: we can recompute the label assignment for all the vertices, using full label inference; or, we can fix some of the previous results, and only update a certain subset of the vertices using an incremental algorithm. To understand the consequences of using an incremental algorithm, we must answer a basic question: how much accuracy loss are incurred and how much speed up are achieved by an incremental algorithm compared to a re-computing approach?\nWe thus define the utility function for the incremental algorithm fI as follows:\nU(fI) = usGain(fI , fR)\u2212 uaLoss(fI , fR) (12)\nwhere Gain(fI , fR) = |G\u222a\u2206G|\n(2\u2212\u03b8)|\u2206G| is the the computational\ngain achieved by the incremental algorithm fI compared to the re-computing algorithm fR, \u2206G is the subgraph induced by the changed vertices \u2206V , Loss(fI , fR) is the information loss of using incremental algorithm fI instead of\nre-computing approach fR, us is the reward unit for speed up and ua is the reward unit for accuracy.\nWe next examine where the information loss of an incremental algorithm comes. Basically, the accuracy loss of the incremental algorithm comes from two parts: the loss on V \u2212 due to fixing their label assignments, and the loss on V + due to the approximating label assignments for V + with fixed previous assigned labels for V \u2212. Therefore, we have:\nLoss(fI , fR) = D(Y |fR, Y |fI , V +) +D(Y |fR, Yo, V \u2212) (13) where Y |f denotes the label assignments using label inference operator f , Yo denotes the previous assigned labels, and D(Y1, Y2, V ) denotes the label assignment differences of vertices set V between two assignments Y1 and Y2.\nUnfortunately, it is non-trivial to examine differences between label assignments by a re-computing and those by an incremental algorithm. This is a chicken and egg paradox: one wants to decide which algorithm to use based on information loss (label assignments differences); while without applying both algorithms, one can not get an accurate understanding about the differences. In order to proceed, we estimate the information loss by examining the similarity between old data and new data, which is inspired by the concept drift modeling [12, 45] for supervised learning. The intuition is that if the distribution of new data significantly varies from that of old data, an incremental update results in higher information loss; while the new data are very similar to old data, an incremental update leads to much less information loss. Specifically, we use the graph proximity heuristic defined in Eq. (3) to simulate the similarity between new data and old data, and consequently the information loss, which leads to the following equations:\nD(Y |fR, Y |fI , V +) =| avg u\u2208V\u2212,v\u2208V + sim(u, v,G \u222a\u2206G)\n\u2212 avg u\u2208V\u2212,v\u2208V +\nsim(u, v,\u2206G)| (14)\nD(Y |fR, Yo, V \u2212) =| avg u\u2208V L,v\u2208V\u2212 sim(u, v,G \u222a\u2206G)\n\u2212 avg u\u2208V L,v\u2208V\u2212\nsim(u, v,G)| (15)\nThough information loss might happen when using an incremental algorithm, much less time is consumed in computation compared to any re-computing operation, especially when the percentage of changes is very small."}, {"heading": "6. EXPERIMENTS", "text": ""}, {"heading": "6.1 Datasets and Settings", "text": "We evaluate the proposed approaches on four real datasets with three different classification tasks: sentiment classification, topic classification and rating classification. Among the four datasets, Prop 30 and Prop 37 are two tripartite graphs created from 2012 November California Ballot Twitter Data [42], each of which consists of tweet vertices, user vertices, and word vertices. The label classes are sentiments: positive, negative, and neutral. The PubMed dataset [24] is represented as a tripartite graph with three types of vertices: papers, reviewers, and words. Each paper/reviewer is associated with multiple labels, which denote the set of subtopics. The MovieLen dataset [31] represents the folksonomy information among users, movies, and tags. Each movie vertex is associated with a single class label. Here\nwe use three coarse-grain rating classes, \u201cgood\u201d, \u201cneutral\u201d, and \u201cbad\u201d as the ground truth labels. A short description of each dataset is summarized in Table 4.\nLet MRG/ARG denote the multiplicative/additive rule update with graph heuristic initialization. We compare our approaches with three baselines: GRF [44], MHV [9] and BHP [13]. GRF is the most representative traditional label propagation algorithm (i.e., no B matrices or B matrices are identity matrices), MHV is selected as a representative method that supports vertex-level heterogeneity (B matrices are diagonal), and BHP denotes the label propagation algorithm that allows propagation-level heterophily and utilizes a single matrix B. For all of these approaches, we begin with the same initialized state, and we use the same regularizations/or no regularizations for all approaches. Note that our goal in this paper is not to justify the performance of semi-supervised learning for different classification tasks (various surveys have justified the advantage of semisupervised learning with fewer labeled data), but rather to propose a better semi-supervised label propagation algorithm for tripartite graphs. Therefore, we do not compare our approaches with other supervised methods such as support vector machine.\nWe evaluate the effectiveness of each approach in terms of classification accuracy. Specifically, we select [1%, 5%, 10%] of vertices with ground truth labels as the set of seed labeled vertices, and then run different label propagation algorithms over the entire network to label the remaining vertices. Note that the selection of seed labeled vertices is not the focus of this work, and thus we simply adopt the degree centrality to select the top [1%, 5%, 10%] of vertices as seed nodes. For both single- and multi-class labels, we assign label li to a vertex u if Y (u, li) > 1/k and validate the results with ground truth labels. We represent classification results as a contingency matrix A, with Aij for i, j \u2208 L = {l1, \u00b7 \u00b7 \u00b7 , lk} where k is the number of class labels, and Aij is the number of times that a vertex of true label li is classified as label lj . With the contingency matrix A, the classification accuracy\nis defined as: Accuracy = \u2211\ni Aii\u2211 ij Aij .\nThe classification accuracy can not well evaluate the performance if label distribution is skewed. Therefore, we also use Balanced Error Rate [30] to evaluate the classification quality. The balanced error rate is defined as BER = 1 \u2212 1 k \u2211 i Aii\u2211 j Aij .\nAll the algorithms were implemented in Java 8 in a PC with i7-3.0HZ CPU and 8G memory.\n0.2\n0.4\n0.6\n0.8\nProp 30 Prop 37 MovieLen PubMed\nA cc\nur ac\ny\nMRG ARG GRF MHV BHP\n0.2\n0.4\n0.6\n0.8\nProp 30 Prop 37 MovieLen PubMed\nB E\nR\nMRG ARG GRF MHV BHP"}, {"heading": "6.2 Static Approach Evaluation", "text": "In this section, we evaluate the performance of Algorithm 1 in terms of convergence, efficiency and classification accuracy.\nQuestion 1 Accuracy: How does Algorithm 1 perform compared to the baselines?\nResult 1 Algorithm 1 outperforms the baselines in terms of classification accuracy and balanced error rate when data exhibit heterophily and/or with multi-class labels.\nFig. 6 reports the classification accuracy and balanced error rate comparisons for all the approaches. Here we set the convergence tolerance value to 10\u22126. The parameter \u03b2 is set to 5. This is because in our preliminary experiments, we notice that the accuracy increases when \u03b2 is increased from 1 to 5, but after that, increasing \u03b2 does not lead to significant changes in accuracy. Clearly, our approaches MRG and ARG, performs much better than the other approaches on Prop 30, MovieLen, PubMed, and perform similarly with GRF on Prop 37. The results validate the advantage of inferred B matrix in terms of supporting vertex-level heterogeneity, propagation-level heterogeneity and the multi-class label propagation.\nFor example, Prop 37 is about labeling genetically engineered foods, and the majority of people have positive attitude. As shown in Fig. 7 (b), all of the B matrices learned by ARG are diagonal matrices, which illustrates that the link formation exactly follows homophily assumption. Hence, on Prop 37, forcing B matrices as identity matrices such as GRF obtains comparable quality with our approaches. Moreover, although both PubMed (see Fig. 7 (d)) and Prop\n37 exactly follow homophily assumption, the proposed approaches perform better than other approaches on PubMed. This is because PubMed has multi-class labels and our approaches well support multi-class label propagation compared to other approaches. On Prop 30 and MovieLen, the B matrices are a mixture of different forms of matrices. Under this situation, our approaches MRG and ARG perform better than all the approaches including BHP (using a single arbitrary B matrix).\nQuestion 2 Convergence: Do multiplicative and additive rules in Algorithm 1 converge?\nResult 2 Both multiplicative and additive rules are guaranteed to converge into local optima and their convergence rate in terms of objective values are much faster than the baselines.\nInstead of fixing convergence tolerance value, now we fix the maximum iteration number to 100, and validate the convergence performance of the proposed algorithms. Fig. 8 shows that the objective values are non-increasing using both multiplicative rules and additive rules. In addition, the proposed algorithms decrease the objective value much faster than other algorithms.\nQuestion 3 Efficiency: Are the proposed algorithms scalable?\nResult 3 In each iteration, the proposed algorithms MRG and ARG require more computational cost than the baselines. However, since they converge much faster than other approaches, the total running time of the proposed algorithms, are still faster than the baselines.\nWe again fix the convergence tolerance value to 10\u22126, and report the average running time per iteration and the total\nrunning time in Fig. 9. Because on average they converge 2-3 times faster than all the baselines, computationally expensive per iteration due to the incurred additional cost for computing the propagation matrices B, the proposed algorithms are still very efficient for large-scale data.\nQuestion 4 Regularization: What is the effect of the graph regularization?\nResult 4 We validate that graph regularization term is helpful for sentiment classification tasks on Prop 30 and Prop 37.\nIntuitively user-to-user graph (e.g., friendship graph) or document graph (e.g., citation graph) will be very helpful for labeling users or documents. Unfortunately, we do not have such graphs for MovieLen and PubMed. Therefore, although our framework is very general and supports various regularization, we only compare the classification accuracy w/o graph regularization on Prop 30 and prop 37 (i.e., the two data sets that have additional user to user re-tweeting graphs). With the additional regularization, the classification accuracy increases by 6.1% on Prop 30 and 2% on Prop 37.\nQuestion 5 MRG V.S. ARG: Is MRG perform better than ARG, or vice verse?\nResult 5 The two update rules exhibit similar behaviors in terms of accuracy, convergence and running time.\nInterestingly, we observe that there is no clear winner between the two update rules. The result demonstrates that our unified algorithm can serve as a framework for comparison between different update rules."}, {"heading": "6.3 Incremental Approach Evaluation", "text": "We justify the advantage of our incremental approaches in terms of guaranteed speed up and decent classification\nProp 30 Prop 37 MovieLen PubMed\nProp 30 Prop 37 MovieLen PubMed\na\n.\nquality compared to the re-computing approach. In the recomputing approach, we apply Algorithm 1 to the entire network, on top of old label assignment results. Since the purpose of this group of experiments is to evaluate the incremental framework, not the inference algorithm, we simply choose a representative algorithm MRG in all the following experiments.\nQuestion 6 In terms of both efficiency and quality, what is the effect of confidence parameter \u03b8?\nResult 6 The running time speed up increases with \u03b8, especially when \u03b8 >50%. On the contrary, the accuracy decreases with \u03b8.\nWe first fix the percentage of new data as 10%, vary the confidence parameter \u03b8 from 0.1 to 0.9, and present the results in Fig. 10. Overall the running time speed up is increasing with \u03b8, though the curves have some zigzags due to the ignored effect of \u03b8 on convergence speed. When \u03b8 is greater than 0.5, the running time speed up increases significantly. On the other hand, we do lose information by skipping updates for 100(1-\u03b8)% of neighbors and thus the accuracy loss is also increased with \u03b8 but much more slowly.\nQuestion 7 What is the effect of new data percentage?\nWe next fix the confidence parameter \u03b8 as 0.5, vary the percentage of new data from 10% to 90%, and report the running time and accuracy in Fig. 11. The results clearly indicate that the relative speed up is around 3.5 to 6 when we apply the incremental approach with 10% of new data. When the percentage of new data is larger than that of old data, the relative speed up decreases to around 2.4 to 3.4. We initially expect that the accuracy loss might also increase when the new data become dominant. However, Fig. 11 reports that on Prop 37 and PubMed, the accuracy of applying incremental updates on 90% new data is similar to that on 10% new data (or even better than). Therefore, it is non-trivial to make a simple decision when and on what condition we should favor the incremental approach purely based on the percentage of new data. We should further examine the similarity between old and new data, as suggested by the utility function defined in Eq. (12).\nQuestion 8 Is the utility function helpful in making decisions about when we should use the incremental approach?\nResult 8 The priority of an incremental approach, is correlated with its utility value. A high utility score for an incremental approach suggests a stronger preference to the incremental approach.\nIn this set of experiments, we evaluate the goodness of utility functions. We firstassume that accuracy loss is more concerned and thus we set ua = 100 and us = 60. We randomly select 10 different subsets of 10% new data on Prop 37, and then apply both incremental algorithm and recomputing algorithm. For the confidence parameter in the incremental algorithm, we vary it from 0.1 to 0.5. The average running time speed up, the average accuracy loss, the average and standard deviation of utility function are reported in Table 6. Clearly, the utility function is highly correlated with the accuracy loss: a higher utility score leads to less loss in accuracy. Therefore, once we fixed the parameters in the utility function according to application requirements, we are able to safely decide whether we should use the incremental approach based on the utility scores. For example, in Table 6, a straightforward strategy is that if the utility score is greater than 200, we opt to use an incremental approach;\notherwise, a re-computing approach is preferred. Moreover, the results also indicate that the percentage of new data is not a good utility measure: We have the same percentage of new data, but accuracy loss, speed up and utility scores vary significantly. We have also changed the setting to ua = 60 and us = 100 (presented in Table 7 in Appendix), and the results show that the utility function is highly correlated with the speed up."}, {"heading": "7. CONCLUSIONS AND FUTURE WORKS", "text": "In this work, we studied the problem of label propagation in K-partite graphs. We proposed a rich label propagation model that supports both heterogeneity and heterophily propagation, by allowing two connected nodes of different types have either similar or opposite labels. We developed a unified label inference framework with two representative update rules. In order to support the dynamic property of real networks, we further presented a fast algorithm, which incrementally assigns labels to new data, or updates old labels according to new feedbacks. Instead of using the percentage of new data, a utility function was further designed to determine when incremental approach is favored.\nIn the future, we plan to develop an effective adaptive seeding approach, which selects the minimum number of vertices to be labeled but achieves highest guaranteed accuracy. This will greatly reduce the effort and expense of human labeling.\nAppendix"}, {"heading": "7.1 Proof of Lemma 1", "text": "We first consider the general solution to Eq. (1) without the regularization function. With the observed graph structure, the objective in Eq. (1) without regularization term can be rewritten as the following dual form:\narg min Y,B { \u2211 (u,v)\u2208E (G(u, v)\u2212 Y (u)TBt(u)t(v)Y (v))2\n+ \u2211\n(u,v) 6\u2208E, t(u)6=t(v)\n(Y (u)TBt(u)t(v)Y (v)) 2\n+ \u03b2 \u2211 u\u2208V L \u2016Y (u)\u2212 Y \u2217(u)\u20162F }\n(16)\nwhere we separate the heterogenous node pairs into two parts: linked parts and non-linked parts. Note that homogeneous node pairs (pairs of nodes of the same type) are omitted since they are always unlinked in the K-partite graph.\nAs introduced earlier, we perform vertex-centric update for Y . That is, in each iteration, we focus on minimizing the following sub objective:\nJ(Y (u)) = \u2211\nv\u2208N(u) (G(u, v)\u2212 Y (u)TBt(u)t(v)Y (v))2\n+ \u2211\nv 6\u2208N(u),t(v)6=t(u) (Y (u)TBt(u)t(v)Y (v)) 2\n+ 1V L (u)\u03b2\u2016Y (u)\u2212 Y \u2217(u)\u20162F\n(17)\nwhere 1A(x) is the indicator function, which is one if x \u2208 A and zero otherwise.\nWe introduce the Largrangian multiplier \u039b for non-negative constraint (i.e., Y (u) \u2265 0) in Eq. (17), which leads to the following Largrangian function J (Y (u)):\nJ (Y (u)) = \u2211\nv\u2208N(u) (G(u, v)\u2212 Y (u)TBt(u)t(v)Y (v))2\n+ \u2211\nv 6\u2208N(u),t(v)6=t(u) (Y (u)TBt(u)t(v)Y (v)) 2\n+ 1V L (u)\u2016Y (u)\u2212 Y \u2217(u)\u20162F + tr(\u039bY (u)Y (u) T )\nThe next step is to optimize the above terms w.r.t. Y (u). We set \u2207Y (u)=0, and obtain: \u039bY (u) =\u2212 2( \u2211\nv 6\u2208Vt(u)\nBt(u)t(v)Y (v)Y (v) TBTt(u)t(v) + 1V L (u)I k)Y (u)\n+ 2(1V L (u)Y \u2217(u) + \u2211 v\u2208N(u) G(u, v)Bt(u)t(v)Y (v))\nUsing the KKT condition \u039bY (u) \u25e6 Y (u)=0 [23], where \u25e6 denotes the element-wise multiplicative, we obtain:\n[\u2212( \u2211\nv 6\u2208Vt(u)\nBt(u)t(v)Y (v)Y (v) TBTt(u)t(v) + 1V L (u)I k)Y (u))\n+ (1V L (u)Y \u2217(u) + \u2211 v\u2208N(u) G(u, v)Bt(u)t(v)Y (v))] \u25e6 Y (u) = 0\nFollowing the updating rules proposed and proved in [8] [16] [42], we have:\nY (u) = Y (u)\u25e6\u221a\u221a\u221a\u221a\u2211v\u2208N(u)G(u, v)Bt(u)t(v)Y (v) + 1V L (u)Y \u2217(u) At(u)Y (u) + 1V L (u)Y (u) +\nwhere > 0 is a very small positive value (e.g., 1\u22129), and At is defined as follows:\nAt = \u2211 v 6\u2208Vt Btt(v)Y (v)Y (v) TBTtt(v)\nNote that in the above equation and in all of the following equations, we add a very small positive value into denominator to avoid zero division.\nThis completes the proof.\n7.2 Lipschitz constant for \u2207J(Y (u)) Given a convex function f , \u2207 f is Lipschitz continuous on Dom h if:\n||\u2207f(x)\u2212\u2207f(y)||2 \u2264 L||x\u2212 y||2, \u2200 x, y in Dom h\nTherefore, given any two y1 and y2, which denotes two different values for Y (u), we have:\n||\u2207J(y1)\u2212\u2207J(y2)||2 =||2At(y1 \u2212 y2)||2 \u2264 ||2At||2||y1 \u2212 y2||2\nWe then have ||\u2207J(y1)\u2212\u2207J(y2)||F \u2264 2||At||F ||y1\u2212y2||F , and thus the lipschitz constant for \u2207J(Y (u)) is 2||At||F ."}, {"heading": "7.3 Proof of Lemma 3", "text": "Similar to the proof for Lemma 1, we first introduce the Largrangian multiplier \u039b for non-negative constraint (i.e.,\nB \u2265 0), which leads to the following Largrangian function J (B):\nJ (Btt\u2032 ) = \u2016Gtt\u2032 \u2212 YtBtt\u2032Y Tt\u2032 \u2016 2 + tr(\u039bBtt\u2032B T tt\u2032 ) (18)\nThe next step is to optimize the above terms w.r.t. Btt\u2032 . We set \u2207J (Btt\u2032)=0, and obtain:\n\u039bBtt\u2032 = 2Y T t Gtt\u2032Yt\u2032 \u2212 2Y Tt YtBtt\u2032Y Tt\u2032 Yt\u2032\nWith the K.K.T. condition [23], \u039bBtt\u2032 \u25e6Btt\u2032=0, we have:\n(Y Tt Gtt\u2032Yt\u2032 \u2212 Y Tt YtBtt\u2032Y Tt\u2032 Yt\u2032) \u25e6Btt\u2032 = 0\nFollowing the updating rules proposed and proved in [8] [16] [42], we have:\nBtt\u2032 = Btt\u2032 \u25e6\n\u221a Y Tt Gtt\u2032Yt\u2032\nY Tt YtBtt\u2032Y T t\u2032 Yt\u2032\n(19)\nThis completes the proof."}, {"heading": "7.4 Lipschitz constant for \u2207J(B)", "text": "Similar to the proof shown in Section 7.2, given any B1, B2 representing different values of Btt\u2032 , we have:\n||\u2207J(B1)\u2212\u2207J(B2)||2 =||2Y Tt YtB1Y Tt\u2032 Yt\u2032 \u2212 2Y T t YtB2Y T t\u2032 Yt\u2032 ||2 \u2264tr(2Y Tt\u2032 Yt\u2032Y T t Yt)||B1 \u2212B2||2\nWe then have ||\u2207J(B1)\u2212\u2207J(B2)||F \u2264 2||Y Tt\u2032 Yt\u2032Y Tt Yt||F ||B1\u2212 B2||F , and thus the lipschitz constant is 2||Y Tt\u2032 Yt\u2032Y Tt Yt||F ."}, {"heading": "7.5 Proof of Lemma 6", "text": "Lemma 6 Updating label assignment Y vertex by vertex using Lemma 1 is identical to the following traditional multiplicative rule [42]:\nYt = Yt \u25e6\n\u221a \u2211 t\u2032 6=tGtt\u2032YtB\nT tt\u2032 + \u03b2StY0\u2211\nt\u2032 6=t YtBtt\u2032Y T t\u2032 Yt\u2032B T tt\u2032 + \u03b2StYt\nwhere S \u2208 Rn\u00d7n is the label indicator matrix, of which Suu = 1 if u \u2208 V L and zero for all the other entries, and St is the sub matrix of S for t-type vertices.\nSubstituting the multipliers in the preliminary update rule proposed by Zhu et. al. [42], we obtain an optimization algorithm which iterates the following multiplicative update rule for Yt:\nYt = Yt \u25e6\n\u221a \u2211 t\u2032 6=tGtt\u2032Yt\u2032B\nT tt\u2032 + StY0\u2211\nt\u2032 6=t YtBtt\u2032Y T t\u2032 Yt\u2032B T tt\u2032 + StYt\nNote that for each vertex u, we have Y (u) = col(Y T , u), where col(A, i) denotes the specific ith-column of a matrix A. Therefore, we have Y (u):\n=Y (u) \u25e6 \u221a\u221a\u221a\u221a col(\u2211t\u2032 6=t(u)Btt\u2032Y Tt\u2032 GTt(u)t\u2032 , u) + 1V L (u)Y \u2217(u) col( \u2211 t\u2032 6=t(u)(Btt\u2032Y T t\u2032 Yt\u2032B T tt\u2032 ) TY T t(u) , u) + 1V L (u)Y (u)\n=Y (u) \u25e6 \u221a\u221a\u221a\u221a \u2211t\u2032 6=t(u\u2032) Btt\u2032Y Tt\u2032 col(GTt(u)t\u2032 , u) + 1V L (u)Y \u2217(u)\u2211 t\u2032 6=t(u)(Btt\u2032Y T t\u2032 Yt\u2032B T tt\u2032 ) T col(Y T t(u) , u) + 1V L (u)Y (u)\n=Y (u) \u25e6 \u221a\u221a\u221a\u221a \u2211v\u2208N(u) G(u, v)Bt(u)t(v)Y (v) + 1V L (u)Y \u2217(u)\u2211 v 6\u2208Vt(u) Bt(u)t(v)Y (v)Y (v) TBT t(u)t(v) Y (u) + 1V L (u)Y (u)\nCache the term \u2211 v 6\u2208Vt Btt(v)Y (v)Y (v)\nTBTtt(v) as At, and add to the denominator, we have:\nY (u) = Y (u) \u25e6\n\u221a\u2211 v\u2208N(u) G(u, v)Bt(u)t(v)Y (v) + 1V L(u)Y \u2217(u)\nAt(u)Y (u) + 1V L(u)Y (u) +\nThis is identical to the update rule proposed in Lemma 1 and thus it completes the proof."}, {"heading": "7.6 Additional tables", "text": "Table 7 reports additional results on utility functions."}, {"heading": "Acknowledgment", "text": "We are very grateful to Dr. Kristina Lerman, and Dr. Wolfgang Gatterbauer for their insightful discussions."}, {"heading": "8. REFERENCES", "text": "[1] J. Abernethy, O. Chapelle, and C. Castillo. Graph\nregularization methods for web spam detection. Machine Learning, 81(2):207\u2013225, 2010.\n[2] L. A. Adamic and E. Adar. Friends and neighbors on the web. SOCIAL NETWORKS, 25:211\u2013230, 2001.\n[3] S. Amer-Yahia, M. Fernandez, R. Greer, and D. Srivastava. Logical and physical support for heterogeneous data. In CIKM Conference, pages 270\u2013281. ACM, 2002.\n[4] A. Blum, J. Lafferty, M. R. Rwebangira, and R. Reddy. Semi-supervised learning using randomized mincuts. In ICML Conference, 2004.\n[5] D. Cai, X. He, J. Han, and T. S. Huang. Graph regularized nonnegative matrix factorization for data representation. IEEE Trans. Pattern Anal. Mach. Intell., 33(8):1548\u20131560, 2011.\n[6] P. H. Calamai and J. J. More\u0301. Projected gradient methods for linearly constrained problems. Math. Program., 39(1):93\u2013116, 1987.\n[7] D. Chakrabarti, S. Funiak, J. Chang, and S. A. Macskassy. Joint inference of multiple label types in large networks. In ICML Conference, pages 874\u2013882, 2014.\n[8] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal nonnegative matrix t-factorizations for clustering. In SIGKDD Conference, pages 126\u2013135. ACM, 2006.\n[9] C. Ding, T. Li, and D. Wang. Label propagation on k-partite graphs. In ICMLA Conference, pages 273\u2013278, 2009.\n[10] C. Faloutsos. Large graph mining: Patterns, cascades, fraud detection, and algorithms. In WWW Conference, pages 1\u20132, 2014.\n[11] P. F. Felzenszwalb and D. P. Huttenlocher. Efficient belief propagation for early vision. International journal of computer vision, 70(1):41\u201354, 2006.\n[12] J. a. Gama, I. Z\u030cliobaite\u0307, A. Bifet, M. Pechenizkiy, and A. Bouchachia. A survey on concept drift adaptation. ACM Comput. Surv., 46(4):44:1\u201344:37, 2014.\n[13] W. Gatterbauer, S. Gu\u0308nnemann, D. Koutra, and C. Faloutsos. Linearized and single-pass belief propagation. Proc. VLDB Endow., 8(5):581\u2013592, 2015.\n[14] S. Gilpin, T. Eliassi-Rad, and I. Davidson. Guided learning for role discovery (glrd): Framework, algorithms, and applications. In SIGKDD Conference, pages 113\u2013121, 2013.\n[15] A. B. Goldberg and X. Zhu. Seeing stars when there aren\u2019t many stars: graph-based semi-supervised learning for sentiment categorization. In Graph Based Methods for Natural Language Processing, pages 45\u201352, 2006.\n[16] Q. Gu and J. Zhou. Co-clustering on manifolds. In SIGKDD Conference, pages 359\u2013368. ACM, 2009.\n[17] N. Guan, D. Tao, Z. Luo, and B. Yuan. Nenmf: An optimal gradient method for nonnegative matrix factorization. IEEE Trans. on Signal Processing, pages 2882\u20132898, 2012.\n[18] N.-D. Ho. NONNEGATIVE MATRIX FACTORIZATION ALGORITHMS AND APPLICATIONS. PhD thesis, 2008.\n[19] A. T. Ihler, J. Iii, and A. S. Willsky. Loopy belief propagation: Convergence and effects of message errors. In Journal of Machine Learning Research, pages 905\u2013936, 2005.\n[20] Y. Jacob, L. Denoyer, and P. Gallinari. Learning latent representations of nodes for classifying in heterogeneous social networks. In WSDM Conference, pages 373\u2013382, 2014.\n[21] J. Kim, Y. He, and H. Park. Algorithms for nonnegative matrix and tensor factorizations: A unified view based on block coordinate descent framework. J. of Global Optimization, 58(2):285\u2013319, 2014.\n[22] S.-R. Kim and Y. Sano. The competition numbers of complete tripartite graphs. Discrete Applied Mathematics, 156(18):3522\u20133524, 2008.\n[23] H. W. Kuhn and A. W. Tucker. Nonlinear programming. In Proceedings of the 2nd Berkeley Symposium on Mathematical Statistics and Probability, pages 481\u2013492, 1950.\n[24] I. S. Laboratory. Computer department of sharif university of technology, pubmed dataset. http://isl.ce.sharif.edu/pubmed-dataset/.\n[25] D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In NIPS\nConference, pages 556\u2013562. MIT Press, 2000.\n[26] C.-J. Lin. Projected gradient methods for nonnegative matrix factorization. Neural Comput., 19(10):2756\u20132779, 2007.\n[27] B. Long, X. Wu, Z. M. Zhang, and P. S. Yu. Unsupervised learning on k-partite graphs. In SIGKDD Conference, pages 317\u2013326, 2006.\n[28] Y. Nesterov. Introductory lectures on convex optimization : a basic course. Kluwer Academic Publ., 2004.\n[29] Y. Pei, N. Chakraborty, and K. P. Sycara. Nonnegative matrix tri-factorization with graph regularization for community detection in social networks. In IJCAI, pages 2083\u20132089. AAAI Press, 2015.\n[30] I. Read and S. Cox. Automatic pitch accent prediction for text-to-speech synthesis. In Interspeech, pages 482\u2013485, 2007.\n[31] S. Sen, J. Vig, and J. Riedl. Tagommenders: Connecting users to items through tags. In WWW Conference, pages 671\u2013680, 2009.\n[32] C. Shi, Y. Li, J. Zhang, Y. Sun, and P. S. Yu. A survey of heterogeneous information network analysis. CoRR, abs/1511.04854, 2015.\n[33] A. Subramanya and J. Bilmes. Semi-supervised learning with measure propagation. J. Mach. Learn. Res., 12:3311\u20133370, 2011.\n[34] P. P. Talukdar and K. Crammer. New regularized algorithms for transductive learning. In Machine Learning and Knowledge Discovery in Databases, pages 442\u2013457. Springer, 2009.\n[35] J. Ugander and L. Backstrom. Balanced label propagation for partitioning massive graphs. In WSDM Conference, pages 507\u2013516, 2013.\n[36] S. A. Vavasis. On the complexity of nonnegative matrix factorization. J. on Optimization, 20(3):1364\u20131377, 2009.\n[37] Y. Yamaguchi, C. Faloutsos, and H. Kitagawa. Omni-prop: Seamless node classification on arbitrary label correlation. In AAAI Conference, pages 3122\u20133128. AAAI Press, 2015.\n[38] P. Yang and J. He. A graph-based hybrid framework for modeling complex heterogeneity. In ICDM Conference, pages 1081\u20131086, 2015.\n[39] J. S. Yedidia, W. T. Freeman, and Y. Weiss. Understanding belief propagation and its generalizations. Exploring artificial intelligence in the new millennium, 2003.\n[40] C. Yu, L. V. Lakshmanan, and S. Amer-Yahia. Recommendation diversification using explanations. In IEEE 25th International Conference on Data Engineering, pages 1299\u20131302, 2009.\n[41] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Scho\u0308lkopf. Learning with local and global consistency. In NIPS Conference, pages 321\u2013328, 2003.\n[42] L. Zhu, A. Galstyan, J. Cheng, and K. Lerman. Tripartite graph clustering for dynamic sentiment analysis on social media. In SIGMOD Conference, pages 1531\u20131542, 2014.\n[43] L. Zhu, D. Guo, J. Yin, G. V. Steeg, and A. Galstyan. Scalable temporal latent space inference for link\nprediction in dynamic social networks. IEEE Transactions on Knowledge and Data Engineering, 28(10):2765\u20132777, Oct 2016.\n[44] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In ICML Conference, pages 912\u2013919, 2003.\n[45] I. Z\u030cliobaite\u0307. Learning under concept drift: an overview. arXiv preprint arXiv:1010.4784, 2010."}], "references": [{"title": "Graph regularization methods for web spam detection", "author": ["J. Abernethy", "O. Chapelle", "C. Castillo"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Friends and neighbors on the web", "author": ["L.A. Adamic", "E. Adar"], "venue": "SOCIAL NETWORKS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Logical and physical support for heterogeneous data", "author": ["S. Amer-Yahia", "M. Fernandez", "R. Greer", "D. Srivastava"], "venue": "In CIKM Conference,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Semi-supervised learning using randomized mincuts", "author": ["A. Blum", "J. Lafferty", "M.R. Rwebangira", "R. Reddy"], "venue": "In ICML Conference,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Graph regularized nonnegative matrix factorization for data representation", "author": ["D. Cai", "X. He", "J. Han", "T.S. Huang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Projected gradient methods for linearly constrained problems", "author": ["P.H. Calamai", "J.J. Mor\u00e9"], "venue": "Math. Program.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1987}, {"title": "Joint inference of multiple label types in large networks", "author": ["D. Chakrabarti", "S. Funiak", "J. Chang", "S.A. Macskassy"], "venue": "In ICML Conference,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Orthogonal nonnegative matrix t-factorizations for clustering", "author": ["C. Ding", "T. Li", "W. Peng", "H. Park"], "venue": "In SIGKDD Conference,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Label propagation on k-partite graphs", "author": ["C. Ding", "T. Li", "D. Wang"], "venue": "In ICMLA Conference,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Large graph mining: Patterns, cascades, fraud detection, and algorithms", "author": ["C. Faloutsos"], "venue": "In WWW Conference,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Efficient belief propagation for early vision", "author": ["P.F. Felzenszwalb", "D.P. Huttenlocher"], "venue": "International journal of computer vision,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "A survey on concept drift adaptation", "author": ["J. a. Gama", "I. \u017dliobait\u0117", "A. Bifet", "M. Pechenizkiy", "A. Bouchachia"], "venue": "ACM Comput. Surv.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Linearized and single-pass belief propagation", "author": ["W. Gatterbauer", "S. G\u00fcnnemann", "D. Koutra", "C. Faloutsos"], "venue": "Proc. VLDB Endow.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Guided learning for role discovery (glrd): Framework, algorithms, and applications", "author": ["S. Gilpin", "T. Eliassi-Rad", "I. Davidson"], "venue": "In SIGKDD Conference,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Seeing stars when there aren\u2019t many stars: graph-based semi-supervised learning for sentiment categorization", "author": ["A.B. Goldberg", "X. Zhu"], "venue": "In Graph Based Methods for Natural Language Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Co-clustering on manifolds", "author": ["Q. Gu", "J. Zhou"], "venue": "In SIGKDD Conference,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Nenmf: An optimal gradient method for nonnegative matrix factorization", "author": ["N. Guan", "D. Tao", "Z. Luo", "B. Yuan"], "venue": "IEEE Trans. on Signal Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "NONNEGATIVE MATRIX FACTORIZATION ALGORITHMS AND APPLICATIONS", "author": ["N.-D. Ho"], "venue": "PhD thesis,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Loopy belief propagation: Convergence and effects of message errors", "author": ["A.T. Ihler", "J. Iii", "A.S. Willsky"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Learning latent representations of nodes for classifying in heterogeneous social networks", "author": ["Y. Jacob", "L. Denoyer", "P. Gallinari"], "venue": "In WSDM Conference,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Algorithms for nonnegative matrix and tensor factorizations: A unified view based on block coordinate descent framework", "author": ["J. Kim", "Y. He", "H. Park"], "venue": "J. of Global Optimization,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "The competition numbers of complete tripartite graphs", "author": ["S.-R. Kim", "Y. Sano"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Nonlinear programming", "author": ["H.W. Kuhn", "A.W. Tucker"], "venue": "In Proceedings of the 2nd Berkeley Symposium on Mathematical Statistics and Probability,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1950}, {"title": "Algorithms for non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "In NIPS  Conference,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2000}, {"title": "Projected gradient methods for nonnegative matrix factorization", "author": ["C.-J. Lin"], "venue": "Neural Comput.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Unsupervised learning on k-partite graphs", "author": ["B. Long", "X. Wu", "Z.M. Zhang", "P.S. Yu"], "venue": "In SIGKDD Conference,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Introductory lectures on convex optimization : a basic course", "author": ["Y. Nesterov"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2004}, {"title": "Nonnegative matrix tri-factorization with graph regularization for community detection in social networks. In IJCAI, pages 2083\u20132089", "author": ["Y. Pei", "N. Chakraborty", "K.P. Sycara"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Automatic pitch accent prediction for text-to-speech synthesis", "author": ["I. Read", "S. Cox"], "venue": "In Interspeech,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "Tagommenders: Connecting users to items through tags", "author": ["S. Sen", "J. Vig", "J. Riedl"], "venue": "In WWW Conference,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "A survey of heterogeneous information network analysis", "author": ["C. Shi", "Y. Li", "J. Zhang", "Y. Sun", "P.S. Yu"], "venue": "CoRR, abs/1511.04854,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Semi-supervised learning with measure propagation", "author": ["A. Subramanya", "J. Bilmes"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "New regularized algorithms for transductive learning", "author": ["P.P. Talukdar", "K. Crammer"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2009}, {"title": "Balanced label propagation for partitioning massive graphs", "author": ["J. Ugander", "L. Backstrom"], "venue": "In WSDM Conference,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "On the complexity of nonnegative matrix factorization", "author": ["S.A. Vavasis"], "venue": "J. on Optimization,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2009}, {"title": "Omni-prop: Seamless node classification on arbitrary label correlation", "author": ["Y. Yamaguchi", "C. Faloutsos", "H. Kitagawa"], "venue": "In AAAI Conference,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "A graph-based hybrid framework for modeling complex heterogeneity", "author": ["P. Yang", "J. He"], "venue": "In ICDM Conference,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Understanding belief propagation and its generalizations", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "Exploring artificial intelligence in the new millennium,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2003}, {"title": "Recommendation diversification using explanations", "author": ["C. Yu", "L.V. Lakshmanan", "S. Amer-Yahia"], "venue": "In IEEE 25th International Conference on Data Engineering,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2009}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "In NIPS Conference,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2003}, {"title": "Tripartite graph clustering for dynamic sentiment analysis on social media", "author": ["L. Zhu", "A. Galstyan", "J. Cheng", "K. Lerman"], "venue": "In SIGMOD Conference,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Scalable temporal latent space inference for link 14  prediction in dynamic social networks", "author": ["L. Zhu", "D. Guo", "J. Yin", "G.V. Steeg", "A. Galstyan"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"], "venue": "In ICML Conference,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2003}, {"title": "Learning under concept drift: an overview", "author": ["I. \u017dliobait\u0117"], "venue": "arXiv preprint arXiv:1010.4784,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2010}], "referenceMentions": [{"referenceID": 42, "context": "Label propagation [44] is one of the classic algorithms to learn the label information for each vertex in a network (or graph).", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "Recently, label propagation has received renewed interests from both academia and industry due to its various applications in many domains such as in spam detection [1], fraud detection [10], sentiment analysis [15], and graph partitioning [35].", "startOffset": 165, "endOffset": 168}, {"referenceID": 9, "context": "Recently, label propagation has received renewed interests from both academia and industry due to its various applications in many domains such as in spam detection [1], fraud detection [10], sentiment analysis [15], and graph partitioning [35].", "startOffset": 186, "endOffset": 190}, {"referenceID": 14, "context": "Recently, label propagation has received renewed interests from both academia and industry due to its various applications in many domains such as in spam detection [1], fraud detection [10], sentiment analysis [15], and graph partitioning [35].", "startOffset": 211, "endOffset": 215}, {"referenceID": 33, "context": "Recently, label propagation has received renewed interests from both academia and industry due to its various applications in many domains such as in spam detection [1], fraud detection [10], sentiment analysis [15], and graph partitioning [35].", "startOffset": 240, "endOffset": 244}, {"referenceID": 10, "context": "ent algorithms [11, 19, 33, 39, 41, 44] have been proposed to perform label propagation on trees or arbitrary graphs.", "startOffset": 15, "endOffset": 39}, {"referenceID": 18, "context": "ent algorithms [11, 19, 33, 39, 41, 44] have been proposed to perform label propagation on trees or arbitrary graphs.", "startOffset": 15, "endOffset": 39}, {"referenceID": 31, "context": "ent algorithms [11, 19, 33, 39, 41, 44] have been proposed to perform label propagation on trees or arbitrary graphs.", "startOffset": 15, "endOffset": 39}, {"referenceID": 37, "context": "ent algorithms [11, 19, 33, 39, 41, 44] have been proposed to perform label propagation on trees or arbitrary graphs.", "startOffset": 15, "endOffset": 39}, {"referenceID": 39, "context": "ent algorithms [11, 19, 33, 39, 41, 44] have been proposed to perform label propagation on trees or arbitrary graphs.", "startOffset": 15, "endOffset": 39}, {"referenceID": 42, "context": "ent algorithms [11, 19, 33, 39, 41, 44] have been proposed to perform label propagation on trees or arbitrary graphs.", "startOffset": 15, "endOffset": 39}, {"referenceID": 2, "context": "Unfortunately, many real networks such as social networks are heterogeneous systems [3, 32, 38] that contain objects of multiple types and are interlinked via various relations.", "startOffset": 84, "endOffset": 95}, {"referenceID": 30, "context": "Unfortunately, many real networks such as social networks are heterogeneous systems [3, 32, 38] that contain objects of multiple types and are interlinked via various relations.", "startOffset": 84, "endOffset": 95}, {"referenceID": 36, "context": "Unfortunately, many real networks such as social networks are heterogeneous systems [3, 32, 38] that contain objects of multiple types and are interlinked via various relations.", "startOffset": 84, "endOffset": 95}, {"referenceID": 40, "context": "In addition, the label information of users is much more reliable than that of words in terms of deciding the labels of tweets [42], and thus the user vertices should have stronger propagation strengths than word vertices.", "startOffset": 127, "endOffset": 131}, {"referenceID": 8, "context": "There are few notable exceptions [9, 13, 20] (see more in related works) that support either heterogeneous types of vertices or heterophily propagation, but not both.", "startOffset": 33, "endOffset": 44}, {"referenceID": 12, "context": "There are few notable exceptions [9, 13, 20] (see more in related works) that support either heterogeneous types of vertices or heterophily propagation, but not both.", "startOffset": 33, "endOffset": 44}, {"referenceID": 19, "context": "There are few notable exceptions [9, 13, 20] (see more in related works) that support either heterogeneous types of vertices or heterophily propagation, but not both.", "startOffset": 33, "endOffset": 44}, {"referenceID": 23, "context": "To infer our model, we first propose a framework that supports both multiplicative [25] and addictive rules [18] (i.", "startOffset": 83, "endOffset": 87}, {"referenceID": 17, "context": "To infer our model, we first propose a framework that supports both multiplicative [25] and addictive rules [18] (i.", "startOffset": 108, "endOffset": 112}, {"referenceID": 3, "context": "Method H-V H-P Auto Incre Joint [4] [11] [33] [34] X X X X X [39] [41] [44] [7] X X X X X [9] [20] X X X X ? [13] [37] X X X X X Proposed X X X X X", "startOffset": 32, "endOffset": 35}, {"referenceID": 10, "context": "Method H-V H-P Auto Incre Joint [4] [11] [33] [34] X X X X X [39] [41] [44] [7] X X X X X [9] [20] X X X X ? [13] [37] X X X X X Proposed X X X X X", "startOffset": 36, "endOffset": 40}, {"referenceID": 31, "context": "Method H-V H-P Auto Incre Joint [4] [11] [33] [34] X X X X X [39] [41] [44] [7] X X X X X [9] [20] X X X X ? [13] [37] X X X X X Proposed X X X X X", "startOffset": 41, "endOffset": 45}, {"referenceID": 32, "context": "Method H-V H-P Auto Incre Joint [4] [11] [33] [34] X X X X X [39] [41] [44] [7] X X X X X [9] [20] X X X X ? [13] [37] X X X X X Proposed X X X X X", "startOffset": 46, "endOffset": 50}, {"referenceID": 37, "context": "Method H-V H-P Auto Incre Joint [4] [11] [33] [34] X X X X X [39] [41] [44] [7] X X X X X [9] [20] X X X X ? [13] [37] X X X X X Proposed X X X X X", "startOffset": 61, "endOffset": 65}, {"referenceID": 39, "context": "Method H-V H-P Auto Incre Joint [4] [11] [33] [34] X X X X X [39] [41] [44] [7] X X X X X [9] [20] X X X X ? [13] [37] X X X X X Proposed X X X X X", "startOffset": 66, "endOffset": 70}, {"referenceID": 42, "context": "Method H-V H-P Auto Incre Joint [4] [11] [33] [34] X X X X X [39] [41] [44] [7] X X X X X [9] [20] X X X X ? [13] [37] X X X X X Proposed X X X X X", "startOffset": 71, "endOffset": 75}, {"referenceID": 6, "context": "Method H-V H-P Auto Incre Joint [4] [11] [33] [34] X X X X X [39] [41] [44] [7] X X X X X [9] [20] X X X X ? [13] [37] X X X X X Proposed X X X X X", "startOffset": 76, "endOffset": 79}, {"referenceID": 8, "context": "Method H-V H-P Auto Incre Joint [4] [11] [33] [34] X X X X X [39] [41] [44] [7] X X X X X [9] [20] X X X X ? [13] [37] X X X X X Proposed X X X X X", "startOffset": 90, "endOffset": 93}, {"referenceID": 19, "context": "Method H-V H-P Auto Incre Joint [4] [11] [33] [34] X X X X X [39] [41] [44] [7] X X X X X [9] [20] X X X X ? [13] [37] X X X X X Proposed X X X X X", "startOffset": 94, "endOffset": 98}, {"referenceID": 12, "context": "Method H-V H-P Auto Incre Joint [4] [11] [33] [34] X X X X X [39] [41] [44] [7] X X X X X [9] [20] X X X X ? [13] [37] X X X X X Proposed X X X X X", "startOffset": 109, "endOffset": 113}, {"referenceID": 35, "context": "Method H-V H-P Auto Incre Joint [4] [11] [33] [34] X X X X X [39] [41] [44] [7] X X X X X [9] [20] X X X X ? [13] [37] X X X X X Proposed X X X X X", "startOffset": 114, "endOffset": 118}, {"referenceID": 10, "context": "as belief propagation [11] [39], loopy belief propagation [19], Gaussian Random Field (GRF) [44], MP [33], MAD [34], and local consistency [41].", "startOffset": 22, "endOffset": 26}, {"referenceID": 37, "context": "as belief propagation [11] [39], loopy belief propagation [19], Gaussian Random Field (GRF) [44], MP [33], MAD [34], and local consistency [41].", "startOffset": 27, "endOffset": 31}, {"referenceID": 18, "context": "as belief propagation [11] [39], loopy belief propagation [19], Gaussian Random Field (GRF) [44], MP [33], MAD [34], and local consistency [41].", "startOffset": 58, "endOffset": 62}, {"referenceID": 42, "context": "as belief propagation [11] [39], loopy belief propagation [19], Gaussian Random Field (GRF) [44], MP [33], MAD [34], and local consistency [41].", "startOffset": 92, "endOffset": 96}, {"referenceID": 31, "context": "as belief propagation [11] [39], loopy belief propagation [19], Gaussian Random Field (GRF) [44], MP [33], MAD [34], and local consistency [41].", "startOffset": 101, "endOffset": 105}, {"referenceID": 32, "context": "as belief propagation [11] [39], loopy belief propagation [19], Gaussian Random Field (GRF) [44], MP [33], MAD [34], and local consistency [41].", "startOffset": 111, "endOffset": 115}, {"referenceID": 39, "context": "as belief propagation [11] [39], loopy belief propagation [19], Gaussian Random Field (GRF) [44], MP [33], MAD [34], and local consistency [41].", "startOffset": 139, "endOffset": 143}, {"referenceID": 19, "context": "[20] focused on learning the unified latent space representation through supervised learning for heterogeneous networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] proposed a cross propagation algorithm for K-partite graphs, which distinguishes vertices of different types, and propagates label information from vertices of one type to vertices of another type.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13] proposed a heterophily belief propagation algorithm for homogeneous graphs with the same type of vertices (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[37] also proposed a heterophily propagation algorithm that connects to random walk and Gaussian Random Field.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] proposed a heuristic incremental belief propagation algorithm with new data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] proposed a framework with joint inference of label types such as hometown, current city, and employers, for users connected in a social network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "2 K-partite Graphs K-partite graph analysis has wide applications in many domains such as topic modeling [27], community detection [29], and sentiment analysis [42].", "startOffset": 105, "endOffset": 109}, {"referenceID": 27, "context": "2 K-partite Graphs K-partite graph analysis has wide applications in many domains such as topic modeling [27], community detection [29], and sentiment analysis [42].", "startOffset": 131, "endOffset": 135}, {"referenceID": 40, "context": "2 K-partite Graphs K-partite graph analysis has wide applications in many domains such as topic modeling [27], community detection [29], and sentiment analysis [42].", "startOffset": 160, "endOffset": 164}, {"referenceID": 25, "context": "[27] proposed a general model, the relation summary network, to find the hidden structures (the local cluster structures and the global community structures) from a K-partite graph; Zhu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[42] addressed both static tripartite graph clustering and online tripartite graph clustering with matrices co-factorization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "There are other works which study theoretical issues such as competition numbers of tripartite graphs [22].", "startOffset": 102, "endOffset": 106}, {"referenceID": 4, "context": "where t denotes the type of vertex, Yt denote a sub matrix of Y , which gives the label assignment for the set of t-type vertices Vt, \u03b2 and \u03bb are parameters that control the contribution of different terms, and regularizer(G,Y ) denotes a regularization approach such as graph regularization [5], sparsity [14], diversity [40], and complexity regularization.", "startOffset": 292, "endOffset": 295}, {"referenceID": 13, "context": "where t denotes the type of vertex, Yt denote a sub matrix of Y , which gives the label assignment for the set of t-type vertices Vt, \u03b2 and \u03bb are parameters that control the contribution of different terms, and regularizer(G,Y ) denotes a regularization approach such as graph regularization [5], sparsity [14], diversity [40], and complexity regularization.", "startOffset": 306, "endOffset": 310}, {"referenceID": 38, "context": "where t denotes the type of vertex, Yt denote a sub matrix of Y , which gives the label assignment for the set of t-type vertices Vt, \u03b2 and \u03bb are parameters that control the contribution of different terms, and regularizer(G,Y ) denotes a regularization approach such as graph regularization [5], sparsity [14], diversity [40], and complexity regularization.", "startOffset": 322, "endOffset": 326}, {"referenceID": 34, "context": "First, the NP-hardness of Problem 1 (the sub problem of nonnegative matrix factorization is NP-hard [36]) requires efficient solutions for largescale real problems.", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "Considering these nonnegative constraints, different approaches [18, 26, 21] have been proposed to solve this non-convex optimization problem.", "startOffset": 64, "endOffset": 76}, {"referenceID": 24, "context": "Considering these nonnegative constraints, different approaches [18, 26, 21] have been proposed to solve this non-convex optimization problem.", "startOffset": 64, "endOffset": 76}, {"referenceID": 20, "context": "Considering these nonnegative constraints, different approaches [18, 26, 21] have been proposed to solve this non-convex optimization problem.", "startOffset": 64, "endOffset": 76}, {"referenceID": 23, "context": "Among them, the multiplicative update [25] and additive update rules [26] are two most popular approaches because of their effectiveness.", "startOffset": 38, "endOffset": 42}, {"referenceID": 24, "context": "Among them, the multiplicative update [25] and additive update rules [26] are two most popular approaches because of their effectiveness.", "startOffset": 69, "endOffset": 73}, {"referenceID": 24, "context": "This transformation allows us to unify both rules under the same vertex-centric label propagation framework because many addictive rules are updating each row per iterations [26, 21].", "startOffset": 174, "endOffset": 182}, {"referenceID": 20, "context": "This transformation allows us to unify both rules under the same vertex-centric label propagation framework because many addictive rules are updating each row per iterations [26, 21].", "startOffset": 174, "endOffset": 182}, {"referenceID": 1, "context": "In our experiments, we define sim(u, v,G) as the normalized Admic-Adar score [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 0, "context": "(3), and then normalize all the scores into the range [0, 1].", "startOffset": 54, "endOffset": 60}, {"referenceID": 23, "context": "Proof : The proof can be derived in spirit of the classic multiplicative algorithm [25] for Non-negative matrix factorization with the KKT condition [23].", "startOffset": 83, "endOffset": 87}, {"referenceID": 22, "context": "Proof : The proof can be derived in spirit of the classic multiplicative algorithm [25] for Non-negative matrix factorization with the KKT condition [23].", "startOffset": 149, "endOffset": 153}, {"referenceID": 16, "context": "We use Nesterov\u2019s method [17],[28] and [43] to compute the step size \u03b7, which can be estimated using the Lipschitz constant L for \u2207J(Y (u)), see Appendix 7.", "startOffset": 25, "endOffset": 29}, {"referenceID": 26, "context": "We use Nesterov\u2019s method [17],[28] and [43] to compute the step size \u03b7, which can be estimated using the Lipschitz constant L for \u2207J(Y (u)), see Appendix 7.", "startOffset": 30, "endOffset": 34}, {"referenceID": 41, "context": "We use Nesterov\u2019s method [17],[28] and [43] to compute the step size \u03b7, which can be estimated using the Lipschitz constant L for \u2207J(Y (u)), see Appendix 7.", "startOffset": 39, "endOffset": 43}, {"referenceID": 40, "context": "Lemma 5 Updating label assignment Y vertex by vertex using Lemma 1 is identical to the following traditional multiplicative rule [42]:", "startOffset": 129, "endOffset": 133}, {"referenceID": 40, "context": "[42], it leads to O(nanbk+ nanck+ nbnck) computation complexity per iteration.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Based on Corollary 1, 2 and 3 [21], any limited point of the sequence generated by Algorithm 1 reaches the stationary point if the update rules remain non-zero and achieve optimum.", "startOffset": 30, "endOffset": 34}, {"referenceID": 5, "context": "4 [6], the proposed additive rules still converge into a stationary point.", "startOffset": 2, "endOffset": 5}, {"referenceID": 40, "context": "[42] have proved that the value of objective function is non-increasing with the traditional multiplicative rules.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "In order to proceed, we estimate the information loss by examining the similarity between old data and new data, which is inspired by the concept drift modeling [12, 45] for supervised learning.", "startOffset": 161, "endOffset": 169}, {"referenceID": 43, "context": "In order to proceed, we estimate the information loss by examining the similarity between old data and new data, which is inspired by the concept drift modeling [12, 45] for supervised learning.", "startOffset": 161, "endOffset": 169}, {"referenceID": 40, "context": "Among the four datasets, Prop 30 and Prop 37 are two tripartite graphs created from 2012 November California Ballot Twitter Data [42], each of which consists of tweet vertices, user vertices, and word vertices.", "startOffset": 129, "endOffset": 133}, {"referenceID": 29, "context": "The MovieLen dataset [31] represents the folksonomy information among users, movies, and tags.", "startOffset": 21, "endOffset": 25}, {"referenceID": 42, "context": "We compare our approaches with three baselines: GRF [44], MHV [9] and BHP [13].", "startOffset": 52, "endOffset": 56}, {"referenceID": 8, "context": "We compare our approaches with three baselines: GRF [44], MHV [9] and BHP [13].", "startOffset": 62, "endOffset": 65}, {"referenceID": 12, "context": "We compare our approaches with three baselines: GRF [44], MHV [9] and BHP [13].", "startOffset": 74, "endOffset": 78}, {"referenceID": 28, "context": "Therefore, we also use Balanced Error Rate [30] to evaluate the classification quality.", "startOffset": 43, "endOffset": 47}, {"referenceID": 22, "context": "Using the KKT condition \u039bY (u) \u25e6 Y (u)=0 [23], where \u25e6 denotes the element-wise multiplicative, we obtain:", "startOffset": 41, "endOffset": 45}, {"referenceID": 7, "context": "Following the updating rules proposed and proved in [8] [16] [42], we have:", "startOffset": 52, "endOffset": 55}, {"referenceID": 15, "context": "Following the updating rules proposed and proved in [8] [16] [42], we have:", "startOffset": 56, "endOffset": 60}, {"referenceID": 40, "context": "Following the updating rules proposed and proved in [8] [16] [42], we have:", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "condition [23], \u039bBtt\u2032 \u25e6Btt\u2032=0, we have: (Y T t Gtt\u2032Yt\u2032 \u2212 Y T t YtBtt\u2032Y T t\u2032 Yt\u2032) \u25e6Btt\u2032 = 0", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "Following the updating rules proposed and proved in [8] [16] [42], we have:", "startOffset": 52, "endOffset": 55}, {"referenceID": 15, "context": "Following the updating rules proposed and proved in [8] [16] [42], we have:", "startOffset": 56, "endOffset": 60}, {"referenceID": 40, "context": "Following the updating rules proposed and proved in [8] [16] [42], we have:", "startOffset": 61, "endOffset": 65}, {"referenceID": 40, "context": "Lemma 6 Updating label assignment Y vertex by vertex using Lemma 1 is identical to the following traditional multiplicative rule [42]:", "startOffset": 129, "endOffset": 133}, {"referenceID": 40, "context": "[42], we obtain an optimization algorithm which iterates the following multiplicative update rule for Yt:", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "In this paper, for the first time, we study label propagation in heterogeneous graphs under heterophily assumption. Homophily label propagation (i.e., two connected nodes share similar labels) in homogeneous graph (with same types of vertices and relations) has been extensively studied before. Unfortunately, real-life networks are heterogeneous, they contain different types of vertices (e.g., users, images, texts) and relations (e.g., friendships, co-tagging) and allow for each node to propagate both the same and opposite copy of labels to its neighbors. We propose a K-partite label propagation model to handle the mystifying combination of heterogeneous nodes/relations and heterophily propagation. With this model, we develop a novel label inference algorithm framework with update rules in near-linear time complexity. Since real networks change over time, we devise an incremental approach, which supports fast updates for both new data and evidence (e.g., ground truth labels) with guaranteed efficiency. We further provide a utility function to automatically determine whether an incremental or a remodeling approach is favored. Extensive experiments on real datasets have verified the effectiveness and efficiency of our approach, and its superiority over the state-of-the-art label propagation methods.", "creator": "LaTeX with hyperref package"}}}