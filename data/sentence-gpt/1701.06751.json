{"id": "1701.06751", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jan-2017", "title": "Collective Vertex Classification Using Recursive Neural Network", "abstract": "Collective classification of vertices is a task of assigning categories to each vertex in a graph based on both vertex attributes and link structure. Nevertheless, some existing approaches do not use the features of neighbouring vertices properly, due to the noise introduced by these features. In this paper, we propose a graph-based recursive neural network framework for collective vertex classification and linked structures. It is possible for us to describe the potential features of each vertex in a graph based on both vertices and link structure. A network such as this would be useful to understand all the possible parameters related to each vertex. To define an algorithm for the individual vertices, we simply use the following structure:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 24 Jan 2017 07:07:15 GMT  (900kb,D)", "http://arxiv.org/abs/1701.06751v1", "7 pages, 5 figures"]], "COMMENTS": "7 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.SI", "authors": ["qiongkai xu", "qing wang", "chenchen xu", "lizhen qu"], "accepted": false, "id": "1701.06751"}, "pdf": {"name": "1701.06751.pdf", "metadata": {"source": "CRF", "title": "Collective Vertex Classification Using Recursive Neural Network", "authors": ["Qiongkai Xu", "Qing Wang", "Chenchen Xu", "Lizhen Qu"], "emails": ["Xu.Qiongkai@data61.csiro.au", "qing.wang@anu.edu.au", "Xu.Chenchen@data61.csiro.au", "Qu.Lizhen@data61.csiro.au"], "sections": [{"heading": "Introduction", "text": "In everyday life, graphs are ubiquitous, e.g. social networks, sensor networks, and citation networks. Mining useful knowledge from graphs and studying properties of various kinds of graphs have been gaining popularity in recent years. Many studies formulate their graph problems as predictive tasks such as vertex classification (London and Getoor 2014), link prediction (Tang et al. 2015), and graph classification (Niepert, Ahmed, and Kutzkov 2016).\nIn this paper, we focus on vertex classification task which studies the properties of vertices by categorising them. Algorithms for classifying vertices are widely adopted in web page analysis, citation analysis and social network analysis (London and Getoor 2014). Naive approaches for vertex classification use traditional machine learning techniques to classify a vertex only based on the attributes or features provided by this vertex, e.g. such attributes can be words of web pages or user profiles in a social network. Another series of approaches are collective vertex classification, where instances are classified simultaneously as opposed to independently. Based on the observation that information of neighbouring vertices may help classifying current vertex, some approaches incorporate attributes of neighbouring vertices into classification process, which\nhowever introduce noise at the same time and result in reduced performance (Chakrabarti, Dom, and Indyk 1998; Myaeng and Lee 2000). Other approaches incorporate the labels of its neighbours. For instance, the Iterative Classification Approach (ICA) integrates the label distribution of neighbouring vertices to assist classification (Lu and Getoor 2003) and the Label Propagation approach (LP) fine-tunes predictions of the vertex using the labels of its neighbouring vertices (Wang and Zhang 2008). However, labels of neighbouring vertices are not representative enough for learning sophisticated relationships of vertices, while using their attributes directly would involve noise. We thus need an approach that is capable of capturing information from neighbouring vertices, while in the mean time reducing noise of attributes. Utilizing representations learned from neural networks instead of neighbouring attributes or labels is one of the possible approaches (Schmidhuber 2015). As graphs normally provide rich structural information, we exploit the neural networks with sufficient complexity for capturing such structures.\nRecurrent neural networks were developed to utilize sequence structures by processing input in order, in which the representation of the previous node is used to generate the representation of the current node. Recursive neural networks exploit representation learning on tree structures. Both of the approaches achieve success in learning representation of data with implicit structures which indicate the order of processing vertices (Tang, Qin, and Liu 2015; Tai, Socher, and Manning 2015). Following these work, we explore the possibility of integrating graph structures into recursive neural network. However, graph structures, especially cyclic graphs, do not provide such an processing order. In this paper, we propose a graph-based recursive neural network (GRNN), which allows us to transform graph structures to tree structures and use recursive neural networks to learn representations for the vertices to classify. This framework consists of two main components, as illustrated in Figure 1:\n1. Tree construction: For each vertex to classify vt, we generate a search tree rooted at vt. Starting from vt, we add its neighbouring vertices into the tree layer by layer.\n2. Recursive neural network construction: We build a recursive neural network for the constructed tree, by aug-\nar X\niv :1\n70 1.\n06 75\n1v 1\n[ cs\n.L G\n] 2\n4 Ja\nn 20\n17\nmenting each vertex with one recursive neural unit. The inputs of each vertex are its features and hidden states of its child vertices. The output of a vertex is its hidden states.\nOur main contributions are: (1) We introduce recursive neural networks to solve the collective vertex classification problem. (2) We propose a method that can transfer vertices for classification to a locally constructed tree. Based on the tree, recursive neural network can extract representations for target vertices. (3) Our experimental results show that the proposed approach outperforms several baseline methods. Particularly, we demonstrate that including information from neighbouring vertices can improve performance of classification."}, {"heading": "Related Work", "text": "There has been a growing trend to represent data using graphs (Angles and Gutierrez 2008). Discovering knowledge from graphs becomes an exciting research area, such as vertex classification (London and Getoor 2014) and graph classification (Niepert, Ahmed, and Kutzkov 2016). Graph classification analyzes the properties of the graph as a whole, while vertex classification focuses on predicting labels of vertices in the graph. In this paper, we discuss the problem of vertex classification. The main-stream approaches for vertex classification are collective vertex classification (Lu and Getoor 2003) which classify vertices using information provided by neighbouring vertices. Iterative classification approach (Lu and Getoor 2003) models neighbours\u2019 label distribution as link features to facilitate classification. Label propagation approach (Wang and Zhang 2008) assigns a probabilistic label for each vertex and then fine-tunes the probability using graph structure. However, labels of neighbouring vertices are not representative enough to include all useful information. Some researchers tried to introduce attributes from neighbouring vertices to improve classification performance. Nevertheless, as reported in (Chakrabarti, Dom, and Indyk 1998; Myaeng and Lee 2000), naively incorporating these features may reduce the performance of classification, when original features of neighbouring vertices are too noisy.\nRecently, some researchers analysed graphs using deep\nneural network technologies. Deepwalk (Perozzi, Al-Rfou, and Skiena 2014) is an unsupervised learning algorithm to learn vertex embeddings using link structure, while content of each vertex is not considered. Convolutional neural network for graphs (Niepert, Ahmed, and Kutzkov 2016) learns feature representations for the graphs as a whole. Recurrent neural collective classification (Monner and Reggia 2013) encodes neighbouring vertices via a recurrent neural network, which is hard to capture the information from vertices that are more than several steps away.\nRecursive neural networks (RNN) are a series of models that deal with tree-structured information. RNN has been implemented in natural scenes parsing (Socher et al. 2011) and tree-structured sentence representation learning (Tai, Socher, and Manning 2015). Under this framework, representations can be learned from both input features and representations of child nodes. Graph structures are more widely used and more complicated than tree or sequence structures. Due to the lack of notable order for processing vertices in a graph, few studies have investigated the vertex classification problem using recursive neural network techniques. The graph-based recursive neural network framework proposed in this paper can generate the processing order for neural network according to the vertex to classify and the local graph structure."}, {"heading": "Graph-based Recursive Neural Networks", "text": "In this section, we present the framework of Graph-based Recursive Neural Networks (GRNN). A graph G = (V,E) consists of a set of vertices V = {v1, v2, . . . , vN} and a set of edges E \u2286 V \u00d7 V . Graphs may contain cycles, where a cycle is a path from a vertex back to itself. Let X = {x1, x2, \u00b7 \u00b7 \u00b7 , xN} be a set of feature vectors, where each xi \u2208 X is associated with a vertex vi \u2208 V , L be a set of labels, and vt \u2208 V be a vertex to be classified, called target vertex. Then, the collective vertex classification problem is to predict the label yt of vt, such that\ny\u0302t = argmax yt\u2208L\nP\u03b8(yt|vt, G,X ) (1)\nusing a recursive neural network with parameters \u03b8.\nAlgorithm 1 Tree Construction Algorithm Require: Graph G, Target vertex vt, Tree depth d Ensure: Tree T = (VT , ET )\n1. Let Q be a queue 2. VT = {vt}, ET = \u2205 3. Q.push(vt) 4. while Q is not empty do 5. v = Q.pop() 6. if T .depth(v) \u2264 d then 7. for w in G.outgoingVertices(v) do 8. // add w to T as child of v 9. VT = VT \u222a {w}\n10. ET = ET \u222a {(v, w)} 11. Q.push(w) 12. end for 13. end if 14. end while 15. return T"}, {"heading": "Tree Construction", "text": "In a neural network, neurons are arranged in layers and different layers are processed following a predefined order. For example, recurrent neural networks process inputs in sequential order and recursive neural networks deal with treestructures in a bottom-up manner. However, graphs, particularly cyclic graphs, do not have an explicit order. How to construct an ordered structure from a graph is challenging.\nGiven a graphG = (V,E), a target vertex vt \u2208 V and tree depth d \u2208 N, we can construct a tree T = (VT , ET ) rooted at vt using breadth-first-search, where VT is a vertex set, ET is an edge set, and (v, w) \u2208 ET means an edge from parent vertex v to child vertex w. The depth of a vertex v in T is the length of the path from vt to v, denoted as T .depth(v). The depth of a tree is maximum depth of vertices in T . We use G.outgoingVertices(v) to denote a set of outgoing vertices from v, i.e. {w|(v, w) \u2208 G}. The tree construction algorithm is described in Algorithm 1. Firstly, a first-in-first-out queue (Q) is initialized with vt (lines 1-3). The algorithm iteratively check the vertices in Q. If there is a vertex whose depth is less than d, we pop it out from Q (line 6), add all its neighbouring vertices in G as its children in T and push them to the end of Q (lines 9-11).\nIn general, there are two approaches to deal with cycles in a graph. One is to remove vertices that have been visited, and the other is to keep duplicate vertices. Fig 2.a describes an example of a graph with a cycle between v1 and v2. Let us start with the target vertex v1. In the first approach, there will be no child vertex for v2, since v1 is already visited. The corresponding tree is shown in Fig 2.b. In the second approach, we will add v1 as a child vertex to v2 iteratively and terminate after certain steps. The corresponding tree is illustrated in Fig 2.c. When generating the representation of a vertex, say v2, any information from its neighbours may help. We thus include v1 as a child vertex of v2. In this paper, we use the second manner for tree construction."}, {"heading": "Recursive Neural Network Construction", "text": "Now we construct a recursive neural unit (RNU) for each vertex vk \u2208 T . Each RNU takes a feature vector xk and hidden states of its child vertices as input. We explore two kinds of recursive neural units which are discussed in (Socher et al. 2011; Tai, Socher, and Manning 2015)."}, {"heading": "Naive Recursive Neural Unit (NRNU)", "text": "Each NRNU for a vertex vk \u2208 T takes a feature vector xk and the aggregation of the hidden states from all children of vk. The transition equations of NRNU are given as follows:\nh\u0303k = max vr\u2208C(vk)\n{hr} (2)\nhk = tanh(W (h)xk + U (h)h\u0303k + b (h)) (3)\nwhere C(vk) is the set of child vertices of vk, W (h) and U (h) are weight matrix, and b(h) is the bias. The generated hidden state hk of vk is related to the input vector xk and aggregated hidden state h\u0303k. Different from summing up all hidden states as in (Tai, Socher, and Manning 2015), we use max pooling for h\u0303k (see Eq 2). This is because, in real-life situations, the number of neighbours for a vertex can be very large and some of them are irrelevant for the vertex to classify (Zhang et al. 2013). We use G-NRNN to refer to the graph-based naive recursive neural network which incorporates NRNU as recursive neural units."}, {"heading": "Long Short-Term Memory Unit (LSTMU)", "text": "LSTMU is one variation on RNU, which can handle the long term dependency problem by introducing memory cells and gated units (Hochreiter and Schmidhuber 1997). LSTMU is composed of an input gate ik, a forget gate fk, an output gate ok, a memory cell ck and a hidden state hk. The transition equations of LSTMU are given as follows:\nh\u0303k = max vr\u2208C(vk)\n{hr} (4)\nik = \u03c3(W (i)xk + U (i)h\u0303k + b (i)) (5)\nfkr = \u03c3(W (f)xk + U (f)hr + b (f)) (6)\nok = \u03c3(W (o)xk + U (o)h\u0303k + b (o)) (7)\nuk = tanh(W (u)xk + U (u)h\u0303k + b (u)) (8)\nck = ik uk + \u2211\nvr\u2208C(vk)\nfkr cr (9)\nhk = ok tanh(ck) (10) where C(vk) is the set of child vertices of vk, vr \u2208 C(vk), xk is corresponding feature vector of the child vertex vk, is element-wise multiplication and \u03c3 is sigmoid function, W (\u2217) and U (\u2217) are weight matrices, and b(\u2217) are the biases. h\u0303k is a vector aggregated from the hidden states of the child vertices. We use G-LSTM to refer to the graph-based long short-term memory network which incorporates LSTMU as recursive neural units.\nAfter constructing GRNN, we calculate the hidden states of all vertices in T from leaves to root, then we use a softmax classifier to predict label yt of the target vertex vt using its hidden states ht (see Eq 11).\nP\u03b8(yt|vt, G,X ) = softmax(W (s)ht + b(s)) (11)\ny\u0302t = argmax yt\u2208L\nP\u03b8(yt|vt, G,X ) (12)\nCross-entropy J(\u03b8) = \u2212 1N \u2211N t=1 logP\u03b8(yt|vt, G,X ) is used as cost function, where N is the number of vertices in a training set."}, {"heading": "Experimental Setup", "text": "To verify the effectiveness of our approach, we have conducted experiments on four datasets and compared our approach with three baseline methods. We will describe the datasets, baseline methods, and experimental settings."}, {"heading": "Datasets", "text": "We have tested our approach on four real-world network datasets.\n\u2022 Cora (McCallum et al. 2000) is a citation network dataset which is composed of 2708 scientific publications and 5429 citations between publications. All publications are classified into seven classes: Rule Learning (RU), Genetic Algorithms (GE), Reinforcement Learning (RE), Neural Networks (NE), Probabilistic Methods (PR), Case Based (CA) and Theory (TH).\n\u2022 Citeseer (Giles, Bollacker, and Lawrence 1998) is another citation network dataset which is larger than Cora. Citeseer is composed of 3312 scientific publications and 4723 citations. All publications are classified into six classes: Agents, AI, DB, IR, ML and HCI.\n\u2022 WebKB (Craven et al. 1998) is a website network collected from four computer science departments in different universities which consists of 877 web pages, 1608 hyper-links between web pages. All websites are classified into five classes: faculty, students, project, course and other.\n\u2022 WebKB-sim is a network dataset which is generated from WebKB based on the cosine similarity between each vertex and its top 3 similar vertices according to their feature vectors (Baeza-Yates, Ribeiro-Neto, and others 1999). We use same feature vectors as the ones in WebKB. This\ndataset is used to demonstrate the effectiveness of our framework on datasets which may not have explicit relationship represented as edges between vertices, but can be treated as graphs whose edges are based on some metrics such as similarity of vertices.\nWe use abstracts of publications in Cora and Citeseer, and contents of web pages in WebKB to generate features of vertices. For the above datasets, all words are stemmed first, then stop words and words with document frequency less than 10 are discarded. A dictionary is generated by including all these words. We have 1433, 3793, and 1703 for Cora, Citeseer and WebKB, respectively. Each vertex is represented by a bag-of-words vector where each dimension indicates absence or occurrence of a word in the dictionary of the corresponding dataset1."}, {"heading": "Baseline Methods", "text": "We have implemented the following three baseline methods: \u2022 Logistic Regression (LR) (Hosmer Jr and Lemeshow\n2004) predicts the label of a vertex using its own attributes through a logistic regression model.\n\u2022 Iterative classification approach (ICA) (Lu and Getoor 2003; London and Getoor 2014) utilizes the combination of link structure and vertex features as input of a statistical machine learning model. We use two variants of ICA: ICA-binary uses the occurrence of labels of neighbouring vertices, ICA-count uses the frequency of labels of neighbouring vertices.\n\u2022 Label propagation (LP) (Wang and Zhang 2008; London and Getoor 2014) uses a statistical machine learning to give a label probability for each vertex, then propagates the label probability to all its neighbours. The propagation steps are repeated until all label probabilities converge. To make experiments consistent, logistic regression is used for all statistical machine learning components in ICA and LP. We run 5 iterations for each ICA experiment and 20 iterations for each LP experiment2."}, {"heading": "Experimental Settings", "text": "In our experiments, we split each dataset into two parts: training set and testing set, with different proportions (70% to 95% for training). For each proportion setting, we randomly generate 5 pairs of training and testing sets. For each experiment on a pair of training and testing sets, we run 10 epochs on the training set and record the highest Micro-F1 score (Baeza-Yates, Ribeiro-Neto, and others 1999) on the testing set. Then we report the averaged results from the experiments with the same proportion setting. According to preliminary experiments, the learning rate is set to 0.1 for LR, ICA and LP and 0.01 for all GRNN models. We empirically set number of hidden states to 200 for all GRNN models. Adagrad (Duchi, Hazan, and Singer 2011) is used as the optimization method in our experiments.\n1Cora, Citeseer and WebKB can be downloaded from LINQS. We will publish WebKB-sim along with our code.\n2According to our preliminary experiments, LP converges slower than ICA.\n0.70 0.75 0.80 0.85 0.90 0.95\nTraining Proportion\n76\n78\n80\n82\n84\n86\nM ic\nro F\n1- sc\nor e\n(% )\n(a) Cora LR ICA-binary ICA-count LP G-NRNN_d2 G-LSTM_d2\n0.70 0.75 0.80 0.85 0.90 0.95\nTraining Proportion\n68\n70\n72\n74\n76\n78\n(b) Citeseer\n0.70 0.75 0.80 0.85 0.90 0.95\nTraining Proportion\n80\n82\n84\n86\n88\n90\n(c) WebKB\n0.70 0.75 0.80 0.85 0.90 0.95\nTraining Proportion\n80\n82\n84\n86\n88\n90\n(d) WebKB-sim\nFigure 4: Comparison of G-LSTM with LR, LP and ICA on four datasets: (a) Cora, (b) Citeseer, (c) WebKB and (d) WebKB-sim."}, {"heading": "Results and Discussion", "text": ""}, {"heading": "Model and Parameter Selection", "text": "Figure 3 illustrates the performance of G-NRNN and GLSTM on four datasets. We use G-NRNN di and GLSTM di to refer to G-NRNN and G-LSTM over trees of depth i, respectively, where i = 0, 1, 2.\nFor the experiments on G-NRNN and G-LSTM over trees of different steps, d1 and d2 outperform d0 in most cases3. Particularly, the experiments with d1 and d2 perform better with more than 2% improvement than d0 on Cora, and d1 and d2 enjoy a consistent improvement over d0 on Citeseer and WebKB-sim. This performance difference is also obvious in WebKB, when the training proportion is larger than 85%. These mean that introducing neighbouring vertices can improve the performance of classification and more neighbouring information can be obtained by increasing the depth\n3When d = 0, each constructed tree contains only one vertex.\nof trees. Using same RNU setting, d2 outperforms d1 in most experiments on Cora, Citeseer and WebKB-sim. However, for WebKB, d2 does not always outperforms d1. That is to say, introducing more layers of vertices may help improving the performance, while the choice of the best tree depth depends on applications.\nIn Table 1, we compare three different pooling strategies used in G-LSTM4, sum, mean and max pooling. We use 85% for training for all datasets here. In general, mean and max outperform sum which is used in (Tai, Socher, and Manning 2015). This is probably because, the number of neighbours for a vertex can be very large and summing them up can make h\u0303 large for some extreme cases. max slightly outperforms mean in our experiments, which is probably due to max pooling can select the most influential information of child vertices which filters out noise to some extend."}, {"heading": "Baseline Comparison", "text": "We compare our approach with the baseline methods on four network datasets. As our models with d2 provide better performance, we choose G-NRNN d2 and G-LSTM d2 as representative models.\nIn Figure 4.a and Figure 4.b, we compare our approach with the baseline methods on two citation networks, Cora and Citeseer. The collective vertex classification approaches, i.e. LP, ICA, G-NRNN and G-LSTM, largely outperform LR which only uses attributes of the target vertex. Both G-NRNN d2 and G-LSTM d2 consistently outperform all baseline methods on the citation networks, which indicates the effectiveness of our approach. In Figure 4.c and Figure 4.d, we compare our approach with the baseline methods on WebKB and WebKB-sim. For WebKB, our method obtains competitive results in comparison with ICA. LP works worse than LR on WebKB, where Micro-F1 score is less than 0.7. For this reason, it is not presented in Figure 4.c, and we will discuss this in detail in the next subsection. For WebKB-sim, our approaches consistently outperform all baseline methods, and G-LSTM d2 outperforms GNRNN d1 when the training proportion is larger than 80%.\nIn general, G-LSTM d2 outperforms G-NRNN d2. This\n4As G-NRNN gives similar results, we only illustrate results for G-LSTM here\nis likely due to the LSTMU\u2019s capability of memorizing information using memory cells. G-LSTM can thus better capture correlations between representations with long dependencies (Hochreiter and Schmidhuber 1997)."}, {"heading": "Dataset Comparison", "text": "To analyse co-occurrence of neighbouring labels, we compute the transition probability from target vertices to their neighbouring vertices. We first calculate the label cooccurrence matrix Md, where Mdi,j indicates co-occurred times of labels li of target vertices vi and labels lj of d-step away vertices vj . Then we obtain a transition probability matrix T d, where T di,j = M d i,j/( \u2211 kM d i,k). The heat maps of T d on four datasets are demonstrated in Figure 5. For Cora and Citeseer, neighbouring vertices tend to share same labels. When d increases to 2, labels are still tightly correlated. That is probably why all ICA, LP G-NRNN and G-LSTM work well on Cora and Citeseer. In this situation, GRNN integrates features of d-step away vertices which may directly help classify a target vertex. For WebKB, correlation of labels is not clear, some label can be strongly related to more than two labels, e.g. students connects to course, project and student. Introducing vertices which are more steps away makes the correlation even worse for WebKB, e.g.all labels are most related to student. In this situation, LP totally fails, while ICA can learn the correlation of labels that are not same, i.e. student may relate to course instead of student itself. For this dataset, GRNN still achieves competitive results with the best baseline approach. For WebKB-sim, although student is still the label with highest frequency, the correlation between labels is clearer than WebKB, i.e. project relates to project and student. That is probably the reason why, the performance of our approach is good on WebKB-sim for both settings and G-LSTM d2 achieves better results than G-NRNN d2 when the training proportion is larger."}, {"heading": "Conclusions and Future work", "text": "In this paper, we have presented a graph-based recursive neural network framework(GRNN) for vertex classification on graphs. We have compared two recursive units, NRNU and LSTMU within this framework. It turns out that LSTMU works better than NRNU on most experiments. Finally, the performance of our proposed methods outperformed several state-of-the-art statistical machine learning based methods.\nIn the future, we intend to extend this work in several directions. We aim to apply GRNN to large scale graphs. We also aim to improve the efficiency of GRNN and conduct time complexity analysis."}], "references": [{"title": "Survey of graph database models", "author": ["R. Angles", "C. Gutierrez"], "venue": "ACM Computing Surveys (CSUR) 40(1):1.", "citeRegEx": "Angles and Gutierrez,? 2008", "shortCiteRegEx": "Angles and Gutierrez", "year": 2008}, {"title": "Modern information retrieval, volume 463", "author": ["R. Baeza-Yates", "B Ribeiro-Neto"], "venue": null, "citeRegEx": "Baeza.Yates and Ribeiro.Neto,? \\Q1999\\E", "shortCiteRegEx": "Baeza.Yates and Ribeiro.Neto", "year": 1999}, {"title": "Enhanced hypertext categorization using hyperlinks", "author": ["S. Chakrabarti", "B. Dom", "P. Indyk"], "venue": "ACM SIGMOD Record, volume 27, 307\u2013318. ACM.", "citeRegEx": "Chakrabarti et al\\.,? 1998", "shortCiteRegEx": "Chakrabarti et al\\.", "year": 1998}, {"title": "Learning to extract symbolic knowledge from the world wide web", "author": ["M. Craven", "D. DiPasquo", "D. Freitag", "A. McCallum"], "venue": "Proceedings of the 15th National Conference on Artificial Intelligence, 509\u2013516. American Association for Artificial Intelligence.", "citeRegEx": "Craven et al\\.,? 1998", "shortCiteRegEx": "Craven et al\\.", "year": 1998}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research 12(Jul):2121\u2013 2159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Citeseer: An automatic citation indexing system", "author": ["C.L. Giles", "K.D. Bollacker", "S. Lawrence"], "venue": "Proceedings of the 3rd ACM conference on Digital libraries, 89\u201398. ACM.", "citeRegEx": "Giles et al\\.,? 1998", "shortCiteRegEx": "Giles et al\\.", "year": 1998}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Applied logistic regression", "author": ["D.W. Hosmer Jr", "S. Lemeshow"], "venue": "John Wiley & Sons.", "citeRegEx": "Jr and Lemeshow,? 2004", "shortCiteRegEx": "Jr and Lemeshow", "year": 2004}, {"title": "Collective classification of network data", "author": ["B. London", "L. Getoor"], "venue": "Data Classification: Algorithms and Applications 399.", "citeRegEx": "London and Getoor,? 2014", "shortCiteRegEx": "London and Getoor", "year": 2014}, {"title": "Link-based classification", "author": ["Q. Lu", "L. Getoor"], "venue": "Proceedings of the 20th International Conference on Machine Learning, volume 3, 496\u2013503.", "citeRegEx": "Lu and Getoor,? 2003", "shortCiteRegEx": "Lu and Getoor", "year": 2003}, {"title": "Automating the construction of internet portals with machine learning", "author": ["A.K. McCallum", "K. Nigam", "J. Rennie", "K. Seymore"], "venue": "Information Retrieval 3(2):127\u2013163.", "citeRegEx": "McCallum et al\\.,? 2000", "shortCiteRegEx": "McCallum et al\\.", "year": 2000}, {"title": "Recurrent neural collective classification", "author": ["D.D. Monner", "J.A. Reggia"], "venue": "IEEE transactions on neural networks and learning systems 24(12):1932\u20131943.", "citeRegEx": "Monner and Reggia,? 2013", "shortCiteRegEx": "Monner and Reggia", "year": 2013}, {"title": "A practical hypertext categorization method using links and incrementally available class information", "author": ["S.H. Myaeng", "Lee", "M.-h."], "venue": "Proceedings of the 23rd ACM International Conference on Research and Development in Information Retrieval. ACM.", "citeRegEx": "Myaeng et al\\.,? 2000", "shortCiteRegEx": "Myaeng et al\\.", "year": 2000}, {"title": "Learning convolutional neural networks for graphs", "author": ["M. Niepert", "M. Ahmed", "K. Kutzkov"], "venue": "Proceedings of the 33rd International Conference on Machine Learning.", "citeRegEx": "Niepert et al\\.,? 2016", "shortCiteRegEx": "Niepert et al\\.", "year": 2016}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 701\u2013710. ACM.", "citeRegEx": "Perozzi et al\\.,? 2014", "shortCiteRegEx": "Perozzi et al\\.", "year": 2014}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks 61:85\u2013117.", "citeRegEx": "Schmidhuber,? 2015", "shortCiteRegEx": "Schmidhuber", "year": 2015}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["R. Socher", "C.C. Lin", "C. Manning", "A.Y. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning, 129\u2013136.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["K.S. Tai", "R. Socher", "C.D. Manning"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistic. ACL.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Negative link prediction in social media", "author": ["J. Tang", "S. Chang", "C. Aggarwal", "H. Liu"], "venue": "Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, 87\u201396. ACM.", "citeRegEx": "Tang et al\\.,? 2015", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["D. Tang", "B. Qin", "T. Liu"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 1422\u20131432.", "citeRegEx": "Tang et al\\.,? 2015", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Label propagation through linear neighborhoods", "author": ["F. Wang", "C. Zhang"], "venue": "IEEE Transactions on Knowledge and Data Engineering 20(1):55\u201367.", "citeRegEx": "Wang and Zhang,? 2008", "shortCiteRegEx": "Wang and Zhang", "year": 2008}, {"title": "Social influence locality for modeling retweeting behaviors", "author": ["J. Zhang", "B. Liu", "J. Tang", "T. Chen", "J. Li"], "venue": "Proceeding of the 23rd International Joint Conference on Artificial Intelligence, volume 13, 2761\u20132767. Citeseer.", "citeRegEx": "Zhang et al\\.,? 2013", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 8, "context": "Many studies formulate their graph problems as predictive tasks such as vertex classification (London and Getoor 2014), link prediction (Tang et al.", "startOffset": 94, "endOffset": 118}, {"referenceID": 18, "context": "Many studies formulate their graph problems as predictive tasks such as vertex classification (London and Getoor 2014), link prediction (Tang et al. 2015), and graph classification (Niepert, Ahmed, and Kutzkov 2016).", "startOffset": 136, "endOffset": 154}, {"referenceID": 8, "context": "Algorithms for classifying vertices are widely adopted in web page analysis, citation analysis and social network analysis (London and Getoor 2014).", "startOffset": 123, "endOffset": 147}, {"referenceID": 9, "context": "For instance, the Iterative Classification Approach (ICA) integrates the label distribution of neighbouring vertices to assist classification (Lu and Getoor 2003) and the Label Propagation approach (LP) fine-tunes predictions of the vertex using the labels of its neighbouring vertices (Wang and Zhang 2008).", "startOffset": 142, "endOffset": 162}, {"referenceID": 20, "context": "For instance, the Iterative Classification Approach (ICA) integrates the label distribution of neighbouring vertices to assist classification (Lu and Getoor 2003) and the Label Propagation approach (LP) fine-tunes predictions of the vertex using the labels of its neighbouring vertices (Wang and Zhang 2008).", "startOffset": 286, "endOffset": 307}, {"referenceID": 15, "context": "Utilizing representations learned from neural networks instead of neighbouring attributes or labels is one of the possible approaches (Schmidhuber 2015).", "startOffset": 134, "endOffset": 152}, {"referenceID": 0, "context": "There has been a growing trend to represent data using graphs (Angles and Gutierrez 2008).", "startOffset": 62, "endOffset": 89}, {"referenceID": 8, "context": "Discovering knowledge from graphs becomes an exciting research area, such as vertex classification (London and Getoor 2014) and graph classification (Niepert, Ahmed, and Kutzkov 2016).", "startOffset": 99, "endOffset": 123}, {"referenceID": 9, "context": "The main-stream approaches for vertex classification are collective vertex classification (Lu and Getoor 2003) which classify vertices using information provided by neighbouring vertices.", "startOffset": 90, "endOffset": 110}, {"referenceID": 9, "context": "Iterative classification approach (Lu and Getoor 2003) models neighbours\u2019 label distribution as link features to facilitate classification.", "startOffset": 34, "endOffset": 54}, {"referenceID": 20, "context": "Label propagation approach (Wang and Zhang 2008) assigns a probabilistic label for each vertex and then fine-tunes the probability using graph structure.", "startOffset": 27, "endOffset": 48}, {"referenceID": 11, "context": "Recurrent neural collective classification (Monner and Reggia 2013) encodes neighbouring vertices via a recurrent neural network, which is hard to capture the information from vertices that are more than several steps away.", "startOffset": 43, "endOffset": 67}, {"referenceID": 16, "context": "RNN has been implemented in natural scenes parsing (Socher et al. 2011) and tree-structured sentence representation learning (Tai, Socher, and Manning 2015).", "startOffset": 51, "endOffset": 71}, {"referenceID": 16, "context": "We explore two kinds of recursive neural units which are discussed in (Socher et al. 2011; Tai, Socher, and Manning 2015).", "startOffset": 70, "endOffset": 121}, {"referenceID": 21, "context": "This is because, in real-life situations, the number of neighbours for a vertex can be very large and some of them are irrelevant for the vertex to classify (Zhang et al. 2013).", "startOffset": 157, "endOffset": 176}, {"referenceID": 6, "context": "Long Short-Term Memory Unit (LSTMU) LSTMU is one variation on RNU, which can handle the long term dependency problem by introducing memory cells and gated units (Hochreiter and Schmidhuber 1997).", "startOffset": 161, "endOffset": 194}, {"referenceID": 10, "context": "\u2022 Cora (McCallum et al. 2000) is a citation network dataset which is composed of 2708 scientific publications and 5429 citations between publications.", "startOffset": 7, "endOffset": 29}, {"referenceID": 3, "context": "\u2022 WebKB (Craven et al. 1998) is a website network collected from four computer science departments in different universities which consists of 877 web pages, 1608 hyper-links between web pages.", "startOffset": 8, "endOffset": 28}, {"referenceID": 9, "context": "\u2022 Iterative classification approach (ICA) (Lu and Getoor 2003; London and Getoor 2014) utilizes the combination of link structure and vertex features as input of a statistical machine learning model.", "startOffset": 42, "endOffset": 86}, {"referenceID": 8, "context": "\u2022 Iterative classification approach (ICA) (Lu and Getoor 2003; London and Getoor 2014) utilizes the combination of link structure and vertex features as input of a statistical machine learning model.", "startOffset": 42, "endOffset": 86}, {"referenceID": 20, "context": "\u2022 Label propagation (LP) (Wang and Zhang 2008; London and Getoor 2014) uses a statistical machine learning to give a label probability for each vertex, then propagates the label probability to all its neighbours.", "startOffset": 25, "endOffset": 70}, {"referenceID": 8, "context": "\u2022 Label propagation (LP) (Wang and Zhang 2008; London and Getoor 2014) uses a statistical machine learning to give a label probability for each vertex, then propagates the label probability to all its neighbours.", "startOffset": 25, "endOffset": 70}, {"referenceID": 6, "context": "G-LSTM can thus better capture correlations between representations with long dependencies (Hochreiter and Schmidhuber 1997).", "startOffset": 91, "endOffset": 124}], "year": 2017, "abstractText": "Collective classification of vertices is a task of assigning categories to each vertex in a graph based on both vertex attributes and link structure. Nevertheless, some existing approaches do not use the features of neighbouring vertices properly, due to the noise introduced by these features. In this paper, we propose a graphbased recursive neural network framework for collective vertex classification. In this framework, we generate hidden representations from both attributes of vertices and representations of neighbouring vertices via recursive neural networks. Under this framework, we explore two types of recursive neural units, naive recursive neural unit and long short-term memory unit. We have conducted experiments on four real-world network datasets. The experimental results show that our framework with long short-term memory model achieves better results and outperforms several competitive baseline methods.", "creator": "LaTeX with hyperref package"}}}