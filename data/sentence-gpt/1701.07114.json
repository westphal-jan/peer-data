{"id": "1701.07114", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jan-2017", "title": "On the Effectiveness of Discretizing Quantitative Attributes in Linear Classifiers", "abstract": "Learning algorithms that learn linear models often have high representation bias on real-world problems. In this paper, we show that this representation bias can be greatly reduced by discretization. Discretization is a common procedure in machine learning that is used to convert a quantitative attribute into a qualitative one. It is often motivated by the limitation of some learners to qualitative data. Discretization loses information, as fewer distinctions between instances are possible using discretized data relative to undiscretized data. In consequence, where discretization is not essential, it might appear desirable to avoid it. However, it has been shown that discretization often substantially reduces the error of the linear generative Bayesian classifier naive Bayes. This motivates a systematic study of the effectiveness of discretizing quantitative attributes for other linear classifiers. In this work, we study the effect of discretization on the performance of linear classifiers optimizing three distinct discriminative objective functions --- logistic regression (optimizing negative log-likelihood), support vector classifiers (optimizing hinge loss) and a zero-hidden layer artificial neural network (optimizing mean-square-error). We show that discretization can greatly increase the accuracy of these linear discriminative learners by reducing their representation bias, especially on big datasets. We substantiate our claims with an empirical study on $42$ benchmark datasets. This work illustrates the practical applications of discretization and suggests that discretization is a useful resource for estimating accuracy in linear classifiers.", "histories": [["v1", "Tue, 24 Jan 2017 23:57:32 GMT  (8075kb,D)", "http://arxiv.org/abs/1701.07114v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nayyar a zaidi", "yang du", "geoffrey i webb"], "accepted": false, "id": "1701.07114"}, "pdf": {"name": "1701.07114.pdf", "metadata": {"source": "CRF", "title": "On the Effectiveness of Discretizing Quantitative Attributes in Linear Classifiers", "authors": ["Nayyar A. Zaidi", "Yang Du", "Geoffrey I. Webb"], "emails": ["nayyar.zaidi@monash.edu", "ydu32@student.monash.edu", "geoff.webb@monash.edu"], "sections": [{"heading": null, "text": "Learning algorithms that learn linear models often have high representation bias on real-world problems. In this paper, we show that this representation bias can be greatly reduced by discretization. Discretization is a common procedure in machine learning that is used to convert a quantitative attribute into a qualitative one. It is often motivated by the limitation of some learners to qualitative data. Discretization loses information, as fewer distinctions between instances are possible using discretized data relative to undiscretized data. In consequence, where discretization is not essential, it might appear desirable to avoid it. However, it has been shown that discretization often substantially reduces the error of the linear generative Bayesian classifier naive Bayes. This motivates a systematic study of the effectiveness of discretizing quantitative attributes for other linear classifiers. In this work, we study the effect of discretization on the performance of linear classifiers optimizing three distinct discriminative objective functions \u2014 logistic regression (optimizing negative log-likelihood), support vector classifiers (optimizing hinge loss) and a zero-hidden layer artificial neural network (optimizing mean-square-error). We show that discretization can greatly increase the accuracy of these linear discriminative learners by reducing their representation bias, especially on big datasets. We substantiate our claims with an empirical study on 42 benchmark datasets.\nKeywords: discretization, classification, logistic regression, support vector classifier, big datasets, bias-variance analysis"}, {"heading": "1. Introduction", "text": "One of the many factors that affect the error of a learning system is its representation bias (van der Putten and van Someren, 2004), or, as it is also called, its hypothesis language bias (Mitchell, 1980). We define representation bias herein as the minimum loss of any model in the space of models available to the learner. It is clearly desirable in the general case to use a space of models that minimizes representation bias for a given problem. Learning algorithms that use linear models, such as logistic regression (LR) (Murphy, 2012) and support vector classifiers (SVC) (Chih-Jen, 2010), are very popular, possibly in part due to their lending themselves to convex optimization. In this paper we argue that learning algorithms that\nc\u00a92017 Nayyar A. Zaidi, Yang. Du, Geoffrey I. Webb.\nar X\niv :1\n70 1.\n07 11\n4v 1\n[ cs\n.L G\n] 2\n4 Ja\nlearn linear models often have high representation bias on real-world problems, and that often this bias can be reduced by discretization. We illustrate this in Figure 1 which shows on simple synthetic data how a linear classifier cannot create an accurate classifier on the numeric data but can when the data are discretized using simple univariate discretization.\nThere are two important observations that motivate this work:\n\u2022 A linear classifier with discretization is not linear with respect to the original data.\n\u2022 Contrary to what might be thought given that discretization loses information, a linear classifier with discretization can reduce representation bias and may consequently reduce error.\nNote that we do not claim that discretization is the only useful feature transformation or a substitute for other approaches to creating non-linear classifiers, such as multi-layer perceptrons. The simple AND problem illustrated in Figure 1 is a case where where discretization is as effective as any other method for obtaining non-linear decision surfaces. However, there are problems where discretization will not be this effective. One example is the X-OR problem illustrated in Figure 2.\nNote also that we do not claim that discretization always reduces representation bias. However, linear models make a strong implicit assumption, that the data are well modeled by a linear decision boundary. As illustrated in Figure 1 discretization can overcome this assumption. The startling result that we reveal is that doing so is often useful in practice. We show that discretization often reduces the bias of a linear classifier and that this reduction in bias frequently results in lower error when the data quantity is sufficiently large to minimize overfitting.\nThe rest of this paper is organized as follows. Some preliminary background and terminology is given in Section 2.1. We discuss discretization in general in Section 2.2. Linear\nclassifiers based on Conditional Log-Likelihood (CLL) Hinge Loss (HL) and Mean-squareerror loss are discussed in Sections 2.3, 2.4 and 2.5 respectively. Optimization strategies for training these linear classifiers are discussed in Section 2.6. An overview of related work is given in Section 3. Experimental results are given in Section 4. We conclude in Section 5 with pointers to directions for future work."}, {"heading": "2. Background", "text": ""}, {"heading": "2.1 Terminology", "text": "In machine learning and data mining research, there exists variation in the terminology when it comes to characterizing the nature of an attribute (or feature). For example, \u2018continuous vs. discrete\u2019, \u2018numeric vs. categorical\u2019 and \u2018quantitative vs. qualitative\u2019. We believe that the \u2018quantitative vs. qualitative\u2019 distinction is best suited for our study in this paper and hence, this is used throughout the paper.\nQualitative attributes are the attributes on which arithmetic operations can not be applied. The values of a qualitative attribute can be placed or categorized in distinct categories. Sometimes there exist a meaningful rank among these categories, resulting in distinction of ordinal and nominal among quantitative attributes. For example, Student Grade: {HD, D, C, P, F} and Pool Depth: {Very Deep, Deep, Shallow} are ordinal attributes, while Marital Status: {Married, Never-married, Divorced, Widow, Widower} and Nationality: {Australian, American, British} are nominal attributes.\nQuantitative attributes, on the hand, are the attributes on which arithmetic operations can be applied. They can be both discrete and continuous. For example, Number of Children is a discrete-quantitative attribute (values determined by counting), whereas Temperature is a continuous-quantitative attribute (values determined by measuring).\nA list of the various symbols used in this work is given in Table 1."}, {"heading": "2.2 Discretization", "text": "Discretization is a common process in machine learning that is used to convert a quantitative into a qualitative attribute (Liu et al., 2002; Garcia et al., 2013). The need for discretization originates from the facts that some classifiers can only handle, and some others sometimes to operate better with qualitative attributes. The process involves finding cut-points within the range of the quantitative attribute and to group values into intervals based on these cut-points. This removes the ability to distinguish between data points falling in the same interval. Therefore, discretization entails information loss.\nDiscretization methods can be categorized into two categories: Supervised and Unsupervised. In the unsupervised case, class information is not used during cut-point determination process. Popular approaches are equal-frequency and equal-width discretization. Equal-width discretization (EWD) divides the quantitative attribute\u2019s range (maximum value xmaxi and minimum value x min i ) into k equal-width intervals where k is provided by the user. Each interval will have a width of w = xmaxi \u2212xmini\nk . Equal-frequency discretization (EFD), on the other hand, divides the sorted values of a quantitative attribute such that each interval has approximately k number of data points. Each interval will contain N/k data points. It is also important that data points with identical value are placed in the same interval, therefore, in practice, each interval will have slightly different number of data points. Choosing EWD or EFD and the number of bins is problem specific and can have huge impact on the overall performance of any model. Of course, choosing a large k will result in less information loss, but can result in over-fitting on small datasets.\nSupervised discretization methods, on the other hand, utilize the class information of the data point to better define the cut-points. For example, state-of-the-art discretization\ntechnique Entropy-Minimization Discretization (EMD) sorts the quantitative attribute\u2019s values and then finds the cut-point such the information gain is maximized across the splits (Kohavi and Sahami, 1996). The technique is applied recursively on the successive splits and the minimum-description-length (MDL) criterion is used to determine when to stop splitting."}, {"heading": "2.3 Linear Classifier - CLL", "text": "A Logistic Regression classifier optimizes the conditional log-likelihood (CLL) which is defined as:\nCLL(\u03b2) = N\u2211 l=1 log P(y(l) |x(l)), (1)\nwhere\nP(y(l) |x(l)) = exp(\u03b2y,0 + \u03b2\nT y x (l))\u2211C c=1 exp(\u03b2y,0 + \u03b2 T c x (l)) . (2)\nThe term \u03b2y,0 + \u03b2 T y x (l) is expanded as: \u03b2y,0x (l) 0 + \u03b2y,1x (l) 1 + \u03b2y,2x (l) 2 + \u00b7 \u00b7 \u00b7 + \u03b2y,nx (l) n , where x0 can be assumed to be 1 for all data points. Since the objective function as defined in Equation 1, is linear in x, it is a linear classifier.\nEquation 2 leads to a multi-class softmax objective function. Since, a set of parameters are learned for each class, we have made this distinction explicit with subscript y in parameter notation, that is, \u03b2y,j denotes a parameter for class y and attribute j. Typically, an LR minimizes the negative of the CLL known as negative log-likelihood (NLL), which is defined as:\nNLL(\u03b2) = \u2212 N\u2211 l=1\n(( \u03b2y,0 + \u03b2 T y x (l) ) \u2212 log ( C\u2211 c=1 exp(\u03b2c,0 + \u03b2 T c x (l)) )) . (3)\nNote, in the following, for simplicity, we will drop the superscript (l) notation. It should be noted that many software libraries for multi-class LR are either based on implementing multi-class (softmax) objective function of Equation 3 or they optimize a more simpler binary objective function of the following form:\nNLL(\u03b2) = N\u2211 l=1 ( log ( 1 + exp(\u03b20 + \u03b2 Tx(l)) )) , (4)\nand solve a one-versus-all classification problem. Note that in the case of binary classifiers, there is only one set of parameters for the two classes as oppose to C set of parameter that needed to be optimized for the softmax case. At classification time, one needs to apply C different trained LR classifiers and choose one with the highest probability.\nNonetheless, optimizing a standard LR with NLL based either on Equation 3 or Equation 4 requires substantial input manipulation, i.e., appending 1 to all data points and then, converting qualitative attributes using one-hot-encoding. For example, a qualitative\nattribute Xj taking values {a, b, c}, will be converted into three attributes Xj , Xj+1, Xj+2, each taking values either 0 or 1. An alternative to manipulating the input is to modify the model and optimize the following objective function instead:\nNLL(\u03b2) = \u2212 N\u2211 l=1 \u03b2y,0 + I\u2211 i=1 \u03b2y,ixi + K\u2211 k=1 |Xk|\u2211 j=1 \u03b2y,k,j1Xk=xj  (5) \u2212 log\n C\u2211 c=1 exp(\u03b2c,0 + I\u2211 i=1 \u03b2c,ixi + K\u2211 k=1 |Xk|\u2211 j=1 \u03b2c,k,j1Xk=xj )  . Note that the models expressed in Equation 3 and 5 are exactly equivalent and will lead to the same results. The only difference is that the model in Equation 3 requires converting all qualitative attributes into quantitative ones using one-hot-encoding, whereas the model in Equation 5 does not. Equation 5 can be simplified even further \u2013 for datasets with only qualitative attributes, and including only terms that are not canceled out, we have:\nNLL(\u03b2) = \u2212 N\u2211 l=1\n( \u03b2y,0 +\nK\u2211 k=1 \u03b2y,k,j1Xk=xj ,Y=y\n\u2212 log ( C\u2211 c=1 exp(\u03b2c,0 + K\u2211 k=1 \u03b2c,k,j1Xk=xj ,Y=c) )) . (6)\nInstead of converting qualitative attributes into quantitative ones and using the model of Equation 3, one can convert quantitative attributes into qualitative ones using discretization methods as discussed in Section 2.2 and use the model of Equation 6. It can be seen that with Equation 3, the number of parameters optimized are: (C \u2212 1) + (C \u2212 1)n. Whereas, with Equation 6, (C\u22121)+(C\u22121) \u2211n i=1 |Xi| parameters are optimized. Since the two models are not equivalent, this will result in different training time, speed and rate of convergence and of course, classifications."}, {"heading": "2.4 Linear Classifier \u2013 Hinge Loss", "text": "Hinge Loss (HL) is widely used as an alternative to CLL and has been the basis of Support Vector Machines. A classifier optimizing either a Hinge Loss objective function or its variant is a linear classifier and is known as the Support Vector Classifier (SVC). Here we define L2-Loss HL as:\nHL(\u03b2) = N\u2211 l=1 max(0, 1\u2212 y\u03b2Tx)2. (7)\nAn alternative is L1-Loss HL which is equal to: \u2211N\nl=1 max(0, 1 \u2212 y\u03b2Tx). In this work, we will focus only on the L2-Loss. In practice, a penalty term is also added for regularizing the objective function as:\nHL(\u03b2) = 1\n2 ||\u03b2T\u03b2||2 + \u03bb N\u2211 l=1 (max(0, 1\u2212 y\u03b2Tx))2, (8)\nwhere \u03bb is the regularization parameter. We will discuss the gradient and Hessian of this objective function later in Section 2.6."}, {"heading": "2.5 Linear Classifier \u2013 Mean-Square-Error", "text": "Another linear classifier is based on optimizing the Mean-Square-Error (MSE) objective function and is defined as:\nMSE(\u03b2) = 1\n2 N\u2211 l=1 C\u2211 c=1 (P\u0303(c |x)\u2212 P(c |x))2, (9)\nwhere P(c |x) is given in Equation 2 and P\u0303(c |x) is the actual probability of class c given data instance x. This will be a vector of size C with all zeros except at the location of the label of x, where it will be 1 (assuming there are no duplicate data points in the dataset). The objective function of Equation 9 is similar to that optimized by artificial neural-networks (ANN). However, in ANN, P(c |x) is defined in terms of multiple layers. We can interpret Equation 9 as the objective function of a zero-layer ANN."}, {"heading": "2.6 Optimization", "text": "There is no closed form solution to optimizing the negative log-likelihood, hinge loss and mean-square-error objective function, and, therefore, one has to resort to iterative minimization procedures such as gradient descent or quasi-Newton. An iterative optimization procedure generates a sequence {\u03b2k}\u221ek=1 converging to an optimal solution. At every iteration k, the following update is made: \u03b2k+1 = \u03b2k + sk, where sk is the search direction vector. The following equation plays the pivotal role as it holds the key to obtain sk by solving a system of linear equations:\n\u22072f(\u03b2k)sk = \u2212\u2207f(\u03b2k), (10)\nwhere f is the objective function that we are optimizing. There are two very important issues that must be addressed when solving for search direction vector using Equation 10 (Nocedal and Wright, 2006). First, it can be infeasible to explicitly compute and store the Hessian, especially on high-dimensional data. Second, the solution obtained using Equation 10, does not guarantee convergence. There are three main strategies for addressing the first issue:\n\u2022 Consider \u22072f(\u03b2k) to be an identity matrix \u2013 in this case, sk = \u2212\u2207f(\u03b2k). This leads to a family of algorithms known as first-order methods such as Gradient Descent, Coordinate Descent, etc.\n\u2022 Do not compute \u22072f(\u03b2k) directly, but approximate it from the information present in \u2207f(\u03b2k) instead. This property is useful for large scale problems where we cannot store the Hessian matrix. This leads to approximate second-order methods known as quasi-Newton algorithms, for example, L-BFGS which, is considered to be the most efficient algorithm (de-facto standard) for training LR.\n\u2022 Third, use standard \u2018direct algorithms\u2019 for solving a system of linear equations such as Gaussian elimination to solve for sk. Or, use any one of the iterative algorithm\nsuch as conjugate gradient, etc. For large datasets, generally iterative methods are preferable over direct methods, as the former requires computing the whole Hessian matrix. The optimization method now has two layers of iterations. An outer layer of iteration to update \u03b2k, and an inner layer of iterations to find Newton direction sk. In practice, one can only use an approximate Newton direction in early stages of the outer iterations. This method is known as \u2018Truncated Newton method\u2019 (Nash, 2000).\nIt should be noted that these methods differ in terms of the speed-of-convergence, costper-iteration, iterations-to-convergence, etc. For example, Coordinate Descent updates one component of \u03b2 at every iteration, so the cost-per-iteration is very low, but iterationsto-convergence will be very high. On the other hand, Newton methods, will have high cost-per-iteration, but very low number of iterations-to-convergence. The three methods described above are all affected by the scaling of the axis. Therefore, scaling quantitative attributes or converting quantitative into qualitative attributes will effect the speed and the quality of the convergence.\nThe second issue can be addressed by adjusting the length of the Newton direction. For that, two techniques are mostly used \u2013 line search and trust region. Line search methods are standard in optimization research. We can modify Equation 10 as\u22072f(\u03b2k)sk = \u2212\u03b7k\u2207f(\u03b2k), where \u03b7k is known as the step-size. Standard line searches obtain an optimal step-size as a solution to the following sub-optimization problem: \u03b7k = argmin\u03b7f(\u03b2\nk + \u03b7sk). Trustregion methods, unlike line search, are relatively new in optimization research. Trust-region methods first find a region around the current solution \u2013 in this region, a quadratic (or linear) model is used to approximate the objective function. The step size is determined based on the goodness of fit of the approximate model. If a significant decrease in the objective function is achieved with a forward step, the approximated model is a good representative of the original objective function and vice-versa. The size of the (trust) region is specified as a spherical area of size \u2206k. The convergence of the algorithm is guaranteed by controlling the size of the region which (in each iteration) is proportional to the reduction in the value of objective function in the previous iteration.\nIn the following we will define the gradient and Hessian of the three objective functions conditional log-likelihood, hinge loss and mean square error. Note, we only define the gradient and the Hessian for qualitative attributes here. For softmax CLL, \u2207f(\u03b2k) and \u22072f(\u03b2k) can be written as:\n\u2202NLL(\u03b2)\n\u2202\u03b2y\u2032,i = N\u2211 l=1 (1y=y\u2032 \u2212 P(y\u2032|x))xi,\n\u22022NLL(\u03b2)\n\u2202\u03b2y\u2032,i\u2202\u03b2y\u2032\u2032,j = \u2212 N\u2211 l=1 (1y\u2032=y\u2032\u2032 \u2212 P(y\u2032|x))P(y\u2032\u2032|x)xixj .\nFor Hinge-loss, the (sub-) gradients can be written as:\n\u2202HL(\u03b2)\n\u2202\u03b2i = 2 N \u2032\u2211 l=1 (y\u03b2Tx\u2212 1)yxi,\nwhere N \u2032, are the instances for which y\u03b2Tx < 1 is true. Similarly the Hessian can be written as:\n\u22022HL(\u03b2)\n\u2202\u03b2i\u2202\u03b2j = 2 N \u2032\u2211 l=1 xixj\nFor MSE, one can write the gradients as:\n\u2202MSE(\u03b2)\n\u2202\u03b2j,xj ,k = N\u2211 l=1 C\u2211 c=1 (1y=c \u2212 P(c|x))(1k=c \u2212 P(k|x))xi,\nand the Hessian can be written as:\n\u22022MSE(\u03b2)\n\u2202\u03b2j,xj ,k\u2202\u03b2j\u2032,xj\u2032 ,k\u2032 = \u2212 N\u2211 l=1 [ (\u22121)(1k\u2032=c \u2212 P(k\u2032|x))P(c|x)1j=j\u2032 +\n(1y=c \u2212 P(c|x))(\u22121)(1k\u2032=k \u2212 P(k\u2032|x))P (k|x)1j=j\u2032P(c|x) + (1y=c \u2212 P(c|x))(1k=c \u2212 P(k|x))(1k\u2032=c \u2212 P(k\u2032|x))P(c|x)1j=j\u2032 ] xixj"}, {"heading": "3. Related Work", "text": "Discretization is often motivated by a need to adapt data for a model that cannot handle quantitative attributes. In Statistics and many of its related and applied branches (such as epidemiology, medical research and consumer marketing), it goes by names of \u2018dichotomization\u2019 and \u2018categorization\u2019 (where the two techniques differ as the former splits the measurement scale into two while the later can have more than two categories) \u2013 and has been examined in many studies (Irwin and McClelland, 2003; MacCallum et al., 2002; Greenland, 1995). However, in most of these studies a majority opinion is against the use of dichotomization \u2013 and for categorization, it is advised to be used with caution. The main reason cited for this is that dichotomization and categorization lead to information loss since the variability among the members of the group is subsumed. For example, Altman and Royston (2006) write:\n... Firstly, much information is lost, so the statistical power to detect a relation between variable and patient outcome is reduced ... and considerable variability may be subsumed within each group. Individuals close to but on opposite sides of cut-point are characterized as being very different rather than very similar ...\nIn practical machine learning, the common practice is to discretize an attribute only if necessary (i.e., if a model expects categorical attributes). An exception is for Bayesian classifiers, where it is common practice to discretize numeric attributes (Yang and Webb, 2009).\nThe ambivalence towards discretization is understandable. Obviously, the quality (and sometime quantity) of data is the key to training accurate models and hence getting good results. In many cases, the data are the result of costly and time-consuming efforts (for example in breast cancer research where there are several stake-holders involved just to obtain a few attributes of the data). Losing some of the data (or more precisely, losing some\ndistinction among the instances) due to discretization should be undesirable. However, a number of motivations for discretization have been put forward:\n\u2022 Discretization can lead to simplification of statistical analysis. For example, if a quantitative attribute is split on the median, then one can compare the two groups based on t, \u03c72 or some other test to estimate the difference between the two groups. This may ease interpretation and presentation of results (Altman and Royston, 2006).\n\u2022 If there is error in the measurement scale, discretization can improve the performance of the model by reducing the contamination (Flegal and Keyl, 1991; ReadeChristopher and Kupper, 1991; Fung and Howe, 1984; Shentu and Xie, 2010).\n\u2022 In many domains there exist pre-defined (or standard) thresholds to convert a quantitative to a qualitative scale. In these cases, a discretized attribute might better represent the task at hand as it will be more interpretable or have distinct significance. For example, in medical research doctors might better interpret blood-pressure as high and low rather than on a numeric scale.\n\u2022 A discretized attribute might be better utilized than the quantitative attribute by the learning system. For example, consider a classifier that relies on estimation of conditional probabilities such as P(xi | y). If Xi is quantitative, xi can take infinite many values and if the number of training samples are small, reliable estimation of P(xi | y) from the data is not possible. A common approach is to impose a parametric model to estimate the value of P(xi | y) based on this model in which case the accuracy will depend on the appropriateness of the parametric model selected. Discretization can obviate this problem. By converting a quantitative attribute Xi into a qualitative one X\u2217i , the probabilities will take the form of P(x \u2217 i | y) which may be reliably estimated\nfrom the data as there will be many xi values falling into the same interval (Yang and Webb, 2009).\n\u2022 The final reason for discretization has to do with overcoming a model\u2019s assumptions. It might be the case that discretization help avoid some strong assumption that the learner makes about the data. If those assumptions are correct, discretization will have a negative impact, but if those assumptions are false, discretization may lead to better results (Altman et al., 1994). It is this final motivation that we examine herein.\nThe effect of discretization on various classification algorithms such as naive Bayes, Support Vector Machines and Random Forest is discussed in Lustgarten et al. (2008). On many biomedical datasets, it is shown that discretization can greatly improve the performance of the learning algorithm. The role of discretization as feature selection technique is also explored. On various contrived datasets, Maleki et al. (2009) studied the effect of discretization on the precision and recall of various classification methods.\nThe effectiveness of discretization for naive Bayes classifier is relatively well studied (Hsu et al., 2000; Dougherty et al., 1995; Yang and Webb, 2009). Dougherty et al. (1995) conducted an empirical study of naive Bayes with four well-known discretization methods and found that all the discretization methods result in significantly reducing error relative to a naive Bayes that assumes a Gaussian distribution for the continuous variables. Hsu\net al. (2000) attributes this to the perfect aggregation property of Dirichlet distributions. In naive Bayes settings, a discretized continuous distribution is assumed to have a categorical distribution with Dirichlet priors. The perfect aggregation property of Dirichlet implies that we can learn the class-conditional probability of the discretized interval with arbitrary accuracy. It is also shown that there exists a partition independence assumption, by virtue of that, Dirichlet parameters corresponding to a certain interval depend only on the area below the curve of the probability distribution function, but is independent of the shape of the curve in that interval."}, {"heading": "4. Experiments", "text": "In this section, we compare the performance of linear classifier with discretized linear classifier on various datasets from the UCI repository (Lichman, 2017).\nWe denote linear classifier optimizing the conditional log-likelihood as LR, a linear classifier optimizing the Hinge loss as SVC (support vector classifier) and a linear classifier optimizing the mean-square-error as ANN0 (artificial neural network with zero hidden layers) \u2013 their discrete counterparts are denoted as LR(d), SVC(d) and ANN0(d) respectively.\nIn the remainder of this paper, when discussing results, we will collectively refer to LR, SVC and ANN0 as linear classifiers and denote them by LC. We will collectively refer to LR(d), SVC(d) and ANN0(d) as discretized linear classifiers and denote them by LCd.\nThe details of datasets used in this work are given in Appendix B. For discretized linear classifiers, different supervised and unsupervised discretization techniques were considered. Since, this is not a comparative study on the relative efficacies of various discretization techniques for linear classifiers, we only report results with supervised entropy-based discretization of Fayyad and Irani (1992), which we found gives better results than other discretization methods such as equal-frequency, equal-width, etc.\nEach algorithm is tested on each dataset using either 5 or 10 rounds of 2-fold cross validation.\nDuring the presentation of results, we split our datasets into two categories \u2013 Big and Little. The Big category comprises of datasets with more than 100, 000 instances and the Little category comprises of the remaining datasets with < 100, 000 instances.\nWe compare four different metrics: 0-1 Loss, RMSE, Bias and Variance. We also compare training-time, testing time, and rate of convergence. As discussed in Section 1, the reason for performing bias-variance estimation is that it provides insights into how the learning algorithm might be expected to perform with varying amounts of data. We expect low variance algorithms to have relatively low error for small data and low bias algorithms to have relatively low error for large data (Brain and Webb, 2002). There are a number of different bias-variance decomposition definitions. In this research, we use the bias and variance definitions of Kohavi and Wolpert (1996) together with the repeated cross-validation bias-variance estimation method proposed by Webb (2000).\nWe report Win-Draw-Loss (W-D-L) results when comparing the 0-1 Loss, RMSE, bias and variance of two models. A two-tail binomial sign test is used to determine the significance of the results. Results are considered significant if p \u2264 0.05 and shown in bold.\nFor hinge-loss, a dataset with more than two classes was transformed into a binary dataset. Data points belonging to the majority class were assigned to class A and the remaining data points were assigned to class B.\nMissing values of the quantitative attribute were replaced with the mean of the attribute values whereas missing values of the qualitative attribute were treated as a distinct attribute value.\nQuantitative attributes were also normalized between 0 and 1, as this is often recommended for gradient-based optimization methods.\nThree optimization methods \u2013 gradient descent, quasi-Newton, Trust-region based Newton method (TRON) were used. We found TRON to be converging relatively faster than the other methods. Therefore, in the following, we report results with TRON optimization only. However, it is worth mentioning that a similar pattern of results was seen between LC and LCd for the other optimization methods.\n4.1 Comparison of the Accuracy of LCd and LC\nIn this section, we compare the accuracy of LCd and LC in terms of their 0-1 Loss and RMSE on 52 datasets. Results are shown in Figures 3 and 4. It can be seen that the three LCd classifiers result in much better accuracy than their corresponding LC. In the scatter plots, results on Big datasets are shown in green dots, whereas results on Little datasets are shown in red dots. It can be seen that, on almost all Big datasets, LCd leads to higher accuracy (most green-dots are below the diagonal line). It can also be seen that some of the differences are substantial \u2013 this shows the effectiveness of discretization on LR, SVC and ANN0 especially for big datasets.\nA comparison of the win-draw-loss between the two models is given in Table 2. It can be seen that on big datasets, LCd wins on all except on 2 datasets \u2013 very promising result. This proves our hypothesis that on big datasets, discretization leads to low-bias non-linear classifier resulting in far superior results than a linear classifier with no discretization. On\nsmall datasets, discretization is significantly effective for SVC and non-significantly effective for ANN0. Note that on small datasets, LR(d) and LR leads to similar performances with 13 wins and 14 losses for 0-1 Loss and 12 wins and 15 losses for RMSE. However, one should take into account that the scale of LR(d) wins is much higher than that of LR. This can be seen from the spread of red-dots in the left-most plots of Figures 3 and 4."}, {"heading": "4.2 Comparison of the Bias and Variance", "text": "Figures 5 and 6 present scatter plots of the bias and variance of LC and LCd classifiers. It can be seen that the three LCd classifiers lead to low-bias and high-variance models. Note that we present a bias-variance analysis on only Little datasets. This is because the software we have for obtaining bias-variance estimates is single threaded and could not benefit from\nthe high-performance environment in which most of our experiments were run. As a result it was not feasible to run these experiments on the larger datasets. Nonetheless, results confirm our hypothesis that LCd classifiers tend to have lower bias than LC.\n4.3 Comparison of the Convergence Curves of LCd and LC\nAs training of both LCd and LC classifiers are based on iterative optimization algorithms, they produce a sequence of values as part of their training, i.e., of their objective function which (should ideally) decrease with successive iterations until convergence. A technique that leads to the global minimum faster (steeper curve) and in fewer iterations (shorter curve) is desirable. Note that LC and LCd have different models (and parameterizations) and, therefore, the optimization space for the two problems is also very different. In the following, let us compare the convergences of LC and LCd on some sample datasets. A similar trend was observed on all datasets, here we report results on nine representative datasets only.\n10 0\n10 1\n10 2\n10 3\nNo. of Iterations\n-10\n-9\n-8\n-7\n-6\n-5\n-4\n-3 -2 N e g a t iv e L o g - L ik e li h o o d \u00d710\n4 Localization\nLR LR (d)\n10 0\n10 1\n10 2\n10 3\n10 4\nNo. of Iterations\n-9\n-8.5\n-8\n-7.5\n-7\n-6.5\n-6\n-5.5\n-5\n-4.5\n-4\nN e\ng a\nt iv\ne L\no g\n- L\nik e\nli h\no o\nd\n\u00d710 4 Census-income\nLR LR (d)\n10 0\n10 1\n10 2\n10 3\n10 4\nNo. of Iterations\n-4\n-3.5\n-3\n-2.5\n-2\n-1.5\nN e\ng a\nt iv\ne L\no g\n- L\nik e\nli h\no o\nd\n\u00d710 5 Covtype\nLR LR (d)\nA comparison of the variation in NLL objective function for LR and LR(d) is shown in Figure 7. It can be seen that LR(d) has steeper curve \u2013 that is, it asymptotes to its global minimum much quickly. It is also important to see that LR(d) leads to much lower NLL. Better accuracy of LR(d) is the result of this much lower NLL.\nFigures 8 shows the variation in HL for SVC and SVC(d) whereas, Figure 9 shows the variation in MSE for ANN0(d) and ANN0. A similar trend to NLL can be seen, that is SVC(d) and ANN0(d) leading to a better value of the objective function while converging more rapidly.\n4.4 Comparison of the Learning and Classification Time LCd and LC\nIn this section, we compare the training and classification time of LCd and LC. It can be seen from Figure 10 that LR(d) and ANN0(d) are slightly faster than LR and ANN0 respectively (majority of points below the diagonal line), whereas SVC(d) and SVC have similar trainingtime profiles. We already have seen the superior classification performance of LCd classifiers.\n10 0\n10 1\n10 2\n10 3\nNo. of Iterations\n1.5\n2\n2.5\n3\n3.5 4 H in g e L o s s \u00d710\n4 Localization\nSVC SVC (d)\n10 0\n10 1\n10 2\n10 3\n10 4\nNo. of Iterations\n0\n2\n4\n6\n8\n10\n12\n14\nH in\ng e\nL o\ns s\n\u00d710 4 Census-income\nSVC SVC (d)\n10 0\n10 1\n10 2\n10 3\n10 4\nNo. of Iterations\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\nH in\ng e\nL o\ns s\n\u00d710 5 Covtype\nSVC SVC (d)\nThese training-time results are extremely encouraging as they suggest that LCd can result in much better classification accuracy without compromising computational performance. We have reported the results only on four big datasets. This is because, the results were obtained by running the jobs on a local-desktop computer (i.e., a controlled set-up), rather than the heterogeneous cluster-computing environment in which most of the experimentation was performed. The scatter plots of classification time results for LC and LCd are presented in Figure 11. It can be seen that LC has slightly faster classification time than LCd. However, in most cases the difference in the magnitude is small."}, {"heading": "5. Conclusion and Future Works", "text": "In this paper, we study the role of discretization for linear classifiers in machine learning. Current practice is primarily to apply discretization only when the learner requires qualitative data. Overall, there exists some aversion to discretization as it loses information. We argue that discretization \u2013 despite losing information, can help model non-linear rela-\n10 0\n10 1\n10 2\n10 3\n10 4\nNo. of Iterations\n4000\n4500\n5000\n5500\n6000 6500 M e a n S q u a r e E r r o r Localization\nANN 0 ANN 0 (d)\n10 0\n10 1\n10 2\n10 3\n10 4\nNo. of Iterations\n4500\n5000\n5500\n6000\n6500\n7000\n7500\n8000\n8500\n9000\n9500\nM e\na n\nS q\nu a\nr e\nE r r o\nr\nCensus-income\nANN 0 ANN 0 (d)\n10 0\n10 1\n10 2\n10 3\n10 4\nNo. of Iterations\n1.4\n1.6\n1.8\n2\n2.2\n2.4\n2.6\n2.8\n3\nM e\na n\nS q\nu a\nr e\nE r r o\nr\n\u00d710 4 Covtype\nANN 0 ANN 0 (d)\ntionships in the data and, therefore, can help reduce the bias of a learner that uses linear models. A linear classifier trained on discretized data is not linear any more, which has the potential to help in modeling non-linear decision boundaries which might otherwise require the use of kernels and multi-layer networks.\nWe show that discretization can greatly reduce the error of logistic regression and other discriminative linear classifiers optimizing Hinge Loss and Mean-square-error especially on large datasets. We compare the performance of linear classifiers trained with both qualitative and quantitative attributes (denoted as LC) with LR trained with qualitative attributes only (denoted as LCd), where quantitative attributes were discretized first. Our empirical analysis on 52 datasets showed that LCd led to a low-bias model and, therefore, it resulted in significantly better 0-1 Loss and RMSE performance on large datasets. Quite surprisingly, it also reduced training time and had more desirable convergence, converging more rapidly to models that better fit the data. These substantial benefits come at a cost of a minor increase in classification time.\nGiven the surprising gains from discretization, it is tempting to include both the original quantitative and derived discretized features in the data. Doing so avoids losing any information due to discretization. We undertook some preliminary experiments with this approach. They suggested that while it led to slight lower bias, they did not produce any improvement (in terms of error or convergence) over using only discretized-quantitative features. Further investigation of this research direction has been left as a future work.\nWith faster training, better convergence and low-bias we believe that discretization is worth consideration in any context where linear classifiers are learned from quantitative data."}, {"heading": "6. Code", "text": "The details of the software library fastLC is given in Appendix A. The library along with running instructions can be downloaded from Github: https://github.com/nayyarzaidi/ fastLC.git."}, {"heading": "7. Acknowledgments", "text": "This research has been supported by the Australian Research Council (ARC) under grant DP140100087, and by the Asian Office of Aerospace Research and Development, Air Force Office of Scientific Research under contract FA2386-15-1-4007.\nAppendix A. fastLC \u2013 Linear Classifiers Library\nThe library can handle both quantitative and qualitative attributes. There is no need to do a one-hot-encoding for qualitative attributes, as the LR model built can actually handle the data types.\nOne can execute the code in the library by issuing the following command for LR: >> java -cp /fastLC.jar fastLC.BVDcrossvalx -t /dataset.arff -i 2 -x 2 -W LR.LRClassifier -- -V -O \"Tron\". For SVC, use the following command: >> java -cp /fastLC.jar fastLC.BVDcrossvalx -t /dataset.arff -i 2 -x 2 -W SVC.SVCClassifier -- -V -O \"Tron\". For ANN0, use the following command: >>java -cp /fastLC.jar fastLC.BVDcrossvalx -t /dataset.arff -i 2 -x 2 -W ANN.ANNClassifier -- -V -O \"Tron\". Note, -i 2 -x 2 flags specify two rounds of two-fold cross-validation. -V is the verbosity flag, whereas, -O specifies the solver. One can choose from the following list of solvers: {GD,QN,CG,Tron, SGD}, that is \u2013 gradient descent, conjugate gradient, truncated Newton and stochastic gradient descent.\nFor SVC, the dataset has to be binary (i.e, the number of classes are only two). For non-binary dataset use the following command:\n>> java -cp /fastLC.jar fastLC.BVDcrossvalx -t /dataset.arff -i 2 -x 2\n-W onevsAllSVCclassifier -- -V -O \"Tron\". For computing results on discretized data, either pre discretize the dataset, or use the -D flag to convert quantitative attributes into qualitative one by the learner."}, {"heading": "Appendix B. Details of Datasets", "text": "Domain Case NomAtt NumAtt Class MissVal Description\nHIGGS 11000000 0 28 2 N This dataset is generated using Monte Carlo simulation, related to\nseparating particle-producing collisions from a background source,\nincluding 21 kinematic properties and 7 high level attributes.\nContinued on next page"}], "references": [{"title": "Dangers of using \u201doptimal\u201d cutpoints in the evaluation of prognostic factors", "author": ["D. Altman", "B. Lausen", "W. Sauerbrei", "M. Schumacher"], "venue": "Journal of the National Cancer Institute,", "citeRegEx": "Altman et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Altman et al\\.", "year": 1994}, {"title": "The cost of dichotomising continuous variables", "author": ["D.G. Altman", "P. Royston"], "venue": "BMJ, 332,", "citeRegEx": "Altman and Royston.,? \\Q2006\\E", "shortCiteRegEx": "Altman and Royston.", "year": 2006}, {"title": "The need for low bias algorithms in classification learning from large data sets", "author": ["Damien Brain", "Geoffrey I. Webb"], "venue": "In PKDD,", "citeRegEx": "Brain and Webb.,? \\Q2002\\E", "shortCiteRegEx": "Brain and Webb.", "year": 2002}, {"title": "URL https://www.csie.ntu.edu.tw/~cjlin/ liblinear", "author": ["L. Chih-Jen"], "venue": "Liblinear library,", "citeRegEx": "Chih.Jen.,? \\Q2010\\E", "shortCiteRegEx": "Chih.Jen.", "year": 2010}, {"title": "Supervised and unsupervised discretization of continuous features", "author": ["J. Dougherty", "R. Kohavi", "M. Sahami"], "venue": "In ICML,", "citeRegEx": "Dougherty et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dougherty et al\\.", "year": 1995}, {"title": "On the handling of continuous-valued attributes in decision tree generation", "author": ["U.M. Fayyad", "K.B. Irani"], "venue": "Machine Learning,", "citeRegEx": "Fayyad and Irani.,? \\Q1992\\E", "shortCiteRegEx": "Fayyad and Irani.", "year": 1992}, {"title": "Differential misclassification arising from nondifferential errors in exposure", "author": ["C. Flegal", "P. Keyl", "P. Nieto"], "venue": "measurement. American Journal of Epidemiology,", "citeRegEx": "Flegal et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Flegal et al\\.", "year": 1991}, {"title": "Methodological issues in case-control studies. III: The effect of joint misclassification of risk factors and confounding factors upon estimation and power", "author": ["K.Y. Fung", "G.R. Howe"], "venue": "International Journal of Epidemiology,", "citeRegEx": "Fung and Howe.,? \\Q1984\\E", "shortCiteRegEx": "Fung and Howe.", "year": 1984}, {"title": "A survey of discretization techniques: Taxonomy and empirical analysis in supervised learning", "author": ["S. Garcia", "J. Luengo", "J. Saez", "V. Lopez", "F. Herrera"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Garcia et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Garcia et al\\.", "year": 2013}, {"title": "Dose-response and trend analysis in epidemiology: Alternatives to categorical analysis", "author": ["S. Greenland"], "venue": "Epidemiology, 6:356\u2013365,", "citeRegEx": "Greenland.,? \\Q1995\\E", "shortCiteRegEx": "Greenland.", "year": 1995}, {"title": "Why discretization works for naive bayesian classifiers", "author": ["C.N. Hsu", "H. J Huang", "T.T. Wong"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Hsu et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2000}, {"title": "Implications of the dirichlet assumption for discretization of continuous variables in naive bayesian classifiers", "author": ["C.N. Hsu", "H. J Huang", "T.T. Wong"], "venue": "Machine Learning,", "citeRegEx": "Hsu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2003}, {"title": "Negative consequences of dichotomizing continuous predictor variables", "author": ["J.R. Irwin", "G.H. McClelland"], "venue": "Journal of Marketing Research,", "citeRegEx": "Irwin and McClelland.,? \\Q2003\\E", "shortCiteRegEx": "Irwin and McClelland.", "year": 2003}, {"title": "Error-based and entropy-based discretization of continuous features", "author": ["R. Kohavi", "M. Sahami"], "venue": "In AAAI,", "citeRegEx": "Kohavi and Sahami.,? \\Q1996\\E", "shortCiteRegEx": "Kohavi and Sahami.", "year": 1996}, {"title": "Bias plus variance decomposition for zero-one loss functions", "author": ["R. Kohavi", "D. Wolpert"], "venue": "In ICML,", "citeRegEx": "Kohavi and Wolpert.,? \\Q1996\\E", "shortCiteRegEx": "Kohavi and Wolpert.", "year": 1996}, {"title": "UCI machine learning repository, 2017", "author": ["M. Lichman"], "venue": "URL http://archive.ics.uci.edu/ ml", "citeRegEx": "Lichman.,? \\Q2017\\E", "shortCiteRegEx": "Lichman.", "year": 2017}, {"title": "Discretization: An enabling technique", "author": ["H. Liu", "F. Hussain", "C.L. Tan", "M. Dash"], "venue": "Machine Learning,", "citeRegEx": "Liu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2002}, {"title": "Improving classification performance with discretization on biomedical datasets", "author": ["J.L. Lustgarten", "V. Gopalakrishnan", "H. Grover", "S. Visweswaran"], "venue": "In AMIA Symposium Proceedings,", "citeRegEx": "Lustgarten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lustgarten et al\\.", "year": 2008}, {"title": "On the practice of dichotomization of quantitative variables", "author": ["R.C. MacCallum", "S. Zhang", "K.J. Preacher", "D.D. Rucker"], "venue": "Psychological Methods,", "citeRegEx": "MacCallum et al\\.,? \\Q2002\\E", "shortCiteRegEx": "MacCallum et al\\.", "year": 2002}, {"title": "An experimental investigation of the effect of discrete attributes on the precision of classification methods", "author": ["R.E. Maleki", "S.M. Iranmanesh", "B.M. Bidgoli"], "venue": "In Information and Communication Technologies,", "citeRegEx": "Maleki et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Maleki et al\\.", "year": 2009}, {"title": "The need for biases in learning generalizations", "author": ["T.M. Mitchell"], "venue": "Technical Report CBMTR-117,", "citeRegEx": "Mitchell.,? \\Q1980\\E", "shortCiteRegEx": "Mitchell.", "year": 1980}, {"title": "Machine learning: a probabilistic perspective", "author": ["K Murphy"], "venue": null, "citeRegEx": "Murphy.,? \\Q2012\\E", "shortCiteRegEx": "Murphy.", "year": 2012}, {"title": "A survey of truncated newton methods", "author": ["S.G. Nash"], "venue": "Journal of Computational and Applied Mathematics,", "citeRegEx": "Nash.,? \\Q2000\\E", "shortCiteRegEx": "Nash.", "year": 2000}, {"title": "Effect of exposure misclassification on regression analysis", "author": ["S. Reade-Christopher", "L. Kupper"], "venue": "Biometrics, 47:535\u2013548,", "citeRegEx": "Reade.Christopher and Kupper.,? \\Q1991\\E", "shortCiteRegEx": "Reade.Christopher and Kupper.", "year": 1991}, {"title": "A note on dichotomization of continuous response variable in the presence of contamination and model specification", "author": ["Y. Shentu", "M. Xie"], "venue": "Statistics in Medicine,", "citeRegEx": "Shentu and Xie.,? \\Q2010\\E", "shortCiteRegEx": "Shentu and Xie.", "year": 2010}, {"title": "A bias-variance analysis of a real world learning problem: The coil challenge", "author": ["P. van der Putten", "M. van Someren"], "venue": "Machine Learning,", "citeRegEx": "Putten and Someren.,? \\Q2000\\E", "shortCiteRegEx": "Putten and Someren.", "year": 2000}, {"title": "Multiboosting: A technique for combining boosting and wagging", "author": ["G.I. Webb"], "venue": "Machine Learning,", "citeRegEx": "Webb.,? \\Q2000\\E", "shortCiteRegEx": "Webb.", "year": 2000}, {"title": "Discretization for naive-Bayes learning: managing discretization bias and variance", "author": ["Y. Yang", "G.I. Webb"], "venue": "Machine Learning,", "citeRegEx": "Yang and Webb.,? \\Q2009\\E", "shortCiteRegEx": "Yang and Webb.", "year": 2009}], "referenceMentions": [{"referenceID": 20, "context": "One of the many factors that affect the error of a learning system is its representation bias (van der Putten and van Someren, 2004), or, as it is also called, its hypothesis language bias (Mitchell, 1980).", "startOffset": 189, "endOffset": 205}, {"referenceID": 21, "context": "Learning algorithms that use linear models, such as logistic regression (LR) (Murphy, 2012) and support vector classifiers (SVC) (Chih-Jen, 2010), are very popular, possibly in part due to their lending themselves to convex optimization.", "startOffset": 77, "endOffset": 91}, {"referenceID": 3, "context": "Learning algorithms that use linear models, such as logistic regression (LR) (Murphy, 2012) and support vector classifiers (SVC) (Chih-Jen, 2010), are very popular, possibly in part due to their lending themselves to convex optimization.", "startOffset": 129, "endOffset": 145}, {"referenceID": 16, "context": "2 Discretization Discretization is a common process in machine learning that is used to convert a quantitative into a qualitative attribute (Liu et al., 2002; Garcia et al., 2013).", "startOffset": 140, "endOffset": 179}, {"referenceID": 8, "context": "2 Discretization Discretization is a common process in machine learning that is used to convert a quantitative into a qualitative attribute (Liu et al., 2002; Garcia et al., 2013).", "startOffset": 140, "endOffset": 179}, {"referenceID": 13, "context": "technique Entropy-Minimization Discretization (EMD) sorts the quantitative attribute\u2019s values and then finds the cut-point such the information gain is maximized across the splits (Kohavi and Sahami, 1996).", "startOffset": 180, "endOffset": 205}, {"referenceID": 22, "context": "This method is known as \u2018Truncated Newton method\u2019 (Nash, 2000).", "startOffset": 50, "endOffset": 62}, {"referenceID": 12, "context": "In Statistics and many of its related and applied branches (such as epidemiology, medical research and consumer marketing), it goes by names of \u2018dichotomization\u2019 and \u2018categorization\u2019 (where the two techniques differ as the former splits the measurement scale into two while the later can have more than two categories) \u2013 and has been examined in many studies (Irwin and McClelland, 2003; MacCallum et al., 2002; Greenland, 1995).", "startOffset": 359, "endOffset": 428}, {"referenceID": 18, "context": "In Statistics and many of its related and applied branches (such as epidemiology, medical research and consumer marketing), it goes by names of \u2018dichotomization\u2019 and \u2018categorization\u2019 (where the two techniques differ as the former splits the measurement scale into two while the later can have more than two categories) \u2013 and has been examined in many studies (Irwin and McClelland, 2003; MacCallum et al., 2002; Greenland, 1995).", "startOffset": 359, "endOffset": 428}, {"referenceID": 9, "context": "In Statistics and many of its related and applied branches (such as epidemiology, medical research and consumer marketing), it goes by names of \u2018dichotomization\u2019 and \u2018categorization\u2019 (where the two techniques differ as the former splits the measurement scale into two while the later can have more than two categories) \u2013 and has been examined in many studies (Irwin and McClelland, 2003; MacCallum et al., 2002; Greenland, 1995).", "startOffset": 359, "endOffset": 428}, {"referenceID": 27, "context": "An exception is for Bayesian classifiers, where it is common practice to discretize numeric attributes (Yang and Webb, 2009).", "startOffset": 103, "endOffset": 124}, {"referenceID": 1, "context": "For example, Altman and Royston (2006) write: .", "startOffset": 13, "endOffset": 39}, {"referenceID": 1, "context": "This may ease interpretation and presentation of results (Altman and Royston, 2006).", "startOffset": 57, "endOffset": 83}, {"referenceID": 7, "context": "\u2022 If there is error in the measurement scale, discretization can improve the performance of the model by reducing the contamination (Flegal and Keyl, 1991; ReadeChristopher and Kupper, 1991; Fung and Howe, 1984; Shentu and Xie, 2010).", "startOffset": 132, "endOffset": 233}, {"referenceID": 24, "context": "\u2022 If there is error in the measurement scale, discretization can improve the performance of the model by reducing the contamination (Flegal and Keyl, 1991; ReadeChristopher and Kupper, 1991; Fung and Howe, 1984; Shentu and Xie, 2010).", "startOffset": 132, "endOffset": 233}, {"referenceID": 27, "context": "By converting a quantitative attribute Xi into a qualitative one X\u2217 i , the probabilities will take the form of P(x \u2217 i | y) which may be reliably estimated from the data as there will be many xi values falling into the same interval (Yang and Webb, 2009).", "startOffset": 234, "endOffset": 255}, {"referenceID": 0, "context": "If those assumptions are correct, discretization will have a negative impact, but if those assumptions are false, discretization may lead to better results (Altman et al., 1994).", "startOffset": 156, "endOffset": 177}, {"referenceID": 10, "context": "The effectiveness of discretization for naive Bayes classifier is relatively well studied (Hsu et al., 2000; Dougherty et al., 1995; Yang and Webb, 2009).", "startOffset": 90, "endOffset": 153}, {"referenceID": 4, "context": "The effectiveness of discretization for naive Bayes classifier is relatively well studied (Hsu et al., 2000; Dougherty et al., 1995; Yang and Webb, 2009).", "startOffset": 90, "endOffset": 153}, {"referenceID": 27, "context": "The effectiveness of discretization for naive Bayes classifier is relatively well studied (Hsu et al., 2000; Dougherty et al., 1995; Yang and Webb, 2009).", "startOffset": 90, "endOffset": 153}, {"referenceID": 14, "context": "The effect of discretization on various classification algorithms such as naive Bayes, Support Vector Machines and Random Forest is discussed in Lustgarten et al. (2008). On many biomedical datasets, it is shown that discretization can greatly improve the performance of the learning algorithm.", "startOffset": 145, "endOffset": 170}, {"referenceID": 14, "context": "The effect of discretization on various classification algorithms such as naive Bayes, Support Vector Machines and Random Forest is discussed in Lustgarten et al. (2008). On many biomedical datasets, it is shown that discretization can greatly improve the performance of the learning algorithm. The role of discretization as feature selection technique is also explored. On various contrived datasets, Maleki et al. (2009) studied the effect of discretization on the precision and recall of various classification methods.", "startOffset": 145, "endOffset": 423}, {"referenceID": 4, "context": ", 2000; Dougherty et al., 1995; Yang and Webb, 2009). Dougherty et al. (1995) conducted an empirical study of naive Bayes with four well-known discretization methods and found that all the discretization methods result in significantly reducing error relative to a naive Bayes that assumes a Gaussian distribution for the continuous variables.", "startOffset": 8, "endOffset": 78}, {"referenceID": 15, "context": "In this section, we compare the performance of linear classifier with discretized linear classifier on various datasets from the UCI repository (Lichman, 2017).", "startOffset": 144, "endOffset": 159}, {"referenceID": 2, "context": "We expect low variance algorithms to have relatively low error for small data and low bias algorithms to have relatively low error for large data (Brain and Webb, 2002).", "startOffset": 146, "endOffset": 168}, {"referenceID": 4, "context": "Since, this is not a comparative study on the relative efficacies of various discretization techniques for linear classifiers, we only report results with supervised entropy-based discretization of Fayyad and Irani (1992), which we found gives better results than other discretization methods such as equal-frequency, equal-width, etc.", "startOffset": 198, "endOffset": 222}, {"referenceID": 2, "context": "We expect low variance algorithms to have relatively low error for small data and low bias algorithms to have relatively low error for large data (Brain and Webb, 2002). There are a number of different bias-variance decomposition definitions. In this research, we use the bias and variance definitions of Kohavi and Wolpert (1996) together with the repeated cross-validation bias-variance estimation method proposed by Webb (2000).", "startOffset": 147, "endOffset": 331}, {"referenceID": 2, "context": "We expect low variance algorithms to have relatively low error for small data and low bias algorithms to have relatively low error for large data (Brain and Webb, 2002). There are a number of different bias-variance decomposition definitions. In this research, we use the bias and variance definitions of Kohavi and Wolpert (1996) together with the repeated cross-validation bias-variance estimation method proposed by Webb (2000). We report Win-Draw-Loss (W-D-L) results when comparing the 0-1 Loss, RMSE, bias and variance of two models.", "startOffset": 147, "endOffset": 431}], "year": 2017, "abstractText": "Learning algorithms that learn linear models often have high representation bias on real-world problems. In this paper, we show that this representation bias can be greatly reduced by discretization. Discretization is a common procedure in machine learning that is used to convert a quantitative attribute into a qualitative one. It is often motivated by the limitation of some learners to qualitative data. Discretization loses information, as fewer distinctions between instances are possible using discretized data relative to undiscretized data. In consequence, where discretization is not essential, it might appear desirable to avoid it. However, it has been shown that discretization often substantially reduces the error of the linear generative Bayesian classifier naive Bayes. This motivates a systematic study of the effectiveness of discretizing quantitative attributes for other linear classifiers. In this work, we study the effect of discretization on the performance of linear classifiers optimizing three distinct discriminative objective functions \u2014 logistic regression (optimizing negative log-likelihood), support vector classifiers (optimizing hinge loss) and a zero-hidden layer artificial neural network (optimizing mean-square-error). We show that discretization can greatly increase the accuracy of these linear discriminative learners by reducing their representation bias, especially on big datasets. We substantiate our claims with an empirical study on 42 benchmark datasets.", "creator": "TeX"}}}