{"id": "1701.07266", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2017", "title": "k*-Nearest Neighbors: From Global to Local", "abstract": "The weighted k-nearest neighbors algorithm is one of the most fundamental non-parametric methods in pattern recognition and machine learning. The question of setting the optimal number of neighbors as well as the optimal weights has received much attention throughout the years, nevertheless this problem seems to have remained unsettled. In this paper we offer a simple approach to locally weighted regression/classification, where we make the bias-variance tradeoff explicit (i.e., that we are not only the least likely to do a better job of modeling the probability of a random distribution for each neighbor, but also to show that the probability of a random distribution for each neighbor is actually less than one in 10). For a subset of the same problem we can use randomization and a classifier, we have a classifier that is trained to show the optimal weights of a set of neighbors, i.e., a random distribution. For a subset of the same problem we can use randomization and a classifier, we have a classifier that is trained to show the optimal weights of a set of neighbors, i.e., a random distribution. For a subset of the same problem we can use randomization and a classifier, we have a classifier that is trained to show the optimal weights of a set of neighbors, i.e., a random distribution. For a subset of the same problem we can use randomization and a classifier that is trained to show the optimal weights of a set of neighbors, i.e., a random distribution. For a subset of the same problem we can use randomization and a classifier that is trained to show the optimal weights of a set of neighbors, i.e., a random distribution. For a subset of the same problem we can use randomization and a classifier that is trained to show the optimal weights of a set of neighbors, i.e., a random distribution. For a subset of the same problem we can use randomization and a classifier that is trained to show the optimal weights of a set of neighbors, i.e., a random distribution. For a subset of the same problem we can use randomization and a classifier that is trained to show the optimal weights of a set of neighbors, i.e., a random distribution. For a subset of the same problem we can use randomization and a classifier that is trained to show the optimal weights of a set of neighbors, i.e., a random distribution. For a subset of the same problem we can use randomization and a", "histories": [["v1", "Wed, 25 Jan 2017 11:18:18 GMT  (38kb,D)", "http://arxiv.org/abs/1701.07266v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["oren anava", "kfir y levy"], "accepted": true, "id": "1701.07266"}, "pdf": {"name": "1701.07266.pdf", "metadata": {"source": "CRF", "title": "k\u2217-Nearest Neighbors: From Global to Local", "authors": ["Oren Anava", "Kfir Y. Levy"], "emails": ["oren@voleon.com.", "yehuda.levy@inf.ethz.ch."], "sections": [{"heading": "1 Introduction", "text": "The k-nearest neighbors (k-NN) algorithm Cover and Hart (1967); Fix and Hodges Jr (1951), and Nadarays-Watson estimation Nadaraya (1964); Watson (1964) are the cornerstones of non-parametric learning. Owing to their simplicity and flexibility, these procedures had become the methods of choice in many scenarios Wu et al. (2008), especially in settings where the underlying model is complex. Modern applications of the k-NN algorithm include recommendation systems Adeniyi et al. (2016), text categorization Trstenjak et al. (2014), heart disease classification Deekshatulu et al. (2013), and financial market prediction Imandoust and Bolandraftar (2013), amongst others.\n\u2217The Voleon Group. Email: oren@voleon.com. \u2020Department of Computer Science, ETH Zu\u0308rich. Email: yehuda.levy@inf.ethz.ch.\nar X\niv :1\n70 1.\n07 26\n6v 1\n[ st\nat .M\nL ]\n2 5\nJa n\nA successful application of the weighted k-NN algorithm requires a careful choice of three ingredients: the number of nearest neighbors k, the weight vector \u03b1, and the distance metric. The latter requires domain knowledge and is thus henceforth assumed to be set and known in advance to the learner. Surprisingly, even under this assumption, the problem of choosing the optimal k and \u03b1 is not fully understood and has been studied extensively since the 1950\u2019s under many different regimes. Most of the theoretic work focuses on the asymptotic regime in which the number of samples n goes to infinity Devroye et al. (2013); Samworth et al. (2012); Stone (1977), and ignores the practical regime in which n is finite. More importantly, the vast majority of k-NN studies aim at finding an optimal value of k per dataset, which seems to overlook the specific structure of the dataset and the properties of the data points whose labels we wish to estimate. While kernel based methods such as NadarayaWatson enable an adaptive choice of the weight vector \u03b1, theres still remains the question of how to choose the kernel\u2019s bandwidth \u03c3, which could be thought of as the parallel of the number of neighbors k in k-NN. Moreover, there is no principled approach towards choosing the kernel function in practice.\nIn this paper we offer a coherent and principled approach to adaptively choosing the number of neighbors k and the corresponding weight vector \u03b1 \u2208 Rk per decision point. Given a new decision point, we aim to find the best locally weighted predictor, in the sense of minimizing the distance between our prediction and the ground truth. In addition to yielding predictions, our approach enbles us to provide a per decision point guarantee for the confidence of our predictions. Fig. 1 illustrates the importance of choosing k adaptively. In contrast to previous works on non-parametric regression/classification, we do not assume that the data {(xi, yi)}ni=1 arrives from some (unknown) underlying distribution, but rather make a weaker assumption that the labels {yi}ni=1 are independent given the data points {xi}ni=1, allowing the latter to be chosen arbitrarily. Alongside providing a theoretical basis for our approach, we conduct an empirical study that demonstrates its superiority with respect to the state-of-the-art.\nThis paper is organized as follows. In Section 2 we introduce our setting and assumptions, and derive the locally optimal prediction problem. In Section 3 we analyze the solution of the above prediction problem, and introduce a greedy algorithm designed to efficiently find the exact solution. Section 4 presents our experimental study, and Section 5 concludes."}, {"heading": "1.1 Related Work", "text": "Asymptotic universal consistency is the most widely known theoretical guarantee for k-NN. This powerful guarantee implies that as the number of samples n goes to infinity, and also k \u2192\u221e, k/n\u2192 0, then the risk of the k-NN rule converges to the risk of the Bayes classifier for any underlying data distribution. Similar guarantees hold for weighted k-NN rules, with the additional assumptions that \u2211k i=1 \u03b1i = 1 and maxi\u2264n \u03b1i \u2192 0, Stone (1977); Devroye et al. (2013). In the regime of practical interest where the number of samples n is finite, using k = b \u221a nc neighbors is a widely mentioned rule of thumb Devroye et al. (2013). Nevertheless, this rule often yields poor results, and in the regime of finite samples it is usually advised to choose k using cross-validation. Similar consistency results apply to kernel based local methods Devroye et al. (1980); Gyo\u0308rfi et al. (2006).\nA novel study of k-NN by Samworth, Samworth et al. (2012), derives a closed form expression for the optimal weight vector, and extracts the optimal number of neighbors. However, this result is only optimal under several restrictive assumptions, and only holds for the asymptotic regime where n \u2192 \u221e. Furthermore, the above optimal number of neighbors/weights do not adapt, but are rather fixed over all decision points given the dataset. In the context of kernel based methods, it is possible to extract an expression for the optimal kernel\u2019s bandwidth \u03c3 Gyo\u0308rfi et al. (2006); Fan and Gijbels (1996). Nevertheless, this bandwidth is fixed over all decision points, and is only optimal under several restrictive assumptions.\nThere exist several heuristics to adaptively choosing the number of neighbors and weights separately for each decision point. In Wettschereck and Dietterich (1994); Sun and Huang it is suggested to use local cross-validation in order to adapt the value of k to different decision points. Conversely, Ghosh Ghosh (2007) takes a Bayesian approach towards choosing k adaptively. Focusing on the multiclass classification setup, it is suggested in Baoli et al. (2004) to consider different values of k for each class, choosing k proportionally to the class populations. Similarly, there exist several attitudes towards adaptively choosing the kernel\u2019s bandwidth \u03c3, for kernel based methods Abramson (1982); Silverman (1986); Demir and Toktamis\u0327 (2010); Aljuhani et al. (2014).\nLearning the distance metric for k-NN was extensively studied throughout the last decade. There are several approaches towards metric learning, which roughly divide into linear/non-linear learning methods. It was found that metric learning may significantly affect the performance of k-NN in numerous applications, including computer vision, text analysis, program analysis and more. A comprehensive survey by Kulis Kulis (2012) provides a review of the metric learning literature. Throughout this work we assume that the distance metric is fixed, and thus the focus is on finding the best (in a sense) values of k and \u03b1 for each new data point.\nTwo comprehensive monographs, Devroye et al. (2013) and Biau and Devroye (2015), provide an extensive survey of the existing literature regarding k-NN rules, including theoretical guarantees, useful practices, limitations and more."}, {"heading": "2 Problem Definition", "text": "In this section we present our setting and assumptions, and formulate the locally weighted optimal estimation problem. Recall we seek to find the best local prediction in a sense of minimizing the distance between this prediction and the ground truth. The problem at hand is thus defined as follows: We are given n data points x1, . . . , xn \u2208 Rd, and n corresponding labels1 y1, . . . , yn \u2208 R. Assume that for any i \u2208 {1, . . . , n} = [n] it holds that yi = f(xi) + i, where f(\u00b7) and i are such that:\n(1) f(\u00b7) is a Lipschitz continuous function: For any x, y \u2208 Rd it holds that |f(x)\u2212 f(y)| \u2264 L \u00b7 d(x, y), where the distance function d(\u00b7, \u00b7) is set and known in advance. This assumption is rather standard when considering nearest\n1Note that our analysis holds for both setups of classification/regression. For brevity we use a classification task terminology, relating to the yi\u2019s as labels. Our analysis extends directly to the regression setup.\nneighbors-based algorithms, and is required in our analysis to bound the socalled bias term (to be later defined). In the binary classification setup we assume that f : Rd 7\u2192 [0, 1], and that given x its label y \u2208 {0, 1} is distributed Bernoulli(f(x)).\n(2) i\u2019s are noise terms: For any i \u2208 [n] it holds that E [ i|xi] = 0 and | i| \u2264 b for some given b > 0. In addition, it is assumed that given the data points {xi}ni=1 then the noise terms { i}ni=1 are independent. This assumption is later used in our analysis to apply Hoeffding\u2019s inequality and bound the so-called variance term (to be later defined). Alternatively, we could assume that E [ 2i |xi] \u2264 b (instead of | i| \u2264 b), and apply Bernstein inequalities. The results and analysis remain qualitatively similar.\nGiven a new data point x0, our task is to estimate f(x0), where we restrict the estimator f\u0302(x0) to be of the form f\u0302(x0) = \u2211n i=1 \u03b1iyi. That is, the estimator is a weighted average of the given noisy labels. Formally, we aim at minimizing the absolute distance between our prediction and the ground truth f(x0), which translates into\nmin \u03b1\u2208\u2206n \u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 \u03b1iyi \u2212 f(x0) \u2223\u2223\u2223\u2223\u2223 (P1), where we minimize over the simplex, \u2206n = {\u03b1 \u2208 Rn| \u2211n i=1 \u03b1i = 1 and \u03b1i \u2265 0, \u2200i}. Decomposing the objective of (P1) into a sum of bias and variance terms, we arrive at the following relaxed objective:\u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 \u03b1iyi \u2212 f(x0) \u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 \u03b1i (yi \u2212 f(xi) + f(xi))\u2212 f(x0)\n\u2223\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 \u03b1i i + n\u2211 i=1 \u03b1i (f(xi)\u2212 f(x0)) \u2223\u2223\u2223\u2223\u2223 \u2264\n\u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 \u03b1i i \u2223\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 \u03b1i (f(xi)\u2212 f(x0)) \u2223\u2223\u2223\u2223\u2223 \u2264\n\u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 \u03b1i i \u2223\u2223\u2223\u2223\u2223+ L n\u2211 i=1 \u03b1id(xi, x0).\nBy Hoeffding\u2019s inequality (see supplementary material) it follows that | \u2211n\ni=1 \u03b1i i| \u2264 C\u2016\u03b1\u20162 for C = b \u221a 2 log ( 2 \u03b4 ) , w.p. at least 1\u2212 \u03b4. We thus arrive at a new optimization problem (P2), such that solving it would yield a guarantee for (P1) with high\nprobability:\nmin \u03b1\u2208\u2206n C\u2016\u03b1\u20162 + L n\u2211 i=1 \u03b1id(xi, x0) (P2).\nThe first term in (P2) corresponds to the noise in the labels and is therefore denoted as the variance term, whereas the second term corresponds to the distance between f(x0) and {f(xi)}ni=1 and is thus denoted as the bias term."}, {"heading": "3 Algorithm and Analysis", "text": "In this section we discuss the properties of the optimal solution for (P2), and present a greedy algorithm which is designed in order to efficiently find the exact solution of the latter objective (see Section 3.1). Given a decision point x0, Theorem 3.1 demonstrates that the optimal weight \u03b1i of the data point xi is proportional to \u2212d(xi, x0) (closer points are given more weight). Interestingly, this weight decay is quite slow compared to popular weight kernels, which utilize sharper decay schemes, e.g., exponential/inversely-proportional. Theorem 3.1 also implies a cutoff effect, meaning that there exists k\u2217 \u2208 [n], such that only the k\u2217 nearest neighbors of x0 donate to the prediction of its label. Note that both \u03b1 and k\u2217 may adapt from one x0 to another. Also notice that the optimal weights depend on a single parameter L/C, namely the Lipschitz to noise ratio. As L/C grows k\u2217 tends to be smaller, which is quite intuitive.\nWithout loss of generality, assume that the points are ordered in ascending order according to their distance from x0, i.e., d(x1, x0) \u2264 d(x2, x0) \u2264 . . . \u2264 d(xn, x0). Also, let \u03b2 \u2208 Rn be such that \u03b2i = Ld(xi, x0)/C. Then, the following is our main theorem:\nTheorem 3.1. There exists \u03bb > 0 such that the optimal solution of (P2) is of the form\n\u03b1\u2217i = (\u03bb\u2212 \u03b2i) \u00b7 1 {\u03b2i < \u03bb}\u2211n i=1 (\u03bb\u2212 \u03b2i) \u00b7 1 {\u03b2i < \u03bb} . (1)\nFurthermore, the value of (P2) at the optimum is C\u03bb.\nFollowing is a direct corollary of the above Theorem:\nCorollary 3.2. There exists 1 \u2264 k\u2217 \u2264 n such that for the optimal solution of (P2) the following applies:\n\u03b1\u2217i > 0; \u2200i \u2264 k\u2217 and \u03b1\u2217i = 0; \u2200i > k\u2217.\nProof of Theorem 3.1. Notice that (P2) may be written as follows:\nmin \u03b1\u2208\u2206n\nC ( \u2016\u03b1\u20162 + \u03b1>\u03b2 ) (P2).\nWe henceforth ignore the parameter C. In order to find the solution of (P2), let us first consider its Lagrangian:\nL(\u03b1, \u03bb,\u03b8) = \u2016\u03b1\u20162 + \u03b1>\u03b2 + \u03bb ( 1\u2212\nn\u2211 i=1 \u03b1i\n) \u2212\nn\u2211 i=1 \u03b8i\u03b1i,\nwhere \u03bb \u2208 R is the multiplier of the equality constraint \u2211\ni \u03b1i = 1, and \u03b81, . . . , \u03b8n \u2265 0 are the multipliers of the inequality constraints \u03b1i \u2265 0, \u2200i \u2208 [n]. Since (P2) is convex, any solution satisfying the KKT conditions is a global minimum. Deriving the Lagrangian with respect to \u03b1, we get that for any i \u2208 [n]:\n\u03b1i \u2016\u03b1\u20162 = \u03bb\u2212 \u03b2i + \u03b8i.\nDenote by \u03b1\u2217 the optimal solution of (P2). By the KKT conditions, for any \u03b1\u2217i > 0 it follows that \u03b8i = 0. Otherwise, for any i such that \u03b1 \u2217 i = 0 it follows that \u03b8i \u2265 0, which implies \u03bb \u2264 \u03b2i. Thus, for any nonzero weight \u03b1\u2217i > 0 the following holds: \u03b1\u2217i \u2016\u03b1\u2217\u20162 = \u03bb\u2212 \u03b2i. (2)\nSquaring and summing Equation (2) over all the nonzero entries of \u03b1, we arrive at the following equation for \u03bb:\n1 = \u2211 \u03b1\u2217i>0 (\u03b1\u2217i ) 2 \u2016\u03b1\u2217\u201622 = \u2211 \u03b1\u2217i>0 (\u03bb\u2212 \u03b2i)2. (3)\nNext, we show that the value of the objective at the optimum is C\u03bb. Indeed, note that by Equation (2) and the equality constraint \u2211 i \u03b1 \u2217 i = 1, any \u03b1 \u2217 i > 0 satisfies\n\u03b1\u2217i = \u03bb\u2212 \u03b2i A , where A = \u2211 \u03b1\u2217i>0 (\u03bb\u2212 \u03b2i). (4)\nPlugging the above into the objective of (P2) yields C ( \u2016\u03b1\u2217\u20162 + \u03b1\u2217>\u03b2 ) = C\nA \u221a\u2211 \u03b1\u2217i>0 (\u03bb\u2212 \u03b2i)2 + C A \u2211 \u03b1\u2217i>0 (\u03bb\u2212 \u03b2i)(\u03b2i \u2212 \u03bb+ \u03bb)\n= C A \u2212 C A \u2211 \u03b1\u2217i>0 (\u03bb\u2212 \u03b2i)2 + C\u03bb A \u2211 \u03b1\u2217i>0 (\u03bb\u2212 \u03b2i)\n= C\u03bb,\nwhere in the last equality we used Equation (3), and substituted A = \u2211\n\u03b1\u2217i>0 (\u03bb \u2212\n\u03b2i).\n3.1 Solving (P2) Efficiently\nNote that (P2) is a convex optimization problem, and it can be therefore (approximately) solved efficiently, e.g., via any first order algorithm. Concretely, given an accuracy > 0, any off-the-shelf convex optimization method would require a running time which is poly(n, 1 ) in order to find an -optimal solution to (P2)2. Note that the calculation of (the unsorted) \u03b2 requires an additional computational cost of O(nd).\nHere we present an efficient method that computes the exact solution of (P2). In addition to the O(nd) cost for calculating \u03b2, our algorithm requires an O(n log n) cost for sorting the entries of \u03b2, as well as an additional running time of O(k\u2217), where k\u2217 is the number of non-zero elements at the optimum. Thus, the running time of our method is independent of any accuracy , and may be significantly better compared to any off-the-shelf optimization method. Note that in some cases Indyk and Motwani (1998), using advanced data structures may decrease the cost of finding the nearest neighbors (i.e., the sorted \u03b2), yielding a running time substantially smaller than O(nd+ n log n).\nOur method is depicted in Algorithm 1. Quite intuitively, the core idea is to greedily add neighbors according to their distance form x0 until a stopping condition is fulfilled (indicating that we have found the optimal solution). Letting CsortNN, be the computational cost of calculating the sorted vector \u03b2, the following theorem presents our guarantees.\nTheorem 3.3. Algorithm 1 finds the exact solution of (P2) within k\u2217 iterations, with an O(k\u2217 + CsortNN) running time.\nProof of Theorem 3.3. Denote by \u03b1\u2217 the optimal solution of (P2), and by k\u2217 the corresponding number of nonzero weights. By Corollary 3.2, these k\u2217 nonzero weights correspond to the k\u2217 smallest values of \u03b2. Thus, we are left to show that (1) the optimal \u03bb is of the form calculated by the algorithm; and (2) the algorithm halts after exactly k\u2217 iterations and outputs the optimal solution.\n2Note that (P2) is not strongly-convex, and therefore the polynomial dependence on 1/ rather than log(1/ ) for first order methods. Other methods such as the Ellipsoid depend logarithmically on 1/ , but suffer a worse dependence on n compared to first order methods.\nAlgorithm 1 k\u2217-NN\nInput: vector of ordered distances \u03b2 \u2208 Rn, noisy labels y1, . . . , yn \u2208 R Set: \u03bb0 = \u03b21 + 1, k = 0 while \u03bbk > \u03b2k+1 and k \u2264 n\u2212 1 do\nUpdate: k \u2190 k + 1\nCalculate: \u03bbk = 1 k (\u2211k i=1 \u03b2i + \u221a k + (\u2211k i=1 \u03b2i )2 \u2212 k \u2211k i=1 \u03b2 2 i ) end while Return: estimation f\u0302(x0) = \u2211 i \u03b1iyi, where \u03b1 \u2208 \u2206n is a weight vector such\n\u03b1i = (\u03bbk\u2212\u03b2i)\u00b71{\u03b2i<\u03bbk}\u2211n i=1(\u03bbk\u2212\u03b2i)\u00b71{\u03b2i<\u03bbk}\nLet us first find the optimal \u03bb. Since the non-zero elements of the optimal solution correspond to the k\u2217 smallest values of \u03b2, then Equation (3) is equivalent to the following quadratic equation in \u03bb:\nk\u2217\u03bb2 \u2212 2\u03bb k\u2217\u2211 i=1 \u03b2i + ( k\u2217\u2211 i=1 \u03b22i \u2212 1 ) = 0.\nSolving for \u03bb and neglecting the solution that does not agree with \u03b1i \u2265 0, \u2200i \u2208 [n], we get\n\u03bb = 1\nk\u2217  k\u2217\u2211 i=1 \u03b2i + \u221a\u221a\u221a\u221ak\u2217 +( k\u2217\u2211 i=1 \u03b2i )2 \u2212 k\u2217 k\u2217\u2211 i=1 \u03b22i  . (5) The above implies that given k\u2217, the optimal solution (satisfying KKT) can be directly derived by a calculation of \u03bb according to Equation (5) and computing the \u03b1i\u2019s according to Equation (1). Since Algorithm 1 calculates \u03bb and \u03b1 in the form appearing in Equations (5) and (1) respectively, it is therefore sufficient to show that it halts after exactly k\u2217 iterations in order to prove its optimality. The latter is a direct consequence of the following conditions:\n(1) Upon reaching iteration k\u2217 Algorithm 1 necessarily halts.\n(2) For any k \u2264 k\u2217 it holds that \u03bbk \u2208 R.\n(3) For any k < k\u2217 Algorithm 1 does not halt.\nNote that the first condition together with the second condition imply that \u03bbk is well defined until the algorithm halts (in the sense that the \u201c > \u201doperation in the while condition is meaningful). The first condition together with the third condition imply that the algorithm halts after exactly k\u2217 iterations, which concludes the proof. We are now left to show that the above three conditions hold:\nCondition (1): Note that upon reaching k\u2217, Algorithm 1 necessarily calculates the optimal \u03bb = \u03bbk\u2217 . Moreover, the entries of \u03b1 \u2217 whose indices are greater than k\u2217 are necessarily zero, and in particular, \u03b1\u2217k\u2217+1 = 0. By Equation (1), this implies that \u03bbk\u2217 \u2264 \u03b2k\u2217+1, and therefore the algorithm halts upon reaching k\u2217.\nIn order to establish conditions (2) and (3) we require the following lemma:\nLemma 3.4. Let \u03bbk be as calculated by Algorithm 1 at iteration k. Then, for any k \u2264 k\u2217 the following holds:\n\u03bbk = min \u03b1\u2208\u2206(k)n\n( \u2016\u03b1\u20162 + \u03b1>\u03b2 ) , where \u2206(k)n = {\u03b1 \u2208 \u2206n : \u03b1i = 0, \u2200i > k}\nThe proof of Lemma 3.4 appears in Appendix B. We are now ready to prove the remaining conditions.\nCondition (2): Lemma 3.4 states that \u03bbk is the solution of a convex program over a nonempty set, therefore \u03bbk \u2208 R.\nCondition (3): By definition \u2206 (k) n \u2282 \u2206(k+1)n for any k < n. Therefore, Lemma 3.4 implies that \u03bbk \u2265 \u03bbk+1 for any k < k\u2217 (minimizing the same objective with stricter constraints yields a higher optimal value). Now assume by contradiction that Algorithm 1 halts at some k0 < k\n\u2217, then the stopping condition of the algorithm implies that \u03bbk0 \u2264 \u03b2k0+1. Combining the latter with \u03bbk \u2265 \u03bbk+1, \u2200k \u2264 k\u2217, and using \u03b2k \u2264 \u03b2k+1, \u2200k \u2264 n, we conclude that:\n\u03bbk\u2217 \u2264 \u03bbk0+1 \u2264 \u03bbk0 \u2264 \u03b2k0+1 \u2264 \u03b2k\u2217 .\nThe above implies that \u03b1k\u2217 = 0 (see Equation (1)), which contradicts Corollary 3.2 and the definition of k\u2217.\nRunning time: Note that the main running time burden of Algorithm 1 is the calculation of \u03bbk for any k \u2264 k\u2217. A naive calculation of \u03bbk requires an O(k) running time. However, note that \u03bbk depends only on \u2211k i=1 \u03b2i and \u2211k i=1 \u03b2 2 i . Updating these sums incrementally implies that we require only O(1) running time per iteration, yielding a total running time of O(k\u2217). The remaining O(CsortNN) running time is required in order to calculate the (sorted) \u03b2."}, {"heading": "3.2 Special Cases", "text": "The aim of this section is to discuss two special cases in which the bound of our algorithm coincides with familiar bounds in the literature, thus justifying the relaxed objective of (P2). We present here only a high-level description of both cases, and defer the formal details to the full version of the paper.\nThe solution of (P2) is a high probability upper-bound on the true prediction error | \u2211n i=1 \u03b1iyi \u2212 f(x0)|. Two interesting cases to consider in this context are \u03b2i = 0 for all i \u2208 [n], and \u03b21 = . . . = \u03b2n = \u03b2 > 0. In the first case, our algorithm includes all labels in the computation of \u03bb, thus yielding a confidence bound of 2C\u03bb = 2b \u221a\n(2/n) log (2/\u03b4) for the prediction error (with probability 1\u2212\u03b4). Not surprisingly, this bound coincides with the standard Hoeffding bound for the task of estimating the mean value of a given distribution based on noisy observations drawn from this distribution. Since the latter is known to be tight (in general), so is the confidence bound obtained by our algorithm. In the second case as well, our algorithm will use all data points to arrive at the confidence bound 2C\u03bb = 2Ld + 2b \u221a (2/n) log (2/\u03b4), where we denote d(x1, x0) = . . . = d(xn, x0) = d. The second term is again tight by concentration arguments, whereas the first term cannot be improved due to Lipschitz property of f(\u00b7), thus yielding an overall tight confidence bound for our prediction in this case."}, {"heading": "4 Experimental Results", "text": "The following experiments demonstrate the effectiveness of the proposed algorithm on several datasets. We start by presenting the baselines used for the comparison."}, {"heading": "4.1 Baselines", "text": "The standard k-NN: Given k, the standard k-NN finds the k nearest data points to x0 (assume without loss of generality that these data points are x1, . . . , xk), and then estimates f\u0302(x0) = 1 k \u2211k i=1 yi.\nThe Nadaraya-Watson estimator: This estimator assigns the data points with weights that are proportional to some given similarity kernel K : Rd \u00d7 Rd 7\u2192 R+. That is,\nf\u0302(x0) = \u2211n i=1K(xi, x0)yi\u2211n i=1K(xi, x0) .\nPopular choices of kernel functions include the Gaussian kernelK(xi, xj) = 1 \u03c3 e\u2212 \u2016xi\u2212xj\u2016\n2\n2\u03c32 ;\nEpanechnikov Kernel K(xi, xj) = 3 4\n( 1\u2212 \u2016xi\u2212xj\u2016 2\n\u03c32 ) 1{\u2016xi\u2212xj\u2016\u2264\u03c3}; and the triangular\nkernel K(xi, xj) = ( 1\u2212 \u2016xi\u2212xj\u2016 \u03c3 ) 1{\u2016xi\u2212xj\u2016\u2264\u03c3}. Due to lack of space, we present here only the best performing kernel function among the three listed above (on the tested datasets), which is the Gaussian kernel."}, {"heading": "4.2 Datasets", "text": "In our experiments we use 8 real-world datasets, all are available in the UCI repository website (https://archive.ics.uci.edu/ml/). In each of the datasets, the features vector consists of real values only, whereas the labels take different forms: in the first 6 datasets (QSAR, Diabetes, PopFailures, Sonar, Ionosphere, and Fertility), the labels are binary yi \u2208 {0, 1}. In the last two datasets (Slump and Yacht), the labels are real-valued. Note that our algorithm (as well as the other two baselines) applies to all datasets without requiring any adjustment. The number of samples n and the dimension of each sample d are given in Table 1 for each dataset."}, {"heading": "4.3 Experimental Setup", "text": "We randomly divide each dataset into two halves (one used for validation and the other for test). On the first half (the validation set), we run the two baselines and our algorithm with different values of k, \u03c3 and L/C (respectively), using 5-fold cross\nvalidation. Specifically, we consider values of k in {1, 2, . . . , 10} and values of \u03c3 and L/C in {0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10}. The best values of k, \u03c3 and L/C are then used in the second half of the dataset (the test set) to obtain the results presented in Table 1. For our algorithm, the range of k that corresponds to the selection of L/C is also given. Notice that we present here the average absolute error of our prediction, as a consequence of our theoretical guarantees."}, {"heading": "4.4 Results and Discussion", "text": "As evidenced by Table 1, our algorithm outperforms the baselines on 7 (out of 8) datasets, where on 3 datasets the outperformance is significant. It can also be seen that whereas the standard k-NN is restricted to choose one value of k per dataset, our algorithm fully utilizes the ability to choose k adaptively per data point. This validates our theoretical findings, and highlights the advantage of adaptive selection of k."}, {"heading": "5 Conclusions and Future Directions", "text": "We have introduced a principled approach to locally weighted optimal estimation. By explicitly phrasing the bias-variance tradeoff, we defined the notion of optimal weights and optimal number of neighbors per decision point, and consequently devised an efficient method to extract them. Note that our approach could be extended to handle multiclass classification, as well as scenarios in which predictions of different data points correlate (and we have an estimate of their correlations). Due to lack of space we leave these extensions to the full version of the paper.\nA shortcoming of current non-parametric methods, including our k\u2217-NN algorithm, is their limited geometrical perspective. Concretely, all of these methods only consider the distances between the decision point and dataset points, i.e., {d(x0, xi)}ni=1, and ignore the geometrical relation between the dataset points, i.e., {d(xi, xj)}ni,j=1. We believe that our approach opens an avenue for taking advantage of this additional geometrical information, which may have a great affect over the quality of our predictions."}, {"heading": "A Hoeffding\u2019s Inequality", "text": "Theorem (Hoeffding). Let { i}ni=1 \u2208 [Li, Ui]n be a sequence of independent random variables, such that E [ i] = \u00b5i. Then, it holds that\nP (\u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 i \u2212 n\u2211 i=1 \u00b5i \u2223\u2223\u2223\u2223\u2223 \u2265 \u03b5 ) \u2264 2e \u2212 2\u03b5 2\u2211n i=1 (Ui\u2212Li)2 ."}, {"heading": "B Proof of Lemma 3.4", "text": "Proof. First note that for k = k\u2217 the lemma holds immediately by Theorem 3.1. In what follows, we establish the lemma for k < k\u2217. Thus, set k, let \u2206 (k) n = {\u03b1 \u2208 \u2206n : \u03b1i = 0, \u2200i > k}, and consider the following optimization problem:\nmin \u03b1\u2208\u2206(k)n\n( \u2016\u03b1\u20162 + \u03b1>\u03b2 ) (P2k).\nSimilarly to the proof of Theorem 3.1 and Corollary 3.2, it can be shown that there exists k\u0304 \u2264 k such that the optimal solution of (P2k) is of the form (\u03b11, . . . , \u03b1k\u0304, 0 . . . , 0), where \u03b1i > 0, \u2200i \u2264 k\u0304. Moreover, given k\u0304 it can be shown that the value of (P2k) at the optimum equals \u03bb, where\n\u03bb = 1\nk\u0304  k\u0304\u2211 i=1 \u03b2i + \u221a\u221a\u221a\u221a k\u0304 + ( k\u0304\u2211 i=1 \u03b2i )2 \u2212 k\u0304 k\u0304\u2211 i=1 \u03b22i  , which is of the form calculated in Algorithm 1. The above implies that showing k\u0304 = k concludes the proof. Now, assume by contradiction that k\u0304 < k, then it is immediate to show that the resulting solution of (P2k) also satisfies the KKT conditions of the original problem (P2), and is therefore an optimal solution to (P2). However, this stands in contradiction to the fact that k\u0304 < k\u2217, and thus it must hold that k\u0304 = k, which establishes the lemma."}], "references": [{"title": "On bandwidth variation in kernel estimates-a square root law", "author": ["I.S. Abramson"], "venue": "The annals of Statistics,", "citeRegEx": "Abramson.,? \\Q1982\\E", "shortCiteRegEx": "Abramson.", "year": 1982}, {"title": "Automated web usage data mining and recommendation system using k-nearest neighbor (knn) classification method", "author": ["D. Adeniyi", "Z. Wei", "Y. Yongquan"], "venue": "Applied Computing and Informatics,", "citeRegEx": "Adeniyi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Adeniyi et al\\.", "year": 2016}, {"title": "Modification of the adaptive nadaraya-watson kernel regression estimator", "author": ["K.H. Aljuhani"], "venue": "Scientific Research and Essays,", "citeRegEx": "Aljuhani,? \\Q2014\\E", "shortCiteRegEx": "Aljuhani", "year": 2014}, {"title": "An adaptive k-nearest neighbor text categorization strategy", "author": ["L. Baoli", "L. Qin", "Y. Shiwen"], "venue": "ACM Transactions on Asian Language Information Processing (TALIP),", "citeRegEx": "Baoli et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Baoli et al\\.", "year": 2004}, {"title": "Nearest neighbor pattern classification", "author": ["T.M. Cover", "P.E. Hart"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cover and Hart.,? \\Q1967\\E", "shortCiteRegEx": "Cover and Hart.", "year": 1967}, {"title": "Classification of heart disease using k-nearest neighbor and genetic algorithm", "author": ["B. Deekshatulu", "P. Chandra"], "venue": "Procedia Technology,", "citeRegEx": "Deekshatulu and Chandra,? \\Q2013\\E", "shortCiteRegEx": "Deekshatulu and Chandra", "year": 2013}, {"title": "Toktami\u015f. On the adaptive nadaraya-watson kernel regression estimators", "author": ["\u00d6.S. Demir"], "venue": "Hacettepe Journal of Mathematics and Statistics,", "citeRegEx": "Demir,? \\Q2010\\E", "shortCiteRegEx": "Demir", "year": 2010}, {"title": "A probabilistic theory of pattern recognition, volume 31", "author": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": "Springer Science & Business Media,", "citeRegEx": "Devroye et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Devroye et al\\.", "year": 2013}, {"title": "Distribution-free consistency results in nonparametric discrimination and regression function estimation", "author": ["L.P. Devroye", "T. Wagner"], "venue": "The Annals of Statistics,", "citeRegEx": "Devroye and Wagner,? \\Q1980\\E", "shortCiteRegEx": "Devroye and Wagner", "year": 1980}, {"title": "Local polynomial modelling and its applications: monographs on statistics and applied probability 66, volume 66", "author": ["J. Fan", "I. Gijbels"], "venue": null, "citeRegEx": "Fan and Gijbels.,? \\Q1996\\E", "shortCiteRegEx": "Fan and Gijbels.", "year": 1996}, {"title": "Discriminatory analysis-nonparametric discrimination: consistency properties", "author": ["E. Fix", "J.L. Hodges Jr."], "venue": "Technical report, DTIC Document,", "citeRegEx": "Fix and Jr.,? \\Q1951\\E", "shortCiteRegEx": "Fix and Jr.", "year": 1951}, {"title": "On nearest neighbor classification using adaptive choice of k", "author": ["A.K. Ghosh"], "venue": "Journal of computational and graphical statistics,", "citeRegEx": "Ghosh.,? \\Q2007\\E", "shortCiteRegEx": "Ghosh.", "year": 2007}, {"title": "A distribution-free theory of nonparametric regression", "author": ["L. Gy\u00f6rfi", "M. Kohler", "A. Krzyzak", "H. Walk"], "venue": "Springer Science & Business Media,", "citeRegEx": "Gy\u00f6rfi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gy\u00f6rfi et al\\.", "year": 2006}, {"title": "Application of k-nearest neighbor (knn) approach for predicting economic events: Theoretical background", "author": ["S.B. Imandoust", "M. Bolandraftar"], "venue": "International Journal of Engineering Research and Applications,", "citeRegEx": "Imandoust and Bolandraftar.,? \\Q2013\\E", "shortCiteRegEx": "Imandoust and Bolandraftar.", "year": 2013}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "In Proceedings of the thirtieth annual ACM symposium on Theory of computing,", "citeRegEx": "Indyk and Motwani.,? \\Q1998\\E", "shortCiteRegEx": "Indyk and Motwani.", "year": 1998}, {"title": "Metric learning: A survey", "author": ["B. Kulis"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Kulis.,? \\Q2012\\E", "shortCiteRegEx": "Kulis.", "year": 2012}, {"title": "On estimating regression", "author": ["E.A. Nadaraya"], "venue": "Theory of Probability & Its Applications,", "citeRegEx": "Nadaraya.,? \\Q1964\\E", "shortCiteRegEx": "Nadaraya.", "year": 1964}, {"title": "Optimal weighted nearest neighbour classifiers", "author": ["R.J. Samworth"], "venue": "The Annals of Statistics,", "citeRegEx": "Samworth,? \\Q2012\\E", "shortCiteRegEx": "Samworth", "year": 2012}, {"title": "Density estimation for statistics and data analysis, volume 26", "author": ["B.W. Silverman"], "venue": "CRC press,", "citeRegEx": "Silverman.,? \\Q1986\\E", "shortCiteRegEx": "Silverman.", "year": 1986}, {"title": "Consistent nonparametric regression", "author": ["C.J. Stone"], "venue": "The Annals of Statistics,", "citeRegEx": "Stone.,? \\Q1977\\E", "shortCiteRegEx": "Stone.", "year": 1977}, {"title": "An adaptive k-nearest neighbor algorithm", "author": ["S. Sun", "R. Huang"], "venue": "In 2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery", "citeRegEx": "Sun and Huang.,? \\Q2010\\E", "shortCiteRegEx": "Sun and Huang.", "year": 2010}, {"title": "Knn with tf-idf based framework for text categorization", "author": ["B. Trstenjak", "S. Mikac", "D. Donko"], "venue": "Procedia Engineering,", "citeRegEx": "Trstenjak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Trstenjak et al\\.", "year": 2014}, {"title": "Smooth regression analysis", "author": ["G.S. Watson"], "venue": "Sankhya\u0304: The Indian Journal of Statistics,", "citeRegEx": "Watson.,? \\Q1964\\E", "shortCiteRegEx": "Watson.", "year": 1964}, {"title": "Locally adaptive nearest neighbor algorithms", "author": ["D. Wettschereck", "T.G. Dietterich"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Wettschereck and Dietterich.,? \\Q1994\\E", "shortCiteRegEx": "Wettschereck and Dietterich.", "year": 1994}, {"title": "Top 10 algorithms in data mining", "author": ["X. Wu", "V. Kumar", "J.R. Quinlan", "J. Ghosh", "Q. Yang", "H. Motoda", "G.J. McLachlan", "A. Ng", "B. Liu", "S.Y. Philip"], "venue": "Knowledge and information systems,", "citeRegEx": "Wu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 3, "context": "The k-nearest neighbors (k-NN) algorithm Cover and Hart (1967); Fix and Hodges Jr (1951), and Nadarays-Watson estimation Nadaraya (1964); Watson (1964) are the cornerstones of non-parametric learning.", "startOffset": 41, "endOffset": 63}, {"referenceID": 3, "context": "The k-nearest neighbors (k-NN) algorithm Cover and Hart (1967); Fix and Hodges Jr (1951), and Nadarays-Watson estimation Nadaraya (1964); Watson (1964) are the cornerstones of non-parametric learning.", "startOffset": 41, "endOffset": 89}, {"referenceID": 3, "context": "The k-nearest neighbors (k-NN) algorithm Cover and Hart (1967); Fix and Hodges Jr (1951), and Nadarays-Watson estimation Nadaraya (1964); Watson (1964) are the cornerstones of non-parametric learning.", "startOffset": 41, "endOffset": 137}, {"referenceID": 3, "context": "The k-nearest neighbors (k-NN) algorithm Cover and Hart (1967); Fix and Hodges Jr (1951), and Nadarays-Watson estimation Nadaraya (1964); Watson (1964) are the cornerstones of non-parametric learning.", "startOffset": 41, "endOffset": 152}, {"referenceID": 3, "context": "The k-nearest neighbors (k-NN) algorithm Cover and Hart (1967); Fix and Hodges Jr (1951), and Nadarays-Watson estimation Nadaraya (1964); Watson (1964) are the cornerstones of non-parametric learning. Owing to their simplicity and flexibility, these procedures had become the methods of choice in many scenarios Wu et al. (2008), especially in settings where the underlying model is complex.", "startOffset": 41, "endOffset": 329}, {"referenceID": 1, "context": "Modern applications of the k-NN algorithm include recommendation systems Adeniyi et al. (2016), text categorization Trstenjak et al.", "startOffset": 73, "endOffset": 95}, {"referenceID": 1, "context": "Modern applications of the k-NN algorithm include recommendation systems Adeniyi et al. (2016), text categorization Trstenjak et al. (2014), heart disease classification Deekshatulu et al.", "startOffset": 73, "endOffset": 140}, {"referenceID": 1, "context": "Modern applications of the k-NN algorithm include recommendation systems Adeniyi et al. (2016), text categorization Trstenjak et al. (2014), heart disease classification Deekshatulu et al. (2013), and financial market prediction Imandoust and Bolandraftar (2013), amongst others.", "startOffset": 73, "endOffset": 196}, {"referenceID": 1, "context": "Modern applications of the k-NN algorithm include recommendation systems Adeniyi et al. (2016), text categorization Trstenjak et al. (2014), heart disease classification Deekshatulu et al. (2013), and financial market prediction Imandoust and Bolandraftar (2013), amongst others.", "startOffset": 73, "endOffset": 263}, {"referenceID": 7, "context": "Most of the theoretic work focuses on the asymptotic regime in which the number of samples n goes to infinity Devroye et al. (2013); Samworth et al.", "startOffset": 110, "endOffset": 132}, {"referenceID": 7, "context": "Most of the theoretic work focuses on the asymptotic regime in which the number of samples n goes to infinity Devroye et al. (2013); Samworth et al. (2012); Stone (1977), and ignores the practical regime in which n is finite.", "startOffset": 110, "endOffset": 156}, {"referenceID": 7, "context": "Most of the theoretic work focuses on the asymptotic regime in which the number of samples n goes to infinity Devroye et al. (2013); Samworth et al. (2012); Stone (1977), and ignores the practical regime in which n is finite.", "startOffset": 110, "endOffset": 170}, {"referenceID": 15, "context": "Similar guarantees hold for weighted k-NN rules, with the additional assumptions that \u2211k i=1 \u03b1i = 1 and maxi\u2264n \u03b1i \u2192 0, Stone (1977); Devroye et al.", "startOffset": 119, "endOffset": 132}, {"referenceID": 7, "context": "Similar guarantees hold for weighted k-NN rules, with the additional assumptions that \u2211k i=1 \u03b1i = 1 and maxi\u2264n \u03b1i \u2192 0, Stone (1977); Devroye et al. (2013). In the regime of practical interest where the number of samples n is finite, using k = b \u221a nc neighbors is a widely mentioned rule of thumb Devroye et al.", "startOffset": 133, "endOffset": 155}, {"referenceID": 7, "context": "Similar guarantees hold for weighted k-NN rules, with the additional assumptions that \u2211k i=1 \u03b1i = 1 and maxi\u2264n \u03b1i \u2192 0, Stone (1977); Devroye et al. (2013). In the regime of practical interest where the number of samples n is finite, using k = b \u221a nc neighbors is a widely mentioned rule of thumb Devroye et al. (2013). Nevertheless, this rule often yields poor results, and in the regime of finite samples it is usually advised to choose k using cross-validation.", "startOffset": 133, "endOffset": 318}, {"referenceID": 7, "context": "Similar guarantees hold for weighted k-NN rules, with the additional assumptions that \u2211k i=1 \u03b1i = 1 and maxi\u2264n \u03b1i \u2192 0, Stone (1977); Devroye et al. (2013). In the regime of practical interest where the number of samples n is finite, using k = b \u221a nc neighbors is a widely mentioned rule of thumb Devroye et al. (2013). Nevertheless, this rule often yields poor results, and in the regime of finite samples it is usually advised to choose k using cross-validation. Similar consistency results apply to kernel based local methods Devroye et al. (1980); Gy\u00f6rfi et al.", "startOffset": 133, "endOffset": 550}, {"referenceID": 7, "context": "Similar guarantees hold for weighted k-NN rules, with the additional assumptions that \u2211k i=1 \u03b1i = 1 and maxi\u2264n \u03b1i \u2192 0, Stone (1977); Devroye et al. (2013). In the regime of practical interest where the number of samples n is finite, using k = b \u221a nc neighbors is a widely mentioned rule of thumb Devroye et al. (2013). Nevertheless, this rule often yields poor results, and in the regime of finite samples it is usually advised to choose k using cross-validation. Similar consistency results apply to kernel based local methods Devroye et al. (1980); Gy\u00f6rfi et al. (2006). A novel study of k-NN by Samworth, Samworth et al.", "startOffset": 133, "endOffset": 572}, {"referenceID": 7, "context": "Similar guarantees hold for weighted k-NN rules, with the additional assumptions that \u2211k i=1 \u03b1i = 1 and maxi\u2264n \u03b1i \u2192 0, Stone (1977); Devroye et al. (2013). In the regime of practical interest where the number of samples n is finite, using k = b \u221a nc neighbors is a widely mentioned rule of thumb Devroye et al. (2013). Nevertheless, this rule often yields poor results, and in the regime of finite samples it is usually advised to choose k using cross-validation. Similar consistency results apply to kernel based local methods Devroye et al. (1980); Gy\u00f6rfi et al. (2006). A novel study of k-NN by Samworth, Samworth et al. (2012), derives a closed form expression for the optimal weight vector, and extracts the optimal number of neighbors.", "startOffset": 133, "endOffset": 631}, {"referenceID": 7, "context": "Similar guarantees hold for weighted k-NN rules, with the additional assumptions that \u2211k i=1 \u03b1i = 1 and maxi\u2264n \u03b1i \u2192 0, Stone (1977); Devroye et al. (2013). In the regime of practical interest where the number of samples n is finite, using k = b \u221a nc neighbors is a widely mentioned rule of thumb Devroye et al. (2013). Nevertheless, this rule often yields poor results, and in the regime of finite samples it is usually advised to choose k using cross-validation. Similar consistency results apply to kernel based local methods Devroye et al. (1980); Gy\u00f6rfi et al. (2006). A novel study of k-NN by Samworth, Samworth et al. (2012), derives a closed form expression for the optimal weight vector, and extracts the optimal number of neighbors. However, this result is only optimal under several restrictive assumptions, and only holds for the asymptotic regime where n \u2192 \u221e. Furthermore, the above optimal number of neighbors/weights do not adapt, but are rather fixed over all decision points given the dataset. In the context of kernel based methods, it is possible to extract an expression for the optimal kernel\u2019s bandwidth \u03c3 Gy\u00f6rfi et al. (2006); Fan and Gijbels (1996).", "startOffset": 133, "endOffset": 1148}, {"referenceID": 7, "context": "Similar guarantees hold for weighted k-NN rules, with the additional assumptions that \u2211k i=1 \u03b1i = 1 and maxi\u2264n \u03b1i \u2192 0, Stone (1977); Devroye et al. (2013). In the regime of practical interest where the number of samples n is finite, using k = b \u221a nc neighbors is a widely mentioned rule of thumb Devroye et al. (2013). Nevertheless, this rule often yields poor results, and in the regime of finite samples it is usually advised to choose k using cross-validation. Similar consistency results apply to kernel based local methods Devroye et al. (1980); Gy\u00f6rfi et al. (2006). A novel study of k-NN by Samworth, Samworth et al. (2012), derives a closed form expression for the optimal weight vector, and extracts the optimal number of neighbors. However, this result is only optimal under several restrictive assumptions, and only holds for the asymptotic regime where n \u2192 \u221e. Furthermore, the above optimal number of neighbors/weights do not adapt, but are rather fixed over all decision points given the dataset. In the context of kernel based methods, it is possible to extract an expression for the optimal kernel\u2019s bandwidth \u03c3 Gy\u00f6rfi et al. (2006); Fan and Gijbels (1996). Nevertheless, this bandwidth is fixed over all decision points, and is only optimal under several restrictive assumptions.", "startOffset": 133, "endOffset": 1172}, {"referenceID": 14, "context": "In Wettschereck and Dietterich (1994); Sun and Huang it is suggested to use local cross-validation in order to adapt the value of k to different decision points.", "startOffset": 3, "endOffset": 38}, {"referenceID": 6, "context": "Conversely, Ghosh Ghosh (2007) takes a Bayesian approach towards choosing k adaptively.", "startOffset": 12, "endOffset": 31}, {"referenceID": 1, "context": "Focusing on the multiclass classification setup, it is suggested in Baoli et al. (2004) to consider different values of k for each class, choosing k proportionally to the class populations.", "startOffset": 68, "endOffset": 88}, {"referenceID": 0, "context": "Similarly, there exist several attitudes towards adaptively choosing the kernel\u2019s bandwidth \u03c3, for kernel based methods Abramson (1982); Silverman (1986); Demir and Toktami\u015f (2010); Aljuhani et al.", "startOffset": 120, "endOffset": 136}, {"referenceID": 0, "context": "Similarly, there exist several attitudes towards adaptively choosing the kernel\u2019s bandwidth \u03c3, for kernel based methods Abramson (1982); Silverman (1986); Demir and Toktami\u015f (2010); Aljuhani et al.", "startOffset": 120, "endOffset": 154}, {"referenceID": 0, "context": "Similarly, there exist several attitudes towards adaptively choosing the kernel\u2019s bandwidth \u03c3, for kernel based methods Abramson (1982); Silverman (1986); Demir and Toktami\u015f (2010); Aljuhani et al.", "startOffset": 120, "endOffset": 181}, {"referenceID": 0, "context": "Similarly, there exist several attitudes towards adaptively choosing the kernel\u2019s bandwidth \u03c3, for kernel based methods Abramson (1982); Silverman (1986); Demir and Toktami\u015f (2010); Aljuhani et al. (2014). Learning the distance metric for k-NN was extensively studied throughout the last decade.", "startOffset": 120, "endOffset": 205}, {"referenceID": 0, "context": "Similarly, there exist several attitudes towards adaptively choosing the kernel\u2019s bandwidth \u03c3, for kernel based methods Abramson (1982); Silverman (1986); Demir and Toktami\u015f (2010); Aljuhani et al. (2014). Learning the distance metric for k-NN was extensively studied throughout the last decade. There are several approaches towards metric learning, which roughly divide into linear/non-linear learning methods. It was found that metric learning may significantly affect the performance of k-NN in numerous applications, including computer vision, text analysis, program analysis and more. A comprehensive survey by Kulis Kulis (2012) provides a review of the metric learning literature.", "startOffset": 120, "endOffset": 635}, {"referenceID": 0, "context": "Similarly, there exist several attitudes towards adaptively choosing the kernel\u2019s bandwidth \u03c3, for kernel based methods Abramson (1982); Silverman (1986); Demir and Toktami\u015f (2010); Aljuhani et al. (2014). Learning the distance metric for k-NN was extensively studied throughout the last decade. There are several approaches towards metric learning, which roughly divide into linear/non-linear learning methods. It was found that metric learning may significantly affect the performance of k-NN in numerous applications, including computer vision, text analysis, program analysis and more. A comprehensive survey by Kulis Kulis (2012) provides a review of the metric learning literature. Throughout this work we assume that the distance metric is fixed, and thus the focus is on finding the best (in a sense) values of k and \u03b1 for each new data point. Two comprehensive monographs, Devroye et al. (2013) and Biau and Devroye (2015), provide an extensive survey of the existing literature regarding k-NN rules, including theoretical guarantees, useful practices, limitations and more.", "startOffset": 120, "endOffset": 904}, {"referenceID": 0, "context": "Similarly, there exist several attitudes towards adaptively choosing the kernel\u2019s bandwidth \u03c3, for kernel based methods Abramson (1982); Silverman (1986); Demir and Toktami\u015f (2010); Aljuhani et al. (2014). Learning the distance metric for k-NN was extensively studied throughout the last decade. There are several approaches towards metric learning, which roughly divide into linear/non-linear learning methods. It was found that metric learning may significantly affect the performance of k-NN in numerous applications, including computer vision, text analysis, program analysis and more. A comprehensive survey by Kulis Kulis (2012) provides a review of the metric learning literature. Throughout this work we assume that the distance metric is fixed, and thus the focus is on finding the best (in a sense) values of k and \u03b1 for each new data point. Two comprehensive monographs, Devroye et al. (2013) and Biau and Devroye (2015), provide an extensive survey of the existing literature regarding k-NN rules, including theoretical guarantees, useful practices, limitations and more.", "startOffset": 120, "endOffset": 932}, {"referenceID": 14, "context": "Note that in some cases Indyk and Motwani (1998), using advanced data structures may decrease the cost of finding the nearest neighbors (i.", "startOffset": 24, "endOffset": 49}], "year": 2017, "abstractText": "The weighted k-nearest neighbors algorithm is one of the most fundamental non-parametric methods in pattern recognition and machine learning. The question of setting the optimal number of neighbors as well as the optimal weights has received much attention throughout the years, nevertheless this problem seems to have remained unsettled. In this paper we offer a simple approach to locally weighted regression/classification, where we make the biasvariance tradeoff explicit. Our formulation enables us to phrase a notion of optimal weights, and to efficiently find these weights as well as the optimal number of neighbors efficiently and adaptively, for each data point whose value we wish to estimate. The applicability of our approach is demonstrated on several datasets, showing superior performance over standard locally weighted methods.", "creator": "LaTeX with hyperref package"}}}