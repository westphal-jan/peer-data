{"id": "1701.07852", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2017", "title": "An Empirical Analysis of Feature Engineering for Predictive Modeling", "abstract": "Machine learning models, such as neural networks, decision trees, random forests and gradient boosting machines accept a feature vector and provide a prediction. These models learn in a supervised fashion where a set of feature vectors with expected output is provided. It is very common practice to engineer new features from the provided feature set. Such engineered features will either augment, or replace portions of the existing feature vector. These engineered features are essentially calculated fields, based on the values of the other features. If the expected output is a prediction, the expected output is shown with the expected output. The expected output has an expected output and if it is not, the expected output is the prediction. The best possible model for modeling this feature is in our examples. The models in this example are also described in a description of the feature. An alternative, known as the \u201csupervised algorithm,\u201d can be seen by the modeling and the inference of the predicted output. This feature is called \u2011supervised model. This is an algorithm that is built for machine learning.\u201d The implementation of this feature depends on the model being used. The model is described in the above description.\n\nThe model is a simple and straightforward model. It has a common configuration of the features used in the training and optimization of the training tasks. Each feature variable is represented in the following table. It includes the training and optimization information. Each model may have a variable of its own. The training and optimization information is based on the parameters associated with training task.\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\nTraining\n", "histories": [["v1", "Thu, 26 Jan 2017 19:29:41 GMT  (32kb,D)", "http://arxiv.org/abs/1701.07852v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jeff heaton"], "accepted": false, "id": "1701.07852"}, "pdf": {"name": "1701.07852.pdf", "metadata": {"source": "CRF", "title": "An Empirical Analysis of Feature Engineering for Predictive Modeling", "authors": ["Jeff Heaton"], "emails": ["jeffheaton@acm.org"], "sections": [{"heading": null, "text": "Engineering such features is primarily a manual, timeconsuming task. Additionally, each type of model will respond differently to different types of engineered features. This paper reports on empirical research to demonstrate what types of engineered features are best suited to which machine learning model type. This is accomplished by generating several datasets that are designed to benefit from a particular type of engineered feature. The experiment demonstrates to what degree the machine learning model is capable of synthesizing the needed feature on its own. If a model is capable of synthesizing an engineered feature, it is not necessary to provide that feature. The research demonstrated that the studied models do indeed perform differently with various types of engineered features.\nI. INTRODUCTION Feature engineering is an important but labor-intensive component of machine learning applications [1]. Most machinelearning performance is heavily dependent on the representation of the feature vector. As a result, much of the actual effort in deploying machine learning algorithms goes into the design of preprocessing pipelines and data transformations [1].\nTo make use of feature engineering a model\u2019s feature vector is expanded by adding new features that are calculations based on the other features [2]. These new features might be ratios, differences, or other mathematical transformations of existing features. This process is similar to transformations that human analysts perform as they construct new features such as body mass index (BMI), wind chill, or Triglyceride/HDL cholesterol ratio to help understand the interactions of existing features.\nKaggle and ACM\u2019s KDD Cup have seen feature engineering play a very important part in several winning submissions. Feature engineering was successfully applied to the winning KDD Cup 2010 competition entry [3]. Additionally, the Kaggle Algorithmic Trading Challenge was won with an ensemble of models and feature engineering. The features engineered for these competitions were created by hand.\nTechnologies such as deep learning [4] can benefit from feature engineering. Most research into feature engineering in\nthe deep learning space has been in the areas of image and speech recognition [1]. Such techniques are successful in the high-dimension space of image processing and often amount to dimensionality reduction techniques [5] such as PCA [6] and auto-encoders [7]."}, {"heading": "II. BACKGROUND AND PRIOR WORK", "text": "Feature engineering grew out of the desire to transform linear regression inputs that are not normally distributed. Such transformation can be helpful for linear regression. The seminal work by George Box and David Cox in 1964 introduced a method for determining which of several power functions might be a useful transformation for the outcome of linear regression [8]. This became known as the Box-Cox transformation.\nThe alternating conditional expectation (ACE) algorithm [9] works similarly to the Box-Cox transformation, in that a mathematical function is applied to each component of the feature vector and outcome. However, unlike the Box-Cox transformation, ACE is able to guarantee optimal transformations for linear regression.\nLinear regression is not the only machine-learning model that can benefit from feature engineering and other transformations. In 1999, it was demonstrated that feature engineering could enhance the performance of rules learning for text classification [10]. Feature engineering was successfully applied to the KDD Cup 2010 competition using a variety of machine learning models."}, {"heading": "III. EXPERIMENT DESIGN AND METHODOLOGY", "text": "Different machine learning model types have varying degrees of mathematical ability. If the model can learn to synthesize an engineered feature on its own, there was no reason to engineer the feature in the first place. This empirically determines what type of engineered feature performs best with which machine-learning model. To accomplish this, ten datasets were generated that each contain the inputs and outputs that correspond to a particular type of engineered feature. If the machine-learning model can learn to reproduce that feature, with a low error, it means that that particular model could have learned that engineered feature without assistance. The following regression machine learning models were examined in this experiment.\nar X\niv :1\n70 1.\n07 85\n2v 1\n[ cs\n.L G\n] 2\n6 Ja\nn 20\n17\n\u2022 Deep Neural Networks (DANN) \u2022 Gradient Boosted Machines (GBM) \u2022 Random Forests \u2022 Support Vector Machines for Regression (SVR) To mitigate the stochastic nature of some of these machine learning models, each experiment was run 5 times, and the best run\u2019s outcome was used for the comparison. These experiments were conducted in the Python programming language, using the following third-party packages: Scikit-Learn [11], Lasange, and Nolearn. Using this combination of packages, model types of support vector machine (SVM) [12][13], deep neural network [14], random forest [15], and gradient boosting machine (GBM) [16] were evaluated against the following ten selected engineered features:\n\u2022 Counts \u2022 Differences \u2022 Logarithms \u2022 Polynomials \u2022 Powers \u2022 Ratios \u2022 Rational Differences \u2022 Rational Polynomials \u2022 Root Distance \u2022 Square Roots The techniques used to create each of these datasets are described in the following sections. The Python source code for these experiments can be downloaded from the author\u2019s GitHub page [17]."}, {"heading": "A. Logarithms and Power Functions", "text": "Logarithms and power functions have long been used to transform the inputs to linear regression [8]. The usefulness of these functions for transformation has been shown for other model types, such as neural networks [18]. The log and power transforms used in this paper are of the type shown in Equations 1,2, and 3.\ny = log(x) (1)\ny = x2 (2)\ny = x 1 2 (3)\nThis paper investigates using the natural log function, the second power, and the square root. For both the log and root transform, random x values were uniformly sampled in the real number range [1, 100]. For the second power transformation, the x values were uniformly sampled in the real number range [1, 10]. The partial dataset from the resulting log transformations (Equation 1) is shown in Table I.\nA single x1 observation is used to generate a single y1 observation. The x1 values are simply random numbers that produce the expected y1 values by applying the logarithm function."}, {"heading": "B. Differences and Ratios", "text": "Differences and ratios are common choices for feature engineering. To evaluate this feature type a dataset is generated with two x observations uniformly sampled in the real number range [0, 1], a single y prediction is also generated that is either the difference or ratio of the two observations. When sampling uniform real numbers for the denominator, the range [0.1, 1] is used to avoid division by zero. The differences and ratio transformations are shown by Equations 4 and 5.\ny = x1 \u2212 x2 (4)\ny = x1 x2\n(5)\nSeveral rows that demonstrate the difference transformation (Equation 4) are shown in Table II. The x2 value is simply subtracted from the x1 value resulting in y1."}, {"heading": "C. Counts", "text": "The count engineered feature counts the number of elements in the feature vector that satisfies a certain condition. For example, a count feature might be generated that gives the count of other features that are above a specified threshold, such as zero. Equation 6 defines how a count feature might be engineered.\ny = n\u2211 i=1 1 if xi > t else 0 (6)\nThe x-vector represents the input vector of length n. The resulting y contains an integer equal to the number of x values that were above the threshold (t). To generate a count dataset the resulting y-count was uniformly sampled from integers in the range [1, 50], and corresponding input vectors are created. This process is demonstrated by Algorithm 1.\nSeveral example rows of the count input vector are shown in Table III. The y1 value simply holds the count of the number of features x1 through x50 that contain a value greater than 0.\nAlgorithm 1 Generate count test dataset 1: INPUT: The number of rows to generate r. 2: OUTPUT: A dataset where y contains random integers\nsampled from [0, 50], and x contains 50 columns randomly chosen to sum to y. 3: METHOD: 4: x\u2190 [...empty set...] 5: y \u2190 [...empty set...] 6: for n\u2190 1 TO r do 7: v \u2190 zeros(50) . Vector of length 50 8: o\u2190 uniform random int(0, 50) . Outcome(y) 9: r \u2190 o . remaining\n10: while r \u2265 0 do: 11: i\u2190 uniform random int(0, len(x)\u2212 1) 12: if x[i] = 0 then 13: v[i]\u2190 1 14: r \u2190 r \u2212 1 15: x.append(x) 16: y.append(o)\nreturn [x, y]"}, {"heading": "D. Polynomials", "text": "Engineered features might take the form of polynomials. This paper investigated the machine learning models\u2019 ability to synthesize features that follow the polynomial given by Equation 7.\ny = 1 + 5x+ 8x2 (7)\nAn equation such as this shows the models\u2019 ability to synthesize features that contain several multiplications and an exponent. The data set was generated by uniformly sampling x from real numbers in the range [0, 2). Example observations from this dataset are shown in Table IV. The y1 value is simply calculated based on x1 as input to Equation 7."}, {"heading": "E. Rational Differences and Polynomials", "text": "Useful features might also come from combinations of rational equations of polynomials. Equations 8 & 9 show the types of rational combinations of differences and polynomials tested by this paper.\nTo generate a dataset containing rational differences (Equation 8), four observations are uniformly sampled from real numbers of the range [1, 10]. Generating a dataset of rational polynomials, a single observation is uniformly sampled from real numbers of the range [1, 10]. Several example observations from this training set are shown in Table VI."}, {"heading": "F. The Quadratic Equation", "text": "It is also useful to see how capable the four machine learning models are at synthesizing common mathematical equations. The final synthesized feature is based on the distance between the roots of a quadratic equation [19]. The distance between roots of a quadratic equation can easily be calculated by taking the difference of the two outputs of the quadratic formula, as given in Equation 10, in its unsimplified form.\ny = \u2223\u2223\u2223\u2223\u2223\u2212b+ \u221a b2 \u2212 4ac 2a \u2212 \u2212b\u2212 \u221a b2 \u2212 4ac 2a \u2223\u2223\u2223\u2223\u2223 (10) The dataset for the transformation represented by Equation 10 is generated by uniformly sampling x values from the real number range [\u221210, 10]. Such a range will generate some invalid results, which can be discarded. Table VII demonstrates the appearance of this dataset."}, {"heading": "IV. RESULTS ANALYSIS", "text": "The results obtained by the experiments performed in this paper clearly indicate that some model types perform much better with certain classes of engineered features than other model types. The simple transformations that only involved a single feature were all easily learned by all four models. This included the log, polynomial, power, and root. However, none of the models were able to successfully learn the ratio difference feature. The model specific results from this experiment are summarized in the following sections."}, {"heading": "A. Neural Network Results", "text": "For each engineered feature experiment a stochastic gradient descent [20] trained deep neural network was used. A learning rate of 1\u00d7 10\u22125 and a momentum of 0.9 were used. The same set of hyper-parameters were used for each engineered feature experiment. The deep neural network contained a number of input neurons equal to the number of inputs needed to test that engineered feature type. Likewise, a single output neuron was used to provide the value generated by the specified engineered feature. There are 5 hidden layers that, when viewed from the input to the output layer contain 400, 200, 100, 50, and 25 neurons respectively. Each hidden layer makes use of a rectifier transfer function [21], making each hidden neuron a rectified linear unit (ReLU). The results of these deep neural network engineered feature experiments are provided in Figure 1.\nAll mean square (MSE) error values for Figures 1-4 are clamped at 0.05. For all MSE values above 0.05, the model is considered to have failed the feature engineering experiment. The deep neural network failed to synthesize the ratio and ratio-difference features. All other feature experiments were within an acceptable MSE level.\nAn examination of the calculations performed by a neural network will provide some insight into this performance. A single-layer neural network is essentially a weighted sum of the input vector transformed by a transfer function, as shown in Equation 11.\nf(x,w, b) = \u03c6 (\u2211 n (wixi) + b ) (11)\nThe vector x represents the input vector, the vector w represents the weights, and the scalar variable b represents the bias. The symbol \u03c6 represents the transfer function. The experiments in this paper used the rectifier transfer function [21] for hidden neurons and a simple identity linear function for output neurons. The weights and biases are adjusted, as the neural network is trained. A deep neural network contains many layers of these neurons, where each layer can form the input (represented by x) into the next layer. This allows the neural network to be adjusted to perform many mathematical operations, and can explain some of the results shown in Figure 1. The neural network can easily add, sum and multiply. This made the counts, diff, power, and rational polynomial engineered features all relatively easy to synthesize, by using layers of Equation 11."}, {"heading": "B. Support Vector Machine Results", "text": "The two primary hyper-parameters of a SVM are C and \u03b3. It is customary to perform a grid search to find an optimal combination of C and \u03b3 [22]. The 3 C values of 0.001, 1, and 100 were tried, in combination with the 3 \u03b3 values of 0.1, 1, and 10. This resulted in 9 different SVMs to evaluate. The experiment results given are from the best combination of C and \u03b3 for each feature type. A third hyper-parameter specifies the type of kernel that the SVM uses, which in this case is a Gaussian kernel. Because support vector machines benefit from their input feature vectors being normalized to a specific range [22], we normalized all SVM input to [0,1]. This required normalization step for the SVM does add additional calculations on to the feature being investigated. Therefore, the results obtained for the SVM are not as pure of a feature engineering experiment as the other models. The results of the SVM engineered features are provided in Figure 2.\nThe support vector machine failed to synthesize the quadratic, ratio, and ratio-difference features. All other feature experiments were within an acceptable MSE level. Smola and Vapnik extended the original support vector machine to include regression; the resulting algorithm is called a support vector regression (SVR) [23]. A full discussion of how a SVR is fitted and calculated is beyond the scope of this paper. However, for the purposes of this paper\u2019s research, the primary concern is how a SVR calculates its final output. This calculation can help determine the transformations that a SVR can synthesize. The final output for a SVR is given by the decision function, shown in Equation 12.\ny = n\u2211 i=1 (\u03b1i \u2212 \u03b1\u2217i )K(xi, x) + \u03c1 (12)\nThe vector x represents the input vector; the difference between the two alphas is called the coefficient of the SVR. The weights of the neural network are somewhat analogous to the coefficients of an SVR. The function K represents a kernel function that introduces non-linearity. This paper used a radial basis function (RBF) kernel based on the Gaussian function. The variable \u03c1 represents the intercept of the SVR, which is somewhat analogous to the bias of a neural network.\nLike the neural network, the SVR has the ability to perform multiplications and summations. Though there are many differences between a neural network and SVR, the final calculations share many similarities. Because of these similarities it is not too surprising that the neural network and SVR both fail to synthesize some of the same types of engineered features."}, {"heading": "C. Random Forest Results", "text": "Random forests are an ensemble model made up of decision trees. The training data is randomly sampled to produce a forest of trees that together, will usually outperform the individual trees. The random forests used in this paper all use 100 classifier trees. This tree count is a hyper-parameter for the random forest algorithm. The result of the random forest model\u2019s attempt to synthesize the engineered features is shown in Figure 3.\nThe random forest model failed to synthesize the counts and ratio-difference features. It is not too surprising that the random forest fails on the synthesized count feature. Trees do not have any inherent way to handle multiple inputs simultaneously other than branching. While a neural network or SVM can produce a count by a simple summation, a tree would need to create branches for every combination of the 50 binary input variables. This would have terrible scalability and would be bounded according to a Catalan number."}, {"heading": "D. Gradient Boosted Machine", "text": "The gradient boosted machine (GBM) model operates very similarly to random forests. However, the GBM algorithm uses the gradient of the training objective to attempt to produce optimal combinations of the trees. This additional optimization sometimes gives GBM a performance advantage over random forests. The gradient boosting machines used in this paper all used the same hyper-parameters. The maximum depth was 10 levels, the number of estimators was 100 and the learning rate was 0.05. The results of the GBM engineered features are provided in Figure 4.\nLike the random forest model, the gradient boosted machine failed to synthesize the counts and ratio-difference features. Though the gradient boosting machine achieved a satisfactory result on the ratio feature, it performed worse than the random forest."}, {"heading": "V. CONCLUSION & FURTHER RESEARCH", "text": "Figures 1-4 clearly illustrate that machine learning models such as neural networks, support vector machines, random forests, and gradient boosting machines benefit from a different set of synthesized features. Neural networks and support vector machines generally benefit from the same types of engineered features; similarly, random forests and gradient boosting machines also generally benefit from the same set of engineered features. The results of this research allow for recommendations to be made for both the types of features to use for a particular machine learning model type, as well as the types of models that will work well with each other in an ensemble.\nBased on the experiments performed in this research, the type of machine learning model used has a great deal of influence on the types of engineered features to consider. Engineered features based on a ratio of differences were not synthesized well by any of the models explored in this paper. Because of this ratios of difference might be useful to a wide array of models. Neural networks and support vector machines, might also have benefited from engineered features based on ratios. For random forests, and gradient boosting machines, engineered features based on counts could be very useful.\nThe research performed by this paper also empirically demonstrates one of the reasons why ensembles of models typically perform better than individual models. Because neural networks and support vector machines can synthesize different features than random forests and gradient boosting machines, ensembles made up of a model from each of these two groups might perform very well. A neural network or support vector machine might ensemble well with a random forest or gradient boosting machine.\nSignificant time was not spend tuning the models for each of the datasets. Rather, reasonably generic choices were made for the hyper-parameters chosen for the models. Results for individual models and datasets might have shown some improvement for additional time spent tuning the hyperparameters.\nFuture research will focus on exploring other engineered features with a wider set of machine learning models. Engineered features that are made up of multiple input features seem a logical focus. Possible candidate engineered features for future research include maximums, sums, minimums, means, standard deviations and other functions that could be used over part or all of the input feature vector.\nThis paper examined 10 different engineered features for four popular machine learning model types. Further research is needed to understand what features might be useful for other machine learning models. Such research could help guide the creation of ensembles that use a variety of machine learning\nmodel types. Additional types of engineered features should also be examined. It would be useful to see how more complex classes of features affect the performance of machine learning models."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1798\u20131828, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1828}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "International conference on artificial intelligence and statistics, 2011, pp. 215\u2013223.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Feature engineering and classifier ensemble for kdd cup 2010", "author": ["H.-F. Yu", "H.-Y. Lo", "H.-P. Hsieh", "J.-K. Lou", "T.G. McKenzie", "J.-W. Chou", "P.-H. Chung", "C.-H. Ho", "C.-F. Chang", "Y.-H. Wei"], "venue": "KDD Cup, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Principal component analysis", "author": ["I. Jolliffe"], "venue": "Wiley Online Library,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen"], "venue": "Nature, vol. 381, no. 6583, pp. 607\u2013609, 1996.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "An analysis of transformations", "author": ["G.E. Box", "D.R. Cox"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp. 211\u2013252, 1964.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1964}, {"title": "Estimating optimal transformations for multiple regression and correlation", "author": ["L. Breiman", "J.H. Friedman"], "venue": "Journal of the American statistical Association, vol. 80, no. 391, pp. 580\u2013598, 1985.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1985}, {"title": "Feature engineering for text classification", "author": ["S. Scott", "S. Matwin"], "venue": "ICML, vol. 99. Citeseer, 1999, pp. 379\u2013388.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1999}, {"title": "Scikit-learn: Machine learning in python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Theory of pattern recognition", "author": ["V.N. Vapnik", "A.J. Chervonenkis"], "venue": "1974.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1974}, {"title": "A training algorithm for optimal margin classifiers", "author": ["B.E. Boser", "I.M. Guyon", "V.N. Vapnik"], "venue": "Proceedings of the fifth annual workshop on Computational learning theory. ACM, 1992, pp. 144\u2013152.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1992}, {"title": "A logical calculus of the ideas immanent in nervous activity", "author": ["W.S. McCulloch", "W. Pitts"], "venue": "The bulletin of mathematical biophysics, vol. 5, no. 4, pp. 115\u2013133, 1943.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1943}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine learning, vol. 45, no. 1, pp. 5\u201332, 2001.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Boosting algorithms as gradient descent in function space.", "author": ["L. Mason", "J. Baxter", "P. Bartlett", "M. Frean"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Jeff Heaton\u2019s GitHub Repository - Conference/Paper Source Code", "author": ["J Heaton"], "venue": "https://github.com/jeffheaton/papers, accessed: 2016-01-31.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic neural network modeling for univariate time series", "author": ["S.D. Balkin", "J.K. Ord"], "venue": "International Journal of Forecasting, vol. 16, no. 4, pp. 509\u2013515, 2000.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "On the distance between roots of integer polynomials", "author": ["Y. Bugeaud", "M. Mignotte"], "venue": "Proceedings of the Edinburgh Mathematical Society (Series 2), vol. 47, no. 03, pp. 553\u2013556, 2004.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Introduction to stochastic search and optimization: estimation, simulation, and control", "author": ["J.C. Spall"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, 2011, pp. 315\u2013323.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "A practical guide to support vector classification", "author": ["C.-W. Hsu", "C.-C. Chang", "C.-J. Lin"], "venue": "2003.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "Support vector regression machines", "author": ["A. Smola", "V. Vapnik"], "venue": "Advances in neural information processing systems, vol. 9, pp. 155\u2013161, 1997.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "Feature engineering is an important but labor-intensive component of machine learning applications [1].", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "As a result, much of the actual effort in deploying machine learning algorithms goes into the design of preprocessing pipelines and data transformations [1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "To make use of feature engineering a model\u2019s feature vector is expanded by adding new features that are calculations based on the other features [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 2, "context": "Feature engineering was successfully applied to the winning KDD Cup 2010 competition entry [3].", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "Technologies such as deep learning [4] can benefit from feature engineering.", "startOffset": 35, "endOffset": 38}, {"referenceID": 0, "context": "Most research into feature engineering in the deep learning space has been in the areas of image and speech recognition [1].", "startOffset": 120, "endOffset": 123}, {"referenceID": 4, "context": "Such techniques are successful in the high-dimension space of image processing and often amount to dimensionality reduction techniques [5] such as PCA [6] and auto-encoders [7].", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "Such techniques are successful in the high-dimension space of image processing and often amount to dimensionality reduction techniques [5] such as PCA [6] and auto-encoders [7].", "startOffset": 151, "endOffset": 154}, {"referenceID": 6, "context": "Such techniques are successful in the high-dimension space of image processing and often amount to dimensionality reduction techniques [5] such as PCA [6] and auto-encoders [7].", "startOffset": 173, "endOffset": 176}, {"referenceID": 7, "context": "The seminal work by George Box and David Cox in 1964 introduced a method for determining which of several power functions might be a useful transformation for the outcome of linear regression [8].", "startOffset": 192, "endOffset": 195}, {"referenceID": 8, "context": "The alternating conditional expectation (ACE) algorithm [9] works similarly to the Box-Cox transformation, in that a mathematical function is applied to each component of the feature vector and outcome.", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "In 1999, it was demonstrated that feature engineering could enhance the performance of rules learning for text classification [10].", "startOffset": 126, "endOffset": 130}, {"referenceID": 10, "context": "These experiments were conducted in the Python programming language, using the following third-party packages: Scikit-Learn [11], Lasange, and Nolearn.", "startOffset": 124, "endOffset": 128}, {"referenceID": 11, "context": "Using this combination of packages, model types of support vector machine (SVM) [12][13], deep neural network [14], random forest [15], and gradient boosting machine (GBM) [16] were evaluated against the following ten selected engineered features:", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "Using this combination of packages, model types of support vector machine (SVM) [12][13], deep neural network [14], random forest [15], and gradient boosting machine (GBM) [16] were evaluated against the following ten selected engineered features:", "startOffset": 84, "endOffset": 88}, {"referenceID": 13, "context": "Using this combination of packages, model types of support vector machine (SVM) [12][13], deep neural network [14], random forest [15], and gradient boosting machine (GBM) [16] were evaluated against the following ten selected engineered features:", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "Using this combination of packages, model types of support vector machine (SVM) [12][13], deep neural network [14], random forest [15], and gradient boosting machine (GBM) [16] were evaluated against the following ten selected engineered features:", "startOffset": 130, "endOffset": 134}, {"referenceID": 15, "context": "Using this combination of packages, model types of support vector machine (SVM) [12][13], deep neural network [14], random forest [15], and gradient boosting machine (GBM) [16] were evaluated against the following ten selected engineered features:", "startOffset": 172, "endOffset": 176}, {"referenceID": 16, "context": "The Python source code for these experiments can be downloaded from the author\u2019s GitHub page [17].", "startOffset": 93, "endOffset": 97}, {"referenceID": 7, "context": "Logarithms and power functions have long been used to transform the inputs to linear regression [8].", "startOffset": 96, "endOffset": 99}, {"referenceID": 17, "context": "The usefulness of these functions for transformation has been shown for other model types, such as neural networks [18].", "startOffset": 115, "endOffset": 119}, {"referenceID": 0, "context": "For both the log and root transform, random x values were uniformly sampled in the real number range [1, 100].", "startOffset": 101, "endOffset": 109}, {"referenceID": 0, "context": "For the second power transformation, the x values were uniformly sampled in the real number range [1, 10].", "startOffset": 98, "endOffset": 105}, {"referenceID": 9, "context": "For the second power transformation, the x values were uniformly sampled in the real number range [1, 10].", "startOffset": 98, "endOffset": 105}, {"referenceID": 0, "context": "To evaluate this feature type a dataset is generated with two x observations uniformly sampled in the real number range [0, 1], a single y prediction is also generated that is either the difference or ratio of the two observations.", "startOffset": 120, "endOffset": 126}, {"referenceID": 0, "context": "To generate a count dataset the resulting y-count was uniformly sampled from integers in the range [1, 50], and corresponding input vectors are created.", "startOffset": 99, "endOffset": 106}, {"referenceID": 0, "context": "To generate a dataset containing rational differences (Equation 8), four observations are uniformly sampled from real numbers of the range [1, 10].", "startOffset": 139, "endOffset": 146}, {"referenceID": 9, "context": "To generate a dataset containing rational differences (Equation 8), four observations are uniformly sampled from real numbers of the range [1, 10].", "startOffset": 139, "endOffset": 146}, {"referenceID": 0, "context": "Generating a dataset of rational polynomials, a single observation is uniformly sampled from real numbers of the range [1, 10].", "startOffset": 119, "endOffset": 126}, {"referenceID": 9, "context": "Generating a dataset of rational polynomials, a single observation is uniformly sampled from real numbers of the range [1, 10].", "startOffset": 119, "endOffset": 126}, {"referenceID": 18, "context": "The final synthesized feature is based on the distance between the roots of a quadratic equation [19].", "startOffset": 97, "endOffset": 101}, {"referenceID": 19, "context": "For each engineered feature experiment a stochastic gradient descent [20] trained deep neural network was used.", "startOffset": 69, "endOffset": 73}, {"referenceID": 20, "context": "Each hidden layer makes use of a rectifier transfer function [21], making each hidden neuron a rectified linear unit (ReLU).", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "The experiments in this paper used the rectifier transfer function [21] for hidden neurons and a simple identity linear function for output neurons.", "startOffset": 67, "endOffset": 71}, {"referenceID": 21, "context": "It is customary to perform a grid search to find an optimal combination of C and \u03b3 [22].", "startOffset": 83, "endOffset": 87}, {"referenceID": 21, "context": "Because support vector machines benefit from their input feature vectors being normalized to a specific range [22], we normalized all SVM input to [0,1].", "startOffset": 110, "endOffset": 114}, {"referenceID": 0, "context": "Because support vector machines benefit from their input feature vectors being normalized to a specific range [22], we normalized all SVM input to [0,1].", "startOffset": 147, "endOffset": 152}, {"referenceID": 22, "context": "Smola and Vapnik extended the original support vector machine to include regression; the resulting algorithm is called a support vector regression (SVR) [23].", "startOffset": 153, "endOffset": 157}], "year": 2017, "abstractText": "Machine learning models, such as neural networks, decision trees, random forests and gradient boosting machines accept a feature vector and provide a prediction. These models learn in a supervised fashion where a set of feature vectors with expected output is provided. It is very common practice to engineer new features from the provided feature set. Such engineered features will either augment, or replace portions of the existing feature vector. These engineered features are essentially calculated fields, based on the values of the other features. Engineering such features is primarily a manual, timeconsuming task. Additionally, each type of model will respond differently to different types of engineered features. This paper reports on empirical research to demonstrate what types of engineered features are best suited to which machine learning model type. This is accomplished by generating several datasets that are designed to benefit from a particular type of engineered feature. The experiment demonstrates to what degree the machine learning model is capable of synthesizing the needed feature on its own. If a model is capable of synthesizing an engineered feature, it is not necessary to provide that feature. The research demonstrated that the studied models do indeed perform differently with various types of engineered features.", "creator": "LaTeX with hyperref package"}}}