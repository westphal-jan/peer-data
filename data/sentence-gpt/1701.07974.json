{"id": "1701.07974", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2017", "title": "Reinforced backpropagation for deep neural network learning", "abstract": "Standard error backpropagation is used in almost all modern deep network training. However, it typically suffers from proliferation of saddle points in high-dimensional parameter space. Therefore, it is highly desirable to design an efficient algorithm to escape from these saddle points and reach a good parameter region of better generalization capabilities, especially based on rough insights about the landscape of the error surface. Here, we propose a simple extension of the backpropagation, namely reinforced backpropagation, which simply adds previous first-order gradients in a stochastic manner with a probability that increases with learning time. Extensive numerical simulations on a toy deep learning model verify its excellent performance. The reinforced backpropagation can significantly improve test performance of the deep network training, especially when the data are scarce. The performance is even better than that of state-of-the-art stochastic optimization algorithm called Adam, with an extra advantage of less computer memory required. The neural network's performance is especially impressive when the data are scarce, which is due to the additional storage and memory space required in the training. Moreover, in addition, it can significantly improve test performance of the deep network training using the reinforcement learning model of Adam. In addition, it can improve performance of the deep network training in the deep network training without using the reinforcement learning model of Adam.\n\n\nAcknowledgments We would like to thank our colleagues and colleagues in our previous research on the topic of reinforcement learning.", "histories": [["v1", "Fri, 27 Jan 2017 08:49:19 GMT  (89kb,D)", "https://arxiv.org/abs/1701.07974v1", "7 pages and 5 figures"], ["v2", "Tue, 7 Mar 2017 05:34:08 GMT  (626kb,D)", "http://arxiv.org/abs/1701.07974v2", "9 pages and 7 figures, results added, learning rate typo corrected"], ["v3", "Wed, 24 May 2017 09:30:54 GMT  (663kb,D)", "http://arxiv.org/abs/1701.07974v3", "10 pages and 8 figures, results added"], ["v4", "Tue, 19 Sep 2017 02:57:24 GMT  (674kb,D)", "http://arxiv.org/abs/1701.07974v4", "12 pages and 9 figures, extensively revised"]], "COMMENTS": "7 pages and 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["haiping huang", "taro toyoizumi"], "accepted": false, "id": "1701.07974"}, "pdf": {"name": "1701.07974.pdf", "metadata": {"source": "CRF", "title": "Reinforced stochastic gradient descent for deep neural network learning", "authors": ["Haiping Huang", "Taro Toyoizumi"], "emails": [], "sections": [{"heading": null, "text": "Reinforced stochastic gradient descent for deep neural network learning\nHaiping Huang\u2217 and Taro Toyoizumi RIKEN Brain Science Institute, Wako-shi, Saitama 351-0198, Japan\n(Dated: September 20, 2017)\nStochastic gradient descent (SGD) is a standard optimization method to minimize a training error with respect to network parameters in modern neural network learning. However, it typically suffers from proliferation of saddle points in the high-dimensional parameter space. Therefore, it is highly desirable to design an efficient algorithm to escape from these saddle points and reach a parameter region of better generalization capabilities. Here, we propose a simple extension of SGD, namely reinforced SGD, which simply adds previous first-order gradients in a stochastic manner with a probability that increases with learning time. As verified in a simple synthetic dataset, this method significantly accelerates learning compared with the original SGD. Surprisingly, it dramatically reduces over-fitting effects, even compared with state-of-the-art adaptive learning algorithm\u2014Adam. For a benchmark handwritten digits dataset, the learning performance is comparable to Adam, yet with an extra advantage of requiring one-fold less computer memory. The reinforced SGD is also compared with SGD with fixed or adaptive momentum parameter and Nesterov\u2019s momentum, which shows that the proposed framework is able to reach a similar generalization accuracy with less computational costs. Overall, our method introduces stochastic memory into gradients, which plays an important role in understanding how gradient-based training algorithms can work and its relationship with generalization abilities of deep networks.\nKeywords: Neuronal networks, Machine learning, Backpropagation\nI. INTRODUCTION\nMultilayer neural networks have achieved state-of-the-art performances in image recognition [1], speech recognition, and even natural language processing [2]. This impressive success is based on a simple powerful stochastic gradient descent (SGD) algorithm [3], and its variants. This algorithm estimates gradients of an error function based on mini-batches of an entire dataset. Gradient noise caused by mini-batches helps exploration of parameter space to some extent. However, the parameter space is highly non-convex for a typical deep network training, and finding a good path for SGD to improve generalization ability of deep neural networks is thus challenging [4].\nAs found in standard spin glass models of neural networks [5, 6], a non-convex error surface is accompanied by exponentially many local minima, which hides the (isolated) global minima and thus makes any local search algorithms easily get trapped. In addition, the error surface structure of deep networks might behave similarly to random Gaussian error surface [7, 8], which demonstrates that critical points (defined as zero-gradient points) of high error have a large number of negative eigenvalues of the corresponding Hessian matrix. Consistent with this theoretical study, empirical studies on deep network training [9, 10] showed that SGD is slowed down by a proliferation of saddle points with many negative curvatures and even plateaus (eigenvalues close to zero in many directions). The prevalence of saddle points poses an obstacle to attain better generalization properties for a deep network, especially for SGD based on first-order optimization, while second-order optimization relying on Hessian-vector products is more computationally expensive [11]. The second order method that relies on positive-definite curvature approximations, can not follow directions with negative curvature, and is easily trapped by saddle points [12].\nIn this paper, we show a heuristic strategy to overcome the plateaus problem for SGD learning. We call this strategy reinforced SGD (R-SGD), which provides a new effective strategy to use the gradient information, i.e., to update one network parameter, an instantaneous gradient is reinforced by (accumulated) previous gradients with an increasing reinforcement probability that grows with learning time steps. In other words, the reinforcement may be turned off, and then only the instantaneous gradient is used for learning. This kind of stochastic reinforcement enhances the exploration of parameter space. The excellent performance of R-SGD is verified first on training a toy fully-connected deep network model to learn a simple non-linear mapping generated by a two-layer feedforward network, and then on a benchmark handwritten digits dataset [13], in comparison to both vanilla backpropagation (BackProp) [14] and state-of-the-art Adam algorithm [15]. In the benchmark dataset, we also clarify the performance difference between R-SGD and SGD with fixed or adaptive momentum parameter [16] and Nesterov\u2019s momentum [17].\n\u2217Electronic address: physhuang@gmail.com\nar X\niv :1\n70 1.\n07 97\n4v 4\n[ cs\n.L G\n] 1\n9 Se\np 20\n17\n2"}, {"heading": "II. FULLY-CONNECTED DEEP NETWORKS", "text": "We consider a toy deep network model with L layers of fully-connected feedforward architecture. Each layer has nk neurons (so-called width of layer k). We define the input as n1-dimensional vector v, and the weight matrix Wk specifies the symmetric connections between layer k and layer k\u2212 1. The symmetry means that the same connections are used to backpropagate the error during training. A bias parameter can also be incorporated into the weight matrix by assuming an additional constant input. The output at the final layer is expressed as:\ny = fL ( WLfL\u22121(W L\u22121 \u00b7 \u00b7 \u00b7 f2(W2v)) ) , (1)\nwhere fk(\u00b7) is an element-wise sigmoid function for neurons at layer k, defined as f(x) = 11+e\u2212x , unless otherwise specified (e.g., ReLU activation funtion). The network is trained to learn the target mapping generated randomly as {vm,ym\u2217 }Mm=1, where the input is generated from a standard normal distribution with zero mean and unit variance, and the target label y\u2217 is generated according to the non-linear mapping y\u2217 = f(Wgv), in which each entry of the data-generating matrix Wg follows independently a standard normal distribution as well. The deep network is trained to learn this non-linear mapping (continuous target labels) from a set of examples. We generate a total of 2M examples, in which the first M examples are used for training and the last M examples are used for testing to evaluate the generalization ability of the learned model.\nIn simulations, we use deep network architecture of L = 4 layers to learn the target non-linear mapping, in which the network is thus specified by n1-n2-n3-n4, with n1 indicating the dimension of the input data and nL the dimension of the output. We use this simple toy setting to test our idea first, and then the idea is further verified in the handwritten digits dataset."}, {"heading": "III. BACKPROPAGATION AND ITS VARIANTS", "text": "We first introduce the vanilla BackProp [14] for training the deep network defined in Sec. II. We use quadratic loss (error) function defined as E = 12\nT , where T denotes a vector (matrix) transpose operation, and defines the difference between the target and actual outputs as = y\u2217 \u2212 y. To backpropagate the error, we also define two associated quantities: one is the state of neurons at k-th layer defined by sk (e.g., sL = y, s1 = v), and the other is the weighted-sum input to neurons at k-th layer defined by hk \u2261 Wksk\u22121. Accordingly, we define two related gradient vectors:\n\u03b4k \u2261 \u2202E \u2202sk , (2a) \u03bak \u2261 \u2202E \u2202hk , (2b)\nwhich will be used to derive the propagation equation based on the chain rule. It is straightforward to derive \u03baL = \u2212 \u25e6 y\u2032, where \u25e6 indicates the element-wise multiplication, and y\u2032 is the derivative of the non-linear transfer function with respect to its argument. By applying the chain rule, we obtain the weight update equation for the top layer as\n\u2206WL = \u2212\u03b7\u03baL(sL\u22121)T, (3)\nwhere \u03b7 is the learning rate, and the remaining part is the gradient information, which indicates how a small perturbation to the weight affects the change of the error computed at the top (output) layer.\nTo update the weight parameters at lower layers, we first derive the propagating equations for gradient vectors as follows:\n\u03b4k = (Wk+1)T\u03bak+1, (4a)\n\u03bak = \u03b4k \u25e6 (fk)\u2032, (4b)\nwhere k \u2264 L\u2212 1. Using the above backpropagation equation, the weight at lower layers is updated as:\n\u2206Wk = \u2212\u03b7\u03bak(sk\u22121)T, (5)\nwhere k \u2264 L\u2212 1. The neural state used to update the weight parameters comes from a forward pass from the input vector to the output vector at the top layer. A forward pass combined with a backward propagation of the error\n3 forms the vanilla BackProp widely used in training deep networks given the labeled data [18]. To improve the training efficiency, one usually divides the entire large dataset into a set of mini-batches, each of which is used to get the average gradients across the examples within that mini-batch. One epoch corresponds to a sweep of the full dataset. The learning time is thus measured in units of epochs. For one epoch, the weight is actually updated for M/B times (B is the size of a mini-batch). This process is usually termed SGD.\nHere, we briefly introduce two kinds of SGD with momentum techniques. The first one is the SGD with momentum (SGDM). The learning equation is revised as\n\u03bdt = \u03c1t\u03bdt\u22121 + gt, (6a) \u2206Wt = \u2212\u03b7\u03bdt, (6b)\nwhere gt \u2261 \u2207WE(Wt\u22121) denotes the gradient estimated from the average over examples within the current minibatch, and \u03c1t is the momentum parameter, which can be either prefixed (\u03c1t = \u03c1 in the classical SGDM) or varied over learning steps (t).\nThe second one is Nesterov\u2019s accelerated gradient (NAG) [17], which first implements a partial update to Wt, and then uses the updated Wt to evaluate gradients, i.e.,\n\u03bdt = \u03c1t\u03bdt\u22121 \u2212 \u03b7g\u2032t, (7a) \u2206Wt = \u03bdt, (7b)\nwhere g\u2032t \u2261 \u2207WE(Wt\u22121 + \u03c1t\u03bdt\u22121). Note that the partial update takes an extra computational cost of TmaxMB |W|, where Tmax denotes the maximal number of epochs, and |W| denotes the total amount of network parameters."}, {"heading": "IV. REINFORCED STOCHASTIC GRADIENT DESCENT", "text": "In the above vanilla BackProp, only current gradients are used to update the weight matrix. Therefore in a nonconvex optimization, the backpropation gets easily stalled by the plateaus or saddle points on the error surface, and it is hard to escape from these regions. During training, gradients may be very noisy with large fluctuations. If update directions along some weight components are stochastically allowed to accumulate the history of gradient information, while other directions still follow the current gradients, the learning performance may be boosted. This stochastic rule of turning on accumulation may help SGD to handle the uncertainty of updating the weights. We will test this idea in the following deep neural network learning.\nTo enable SGD to use previous gradient information, we define a stochastic process for updating modified gradient g\u0303t used at each learning step as follows:\n(g\u0303t)i \u2190 { (gt)i, with prob. 1\u2212 \u0393(t), (gt)i + (g\u0303t\u22121)i, with prob. \u0393(t).\n(8)\nwhere the stochastic reinforcement is independently applied to each weight component, and g\u0303t\u22121 contains information about the history of the evolving gradients, and the current gradients are reinforced by the previous accumulated gradients with a reinforcement probability defined by \u0393(t). The stochastic rule in Eq. (8) is a switch-like (all-ornone) event; its smooth averaged version given g\u0303t\u22121 is E[g\u0303t|g\u0303t\u22121] = gt + \u0393(t)g\u0303t\u22121, where \u0393(t) is equivalent to \u03c1t in SGDM with time-dependent momentum parameter. Using adaptive momentum parameter is important in boosting the learning performance of SGDM. However, the switch-like property is able to reach a better or equivalent test accuracy with fewer training steps. Comparisons will be made on the handwritten digits dataset.\nEq. (8) is a very simple way to re-use the previous gradient information, and forms the key component of R-SGD. We first choose \u0393(t) = 1 \u2212 \u03b3t, where \u03b3 = \u03b30e\u2212\u03bbtep . \u03b30 and \u03bb are prefixed constants, and tep refers to the learning time in units of epochs. \u0393(t) can be rewritten as 1 \u2212 e\u2212t/\u03c4R , where \u03c4R \u2261 \u2212 1ln \u03b30\u2212\u03bbtep setting the time scale of the dynamics of the reinforcement probability. \u03b30 is usually fixed to a value very close to one, and \u03bb takes a small value. Therefore the current gradient has an increasing probability to be reinforced by the previous gradients, and retains its instantaneous value otherwise. This reinforcement probability is not the unique choice, e.g., 1\u2212 a0/tb0 (a0 and b0 are constants) is also a candidate (discussed in Sec. V B). We show a typical trace of the reinforcement probability and \u03b3 in Fig. 1 (a), and will test effects of hyper-parameters (\u03b30, \u03bb) on training dynamics in Sec. V A. Note that by setting (\u03b30, \u03bb) = (1, 0), one recovers the vanilla BackProp. In all simulations, we use an exponentially-decaying learning rate \u03b7tep = \u03b7tep\u22121\u03b2\ntep , where \u03b70 = 0.8 and \u03b2 = 0.999, with a minimal learning rate of 0.02, unless otherwise specified. R-SGD is summarized in algorithm 1.\n4 Algorithm 1 R-SGD (\u03b70, \u03b2, \u03b30, \u03bb)\nInput: data {vm,ym\u2217 }Mm=1, Tmax = 100, mini-batch size B t\u2190 0 \u03b3 \u2190 \u03b30 \u03b7 \u2190 \u03b70 for tep = 1 to Tmax do\nfor l = 1 to M/B do gt \u2190 1B \u2211B m=1\u2207Wt\u22121E(v\nm,ym\u2217 ) g\u0303t \u2190 Eq. (8) using \u03b3 Wt \u2190Wt\u22121 \u2212 \u03b7g\u0303t t\u2190 t+ 1\nend for \u03b3 \u2190 \u03b30e\u2212\u03bbtep \u03b7 \u2190 \u03b7\u03b2tep\nend for\nThe gradient used in R-SGD may be the accumulated one (over an unfixed or stochastic number of consecutive steps), which contains short or long-term memory of previous gradient information (Fig. 1 (b)). Hence, the step-size is a sum of previous gradients over a memory length L, which follows a probability Pt(L) decaying with L (Fig. 1 (b)). This stochastic process is summarized by\n\u2206WR\u2212SGDt = \u2212\u03b7t t\u2211\nl=t\u2212L\ngl, (9a)\nL \u223c Pt(L) = (1\u2212 \u0393(t\u2212 L)) exp\n( t\u2211\nl=t\u2212L+1\nln \u0393(l) ) , (9b)\nwhere the prefactor indicates the probability that the memory is cleaned before accumulation. In the SGDM, the momentum term is deterministically added to the learning step size with coefficient \u03c1t (Eq. (6)). Unfolding this process, we obtain the step-size as\n\u2206WSGDMt = \u2212\u03b7t t\u2211 l=1 ( t\u220f l\u2032=l+1:l 6=t \u03c1l\u2032 + \u03b4l,t ) gl, (10)\nwhere \u03b4l,t is a Kronecker delta function, and each gradient is weighted by a value smaller than one. To show the efficiency of R-SGD, we also compare its performance with that of a state-of-the-art stochastic optimization algorithm, namely adaptive moment estimation (Adam) [15]. Adam performs a running average of gradients and their second raw moments, which are used to adaptively change the learning step-size. We use heuristic parameters of Adam given in [15], except that \u03b70 = 0.01 with the lowest value set to 0.001. Large \u03b70 as we use in R-SGD does not work in our simulations for Adam."}, {"heading": "V. RESULTS AND DISCUSSION", "text": ""}, {"heading": "A. Learning performance in simple synthetic dataset", "text": "We first test our idea in the simple synthetic dataset described in Sec. II. We use a 4-layer deep network architecture as 100-400-200-10. Training examples are divided into mini-batches of size B = 100. In simulations, we use the parameters (\u03b30, \u03bb) = (0.9995, 0.0001), unless otherwise specified. Although the chosen parameters are not optimal to achieve the best performance, we still observe the outstanding performance of R-SGD. In Fig. 2 (a), we compare the vanilla BackProp with R-SGD. We clearly see that the test performance is finally improved at 100-th epoch by a significant amount (about 77.5%). Meanwhile, the training error is also significantly lower than that of BackProp. A salient feature of R-SGD is that, at the intermediate stage, the reinforcement strategy guides SGD to escape from possible plateau regions of high error surrounding saddle points, and finally reach a region of very nice generalization properties. This process is indicated by the temporary peak in both training and test errors for R-SGD. Remarkably, even before or after this peak, there are a few less significant fluctuations in both training and test errors. These fluctuations play an important role in the exploration of the parameter space.\nF1 F2 F3\nCompared to state-of-the-art Adam, R-SGD still improves the final test performance by a significant amount (about 49.1%, see Fig. 2 (b)). Note that Adam is able to decrease both training and test errors very quickly, but the decrease becomes slow after about 40 epochs. In contrast, R-SGD keeps decreasing both errors by a more significant amount than Adam, despite the presence of slightly significant fluctuations. Another key feature of Fig. 2 (b) is that, a region in the parameter space with low training error does not generally have low test error. The training error reached by R-SGD is clearly higher than that of Adam, but the network architecture learned by R-SGD has nicer generalization property. This observation is consistent with a recent study of maximizing local entropy in deep networks [19].\nWe then study the effects of reinforcement parameters (\u03b30, \u03bb) on the learning performance, as shown in Fig. 3. If the exponential decay rate \u03bb is large, R-SGD over-fits the data rapidly at around 17 epochs. This is because, \u03b3 decays rapidly from \u03b30, and thus a stochastic fluctuation at earlier stages of learning is strongly suppressed, which limits severely the exploration ability of R-SGD in the high-dimensional parameter space. In this case, R-SGD is prone to get stuck by bad regions with poor generalization performances. However, maintaining the identical small decay rate, we find that a larger value of \u03b30 leads to a smaller test error (inset of Fig. 3). For relatively large values of \u03b30, the learning performance is not radically different.\nWe also study the effects of training data size (M) on the learning performances. Clearly, we see from Fig. 4, the test error decreases with the training data size as expected. R-SGD outperforms the vanilla BackProp, and even\nAdam. For the simple toy model, SGDM with fixed momentum parameter could outperform Adam with a careful optimization of momentum parameter (e.g., \u03c1t = 0.9 \u2200t), but R-SGD still outperforms the classical SGDM (fixed momentum parameter) by about 14.3% when M = 1000. By adaptively changing the momentum parameter whose value is the same as the reinforcement probability of R-SGD (i.e., a smooth averaged version of R-SGD), SGDM can reach a similar performance to that of R-SGD. Because the synthetic data in the toy model is relatively simple and the reinforcement probability used in the synthetic data is rapidly saturated to one (Fig. 1 (a)), it is difficult to show the performance difference between R-SGD and SGDM with the adaptive momentum parameter. Therefore, for MNIST classification task in the next section, we use the reinforcement probability of \u0393(t) = 1\u22121/ \u221a t, which does not rapidly approach one, and compare R-SGD with different variants of momentum-based SGD."}, {"heading": "B. Learning performance in MNIST dataset", "text": "Finally, we evaluate the test performance of R-SGD on MNIST dataset. The MNIST handwritten digits dataset contains 60000 training images and an extra 10000 images for testing. Each image is one of ten handwritten digits (0 to 9) with 28\u00d7 28 pixels. Therefore the input dimension is n1 = 784. For simplicity, we choose the network structure as 784-100-200-10.\nt=700 t=500 t=300 re in f. p ro b\nAlthough the reinforcement probability specified by (\u03b30, \u03bb) we used in the synthetic dataset works on the MNIST dataset (see an example in Fig. 7 (c)), we found that the reinforcement probability \u0393(t) = 1 \u2212 1/ \u221a t works better (shortening the training time to reach a lower generalization error) for MNIST dataset, and furthermore, this choice does not saturate the reinforcement probability to one within the explored range of learning time (Fig. 5), which offers a nice candidate to demonstrate the performance difference between R-SGD and SGDM. Fig. 6 shows that R-SGD improves significantly over BackProp, reaching a similar test performance to that of Adam with moderate training data size (M = 10K). R-SGD achieves a test error of 0.0535 \u00b1 0.0021, compared with BackProp reaching 0.1047\u00b1 0.0020, and Adam reaching 0.0535\u00b1 0.0016 (as in Table I). The test error is averaged over five independent runs (different sets of training and test examples). Note that, as training size increases (e.g., M = 15K), the test performance of R-SGD becomes slightly better than that of Adam (Table I). Adaptive methods such as Adam often show faster initial progress, but their performances get quickly trapped by a test error plateau [20]. In contrast, as shown in the inset of Fig. 6, R-SGD as a non-adaptive method is able to reach a lower test error by taking a few more epochs (note that Adam needs more computer memory to store the uncentered variance of the gradients).\n8\nAs shown in Fig. 7 (a), SGDM with adaptive momentum parameter reaches a higher test error than R-SGD, which confirms that the switch-like event plays an important role in guiding R-SGD to a good region. In R-SGD, the reinforcement along some weight components is turned off with a finite probability, then along these directions, only the current gradients are used (the same as those used in BackProp). But the reinforcement may be turned on along these directions once again during training. In contrast, for SGDM, the momentum term with adaptive momentum parameter is always applied to update all weight components during training. This mechanism difference leads to different test performances observed in Fig. 7 (a). The training dataset may change the error surface in practice. It seems that the performance of R-SGD is robust to this change (Table I). NAG with adaptive momentum parameter \u03c1t = \u0393(t) learns quickly but gets trapped by a slightly higher test error. In addition, a single epoch in NAG takes an extra computational cost of MB |W| due to the partial update.\nFor fixed-momentum-parameter SGDM (\u03c1t does not change over time, unlike the reinforcement probability in RSGD), the learning performance gets worse (Fig. 7 (b)). When applied to a deep network with ReLU activation and cross-entropy as the objective function, R-SGD still shows competitive performance (Fig. 7 (c)).\nWe also use bilinear interpolation method to qualitatively analyze the test error surface of the three algorithms (BackProp, R-SGD and Adam). Using the bilinear interpolation method [21], one can visualize the error surface in 3D subspace spanning four high-dimensional weight configurations. These four weight configurations defined as {Wi}4i=1 are chosen either from one learning trajectory or from solutions obtained starting from four different random initializations. Based on these four configurations, the error function is varied as a function of a new constructed weight matrix specified by W = \u03b2(\u03b1W1 + (1\u2212 \u03b1)W2)) + (1\u2212 \u03b2)(\u03b1W3 + (1\u2212 \u03b1)W4), where \u03b1 \u2208 [0, 1] and \u03b2 \u2208 [0, 1] are two control parameters. Results are shown in Fig. 8 for one trajectory interpolation. BackProp decreases the test error slowly, and finally reaches a plateau and get stuck there. Adam decreases the error very quickly, and reaches a more apparent and lower plateau than that of BackProp. Remarkably, R-SGD first decreases the error quickly, which is followed by a plateau. The plateau is then passed and finally R-SGD reaches a region with less apparent flatness. Interpolation results of four different solutions are shown in Fig. 9. Clearly, BackProp can get stuck by different solutions of different qualities, depending on initializations. Both Adam and R-SGD can reach solutions of nearly the same quality, despite different initializations. However, the subspace looks sharper for R-SGD than for Adam. This rough analysis by using bilinear interpolation method implies that the high dimensional topology of weight space seen by the three algorithms might be intrinsically different. However, to characterize necessary properties of a good region with nice generalization capabilities requires a theoretical understanding of the entire high dimensional weight space in terms of analyzing some non-local quantity. Establishing a theoretical relationship between learning performances of various SGD algorithms and the intrinsic structure of the error surface is still an extremely challenging task in future studies."}, {"heading": "VI. CONCLUDING REMARKS", "text": "In this paper, we propose a new type of effective strategy to guide SGD to overcome the plateau problem typical in deep neural network learning. This strategy takes into account previous (accumulated) gradient information in a probabilistic way when updating current weight matrix. It introduces a stochastic reinforcement to current gradients, and thus enhances the exploration ability of the original SGD. This strategy is essentially different from an independently random noise added to the gradient during the learning process [22, 23]. In fact, we add time-dependent Gaussian noise to gradients during training in our simulations using default hyper-parameters [22], whose performance could not be comparable to that of R-SGD within 100-epochs training (or it requires longer convergence time).\nA similar reinforcement strategy has been used in a single layer neural network with discrete weights [24], where local fields in a belief propagation equation are reinforced. In our work, we study deep neural networks with continuous weights, and thus the gradients are reinforced. The reinforced belief propagation is conjectured to be related to local entropy maximization [6, 25] in discrete neural networks. Whether R-SGD reshapes the original non-convex error\n9\nsurface in some way, such that searching for a parameter region of good-quality is facilitated, remains an interesting open question. We leave this for future work.\nIn the current setting, the learning performance of R-SGD is comparable to (in MNIST) or even better than that of Adam (in the synthetic dataset), which requires one-fold more computer memory to store the uncentered variance of the gradients. The learning step-size of Adam is adaptively changed, which means that the step-size is automatically tuned to be closer to zero when there is greater uncertainty about the direction of the true gradient [15], while RSGD uses the stochastic reinforcement of the gradient information to deal with this kind of uncertainty, and shows comparable and even better performance.\nA recent study argued that adaptive learning methods such as Adam generalize worse than SGD or SGDM on CIFAR-10 image classification tasks [20]. It is thus very interesting to evaluate the performance of R-SGD on more complicated deep network model and complex datasets, which are left for future systematic studies [26].\nR-SGD may be able to avoid vanishing or exploding gradient problem typical in training a very deep network [4], probably thanks to accumulation of gradients used stochastically. In addition, it may take effect in recurrent neural network training [27]. In fact, previous gradient information at each step can be weighted before accumulation according to its importance in guiding SGD. This is a very interesting direction for future studies on fundamental properties of R-SGD.\n10"}, {"heading": "Acknowledgments", "text": "We are grateful to the anonymous referee for many constructive comments. H.H. thanks Dr. Alireza Goudarzi for a lunch discussion which later triggered the idea of this work. This work was supported by the program for Brain Mapping by Integrated Neurotechnologies for Disease Studies (Brain/MINDS) from Japan Agency for Medical Research and development, AMED, and by RIKEN Brain Science Institute.\n[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In P. Bartlett, F.c.n. Pereira, C.j.c. Burges, L. Bottou, and K.q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1106\u20131114, 2012. [2] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, Cambridge, MA, 2016. [3] Herbert Robbins and Sutton Monro. A stochastic approximation method. Ann. Math. Statist., 22:400\u2013407, 1951. [4] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Yee W.\nTeh and D. M. Titterington, editors, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS-10), volume 9, pages 249\u2013256, 2010. [5] Haiping Huang and Yoshiyuki Kabashima. Origin of the computational hardness for learning with binary synapses. Phys. Rev. E, 90:052813, 2014. [6] Carlo Baldassi, Christian Borgs, Jennifer T. Chayes, Alessandro Ingrosso, Carlo Lucibello, Luca Saglietti, and Riccardo Zecchina. Unreasonable effectiveness of learning neural networks: From accessible states and robust ensembles to basic\n11\nalgorithmic schemes. Proceedings of the National Academy of Sciences, 113(48):E7655\u2013E7662, 2016. [7] Alan J. Bray and David S. Dean. Statistics of critical points of gaussian fields on large-dimensional spaces. Phys. Rev.\nLett., 98:150201, 2007. [8] Yan V. Fyodorov and Ian Williams. Replica symmetry breaking condition exposed by random matrix calculation of\nlandscape complexity. Journal of Statistical Physics, 129(5):1081\u20131116, 2007. [9] A. Choromanska, M. Henaff, M. Mathieu, G. Ben Arous, and Y. LeCun. The loss surfaces of multilayer networks. ArXiv\ne-prints 1412.0233, 2014. [10] Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle point\nproblem in high-dimensional non-convex optimization. ArXiv e-prints 1406.2572, 2014. [11] R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping From Saddle Points \u2014 Online Stochastic Gradient for Tensor Decompo-\nsition. ArXiv e-prints 1503.02101, 2015. [12] Jimmy Ba, Roger Grosse, and James Martens. Distributed Second-Order Optimization using Kronecker-Factored Approx-\nimations. ICLR-2017, 2017. [13] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of\nthe IEEE, 86:2278\u20132324, 1998. [14] Rumelhart David E., Hinton Geoffrey E., and Williams Ronald J. Learning representations by back-propagating errors.\nNature, 323:533\u2013536, 1986. [15] D. Kingma and J. Ba. Adam: A method for stochastic optimization. ArXiv e-prints 1412.6980, 2014. [16] Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance of initialization and momentum\nin deep learning. In Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28, ICML\u201913, pages 1139\u20131147, 2013. [17] Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer US, 2004. [18] Hugo Larochelle, Yoshua Bengio, Je\u0301rome Louradour, and Pascal Lamblin. Exploring strategies for training deep neural\n12\nnetworks. J. Mach. Learn. Res., 10:1\u201340, 2009. [19] P. Chaudhari, A. Choromanska, S. Soatto, Y. LeCun, C. Baldassi, C. Borgs, J. Chayes, L. Sagun, and R. Zecchina.\nEntropy-SGD: Biasing Gradient Descent Into Wide Valleys. ArXiv e-prints 1611.01838, 2016. [20] A. C. Wilson, R. Roelofs, M. Stern, N. Srebro, and B. Recht. The Marginal Value of Adaptive Gradient Methods in\nMachine Learning. ArXiv e-prints 1705.08292, 2017. [21] D. Jiwoong Im, M. Tao, and K. Branson. An Empirical Analysis of Deep Network Loss Surfaces. ArXiv e-prints 1612.04010,\n2016. [22] A. Neelakantan, L. Vilnis, Q. V. Le, I. Sutskever, L. Kaiser, K. Kurach, and J. Martens. Adding gradient noise improves\nlearning for very deep networks. ArXiv e-prints 1511.06807, 2015. [23] P. Chaudhari and S. Soatto. The effect of gradient noise on the energy landscape of deep networks. ArXiv e-prints\n1511.06485, 2015. [24] A. Braunstein and R. Zecchina. Learning by message passing in networks of discrete synapses. Phys. Rev. Lett, 96:030201,\n2006. [25] Carlo Baldassi, Alessandro Ingrosso, Carlo Lucibello, Luca Saglietti, and Riccardo Zecchina. Subdominant dense clusters\nallow for simple learning and high computational performance in neural networks with discrete synapses. Phys. Rev. Lett., 115:128101, 2015. [26] Alireza Goudarzi, Haiping Huang, and Taro Toyoizumi. in preparation, 2017. [27] R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training Recurrent Neural Networks. ArXiv e-prints 1211.5063,\n2012."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Deep Learning", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "A stochastic approximation method", "author": ["Herbert Robbins", "Sutton Monro"], "venue": "Ann. Math. Statist.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1951}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS-10),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Origin of the computational hardness for learning with binary synapses", "author": ["Haiping Huang", "Yoshiyuki Kabashima"], "venue": "Phys. Rev. E,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Statistics of critical points of gaussian fields on large-dimensional spaces", "author": ["Alan J. Bray", "David S. Dean"], "venue": "Phys. Rev. Lett.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Replica symmetry breaking condition exposed by random matrix calculation of landscape complexity", "author": ["Yan V. Fyodorov", "Ian Williams"], "venue": "Journal of Statistical Physics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "The loss surfaces of multilayer networks", "author": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G. Ben Arous", "Y. LeCun"], "venue": "ArXiv e-prints", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "ArXiv e-prints", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Escaping From Saddle Points \u2014 Online Stochastic Gradient for Tensor Decomposition", "author": ["R. Ge", "F. Huang", "C. Jin", "Y. Yuan"], "venue": "ArXiv e-prints", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations", "author": ["Jimmy Ba", "Roger Grosse", "James Martens"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ArXiv e-prints 1412.6980,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George E. Dahl", "Geoffrey E. Hinton"], "venue": "In Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Introductory lectures on convex optimization: A basic course, volume 87", "author": ["Y. Nesterov"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Exploring strategies for training deep neural  12 networks", "author": ["Hugo Larochelle", "Yoshua Bengio", "J\u00e9rome Louradour", "Pascal Lamblin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "author": ["P. Chaudhari", "A. Choromanska", "S. Soatto", "Y. LeCun", "C. Baldassi", "C. Borgs", "J. Chayes", "L. Sagun", "R. Zecchina"], "venue": "ArXiv e-prints", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "The Marginal Value of Adaptive Gradient Methods in Machine Learning", "author": ["A.C. Wilson", "R. Roelofs", "M. Stern", "N. Srebro", "B. Recht"], "venue": "ArXiv e-prints", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "An Empirical Analysis of Deep Network Loss Surfaces", "author": ["D. Jiwoong Im", "M. Tao", "K. Branson"], "venue": "ArXiv e-prints", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["A. Neelakantan", "L. Vilnis", "Q.V. Le", "I. Sutskever", "L. Kaiser", "K. Kurach", "J. Martens"], "venue": "ArXiv e-prints", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "The effect of gradient noise on the energy landscape of deep networks", "author": ["P. Chaudhari", "S. Soatto"], "venue": "ArXiv e-prints", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Learning by message passing in networks of discrete synapses", "author": ["A. Braunstein", "R. Zecchina"], "venue": "Phys. Rev. Lett,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Subdominant dense clusters allow for simple learning and high computational performance in neural networks with discrete synapses", "author": ["Carlo Baldassi", "Alessandro Ingrosso", "Carlo Lucibello", "Luca Saglietti", "Riccardo Zecchina"], "venue": "Phys. Rev. Lett.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "On the difficulty of training Recurrent Neural Networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ArXiv e-prints", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Multilayer neural networks have achieved state-of-the-art performances in image recognition [1], speech recognition, and even natural language processing [2].", "startOffset": 92, "endOffset": 95}, {"referenceID": 1, "context": "Multilayer neural networks have achieved state-of-the-art performances in image recognition [1], speech recognition, and even natural language processing [2].", "startOffset": 154, "endOffset": 157}, {"referenceID": 2, "context": "This impressive success is based on a simple powerful stochastic gradient descent (SGD) algorithm [3], and its variants.", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "However, the parameter space is highly non-convex for a typical deep network training, and finding a good path for SGD to improve generalization ability of deep neural networks is thus challenging [4].", "startOffset": 197, "endOffset": 200}, {"referenceID": 4, "context": "As found in standard spin glass models of neural networks [5, 6], a non-convex error surface is accompanied by exponentially many local minima, which hides the (isolated) global minima and thus makes any local search algorithms easily get trapped.", "startOffset": 58, "endOffset": 64}, {"referenceID": 5, "context": "In addition, the error surface structure of deep networks might behave similarly to random Gaussian error surface [7, 8], which demonstrates that critical points (defined as zero-gradient points) of high error have a large number of negative eigenvalues of the corresponding Hessian matrix.", "startOffset": 114, "endOffset": 120}, {"referenceID": 6, "context": "In addition, the error surface structure of deep networks might behave similarly to random Gaussian error surface [7, 8], which demonstrates that critical points (defined as zero-gradient points) of high error have a large number of negative eigenvalues of the corresponding Hessian matrix.", "startOffset": 114, "endOffset": 120}, {"referenceID": 7, "context": "Consistent with this theoretical study, empirical studies on deep network training [9, 10] showed that SGD is slowed down by a proliferation of saddle points with many negative curvatures and even plateaus (eigenvalues close to zero in many directions).", "startOffset": 83, "endOffset": 90}, {"referenceID": 8, "context": "Consistent with this theoretical study, empirical studies on deep network training [9, 10] showed that SGD is slowed down by a proliferation of saddle points with many negative curvatures and even plateaus (eigenvalues close to zero in many directions).", "startOffset": 83, "endOffset": 90}, {"referenceID": 9, "context": "The prevalence of saddle points poses an obstacle to attain better generalization properties for a deep network, especially for SGD based on first-order optimization, while second-order optimization relying on Hessian-vector products is more computationally expensive [11].", "startOffset": 268, "endOffset": 272}, {"referenceID": 10, "context": "The second order method that relies on positive-definite curvature approximations, can not follow directions with negative curvature, and is easily trapped by saddle points [12].", "startOffset": 173, "endOffset": 177}, {"referenceID": 11, "context": "The excellent performance of R-SGD is verified first on training a toy fully-connected deep network model to learn a simple non-linear mapping generated by a two-layer feedforward network, and then on a benchmark handwritten digits dataset [13], in comparison to both vanilla backpropagation (BackProp) [14] and state-of-the-art Adam algorithm [15].", "startOffset": 240, "endOffset": 244}, {"referenceID": 12, "context": "The excellent performance of R-SGD is verified first on training a toy fully-connected deep network model to learn a simple non-linear mapping generated by a two-layer feedforward network, and then on a benchmark handwritten digits dataset [13], in comparison to both vanilla backpropagation (BackProp) [14] and state-of-the-art Adam algorithm [15].", "startOffset": 344, "endOffset": 348}, {"referenceID": 13, "context": "In the benchmark dataset, we also clarify the performance difference between R-SGD and SGD with fixed or adaptive momentum parameter [16] and Nesterov\u2019s momentum [17].", "startOffset": 133, "endOffset": 137}, {"referenceID": 14, "context": "In the benchmark dataset, we also clarify the performance difference between R-SGD and SGD with fixed or adaptive momentum parameter [16] and Nesterov\u2019s momentum [17].", "startOffset": 162, "endOffset": 166}, {"referenceID": 15, "context": "forms the vanilla BackProp widely used in training deep networks given the labeled data [18].", "startOffset": 88, "endOffset": 92}, {"referenceID": 14, "context": "The second one is Nesterov\u2019s accelerated gradient (NAG) [17], which first implements a partial update to Wt, and then uses the updated Wt to evaluate gradients, i.", "startOffset": 56, "endOffset": 60}, {"referenceID": 12, "context": "To show the efficiency of R-SGD, we also compare its performance with that of a state-of-the-art stochastic optimization algorithm, namely adaptive moment estimation (Adam) [15].", "startOffset": 173, "endOffset": 177}, {"referenceID": 12, "context": "We use heuristic parameters of Adam given in [15], except that \u03b70 = 0.", "startOffset": 45, "endOffset": 49}, {"referenceID": 16, "context": "This observation is consistent with a recent study of maximizing local entropy in deep networks [19].", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "Adaptive methods such as Adam often show faster initial progress, but their performances get quickly trapped by a test error plateau [20].", "startOffset": 133, "endOffset": 137}, {"referenceID": 18, "context": "Using the bilinear interpolation method [21], one can visualize the error surface in 3D subspace spanning four high-dimensional weight configurations.", "startOffset": 40, "endOffset": 44}, {"referenceID": 0, "context": "Based on these four configurations, the error function is varied as a function of a new constructed weight matrix specified by W = \u03b2(\u03b1W1 + (1\u2212 \u03b1)W2)) + (1\u2212 \u03b2)(\u03b1W3 + (1\u2212 \u03b1)W4), where \u03b1 \u2208 [0, 1] and \u03b2 \u2208 [0, 1] are two control parameters.", "startOffset": 186, "endOffset": 192}, {"referenceID": 0, "context": "Based on these four configurations, the error function is varied as a function of a new constructed weight matrix specified by W = \u03b2(\u03b1W1 + (1\u2212 \u03b1)W2)) + (1\u2212 \u03b2)(\u03b1W3 + (1\u2212 \u03b1)W4), where \u03b1 \u2208 [0, 1] and \u03b2 \u2208 [0, 1] are two control parameters.", "startOffset": 201, "endOffset": 207}, {"referenceID": 19, "context": "This strategy is essentially different from an independently random noise added to the gradient during the learning process [22, 23].", "startOffset": 124, "endOffset": 132}, {"referenceID": 20, "context": "This strategy is essentially different from an independently random noise added to the gradient during the learning process [22, 23].", "startOffset": 124, "endOffset": 132}, {"referenceID": 19, "context": "In fact, we add time-dependent Gaussian noise to gradients during training in our simulations using default hyper-parameters [22], whose performance could not be comparable to that of R-SGD within 100-epochs training (or it requires longer convergence time).", "startOffset": 125, "endOffset": 129}, {"referenceID": 21, "context": "A similar reinforcement strategy has been used in a single layer neural network with discrete weights [24], where local fields in a belief propagation equation are reinforced.", "startOffset": 102, "endOffset": 106}, {"referenceID": 22, "context": "The reinforced belief propagation is conjectured to be related to local entropy maximization [6, 25] in discrete neural networks.", "startOffset": 93, "endOffset": 100}, {"referenceID": 12, "context": "The learning step-size of Adam is adaptively changed, which means that the step-size is automatically tuned to be closer to zero when there is greater uncertainty about the direction of the true gradient [15], while RSGD uses the stochastic reinforcement of the gradient information to deal with this kind of uncertainty, and shows comparable and even better performance.", "startOffset": 204, "endOffset": 208}, {"referenceID": 17, "context": "A recent study argued that adaptive learning methods such as Adam generalize worse than SGD or SGDM on CIFAR-10 image classification tasks [20].", "startOffset": 139, "endOffset": 143}, {"referenceID": 3, "context": "R-SGD may be able to avoid vanishing or exploding gradient problem typical in training a very deep network [4], probably thanks to accumulation of gradients used stochastically.", "startOffset": 107, "endOffset": 110}, {"referenceID": 23, "context": "In addition, it may take effect in recurrent neural network training [27].", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Herbert Robbins and Sutton Monro.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Xavier Glorot and Yoshua Bengio.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Haiping Huang and Yoshiyuki Kabashima.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] Alan J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] Yan V.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] Jimmy Ba, Roger Grosse, and James Martens.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] Ilya Sutskever, James Martens, George E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] Hugo Larochelle, Yoshua Bengio, J\u00e9rome Louradour, and Pascal Lamblin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25] Carlo Baldassi, Alessandro Ingrosso, Carlo Lucibello, Luca Saglietti, and Riccardo Zecchina.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[27] R.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Stochastic gradient descent (SGD) is a standard optimization method to minimize a training error with respect to network parameters in modern neural network learning. However, it typically suffers from proliferation of saddle points in the high-dimensional parameter space. Therefore, it is highly desirable to design an efficient algorithm to escape from these saddle points and reach a parameter region of better generalization capabilities. Here, we propose a simple extension of SGD, namely reinforced SGD, which simply adds previous first-order gradients in a stochastic manner with a probability that increases with learning time. As verified in a simple synthetic dataset, this method significantly accelerates learning compared with the original SGD. Surprisingly, it dramatically reduces over-fitting effects, even compared with state-of-the-art adaptive learning algorithm\u2014Adam. For a benchmark handwritten digits dataset, the learning performance is comparable to Adam, yet with an extra advantage of requiring one-fold less computer memory. The reinforced SGD is also compared with SGD with fixed or adaptive momentum parameter and Nesterov\u2019s momentum, which shows that the proposed framework is able to reach a similar generalization accuracy with less computational costs. Overall, our method introduces stochastic memory into gradients, which plays an important role in understanding how gradient-based training algorithms can work and its relationship with generalization abilities of deep networks.", "creator": "LaTeX with hyperref package"}}}