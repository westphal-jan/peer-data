{"id": "1701.08756", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2017", "title": "A Review of Methodologies for Natural-Language-Facilitated Human-Robot Cooperation", "abstract": "Natural Language (NL) for transferring knowledge from a human to a robot. Recently, research on using NL to support human-robot cooperation (HRC) has received increasing attention in several domains such as robotic daily assistance, robotic health caregiving, intelligent manufacturing, autonomous navigation and robot social accompany. However, a high-level review that can reveal the realization process and the latest methodologies of using NL to facilitate HRC is missing many important areas.\n\n\n\n\n\n\nThe results from the study are published online online in Proceedings of the National Academy of Sciences.", "histories": [["v1", "Mon, 30 Jan 2017 18:59:04 GMT  (1854kb)", "http://arxiv.org/abs/1701.08756v1", "30 pages, 15 figures, article submitted to Knowledge-based Systems, 2017 Jan"], ["v2", "Mon, 26 Jun 2017 19:20:07 GMT  (1866kb)", "http://arxiv.org/abs/1701.08756v2", "30 pages, 15 figures"], ["v3", "Thu, 17 Aug 2017 19:52:20 GMT  (883kb)", "http://arxiv.org/abs/1701.08756v3", "13 pages, 9 figures"]], "COMMENTS": "30 pages, 15 figures, article submitted to Knowledge-based Systems, 2017 Jan", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.CL cs.HC", "authors": ["rui liu", "xiaoli zhang"], "accepted": false, "id": "1701.08756"}, "pdf": {"name": "1701.08756.pdf", "metadata": {"source": "CRF", "title": "Methodologies realizing natural-language-facilitated human-robot cooperation: A review", "authors": ["Rui Liu", "Xiaoli Zhang"], "emails": ["xlzhang}@mines.edu"], "sections": [{"heading": null, "text": "submitted to Knowledge-based Systems, January 2017\nRecently, research on using NL to support human-robot cooperation (HRC) has received increasing attention in several domains such as robotic daily assistance, robotic health caregiving, intelligent manufacturing, autonomous navigation and robot social accompany. However, a high-level review that can reveal the realization process and the latest methodologies of using NL to facilitate HRC is missing. In this review, a comprehensive summary about the methodology development of natural-language-facilitated human-robot cooperation (NLC) has been made. We first analyzed driving forces for NLC developments. Then, with a temporal realization order, we reviewed three main steps of NLC: human NL understanding, knowledge representation, and knowledge-world mapping. Last, based on our paper review and perspectives, potential research trends in NLC were discussed. Keywords: natural language, human-robot cooperation, NL understanding, robot knowledge representation, knowledge-world mapping\n1. Introduction\nAttracted by the naturalness of natural language (NL) communications among humans, the intelligent robots start to use NL to interact with humans during the cooperation [1]. NL is used with either a spoken or written manner for delivering humans\u2019 cooperation requests to a robot, facilitating an intuitive human-robot cooperation (HRC)[2][3][4] [5]. Natural-language-facilitated human-robot cooperation (NLC) has received increasing attention in human-involved intelligent robotics research over the recent decade. By using NL, human intelligence at high-level task planning and robot physical capability (such as force [6], precision [7] and speed [2]) on low-level task executions are combined to perform an intuitive cooperation [6][8].\nCurrently, typical human-robot communication methods include tactile indications such as contact location [9], force strength [10][11], and visual indications such as body pose [12][13], and motion [14][15] etc. Compared with these methods, using NL to conduct an intuitive NLC has several advantages. First, NL makes HRC natural. For traditional methods mentioned above, humans involved in HRC need to be trained to use certain actions/poses for making themselves understandable [16][17]. While in NLC, even non-expert users without prior training can cooperate with a robot by using human-like communication [18][19]. Second, cooperation requests are described accurately. Traditional methods using actions/poses only provide limited patterns to roughly describe cooperation requests due to information loss in action/pose simplification (such as using markers to simplify actions) [20][21][22][23]. While in NLC, cooperation requests related to action, speed, tool and location are already defined in NL expressions [7][24]. With these expressions, cooperation requests for various task executions are described accurately. Third, NL transfers cooperation requests efficiently. The information-transferring method using actions/poses requires the design of informative patterns for different cooperation requests [20][21]. Existing languages, such as English, Chinese and German, already have standard linguistic structures, which contain abundant informative expressions to serve as patterns [25][26]. NL-based methods do not need to design specific informative patterns for various cooperation requests, making HRC efficient. Lastly, since NL instructions are delivered orally instead of being physically involved, human hands are set free to perform more important executions.\nWith the above advantages, NLC has been widely explored in areas, including daily assistances [27][28], medical caregiving [29][30], manufacturing [8][31], indoor/outdoor navigation [36][37], social accompany [38][39] etc. Typical areas using NLC systems are shown in Fig. 1.\n_______________________________________ *Corresponding author: Tel.:+1-303-384-2343; fax:+1-303-273-3602, email:xlzhang@mines.edu.\nsubmitted to Knowledge-based Systems, January 2017\n2. Motivation\nFrom the realization perspective, motivations of NLC research are concluded as research developments in natural\nlanguage processing (NLP) and HRC.\n2.1. NLC is motivated by NLP\nNLP automatically analyzes text\u2019s semantic meaning by modeling computational models and extracting linguistic features. Pushed by recent improvements of machine learning technics in classification [40], clustering [41] and feature extraction [42], NLP has been developed from simply syntax-driven processing, which builds syntax representations of sentence structures, to semantically-driven processing, which builds semantic networks for sentence meanings [43].\nStarting in the year 1950, the simplest semantic analysis method, keyword-based understanding method, was designed [44], enabling a na\u00efve word-symbol understanding by identifying single/multiple keywords [45][46], lexical affinities [47][48], and word/affinity occurrences [49][50]. The word-based NLP method separately understood word meanings, and sentence meaning was unknown.\nTo enhance semantic analysis capability of a NLP method, concept-based method was designed to allow sentencemeaning-level understanding of NL expressions. The concept-based method modeled semantic meanings of sentences and paragraphs by exploring embedded concepts, mainly including implicit NL indications [51][52], hierarchical ontologies [53][54], and semantic correlations [55][56]. The concept-based method not only understood explicit facts, such as involvements of action/object/events/persons, but also understood implicit indications, such as action purpose, object usage, event meaning, and human intentions.\nTo practically implement NLP methods in real-world robot-involved situations, a narrative-based method was developed to create a more sophisticated knowledge representation in a decision-making-focused [57], real-worldaware [58] and human-cognition-imitated manner [59][60]. In this method, mechanisms of human reasoning and planning, knowledge-to-world mapping, and logic-based human understanding & learning were focused in NLP process. Supported by this method, knowledge was practically used in NLC, further improving robots\u2019 knowledge scalability and human-robot-cooperation flexibility. The development stages of NLP research are summarized in Fig. 2 [44].\n2.2. NLC is motivated by HRC\nIn an early period (about 1940s\u2019 [61]), humans started to interact with robots by using remote controllers, developing an initial HRC, in which cooperation requirements for action mapping, task goal mapping, and cooperation naturalness/effectiveness were not considered. As tasks became complicated, both the robot and human in HRC were assigned with different roles, such as leader-follower and cooperator-cooperator, to perform different parts of a task with considerations of task goal accomplishments, human-robot communications, robot/human statuses and physical/mental capabilities. Compared with HRI, which focuses on general interactions (detailed HRI reviews are in [62][63]) for physical/mental assistances without task-goal constrains, HRC focuses on specific cooperation for task fulfillment with task-goal constrains, such as task planning and adjusting (detailed HRC reviews are [61][64]). In this paper, we emphasized HRC, specifically exploring state-of-the-art robotic systems using NL to facilitate HRC in various research domains.\nRecently, HRC has been developing: it began largely from a low-cognition-level action research where actions\nsubmitted to Knowledge-based Systems, January 2017\nwere designed and selected according to human instructions, grew to a middle-cognition-level interaction research where shallow-level understanding of human motions, activities, tasks and safety concerns was conducted, and is now expanding to a high-cognition-level human-centered engagement research where human\u2019s psychological and cognitive statuses, such as attention, motivation, emotion and user models, are considered to improve HRC effectiveness and naturalness. Increasing human involvements in HRC is shown in Fig. 3.\nThe first stage of HRC development is action-based HRC. It started from motor-control-based action design where a robot followed simple human instructions to adjust its actions [65][66]. To make robot actions natural, a human-like motion style was then adopted in robot action design [67][68]. Though robots\u2019 motion behavior was similar with that of a human, robots\u2019 cooperation performances were still poor due to the limited action-understanding being insufficient to support its adaptations towards users/environments [69][70]. To improve robots\u2019 adaptability, action interpretations were added for a robot in HRC. For example, \u2018cup manipulation\u2019 in \u2018drinking\u2019 activity meant \u2018containing liquid\u2019\u201d [15]. Though the understanding was still limited to an action level, robots\u2019 understanding towards human behaviors in HRC was improved [71].\nThe second stage of HRC development is interaction-based HRC. It started from action-understanding-based\nsubmitted to Knowledge-based Systems, January 2017\nmovement imitation [72][73] where a robot was required to learn from human demonstrations, understand human movements and develop its own movements. To improve the understanding of human movements, robots were provided with various informative motion data, such as human action trajectories [74], hand/body poses [75], and biosignals [76]. To further improve robots\u2019 cooperation performance, robots were trained to explore the mutual influence between a human and his/her surrounding environment [77][78].\nThe third stage of HRC development is engagement-based HRC. Individual-level factors, such as individual attentions [79][80], personalities [81][82], emotions [83][84] and safety factors [70][88], were considered by robots in the cooperation. With engagement-based HRC method, robot cognition was further improved by adapting various individuals. This method is widely used in present-day HRC research. Given the urgent needs for intuitive HRC, a natural and effective cooperation manner such as NL-based cooperation has been focused on.\n2.3. Systematic overview of NLC research\nWith cross-disciplinary technique supports, NLC has been developed from low-cognition-level symbol matching control, such as using \u201cyes/no\u201d for controlling robotic execution, to high-cognition-level task understanding, such as \u201cgo straight and turn left at the second cross\u201d.\nAs a result of NLC research, a substantial number of projects were launched, including \u201ccollaborative research: jointly learning language and affordances\u201d from Cornell University [85], \u201crobots that learn to communicate with humans through natural dialog\u201d from University of Texas at Austin [86], \u201ccollaborative research: modeling and verification of language-based interaction\u201d from MIT [87], \u201clanguage grounding in robotics\u201d in University of Washington [89], \u201csemantic systems\u201d from Lund University [90] etc. NLC research is regularly published in international journals such as IJRR [91], TRO [92], AI [93] and KBS [94], and international conferences such as ICRA [95], IROS [96] and AAAI [97]. The publication trend is shown in Fig. 4, where the increasing significance of using NL to facilitate HRC is reflected by steadily increasing publication numbers.\nCompared with existing review papers about HRC using gesture/pose [98][99], action/motion [100], and tactile [101], a review paper about NLC is lacking. Given the promising potential and increasing attention of NLC, it is necessary to make a summary of state-of-the-art robotic systems in wide-range domains, revealing the current research progress and signposting future NLC research. The organization of this review paper is shown in Fig. 5.\n3. Framework of NLC realization\nThe realization of NLC in HRC is challenging due to technique limitations in environment perceiving, task reasoning, and cooperation acting to support intuitive robot performances. Various methodologies have been developed to realize an intuitive NLC in robotic systems. By tracking knowledge flow, the general process of NLC realization are categorized into three main stages: NL understanding, knowledge representation, and knowledge-world mapping. In each stage, the rationale, realization mechanism, solved problems, advantages and disadvantages of existing methodologies are summarized and compared. In NL understanding, cooperation-related human NL descriptions are analyzed to understand humans\u2019 cooperation requests. In knowledge representation, piecemeal knowledge extracted in the NL understanding stage is represented with an algorithm structure to support robots\u2019\nsubmitted to Knowledge-based Systems, January 2017\ndecision making about task execution planning, human intention prediction, and human need detection. In knowledgeworld mapping, theoretical knowledge in robots\u2019 databases is mapped to practical things and relations, such as action, location, object, and human status, in the real world. Only with success mapping between theoretical knowledge and real-world things can NLC be conducted successfully. The flowchart of a general NLC realization process is shown in Fig. 6.\nFig.5. Organization of this review paper. This review systematically summarized methodologies for using NL to facilitate HRC. It includes four main sections: NLC background in which NLC-research\u2019s motivation is introduced, NLC methodologies in which detailed NLC approaches are summarized, NLC applications, in which typical NLC applications are introduced at a high level, and NLC trends in which the future NLC research directions, are summarized. Among four sections, the methodologies have been comprehensively summarized by following a temporal execution order.\nsubmitted to Knowledge-based Systems, January 2017\n4. NL understanding\nNL understanding enables a robot to receive human-assigned tasks, follow human-preferred execution procedures, and understand the surrounding environment, thereby deciding both accuracy and intuitiveness of HRC. The realization process of NL understanding is shown in Fig. 7. To theoretically support human NL instruction understanding in NLC, two types of semantic analysis models were developed: feature-based models and aspect-based models.\n4.1 Models\nIn feature-based models, linguistic features, such as words, Part-of-Speech (PoS) tags, word dependencies, word references and sentence syntax structures, were mainly used to analyze the meaning of human NL instructions in NLC, shown in Fig. 8. In aspect-based models, semantic features, such as temporal/spatial relation, object categories, and object functional roles, were mainly used to analyze the meaning of human NL instructions in NLC, shown in Fig. 9. A. Feature-based models\nPredefinition model. The simplest model for NL understanding is the predefinition model, which is also the earliest NL-understanding model for NLC [1]. By identifying targeted keywords [102][103] and phrases [104][105] in human NL, a robot accurately understood symbolic and concise human cooperation requests when these requests were predefined exactly. Then, to disambiguate word meanings, PoS tags for keywords were defined for polysemy situations [106][107], in which one word/phrase could have multiple meanings and one meaning could be expressed by multiple words/expressions. Labeling words by PoS tags improved NL understanding performances, endowing predefinition models with initial semantic analysis capability. To model meanings of NL expressions in NLC, structural correlations, including keyword dependency, PoS dependency, and keyword meaning similarities, were summarized [32][108]. By using these structural correlations, the meaning of a whole sentence could be modeled for understanding human\u2019s cooperation requests. Predefinition models had good performances (success feature understanding rate 83%~100% [109]) in trained situations, whereas humans\u2019 NL expressions were defined in robots\u2019 knowledge. However, this type of models had a poor NL-understanding performance in untrained situations where NL expressions were not defined, or in similar situations where NL expressions were partially defined [102]. With a predefinition manner, potentially-encountered expressions were listed out explicitly. Only when NL expressions were exactly matching expressions in robots\u2019 databases could human NL requests be identified. The word-based meaning modeling is at a shallow level, merely suitable for static situations.\nGrammar model. To disambiguate the meanings of human NL requests and further improve robots\u2019 adaptability towards various NL expressions in NLC, feature-based grammar models were developed by summarizing general linguistic rules from humans\u2019 NL requests. Some feature-based grammars explored execution logics. For example,\nsubmitted to Knowledge-based Systems, January 2017\nverb word and noun word were combined to describe a type of actions such as V(go) + NN(Hallway) and V(grasp) + NN(cup) [103][108][114][115]. Some feature-based grammars explored temporal relations, such as the if-then relation \u201cif door open, then turn right\u201d and the step1-step2 relation \u201cgo -- grasp\u201d [116][117][118]. Some feature-based grammars explored spatial relations, such as the IN relation \u201ccup IN room\u201d and the CloseTo relation \u201ccup CloseTo plate\u201d [37][119][120]. The rationale of feature-based grammar method is that sentences with a similar meaning have similar syntax structures. By evaluating the syntax structure similarity, similarity of NL meanings were detected.\nInterpretation model. To understand abstract and implicit NL requests, interpretation models were developed. One typical interpretation model is a probabilistic interpretation model in which informative features such as keywords were implicitly correlated with probability likelihood. Typical works are [116][121][122][123]. Needed information, such as \u201cbeverage is likely to be juice\u201d, was recommended for understanding abstract requests, such as \u201cput beverage in the fridge\u201d, according to the correlation likelihood [123]. Probabilistic model recommends human-unmentioned knowledge for a robot according to the probabilistic correlations. With this probabilistic interpretation model, the uncertainty in NL expressions was modeled, disambiguating cooperation requests and improving robots\u2019 adaptation towards different human users with various NL expressions. Another typical interpretation model is a commonsense interpretation model. Its rationale is that general commonsense knowledge accumulated from daily life experiences could be used in specific situations for specifying abstract/ambiguous human NL requests. Compared with probabilistic interpretation models, which used objective probabilistic correlations, the commonsense interpretation model uses subjective empirical interpretation. Typical usages include the following types. By defining sensor value ranges for ambiguous descriptions, such as \u201cslowly, frequently, heavy\u201d, the commonsense interpretation model made quantitative interpretations of ambiguous execution requests in NLC [118][124]. By integrating key aspects, such as precondition, action sequence, human preference, tool usages and location information, the commonsense interpretation model made machine-executable task plans based on human-described high-level abstract task plans [7] [37][107][115][125]. By using discrete fuzzy statuses to map continuous sensor data, unlimited objective sensor values were \u2018translated\u2019 into limited subjective NL expressions such as \u201cclose to the robot, in the kitchen\u201d [112][126]. By combining human factors such as \u2018human\u2019s visual scope\u2019 with linguistic features such as a keyword \u201cwrench\u201d, the commonsense interpretation model became context-sensitive, making a robot understand human NL requests such as \u201c deliver him a wrench\u201d from the human perspective \u201chuman desired wrench is actually the human-visible wrench\u201d\nFig.8. Typical feature-based models for NL request understanding. (a) [102] and (b) [110] are predefined models. In (a), the predefined keywords such as \u201cobjects: book pen, animate: person, robot\u201d were detected from a user\u2019s NL instructions, assisting a Jijo-2 office robot with executing tasks such as \u201cfind a person, delivery a book\u201d. In (b), the robotic arm\u2019s motion was controlled by predefined vowels in human speech. (c) [111] is a grammar model, in which object manipulation methods are defined as linguistic structures such as \u201cfind(cup), grasp(milkbox), \u2026\u201d. (d) [112] (e) [113] and (f) [112] are interpretation models, in which a command for a robot arm is interpreted into execution parameters. For example, in (d), ambiguous NL expressions such as \u201cvery little\u201d was interpreted as a very small distance value. In (e), NL expression such as \u201cOpenLeft\u201d was interpreted as specific parameter \u201copen left hand for 1 DOF.\u201d\nsubmitted to Knowledge-based Systems, January 2017\n[111][117][127][128]. The advantages are that robots\u2019 cognition levels are improved by means of knowledge scaling up and knowledge mutual compensation. With this interpretation model, a robot can explore unstructured environments by exploiting its learned and existing knowledge. B. Aspect-based models\nSingle-modality model. Single-modality models are similar with feature-based models in that both of them have predefinition models, grammar models, and interpretation models. One difference is that feature-based models merely use linguistic features while single-modality models use both linguistic features and semantic aspects features. The rationale of single-modality models is that analyze meanings of NL requests by exploring semantic correlations, such as \u201cgoal-substep, substep-substep, substep-toolUsage \u2026\u201d, among semantic aspects, such as the logic order, sub-steps, tool usages, during task executions. Typical semantic aspects include object categories such as \u201cbox, ball\u201d [127], task-\nrelated facts such as execution order \u201ccommand \u2192 global constraint \u2192 \u2026\u201d plan statuses such as \u201cplanned, failed,\nsuspended, cancelled, complete\u201d [129], human factors such as \u201caction type, current location, voice amplitude, head motion\u201d [130][131], environment context such as \u201cobject is at behand, elevators are in right \u201d [132][133] etc. Manners for using semantic aspects in single-modality models mainly include first-order logic, conditional/joint probability distribution, and feature space classification. Based on an aspect-based model, either explicit correlations, such as temporal/spatial correlation, or implicit correlations, such as human/robot/task statuses, were modeled. Typical problems solved by single modality models include: human-desired tool identification using semantic aspects such as color, shape and weight [131][134], human visual perspective understanding by using cognition aspects such as human preference, human physical ability and human needs [135][136], task motivation understanding using executionrelated key factors such as action sequences, tool functions, locations [7][132] etc. The aspect-based model is generally better than the feature-based method in understanding human NL requests in NLC [122]. This is because the aspectbased model is abstract with a better summary of essential rules behind various human NL expressions, adapting a wide range of NL expression manners and contents to avoid the cold-start phenomenon (it is an untrained situation [142]). Moreover, aspect-based models are more similar to human cognitive process. For example, semantic aspects \u201caction-effect\u201d and \u201cobject-function\u201d were human\u2019s cognitive perspectives for understanding task cooperation [15].\nFig.9. Typical aspect-based models for NL request understanding. (a) [115], (b) [7], (c) [137] and (d) [123] are typical single-modality aspectbased models. In (a), only physical-property aspects such as \u201ccolor: yellow, size: short, ..\u201d were involved in NL requests to describe a humandesired object. In (b), only execution parameters such as \u201caction:rub, wipe, sweep. objective: debris, junk, saqdust. location: hole, point, corner.\u201d were considered to describe a task-execution process. In (c), spatial relations such as \u201cthrough, down, to, ..\u201d were detected in human NL instructions, assisting an robot with its navigation. (e) [138], (f) [139], (g) [140], and (h) [141] are multi-modality aspect-based models. In (e), robot memory, real-world states and human NL instructions were integrated to instruct a robotic with plan executions. In (f), the face/gaze tracking, hand poses, object identities, and human NL instructions were integrated to assign object manipulation tasks to a robot. In (g), the tactile sensing, and human NL instructions were integrated to assist a robot with object delivery. In (h), body poses and human NL instructions were integrated together to assist a robot with its environment adaptation.\nsubmitted to Knowledge-based Systems, January 2017\nIn single-modality models, all information used for understanding human NL requests is extracted from human NL requests themselves.\nMulti-modality model. The single-modality model effectively captures informative patterns in NL expressions to understand humans\u2019 cooperation requests. However, human requests are usually situated such that humans\u2019 NL requests are closely correlated with situation-related information such as task execution progress, environmental conditions, and human status. An accurate understanding of human NL requests needs the integration of multimodality semantic features instead of merely exploring information from human NL expressions. A multi-modality model was designed to integrate different modality information together for comprehensive meaning modeling. The rationale of multi-modality models is that a human is considered to be dependent from the surrounding environment and better understanding comes from environment exploration. With multi-modality models, information from multimodalities was aligned to establish semantic corrections. From the perspective of knowledge origination, multimodality models are categorized into models using human-related features and models using environment/robotrelated features. For multi-modality models using human-related features to understand human NL expressions, typical features beyond linguistic features considered in single-modality models also include facial expressions (joy, sad etc.) [143], individual identity [139], touch events [140], hand poses [144], and head orientations [75]. Supported by rich information from multi-modality features, typical solved problems include complex-instruction understanding [141], human-like HRC [75], social behavior interpretation [126] etc. For multi-modality models using environment/robot-related features to understand human NL requests in NLC, including linguistic features considered in single-modality models, typical features also include temporal dependencies among speech-head orientation, hand gesture domains [75], spatial relations among human, robot, and object [141], visual-auditory-combined indications [145], knowledge/sensorimotor-combined intention predictions [138], etc. Supported by rich information from these features, typical solved problems include real-time communication, context-sensitive cooperation (sensor-speech alignment), machine-executable task plan generation, implicit human request interpretation, etc. Typical algorithms used for constructing multi-modality models include hidden Markov model (HMM) for modeling hidden probabilistic relations among semantic aspects [145][146], Bayesian Network for modeling probabilistic transitions among taskexecution steps [147][148], first-order logic for modeling semantic constraints among semantic aspects [130][149] etc. These algorithms integrate different modalities with appropriate contribution distributions and extract contributive feature patterns among modalities. Multi-modality models have three potential advantages in understanding human NL expressions: (1) By exploring multi-modality-information sources, rich information can be extracted for an accurate NL understanding. (2) Information in one modality can be compensated by information learned from other modalities for better NL disambiguation. (3) Consistency of multi-modality information enables mutual confirmations among knowledge from multiple modalities. A reliable NL request understanding could be conducted. Supported by these advantages, multi-modality models have the potential to understand complex cooperation plans and various users, and to perform practical NL understandings in real-world NLC situations.\n4.2. Open Problems A. Feature-based method\nPredefinition model. In practical situations, a human partner usually uses various expressions to express the same meaning or express various meanings using the same expression. Understanding these varieties beyonds the capability of a predefined model in analyzing semantic meanings of human NL requests.\nGrammar model. Although models\u2019 adaptability towards NL understanding has been enhanced, the drawback is that feature correlations needed for understanding have been exhaustively listed. It is difficult to summarize likelyencountered grammar rules. This feature-based grammar method could only deal with trained NL expressions, and is incapable of dealing with untrained NL expressions. Considering that usages of words and sentence structures in NL expressions varies according to different partners/tasks/environments/robots, NL-understanding-models\u2019 adaptability is still limited and insufficient to support natural communications in NLC. Moreover, human NL requests are usually abstract and often some detailed NLC-related information, such as logic relations and execution-specific location/tool/action, is ignored at the keyword level. Understanding word meaning is insufficient to understand sentence meanings in NLC.\nInterpretation model. One drawback of interpretation models is that all interpretations need to be manually defined, which is time-consuming and labor-intensive. Moreover, even though the interpretation model could interpret abstract linguistic features into detailed specific information, it still suffers the feature ambiguity problem. This issue arises due to interpreted information being likely to be mapped onto wrong abstract features, decreasing the accuracy\nsubmitted to Knowledge-based Systems, January 2017\nand reliability of NL understanding and further decreasing robots\u2019 user/environment adaptability. B. Aspect-based method\nSingle-modality model. Even though single modality models understand NL requests in a semantic-analysis manner, performances of these single modality models are still limited when human NL expressions are relatively complex in either description manner or content. The main reason is that human NL requests are correlated with practical situations in the real world, and consist of information about tasks\u2019 working status, human/robot statuses and environment conditions. Information from a single NL modality is insufficient to support the meaning understanding towards human NL requests during NLC.\nMulti-modality model. Even though multi-modality models are capable of comprehensively understanding human NL requests by considering practical situations, challenges still exist within this model. First, it is difficult to combine different types of modalities such as motion, speech, and visual cues with an appropriate manner to reveal practical contribution distributions for different modalities. Second, it is difficult to extract contributive features for describing both distinctive and common aspects of one modality in understanding NL requests. Third, the overfitting problem still exists when using multi-modality information to understand NL requests. NL understandings based on different modalities could be mutually conflicting, thereby preventing the practical implementation of multi-modality models.\n5. Knowledge representation\nKnowledge representation organizes previously learned knowledge with specific algorithm structures, providing a theoretical foundation for formulizing robots\u2019 decision making mechanisms. The realization process for knowledge representation is shown in Fig. 10. In this representation, correlations among NLC-related knowledge, such as execution steps, step transitions, and actions/tools/locations involvements, and their temporal/spatial/logic relations are defined. From the perspective of algorithm structures, knowledge representation models have three main types: probabilistic model, logic model and cognitive model.\n5.1 Models\nThe probabilistic model explored probabilistic dependencies among NLC-task-related knowledge, focusing on cooperation accuracy, shown in Fig. 11. The logic model explored logic constraints among NLC-related knowledge, focusing on cooperation success, shown in Fig. 12. The cognitive model explored the motivations among NLC-taskrelated knowledge, focusing on cooperation reasonability, shown in Fig. 13. A. Probabilistic model\nIn regards to probabilistic relations, probabilistic models for knowledge representations are categorized into a generative model, which is based on generative algorithms, such as Na\u00efve Bayesian network (BN) [150], Hidden Markov model (HMM) [151], and Markov Random Field (MRF) [152], and a discriminative model, which is based on discriminative algorithms such as Support Vector Machine (SVM) [153] and Conditional Random Field (CRF) [154]. Given that generative algorithms are constructed by joint probabilistic relations, generative models are good at exploring semantic associations among knowledge inside one task; given that discriminative algorithms are constructed by conditional probabilistic relations, discriminative models are good at exploring the mutual distinctiveness among knowledge crossing various tasks. Therefore, the generative model is typically used to represent one task, while the discriminative model is typically used to distinguish multiple tasks.\n(i). Generative models Joint-probabilistic BN. Joint-probability Bayesian Network (BN) is the simplest generative model involved in NLC. By using a single joint probability p(x, y), the probabilistic association p of a human\u2019s cooperation request y, such as \u201cmove\u201d, and one execution parameter x, such as object \u201cball\u201d, was made [155]. The rationale of jointprobabilistic BN is that observations of entity co-occurrences in practical situations are consistent with knowledge of entity semantic correlations in theoretical representations. Based on this observation-knowledge-consistency assumption, knowledge representations are directly extracted from large amounts of sensor-based observations. Typical joint-probability associations in NLC include activity-object associations such as drink-cup [111], activityenvironment associations such as drink-hotDay [77], and action-sensor associations such as pickup-0.1m/s [156]. In knowledge representation, a single joint-probabilistic BN association is used as independent evidence to describe one semantic aspect of a task. For using multiple joint-probabilistic associations \u220f \ud835\udc5d(\ud835\udc65\ud835\udc56 , \ud835\udc66\ud835\udc56)\ud835\udc56 , the semantic aspects of NLC task are collected from various NL descriptions and sensor data patterns. Typical methods using multiple jointprobability associations include Viterbi algorithm [156], Na\u00efve Bayesian (NB) algorithm [77] and Markov Random Field (MRF) [78]. With these algorithms, the most complete representation described in human\u2019s requests is selected\nsubmitted to Knowledge-based Systems, January 2017\nas a human-desired cooperation task. The rationale of multi-joint-probabilistic BN models is that associations describe semantic aspects of a cooperation task. With multi-joint-probabilistic BN models, solved problems include: modeling tasks\u2019 meaning distributions on linguistic features such as NL keywords and expression patterns [78][157]; aligning multi-view sensor data such as speech meaning, task execution statuses, and robot/human motion statuses [115][158];\nTable. 1. Summary of NL understanding methods"}, {"heading": "Feature-based Models Aspect-based Models Predefinition Grammar Interpretation Single-modality Multi-modality", "text": "Knowledge format\nsymbolic keywords linguistic structures meaningful concepts Semantic\ncorrelations\nSemantic correlations\nFeatures words, PoS tags word patterns, PoS\npatterns\nprobabilistic correlations, ontology correlations object categories, temporal\nexecution orders, task / human / environment statuses\nobject categories, temporal execution orders, task / human / environment statuses\nAlgorithms NAN first-order logic ontology tree typical\nclassification algorithms (NB, SVM), FirstOrder Logic\ntypical classification algorithms (NB, SVM), First-Order Logic\nUnderstanding level\nlow low low ~ moderate moderate ~ high moderate ~ high\nUser adaptability\nlow low low moderate high\nCold start serious serious moderate rare rare Rationale detect keywords/PoS detect keyword/PoS\npatterns\ninterpret abstract words into meanings summarize semantic aspects summarize semantic aspects\nand their internal correlations cross different information modalities\nSolved problems\ninitially understand human-mentioned entities such as objects, actions, directions, locations etc. further understand logic relations in task procedures, temporal relations of execution actions, spatial relations of task-\nrelated entities.\nspecify abstract executions into machine-executable executions\nentity identification, human visual perspective understanding, task-execution motivation understanding\ncomplex task instruction understanding, human-like HRC, social behavior understanding, context-sensitive cooperation\nAdvantages performance is good\nand steady in trained situations\nperformance is good and steady in trained situations model human cognitive process, scaling up robot\nknowledge\ngood adaptability towards human linguistic habits, model human cognitive process more cooperationrelated information is involved. information is more reliable.\nknowledge mutual compensation and verification\nDisadvantages exhaustive listing of\nNL commands, timeconsuming and labor-intensive. poor adaptability to various tasks/environments\nexhaustive listing of NL commands, timeconsuming and labor-intensive. poor adaptability to various tasks/environments exhaustive listing of NL commands, time consuming, laborintensive, lacking protocols for concept interpretation poor understanding / interpretation capability in dealing with ambiguous and incomplete NL\ninstructions\ndifficult to combine differentmodality features, difficult to extract important NL features, difficult to make a balance between conflicted evidences.\nTypical references\n[103][104][105] [106] [107]\n[116] [117][118] [119][120]\n[121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]\nsubmitted to Knowledge-based Systems, January 2017\nexploring the in-depth semantic meaning of a human-described task by associating shallow-level aspects such as words and actions with in-depth-level aspects such as execution effects [159][160]; intuitive task understanding by integrating current NL descriptions with previous execution experiences [161]; smart knowledge implementation by associating the theoretical knowledge such as objects with practical real-world evidences such as object color and placement locations [137][162]. For a summary of the problem-solving manners, the generative model can use large amounts of data to build accurate task representations by integrating relatively-complete semantic aspects for one NLC task. One common character of generative models such as na\u00efve Bayesian (NB) is that dependencies among task features are simplified to be fully or partially independent [77]. In practical situations, when a set of observations are made, observed evidences such as speech/object/context/action involvements in the cooperation are actually not mutually-independent [163]. As for task representation, this simplification brings both negative effects, such as undermining the representation accuracy, and positive effects, such as preventing overfitting problems in taskrepresentation process. It is also worth mentioning that, different from other multi-joint-probabilistic BN models, MRF can model the circle-type association in which repeated and cyclic occurrences are modeled within [164]. The unique function of a MRF is that mutual correlations among task\u2019s semantic aspects could be described. The common problem of multi-joint-probabilistic BN models is that temporal associations are ignored. Knowledge representations by multi-joint-probabilistic BN models are incapable of revealing temporal associations among tasks\u2019 semantic aspects, limiting the implementations of real-time NLC.\nDynamic BN. Dynamic Bayesian Network (DBN) explores temporal correlations by exploring sequential dependencies propagation p(\ud835\udc65\ud835\udc61|\ud835\udc65\ud835\udc61\u22121) among NLC-related features [165]. Given that the final format of DBN is the joint probabilistic form p(y, \ud835\udc65\ud835\udc61, \ud835\udc65\ud835\udc61\u22121, ..), DBN is still a joint-probabilistic model. A widely-used DBN algorithm in NLC is hidden Markov mode (HMM) algorithm [166], which uses a Markov chain assumption to explore the hidden influence of previous task-related features on the current NLC status. The rationale of HMM in NLC is that humandesired cooperation, such as going to a position, grasping a tool, and lifting a robot hand, are decided by the previous NLC working statuses (\ud835\udc65\ud835\udc61\u22121) and the current statuses \ud835\udc65\ud835\udc61. These statuses include environmental working conditions, task execution progress, human NL instructions, human/robot statuses etc. HMM uses both observation probabilities (absolute probability p(x)) and transitions abilities (conditional probability p(Y/X)) for modeling associations P(x, y) among NLC-related knowledge [166][167]. With a simple BN propagation, associations among NLC-related knowledge could be modeled efficiently. The Markov assumption in HMM efficiently models dynamic relations by considering previous statuses (\ud835\udc65\ud835\udc61\u22122, \ud835\udc65\ud835\udc61\u22123, \ud835\udc65\ud835\udc61\u22124, \u2026) as non-informative features and then ignoring all of them in task representation. With HMM models, solved problems mainly include real-time human NL request understanding [168][169], dynamical human status identification [167][170], accurate gesture recognition by simultaneously fusing multi-view data such as NL instruction, shoulder coordinates, shoulders-elbows\u2019 3D angle data, and hand poses [166][171] etc. Limited by Markov assumptions, HMM is only capable of modeling shallow-level hidden correlations among NLC-related knowledge. Moreover, given that hidden statuses need to be explored for HMM modeling, a large\nsubmitted to Knowledge-based Systems, January 2017\namount of training data is needed, limiting HMM implementations in unstructured scenarios with limited training data availability.\n(ii). Discriminative models Instead of representing tasks in generative models, a discriminative model was designed to distinguish a task from others. In a generative model, various semantic aspects are collected from a large amount of training data for describing a relatively-complete task representation. In a discriminative model, only the most distinctive aspects are collected from a relatively-small amount of data for identifying a task [172].\nConditional-probabilistic BN. Conditional-probabilistic BN is the simplest discriminative model with a single conditional relation p(y/x), modeling a probabilistic dependency between a human-desired cooperation task y and an evidence x such as a keyword and an action type [173]. Different from direct observations in joint-probabilistic BN, the conditional dependencies between human NL requests and a piece of evidence could not be directly observed due to the conditional dependency being implicit. Typical procedures for exploring conditional dependencies are the following: first, intentionally assume implicit dependencies among a human-requested task and its potential evidences; then, conduct learning to compute dependency strengths; last, keep influential dependencies by filtering out dependences with weak strengths [174][175]. The rationale of conditional-probabilistic BN models is that the evidence which frequently appears inside a task representation has a conditional dependency with the task. Typical solved problems include semantic meaning disambiguation such as object-feature dependencies [176], word-object mapping [7], word-sensor mapping [177], word-to-interpretation modeling [178] etc. A conditional-probabilistic dependency is considered as a knowledge component for constructing complicated discriminative models such as conditional random field (CRF). CRF is good at modeling semantic meaning propagation, in which semantic meanings of human NL instructions could be modeled by considering the likely task-related temporal dependencies. Different from HMM, in which only independent features and non-circle dependences are modeled, CRF is capable of modeling dependent\nFig.11. Typical probabilistic models for knowledge representation. (a) [167] and (b) [155] are generative models. (a) is a HMM model, in which NLC task\u2019s potential execution sequences are modeled by hidden Markov statuses. (b) is a na\u00efve Bayesian model, in which observations \u201cobject size, object shape\u201d and their conditional correlations such as \u201csize-big, shape-ball, ..\u201d are combined to form a joint-probability correlations such as \u201cobject-size-shape, \u2026.\u201d. (c) [174] and (d) [136] are discriminative models. (c) is a condition random filed model (CRF), in which task-related features such as \u201cpath length, landmarks, \u2026\u201d are involved to distinguish a navigation path from other paths. (d) is a SVM model, in which the most distinguishable pieces of evidence, such as \u201csocial affordance \u2018push-left\u2019, and voice behavior \u2018pass me\u2019\u201d, have been considered to identify human desired tasks such as \u201cPass/Take\u201d.\nsubmitted to Knowledge-based Systems, January 2017\nfeatures with circle dependencies [164]. Moreover, compared with HMM, which is modeling a task by using implicit hidden statuses, CRF models a task by using explicit feature dependencies.\nSupport vector machine (SVM). SVM is a typical discriminative model because SVM merely uses distinctive\nfeatures to distinguish NLC task representation from others, even when there are numerous features available. With this rationale, SVM is used to fuse the sensor data from multiple modalities. Typical solved problems include accurate intention inferences by combining motion/vision/speech information [179][180], robotic task executions using motion/speech commands [181][182], human NL request disambiguation [171][183] etc. B. Logic model\nTo model subsequent relations among task procedures, logic models for task knowledge representation were designed. The existing logic models mainly include an ontology model and first-order logic model. Ontology models focus on using hierarchical ontology structures to interpret a simple action/status, while the first-order-logic model focuses on using a logic workflow to describe a complex execution-procedure.\nOntology model. An ontology model uses a hierarchical semantic structure to describe inner correlations among NLC-related knowledge (a sample structure is shown in Fig. x) [27]. The rationale of an ontology model is that a detailed execution step could be expressed by a multi-layer ontology tree, inside which necessary execution parameters, such as action sequences, tool usages and locations, are defined. The benefit of the ontology-tree task representation is that the exchangeability of included procedures is increased by replacing unavailable ontology execution parameters with the available same-type-ontology parameters. The environment adaptability of a taskrepresentation model is improved [15][27]. Typical solved problems include intuitive object usages by defining object categories and replacing the missing object by the same-semantic-category object [15][184], complex executionprocedure modeling by integrating crucial execution components, such as task execution procedures, objects and actions, and detailed execution parameters, such as speed, location, action and tool usage, into an ontology tree [185][186], intelligent robot control in which a control goal was decomposed into sub-strategies such as {notActive, run, hold done, \u2026} and each strategy was endowed with ontologies constrains such as robot motion statuses, robot physical capabilities, and human safety considerations [112][187], human NL command disambiguation by looking up ambiguous words in an ontology tree to get extra semantic evidences, such as \u201cCup-UnitOfVolume, action: put, teaspoon-UnitOfVolume\u201d [27][78], intuitive robot conceptualized programming, such as telling the robot to focus on\nobjects that are within range (perceived object \u222a imagined object \u222a mathematical object), adapting dynamic\nenvironments in task execution by defining detailed replaceable parameters for execution components, such as \u201cmanipulation types, handing skills, \u2026\u201d, in an ontology tree [22][109], imitating human execution manners by defining both execution procedures and sensor parameters such as \u201clocation, speed, velocity, \u2026\u201d in an ontology tree [188][189], autonomous manufacturing by defining abstract manufacturing plans such as assembly plan { Task/skill/process\u2014assemblyTask/GenericTask/WoodworkingTask/weldingTask\u2014assemblePartsTas/pickTask} in an ontology tree to simplify user involvements [190][191], environment perceiving by using ontology interpretations on the 3D point cloud [192][193] etc. Compared with probabilistic models, the relative advantage of using an ontology model is that task execution procedures could be flexibly modeled, increasing the robot task execution robustness in various environments.\nFirst-order logic model. In NLC, a first-order-logic model describes execution procedures as first-order logic\nformulas, such as \u201cin possible worlds a kitchen is a region (\u2200w\u2200x(kitchen(w,x) \u2192 region(w,x)))\u201d [194]. An ontology\nFig.12. Typical logic models for knowledge representation. (a) [185] is an ontology model, in which an assembly task \u201cbearing assembly\u201d is decomposed into several layers of sub-steps such as \u201cbearing + pipe, bearing 2 + tree, \u2026.\u201d. (b) [33] is a first-order-logic model, in which NLC tasks such as \u201cfinding a cup\u201d is decomposed into different logic constraints such as \u201ccupAction + graspingPose \u2192 detectCup success\u201d. (c) [114] is also an first-order-logic model, in which logic relations such as \u201cmove=(grasp, place), \u2026\u201d is defined to control robots\u2019 motion in NLC.\nsubmitted to Knowledge-based Systems, January 2017\nmodel is describing a single time frame, in which necessary information for one robot execution has been detailed in an ontology manner, enabling a flexible robot execution in NLC. A first-order logic model meanwhile describes logic relations among multiple time frames, in which necessary sequences for robot execution have been detailed in a firstorder logic manner, enabling a reasonable robot execution in NLC. The rationale of first-order logic models in NLC is that an NLC task is decomposed into sequential logic formulas by satisfying which specific NLC task could be accomplished. In a first-order logic model, logics are equally important without contribution differences towards execution success. Logic relations in first-order-logic models, including tool usages, action sequences, locations etc., are defined in the structure. Typical solved problems include autonomous robot navigation by using logic navigation sequences, such as going to a location \u201challway\u201d then going to a new location \u201crest room\u201d [147][149][194], environment uncertainty modeling by summarizing potential executions such as \u201cground atoms (boolean random variables) eats(Dominik, Cereals), uses(Dominik, Bowl), eats(Michael, Cereals) and uses(Michael, Bowl)\u201d [195], robot action control by defining action-usage logics such as \u201cmove (grasp piece(location, grip), place piece(location, ungrip))\u201d [147][196][197][198], autonomous failure analysis by looking up first-order logic representations to detect the missing knowledge such as \u201ctool brush, action: sweep\u201d [7][198], and NL-based robot programming by using the grammar language such as point(object, arm-side), lookat(object), and rotate(rot-dir, arm-side) [130]. The drawback of first-order logic models in modeling NLC tasks is that logic relations defined in the model are hard constraints. If one logic formula was violated in practical execution processes, the whole logic structure would be inapplicable and the task execution would fail. This drawback limits model\u2019s implementation scopes and reduces robots\u2019 environment adaptability. Moreover, hard constraints were defined indifferently, ignoring the relative importance of executions. The execution flexibility is undermined due to critical executions not being focused and trivial executions not being ignored when the NLC plan modifications are necessary. C. Cognitive model\nHuman cognition in task planning is reflected in flexibly-changing execution plans (different procedures), adjusting execution orders (same procedures, different orders), removing some less-important execution procedures or adding more critical executions procedures (similar procedures, similar orders). According to the logic influence definition manner, cognitive models could be categorized into two main types: fuzzy-logic model and weighted logic model.\nFuzzy-logic model. A fuzzy-logic model represents a human-desired NLC task by fully or partially fuzzy statuses, which are consistent with ambiguous descriptions in human NL requests. The rationale of a fuzzy-logic model is that human NL requests in NLC are information-rich and are meanwhile ambiguous and abstract, thereby requiring detailed interpretation manners such as fuzzy logic to decode embedded NLC-related information for a practical and intuitive robot execution. Typical solved problems include modeling motion statuses such as \u201cquick, slow\u201d by interpreting sensor data [199], sophisticated robot-manipulation control by interpreting NL control commands, such as \u201cmove little right, bend your elbow little\u201d, into fuzzy execution value range [112], emotion modeling by classifying\nFig.13. Typical cognitive models for knowledge representation. (a) [200] is a fuzzy emotion model, in which human\u2019s motion in NL expressions have been defined as fuzzy statuses \u201cvery low happy, vey high happy, \u2026\u201d, assisting HRC. (b) [191] is a weighted logic model, in which human\u2019s cognitive process in decision making is simulated by logics with different influence weights, based on which important logics with larger weights could be emphasized and trivial logics with smaller weights could be ignored. With this weighted-logic manner, the flexible cooperation between a human and a robot could be conducted.\nsubmitted to Knowledge-based Systems, January 2017\nemotion statuses such as happy {very low, low, medium, high, very high} in a fuzzy manner [200] etc. The advantage of using fuzzy logic is that the sensor data from perceiving systems for working status monitory, NL analysis, and environment condition assessment is continuous and unlimited. It is necessary to interpret these unlimited and objective data values into limited and subjective situation statuses, which are consistent with human perspective and easily understood by a robot. The major drawback of using the fuzzy-logic model in representing NLC cooperation task is that the fuzzy logic model only focuses on modeling status components inside one execution step and ignores mutual correlations among the steps.\nWeighted-logic model. A weighted-logic model represents human\u2019s cognitive processes by logic formulas as well as their weighted importance. A typical weighted-logic model is Markov logic network (MLN) model. MLN represents NLC task in a way such as \u201c0.3Drill(\ud835\udc5a\ud835\udc52\ud835\udc601) ^ 0.3TransitionFeasible(\ud835\udc5a\ud835\udc52\ud835\udc601,\ud835\udc5a\ud835\udc52\ud835\udc602) ^ 0.3Clean(\ud835\udc5a\ud835\udc52\ud835\udc602) \u21d2 0.9Task(\ud835\udc5a\ud835\udc52\ud835\udc601,\ud835\udc5a\ud835\udc52\ud835\udc602)\u201d [7], imitating human cognition process in task planning. In this model, single execution steps and step transitions were defined by logic clauses, which could be grounded into different logic formulas by substituting real-world conditions. With this cognitive model, a flexible execution plan could be generated by omitting weak-contributed and non-contributed logic formulas and involving strong-contributed logic formulas. Different from hard constraints in first-order logic models, constraints (logic clauses) in MLN are soft. These soft constraints mean when a human\u2019s cooperation requests are partially obeyed by a robot, the task could still be successfully executed. Typical solved problems include using MLN to generate a flexible and intuitive machine-executable plan from human NL instructions for autonomous industrial task execution [191], autonomous task executions in uncertain environments by using MLN to meet constraints from both robots\u2019 knowledge availability and real-world\u2019s knowledge requirements [195][201] etc.\nThe advantage of using weighted-logic models in NLC is that a weighted-logic method is relatively similar to a human\u2019s cognitive process in flexible plan generation. It helps a robot with execution in unfamiliar situations by modifying/replacing execution plans and parameters such as tool/action usages, improving robots\u2019 cognition levels and enhancing its environment adaptability. The major drawback is that MLN is still different from human cognitive processes to consider logic conditions at a deep level to enable plan modification, new plan making and failure analysis. Logic parameters for analyzing real-world conditions are still insufficient to imitate logic relations in human mind, thereby limiting robots\u2019 performances in adapting users and environments.\n5.2. Open Problems\nThese knowledge representation models are effective in many scenarios. However, they are still suffering from their unique shortcomings, limiting implementation scopes and effectiveness of NLC. First, from the perspective of cognition level, probabilistic is close in nature to direct human activity observations and can effectively extract meaningful features form human NL requests; however, the model lacks explorations of indirect human cognitive processes in NLC, limiting the naturalness in robotic executions. Second, the logic model has a higher cognition level compared with probabilistic models due to hierarchical interpretation supporting relatively intuitive task execution mechanisms. The logic model is, however, inflexible and incapable of simulating human\u2019s intuitive planning in realworld environments. Third, the cognitive model is the closest to human cognitive process in simulating flexible decision making process. Task execution is decomposed into cognitive steps with both weighted importance, internal correlations, and corresponding semantic interpretations. The plan flexibility largely increases robots\u2019 adaptability towards various users and environments. Though the human cognitive process is simulated with cognitive models, cognitive models are still suffering from two types of shortcomings. One shortcoming is that cognitive process simulation is still not cognitive process because the fundamental theory of cognitive process modeling is lacking, being insufficient to support a human-like task execution process [195]. The second problem is the difficulties associated with learning cognitive processes for current data collection and algorithm design. Different individuals have difference cognitive process, thus making it difficult to evaluate the reasonability of learned cognitive models [202].\n6. Knowledge-world mapping\nKnowledge-world mapping methods enable a robot to implement the learned knowledge representation into realworld NLC cooperation scenarios. The realization process of knowledge-world mapping is shown in Fig. 14. In considering the implementation process, knowledge-world mapping methods include two main types: theoretical knowledge grounding (Fig. 15) and knowledge gap filling (Fig. 16). In theoretical knowledge grounding, the methods project learned knowledge items, such as objects and spatial/temporal/logic relations, into corresponding\nsubmitted to Knowledge-based Systems, January 2017\nobjects/relations in real-world scenarios. In gap filling, methods detect both the missing knowledge, which is needed in real-world situations but hasn\u2019t been covered by theoretical knowledge representations, and the real-worldinconsistent knowledge, which is provided by a human but could not find corresponding things in practical real-world scenario.\n6.1 Models A. Theoretical knowledge grounding\nFrom the perspective of information content, theoretical knowledge grounding methods are categorized into three\nTable. 2. Summary of knowledge representation methods\nProbabilistic Model Logic Model Cognitive Model Generative Model Discriminative\nModel\nOntology Model 1st-order\nLogic Model\nFuzzy-Logic Model\nWeightedLogic Model\nKnowledge format\njoint probabilistic correlations\nconditional probabilistic correlations\nontology concepts\nlogic formulas sensor values,\nsemantic concepts\nlogic formulas, their weighted influences\nRationale observations\ndescribe a complete task\nobservations distinguish a task from other tasks\na task is consisted with ontology correlations.\ntask execution process is consisted with multiple logic relations\nsensor data could be interpreted into human\u2019s subjective interpretation\na task execution process is consisted with logic correlations and different influences.\nAlgorithms joint BN, NB\nHMM, MRF, Viterbi Algorithm\nconditional BN, CRF, Logistic Regression, SVM\nOntology tree First-order\nLogic\nFuzzy-logic algorithm\nMLN\nUser adaptability\nmoderate moderate low low low high\nCold start high low moderate high moderate low Solved problems modeling meaning distributions on NL\nexpressions, aligning multi-view sensor data, intuitive task understanding, flexible knowledge implementation, Real-time HRC, dynamically action identification\nmeaning disambiguation, entity-sensor data mapping, humanattended object identification, real-time uncertainty assessment during task execution dynamic environment adaptation, object interchanging, complex task representation, robust control, NL command disambiguation,\nautonomous manufacturing\nautonomous robot navigation, environment uncertainty modeling, autonomous execution failure diagnosis, situated robot programming\nfuzzy data fusion, sophisticated robot grasping control, emotion status modeling support a flexible machineexecutable plan implementatio n, task\nexecution in unknown environments\nAdvantages exploring a single\ntask, good at representing a complete task\nexploring various tasks, good at distinguishing tasks, need small amount of training data. autonomous robot execution, less human involvement\nstrong logic correlations among execution steps continuous sensor data could be modeled and\ninterpreted\nflexible task plan, human cognitive process imitating, strong environment/us er adaptability\nDisadvantages weak capability in\nmodeling the mutual distinctiveness among tasks, rely on large amount of training data.\nweak capability in representing a complete task; difficult to define/extract implicit dependencies merely define a single time frame in task execution, ignore sequential relations among time frames. inflexible task execution, weak environment adaptation only model one work procedure, unable to model correlations\namong working procedures\nparameters are difficult to learn, the current weighted logic is still far from a human cognitive process\nTypical references\n[155][156][157] [159][160]\n[172][173][174] [175][176]\n[184][185][186] [187][188]\n[147][149][194] [196][197] [7][199][112] [200] [191][195] [201]\nsubmitted to Knowledge-based Systems, January 2017\nmain methods: direct symbol mapping, general property mapping, and specific status interpretation.\nDirect symbol mapping. Direct symbol mapping methods take both knowledge items, such as objects, actions and locations in a theoretical knowledge database, and practical things, such as a physical object/action/location in realworld scenarios as symbols. By associating corresponding symbols in both the database and in the real world, theoretical knowledge is successfully implemented in practical NLC situations. The rationale of a direct symbol mapping method is that establishments of thing-item correlations enable a robot to know what and how to use knowledge in NLC, with the assumption that real-world things have been completely defined as knowledge items in the robots\u2019 database. Typical solved problems mainly include word-action associations such as word \u201cpick\u201d \u2013 action \u201cPick\u201d [203][204], direction instruction-motion behavior correlations such as word \u201cleft-turning\u201d \u2013 motion behavior \u201cLEFT-TURNING\u201d [19][137], word-object/building/person/location mappings such as \u201cperson-Person, trashcanTRASHCAN, hallway-HALLWAY\u201d [181][205] etc. The benefit of using direct symbol mapping methods is that basic knowledge items are directly mapped into the world, constructing fundamental robot knowledge to perform cooperation in a relatively static and simple NLC situation. The main limitation is that the method cannot deal with complex situations where complex relation mapping is needed.\nGeneral property mapping. Different from the direct symbol mapping method, which has an element-mapping manner, the general property mapping method has a structural mapping manner in which a knowledge item is defined with several properties such as visual properties \u201cobject color/shape\u201d, motion properties \u201caction speed\u201d and execution properties \u201ctool usage, location\u201d etc. The rationale is that a knowledge item can be successfully grounded into the real world by mapping its properties. Different from the ontology representation where the knowledge items are in a robot knowledge database, in the property mapping model, only top-level knowledge items are in robots\u2019 knowledge database while lower-level detailed properties are in the real world. Typical solved problems include: indoor routine identifying by using landmark locations such as \u201ckitchen, lobby\u201d [206]; object searching by using visual properties such as object color, size and shape [207]; motion execution such as \u201cpick up the tire pallet\u201d by defining action sequence \u201cdrive \u2013 insert \u2013 raise \u2013 drive \u2013 set\u201d [178]; NLC scene understanding such as \u201clounge, lab, conference room\u201d by checking spatial-semantic distributions of landmarks such as \u201challway, gym, \u2026\u201d [208] etc. The advantage of using property mapping methods is that knowledge could be mapped into the real world in a flexible manner, in which only parts of properties need to be mapped for grounding a theoretical item into a real-world thing. This manner could improve robots\u2019 adaptability towards users and environments. The limitation is that these property mapping methods still use predefinitions to give a robot knowledge, reducing the intuitiveness of HRC.\nSpecific status interpretation. Specific status interpretation method interprets a knowledge item in robots\u2019\nsubmitted to Knowledge-based Systems, January 2017\ndatabase into an information-rich, parameter-specific, and sensor-measurable thing in the real world. The rationale of the specific status interpretation method is that use the general commonsense accumulated in general scenarios to specify the ambiguous and abstract NL requests in specific scenarios in the real world. With this rationale, solved problems mainly include qualitative motion interpretations such as \u201cslowly \u2013 0.1 m/s, far \u2013 distance larger than 1 m\u201d [124][209], quantitative spatial relation interpretations such as \u201cobject 1 is at the left of an object b\u201d for two objects [37][210], and semantic interpretations such as goal \u201cdrill\u201d is interpreted as \u201cdrill (action: put down, drill, lift; precondition: no hole existing; requirements: slowly; tool: driller; \u2026 )\u201d [191][211] etc. The advantage of using the specific status interpretation method is that with large amounts of general commonsense, human-requested cooperation could be intuitively decomposed into machine-executable sub-goals, increasing robots\u2019 capability in dealing with task complexity and user/environment variety. Human NL instructions in NLC are usually ambiguous and abstract, making instructions non-executable for a robot. With the specific status interpretation, a robot can execute the cooperation even when some critical task-execution-related information such as a key step and a tool involvement is missing. The disadvantage is that commonsense knowledge is difficult to learn, and difficult to decide when and how to use at the proper time in the proper way. B. Knowledge gap filling\nA theoretical-knowledge representation defines an ideal real-world situation. Given unpredicted aspects in a practical situation, even if all defined knowledge has been accurately mapped into the real-world, it is still challenging to ensure the success of NLC by providing all knowledge needed in a practical situation. Especially in real-world situations, human users and environment conditions vary, causing the occurrences of knowledge gaps, which are knowledge required by real-world situations but are missing from robots\u2019 knowledge database. To ensure the success of robot executions, it is crucial to fill in these knowledge gaps. There are three main types of knowledge gaps: 1). Environment gaps, which are constraints such as tool availability and space/location limitations imposed by unfamiliar environments [212][213], 2). Robot gaps, which are constraints such as robot physical structure strength, capable actions and operation precision [17][212][214], and 3) user gaps, which are missing information caused by abstract/ambiguous/incomplete human NL instructions [8][135]. Filling up these knowledge gaps enhances robot capability in adapting dynamic environments and various tasks/users. Knowledge gap filling is challenging in that it is difficult to make a robot aware of its knowledge shortage in specific situations, and it is difficult to make a robot understand how missing knowledge should be compensated for in successful task executions.\nGap detection methods mainly include the following: 1) hierarchical knowledge structure checking, which detects knowledge gaps by checking real-world-available knowledge from top-level goals to low-level NLC execution\nFig.15. Typical methods for theoretical knowledge grounding. (a) [16] is a direct symbol mapping method, by which predefined motion behaviors such as \u201cavoiding, bowing, carring, ..\u201d are directly associated with their corresponding symbolic words. (b) [206] is a general property mapping method, by which special features such as \u201ckitchen location, lab locations, \u2026\u201d are considered to identify human-desired paths.(c) [37] is an interpretation mapping method, by which a navigation task is specifically interpreted by real-world conditions such as \u201caction: observe. namedobj:robot/building, \u2026 mode:quickly. \u2026 \u201d.\nsubmitted to Knowledge-based Systems, January 2017\nparameters defined in a hierarchical knowledge structure [118][214], 2) knowledge-applicability assessment, which detects knowledge gaps by checking the similarities between theoretical scenarios and real-world scenarios [138][214], and 3) performance-triggered knowledge gap estimation, which detects knowledge gaps by considering the final execution performances [8][187]. Hierarchical knowledge structure checking has the rationale that if desired knowledge defined in a knowledge structure is missing in real-world situations, then knowledge gaps exist. Knowledge applicability assessment has a rationale that if the NLC situation is not similar with the previously-trained situations, then knowledge gaps exist. Performance-triggered knowledge gap estimation has a rationale that if the final NLC performances of a robot is not acceptable, then knowledge gaps exist.\nGap filling methods mainly include: using existing alternative knowledge such as \u201cbrush\u201d in robot knowledge base to replace inappropriate knowledge \u201cvacuum cleaner\u201d in NLC tasks such as \u201cclean a surface\u201d [7][187], using general commonsense knowledge \u201cdrilling action needs driller\u201d in a robot database to satisfy the need for a specific type of knowledge such as \u201ctool for drilling a hole in the install a screw task\u201d [187][214], asking knowledge input from human users by proactively asking questions such as \u201cwhere is the table leg\u201d [8][215][216], autonomously learning from the internet [8][215][216] etc. In a big picture, the essence of knowledge filling is a dynamic knowledgeupdating process, in which inaccurate knowledge has been replaced by accurate knowledge and missing knowledge has been compensated for by needed knowledge. During the gap filling, objectives of execution repairing follow this\npriority-descending order: ensuring execution naturalness \u2192 remaining robot autonomy \u2192 minimizing interruption\ntoward human \u2192 ensuring feasibility of task execution \u2192 admitting and reporting task failures. Only when a higher-\npriority objective cannot be achieved can lower-priority objectives be executed.\n6.2 Open problems\nEven with current knowledge-world mapping methods, in which a robot could conduct an effective NL-based cooperation, there are still open problems that impede the cooperation from being broadly implemented in practical daily living and industrial scenarios.\nFirst, problems of ambiguity, abstraction, information-incomplete and real-world inconsistent in human NL instructions during NLC are caused by intrinsic NL characteristics such as omitting, referring and simplifying. But the problems are also caused by the ignorance of environment perceiving and interpretation. For example, if information such as task-related objects\u2019 working status, locations and relative distances from robot/human was ignored, it is difficult for a robot to infer which object the human user needs [111]. Second, when a human mentioned a task operation action or a tool, deep-level meanings such as \u201cobject functional role, operation\u2019s purpose and importance in this plan\u201d were not specified explicitly [217], limiting robots\u2019 environment/user adaptation. Third, NLP is still not accurate enough to accurately extract task-related information [7]. Last, when a robot queries knowledge from either a human or the open knowledge database such as openCYC [218]for filling knowledge gaps, the scalability is limited. For a specific user or a specific open knowledge database, available contents are insufficient to cover general knowledge needs of robots in executing a wide range of NLC tasks. The time/labor cost is high, further undermining knowledge support for NLC. An efficient, low-cost and proactive knowledge learning capability is needed for a robot.\n7. Typical NLC applications\nWith the support of the above methodologies, four main types of NLC applications are developed, including NLbased robot control where only the NL-format control symbol is given and comprehensive instruction understating is not involved [168], NL-based robot training where a comprehensive instruction understanding is required and intuitive task execution is not conducted [219], NL-based robot task execution where comprehensive understanding towards human instructions, practical situation conditions and human intentions is required and intuitive task execution is conducted [113], and robot social companion where human\u2019s social norms are required to be understood and respected, in addition to conducting NL-based execution [143].\nIn NL-based robot control, human hands are set free to reduce human\u2019s physical burdens. NL was initially used to replace physical control means such as a joystick or human hands to realize robot control. During the control, NL plays a role as information-delivering media, which contains human\u2019s control commands for robot executions. The human makes all the decisions in the control process, while a robot merely follows controlling commands delivered by human NL. From the burden-assignment perspective, during the whole cooperation process, the human mainly takes cognitive burdens, while a robot mainly takes physical burdens. Typical applications include control using symbolic words such as \u201cstart, stop\u201d [110], using semantic correlations such as \u201cgoTo-left\u201d [220], using logic\nsubmitted to Knowledge-based Systems, January 2017\nstructures such as \u201cfood type \u2013 vessel shape correlations\u201d [195], and using environmental conditions such as human safety [187], and location/building matching degree [17].\nIn NL-based robot training, a robot is endowed with the capability of advanced task planning and intuitive decision making by using NL to train robots on task executions in a spoken/written manner. During the training, knowledge is transferred from human experts to targeted robots. With consideration of robot physical characteristics such as force/structure/speed [221], human preferences such as motion/emotion [136], and real-world conditions such as object availability, distributions and locations [222], knowledge is specified for robot execution, improving robots\u2019 ability in task understanding, environment perceiving and reasoning during NLC. Different from NL-based control where a robot is not involved in advanced reasoning, in NL-based robot training, a robot is required to reason the mechanisms behind human NL instructions. According to knowledge transferring manners, NL-based robot training is categorized into four main types: training using human speech [222], training using physical demonstration [223], training using human feedback [157] and training using robot proactive querying [224]. For training using human speech, physical demonstration and human feedback, the robot passively learns. The difference is, in natural speech, the human only orally describes and does not participate in the execution. While as in training using demonstration, a human need to execute, and in training using human feedback, a human gives either execution corrections or speech corrections. For training using robot proactive querying, a robot proactively asks and learns from a human.\nDifferent from NL-based robot training in which human NL is helping a robot with its task understanding, in NLbased robot task execution, human NL is helping a robot with its task execution [113]. In NL-based training, by\nTable. 3. Summary of knowledge gap filling methods\nTheoretical knowledge grounding Knowledge gap filling Direct Symbol Mapping General Property Mapping Specific Status Interpretation Hierarchical Knowledge\nStructure Checking\nPerformanceTriggered Knowledge Gap Estimation\nKnowledge Format\nreal-world objects, spatial/temporal/logi c correlations real-world objects, spatial/temporal/logi c correlations real-world objects, spatial/temporal/logi c correlations real-world objects, spatial/temporal/logi c correlations real-world objects, spatial/temporal/logi c correlations\nRationales establishments of\nthing-item correlations enable a robot to know when and how to use knowledge in NLC\na knowledge item could be identified by its properties in the real world. use commonsense knowledge to specify ambiguous and\nabstract NL cooperation requests\nif the situation is not the familiar ones, then there are knowledge gaps. if the final performance is not good, then there much\nbe knowledge gaps.\nKnowledge mapping manner\nsingle-element mapping\nstructural mapping single-element\nmapping\nstructural mapping structural mapping\nSolved Problems\nword-action mapping, directionmotion mapping, word-object / building / person / location mapping indoor routine identifying, accurate object searching, scene understanding qualitative motion interpretation, quantitative spatial relation interpretation, semantic\ninterpretation\nenvironment gap detection, robot knowledge gap detection, user knowledge gap detection environment\u2019s gap detection, robots\u2019 knowledge gap detection, user\u2019s knowledge gap detection\nAdvantages accurate knowledge\ngrounding from database to the realworld.\nflexible knowledge mapping, improve robots\u2019 environment adaptability. abstract task execution process could be decomposed into specific\nmachine-executable plans.\nstrong capability to ensure the smoothness of task executions strong capability to ensure the success of task executions.\nDisadvantage s predefined manner limits robots\u2019\nenvironment adaptability, execution naturalness and intuitiveness\npredefined manner limits robots\u2019 environment adaptability, execution naturalness and intuitiveness predefined manner limits robots\u2019 environment adaptability, execution naturalness and intuitiveness difficult to decide which knowledge can miss difficult to decide which gaps lead to the failure of task executions\nTypical References\n[19][181][203] [137][204]\n[178][206][207] [208]\n[37][124][209] [210] [211]\n[161][187]\n[214]\n[214][215][216] [216]\nsubmitted to Knowledge-based Systems, January 2017\nmanners such as NL instructions, demonstrations, feedback and proactive querying, a robot creates a structurecompleted and execution-specified knowledge representation. But in NL-based task execution, including understanding the task, a robot is also required to perceive surrounding environments [141], predict human intentions [113], and make optimal decisions by satisfying all the environment, task and human requirements [161]. Giving the reasoning work requested in NL-based task execution, robots\u2019 cognition levels in NL-based execution is higher than that in NL-based training. With respect to who leads the execution, applications of NL-based execution could be categorized into the following two categories: 1) human-centered cooperation in which human is cognitively leading the task execution and a robot is required to provide appropriate assistances to facilitate humans\u2019 executions [161], and 2) robot-centered cooperation where a robot is cognitively leading the task execution and the human is providing physical assistances to facilitate robots\u2019 executions [127][149].\nIn NL-based robot social companion, to improve robot performances in NLC, social manners of humans and human communities are explored for robot learning. Different from NL-based task executions, which focus on execution methods, NL-based social companions focus on humans\u2019 social norm understanding and imitating [143]. With a social manner, robots are more naturally integrated with a human to form a human-centered robot task execution system and robots could also be more socially-acceptable for human communities [225][226]. According to implementation manners of social norms, applications using NL-based social companion are mainly categorized into social communication in which social NL is used for facilitating mutual information exchanging during NLC [225][226], and social execution in which social NL is used for facilitating task execution during NLC [227][228].\n8. Future research directions\nEven though the current methodologies mentioned above could effectively support an intuitive NLC, there are still many aspects that could be improved. The following research summarized the potential research directions from our perspectives.\n8.1. Comprehensive NL understanding.\nAlthough semantic aspects about NLC cooperation tasks have been explored in a multi-modality manner, they are still not truly \u201ccomprehensive\u201d. This is due to these methods being incapable of simultaneously performing accurate NL understanding, which is due to limited performances of NLP techniques [44], and intuitive environment interpretation, which is due to the lack of an intuitive sensor data fusion methods[8][191]. A temporal model for both understanding human\u2019s NL instructions and exploring real-time correlations among multi-modality sensor data is in urgent need.\n8.2. Human cognition modeling\nThe human cognitive process is the decision-making mechanism behind human-robot interaction. A reasonable modeling of the human cognitive process influences the flexibility of NLC. First, in the human cognitive process, human-requested cooperation is performed with different importance degrees. Not all human-requirements on execution procedures and orders are essential for the success of task executions. For example, in the task \u201cassembly\u201d, the procedure \u201cinstall the screw\u201d is more important than the procedure \u201cclean the place\u201d [8][191]. Second, each of the specific operation requests is interpreted to practical meanings, such as \u201cdeliver me a brush\u201d is for \u201ccleaning the surface\u201d and \u201ccup\u201d and \u201cglass\u201d in drinking have the same meaning as \u201ccontaining drinkable liquid\u201d [217]. By knowing this, NLC plans and manners could be flexibly changed by intuitively satisfying humans\u2019 essential needs instead of strictly following literal instructions. Third, the theory foundation of the human cognitive process, which is developed in neural science, is lacking in robotics research. In the future, one direction of NLC research could be developing a reasonable human-cognitive-process model, which is with theory supports from neural science to effectively support a flexible decision making for robotics.\n8.3. Failure experience learning\nFailure causes unnatural or incorrect task executions. Learning-from-failure mechanism is implemented in computer science for algorithm performance improvement [229], in material science for new material discovery [230] etc. By exploring useful information in failure experiences, robots\u2019 capability could be improved to avoid similar failures in the future. In NLC, learning from failure is initially involved in a definition-based manner [231], in which the failure is analyzed by comparing practical execution processes with defined knowledge, lacking the understanding of failure causes and meanwhile being passive in failure recovery. From our perspective, a future research direction in NLC could be developing effective failure-learning methodologies. By using these methodologies, failure causes\nsubmitted to Knowledge-based Systems, January 2017\ncould be analyzed and explained to users, useful experiences would be updated into robot knowledge databases, and similar failures could be avoided in subsequent NLC.\n8.4. Knowledge cost reduction\nCost reduction in knowledge collection is critical for intuitive NLC. On one hand, to understand human NL instructions, represent cooperation tasks, or fill in knowledge gaps, a large scale of reliable knowledge is needed. On the other hand, time/economy cost and labor investments need to be reduced. A knowledge-collection method for effectively collecting large amounts of knowledge with keeping costs low is in urgent need. To solve this problem, two trends in developing knowledge-scaling-up methods are designed: existing-knowledge exploitation, and new knowledge exploration. In existing-knowledge exploitation, existing specific knowledge is interpreted and extracted into general knowledge, thereby increasing knowledge interchangeability by making knowledge for specific situations suitable for general situations. In new-knowledge exploration, new knowledge is collected by proactively asking humans and autonomously retrieving from the Word Wide Web [232], books [233], operation logs [234] and videos [235]. These two trends have been initiated in the recent decade. From our perspective, future research could be using state-of-the-art NLP techniques, learning algorithms and information retrieval methods to transfer knowledge from information sources or even other robots to a robot, effectively reducing the cost in knowledge collection.\n8.5. Knowledge personalization\nWhen a robot cooperates with a specific human for a long time, the personalization of robots becomes critical. For personalization, it does not only mean defining individualized knowledge for a robot to adapt to a specific user, but it also means endowing a knowledge-individualization method for a robot to autonomously adapt to variable users [212]. In current NLC research, there are two trends on knowledge personalization: execution-preference-level personalization and the social-manner-level personalization. At the execution-preference level, one execution alternative is more preferred than the other alternatives [7]. In the social-manner level, a user\u2019s emotions, such as \u201chappy, sad, surprising\u201d, and social norms, such as \u201csafe zoon, comfortable distance\u201d, were considered in decision making [212]. Therefore, a future research direction would be developing knowledge-personalization methods to consider both the execution preferences and social norms, enabling a long-term effective knowledge personalization process.\n9. Conclusion\nThis review summarized the state-of-the-art methodologies for realizing natural-language-based human-robot cooperation (NLC), providing an in-depth analysis of the methodology advantages/disadvantages/open problems. From a methodology perspective, this review paper mainly summarized NLC with three steps: human NL understanding, task knowledge representation, and knowledge-world mapping. Potential research directions have also been suggested for future NLC research.\n10. Acknowledgements\nWe would like to thank Ms. Natalie Kalin, Mr. Ian Coberly and Mr. Xu Zhou for helping us with work finalization."}], "references": [{"title": "Understanding natural language", "author": ["T. Winograd"], "venue": "Cognitive Psychology, vol. 3, no. 1, pp. 1-91, 1972.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1972}, {"title": "Initiative in robot assistance during collaborative task execution", "author": ["J. Baraglia", "M. Cakmak", "Y. Nagai", "R. Rao", "M. Asada"], "venue": "ACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 67-74, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Teaching robots parametrized executable plans through spoken interaction", "author": ["G. Gemignani", "E. Bastianelli", "D. Nardi"], "venue": "International Conference on Autonomous Agents and Multiagent Systems, International Foundation for Autonomous Agents and Multiagent Systems, pp. 851-859, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Make it so: continuous, flexible natural language interaction with an autonomous robot", "author": ["J. Brooks", "C. Lignos", "C. Finucane", "M. Medvedev", "I. Perera", "V. Raman"], "venue": "Workshops at the 26th AAAI Conference on Artificial Intelligence, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Collaboration, dialogue, and human-robot interaction", "author": ["T. Fong", "C. Thorpe", "C. Baur"], "venue": "Proceedings of International Symposium of Robotics Research, DOI:10.1007/3-540-36460-9_17, 2001.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["S.A. Tellex", "T.F. Kollar", "S.R. Dickerson", "M.R. Walter", "A. Banerjee", "S. Teller"], "venue": "Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Natural-language-instructed industrial task execution", "author": ["R. Liu", "J. Webb", "X. Zhang"], "venue": "ASME International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, pp. V01BT02A043-V01BT02A04, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Recovering from failure by asking for help", "author": ["R.A. Knepper", "S. Tellex", "A. Li", "N. Roy", "D. Rus"], "venue": "Autonomous Robots, vol. 39, no. 3, pp. 347-362, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Human-robot-contact-state identification based on tactile recognition", "author": ["H. Iwata", "S. Sugano"], "venue": "IEEE Transactions on Industrial Electronics, vol. 52, no. 6, pp. 1468-1477, 2005.  submitted to Knowledge-based Systems, January 2017", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Hand force adjustment: robust control of force-coupled human\u2013robot-interaction in assembly processes", "author": ["J. Kruger", "D. Surdilovic"], "venue": "CIRP Annals - Manufacturing Technology, vol. 57, no. 1, pp. 41-44, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "An Analysis of Contact Instability in Terms of Passive Physical Equivalents", "author": ["E. Colgate", "N. Hogan"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 404-409, 1989.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1989}, {"title": "Intention Estimation and Recommendation System Based on Attention Sharing", "author": ["S. Kim", "J. Jung", "S. Kavuri", "M. Lee"], "venue": "International Conference on Neural Information Processing, 2013, pp. 395-402.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Latent Hierarchical Model for Activity Recognition", "author": ["N. Hu", "G. Englebienne", "Z. Lou"], "venue": "IEEE Transactions on Robotics, vol. 31, no. 6, pp. 1472-1482, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning preferences for manipulation tasks from online coactive feedback", "author": ["A. Jain", "S. Sharma", "T. Joachims", "A. Saxena"], "venue": "International Journa l of Robotics Research, vol. 34, no. 10, pp. 1296-1313, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding Human Behaviors with an Object Functional Role Perspective for Robotics", "author": ["R. Liu", "X. Zhang"], "venue": "IEEE Transactions on Cognitive and Developmental Systems, vol. 8, no. 2, pp. 115-127, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Action database for categorizing and inferring human poses from video sequences.", "author": ["W. Takano", "Y. Nakamura"], "venue": "Robotics and Autonomous Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Sorry Dave, I'm Afraid I Can't Do That: Explaining Unachievable Robot Tasks Using Natural Language.", "author": ["V. Raman", "C. Lignos", "C. Finucane", "K.C. Lee", "M.P. Marcus", "H. Kress-Gazit"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Inferring maps and behaviors from natural language instructions", "author": ["F. Duvallet", "M.R. Walter", "T. Howard", "S. Hemachandra", "J. Oh", "S. Teller", "N. Roy", "A. Stentz"], "venue": "Experimental Robotics, Springer International Publishing, pp. 373-388, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to parse natural language commands to a robot control system", "author": ["C. Matuszek", "E. Herbst", "L. Zettlemoyer", "D. Fox"], "venue": "Experimental Robotics, Springer International Publishing, pp. 403-415, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Motion capture based human motion recognition and imitation by direct marker control", "author": ["C. Ott", "D. Lee", "Y. Nakamura"], "venue": "8th IEEE-RAS International Conference on Humanoid Robots, pp. 399-405, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "A gesture based interface for human-robot interaction", "author": ["S. Waldherr", "R. Romero", "S. Thrun"], "venue": "Autonomous Robots, vol. 9, no. 2, pp. 151-173, 2000.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "Teaching and learning of robot tasks via observation of human performance", "author": ["R. Dillmann"], "venue": "Robotics and Autonomous Systems, vol. 47, no. 2-3, pp. 109-116, 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Flexible cooperation between human and robot by interpreting human intention from gaze information.", "author": ["K. Sakita", "K. Ogawara", "S. Murakami", "K. Kawamura", "K. Ikeuchi"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Information-theoretic dialog to improve spatial-semantic representations", "author": ["S. Hemachandra", "M.R. Walter"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Pattern grammar: A corpus-driven approach to the lexical grammar of English", "author": ["S. Hunston", "G. Francis"], "venue": "Computational Linguistics, vol. 27, no. 2, pp. 318- 320, 2000.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2000}, {"title": "Frequency and the emergence of linguistic structure", "author": ["J.L. Bybee", "P.J. Hopper", "eds."], "venue": "vol. 45. John Benjamins Publishing, 2001.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Understanding and executing instructions for everyday manipulation tasks from the world wide web", "author": ["M. Tenorth", "D. Nyga", "M. Beetz"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 1486-1491, 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Hierarchical semantic labeling for task-relevant rgb-d perception", "author": ["C. Wu", "I. Lenz", "A. Saxena"], "venue": "Robotics: Science and Systems, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning models for following natural language directions in unknown environments", "author": ["S. Hemachandra", "F. Duvallet", "T.M. Howard", "N. Roy", "A. Stentz", "M.R. Walter"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 5608-5615, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "On the feasibility of using a standardized test for evaluating a speech-controlled smart wheelchair", "author": ["J. Pineau", "R. West", "A. Atrash", "J. Villemure", "F. Routhier"], "venue": "International Journal of Intelligent Control and System, vol. 16, no. 2, pp. 124-131, 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "A helping hand: industrial robotics, knowledge and user-oriented services", "author": ["M. Stenmark", "J. Malec"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Object schemas for responsive robotic language use", "author": ["K. Hsiao", "S.Vosoughi", "S. Tellex", "R. Kubat", "D. Roy"], "venue": "Proceedings of the 3rd ACM/IEEE international conference on Human robot interaction, pp. 233-240, 2008.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "A Framework for End-User Instruction of a Robot Assistant for Manufacturing", "author": ["K. Guerin", "C. Lea", "C. Paxton"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 6167-6174, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "AIBO\u2019s first words. The social learning of language and meaning", "author": ["L. Steels", "F. Kaplan"], "venue": "Evolution of Communication, vol. 4, no. 1, pp. 3-32, 2000.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2000}, {"title": "Natural Language Command of an Autonomous Micro-Air Vehicle", "author": ["A. Huang", "S. Tellex", "A. Bachrach"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 2663-2669, 2010.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Robot navigation using human cues: a robot navigation system for symbolic goaldirected exploration", "author": ["R. Schulz", "B. Talbot", "O. Lam", "F. Dayoub", "P. Corke", "B. Upcroft", "G. Wyeth"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 1100-1105, 2015.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Grounding spatial relations for human-robot interaction", "author": ["S. Guadarrama", "L. Riano", "D. Golland", "D. Go \u0308hring", "Y. Jia", "D. Klein", "P. Abbeel", "T. Darrell"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1640-1647, 2013.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Storytelling with robots: Learning companions for preschool children\u2019s language development", "author": ["J. Kory", "C. Breazeal"], "venue": "IEEE International Symposium on Robot and Human Interactive Communication, pp. 643-648, 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Social interactions in HRI: The robot view", "author": ["C. Breazeal"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), vol. 34, no. 2, pp. 181-186, 2004.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2004}, {"title": "Supervised machine learning: A review of classification techniques", "author": ["S. Kotsiantis", "I. Zaharakis", "P. Pintelas"], "venue": "Informatica, vol. 31, no. 3, pp. 249-268, 2007.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey of clustering data mining techniques", "author": ["P. Berkhin"], "venue": "Grouping multidimensional data, vol. 43, no. 1, pp. 25-71, 2006.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2006}, {"title": "A survey on feature extraction for pattern recognition", "author": ["S. Ding", "H. Zhu", "W. Jia", "C. Su"], "venue": "Artificial Intelligence Review, vol. 37, no. 3, pp. 169-180, 2012.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "A Survey on Techniques in NLP", "author": ["N. Ranjan", "K. Mundada", "K. Phaltane"], "venue": "International Journal of Computer Applications, vol. 134, no. 8, pp.6-9, 2016.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}, {"title": "Jumping NLP curves: A review of natural language processing research", "author": ["E. Cambria", "B. White"], "venue": "IEEE Computational Intelligence Magazine, vol. 9, no. 2, pp. 48-57, 2014.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Progress in the application of natural language processing to information retrieval tasks", "author": ["A.F. Smeaton"], "venue": "Computer Journal, vol. 35, no. 3, pp. 268-278, 1992.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1992}, {"title": "The cognitive structure of emotions", "author": ["A. Ortony", "G. Clore", "A. Collins"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1988}, {"title": "The predictive value of transitional probability for word-boundary palatalization in English", "author": ["N. Bush"], "venue": "Unpublished M.S .thesis, Univ. New Mexico, Albuquerque, NM, 1999.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1999}, {"title": "AbstFinder, a prototype natural language text abstraction finder for use in requirements elicitation", "author": ["L. Goldin", "D.M. Berry"], "venue": "Automated Software Engineering, vol. 4, no. 4, pp. 375-412, 1997.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1997}, {"title": "A statistical natural language processor for medical reports", "author": ["R.K. Taira", "S.G. Soderland"], "venue": "Proceedings of the AMIA Symposium, pp. 970, 1999.  submitted to Knowledge-based Systems, January 2017", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1999}, {"title": "Semantic relevance and aspect dependency in a given subject domain: contents-driven algorithmic processing of fuzzy word meanings to form dynamic stereotype representations", "author": ["B.B. Rieger"], "venue": "Proceedings of the 10th International Conference on Computational Linguistics and 22nd annual meeting on Association for Computational Linguistics, pp. 298-301 1984.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1984}, {"title": "HMM-based passage models for document classification and ranking", "author": ["L. Denoyer", "H. Zaragoza", "P. Gallinari"], "venue": "European Colloquium on Information Retrieval Research, pp. 126-135, 2001.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2001}, {"title": "Unsupervised learning by probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "Machine Learning., vol. 42, no.1\u20132, pp. 177\u2013196, 2001.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2001}, {"title": "Ontology-based parser for natural language processing", "author": ["J.E. Busch", "A.D. Lin", "P.J. Graydon", "M. Caudill"], "venue": "U.S. Patent No. 7,027,974, 2006.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2006}, {"title": "Automatic ontology-based knowledge extraction from web documents", "author": ["H. Alani", "S. Kim", "D.E. Millard", "M.J. Weal", "W. Hall", "P.H. Lewis", "N.R. Shadbolt"], "venue": "IEEE Intelligent Systems, vol. 18, no. 1, pp. 14-21, 2003.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2003}, {"title": "Sentic computing: Techniques, tools, and applications", "author": ["E. Cambria", "A. Hussain"], "venue": null, "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2012}, {"title": "COGVIEW & INTELNET: Nuanced energy-based knowledge representation and integrated cognitive-conceptual framework for realistic culture, values, and concept-affected systems simulation", "author": ["D. Olsher"], "venue": "IEEE Symposium on Computational Intelligence for Human-like Intelligence (CIHLI), pp. 82\u201391, 2013.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}, {"title": "Story and discourse: A bipartite model of narrative generation in virtual worlds", "author": ["R. Young"], "venue": "Interaction Studies, vol. 8, pp. 177\u2013208, 2007.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2007}, {"title": "Modeling space and time in narratives about restaurants", "author": ["E. Mueller"], "venue": "Literary Linguistic Comput., vol. 22, no.1, pp. 67\u201384, 2007.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2007}, {"title": "Formalizing argumentative story-based analysis of evidence", "author": ["F. Bex", "H. Prakken", "B. Verheij"], "venue": "Proc. Int. Conf. Artificial Intelligence Law, pp. 1-10, 2007.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2007}, {"title": "Narrative is a key cognitive competency", "author": ["M.A. Finlayson", "P.H. Winston"], "venue": "Annual Meeting Biologically Inspired Cognitive Architectures (BICA), pp. 110, 2011.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2011}, {"title": "The new robotics\u2014towards human-centered machines", "author": ["S. Schaal"], "venue": "HFSP journal, vol. 1, no. 2, pp. 115-126, 2007.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey of socially interactive robots", "author": ["T.W. Fong", "I. Nourbakhsh", "K. Dautenhahn"], "venue": "Robotics and Autonomous Systems, vol. 42, no. 3-4, pp. 143-166, 2002.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2002}, {"title": "Introduction to this special issue on human-robot interaction, in Human-Computer Interaction", "author": ["S. Kiesler", "P. Hinds"], "venue": "vol. 19, no. 1-2, pp. 1-8, 2004.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2004}, {"title": "Human-robot collaboration: A survey", "author": ["A. Bauer", "D. Wollherr", "M. Buss"], "venue": "International Journal of Humanoid Robotics, vol. 5, no. 1, pp. 47-66, 2008.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2008}, {"title": "A novel approach to proactive human-robot cooperation", "author": ["O.C. Schrempf", "U.D. Hanebeck", "A.J. Schmid", "H. Worn"], "venue": "IEEE International Workshop on Robot and Human Interactive Communication, pp. 555-560, 2005.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2005}, {"title": "Effects of anticipatory action on human-robot teamwork efficiency, fluency, and perception of team", "author": ["G. Hoffman", "C. Breazeal"], "venue": "Proceedings of the ACM/IEEE international conference on Human-robot interaction, pp. 1-8, 2007.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2007}, {"title": "The shadow robot mimics human actions", "author": ["P. Tuffield", "H. Elias"], "venue": "Industrial Robot: An International Journal, vol. 30, no. 1, pp. 56-60, 2003.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2003}, {"title": "When humanoid robots become human-like interaction partners: corepresentation of robotic actions", "author": ["A. Stenzel", "E. Chinellato", "M.A.T. Bou", "A.P. del Pobil", "M. Lappe", "R. Liepelt"], "venue": "Journal of Experimental Psychology: Human Perception and Performance, vol. 38, no. 5, pp. 1073, 2012.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2012}, {"title": "An atlas of physical human\u2013robot interaction", "author": ["A. De Santis", "B. Siciliano", "A. De Luca", "A. Bicchi"], "venue": "Mechanism and Machine Theory, vol. 43, no. 3, pp. 253- 270, 2008.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2008}, {"title": "Safe planning for human\u2010robot interaction", "author": ["D. Kulic", "E.A. Croft"], "venue": "Journal of Robotic Systems, vol. 22, no. 7, pp. 383-396, 2005.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2005}, {"title": "Adapting robot behavior for human--robot interaction", "author": ["N. Mitsunaga", "C. Smith", "T. Kanda", "H. Ishiguro", "N. Hagita"], "venue": "IEEE Transactions on Robotics, vol. 24, no. 4, pp. 911-916, 2008.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2008}, {"title": "Is imitation learning the route to humanoid robots?", "author": ["S. Schaal"], "venue": "Trends in Cognitive Science,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 1999}, {"title": "Getting humanoids to move and imitate", "author": ["M. Mataric"], "venue": "IEEE Intelligent Systems, vol. 15, no. 4, pp. 18-24, 2000.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2000}, {"title": "The neuroscience of social interaction: Decoding, imitating, and influencing the actions of others", "author": ["C.D. Frith", "D.M. Wolpert"], "venue": "Journal of Consciousness Studies, vol. 119, no. 4, pp. 664-668, 2004.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2004}, {"title": "Natural human-robot interaction using speech, head pose and gestures", "author": ["R. Stiefelhagen", "C. Fugen", "R. Gieselmann", "H. Holzapfel", "K. Nickel", "A. Waibel"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), vol. 3, pp. 2422-2427, 2004.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2004}, {"title": "Physiological signals based human emotion recognition: a review", "author": ["S. Jerritta", "M. Murugappan", "R. Nagarajan", "K. Wan"], "venue": "IEEE International Colloquium on Signal Processing and its Applications (CSPA), pp. 410-415, 2011.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2011}, {"title": "Context-specific intention awareness through web query in robotic caregiving", "author": ["R. Liu", "X. Zhang", "J. Webb", "S. Li"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 1962-1967, 2015.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 1962}, {"title": "Context-specific grounding of web natural descriptions to human-centered situations", "author": ["R. Liu", "X. Zhang"], "venue": "Knowledge-Based Systems, vol. 111, pp. 1-16, 2016.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2016}, {"title": "The role of expressiveness and attention in human-robot interaction", "author": ["A. Burce", "I. Nourbakhsh", "R. Simmons"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), vol. 4, pp. 4138-4142, 2002.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2002}, {"title": "Investigating joint attention mechanisms through spoken human\u2013robot interaction", "author": ["M. Staudte", "M.W. Crocker"], "venue": "Cognition, vol. 120, no. 2, pp. 268-291, 2011.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2011}, {"title": "The influence of subjec ts' personality traits on personal spatial zones in a human-robot interaction experiment", "author": ["M.L. Walters", "K. Dautenhahn", "R. Te Boekhorst", "K.L. Koay", "C. Kaouri", "S. Woods", "C. Nehaniv", "D. Lee", "I. Werry"], "venue": "IEEE International Workshop on Robot and Human Interactive Communication, pp. 347-352, 2005.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2005}, {"title": "Influences on proxemic behaviors in human-robot interaction", "author": ["L. Takayama", "C. Pantofaru"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 5495-5502, 2009.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2009}, {"title": "An ethological and emotional basis for human\u2013robot interaction", "author": ["R.C. Arkin", "M. Fujita", "T. Takagi", "R. Hasegawa"], "venue": "Robotics and Autonomous Systems, vol. 42, no. 3, pp. 191-201, 2003.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2003}, {"title": "Human-inspired robots", "author": ["S. Coradeschi", "H. Ishiguro", "M. Asada", "S.C. Shapiro", "M. Thielscher", "C. Breazeal", "M.J. Mataric", "H. Ishida"], "venue": "IEEE Intelligent Systems, vol. 21, no. 4, pp. 74\u201385, 2006.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2006}, {"title": "NRI: Collaborative Research: Jointly Learning Language and Affordances [Online", "author": ["B. Selman"], "venue": null, "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2014}, {"title": "NRI: Collaborative Research: Modeling and Verification of Language-based Interaction [Online", "author": ["N. Roy"], "venue": null, "citeRegEx": "87", "shortCiteRegEx": "87", "year": 2014}, {"title": "Design and control of a variable stiffness actuator for safe and fast physical human/robot interaction", "author": ["G. Tonietti", "R. Schiavi", "A. Bicchi"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), 2015.", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2015}, {"title": "2013).[Online]. Available: http://rse-lab.cs.washington.edu/projects/language-grounding", "author": ["D. Fox"], "venue": null, "citeRegEx": "89", "shortCiteRegEx": "89", "year": 2013}, {"title": "Robotics and Sematic Systems(First Edition)", "author": ["K. Nilsson"], "venue": null, "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2016}, {"title": "The international journal of robotics research", "author": ["J. Hollerbach"], "venue": "SAGE. [Online]. Available: http://journals.sagepub.com/home/ijr. Accessed: Jan. 27, 2017.  submitted to Knowledge-based Systems, January 2017", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2017}, {"title": "IEEE Xplore: IEEE transactions on robotics", "author": ["F. Park"], "venue": "IEEE transactions on robotics. [Online]. Available: http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8860. Accessed: Jan. 27, 2017.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2017}, {"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and autonomous systems, vol. 57, no. 5, pp. 469-483, 2009.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2009}, {"title": "Vision based hand gesture recognition for human computer interaction: a survey", "author": ["S.S. Rautaray", "A. Agrawal"], "venue": "Artificial Intelligence Review, vol. 43, no. 1, pp. 1-54, 2015.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2015}, {"title": "Survey of psychophysiology measurements applied to human-robot interaction", "author": ["C. Bethel", "K. Salomon", "R. Murphy", "J. Burke"], "venue": "IEEE International Symposium on Robot and Human Interactive Communication, pp. 732-737, 2007.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey of tactile human\u2013robot interactions", "author": ["B.D. Argall", "A.G. Billard"], "venue": "Robotics and Autonomous Systems, vol. 58, no. 10, pp. 1159-1176, 2010.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2010}, {"title": "Natural Dialogue with the Jijo-2 Office Robot", "author": ["J. Fry", "H. Asoh", "T. Matsui"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), vol. 2, pp. 1278-1283, 1998.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 1998}, {"title": "Natural communication and interaction with humanoid robots", "author": ["R. Bischoff", "T. Jain"], "venue": "Second International Symposium on Humanoid Robots, pp. 121-128, 1999.", "citeRegEx": "103", "shortCiteRegEx": null, "year": 1999}, {"title": "A study of interaction between dialog and decision for human-robot collaborative task achievement", "author": ["A. Clodic", "R. Alami", "V. Montreuil", "S. Li"], "venue": "International Symposium on Robot and Human interactive Communication (RO-MAN), pp. 913-918, 2007.", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2007}, {"title": "Receptionist or information kiosk: How do people talk with a robot?", "author": ["M.K. Lee", "S. Kiesler", "J. Forlizzi"], "venue": "Proceedings of the 2010 ACM conference on Computer supported cooperative work (CSCW),", "citeRegEx": "105", "shortCiteRegEx": "105", "year": 2010}, {"title": "Designing a human-robot interaction framework for home service robot", "author": ["K.W. Lee", "H.R. Kim", "C.Y. Wan", "Y.S. Yoon"], "venue": "International Workshop on Robot and Human interactive Communication (RO-MAN), pp. 286-293. 2005.", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2005}, {"title": "Affective effects of speech-enabled robots for language learning", "author": ["S. Lee", "C. Kim", "J. Lee", "H. Noh"], "venue": "IEEE Spoken Language Technology Workshop (SLT), pp. 145-150, 2010.", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2010}, {"title": "A spoken dialogue system to control robots", "author": ["H. Motallebipour", "A. Bering"], "venue": "Technology and Engineering, 2002.", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2002}, {"title": "Towards an intuitive interface for instructing robots handling tasks based on verbalized physical effects.", "author": ["M. Spangenberg", "D. Henrich"], "venue": "In Robot and Human Interactive Communication,", "citeRegEx": "109", "shortCiteRegEx": "109", "year": 2014}, {"title": "The VoiceBot: A voice controlled robot arm", "author": ["B. House", "J. Malkin", "J. Bilmes"], "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 183-192, 2009.", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2009}, {"title": "Tell me Dave: Context-sensitive grounding of natural language to manipulation instructions", "author": ["D.K. Misra", "J. Sung", "K. Lee", "A. Saxena"], "venue": "Robotics: Science and Systems, vol. 35, no. 1-3, pp. 281-300, 2014.", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2014}, {"title": "Posture control of robot manipulators with fuzzy voice commands using a fuzzy coach\u2013player system", "author": ["C. Jayawardena", "K. Watanabe", "K. Izumi"], "venue": "Advanced Robotics, vol. 21, nos. 3-4, pp. 293-328, 2007.", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2007}, {"title": "Progress in programming the hrp-2 humanoid using spoken language", "author": ["P.F. Dominey", "A. Mallet", "E. Yoshida"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 2169-2174, 2007.", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2007}, {"title": "The motion grammar: Analysis of a linguistic method for robot control", "author": ["N. Dantam", "M. Stillman"], "venue": "IEEE Transactions on Robotics, vol. 29, no. 3, pp. 704- 718, 2013.", "citeRegEx": "114", "shortCiteRegEx": null, "year": 2013}, {"title": "Integrating verbal and nonverbal communication in a dynamic neural field architecture for human\u2013robot interaction", "author": ["E. Bicho", "L. Louro", "W. Erlhagen"], "venue": "Frontiers in Neurorobotics, vol. 4, no. 5, pp. 1-13, 2010.", "citeRegEx": "115", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-modal human-machine communication for instructing robot grasping tasks", "author": ["P. Mcguire", "J. Fritsch", "J.J. Steil", "F. Rothling"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), vol. 2, pp. 1082-1088, 2005.", "citeRegEx": "116", "shortCiteRegEx": null, "year": 2005}, {"title": "Toward human-aware robot task planning", "author": ["R. Alami", "A. Clodic", "V. Montreuil", "E.A. Sisbot", "R. Chatila"], "venue": "AAAI Spring Symposium, pp. 39-46, 2006.", "citeRegEx": "117", "shortCiteRegEx": null, "year": 2006}, {"title": "An Integrated robotic system for spatial understanding and situated interaction in indoor environments", "author": ["H. Zender", "P. Jensfelt", "O.M. Mozos", "G.M. Kruijff", "W. Burgard"], "venue": "Proceedings of the 22nd national conference on Artificial intelligence, vol. 7, pp. 1584-1589, 2007.", "citeRegEx": "118", "shortCiteRegEx": null, "year": 2007}, {"title": "Toward humanlike task-based dialogue processing for human robot interaction", "author": ["M. Scheutz", "R. Cantrell", "P.W. Schermerhorn"], "venue": "Ai Magazine, vol. 32, no. 4, pp. 77-84, 2011.", "citeRegEx": "119", "shortCiteRegEx": null, "year": 2011}, {"title": "Intuitive industrial robot programming through incremental multimodal language and augmented reality", "author": ["B. Akan", "A. Ameri", "B. Curuklu", "L. Asplund"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 3934-3939, 2011.", "citeRegEx": "120", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning environmental knowledge from task-based human-robot dialog", "author": ["T. Kollar", "V. Perara", "D. Nardi", "M. Veloso"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 4304-4309, 2013.", "citeRegEx": "121", "shortCiteRegEx": null, "year": 2013}, {"title": "Using semantic fields to model dynamic spatial relations in a robot architecture for natural language instruction of service robots", "author": ["J. Fasola", "M.J. Mataric"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 143-150, 2013.", "citeRegEx": "122", "shortCiteRegEx": null, "year": 2013}, {"title": "Understanding user instructions by utilizing open knowledge for service robots", "author": ["D. Lu", "F. Wu", "X. Chen"], "venue": "arXiv: 1606.02877v1, 2016.", "citeRegEx": "123", "shortCiteRegEx": null, "year": 2016}, {"title": "Mediating between qualitative and quantitative representations for task-orientated human-robot interaction", "author": ["M. Brenner", "N. Hawes", "J.D. Kelleher", "J.L. Wyatt"], "venue": "International Joint Conference on Artificial Intelligence (IJCAI), pp. 2072-2077, 2007.", "citeRegEx": "124", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning Actions from Human-Robot Dialogues", "author": ["R. Cantrell", "P. Schermerhorn", "M. Scheutz"], "venue": "IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), pp. 125-130, 2011.", "citeRegEx": "125", "shortCiteRegEx": null, "year": 2011}, {"title": "A two-arm situated artificial communicator for human\u2013robot cooperative assembly", "author": ["J. Zhang", "A. Knoll"], "venue": "IEEE Transactions on Industrial Electronics, vol. 50, no. 4, pp. 651-658, 2003.", "citeRegEx": "126", "shortCiteRegEx": null, "year": 2003}, {"title": "The peer-to-peer human-robot interaction project", "author": ["T. Fong", "I. Nourbakhsh", "C. Kunz", "L. Fluckiger", "J. Schreiner", "R. Ambrose", "R. Burridge"], "venue": "AAAI Space Forum, pp. 6750, 2005.", "citeRegEx": "127", "shortCiteRegEx": null, "year": 2005}, {"title": "Using dialog and human observations to dictate tasks to a learning robot assistant", "author": ["P.E. Rybski", "J. Stolarz", "K. Yoon", "M. Veloso"], "venue": "Intelligent Service Robotics, vol. 1, no. 2, pp. 159-167, 2008.", "citeRegEx": "128", "shortCiteRegEx": null, "year": 2008}, {"title": "Generation of collaborative spoken dialogue contributions in dynamic task", "author": ["O. Lemon", "A. Gruenstein", "R. Gullett", "A. Battle", "L. Hiatt", "S. Peters"], "venue": "AAAI Spring Symposium, 2003.  submitted to Knowledge-based Systems, January 2017", "citeRegEx": "129", "shortCiteRegEx": null, "year": 2003}, {"title": "Natural language service for controlling robots and other agents", "author": ["J. Allen", "Q. Duong", "C. Thompson"], "venue": "IEEE Integration of Knowledge Intensive Multi- Agent Systems (KIMAS-05), pp. 592-595, 2005.", "citeRegEx": "130", "shortCiteRegEx": null, "year": 2005}, {"title": "Social and collaborative aspects of interaction with a service robot", "author": ["K. Severinson-Eklundh", "A. Green", "H. Huttenrauch"], "venue": "Robotics and Autonomous Systems, vol. 42, nos. 3-4, pp. 223-234, 2003.", "citeRegEx": "131", "shortCiteRegEx": null, "year": 2003}, {"title": "Robot programming by demonstration with situated spatial language understanding", "author": ["M. Forbes", "R.P.N. Rao", "L. Zettlemoyer", "M. Cakmak"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 2014-2020, 2015.", "citeRegEx": "132", "shortCiteRegEx": null, "year": 2014}, {"title": "Imitation learning for natural language direction following through unknown environments", "author": ["F. Duvallet", "T. Kollar", "A. Stentz"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 1047-1053, 2013.", "citeRegEx": "133", "shortCiteRegEx": null, "year": 2013}, {"title": "Human-robot cooperation using quasi-symbols generated by RNNPB model", "author": ["T. Ogata", "S. Matsumoto", "J. Tani", "K. Komatani"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 2156-2161, 2007.", "citeRegEx": "134", "shortCiteRegEx": null, "year": 2007}, {"title": "RoboFrameNet: Verb-centric semantics for actions in robot middleware", "author": ["B.J. Thomas", "O.C. Jenkins"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 4750-4755, 2012.", "citeRegEx": "135", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning social affordances and using them for planning", "author": ["K.F. Uyanik", "Y. Caliskan", "A.K. Bozcuoglu", "O. Yuruten", "S. Kalkan", "E. Sahin"], "venue": "35th Annual Meeting of the Cognitive Science Society (CogSci), 2013.", "citeRegEx": "136", "shortCiteRegEx": null, "year": 2013}, {"title": "Toward understanding natural language directions", "author": ["T. Kollar", "S. Tellex", "D. Roy", "N. Roy"], "venue": "ACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 259-266, 2010.", "citeRegEx": "137", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-purpose natural language understanding linked to sensorimotor experience in humanoid robots", "author": ["E. Ovchinnikova", "M. Wachter", "V. Wittenbeck", "T. Asfour"], "venue": "IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids), pp. 365-372, 2015.", "citeRegEx": "138", "shortCiteRegEx": null, "year": 2015}, {"title": "Human-robot dialogue for joint construction tasks", "author": ["M.E. Foster", "T. By", "M. Rickert", "A. Knoll"], "venue": "Proceedings of the 8th international conference on Multimodal interfaces (ICMI), pp. 68-71, 2006.", "citeRegEx": "139", "shortCiteRegEx": null, "year": 2006}, {"title": "Dependable multimodal communication and interaction with robotic assistants", "author": ["R. Bischoff", "V. Graefe"], "venue": "IEEE International Workshop on Robot and Human Interactive Communication (RO-MAN), pp. 300-305, 2002.", "citeRegEx": "140", "shortCiteRegEx": null, "year": 2002}, {"title": "Multi-Modal human robot interaction for map generation", "author": ["S.S. Ghidary", "Y. Nakata", "H. Saito", "M. Hattori", "T. Takamori"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), vol. 4, pp. 2246-2251, 2001.", "citeRegEx": "141", "shortCiteRegEx": null, "year": 2001}, {"title": "Recognition of affective communicative intent in robot-directed speech", "author": ["C. Breazeal", "L. Aryananda"], "venue": "Autonomous Robots, vol. 12, no. 1, pp. 83-104, 2002.", "citeRegEx": "143", "shortCiteRegEx": null, "year": 2002}, {"title": "An extensible language interface for robot manipulation", "author": ["J. Connell", "E. Marcheret", "S. Pankanti", "M. Kudoh", "R. Nishiyama"], "venue": "5th International Conference on Artificial General Intelligence (AGI), pp. 21-30, 2012.", "citeRegEx": "144", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatic language acquisition by an autonomous robot", "author": ["S. Levinson", "W. Zhu", "D. Li", "K. Squire", "R. Lin", "M. Kleffner", "M. McClain", "J. Lee"], "venue": "Proceedings of the International Joint Conference on Neural Networks, vol. 4, pp. 2716-2721, 2003.", "citeRegEx": "145", "shortCiteRegEx": null, "year": 2003}, {"title": "An active audition framework for auditory-driven HRI: Application to interactive robot dancing", "author": ["J.L. Oliveira", "G. Ince", "K. Nakamura", "K. Nakadai", "H.G. Okuno", "L.P. Reis", "F. Gouyon"], "venue": "IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), pp. 1078-1085, 2012.", "citeRegEx": "146", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to follow navigational route instructions", "author": ["N. Shimizu", "A.R. Haas"], "venue": "Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI), vol. 9, pp. 1488-1493, 2009.", "citeRegEx": "147", "shortCiteRegEx": null, "year": 2009}, {"title": "Language-Based Sensing Descriptors for Robot Object Grounding", "author": ["G. Gemignani", "M. Veloso", "D. Nardi"], "venue": "Robot Soccer World Cup, pp. 3-15, 2015.", "citeRegEx": "148", "shortCiteRegEx": null, "year": 2015}, {"title": "Applying automated deduction to natural language understanding", "author": ["J. Bos"], "venue": "Journal of Applied Logic, vol. 7, no. 1, pp. 100-112, 2009.", "citeRegEx": "149", "shortCiteRegEx": null, "year": 2009}, {"title": "learning limited dependence bayesian classifiers", "author": ["M. Sahami"], "venue": "KDD, vol. 96, pp. 335-338, 1996.", "citeRegEx": "150", "shortCiteRegEx": null, "year": 1996}, {"title": "Current Opinion in structural biology", "author": ["S. Eddy", "Hidden Markov Models"], "venue": "vol. 6, no.3, pp. 361-365, 1996.", "citeRegEx": "151", "shortCiteRegEx": null, "year": 1996}, {"title": "Markov random field texture models", "author": ["G. Cross", "A. Jain"], "venue": "IEEE Transactions on Pattern Analysis and machine intelligence, vol. 1, pp. 25-39, 1983.", "citeRegEx": "152", "shortCiteRegEx": null, "year": 1983}, {"title": "Least squares support vector machine classifiers", "author": ["J. Suykens", "J. Vandewalle"], "venue": "Neural processing letters, vol. 9, no. 3, pp. 293-300, 1999.", "citeRegEx": "153", "shortCiteRegEx": null, "year": 1999}, {"title": "conditional random fields: probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "Proceedings of the eighteenth international conference on machine learning, vol. 1, pp. 282-289, 2001.", "citeRegEx": "154", "shortCiteRegEx": null, "year": 2001}, {"title": "Language bootstrapping: Learning word meanings from perception\u2013action association", "author": ["G. Salvi", "L. Montesano", "A. Bernardino", "J. Santos-Victor"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 42, no. 3, pp. 660-671, 2011.", "citeRegEx": "155", "shortCiteRegEx": null, "year": 2011}, {"title": "Towards quantitative modeling of task confirmations in human-robot dialog", "author": ["J. Sattar", "G. Dudek"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 1957-1963, 2011.", "citeRegEx": "156", "shortCiteRegEx": null, "year": 1957}, {"title": "A probabilistic approach to learning a visually grounded language model through human-robot interaction", "author": ["H. Dindo", "D. Zambuto"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 790-796, 2010.", "citeRegEx": "157", "shortCiteRegEx": null, "year": 2010}, {"title": "Grounding words in perception and action: computational insights", "author": ["D. Roy"], "venue": "Trends in Cognitive Sciences, vol. 9, no. 8, pp. 389-396, 2005.", "citeRegEx": "158", "shortCiteRegEx": null, "year": 2005}, {"title": "Grounding knowledge in sensors: Unsupervised learning for language and planning", "author": ["J.T. Oates"], "venue": "(2001).", "citeRegEx": "159", "shortCiteRegEx": null, "year": 2001}, {"title": "Affordance based word-to-meaning association", "author": ["V. Krunic", "G. Salvi", "A. Bernardino", "L. Montesano", "J.Santos-Victor"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 4138-4143, 2009.", "citeRegEx": "160", "shortCiteRegEx": null, "year": 2009}, {"title": "Clarifying commands with information-theoretic human-robot dialog", "author": ["R. Deits", "S. Tellex", "P. Thaker", "D. Simeonov", "T. Kolloar", "N. Roy"], "venue": "Journal of Human- Robot Interaction, vol. 1, no. 1, pp. 78-95, 2012.", "citeRegEx": "161", "shortCiteRegEx": null, "year": 2012}, {"title": "A joint model of language and perception for grounded attribute learning.", "author": ["C. Matuszek", "N. FitzGerald", "L. Zettlemoyer", "L. Bo", "D. Fox"], "venue": null, "citeRegEx": "162", "shortCiteRegEx": "162", "year": 2012}, {"title": "Fuzzy naive bayesian classification in robosoccer 3d: A hybrid approach to decision making", "author": ["C. Bustamante", "L. Garrido", "S. Rogelio"], "venue": "Robot Soccer World Cup Springer Berlin Heidelberg, pp. 507-515, 2006.", "citeRegEx": "163", "shortCiteRegEx": null, "year": 2006}, {"title": "Intelligent human-machine interaction based on dynamic bayesian networks probabilistic intention recognition", "author": ["K. Tahboub"], "venue": "Journal of Intelligent & Robotic Systems, vol. 45, no. 1, pp. 31-52, 2006.", "citeRegEx": "165", "shortCiteRegEx": null, "year": 2006}, {"title": "Two-handed gesture recognition and fusion with speech to command a robot", "author": ["B. Burger", "I. Ferrane", "F. Lerasle", "G. Infantes"], "venue": "Autonomous Robots, vol. 32, no. 2, pp. 129-147, 2012.", "citeRegEx": "166", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning motion primitives and annotative texts from crowd-sourcing", "author": ["W. Takano"], "venue": "ROBOMECH Journal, vol. 2, no. 1, pp. 1-9, 2015.", "citeRegEx": "167", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust speech understanding for robot telecontrol", "author": ["G. Antoniol", "R. Cattoni", "M. Cettolo", "M. Federico"], "venue": "Proceedings of the 6th International Conference on Advanced robotics, pp. 205-209, 1993.", "citeRegEx": "168", "shortCiteRegEx": null, "year": 1993}, {"title": "Spoken language interaction with model uncertainty: an adaptive human\u2013robot interaction system", "author": ["F. Doshi", "N. Roy"], "venue": "Connection Science, vol. 20, no. 4, pp. 299-318, 2008.  submitted to Knowledge-based Systems, January 2017", "citeRegEx": "169", "shortCiteRegEx": null, "year": 2008}, {"title": "Bigram-based natural language model and statistical motion symbol model for scalable language of humanoid robots", "author": ["W. Takano", "Y. Nakamura"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 1232-1237, 2012.", "citeRegEx": "170", "shortCiteRegEx": null, "year": 2012}, {"title": "An extensible architecture for robust multimodal human-robot communication.", "author": ["S. Rossi", "E. Leone", "M. Fiore", "A. Finzi", "F. Cutugno"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems", "citeRegEx": "171", "shortCiteRegEx": "171", "year": 2013}, {"title": "A hybrid discriminative/generative approach for modeling human activities", "author": ["J. Lester", "T. Choudhury", "N. Kern", "G. Borriello", "B. Hannaford"], "venue": "pp. 766-722, 2005.", "citeRegEx": "172", "shortCiteRegEx": null, "year": 2005}, {"title": "Bayesian networks based multi-modality fusion for error handling in human\u2013robot dialogues under noisy conditions", "author": ["P. Prodanov", "A. Drygajlo"], "venue": "Speech Communication, vol. 45, no. 3, pp. 231-248, 2005.", "citeRegEx": "173", "shortCiteRegEx": null, "year": 2005}, {"title": "A discriminative model for understanding natural language route directions", "author": ["T. Kollar", "A. Stefanie", "N. Roy"], "venue": "American Association for Artificial Intelligence, 2010.", "citeRegEx": "174", "shortCiteRegEx": null, "year": 2010}, {"title": "Automatic prosody prediction and detection with Conditional Random Field (CRF) models", "author": ["Y. Qian", "Z. Wu", "X. Ma", "F. Soong"], "venue": "the 7th International Symposium on. Chinese Spoken Language Processing (ISCSLP), pp. 135-138, 2010.", "citeRegEx": "175", "shortCiteRegEx": null, "year": 2010}, {"title": "Talking to Godot: Dialogue with a mobile robot", "author": ["C. Theobalt", "J. Bos", "T. Chapman", "A. Espinosa-Romero", "M. Fraser", "G. Hayes"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems, vol. 2, pp. 1338-1348, 2002.", "citeRegEx": "176", "shortCiteRegEx": null, "year": 2002}, {"title": "A framework for learning semantic maps from grounded natural language descriptions", "author": ["MR. Walter", "S. Hemachandra", "B. Homberg", "S. Tellex", "S. Teller"], "venue": "The International Journal of Robotics Research, vol. 33.9, pp. 1167-1190, 2009.", "citeRegEx": "177", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning perceptually grounded word meanings from unaligned parallel data", "author": ["S. Tellex", "P. Thaker", "J. Joseph", "N. Roy"], "venue": "Machine Learning, vol. 94, no. 2, pp. 151-167, 2014.", "citeRegEx": "178", "shortCiteRegEx": null, "year": 2014}, {"title": "Anticipatory robot control for efficient human-robot collaboration", "author": ["C.M. Huang", "B. Mutlu"], "venue": "11th ACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 83-90, 2016.", "citeRegEx": "179", "shortCiteRegEx": null, "year": 2016}, {"title": "Enhanced visual scene understanding through human-robot dialog", "author": ["M. Johnson-Roberson", "J. Bohg", "G. Skantze", "J. Gustafson", "R. Carlson", "B. Rasolzadeh", "D. Kragic"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 3342-3348, 2011.", "citeRegEx": "180", "shortCiteRegEx": null, "year": 2011}, {"title": "Grounded situation models for robots: Bridging language, perception, and action", "author": ["N. Mavridis", "D. Roy"], "venue": "AAAI Workshop on Modular Construction of Human-like Intelligence, 2005.", "citeRegEx": "181", "shortCiteRegEx": null, "year": 2005}, {"title": "Interacting with robots via speech and gestures, an integrated architecture.", "author": ["F. Cutugno", "A. Finzi", "M. Fiore", "E. Leone", "S. Rossi"], "venue": null, "citeRegEx": "182", "shortCiteRegEx": "182", "year": 2013}, {"title": "Robot learning simultaneously a task and how to interpret human instructions", "author": ["J. Grizou", "M. Lopes", "P. Oudeyer"], "venue": "IEEE Third Joint International Conference on Development and Learning and Epigenetic Robotics (ICDL), pp. 1-8, 2013.", "citeRegEx": "183", "shortCiteRegEx": null, "year": 2013}, {"title": "Temporal logic motion planning for mobile robots", "author": ["G.E. Fainekos", "H. Kress-Gazit", "G.J. Pappas"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 2020-2025, 2005.", "citeRegEx": "184", "shortCiteRegEx": null, "year": 2020}, {"title": "Intuitive instruction of industrial robots: Semantic process descriptions for small lot production", "author": ["A. Perzylo", "N. Somani", "S. Profanter", "I. Kessler", "M. Rickert", "A. Knoll"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 2293-2300, 2016.", "citeRegEx": "185", "shortCiteRegEx": null, "year": 2016}, {"title": "An action ontology framework for natural language interfaces to agent systems", "author": ["C. Kemke"], "venue": "Artificial Intelligence Review, Springer, 2007.", "citeRegEx": "186", "shortCiteRegEx": null, "year": 2007}, {"title": "Bridging the gap between discrete symbolic planning and optimization-based robot control", "author": ["E. Scioni", "G. Borghesan", "H. Bruyninckx", "M. Bonfe"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 5075-5081, 2015.", "citeRegEx": "187", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic representation for navigation in large-scale environments", "author": ["R. Drouilly", "P Rives", "B. Morisset"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 1106-1111, 2015.", "citeRegEx": "188", "shortCiteRegEx": null, "year": 2015}, {"title": "Situated Language Understanding with Human-like and Visualization-Based Transparency", "author": ["L. Perlmutter", "E. Kernfeld", "M. Cakmak"], "venue": "Robotics: Science and Systems, DOI: 10.15607/RSS.2016.XII.040, 2016.", "citeRegEx": "189", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards Learning Efficient Models for Natural Language Understanding of Quantifiable Spatial Relationships", "author": ["J. Arkin", "T.M. Howard"], "venue": "Rss Workshop on Model Learning for Human-Robot Communication, 2015.", "citeRegEx": "190", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating machine-executable plans from end-user's natural-language instructions.", "author": ["Liu", "Rui", "Xiaoli Zhang"], "venue": "arXiv preprint arXiv:1611.06468,", "citeRegEx": "191", "shortCiteRegEx": "191", "year": 2016}, {"title": "Fast semantic segmentation of 3D point clouds using a dense CRF with learned parameters", "author": ["D. Wolf", "J. Prankl", "M. Vincze"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 4867-4873, 2015.", "citeRegEx": "192", "shortCiteRegEx": null, "year": 2015}, {"title": "3D semantic interpretation for robot perception inside office environments", "author": ["L. Tamas", "L. Cosmin Goron"], "venue": "Engineering Applications of Artificial Intelligence vol. 32, pp. 76-87, 2014.", "citeRegEx": "193", "shortCiteRegEx": null, "year": 2014}, {"title": "A spoken language interface with a mobile robot", "author": ["J. Bos", "T. Oka"], "venue": "Artificial Life and Robotics, vol. 11, no. 1, pp.42-47, 2017.", "citeRegEx": "194", "shortCiteRegEx": null, "year": 2017}, {"title": "Equipping robot control programs with first-order probabilistic reasoning capabilities", "author": ["D. Jain", "L. Mosenlechner", "M. Beetz"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 3626-3631, 2009.", "citeRegEx": "195", "shortCiteRegEx": null, "year": 2009}, {"title": "What to do and how to do it: Translating natural language directives into temporal and dynamic logic representation for goal management and action execution", "author": ["J. Dzifcak", "M. Scheutz", "C. Baral", "P. Schermerhorn"], "venue": "IEEE International Conference on Robotics and Automation, pp. 4163-4168, 2009.", "citeRegEx": "196", "shortCiteRegEx": null, "year": 2009}, {"title": "LTLMoP: Experimenting with language, temporal logic and robot control", "author": ["C. Finucane", "G. Jing", "H. Kress-Gazit"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1988-1993, 2010.", "citeRegEx": "197", "shortCiteRegEx": null, "year": 1988}, {"title": "Modelling and analysis of natural language controlled robotic systems", "author": ["Y. Cheng", "Y. Jia", "R. Fang", "L. She", "N. Xi", "J. Chai"], "venue": "IFAC Proceedings Volumes, vol. 47, no. 3, pp. 11767-11772, 2014.", "citeRegEx": "198", "shortCiteRegEx": null, "year": 2014}, {"title": "Natural language human-robot interface using evolvable fuzzy neural networks for mobile technology", "author": ["W. Kacalak", "M. Majewski"], "venue": "International Conference on Intelligent Computing, pp. 480-489, 2009.", "citeRegEx": "199", "shortCiteRegEx": null, "year": 2009}, {"title": "Fuzzy emotion recognition in natural speech dialogue", "author": ["A. Austermann", "N. Esau", "L. Kleinjohann", "B. Kleinjohann"], "venue": "International Workshop on Robot and Human interactive Communication (RO-MAN), pp. 317-322, 2005.", "citeRegEx": "200", "shortCiteRegEx": null, "year": 2005}, {"title": "Knowledge processing for autonomous robots.", "author": ["M. Tenorth"], "venue": "Diss. Universita\u0308t Mu\u0308nchen,", "citeRegEx": "201", "shortCiteRegEx": "201", "year": 2011}, {"title": "Vierhuff. \"Towards an autonomous wheelchair: Cognitive aspects in service robotics.", "author": ["C. Mandel", "K. Huebner"], "venue": "Proceedings of Towards Autonomous Robotic Systems (TAROS),", "citeRegEx": "202", "shortCiteRegEx": "202", "year": 2005}, {"title": "Mobile robot programming using natural language", "author": ["S. Lauria", "G. Bugmann", "T. Kyriacou", "E. Klein"], "venue": "Robotics and Autonomous Systems, vol. 38, no. 3-4, pp. 171-181, 2002.", "citeRegEx": "203", "shortCiteRegEx": null, "year": 2002}, {"title": "Grounding robot sensory and symbolic information using the semantic web", "author": ["C. Stanton", "M. Williams"], "venue": "Robot Soccer World Cup, pp. 757-764, 2003.", "citeRegEx": "204", "shortCiteRegEx": null, "year": 2003}, {"title": "Towards Grounded Human-Robot Communication", "author": ["L. Lopes", "Q. Wang"], "venue": "IEEE International Workshop on Robot and Human Interactive Communication, pp. 312-318, 2002.", "citeRegEx": "205", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning spatial-semantic representations from natural language descriptions and scene classifications", "author": ["S. Hemachandra", "M.R. Walter", "S. Tellex", "S. Teller"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 2623-2630, 2015.", "citeRegEx": "206", "shortCiteRegEx": null, "year": 2015}, {"title": "Shared grounding of event descriptions by autonomous robots", "author": ["L. Steels", "J. Baillie"], "venue": "Robotics and Autonomous Systems, vol. 43, no.2, pp. 163-173, 2003.  submitted to Knowledge-based Systems, January 2017", "citeRegEx": "207", "shortCiteRegEx": null, "year": 2003}, {"title": "BIRON, where are you? Enabling a robot to learn new places in a real home environment by integrating spoken dialog and visual localization", "author": ["T. Spexard", "S. Li", "B. Wrede", "J. Fritsch", "G. Sagerer", "O. Booij"], "venue": "IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS),", "citeRegEx": "208", "shortCiteRegEx": "208", "year": 2006}, {"title": "Grounding verbs of motion in natural language commands to robots", "author": ["T. Kollar", "S. Tellex", "D. Roy", "N. Roy"], "venue": "12th International Symposium on Experimental Robotics, pp. 31-47, 2014.", "citeRegEx": "209", "shortCiteRegEx": null, "year": 2014}, {"title": "Grounding spatial relations for outdoor robot navigation", "author": ["A. Boularias", "F. Duvallet", "J. Oh", "A. Stentz"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 1976-1982, 2015.", "citeRegEx": "210", "shortCiteRegEx": null, "year": 1976}, {"title": "Tell me when and why to do it! Run-time planner model updates via natural language instruction.", "author": ["R. Cantrell", "J. Benton", "K. Talamadupula", "S. Kambhampati", "P. Schermerhorn", "M. Scheutz"], "venue": "ACM/IEEE International Conference on Human-Robot Interaction (HRI),", "citeRegEx": "211", "shortCiteRegEx": "211", "year": 2012}, {"title": "Robot self-initiative and personalization by learning through repeated interactions", "author": ["M. Mason", "M. Lopes"], "venue": "ACM/IEEE International Conference on Human- Robot Interaction (HRI), pp. 433-440, 2011.", "citeRegEx": "212", "shortCiteRegEx": null, "year": 2011}, {"title": "Using natural language generation in automatic route", "author": ["R. Dale", "S. Geldof", "J. Prost"], "venue": "Journal of Research and Practice in Information Technology, vol. 37, no. 1, pp. 89, 2005.", "citeRegEx": "213", "shortCiteRegEx": null, "year": 2005}, {"title": "Toward open knowledge enabling for Human-Robot interaction", "author": ["X. Chen", "J. Xie", "J. Ji", "Z. Sui"], "venue": "Journal of Human-Robot Interaction, vol. 1, no. 2, pp. 100- 117, 2012.", "citeRegEx": "214", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised Learning of Multi-Hypothesized Pick-and-Place Task Templates via Crowdsourcing", "author": ["R. Toris", "D. Kent", "S. Chernova"], "venue": "IEEE Conference on Robotics and Automation (ICRA), pp. 4504-4510, 2015.", "citeRegEx": "215", "shortCiteRegEx": null, "year": 2015}, {"title": "Web-enabled Robots", "author": ["M. Tenorth", "U. Klank", "D. Pangercic", "M. Beetz"], "venue": "IEEE Robotics & Automation Magazine, vol. 18, no. 2, pp. 58-68, 2011.", "citeRegEx": "216", "shortCiteRegEx": null, "year": 2011}, {"title": "Structural bootstrapping\u2014A novel generative mechanism for faster and more efficient acquisition of action-knowledge", "author": ["F. Worgotter", "C. Geib", "M. Tamosiunaite", "E.E. Aksoy", "J. Piater", "H. Xiong", "A. Ude", "B. Nemec", "D. Kraft", "N. Kruger", "M. Wachter", "T. Asfour"], "venue": "IEEE Trans. Autonom. Mental Develop.,", "citeRegEx": "217", "shortCiteRegEx": "217", "year": 2015}, {"title": "Robot learning from verbal interaction: A brief survey", "author": ["H. Cuayahuitl"], "venue": "4th International Symposium on New Frontiers in Human-Robot Interaction, 2015.", "citeRegEx": "219", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling of movement control architectures based on motion primitives using domain-specific language", "author": ["A. Nordmann", "S. Wrede", "J. Steil"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 5032-5039, 2015.", "citeRegEx": "220", "shortCiteRegEx": null, "year": 2015}, {"title": "Structural descriptions in human-assisted robot visual learning", "author": ["G.M. Kruijff", "J.D. Kelleher", "G. Berginc", "A. Leonardis"], "venue": "Proceedings of the 1st ACM SIGCHI/SIGART conference on Human-robot interaction (HRI), pp. 343-344, 2006.", "citeRegEx": "221", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning to interpret spatial natural language in terms of qualitative spatial relations", "author": ["P. Kordjamshidi", "J. Hois", "M.V. Otterlo", "M. Moens"], "venue": "Representing space in cognition: interrelations of behavior, language, and formal models, 2013.", "citeRegEx": "222", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning object-manipulation verbs for human-robot communication", "author": ["K. Sugiura", "N. Iwahashi"], "venue": "Proceedings ofworkshop on Multimodal Interfaces in Semantic Interaction (WMISI), pp. 32-38, 2007.", "citeRegEx": "223", "shortCiteRegEx": null, "year": 2007}, {"title": "An experience-driven robotic assistant acquiring human knowledge to improve haptic cooperation", "author": ["J.R. Medina", "M. Lawitzky", "A. Mortl", "D. Lee"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 2416-2422, 2011", "citeRegEx": "224", "shortCiteRegEx": null, "year": 2011}, {"title": "Tutelage and socially guided robot learning", "author": ["A. Lockerd", "C. Breazeal"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), vol. 4, pp. 3475-3480, 2004.", "citeRegEx": "225", "shortCiteRegEx": null, "year": 2004}, {"title": "Toward sociable robots", "author": ["C. Breazeal"], "venue": "Robotics and Autonomous Systems, vol. 42, no. 3, pp. 167-175, 2003.", "citeRegEx": "226", "shortCiteRegEx": null, "year": 2003}, {"title": "A storytelling robot: Modeling and evaluation of human-like gaze behavior", "author": ["B. Mutlu", "J. Forlizzi", "J. Hodgins"], "venue": "6th IEEE-RAS International Conference on Humanoid Robots, pp. 518-523, 2006.", "citeRegEx": "227", "shortCiteRegEx": null, "year": 2006}, {"title": "An emotional storyteller robot", "author": ["A. Chella", "R.E. Barone", "G. Pilato", "R. Sorbello"], "venue": "AAAI Spring Symposium on Emotion, Personality, and Social Behavior, pp. 17-22, 2008", "citeRegEx": "228", "shortCiteRegEx": null, "year": 2008}, {"title": "Reinforcement learning improves behaviour from evaluative feedback", "author": ["M.L. Littman"], "venue": "Nature, vol. 521, no. 7553, pp. 445-451, 2015.", "citeRegEx": "229", "shortCiteRegEx": null, "year": 2015}, {"title": "Machine-learning-assisted materials discovery using failed experiments", "author": ["P. Raccuglia", "K.C. Elbert", "P.D.F. Adler", "C. Falk", "M.B. Wenny", "A. Mollo", "M. Zeller", "S.A. Friedler", "J. Schrier", "A.J. Norquist"], "venue": "Nature, vol. 533, no. 7601, pp.73-76, 2016.", "citeRegEx": "230", "shortCiteRegEx": null, "year": 2016}, {"title": "Online motion planning for failure recovery of modular robotic systems", "author": ["V. Vonasek", "S. Neumann", "D. Oertel", "H. Worn"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pp. 1905-1910, 2015.", "citeRegEx": "231", "shortCiteRegEx": null, "year": 1905}, {"title": "Using the web to interactively learn to find objects", "author": ["M. Samadi", "T. Kollar", "M. Veloso"], "venue": "Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, 2012.", "citeRegEx": "232", "shortCiteRegEx": null, "year": 2012}, {"title": "Bayesian active learning-based robot tutor for children\u2019s word-reading skills", "author": ["G. Gordon", "C. Breazeal"], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, pp. 1343-1349, 2015.", "citeRegEx": "233", "shortCiteRegEx": null, "year": 2015}, {"title": "Cross-organizational collaborative workflow mining from a multi-source log", "author": ["Q. Zeng", "S. Sun", "H. Duan", "C. Liu", "H. Wang"], "venue": "Decision Support Systems, vol. 54, pp. 1280-1301, 2013.", "citeRegEx": "234", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Attracted by the naturalness of natural language (NL) communications among humans, the intelligent robots start to use NL to interact with humans during the cooperation [1].", "startOffset": 169, "endOffset": 172}, {"referenceID": 1, "context": "NL is used with either a spoken or written manner for delivering humans\u2019 cooperation requests to a robot, facilitating an intuitive human-robot cooperation (HRC)[2][3][4] [5].", "startOffset": 161, "endOffset": 164}, {"referenceID": 2, "context": "NL is used with either a spoken or written manner for delivering humans\u2019 cooperation requests to a robot, facilitating an intuitive human-robot cooperation (HRC)[2][3][4] [5].", "startOffset": 164, "endOffset": 167}, {"referenceID": 3, "context": "NL is used with either a spoken or written manner for delivering humans\u2019 cooperation requests to a robot, facilitating an intuitive human-robot cooperation (HRC)[2][3][4] [5].", "startOffset": 167, "endOffset": 170}, {"referenceID": 4, "context": "NL is used with either a spoken or written manner for delivering humans\u2019 cooperation requests to a robot, facilitating an intuitive human-robot cooperation (HRC)[2][3][4] [5].", "startOffset": 171, "endOffset": 174}, {"referenceID": 5, "context": "By using NL, human intelligence at high-level task planning and robot physical capability (such as force [6], precision [7] and speed [2]) on low-level task executions are combined to perform an intuitive cooperation [6][8].", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "By using NL, human intelligence at high-level task planning and robot physical capability (such as force [6], precision [7] and speed [2]) on low-level task executions are combined to perform an intuitive cooperation [6][8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 1, "context": "By using NL, human intelligence at high-level task planning and robot physical capability (such as force [6], precision [7] and speed [2]) on low-level task executions are combined to perform an intuitive cooperation [6][8].", "startOffset": 134, "endOffset": 137}, {"referenceID": 5, "context": "By using NL, human intelligence at high-level task planning and robot physical capability (such as force [6], precision [7] and speed [2]) on low-level task executions are combined to perform an intuitive cooperation [6][8].", "startOffset": 217, "endOffset": 220}, {"referenceID": 7, "context": "By using NL, human intelligence at high-level task planning and robot physical capability (such as force [6], precision [7] and speed [2]) on low-level task executions are combined to perform an intuitive cooperation [6][8].", "startOffset": 220, "endOffset": 223}, {"referenceID": 8, "context": "Currently, typical human-robot communication methods include tactile indications such as contact location [9], force strength [10][11], and visual indications such as body pose [12][13], and motion [14][15] etc.", "startOffset": 106, "endOffset": 109}, {"referenceID": 9, "context": "Currently, typical human-robot communication methods include tactile indications such as contact location [9], force strength [10][11], and visual indications such as body pose [12][13], and motion [14][15] etc.", "startOffset": 126, "endOffset": 130}, {"referenceID": 10, "context": "Currently, typical human-robot communication methods include tactile indications such as contact location [9], force strength [10][11], and visual indications such as body pose [12][13], and motion [14][15] etc.", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "Currently, typical human-robot communication methods include tactile indications such as contact location [9], force strength [10][11], and visual indications such as body pose [12][13], and motion [14][15] etc.", "startOffset": 177, "endOffset": 181}, {"referenceID": 12, "context": "Currently, typical human-robot communication methods include tactile indications such as contact location [9], force strength [10][11], and visual indications such as body pose [12][13], and motion [14][15] etc.", "startOffset": 181, "endOffset": 185}, {"referenceID": 13, "context": "Currently, typical human-robot communication methods include tactile indications such as contact location [9], force strength [10][11], and visual indications such as body pose [12][13], and motion [14][15] etc.", "startOffset": 198, "endOffset": 202}, {"referenceID": 14, "context": "Currently, typical human-robot communication methods include tactile indications such as contact location [9], force strength [10][11], and visual indications such as body pose [12][13], and motion [14][15] etc.", "startOffset": 202, "endOffset": 206}, {"referenceID": 15, "context": "For traditional methods mentioned above, humans involved in HRC need to be trained to use certain actions/poses for making themselves understandable [16][17].", "startOffset": 149, "endOffset": 153}, {"referenceID": 16, "context": "For traditional methods mentioned above, humans involved in HRC need to be trained to use certain actions/poses for making themselves understandable [16][17].", "startOffset": 153, "endOffset": 157}, {"referenceID": 17, "context": "While in NLC, even non-expert users without prior training can cooperate with a robot by using human-like communication [18][19].", "startOffset": 120, "endOffset": 124}, {"referenceID": 18, "context": "While in NLC, even non-expert users without prior training can cooperate with a robot by using human-like communication [18][19].", "startOffset": 124, "endOffset": 128}, {"referenceID": 19, "context": "Traditional methods using actions/poses only provide limited patterns to roughly describe cooperation requests due to information loss in action/pose simplification (such as using markers to simplify actions) [20][21][22][23].", "startOffset": 209, "endOffset": 213}, {"referenceID": 20, "context": "Traditional methods using actions/poses only provide limited patterns to roughly describe cooperation requests due to information loss in action/pose simplification (such as using markers to simplify actions) [20][21][22][23].", "startOffset": 213, "endOffset": 217}, {"referenceID": 21, "context": "Traditional methods using actions/poses only provide limited patterns to roughly describe cooperation requests due to information loss in action/pose simplification (such as using markers to simplify actions) [20][21][22][23].", "startOffset": 217, "endOffset": 221}, {"referenceID": 22, "context": "Traditional methods using actions/poses only provide limited patterns to roughly describe cooperation requests due to information loss in action/pose simplification (such as using markers to simplify actions) [20][21][22][23].", "startOffset": 221, "endOffset": 225}, {"referenceID": 6, "context": "While in NLC, cooperation requests related to action, speed, tool and location are already defined in NL expressions [7][24].", "startOffset": 117, "endOffset": 120}, {"referenceID": 23, "context": "While in NLC, cooperation requests related to action, speed, tool and location are already defined in NL expressions [7][24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 19, "context": "The information-transferring method using actions/poses requires the design of informative patterns for different cooperation requests [20][21].", "startOffset": 135, "endOffset": 139}, {"referenceID": 20, "context": "The information-transferring method using actions/poses requires the design of informative patterns for different cooperation requests [20][21].", "startOffset": 139, "endOffset": 143}, {"referenceID": 24, "context": "Existing languages, such as English, Chinese and German, already have standard linguistic structures, which contain abundant informative expressions to serve as patterns [25][26].", "startOffset": 170, "endOffset": 174}, {"referenceID": 25, "context": "Existing languages, such as English, Chinese and German, already have standard linguistic structures, which contain abundant informative expressions to serve as patterns [25][26].", "startOffset": 174, "endOffset": 178}, {"referenceID": 26, "context": "With the above advantages, NLC has been widely explored in areas, including daily assistances [27][28], medical caregiving [29][30], manufacturing [8][31], indoor/outdoor navigation [36][37], social accompany [38][39] etc.", "startOffset": 94, "endOffset": 98}, {"referenceID": 27, "context": "With the above advantages, NLC has been widely explored in areas, including daily assistances [27][28], medical caregiving [29][30], manufacturing [8][31], indoor/outdoor navigation [36][37], social accompany [38][39] etc.", "startOffset": 98, "endOffset": 102}, {"referenceID": 28, "context": "With the above advantages, NLC has been widely explored in areas, including daily assistances [27][28], medical caregiving [29][30], manufacturing [8][31], indoor/outdoor navigation [36][37], social accompany [38][39] etc.", "startOffset": 123, "endOffset": 127}, {"referenceID": 29, "context": "With the above advantages, NLC has been widely explored in areas, including daily assistances [27][28], medical caregiving [29][30], manufacturing [8][31], indoor/outdoor navigation [36][37], social accompany [38][39] etc.", "startOffset": 127, "endOffset": 131}, {"referenceID": 7, "context": "With the above advantages, NLC has been widely explored in areas, including daily assistances [27][28], medical caregiving [29][30], manufacturing [8][31], indoor/outdoor navigation [36][37], social accompany [38][39] etc.", "startOffset": 147, "endOffset": 150}, {"referenceID": 30, "context": "With the above advantages, NLC has been widely explored in areas, including daily assistances [27][28], medical caregiving [29][30], manufacturing [8][31], indoor/outdoor navigation [36][37], social accompany [38][39] etc.", "startOffset": 150, "endOffset": 154}, {"referenceID": 35, "context": "With the above advantages, NLC has been widely explored in areas, including daily assistances [27][28], medical caregiving [29][30], manufacturing [8][31], indoor/outdoor navigation [36][37], social accompany [38][39] etc.", "startOffset": 182, "endOffset": 186}, {"referenceID": 36, "context": "With the above advantages, NLC has been widely explored in areas, including daily assistances [27][28], medical caregiving [29][30], manufacturing [8][31], indoor/outdoor navigation [36][37], social accompany [38][39] etc.", "startOffset": 186, "endOffset": 190}, {"referenceID": 37, "context": "With the above advantages, NLC has been widely explored in areas, including daily assistances [27][28], medical caregiving [29][30], manufacturing [8][31], indoor/outdoor navigation [36][37], social accompany [38][39] etc.", "startOffset": 209, "endOffset": 213}, {"referenceID": 38, "context": "With the above advantages, NLC has been widely explored in areas, including daily assistances [27][28], medical caregiving [29][30], manufacturing [8][31], indoor/outdoor navigation [36][37], social accompany [38][39] etc.", "startOffset": 213, "endOffset": 217}, {"referenceID": 39, "context": "Pushed by recent improvements of machine learning technics in classification [40], clustering [41] and feature extraction [42], NLP has been developed from simply syntax-driven processing, which builds syntax representations of sentence structures, to semantically-driven processing, which builds semantic networks for sentence meanings [43].", "startOffset": 77, "endOffset": 81}, {"referenceID": 40, "context": "Pushed by recent improvements of machine learning technics in classification [40], clustering [41] and feature extraction [42], NLP has been developed from simply syntax-driven processing, which builds syntax representations of sentence structures, to semantically-driven processing, which builds semantic networks for sentence meanings [43].", "startOffset": 94, "endOffset": 98}, {"referenceID": 41, "context": "Pushed by recent improvements of machine learning technics in classification [40], clustering [41] and feature extraction [42], NLP has been developed from simply syntax-driven processing, which builds syntax representations of sentence structures, to semantically-driven processing, which builds semantic networks for sentence meanings [43].", "startOffset": 122, "endOffset": 126}, {"referenceID": 42, "context": "Pushed by recent improvements of machine learning technics in classification [40], clustering [41] and feature extraction [42], NLP has been developed from simply syntax-driven processing, which builds syntax representations of sentence structures, to semantically-driven processing, which builds semantic networks for sentence meanings [43].", "startOffset": 337, "endOffset": 341}, {"referenceID": 43, "context": "Starting in the year 1950, the simplest semantic analysis method, keyword-based understanding method, was designed [44], enabling a na\u00efve word-symbol understanding by identifying single/multiple keywords [45][46], lexical affinities [47][48], and word/affinity occurrences [49][50].", "startOffset": 115, "endOffset": 119}, {"referenceID": 44, "context": "Starting in the year 1950, the simplest semantic analysis method, keyword-based understanding method, was designed [44], enabling a na\u00efve word-symbol understanding by identifying single/multiple keywords [45][46], lexical affinities [47][48], and word/affinity occurrences [49][50].", "startOffset": 204, "endOffset": 208}, {"referenceID": 45, "context": "Starting in the year 1950, the simplest semantic analysis method, keyword-based understanding method, was designed [44], enabling a na\u00efve word-symbol understanding by identifying single/multiple keywords [45][46], lexical affinities [47][48], and word/affinity occurrences [49][50].", "startOffset": 208, "endOffset": 212}, {"referenceID": 46, "context": "Starting in the year 1950, the simplest semantic analysis method, keyword-based understanding method, was designed [44], enabling a na\u00efve word-symbol understanding by identifying single/multiple keywords [45][46], lexical affinities [47][48], and word/affinity occurrences [49][50].", "startOffset": 233, "endOffset": 237}, {"referenceID": 47, "context": "Starting in the year 1950, the simplest semantic analysis method, keyword-based understanding method, was designed [44], enabling a na\u00efve word-symbol understanding by identifying single/multiple keywords [45][46], lexical affinities [47][48], and word/affinity occurrences [49][50].", "startOffset": 237, "endOffset": 241}, {"referenceID": 48, "context": "Starting in the year 1950, the simplest semantic analysis method, keyword-based understanding method, was designed [44], enabling a na\u00efve word-symbol understanding by identifying single/multiple keywords [45][46], lexical affinities [47][48], and word/affinity occurrences [49][50].", "startOffset": 273, "endOffset": 277}, {"referenceID": 49, "context": "Starting in the year 1950, the simplest semantic analysis method, keyword-based understanding method, was designed [44], enabling a na\u00efve word-symbol understanding by identifying single/multiple keywords [45][46], lexical affinities [47][48], and word/affinity occurrences [49][50].", "startOffset": 277, "endOffset": 281}, {"referenceID": 50, "context": "The concept-based method modeled semantic meanings of sentences and paragraphs by exploring embedded concepts, mainly including implicit NL indications [51][52], hierarchical ontologies [53][54], and semantic correlations [55][56].", "startOffset": 152, "endOffset": 156}, {"referenceID": 51, "context": "The concept-based method modeled semantic meanings of sentences and paragraphs by exploring embedded concepts, mainly including implicit NL indications [51][52], hierarchical ontologies [53][54], and semantic correlations [55][56].", "startOffset": 156, "endOffset": 160}, {"referenceID": 52, "context": "The concept-based method modeled semantic meanings of sentences and paragraphs by exploring embedded concepts, mainly including implicit NL indications [51][52], hierarchical ontologies [53][54], and semantic correlations [55][56].", "startOffset": 186, "endOffset": 190}, {"referenceID": 53, "context": "The concept-based method modeled semantic meanings of sentences and paragraphs by exploring embedded concepts, mainly including implicit NL indications [51][52], hierarchical ontologies [53][54], and semantic correlations [55][56].", "startOffset": 190, "endOffset": 194}, {"referenceID": 54, "context": "The concept-based method modeled semantic meanings of sentences and paragraphs by exploring embedded concepts, mainly including implicit NL indications [51][52], hierarchical ontologies [53][54], and semantic correlations [55][56].", "startOffset": 222, "endOffset": 226}, {"referenceID": 55, "context": "The concept-based method modeled semantic meanings of sentences and paragraphs by exploring embedded concepts, mainly including implicit NL indications [51][52], hierarchical ontologies [53][54], and semantic correlations [55][56].", "startOffset": 226, "endOffset": 230}, {"referenceID": 56, "context": "To practically implement NLP methods in real-world robot-involved situations, a narrative-based method was developed to create a more sophisticated knowledge representation in a decision-making-focused [57], real-worldaware [58] and human-cognition-imitated manner [59][60].", "startOffset": 202, "endOffset": 206}, {"referenceID": 57, "context": "To practically implement NLP methods in real-world robot-involved situations, a narrative-based method was developed to create a more sophisticated knowledge representation in a decision-making-focused [57], real-worldaware [58] and human-cognition-imitated manner [59][60].", "startOffset": 224, "endOffset": 228}, {"referenceID": 58, "context": "To practically implement NLP methods in real-world robot-involved situations, a narrative-based method was developed to create a more sophisticated knowledge representation in a decision-making-focused [57], real-worldaware [58] and human-cognition-imitated manner [59][60].", "startOffset": 265, "endOffset": 269}, {"referenceID": 59, "context": "To practically implement NLP methods in real-world robot-involved situations, a narrative-based method was developed to create a more sophisticated knowledge representation in a decision-making-focused [57], real-worldaware [58] and human-cognition-imitated manner [59][60].", "startOffset": 269, "endOffset": 273}, {"referenceID": 43, "context": "2 [44].", "startOffset": 2, "endOffset": 6}, {"referenceID": 60, "context": "NLC is motivated by HRC In an early period (about 1940s\u2019 [61]), humans started to interact with robots by using remote controllers, developing an initial HRC, in which cooperation requirements for action mapping, task goal mapping, and cooperation naturalness/effectiveness were not considered.", "startOffset": 57, "endOffset": 61}, {"referenceID": 61, "context": "Compared with HRI, which focuses on general interactions (detailed HRI reviews are in [62][63]) for physical/mental assistances without task-goal constrains, HRC focuses on specific cooperation for task fulfillment with task-goal constrains, such as task planning and adjusting (detailed HRC reviews are [61][64]).", "startOffset": 86, "endOffset": 90}, {"referenceID": 62, "context": "Compared with HRI, which focuses on general interactions (detailed HRI reviews are in [62][63]) for physical/mental assistances without task-goal constrains, HRC focuses on specific cooperation for task fulfillment with task-goal constrains, such as task planning and adjusting (detailed HRC reviews are [61][64]).", "startOffset": 90, "endOffset": 94}, {"referenceID": 60, "context": "Compared with HRI, which focuses on general interactions (detailed HRI reviews are in [62][63]) for physical/mental assistances without task-goal constrains, HRC focuses on specific cooperation for task fulfillment with task-goal constrains, such as task planning and adjusting (detailed HRC reviews are [61][64]).", "startOffset": 304, "endOffset": 308}, {"referenceID": 63, "context": "Compared with HRI, which focuses on general interactions (detailed HRI reviews are in [62][63]) for physical/mental assistances without task-goal constrains, HRC focuses on specific cooperation for task fulfillment with task-goal constrains, such as task planning and adjusting (detailed HRC reviews are [61][64]).", "startOffset": 308, "endOffset": 312}, {"referenceID": 31, "context": "(a) is daily robotic assistance using NL[32].", "startOffset": 40, "endOffset": 44}, {"referenceID": 29, "context": "(b) is robotic caregiving using NL [30].", "startOffset": 35, "endOffset": 39}, {"referenceID": 32, "context": "(c) is autonomous manufacturing using NL [33].", "startOffset": 41, "endOffset": 45}, {"referenceID": 33, "context": "(d) is robotic navigation using NL [34].", "startOffset": 35, "endOffset": 39}, {"referenceID": 34, "context": "(e) is social companion [35].", "startOffset": 24, "endOffset": 28}, {"referenceID": 64, "context": "It started from motor-control-based action design where a robot followed simple human instructions to adjust its actions [65][66].", "startOffset": 121, "endOffset": 125}, {"referenceID": 65, "context": "It started from motor-control-based action design where a robot followed simple human instructions to adjust its actions [65][66].", "startOffset": 125, "endOffset": 129}, {"referenceID": 66, "context": "To make robot actions natural, a human-like motion style was then adopted in robot action design [67][68].", "startOffset": 97, "endOffset": 101}, {"referenceID": 67, "context": "To make robot actions natural, a human-like motion style was then adopted in robot action design [67][68].", "startOffset": 101, "endOffset": 105}, {"referenceID": 68, "context": "Though robots\u2019 motion behavior was similar with that of a human, robots\u2019 cooperation performances were still poor due to the limited action-understanding being insufficient to support its adaptations towards users/environments [69][70].", "startOffset": 227, "endOffset": 231}, {"referenceID": 69, "context": "Though robots\u2019 motion behavior was similar with that of a human, robots\u2019 cooperation performances were still poor due to the limited action-understanding being insufficient to support its adaptations towards users/environments [69][70].", "startOffset": 231, "endOffset": 235}, {"referenceID": 14, "context": "For example, \u2018cup manipulation\u2019 in \u2018drinking\u2019 activity meant \u2018containing liquid\u2019\u201d [15].", "startOffset": 82, "endOffset": 86}, {"referenceID": 70, "context": "Though the understanding was still limited to an action level, robots\u2019 understanding towards human behaviors in HRC was improved [71].", "startOffset": 129, "endOffset": 133}, {"referenceID": 43, "context": "The development of NLP methods [44].", "startOffset": 31, "endOffset": 35}, {"referenceID": 60, "context": "Three main stages in HRC development history: action-based HRC, interaction-based HRC and engagement-based HRC [61].", "startOffset": 111, "endOffset": 115}, {"referenceID": 71, "context": "submitted to Knowledge-based Systems, January 2017 movement imitation [72][73] where a robot was required to learn from human demonstrations, understand human movements and develop its own movements.", "startOffset": 70, "endOffset": 74}, {"referenceID": 72, "context": "submitted to Knowledge-based Systems, January 2017 movement imitation [72][73] where a robot was required to learn from human demonstrations, understand human movements and develop its own movements.", "startOffset": 74, "endOffset": 78}, {"referenceID": 73, "context": "To improve the understanding of human movements, robots were provided with various informative motion data, such as human action trajectories [74], hand/body poses [75], and biosignals [76].", "startOffset": 142, "endOffset": 146}, {"referenceID": 74, "context": "To improve the understanding of human movements, robots were provided with various informative motion data, such as human action trajectories [74], hand/body poses [75], and biosignals [76].", "startOffset": 164, "endOffset": 168}, {"referenceID": 75, "context": "To improve the understanding of human movements, robots were provided with various informative motion data, such as human action trajectories [74], hand/body poses [75], and biosignals [76].", "startOffset": 185, "endOffset": 189}, {"referenceID": 76, "context": "To further improve robots\u2019 cooperation performance, robots were trained to explore the mutual influence between a human and his/her surrounding environment [77][78].", "startOffset": 156, "endOffset": 160}, {"referenceID": 77, "context": "To further improve robots\u2019 cooperation performance, robots were trained to explore the mutual influence between a human and his/her surrounding environment [77][78].", "startOffset": 160, "endOffset": 164}, {"referenceID": 78, "context": "Individual-level factors, such as individual attentions [79][80], personalities [81][82], emotions [83][84] and safety factors [70][88], were considered by robots in the cooperation.", "startOffset": 56, "endOffset": 60}, {"referenceID": 79, "context": "Individual-level factors, such as individual attentions [79][80], personalities [81][82], emotions [83][84] and safety factors [70][88], were considered by robots in the cooperation.", "startOffset": 60, "endOffset": 64}, {"referenceID": 80, "context": "Individual-level factors, such as individual attentions [79][80], personalities [81][82], emotions [83][84] and safety factors [70][88], were considered by robots in the cooperation.", "startOffset": 80, "endOffset": 84}, {"referenceID": 81, "context": "Individual-level factors, such as individual attentions [79][80], personalities [81][82], emotions [83][84] and safety factors [70][88], were considered by robots in the cooperation.", "startOffset": 84, "endOffset": 88}, {"referenceID": 82, "context": "Individual-level factors, such as individual attentions [79][80], personalities [81][82], emotions [83][84] and safety factors [70][88], were considered by robots in the cooperation.", "startOffset": 99, "endOffset": 103}, {"referenceID": 83, "context": "Individual-level factors, such as individual attentions [79][80], personalities [81][82], emotions [83][84] and safety factors [70][88], were considered by robots in the cooperation.", "startOffset": 103, "endOffset": 107}, {"referenceID": 69, "context": "Individual-level factors, such as individual attentions [79][80], personalities [81][82], emotions [83][84] and safety factors [70][88], were considered by robots in the cooperation.", "startOffset": 127, "endOffset": 131}, {"referenceID": 86, "context": "Individual-level factors, such as individual attentions [79][80], personalities [81][82], emotions [83][84] and safety factors [70][88], were considered by robots in the cooperation.", "startOffset": 131, "endOffset": 135}, {"referenceID": 84, "context": "As a result of NLC research, a substantial number of projects were launched, including \u201ccollaborative research: jointly learning language and affordances\u201d from Cornell University [85], \u201crobots that learn to communicate with humans through natural dialog\u201d from University of Texas at Austin [86], \u201ccollaborative research: modeling and verification of language-based interaction\u201d from MIT [87], \u201clanguage grounding in robotics\u201d in University of Washington [89], \u201csemantic systems\u201d from Lund University [90] etc.", "startOffset": 179, "endOffset": 183}, {"referenceID": 85, "context": "As a result of NLC research, a substantial number of projects were launched, including \u201ccollaborative research: jointly learning language and affordances\u201d from Cornell University [85], \u201crobots that learn to communicate with humans through natural dialog\u201d from University of Texas at Austin [86], \u201ccollaborative research: modeling and verification of language-based interaction\u201d from MIT [87], \u201clanguage grounding in robotics\u201d in University of Washington [89], \u201csemantic systems\u201d from Lund University [90] etc.", "startOffset": 387, "endOffset": 391}, {"referenceID": 87, "context": "As a result of NLC research, a substantial number of projects were launched, including \u201ccollaborative research: jointly learning language and affordances\u201d from Cornell University [85], \u201crobots that learn to communicate with humans through natural dialog\u201d from University of Texas at Austin [86], \u201ccollaborative research: modeling and verification of language-based interaction\u201d from MIT [87], \u201clanguage grounding in robotics\u201d in University of Washington [89], \u201csemantic systems\u201d from Lund University [90] etc.", "startOffset": 454, "endOffset": 458}, {"referenceID": 88, "context": "As a result of NLC research, a substantial number of projects were launched, including \u201ccollaborative research: jointly learning language and affordances\u201d from Cornell University [85], \u201crobots that learn to communicate with humans through natural dialog\u201d from University of Texas at Austin [86], \u201ccollaborative research: modeling and verification of language-based interaction\u201d from MIT [87], \u201clanguage grounding in robotics\u201d in University of Washington [89], \u201csemantic systems\u201d from Lund University [90] etc.", "startOffset": 500, "endOffset": 504}, {"referenceID": 89, "context": "NLC research is regularly published in international journals such as IJRR [91], TRO [92], AI [93] and KBS [94], and international conferences such as ICRA [95], IROS [96] and AAAI [97].", "startOffset": 75, "endOffset": 79}, {"referenceID": 90, "context": "NLC research is regularly published in international journals such as IJRR [91], TRO [92], AI [93] and KBS [94], and international conferences such as ICRA [95], IROS [96] and AAAI [97].", "startOffset": 85, "endOffset": 89}, {"referenceID": 91, "context": "Compared with existing review papers about HRC using gesture/pose [98][99], action/motion [100], and tactile [101], a review paper about NLC is lacking.", "startOffset": 66, "endOffset": 70}, {"referenceID": 92, "context": "Compared with existing review papers about HRC using gesture/pose [98][99], action/motion [100], and tactile [101], a review paper about NLC is lacking.", "startOffset": 70, "endOffset": 74}, {"referenceID": 93, "context": "Compared with existing review papers about HRC using gesture/pose [98][99], action/motion [100], and tactile [101], a review paper about NLC is lacking.", "startOffset": 90, "endOffset": 95}, {"referenceID": 94, "context": "Compared with existing review papers about HRC using gesture/pose [98][99], action/motion [100], and tactile [101], a review paper about NLC is lacking.", "startOffset": 109, "endOffset": 114}, {"referenceID": 0, "context": "The simplest model for NL understanding is the predefinition model, which is also the earliest NL-understanding model for NLC [1].", "startOffset": 126, "endOffset": 129}, {"referenceID": 95, "context": "By identifying targeted keywords [102][103] and phrases [104][105] in human NL, a robot accurately understood symbolic and concise human cooperation requests when these requests were predefined exactly.", "startOffset": 33, "endOffset": 38}, {"referenceID": 96, "context": "By identifying targeted keywords [102][103] and phrases [104][105] in human NL, a robot accurately understood symbolic and concise human cooperation requests when these requests were predefined exactly.", "startOffset": 38, "endOffset": 43}, {"referenceID": 97, "context": "By identifying targeted keywords [102][103] and phrases [104][105] in human NL, a robot accurately understood symbolic and concise human cooperation requests when these requests were predefined exactly.", "startOffset": 56, "endOffset": 61}, {"referenceID": 98, "context": "By identifying targeted keywords [102][103] and phrases [104][105] in human NL, a robot accurately understood symbolic and concise human cooperation requests when these requests were predefined exactly.", "startOffset": 61, "endOffset": 66}, {"referenceID": 99, "context": "Then, to disambiguate word meanings, PoS tags for keywords were defined for polysemy situations [106][107], in which one word/phrase could have multiple meanings and one meaning could be expressed by multiple words/expressions.", "startOffset": 96, "endOffset": 101}, {"referenceID": 100, "context": "Then, to disambiguate word meanings, PoS tags for keywords were defined for polysemy situations [106][107], in which one word/phrase could have multiple meanings and one meaning could be expressed by multiple words/expressions.", "startOffset": 101, "endOffset": 106}, {"referenceID": 31, "context": "To model meanings of NL expressions in NLC, structural correlations, including keyword dependency, PoS dependency, and keyword meaning similarities, were summarized [32][108].", "startOffset": 165, "endOffset": 169}, {"referenceID": 101, "context": "To model meanings of NL expressions in NLC, structural correlations, including keyword dependency, PoS dependency, and keyword meaning similarities, were summarized [32][108].", "startOffset": 169, "endOffset": 174}, {"referenceID": 102, "context": "Predefinition models had good performances (success feature understanding rate 83%~100% [109]) in trained situations, whereas humans\u2019 NL expressions were defined in robots\u2019 knowledge.", "startOffset": 88, "endOffset": 93}, {"referenceID": 95, "context": "However, this type of models had a poor NL-understanding performance in untrained situations where NL expressions were not defined, or in similar situations where NL expressions were partially defined [102].", "startOffset": 201, "endOffset": 206}, {"referenceID": 96, "context": "submitted to Knowledge-based Systems, January 2017 verb word and noun word were combined to describe a type of actions such as V(go) + NN(Hallway) and V(grasp) + NN(cup) [103][108][114][115].", "startOffset": 170, "endOffset": 175}, {"referenceID": 101, "context": "submitted to Knowledge-based Systems, January 2017 verb word and noun word were combined to describe a type of actions such as V(go) + NN(Hallway) and V(grasp) + NN(cup) [103][108][114][115].", "startOffset": 175, "endOffset": 180}, {"referenceID": 107, "context": "submitted to Knowledge-based Systems, January 2017 verb word and noun word were combined to describe a type of actions such as V(go) + NN(Hallway) and V(grasp) + NN(cup) [103][108][114][115].", "startOffset": 180, "endOffset": 185}, {"referenceID": 108, "context": "submitted to Knowledge-based Systems, January 2017 verb word and noun word were combined to describe a type of actions such as V(go) + NN(Hallway) and V(grasp) + NN(cup) [103][108][114][115].", "startOffset": 185, "endOffset": 190}, {"referenceID": 109, "context": "Some feature-based grammars explored temporal relations, such as the if-then relation \u201cif door open, then turn right\u201d and the step1-step2 relation \u201cgo -- grasp\u201d [116][117][118].", "startOffset": 161, "endOffset": 166}, {"referenceID": 110, "context": "Some feature-based grammars explored temporal relations, such as the if-then relation \u201cif door open, then turn right\u201d and the step1-step2 relation \u201cgo -- grasp\u201d [116][117][118].", "startOffset": 166, "endOffset": 171}, {"referenceID": 111, "context": "Some feature-based grammars explored temporal relations, such as the if-then relation \u201cif door open, then turn right\u201d and the step1-step2 relation \u201cgo -- grasp\u201d [116][117][118].", "startOffset": 171, "endOffset": 176}, {"referenceID": 36, "context": "Some feature-based grammars explored spatial relations, such as the IN relation \u201ccup IN room\u201d and the CloseTo relation \u201ccup CloseTo plate\u201d [37][119][120].", "startOffset": 139, "endOffset": 143}, {"referenceID": 112, "context": "Some feature-based grammars explored spatial relations, such as the IN relation \u201ccup IN room\u201d and the CloseTo relation \u201ccup CloseTo plate\u201d [37][119][120].", "startOffset": 143, "endOffset": 148}, {"referenceID": 113, "context": "Some feature-based grammars explored spatial relations, such as the IN relation \u201ccup IN room\u201d and the CloseTo relation \u201ccup CloseTo plate\u201d [37][119][120].", "startOffset": 148, "endOffset": 153}, {"referenceID": 109, "context": "Typical works are [116][121][122][123].", "startOffset": 18, "endOffset": 23}, {"referenceID": 114, "context": "Typical works are [116][121][122][123].", "startOffset": 23, "endOffset": 28}, {"referenceID": 115, "context": "Typical works are [116][121][122][123].", "startOffset": 28, "endOffset": 33}, {"referenceID": 116, "context": "Typical works are [116][121][122][123].", "startOffset": 33, "endOffset": 38}, {"referenceID": 116, "context": "Needed information, such as \u201cbeverage is likely to be juice\u201d, was recommended for understanding abstract requests, such as \u201cput beverage in the fridge\u201d, according to the correlation likelihood [123].", "startOffset": 193, "endOffset": 198}, {"referenceID": 111, "context": "By defining sensor value ranges for ambiguous descriptions, such as \u201cslowly, frequently, heavy\u201d, the commonsense interpretation model made quantitative interpretations of ambiguous execution requests in NLC [118][124].", "startOffset": 207, "endOffset": 212}, {"referenceID": 117, "context": "By defining sensor value ranges for ambiguous descriptions, such as \u201cslowly, frequently, heavy\u201d, the commonsense interpretation model made quantitative interpretations of ambiguous execution requests in NLC [118][124].", "startOffset": 212, "endOffset": 217}, {"referenceID": 6, "context": "By integrating key aspects, such as precondition, action sequence, human preference, tool usages and location information, the commonsense interpretation model made machine-executable task plans based on human-described high-level abstract task plans [7] [37][107][115][125].", "startOffset": 251, "endOffset": 254}, {"referenceID": 36, "context": "By integrating key aspects, such as precondition, action sequence, human preference, tool usages and location information, the commonsense interpretation model made machine-executable task plans based on human-described high-level abstract task plans [7] [37][107][115][125].", "startOffset": 255, "endOffset": 259}, {"referenceID": 100, "context": "By integrating key aspects, such as precondition, action sequence, human preference, tool usages and location information, the commonsense interpretation model made machine-executable task plans based on human-described high-level abstract task plans [7] [37][107][115][125].", "startOffset": 259, "endOffset": 264}, {"referenceID": 108, "context": "By integrating key aspects, such as precondition, action sequence, human preference, tool usages and location information, the commonsense interpretation model made machine-executable task plans based on human-described high-level abstract task plans [7] [37][107][115][125].", "startOffset": 264, "endOffset": 269}, {"referenceID": 118, "context": "By integrating key aspects, such as precondition, action sequence, human preference, tool usages and location information, the commonsense interpretation model made machine-executable task plans based on human-described high-level abstract task plans [7] [37][107][115][125].", "startOffset": 269, "endOffset": 274}, {"referenceID": 105, "context": "By using discrete fuzzy statuses to map continuous sensor data, unlimited objective sensor values were \u2018translated\u2019 into limited subjective NL expressions such as \u201cclose to the robot, in the kitchen\u201d [112][126].", "startOffset": 200, "endOffset": 205}, {"referenceID": 119, "context": "By using discrete fuzzy statuses to map continuous sensor data, unlimited objective sensor values were \u2018translated\u2019 into limited subjective NL expressions such as \u201cclose to the robot, in the kitchen\u201d [112][126].", "startOffset": 205, "endOffset": 210}, {"referenceID": 95, "context": "(a) [102] and (b) [110] are predefined models.", "startOffset": 4, "endOffset": 9}, {"referenceID": 103, "context": "(a) [102] and (b) [110] are predefined models.", "startOffset": 18, "endOffset": 23}, {"referenceID": 104, "context": "(c) [111] is a grammar model, in which object manipulation methods are defined as linguistic structures such as \u201cfind(cup), grasp(milkbox), .", "startOffset": 4, "endOffset": 9}, {"referenceID": 105, "context": "(d) [112] (e) [113] and (f) [112] are interpretation models, in which a command for a robot arm is interpreted into execution parameters.", "startOffset": 4, "endOffset": 9}, {"referenceID": 106, "context": "(d) [112] (e) [113] and (f) [112] are interpretation models, in which a command for a robot arm is interpreted into execution parameters.", "startOffset": 14, "endOffset": 19}, {"referenceID": 105, "context": "(d) [112] (e) [113] and (f) [112] are interpretation models, in which a command for a robot arm is interpreted into execution parameters.", "startOffset": 28, "endOffset": 33}, {"referenceID": 104, "context": "submitted to Knowledge-based Systems, January 2017 [111][117][127][128].", "startOffset": 51, "endOffset": 56}, {"referenceID": 110, "context": "submitted to Knowledge-based Systems, January 2017 [111][117][127][128].", "startOffset": 56, "endOffset": 61}, {"referenceID": 120, "context": "submitted to Knowledge-based Systems, January 2017 [111][117][127][128].", "startOffset": 61, "endOffset": 66}, {"referenceID": 121, "context": "submitted to Knowledge-based Systems, January 2017 [111][117][127][128].", "startOffset": 66, "endOffset": 71}, {"referenceID": 120, "context": "Typical semantic aspects include object categories such as \u201cbox, ball\u201d [127], task-", "startOffset": 71, "endOffset": 76}, {"referenceID": 122, "context": "suspended, cancelled, complete\u201d [129], human factors such as \u201caction type, current location, voice amplitude, head motion\u201d [130][131], environment context such as \u201cobject is at behand, elevators are in right \u201d [132][133] etc.", "startOffset": 32, "endOffset": 37}, {"referenceID": 123, "context": "suspended, cancelled, complete\u201d [129], human factors such as \u201caction type, current location, voice amplitude, head motion\u201d [130][131], environment context such as \u201cobject is at behand, elevators are in right \u201d [132][133] etc.", "startOffset": 123, "endOffset": 128}, {"referenceID": 124, "context": "suspended, cancelled, complete\u201d [129], human factors such as \u201caction type, current location, voice amplitude, head motion\u201d [130][131], environment context such as \u201cobject is at behand, elevators are in right \u201d [132][133] etc.", "startOffset": 128, "endOffset": 133}, {"referenceID": 125, "context": "suspended, cancelled, complete\u201d [129], human factors such as \u201caction type, current location, voice amplitude, head motion\u201d [130][131], environment context such as \u201cobject is at behand, elevators are in right \u201d [132][133] etc.", "startOffset": 210, "endOffset": 215}, {"referenceID": 126, "context": "suspended, cancelled, complete\u201d [129], human factors such as \u201caction type, current location, voice amplitude, head motion\u201d [130][131], environment context such as \u201cobject is at behand, elevators are in right \u201d [132][133] etc.", "startOffset": 215, "endOffset": 220}, {"referenceID": 124, "context": "Typical problems solved by single modality models include: human-desired tool identification using semantic aspects such as color, shape and weight [131][134], human visual perspective understanding by using cognition aspects such as human preference, human physical ability and human needs [135][136], task motivation understanding using executionrelated key factors such as action sequences, tool functions, locations [7][132] etc.", "startOffset": 148, "endOffset": 153}, {"referenceID": 127, "context": "Typical problems solved by single modality models include: human-desired tool identification using semantic aspects such as color, shape and weight [131][134], human visual perspective understanding by using cognition aspects such as human preference, human physical ability and human needs [135][136], task motivation understanding using executionrelated key factors such as action sequences, tool functions, locations [7][132] etc.", "startOffset": 153, "endOffset": 158}, {"referenceID": 128, "context": "Typical problems solved by single modality models include: human-desired tool identification using semantic aspects such as color, shape and weight [131][134], human visual perspective understanding by using cognition aspects such as human preference, human physical ability and human needs [135][136], task motivation understanding using executionrelated key factors such as action sequences, tool functions, locations [7][132] etc.", "startOffset": 291, "endOffset": 296}, {"referenceID": 129, "context": "Typical problems solved by single modality models include: human-desired tool identification using semantic aspects such as color, shape and weight [131][134], human visual perspective understanding by using cognition aspects such as human preference, human physical ability and human needs [135][136], task motivation understanding using executionrelated key factors such as action sequences, tool functions, locations [7][132] etc.", "startOffset": 296, "endOffset": 301}, {"referenceID": 6, "context": "Typical problems solved by single modality models include: human-desired tool identification using semantic aspects such as color, shape and weight [131][134], human visual perspective understanding by using cognition aspects such as human preference, human physical ability and human needs [135][136], task motivation understanding using executionrelated key factors such as action sequences, tool functions, locations [7][132] etc.", "startOffset": 420, "endOffset": 423}, {"referenceID": 125, "context": "Typical problems solved by single modality models include: human-desired tool identification using semantic aspects such as color, shape and weight [131][134], human visual perspective understanding by using cognition aspects such as human preference, human physical ability and human needs [135][136], task motivation understanding using executionrelated key factors such as action sequences, tool functions, locations [7][132] etc.", "startOffset": 423, "endOffset": 428}, {"referenceID": 115, "context": "The aspect-based model is generally better than the feature-based method in understanding human NL requests in NLC [122].", "startOffset": 115, "endOffset": 120}, {"referenceID": 14, "context": "For example, semantic aspects \u201caction-effect\u201d and \u201cobject-function\u201d were human\u2019s cognitive perspectives for understanding task cooperation [15].", "startOffset": 139, "endOffset": 143}, {"referenceID": 108, "context": "(a) [115], (b) [7], (c) [137] and (d) [123] are typical single-modality aspectbased models.", "startOffset": 4, "endOffset": 9}, {"referenceID": 6, "context": "(a) [115], (b) [7], (c) [137] and (d) [123] are typical single-modality aspectbased models.", "startOffset": 15, "endOffset": 18}, {"referenceID": 130, "context": "(a) [115], (b) [7], (c) [137] and (d) [123] are typical single-modality aspectbased models.", "startOffset": 24, "endOffset": 29}, {"referenceID": 116, "context": "(a) [115], (b) [7], (c) [137] and (d) [123] are typical single-modality aspectbased models.", "startOffset": 38, "endOffset": 43}, {"referenceID": 131, "context": "(e) [138], (f) [139], (g) [140], and (h) [141] are multi-modality aspect-based models.", "startOffset": 4, "endOffset": 9}, {"referenceID": 132, "context": "(e) [138], (f) [139], (g) [140], and (h) [141] are multi-modality aspect-based models.", "startOffset": 15, "endOffset": 20}, {"referenceID": 133, "context": "(e) [138], (f) [139], (g) [140], and (h) [141] are multi-modality aspect-based models.", "startOffset": 26, "endOffset": 31}, {"referenceID": 134, "context": "(e) [138], (f) [139], (g) [140], and (h) [141] are multi-modality aspect-based models.", "startOffset": 41, "endOffset": 46}, {"referenceID": 135, "context": ") [143], individual identity [139], touch events [140], hand poses [144], and head orientations [75].", "startOffset": 2, "endOffset": 7}, {"referenceID": 132, "context": ") [143], individual identity [139], touch events [140], hand poses [144], and head orientations [75].", "startOffset": 29, "endOffset": 34}, {"referenceID": 133, "context": ") [143], individual identity [139], touch events [140], hand poses [144], and head orientations [75].", "startOffset": 49, "endOffset": 54}, {"referenceID": 136, "context": ") [143], individual identity [139], touch events [140], hand poses [144], and head orientations [75].", "startOffset": 67, "endOffset": 72}, {"referenceID": 74, "context": ") [143], individual identity [139], touch events [140], hand poses [144], and head orientations [75].", "startOffset": 96, "endOffset": 100}, {"referenceID": 134, "context": "Supported by rich information from multi-modality features, typical solved problems include complex-instruction understanding [141], human-like HRC [75], social behavior interpretation [126] etc.", "startOffset": 126, "endOffset": 131}, {"referenceID": 74, "context": "Supported by rich information from multi-modality features, typical solved problems include complex-instruction understanding [141], human-like HRC [75], social behavior interpretation [126] etc.", "startOffset": 148, "endOffset": 152}, {"referenceID": 119, "context": "Supported by rich information from multi-modality features, typical solved problems include complex-instruction understanding [141], human-like HRC [75], social behavior interpretation [126] etc.", "startOffset": 185, "endOffset": 190}, {"referenceID": 74, "context": "For multi-modality models using environment/robot-related features to understand human NL requests in NLC, including linguistic features considered in single-modality models, typical features also include temporal dependencies among speech-head orientation, hand gesture domains [75], spatial relations among human, robot, and object [141], visual-auditory-combined indications [145], knowledge/sensorimotor-combined intention predictions [138], etc.", "startOffset": 279, "endOffset": 283}, {"referenceID": 134, "context": "For multi-modality models using environment/robot-related features to understand human NL requests in NLC, including linguistic features considered in single-modality models, typical features also include temporal dependencies among speech-head orientation, hand gesture domains [75], spatial relations among human, robot, and object [141], visual-auditory-combined indications [145], knowledge/sensorimotor-combined intention predictions [138], etc.", "startOffset": 334, "endOffset": 339}, {"referenceID": 137, "context": "For multi-modality models using environment/robot-related features to understand human NL requests in NLC, including linguistic features considered in single-modality models, typical features also include temporal dependencies among speech-head orientation, hand gesture domains [75], spatial relations among human, robot, and object [141], visual-auditory-combined indications [145], knowledge/sensorimotor-combined intention predictions [138], etc.", "startOffset": 378, "endOffset": 383}, {"referenceID": 131, "context": "For multi-modality models using environment/robot-related features to understand human NL requests in NLC, including linguistic features considered in single-modality models, typical features also include temporal dependencies among speech-head orientation, hand gesture domains [75], spatial relations among human, robot, and object [141], visual-auditory-combined indications [145], knowledge/sensorimotor-combined intention predictions [138], etc.", "startOffset": 439, "endOffset": 444}, {"referenceID": 137, "context": "Typical algorithms used for constructing multi-modality models include hidden Markov model (HMM) for modeling hidden probabilistic relations among semantic aspects [145][146], Bayesian Network for modeling probabilistic transitions among taskexecution steps [147][148], first-order logic for modeling semantic constraints among semantic aspects [130][149] etc.", "startOffset": 164, "endOffset": 169}, {"referenceID": 138, "context": "Typical algorithms used for constructing multi-modality models include hidden Markov model (HMM) for modeling hidden probabilistic relations among semantic aspects [145][146], Bayesian Network for modeling probabilistic transitions among taskexecution steps [147][148], first-order logic for modeling semantic constraints among semantic aspects [130][149] etc.", "startOffset": 169, "endOffset": 174}, {"referenceID": 139, "context": "Typical algorithms used for constructing multi-modality models include hidden Markov model (HMM) for modeling hidden probabilistic relations among semantic aspects [145][146], Bayesian Network for modeling probabilistic transitions among taskexecution steps [147][148], first-order logic for modeling semantic constraints among semantic aspects [130][149] etc.", "startOffset": 258, "endOffset": 263}, {"referenceID": 140, "context": "Typical algorithms used for constructing multi-modality models include hidden Markov model (HMM) for modeling hidden probabilistic relations among semantic aspects [145][146], Bayesian Network for modeling probabilistic transitions among taskexecution steps [147][148], first-order logic for modeling semantic constraints among semantic aspects [130][149] etc.", "startOffset": 263, "endOffset": 268}, {"referenceID": 123, "context": "Typical algorithms used for constructing multi-modality models include hidden Markov model (HMM) for modeling hidden probabilistic relations among semantic aspects [145][146], Bayesian Network for modeling probabilistic transitions among taskexecution steps [147][148], first-order logic for modeling semantic constraints among semantic aspects [130][149] etc.", "startOffset": 345, "endOffset": 350}, {"referenceID": 141, "context": "Typical algorithms used for constructing multi-modality models include hidden Markov model (HMM) for modeling hidden probabilistic relations among semantic aspects [145][146], Bayesian Network for modeling probabilistic transitions among taskexecution steps [147][148], first-order logic for modeling semantic constraints among semantic aspects [130][149] etc.", "startOffset": 350, "endOffset": 355}, {"referenceID": 142, "context": "Probabilistic model In regards to probabilistic relations, probabilistic models for knowledge representations are categorized into a generative model, which is based on generative algorithms, such as Na\u00efve Bayesian network (BN) [150], Hidden Markov model (HMM) [151], and Markov Random Field (MRF) [152], and a discriminative model, which is based on discriminative algorithms such as Support Vector Machine (SVM) [153] and Conditional Random Field (CRF) [154].", "startOffset": 228, "endOffset": 233}, {"referenceID": 143, "context": "Probabilistic model In regards to probabilistic relations, probabilistic models for knowledge representations are categorized into a generative model, which is based on generative algorithms, such as Na\u00efve Bayesian network (BN) [150], Hidden Markov model (HMM) [151], and Markov Random Field (MRF) [152], and a discriminative model, which is based on discriminative algorithms such as Support Vector Machine (SVM) [153] and Conditional Random Field (CRF) [154].", "startOffset": 261, "endOffset": 266}, {"referenceID": 144, "context": "Probabilistic model In regards to probabilistic relations, probabilistic models for knowledge representations are categorized into a generative model, which is based on generative algorithms, such as Na\u00efve Bayesian network (BN) [150], Hidden Markov model (HMM) [151], and Markov Random Field (MRF) [152], and a discriminative model, which is based on discriminative algorithms such as Support Vector Machine (SVM) [153] and Conditional Random Field (CRF) [154].", "startOffset": 298, "endOffset": 303}, {"referenceID": 145, "context": "Probabilistic model In regards to probabilistic relations, probabilistic models for knowledge representations are categorized into a generative model, which is based on generative algorithms, such as Na\u00efve Bayesian network (BN) [150], Hidden Markov model (HMM) [151], and Markov Random Field (MRF) [152], and a discriminative model, which is based on discriminative algorithms such as Support Vector Machine (SVM) [153] and Conditional Random Field (CRF) [154].", "startOffset": 414, "endOffset": 419}, {"referenceID": 146, "context": "Probabilistic model In regards to probabilistic relations, probabilistic models for knowledge representations are categorized into a generative model, which is based on generative algorithms, such as Na\u00efve Bayesian network (BN) [150], Hidden Markov model (HMM) [151], and Markov Random Field (MRF) [152], and a discriminative model, which is based on discriminative algorithms such as Support Vector Machine (SVM) [153] and Conditional Random Field (CRF) [154].", "startOffset": 455, "endOffset": 460}, {"referenceID": 147, "context": "By using a single joint probability p(x, y), the probabilistic association p of a human\u2019s cooperation request y, such as \u201cmove\u201d, and one execution parameter x, such as object \u201cball\u201d, was made [155].", "startOffset": 192, "endOffset": 197}, {"referenceID": 104, "context": "Typical joint-probability associations in NLC include activity-object associations such as drink-cup [111], activityenvironment associations such as drink-hotDay [77], and action-sensor associations such as pickup-0.", "startOffset": 101, "endOffset": 106}, {"referenceID": 76, "context": "Typical joint-probability associations in NLC include activity-object associations such as drink-cup [111], activityenvironment associations such as drink-hotDay [77], and action-sensor associations such as pickup-0.", "startOffset": 162, "endOffset": 166}, {"referenceID": 148, "context": "1m/s [156].", "startOffset": 5, "endOffset": 10}, {"referenceID": 148, "context": "Typical methods using multiple jointprobability associations include Viterbi algorithm [156], Na\u00efve Bayesian (NB) algorithm [77] and Markov Random Field (MRF) [78].", "startOffset": 87, "endOffset": 92}, {"referenceID": 76, "context": "Typical methods using multiple jointprobability associations include Viterbi algorithm [156], Na\u00efve Bayesian (NB) algorithm [77] and Markov Random Field (MRF) [78].", "startOffset": 124, "endOffset": 128}, {"referenceID": 77, "context": "Typical methods using multiple jointprobability associations include Viterbi algorithm [156], Na\u00efve Bayesian (NB) algorithm [77] and Markov Random Field (MRF) [78].", "startOffset": 159, "endOffset": 163}, {"referenceID": 77, "context": "With multi-joint-probabilistic BN models, solved problems include: modeling tasks\u2019 meaning distributions on linguistic features such as NL keywords and expression patterns [78][157]; aligning multi-view sensor data such as speech meaning, task execution statuses, and robot/human motion statuses [115][158]; Table.", "startOffset": 172, "endOffset": 176}, {"referenceID": 149, "context": "With multi-joint-probabilistic BN models, solved problems include: modeling tasks\u2019 meaning distributions on linguistic features such as NL keywords and expression patterns [78][157]; aligning multi-view sensor data such as speech meaning, task execution statuses, and robot/human motion statuses [115][158]; Table.", "startOffset": 176, "endOffset": 181}, {"referenceID": 108, "context": "With multi-joint-probabilistic BN models, solved problems include: modeling tasks\u2019 meaning distributions on linguistic features such as NL keywords and expression patterns [78][157]; aligning multi-view sensor data such as speech meaning, task execution statuses, and robot/human motion statuses [115][158]; Table.", "startOffset": 296, "endOffset": 301}, {"referenceID": 150, "context": "With multi-joint-probabilistic BN models, solved problems include: modeling tasks\u2019 meaning distributions on linguistic features such as NL keywords and expression patterns [78][157]; aligning multi-view sensor data such as speech meaning, task execution statuses, and robot/human motion statuses [115][158]; Table.", "startOffset": 301, "endOffset": 306}, {"referenceID": 96, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 19, "endOffset": 24}, {"referenceID": 97, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 24, "endOffset": 29}, {"referenceID": 98, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 29, "endOffset": 34}, {"referenceID": 99, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 35, "endOffset": 40}, {"referenceID": 100, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 41, "endOffset": 46}, {"referenceID": 109, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 47, "endOffset": 52}, {"referenceID": 110, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 53, "endOffset": 58}, {"referenceID": 111, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 58, "endOffset": 63}, {"referenceID": 112, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 64, "endOffset": 69}, {"referenceID": 113, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 69, "endOffset": 74}, {"referenceID": 114, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 75, "endOffset": 80}, {"referenceID": 115, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 80, "endOffset": 85}, {"referenceID": 114, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 85, "endOffset": 90}, {"referenceID": 117, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 90, "endOffset": 95}, {"referenceID": 118, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 96, "endOffset": 101}, {"referenceID": 119, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 101, "endOffset": 106}, {"referenceID": 122, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 107, "endOffset": 112}, {"referenceID": 124, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 112, "endOffset": 117}, {"referenceID": 126, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 117, "endOffset": 122}, {"referenceID": 124, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 123, "endOffset": 128}, {"referenceID": 127, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 128, "endOffset": 133}, {"referenceID": 138, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 134, "endOffset": 139}, {"referenceID": 137, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 139, "endOffset": 144}, {"referenceID": 139, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 145, "endOffset": 150}, {"referenceID": 140, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 150, "endOffset": 155}, {"referenceID": 141, "context": "Typical references [103][104][105] [106] [107] [116] [117][118] [119][120] [121][122][121][124] [125][126] [129][131][133] [131][134] [146][145] [147][148][149]", "startOffset": 155, "endOffset": 160}, {"referenceID": 151, "context": "submitted to Knowledge-based Systems, January 2017 exploring the in-depth semantic meaning of a human-described task by associating shallow-level aspects such as words and actions with in-depth-level aspects such as execution effects [159][160]; intuitive task understanding by integrating current NL descriptions with previous execution experiences [161]; smart knowledge implementation by associating the theoretical knowledge such as objects with practical real-world evidences such as object color and placement locations [137][162].", "startOffset": 234, "endOffset": 239}, {"referenceID": 152, "context": "submitted to Knowledge-based Systems, January 2017 exploring the in-depth semantic meaning of a human-described task by associating shallow-level aspects such as words and actions with in-depth-level aspects such as execution effects [159][160]; intuitive task understanding by integrating current NL descriptions with previous execution experiences [161]; smart knowledge implementation by associating the theoretical knowledge such as objects with practical real-world evidences such as object color and placement locations [137][162].", "startOffset": 239, "endOffset": 244}, {"referenceID": 153, "context": "submitted to Knowledge-based Systems, January 2017 exploring the in-depth semantic meaning of a human-described task by associating shallow-level aspects such as words and actions with in-depth-level aspects such as execution effects [159][160]; intuitive task understanding by integrating current NL descriptions with previous execution experiences [161]; smart knowledge implementation by associating the theoretical knowledge such as objects with practical real-world evidences such as object color and placement locations [137][162].", "startOffset": 350, "endOffset": 355}, {"referenceID": 130, "context": "submitted to Knowledge-based Systems, January 2017 exploring the in-depth semantic meaning of a human-described task by associating shallow-level aspects such as words and actions with in-depth-level aspects such as execution effects [159][160]; intuitive task understanding by integrating current NL descriptions with previous execution experiences [161]; smart knowledge implementation by associating the theoretical knowledge such as objects with practical real-world evidences such as object color and placement locations [137][162].", "startOffset": 526, "endOffset": 531}, {"referenceID": 154, "context": "submitted to Knowledge-based Systems, January 2017 exploring the in-depth semantic meaning of a human-described task by associating shallow-level aspects such as words and actions with in-depth-level aspects such as execution effects [159][160]; intuitive task understanding by integrating current NL descriptions with previous execution experiences [161]; smart knowledge implementation by associating the theoretical knowledge such as objects with practical real-world evidences such as object color and placement locations [137][162].", "startOffset": 531, "endOffset": 536}, {"referenceID": 76, "context": "One common character of generative models such as na\u00efve Bayesian (NB) is that dependencies among task features are simplified to be fully or partially independent [77].", "startOffset": 163, "endOffset": 167}, {"referenceID": 155, "context": "In practical situations, when a set of observations are made, observed evidences such as speech/object/context/action involvements in the cooperation are actually not mutually-independent [163].", "startOffset": 188, "endOffset": 193}, {"referenceID": 156, "context": "Dynamic Bayesian Network (DBN) explores temporal correlations by exploring sequential dependencies propagation p(xt|xt\u22121) among NLC-related features [165].", "startOffset": 149, "endOffset": 154}, {"referenceID": 157, "context": "A widely-used DBN algorithm in NLC is hidden Markov mode (HMM) algorithm [166], which uses a Markov chain assumption to explore the hidden influence of previous task-related features on the current NLC status.", "startOffset": 73, "endOffset": 78}, {"referenceID": 157, "context": "HMM uses both observation probabilities (absolute probability p(x)) and transitions abilities (conditional probability p(Y/X)) for modeling associations P(x, y) among NLC-related knowledge [166][167].", "startOffset": 189, "endOffset": 194}, {"referenceID": 158, "context": "HMM uses both observation probabilities (absolute probability p(x)) and transitions abilities (conditional probability p(Y/X)) for modeling associations P(x, y) among NLC-related knowledge [166][167].", "startOffset": 194, "endOffset": 199}, {"referenceID": 159, "context": "With HMM models, solved problems mainly include real-time human NL request understanding [168][169], dynamical human status identification [167][170], accurate gesture recognition by simultaneously fusing multi-view data such as NL instruction, shoulder coordinates, shoulders-elbows\u2019 3D angle data, and hand poses [166][171] etc.", "startOffset": 89, "endOffset": 94}, {"referenceID": 160, "context": "With HMM models, solved problems mainly include real-time human NL request understanding [168][169], dynamical human status identification [167][170], accurate gesture recognition by simultaneously fusing multi-view data such as NL instruction, shoulder coordinates, shoulders-elbows\u2019 3D angle data, and hand poses [166][171] etc.", "startOffset": 94, "endOffset": 99}, {"referenceID": 158, "context": "With HMM models, solved problems mainly include real-time human NL request understanding [168][169], dynamical human status identification [167][170], accurate gesture recognition by simultaneously fusing multi-view data such as NL instruction, shoulder coordinates, shoulders-elbows\u2019 3D angle data, and hand poses [166][171] etc.", "startOffset": 139, "endOffset": 144}, {"referenceID": 161, "context": "With HMM models, solved problems mainly include real-time human NL request understanding [168][169], dynamical human status identification [167][170], accurate gesture recognition by simultaneously fusing multi-view data such as NL instruction, shoulder coordinates, shoulders-elbows\u2019 3D angle data, and hand poses [166][171] etc.", "startOffset": 144, "endOffset": 149}, {"referenceID": 157, "context": "With HMM models, solved problems mainly include real-time human NL request understanding [168][169], dynamical human status identification [167][170], accurate gesture recognition by simultaneously fusing multi-view data such as NL instruction, shoulder coordinates, shoulders-elbows\u2019 3D angle data, and hand poses [166][171] etc.", "startOffset": 315, "endOffset": 320}, {"referenceID": 162, "context": "With HMM models, solved problems mainly include real-time human NL request understanding [168][169], dynamical human status identification [167][170], accurate gesture recognition by simultaneously fusing multi-view data such as NL instruction, shoulder coordinates, shoulders-elbows\u2019 3D angle data, and hand poses [166][171] etc.", "startOffset": 320, "endOffset": 325}, {"referenceID": 163, "context": "In a discriminative model, only the most distinctive aspects are collected from a relatively-small amount of data for identifying a task [172].", "startOffset": 137, "endOffset": 142}, {"referenceID": 164, "context": "Conditional-probabilistic BN is the simplest discriminative model with a single conditional relation p(y/x), modeling a probabilistic dependency between a human-desired cooperation task y and an evidence x such as a keyword and an action type [173].", "startOffset": 243, "endOffset": 248}, {"referenceID": 165, "context": "Typical procedures for exploring conditional dependencies are the following: first, intentionally assume implicit dependencies among a human-requested task and its potential evidences; then, conduct learning to compute dependency strengths; last, keep influential dependencies by filtering out dependences with weak strengths [174][175].", "startOffset": 326, "endOffset": 331}, {"referenceID": 166, "context": "Typical procedures for exploring conditional dependencies are the following: first, intentionally assume implicit dependencies among a human-requested task and its potential evidences; then, conduct learning to compute dependency strengths; last, keep influential dependencies by filtering out dependences with weak strengths [174][175].", "startOffset": 331, "endOffset": 336}, {"referenceID": 167, "context": "Typical solved problems include semantic meaning disambiguation such as object-feature dependencies [176], word-object mapping [7], word-sensor mapping [177], word-to-interpretation modeling [178] etc.", "startOffset": 100, "endOffset": 105}, {"referenceID": 6, "context": "Typical solved problems include semantic meaning disambiguation such as object-feature dependencies [176], word-object mapping [7], word-sensor mapping [177], word-to-interpretation modeling [178] etc.", "startOffset": 127, "endOffset": 130}, {"referenceID": 168, "context": "Typical solved problems include semantic meaning disambiguation such as object-feature dependencies [176], word-object mapping [7], word-sensor mapping [177], word-to-interpretation modeling [178] etc.", "startOffset": 152, "endOffset": 157}, {"referenceID": 169, "context": "Typical solved problems include semantic meaning disambiguation such as object-feature dependencies [176], word-object mapping [7], word-sensor mapping [177], word-to-interpretation modeling [178] etc.", "startOffset": 191, "endOffset": 196}, {"referenceID": 158, "context": "(a) [167] and (b) [155] are generative models.", "startOffset": 4, "endOffset": 9}, {"referenceID": 147, "context": "(a) [167] and (b) [155] are generative models.", "startOffset": 18, "endOffset": 23}, {"referenceID": 165, "context": "(c) [174] and (d) [136] are discriminative models.", "startOffset": 4, "endOffset": 9}, {"referenceID": 129, "context": "(c) [174] and (d) [136] are discriminative models.", "startOffset": 18, "endOffset": 23}, {"referenceID": 170, "context": "Typical solved problems include accurate intention inferences by combining motion/vision/speech information [179][180], robotic task executions using motion/speech commands [181][182], human NL request disambiguation [171][183] etc.", "startOffset": 108, "endOffset": 113}, {"referenceID": 171, "context": "Typical solved problems include accurate intention inferences by combining motion/vision/speech information [179][180], robotic task executions using motion/speech commands [181][182], human NL request disambiguation [171][183] etc.", "startOffset": 113, "endOffset": 118}, {"referenceID": 172, "context": "Typical solved problems include accurate intention inferences by combining motion/vision/speech information [179][180], robotic task executions using motion/speech commands [181][182], human NL request disambiguation [171][183] etc.", "startOffset": 173, "endOffset": 178}, {"referenceID": 173, "context": "Typical solved problems include accurate intention inferences by combining motion/vision/speech information [179][180], robotic task executions using motion/speech commands [181][182], human NL request disambiguation [171][183] etc.", "startOffset": 178, "endOffset": 183}, {"referenceID": 162, "context": "Typical solved problems include accurate intention inferences by combining motion/vision/speech information [179][180], robotic task executions using motion/speech commands [181][182], human NL request disambiguation [171][183] etc.", "startOffset": 217, "endOffset": 222}, {"referenceID": 174, "context": "Typical solved problems include accurate intention inferences by combining motion/vision/speech information [179][180], robotic task executions using motion/speech commands [181][182], human NL request disambiguation [171][183] etc.", "startOffset": 222, "endOffset": 227}, {"referenceID": 26, "context": "x) [27].", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "The environment adaptability of a taskrepresentation model is improved [15][27].", "startOffset": 71, "endOffset": 75}, {"referenceID": 26, "context": "The environment adaptability of a taskrepresentation model is improved [15][27].", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": "Typical solved problems include intuitive object usages by defining object categories and replacing the missing object by the same-semantic-category object [15][184], complex executionprocedure modeling by integrating crucial execution components, such as task execution procedures, objects and actions, and detailed execution parameters, such as speed, location, action and tool usage, into an ontology tree [185][186], intelligent robot control in which a control goal was decomposed into sub-strategies such as {notActive, run, hold done, .", "startOffset": 156, "endOffset": 160}, {"referenceID": 175, "context": "Typical solved problems include intuitive object usages by defining object categories and replacing the missing object by the same-semantic-category object [15][184], complex executionprocedure modeling by integrating crucial execution components, such as task execution procedures, objects and actions, and detailed execution parameters, such as speed, location, action and tool usage, into an ontology tree [185][186], intelligent robot control in which a control goal was decomposed into sub-strategies such as {notActive, run, hold done, .", "startOffset": 160, "endOffset": 165}, {"referenceID": 176, "context": "Typical solved problems include intuitive object usages by defining object categories and replacing the missing object by the same-semantic-category object [15][184], complex executionprocedure modeling by integrating crucial execution components, such as task execution procedures, objects and actions, and detailed execution parameters, such as speed, location, action and tool usage, into an ontology tree [185][186], intelligent robot control in which a control goal was decomposed into sub-strategies such as {notActive, run, hold done, .", "startOffset": 409, "endOffset": 414}, {"referenceID": 177, "context": "Typical solved problems include intuitive object usages by defining object categories and replacing the missing object by the same-semantic-category object [15][184], complex executionprocedure modeling by integrating crucial execution components, such as task execution procedures, objects and actions, and detailed execution parameters, such as speed, location, action and tool usage, into an ontology tree [185][186], intelligent robot control in which a control goal was decomposed into sub-strategies such as {notActive, run, hold done, .", "startOffset": 414, "endOffset": 419}, {"referenceID": 105, "context": "} and each strategy was endowed with ontologies constrains such as robot motion statuses, robot physical capabilities, and human safety considerations [112][187], human NL command disambiguation by looking up ambiguous words in an ontology tree to get extra semantic evidences, such as \u201cCup-UnitOfVolume, action: put, teaspoon-UnitOfVolume\u201d [27][78], intuitive robot conceptualized programming, such as telling the robot to focus on", "startOffset": 151, "endOffset": 156}, {"referenceID": 178, "context": "} and each strategy was endowed with ontologies constrains such as robot motion statuses, robot physical capabilities, and human safety considerations [112][187], human NL command disambiguation by looking up ambiguous words in an ontology tree to get extra semantic evidences, such as \u201cCup-UnitOfVolume, action: put, teaspoon-UnitOfVolume\u201d [27][78], intuitive robot conceptualized programming, such as telling the robot to focus on", "startOffset": 156, "endOffset": 161}, {"referenceID": 26, "context": "} and each strategy was endowed with ontologies constrains such as robot motion statuses, robot physical capabilities, and human safety considerations [112][187], human NL command disambiguation by looking up ambiguous words in an ontology tree to get extra semantic evidences, such as \u201cCup-UnitOfVolume, action: put, teaspoon-UnitOfVolume\u201d [27][78], intuitive robot conceptualized programming, such as telling the robot to focus on", "startOffset": 341, "endOffset": 345}, {"referenceID": 77, "context": "} and each strategy was endowed with ontologies constrains such as robot motion statuses, robot physical capabilities, and human safety considerations [112][187], human NL command disambiguation by looking up ambiguous words in an ontology tree to get extra semantic evidences, such as \u201cCup-UnitOfVolume, action: put, teaspoon-UnitOfVolume\u201d [27][78], intuitive robot conceptualized programming, such as telling the robot to focus on", "startOffset": 345, "endOffset": 349}, {"referenceID": 21, "context": "\u201d, in an ontology tree [22][109], imitating human execution manners by defining both execution procedures and sensor parameters such as \u201clocation, speed, velocity, .", "startOffset": 23, "endOffset": 27}, {"referenceID": 102, "context": "\u201d, in an ontology tree [22][109], imitating human execution manners by defining both execution procedures and sensor parameters such as \u201clocation, speed, velocity, .", "startOffset": 27, "endOffset": 32}, {"referenceID": 179, "context": "\u201d in an ontology tree [188][189], autonomous manufacturing by defining abstract manufacturing plans such as assembly plan { Task/skill/process\u2014assemblyTask/GenericTask/WoodworkingTask/weldingTask\u2014assemblePartsTas/pickTask} in an ontology tree to simplify user involvements [190][191], environment perceiving by using ontology interpretations on the 3D point cloud [192][193] etc.", "startOffset": 22, "endOffset": 27}, {"referenceID": 180, "context": "\u201d in an ontology tree [188][189], autonomous manufacturing by defining abstract manufacturing plans such as assembly plan { Task/skill/process\u2014assemblyTask/GenericTask/WoodworkingTask/weldingTask\u2014assemblePartsTas/pickTask} in an ontology tree to simplify user involvements [190][191], environment perceiving by using ontology interpretations on the 3D point cloud [192][193] etc.", "startOffset": 27, "endOffset": 32}, {"referenceID": 181, "context": "\u201d in an ontology tree [188][189], autonomous manufacturing by defining abstract manufacturing plans such as assembly plan { Task/skill/process\u2014assemblyTask/GenericTask/WoodworkingTask/weldingTask\u2014assemblePartsTas/pickTask} in an ontology tree to simplify user involvements [190][191], environment perceiving by using ontology interpretations on the 3D point cloud [192][193] etc.", "startOffset": 273, "endOffset": 278}, {"referenceID": 182, "context": "\u201d in an ontology tree [188][189], autonomous manufacturing by defining abstract manufacturing plans such as assembly plan { Task/skill/process\u2014assemblyTask/GenericTask/WoodworkingTask/weldingTask\u2014assemblePartsTas/pickTask} in an ontology tree to simplify user involvements [190][191], environment perceiving by using ontology interpretations on the 3D point cloud [192][193] etc.", "startOffset": 278, "endOffset": 283}, {"referenceID": 183, "context": "\u201d in an ontology tree [188][189], autonomous manufacturing by defining abstract manufacturing plans such as assembly plan { Task/skill/process\u2014assemblyTask/GenericTask/WoodworkingTask/weldingTask\u2014assemblePartsTas/pickTask} in an ontology tree to simplify user involvements [190][191], environment perceiving by using ontology interpretations on the 3D point cloud [192][193] etc.", "startOffset": 364, "endOffset": 369}, {"referenceID": 184, "context": "\u201d in an ontology tree [188][189], autonomous manufacturing by defining abstract manufacturing plans such as assembly plan { Task/skill/process\u2014assemblyTask/GenericTask/WoodworkingTask/weldingTask\u2014assemblePartsTas/pickTask} in an ontology tree to simplify user involvements [190][191], environment perceiving by using ontology interpretations on the 3D point cloud [192][193] etc.", "startOffset": 369, "endOffset": 374}, {"referenceID": 185, "context": "formulas, such as \u201cin possible worlds a kitchen is a region (\u2200w\u2200x(kitchen(w,x) \u2192 region(w,x)))\u201d [194].", "startOffset": 96, "endOffset": 101}, {"referenceID": 176, "context": "(a) [185] is an ontology model, in which an assembly task \u201cbearing assembly\u201d is decomposed into several layers of sub-steps such as \u201cbearing + pipe, bearing 2 + tree, .", "startOffset": 4, "endOffset": 9}, {"referenceID": 32, "context": "(b) [33] is a first-order-logic model, in which NLC tasks such as \u201cfinding a cup\u201d is decomposed into different logic constraints such as \u201ccupAction + graspingPose \u2192 detectCup success\u201d.", "startOffset": 4, "endOffset": 8}, {"referenceID": 107, "context": "(c) [114] is also an first-order-logic model, in which logic relations such as \u201cmove=(grasp, place), .", "startOffset": 4, "endOffset": 9}, {"referenceID": 139, "context": "Typical solved problems include autonomous robot navigation by using logic navigation sequences, such as going to a location \u201challway\u201d then going to a new location \u201crest room\u201d [147][149][194], environment uncertainty modeling by summarizing potential executions such as \u201cground atoms (boolean random variables) eats(Dominik, Cereals), uses(Dominik, Bowl), eats(Michael, Cereals) and uses(Michael, Bowl)\u201d [195], robot action control by defining action-usage logics such as \u201cmove (grasp piece(location, grip), place piece(location, ungrip))\u201d [147][196][197][198], autonomous failure analysis by looking up first-order logic representations to detect the missing knowledge such as \u201ctool brush, action: sweep\u201d [7][198], and NL-based robot programming by using the grammar language such as point(object, arm-side), lookat(object), and rotate(rot-dir, arm-side) [130].", "startOffset": 176, "endOffset": 181}, {"referenceID": 141, "context": "Typical solved problems include autonomous robot navigation by using logic navigation sequences, such as going to a location \u201challway\u201d then going to a new location \u201crest room\u201d [147][149][194], environment uncertainty modeling by summarizing potential executions such as \u201cground atoms (boolean random variables) eats(Dominik, Cereals), uses(Dominik, Bowl), eats(Michael, Cereals) and uses(Michael, Bowl)\u201d [195], robot action control by defining action-usage logics such as \u201cmove (grasp piece(location, grip), place piece(location, ungrip))\u201d [147][196][197][198], autonomous failure analysis by looking up first-order logic representations to detect the missing knowledge such as \u201ctool brush, action: sweep\u201d [7][198], and NL-based robot programming by using the grammar language such as point(object, arm-side), lookat(object), and rotate(rot-dir, arm-side) [130].", "startOffset": 181, "endOffset": 186}, {"referenceID": 185, "context": "Typical solved problems include autonomous robot navigation by using logic navigation sequences, such as going to a location \u201challway\u201d then going to a new location \u201crest room\u201d [147][149][194], environment uncertainty modeling by summarizing potential executions such as \u201cground atoms (boolean random variables) eats(Dominik, Cereals), uses(Dominik, Bowl), eats(Michael, Cereals) and uses(Michael, Bowl)\u201d [195], robot action control by defining action-usage logics such as \u201cmove (grasp piece(location, grip), place piece(location, ungrip))\u201d [147][196][197][198], autonomous failure analysis by looking up first-order logic representations to detect the missing knowledge such as \u201ctool brush, action: sweep\u201d [7][198], and NL-based robot programming by using the grammar language such as point(object, arm-side), lookat(object), and rotate(rot-dir, arm-side) [130].", "startOffset": 186, "endOffset": 191}, {"referenceID": 186, "context": "Typical solved problems include autonomous robot navigation by using logic navigation sequences, such as going to a location \u201challway\u201d then going to a new location \u201crest room\u201d [147][149][194], environment uncertainty modeling by summarizing potential executions such as \u201cground atoms (boolean random variables) eats(Dominik, Cereals), uses(Dominik, Bowl), eats(Michael, Cereals) and uses(Michael, Bowl)\u201d [195], robot action control by defining action-usage logics such as \u201cmove (grasp piece(location, grip), place piece(location, ungrip))\u201d [147][196][197][198], autonomous failure analysis by looking up first-order logic representations to detect the missing knowledge such as \u201ctool brush, action: sweep\u201d [7][198], and NL-based robot programming by using the grammar language such as point(object, arm-side), lookat(object), and rotate(rot-dir, arm-side) [130].", "startOffset": 404, "endOffset": 409}, {"referenceID": 139, "context": "Typical solved problems include autonomous robot navigation by using logic navigation sequences, such as going to a location \u201challway\u201d then going to a new location \u201crest room\u201d [147][149][194], environment uncertainty modeling by summarizing potential executions such as \u201cground atoms (boolean random variables) eats(Dominik, Cereals), uses(Dominik, Bowl), eats(Michael, Cereals) and uses(Michael, Bowl)\u201d [195], robot action control by defining action-usage logics such as \u201cmove (grasp piece(location, grip), place piece(location, ungrip))\u201d [147][196][197][198], autonomous failure analysis by looking up first-order logic representations to detect the missing knowledge such as \u201ctool brush, action: sweep\u201d [7][198], and NL-based robot programming by using the grammar language such as point(object, arm-side), lookat(object), and rotate(rot-dir, arm-side) [130].", "startOffset": 540, "endOffset": 545}, {"referenceID": 187, "context": "Typical solved problems include autonomous robot navigation by using logic navigation sequences, such as going to a location \u201challway\u201d then going to a new location \u201crest room\u201d [147][149][194], environment uncertainty modeling by summarizing potential executions such as \u201cground atoms (boolean random variables) eats(Dominik, Cereals), uses(Dominik, Bowl), eats(Michael, Cereals) and uses(Michael, Bowl)\u201d [195], robot action control by defining action-usage logics such as \u201cmove (grasp piece(location, grip), place piece(location, ungrip))\u201d [147][196][197][198], autonomous failure analysis by looking up first-order logic representations to detect the missing knowledge such as \u201ctool brush, action: sweep\u201d [7][198], and NL-based robot programming by using the grammar language such as point(object, arm-side), lookat(object), and rotate(rot-dir, arm-side) [130].", "startOffset": 545, "endOffset": 550}, {"referenceID": 188, "context": "Typical solved problems include autonomous robot navigation by using logic navigation sequences, such as going to a location \u201challway\u201d then going to a new location \u201crest room\u201d [147][149][194], environment uncertainty modeling by summarizing potential executions such as \u201cground atoms (boolean random variables) eats(Dominik, Cereals), uses(Dominik, Bowl), eats(Michael, Cereals) and uses(Michael, Bowl)\u201d [195], robot action control by defining action-usage logics such as \u201cmove (grasp piece(location, grip), place piece(location, ungrip))\u201d [147][196][197][198], autonomous failure analysis by looking up first-order logic representations to detect the missing knowledge such as \u201ctool brush, action: sweep\u201d [7][198], and NL-based robot programming by using the grammar language such as point(object, arm-side), lookat(object), and rotate(rot-dir, arm-side) [130].", "startOffset": 550, "endOffset": 555}, {"referenceID": 189, "context": "Typical solved problems include autonomous robot navigation by using logic navigation sequences, such as going to a location \u201challway\u201d then going to a new location \u201crest room\u201d [147][149][194], environment uncertainty modeling by summarizing potential executions such as \u201cground atoms (boolean random variables) eats(Dominik, Cereals), uses(Dominik, Bowl), eats(Michael, Cereals) and uses(Michael, Bowl)\u201d [195], robot action control by defining action-usage logics such as \u201cmove (grasp piece(location, grip), place piece(location, ungrip))\u201d [147][196][197][198], autonomous failure analysis by looking up first-order logic representations to detect the missing knowledge such as \u201ctool brush, action: sweep\u201d [7][198], and NL-based robot programming by using the grammar language such as point(object, arm-side), lookat(object), and rotate(rot-dir, arm-side) [130].", "startOffset": 555, "endOffset": 560}, {"referenceID": 6, "context": "Typical solved problems include autonomous robot navigation by using logic navigation sequences, such as going to a location \u201challway\u201d then going to a new location \u201crest room\u201d [147][149][194], environment uncertainty modeling by summarizing potential executions such as \u201cground atoms (boolean random variables) eats(Dominik, Cereals), uses(Dominik, Bowl), eats(Michael, Cereals) and uses(Michael, Bowl)\u201d [195], robot action control by defining action-usage logics such as \u201cmove (grasp piece(location, grip), place piece(location, ungrip))\u201d [147][196][197][198], autonomous failure analysis by looking up first-order logic representations to detect the missing knowledge such as \u201ctool brush, action: sweep\u201d [7][198], and NL-based robot programming by using the grammar language such as point(object, arm-side), lookat(object), and rotate(rot-dir, arm-side) [130].", "startOffset": 706, "endOffset": 709}, {"referenceID": 189, "context": "Typical solved problems include autonomous robot navigation by using logic navigation sequences, such as going to a location \u201challway\u201d then going to a new location \u201crest room\u201d [147][149][194], environment uncertainty modeling by summarizing potential executions such as \u201cground atoms (boolean random variables) eats(Dominik, Cereals), uses(Dominik, Bowl), eats(Michael, Cereals) and uses(Michael, Bowl)\u201d [195], robot action control by defining action-usage logics such as \u201cmove (grasp piece(location, grip), place piece(location, ungrip))\u201d [147][196][197][198], autonomous failure analysis by looking up first-order logic representations to detect the missing knowledge such as \u201ctool brush, action: sweep\u201d [7][198], and NL-based robot programming by using the grammar language such as point(object, arm-side), lookat(object), and rotate(rot-dir, arm-side) [130].", "startOffset": 709, "endOffset": 714}, {"referenceID": 123, "context": "Typical solved problems include autonomous robot navigation by using logic navigation sequences, such as going to a location \u201challway\u201d then going to a new location \u201crest room\u201d [147][149][194], environment uncertainty modeling by summarizing potential executions such as \u201cground atoms (boolean random variables) eats(Dominik, Cereals), uses(Dominik, Bowl), eats(Michael, Cereals) and uses(Michael, Bowl)\u201d [195], robot action control by defining action-usage logics such as \u201cmove (grasp piece(location, grip), place piece(location, ungrip))\u201d [147][196][197][198], autonomous failure analysis by looking up first-order logic representations to detect the missing knowledge such as \u201ctool brush, action: sweep\u201d [7][198], and NL-based robot programming by using the grammar language such as point(object, arm-side), lookat(object), and rotate(rot-dir, arm-side) [130].", "startOffset": 856, "endOffset": 861}, {"referenceID": 190, "context": "Typical solved problems include modeling motion statuses such as \u201cquick, slow\u201d by interpreting sensor data [199], sophisticated robot-manipulation control by interpreting NL control commands, such as \u201cmove little right, bend your elbow little\u201d, into fuzzy execution value range [112], emotion modeling by classifying Fig.", "startOffset": 107, "endOffset": 112}, {"referenceID": 105, "context": "Typical solved problems include modeling motion statuses such as \u201cquick, slow\u201d by interpreting sensor data [199], sophisticated robot-manipulation control by interpreting NL control commands, such as \u201cmove little right, bend your elbow little\u201d, into fuzzy execution value range [112], emotion modeling by classifying Fig.", "startOffset": 278, "endOffset": 283}, {"referenceID": 191, "context": "(a) [200] is a fuzzy emotion model, in which human\u2019s motion in NL expressions have been defined as fuzzy statuses \u201cvery low happy, vey high happy, .", "startOffset": 4, "endOffset": 9}, {"referenceID": 182, "context": "(b) [191] is a weighted logic model, in which human\u2019s cognitive process in decision making is simulated by logics with different influence weights, based on which important logics with larger weights could be emphasized and trivial logics with smaller weights could be ignored.", "startOffset": 4, "endOffset": 9}, {"referenceID": 191, "context": "submitted to Knowledge-based Systems, January 2017 emotion statuses such as happy {very low, low, medium, high, very high} in a fuzzy manner [200] etc.", "startOffset": 141, "endOffset": 146}, {"referenceID": 6, "context": "9Task(mes1,mes2)\u201d [7], imitating human cognition process in task planning.", "startOffset": 18, "endOffset": 21}, {"referenceID": 182, "context": "Typical solved problems include using MLN to generate a flexible and intuitive machine-executable plan from human NL instructions for autonomous industrial task execution [191], autonomous task executions in uncertain environments by using MLN to meet constraints from both robots\u2019 knowledge availability and real-world\u2019s knowledge requirements [195][201] etc.", "startOffset": 171, "endOffset": 176}, {"referenceID": 186, "context": "Typical solved problems include using MLN to generate a flexible and intuitive machine-executable plan from human NL instructions for autonomous industrial task execution [191], autonomous task executions in uncertain environments by using MLN to meet constraints from both robots\u2019 knowledge availability and real-world\u2019s knowledge requirements [195][201] etc.", "startOffset": 345, "endOffset": 350}, {"referenceID": 192, "context": "Typical solved problems include using MLN to generate a flexible and intuitive machine-executable plan from human NL instructions for autonomous industrial task execution [191], autonomous task executions in uncertain environments by using MLN to meet constraints from both robots\u2019 knowledge availability and real-world\u2019s knowledge requirements [195][201] etc.", "startOffset": 350, "endOffset": 355}, {"referenceID": 186, "context": "One shortcoming is that cognitive process simulation is still not cognitive process because the fundamental theory of cognitive process modeling is lacking, being insufficient to support a human-like task execution process [195].", "startOffset": 223, "endOffset": 228}, {"referenceID": 193, "context": "Different individuals have difference cognitive process, thus making it difficult to evaluate the reasonability of learned cognitive models [202].", "startOffset": 140, "endOffset": 145}, {"referenceID": 147, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 19, "endOffset": 24}, {"referenceID": 148, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 24, "endOffset": 29}, {"referenceID": 149, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 29, "endOffset": 34}, {"referenceID": 151, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 35, "endOffset": 40}, {"referenceID": 152, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 40, "endOffset": 45}, {"referenceID": 163, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 46, "endOffset": 51}, {"referenceID": 164, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 51, "endOffset": 56}, {"referenceID": 165, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 56, "endOffset": 61}, {"referenceID": 166, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 62, "endOffset": 67}, {"referenceID": 167, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 67, "endOffset": 72}, {"referenceID": 175, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 73, "endOffset": 78}, {"referenceID": 176, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 78, "endOffset": 83}, {"referenceID": 177, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 83, "endOffset": 88}, {"referenceID": 178, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 89, "endOffset": 94}, {"referenceID": 179, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 94, "endOffset": 99}, {"referenceID": 139, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 100, "endOffset": 105}, {"referenceID": 141, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 105, "endOffset": 110}, {"referenceID": 185, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 110, "endOffset": 115}, {"referenceID": 187, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 116, "endOffset": 121}, {"referenceID": 188, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 121, "endOffset": 126}, {"referenceID": 6, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 127, "endOffset": 130}, {"referenceID": 190, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 130, "endOffset": 135}, {"referenceID": 105, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 135, "endOffset": 140}, {"referenceID": 191, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 141, "endOffset": 146}, {"referenceID": 182, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 147, "endOffset": 152}, {"referenceID": 186, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 152, "endOffset": 157}, {"referenceID": 192, "context": "Typical references [155][156][157] [159][160] [172][173][174] [175][176] [184][185][186] [187][188] [147][149][194] [196][197] [7][199][112] [200] [191][195] [201]", "startOffset": 158, "endOffset": 163}, {"referenceID": 194, "context": "Typical solved problems mainly include word-action associations such as word \u201cpick\u201d \u2013 action \u201cPick\u201d [203][204], direction instruction-motion behavior correlations such as word \u201cleft-turning\u201d \u2013 motion behavior \u201cLEFT-TURNING\u201d [19][137], word-object/building/person/location mappings such as \u201cperson-Person, trashcanTRASHCAN, hallway-HALLWAY\u201d [181][205] etc.", "startOffset": 100, "endOffset": 105}, {"referenceID": 195, "context": "Typical solved problems mainly include word-action associations such as word \u201cpick\u201d \u2013 action \u201cPick\u201d [203][204], direction instruction-motion behavior correlations such as word \u201cleft-turning\u201d \u2013 motion behavior \u201cLEFT-TURNING\u201d [19][137], word-object/building/person/location mappings such as \u201cperson-Person, trashcanTRASHCAN, hallway-HALLWAY\u201d [181][205] etc.", "startOffset": 105, "endOffset": 110}, {"referenceID": 18, "context": "Typical solved problems mainly include word-action associations such as word \u201cpick\u201d \u2013 action \u201cPick\u201d [203][204], direction instruction-motion behavior correlations such as word \u201cleft-turning\u201d \u2013 motion behavior \u201cLEFT-TURNING\u201d [19][137], word-object/building/person/location mappings such as \u201cperson-Person, trashcanTRASHCAN, hallway-HALLWAY\u201d [181][205] etc.", "startOffset": 224, "endOffset": 228}, {"referenceID": 130, "context": "Typical solved problems mainly include word-action associations such as word \u201cpick\u201d \u2013 action \u201cPick\u201d [203][204], direction instruction-motion behavior correlations such as word \u201cleft-turning\u201d \u2013 motion behavior \u201cLEFT-TURNING\u201d [19][137], word-object/building/person/location mappings such as \u201cperson-Person, trashcanTRASHCAN, hallway-HALLWAY\u201d [181][205] etc.", "startOffset": 228, "endOffset": 233}, {"referenceID": 172, "context": "Typical solved problems mainly include word-action associations such as word \u201cpick\u201d \u2013 action \u201cPick\u201d [203][204], direction instruction-motion behavior correlations such as word \u201cleft-turning\u201d \u2013 motion behavior \u201cLEFT-TURNING\u201d [19][137], word-object/building/person/location mappings such as \u201cperson-Person, trashcanTRASHCAN, hallway-HALLWAY\u201d [181][205] etc.", "startOffset": 340, "endOffset": 345}, {"referenceID": 196, "context": "Typical solved problems mainly include word-action associations such as word \u201cpick\u201d \u2013 action \u201cPick\u201d [203][204], direction instruction-motion behavior correlations such as word \u201cleft-turning\u201d \u2013 motion behavior \u201cLEFT-TURNING\u201d [19][137], word-object/building/person/location mappings such as \u201cperson-Person, trashcanTRASHCAN, hallway-HALLWAY\u201d [181][205] etc.", "startOffset": 345, "endOffset": 350}, {"referenceID": 197, "context": "Typical solved problems include: indoor routine identifying by using landmark locations such as \u201ckitchen, lobby\u201d [206]; object searching by using visual properties such as object color, size and shape [207]; motion execution such as \u201cpick up the tire pallet\u201d by defining action sequence \u201cdrive \u2013 insert \u2013 raise \u2013 drive \u2013 set\u201d [178]; NLC scene understanding such as \u201clounge, lab, conference room\u201d by checking spatial-semantic distributions of landmarks such as \u201challway, gym, .", "startOffset": 113, "endOffset": 118}, {"referenceID": 198, "context": "Typical solved problems include: indoor routine identifying by using landmark locations such as \u201ckitchen, lobby\u201d [206]; object searching by using visual properties such as object color, size and shape [207]; motion execution such as \u201cpick up the tire pallet\u201d by defining action sequence \u201cdrive \u2013 insert \u2013 raise \u2013 drive \u2013 set\u201d [178]; NLC scene understanding such as \u201clounge, lab, conference room\u201d by checking spatial-semantic distributions of landmarks such as \u201challway, gym, .", "startOffset": 201, "endOffset": 206}, {"referenceID": 169, "context": "Typical solved problems include: indoor routine identifying by using landmark locations such as \u201ckitchen, lobby\u201d [206]; object searching by using visual properties such as object color, size and shape [207]; motion execution such as \u201cpick up the tire pallet\u201d by defining action sequence \u201cdrive \u2013 insert \u2013 raise \u2013 drive \u2013 set\u201d [178]; NLC scene understanding such as \u201clounge, lab, conference room\u201d by checking spatial-semantic distributions of landmarks such as \u201challway, gym, .", "startOffset": 326, "endOffset": 331}, {"referenceID": 199, "context": "\u201d [208] etc.", "startOffset": 2, "endOffset": 7}, {"referenceID": 117, "context": "1 m/s, far \u2013 distance larger than 1 m\u201d [124][209], quantitative spatial relation interpretations such as \u201cobject 1 is at the left of an object b\u201d for two objects [37][210], and semantic interpretations such as goal \u201cdrill\u201d is interpreted as \u201cdrill (action: put down, drill, lift; precondition: no hole existing; requirements: slowly; tool: driller; .", "startOffset": 39, "endOffset": 44}, {"referenceID": 200, "context": "1 m/s, far \u2013 distance larger than 1 m\u201d [124][209], quantitative spatial relation interpretations such as \u201cobject 1 is at the left of an object b\u201d for two objects [37][210], and semantic interpretations such as goal \u201cdrill\u201d is interpreted as \u201cdrill (action: put down, drill, lift; precondition: no hole existing; requirements: slowly; tool: driller; .", "startOffset": 44, "endOffset": 49}, {"referenceID": 36, "context": "1 m/s, far \u2013 distance larger than 1 m\u201d [124][209], quantitative spatial relation interpretations such as \u201cobject 1 is at the left of an object b\u201d for two objects [37][210], and semantic interpretations such as goal \u201cdrill\u201d is interpreted as \u201cdrill (action: put down, drill, lift; precondition: no hole existing; requirements: slowly; tool: driller; .", "startOffset": 162, "endOffset": 166}, {"referenceID": 201, "context": "1 m/s, far \u2013 distance larger than 1 m\u201d [124][209], quantitative spatial relation interpretations such as \u201cobject 1 is at the left of an object b\u201d for two objects [37][210], and semantic interpretations such as goal \u201cdrill\u201d is interpreted as \u201cdrill (action: put down, drill, lift; precondition: no hole existing; requirements: slowly; tool: driller; .", "startOffset": 166, "endOffset": 171}, {"referenceID": 182, "context": ")\u201d [191][211] etc.", "startOffset": 3, "endOffset": 8}, {"referenceID": 202, "context": ")\u201d [191][211] etc.", "startOffset": 8, "endOffset": 13}, {"referenceID": 203, "context": "Environment gaps, which are constraints such as tool availability and space/location limitations imposed by unfamiliar environments [212][213], 2).", "startOffset": 132, "endOffset": 137}, {"referenceID": 204, "context": "Environment gaps, which are constraints such as tool availability and space/location limitations imposed by unfamiliar environments [212][213], 2).", "startOffset": 137, "endOffset": 142}, {"referenceID": 16, "context": "Robot gaps, which are constraints such as robot physical structure strength, capable actions and operation precision [17][212][214], and 3) user gaps, which are missing information caused by abstract/ambiguous/incomplete human NL instructions [8][135].", "startOffset": 117, "endOffset": 121}, {"referenceID": 203, "context": "Robot gaps, which are constraints such as robot physical structure strength, capable actions and operation precision [17][212][214], and 3) user gaps, which are missing information caused by abstract/ambiguous/incomplete human NL instructions [8][135].", "startOffset": 121, "endOffset": 126}, {"referenceID": 205, "context": "Robot gaps, which are constraints such as robot physical structure strength, capable actions and operation precision [17][212][214], and 3) user gaps, which are missing information caused by abstract/ambiguous/incomplete human NL instructions [8][135].", "startOffset": 126, "endOffset": 131}, {"referenceID": 7, "context": "Robot gaps, which are constraints such as robot physical structure strength, capable actions and operation precision [17][212][214], and 3) user gaps, which are missing information caused by abstract/ambiguous/incomplete human NL instructions [8][135].", "startOffset": 243, "endOffset": 246}, {"referenceID": 128, "context": "Robot gaps, which are constraints such as robot physical structure strength, capable actions and operation precision [17][212][214], and 3) user gaps, which are missing information caused by abstract/ambiguous/incomplete human NL instructions [8][135].", "startOffset": 246, "endOffset": 251}, {"referenceID": 15, "context": "(a) [16] is a direct symbol mapping method, by which predefined motion behaviors such as \u201cavoiding, bowing, carring, .", "startOffset": 4, "endOffset": 8}, {"referenceID": 197, "context": "(b) [206] is a general property mapping method, by which special features such as \u201ckitchen location, lab locations, .", "startOffset": 4, "endOffset": 9}, {"referenceID": 36, "context": "(c) [37] is an interpretation mapping method, by which a navigation task is specifically interpreted by real-world conditions such as \u201caction: observe.", "startOffset": 4, "endOffset": 8}, {"referenceID": 111, "context": "submitted to Knowledge-based Systems, January 2017 parameters defined in a hierarchical knowledge structure [118][214], 2) knowledge-applicability assessment, which detects knowledge gaps by checking the similarities between theoretical scenarios and real-world scenarios [138][214], and 3) performance-triggered knowledge gap estimation, which detects knowledge gaps by considering the final execution performances [8][187].", "startOffset": 108, "endOffset": 113}, {"referenceID": 205, "context": "submitted to Knowledge-based Systems, January 2017 parameters defined in a hierarchical knowledge structure [118][214], 2) knowledge-applicability assessment, which detects knowledge gaps by checking the similarities between theoretical scenarios and real-world scenarios [138][214], and 3) performance-triggered knowledge gap estimation, which detects knowledge gaps by considering the final execution performances [8][187].", "startOffset": 113, "endOffset": 118}, {"referenceID": 131, "context": "submitted to Knowledge-based Systems, January 2017 parameters defined in a hierarchical knowledge structure [118][214], 2) knowledge-applicability assessment, which detects knowledge gaps by checking the similarities between theoretical scenarios and real-world scenarios [138][214], and 3) performance-triggered knowledge gap estimation, which detects knowledge gaps by considering the final execution performances [8][187].", "startOffset": 272, "endOffset": 277}, {"referenceID": 205, "context": "submitted to Knowledge-based Systems, January 2017 parameters defined in a hierarchical knowledge structure [118][214], 2) knowledge-applicability assessment, which detects knowledge gaps by checking the similarities between theoretical scenarios and real-world scenarios [138][214], and 3) performance-triggered knowledge gap estimation, which detects knowledge gaps by considering the final execution performances [8][187].", "startOffset": 277, "endOffset": 282}, {"referenceID": 7, "context": "submitted to Knowledge-based Systems, January 2017 parameters defined in a hierarchical knowledge structure [118][214], 2) knowledge-applicability assessment, which detects knowledge gaps by checking the similarities between theoretical scenarios and real-world scenarios [138][214], and 3) performance-triggered knowledge gap estimation, which detects knowledge gaps by considering the final execution performances [8][187].", "startOffset": 416, "endOffset": 419}, {"referenceID": 178, "context": "submitted to Knowledge-based Systems, January 2017 parameters defined in a hierarchical knowledge structure [118][214], 2) knowledge-applicability assessment, which detects knowledge gaps by checking the similarities between theoretical scenarios and real-world scenarios [138][214], and 3) performance-triggered knowledge gap estimation, which detects knowledge gaps by considering the final execution performances [8][187].", "startOffset": 419, "endOffset": 424}, {"referenceID": 6, "context": "Gap filling methods mainly include: using existing alternative knowledge such as \u201cbrush\u201d in robot knowledge base to replace inappropriate knowledge \u201cvacuum cleaner\u201d in NLC tasks such as \u201cclean a surface\u201d [7][187], using general commonsense knowledge \u201cdrilling action needs driller\u201d in a robot database to satisfy the need for a specific type of knowledge such as \u201ctool for drilling a hole in the install a screw task\u201d [187][214], asking knowledge input from human users by proactively asking questions such as \u201cwhere is the table leg\u201d [8][215][216], autonomously learning from the internet [8][215][216] etc.", "startOffset": 204, "endOffset": 207}, {"referenceID": 178, "context": "Gap filling methods mainly include: using existing alternative knowledge such as \u201cbrush\u201d in robot knowledge base to replace inappropriate knowledge \u201cvacuum cleaner\u201d in NLC tasks such as \u201cclean a surface\u201d [7][187], using general commonsense knowledge \u201cdrilling action needs driller\u201d in a robot database to satisfy the need for a specific type of knowledge such as \u201ctool for drilling a hole in the install a screw task\u201d [187][214], asking knowledge input from human users by proactively asking questions such as \u201cwhere is the table leg\u201d [8][215][216], autonomously learning from the internet [8][215][216] etc.", "startOffset": 207, "endOffset": 212}, {"referenceID": 178, "context": "Gap filling methods mainly include: using existing alternative knowledge such as \u201cbrush\u201d in robot knowledge base to replace inappropriate knowledge \u201cvacuum cleaner\u201d in NLC tasks such as \u201cclean a surface\u201d [7][187], using general commonsense knowledge \u201cdrilling action needs driller\u201d in a robot database to satisfy the need for a specific type of knowledge such as \u201ctool for drilling a hole in the install a screw task\u201d [187][214], asking knowledge input from human users by proactively asking questions such as \u201cwhere is the table leg\u201d [8][215][216], autonomously learning from the internet [8][215][216] etc.", "startOffset": 418, "endOffset": 423}, {"referenceID": 205, "context": "Gap filling methods mainly include: using existing alternative knowledge such as \u201cbrush\u201d in robot knowledge base to replace inappropriate knowledge \u201cvacuum cleaner\u201d in NLC tasks such as \u201cclean a surface\u201d [7][187], using general commonsense knowledge \u201cdrilling action needs driller\u201d in a robot database to satisfy the need for a specific type of knowledge such as \u201ctool for drilling a hole in the install a screw task\u201d [187][214], asking knowledge input from human users by proactively asking questions such as \u201cwhere is the table leg\u201d [8][215][216], autonomously learning from the internet [8][215][216] etc.", "startOffset": 423, "endOffset": 428}, {"referenceID": 7, "context": "Gap filling methods mainly include: using existing alternative knowledge such as \u201cbrush\u201d in robot knowledge base to replace inappropriate knowledge \u201cvacuum cleaner\u201d in NLC tasks such as \u201cclean a surface\u201d [7][187], using general commonsense knowledge \u201cdrilling action needs driller\u201d in a robot database to satisfy the need for a specific type of knowledge such as \u201ctool for drilling a hole in the install a screw task\u201d [187][214], asking knowledge input from human users by proactively asking questions such as \u201cwhere is the table leg\u201d [8][215][216], autonomously learning from the internet [8][215][216] etc.", "startOffset": 535, "endOffset": 538}, {"referenceID": 206, "context": "Gap filling methods mainly include: using existing alternative knowledge such as \u201cbrush\u201d in robot knowledge base to replace inappropriate knowledge \u201cvacuum cleaner\u201d in NLC tasks such as \u201cclean a surface\u201d [7][187], using general commonsense knowledge \u201cdrilling action needs driller\u201d in a robot database to satisfy the need for a specific type of knowledge such as \u201ctool for drilling a hole in the install a screw task\u201d [187][214], asking knowledge input from human users by proactively asking questions such as \u201cwhere is the table leg\u201d [8][215][216], autonomously learning from the internet [8][215][216] etc.", "startOffset": 538, "endOffset": 543}, {"referenceID": 207, "context": "Gap filling methods mainly include: using existing alternative knowledge such as \u201cbrush\u201d in robot knowledge base to replace inappropriate knowledge \u201cvacuum cleaner\u201d in NLC tasks such as \u201cclean a surface\u201d [7][187], using general commonsense knowledge \u201cdrilling action needs driller\u201d in a robot database to satisfy the need for a specific type of knowledge such as \u201ctool for drilling a hole in the install a screw task\u201d [187][214], asking knowledge input from human users by proactively asking questions such as \u201cwhere is the table leg\u201d [8][215][216], autonomously learning from the internet [8][215][216] etc.", "startOffset": 543, "endOffset": 548}, {"referenceID": 7, "context": "Gap filling methods mainly include: using existing alternative knowledge such as \u201cbrush\u201d in robot knowledge base to replace inappropriate knowledge \u201cvacuum cleaner\u201d in NLC tasks such as \u201cclean a surface\u201d [7][187], using general commonsense knowledge \u201cdrilling action needs driller\u201d in a robot database to satisfy the need for a specific type of knowledge such as \u201ctool for drilling a hole in the install a screw task\u201d [187][214], asking knowledge input from human users by proactively asking questions such as \u201cwhere is the table leg\u201d [8][215][216], autonomously learning from the internet [8][215][216] etc.", "startOffset": 590, "endOffset": 593}, {"referenceID": 206, "context": "Gap filling methods mainly include: using existing alternative knowledge such as \u201cbrush\u201d in robot knowledge base to replace inappropriate knowledge \u201cvacuum cleaner\u201d in NLC tasks such as \u201cclean a surface\u201d [7][187], using general commonsense knowledge \u201cdrilling action needs driller\u201d in a robot database to satisfy the need for a specific type of knowledge such as \u201ctool for drilling a hole in the install a screw task\u201d [187][214], asking knowledge input from human users by proactively asking questions such as \u201cwhere is the table leg\u201d [8][215][216], autonomously learning from the internet [8][215][216] etc.", "startOffset": 593, "endOffset": 598}, {"referenceID": 207, "context": "Gap filling methods mainly include: using existing alternative knowledge such as \u201cbrush\u201d in robot knowledge base to replace inappropriate knowledge \u201cvacuum cleaner\u201d in NLC tasks such as \u201cclean a surface\u201d [7][187], using general commonsense knowledge \u201cdrilling action needs driller\u201d in a robot database to satisfy the need for a specific type of knowledge such as \u201ctool for drilling a hole in the install a screw task\u201d [187][214], asking knowledge input from human users by proactively asking questions such as \u201cwhere is the table leg\u201d [8][215][216], autonomously learning from the internet [8][215][216] etc.", "startOffset": 598, "endOffset": 603}, {"referenceID": 104, "context": "For example, if information such as task-related objects\u2019 working status, locations and relative distances from robot/human was ignored, it is difficult for a robot to infer which object the human user needs [111].", "startOffset": 208, "endOffset": 213}, {"referenceID": 208, "context": "Second, when a human mentioned a task operation action or a tool, deep-level meanings such as \u201cobject functional role, operation\u2019s purpose and importance in this plan\u201d were not specified explicitly [217], limiting robots\u2019 environment/user adaptation.", "startOffset": 198, "endOffset": 203}, {"referenceID": 6, "context": "Third, NLP is still not accurate enough to accurately extract task-related information [7].", "startOffset": 87, "endOffset": 90}, {"referenceID": 159, "context": "With the support of the above methodologies, four main types of NLC applications are developed, including NLbased robot control where only the NL-format control symbol is given and comprehensive instruction understating is not involved [168], NL-based robot training where a comprehensive instruction understanding is required and intuitive task execution is not conducted [219], NL-based robot task execution where comprehensive understanding towards human instructions, practical situation conditions and human intentions is required and intuitive task execution is conducted [113], and robot social companion where human\u2019s social norms are required to be understood and respected, in addition to conducting NL-based execution [143].", "startOffset": 236, "endOffset": 241}, {"referenceID": 209, "context": "With the support of the above methodologies, four main types of NLC applications are developed, including NLbased robot control where only the NL-format control symbol is given and comprehensive instruction understating is not involved [168], NL-based robot training where a comprehensive instruction understanding is required and intuitive task execution is not conducted [219], NL-based robot task execution where comprehensive understanding towards human instructions, practical situation conditions and human intentions is required and intuitive task execution is conducted [113], and robot social companion where human\u2019s social norms are required to be understood and respected, in addition to conducting NL-based execution [143].", "startOffset": 373, "endOffset": 378}, {"referenceID": 106, "context": "With the support of the above methodologies, four main types of NLC applications are developed, including NLbased robot control where only the NL-format control symbol is given and comprehensive instruction understating is not involved [168], NL-based robot training where a comprehensive instruction understanding is required and intuitive task execution is not conducted [219], NL-based robot task execution where comprehensive understanding towards human instructions, practical situation conditions and human intentions is required and intuitive task execution is conducted [113], and robot social companion where human\u2019s social norms are required to be understood and respected, in addition to conducting NL-based execution [143].", "startOffset": 578, "endOffset": 583}, {"referenceID": 135, "context": "With the support of the above methodologies, four main types of NLC applications are developed, including NLbased robot control where only the NL-format control symbol is given and comprehensive instruction understating is not involved [168], NL-based robot training where a comprehensive instruction understanding is required and intuitive task execution is not conducted [219], NL-based robot task execution where comprehensive understanding towards human instructions, practical situation conditions and human intentions is required and intuitive task execution is conducted [113], and robot social companion where human\u2019s social norms are required to be understood and respected, in addition to conducting NL-based execution [143].", "startOffset": 729, "endOffset": 734}, {"referenceID": 103, "context": "Typical applications include control using symbolic words such as \u201cstart, stop\u201d [110], using semantic correlations such as \u201cgoTo-left\u201d [220], using logic", "startOffset": 80, "endOffset": 85}, {"referenceID": 210, "context": "Typical applications include control using symbolic words such as \u201cstart, stop\u201d [110], using semantic correlations such as \u201cgoTo-left\u201d [220], using logic", "startOffset": 135, "endOffset": 140}, {"referenceID": 186, "context": "submitted to Knowledge-based Systems, January 2017 structures such as \u201cfood type \u2013 vessel shape correlations\u201d [195], and using environmental conditions such as human safety [187], and location/building matching degree [17].", "startOffset": 110, "endOffset": 115}, {"referenceID": 178, "context": "submitted to Knowledge-based Systems, January 2017 structures such as \u201cfood type \u2013 vessel shape correlations\u201d [195], and using environmental conditions such as human safety [187], and location/building matching degree [17].", "startOffset": 173, "endOffset": 178}, {"referenceID": 16, "context": "submitted to Knowledge-based Systems, January 2017 structures such as \u201cfood type \u2013 vessel shape correlations\u201d [195], and using environmental conditions such as human safety [187], and location/building matching degree [17].", "startOffset": 218, "endOffset": 222}, {"referenceID": 211, "context": "With consideration of robot physical characteristics such as force/structure/speed [221], human preferences such as motion/emotion [136], and real-world conditions such as object availability, distributions and locations [222], knowledge is specified for robot execution, improving robots\u2019 ability in task understanding, environment perceiving and reasoning during NLC.", "startOffset": 83, "endOffset": 88}, {"referenceID": 129, "context": "With consideration of robot physical characteristics such as force/structure/speed [221], human preferences such as motion/emotion [136], and real-world conditions such as object availability, distributions and locations [222], knowledge is specified for robot execution, improving robots\u2019 ability in task understanding, environment perceiving and reasoning during NLC.", "startOffset": 131, "endOffset": 136}, {"referenceID": 212, "context": "With consideration of robot physical characteristics such as force/structure/speed [221], human preferences such as motion/emotion [136], and real-world conditions such as object availability, distributions and locations [222], knowledge is specified for robot execution, improving robots\u2019 ability in task understanding, environment perceiving and reasoning during NLC.", "startOffset": 221, "endOffset": 226}, {"referenceID": 212, "context": "According to knowledge transferring manners, NL-based robot training is categorized into four main types: training using human speech [222], training using physical demonstration [223], training using human feedback [157] and training using robot proactive querying [224].", "startOffset": 134, "endOffset": 139}, {"referenceID": 213, "context": "According to knowledge transferring manners, NL-based robot training is categorized into four main types: training using human speech [222], training using physical demonstration [223], training using human feedback [157] and training using robot proactive querying [224].", "startOffset": 179, "endOffset": 184}, {"referenceID": 149, "context": "According to knowledge transferring manners, NL-based robot training is categorized into four main types: training using human speech [222], training using physical demonstration [223], training using human feedback [157] and training using robot proactive querying [224].", "startOffset": 216, "endOffset": 221}, {"referenceID": 214, "context": "According to knowledge transferring manners, NL-based robot training is categorized into four main types: training using human speech [222], training using physical demonstration [223], training using human feedback [157] and training using robot proactive querying [224].", "startOffset": 266, "endOffset": 271}, {"referenceID": 106, "context": "Different from NL-based robot training in which human NL is helping a robot with its task understanding, in NLbased robot task execution, human NL is helping a robot with its task execution [113].", "startOffset": 190, "endOffset": 195}, {"referenceID": 18, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 19, "endOffset": 23}, {"referenceID": 172, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 23, "endOffset": 28}, {"referenceID": 194, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 28, "endOffset": 33}, {"referenceID": 130, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 34, "endOffset": 39}, {"referenceID": 195, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 39, "endOffset": 44}, {"referenceID": 169, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 45, "endOffset": 50}, {"referenceID": 197, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 50, "endOffset": 55}, {"referenceID": 198, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 55, "endOffset": 60}, {"referenceID": 199, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 61, "endOffset": 66}, {"referenceID": 36, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 67, "endOffset": 71}, {"referenceID": 117, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 71, "endOffset": 76}, {"referenceID": 200, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 76, "endOffset": 81}, {"referenceID": 201, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 82, "endOffset": 87}, {"referenceID": 202, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 88, "endOffset": 93}, {"referenceID": 153, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 94, "endOffset": 99}, {"referenceID": 178, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 99, "endOffset": 104}, {"referenceID": 205, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 105, "endOffset": 110}, {"referenceID": 205, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 111, "endOffset": 116}, {"referenceID": 206, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 116, "endOffset": 121}, {"referenceID": 207, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 121, "endOffset": 126}, {"referenceID": 207, "context": "Typical References [19][181][203] [137][204] [178][206][207] [208] [37][124][209] [210] [211] [161][187] [214] [214][215][216] [216]", "startOffset": 127, "endOffset": 132}, {"referenceID": 134, "context": "But in NL-based task execution, including understanding the task, a robot is also required to perceive surrounding environments [141], predict human intentions [113], and make optimal decisions by satisfying all the environment, task and human requirements [161].", "startOffset": 128, "endOffset": 133}, {"referenceID": 106, "context": "But in NL-based task execution, including understanding the task, a robot is also required to perceive surrounding environments [141], predict human intentions [113], and make optimal decisions by satisfying all the environment, task and human requirements [161].", "startOffset": 160, "endOffset": 165}, {"referenceID": 153, "context": "But in NL-based task execution, including understanding the task, a robot is also required to perceive surrounding environments [141], predict human intentions [113], and make optimal decisions by satisfying all the environment, task and human requirements [161].", "startOffset": 257, "endOffset": 262}, {"referenceID": 153, "context": "With respect to who leads the execution, applications of NL-based execution could be categorized into the following two categories: 1) human-centered cooperation in which human is cognitively leading the task execution and a robot is required to provide appropriate assistances to facilitate humans\u2019 executions [161], and 2) robot-centered cooperation where a robot is cognitively leading the task execution and the human is providing physical assistances to facilitate robots\u2019 executions [127][149].", "startOffset": 311, "endOffset": 316}, {"referenceID": 120, "context": "With respect to who leads the execution, applications of NL-based execution could be categorized into the following two categories: 1) human-centered cooperation in which human is cognitively leading the task execution and a robot is required to provide appropriate assistances to facilitate humans\u2019 executions [161], and 2) robot-centered cooperation where a robot is cognitively leading the task execution and the human is providing physical assistances to facilitate robots\u2019 executions [127][149].", "startOffset": 489, "endOffset": 494}, {"referenceID": 141, "context": "With respect to who leads the execution, applications of NL-based execution could be categorized into the following two categories: 1) human-centered cooperation in which human is cognitively leading the task execution and a robot is required to provide appropriate assistances to facilitate humans\u2019 executions [161], and 2) robot-centered cooperation where a robot is cognitively leading the task execution and the human is providing physical assistances to facilitate robots\u2019 executions [127][149].", "startOffset": 494, "endOffset": 499}, {"referenceID": 135, "context": "Different from NL-based task executions, which focus on execution methods, NL-based social companions focus on humans\u2019 social norm understanding and imitating [143].", "startOffset": 159, "endOffset": 164}, {"referenceID": 215, "context": "With a social manner, robots are more naturally integrated with a human to form a human-centered robot task execution system and robots could also be more socially-acceptable for human communities [225][226].", "startOffset": 197, "endOffset": 202}, {"referenceID": 216, "context": "With a social manner, robots are more naturally integrated with a human to form a human-centered robot task execution system and robots could also be more socially-acceptable for human communities [225][226].", "startOffset": 202, "endOffset": 207}, {"referenceID": 215, "context": "According to implementation manners of social norms, applications using NL-based social companion are mainly categorized into social communication in which social NL is used for facilitating mutual information exchanging during NLC [225][226], and social execution in which social NL is used for facilitating task execution during NLC [227][228].", "startOffset": 232, "endOffset": 237}, {"referenceID": 216, "context": "According to implementation manners of social norms, applications using NL-based social companion are mainly categorized into social communication in which social NL is used for facilitating mutual information exchanging during NLC [225][226], and social execution in which social NL is used for facilitating task execution during NLC [227][228].", "startOffset": 237, "endOffset": 242}, {"referenceID": 217, "context": "According to implementation manners of social norms, applications using NL-based social companion are mainly categorized into social communication in which social NL is used for facilitating mutual information exchanging during NLC [225][226], and social execution in which social NL is used for facilitating task execution during NLC [227][228].", "startOffset": 335, "endOffset": 340}, {"referenceID": 218, "context": "According to implementation manners of social norms, applications using NL-based social companion are mainly categorized into social communication in which social NL is used for facilitating mutual information exchanging during NLC [225][226], and social execution in which social NL is used for facilitating task execution during NLC [227][228].", "startOffset": 340, "endOffset": 345}, {"referenceID": 43, "context": "This is due to these methods being incapable of simultaneously performing accurate NL understanding, which is due to limited performances of NLP techniques [44], and intuitive environment", "startOffset": 156, "endOffset": 160}, {"referenceID": 7, "context": "interpretation, which is due to the lack of an intuitive sensor data fusion methods[8][191].", "startOffset": 83, "endOffset": 86}, {"referenceID": 182, "context": "interpretation, which is due to the lack of an intuitive sensor data fusion methods[8][191].", "startOffset": 86, "endOffset": 91}, {"referenceID": 7, "context": "For example, in the task \u201cassembly\u201d, the procedure \u201cinstall the screw\u201d is more important than the procedure \u201cclean the place\u201d [8][191].", "startOffset": 126, "endOffset": 129}, {"referenceID": 182, "context": "For example, in the task \u201cassembly\u201d, the procedure \u201cinstall the screw\u201d is more important than the procedure \u201cclean the place\u201d [8][191].", "startOffset": 129, "endOffset": 134}, {"referenceID": 208, "context": "Second, each of the specific operation requests is interpreted to practical meanings, such as \u201cdeliver me a brush\u201d is for \u201ccleaning the surface\u201d and \u201ccup\u201d and \u201cglass\u201d in drinking have the same meaning as \u201ccontaining drinkable liquid\u201d [217].", "startOffset": 234, "endOffset": 239}, {"referenceID": 219, "context": "Learning-from-failure mechanism is implemented in computer science for algorithm performance improvement [229], in material science for new material discovery [230] etc.", "startOffset": 105, "endOffset": 110}, {"referenceID": 220, "context": "Learning-from-failure mechanism is implemented in computer science for algorithm performance improvement [229], in material science for new material discovery [230] etc.", "startOffset": 159, "endOffset": 164}, {"referenceID": 221, "context": "In NLC, learning from failure is initially involved in a definition-based manner [231], in which the failure is analyzed by comparing practical execution processes with defined knowledge, lacking the understanding of failure causes and meanwhile being passive in failure recovery.", "startOffset": 81, "endOffset": 86}, {"referenceID": 222, "context": "In new-knowledge exploration, new knowledge is collected by proactively asking humans and autonomously retrieving from the Word Wide Web [232], books [233], operation logs [234] and videos [235].", "startOffset": 137, "endOffset": 142}, {"referenceID": 223, "context": "In new-knowledge exploration, new knowledge is collected by proactively asking humans and autonomously retrieving from the Word Wide Web [232], books [233], operation logs [234] and videos [235].", "startOffset": 150, "endOffset": 155}, {"referenceID": 224, "context": "In new-knowledge exploration, new knowledge is collected by proactively asking humans and autonomously retrieving from the Word Wide Web [232], books [233], operation logs [234] and videos [235].", "startOffset": 172, "endOffset": 177}, {"referenceID": 203, "context": "For personalization, it does not only mean defining individualized knowledge for a robot to adapt to a specific user, but it also means endowing a knowledge-individualization method for a robot to autonomously adapt to variable users [212].", "startOffset": 234, "endOffset": 239}, {"referenceID": 6, "context": "At the execution-preference level, one execution alternative is more preferred than the other alternatives [7].", "startOffset": 107, "endOffset": 110}, {"referenceID": 203, "context": "In the social-manner level, a user\u2019s emotions, such as \u201chappy, sad, surprising\u201d, and social norms, such as \u201csafe zoon, comfortable distance\u201d, were considered in decision making [212].", "startOffset": 177, "endOffset": 182}], "year": 2017, "abstractText": "It is natural and efficient to use Natural Language (NL) for transferring knowledge from a human to a robot. Recently, research on using NL to support human-robot cooperation (HRC) has received increasing attention in several domains such as robotic daily assistance, robotic health caregiving, intelligent manufacturing, autonomous navigation and robot social accompany. However, a high-level review that can reveal the realization process and the latest methodologies of using NL to facilitate HRC is missing. In this review, a comprehensive summary about the methodology development of natural-language-facilitated human-robot cooperation (NLC) has been made. We first analyzed driving forces for NLC developments. Then, with a temporal realization order, we reviewed three main steps of NLC: human NL understanding, knowledge representation, and knowledge-world mapping. Last, based on our paper review and perspectives, potential research trends in NLC were discussed.", "creator": "Microsoft\u00ae Word 2013"}}}