{"id": "1701.08837", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2017", "title": "Emergence of Selective Invariance in Hierarchical Feed Forward Networks", "abstract": "Many theories have emerged which investigate how in- variance is generated in hierarchical networks through sim- ple schemes such as max and mean pooling. The restriction to max/mean pooling in theoretical and empirical studies has diverted attention away from a more general way of generating invariance to nuisance transformations. We con- jecture that hierarchically building selective invariance (i.e. carefully choosing the range of the transformation to be in- variant to at each layer of a hierarchical network) is im- portant for pattern recognition. We utilize a novel pooling layer called adaptive pooling to find linear pooling weights within networks. These networks with the learnt pooling weights have performances on object categorization tasks that are comparable to max/mean pooling networks. In- terestingly, adaptive pooling can converge to mean pooling (when initialized with random pooling weights), find more general linear pooling schemes or even decide not to pool at all. We illustrate the general notion of selective invari- ance through object categorization experiments on large- scale datasets such as SVHN and ILSVRC 2012.", "histories": [["v1", "Mon, 30 Jan 2017 21:44:27 GMT  (298kb,D)", "http://arxiv.org/abs/1701.08837v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["dipan k pal", "vishnu boddeti", "marios savvides"], "accepted": false, "id": "1701.08837"}, "pdf": {"name": "1701.08837.pdf", "metadata": {"source": "CRF", "title": "Emergence of Selective Invariance in Hierarchical Feed Forward Networks", "authors": ["Dipan K. Pal", "Vishnu N. Boddeti", "Marios Savvides"], "emails": ["marioss}@andrew.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "Convolutional nets (ConvNets [15]) have gained immense popularity over the past decade. Despite a lot of studies to improve their generalization abilities, much of their fundamental architecture remains the same. This illustrates that the basic hierarchical modules involving convolution, non-linearity and pooling operations are very effective in various domains and modalities, including vision. Nonetheless, there is much left to answer regarding what fundamen-\ntal task each of these modules is achieving. In this paper, we focus on pooling, which has received relatively less attention compared to filter weights, overall architecture and training procedures.\nTraditionally, pooling is conducted over blocks or regions over the convolution map (after the convolution step) using operators such as max or mean. Essentially, pooling is part of the architecture and defines what the \u201cconnections\u201d between modules are. Whereas learning the filter/kernel weights defines the \u201ctuning\u201d properties in canonical architectures. Some alternative methods of pooling have recently started being explored. However, these studies approach pooling either from an engineering standpoint (making ConvNets flexible in terms of input size [10]), or from a regularization perspective [22, 9]. Nonetheless, the efficacy of such a diverse set of approaches suggest that pooling can\nar X\niv :1\n70 1.\n08 83\n7v 1\n[ cs\n.L G\n] 3\n0 Ja\nn 20\nbe effective even when implemented in various ways. Further suggesting that perhaps deeper and more fundamental objectives are at work.\nOne of the fundamental objective that pooling tries to optimize for, is hypothesized in many works to be generating invariance to input nuisance transformations [3, 19, 1]. Indeed, generating useful representations that are invariant to such transformations is arguably one of the core problems in many fields, such as vision. Pooling is thus, usually seen as a tool for introducing such invariance and it is used and implemented as such. Many successful hierarchical architectures such as ConvNets employ pooling in a deterministic and organized manner, optimized more for engineering benefits, such as fast computation and easy implementation, rather than accuracy. This is also since the specific parameters of the pooling layers (such as pooling field size) are usually selected according to heuristics and intuition due to lack of a deeper understanding of what objective pooling tries to achieve and need for specific pooling schemes. Pooling is traditionally not optimized for the data and only specific hyper parameters are tuned accordingly during validation to improve generalization of the network."}, {"heading": "2. A conjecture involving Selective Invariance", "text": "to Input Transformations\nWe now review and introduce some concepts useful throughout the rest of the paper.\nGroup: A group is a mathematical structure encoding symmetry through a set of elements along with a group operation which acts on any two elements. The structure needs to satisfy four axioms namely, closure, associativity, identity and invertibility in order for the structure to be a valid group. A group can have a finite number of elements resulting in a finite group. In cases where a group is used to model a transformation, any subset of the group can be used to define a particular range of the transformation.\nInvariant and Equivariant features: Any function f over x \u2208 Rd is an invariant feature w.r.t a group G if f(x) = f(gx) \u2200g \u2208 G1. It is an equivariant or a covariant feature w.r.t the group if f(x) \u221d h(g)f(gx) \u2200g \u2208 G where h is a linear function defined over G 7\u2192 R. Ideally, it is desirable for a feature to be invariant to transformations that does not change the class label of the input (intraclass transformations), but be equivariant to transformations which do (inter-class transformations).\nInvariance to transformations: A plethora of literature exists to show that one of the core problems in pattern recognition is to generate invariance to transformations g in the data, leading to significant improvements in recognition performance [21, 4, 1, 16, 11, 17]. Further, features that are\n1With a slight abuse of notation, we denote by gx the action of group element g \u2208 G on x\ninvariant (even partially i.e.invariant to a subset of G) to the transformation group G allow for the sample complexity to be reduced [1, 17]. Previous work such as [1, 16] show that even though complete invariance is unachievable in practice, (as consolation) partial invariance holds. In this paper, we hypothesize and empirically support the claim that selective partial invariance is in fact necessary for good recognition performance.\nIndeed, the need for invariance to specific transformation ranges in the data has been relatively understated. More formally, the specific subset G0 of the group G to which the feature is invariant should be carefully chosen. It is more common to investigate explicit complete invariance to transformation groups such as the rotation group and/or the translation group [5, 7]. Group invariant scattering was also proposed as a theoretical framework for modelling entire translation group invariances in ConvNets as contractions acting on the entire space globally [18].\nSelective Invariance: In this paper, we argue that for vision tasks (and perhaps in general), selective invariance (also equivariance) is important. Consider the range of a transformation, which refers to the extent by which the transformation is applied to a sample point. By selective invariance, we emphasize that it is necessary to be invariant to carefully chosen parts of the range of the transformation. Additionally, it is also equally important to be equivariant to a different part of the range of the same transformation.\nTo illustrate, consider the sub-task of distinguishing between 6 and 9 in an optical character recognition task. If the transformation we consider is rotation, then a 180\u25e6 rotation turns the 6 into a 9. Hence, the classifier not only needs to be invariant to rotation between \u221290\u25e6 and say 90\u25e6 but, in this hypothetical situation, be equivariant to the infinitesimally small transformation as the digit rotates just beyond the 90\u25e6 mark into the other class. A classifier that is completely invariant to the rotation group (i.e. invariant to all angles from 0\u25e6 to 360\u25e6) will fail the task. This simple thought experiment illustrates the need to be invariant to parts of the range of a particular transformation while being equivariant to the other parts.\nA conjecture towards a general theory of pooling: One of the main contributions of this paper is to provide empirical evidence and motivate a theoretical understanding of a more general form of linear pooling. We conjecture that more general forms of linear pooling exist. These more general linear pooling schemes can work comparably well to canonical pooling schemes such as max and mean pooling, thereby forcing one to not be able to ignore them while developing a theoretical model of pooling. We further conjecture as a consequence, that an additional objective for pooling operates, which suggests that though network models must build (partial) invariance hierarchically, it is sufficient to build it selectively. This relaxes the con-\nditions required for pooling in order for architectures to perform well, thereby allowing for more general pooling schemes. In other words, useful invariant (and equivariant) representations can be obtained by carefully choosing specific ranges of the transformations present in the data to be invariant (and equivariant) towards. These ranges of invariance can many times be much larger and more diverse than what mean/max pooling schemes suggest and can also lead to redundancy in pooling, where multiple spatially localized pooling nodes pool across very similar ranges of transformations (see Section 5). Phenomenon such as these invoke the need for a more general theory of pooling.\nMany previous studies have examined pooling schemes empirically and theoretically. Mean/max pooling was examined in detail in terms of discriminability in [3]. The effect of max pooling on hard-vector quantized features was shown to help performance [2]. A study more aligned towards highlighting the importance of pooling (even with random convolution filters) is [12]. All of these efforts were however, restricted to investigating max and/or mean pooling schemes.\nMotivating adaptive pooling: We provide evidence for our conjecture through the use of adaptive pooling. To this effect, we remove constraints on the canonical method of pooling. We utilize a more general linear pooling model (compared to mean pooling), and use the data itself to optimize pooling schemes. The mere existence of these more general linear pooling schemes in networks which perform comparably to max/mean pooling networks (as we find in Section 5) show that a more general and fundamental objective for generating invariance is at play. This could suggest future theories and frameworks for ConvNets (and perhaps other deep learning algorithms) to allow for and model such general pooling schemes. Although max and mean pooling are simple to implement and work well in practice, restricting our theoretical understanding to such specialized pooling schemes might redirect attention away from more powerful and general theories for invariance and perception.\nAdaptive Pooling: In order to investigate what kind of pooling the data requires, we propose a novel adaptive pooling layer that learns or optimizes pooling according to the loss function and the data. The adaptive pooling layer is trained using standard back propagation along with some regularization. Our use for this layer in this paper is very specific. The adaptive pooling layer simply serves as a way to practically prove the existence of a set of network parameters (filter/kernel weights and pooling weights) that work well for the object categorization task. Selective invariance properties emerge in the pooling weights (see Fig. 2 and Fig. 6) as we find in our experiments later. We also model adaptive pooling in a group invariant framework and show how it invokes selective invariance and equivariance properties (see Section 4)."}, {"heading": "3. Partially Invariant Features through Partial", "text": "Group Integration\nA number of theories of invariance have emerged over the years. Most of them require some assumption regarding the structure of the transformations. One of the most common assumptions is that the transformations form a group [1, 14, 18]. This seems valid since transformations in many fields in which the importance of invariant features seems natural such as vision, do indeed deal with common transformations that form a group, further, they are unitary (e.g. translation and rotation). We motivate the use of adaptive pooling through such a group invariant framework. However, since in practice, all members of the group are not observed, we utilize theories which have been shown to work under partial observation of the group 2.\nConsider a unitary group of transformations G with group elements g with finite cardinality (|G|). Unitary Group: A unitary group is a group with elements satisfying the unitary property, i.e. \u3008gx, gy\u3009 = \u3008x, y\u3009 \u2200x, y \u2200g \u2208 G.\nWe have the action of a group element g on a sample point x as gx and following this, an orbit is generated as the set {gx | g \u2208 G}. This orbit is unique to every point since it the the set of all variations or transformations of the point as defined by G. A measure which introduces invariance and allows us to compare two orbits is the distribution Px induced by G on a sample x. It can be shown that x \u223c x\u2032 \u21d4 Px = Px\u2032 i.e. if two images (x, x\u2032) are equivalent under some g, then their distributions are identical [1]. This is important since we would like to be invariant to G but nonetheless have a common discriminative signature or feature for {gx | g \u2208 G}. One can form such a discriminative feature by measuring properties of the distribution or trying to characterize the distribution. In order to do so, any template or filter can be utilized along with the powerful property of unitarity of the group G.\nA single filter provides a 1-D projection of the distribution thereby providing one measurement. We can obtain many such measurements in order to be more discriminative. Such a collection of many such filters together uniquely characterizes the orbit. More importantly, unitarity of the group allows the following for a filter t\n\u3008gx, t\u3009 = \u3008x, g\u22121t\u3009 (1)\nHence, the distribution of the set {\u3008gx, t\u3009},\u2200g \u2208 G is exactly as that of {\u3008x, g\u22121t\u3009},\u2200g \u2208 G. Following this, in order to characterize the orbit of a novel sample under a group, it is not necessary to explicitly observe all its transformations under the group. Since the orbit and its\n2A group is said to be fully observed, if during training, samples are available that have been acted upon by all members of the group. Partial observance refers to setting where only a subset of those samples are available for use.\nP oo\nlin g\nw ei\ngh t (\n\u03b1)\nRange of finite partial transformation group\n1 2 3 4\nInvariant\nInvariant\nEq ui\nva ria\nnt\nEquivariant\n(a)\nP ar\ntia l g\nro up\na ct\nio n\nin in\npu t s\npa ce\nFeature space\nInv aria\nnt r egi\non\nInvariant region\nEquivariant region g1\ng2\ng3 v1\nv2\nv3\n(b)\nFigure 2: (a) Illustrative depiction of pooling weights over the range of a partial transformation group. Different sections are either agnostic, invariant or equivariant depending on the weights over that particular range (subset of the partial group). (b) Diagram illustrating how subsets of the group of transformation (ranges, such as g1, g2, g3) map to different parts of the feature space depending on the pooling weights. v1, v3 depict invariant regions for a single pooling weight (corresponding to a single pooling element) which are invariant w.r.t to ranges g1, g3 and map them to single points (in red) for a given input. v2 maps g2 to a line (bold) since it is equivariant to g2.\ncorresponding distribution is invariant to G, many possible invariant features or measures can be computed. Two sets of examples are 1) the moments of the distribution and 2) a possibly non-linear function of the inner-product (i.e. f(x) = \u03b7(\u3008x, g\u22121t\u3009), where \u03b7 is a non-linear thresholding function). Further, measures of the distribution of {\u3008x, g\u22121t\u3009},\u2200g \u2208 G0 where G0 \u2286 G are also invariant owing to partial integrals over partial groups [1]. This sets the framework for the incorporation of selective invariance. Carefully choosing G0, one can control which range of the transformation a feature is partially invariant towards.\nA previous study that allowed partial invariance although in a much simplistic non-hierarchical setting is [20]. However, the incorporation of partial (non-selective) invariance was due to relaxation in an optimization framework. It has also been argued that local features should only have enough invariance (as opposed to complete invariance) as required by the application [23]. Nonetheless, the observation was presented as a general recommendation and was not explicitly studied."}, {"heading": "4. Adaptive Pooling Module for Learning Generalized Linear Pooling", "text": "We now describe the adaptive pooling module which generalizes mean pooling. In traditional ConvNets, mean pooling is highly structured. It is a linear operation and thus can be approximated using a matrix A. Each row of A is called a pooling node/element with an associated pooling weight, and it performs pooling on some region of the convolution map through the inner product and hence is linear. Max pooling can be modelled in this framework by optimizing the support of the max operation instead. However, we focus on generalized linear pooling in this study.\nNotation: We let every input to the kth layer be denoted by uk \u2208 Rm and the output of the layer be denoted by\nvk \u2208 Rn. Let L be the loss that the network optimizes for. The adaptive pooling matrix is defined by A \u2208 Rm\u00d7n with n pooling elements.\nWe thus have vk = ATuk. We learn the pooling matrix using back-propagation. The gradient w.r.t to the ith row of A (i.e. Ai) then becomes.\n\u2202L \u2202Ai = \u2202vk \u2202Ai ( \u2202L \u2202vk ) = U\u03b1 (2)\nNote that U = \u2202vk\u2202Ai is simply a matrix with the every column as the input uk. Whereas \u03b1 = \u2202L\u2202vk is a vector of coefficients. Now recall that the response map uk (i.e. input to the adaptive pooling layer) in the case of ConvNets is a collection of the inner products of the input map of the previous convolution layer with a convolutional kernel \u03c9 passed through a non-linearity \u03b7. This can be modelled as a partial translation group GT (composed of m translation operators) that acts upon a convolutional filter \u03c9 to form the set {gT\u03c9 | gT \u2208 GT }. A translation group is a group composed of translation operators. The elements of uk are then in the form of the set {\u03b7(\u3008gT\u03c9, x\u3009) | gT \u2208 GT }. Here, x is the input to the previous convolution layer. Other network architectures which incorporate more transformations (such as [8, 6]) can also be modelled in this framework as long as the transformation is unitary.\nThe response of the adaptive pooling layer, i.e. the ith pooling element at the kth layer with a pooling weight vector \u03b1i computes\nvki = M\u2211 j=1 \u03b1ij\u03b7(\u3008gTj\u03c9, x\u3009) = M\u2211 j=1 \u03b1ij\u03b7(\u3008\u03c9, g\u22121Tj x\u3009) \u2200gTj \u2208 GT\n(3)\nThe second equality holds from the unitary property of G and the fact that G is a group (partial groups have corre-\nsponding inverses which also form a partial group). Hence, the pooling vector \u03b1 effectively pools over the transformations of the input. This computes a measure of the orbit of the input under the partial group. In the case when \u03b1 is a vector with the same coefficients, this is exactly the group integral over the finite partial translation group G\u22121T . It has been shown that a non-linear feature of the form as Equation 3 is partially invariant given a finite partial group [1]. In practice however, \u03b1 also includes significantly varying coefficients, thus introducing selective invariance (and equivaraince) to certain transformation ranges in the partial group. In order to reduce overfitting and improve learnability of the pooling weights, we constrain the `1 norm of each pooling element to be 13. Under such regularization, \u03b1 will compose of non-zero elements as well as elements that are negligibly small. This defines the support of the range of the transformations that the pooling element is invariant to. Fig. 2(a) illustrates the range of transformations that the pooling weight is invariant and equivariant to for a typical pooling response. Part 1 of the pooling element in Fig. 2(a) contains near zero weights, thus the pooling element is agnostic to all input in that range of transformation. Part 3 is comprised of a range which has approximately constant weight. This part will be invariant to that particular range of the transformation while providing an invariant descriptor of the orbit of the input under the partial group corresponding to that range.\nPart 2 and 4 in Fig. 2(a) however, are parts which are not invariant, but rather approximately equivariant or co-\n3Recall that the goal is to find pooling architectures and parameters that work well in practice which deviate from the standard pooling schemes. The regularization is thus justified, since merely proving the existence of such parameterizations is enough to showcase a more general form of pooling and hint towards the more fundamental goal of selective invariance.\nvariant wherein the response of the pooling element is linearly proportional to the group element i.e. vki \u221d h(gi)\u03b7(\u3008gi\u03c9, x\u3009) \u2200gi \u2208 G\u22121equiT \u2286 G \u22121 T , where G \u22121 equiT defines the range of transformations in G\u22121T that the pooling weight vki is equivariant to, and h is a linear function defined over G 7\u2192 R. Such a linear function h exists since the group GT is composed of transformations that are linearly related to each other. The selectivity in each pooling element is also derived from its equivariant or covariant responses to certain ranges of the transformation apart from the invariant responses to other ranges.\nPotentially redundant pooling elements: Adaptive pooling is a generalization of mean pooling. Unlike mean pooling however, every pooled element is not restricted to pool over a pre-defined p \u00d7 p grid (usually with p = 2, 3). Each pooling element can in fact, in theory, pool over the entire response map of the previous layer. The overlap between different pooling elements is not pre-defined to be complimentary and can in fact be similar for multiple pooling elements, thereby introducing redundancy. This is a phenomenon we do indeed observe in our experiments (see Section 5). This is somewhat counter-intuitive to what we might expect to provide a good representation, wherein each element might be expected to capture a different feature or aspect of the data. Nonetheless, in practice such a configuration emerges to perform just as well, and hence it provides more insight into pooling and representation.\nNeed for redundancy: Redundancy in pooling elements could be hypothesized to provide and retain significant activations at any given layer, where pooling reduces the size of the activation map up the hierarchy. Hence pooling elements must compete and/or cooperate to utilize the limited space available in the activation map for activations more\nuseful for the task. Having redundant pooling elements that are located in the near vicinity of each other spatially preserve the locality and contiguity of the activation of the object."}, {"heading": "5. Emergence of Selective Invariance and Redundant Pooling", "text": "We use standard ConveNets architectures while replacing the mean and max pooling layers with adaptive pooling. We train the networks to minimize the logistic soft max loss on large-scale classification benchmarks such as the Street View House Numbers (SVHN) and the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 datasets. Mean and max pooling field sizes were fixed at 2\u00d7 2 for all experiments."}, {"heading": "5.1. Street View House Numbers (SVHN)", "text": "The SVHN dataset has 10 classes corresponding to 10 digits and a training and testing data size of about 73,000 and 26,000 samples. For this dataset we use a network with two convolution layers (64 filters of 5\u00d75 each ) each followed by an adaptive pooling layer. The last two layers were fully connected (1600 and 128 nodes). Non-linear layers were used after every convolution. The network parameters were randomly initialised including the adaptive pooling weights. All networks were trained using dropout for 400 epochs.\nResults: Fig. 5 shows the progression of train and test accuracies for all three pooling schemes. Although adaptive pooling suffers in performance initially, it recovers over epochs achieving close to \u223c 91% accuracy compared to the mean/max pooling result of \u223c 93%. The adaptive pooling network, in this particular experiment, was initialized using random weights. One might expect the network to have difficulty learning given the large number of parameters, however, given the easier task (compared to an even larger scale classification task such as ILSVRC 2012), the network gradients are informative and the network performance improves.\nFig. 3(a) and Fig. 3(b) shows some representative pooling weights from the final model learned using adaptive pooling. We find that at layer 2, pooling elements signif-\nicantly deviate from the canonical mean pooling scheme. More interestingly, mean pooling emerges in the layer 1 pooling weights despite the random pooling weights initialization. The pooling elements adapt to the transformations present in the dataset by being selectively invariant to multiple ranges of transformations (first column of Fig. 3(b) circled in red) or larger contiguous ranges (circled in blue). Also interestingly, a few pooling elements in layer 1 tune to being completely agnostic to all inputs (second column in Fig. 3(a)). This seems to be an artifact of the dataset which is composed of optical characters that usually lie near the center as shown in Fig. 3. The backgrounds around the digits near the edges are irrelevant to the task and hence transformations of those areas are not useful. The adaptive pooling elements learns to be completely agnostic to transformations of all inputs in that locality.\nComparison of Adaptive Pooling to max/mean pooling performance. Adaptive pooling has many more parameters than max/mean pooling making it susceptible to over-fitting and also having the effects of local minima be more pronounced. There exist methods to help adaptive pooling achieve better performance through regularization etc. However this is not the goal of this study. The goal of the study is to let patterns emerge from the data with minimal regularization and to use very simple and canonical optimization techniques such as gradient descent."}, {"heading": "5.2. ImageNet Large Scale Visual Recognition", "text": "Challenge (ILSVRC) 2012\nThe ILSVRC 2012 challenge has about 1,000 classes and over 1.2 million images for training and about 50,000 images for validation. We use the standard AlexNet architecture [13] for this task. We benchmark against standard mean and max pooling. To incorporate adaptive pooling, we replace all three pooling layers in AlexNet with adaptive pooling.\nInitialization: Convolution filters for baseline networks with mean and max pooling are always initialized randomly. We initialized the network with adaptive pooling in two ways. First, we tried initializing adaptive pooling parameters along with convolution parameters randomly. This resulted in extremely slow learning owing to the increased number of parameters, no regularization and a harder classification task. Second, we pre-trained AlexNet with mean pooling for 25 and then 65 epochs (with convolutional layers initialized randomly) and then replaced the mean pooling layers with adaptive pooling for the models at epoch 25 and epoch 65. The adaptive pooling layers were then initialized with mean pooled weights and training continued.\nResults: Fig. 5(b) shows the average training and validation loss on ILSVRC 2012 for max, mean and adaptive pooling. After the learning rate drop during training (after first 10 epochs), adaptive pooling (epoch 65) almost matches mean and max pooling despite the large increase in the number of parameters. Initialization of the network to mean pooling weights and the convolution layers to pretrained convolutional filters (from epoch 25 and 65 respectively) help overcome adverse effects that accompany the increase. Fig. 2 and Figs. 7(a), 7(b), 7(c), 7(d) show some representative pooling weights from the adaptive pooling layers (layers 1, 2 and layer 3 respectively) of the model learned using AlexNet pre-trained up until epoch 65. The adaptive pooling layer was fine tuned for about 50 additional epochs."}, {"heading": "6. Discussion", "text": "Emergence of Selective Invariance: Our first observation through the SVHN (see Fig. 5(a)) and the ILSVRC 2012 (see Fig. 5(b)) experiments, is that adaptive pooling can perform comparably to max/mean pooling schemes. This validates the efficacy of the generalized linear pooling parameterization that adaptive pooling finds. In many cases, the pooling weights were found to deviate significantly from mean pooling schemes. Additionally, a few pooling elements were found to become completely agnostic to all inputs despite being initialized to random pooling weights (see Fig. 2). Both these observations illustrate the emergence of selectively invariant pooling elements in the networks.\nPooling elements at lower layers tend to be selectively invariant to smaller contiguous ranges of transformations: It is interesting to find that adaptive pooling elements initialized with random pooling weights converge to mean pooling at layer 1 for SVHN (see Fig. 3(a)). Further, even though adaptive pooling for AlexNet on ILSVRC 2012 was initialized to mean pooling weights, mean pooling was preserved for layers 1 and 2 (see Fig. 5). This leads to an observation that invariance should be generated locally for local features in low level representations. This agrees with the hypothesis that low level object parts and their features have fewer transformations that they can undergo, which have a smaller support over the input space and hence are local. Pooling elements invariant to those transformations are also localized. Mean pooling therefore seems to be a good approximation for invariant features at lower levels in hierarchical networks.\nPooling elements at higher layers tend to be selectively invariant to larger (possibly non-contiguous) ranges of transformations: As a general trend we also observe that the pooling elements at higher layers such as layer 3 for AlexNet (see Fig. 6) and layer 2 for the SVHN network (see Fig. 3(b)) need specialized invariant features since the pooling weights deviate significantly from mean pooling. This is despite AlexNet pooling layers being initialized to mean pooling weights. This also agrees with the hypothesis that high level object parts and complete objects can undergo a more complex set of transformations. Pooling elements that are selectively invariant at higher layers can sometimes be redundant and be invariant to extremely large contiguous ranges or even multiple smaller ranges. Hence, pooling at higher layers needs more careful handling. Perhaps mean pooling at higher layers is sub-optimal and more effective pooling strategies that are selectively invariant could help improve performance of these networks in general."}], "references": [{"title": "On invariance and selectivity in representation learning", "author": ["F. Anselmi", "L. Rosasco", "T. Poggio"], "venue": "arXiv preprint arXiv:1503.05938", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning mid-level features for recognition", "author": ["Y.-L. Boureau", "F. Bach", "Y. LeCun", "J. Ponce"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 2559\u20132566. IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "A theoretical analysis of feature pooling in visual recognition", "author": ["Y.-L. Boureau", "J. Ponce", "Y. LeCun"], "venue": "Proceedings of the 27th international conference on machine learning (ICML-10), pages 111\u2013118", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning stable group invariant representations with convolutional networks", "author": ["J. Bruna", "A. Szlam", "Y. LeCun"], "venue": "arXiv preprint arXiv:1301.3537", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Group equivariant convolutional networks", "author": ["T.S. Cohen", "M. Welling"], "venue": "CoRR, abs/1602.07576", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploiting cyclic symmetry in convolutional neural networks", "author": ["S. Dieleman", "J. De Fauw", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.02660", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploiting cyclic symmetry in convolutional neural networks", "author": ["S. Dieleman", "J.D. Fauw", "K. Kavukcuoglu"], "venue": "CoRR, abs/1602.02660", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep symmetry networks", "author": ["R. Gens", "P.M. Domingos"], "venue": "Advances in neural information processing systems, pages 2537\u20132545", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Fractional max-pooling", "author": ["B. Graham"], "venue": "CoRR, abs/1412.6071", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR, abs/1406.4729", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning translation invariant recognition in a massively parallel networks", "author": ["G.E. Hinton"], "venue": "PARLE Parallel Architectures and Languages Europe, pages 1\u201313. Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1987}, {"title": "What is the best multi-stage architecture for object recognition? In Computer Vision", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "2009 IEEE 12th International Conference on, pages 2146\u20132153. IEEE", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Transformation-invariant convolutional jungles", "author": ["D. Laptev", "J.M. Buhmann"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3043\u20133051", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Subtasks of unconstrained face recognition", "author": ["J.Z. Leibo", "Q. Liao", "T. Poggio"], "venue": "International Joint Conference on Computer Vision, Imaging and Computer Graphics, VISI- GRAPP", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning invariant representations and applications to face verification", "author": ["Q. Liao", "J.Z. Leibo", "T. Poggio"], "venue": "Advances in Neural Information Processing Systems, pages 3057\u20133065", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Group invariant scattering", "author": ["S. Mallat"], "venue": "Communications on Pure and Applied Mathematics, 65(10):1331\u20131398", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "On random weights and unsupervised feature learning", "author": ["A. Saxe", "P.W. Koh", "Z. Chen", "M. Bhand", "B. Suresh", "A.Y. Ng"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), pages 1089\u20131096", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Convex learning with invariances", "author": ["C.H. Teo", "A. Globerson", "S.T. Roweis", "A.J. Smola"], "venue": "Advances in neural information processing systems, pages 1489\u20131496", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Representation theory and invariant neural networks", "author": ["J. Wood", "J. Shawe-Taylor"], "venue": "Discrete applied mathematics, 69(1):33\u201360", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "arXiv preprint arXiv:1301.3557", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Local features and kernels for classification of texture and object categories: A comprehensive study", "author": ["J. Zhang", "M. Marsza\u0142ek", "S. Lazebnik", "C. Schmid"], "venue": "International journal of computer vision, 73(2):213\u2013238", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 14, "context": "Convolutional nets (ConvNets [15]) have gained immense popularity over the past decade.", "startOffset": 29, "endOffset": 33}, {"referenceID": 9, "context": "However, these studies approach pooling either from an engineering standpoint (making ConvNets flexible in terms of input size [10]), or from a regularization perspective [22, 9].", "startOffset": 127, "endOffset": 131}, {"referenceID": 21, "context": "However, these studies approach pooling either from an engineering standpoint (making ConvNets flexible in terms of input size [10]), or from a regularization perspective [22, 9].", "startOffset": 171, "endOffset": 178}, {"referenceID": 8, "context": "However, these studies approach pooling either from an engineering standpoint (making ConvNets flexible in terms of input size [10]), or from a regularization perspective [22, 9].", "startOffset": 171, "endOffset": 178}, {"referenceID": 2, "context": "One of the fundamental objective that pooling tries to optimize for, is hypothesized in many works to be generating invariance to input nuisance transformations [3, 19, 1].", "startOffset": 161, "endOffset": 171}, {"referenceID": 18, "context": "One of the fundamental objective that pooling tries to optimize for, is hypothesized in many works to be generating invariance to input nuisance transformations [3, 19, 1].", "startOffset": 161, "endOffset": 171}, {"referenceID": 0, "context": "One of the fundamental objective that pooling tries to optimize for, is hypothesized in many works to be generating invariance to input nuisance transformations [3, 19, 1].", "startOffset": 161, "endOffset": 171}, {"referenceID": 20, "context": "Invariance to transformations: A plethora of literature exists to show that one of the core problems in pattern recognition is to generate invariance to transformations g in the data, leading to significant improvements in recognition performance [21, 4, 1, 16, 11, 17].", "startOffset": 247, "endOffset": 269}, {"referenceID": 3, "context": "Invariance to transformations: A plethora of literature exists to show that one of the core problems in pattern recognition is to generate invariance to transformations g in the data, leading to significant improvements in recognition performance [21, 4, 1, 16, 11, 17].", "startOffset": 247, "endOffset": 269}, {"referenceID": 0, "context": "Invariance to transformations: A plethora of literature exists to show that one of the core problems in pattern recognition is to generate invariance to transformations g in the data, leading to significant improvements in recognition performance [21, 4, 1, 16, 11, 17].", "startOffset": 247, "endOffset": 269}, {"referenceID": 15, "context": "Invariance to transformations: A plethora of literature exists to show that one of the core problems in pattern recognition is to generate invariance to transformations g in the data, leading to significant improvements in recognition performance [21, 4, 1, 16, 11, 17].", "startOffset": 247, "endOffset": 269}, {"referenceID": 10, "context": "Invariance to transformations: A plethora of literature exists to show that one of the core problems in pattern recognition is to generate invariance to transformations g in the data, leading to significant improvements in recognition performance [21, 4, 1, 16, 11, 17].", "startOffset": 247, "endOffset": 269}, {"referenceID": 16, "context": "Invariance to transformations: A plethora of literature exists to show that one of the core problems in pattern recognition is to generate invariance to transformations g in the data, leading to significant improvements in recognition performance [21, 4, 1, 16, 11, 17].", "startOffset": 247, "endOffset": 269}, {"referenceID": 0, "context": "invariant to a subset of G) to the transformation group G allow for the sample complexity to be reduced [1, 17].", "startOffset": 104, "endOffset": 111}, {"referenceID": 16, "context": "invariant to a subset of G) to the transformation group G allow for the sample complexity to be reduced [1, 17].", "startOffset": 104, "endOffset": 111}, {"referenceID": 0, "context": "Previous work such as [1, 16] show that even though complete invariance is unachievable in practice, (as consolation) partial invariance holds.", "startOffset": 22, "endOffset": 29}, {"referenceID": 15, "context": "Previous work such as [1, 16] show that even though complete invariance is unachievable in practice, (as consolation) partial invariance holds.", "startOffset": 22, "endOffset": 29}, {"referenceID": 4, "context": "It is more common to investigate explicit complete invariance to transformation groups such as the rotation group and/or the translation group [5, 7].", "startOffset": 143, "endOffset": 149}, {"referenceID": 6, "context": "It is more common to investigate explicit complete invariance to transformation groups such as the rotation group and/or the translation group [5, 7].", "startOffset": 143, "endOffset": 149}, {"referenceID": 17, "context": "Group invariant scattering was also proposed as a theoretical framework for modelling entire translation group invariances in ConvNets as contractions acting on the entire space globally [18].", "startOffset": 187, "endOffset": 191}, {"referenceID": 2, "context": "Mean/max pooling was examined in detail in terms of discriminability in [3].", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "The effect of max pooling on hard-vector quantized features was shown to help performance [2].", "startOffset": 90, "endOffset": 93}, {"referenceID": 11, "context": "A study more aligned towards highlighting the importance of pooling (even with random convolution filters) is [12].", "startOffset": 110, "endOffset": 114}, {"referenceID": 0, "context": "One of the most common assumptions is that the transformations form a group [1, 14, 18].", "startOffset": 76, "endOffset": 87}, {"referenceID": 13, "context": "One of the most common assumptions is that the transformations form a group [1, 14, 18].", "startOffset": 76, "endOffset": 87}, {"referenceID": 17, "context": "One of the most common assumptions is that the transformations form a group [1, 14, 18].", "startOffset": 76, "endOffset": 87}, {"referenceID": 0, "context": "if two images (x, x\u2032) are equivalent under some g, then their distributions are identical [1].", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "Further, measures of the distribution of {\u3008x, g\u22121t\u3009},\u2200g \u2208 G0 where G0 \u2286 G are also invariant owing to partial integrals over partial groups [1].", "startOffset": 140, "endOffset": 143}, {"referenceID": 19, "context": "A previous study that allowed partial invariance although in a much simplistic non-hierarchical setting is [20].", "startOffset": 107, "endOffset": 111}, {"referenceID": 22, "context": "It has also been argued that local features should only have enough invariance (as opposed to complete invariance) as required by the application [23].", "startOffset": 146, "endOffset": 150}, {"referenceID": 7, "context": "Other network architectures which incorporate more transformations (such as [8, 6]) can also be modelled in this framework as long as the transformation is unitary.", "startOffset": 76, "endOffset": 82}, {"referenceID": 5, "context": "Other network architectures which incorporate more transformations (such as [8, 6]) can also be modelled in this framework as long as the transformation is unitary.", "startOffset": 76, "endOffset": 82}, {"referenceID": 0, "context": "It has been shown that a non-linear feature of the form as Equation 3 is partially invariant given a finite partial group [1].", "startOffset": 122, "endOffset": 125}, {"referenceID": 12, "context": "We use the standard AlexNet architecture [13] for this task.", "startOffset": 41, "endOffset": 45}], "year": 2017, "abstractText": "Many theories have emerged which investigate how invariance is generated in hierarchical networks through simple schemes such as max and mean pooling. The restriction to max/mean pooling in theoretical and empirical studies has diverted attention away from a more general way of generating invariance to nuisance transformations. In this exploratory study, we study the conjecture that hierarchically building selective invariance is important for pattern recognition. We define selective invariance as carefully choosing the range of the transformation to be invariant to at each layer of a hierarchical network. For the purpose of our study, we utilize a novel method called adaptive pooling where the pooling weights are not constrained and in fact can adapt their pooling regions to the data. These networks with the adapted pooling regions maintain performances on object categorization tasks comparable to max/mean pooling networks despite being more prone to overfitting. Interestingly, adaptive pooling regions can converge to mean pooling (even when initialized with random pooling regions), find more general linear pooling schemes or even decide not to pool at all. The pooling regions that emerge from the data are not random but rather contiguous, illustrating invariance to contiguous ranges of transformations. We illustrate the general notion of selective invariance through object categorization experiments on largescale datasets such as SVHN and ILSVRC 2012.", "creator": "LaTeX with hyperref package"}}}