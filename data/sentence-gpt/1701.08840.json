{"id": "1701.08840", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2017", "title": "Spatial Projection of Multiple Climate Variables Using Hierarchical Multitask Learning", "abstract": "Future projection of climate is typically obtained by combining outputs from multiple Earth System Models (ESMs) for several climate variables such as temperature and precipitation. While IPCC has traditionally used a simple model output average, recent work has illustrated potential advantages of using a multitask learning (MTL) framework for projections of individual climate variables. In this paper we introduce a framework for hierarchical multitask learning (HMTL) with two levels of tasks such that each super-task, i.e., task at the top level, is itself a multitask learning problem over sub-tasks. For climate projections, each super-task focuses on projections of specific climate variables spatially using an MTL formulation. For the proposed HMTL approach, a group lasso regularization is added to couple parameters across the super-tasks, which in the climate context helps exploit relationships among the behavior of different climate variables at a given spatial location. We show that some recent works on MTL based on learning task dependency structures can be viewed as special cases of HMTL. Experiments on synthetic and real climate data show that HMTL produces better results than decoupled MTL methods applied separately on the super-tasks and HMTL significantly outperforms baselines for climate projection. While a real-world model can be modeled as a linear model, this approach is not suitable for the complex modeling models such as climate change or long-term climate change. We are particularly interested in using a multivariate multivariate multivariate model with an MTL model of model strength as the basis for models of model strength. We describe this model as the multivariate multivariate multivariate model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 30 Jan 2017 21:56:18 GMT  (3279kb,D)", "http://arxiv.org/abs/1701.08840v1", "Accepted for the 31st AAAI Conference on Artificial Intelligence (AAAI-17)"]], "COMMENTS": "Accepted for the 31st AAAI Conference on Artificial Intelligence (AAAI-17)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["andr\u00e9 ricardo gon\u00e7alves", "arindam banerjee", "fernando j von zuben"], "accepted": true, "id": "1701.08840"}, "pdf": {"name": "1701.08840.pdf", "metadata": {"source": "CRF", "title": "Spatial Projection of Multiple Climate Variables Using Hierarchical Multitask Learning", "authors": ["Andr\u00e9 R. Gon\u00e7alves", "Arindam Banerjee", "Fernando J. Von Zuben"], "emails": ["{andre@cs.umn.edu,", "banerjee@cs.umn.edu,", "vonzuben@dca.fee.unicamp.br}"], "sections": [{"heading": null, "text": "Keywords\u2014 Multitask Learning, Structure Learning, Spatial Regression, Structured Regression, Earth System Models Ensemble."}, {"heading": "1 Introduction", "text": "Future projections of climate variables such as temperature, precipitation, and pressure are usually produced by physics-based models known as Earth System Models (ESMs). ESMs consist of four components and their interactions, viz. atmosphere, oceans, land, and sea ice (Tebaldi and Knutti, 2007; IPCC, 2013). Climate projections generated from ESMs form the basis for understanding and inferring future climate change, global warming, greenhouse gas concentration and its impact on Earth systems and other complex phenomena such as El Nin\u0303o Southern Oscillation (ENSO). ENSO, for instance, has a global impact, ranging from droughts in Australia and northeast Brazil to heavy rains over Malaysia, the Philippines, and Indonesia (IPCC, 2013). Then, producing accurate projections of climate variables is essential to anticipate extreme events.\n\u2217Paper published in the 31st AAAI Conference on Artificial Intelligence (AAAI\u201917). San Francisco, CA. 2017.\nar X\niv :1\n70 1.\n08 84\n0v 1\n[ cs\n.L G\n] 3\n0 Ja\nn 20\nMany ESMs have been developed by climate research institutes. A single and possibly more robust projection can be built as a combination (ensemble) of multiple ESMs simulations (Tebaldi and Knutti, 2007; McQuade and Monteleoni, 2013; Sanderson et al., 2015).\nRecently, the problem of constructing ESMs ensemble was approached from a multitask learning (MTL) perspective (Gonc\u0327alves et al., 2014), where building an ESMs ensemble for each geographical location was viewed as a learning task. The joint estimation of the ESM ensemble produced more accurate projections than when independent estimation was performed for each location. The MTL method was able to capture the relationship among geographical locations (tasks) and used it to guide information sharing among tasks.\nModeling task relationship in multitask learning has been the focus of recent research (Zhang and Schneider, 2010; Zhang and Yeung, 2010; Yang et al., 2013; Gonc\u0327alves et al., 2016). This is a fundamental step to promote information sharing only among related tasks, while avoiding the unrelated ones. Besides estimating task specific parameters (\u0398), the task dependency structure (\u2126) is also estimated from the data. The latter is usually estimated from the former, i.e., task dependency is defined based on the relation of the task parameters. Two tasks are said to be related if their parameters are related in some sense.\nInconsistently estimated task dependency structure in MTL can misguide information sharing and, hence, can be harmful to the MTL method performance. The problem of estimating statistical dependency structure of a set of random variables is known as structure learning (Meinshausen and Buhlmann, 2006). Existing methods for the problem guarantee to recover the true underlying dependence structure given a sufficient amount of data samples. In the MTL case (Gonc\u0327alves et al., 2016), the random variables are tasks parameters and, depending on the ratio between dimensionality and the number of tasks, the amount of data samples may not be sufficient.\nIn this paper, we introduce the Hierarchical Multitask Learning (HMTL) method that jointly learn multiple tasks by letting each task be, by itself, a MTL problem. The associated hierarchical multitask learning problem is then said to be composed of super-tasks and the tasks involved in each super-task as sub-tasks. Our formulation is motivated by the problem of constructing ESM ensembles for multiple climate variables, with multiple geolocations for each variable. The problem of obtaining ESMs weights for all regions for a certain climate variable is a super-task.\nThe paper is organized as follows. We first discuss existing MTL methods with task dependency estimation, on which our proposed ideas are built; then, we briefly talk about the climate projection problem that motived our work. In the sequel, we present our proposed framework, perform experiments and analyze the results. Concluding remarks and next research steps complete the paper.\nNotation. Let T be the number of super-tasks, d the problem dimension, and n(t,k) the number of samples for the (t, k)-th sub-task. For the purposes of the current paper, we assume that all super-tasks have m sub-tasks and all sub-tasks have problem dimension d. X(t,k) \u2208 Rn(t,k)\u00d7d and y(t,k) \u2208 Rn(t,k)\u00d71 are the input and output data for the k-th sub-task of the t-th super-task. \u0398(t) \u2208 Rd\u00d7m is the matrix whose columns are the set of weights for all sub-tasks for the t-th super-task, that is, \u0398(t) = [\u03b8(t,1), ...,\u03b8(t,m)]. For the ease of exposition, we represent {X} = X(t,k) and {Y } = y(t,k), k = 1, ...,mt; t = 1, ..., T . For the weight and precision matrices, {\u0398} = \u0398(t) and {\u2126} = \u2126(t),\u2200t = 1, ..., T . Identity matrix of dimension m\u00d7m is denoted by Im. U(a, b) is an uniform probability distribution in the range [a,b]."}, {"heading": "2 Multitask Learning with Task Dependence Estimation", "text": "Explicitly modeling task dependencies has been made by means of Bayesian models. Features across tasks (rows of the parameter matrix \u0398) were assumed to be drawn from a multivariate Gaussian distribution. Task relationship is then encoded in the inverse of the covariance matrix \u03a3\u22121 = \u2126, also known as precision matrix. Sparsity is desired in such matrix, as zero entries of the precision matrix indicate conditional independence between the corresponding two random variables (tasks) (Friedman et al., 2008). The associated learning problem (1) consists of jointly estimating the task parameters \u0398 and the precision matrix \u2126, which is done by an alternating optimization procedure.\nmin \u0398,\u2126 m\u2211 k=1 L(Xk,yk,\u0398)\u2212 log |\u2126|+ \u03bb0tr(\u0398\u2126\u0398>) +R(\u0398,\u2126)\ns.t. \u2126 0. (1)\nNote that in (1) the regularization penalty R(\u0398,\u2126) is a general penalization function that will be discussed later in the paper. A solution for (1) alternates between the following two steps until a stopping criterion is met:\nStep 1 Estimate \u0398 from current estimation of \u2126;\nStep 2 Estimate \u2126 from updated parameters \u0398.\nSetting initial \u2126 to identity matrix, i.e., all tasks are independent at the beginning, is usually a suitable start.\nIn Step 1, task dependency information is incorporated into the joint cost function through the trace term penalty tr(\u0398\u2126\u0398>). It helps to promote information exchange among tasks. The problem associated with Step 2, known as sparse inverse covariance selection problem (Friedman et al., 2008), seeks to find some sparsity pattern in the precision matrix. Experimental analysis have shown that these approaches usually outperform MTL with pre-defined task dependency structure for a variety of problems (Zhang and Schneider, 2010; Gonc\u0327alves et al., 2014)."}, {"heading": "3 Mathematical Formulation of ESMs Climate Projection", "text": "A common projection method is to perform the combination of multiple ESMs in a least square sense, that is, to estimate a set of weights for the ESMs based on past observations.\nFor a given location k the predicted climate variable (temperature, for example) for a certain timestamp i (expected mean temperature for a certain month/year, for example) is given by:\ny\u0302ik = \u3008xik,\u03b8k\u3009+ ik (2)\nwhere xik is the set of values predicted by the ESMs for the k-th location in the timestamp i, \u03b8k is the weight vector of each ESM for the k-th location, and ik is a residual. The weight vector \u03b8k is estimated from a training data. The combined estimate y\u0302ik is then used as a more robust prediction of temperature for the k-th location in a certain month/year in the future.\nESMs weights (\u03b8k) are defined for each geographical location and it possibly varies for different locations. It is possible that some ESMs are more accurate for some regions/climate than others\nand the difference between weights of two locations will reflect this behavior. In summary, the ESMs ensemble consists of solving a least square problem for each geographical location.\nThe ESMs weights may vary for the prediction of different climate variables, such as precipitation, temperature, and pressure. Then, solving an MTL problem for each climate variable is required. In this paper, we propose to simultaneously tackle multiple MTL problems through a hierarchical (two-level) MTL formulation."}, {"heading": "4 The HMTL Formulation", "text": "The HMTL formulation seeks to minimize the following cost function C(\u0393) with \u0393 = {{\u0398}, {\u2126}, \u03bb0}:\nL(\u0393) = T\u2211 t=1 ( mt\u2211 k=1 L ( X(t,k)\u03b8(t,k),y(t,k) ) \u2212 log |\u2126(t)|+ \u03bb0tr ( S(t)\u2126(t) )) +R({\u2126}) (3)\nwhere R({\u2126}) is a regularization term over the precision matrices, S(t) is the sample covariance matrix of the task parameters for the t-th super-task. For simplicity, we dropped the `1-penalization on the weight matrix \u0398 as is often done in MTL (Zhang and Schneider, 2010; Yang et al., 2013; Gonc\u0327alves et al., 2016). However, it can be added, if one desires sparse \u0398, with minor changes in the algorithm. All super-tasks are assumed to have the same number of sub-tasks. Squared loss was used as loss function.\nThe formulation (3) is a penalized cumulative cost function of the form (1) for several multitask learning problems. The penalty function R({\u2126}) is to favor common structural sparseness across the precision matrices. Here, we focus on the group lasso penalty (Yuan and Lin, 2006), which we denote by RG, and is defined as\nRG({\u2126}) = \u03bb1 T\u2211 t=1 \u2211 k 6=j |\u2126(t)kj |+ \u03bb2 \u2211 k 6=j \u221a\u221a\u221a\u221a T\u2211 t=1 \u2126 (t)2 kj (4)\nwhere \u03bb1 and \u03bb2 are two nonnegative tuning parameters. The first penalty term is an `1-penalization of the off-diagonal elements, so that non-structured sparsity in the precision matrices is enforced. The larger the value of \u03bb1, the sparser the precision matrices. The second term of the group sparsity penalty encourages the precision matrices for different super-tasks to have the same sparsity pattern. Group lasso does not impose any restriction on the degree of similarity of the non-zero entries of the precision matrices.\nSetting \u03bb2 to zero, super-tasks are decoupled into independent MTL formulations. Then, \u03bb2 can be seen as a coupling parameter, as larger values push the super-tasks to be coupled, so that different \u2126(t) have similar sparsity patterns. On the other hand, lower (or zero) values lead to decoupled super-tasks, recovering existing MTL formulations, such as in (Gonc\u0327alves et al., 2014).\nFigure 1 shows the hierarchy of tasks for the projection of multiple climate variables. At the level of super-tasks, group lasso regularization encourages precision matrices to have a similar sparseness pattern. The learned precision matrices are consequently used to control with whom each sub-task will share information.\nThe correspondence between the variables in the HMTL formulation and the elements in the climate problem is shown in Table 1."}, {"heading": "4.1 Optimization", "text": "Optimization problem (3) is not jointly convex on {\u0398} and {\u2126}, particularly due to the trace term which involves both variables. We then use an alternating minimization, in which {\u0398} is held fix and optimize for {\u2126} (we call it \u2126-step), and similarly fix {\u2126} and optimize for {\u0398} (we call it \u0398step). Both steps now consist of convex problems, for which efficient methods have been proposed. In the experiments, 20 to 30 iterations were required for convergence."}, {"heading": "4.1.1 Solving \u0398-step", "text": "The convex problem associated with this step is defined as\nmin {\u0398} T\u2211 t=1 mk\u2211 k=1 L ( X(t,k)\u03b8(t,k),y(t,k) ) + \u03bb0tr ( S(t)\u2126(t) ) . (5)\nConsidering the squared loss function, \u0398-step consists of two quadratic terms, as {\u2126} are positive semidefinite matrices. Note that the optimization for each super-task weight matrix \u0398(t) are independent and can be performed in parallel. We used the L-BFGS (Liu and Nocedal, 1989) method in the experiments."}, {"heading": "4.1.2 Solving \u2126-step", "text": "The \u2126-step is to solve the following optimization problem\nmin {\u2126} T\u2211 t=1 ( \u2212 log |\u2126(t)|+ \u03bb0tr(S(t)\u2126(t)) ) +RG({\u2126}) s.t. \u2126(t) 0, \u2200t = 1, ..., T.\n(6)\nThis step corresponds to the problem of joint learning multiple Gaussian graphical models and has been recently studied (Honorio and Samaras, 2010; Danaher et al., 2014; Mohan et al., 2014). These formulations seek to minimize the penalized joint negative log likelihood in the form of (6) and they basically differ in the penalization term R({\u2126}). Researchers have shown that the graphical models jointly estimated were able to increase the number of edges correctly identified while reducing the number of edges incorrectly identified, when compared to those independently estimated. An alternating direction method of multipliers (ADMM) proposed in (Danaher et al., 2014) was used to solve problem (6).\nAlgorithm (1) presents the pseudo-code for the proposed HMTL algorithm.\nAlgorithm 1: HMTL algorithm. Data: {X}, {Y}. Input: \u03bb0 > 0, \u03bb1 > 0 and \u03bb2 > 0. Result: {\u0398}, {\u2126}.\n1 begin 2 \u2126(t) = Imt ,\u2200t = 1, ..., T. 3 \u0398(t) = U(\u22120.5, 0.5),\u2200t = 1, ..., T. 4 repeat 5 Update {\u0398} by solving (5); 6 Update {\u2126} by solving (6); 7 until stopping condition met"}, {"heading": "5 Related Works", "text": "A lattice graph is used to represent the regional relationship in (Subbian and Banerjee, 2013), where immediate neighbor locations are assumed to have similar ESMs weights. Weights for all geolocations are estimated jointly (in a least square sense) with graph Laplacian regularization to encourage spatial smoothness.\nIn (McQuade and Monteleoni, 2013) the online ESMs ensemble problem is tackled by using a lattice Markov Random Field. The state of each hidden variable, which is associated with a geographical location, is the identity of the best ESM for that specific location. The marginal probabilities of the hidden variables act as the weights of the ensemble. Hence, ESMs with a higher probability of being the best has a larger weight in the ensemble. At each time step marginal probabilities are updated based on the performance of the ESMs in the previous time step via the loopy belief propagation algorithm.\nDifferently, (Gonc\u0327alves et al., 2014) do not assume any fixed dependence graph, but, instead, estimate it through a multitask learning model that makes use of a sparse Gaussian Markov Random Field (GMRF) to capture the relationship of ESMs weights among locations. ESM weights for all locations and the GMRF are jointly estimated via an alternating minimization scheme.\nOur work differs from the existing research as it (1) leverages information not only from immediate neighbors but also from any related geographical locations; (2) allows two levels of information sharing: ESMs weights and precision matrices that encodes the relationship of locations; and (3) handles the projection of multiple climate variables (exploring their resemblance) simultaneously."}, {"heading": "6 Experiments", "text": "In this section we present experiments to compare the proposed HMTL with existing methods in the literature for both synthetic and real climate data."}, {"heading": "6.1 Synthetic Data", "text": "We first generated a synthetic dataset to assess the performance of HMTL over traditional MTL methods. For comparison, we used the MTL method proposed in (Gonc\u0327alves et al., 2014, 2016), called MSSL, that has shown to be competitive with existing MTL algorithms including (Zhang and Yeung, 2010; Kumar and Daume III, 2012; Kang et al., 2011). We also seek to investigate the effect of the increase in the number of super-tasks. For this analysis we generated 7 super-tasks containing 15 sub-tasks each, with dimension of 50. For each sub-task 100 data samples were generated. Inverse covariance matrices \u2126(t), t = 1, ..., T were drawn from a Wishart distribution with a scale matrix \u039b and n = 10 degrees of freedom. Scale matrix \u039b was designed to reflect a structure containing three groups of related variables. As sampled precision matrices are also likely to present the group structure, jointly sparse precision matrices are suitable to capture such pattern, which is precisely what HMTL formulation assumes.\nGiven \u2126(t), the sub-tasks parameters \u0398(t) were constructed as: \u0398(t)(j, :) = N (0,\u2126(t)), j = 1, .., d; and, finally, the design matrix X(t,k) were sampled from N (0, I), and Y (t,k) = X(t,k)\u0398(t) + (t,k), where (t,k) \u223c N (0, 0.1), \u2200k = 1, ..,m; t = 1, .., T.\nTen independent runs for both HMTL and MSSL were carried out. Each run with a different random train/test data split. Table 2 shows the relative improvement1 of HMTL over MSSL for distinct number of super-tasks. Two scenarios were tested: using 50 and 30 samples for training and the remaining for test (50 and 70, respectively). Penalization parameters (\u03bb\u2019s) were chosen by cross-validation on the training set.\nFrom Table 2, we observe that HMTL has a continuous increase on the relative improvement compared to independently running MSSL, as the number of super-tasks grows. From the sixth super-task on, the relative improvement shows a tendency to stabilize. Such behavior is also observed in the scenario with only 30 training samples. It is worth noting that the smaller a set of training data (30 samples) the sharper the improvements obtained by HMTL over MSSL."}, {"heading": "6.2 Climate Data", "text": "We collected monthly land temperature and precipitation data of 32 CMIP5 ESMs (Taylor et al., 2012), from 1901 to 2000, in South America. Observed data provided by (Willmott and Matsuura, 2001) was used. ESMs predictions and observed values from 250 locations in South America (distributed in a grid shape) were considered. From the HMTL perspective, the problem involves: two super-tasks, 250 sub-tasks (per super-task) with dimensionality of 32.\nIn the climate domain, it is common to work with the relative measure of the climate variable to a value of reference, which is obtained from past information. In our experiments, we directly work on the raw data (not detrended). We investigate the performance of the algorithm in both seasonal and annual time scales, with focus on winter and summer. All ESMs and observed data are in the same time and spatial resolution. Temperature is in degree Celsius and precipitation in cm."}, {"heading": "6.2.1 Experimental Setup:", "text": "Based on climate data from a certain past (training) period, model parameters are estimated and the inference produces its projections for the future (test). Clearly, the length of the training period affects the performance of the algorithm. A moving window of 20, 30 and 50 years were used for training and the next 10 years for test. The performance is measured in terms of root-mean-squared error (RMSE).\nSeasonality strongly affect climate data analysis. Winter and summer precipitation patterns, for example, are distinct. Also, by looking at seasonal data, it becomes easier to identify anomalous\n1Relative improvement is given by the difference between MSSL and HMTL performance (RMSE) divided by MSSL performance as percents (%).\npatterns, possibly useful to characterize climate phenomena as El Nin\u0303o. We extracted summer and winter data and performed climate variable projection specifically for these seasons.\nFive baseline algorithms were considered:\n1. multi-model average (MMA): set equal weights for all ESMs. This is currently performed by IPCC (IPCC, 2013);\n2. best-ESM in training phase: it is not an ensemble, but a single best ESM in terms of mean squared error;\n3. ordinary least square (OLS): perform independent OLS for each location and climate variable;\n4. S2M2R (Subbian and Banerjee, 2013): can be seen as an MTL method with pre-defined location dependence given by the graph Laplacian. It incorporates spatial smoothing on ESMs weights.\n5. MSSL (Gonc\u0327alves et al., 2014): run MSSL for each climate variable projection independently. The parameter-based version (p-MSSL) was used.\nAll the penalization parameters of the methods (\u03bb\u2019s in MSSL and HMTL) were chosen by crossvalidation. From the training set, we selected the first 80% for training and the next 20% for validation. The best values in the validation set were selected. For example, in the scenario with 20 years of measurements for training, we took the first 16 years to really train the model, and the next 4 years to analyze the performance of the method using a specific setting of \u03bb\u2019s. Using this protocol, the selected parameter values were: S2M2R used \u03bb = 1000; MSSL \u03bb0 = 0.1 and \u03bb1 = 0.1; and HMTL \u03bb0 = 0.1, \u03bb1 = 0.0002, \u03bb2 = 0.01."}, {"heading": "6.2.2 Results:", "text": "Table 3 shows the RMSE of the projections produced by the algorithms and the observed values, for precipitation and temperature. First, we note that simply assigning equal weights to all ESMs does not seem to exploit the potential of ensemble methods. MMA (as used by IPCC, (IPCC, 2013)) presented the largest RMSE among the algorithms for the majority of periods (summer, winter and year) and number of years for training. Second, the MTL methods, MSSL and HMTL, clearly outperform the baseline methods. S2M2R does not always produce better projections than OLS. In fact, it is slightly worse for the year dataset. As expected, the assumption of spatial neighborhood dependence does not seem to hold for all climate variables.\nHMTL presented better results than performing MSSL for precipitation and temperature independently in many situations. HMTL was able to significantly reduce RMSE in summer precipitation projections, which has shown to be the most challenging scenario. Significant improvement was also seen for winter and year temperature projections.\nRMSE per geographical location for precipitation and temperature are presented in Figure 2. For precipitation, more accurate projections (lower RMSE) was obtained by HMTL in Northernmost regions of South America, including Colombia and Venezuela. More accurate temperature projections were obtained in central North region of South America, which comprises part of the Amazon rainforest.\nWe believe that the improvements of the hierarchical MTL model are due to the same reasons as general MTL models: reduction of sample complexity by leveraging information from other related super-tasks. As a consequence, the precision matrices, which guide information sharing among subtasks, are estimated more accurately. The reduced sample complexity probably explains the better climate variable prediction capacity in situations with limited amount of measurements (samples), as the results shown in Table 2."}, {"heading": "7 Concluding Remarks", "text": "A hierarchical multitask learning (HMTL) framework to deal with multiple MTL problems is proposed. It was motived by the problem of constructing Earth System Models (ESMs) ensemble for the simultaneous prediction of multiple climate variables. The formulation allows two levels of information sharing: (1) model parameters (coefficients of linear regression); and (2) precision matrices, which encodes the relationship of linear regressors. A group lasso regularization is responsible for capturing similar sparsity patterns across multiple precision matrices.\nExperiments on joint projection of temperature and precipitation in South America showed that the HMTL produced more accurate predictions in many situations, when compared to the independent execution of existing MTL methods for each climate variable. Simulations on synthetic datasets also showed that the proposed HMTL achieved higher performance as the number of internal MTL problems increase. Here, only temperature and precipitation were used, as they are two of the most studied variables in the climate literature. Future works include a wider analysis with other climate variables, such as geopotential heights and wind directions."}, {"heading": "8 Acknowledgments", "text": "We thank the anonymous reviewers for their valuable comments. AB was supported by NSF grants IIS-1563950, IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, CNS- 1314560, IIS-0953274, IIS-1029711, NASA grant NNX12AQ39A, and gifts from Adobe, IBM, and Yahoo. FJVZ thanks CNPq for the financial support (Process no. 309115/2014-0)."}], "references": [{"title": "The joint graphical lasso for inverse covariance", "author": ["P. Danaher", "P. Wang", "D.M. Witten"], "venue": null, "citeRegEx": "Danaher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Danaher et al\\.", "year": 2014}, {"title": "Sparse inverse covariance estimation", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "J. R. Stat. Soc. Series B Stat. Methodol.,", "citeRegEx": "Friedman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2008}, {"title": "Multi-task Sparse Structure Learning", "author": ["A.R. Gon\u00e7alves", "F.J. Von Zuben", "A. Banerjee"], "venue": "In ACM CIKM ,", "citeRegEx": "Gon\u00e7alves et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gon\u00e7alves et al\\.", "year": 2016}, {"title": "Intergovernmental Panel on Climate Change Fifth Assessment", "author": ["Z. Kang", "K. Grauman", "F. Sha"], "venue": "IPCC", "citeRegEx": "Kang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2013}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["A. Kumar", "H. Daume III"], "venue": null, "citeRegEx": "Kumar and III,? \\Q2012\\E", "shortCiteRegEx": "Kumar and III", "year": 2012}, {"title": "On the limited memory BFGS method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical Programming ,", "citeRegEx": "Liu and Nocedal,? \\Q1989\\E", "shortCiteRegEx": "Liu and Nocedal", "year": 1989}, {"title": "MRF-Based Spatial Expert Tracking of the Multi-Model Ensemble", "author": ["S. McQuade", "C. Monteleoni"], "venue": "In International Workshop on Climate Informatics", "citeRegEx": "McQuade and Monteleoni,? \\Q2013\\E", "shortCiteRegEx": "McQuade and Monteleoni", "year": 2013}, {"title": "High-dimensional graphs and variable selection with the lasso", "author": ["N. Meinshausen", "P. Buhlmann"], "venue": "Annals of Statistics,", "citeRegEx": "Meinshausen and Buhlmann,? \\Q2006\\E", "shortCiteRegEx": "Meinshausen and Buhlmann", "year": 2006}, {"title": "Node-based learning of multiple Gaussian graphical models", "author": ["K. Mohan", "P. London", "M. Fazel", "D. Witten", "Lee", "S.-I"], "venue": null, "citeRegEx": "Mohan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mohan et al\\.", "year": 2014}, {"title": "Addressing interdependency in a multimodel ensemble by interpolation of model properties", "author": ["B.M. Sanderson", "R. Knutti", "P. Caldwell"], "venue": "J. of Climate,", "citeRegEx": "Sanderson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sanderson et al\\.", "year": 2015}, {"title": "Climate Multi-model Regression using Spatial Smoothing", "author": ["K. Subbian", "A. Banerjee"], "venue": "In SIAM SDM ,", "citeRegEx": "Subbian and Banerjee,? \\Q2013\\E", "shortCiteRegEx": "Subbian and Banerjee", "year": 2013}, {"title": "An overview of CMIP5 and the experiment design", "author": ["K. Taylor", "R. Stouffer", "G. Meehl"], "venue": "Bull. of the Am. Met. Soc.,", "citeRegEx": "Taylor et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2012}, {"title": "The use of the multi-model ensemble in probabilistic climate projections", "author": ["C. Tebaldi", "R. Knutti"], "venue": "Phil. Trans. R. Soc. A,", "citeRegEx": "Tebaldi and Knutti,? \\Q2007\\E", "shortCiteRegEx": "Tebaldi and Knutti", "year": 2007}, {"title": "Terrestrial Air Temperature and Precipitation: Monthly and Annual Time Series (1950", "author": ["C.J. Willmott", "K. Matsuura"], "venue": null, "citeRegEx": "Willmott and Matsuura,? \\Q2001\\E", "shortCiteRegEx": "Willmott and Matsuura", "year": 2001}, {"title": "Multi-task learning with Gaussian matrix generalized inverse Gaussian model", "author": ["M. Yang", "Y. Li", "Z. Zhang"], "venue": "In ICML,", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "J. R. Stat. Soc. Series B Stat. Methodol.,", "citeRegEx": "Yuan and Lin,? \\Q2006\\E", "shortCiteRegEx": "Yuan and Lin", "year": 2006}, {"title": "Learning multiple tasks with sparse matrix-normal penalty", "author": ["Y. Zhang", "J. Schneider"], "venue": null, "citeRegEx": "Zhang and Schneider,? \\Q2010\\E", "shortCiteRegEx": "Zhang and Schneider", "year": 2010}, {"title": "A convex formulation for learning task relationships in multitask learning", "author": ["Y. Zhang", "Yeung", "D.-Y"], "venue": "In UAI ,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 12, "context": "atmosphere, oceans, land, and sea ice (Tebaldi and Knutti, 2007; IPCC, 2013).", "startOffset": 38, "endOffset": 76}, {"referenceID": 12, "context": "A single and possibly more robust projection can be built as a combination (ensemble) of multiple ESMs simulations (Tebaldi and Knutti, 2007; McQuade and Monteleoni, 2013; Sanderson et al., 2015).", "startOffset": 115, "endOffset": 195}, {"referenceID": 6, "context": "A single and possibly more robust projection can be built as a combination (ensemble) of multiple ESMs simulations (Tebaldi and Knutti, 2007; McQuade and Monteleoni, 2013; Sanderson et al., 2015).", "startOffset": 115, "endOffset": 195}, {"referenceID": 9, "context": "A single and possibly more robust projection can be built as a combination (ensemble) of multiple ESMs simulations (Tebaldi and Knutti, 2007; McQuade and Monteleoni, 2013; Sanderson et al., 2015).", "startOffset": 115, "endOffset": 195}, {"referenceID": 16, "context": "Modeling task relationship in multitask learning has been the focus of recent research (Zhang and Schneider, 2010; Zhang and Yeung, 2010; Yang et al., 2013; Gon\u00e7alves et al., 2016).", "startOffset": 87, "endOffset": 180}, {"referenceID": 14, "context": "Modeling task relationship in multitask learning has been the focus of recent research (Zhang and Schneider, 2010; Zhang and Yeung, 2010; Yang et al., 2013; Gon\u00e7alves et al., 2016).", "startOffset": 87, "endOffset": 180}, {"referenceID": 2, "context": "Modeling task relationship in multitask learning has been the focus of recent research (Zhang and Schneider, 2010; Zhang and Yeung, 2010; Yang et al., 2013; Gon\u00e7alves et al., 2016).", "startOffset": 87, "endOffset": 180}, {"referenceID": 7, "context": "The problem of estimating statistical dependency structure of a set of random variables is known as structure learning (Meinshausen and Buhlmann, 2006).", "startOffset": 119, "endOffset": 151}, {"referenceID": 2, "context": "In the MTL case (Gon\u00e7alves et al., 2016), the random variables are tasks parameters and, depending on the ratio between dimensionality and the number of tasks, the amount of data samples may not be sufficient.", "startOffset": 16, "endOffset": 40}, {"referenceID": 1, "context": "Sparsity is desired in such matrix, as zero entries of the precision matrix indicate conditional independence between the corresponding two random variables (tasks) (Friedman et al., 2008).", "startOffset": 165, "endOffset": 188}, {"referenceID": 1, "context": "The problem associated with Step 2, known as sparse inverse covariance selection problem (Friedman et al., 2008), seeks to find some sparsity pattern in the precision matrix.", "startOffset": 89, "endOffset": 112}, {"referenceID": 16, "context": "Experimental analysis have shown that these approaches usually outperform MTL with pre-defined task dependency structure for a variety of problems (Zhang and Schneider, 2010; Gon\u00e7alves et al., 2014).", "startOffset": 147, "endOffset": 198}, {"referenceID": 16, "context": "For simplicity, we dropped the `1-penalization on the weight matrix \u0398 as is often done in MTL (Zhang and Schneider, 2010; Yang et al., 2013; Gon\u00e7alves et al., 2016).", "startOffset": 94, "endOffset": 164}, {"referenceID": 14, "context": "For simplicity, we dropped the `1-penalization on the weight matrix \u0398 as is often done in MTL (Zhang and Schneider, 2010; Yang et al., 2013; Gon\u00e7alves et al., 2016).", "startOffset": 94, "endOffset": 164}, {"referenceID": 2, "context": "For simplicity, we dropped the `1-penalization on the weight matrix \u0398 as is often done in MTL (Zhang and Schneider, 2010; Yang et al., 2013; Gon\u00e7alves et al., 2016).", "startOffset": 94, "endOffset": 164}, {"referenceID": 15, "context": "Here, we focus on the group lasso penalty (Yuan and Lin, 2006), which we denote by RG, and is defined as", "startOffset": 42, "endOffset": 62}, {"referenceID": 5, "context": "We used the L-BFGS (Liu and Nocedal, 1989) method in the experiments.", "startOffset": 19, "endOffset": 42}, {"referenceID": 0, "context": "This step corresponds to the problem of joint learning multiple Gaussian graphical models and has been recently studied (Honorio and Samaras, 2010; Danaher et al., 2014; Mohan et al., 2014).", "startOffset": 120, "endOffset": 189}, {"referenceID": 8, "context": "This step corresponds to the problem of joint learning multiple Gaussian graphical models and has been recently studied (Honorio and Samaras, 2010; Danaher et al., 2014; Mohan et al., 2014).", "startOffset": 120, "endOffset": 189}, {"referenceID": 0, "context": "An alternating direction method of multipliers (ADMM) proposed in (Danaher et al., 2014) was used to solve problem (6).", "startOffset": 66, "endOffset": 88}, {"referenceID": 10, "context": "A lattice graph is used to represent the regional relationship in (Subbian and Banerjee, 2013), where immediate neighbor locations are assumed to have similar ESMs weights.", "startOffset": 66, "endOffset": 94}, {"referenceID": 6, "context": "In (McQuade and Monteleoni, 2013) the online ESMs ensemble problem is tackled by using a lattice Markov Random Field.", "startOffset": 3, "endOffset": 33}, {"referenceID": 11, "context": "We collected monthly land temperature and precipitation data of 32 CMIP5 ESMs (Taylor et al., 2012), from 1901 to 2000, in South America.", "startOffset": 78, "endOffset": 99}, {"referenceID": 13, "context": "Observed data provided by (Willmott and Matsuura, 2001) was used.", "startOffset": 26, "endOffset": 55}, {"referenceID": 10, "context": "S2M2R (Subbian and Banerjee, 2013): can be seen as an MTL method with pre-defined location dependence given by the graph Laplacian.", "startOffset": 6, "endOffset": 34}], "year": 2017, "abstractText": "Future projection of climate is typically obtained by combining outputs from multiple Earth System Models (ESMs) for several climate variables such as temperature and precipitation. While IPCC has traditionally used a simple model output average, recent work has illustrated potential advantages of using a multitask learning (MTL) framework for projections of individual climate variables. In this paper we introduce a framework for hierarchical multitask learning (HMTL) with two levels of tasks such that each super-task, i.e., task at the top level, is itself a multitask learning problem over sub-tasks. For climate projections, each super-task focuses on projections of specific climate variables spatially using an MTL formulation. For the proposed HMTL approach, a group lasso regularization is added to couple parameters across the supertasks, which in the climate context helps exploit relationships among the behavior of different climate variables at a given spatial location. We show that some recent works on MTL based on learning task dependency structures can be viewed as special cases of HMTL. Experiments on synthetic and real climate data show that HMTL produces better results than decoupled MTL methods applied separately on the super-tasks and HMTL significantly outperforms baselines for climate projection. Keywords\u2014 Multitask Learning, Structure Learning, Spatial Regression, Structured Regression, Earth System Models Ensemble.", "creator": "LaTeX with hyperref package"}}}