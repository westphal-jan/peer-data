{"id": "1701.08978", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2017", "title": "Mixed Low-precision Deep Learning Inference using Dynamic Fixed Point", "abstract": "We propose a cluster-based quantization method to convert pre-trained full precision weights into ternary weights with minimal impact on the accuracy. In addition, we also constrain the activations to 8-bits thus enabling sub 8-bit full integer inference pipeline. Our method uses smaller clusters of N filters with a common scaling factor to minimize the quantization loss, while also maximizing the number of ternary operations. We show that with a cluster size of N=4 on Resnet-101, can achieve 71.8% TOP-1 accuracy, within 6% of the best full precision results while replacing \\approx 85% of all multiplications with 8-bit accumulations. Using the same method with 4-bit weights achieves 76.3% TOP-1 accuracy which within 2% of the full precision result. We also study the impact of the size of the cluster on both performance and accuracy, larger cluster sizes N=64 can replace \\approx 98% of the multiplications with ternary operations but introduces significant drop in accuracy which necessitates fine tuning the parameters with retraining the network at lower precision. To address this we have also trained low-precision Resnet-50 with 8-bit activations and ternary weights by pre-initializing the network with full precision weights and achieve 68.9% TOP-1 accuracy within 4 additional epochs. Our final quantized model can run on a full 8-bit compute pipeline, with a potential 16x improvement in performance compared to baseline full-precision models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 31 Jan 2017 10:28:37 GMT  (70kb,D)", "https://arxiv.org/abs/1701.08978v1", null], ["v2", "Wed, 1 Feb 2017 04:09:31 GMT  (70kb,D)", "http://arxiv.org/abs/1701.08978v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["naveen mellempudi", "abhisek kundu", "dipankar das", "dheevatsa mudigere", "bharat kaul"], "accepted": false, "id": "1701.08978"}, "pdf": {"name": "1701.08978.pdf", "metadata": {"source": "CRF", "title": "Mixed Low-precision Deep Learning Inference using Dynamic Fixed Point", "authors": ["Naveen Mellempudi", "Abhisek Kundu", "Dipankar Das", "Dheevatsa Mudigere", "Bharat Kaul"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Deep Learning has achieved unparalleled success with large-scale machine learning. Deep Learning models are used for achieving state-of-the-art results on a wide variety of tasks including Computer Vision, Natural Language Processing, Automatic Speech Recognition and Reinforcement Learning [1]. Mathematically this involves solving a complex non-convex optimization problem with order of millions or more parameters. Solving this optimization problem - also referred to as training the neural network is a compute-intensive process that for current state-of-art networks requires days to weeks. Once trained, the DNN is used by evaluating this many-parameter function on specific input data - usually referred to as inference. While the compute intensity for inference is much lower than that of training, inference also involves significant amount of compute. Moreover, owing to the fact that inference is done on a large number of input data, the total computing resources spent on inference is likely to dwarf those that are spent on training. Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11]. Furthermore, there some theoretical evidence and numerous empirical observations that deep learning operations can be successfully done with much lower precision.\nIn this work we focus on reducing the compute requirements for deep learning inference, by directly quantizing pre-trained models with minimum (or no) retraining and achieve near state-of-art accuracy.\nar X\niv :1\n70 1.\n08 97\n8v 2\n[ cs\n.L G\n] 1\nF eb\n2 01\nOur paper makes the following contributions:\n1. We propose a novel cluster-based quantization method to convert pre-trained weights to lower precision representation with minimal loss in test accuracy.\n2. On Resnet-101 with 8-bit activations and using cluster size (N=4) to quantize weights, we achieve 76.3% TOP-1 accuracy with 4-bit weights and 71.8% TOP-1 accuracy with 2-bit ternary weights. To the best of our knowledge this is the best reported accuracy with ternary weights on ImageNet dataset[3], without retraining the network.\n3. We explore the performance-accuracy trade-off using different cluster sizes with ternary weight representation. For a cluster size of N, we reduce the higher precision ops (8-bit multiply) to one for every N \u2217K2 lower precision ops (8-bit accumulation), which results significant reduction in computation complexity. Using smaller cluster size of N=4 we achieve state-of-the-accuracy, but larger cluster sizes (N=64) would require retraining the network at lower precision to achieve comparable accuracy.\n4. We train a pre-initialized low precision Resnet-50 using 8-bit activations and 2-bit weights using larger cluster (N=64) and achieve 68.9% TOP-1 accuracy on ImageNet dataset[3] within 4-epochs of fine-tuning."}, {"heading": "2 Related Work", "text": "Deep learning training and inferencing are highly compute intensive operations, however using full precision (FP32) computations on conventional hardware is inefficient and not strictly warranted from functional point-of-view. To address this issue, there has been a lot of interest at using lower precision for deep learning, in an attempt to identify the minimum required precision to ensure functional correctness within acceptable thresholds.\nIn the past many researches have proposed low-precision alternatives to perform deep learning tasks. Vanhoucke et al.[12] showed that using 8-bit fixed-point arithmetic convolution networks can be sped up by up to 10x on speech recognition tasks on general purpose CPU hardware. Gupta et al.[4] have successfully trained networks using 16-bit fixed point on custom hardware. Miyashita et al.[8] used log quantization on pre-trained models and achieved good accuracy by tuning the bit length for each layer. More recently, Venkatesh et al.[13] achieved near state of the art results using 32b activations with 2-bit ternary weights on Imagenet dataset. Hubara et al.[5] have demonstrated that with weights as binary values training from scratch can achieve near state-of-the-art results for ILSVRC 2012 image classification task[9]."}, {"heading": "3 Low Precision Inference", "text": "In this paper, we primarily focus on improving the performance and accuracy of the inference task. We explore possibility of achieving high accuracy using sub 8-bit precision on state-of-the-art networks without expensive retraining. Previous work from Miyashita et al.[8] showed that by compressing the dynamic range of the input, it is possible to minimize the quantization loss and achieve high accuracy. We take a different approach to minimize the impact of dynamic range on quantization. We propose a cluster-based quantization method that groups weights into smaller clusters and quantize each cluster with a unique scaling factor. We use static clustering to group filters that accumulate to the same output feature to simplify the convolution operations. Empirical evidence also suggests that these clusters which learn similar features tend to have smaller dynamic range. Using dynamic fixed point representation, this method can effectively minimize the quantization errors and improve the inference accuracy of quantized networks. Applying this scheme on a pre-trained Resnet-101 model, with 4-bit weights and 8-bit activations, we achieve 76.3% TOP-1 accuracy on ImageNet dataset[3], without any retraining."}, {"heading": "3.1 2-bit Ternary Weights", "text": "Going below 4-bits we use the the ternary representation for weights, following the threshold based approximation proposed by Li et al [7], i.e., approximate full-precision weight W \u2248 \u03b1W\u0302 in `2 norm\nAlgorithm 1 Ternarize Weights\n1: Input: Learned full-precision weights W of a layer with d filters. 2: Group filters into k clusters: {Gj}, j = 1, ..., k. Let N = |Gj | (number of filters in Gj). 3: For each cluster Gj 4: Run Algorithm 2 on each filter W \u2208 Gj , and store the thresholds as a vector \u03b1. 5: For t = 1, ..., N , Tt = {i : \u03b1i belongs to the top t elements of sorted \u03b1}. 6: Set \u03b1t = \u221a\u2211 i\u2208Tt \u03b1 2 i /|Tt|. 7: Construct W\u0302(t), such that, W\u0302(t)i = Sign(Wi), if |Wi| > \u03b1t, and 0 otherwise. 8: Find \u03b1t\u2217 and W\u0302\u2217(t) that minimizes \u2211 W\u2208Gj \u2016W \u2212 \u03b1tW\u0302\n(t)\u20162F . 9: Let \u03b1\u0302t\u2217 be a reduced-precision representation of \u03b1t\u2217 .\n10: Output: k number of \u03b1\u0302t\u2217 and the group of ternary weights W\u0302.\nAlgorithm 2 Threshold Selection\n1: Input: W \u2208 Rn. 2: Sort elements of W according to magnitude. 3: For \u03c4 \u2208 [0, 1], I\u03c4 = {i : |Wi| belongs to the top b \u03c4 \u00b7 n c elements of sorted list}. 4: Construct W\u0302(\u03c4), such that, W\u0302(\u03c4)i = Sign(Wi), for i \u2208 I\u03c4 , and 0 otherwise. 5: Set \u03b1\u03c4 = \u221a\u2211 i\u2208I\u03c4 W 2 i /|I\u03c4 |.\n6: Compute \u03b1\u03c4\u2217 that minimizes \u2016W \u2212 \u03b1\u03c4W\u0302(\u03c4)\u20162F , for \u03c4 \u2208 [0, 1]. 7: Output: \u03b1\u03c4\u2217\n\u2016 \u00b7 \u2016F , where \u03b1 is a scaling factor and W\u0302 is a ternary weight with W\u0302i \u2208 {\u22121, 0,+1}. Where W is the matrix representing learned full-precision weights, and W\u0302 represents the corresponding ternary representation. We apply the block-quantization method described in section-3, to compute multiple scaling factors for each layer to minimize the accuracy loss. Our method differs from [7] in the approximation used for computing scaling factor (\u03b1). We use the RMS formulation as shown by the equation (1). The intuition behind using RMS term is to push the threshold parameter towards larger values within the cluster which helps speed up weight pruning.\n\u03b1 =\n\u221a\u2211 i\u2208I\u03c4 W 2 i\n|I\u03c4 | ,where |I\u03c4 | is the number of elements in I\u03c4 . (1)\nIn addition, we run our search algorithm1 in hierarchical fashion by minimizing the error within each filter first and then within the cluster of filters. Experimental evidence shows that these improvements help finding the optimal scaling factor that minimizes quantization loss.\nUsing multiple scaling factors can lead to more 8-bit multiplications. Hence, we choose the cluster size carefully to improve the ratio of low-precision (2-bit) to high-precision (8-bit) operations (Section 3.3). Our algorithm (Algorithm 1) takes the full-precision learned weights and returns clusters of ternary representation of groups of kernels along with their scaling factors. We further quantize the scaling factors down to 8-bit to eliminate any operation that requires more than 8 bits. Applying this scheme on pre-trained ResNet-101 model, using 8-bit activations we achieve 71.8% TOP-1 accuracy on ImageNet dataset."}, {"heading": "3.2 C1 and BatchNorm Layers", "text": "In our experiments we keep weights of the first convolution layers at 8-bits to prevent from accumulating losses while the rest of the layers including fully connected layers operate at lower precision. We also recompute the batch norm parameters during the inference phase to compensate for the shift in variance introduced by quantization. This is essential for making it work, when we are not retraining at lower precision. We are exploring the possibility of fusing batch normalization layers with the convolution layers before quantization to avoid this extra computation."}, {"heading": "3.3 Performance Implications", "text": "Choosing the right cluster size is a trade-off between performance and accuracy, while having one cluster per layer favors higher compute density by eliminating all multiplications, it\u2019s not ideal for achieving high accuracy. Although, previous research in this space[13] showed that it is possible to recover some of this lost accuracy through retraining. It\u2019s not always ideal, because of the costs involved in retraining these networks in low-precision, not to mention the technical difficulties involved in achieving reasonable solution on these networks.\nWe explored the accuracy-performance trade-off with various cluster sizes. Our experiments show on Resnet-101, using a cluster size of N = 4 can achieve 71.8% TOP-1 accuracy, within 6% of the full precision result. This result significant because this is to the best of our knowledge highest accuracy achieved on Imagenet dataset[3] without retraining the network in low-precision. In terms of performance impact, the clustering will result in one 8-bit multiplication for the entire cluster (N \u2217 K2) of ternary accumulations. Assuming roughly 50% of the convolutions are 3x3 and the rest are 1x1, using this block size can potentially replace 85% of multiplications in Resnet-101 convolution layers with simple 8-bit accumulations. For networks that predominantly use filters that are 3x3 or bigger, this ratio would be greater than 95%. We explored the accuracy-performance trade-off with various cluster sizes, we concluded that using cluster size of N = 64, we can replace \u2248 98% of multiplications in Resnet-101 with 8-bit accumulations, but with a significant loss to the accuracy. At this point retraining the network at lower precision would be necessary."}, {"heading": "4 Training with Low-precision", "text": "We trained the low precision ResNet-50 on ImageNet dataset using 2-bit weights and 8-bit activations by initializing the network with pre-trained full precision model. We take the approach proposed by Marcel et al.[10], and replace data pre-processing steps such as mean-subtraction and jittering with batch normalization layer inserted right after the data later. We obtained the pre-trained models published by Marcel et al.[10] and fine-tune the parameters of our low-precision network. In the forward pass, the weights are converted to 2-bit ternary values using the algorithm described in 1 in all convolution layers, except the first layer, where the weights are quantized to 8-bit fixed point representation. Activations are quantized to 8-bit fixed point in all layers including ReLU, BatchNorm layers. We did not quantize the weights in FC layer for the training exercise. Gradient updates are performed in full precision for convolution and FC layers. We reduced the learning rate to an order of 1e-4, in order to avoid exploding gradients problem, while we keep all the other hyper parameters same as that of full precision training. After running for 4-epochs, we recovered most of the accuracy\nand achieved 68.6% Top-1 and 88.7% Top-5 accuracy compared to our baseline 75.02%(Top-1), 92.2%(Top-5)."}, {"heading": "5 Conclusion", "text": "We propose a clustering based quantization method which exploits local correlations in dynamic range of the parameters to minimize the impact of quantization on overall accuracy. We demonstrate near SOTA accuracy on Imagenet data-set using pre-trained models with quantized networks without any low precision training. On Resnet-101 using 8-bit activations the error from the best published full precision (FP32) result is within \u2248 6% for ternary weights and within \u2248 2% for 4-bit weights. To the best of our knowledge this is the best achieved accuracy with ternary weights for Imagenet dataset.\nOur clustering based approach allows for tailoring solutions for specific hardware, based on the accuracy and performance requirements. Smaller cluster sizes achieves best accuracy, with N=4 \u2248 85% of the computations as low precision operations (simple 8-bit accumulations) and this is better suited for implementation on specialized hardware. Larger cluster sizes are more suited to current general purpose hardware, with a larger portion of computations as low precision operations (> 98% for N=64), however this comes with the cost of reduced accuracy. This gap can be bridged with additional low precision training as show in section 4, work is underway to further improve this accuracy. Our final quantized model can be efficiently run on full 8-bit compute pipeline, thus offering a potential 16X performance-power benefit.\nFurthermore as continuation of this work, we are looking in a more theoretical exploration to better understand the formal relationship between the clustering and final accuracy, with an attempt establish realistic bounds for given network-performance-accuracy requirement."}], "references": [{"title": "Deep learning. Book in preparation for", "author": ["Yoshua Bengio", "Ian Goodfellow", "Aaron Courville"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1", "author": ["Matthieu Courbariaux", "Itay Hubara", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Deep learning with limited numerical precision", "author": ["Suyog Gupta", "Ankur Agrawal", "Kailash Gopalakrishnan", "Pritish Narayanan"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Binarized neural networks", "author": ["Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Quantized neural networks: Training neural networks with low precision weights and activations", "author": ["Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.07061,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Ternary weight networks", "author": ["Fengfu Li", "Bo Zhang", "Bin Liu"], "venue": "arXiv preprint arXiv:1605.04711,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Convolutional neural networks using logarithmic data representation", "author": ["Daisuke Miyashita", "Edward H Lee", "Boris Murmann"], "venue": "arXiv preprint arXiv:1603.01025,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Imagenet pre-trained models with batch normalization", "author": ["Marcel Simon", "Erik Rodner", "Joachim Denzler"], "venue": "arXiv preprint arXiv:1612.01452v2,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Finn: A framework for fast, scalable binarized neural network inference", "author": ["Yaman Umuroglu", "Nicholas J Fraser", "Giulio Gambardella", "Michaela Blott", "Philip Leong", "Magnus Jahre", "Kees Vissers"], "venue": "arXiv preprint arXiv:1612.07119,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vincent Vanhoucke", "Andrew Senior", "Mark Z Mao"], "venue": "In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Accelerating deep convolutional networks using low-precision and sparsity", "author": ["Ganesh Venkatesh", "Eriko Nurvitadhi", "Debbie Marr"], "venue": "arXiv preprint arXiv:1610.00324,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients", "author": ["Shuchang Zhou", "Yuxin Wu", "Zekun Ni", "Xinyu Zhou", "He Wen", "Yuheng Zou"], "venue": "arXiv preprint arXiv:1606.06160,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Trained ternary quantization", "author": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1612.01064,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Deep Learning models are used for achieving state-of-the-art results on a wide variety of tasks including Computer Vision, Natural Language Processing, Automatic Speech Recognition and Reinforcement Learning [1].", "startOffset": 208, "endOffset": 211}, {"referenceID": 5, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 175, "endOffset": 194}, {"referenceID": 1, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 175, "endOffset": 194}, {"referenceID": 4, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 175, "endOffset": 194}, {"referenceID": 13, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 175, "endOffset": 194}, {"referenceID": 7, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 175, "endOffset": 194}, {"referenceID": 6, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 175, "endOffset": 194}, {"referenceID": 3, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 281, "endOffset": 296}, {"referenceID": 14, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 281, "endOffset": 296}, {"referenceID": 12, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 281, "endOffset": 296}, {"referenceID": 10, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 281, "endOffset": 296}, {"referenceID": 2, "context": "To the best of our knowledge this is the best reported accuracy with ternary weights on ImageNet dataset[3], without retraining the network.", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "9% TOP-1 accuracy on ImageNet dataset[3] within 4-epochs of fine-tuning.", "startOffset": 37, "endOffset": 40}, {"referenceID": 11, "context": "[12] showed that using 8-bit fixed-point arithmetic convolution networks can be sped up by up to 10x on speech recognition tasks on general purpose CPU hardware.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] have successfully trained networks using 16-bit fixed point on custom hardware.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] used log quantization on pre-trained models and achieved good accuracy by tuning the bit length for each layer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13] achieved near state of the art results using 32b activations with 2-bit ternary weights on Imagenet dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] have demonstrated that with weights as binary values training from scratch can achieve near state-of-the-art results for ILSVRC 2012 image classification task[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[5] have demonstrated that with weights as binary values training from scratch can achieve near state-of-the-art results for ILSVRC 2012 image classification task[9].", "startOffset": 162, "endOffset": 165}, {"referenceID": 7, "context": "[8] showed that by compressing the dynamic range of the input, it is possible to minimize the quantization loss and achieve high accuracy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "3% TOP-1 accuracy on ImageNet dataset[3], without any retraining.", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "Going below 4-bits we use the the ternary representation for weights, following the threshold based approximation proposed by Li et al [7], i.", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": "3: For \u03c4 \u2208 [0, 1], I\u03c4 = {i : |Wi| belongs to the top b \u03c4 \u00b7 n c elements of sorted list}.", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "6: Compute \u03b1\u03c4\u2217 that minimizes \u2016W \u2212 \u03b1\u03c4\u0174\u2016F , for \u03c4 \u2208 [0, 1].", "startOffset": 51, "endOffset": 57}, {"referenceID": 6, "context": "Our method differs from [7] in the approximation used for computing scaling factor (\u03b1).", "startOffset": 24, "endOffset": 27}, {"referenceID": 12, "context": "Although, previous research in this space[13] showed that it is possible to recover some of this lost accuracy through retraining.", "startOffset": 41, "endOffset": 45}, {"referenceID": 2, "context": "This result significant because this is to the best of our knowledge highest accuracy achieved on Imagenet dataset[3] without retraining the network in low-precision.", "startOffset": 114, "endOffset": 117}, {"referenceID": 9, "context": "[10], and replace data pre-processing steps such as mean-subtraction and jittering with batch normalization layer inserted right after the data later.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] and fine-tune the parameters of our low-precision network.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "We propose a cluster-based quantization method to convert pre-trained full precision weights into ternary weights with minimal impact on the accuracy. In addition we also constrain the activations to 8-bits thus enabling sub 8-bit full integer inference pipeline. Our method uses smaller clusters of N filters with a common scaling factor to minimize the quantization loss, while also maximizing the number of ternary operations. We show that with cluster size of N=4 on Resnet-101, can achieve 71.8% TOP-1 accuracy, within 6% of the best full precision result, while replacing \u2248 85% of all multiplications with 8-bit accumulations. Using the same method with 4-bit weights achieves 76.3% TOP-1 accuracy which within 2% of the full precision result. We also study the impact of the size of the cluster on both performance and accuracy, larger cluster sizes N=64 can replace \u2248 98% of the multiplications with ternary operations but introduces significant drop in accuracy which necessitates fine tuning the parameters with retraining the network at lower precision. To address this we have also trained low-precision Resnet-50 with 8-bit activations and ternary weights by pre-initializing the network with full precision weights and achieve 68.9% TOP-1 accuracy within 4 additional epochs. Our final quantized model can run on a full 8-bit compute pipeline, with a potential 16x improvement in performance compared to baseline full-precision models.", "creator": "LaTeX with hyperref package"}}}