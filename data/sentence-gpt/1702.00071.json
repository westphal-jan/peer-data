{"id": "1702.00071", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2017", "title": "On orthogonality and learning recurrent networks with long term dependencies", "abstract": "It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. The goal of this paper is to explain the process of gradient training and the process of learning in depth. The goal of this paper is to introduce a system for developing neural networks in deep neural networks that will be the basis for a system based on neural networks. For this purpose, we propose the principle that reinforcement learning will be a very powerful learning technique in the discipline of reinforcement learning. Neural networks provide powerful ways to reward individuals for their actions and motivate them to do so.\n\n\n\n\n\nWe thank Dr. Gage, Dr. John E. Johnson, and Dr. Jorgensen, Dr. Radeem Jankovi\u0107, Dr. Sadeem Jankovi\u0107, Dr. A. Hurd, Dr. Zorljana Sefaik, Dr. Radeem Jankovi\u0107, Dr. C. P. Sadeem Jankovi\u0107, Dr. G. Kudrin, Dr. Radeem Jankovi\u0107, Dr. Radeem Jankovi\u0107, Dr. D. J. Radeem Jankovi\u0107, Dr. B. Radeem Jankovi\u0107, Dr. S. Zorljana Sefaik, Dr. L. Wieger, Dr. J. Radeem Jankovi\u0107, Dr. Radeem Jankovi\u0107, Dr. L. Wieger, Dr. J. Wieger, Dr. J. Wieger, Dr. L. Wieger, Dr. J. Wieger, Dr. T. Wieger, Dr. J. Wieger, Dr. L. Wieger, Dr. L. Wieger, Dr. Radeem Jankovi\u0107, Dr. Wieger, Dr. Radeem Jankovi\u0107, Dr. L. Wieger, Dr. M. Wieger, Dr. Radeem Jankovi\u0107, Dr. Radeem Jankovi\u0107, Dr. D. J. Radeem Jankovi\u0107, Dr. T. Wieger,", "histories": [["v1", "Tue, 31 Jan 2017 22:14:59 GMT  (5131kb,D)", "http://arxiv.org/abs/1702.00071v1", null], ["v2", "Fri, 3 Mar 2017 11:09:28 GMT  (4031kb,D)", "http://arxiv.org/abs/1702.00071v2", null], ["v3", "Mon, 12 Jun 2017 23:12:14 GMT  (7139kb,D)", "http://arxiv.org/abs/1702.00071v3", null], ["v4", "Thu, 12 Oct 2017 17:18:51 GMT  (8317kb,D)", "http://arxiv.org/abs/1702.00071v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["eugene vorontsov", "chiheb trabelsi", "samuel kadoury", "chris pal"], "accepted": true, "id": "1702.00071"}, "pdf": {"name": "1702.00071.pdf", "metadata": {"source": "CRF", "title": "ON ORTHOGONALITY AND LEARNING RECURRENT NETWORKS WITH LONG TERM DEPENDENCIES", "authors": ["Eugene Vorontsov", "Chiheb Trabelsi", "Samuel Kadoury", "Chris Pal"], "emails": ["christopher.pal}@polymtl.ca"], "sections": [{"heading": "1 INTRODUCTION", "text": "The depth of deep neural networks confers representational power, but also makes model optimization more challenging. Training deep networks with gradient descent based methods is known to be difficult as a consequence of the vanishing and exploding gradient problem (Hochreiter & Schmidhuber, 1997). Typically, exploding gradients are avoided by clipping large gradients (Pascanu et al., 2013) or introducing an L2 or L1 weight norm penalty. The latter has the effect of bounding the spectral radius of the linear transformations, thus limiting the maximal gain across the transformation. Krueger & Memisevic (2015) attempt to stabilize the norm of propagating signals directly by penalizing differences in successive norm pairs in the forward pass and Pascanu et al. (2013) propose to penalize successive gradient norm pairs in the backward pass. These regularizers affect the network parameterization with respect to the data instead of penalizing weights directly.\nBoth expansivity and contractivity of linear transformations can also be limited by more tightly bounding their spectra. By limiting the transformations to be orthogonal, their singular spectra are limited to unitary gain causing the transformations to be norm-preserving. Le et al. (2015) and Henaff et al. (2016) have respectively shown that identity initialization and orthogonal initialization can be beneficial. Arjovsky et al. (2015) have gone beyond initialization, building unitary recurrent neural network (RNN) models with transformations that are unitary by construction which they achieved by composing multiple basic unitary transformations. The resulting transformations, for some n-dimensional input, cover only some subset of possible n \u00d7 n unitary matrices but appear to perform well on simple tasks and have the benefit of having low complexity in memory and computation.\nThe entire set of possible unitary or orthogonal parameterizations forms the Stiefel manifold. At a much higher computational cost, gradient descent optimization directly along this manifold can be done via geodesic steps (Nishimori, 2005; Tagare, 2011). Recent work (Wisdom et al., 2016) has proposed the optimization of unitary matrices along the Stiefel manifold using geodesic gradient descent. To produce a full-capacity parameterization for unitary matrices they use some insights\nar X\niv :1\n70 2.\n00 07\n1v 1\n[ cs\n.L G\n] 3\n1 Ja\nn 20\nfrom Tagare (2011), combining the use of a canonical inner products and Cayley transformations. Their experimental work indicates that full capacity unitary RNN models can solve the copy memory problem whereas both LSTM networks and restricted capacity unitary RNN models having similar complexity appear unable to solve the task for a longer sequence length (T = 2000).\nIn contrast, here we explore the optimization of real valued matrices within a configurable margin about the Stiefel manifold. We suspect that a strong constraint of orthogonality limits the model\u2019s representational power, hindering its performance, and may make optimization more difficult. We explore this hypothesis empirically by employing a factorization technique that allows us to limit the degree of deviation from the Stiefel manifold. While we use geodesic gradient descent, we simultaneously update the singular spectra of our matrices along Euclidean steps, allowing optimization to step away from the manifold while still curving about it."}, {"heading": "1.1 VANISHING AND EXPLODING GRADIENTS", "text": "The issue of vanishing and exploding gradients as it pertains to the parameterization of neural networks can be illuminated by looking at the gradient back-propagation chain through a network.\nA neural network with n hidden layers has pre-activations\nai(hi\u22121) = Wi hi\u22121 + bi, i \u2208 {2, \u00b7 \u00b7 \u00b7 , n} (1)\nFor notational convenience, we combine parameters Wi and bi to form an affine matrix \u03b8. We can see that for some loss function L at layer n , the derivative with respect to parameters \u03b8i is:\n\u2202L \u2202\u03b8i = \u2202an+1 \u2202\u03b8i \u2202L \u2202an+1 (2)\nThe partial derivatives for the pre-activations can be decomposed as follows:\n\u2202ai+1 \u2202\u03b8i = \u2202ai \u2202\u03b8i \u2202hi \u2202ai \u2202ai+1 \u2202hi\n= \u2202ai \u2202\u03b8i DiWi+1 \u2192 \u2202ai+1 \u2202ai = DiWi+1,\n(3)\nwhere Di is the Jacobian corresponding to the activation function, containing partial derivatives of the hidden units at layer i + 1 with respect to the pre-activation inputs. Typically, D is diagonal. Following the above, the gradient in equation 2 can be fully decomposed into a recursive chain of matrix products:\n\u2202L \u2202\u03b8i = \u2202ai \u2202\u03b8i n\u220f j=i (DjWj+1) \u2202L \u2202an+1 (4)\nIn (Pascanu et al., 2013), it is shown that the 2-norm of \u2202ai+1 \u2202ai is bounded by the product of the norms of the non-linearity\u2019s Jacobian and transition matrix at time t (layer i ), as follows:\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2202at+1\u2202at \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2264 ||Dt|| ||Wt|| \u2264 \u03bbDt \u03bbWt = \u03b7t,\n\u03bbDt , \u03bbWt \u2208 R. (5)\nwhere \u03bbDt and \u03bbWt are the largest singular values of the non-linearity\u2019s Jacobian Dt and the transition matrix Wt . In RNNs, Wt is shared across time and can be simply denoted as W.\nEquation 5 shows that the gradient can grow or shrink at each layer depending on the gain of each layer\u2019s linear transformation W and the gain of the Jacobian D. The gain caused by each layer is magnified across all time steps or layers. It is easy to have extreme amplification in a recurrent neural network where W is shared across time steps and a non-unitary gain in W is amplified exponentially. The phenomena of extreme growth or contraction of the gradient across time steps or layers are known as the exploding and the vanishing gradient problems, respectively. It is sufficient for RNNs to have \u03b7t \u2264 1 at each time t to enable the possibility of vanishing gradients, typically for some large number of time steps T . The rate at which a gradient (or forward signal) vanishes\ndepends on both the parameterization of the model and on the input data. The parameterization may be conditioned by placing appropriate constraints on W. It is worth keeping in mind that the Jacobian D is typically contractive, thus tending to be norm-reducing) and is also data-dependent, whereas W can vary from being contractive to norm-preserving, to expansive and applies the same gain on the forward signal as on the back-propagated gradient signal."}, {"heading": "2 OUR APPROACH", "text": "Vanishing and exploding gradients can be controlled to a large extent by controlling the maximum and minimum gain of W. The maximum gain of a matrix W is given by the spectral norm which is given by\n||W||2 = max [ ||Wx|| ||x|| ] . (6)\nBy keeping our weight matrix W close to orthogonal, one can ensure that it is close to a normpreserving transformation (where the spectral norm is equal to one, but the minimum gain is also one). One way to achieve this is via a simple soft constraint or regularization term of the form:\n\u03bb \u2211 i ||WTi Wi \u2212 I||2. (7)\nHowever, it is possible to formulate a more direct parameterization or factorization for W which permits hard bounds on the amount of expansion and contraction induced by W. This can be achieved by simply parameterizing W according to its singular value decomposition, which consists of the composition of orthogonal basis matrices U and V with a diagonal spectral matrix S containing the singular values which are real and positive by definition. We have\nW = USVT . (8)\nSince the spectral norm or maximum gain of a matrix is equal to its largest singular value, this decomposition allows us to control the maximum gain or expansivity of the weight matrix by controlling the magnitude of the largest singular value. Similarly, the minimum gain or contractivity of a matrix can be obtained from the minimum singular value.\nWe can keep the bases U and V orthogonal via geodesic gradient descent along the set of weights that satisfy UTU = I and VTV = I respectively. The submanifolds that satisfy these constraints are called Stiefel manifolds. We discuss how this is achieved in more detail below, then discuss our construction for bounding the singular values.\nDuring optimization, in order to maintain the orthogonality of an orthogonally-initialized matrix M, i.e. where M = U, M = V or M = W if so desired, we employ a Cayley transformation of the update step onto the Stiefel manifold of (semi-)orthogonal matrices, as in Nishimori (2005) and Tagare (2011). Given an orthogonally-initialized parameter matrix M and its Jacobian, G with respect to the objective function, an update is performed as follows:\nA = GMT \u2212MGT\nMnew = M+ (I+ \u03b7 2 A)\u22121(I\u2212 \u03b7 2 A),\n(9)\nwhere A is a skew-symmetric matrix (that depends on the Jacobian and on the parameter matrix) which is mapped to an orthogonal matrix via a Cayley transform and \u03b7 is the learning rate.\nWhile the update rule in (9) allows us to maintain an orthogonal hidden to hidden transition matrix W if desired, we are interested in exploring the effect of stepping away from the Stiefel manifold. As such, we parameterize the transition matrix W in factorized form, as a singular value decomposition with orthogonal bases U and V updated by geodesic gradient descent using the Cayley transform approach above.\nIf W is an orthogonal matrix, the singular values in the diagonal matrix S are all equal to one. However, in our formulation we allow these singular values to deviate from one and employ a sigmoidal parameterization to apply a hard constraint on the maximum and minimum amount of\ndeviation. Specifically, we define a margin m around 1 within which the singular values must lie. This is achieved with the parameterization\nsi = 2m(\u03c3(pi)\u2212 0.5) + 1, si \u2208 {diag(S)}, m \u2208 [0, 1]. (10)\nThe singular values are thus restricted to the range [1\u2212m, 1 +m] and the underlying parameters pi are updated freely via stochastic gradient descent. Note that this parameterization strategy also has implications on the step sizes that gradient descent based optimization will take when updating the singular values \u2013 they tend to be smaller compared to models with no margin constraining their values. Specifically, a singular value\u2019s progression toward a margin is slowed the closer it is to the margin. The sigmoidal parameterization can also impart another effect on the step size along the spectrum which needs to be accounted for. Considering 10, the gradient backpropagation of some loss L toward parameters pi is found as\ndL dpi = dsi dpi dL dsi = 2m d\u03c3(pi) dpi dL dsi . (11)\nFrom (11), it can be seen that the magnitude of the update step for pi is scaled by the margin hyperparameter m . This means for example that for margins less than one, the effective learning rate for the spectrum is reduced in proportion to the margin. Consequently, we adjust the learning rate along the spectrum to be independent of the margin by renormalizing it by 2m .\nThis margin formulation both guarantees singular values lie within a well defined range and slows deviation from orthogonality. Alternatively, one could enforce the orthogonality of U and V and impose a regularization term corresponding to a mean one Gaussian prior on these singular values. This encourages the weight matrix W to be norm preserving with a controllable strength equivalent to the variance of the Gaussian. We also explore this approach further below."}, {"heading": "3 EXPERIMENTS", "text": "In this section, we explore hard and soft orthogonality constraints on factorized weight matrices for recurrent neural network hidden to hidden transitions. With hard orthogonality constraints on U and V, we investigate the effect of widening the spectral margin or bounds on convergence and performance. Loosening these bounds allows increasingly larger margins within which the transition matrix W can deviate from orthogonality. We confirm that orthogonal initialization is useful as noted in Henaff et al. (2016), and we show that although strict orthogonality guarantees stable gradient norm, loosening orthogonality constraints can increase the rate of gradient descent convergence. We begin our analyses on tasks that are designed to stress memory: a sequence copying task and a basic addition task (Hochreiter & Schmidhuber, 1997). We then move on to tasks on real data that require models to capture long-range dependencies: digit classification based on sequential and permuted MNIST vectors (Le et al., 2015; LeCun et al., 1998). Finally, we look at a basic language modeling task using the Penn Treebank dataset (Marcus et al., 1993).\nThe copy and adding tasks, introduced by Hochreiter & Schmidhuber (1997), are synthetic benchmarks with pathologically hard long distance dependencies that require long-term memory in models. The copy task consists of an input sequence that must be remembered by the network, followed by a series of blank inputs terminated by a delimiter that denotes the point at which the network must begin to output a copy of the initial sequence. We use an input sequence of T + 20 elements that begins with a sub-sequence of 10 elements to copy, each containing a symbol ai \u2208 {a1 , ..., ap} out of p = 8 possible symbols. This sub-sequence is followed by T \u2212 1 elements of the blank category a0 which is terminated at step T by a delimiter symbol ap+1 and 10 more elements of the blank category. The network must learn to remember the initial 10 element sequence for T time steps and output it after receiving the delimiter symbol.\nThe goal of the adding task is to add two numbers together after a long delay. Each number is randomly picked at a unique position in a sequence of length T . The sequence is composed of T values sampled from a uniform distribution in the range [0, 1), with each value paired with an indicator value that identifies the value as one of the two numbers to remember (marked 1) or as a value to ignore (marked 0). The two numbers are positioned randomly in the sequence, the first in the range [0, T2 \u2212 1] and the second in the range [ T 2 , T \u2212 1], where 0 marks the first element. The network must learn to identify and remember the two numbers and output their sum.\nThe sequential MNIST task from Le et al. (2015), MNIST digits are flattened into vectors that can be traversed sequentially by a recurrent neural network. The goal is to classify the digit based on the sequential input of pixels. The simple variant of this task is with a simple flattening of the image matrices; the harder variant of this task includes a random permutation of the pixels in the input vector that is determined once for an experiment. The latter formulation introduces longer distance dependencies between pixels that must be interpreted by the classification model.\nThe English Penn Treebank (PTB) dataset from Marcus et al. (1993) is an annotated corpus of English sentences, commonly used for benchmarking language models. We employ a sequential character prediction task: given a sentence, a recurrent neural network must predict the next character at each step, from left to right. We use input sequences of variable length, with each sequence containing one sentence. We model 49 characters including lowercase letters (all strings are in lowercase), numbers, common punctuation, and an unknown character placeholder. In our experiments on two subsets of the data: in the first, we first use 23% of the data with strings with up to 75 characters and in the second we include over 99% of the dataset, picking strings with up to 300 characters."}, {"heading": "3.1 LOOSENING HARD ORTHOGONALITY CONSTRAINTS", "text": "In this section, we experimentally explore the effect of loosening hard orthogonality constraints through loosening the spectral margin defined above for the hidden to hidden transition matrix.\nIn all experiments, we employed RMSprop (Tieleman & Hinton, 2012) when not using geodesic gradient descent. We used minibatches of size 50 and for generated data (the copy and adding tasks), we assumed an epoch length of 100 minibatches. We cautiously introduced gradient clipping at magnitude 100 (unless stated otherwise) in all of our RNN experiments although it may not be required and we consistently applied a small weight decay of 0.0001. Unless otherwise specified, we trained all simple recurrent neural networks with the hidden to hidden matrix factorization as in (8) using geodesic gradient descent on the bases (learning rate 10\u22126) and RMSprop on the other parameters (learning rate 0.0001), using a tanh transition nonlinearity, and clipping gradients of 100 magnitude. The neural network code was built on the Theano framework (Theano Development Team, 2016). When parameterizing a matrix in factorized form, we apply the weight decay on the composite matrix rather than on the factors in order to be consistent across experiments. For MNIST and PTB, test set metrics were computed based on the parameterization that gave the best validation set accuracy."}, {"heading": "3.1.1 CONVERGENCE ON SYNTHETIC MEMORY TASKS", "text": "For different sequence lengths T of the copy and adding tasks, we trained a factorized RNN with 128 hidden units and various spectral margins m . For the copy task, we used Elman networks without a transition non-linearity as in Henaff et al. (2016). We discuss our investigations into the use of a non-linearity on the copy task in the Appendix.\nAs shown in Figure 1 we see an increase in the rate of convergence as we increase the spectral margin. This observation generally holds across the tested sequence lengths (T = 200, T = 500, T = 1000, T = 10000); however, large spectral margins hinder convergence on extremely long sequence lengths. At sequence length T = 10000, parameterizations with spectral margins larger than 0.001 converge slower than when using a margin of 0.001. In addition, the experiment without a margin failed to converge on the longest sequence length. This follows the expected pattern where stepping away from the Stiefel manifold may help with gradient descent optimization but loosening orthogonality constraints can reduce the stability of signal propagation through the network.\nFor the adding task, we trained a factorized RNN on T = 1000 length sequences, using a ReLU activation function on the hidden to hidden transition matrix. The mean squared error (MSE) is shown for different spectral margins in Figure 5 in the Appendix. Testing spectral margins m = 0, m = 1, m = 10, m = 100, and no margin, we find that the models with the purely orthogonal (m = 0) and the unconstrained (no margin) transition matrices failed to begin converging beyond baseline MSE within 2000 epochs."}, {"heading": "3.1.2 PERFORMANCE ON REAL DATA", "text": "Having confirmed that an orthogonality constraint can negatively impact convergence rate, we seek to investigate the effect on model performance for tasks on real data. We show the results of experiments on permuted sequential MNIST in Table 2 and ordered sequential MNIST in Table 1. The loss curves are shown in Figure 6 in the Appendix and reveal an increased convergence rate for larger spectral margins. We trained the factorized RNN models with 128 hidden units for 120 epochs. We also trained an LSTM with 128 hidden units on both tasks for 150 epochs, configured with peephole connections, orthogonally initialized (and forget gate bias initialized to one), and trained with RMSprop (learning rate 0.0001, clipping gradients of magnitude 1).\nWe show the results of experiments on PTB character prediction, in terms of bits per character (bpc) and prediction accuracy, for a subset of short sequences (up to 75 characters; 23% of data) in Table 3 and for a subset of long sequences (up to 300 characters; 99% of data) in Table 4. We trained factorized RNN models with 512 hidden units for 200 epochs with geodesic gradient descent on the bases (learning rate 10\u22126) and RMSprop on the other parameters (learning rate 0.001), using a tanh transition nonlinearity, and clipping gradients of 30 magnitude.\nInterestingly, for both the ordered and permuted sequential MNIST tasks, models with a non-zero margin significantly outperform those that are constrained to have purely orthogonal transition matri-\nces (margin of zero). The best results on both the ordered and sequential MNIST tasks were yielded by models with a spectral margin of 0.1, at 94.10% accuracy and 91.44% accuracy, respectively. An LSTM outperformed the RNNs in both tasks; nevertheless, RNNs with hidden to hidden transitions initialized as orthogonal matrices performed admirably without a memory component and without all of the additional parameters associated with gates. Indeed, orthogonally initialized RNNs performed almost on par with the LSTM in the permuted sequential MNIST task which presents longer distance dependencies than the ordered task. Although the optimal margin appears to be 0.1, RNNs with large margins perform almost identically to an RNN without a margin, as long as the transition matrix is initialized as orthogonal. On these tasks, orthogonal initialization appears to significantly outperform Glorot normal initialization (Glorot & Bengio, 2010) or initializing the matrix as identity. It is interesting to note that for the MNIST tasks, orthogonal initialization appears useful while orthogonality constraints appear mainly detrimental. This suggests that while orthogonality helps early training by stabilizing gradient flow across many time steps, orthogonality constraints may need to be loosened on some tasks so as not to over-constrain the model\u2019s representational ability.\nCuriously, larger margins and even models without sigmoidal constraints on the spectrum (no margin) performed well as long as they were initialized to be orthogonal, suggesting that evolution away from orthogonality is not a serious problem on MNIST. It is not surprising that orthogonality is useful for the MNIST tasks since they depend on long distance signal propagation with a single output at the end of the input sequence. On the other hand, character prediction with PTB produces an output at every time step. Constraining deviation from orthogonality proved detrimental for short sentences (Table 3) and beneficial when long sentences were included (Table 4). Furthermore, Glorot normal initialization did not perform worse than orthogonal initialization for PTB. Since an output is generated for every character in a sentence, short distance signal propagation is possible. Thus it is possible that the RNN is first learning very local dependencies between neighbouring characters and that given enough context, constraining deviation from orthogonality can help force the network to learn longer distance dependencies."}, {"heading": "3.1.3 SPECTRAL AND GRADIENT EVOLUTION", "text": "It is interesting to note that even long sequence lengths (T=1000) in the copy task can be solved efficiently with rather large margins on the spectrum. In Figure 2 we look at the gradient propagation of the loss from the last time step in the network with respect to the hidden activations. We can see that for a purely orthogonal parameterization of the transition matrix (when the margin is zero), the gradient norm is preserved across time steps, as expected. We further observe that with increasing margin size, the number of update steps over which this norm preservation survives decreases, though surprisingly not as quickly as expected.\nAlthough the deviation of singular values from one should be slowed by the sigmoidal parameterizations, even parameterizations without a sigmoid (no margin) can be effectively trained for all but the longest sequence lengths. This suggests that the spectrum is not deviating far from orthogonality and that inputs to the hidden to hidden transitions are mostly not aligned along the dimensions of great-\nest expansion or contraction. We evaluated the spread of the spectrum in all of our experiments and found that indeed, singular values tend to stay well within their prescribed bounds and only reach the margin when using a very large learning rate that does not permit convergence. Furthermore, when transition matrices are initialized as orthogonal, singular values remain near one throughout training even without a sigmoidal margin for tasks that require long term memory (copy, adding, sequential MNIST). On the other hand, singular value distributions tend to drift away from one for PTB character prediction which may help explain why enforcing an orthogonality constraint can be helpful for this task, when modeling long sequences. Interestingly, singular values spread out less for longer sequence lengths (nevertheless, the T=10000 copy task could not be solved with no sigmoid on the spectrum).\nWe visualize the spread of singular values for different model parameterizations on the permuted sequential MNIST task in Figure 3. Curiously, we find that the distribution of singular values tends to shift upward to a mean of approximately 1.05 on both the ordered and permuted sequential MNIST tasks. We note that in those experiments, a tanh transition nonlinearity was used which is contractive in both the forward signal pass and the gradient backward pass. An upward shift in the distribution of singular values of the transition matrix would help compensate for that contraction. Indeed, (Saxe et al., 2013) describe this as a possibly good regime for learning in deep neural networks. That the model appears to evolve toward this regime suggests that deviating from it may incur a cost. This is interesting because the cost function cannot take into account numerical issues such as vanishing or exploding gradients (or forward signals); we do not know what could make this deviation costly. That the transition matrix may be compensating for the contraction of the tanh is supported by further experiments: applying a 1.05 pre-activation gain appears to allow a model with a margin of 0 to nearly match the top performance reached on both of the MNIST tasks. Furthermore, when using the OPLU norm-preserving activation function (Chernodub & Nowicki, 2016), we found that orthogonally initialized models performed equally well with all margins, achieving over 90% accuracy on the permuted sequential MNIST task. Unlike orthgonally initialized models, the RNN on the bottom right of Figure 3 with Glorot normal initialized transition matrices, begins and ends with a wide singular spectrum. While there is no clear positive shift in the distribution of singular values, the mean value appears to very gradually increase for both the ordered and permuted sequential MNIST tasks. If the model is to be expected to positively shift singular values to compensate for the contractivity of the tanh nonlinearity, it is not doing so well for the Glorot-initialized case; however, this may be due to the inefficiency of training as a result of vanishing gradients, given that initialization."}, {"heading": "3.2 EXPLORING SOFT ORTHOGONALITY CONSTRAINTS", "text": "Having established that it may indeed be useful to step away from orthogonality, here we explore two forms of soft constraints (rather than hard bounds as above) on hidden to hidden transition matrix orthogonality. The first is a simple penalty that directly encourages a transition matrix W to be orthogonal, of the form \u03bb||WTW \u2212 I||22. This is similar to the orthogonality penalty introduced by Henaff et al. (2016). In the first two subfigures on the left of Figure 4, we explore the effect of weakening this form of regularization. We trained both a regular non-factorized RNN on the T = 200 copy task and a factorized RNN with orthogonal bases on the T = 500 copy task. For the regular RNN, we had to reduce the learning rate to 10\u22125. Here again we see that weakening the strength of the orthogonality-encouraging penalty can increase convergence speed.\nThe second approach we explore replaces the sigmoidal margin parameterization with a mean one Gaussian prior on the singular values. In the two right subfigures of Figure 4, we visualize the accuracy on the length 200 copy task, using geoSGD (learning rate 10\u22126) to keep U and V orthogonal and different strengths of a Gaussian prior with mean one on the singular values. We trained these experiments with regular SGD on the spectrum and other non-orthogonal parameter matrices, using a 10\u22125 learning rate. We see that priors which are too strong lead to slow convergence. Loosening the strength of the prior makes the optimization more efficient. Furthermore, we compare a direct parameterization of the spectrum (no sigmoid) in Figure 4 with a sigmoidal parameterization, using a large margin of 1. Without the sigmoidal parameterization, optimization quickly becomes unstable; on the other hand, the optimization also becomes unstable if the prior is removed completely in the sigmoidal formulation (margin 1). These results further motivate the idea that parameterizations that deviate from orthogonality may perform better than purely orthogonal ones, as long as they are sufficiently constrained to avoid instability during training."}, {"heading": "4 CONCLUSIONS", "text": "We have explored a number of methods for controlling the expansivity of gradients during backpropagation based learning in RNNs through manipulating orthogonality constraints and regularization on matrices. Our experiments indicate that while orthogonal initialization may be beneficial, maintaining constraints on orthogonality can be detrimental. Indeed, moving away from hard constraints on matrix orthogonality can help improve optimization convergence rate and model performance. However, we also observe with synthetic tasks that relaxing regularization which encourages the spectral norms of weight matrices to be close to one, or allowing bounds on the spectral norms of weight matrices to be too wide, can reverse these gains and may lead to unstable optimization."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank the Natural Sciences and Engineeering Research Council (NSERC) of Canada and Samsung for supporting this research."}, {"heading": "5 APPENDIX", "text": ""}, {"heading": "5.1 ADDITIONAL FIGURES", "text": ""}, {"heading": "5.2 COPY TASK NONLINEARITY", "text": "We found that nonlinearities such as a rectified linear unit (ReLU) (Nair & Hinton, 2010) or hyperbolic tangent (tanh) made the copy task far more difficult to solve. Using tanh, a short sequence length (T = 100) copy task required both a soft constraint that encourages orthogonality and thousands of epochs for training. It is worth noting that in the unitary evolution recurrent neural network of Arjovsky et al. (2015), the non-linearity (referred to as the \u201dmodReLU\u201d) is actually initialized as an identity operation that is free to deviate from identity during training. Furthermore, Henaff et al. (2016) derive a solution mechanism for the copy task that drops the non-linearity from an RNN. To explore this further, we experimented with a parametric leaky ReLU activation function (PReLU) which introduces a trainable slope \u03b1 for negative valued inputs x , producing f (x ) = max(x , 0) + \u03b1min(x , 0) (He et al., 2015). Setting the slope \u03b1 to one would make the PReLU equivalent to an identity function. We experimented with clamping \u03b1 to 0.5, 0.7 or 1 in a factorized RNN with a spectral margin of 0.3 and found that only the model with \u03b1 = 1 solved the T = 1000 length copy task. We also experimented with a trainable slope \u03b1, initialized to 0.7 and found that it converges to 0.96, further suggesting the optimal solution for the copy task is without a transition nonlinearity. Since the copy task is purely a memory task, one may imagine that a transition nonlinearity such as a tanh or ReLU may be detrimental to the task as it can lose information. Thus, we also tried a recent activation function that preserves information, called an orthogonal permutation linear unit (OPLU) (Chernodub & Nowicki, 2016). The OPLU preserves norm, making a fully norm-preserving RNN possible. Interestingly, this activation function allowed us to recover identical results on the copy task to those without a nonlinearity for different spectral margins."}, {"heading": "5.3 METHOD RUNNING TIME", "text": "Although the method proposed in section 2 relies on a matrix inversion, an operation with O(n3) complexity for an n \u00d7 n matrix, the running time of an RNN factorized in such a way actually remains reasonable. This running time is summarized in Table 5 and includes all computations in the graph, together with the matrix inversion. As this method is meant to be used only for the analysis in this work, we find the running times acceptable for that purpose. Models were run on an Nvidia GTX-770 GPU and were run against the T=100 length copy task."}], "references": [{"title": "Unitary evolution recurrent neural networks", "author": ["Martin Arjovsky", "Amar Shah", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1511.06464,", "citeRegEx": "Arjovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arjovsky et al\\.", "year": 2015}, {"title": "Norm-preserving orthogonal permutation linear unit activation functions (oplu)", "author": ["Artem Chernodub", "Dimitri Nowicki"], "venue": "arXiv preprint arXiv:1604.02313,", "citeRegEx": "Chernodub and Nowicki.,? \\Q2016\\E", "shortCiteRegEx": "Chernodub and Nowicki.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE international conference on computer vision,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Orthogonal rnns and long-memory tasks", "author": ["Mikael Henaff", "Arthur Szlam", "Yann LeCun"], "venue": "arXiv preprint arXiv:1602.06662,", "citeRegEx": "Henaff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Henaff et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Regularizing rnns by stabilizing activations", "author": ["David Krueger", "Roland Memisevic"], "venue": "arXiv preprint arXiv:1511.08400,", "citeRegEx": "Krueger and Memisevic.,? \\Q2015\\E", "shortCiteRegEx": "Krueger and Memisevic.", "year": 2015}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "A note on riemannian optimization methods on the stiefel and the grassmann manifolds", "author": ["Yasunori Nishimori"], "venue": null, "citeRegEx": "Nishimori.,? \\Q2005\\E", "shortCiteRegEx": "Nishimori.", "year": 2005}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "ICML (3),", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Notes on optimization on stiefel manifolds", "author": ["Hemant D Tagare"], "venue": "Technical report, Tech. Rep., Yale University,", "citeRegEx": "Tagare.,? \\Q2011\\E", "shortCiteRegEx": "Tagare.", "year": 2011}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Full-capacity unitary recurrent neural networks", "author": ["Scott Wisdom", "Thomas Powers", "John R. Hershey", "Jonathan Le Roux", "Les Atlas"], "venue": null, "citeRegEx": "Wisdom et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wisdom et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "Typically, exploding gradients are avoided by clipping large gradients (Pascanu et al., 2013) or introducing an L2 or L1 weight norm penalty.", "startOffset": 71, "endOffset": 93}, {"referenceID": 11, "context": "At a much higher computational cost, gradient descent optimization directly along this manifold can be done via geodesic steps (Nishimori, 2005; Tagare, 2011).", "startOffset": 127, "endOffset": 158}, {"referenceID": 14, "context": "At a much higher computational cost, gradient descent optimization directly along this manifold can be done via geodesic steps (Nishimori, 2005; Tagare, 2011).", "startOffset": 127, "endOffset": 158}, {"referenceID": 16, "context": "Recent work (Wisdom et al., 2016) has proposed the optimization of unitary matrices along the Stiefel manifold using geodesic gradient descent.", "startOffset": 12, "endOffset": 33}, {"referenceID": 8, "context": "Typically, exploding gradients are avoided by clipping large gradients (Pascanu et al., 2013) or introducing an L2 or L1 weight norm penalty. The latter has the effect of bounding the spectral radius of the linear transformations, thus limiting the maximal gain across the transformation. Krueger & Memisevic (2015) attempt to stabilize the norm of propagating signals directly by penalizing differences in successive norm pairs in the forward pass and Pascanu et al.", "startOffset": 72, "endOffset": 316}, {"referenceID": 8, "context": "Typically, exploding gradients are avoided by clipping large gradients (Pascanu et al., 2013) or introducing an L2 or L1 weight norm penalty. The latter has the effect of bounding the spectral radius of the linear transformations, thus limiting the maximal gain across the transformation. Krueger & Memisevic (2015) attempt to stabilize the norm of propagating signals directly by penalizing differences in successive norm pairs in the forward pass and Pascanu et al. (2013) propose to penalize successive gradient norm pairs in the backward pass.", "startOffset": 72, "endOffset": 475}, {"referenceID": 5, "context": "Le et al. (2015) and Henaff et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 3, "context": "(2015) and Henaff et al. (2016) have respectively shown that identity initialization and orthogonal initialization can be beneficial.", "startOffset": 11, "endOffset": 32}, {"referenceID": 0, "context": "Arjovsky et al. (2015) have gone beyond initialization, building unitary recurrent neural network (RNN) models with transformations that are unitary by construction which they achieved by composing multiple basic unitary transformations.", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "from Tagare (2011), combining the use of a canonical inner products and Cayley transformations.", "startOffset": 5, "endOffset": 19}, {"referenceID": 12, "context": "In (Pascanu et al., 2013), it is shown that the 2-norm of \u2202ai+1 \u2202ai is bounded by the product of the norms of the non-linearity\u2019s Jacobian and transition matrix at time t (layer i ), as follows: \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2202at+1 \u2202at \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2264 ||Dt|| ||Wt|| \u2264 \u03bbDt \u03bbWt = \u03b7t, \u03bbDt , \u03bbWt \u2208 R.", "startOffset": 3, "endOffset": 25}, {"referenceID": 11, "context": "where M = U, M = V or M = W if so desired, we employ a Cayley transformation of the update step onto the Stiefel manifold of (semi-)orthogonal matrices, as in Nishimori (2005) and Tagare (2011).", "startOffset": 159, "endOffset": 176}, {"referenceID": 11, "context": "where M = U, M = V or M = W if so desired, we employ a Cayley transformation of the update step onto the Stiefel manifold of (semi-)orthogonal matrices, as in Nishimori (2005) and Tagare (2011). Given an orthogonally-initialized parameter matrix M and its Jacobian, G with respect to the objective function, an update is performed as follows:", "startOffset": 159, "endOffset": 194}, {"referenceID": 7, "context": "We then move on to tasks on real data that require models to capture long-range dependencies: digit classification based on sequential and permuted MNIST vectors (Le et al., 2015; LeCun et al., 1998).", "startOffset": 162, "endOffset": 199}, {"referenceID": 8, "context": "We then move on to tasks on real data that require models to capture long-range dependencies: digit classification based on sequential and permuted MNIST vectors (Le et al., 2015; LeCun et al., 1998).", "startOffset": 162, "endOffset": 199}, {"referenceID": 9, "context": "Finally, we look at a basic language modeling task using the Penn Treebank dataset (Marcus et al., 1993).", "startOffset": 83, "endOffset": 104}, {"referenceID": 4, "context": "We confirm that orthogonal initialization is useful as noted in Henaff et al. (2016), and we show that although strict orthogonality guarantees stable gradient norm, loosening orthogonality constraints can increase the rate of gradient descent convergence.", "startOffset": 64, "endOffset": 85}, {"referenceID": 4, "context": "We confirm that orthogonal initialization is useful as noted in Henaff et al. (2016), and we show that although strict orthogonality guarantees stable gradient norm, loosening orthogonality constraints can increase the rate of gradient descent convergence. We begin our analyses on tasks that are designed to stress memory: a sequence copying task and a basic addition task (Hochreiter & Schmidhuber, 1997). We then move on to tasks on real data that require models to capture long-range dependencies: digit classification based on sequential and permuted MNIST vectors (Le et al., 2015; LeCun et al., 1998). Finally, we look at a basic language modeling task using the Penn Treebank dataset (Marcus et al., 1993). The copy and adding tasks, introduced by Hochreiter & Schmidhuber (1997), are synthetic benchmarks with pathologically hard long distance dependencies that require long-term memory in models.", "startOffset": 64, "endOffset": 788}, {"referenceID": 7, "context": "The sequential MNIST task from Le et al. (2015), MNIST digits are flattened into vectors that can be traversed sequentially by a recurrent neural network.", "startOffset": 31, "endOffset": 48}, {"referenceID": 7, "context": "The sequential MNIST task from Le et al. (2015), MNIST digits are flattened into vectors that can be traversed sequentially by a recurrent neural network. The goal is to classify the digit based on the sequential input of pixels. The simple variant of this task is with a simple flattening of the image matrices; the harder variant of this task includes a random permutation of the pixels in the input vector that is determined once for an experiment. The latter formulation introduces longer distance dependencies between pixels that must be interpreted by the classification model. The English Penn Treebank (PTB) dataset from Marcus et al. (1993) is an annotated corpus of English sentences, commonly used for benchmarking language models.", "startOffset": 31, "endOffset": 650}, {"referenceID": 4, "context": "For the copy task, we used Elman networks without a transition non-linearity as in Henaff et al. (2016). We discuss our investigations into the use of a non-linearity on the copy task in the Appendix.", "startOffset": 83, "endOffset": 104}, {"referenceID": 13, "context": "Indeed, (Saxe et al., 2013) describe this as a possibly good regime for learning in deep neural networks.", "startOffset": 8, "endOffset": 27}, {"referenceID": 4, "context": "This is similar to the orthogonality penalty introduced by Henaff et al. (2016). In the first two subfigures on the left of Figure 4, we explore the effect of weakening this form of regularization.", "startOffset": 59, "endOffset": 80}], "year": 2017, "abstractText": "It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and can therefore be a desirable property; however, we find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance. This paper explores the issues of optimization convergence, speed and gradient stability using a variety of different methods for encouraging or enforcing orthogonality. In particular we propose a weight matrix factorization and parameterization strategy through which we can bound matrix norms and therein control the degree of expansivity induced during backpropagation.", "creator": "LaTeX with hyperref package"}}}