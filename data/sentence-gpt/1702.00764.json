{"id": "1702.00764", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Feb-2017", "title": "Symbolic, Distributed and Distributional Representations for Natural Language Processing in the Era of Deep Learning: a Survey", "abstract": "Natural language and symbols are intimately correlated. Recent advances in machine learning (ML) and in natural language processing (NLP) seem to contradict the above intuition: symbols are fading away, erased by vectors or tensors called distributed and distributional representations. However, there is a strict link between distributed/distributional representations and symbols, being the first an approximation of the second. A clearer understanding of the strict link between distributed/distributional representations and symbols will certainly lead to radically new deep learning networks. In this paper we make a survey that aims to draw the link between symbolic representations and distributed/distributional representations. This is the right time to revitalize the area of interpreting how symbols are represented inside neural networks.\n\n\nIn a previous paper, I showed that machine learning (MNN) has a large degree of power and potential to help us understand some of the important issues related to machine learning and related topics. It is a promising field and it is possible that there will be other applications that will benefit from the technique. A small sample size of the paper were published in Computer Science Letters . We are also interested in the possibilities that MNN will help solve some of these topics. The work is supported by the RBS grant M.K.K.G.D.S. and the National Science Foundation, S.J.A.H.A.G.D.S. .\nThere are more news from the journal, Nature, which also published the paper. The paper is available online at: http://dx.doi.org/10.1038/nature1605", "histories": [["v1", "Thu, 2 Feb 2017 17:53:29 GMT  (129kb,D)", "http://arxiv.org/abs/1702.00764v1", "25 pages"]], "COMMENTS": "25 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lorenzo ferrone", "fabio massimo zanzotto"], "accepted": false, "id": "1702.00764"}, "pdf": {"name": "1702.00764.pdf", "metadata": {"source": "CRF", "title": "A Symbolic, Distributed and Distributional Representations for Natural Language Processing in the Era of Deep Learning: a Survey", "authors": ["LORENZO FERRONE", "FABIO MASSIMO ZANZOTTO"], "emails": [], "sections": [{"heading": null, "text": "A Symbolic, Distributed and Distributional Representations for Natural Language Processing in the Era of Deep Learning: a Survey\nLORENZO FERRONE, University of Rome Tor Vergata\nand FABIO MASSIMO ZANZOTTO, University of Rome Tor Vergata\nNatural language and symbols are intimately correlated. Recent advances in machine learning (ML) and in natural language processing (NLP) seem to contradict the above intuition: symbols are fading away, erased by vectors or tensors called distributed and distributional representations. However, there is a strict link between distributed/distributional representations and symbols, being the first an approximation of the second. A clearer understanding of the strict link between distributed/distributional representations and symbols will certainly lead to radically new deep learning networks. In this paper we make a survey that aims to draw the link between symbolic representations and distributed/distributional representations. This is the right time to revitalize the area of interpreting how symbols are represented inside neural networks."}, {"heading": "1. INTRODUCTION", "text": "Natural language and symbols are intimately correlated. Sounds are transformed in letters or ideograms and these symbols are composed to obtain words. Words then form sentences and sentences form texts, discourses, dialogs, which ultimately convey knowledge, emotions, and so on. This composition of symbols in words and of words in sentences follow rules that both the hearer and the speaker know [Chomsky 1957]. Hence, thinking to natural language understanding systems, which are not based on symbols, seems to be an heresy.\nRecent advances in machine learning (ML) and in natural language processing (NLP) seem to contradict the above intuition: symbols are fading away, erased by vectors or tensors called distributed and distributional representations. In ML, distributed representations are pushing deep learning models [LeCun et al. 2015; Schmidhuber 2015] towards amazing results in many high-level tasks such as image recognition [He et al. 2016; Simonyan and Zisserman 2014; Zeiler and Fergus 2014], image generation [Goodfellow et al. 2014] and image captioning [Vinyals et al. 2015b; Xu et al. 2015], machine translation [Bahdanau et al. 2014; Zou et al. 2013], syntactic parsing [Vinyals et al. 2015a; Weiss et al. 2015] and even game playing at human level [Silver et al. 2016; Mnih et al. 2013]. In NLP, distributional representations are pursued as a more flexible way to represent semantics of natural language, the so-called distributional semantics (see [Turney and Pantel 2010]). Distributed representations are vectors or tensors of real number representing the meaning of words, phrases and sentences. Vectors for words are obtained using corpora. Instead, as a complete semantic representation, vectors for phrases [Mitchell and Lapata 2008; Baroni and Zamparelli 2010; Clark et al. 2008; Grefenstette and Sadrzadeh 2011; Zanzotto et al. 2010] and sentences [Socher et al. 2011; Socher et al. 2012; Kalchbrenner and Blunsom 2013] are obtained by composing vectors for words. Hence, reasoning with symbols in natural language applications seem to be a relic of an ancient past.\nThe success of distributed and distributional representations over symbolic approaches is mainly due to the advent of new parallel paradigms that pushed neural networks [Rosenblatt 1958; Werbos 1974] towards deep learning [LeCun et al. 2015; Schmidhuber 2015]. Massively parallel algorithms running on Graphic Processing Units (GPUs) [Chetlur et al. 2014; Cui et al. 2015] crunch vectors, matrices and tensors faster than decades ago. The backpropagation algorithm can be now computed for complex and large neural networks. Symbols are not needed any more during \u201cresoning\u201d, that is, the neural network learning and its application. They only survive as input and output of these wonderful learning machines.\nar X\niv :1\n70 2.\n00 76\n4v 1\n[ cs\n.C L\n] 2\nF eb\n2 01\n7\nHowever, there is a strict link between distributed/distributional representations and symbols, being the first an approximation of the second [Plate 1994; Plate 1995; Ferrone and Zanzotto 2014; Ferrone et al. 2015]. The representation of the input and the output of these networks is not that far from their internal representation. The similarity and the interpretation of the internal representation is clearer in image processing. In fact, networks are generally interpreted visualizing how subparts represent salient subparts of target images. Both input images and subparts are tensors of real number. Hence, these networks can be examined and understood. The same does not apply to natural language processing with its symbols. NLP-related networks are a deep mystery.\nA clearer understanding of the strict link between distributed/distributional representations and symbols will certainly lead to radically new deep learning networks. It is then the dawn of a new range of possibilities: understanding what part of the current symbolic techniques for natural language processing have a sufficient representation in deep neural networks; and, ultimately, understanding whether a more brain-like model \u2013 the neural networks \u2013 is compatible with methods for syntactic parsing or semantic processing that have been defined in these decades of studies in computational linguistics and natural language processing. There is thus a tremendous opportunity to understand whether and how symbolic representations are used and emitted in a brain model.\nIn this paper we make a survey that aims to draw the link between symbolic representations and distributed/distributional representations. This is the right time to revitalize the area of interpreting how symbols are represented inside neural networks. In our opinion, this survey will help to devise new deep neural networks that can exploit existing and novel symbolic models of classical natural language processing tasks.\nThe paper is structured as follow: first we give an introduction to the very general concept of representations and the difference between local and distributed representations [Plate 1995]. After that we present each techniques in detail. Afterwards, we focus on distributional representations [Turney and Pantel 2010], which we treat as a specific example of a distributed representation. Finally we discuss more in depth the general issue of compositionality, analyzing three different approaches to the problem: compositional distributional semantics [Clark et al. 2008; Baroni et al. 2014], holographic reduced representations [Plate 1994; Neumann 2001], and recurrent neural networks [Kalchbrenner and Blunsom 2013; Socher et al. 2012]."}, {"heading": "2. SYMBOLIC AND DISTRIBUTED REPRESENTATIONS: INTERPRETABILITY AND COMPOSABILITY", "text": "The winning models for learning and searching over textual data have imposed distributed representations as a way to observe symbolic representations, that is, symbols, words, word sequences, syntactic structures and semantic assertions.\nSymbolic representations have two important features:\ninterpretability \u2013 symbolic representations are intepretable in the sense that we can read them;\ncomposability \u2013 more complex symbolic representations can be obtained composing basic symbols with strong combination rules and basic symbols are still recognizable in complex representations.\nAn example of composability is the following. Given the set of basic symbols D = {mouse,cat,a,catches,(,)}, we can compose symbols as sequences, e.g.:\na cat catches a mouse,\nor as structures, e.g.:\n((a cat) (catches (a mouse)))\nIf symbols and composing rules are interpretable, sequences or structures of symbols are still interpretable. Moreover, composing symbols can be recovered from these complex representations.\nDistributed representations are vectors or tensors in metric spaces which underly learning models such as neural networks and also some models based on kernel methods [Zanzotto and Dell\u2019Arciprete 2012a]. Hence, observing textual data as distributed representations is the only way to reason about texts in these metric spaces.\nTo clarify what are distributed representations, we first describe what are local representations (as referred in [Plate 1995]), that is, common feature vectors used to represent symbols, sequences of symbols [Lodhi et al. 2002; Cancedda et al. 2003] or symbolic structures [Haussler 1999; Collins and Duffy 2001] in kernel machines [Cristianini and Shawe-Taylor 2000] or in searching models such as the vector space model [Salton 1989]. Given a set of symbols D, a local representation maps the i-th symbol in D to the i-th unit vector ei in Rn, where n is the cardinality of D. Hence, the i-th unit vector represents the i-th symbol. For example, the above set D may be represented as:\nmouse\u2192 e1 = (1 0 0 0 0 0) T\ncat\u2192 e2 = (0 1 0 0 0 0) T\na\u2192 e3 = (0 0 1 0 0 0) T\ncatches\u2192 e4 = (0 0 0 1 0 0) T\n(\u2192 e5 = (0 0 0 0 1 0) T\n)\u2192 e6 = (0 0 0 0 0 1) T\nRepresenting a sequence s of symbols with local representations is generally done in two ways: (1) a sequence of vectors and (2) a bag-of-symbols. In the first way, a sequence is represented with the sequence of vectors representing the symbols in the sequence, for example, the sequence a cat catches a mouse is:\na cat catches a mouse \u2192 s =  0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0  This first way is used in recurrent neural networks. In the second way, a sequence is represented with one vector generally obtained with a weighted sum of vectors representing symbols, that is, it is generally referred as bag-of-symbols. For example, the previous sequence is:\na cat catches a mouse \u2192 s =  1 1 2 1 0 0 \nwhere vectors have been summed with a weight of 1. This second way is often referred as bag-of-word model in information retrieval [Salton 1989] when basic symbols are words. The part of the name bag-of wants to stress that words or other symbols are treated independently although these words are in sequences.\nA local representation is a wonderful proxy for a symbolic representation. When sequences are sequence of vectors, this is straightforward. Sequences of vectors represents sequences as they are. Each vector has a one-to-one mapping to a symbol. Hence, symbolic sequences can be fully reconstructed. When sequences are bag-of-symbol vectors, symbolic sequences cannot be fully reconstructed but it is possible to know which symbols were in the sequence. Hence, local representations are a convenient way to map a symbolic representation in a form that can be used in a learning model.\nHowever, local representations are extremely inefficient in situations when the size of the symbol set grows and where sparse vectors cannot be used (for example, in deep neural networks). Moreover, these local representations lack in the ability to capture interesting properties of datasets. This is where distributed representations come from.\nInstead, distributed representations hide symbols and sequences of symbols in vectors where each dimension is not a symbol but the whole vector represent a symbol or a sequence of symbols. To give an intuition on how this can be pursued, let\u2019s take the above set D. A different way to represent the set D is to treat each element not as a unanalyzable entity, uncorrelated to everything else, but as something which possess some properties, and then use those properties to represent each item. Words and symbols is D can be distinguished with these properties: number of vowels, number of consonants and, finally, number of non-alphabetic symbols. Given these properties one could then devise the following representation:\nmouse\u2192 (3 2 0)T\ncat\u2192 (2 1 0)T\na\u2192 (1 0 0)T\ncatches\u2192 (5 2 0)T\n(\u2192 (0 0 1)T\n)\u2192 (0 0 1)T\nThis is a simple example of a distributed representation. In a distributed representation [Plate 1995; Hinton et al. 1986] the informational content is distributed (hence the name) among multiple units, and at the same time each unit can contribute to the representation of multiple elements.\nA distributed representation has two evident advantages with respect to a local representation: it is more efficient (in the example, the representation uses only 3 numbers instead of 6) and it does not treat each element as being equally different to any other. In fact, mouse and cat in this representation are more similar than mouse and a. In other words, this representation captures by construction something interesting about the set of symbols.\nUnfortunately, distributed representations are harder to be interpreted than symbolic structures and have not a clear model to compose symbols in larger structures.\nWe then re-define the two properties Interpretability and Composability for distributed representations. These two properties want to measure how far are distributed representations from symbolic representations.\nInterpretability. Interpretability is the possibility of decoding distributed representations, that is, extracting the embedded symbolic representations. This is an important characteristic but it must be noted that it\u2019s not a simple yes-or-no classification. It is more a degree associated to specific representations. In fact, even if each component of a vector representation does not have a specific meaning, this does not mean that the representation is not interpretable as a whole, or that symbolic information cannot be recovered from it. For this reason, we can categorize the degree of interpretability of a representation as follows:\nhuman-interpretable \u2013 each dimension of a representation has a specific meaning;\ndecodable \u2013 the representation may be obscure, but it can be decoded into an interpretable, symbolic representation.\nComposability. Composability is the possibility of composing basic distributed representations with strong rules and of decomposing back composed representations with inverse rules. Generally, in NLP, basic distributed representations refer to basic symbols. It is worth noticing that that composability is related but is not the same concept of compositionality. Composability describes how vectors or symbols can be composed to represent larger structures. Compositionality instead is describes how the semantic of large structures can be obtained using the semantic of the components. In distributed or, better, in distributional semantic representations [Baroni et al. 2014; Mitchell and Lapata 2010] these two concepts often coincide.\nThe two axes of Interpretability and Composability will be used to describe the presented distributed representations as we are interested in understanding whether or not a representation can be used to represent structures or sequences and whether it is possible to extract back the underlying structure or sequence given a distributed representation. It is clear that a local representation is more interpretable than a distributed representation. Yet, both representations lack in composability when sequences or structures are collapsed in vectors or tensors that do not depend on the length of represented sequences or structures. For example, the bag-of-symbol local representation does not take into consideration the order of the symbols in the sequence."}, {"heading": "3. STRATEGIES TO OBTAIN DISTRIBUTED REPRESENTATIONS FROM SYMBOLS", "text": "There is a wide range of techniques to transform symbolic representations in distributed representations. When combining natural language processing and machine learning, this is a major issue: transforming symbols, sequences of symbols or symbolic structures in vectors or tensors that can be used in learning machines. These techniques generally propose a function \u03b7 to transform a local representation with a large dimensionality in a distributed representation with a low dimensionality:\n\u03b7 : Rn \u2192 Rd\nThis function is often called encoder.\nThe techniques described in this section generally starts from a collection of vectors obtained from a collection of sequences of symbols. These vectors generally represent sequences or structures with a bag-of-symbol representation. As running example, we give the following where bag-of-symbol representations are obtained by simple counting the occurrences of\nsymbols in the sequence. Symbols are words. The collection of sequences, that is the corpus, is Table I.\nHence, the local representation of these sequences is represented by the following matrix:\nX =\n s1 s2 s3 a 2 2 2 cat 1 0 1 dog 0 1 1\nmouse 1 1 0 catches 1 0 1 eats 0 1 0  (1) where rows represent symbols and columns are the bag-of-symbol representations of the sequences of symbols si in the collection.\nWe propose to categorize techniques to obtain distributed representations in two broad categories, showing some degree of overlapping:\n\u2014 representations derived from dimensionality reduction techniques;\n\u2014 learned representations\nIn the rest of the section, we will introduce the different strategies according to the proposed categorization. Moreover, we will emphasize its degree of interpretability for each representation and its related function \u03b7 by answering to two questions:\n\u2014 Has a specific dimension in Rd a clear meaning?\n\u2014 Can we decode an encoded symbolic representation? In other words, assuming a decoding function \u03b4 : Rd \u2192 Rn, how far is v \u2208 Rn, which represents a symbolic representation, from v\u2032 = \u03b4(\u03b7(v))?\nInstead, composability of the resulting representations will be analyzed in Sec. 5."}, {"heading": "3.1. Dimensionality reduction", "text": "Dimensionality reduction techniques were born in order to deal with high-dimensional data. High-dimensional data present two main challenges to work with. The first is computational in nature: some algorithm scale poorly with the increasing of the dimensions, making some problems intractable. The second is theoretical and is usually called the curse of dimensionality [Friedman 1997; Daum and Huang 2003; Keogh and Mueen 2011; Bellman and Corporation 1957]: the number of samples one needs in order to perform useful inference on a dataset increases exponentially with the dimensionality of the representation.\nDimensionality reduction techniques are thus algorithms that generally extract a linear function \u03b7 = W \u2208 Rd\u00d7n that transform local representations v in a space Rn into distributed representations\n\u03b7(v) = Wv\nin a lower dimensional space Rd, while preserving as much as possible the properties of the original space. The properties that algorithms preserve vary among techniques and can be divided statistical properties and in topological properties:\n\u2014 statistical properties concerns things like variance of the dataset (as in Principal Component Analysis [Markovsky 2012]) or other measures of information content.\n\u2014 topological properties concern things like nearest-neighbour or pairwise distance of elements as in Laplacian Eigenmaps [Belkin and Niyogi 2001] or random projections [Johnson and Lindenstrauss 1984; Sahlgren 2005; Bingham and Mannila 2001]\n3.1.1. Principal Component Analysis. Principal Component Analysis (PCA) [Markovsky 2012; Pearson 1901] is a linear method which reduces the number of dimensions by projecting Rn into the \u201cbest\u201d linear subspace of a given dimension d by using the a set of data points. The \u201cbest\u201d linear subspace is a subspace where dimensions maximize the variance of the data points in the set. PCA can be interpreted either as a probabilistic method or as a matrix approximation and is then usually known as truncated singular value decomposition. We are here interested in describing PCA as probabilistic method as it related to the interpretability of the related distributed representation.\nAs a probabilistic method, PCA finds an orthogonal projection matrix Wd \u2208 Rn\u00d7d such that the variance of the projected set of data points is maximized. The set of data points is referred as a matrix X \u2208 Rm\u00d7n where each row xTi \u2208 Rn is a single observation. Hence, the variance that is maximized is X\u0302d = XW T d \u2208 Rm\u00d7d.\nMore specifically, let\u2019s consider the first weight vector w1, which maps an element of the dataset x into a single number \u3008x,w1\u3009. Maximizing the variance means that w is such that:\nw1 = arg max \u2016w\u2016=1 \u2211 i (\u3008xi,w\u3009)2\nand it can be shown that the optimal value is achieved when w is the eigenvector of XTX with largest eigenvalue. This then produces a projected dataset:\nX\u03021 = X TW1 = X Tw1\nThe algorithm can then compute iteratively the second and further components by first subtracting the components already computed from X:\nX \u2212Xw1w1T\nand then proceed as before. However, it turns out that all subsequent components are related to the eigenvectors of the matrix XTX, that is, the d-th weight vector is the eigenvector of XTX with the d-th largest corresponding eigenvalue.\nThe encoding matrix for distributed representations derived with a PCA method is the matrix:\nWd =  w1w2. . . wd  \u2208 Rd\u00d7n where wi are eigenvectors with eigenvalues decreasing with i. Hence, local representations v \u2208 Rn are represented in distributed representations in Rd as:\n\u03b7(v) = Wdv\nHence, vectors \u03b7(v) are human-interpretable as their dimensions represent linear combinations of dimensions in the original local representation and these dimensions are ordered according to their importance in the dataset, that is, their variance. Moreover, each dimension is a linear combination of the original symbols. Then, the matrix Wd reports on which combination of the original symbols is more important to distinguish data points in the set.\nMoreover, vectors \u03b7(v) are decodable. The decoding function is:\n\u03b4(v\u2032) = WTd v \u2032\nand WTd Wd = I if d is the rank of the matrix X, otherwise it is a degraded approximation (for more details refer to [Fodor 2002; Sorzano et al. 2014]). Hence, distributed vectors in Rd can be decoded back in the original symbolic representation with a degree of approximation that depends on the distance between d and the rank of the matrix X.\nThe compelling limit of PCA is that all the data points have to be used in order to obtain the encoding/decoding matrices. This is not feasible in two cases. First, when the model has to deal with big data. Second, when the set of symbols to be encoded in extremely large. In this latter case, local representations cannot be used to produce matrices X for applying PCA.\nMoreover, PCA is not composable. In fact, PCA-related distributed representation can encode only bag-of-symbols.\n3.1.2. Random Projections. Random projection (RP) [Bingham and Mannila 2001; Fodor 2002] is a technique based on random matrices Wd \u2208 Rd\u00d7n. Generally, the rows of the matrix Wd are sampled from a Gaussian distribution with zero mean, and normalized as to have unit length [Johnson and Lindenstrauss 1984] or even less complex random vectors [Achlioptas 2003]. Random projections from Gaussian distributions approximately preserves pairwise distance between points (see the Johnsonn-Lindenstrauss Lemma [Johnson and Lindenstrauss 1984]), that is, for any vector x, y \u2208 X:\n(1\u2212 \u03b5) \u2016x\u2212 y\u20162 \u2264 \u2016Wx\u2212Wy\u20162 \u2264 (1 + \u03b5) \u2016x\u2212 y\u20162\nwhere the approximation factor \u03b5 depends on the dimension of the projection, namely, to assure that the approximation factor is \u03b5, the dimension k must be chosen such that:\nk \u2265 8 log(m) \u03b52\nConstraints for building the matrix W can be significantly relaxed to less complex random vectors [Achlioptas 2003]. Rows of the matrix can be sampled from very simple zero-mean distributions such as:\nWij = \u221a 3  +1 with probability 16 \u22121 with probability 16 0 with probability 23\nwithout the need to manually ensure unit-length of the rows, and at the same time providing a significant speed up in computation due to the sparsity of the projection.\nUnfortunately, vectors \u03b7(v) are not human-interpretable as, even if their dimensions represent linear combinations of dimensions in the original local distribution, these dimensions have not an interpretation or particular properties.\nOn the contrary, vectors \u03b7(v) are decodable. The decoding function is:\n\u03b4(v\u2032) = WTd v \u2032\nand WTd Wd \u2248 I when Wd is derived using Gaussian random vectors. Hence, distributed vectors in Rd can be approximately decoded back in the original symbolic representation with a degree of approximation that depends on the distance between d .\nThe major advantage of RP with respect to PCA is that the matrix X of all the data points is not needed to derive the matrix Wd. Moreover, the matrix Wd can be produced -la-carte starting from the symbols encountered so far in the encoding procedure. In fact, it is sufficient to generate new Gaussian vectors for new symbols when they appear.\nHowever, also RP as described in this section are not composable. As PCA-related distributed representation, RP can model only bag-of-symbols."}, {"heading": "3.2. Learned representation", "text": "Learned representations differ from the dimensionality reduction techniques by the fact that: (1) encoding/decoding functions may not be linear; (2) learning can optimize functions that are different with respect to the target of PCA; and, (3) solutions are not derived in a closed form but are obtained using optimization techniques such as stochastic gradient decent.\nLearned representation can be further classified into:\n\u2014 task-independent representations learned with a standalone algorithm (as in autoencoders [Socher et al. 2011; Liou et al. 2014]) which is independent from any task, and which learns a representation that only depends from the dataset used;\n\u2014 task-dependent representations learned as the first step of another algorithm (this is called end-to-end training), usually the first layer of a deep neural network. In this case the new representation is driven by the task.\n3.2.1. Autoencoder. Autoencoders are a task independent technique to learn a distributed representation encoder \u03b7 : Rn \u2192 Rd by using local representations of a set of examples [Socher et al. 2011; Liou et al. 2014]. The distributed representation encoder \u03b7 is half of an autoencoder.\nAn autoencoder is a neural network that aims to reproduce an input vector in Rn as output by passing in a hidden layer(s) that are in Rd. Given \u03b7 : Rn \u2192 Rd and \u03b4 : Rd \u2192 Rn as the encoder and the decoder, respectively, an autoencoder aims to maximize the following function:\nL(x,x\u2032) = \u2016x\u2212 x\u2032\u20162\nwhere\nx\u2032 = \u03b4(\u03b7(x))\nThe encoding and decoding module are two neural networks, which means that they are functions depending on a set of parameters \u03b8 of the form\n\u03b7\u03b8(x) = s(Wx+ b) \u03b4\u03b8\u2032(y) = s(W \u2032y + b\u2032)\nwhere the parameters of the entire model are \u03b8, \u03b8\u2032 = {W, b,W \u2032, b\u2032} with W,W \u2032 matrices, b, b\u2032 vectors and s is a function that can be either a non-linearity sigmoid shaped function, or in some cases the identity function. In some variants the matrices W and W \u2032 are constrained to WT = W \u2032. This model is different with respect to PCA due to the target loss function and the use of non-linear functions.\nAutoencoders have been further improved with denoising autoencoders [Vincent et al. 2010; Vincent et al. 2008; Masci et al. 2011] that are a variant of autoencoders where the goal is to reconstruct the input from a corrupted version. The intuition is that higher level features should be robust with regard to small noise in the input. In particular, the input x gets corrupted via a stochastic function:\nx\u0303 = g(x)\nand then one minimizes again the reconstruction error, but with regard to the original (uncorrupted) input:\nL(x,x\u2032) = \u2016x\u2212 \u03b4(\u03b7(g(x)))\u20162\nUsually g can be either:\n\u2014 adding gaussian noise: g(x) = x + \u03b5, where \u03b5 \u223c N (0, \u03c3I); \u2014 masking noise: where a given a fraction \u03bd of the components of the input gets set to 0\nFor what concerns intepretability, as for random projection, distributed representations \u03b7(v) obtained with encoders from autoencoders and denoising autoencoders are not humaninterpretable but are decodable as this is the nature of autoencoders.\nMoreover, composability is not covered by this formulation of autoencoders.\n3.2.2. Embedding layers. Embedding layers are generally the first layers of more complex neural networks which are responsible to transform an initial local representation in the first internal distributed representation. The main difference with autoencoders is that these layers are shaped by the entire overall learning process. The learning process is generally task dependent. Hence, these first embedding layers depend on the final task.\nIt is argued that each layers learn a higher-level representation of its input. This is particularly visible with convolutional network [Krizhevsky et al. 2012] applied to computer vision tasks. In these suggestive visualizations [Zeiler and Fergus 2014], the hidden layers are seen to correspond to abstract feature of the image, starting from simple edges (in lower layers) up to faces in the higher ones.\nHowever, these embedding layers produce encoding functions and, thus, distributed representations that are not interpretable when applied to symbols. In fact, these distributed representations are not human-interpretable as dimensions are not clearly related to specific aggregations of symbols. Moreover, these embedding layers do not naturally provide decoders. Hence, this distributed representation is not decodable."}, {"heading": "4. DISTRIBUTIONAL REPRESENTATIONS AS ANOTHER SIDE OF THE COIN", "text": "Distributional semantics is an important area of research in natural language processing that aims to describe meaning of words and sentences with vectorial representations (see [Turney and Pantel 2010] for a survey). These representations are called distributional representations.\nIt is a strange historical accident that two similar sounding names \u2013 distributed and distributional \u2013 have been given to two concepts that should not be confused for many. Maybe, this has happened because the two concepts are definitely related. We argue that distributional representation are nothing more than a subset of distributed representations, and in fact can be categorized neatly into the divisions presented in the previous section\nDistributional semantics is based on a famous slogan \u2013 \u201cyou shall judge a word by the company it keeps\u201d [Firth 1957] \u2013 and on the distributional hypothesis [Harris 1964] \u2013 words have similar meaning if used in similar contexts, that is, words with the same or similar\ndistribution. Hence, the name distributional as well as the core hypothesis comes from a linguistic rather than computer science background.\nDistributional vectors represent words by describing information related to the contexts in which they appear. Put in this way it is apparent that a distributional representation is a specific case of a distributed representation, and the different name is only an indicator of the context in which this techniques originated. Representations for sentences are generally obtained combining vectors representing words.\nHence, distributional semantics is a special case of distributed representations with a restriction on what can be used as features in vector spaces: features represent a bit of contextual information. Then, the largest body of research is on what should be used to represent contexts and how it should be taken into account. Once this is decided, large matrices X representing words in context are collected and, then, dimensionality reduction techniques are applied to have treatable and more discriminative vectors.\nIn the rest of the section, we present how to build matrices representing words in context, we will shortly recap on how dimensionality reduction techniques have been used in distributional semantics, and, finally, we report on word2vec [Mikolov et al. 2013], which is a novel distributional semantic techniques based on deep learning."}, {"heading": "4.1. Building distributional representations for words from a corpus", "text": "The major issue in distributional semantics is how to build distributional representations for words by observing word contexts in a collection of documents. In this section, we will describe these techniques using the example of the corpus in Table I.\nA first and simple distributional semantic representations of words is given by word vs. document matrices as those typical in information retrieval [Salton 1989]. Word context are represented by document indexes. Then, words are similar if these words similarly appear in documents. This is generally referred as topical similarity [Landauer and Dumais 1997] as words belonging to the same topic tend to be more similar. An example of this approach is given by the matrix in Eq. 1. In fact, this matrix is already a distributional and distributed representation for words which are represented as vectors in rows.\nA second strategy to build distributional representations for words is to build word vs. contextual feature matrices. These contextual features represent proxies for semantic attributes of modeled words [Baroni and Lenci 2010]. For example, contexts of the word dog will somehow have relation with the fact that a dog has four legs, barks, eats, and so on. In this case, these vectors capture a similarity that is more related to a co-hyponymy, that is, words sharing similar attributes are similar. For example, dog is more similar to cat than to car as dog and cat share more attributes than dog and car. This is often referred as attributional similarity [Turney 2006].\nA simple example of this second strategy are word-to-word matrices obtained by observing n-word windows of target words. For example, a word-to-word matrix obtained for the corpus in Table I by considering a 1-word window is the following:\nX =\n a cat dog mouse catches eats a 0 1 2 2 2 2 cat 2 0 0 0 1 0 dog 2 0 0 0 1 1\nmouse 2 0 0 0 0 0 catches 2 1 1 0 0 0 eats 1 0 1 0 0 0\n (2)\nHence, the word cat is represented by the vector cat = (2 0 0 0 1 0) and the similarity between cat and dog is higher than the similarity between cat and mouse as the cosine similarity cos(cat,dog) is higher than the cosine similarity cos(cat,mouse).\nThe research on distributional semantics focuses on two aspects: (1) the best features to represent contexts; (2) the best correlation measure among target words and features.\nHow to represent contexts is a crucial problem in distributional semantics. This problem is strictly correlated to the classical question of feature definition and feature selection in machine learning. A wide variety of features have been tried. Contexts have been represented as set of relevant words, sets of relevant syntactic triples involving target words [Pado and Lapata 2007; Rothenha\u0308usler and Schu\u0308tze 2009] and sets of labeled lexical triples [Baroni and Lenci 2010].\nFinding the best correlation measure among target words and their contextual features is the other issue. Many correlation measures have been tried. The classical measures are term frequency-inverse document frequency (tf-idf ) [Salton 1989] and point-wise mutual information (pmi). These, among other measures, are used to better capture the importance of contextual features for representing distributional semantic of words.\nThis first formulation of distributional semantics is a distributed representation that is interpretable. In fact, features represent contextual information which is a proxy for semantic attributes of target words [Baroni and Lenci 2010]."}, {"heading": "4.2. Compacting distributional representations", "text": "As distributed representations, distributional representations can undergo the process of dimensionality reduction (see Sec. 3.1) with Principal Component Analysis and Random Indexing. This process is used for two issues. The first is the classical problem of reducing the dimensions of the representation to obtain more compact representations. The second instead want to help the representation to focus on more discriminative dimensions. This latter issue focuses on the feature selection and merging which is an important task in making these representations more effective on the final task of similarity detection.\nPrincipal Component Analysis (PCA) is largely applied in compacting distributional representations: Latent Semantic Analysis (LSA) is a prominent example [Landauer and Dumais 1997]. LSA were born in Information Retrieval with the idea of reducing word-to-document matrices. Hence, in this compact representation, word context are documents and distributional vectors of words report on the documents where words appear. This or similar matrix reduction techniques have been then applied to word-to-word matrices.\nIn Distributional Semantics, random indexing has been used to solve some issues that arise naturally with PCA when working with large vocabularies and large corpora. PCA has some scalability problems:\n\u2014 The original co-occurrence matrix is very costly to obtain and store, moreover, it is only needed to be later transformed;\n\u2014 Dimensionality reduction is also very costly, moreover, with the dimensions at hand it can only be done with iterative methods;\n\u2014 The entire method is not incremental, if we want to add new words to our corpus we have to recompute the entire co-occurrence matrix and then re-perform the PCA step.\nRandom Indexing [Sahlgren 2005] solves these problems: it is an incremental method (new words can be easily added any time at low computational cost) which creates word vector of reduced dimension without the need to create the full dimensional matrix.\nInterpretability of compacted distributional semantic vectors is comparable to the interpretability of distributed representations obtained with the same techniques."}, {"heading": "4.3. Learning representations: word2vec", "text": "Recently, distributional hypothesis has invaded neural networks: word2vec [Mikolov et al. 2013] uses contextual information to learn word vectors. Hence, we discuss this technique in the section devoted to distributional semantics.\nThe name word2Vec comprises two similar techniques, called skip grams and continuous bag of words (CBOW). Both methods are neural networks, the former takes input a word and try to predict its context, while the latter does the reverse process, predicting a word from the words surrounding it. With this technique there is no explicitly computed co-occurrence matrix, and neither there is an explicit association feature between pairs of words, instead, the regularities and distribution of the words are learned implicitly by the network.\nWe describe only CBOW because it is conceptually simpler and because the core ideas are the same in both cases. The full network is generally realized with two layers W1n\u00d7k and W2k\u00d7n plus a softmax layer to reconstruct the final vector representing the word. In the learning phase, the input and the output of the network are local representation for words. In CBOW, the network aims to predict a target word given context words. For example, given the sentence s1 of the corpus in Table I, the network has to predict catches given its context (see Figure 1).\nHence, CBOW offers an encoder W1n\u00d7k, that is, a linear word encoder from data where n is the size of the vocabulary and k is the size of the distributional vector. This encoder models contextual information learned by maximizing the prediction capability of the network. A nice description on how this approach is related to previous techniques is given in [Goldberg and Levy 2014].\nClearly, CBOW distributional vectors are not easily human and machine interpretable. In fact, specific dimensions of vectors have not a particular meaning and, differently from what happens for auto-encoders (see Sec. 3.2.1), these networks are not trained to be invertible."}, {"heading": "5. COMPOSING DISTRIBUTED REPRESENTATIONS", "text": "In the previous sections, we described how one symbol or a bag-of-symbols can be transformed in distributed representations focusing on whether these distributed representations are interpretable. In this section, we want to investigate a second and important aspect of these representations, that is, are these representation composable as symbolic representations? And, if these representations are composed, are still interpretable?\nComposability is the ability of a symbolic representation to describe sequences or structures by composing symbols with specific rules. In this process, symbols remain distinct and composing rules are clear. Hence, final sequences and structures can be used for subsequent steps as knowledge repositories.\nComposability is an important aspect for any representation and, then, for a distributed representation. Understanding to what extent a distributed representation is composable and how information can be recovered is then a critical issue. In fact, this issue has been strongly posed by Plate [Plate 1995; Plate 1994] who analyzed how same specific distributed representations encode structural information and how this structural information can be recovered back.\nCurrent approaches for treating distributed/distributional representation of sequences and structures mix two aspects in one model: a \u201csemantic\u201d aspect and a representational aspect. Generally, the semantic aspect is the predominant and the representational aspect is left aside. For \u201csemantic\u201d aspect, we refer to the reason why distributed symbols are composed: a final task in neural network applications or the need to give a distributional semantic vector for sequences of words. This latter is the case for compositional distributional semantics [Clark et al. 2008; Baroni et al. 2014]. For the representational aspect, we refer to the fact that composed distributed representations are in fact representing structures and these representations can be decoded back in order to extract what is in these structures.\nAlthough the \u201csemantic\u201d aspect seems to be predominant in models-that-compose, the convolution conjecture [Zanzotto et al. 2015] hypothesizes that the two aspects coexist and the representational aspect plays always a crucial role. According to this conjecture, structural information is preserved in any model that composes and structural information emerges back when comparing two distributed representations with dot product to determine their similarity.\nHence, given the convolution conjecture, models-that-compose produce distributed representations for structures that can be interpreted back. Interpretability is a very important feature in these models-that-compose which will drive our analysis.\nIn this section we will explore the issues faced with the compositionality of representations, and the main \u201ctrends\u201d, which correspond somewhat to the categories already presented. In particular we will start from the work on compositional distributional semantics, then we revise the work on holographic reduced representations [Plate 1995; Neumann 2001] and, finally, we analyze the recent approaches with recurrent and recursive neural networks. Again, these categories are not entirely disjoint, and methods presented in one class can be often interpreted to belonging into another class."}, {"heading": "5.1. Compositional Distributional Semantics", "text": "In distributional semantics, models-that-compose have the name of compositional distributional semantics models (CDSMs) [Baroni et al. 2014; Mitchell and Lapata 2010] and aim to apply the principle of compositionality [Frege 1884; Montague 1974] to compute distributional semantic vectors for phrases. These CDSMs produce distributional semantic vectors of phrases by composing distributional vectors of words in these phrases. These models generally exploit structured or syntactic representations of phrases to derive their distributional meaning. Hence, CDSMs aim to give a complete semantic model for distributional semantics.\nAs in distributional semantics for words, the aim of CDSMs is to produce similar vectors for semantically similar sentences regardless their lengths or structures. For example, words and word definitions in dictionaries should have similar vectors as discussed in [Zanzotto et al. 2010]. As usual in distributional semantics, similarity is captured with dot products (or similar metrics) among distributional vectors.\nThe applications of these CDSMs encompass multi-document summarization, recognizing textual entailment [Dagan et al. 2013] and, obviously, semantic textual similarity detection [Agirre et al. 2013].\nApparently, these CDSMs are far from composable distributed representations that can be interpreted back. In some sense, their nature wants that resulting vectors forget how these are obtained and focus on the final distributional meaning of phrases. There is some evidence that this is not exactly the case.\nThe convolution conjecture [Zanzotto et al. 2015] suggests that many CDSMs produce distributional vectors where structural information and vectors for individual words can be still intepreted. Hence, many CDSMs are composable and interpretable.\nIn the rest of this section, we will show some classes of these CDSMs and we focus on describing how these morels are interpretable.\n5.1.1. Additive Models. Additive models for compositional distributional semantics are important examples of models-that-composes where semantic and representational aspects is clearly separated. Hence, these models can be highly interpretable.\nThese additive models have been formally captured in the general framework for two words sequences proposed by Mitchell&Lapata [2008]. The general framework for composing distributional vectors of two word sequences \u201cu v\u201d is the following:\np = f(u,v;R;K) (3)\nwhere p \u2208 Rn is the composition vector, u and v are the vectors for the two words u and v, R is the grammatical relation linking the two words and K is any other additional knowledge used in the composition operation. In the additive model, this equation has the following form:\np = f(u,v;R;K) = ARu +BRv (4)\nwhere AR and BR are two square matrices depending on the grammatical relation R which may be learned from data [Zanzotto et al. 2010; Guevara 2010].\nBefore investigating if these models are interpretable, let introduce a recursive formulation of additive models which can be applied to structural representations of sentences. For this purpose, we use dependency trees. A dependency tree can be defined as a tree whose nodes are words and the typed links are the relations between two words. The root of the tree represents the word that governs the meaning of the sentence. A dependency tree T is then a\nword if it is a final node or it has a root rT and links (rT , R, Ci) where Ci is the i-th subtree of the node rT and R is the relation that links the node rT with Ci. The dependency trees of two example sentences are reported in Figure 2. The recursive formulation is then the following:\nfr(T ) = \u2211 i (ARrT +BRfr(Ci))\nAccording to the recursive definition of the additive model, the function fr(T ) results in a linear combination of elements Msws where Ms is a product of matrices that represents the structure and ws is the distributional meaning of one word in this structure, that is:\nfr(T ) = \u2211\ns\u2208S(T )\nMsws\nwhere S(T ) are the relevant substructures of T . In this case, S(T ) contains the link chains. For example, the first sentence in Fig. 2 has a distributed vector defined in this way:\nfr(cows eat animal extracts) =\n= AV Neat +BV Ncows +AV Neat +\n+BV Nfr(animal extracts) =\n= AV Neat +BV Ncows +AV Neat +\n+BV NANNextracts +BV NBNNanimal\nEach term of the sum has a part that represents the structure and a part that represents the meaning, for example:\nstructure\ufe37 \ufe38\ufe38 \ufe37 BV NBNN beef\ufe38 \ufe37\ufe37 \ufe38\nmeaning\nHence, this recursive additive model for compositional semantics is a model-that-composes which, in principle, can be highly interpretable. By selecting matrices Ms such that:\nMTs1Ms2 \u2248 { I s1 = s2 0 s1 6= s2\n(5)\nit is possible to recover distributional semantic vectors related to words that are in specific parts of the structure. For example, the main verb of the sample sentence in Fig. 2 with a matrix ATV N , that is:\nATV Nfr(cows eat animal extracts) \u2248 2eat In general, matrices derived for compositional distributional semantic models [Guevara 2010; Zanzotto et al. 2010] do not have this property but it is possible to obtain matrices with this property by applying thee Jonson-Linderstrauss Tranform [Johnson and Lindenstrauss 1984] or similar techniques as discussed also in [Zanzotto et al. 2015].\n5.1.2. Lexical Functional Compositional Distributional Semantic Models. Lexical Functional Models are compositional distributional semantic models where words are tensors and each type of word is represented by tensors of different order. Composing meaning is then composing these tensors to obtain vectors. These models have solid mathematical background linking Lambek pregroup theory, formal semantics and distributional semantics [Coecke et al. 2010]. Lexical Function models are composable distributed representations, yet, in the following, we will examine whether these models produce vectors that my be interpreted.\nTo determine whether these models produce interpretable vectors, we start from a simple Lexical Function model applied to two word sequences. This model has been largely analyzed in [Baroni and Zamparelli 2010] as matrices were considered better linear models to encode adjectives.\nIn Lexical Functional models over two words sequences, there is one of the two words which as a tensor of order 2 (that is, a matrix) and one word that is represented by a vector. For example, adjectives are matrices and nouns are vectors [Baroni and Zamparelli 2010] in adjective-noun sequences. Hence, adjective-noun sequences like \u201cblack cat\u201d or \u201cwhite dog\u201d are represented as:\nf(black cat) = BLACKcat\nf(white dog) = WHITEdog\nwhere BLACK and WHITE are matrices representing the two adjectives and cat and dog are the two vectors representing the two nouns.\nThese two words models are partially interpretable: knowing the adjective it is possible to extract the noun but not vice-versa. In fact, if matrices for adjectives are invertible, there is the possibility of extracting which nouns has been related to particular adjectives. For example, if BLACK is invertible, the inverse matrix BLACK\u22121 can be used to extract the vector of cat from the vector f(black cat):\ncat = BLACK\u22121f(black cat)\nThis contributes to the interpretability of this model. Moreover, if matrices for adjectives are built using Jonson-Lindestrauss Transforms [Johnson and Lindenstrauss 1984], that is matrices with the property in Eq. 5, it is possible to pack different pieces of sentences in a single vector and, then, select only relevant information, for example:\ncat \u2248 BLACKT (f(black cat) + f(white dog)) On the contrary, knowing noun vectors, it is not possible to extract back adjective matrices. This is a strong limitation in term of interpretability.\nLexical Functional models for larger structures are composable but not interpretable at all. In fact, in general these models have tensors in the middle and these tensors are the only parts that can be inverted. Hence, in general these models are not interpretable. However, using the convolution conjecture [Zanzotto et al. 2015], it is possible to know whether subparts are contained in some final vectors obtained with these models."}, {"heading": "5.2. Holographic Representations", "text": "Holographic reduced representations (HRRs) are models-that-compose expressly designed to be interpretable [Plate 1995; Neumann 2001]. In fact, these models to encode flat structures\nrepresenting assertions and these assertions should be then searched in oder to recover pieces of knowledge that is in. For example, these representations have been used to encode logical propositions such as eat(John, apple). In this case, each atomic element has an associated vector and the vector for the compound is obtained by combining these vectors. The major concern here is to build encoding functions that can be decoded, that is, it should be possible to retrieve composing elements from final distributed vectors such as the vector of eat(John, apple).\nIn HRRs, nearly orthogonal unit vectors [Johnson and Lindenstrauss 1984] for basic symbols, circular convolution \u2297 and circular correlation \u2295 guarantees composability and intepretability. HRRs are the extension of Random Indexing (see Sec. 3.1.2) to structures. Hence, symbols are represented with vectors sampled from a multivariate normal distribution N(0, 1dId). The composition function is the circular convolution indicated as \u2297 and defined as:\nzj = (a\u2297 b)j = d\u22121\u2211 k=0 akbj\u2212k\nwhere subscripts are modulo d. Circular convolution is commutative and bilinear. This operation can be also computed using circulant matrices:\nz = (a\u2297 b) = A\u25e6b = B\u25e6a where A\u25e6 and B\u25e6 are circulant matrices of the vectors a and b. Given the properties of vectors a and b, matrices A\u25e6 and B\u25e6 have the property in Eq. 5. Hence, circular convolution is approximately invertible with the circular correlation function (\u2295) defined as follows:\ncj = (z\u2295 b)j = d\u22121\u2211 k=0 zkbj+k\nwhere again subscripts are modulo d. Circular correlation is related to inverse matrices of circulant matrices, that is BT\u25e6 . In the decoding with \u2295, parts of the structures can be derived in an approximated way, that is:\n(a\u2297 b)\u2295 b \u2248 a\nHence, circular convolution \u2297 and circular correlation \u2295 allow to build interpretable representations. For example, having the vectors e, J, and a for eat, John and apple, respectively, the following encoding and decoding produces a vector that approximates the original vector for John:\nJ \u2248 (J\u2297 e\u2297 a)\u2295 (e\u2297 a) The \u201cinvertibility\u201d of these representations is important because it allow us not to consider these representations as black boxes.\nHowever, holographic representations have severe limitations as these can encode and decode simple, flat structures. In fact, these representations are based on the circular convolution, which is a commutative function; this implies that the representation cannot keep track of composition of objects where the order matters and this phenomenon is particularly important when encoding nested structures.\nDistributed trees [Zanzotto and Dell\u2019Arciprete 2012b] have shown that the principles expressed in holographic representation can be applied to encode larger structures, overcoming the problem of reliably encoding the order in which elements are composed using the shuffled circular convolution function as the composition operator. Distributed trees are encoding functions that transform trees into low-dimensional vectors that also contain the\nencoding of every substructures of the tree. Thus, these distributed trees are particularly attractive as they can be used to represent structures in linear learning machines which are computationally efficient.\nDistributed trees and, in particular, distributed smoothed trees [Ferrone and Zanzotto 2014] represent an interesting middle way between compositional distributional semantic models and holographic representation."}, {"heading": "5.3. Compositional Models in Neural Networks", "text": "When neural networks are applied to sequences or structured data, these networks are in fact models-that-compose. However, these models result in models-that-compose which are not interpretable. In fact, composition functions are trained on specific tasks and not on the possibility of reconstructing the structured input, unless in some rare cases [Socher et al. 2011]. The input of these networks are sequences or structured data where basic symbols are embedded in local representations or distributed representations obtained with word embedding (see Sec. 4.3). The output are distributed vectors derived for specific tasks. Hence, these models-that-compose are not interpretable in our sense for their final aim and for the fact that non linear functions are adopted in the specification of the neural networks.\nIn this section, we revise some prominent neural network architectures that can be interpreted as models-that-compose: the recurrent neural networks [Krizhevsky et al. 2012; He et al. 2016; Vinyals et al. 2015a; Graves 2013] and the recursive neural networks [Socher et al. 2012].\n5.3.1. Recurrent Neural Networks. Recurrent neural networks form a very broad family of neural networks architectures that deal with the representation (and processing) of complex objects. At its core a recurrent neural network (RNN) is a network which takes in input the current element in the sequence and processes it based on an internal state which depends on previous inputs. At the moment the most powerful network architectures are convolutional neural networks [Krizhevsky et al. 2012; He et al. 2016] for vision related tasks and LSTM-type network for language related task [Vinyals et al. 2015a; Graves 2013].\nA recurrent neural network takes as input a sequence x = (x1 . . . xn) and produce as output a single vector y \u2208 Rn which is a representation of the entire sequence. At each step 1 t the network takes as input the current element xt, the previous output ht\u22121 and performs the following operation to produce the current output ht\nht = \u03c3(W [ht\u22121 xt] + b) (6)\nwhere \u03c3 is a non-linear function such as the logistic function or the hyperbolic tangent and [ht\u22121 xt] denotes the concatenation of the vectors ht\u22121 and xt. The parameters of the model are the matrix W and the bias vector b.\nHence, a recurrent neural network is effectively a learned composition function, which dynamically depends on its current input, all of its previous inputs and also on the dataset on which is trained. However, this learned composition function is basically impossible to analyze or interpret in any way. Sometime an \u201cintuitive\u201d explanation is given about what the learned weights represent: with some weights representing information that must be remembered or forgotten.\n1we can usually think of this as a timestep, but not all applications of recurrent neural network have a temporal interpretation\nEven more complex recurrent neural networks as long-short term memory (LSTM) [Hochreiter and Schmidhuber 1997] have the same problem of interpretability. LSTM are a recent and successful way for neural network to deal with longer sequences of inputs, overcoming some difficulty that RNN face in the training phase. As with RNN, LSTM network takes as input a sequence x = (x1 . . . xn) and produce as output a single vector y \u2208 Rn which is a representation of the entire sequence. At each step t the network takes as input the current element xt, the previous output ht\u22121 and performs the following operation to produce the current output ht and update the internal state ct.\nft = \u03c3(Wf [ht\u22121 xt] + bf )\nit = \u03c3(Wi [ht\u22121 xt] + bi)\not = \u03c3(Wo [ht\u22121 xt] + bo)\nc\u0303t = tanh(Wc [ht\u22121 xt] + bc)\nct = ft ct\u2212i + it c\u0303t ht = ot tanh(ct)\nwhere stands for element-wise multiplication, and the parameters of the model are the matrices Wf ,Wi,Wo,Wc and the bias vectors bf , bi, bo, bc.\nGenerally, the interpretation offered for recursive neural networks is functional or \u201cpsychological\u201d and not on the content of intermediate vectors. For example, an interpretation of the parameters of LSTM is the following:\n\u2014 ft is the forget gate: at each step takes in consideration the new input and output computed so far to decide which information in the internal state must be forgotten (that is, set to 0);\n\u2014 it is the input gate: it decides which position in the internal state will be updated, and by how much;\n\u2014 c\u0303t is the proposed new internal state, which will then be updated effectively combining the previous gate;\n\u2014 ot is the output gate: it decides how to modulate the internal state to produce the output\nThese models-that-compose have high performance on final tasks but are definitely not interpretable.\n5.3.2. Recursive Neural Network. The last class of models-that-compose that we present is the class of recursive neural networks [Socher et al. 2012]. These networks are applied to data structures as trees and are in fact applied recursively on the structure. Generally, the aim of the network is a final task as sentiment analysis or paraphrase detection.\nRecursive neural networks is then a basic block (see Fig. 4) which is recursively applied on trees like the one in Fig. 3. The formal definition is the following:\np = fU,V (u,v) = f(V u, Uv) = g(W ( V u Uv ) )\nwhere g is a component-wise sigmoid function or tanh, and W is a matrix that maps the\nconcatenation vector ( V u Uv ) to have the same dimension.\nThis method deals naturally with recursion: given a binary parse tree of a sentence s, the algorithm creates vectors and matrices representation for each node, starting from the terminal nodes. Words are represented by distributed representations or local representations. For example, the tree in Fig. 3 is processed by the recursive network in the following way. First, the network in Fig. 4 is applied to the pair (animal,extracts) and fUV (animal, extract) is obtained. Then, the network is applied to the result and eat and fUV (eat, fUV (animal, extract)) is obtained and so on.\nRecursive neural networks are not easily interpretable even if quite similar to the additive compositional distributional semantic models as those presented in Sec. 5.1.1. In fact, the non-linear function g is the one that makes final vectors less interpretable."}, {"heading": "6. CONCLUSIONS", "text": "Natural language and symbols are intimately correlated. Thinking to natural language understanding systems which are not based on symbols seems to be an heresy. However, recent advances in machine learning (ML) and in natural language processing (NLP) seem to contradict the above intuition: symbols are fading away, erased by vectors or tensors called distributed and distributional representations.\nWe made this survey to show the not-surprising link between symbolic representations and distributed/distributional representations. This is the right time to revitalize the area of interpreting how symbols are represented inside neural networks. In our opinion, this survey will help to devise new deep neural networks that can exploit existing and novel symbolic models of classical natural language processing tasks. We believe that a clearer understanding of the strict link between distributed/distributional representations and symbols will certainly lead to radically new deep learning networks."}], "references": [{"title": "Database-friendly random projections: Johnson-Lindenstrauss with binary coins", "author": ["Dimitris Achlioptas."], "venue": "Journal of computer and System Sciences 66, 4 (2003), 671\u2013687.", "citeRegEx": "Achlioptas.,? 2003", "shortCiteRegEx": "Achlioptas.", "year": 2003}, {"title": "SEM 2013 shared task: Semantic Textual Similarity", "author": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo."], "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity. Association for Computational Linguistics, Atlanta, Georgia, USA, 32\u201343. http://www. aclweb.org/anthology/S13-1004", "citeRegEx": "Agirre et al\\.,? 2013", "shortCiteRegEx": "Agirre et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 (2014).", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Frege in space: A program of compositional distributional semantics", "author": ["Marco Baroni", "Raffaela Bernardi", "Roberto Zamparelli."], "venue": "LiLT (Linguistic Issues in Language Technology) 9 (2014).", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Marco Baroni", "Alessandro Lenci."], "venue": "Comput. Linguist. 36, 4 (Dec. 2010), 673\u2013721. DOI:http://dx.doi.org/10.1162/coli a 00016", "citeRegEx": "Baroni and Lenci.,? 2010", "shortCiteRegEx": "Baroni and Lenci.", "year": 2010}, {"title": "Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space", "author": ["Marco Baroni", "Roberto Zamparelli."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Cambridge, MA, 1183\u20131193. http://www.aclweb.org/anthology/D10-1115", "citeRegEx": "Baroni and Zamparelli.,? 2010", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "In NIPS,", "citeRegEx": "Belkin and Niyogi.,? \\Q2001\\E", "shortCiteRegEx": "Belkin and Niyogi.", "year": 2001}, {"title": "Dynamic Programming", "author": ["R. Bellman", "Rand Corporation."], "venue": "Princeton University Press. https://books. google.it/books?id=wdtoPwAACAAJ", "citeRegEx": "Bellman and Corporation.,? 1957", "shortCiteRegEx": "Bellman and Corporation.", "year": 1957}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["Ella Bingham", "Heikki Mannila."], "venue": "Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 245\u2013250.", "citeRegEx": "Bingham and Mannila.,? 2001", "shortCiteRegEx": "Bingham and Mannila.", "year": 2001}, {"title": "Word Sequence Kernels", "author": ["Nicola Cancedda", "Eric Gaussier", "Cyril Goutte", "Jean Michel Renders."], "venue": "J. Mach. Learn. Res. 3 (March 2003), 1059\u20131082. http://dl.acm.org/citation.cfm?id=944919.944963", "citeRegEx": "Cancedda et al\\.,? 2003", "shortCiteRegEx": "Cancedda et al\\.", "year": 2003}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["Sharan Chetlur", "Cliff Woolley", "Philippe Vandermersch", "Jonathan Cohen", "John Tran", "Bryan Catanzaro", "Evan Shelhamer."], "venue": "arXiv preprint arXiv:1410.0759 (2014).", "citeRegEx": "Chetlur et al\\.,? 2014", "shortCiteRegEx": "Chetlur et al\\.", "year": 2014}, {"title": "Aspect of Syntax Theory", "author": ["Naom Chomsky."], "venue": "MIT Press, Cambridge, Massachussetts.", "citeRegEx": "Chomsky.,? 1957", "shortCiteRegEx": "Chomsky.", "year": 1957}, {"title": "A Compositional Distributional Model of Meaning", "author": ["Stephen Clark", "Bob Coecke", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the Second Symposium on Quantum Interaction (QI-2008) (2008), 133\u2013140.", "citeRegEx": "Clark et al\\.,? 2008", "shortCiteRegEx": "Clark et al\\.", "year": 2008}, {"title": "Mathematical Foundations for a Compositional Distributional Model of Meaning", "author": ["Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark."], "venue": "CoRR abs/1003.4394 (2010).", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "Convolution Kernels for Natural Language", "author": ["Michael Collins", "Nigel Duffy."], "venue": "NIPS. 625\u2013632.", "citeRegEx": "Collins and Duffy.,? 2001", "shortCiteRegEx": "Collins and Duffy.", "year": 2001}, {"title": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods", "author": ["Nello Cristianini", "John Shawe-Taylor."], "venue": "Cambridge University Press. http://www.amazon.ca/exec/obidos/ redirect?tag=citeulike09-20&amp; path=ASIN/0521780195", "citeRegEx": "Cristianini and Shawe.Taylor.,? 2000", "shortCiteRegEx": "Cristianini and Shawe.Taylor.", "year": 2000}, {"title": "Scalable deep learning on distributed GPUs with a GPU-specialized parameter server", "author": ["Henggang Cui", "Gregory R Ganger", "Phillip B Gibbons."], "venue": "Technical Report. CMU PDL Technical Report (CMU-PDL15-107).", "citeRegEx": "Cui et al\\.,? 2015", "shortCiteRegEx": "Cui et al\\.", "year": 2015}, {"title": "Recognizing Textual Entailment: Models and Applications", "author": ["Ido Dagan", "Dan Roth", "Mark Sammons", "Fabio Massimo Zanzotto."], "venue": "Morgan & Claypool Publishers. 1\u2013220 pages.", "citeRegEx": "Dagan et al\\.,? 2013", "shortCiteRegEx": "Dagan et al\\.", "year": 2013}, {"title": "Curse of dimensionality and particle filters", "author": ["Fred Daum", "Jim Huang."], "venue": "Aerospace Conference, 2003. Proceedings. 2003 IEEE, Vol. 4. IEEE, 4 1979\u20134 1993.", "citeRegEx": "Daum and Huang.,? 2003", "shortCiteRegEx": "Daum and Huang.", "year": 2003}, {"title": "Towards Syntax-aware Compositional Distributional Semantic Models", "author": ["Lorenzo Ferrone", "Fabio Massimo Zanzotto."], "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin City University and Association for Computational Linguistics, Dublin, Ireland, 721\u2013730. http://www.aclweb.org/anthology/C14-1068", "citeRegEx": "Ferrone and Zanzotto.,? 2014", "shortCiteRegEx": "Ferrone and Zanzotto.", "year": 2014}, {"title": "Decoding Distributed Tree Structures", "author": ["Lorenzo Ferrone", "Fabio Massimo Zanzotto", "Xavier Carreras."], "venue": "Statistical Language and Speech Processing - Third International Conference, SLSP 2015, Budapest, Hungary, November 24-26, 2015, Proceedings. 73\u201383. DOI:http://dx.doi.org/10.1007/978-3-319-25789-1 8", "citeRegEx": "Ferrone et al\\.,? 2015", "shortCiteRegEx": "Ferrone et al\\.", "year": 2015}, {"title": "Papers in Linguistics", "author": ["John R. Firth."], "venue": "Oxford University Press., London.", "citeRegEx": "Firth.,? 1957", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "A Survey of Dimension Reduction Techniques", "author": ["Imola Fodor."], "venue": "Technical Report.", "citeRegEx": "Fodor.,? 2002", "shortCiteRegEx": "Fodor.", "year": 2002}, {"title": "Die Grundlagen der Arithmetik (The Foundations of Arithmetic): eine logischmathematische Untersuchung ber den Begriff der Zahl", "author": ["Gottlob Frege."], "venue": "Breslau.", "citeRegEx": "Frege.,? 1884", "shortCiteRegEx": "Frege.", "year": 1884}, {"title": "On bias, variance, 0/1loss, and the curse-of-dimensionality", "author": ["Jerome H Friedman."], "venue": "Data mining and knowledge discovery 1, 1 (1997), 55\u201377.", "citeRegEx": "Friedman.,? 1997", "shortCiteRegEx": "Friedman.", "year": 1997}, {"title": "word2vec Explained: deriving Mikolov et al.\u2019s negative-sampling word-embedding method", "author": ["Yoav Goldberg", "Omer Levy"], "venue": "arXiv preprint arXiv:1402.3722", "citeRegEx": "Goldberg and Levy.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg and Levy.", "year": 2014}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."], "venue": "Advances in Neural Information Processing Systems. 2672\u20132680.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Generating Sequences With Recurrent Neural Networks", "author": ["Alex Graves."], "venue": "CoRR abs/1308.0850 (2013). http://arxiv.org/abs/1308.0850", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP \u201911). Association for Computational Linguistics, Stroudsburg, PA, USA, 1394\u20131404. http://dl.acm.org/citation.cfm?id=2145432.2145580", "citeRegEx": "Grefenstette and Sadrzadeh.,? 2011", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "A Regression Model of Adjective-Noun Compositionality in Distributional Semantics", "author": ["Emiliano Guevara."], "venue": "Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics. Association for Computational Linguistics, Uppsala, Sweden, 33\u201337. http://www.aclweb.org/anthology/W10-2805", "citeRegEx": "Guevara.,? 2010", "shortCiteRegEx": "Guevara.", "year": 2010}, {"title": "Distributional Structure", "author": ["Zellig Harris."], "venue": "The Philosophy of Linguistics, Jerrold J. Katz and Jerry A. Fodor (Eds.). Oxford University Press, New York.", "citeRegEx": "Harris.,? 1964", "shortCiteRegEx": "Harris.", "year": 1964}, {"title": "Convolution kernels on discrete structures", "author": ["David Haussler."], "venue": "Technical Report. University of California at Santa Cruz. http://www.cbse.ucsc.edu/staff/haussler pubs/convolutions.pdf", "citeRegEx": "Haussler.,? 1999", "shortCiteRegEx": "Haussler.", "year": 1999}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "arXiv preprint arXiv:1603.05027 (2016).", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Distributed representations", "author": ["G.E. Hinton", "J.L. McClelland", "D.E. Rumelhart."], "venue": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations, D. E. Rumelhart and J. L. McClelland (Eds.). MIT Press, Cambridge, MA.", "citeRegEx": "Hinton et al\\.,? 1986", "shortCiteRegEx": "Hinton et al\\.", "year": 1986}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9, 8 (1997), 1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["W. Johnson", "J. Lindenstrauss."], "venue": "Contemp. Math. 26 (1984), 189\u2013206.", "citeRegEx": "Johnson and Lindenstrauss.,? 1984", "shortCiteRegEx": "Johnson and Lindenstrauss.", "year": 1984}, {"title": "Recurrent Convolutional Neural Networks for Discourse Compositionality", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Workshop on Continuous Vector Space Models and their Compositionality (2013).", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Curse of dimensionality", "author": ["Eamonn Keogh", "Abdullah Mueen."], "venue": "Encyclopedia of Machine Learning. Springer, 257\u2013258.", "citeRegEx": "Keogh and Mueen.,? 2011", "shortCiteRegEx": "Keogh and Mueen.", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems. 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A Solution to Plato\u2019s Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge", "author": ["Thomas K. Landauer", "Susan T. Dumais."], "venue": "Psychological Review 104, 2 (April 1997), 211\u2013240. http://www.sciencedirect.com/science/article/B6X04-46P4NMC-Y/2/ f82804d09e673bd79321d50d30279792", "citeRegEx": "Landauer and Dumais.,? 1997", "shortCiteRegEx": "Landauer and Dumais.", "year": 1997}, {"title": "Deep learning", "author": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton."], "venue": "Nature 521, 7553 (2015), 436\u2013444.", "citeRegEx": "LeCun et al\\.,? 2015", "shortCiteRegEx": "LeCun et al\\.", "year": 2015}, {"title": "Autoencoder for words", "author": ["Cheng-Yuan Liou", "Wei-Chen Cheng", "Jiun-Wei Liou", "Daw-Ran Liou."], "venue": "Neurocomputing 139 (2014), 84 \u2013 96. DOI:http://dx.doi.org/10.1016/j.neucom.2013.09.055", "citeRegEx": "Liou et al\\.,? 2014", "shortCiteRegEx": "Liou et al\\.", "year": 2014}, {"title": "Text classification using string kernels", "author": ["Huma Lodhi", "Craig Saunders", "John Shawe-Taylor", "Nello Cristianini", "Chris Watkins."], "venue": "J. Mach. Learn. Res. 2 (March 2002), 419\u2013444. DOI:http://dx.doi.org/10.1162/153244302760200687", "citeRegEx": "Lodhi et al\\.,? 2002", "shortCiteRegEx": "Lodhi et al\\.", "year": 2002}, {"title": "Low Rank Approximation: Algorithms, Implementation, Applications", "author": ["Ivan Markovsky."], "venue": "(January 2012). http://eprints.soton.ac.uk/273101/", "citeRegEx": "Markovsky.,? 2012", "shortCiteRegEx": "Markovsky.", "year": 2012}, {"title": "Stacked convolutional autoencoders for hierarchical feature extraction", "author": ["Jonathan Masci", "Ueli Meier", "Dan Cire\u015fan", "J\u00fcrgen Schmidhuber."], "venue": "International Conference on Artificial Neural Networks. Springer, 52\u201359.", "citeRegEx": "Masci et al\\.,? 2011", "shortCiteRegEx": "Masci et al\\.", "year": 2011}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR abs/1301.3781 (2013). http://arxiv.org/abs/1301.3781", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Vector-based Models of Semantic Composition", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Proceedings of ACL-08: HLT. Association for Computational Linguistics, Columbus, Ohio, 236\u2013244. http://www. aclweb.org/anthology/P/P08/P08-1028", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Composition in Distributional Models of Semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive Science (2010). DOI:http://dx.doi.org/10.1111/j.1551-6709.2010.01106.x", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller."], "venue": "arXiv preprint arXiv:1312.5602 (2013).", "citeRegEx": "Mnih et al\\.,? 2013", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "English as a Formal Language", "author": ["Richard Montague."], "venue": "Formal Philosophy: Selected Papers of Richard Montague, Richmond Thomason (Ed.). Yale University Press, New Haven, 188\u2013221.", "citeRegEx": "Montague.,? 1974", "shortCiteRegEx": "Montague.", "year": 1974}, {"title": "Holistic processing of hierarchical structures in connectionist networks", "author": ["Jane Neumann."], "venue": "Ph.D. Dissertation. University of Edinburgh.", "citeRegEx": "Neumann.,? 2001", "shortCiteRegEx": "Neumann.", "year": 2001}, {"title": "Dependency-based construction of semantic space models", "author": ["Sebastian Pado", "Mirella Lapata."], "venue": "Computational Linguistics 33, 2 (2007), 161\u2013199.", "citeRegEx": "Pado and Lapata.,? 2007", "shortCiteRegEx": "Pado and Lapata.", "year": 2007}, {"title": "Principal components analysis", "author": ["Karl Pearson."], "venue": "The London, Edinburgh and Dublin Philosophical Magazine and Journal 6, 2 (1901), 566.", "citeRegEx": "Pearson.,? 1901", "shortCiteRegEx": "Pearson.", "year": 1901}, {"title": "Distributed Representations and Nested Compositional Structure", "author": ["T.A. Plate."], "venue": "Ph.D. Dissertation. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.5527", "citeRegEx": "Plate.,? 1994", "shortCiteRegEx": "Plate.", "year": 1994}, {"title": "Holographic reduced representations", "author": ["T.A. Plate."], "venue": "IEEE Transactions on Neural Networks 6, 3 (1995), 623\u2013641. DOI:http://dx.doi.org/10.1109/72.377968", "citeRegEx": "Plate.,? 1995", "shortCiteRegEx": "Plate.", "year": 1995}, {"title": "The perceptron: a probabilistic model for information storage and organization in the brain", "author": ["Frank Rosenblatt."], "venue": "Psychological Reviews 65, 6 (November 1958), 386\u2013408. http://www.ncbi.nlm.nih.gov/ pubmed/13602029", "citeRegEx": "Rosenblatt.,? 1958", "shortCiteRegEx": "Rosenblatt.", "year": 1958}, {"title": "Unsupervised Classification with Dependency Based Word Spaces", "author": ["Klaus Rothenh\u00e4usler", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the Workshop on Geometrical Models of Natural Language Semantics (GEMS \u201909). Association for Computational Linguistics, Stroudsburg, PA, USA, 17\u201324. http://dl.acm.org/ citation.cfm?id=1705415.1705418", "citeRegEx": "Rothenh\u00e4usler and Sch\u00fctze.,? 2009", "shortCiteRegEx": "Rothenh\u00e4usler and Sch\u00fctze.", "year": 2009}, {"title": "An introduction to random indexing", "author": ["Magnus Sahlgren."], "venue": "Proceedings of the Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering TKE. Copenhagen, Denmark.", "citeRegEx": "Sahlgren.,? 2005", "shortCiteRegEx": "Sahlgren.", "year": 2005}, {"title": "Automatic text processing: the transformation, analysis and retrieval of information by computer", "author": ["G. Salton."], "venue": "Addison-Wesley.", "citeRegEx": "Salton.,? 1989", "shortCiteRegEx": "Salton.", "year": 1989}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber."], "venue": "Neural Networks 61 (2015), 85\u2013117.", "citeRegEx": "Schmidhuber.,? 2015", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman."], "venue": "arXiv preprint arXiv:1409.1556 (2014).", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection", "author": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennington", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Advances in Neural Information Processing Systems 24.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic Compositionality Through Recursive Matrix-Vector Spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "A survey of dimensionality reduction techniques", "author": ["Carlos Oscar S\u00e1nchez Sorzano", "Javier Vargas", "A Pascual Montano."], "venue": "arXiv preprint arXiv:1403.2877 (2014).", "citeRegEx": "Sorzano et al\\.,? 2014", "shortCiteRegEx": "Sorzano et al\\.", "year": 2014}, {"title": "Similarity of Semantic Relations", "author": ["Peter D. Turney."], "venue": "Comput. Linguist. 32, 3 (2006), 379\u2013416. DOI:http://dx.doi.org/10.1162/coli.2006.32.3.379", "citeRegEx": "Turney.,? 2006", "shortCiteRegEx": "Turney.", "year": 2006}, {"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["Peter D. Turney", "Patrick Pantel."], "venue": "J. Artif. Intell. Res. (JAIR) 37 (2010), 141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol."], "venue": "Proceedings of the 25th international conference on Machine learning. ACM, 1096\u20131103.", "citeRegEx": "Vincent et al\\.,? 2008", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol."], "venue": "J. Mach. Learn. Res. 11 (Dec. 2010), 3371\u20133408. http://dl.acm.org/citation.cfm?id=1756006. 1953039", "citeRegEx": "Vincent et al\\.,? 2010", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Grammar as a Foreign Language", "author": ["Oriol Vinyals", "L ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems 28, C. Cortes, N. D.", "citeRegEx": "Vinyals et al\\.,? 2015a", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 3156\u20133164.", "citeRegEx": "Vinyals et al\\.,? 2015b", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "arXiv preprint arXiv:1506.06158 (2015).", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Beyond regression: New tools for prediction and analysis in the behavioral sciences", "author": ["Paul Werbos."], "venue": "(1974).", "citeRegEx": "Werbos.,? 1974", "shortCiteRegEx": "Werbos.", "year": 1974}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1502.03044 2, 3 (2015), 5.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Distributed tree kernels", "author": ["F.M. Zanzotto", "L. Dell\u2019Arciprete"], "venue": "In Proceedings of International Conference on Machine Learning. Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Zanzotto and Dell.Arciprete.,? \\Q2012\\E", "shortCiteRegEx": "Zanzotto and Dell.Arciprete.", "year": 2012}, {"title": "Distributed Tree Kernels", "author": ["Fabio Massimo Zanzotto", "Lorenzo Dell\u2019Arciprete"], "venue": "In Proceedings of International Conference on Machine", "citeRegEx": "Zanzotto and Dell.Arciprete.,? \\Q2012\\E", "shortCiteRegEx": "Zanzotto and Dell.Arciprete.", "year": 2012}, {"title": "When the Whole is Not Greater Than the Combination of Its Parts: A \u201dDecompositional\u201d Look at Compositional Distributional Semantics", "author": ["Fabio Massimo Zanzotto", "Lorenzo Ferrone", "Marco Baroni."], "venue": "Comput. Linguist. 41, 1 (March 2015), 165\u2013173. DOI:http://dx.doi.org/10.1162/COLI a 00215", "citeRegEx": "Zanzotto et al\\.,? 2015", "shortCiteRegEx": "Zanzotto et al\\.", "year": 2015}, {"title": "Estimating Linear Models for Compositional Distributional Semantics", "author": ["Fabio Massimo Zanzotto", "Ioannis Korkontzelos", "Francesca Fallucchi", "Suresh Manandhar."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (COLING).", "citeRegEx": "Zanzotto et al\\.,? 2010", "shortCiteRegEx": "Zanzotto et al\\.", "year": 2010}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus."], "venue": "European Conference on Computer Vision. Springer, 818\u2013833.", "citeRegEx": "Zeiler and Fergus.,? 2014", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}, {"title": "Bilingual Word Embeddings for Phrase-Based Machine Translation", "author": ["Will Y Zou", "Richard Socher", "Daniel M Cer", "Christopher D Manning"], "venue": null, "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 40, "context": "In ML, distributed representations are pushing deep learning models [LeCun et al. 2015; Schmidhuber 2015] towards amazing results in many high-level tasks such as image recognition [He et al.", "startOffset": 68, "endOffset": 105}, {"referenceID": 32, "context": "2015; Schmidhuber 2015] towards amazing results in many high-level tasks such as image recognition [He et al. 2016; Simonyan and Zisserman 2014; Zeiler and Fergus 2014], image generation [Goodfellow et al.", "startOffset": 99, "endOffset": 168}, {"referenceID": 26, "context": "2016; Simonyan and Zisserman 2014; Zeiler and Fergus 2014], image generation [Goodfellow et al. 2014] and image captioning [Vinyals et al.", "startOffset": 77, "endOffset": 101}, {"referenceID": 69, "context": "2014] and image captioning [Vinyals et al. 2015b; Xu et al. 2015], machine translation [Bahdanau et al.", "startOffset": 27, "endOffset": 65}, {"referenceID": 72, "context": "2014] and image captioning [Vinyals et al. 2015b; Xu et al. 2015], machine translation [Bahdanau et al.", "startOffset": 27, "endOffset": 65}, {"referenceID": 2, "context": "2015], machine translation [Bahdanau et al. 2014; Zou et al. 2013], syntactic parsing [Vinyals et al.", "startOffset": 27, "endOffset": 66}, {"referenceID": 78, "context": "2015], machine translation [Bahdanau et al. 2014; Zou et al. 2013], syntactic parsing [Vinyals et al.", "startOffset": 27, "endOffset": 66}, {"referenceID": 68, "context": "2013], syntactic parsing [Vinyals et al. 2015a; Weiss et al. 2015] and even game playing at human level [Silver et al.", "startOffset": 25, "endOffset": 66}, {"referenceID": 70, "context": "2013], syntactic parsing [Vinyals et al. 2015a; Weiss et al. 2015] and even game playing at human level [Silver et al.", "startOffset": 25, "endOffset": 66}, {"referenceID": 48, "context": "2015] and even game playing at human level [Silver et al. 2016; Mnih et al. 2013].", "startOffset": 43, "endOffset": 81}, {"referenceID": 12, "context": "Instead, as a complete semantic representation, vectors for phrases [Mitchell and Lapata 2008; Baroni and Zamparelli 2010; Clark et al. 2008; Grefenstette and Sadrzadeh 2011; Zanzotto et al. 2010] and sentences [Socher et al.", "startOffset": 68, "endOffset": 196}, {"referenceID": 76, "context": "Instead, as a complete semantic representation, vectors for phrases [Mitchell and Lapata 2008; Baroni and Zamparelli 2010; Clark et al. 2008; Grefenstette and Sadrzadeh 2011; Zanzotto et al. 2010] and sentences [Socher et al.", "startOffset": 68, "endOffset": 196}, {"referenceID": 61, "context": "2010] and sentences [Socher et al. 2011; Socher et al. 2012; Kalchbrenner and Blunsom 2013] are obtained by composing vectors for words.", "startOffset": 20, "endOffset": 91}, {"referenceID": 62, "context": "2010] and sentences [Socher et al. 2011; Socher et al. 2012; Kalchbrenner and Blunsom 2013] are obtained by composing vectors for words.", "startOffset": 20, "endOffset": 91}, {"referenceID": 40, "context": "The success of distributed and distributional representations over symbolic approaches is mainly due to the advent of new parallel paradigms that pushed neural networks [Rosenblatt 1958; Werbos 1974] towards deep learning [LeCun et al. 2015; Schmidhuber 2015].", "startOffset": 222, "endOffset": 259}, {"referenceID": 10, "context": "Massively parallel algorithms running on Graphic Processing Units (GPUs) [Chetlur et al. 2014; Cui et al. 2015] crunch vectors, matrices and tensors faster than decades ago.", "startOffset": 73, "endOffset": 111}, {"referenceID": 16, "context": "Massively parallel algorithms running on Graphic Processing Units (GPUs) [Chetlur et al. 2014; Cui et al. 2015] crunch vectors, matrices and tensors faster than decades ago.", "startOffset": 73, "endOffset": 111}, {"referenceID": 20, "context": "However, there is a strict link between distributed/distributional representations and symbols, being the first an approximation of the second [Plate 1994; Plate 1995; Ferrone and Zanzotto 2014; Ferrone et al. 2015].", "startOffset": 143, "endOffset": 215}, {"referenceID": 12, "context": "Finally we discuss more in depth the general issue of compositionality, analyzing three different approaches to the problem: compositional distributional semantics [Clark et al. 2008; Baroni et al. 2014], holographic reduced representations [Plate 1994; Neumann 2001], and recurrent neural networks [Kalchbrenner and Blunsom 2013; Socher et al.", "startOffset": 164, "endOffset": 203}, {"referenceID": 3, "context": "Finally we discuss more in depth the general issue of compositionality, analyzing three different approaches to the problem: compositional distributional semantics [Clark et al. 2008; Baroni et al. 2014], holographic reduced representations [Plate 1994; Neumann 2001], and recurrent neural networks [Kalchbrenner and Blunsom 2013; Socher et al.", "startOffset": 164, "endOffset": 203}, {"referenceID": 62, "context": "2014], holographic reduced representations [Plate 1994; Neumann 2001], and recurrent neural networks [Kalchbrenner and Blunsom 2013; Socher et al. 2012].", "startOffset": 101, "endOffset": 152}, {"referenceID": 42, "context": "To clarify what are distributed representations, we first describe what are local representations (as referred in [Plate 1995]), that is, common feature vectors used to represent symbols, sequences of symbols [Lodhi et al. 2002; Cancedda et al. 2003] or symbolic structures [Haussler 1999; Collins and Duffy 2001] in kernel machines [Cristianini and Shawe-Taylor 2000] or in searching models such as the vector space model [Salton 1989].", "startOffset": 209, "endOffset": 250}, {"referenceID": 9, "context": "To clarify what are distributed representations, we first describe what are local representations (as referred in [Plate 1995]), that is, common feature vectors used to represent symbols, sequences of symbols [Lodhi et al. 2002; Cancedda et al. 2003] or symbolic structures [Haussler 1999; Collins and Duffy 2001] in kernel machines [Cristianini and Shawe-Taylor 2000] or in searching models such as the vector space model [Salton 1989].", "startOffset": 209, "endOffset": 250}, {"referenceID": 33, "context": "In a distributed representation [Plate 1995; Hinton et al. 1986] the informational content is distributed (hence the name) among multiple units, and at the same time each unit can contribute to the representation of multiple elements.", "startOffset": 32, "endOffset": 64}, {"referenceID": 3, "context": "In distributed or, better, in distributional semantic representations [Baroni et al. 2014; Mitchell and Lapata 2010] these two concepts often coincide.", "startOffset": 70, "endOffset": 116}, {"referenceID": 63, "context": "The decoding function is: \u03b4(v\u2032) = W d v \u2032 and W d Wd = I if d is the rank of the matrix X, otherwise it is a degraded approximation (for more details refer to [Fodor 2002; Sorzano et al. 2014]).", "startOffset": 159, "endOffset": 192}, {"referenceID": 61, "context": "\u2014 task-independent representations learned with a standalone algorithm (as in autoencoders [Socher et al. 2011; Liou et al. 2014]) which is independent from any task, and which learns a representation that only depends from the dataset used;", "startOffset": 91, "endOffset": 129}, {"referenceID": 41, "context": "\u2014 task-independent representations learned with a standalone algorithm (as in autoencoders [Socher et al. 2011; Liou et al. 2014]) which is independent from any task, and which learns a representation that only depends from the dataset used;", "startOffset": 91, "endOffset": 129}, {"referenceID": 61, "context": "Autoencoders are a task independent technique to learn a distributed representation encoder \u03b7 : R \u2192 R by using local representations of a set of examples [Socher et al. 2011; Liou et al. 2014].", "startOffset": 154, "endOffset": 192}, {"referenceID": 41, "context": "Autoencoders are a task independent technique to learn a distributed representation encoder \u03b7 : R \u2192 R by using local representations of a set of examples [Socher et al. 2011; Liou et al. 2014].", "startOffset": 154, "endOffset": 192}, {"referenceID": 67, "context": "Autoencoders have been further improved with denoising autoencoders [Vincent et al. 2010; Vincent et al. 2008; Masci et al. 2011] that are a variant of autoencoders where the goal is to reconstruct the input from a corrupted version.", "startOffset": 68, "endOffset": 129}, {"referenceID": 66, "context": "Autoencoders have been further improved with denoising autoencoders [Vincent et al. 2010; Vincent et al. 2008; Masci et al. 2011] that are a variant of autoencoders where the goal is to reconstruct the input from a corrupted version.", "startOffset": 68, "endOffset": 129}, {"referenceID": 44, "context": "Autoencoders have been further improved with denoising autoencoders [Vincent et al. 2010; Vincent et al. 2008; Masci et al. 2011] that are a variant of autoencoders where the goal is to reconstruct the input from a corrupted version.", "startOffset": 68, "endOffset": 129}, {"referenceID": 38, "context": "This is particularly visible with convolutional network [Krizhevsky et al. 2012] applied to computer vision tasks.", "startOffset": 56, "endOffset": 80}, {"referenceID": 45, "context": "In the rest of the section, we present how to build matrices representing words in context, we will shortly recap on how dimensionality reduction techniques have been used in distributional semantics, and, finally, we report on word2vec [Mikolov et al. 2013], which is a novel distributional semantic techniques based on deep learning.", "startOffset": 237, "endOffset": 258}, {"referenceID": 45, "context": "Recently, distributional hypothesis has invaded neural networks: word2vec [Mikolov et al. 2013] uses contextual information to learn word vectors.", "startOffset": 74, "endOffset": 95}, {"referenceID": 12, "context": "This latter is the case for compositional distributional semantics [Clark et al. 2008; Baroni et al. 2014].", "startOffset": 67, "endOffset": 106}, {"referenceID": 3, "context": "This latter is the case for compositional distributional semantics [Clark et al. 2008; Baroni et al. 2014].", "startOffset": 67, "endOffset": 106}, {"referenceID": 75, "context": "Although the \u201csemantic\u201d aspect seems to be predominant in models-that-compose, the convolution conjecture [Zanzotto et al. 2015] hypothesizes that the two aspects coexist and the representational aspect plays always a crucial role.", "startOffset": 106, "endOffset": 128}, {"referenceID": 3, "context": "In distributional semantics, models-that-compose have the name of compositional distributional semantics models (CDSMs) [Baroni et al. 2014; Mitchell and Lapata 2010] and aim to apply the principle of compositionality [Frege 1884; Montague 1974] to compute distributional semantic vectors for phrases.", "startOffset": 120, "endOffset": 166}, {"referenceID": 76, "context": "For example, words and word definitions in dictionaries should have similar vectors as discussed in [Zanzotto et al. 2010].", "startOffset": 100, "endOffset": 122}, {"referenceID": 17, "context": "The applications of these CDSMs encompass multi-document summarization, recognizing textual entailment [Dagan et al. 2013] and, obviously, semantic textual similarity detection [Agirre et al.", "startOffset": 103, "endOffset": 122}, {"referenceID": 1, "context": "2013] and, obviously, semantic textual similarity detection [Agirre et al. 2013].", "startOffset": 60, "endOffset": 80}, {"referenceID": 75, "context": "The convolution conjecture [Zanzotto et al. 2015] suggests that many CDSMs produce distributional vectors where structural information and vectors for individual words can be still intepreted.", "startOffset": 27, "endOffset": 49}, {"referenceID": 76, "context": "where AR and BR are two square matrices depending on the grammatical relation R which may be learned from data [Zanzotto et al. 2010; Guevara 2010].", "startOffset": 111, "endOffset": 147}, {"referenceID": 76, "context": "2 with a matrix AV N , that is: AV Nfr(cows eat animal extracts) \u2248 2eat In general, matrices derived for compositional distributional semantic models [Guevara 2010; Zanzotto et al. 2010] do not have this property but it is possible to obtain matrices with this property by applying thee Jonson-Linderstrauss Tranform [Johnson and Lindenstrauss 1984] or similar techniques as discussed also in [Zanzotto et al.", "startOffset": 150, "endOffset": 186}, {"referenceID": 75, "context": "2010] do not have this property but it is possible to obtain matrices with this property by applying thee Jonson-Linderstrauss Tranform [Johnson and Lindenstrauss 1984] or similar techniques as discussed also in [Zanzotto et al. 2015].", "startOffset": 212, "endOffset": 234}, {"referenceID": 13, "context": "These models have solid mathematical background linking Lambek pregroup theory, formal semantics and distributional semantics [Coecke et al. 2010].", "startOffset": 126, "endOffset": 146}, {"referenceID": 75, "context": "However, using the convolution conjecture [Zanzotto et al. 2015], it is possible to know whether subparts are contained in some final vectors obtained with these models.", "startOffset": 42, "endOffset": 64}, {"referenceID": 61, "context": "In fact, composition functions are trained on specific tasks and not on the possibility of reconstructing the structured input, unless in some rare cases [Socher et al. 2011].", "startOffset": 154, "endOffset": 174}, {"referenceID": 38, "context": "In this section, we revise some prominent neural network architectures that can be interpreted as models-that-compose: the recurrent neural networks [Krizhevsky et al. 2012; He et al. 2016; Vinyals et al. 2015a; Graves 2013] and the recursive neural networks [Socher et al.", "startOffset": 149, "endOffset": 224}, {"referenceID": 32, "context": "In this section, we revise some prominent neural network architectures that can be interpreted as models-that-compose: the recurrent neural networks [Krizhevsky et al. 2012; He et al. 2016; Vinyals et al. 2015a; Graves 2013] and the recursive neural networks [Socher et al.", "startOffset": 149, "endOffset": 224}, {"referenceID": 68, "context": "In this section, we revise some prominent neural network architectures that can be interpreted as models-that-compose: the recurrent neural networks [Krizhevsky et al. 2012; He et al. 2016; Vinyals et al. 2015a; Graves 2013] and the recursive neural networks [Socher et al.", "startOffset": 149, "endOffset": 224}, {"referenceID": 62, "context": "2015a; Graves 2013] and the recursive neural networks [Socher et al. 2012].", "startOffset": 54, "endOffset": 74}, {"referenceID": 38, "context": "At the moment the most powerful network architectures are convolutional neural networks [Krizhevsky et al. 2012; He et al. 2016] for vision related tasks and LSTM-type network for language related task [Vinyals et al.", "startOffset": 88, "endOffset": 128}, {"referenceID": 32, "context": "At the moment the most powerful network architectures are convolutional neural networks [Krizhevsky et al. 2012; He et al. 2016] for vision related tasks and LSTM-type network for language related task [Vinyals et al.", "startOffset": 88, "endOffset": 128}, {"referenceID": 68, "context": "2016] for vision related tasks and LSTM-type network for language related task [Vinyals et al. 2015a; Graves 2013].", "startOffset": 79, "endOffset": 114}, {"referenceID": 62, "context": "The last class of models-that-compose that we present is the class of recursive neural networks [Socher et al. 2012].", "startOffset": 96, "endOffset": 116}], "year": 2017, "abstractText": "Natural language and symbols are intimately correlated. Recent advances in machine learning (ML) and in natural language processing (NLP) seem to contradict the above intuition: symbols are fading away, erased by vectors or tensors called distributed and distributional representations. However, there is a strict link between distributed/distributional representations and symbols, being the first an approximation of the second. A clearer understanding of the strict link between distributed/distributional representations and symbols will certainly lead to radically new deep learning networks. In this paper we make a survey that aims to draw the link between symbolic representations and distributed/distributional representations. This is the right time to revitalize the area of interpreting how symbols are represented inside neural networks.", "creator": "LaTeX with hyperref package"}}}