{"id": "1702.01923", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2017", "title": "Comparative Study of CNN and RNN for Natural Language Processing", "abstract": "Deep neural networks (DNN) have revolutionized the field of natural language processing (NLP). Convolutional neural network (CNN) and recurrent neural network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting position-invariant features and RNN at modeling units in sequence. The state of the art on many NLP tasks often switches due to the battle between CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection.\n\n\n\nThe current approach of CNN and RNN, which was originally based on the Mapping of Functional Networks and Networking, is a much more thorough and comprehensive approach to network and network classification. However, this approach has been abandoned due to a fundamental change in the problem of network inference. The Mapping of Functional Networks (MNNN) model and RNN (RNN) models is currently used for analysis of multiple NLP tasks. A single MNNN-generated neural network of five NLP tasks can be found here.\nMapping of Neural Networks\nOne of the major problems with MNNN is that each network can only have a limited amount of NLP (i.e. the amount of neurons in the network, i.e. only a single layer of neurons, in the data structure, is used to predict the likelihood of a specific outcome), and the MNNN can not be considered a true network at all (since MNNNs are more important than any other model in the world). As such, MNNNs may not have very effective predictors in a given task, but they are very useful in understanding the information of the networks. In particular, MNNNs can be useful when the inputs of these NLP tasks are given, and for a long time, when inputs are required. In other words, MNNNs are quite limited in predicting which network to classify. However, MNNNs are quite expensive to train for certain types of task, so it is still difficult to estimate the value of MNNNs. The average cost of MNNNs was not much higher in the original MNNN model, and the cost of MNNNs was quite low in the original MNNN model. The cost of MNNNs was higher in the original MNNN model, but the cost of MNNNs was lower in the original model", "histories": [["v1", "Tue, 7 Feb 2017 08:33:35 GMT  (88kb,D)", "http://arxiv.org/abs/1702.01923v1", "7 pages, 11 figures"]], "COMMENTS": "7 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenpeng yin", "katharina kann", "mo yu", "hinrich sch\\\"utze"], "accepted": false, "id": "1702.01923"}, "pdf": {"name": "1702.01923.pdf", "metadata": {"source": "CRF", "title": "Comparative Study of CNN and RNN for Natural Language Processing", "authors": ["Wenpeng Yin", "Katharina Kann", "Mo Yu", "Hinrich Sch\u00fctze"], "emails": ["wenpeng@cis.lmu.de,", "kann@cis.lmu.de,", "yum@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "Natural language processing (NLP) has benefited greatly from the resurgence of deep neural networks (DNNs), due to their high performance with less need of engineered features. There are two main DNN architectures: convolutional neural network (CNN) (LeCun et al., 1998) and recurrent neural network (RNN) (Elman, 1990). Gating mechanisms have been developed to alleviate some limitations of the basic RNN, resulting in two prevailing RNN types: long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent unit (GRU) (Cho et al., 2014).\nGenerally speaking, CNNs are hierarchical and RNNs sequential architectures. How should we choose between them for processing language? Based on the characterization \u201chierarchical (CNN) vs. sequential (RNN)\u201d, it is tempting to choose a CNN for classification tasks like sentiment classification since sentiment is usually determined by some key phrases; and to choose RNNs for a se-\nquence modeling task like language modeling as it requires flexible modeling of context dependencies. But current NLP literature does not support such a clear conclusion. For example, RNNs perform well on document-level sentiment classification (Tang et al., 2015); and Dauphin et al. (2016) recently showed that gated CNNs outperform LSTMs on language modeling tasks, even though LSTMs had long been seen as better suited. In summary, there is no consensus on DNN selection for any particular NLP problem.\nThis work compares CNNs, GRUs and LSTMs systematically on a broad array of NLP tasks: sentiment/relation classification, textual entailment, answer selection, question-relation matching in Freebase, Freebase path query answering and partof-speech tagging.\nOur experiments support two key findings. (i) CNNs and RNNs provide complementary information for text classification tasks. Which architecture performs better depends on how important it is to semantically understand the whole sequence. (ii) Learning rate changes performance relatively smoothly, while changes to hidden size and batch size result in large fluctuations."}, {"heading": "2 Related Work", "text": "To our knowledge, there has been no systematic comparison of CNN and RNN on a large array of NLP tasks.\nVu et al. (2016) investigate CNN and basic RNN (i.e., no gating mechanisms) for relation classification. They report higher performance of CNN than RNN and give evidence that CNN and RNN provide complementary information: while the RNN computes a weighted combination of all words in the sentence, the CNN extracts the most informative ngrams for the relation and only considers their resulting activations.\nar X\niv :1\n70 2.\n01 92\n3v 1\n[ cs\n.C L\n] 7\nF eb\n2 01\n7\nBoth Wen et al. (2016) and Adel and Schu\u0308tze (2017) support CNN over GRU/LSTM for classification of long sentences. In addition, Yin et al. (2016) achieve better performance of attentionbased CNN than attention-based LSTM for answer selection. Dauphin et al. (2016) further argue that a fine-tuned gated CNN can also model longcontext dependency, getting new state-of-the-art in language modeling above all RNN competitors\nIn contrast, Arkhipenko et al. (2016) compare word2vec (Mikolov et al., 2013), CNN, GRU and LSTM in sentiment analysis of Russian tweets, and find GRU outperforms LSTM and CNN.\nIn empirical evaluations, Chung et al. (2014) and Jozefowicz et al. (2015) found there is no clear winner between GRU and LSTM. In many tasks, they yield comparable performance and tuning hyperparameters like layer size is often more important than picking the ideal architecture."}, {"heading": "3 Models", "text": "This section gives a brief introduction of CNN, GRU and LSTM."}, {"heading": "3.1 Convolutional Neural Network (CNN)", "text": "Input Layer Sequence x contains n entries. Each entry is represented by a d-dimensional dense vector; thus the input x is represented as a feature map of dimensionality d \u00d7 n. Figure 1(a) shows the input layer as the lower rectangle with multiple columns.\nConvolution Layer is used for representation learning from sliding w-grams. For an input se-\nquence with n entries: x1, x2, . . . , xn, let vector ci \u2208 Rwd be the concatenated embeddings of w entries xi\u2212w+1, . . . , xi where w is the filter width and 0 < i < s + w. Embeddings for xi, i < 1 or i > n, are zero padded. We then generate the representation pi \u2208 Rd for the w-gram xi\u2212w+1, . . . , xi using the convolution weights W \u2208 Rd\u00d7wd:\npi = tanh(W \u00b7 ci + b) (1)\nwhere bias b \u2208 Rd.\nMaxpooling All w-gram representations pi (i = 1 \u00b7 \u00b7 \u00b7 s + w \u2212 1) are used to generate the representation of input sequence x by maxpooling: xj = max(p1,j ,p2,j , \u00b7 \u00b7 \u00b7 ) (j = 1, \u00b7 \u00b7 \u00b7 , d)."}, {"heading": "3.2 Gated Recurrent Unit (GRU)", "text": "GRU, as shown in Figure 1(b), models text x as follows:\nz = \u03c3(xtU z + ht\u22121W z) (2) r = \u03c3(xtU r + ht\u22121W r) (3)\nst = tanh(xtU s + (ht\u22121 \u25e6 r)Ws) (4)\nht = (1\u2212 z) \u25e6 st + z \u25e6 ht\u22121 (5)\nxt \u2208 Rd represents the token in x at position t, ht \u2208 Rh is the hidden state at t, supposed to encode the history x1, \u00b7 \u00b7 \u00b7 , xt. z and r are two gates. All U \u2208 Rd\u00d7h,W \u2208 Rh\u00d7h are parameters."}, {"heading": "3.3 Long Short-Time Memory (LSTM)", "text": "LSTM is denoted in Figure 1(c). It models the word sequence x as follows:\nit = \u03c3(xtU i + ht\u22121W i + bi) (6) ft = \u03c3(xtU f + ht\u22121W f + bf ) (7)\not = \u03c3(xtU o + ht\u22121W o + bo) (8) qt = tanh(xtU q + ht\u22121W q + bq) (9) pt = ft \u2217 pt\u22121 + it \u2217 qt (10) ht = ot \u2217 tanh(pt) (11)\nLSTM has three gates: input gate it, forget gate ft and output gate ot. All gates are generated by a sigmoid function over the ensemble of input xt and the preceding hidden state ht\u22121. In order to generate the hidden state at current step t, it first generates a temporary result qt by a tanh nonlinearity over the ensemble of input xt and the preceding hidden state ht\u22121, then combines this temporary result qt with history pt\u22121 by input gate it and forget gate ft respectively to get an updated history pt, finally uses output gate ot over this updated history pt to get the final hidden state ht."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Tasks", "text": "Sentiment Classification (SentiC) on Stanford Sentiment Treebank (SST) (Socher et al., 2013). This dataset predicts the sentiment (positive or negative) of movie reviews. We use the given split of 6920 train, 872 dev and 1821 test sentences. As in (Kalchbrenner et al., 2014; Le and Mikolov, 2014), we treat labeled phrases that occur as subparts of training sentences as independent training instances. Measure: accuracy.\nRelation Classification (RC) on SemEval 2010 task 8 (Hendrickx et al., 2009). It consists of sentences which have been manually labeled with 19 relations (18 directed relations and Other), 8000 sentences in train and 2717 in test. As there is no dev set, we use 1500 training examples as dev, similar to Vu et al. (2016). Measure: F1.\nTextual Entailment (TE) on Stanford Natural Language Inference (SNLI) (Bowman et al., 2015). SNLI contains premise-hypothesis pairs, labeled with a relation (entailment, contradiction, neutral). After removing unlabeled pairs, we end up having 549,367 pairs for train, 9,842 for dev and 9,824 for test. Measure: accuracy.\nAnswer Selection (AS) on WikiQA (Yang et al., 2015), an open domain question-answer\ndataset. We use the subtask that assumes that there is at least one correct answer for a question. The corresponding dataset consists of 20,360 questioncandidate pairs in train, 1,130 in dev and 2,352 in test where we adopt the standard setup of only considering questions with correct answers in test. The task is to choose the correct answer(s) from some candidates for a question. Measures: MAP and MRR.\nQuestion Relation Match (QRM). We utilize WebQSP (Yih et al., 2016) dataset to create a large-scale relation detection task, benefitting from the availability of labeled semantic parses of questions. For each question, we (i) select the topic entity from the parse; (ii) select all the relations/relation chains (length \u2264 2) connecting to the topic entity; and (iii) set the relations/relationchains in the labeled parse as positive and all the others as negative. Following Yih et al. (2016) and Xu et al. (2016), we formulate this task as a sequence matching problem. Ranking-loss is used for training. Measure: accuracy.\nPath Query Answering (PQA) on the path query dataset released by Guu et al. (2015). It contains KB paths like eh, r0, r1, \u00b7 \u00b7 \u00b7 , rt, et, where head entity eh and relation sequence r0, r1, \u00b7 \u00b7 \u00b7 , rt are encoded to predict the tail entity et. There are 6,266,058/27,163/109,557 paths in train/dev/test, respectively. Measure: hit@10.\nPart-of-Speech Tagging on WSJ. We use the setup of (Blitzer et al., 2006; Petrov and McDonald, 2012): sections 2-21 are train, section 22 is dev and section 23 is test. Measure: accuracy.\nWe organize above tasks in four categories. (i) TextC. Text classification, including SentiC and RC. (ii) SemMatch including TE, AS and QRM. (iii) SeqOrder. Sequence order, i.e., PQA. (iv) ContextDep. Context dependency, i.e., POS tagging. By investigating these four categories, we aim to discover some basic principles involved in utilizing CNNs / RNNs."}, {"heading": "4.2 Experimental Setup", "text": "To fairly study the encoding capability of different basic DNNs, our experiments have the following design. (i) Always train from scratch, no extra knowledge, e.g., no pretrained word embeddings. (ii) Always train using a basic setup without complex tricks such as batch normalization. (iii) Search for optimal hyperparameters for each task and each model separately, so that all results are\nbased on optimal hyperparameters. (iv) Investigate the basic architecture and utilization of each model: CNN consists of a convolution layer and a max-pooling layer; GRU and LSTM model the input from left to right and always use the last hidden state as the final representation of the input. An exception is for POS tagging, we also report bi-directional RNNs as this can make sure each word\u2019s representation can encode the word\u2019s context of both sides, like the CNN does.\nHyperparameters are tuned on dev: hidden size, minibatch size, learning rate, maximal sentence length, filter size (for CNN only) and margin in ranking loss in AS, QRM and PQA tasks."}, {"heading": "4.3 Results & Analysis", "text": "Table 1 shows experimental results for all tasks and models and corresponding hyperparameters. For TextC, GRU performs best on SentiC and comparably with CNN in RC. For SemMatch, CNN performs best on AS and QRM while GRU (and also LSTM) outperforms CNN on TE. For SeqOrder (PQA), both GRU and LSTM outperform CNN. For ContextDep (POS tagging), CNN outperforms one-directional RNNs, but lags behind bi-directional RNNs.\nThe results for SeqOrder and ContextDep are as expected: RNNs are well suited to encode order information (for PQA) and long-range context dependency (for POS tagging). But for the other\ntwo categories, TextC and SemMatch, some unexpected observations appear. CNNs are considered good at extracting local and position-invariant features and therefore should perform well on TextC; but in our experiments they are outperformed by RNNs, especially in SentiC. How can this be explained? RNNs can encode the structuredependent semantics of the whole input, but how likely is this helpful for TextC tasks that mostly depend on a few local regions? To investigate the unexpected observations, we do some error analysis on SentiC.\nQualitative analysis Table 2 shows examples (1) \u2013 (4) in which CNN predicts correctly while GRU predicts falsely or vice versa. We find that GRU is better when sentiment is determined by the entire sentence or a long-range semantic de-\npendency \u2013 rather than some local key-phrases \u2013 is involved. Example (1) contains the phrases \u201cwon\u2019t\u201d and \u201cmiss\u201d that usually appear with negative sentiment, but the whole sentence describes a positive sentiment; thus, an architecture like GRU is needed that handles long sequences correctly. On the other hand, modeling the whole sentence sometimes is a burden \u2013 neglecting the key parts. The GRU encodes the entire word sequence of the long example (3), making it hard for the negative keyphrase \u201cloosens\u201d to play a main role in the final representation. The first part of example (4) seems positive while the second part seems negative. As GRU chooses the last hidden state to represent the sentence, this might result in the wrong prediction.\nStudying acc vs sentence length can also support this. Figure 2 (left) shows sentence lengths in SST are mostly short in train while close to nor-\nmal distribution around 20 in dev and test. Figure 2 (right) depicts the accuracies w.r.t length ranges. We found that GRU and CNN are comparable when lengths are small, e.g., <10, then GRU gets increasing advantage over CNN when meet longer sentences. Error analysis shows that long sentences in SST mostly consist of clauses of inverse semantic such as \u201cthis version is not classic like its predecessor, but its pleasures are still plentiful\u201d. This kind of clause often include a local strong indicator for one sentiment polarity, like \u201cis not\u201d above, but the successful classification relies on the comprehension of the whole clause.\nHence, which DNN type performs better in text classification task depends on how often the comprehension of global/long-range semantics is required.\nThis can also explain the phenomenon in SemMatch \u2013 GRU/LSTM surpass CNN in TE while CNN dominates in AS, as textual entailment relies on the comprehension of the whole sentence (Bowman et al., 2015), question-answer in AS instead can be effectively identified by key-phrase matching (Yin et al., 2016).\nSensitivity to hyperparameters We next check how stable the performance of CNN and GRU are when hyperparameter values are varied. Figure 3 shows the performance of CNN, GRU and LSTM for different learning rates, hidden sizes and batch sizes. All models are relativly smooth with respect to learning rate changes. In contrast, variation in hidden size and batch size cause large oscillations. Nevertheless, we still can observe that CNN curve is mostly below the curves of GRU and LSTM in SentiC task, contrarily located at the higher place in AS task."}, {"heading": "5 Conclusions", "text": "This work compared the three most widely used DNNs \u2013 CNN, GRU and LSTM \u2013 in representative sample of NLP tasks. We found that RNNs perform well and robust in a broad range of tasks except when the task is essentially a keyphrase recognition task as in some sentiment detection and question-answer matching settings. In addition, hidden size and batch size can make DNN performance vary dramatically. This suggests that optimization of these two parameters is crucial to good performance of both CNNs and RNNs."}], "references": [{"title": "Exploring different dimensions of attention for uncertainty detection", "author": ["Heike Adel", "Hinrich Sch\u00fctze."], "venue": "Proceedings of EACL.", "citeRegEx": "Adel and Sch\u00fctze.,? 2017", "shortCiteRegEx": "Adel and Sch\u00fctze.", "year": 2017}, {"title": "Comparison of neural network architectures for sentiment analysis of russian tweets", "author": ["K. Arkhipenko", "I. Kozlov", "J. Trofimovich", "K. Skorniakov", "A. Gomzin", "D. Turdakov."], "venue": "Proceedings of \u201cDialogue 2016\u201d.", "citeRegEx": "Arkhipenko et al\\.,? 2016", "shortCiteRegEx": "Arkhipenko et al\\.", "year": 2016}, {"title": "Domain adaptation with structural correspondence learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira."], "venue": "Proceedings of EMNLP. pages 120\u2013128.", "citeRegEx": "Blitzer et al\\.,? 2006", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning."], "venue": "Proceedings of EMNLP. pages 632\u2013642.", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation .", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555 .", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Language modeling with gated convolutional networks", "author": ["Yann N Dauphin", "Angela Fan", "Michael Auli", "David Grangier."], "venue": "arXiv preprint arXiv:1612.08083 .", "citeRegEx": "Dauphin et al\\.,? 2016", "shortCiteRegEx": "Dauphin et al\\.", "year": 2016}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman."], "venue": "Cognitive Science 14(2):179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Traversing knowledge graphs in vector space", "author": ["Kelvin Guu", "John Miller", "Percy Liang."], "venue": "Proceedings of EMNLP. pages 318\u2013327.", "citeRegEx": "Guu et al\\.,? 2015", "shortCiteRegEx": "Guu et al\\.", "year": 2015}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations", "author": ["Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2009}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings of ICML. pages 2342\u20132350.", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of ACL.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "Proceedings of ICML. pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner."], "venue": "Proceedings of the IEEE 86(11):2278\u20132324.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "Proceedings of NIPS. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Overview of the 2012 shared task on parsing the web", "author": ["Slav Petrov", "Ryan McDonald."], "venue": "Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL). volume 59.", "citeRegEx": "Petrov and McDonald.,? 2012", "shortCiteRegEx": "Petrov and McDonald.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings EMNLP. volume 1631, page", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu."], "venue": "Proceedings of EMNLP. pages 1422\u20131432.", "citeRegEx": "Tang et al\\.,? 2015", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Combining recurrent and convolutional neural networks for relation classification", "author": ["Ngoc Thang Vu", "Heike Adel", "Pankaj Gupta", "Hinrich Sch\u00fctze."], "venue": "Proceedings of NAACL HLT . pages 534\u2013539.", "citeRegEx": "Vu et al\\.,? 2016", "shortCiteRegEx": "Vu et al\\.", "year": 2016}, {"title": "Learning text representation using recurrent convolutional neural network with highway layers", "author": ["Ying Wen", "Weinan Zhang", "Rui Luo", "Jun Wang."], "venue": "SIGIR Workshop on Neural Information Retrieval .", "citeRegEx": "Wen et al\\.,? 2016", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Enhancing freebase question answering using textual evidence", "author": ["Kun Xu", "Yansong Feng", "Siva Reddy", "Songfang Huang", "Dongyan Zhao."], "venue": "CoRR abs/1603.00957.", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yi Yang", "Wen-tau Yih", "Christopher Meek."], "venue": "Proceedings of EMNLP. pages 2013\u20132018.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "The value of semantic parse labeling for knowledge base question answering", "author": ["Wen-tau Yih", "Matthew Richardson", "Chris Meek", "MingWei Chang", "Jina Suh."], "venue": "Proceedings of ACL. pages 201\u2013206.", "citeRegEx": "Yih et al\\.,? 2016", "shortCiteRegEx": "Yih et al\\.", "year": 2016}, {"title": "ABCNN: attention-based convolutional neural network for modeling sentence pairs", "author": ["Wenpeng Yin", "Hinrich Sch\u00fctze", "Bing Xiang", "Bowen Zhou."], "venue": "TACL 4:259\u2013272.", "citeRegEx": "Yin et al\\.,? 2016", "shortCiteRegEx": "Yin et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "There are two main DNN architectures: convolutional neural network (CNN) (LeCun et al., 1998) and recurrent neural network (RNN) (Elman, 1990).", "startOffset": 73, "endOffset": 93}, {"referenceID": 7, "context": ", 1998) and recurrent neural network (RNN) (Elman, 1990).", "startOffset": 43, "endOffset": 56}, {"referenceID": 10, "context": "Gating mechanisms have been developed to alleviate some limitations of the basic RNN, resulting in two prevailing RNN types: long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent unit (GRU) (Cho et al.", "startOffset": 155, "endOffset": 189}, {"referenceID": 4, "context": "Gating mechanisms have been developed to alleviate some limitations of the basic RNN, resulting in two prevailing RNN types: long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent unit (GRU) (Cho et al., 2014).", "startOffset": 221, "endOffset": 239}, {"referenceID": 18, "context": "For example, RNNs perform well on document-level sentiment classification (Tang et al., 2015); and Dauphin et al.", "startOffset": 74, "endOffset": 93}, {"referenceID": 4, "context": "Gating mechanisms have been developed to alleviate some limitations of the basic RNN, resulting in two prevailing RNN types: long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent unit (GRU) (Cho et al., 2014). Generally speaking, CNNs are hierarchical and RNNs sequential architectures. How should we choose between them for processing language? Based on the characterization \u201chierarchical (CNN) vs. sequential (RNN)\u201d, it is tempting to choose a CNN for classification tasks like sentiment classification since sentiment is usually determined by some key phrases; and to choose RNNs for a sequence modeling task like language modeling as it requires flexible modeling of context dependencies. But current NLP literature does not support such a clear conclusion. For example, RNNs perform well on document-level sentiment classification (Tang et al., 2015); and Dauphin et al. (2016) recently showed that gated CNNs outperform LSTMs on language modeling tasks, even though LSTMs had long been seen as better suited.", "startOffset": 222, "endOffset": 914}, {"referenceID": 19, "context": "Vu et al. (2016) investigate CNN and basic RNN (i.", "startOffset": 0, "endOffset": 17}, {"referenceID": 15, "context": "(2016) compare word2vec (Mikolov et al., 2013), CNN, GRU and LSTM in sentiment analysis of Russian tweets, and find GRU outperforms LSTM and CNN.", "startOffset": 24, "endOffset": 46}, {"referenceID": 14, "context": "Both Wen et al. (2016) and Adel and Sch\u00fctze (2017) support CNN over GRU/LSTM for classification of long sentences.", "startOffset": 5, "endOffset": 23}, {"referenceID": 0, "context": "(2016) and Adel and Sch\u00fctze (2017) support CNN over GRU/LSTM for classification of long sentences.", "startOffset": 11, "endOffset": 35}, {"referenceID": 0, "context": "(2016) and Adel and Sch\u00fctze (2017) support CNN over GRU/LSTM for classification of long sentences. In addition, Yin et al. (2016) achieve better performance of attentionbased CNN than attention-based LSTM for answer selection.", "startOffset": 11, "endOffset": 130}, {"referenceID": 0, "context": "(2016) and Adel and Sch\u00fctze (2017) support CNN over GRU/LSTM for classification of long sentences. In addition, Yin et al. (2016) achieve better performance of attentionbased CNN than attention-based LSTM for answer selection. Dauphin et al. (2016) further argue that a fine-tuned gated CNN can also model longcontext dependency, getting new state-of-the-art in language modeling above all RNN competitors In contrast, Arkhipenko et al.", "startOffset": 11, "endOffset": 249}, {"referenceID": 0, "context": "(2016) and Adel and Sch\u00fctze (2017) support CNN over GRU/LSTM for classification of long sentences. In addition, Yin et al. (2016) achieve better performance of attentionbased CNN than attention-based LSTM for answer selection. Dauphin et al. (2016) further argue that a fine-tuned gated CNN can also model longcontext dependency, getting new state-of-the-art in language modeling above all RNN competitors In contrast, Arkhipenko et al. (2016) compare word2vec (Mikolov et al.", "startOffset": 11, "endOffset": 444}, {"referenceID": 0, "context": "(2016) and Adel and Sch\u00fctze (2017) support CNN over GRU/LSTM for classification of long sentences. In addition, Yin et al. (2016) achieve better performance of attentionbased CNN than attention-based LSTM for answer selection. Dauphin et al. (2016) further argue that a fine-tuned gated CNN can also model longcontext dependency, getting new state-of-the-art in language modeling above all RNN competitors In contrast, Arkhipenko et al. (2016) compare word2vec (Mikolov et al., 2013), CNN, GRU and LSTM in sentiment analysis of Russian tweets, and find GRU outperforms LSTM and CNN. In empirical evaluations, Chung et al. (2014) and Jozefowicz et al.", "startOffset": 11, "endOffset": 629}, {"referenceID": 0, "context": "(2016) and Adel and Sch\u00fctze (2017) support CNN over GRU/LSTM for classification of long sentences. In addition, Yin et al. (2016) achieve better performance of attentionbased CNN than attention-based LSTM for answer selection. Dauphin et al. (2016) further argue that a fine-tuned gated CNN can also model longcontext dependency, getting new state-of-the-art in language modeling above all RNN competitors In contrast, Arkhipenko et al. (2016) compare word2vec (Mikolov et al., 2013), CNN, GRU and LSTM in sentiment analysis of Russian tweets, and find GRU outperforms LSTM and CNN. In empirical evaluations, Chung et al. (2014) and Jozefowicz et al. (2015) found there is no clear winner between GRU and LSTM.", "startOffset": 11, "endOffset": 658}, {"referenceID": 17, "context": "1 Tasks Sentiment Classification (SentiC) on Stanford Sentiment Treebank (SST) (Socher et al., 2013).", "startOffset": 79, "endOffset": 100}, {"referenceID": 12, "context": "As in (Kalchbrenner et al., 2014; Le and Mikolov, 2014), we treat labeled phrases that occur as subparts of training sentences as independent training instances.", "startOffset": 6, "endOffset": 55}, {"referenceID": 13, "context": "As in (Kalchbrenner et al., 2014; Le and Mikolov, 2014), we treat labeled phrases that occur as subparts of training sentences as independent training instances.", "startOffset": 6, "endOffset": 55}, {"referenceID": 9, "context": "Relation Classification (RC) on SemEval 2010 task 8 (Hendrickx et al., 2009).", "startOffset": 52, "endOffset": 76}, {"referenceID": 3, "context": "Textual Entailment (TE) on Stanford Natural Language Inference (SNLI) (Bowman et al., 2015).", "startOffset": 70, "endOffset": 91}, {"referenceID": 22, "context": "Answer Selection (AS) on WikiQA (Yang et al., 2015), an open domain question-answer dataset.", "startOffset": 32, "endOffset": 51}, {"referenceID": 23, "context": "We utilize WebQSP (Yih et al., 2016) dataset to create a large-scale relation detection task, benefitting from the availability of labeled semantic parses of questions.", "startOffset": 18, "endOffset": 36}, {"referenceID": 2, "context": "We use the setup of (Blitzer et al., 2006; Petrov and McDonald, 2012): sections 2-21 are train, section 22 is dev and section 23 is test.", "startOffset": 20, "endOffset": 69}, {"referenceID": 16, "context": "We use the setup of (Blitzer et al., 2006; Petrov and McDonald, 2012): sections 2-21 are train, section 22 is dev and section 23 is test.", "startOffset": 20, "endOffset": 69}, {"referenceID": 6, "context": "Relation Classification (RC) on SemEval 2010 task 8 (Hendrickx et al., 2009). It consists of sentences which have been manually labeled with 19 relations (18 directed relations and Other), 8000 sentences in train and 2717 in test. As there is no dev set, we use 1500 training examples as dev, similar to Vu et al. (2016). Measure: F1.", "startOffset": 53, "endOffset": 321}, {"referenceID": 2, "context": "Textual Entailment (TE) on Stanford Natural Language Inference (SNLI) (Bowman et al., 2015). SNLI contains premise-hypothesis pairs, labeled with a relation (entailment, contradiction, neutral). After removing unlabeled pairs, we end up having 549,367 pairs for train, 9,842 for dev and 9,824 for test. Measure: accuracy. Answer Selection (AS) on WikiQA (Yang et al., 2015), an open domain question-answer dataset. We use the subtask that assumes that there is at least one correct answer for a question. The corresponding dataset consists of 20,360 questioncandidate pairs in train, 1,130 in dev and 2,352 in test where we adopt the standard setup of only considering questions with correct answers in test. The task is to choose the correct answer(s) from some candidates for a question. Measures: MAP and MRR. Question Relation Match (QRM). We utilize WebQSP (Yih et al., 2016) dataset to create a large-scale relation detection task, benefitting from the availability of labeled semantic parses of questions. For each question, we (i) select the topic entity from the parse; (ii) select all the relations/relation chains (length \u2264 2) connecting to the topic entity; and (iii) set the relations/relationchains in the labeled parse as positive and all the others as negative. Following Yih et al. (2016) and Xu et al.", "startOffset": 71, "endOffset": 1306}, {"referenceID": 2, "context": "Textual Entailment (TE) on Stanford Natural Language Inference (SNLI) (Bowman et al., 2015). SNLI contains premise-hypothesis pairs, labeled with a relation (entailment, contradiction, neutral). After removing unlabeled pairs, we end up having 549,367 pairs for train, 9,842 for dev and 9,824 for test. Measure: accuracy. Answer Selection (AS) on WikiQA (Yang et al., 2015), an open domain question-answer dataset. We use the subtask that assumes that there is at least one correct answer for a question. The corresponding dataset consists of 20,360 questioncandidate pairs in train, 1,130 in dev and 2,352 in test where we adopt the standard setup of only considering questions with correct answers in test. The task is to choose the correct answer(s) from some candidates for a question. Measures: MAP and MRR. Question Relation Match (QRM). We utilize WebQSP (Yih et al., 2016) dataset to create a large-scale relation detection task, benefitting from the availability of labeled semantic parses of questions. For each question, we (i) select the topic entity from the parse; (ii) select all the relations/relation chains (length \u2264 2) connecting to the topic entity; and (iii) set the relations/relationchains in the labeled parse as positive and all the others as negative. Following Yih et al. (2016) and Xu et al. (2016), we formulate this task as a sequence matching problem.", "startOffset": 71, "endOffset": 1327}, {"referenceID": 2, "context": "Textual Entailment (TE) on Stanford Natural Language Inference (SNLI) (Bowman et al., 2015). SNLI contains premise-hypothesis pairs, labeled with a relation (entailment, contradiction, neutral). After removing unlabeled pairs, we end up having 549,367 pairs for train, 9,842 for dev and 9,824 for test. Measure: accuracy. Answer Selection (AS) on WikiQA (Yang et al., 2015), an open domain question-answer dataset. We use the subtask that assumes that there is at least one correct answer for a question. The corresponding dataset consists of 20,360 questioncandidate pairs in train, 1,130 in dev and 2,352 in test where we adopt the standard setup of only considering questions with correct answers in test. The task is to choose the correct answer(s) from some candidates for a question. Measures: MAP and MRR. Question Relation Match (QRM). We utilize WebQSP (Yih et al., 2016) dataset to create a large-scale relation detection task, benefitting from the availability of labeled semantic parses of questions. For each question, we (i) select the topic entity from the parse; (ii) select all the relations/relation chains (length \u2264 2) connecting to the topic entity; and (iii) set the relations/relationchains in the labeled parse as positive and all the others as negative. Following Yih et al. (2016) and Xu et al. (2016), we formulate this task as a sequence matching problem. Ranking-loss is used for training. Measure: accuracy. Path Query Answering (PQA) on the path query dataset released by Guu et al. (2015). It contains KB paths like eh, r0, r1, \u00b7 \u00b7 \u00b7 , rt, et, where head entity eh and relation sequence r0, r1, \u00b7 \u00b7 \u00b7 , rt are encoded to predict the tail entity et.", "startOffset": 71, "endOffset": 1520}, {"referenceID": 3, "context": "This can also explain the phenomenon in SemMatch \u2013 GRU/LSTM surpass CNN in TE while CNN dominates in AS, as textual entailment relies on the comprehension of the whole sentence (Bowman et al., 2015), question-answer in AS instead can be effectively identified by key-phrase matching (Yin et al.", "startOffset": 177, "endOffset": 198}, {"referenceID": 24, "context": ", 2015), question-answer in AS instead can be effectively identified by key-phrase matching (Yin et al., 2016).", "startOffset": 92, "endOffset": 110}], "year": 2017, "abstractText": "Deep neural networks (DNNs) have revolutionized the field of natural language processing (NLP). Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting positioninvariant features and RNN at modeling units in sequence. The state-of-the-art on many NLP tasks often switches due to the battle of CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection.", "creator": "LaTeX with hyperref package"}}}