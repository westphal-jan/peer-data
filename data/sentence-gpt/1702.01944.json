{"id": "1702.01944", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2017", "title": "EliXa: A Modular and Flexible ABSA Platform", "abstract": "This paper presents a supervised Aspect Based Sentiment Analysis (ABSA) system. Our aim is to develop a modular platform which allows to easily conduct experiments by replacing the modules or adding new features. We obtain the best result in the Opinion Target Extraction (OTE) task (slot 2) using an off-the-shelf sequence labeler. The target polarity classification (slot 3) is addressed by means of a multiclass SVM algorithm which includes lexical based features such as the polarity values obtained from domain and open polarity lexicons. The system obtains accuracies of 0.70 and 0.73 for the restaurant and laptop domain respectively, and performs second best in the out-of-domain hotel, achieving an accuracy of 0.80. The algorithms are used in the design of an algorithm which is a self-producible model for performing a restaurant and laptop domain. The algorithm is written using the algorithm by the researchers in order to achieve an overall result. The algorithm achieves a accuracy of 0.75 and 0.78. The algorithm then performs a test with the most precise algorithm. The algorithm is written using the algorithm with the highest accuracy, as well as using the algorithm for the first time in the future. The algorithm then performs an out-of-house test with the least optimal result in the out-of-house and laptop domain. In this case, the algorithm receives the lowest and most accurate results in the out-of-house, laptop and laptop domain. The algorithm then performs a self-producible model for performing a restaurant and laptop domain, achieving an accuracy of 0.85. The algorithm then performs a test with the most precise algorithm. The algorithm then performs a test with the highest accuracy, as well as using the algorithm for the first time in the future. The algorithm then performs a test with the most precise algorithm. The algorithm then performs a test with the most precise algorithm. The algorithm then performs a test with the highest accuracy, as well as using the algorithm for the first time in the future. The algorithm then performs a test with the most precise algorithm. The algorithm then performs a test with the highest accuracy, as well as using the algorithm for the first time in the future. The algorithm then performs a test with the highest accuracy, as well as using the algorithm for the first time in the future. The algorithm then performs a test with the highest accuracy, as well as using the algorithm for the first time in the future. The algorithm then performs a test", "histories": [["v1", "Tue, 7 Feb 2017 10:18:07 GMT  (19kb)", "http://arxiv.org/abs/1702.01944v1", "5 pages, conference"]], "COMMENTS": "5 pages, conference", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["i\\~naki san vicente", "xabier saralegi", "rodrigo agerri"], "accepted": false, "id": "1702.01944"}, "pdf": {"name": "1702.01944.pdf", "metadata": {"source": "CRF", "title": "EliXa: A modular and flexible ABSA platform", "authors": ["I\u00f1aki San Vicente", "Xabier Saralegi", "Rodrigo Agerri"], "emails": ["i.sanvicente@elhuyar.com", "x.saralegi@elhuyar.com", "rodrigo.agerri@ehu.eus"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n01 94\n4v 1\n[ cs\n.C L\n] 7\nF eb\nThis paper presents a supervised Aspect Based Sentiment Analysis (ABSA) system. Our aim is to develop a modular platform which allows to easily conduct experiments by replacing the modules or adding new features. We obtain the best result in the Opinion Target Extraction (OTE) task (slot 2) using an off-the-shelf sequence labeler. The target polarity classification (slot 3) is addressed by means of a multiclass SVM algorithm which includes lexical based features such as the polarity values obtained from domain and open polarity lexicons. The system obtains accuracies of 0.70 and 0.73 for the restaurant and laptop domain respectively, and performs second best in the out-of-domain hotel, achieving an accuracy of 0.80."}, {"heading": "1 Introduction", "text": "Nowadays Sentiment Analysis is proving very useful for tasks such as decision making and market analysis. The ever increasing interest is also shown in the number of related shared tasks organized: TASS (Villena-Roma\u0301n et al., 2012; Villena-Roma\u0301n et al., 2014), SemEval (Nakov et al., 2013; Pontiki et al., 2014; Rosenthal et al., 2014), or the SemSA Challenge at ESWC20141. Research has also been evolving towards specific opinion elements such as entities or properties of a certain opinion target, which is also known as ABSA. The Semeval\n1http://challenges.2014.eswcconferences.org/index.php/SemSA\n2015 ABSA shared task aims at covering the most common problems in an ABSA task: detecting the specific topics an opinion refers to (slot1); extracting the opinion targets (slot2), combining the topic and target identification (slot1&2) and, finally, computing the polarity of the identified word/targets (slot3). Participants were allowed to send one constrained (no external resources allowed) and one unconstrained run for each subtask. We participated in the slot2 and slot3 subtasks.\nOur main is to develop an ABSA system to be used in the future for further experimentation. Thus, rather than focusing on tuning the different modules our goal is to develop a platform to facilitate future experimentation. The EliXa system consists of three independent supervised modules based on the IXA pipes tools (Agerri et al., 2014) and Weka (Hall et al., 2009). Next section describes the external resources used in the unconstrained systems. Sections 3 and 4 describe the systems developed for each subtask and briefly discuss the obtained results."}, {"heading": "2 External Resources", "text": "Several polarity Lexicons and various corpora were used for the unconstrained versions of our systems. To facilitate reproducibility of results, every resource listed here is publicly available."}, {"heading": "2.1 Corpora", "text": "For the restaurant domain we used the Yelp Dataset Challenge dataset2. Following (Kiritchenko et al., 2014), we manually filtered\n2http://www.yelp.com/dataset challenge\nout categories not corresponding to food related businesses (173 out of 720 were finally selected). A total of 997,721 reviews (117.1M tokens) comprise what we henceforth call the Yelp food corpus (CY elp).\nFor the laptop domain we leveraged a corpus composed of Amazon reviews of electronic devices (Jo and Oh, 2011). Although only 17,53% of the reviews belong to laptop products, early experiments showed the advantage of using the full corpus for both slot 2 and slot 3 subtasks. The Amazon electronics corpus (CAmazon) consists of 24,259 reviews (4.4M tokens). Finally, the English Wikipedia was also used to induce word clusters using word2vec (Mikolov et al., 2013)."}, {"heading": "2.2 Polarity Lexicons", "text": "We generated two types of polarity lexicons to represent polarity in the slot3 subtasks: general purpose and domain specific polarity lexicons.\nA general purpose polarity lexicon Lgen was built by combining four well known polarity lexicons: SentiWordnet SWN (Baccianella et al., 2010), General Inquirer GI (Stone et al., 1966), Opinion Finder OF (Wilson et al., 2005) and Liu\u2019s sentiment lexicon Liu (Hu and Liu, 2004). When a lemma occurs in several lexicons, its polarity is solved according to the following priority order: Liu > OF > GI > SWN . The order was set based on the results of (San Vicente et al., 2014). All polarity weights were normalized to a [\u22121, 1] interval. Polarity categories were mapped to weights for GI (neg+\u2192\u22120.8; neg\u2192-0.6; neg\u2212\u2192-0.2; pos\u2212\u21920.2; pos\u21920.6; pos+\u21920.8), Liu and OF (neg\u2192-0.7; pos\u21920.7 for both). In addition, a restricted lexicon Lgenres including only the strongest polarity words was derived from Lgen by applying a threshold of \u00b10.6.\nDomain specific polarity lexicons LY elp and\nLAmazon were automatically extracted from CY elp and CAmazon reviews corpora. Reviews are rated in a [1..5] interval, being 1 the most negative and 5 the most positive. Using the Log-likelihood ratio (LLR) (Dunning, 1993) we obtained the ranking of the words which occur more with negative and positive reviews respectively. We considered reviews with 1 and 2 rating as negative and those with 4 and 5 ratings as positive. LLR scores were normalized to a [\u22121, 1] interval and included in LY elp and LAmazon lexicons as polarity weights."}, {"heading": "3 Slot2 Subtask: Opinion Target Extraction", "text": "The Opinion Target Extraction task (OTE) is addressed as a sequence labeling problem. We use the ixa-pipe-nerc Named Entity Recognition system3 (Agerri et al., 2014) off-the-shelf to train our OTE models; the system learns supervised models via the Perceptron algorithm as described by (Collins, 2002). ixa-pipe-nerc uses the Apache OpenNLP project implementation of the Perceptron algorithm4 customized with its own features. Specifically, ixa-pipe-nerc implements basic nonlinguistic local features and on top of those a combination of word class representation features partially inspired by (Turian et al., 2010). The word representation features use large amounts of unlabeled data. The result is a quite simple but competitive system which obtains the best constrained and unconstrained results and the first and third best overall results.\nThe local features implemented are: current token and token shape (digits, lowercase, punctuation, etc.) in a 2 range window, previous prediction, beginning of sentence, 4 characters in prefix and suffix, bigrams and trigrams (token and shape). On top of them we induce three types of word representations:\n\u2022 Brown (Brown et al., 1992) clusters, taking the 4th, 8th, 12th and 20th node in the path. We induced 1000 clusters on the Yelp reviews dataset described in section 2.1 using the tool implemented by Liang5.\n3https://github.com/ixa-ehu/ixa-pipe-nerc 4http://opennlp.apache.org/ 5https://github.com/percyliang/brown-cluster\n\u2022 Clark (Clark, 2003) clusters, using the standard configuration to induce 200 clusters on the Yelp reviews dataset and 100 clusters on the food portion of the Yelp reviews dataset.\n\u2022 Word2vec (Mikolov et al., 2013) clusters, based on K-means applied over the extracted word vectors using the skip-gram algorithm6; 400 clusters were induced using the Wikipedia.\nThe implementation of the clustering features looks for the cluster class of the incoming token in one or more of the clustering lexicons induced following the three methods listed above. If found, then we add the class as a feature. The Brown clusters only apply to the token related features, which are duplicated. We chose the best combination of features using 5-fold cross validation, obtaining 73.03 F1 score with local features (e.g. constrained mode) and 77.12 adding the word clustering features, namely, in unconstrained mode. These two configurations were used to process the test set in this task. Table 2 lists the official results for the first 4 systems in the task.\nThe results show that leveraging unlabeled text is helpful in the OTE task, obtaining an increase of 7 points in recall. It is also worth mentioning that our constrained system (using non-linguistic local features) performs very closely to the second best overall system by the NLANGP team (unconstrained). Finally, we would like to point out to the overall low results in this task (for example, compared to the 2014 edition), due to the very small and difficult training set (e.g., containing many short samples such as \u201cTasty Dog!\u201d) which made it extremely hard to learn good models for this task. The OTE models will be made freely available in the ixa-pipe-nerc website in time for SemEval 2015.\n6https://code.google.com/p/word2vec/"}, {"heading": "4 Slot3 Subtask: Sentiment Polarity", "text": "The EliXa system implements a single multiclass SVM classifier. We use the SMO implementation provided by the Weka library (Hall et al., 2009). All the classifiers built over the training data were evaluated via 10-fold cross validation. The complexity parameter was optimized as (C = 1.0). Many configurations were tested in this experiments, but in the following we only will describe the final setting."}, {"heading": "4.1 Baseline", "text": "The very first features we introduced in our classifier were token ngrams. Initial experiments showed that lemma ngrams (lgrams) performed better than raw form ngrams. One feature per lgram is added to the vector representation, and lemma frequency is stored. With respect to the ngram size used, we tested up to 4-gram features and improvement was achieved in laptop domain but only when not combined with other features."}, {"heading": "4.2 PoS", "text": "PoS tag and lemma information, obtained using the IXA pipes tools (Agerri et al., 2014), were also included as features. One feature per PoS tag was added again storing the number of occurrences of a tag in the sentence. These features slightly improve over the baseline only in the restaurant domain."}, {"heading": "4.3 Window", "text": "Given that a sentence may contain multiple opinions, we define a window span around a given opinion target (5 words before and 5 words after). When the target of an opinion is null the whole sentence is taken as span. Only the restaurant and hotel domains contained gold target annotations so we did not use this feature in the laptop domain."}, {"heading": "4.4 Polarity Lexicons", "text": "The positive and negative scores we extracted as features from both general purpose and domain specific lexicons. Both scores are calculated as the sum of every positive/negative score in the corresponding lexicon divided by the number of words in the sentence. Features obtained from the general lexicons provide a slight improvement. Lgenres is better for restaurant domain, while Lgen is better for laptops.\nDomain specific lexicons LAmazon and LY elp also help as shown by tables 3 and 4."}, {"heading": "4.5 Word Clusters", "text": "Word2vec clustering features combine best with the rest as shown by table 3. These features only were useful for the restaurant domain, perhaps due to the small size of the laptops domain data."}, {"heading": "4.6 Feature combinations", "text": "Every feature, when used in isolation, only marginally improves the baseline. Some of them, such as the E&A features (using the gold information from the slot1 subtask) for the laptop domain, only help when combined with others. Best performance is achieved when several features are combined. As shown by tables 4 and 5, improvement over the baseline ranges between 2,8% and 1,9% in the laptop and restaurant domains respectively."}, {"heading": "4.7 Results", "text": "Table 5 shows the result achieved by our sentiment polarity classifier. Although for both restaurant and laptops domains we obtain results over the baseline both performance are modest.\nIn contrast, for the out of domain track, which was evaluated on hotel reviews our system obtains the third highest score. Because of the similarity of the domains, we straightforwardly applied our restaurant domain models. The good results of the con-\nstrained system could mean that the feature combination used may be robust across domains. With respect to the unconstrained system, we suspect that such a good performance is achieved due to the fact that word cluster information was very adequate for the hotel domain, because Cyelp contains a 10.55% of hotel reviews."}, {"heading": "5 Conclusions", "text": "We have presented a modular and supervised ABSA platform developed to facilitate future experimentation in the field. We submitted runs corresponding to the slot2 and slot3 subtasks, obtaining competitive results. In particular, we obtained the best results in slot2 (OTE) and for slot3 we obtain 3rd best result in the out-of-domain track, which is nice for a supervised system. Finally, a system for topic detection (slot1) is currently under development."}, {"heading": "6 Acknowledgments", "text": "This work has been supported by the following projects: ADi project (Etortek grant No. IE-14-382), NewsReader (FP7-ICT 2011-8-316404), SKaTer (TIN2012-38584-C06-02) and Tacardi (TIN201238523-C02-01)."}], "references": [{"title": "Ixa pipeline: Efficient and ready to use multilingual nlp tools", "author": ["Josu Bermudez", "German Rigau"], "venue": "In Proceedings of the 9th Language Resources and Evaluation Conference", "citeRegEx": "Agerri et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agerri et al\\.", "year": 2014}, {"title": "SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining", "author": ["A. Esuli", "F. Sebastiani"], "venue": "In Seventh conference on International Language Resources and Evaluation (LREC-", "citeRegEx": "Baccianella et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baccianella et al\\.", "year": 2010}, {"title": "Class-based n-gram models of natural language", "author": ["Brown et al.1992] Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Combining distributional and morphological information for part of speech induction", "author": ["Alexander Clark"], "venue": "In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics-Volume", "citeRegEx": "Clark.,? \\Q2003\\E", "shortCiteRegEx": "Clark.", "year": 2003}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume", "author": ["Michael Collins"], "venue": null, "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Accurate methods for the statistics of surprise and coincidence", "author": ["Ted Dunning"], "venue": "Computacional Linguistics,", "citeRegEx": "Dunning.,? \\Q1993\\E", "shortCiteRegEx": "Dunning.", "year": 1993}, {"title": "The WEKA data mining software: an update", "author": ["Hall et al.2009] Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H. Witten"], "venue": "SIGKDD Explor. Newsl.,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Mining and summarizing customer reviews", "author": ["Hu", "Liu2004] M. Hu", "B. Liu"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Aspect and sentiment unification model for online review", "author": ["Jo", "Oh2011] Yohan Jo", "Alice H. Oh"], "venue": null, "citeRegEx": "Jo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jo et al\\.", "year": 2011}, {"title": "NRCcanada-2014: Detecting aspects and sentiment in customer reviews", "author": ["Xiaodan Zhu", "Colin Cherry", "Saif Mohammad"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Kiritchenko et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiritchenko et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "SemEval-2013 task 2: Sentiment analysis in twitter", "author": ["Nakov et al.2013] Preslav Nakov", "Sara Rosenthal", "Zornitsa Kozareva", "Veselin Stoyanov", "Alan Ritter", "Theresa Wilson"], "venue": "In Proceedings of the Seventh International Workshop on Semantic Evaluation (Se-", "citeRegEx": "Nakov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2013}, {"title": "Semeval2014 task 4: Aspect based sentiment analysis", "author": ["Dimitrios Galanis", "John Pavlopoulos", "Harris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar"], "venue": "In Proceedings of the International Workshop on Semantic", "citeRegEx": "Pontiki et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pontiki et al\\.", "year": 2014}, {"title": "Semeval2014 task 9: Sentiment analysis in twitter", "author": ["Preslav Nakov", "Alan Ritter", "Veselin Stoyanov"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation, SemEval,", "citeRegEx": "Rosenthal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rosenthal et al\\.", "year": 2014}, {"title": "Simple, robust and (almost) unsupervised generation of polarity lexicons for multiple languages", "author": ["Rodrigo Agerri", "German Rigau"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Associa-", "citeRegEx": "Vicente et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vicente et al\\.", "year": 2014}, {"title": "The General Inquirer: A Computer Approach to Content Analysis", "author": ["Stone et al.1966] P. Stone", "D. Dunphy", "M. Smith", "D. Ogilvie"], "venue": null, "citeRegEx": "Stone et al\\.,? \\Q1966\\E", "shortCiteRegEx": "Stone et al\\.", "year": 1966}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["Paul Hoffmann."], "venue": "Proceedings", "citeRegEx": "Hoffmann.,? 2005", "shortCiteRegEx": "Hoffmann.", "year": 2005}], "referenceMentions": [{"referenceID": 11, "context": ", 2014), SemEval (Nakov et al., 2013; Pontiki et al., 2014; Rosenthal et al., 2014), or the SemSA Challenge at ESWC20141.", "startOffset": 17, "endOffset": 83}, {"referenceID": 12, "context": ", 2014), SemEval (Nakov et al., 2013; Pontiki et al., 2014; Rosenthal et al., 2014), or the SemSA Challenge at ESWC20141.", "startOffset": 17, "endOffset": 83}, {"referenceID": 13, "context": ", 2014), SemEval (Nakov et al., 2013; Pontiki et al., 2014; Rosenthal et al., 2014), or the SemSA Challenge at ESWC20141.", "startOffset": 17, "endOffset": 83}, {"referenceID": 0, "context": "The EliXa system consists of three independent supervised modules based on the IXA pipes tools (Agerri et al., 2014) and Weka (Hall et al.", "startOffset": 95, "endOffset": 116}, {"referenceID": 6, "context": ", 2014) and Weka (Hall et al., 2009).", "startOffset": 17, "endOffset": 36}, {"referenceID": 9, "context": "Following (Kiritchenko et al., 2014), we manually filtered", "startOffset": 10, "endOffset": 36}, {"referenceID": 10, "context": "Finally, the English Wikipedia was also used to induce word clusters using word2vec (Mikolov et al., 2013).", "startOffset": 84, "endOffset": 106}, {"referenceID": 1, "context": "A general purpose polarity lexicon Lgen was built by combining four well known polarity lexicons: SentiWordnet SWN (Baccianella et al., 2010), General Inquirer GI (Stone et al.", "startOffset": 115, "endOffset": 141}, {"referenceID": 15, "context": ", 2010), General Inquirer GI (Stone et al., 1966), Opinion Finder OF (Wilson et al.", "startOffset": 29, "endOffset": 49}, {"referenceID": 5, "context": "Using the Log-likelihood ratio (LLR) (Dunning, 1993) we obtained the ranking of the words which occur more with negative and positive reviews respectively.", "startOffset": 37, "endOffset": 52}, {"referenceID": 0, "context": "We use the ixa-pipe-nerc Named Entity Recognition system3 (Agerri et al., 2014) off-the-shelf to train our OTE models; the system learns supervised models via the Perceptron algorithm as described by (Collins, 2002).", "startOffset": 58, "endOffset": 79}, {"referenceID": 4, "context": ", 2014) off-the-shelf to train our OTE models; the system learns supervised models via the Perceptron algorithm as described by (Collins, 2002).", "startOffset": 128, "endOffset": 143}, {"referenceID": 16, "context": "Specifically, ixa-pipe-nerc implements basic nonlinguistic local features and on top of those a combination of word class representation features partially inspired by (Turian et al., 2010).", "startOffset": 168, "endOffset": 189}, {"referenceID": 2, "context": "\u2022 Brown (Brown et al., 1992) clusters, taking the 4th, 8th, 12th and 20th node in the path.", "startOffset": 8, "endOffset": 28}, {"referenceID": 3, "context": "\u2022 Clark (Clark, 2003) clusters, using the standard configuration to induce 200 clusters on the Yelp reviews dataset and 100 clusters on the food portion of the Yelp reviews dataset.", "startOffset": 8, "endOffset": 21}, {"referenceID": 10, "context": "\u2022 Word2vec (Mikolov et al., 2013) clusters, based on K-means applied over the extracted word vectors using the skip-gram algorithm6; 400 clusters were induced using the Wikipedia.", "startOffset": 11, "endOffset": 33}, {"referenceID": 6, "context": "We use the SMO implementation provided by the Weka library (Hall et al., 2009).", "startOffset": 59, "endOffset": 78}, {"referenceID": 0, "context": "PoS tag and lemma information, obtained using the IXA pipes tools (Agerri et al., 2014), were also included as features.", "startOffset": 66, "endOffset": 87}], "year": 2017, "abstractText": "This paper presents a supervised Aspect Based Sentiment Analysis (ABSA) system. Our aim is to develop a modular platform which allows to easily conduct experiments by replacing the modules or adding new features. We obtain the best result in the Opinion Target Extraction (OTE) task (slot 2) using an off-the-shelf sequence labeler. The target polarity classification (slot 3) is addressed by means of a multiclass SVM algorithm which includes lexical based features such as the polarity values obtained from domain and open polarity lexicons. The system obtains accuracies of 0.70 and 0.73 for the restaurant and laptop domain respectively, and performs second best in the out-of-domain hotel, achieving an accuracy of 0.80.", "creator": "LaTeX with hyperref package"}}}