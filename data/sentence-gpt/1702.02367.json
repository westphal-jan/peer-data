{"id": "1702.02367", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "Iterative Multi-document Neural Attention for Multiple Answer Prediction", "abstract": "People have information needs of varying complexity, which can be solved by an intelligent agent able to answer questions formulated in a proper way, eventually considering user context and preferences. In a scenario in which the user profile can be considered as a question, intelligent agents able to answer questions can be used to find the most relevant answers for a given user. In this work we propose a novel model based on Artificial Neural Networks to answer questions with multiple answers by exploiting multiple facts retrieved from a knowledge base. In this novel model of intelligent agents we propose an approach with multiple logical questions for each user, namely the question of the topic as a question, and the question of the subject as a question. In this approach, all queries should be answered as an answer for the user.\n\n\n\nThis paper has been presented in a special issue of Computational Mathematics. It is published online in online issue No. 9.", "histories": [["v1", "Wed, 8 Feb 2017 10:58:02 GMT  (17kb)", "http://arxiv.org/abs/1702.02367v1", "Paper accepted and presented at the Deep Understanding and Reasoning: A challenge for Next-generation Intelligent Agents (URANIA) workshop, held in the context of the AI*IA 2016 conference"]], "COMMENTS": "Paper accepted and presented at the Deep Understanding and Reasoning: A challenge for Next-generation Intelligent Agents (URANIA) workshop, held in the context of the AI*IA 2016 conference", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["claudio greco", "alessandro suglia", "pierpaolo basile", "gaetano rossiello", "giovanni semeraro"], "accepted": false, "id": "1702.02367"}, "pdf": {"name": "1702.02367.pdf", "metadata": {"source": "CRF", "title": "Iterative Multi-document Neural Attention for Multiple Answer Prediction", "authors": ["Claudio Greco"], "emails": ["claudiogaetanogreco@gmail.com", "alessandro.suglia@gmail.com", "pierpaolo.basile@uniba.it", "gaetano.rossiello@uniba.it", "giovanni.semeraro@uniba.it"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n02 36\n7v 1"}, {"heading": "1 Motivation and Background", "text": "We are surrounded by a huge variety of technological artifacts which \u201clive\u201d with us today. These artifacts can help us in several ways because they have the power to accomplish complex and timeconsuming tasks. Unfortunately, common software systems can do for us only specific types of tasks, in a strictly algorithmic way which is pre-defined by the software designer. Machine Learning (ML), a branch of Artificial Intelligence (AI), gives machines the ability to learn to complete tasks without being explicitly programmed.\nPeople have information needs of varying complexity, ranging from simple questions about common facts which can be found in encyclopedias, to more sophisticated cases in which they need to know what movie to watch during a romantic evening. These tasks can be solved by an intelligent agent able to answer questions formulated in a proper way, eventually considering user context and preferences.\nQuestion Answering (QA) emerged in the last decade as one of the most promising fields in AI, since it allows to design intelligent systems which are able to give correct answers to user questions\nexpressed in natural language. Whereas, recommender systems produce individualized recommendations as output and have the effect of guiding the user in a personalized way to interesting or useful objects in a large space of possible options. In a scenario in which the user profile (the set of user preferences) can be represented by a question, intelligent agents able to answer questions can be used to find the most appealing items for a given user, which is the classical task that recommender systems can solve. Despite the efficacy of classical recommender systems, generally they are not able to handle a conversation with the user so they miss the possibility of understanding his contextual information, emotions and feedback to refine the user profile and provide enhanced suggestions. Conversational recommender systems assist online users in their information-seeking and decision making tasks by supporting an interactive process [10] which could be goal oriented with the task of starting general and, through a series of interaction cycles, narrowing down the user interests until the desired item is obtained [17].\nIn this work we propose a novel model based on Artificial Neural Networks to answer questions exploiting multiple facts retrieved from a knowledge base and evaluate it on a QA task. Moreover, the effectiveness of the model is evaluated on the top-n recommendation task, where the aim of the system is to produce a list of suggestions ranked according to the user preferences. After having assessed the performance of the model on both tasks, we try to define the long-term goal of a conversational recommender system able to interact with the user using natural language and to support him in the information seeking process in a personalized way.\nIn order to fulfill our long-term goal of building a conversational recommender system we need to assess the performance of our model on specific tasks involved in this scenario. A recent work which goes in this direction is reported in [6], which presents the bAbI Movie Dialog dataset, composed by different tasks such as factoid QA, top-n recommendation and two more complex tasks, one which mixes QA and recommendation and one which contains turns of dialogs taken from Reddit. Having more specific tasks like QA and recommendation, and a more complex one which mixes both tasks gives us the possibility to evaluate our model on different levels of granularity. Moreover, the subdivision in turns of the more complex task provides a proper benchmark of the model capability to handle an effective dialog with the user.\nFor the task related to QA, a lot of datasets have been released in order to assess the machine reading and comprehension capabilities and a lot of neural network-based models have been proposed. Our model takes inspiration from [19], which is able to answer Cloze-style [22] questions repeating an attention mechanism over the query and the documents multiple times. Despite the effectiveness on the Cloze-style task, the original model does not consider multiple documents as a source of information to answer questions, which is fundamental in order to extract the answer from different relevant facts. The restricted assumption that the answer is contained in the given document does not allow the model to provide an answer which does not belong to the document. Moreover, this kind of task does not expect multiple answers for a given question, which is important for the complex information needs required for a conversational recommender system.\nAccording to our vision, the main outcomes of our work can be considered as building blocks for a conversational recommender system and can be summarized as follows:\n1. we extend the model reported in [19] to let the inference process exploit evidences observed in multiple documents coming from an external knowledge base represented as a collection of textual documents;\n2. we design a model able to leverage the attention weights generated by the inference process to provide multiple answers which does not necessarily belong to the documents through a multi-layer neural network which may uncover possible relationships between the most relevant evidences;\n3. we assess the efficacy of our model through an experimental evaluation on factoid QA and top-n recommendation tasks supporting our hypothesis that a QA model can be used to solve top-n recommendation, too.\nThe paper is organized as follows: Section 2 describes our model, while Section 3 summarizes the evaluation of the model on the two above-mentioned tasks and the comparison with respect to stateof-the-art approaches. Section 4 gives an overview of the literature of both QA and recommender systems, while final remarks and our long-term vision are reported in Section 5."}, {"heading": "2 Methodology", "text": "Given a query q, an operator \u03c8 : Q \u2192 D that produces the set of documents relevant for q, where Q is the set of all queries and D is the set of all documents. Our model defines a workflow in which a sequence of inference steps are performed in order to extract relevant information from \u03c8(q) to generate the answers for q.\nFollowing [19], our workflow consists of three steps: (1) the encoding phase, which generates meaningful representations for query and documents; (2) the inference phase, which extracts relevant semantic relationships between the query and the documents by using an iterative attention mechanism and finally (3) the prediction phase, which generates a score for each candidate answer."}, {"heading": "2.1 Encoding phase", "text": "The input of the encoding phase is given by a query q and a set of documents \u03c8(q) = {d1, d2, . . . , d|Dq|} \u2261 Dq. Both queries and documents are represented by a sequence of words X = (x1, x2, . . . , x|X|), drawn from a vocabulary V . Each word is represented by a continuous d-dimensional word embedding x \u2208 Rd stored in a word embedding matrix X \u2208 R|V |\u00d7d.\nThe sequences of dense representations for q and dj are encoded using a bidirectional recurrent neural network encoder with Gated Recurrent Units (GRU) as in [19] which represents each word xi \u2208 X as the concatenation of a forward encoding \u2212\u2192 hk \u2208 Rh and a backward encoding \u2190\u2212 hk \u2208 R h. From now on, we denote the contextual representation for the word qi by q\u0303i \u2208 R2h and the contextual representation for the word dj,i in the document dj by d\u0303j,i \u2208 R2h. Differently from [19], we build a unique representation for the whole set of documents Dq related to the query q by stacking each contextual representation d\u0303j,i obtaining a matrix D\u0303q \u2208 Rl\u00d72h, where l = |d1|+ |d2|+ . . .+ |d|Dq ||."}, {"heading": "2.2 Inference phase", "text": "This phase uncovers a possible inference chain which models meaningful relationships between the query and the set of related documents. The inference chain is obtained by performing, for each inference step t = 1, 2, . . . , T , the attention mechanisms given by the query attentive read and the document attentive read keeping a state of the inference process given by an additional recurrent neural network with GRU units. In this way, the network is able to progressively refine the attention weights focusing on the most relevant tokens of the query and the documents which are exploited by the prediction neural network to select the correct answers among the candidate ones."}, {"heading": "2.2.1 Query attentive read", "text": "Given the contextual representations for the query words (q\u03031, q\u03032, . . . , q\u0303|q|) and the inference GRU state st\u22121 \u2208 Rs, we obtain a refined query representation qt (query glimpse) by performing an attention mechanism over the query at inference step t:\nq\u0302i,t = softmax i=1,...,|q| q\u0303\u22a4i (Aqst\u22121 + aq),\nqt = \u2211\ni\nq\u0302i,tq\u0303i\nwhere q\u0302i,t are the attention weights associated to the query words, Aq \u2208 R2h\u00d7s and aq \u2208 R2h are respectively a weight matrix and a bias vector which are used to perform the bilinear product with the query token representations q\u0303i. The attention weights can be interpreted as the relevance scores for each word of the query dependent on the inference state st\u22121 at the current inference step t."}, {"heading": "2.2.2 Document attentive read", "text": "Given the query glimpse qt and the inference GRU state st\u22121 \u2208 Rs, we perform an attention mechanism over the contextual representations for the words of the stacked documents D\u0303q:\nd\u0302i,t = softmax i=1,...,l D\u0303\u22a4qi(Ad[st\u22121,qt] + ad),\ndt = \u2211\ni\nd\u0302i,tD\u0303qi\nwhere D\u0303qi is the i-th row of D\u0303q , d\u0302i,t are the attention weights associated to the document words, Ad \u2208 R2h\u00d7s and ad \u2208 R2h are respectively a weight matrix and a bias vector which are used to perform the bilinear product with the document token representations D\u0303qi . The attention weights can be interpreted as the relevance scores for each word of the documents conditioned on both the query glimpse and the inference state st\u22121 at the current inference step t. By combining the set of relevant documents in D\u0303q , we obtain the probability distribution (d\u03021,t, d\u03022,t, . . . d\u0302l,t) over all the relevant document tokens using the above-mentioned attention mechanism."}, {"heading": "2.2.3 Gating search results", "text": "The inference GRU state at the inference step t is updated according to st = GRU([rq \u00b7 qt, rd \u00b7 dt], st\u22121), where rq and rd are the results of a gating mechanism obtained by evaluating g([st\u22121,qt,dt,qt \u00b7 dt]) for the query and the documents, respectively. The gating function g : Rs+6h \u2192 R2h is defined as a 2-layer feed-forward neural network with a Rectified Linear Unit (ReLU) [12] activation function in the hidden layer and a sigmoid activation function in the output layer. The purpose of the gating mechanism is to retain useful information for the inference process about query and documents and forget useless one."}, {"heading": "2.3 Prediction phase", "text": "The prediction phase, which is completely different from the pointer-sum loss reported in [19], is able to generate, given the query q, a relevance score for each candidate answer a \u2208 A by using the document attention weights d\u0302i,T computed in the last inference step T . The relevance score of each word w is obtained by summing the attention weights of w in each document related to q. Formally the relevance score for a given word w is defined as:\nscore(w) = 1\n\u03c0(w)\nl\u2211\ni=1\n\u03c6(i, w)\nwhere \u03c6(i, w) returns 0 if \u03c3(i) 6= w, d\u0302i,T otherwise; \u03c3(i) returns the word in position i of the stacked documents matrix D\u0303q and \u03c0(w) returns the frequency of the word w in the documents Dq related to the query q. The relevance score takes into account the importance of token occurrences in the considered documents given by the computed attention weights. Moreover, the normalization term\n1 \u03c0(w) is applied to the relevance score in order to mitigate the weight associated to highly frequent tokens.\nThe evaluated relevance scores are concatenated in a single vector representation z = [score(w1), score(w2), . . . , score(w|V |)] which is given in input to the answer prediction neural network defined as:\ny = sigmoid(Who relu(Wihz+ bih) + bho)\nwhere u is the hidden layer size, Wih \u2208 Ru\u00d7|V | and Who \u2208 R|A|\u00d7u are weight matrices, bih \u2208 Ru, bho \u2208 R\n|A| are bias vectors, sigmoid(x) = 11+e\u2212x is the sigmoid function and relu(x) = max(0, x) is the ReLU activation function, which are applied pointwise to the given input vector.\nThe neural network weights are supposed to learn latent features which encode relationships between the most relevant words for the given query to predict the correct answers. The outer sigmoid activation function is used to treat the problem as a multi-label classification problem, so that each candidate answer is independent and not mutually exclusive. In this way the neural network\ngenerates a score which represents the probability that the candidate answer is correct. Moreover, differently from [19], the candidate answer A can be any word, even those which not belong to the documents related to the query.\nThe model is trained by minimizing the binary cross-entropy loss function comparing the neural network output y with the target answers for the given query q represented as a binary vector, in which there is a 1 in the corresponding position of the correct answer, 0 otherwise."}, {"heading": "3 Experimental evaluation", "text": "The model performance is evaluated on the QA and Recs tasks of the bAbI Movie Dialog dataset using HITS@k evaluation metric, which is equal to the number of correct answers in the top-k results. In particular, the performance for the QA task is evaluated according to HITS@1, while the performance for the Recs task is evaluated according to HITS@100.\nDifferently from [6], the relevant knowledge base facts, taken from the knowledge base in triple form distributed with the dataset, are retrieved by \u03c8 implemented by exploiting the Elasticsearch engine and not according to an hash lookup operator which applies a strict filtering procedure based on word frequency. In our work, \u03c8 returns at most the top 30 relevant facts for q. Each entity in questions and documents is recognized using the list of entities provided with the dataset and considered as a single word of the dictionary V .\nQuestions, answers and documents given in input to the model are preprocessed using the NLTK toolkit [2] performing only word tokenization. The question given in input to the \u03c8 operator is preprocessed performing word tokenization and stopword removal.\nThe optimization method and tricks are adopted from [19]. The model is trained using ADAM [9] optimizer (learning rate=0.001) with a batch size of 128 for at most 100 epochs considering the best model until the HITS@k on the validation set decreases for 5 consecutive times. Dropout [20] is applied on rq and on rd with a rate of 0.2 and on the prediction neural network hidden layer with a rate of 0.5. L2 regularization is applied to the embedding matrix X with a coefficient equal to 0.0001. We clipped the gradients if their norm is greater than 5 to stabilize learning [14]. Embedding size d is fixed to 50. All GRU output sizes are fixed to 128. The number of inference steps T is set to 3. The size of the prediction neural network hidden layer u is fixed to 4096. Biases bih and bho are initialized to zero vectors. All weight matrices are initialized sampling from the normal distribution N (0, 0.05). The ReLU activation function in the prediction neural network has been experimentally chosen comparing different activation functions such as sigmoid and tanh and taking the one which leads to the best performance. The model is implemented in TensorFlow [1] and executed on an NVIDIA TITAN X GPU.\nFollowing the experimental design, the results in Table 1 are promising because our model outperforms all other systems on both tasks except for the QA SYSTEM on the QA task. Despite the advantage of the QA SYSTEM, it is a carefully designed system to handle knowledge base data in the form of triples, but our model can leverage data in the form of documents, without making any assumption about the form of the input data and can be applied to different kind of tasks. Additionally, the model MEMN2N is a neural network whose weights are pre-trained on the same dataset without using the long-term memory and the models JOINT SUPERVISED EMBEDDINGS and JOINT MEMN2N are models trained across all the tasks of the dataset in order to boost performance. De-\nspite that, our model outperforms the three above-mentioned ones without using any supplementary trick. Even though our model performance is higher than all the others on the Recs task, we believe that the obtained result may be improved and so we plan a further investigation. Moreover, the need for further investigation can be justified by the work reported in [18] which describes some issues regarding the Recs task.\nFigure 1 shows the attention weights computed in the last inference step of the iterative attention mechanism used by the model to answer to a given question. Attention weights, represented as red boxes with variable color shades around the tokens, can be used to interpret the reasoning mechanism applied by the model because higher shades of red are associated to more relevant tokens on which the model focus its attention. It is worth to notice that the attention weights associated to each token are the result of the inference mechanism uncovered by the model which progressively tries to focus on the relevant aspects of the query and the documents which are exploited to generate the answers.\nQuestion: what does Larenz Tate act in ?\nGround truth answers: The Postman, A Man Apart, Dead Presidents, Love Jones, Why Do Fools Fall in Love, The Inkwell\nGiven the question \u201cwhat does Larenz Tate act in?\u201d shown in the above-mentioned figure, the model is able to understand that \u201cLarenz Tate\u201d is the subject of the question and \u201cact in\u201d represents the intent of the question. Reading the related documents, the model associates higher attention weights to the most relevant tokens needed to answer the question, such as \u201cThe Postman\u201d, \u201cA Man Apart\u201d and so on."}, {"heading": "4 Related work", "text": "We think that it is necessary to consider models and techniques coming from research both in QA and recommender systems in order to pursue our desire to build an intelligent agent able to assist the user in decision-making tasks. We cannot fill the gap between the above-mentioned research areas if we do not consider the proposed models in a synergic way by virtue of the proposed analogy between the user profile (the set of user preferences) and the items to be recommended, as the question and the correct answers. The first work which goes in this direction is reported in [11], which exploits movie descriptions to suggest appealing movies for a given user using an architecture tipically used for QA tasks. In fact, most of the research in the recommender systems field presents ad-hoc systems which exploit neighbourhood information like in Collaborative Filtering techniques [13], item descriptions and metadata like in Content-based systems [5]. Recently presented neural network models [4, 3] systems are able to learn latent representations in the network weights leveraging information coming from user preferences and item information.\nIn recent days, a lot of effort is devoted to create benchmarks for artificial agents to assess their ability to comprehend natural language and to reason over facts. One of the first attempt is the bAbI [24] dataset which is a synthetic dataset containing elementary tasks such as selecting an answer between one or more candidate facts, answering yes/no questions, counting operations over lists and sets and basic induction and deduction tasks. Another relevant benchmark is the one described in [7], which provides CNN/Daily Mail datasets consisting of document-query-answer triples where an entity in the query is replaced by a placeholder and the system should identify the correct entity by\nreading and comprehending the given document. MCTest [16] requires machines to answer multiplechoice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Finally, SQuAD [15] consists in a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage.\nAccording to the experimental evaluations conducted on the above-mentioned datasets, high-level performance can be obtained exploiting complex attention mechanisms which are able to focus on relevant evidences in the processed content. One of the earlier approaches used to solve these tasks is given by the general Memory Network [23, 21] framework which is one of the first neural network models able to access external memories to extract relevant information through an attention mechanism and to use them to provide the correct answer. A deep Recurrent Neural Network with Long Short-Term Memory units is presented in [7], which solves CNN/Daily Mail datasets by designing two different attention mechanisms called Impatient Reader and Attentive Reader. Another way to incorporate attention in neural network models is proposed in [8] which defines a pointer-sum loss whose aim is to maximize the attention weights which lead to the correct answer."}, {"heading": "5 Conclusions and Future Work", "text": "In this work we propose a novel model based on Artificial Neural Networks to answer questions with multiple answers by exploiting multiple facts retrieved from a knowledge base. The proposed model can be considered a relevant building block of a conversational recommender system. Differently from [19], our model can consider multiple documents as a source of information in order to generate multiple answers which may not belong to the documents. As presented in this work, common tasks such as QA and top-n recommendation can be solved effectively by our model.\nIn a common recommendation system scenario, when a user enters a search query, it is assumed that his preferences are known. This is a stringent requirement because users cannot have a clear idea of their preferences at that point. Conversational recommender systems support users to fulfill their information needs through an interactive process. In this way, the system can provide a personalized experience dynamically adapting the user model with the possibility to enhance the generated predictions. Moreover, the system capability can be further enhanced giving explanations to the user about the given suggestions.\nTo reach our goal, we should improve our model by designing a \u03c8 operator able to return relevant facts recognizing the most relevant information in the query, by exploiting user preferences and contextual information to learn the user model and by providing a mechanism which leverages attention weights to give explanations. In order to effectively train our model, we plan to collect real dialog data containing contextual information associated to each user and feedback for each dialog which represents if the user is satisfied with the conversation. Given these enhancements, we should design a system able to hold effectively a dialog with the user recognizing his intent and providing him the most suitable contents.\nWith this work we try to show the effectiveness of our architecture for tasks which go from pure question answering to top-n recommendation through an experimental evaluation without any assumption on the task to be solved. To do that, we do not use any hand-crafted linguistic features but we let the system learn and leverage them in the inference process which leads to the answers through multiple reasoning steps. During these steps, the system understands relevant relationships between question and documents without relying on canonical matching, but repeating an attention mechanism able to unconver related aspects in distributed representations, conditioned on an encoding of the inference process given by another neural network. Equipping agents with a reasoning mechanism like the one described in this work and exploiting the ability of neural network models to learn from data, we may be able to create truly intelligent agents."}, {"heading": "6 Acknowledgments", "text": "This work is supported by the IBM Faculty Award \"Deep Learning to boost Cognitive Question Answering\". The Titan X GPU used for this research was donated by the NVIDIA Corporation."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I.J. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. J\u00f3zefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D.G. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P.A. Tucker", "V. Vanhoucke", "V. Vasudevan", "F.B. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": "CoRR, abs/1603.04467", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Nltk: the natural language toolkit", "author": ["S. Bird"], "venue": "Proceedings of the COLING/ACL on Interactive presentation sessions, pages 69\u201372. Association for Computational Linguistics", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Wide & deep learning for recommender systems", "author": ["H. Cheng", "L. Koc", "J. Harmsen", "T. Shaked", "T. Chandra", "H. Aradhye", "G. Anderson", "G. Corrado", "W. Chai", "M. Ispir", "R. Anil", "Z. Haque", "L. Hong", "V. Jain", "X. Liu", "H. Shah"], "venue": "CoRR, abs/1606.07792", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep neural networks for youtube recommendations", "author": ["P. Covington", "J. Adams", "E. Sargin"], "venue": "Proceedings of the 10th ACM Conference on Recommender Systems, New York, NY, USA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Semantics-aware contentbased recommender systems", "author": ["M. de Gemmis", "P. Lops", "C. Musto", "F. Narducci", "G. Semeraro"], "venue": "In Recommender Systems Handbook,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Evaluating prerequisite qualities for learning end-to-end dialog systems", "author": ["J. Dodge", "A. Gane", "X. Zhang", "A. Bordes", "S. Chopra", "A. Miller", "A. Szlam", "J. Weston"], "venue": "arXiv preprint arXiv:1511.06931", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Kocisk\u00fd", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "Advances in Neural Information Processing Systems, pages 1693\u20131701", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Text understanding with the attention sum reader network", "author": ["R. Kadlec", "M. Schmid", "O. Bajgar", "J. Kleindienst"], "venue": "arXiv preprint arXiv:1603.01547", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving recommender systems with adaptive conversational strategies", "author": ["T. Mahmood", "F. Ricci"], "venue": "Proceedings of the 20th ACM conference on Hypertext and hypermedia, pages 73\u201382. ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Ask me any rating: A content-based recommender system based on recurrent neural networks", "author": ["C. Musto", "C. Greco", "A. Suglia", "G. Semeraro"], "venue": "Proceedings of the 7th Italian Information Retrieval Workshop, Venezia, Italy, May 30-31", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807\u2013814", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "A comprehensive survey of neighborhood-based recommendation methods", "author": ["X. Ning", "C. Desrosiers", "G. Karypis"], "venue": "Recommender Systems Handbook, pages 37\u201376. Springer", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ICML (3), 28:1310\u20131318", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Squad: 100", "author": ["P. Rajpurkar", "J. Zhang", "K. Lopyrev", "P. Liang"], "venue": "000+ questions for machine comprehension of text. CoRR, abs/1606.05250", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Mctest: A challenge dataset for the opendomain machine comprehension of text", "author": ["M. Richardson", "C.J.C. Burges", "E. Renshaw"], "venue": "EMNLP", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Active learning in recommender systems", "author": ["N. Rubens", "D. Kaplan", "M. Sugiyama"], "venue": "Recommender Systems Handbook, pages 809\u2013846. Springer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Why \u201cblow out\u201d? a structural analysis of the movie dialog dataset", "author": ["R. Searle", "M. Bingham-Walker"], "venue": "ACL 2016, page 215", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Iterative alternating neural attention for machine reading", "author": ["A. Sordoni", "P. Bachman", "Y. Bengio"], "venue": "arXiv preprint arXiv:1606.02245", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "Advances in neural information processing systems, pages 2440\u20132448", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Cloze procedure: a new tool for measuring readability", "author": ["W.L. Taylor"], "venue": "Journalism and Mass Communication Quarterly, 30(4):415", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1953}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv preprint arXiv:1410.3916", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "CoRR, abs/1502.05698", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Conversational recommender systems assist online users in their information-seeking and decision making tasks by supporting an interactive process [10] which could be goal oriented with the task of starting general and, through a series of interaction cycles, narrowing down the user interests until the desired item is obtained [17].", "startOffset": 147, "endOffset": 151}, {"referenceID": 16, "context": "Conversational recommender systems assist online users in their information-seeking and decision making tasks by supporting an interactive process [10] which could be goal oriented with the task of starting general and, through a series of interaction cycles, narrowing down the user interests until the desired item is obtained [17].", "startOffset": 329, "endOffset": 333}, {"referenceID": 5, "context": "A recent work which goes in this direction is reported in [6], which presents the bAbI Movie Dialog dataset, composed by different tasks such as factoid QA, top-n recommendation and two more complex tasks, one which mixes QA and recommendation and one which contains turns of dialogs taken from Reddit.", "startOffset": 58, "endOffset": 61}, {"referenceID": 18, "context": "Our model takes inspiration from [19], which is able to answer Cloze-style [22] questions repeating an attention mechanism over the query and the documents multiple times.", "startOffset": 33, "endOffset": 37}, {"referenceID": 21, "context": "Our model takes inspiration from [19], which is able to answer Cloze-style [22] questions repeating an attention mechanism over the query and the documents multiple times.", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "we extend the model reported in [19] to let the inference process exploit evidences observed in multiple documents coming from an external knowledge base represented as a collection of textual documents;", "startOffset": 32, "endOffset": 36}, {"referenceID": 18, "context": "Following [19], our workflow consists of three steps: (1) the encoding phase, which generates meaningful representations for query and documents; (2) the inference phase, which extracts relevant semantic relationships between the query and the documents by using an iterative attention mechanism and finally (3) the prediction phase, which generates a score for each candidate answer.", "startOffset": 10, "endOffset": 14}, {"referenceID": 18, "context": "The sequences of dense representations for q and dj are encoded using a bidirectional recurrent neural network encoder with Gated Recurrent Units (GRU) as in [19] which represents each word xi \u2208 X as the concatenation of a forward encoding \u2212\u2192 hk \u2208 R and a backward encoding \u2190\u2212 hk \u2208 R .", "startOffset": 158, "endOffset": 162}, {"referenceID": 18, "context": "Differently from [19], we build a unique representation for the whole set of documents Dq related to the query q by stacking each contextual representation d\u0303j,i obtaining a matrix D\u0303q \u2208 R, where l = |d1|+ |d2|+ .", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "The gating function g : R \u2192 R is defined as a 2-layer feed-forward neural network with a Rectified Linear Unit (ReLU) [12] activation function in the hidden layer and a sigmoid activation function in the output layer.", "startOffset": 118, "endOffset": 122}, {"referenceID": 18, "context": "The prediction phase, which is completely different from the pointer-sum loss reported in [19], is able to generate, given the query q, a relevance score for each candidate answer a \u2208 A by using the document attention weights d\u0302i,T computed in the last inference step T .", "startOffset": 90, "endOffset": 94}, {"referenceID": 18, "context": "Moreover, differently from [19], the candidate answer A can be any word, even those which not belong to the documents related to the query.", "startOffset": 27, "endOffset": 31}, {"referenceID": 5, "context": "Differently from [6], the relevant knowledge base facts, taken from the knowledge base in triple form distributed with the dataset, are retrieved by \u03c8 implemented by exploiting the Elasticsearch engine and not according to an hash lookup operator which applies a strict filtering procedure based on word frequency.", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": "Questions, answers and documents given in input to the model are preprocessed using the NLTK toolkit [2] performing only word tokenization.", "startOffset": 101, "endOffset": 104}, {"referenceID": 18, "context": "The optimization method and tricks are adopted from [19].", "startOffset": 52, "endOffset": 56}, {"referenceID": 8, "context": "The model is trained using ADAM [9] optimizer (learning rate=0.", "startOffset": 32, "endOffset": 35}, {"referenceID": 19, "context": "Dropout [20] is applied on rq and on rd with a rate of 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "We clipped the gradients if their norm is greater than 5 to stabilize learning [14].", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "The model is implemented in TensorFlow [1] and executed on an NVIDIA TITAN X GPU.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "0 Table 1: Comparison between our model and baselines from [6] on the QA and Recs tasks evaluated according to HITS@1 and HITS@100, respectively.", "startOffset": 59, "endOffset": 62}, {"referenceID": 17, "context": "Moreover, the need for further investigation can be justified by the work reported in [18] which describes some issues regarding the Recs task.", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "The first work which goes in this direction is reported in [11], which exploits movie descriptions to suggest appealing movies for a given user using an architecture tipically used for QA tasks.", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "In fact, most of the research in the recommender systems field presents ad-hoc systems which exploit neighbourhood information like in Collaborative Filtering techniques [13], item descriptions and metadata like in Content-based systems [5].", "startOffset": 170, "endOffset": 174}, {"referenceID": 4, "context": "In fact, most of the research in the recommender systems field presents ad-hoc systems which exploit neighbourhood information like in Collaborative Filtering techniques [13], item descriptions and metadata like in Content-based systems [5].", "startOffset": 237, "endOffset": 240}, {"referenceID": 3, "context": "Recently presented neural network models [4, 3] systems are able to learn latent representations in the network weights leveraging information coming from user preferences and item information.", "startOffset": 41, "endOffset": 47}, {"referenceID": 2, "context": "Recently presented neural network models [4, 3] systems are able to learn latent representations in the network weights leveraging information coming from user preferences and item information.", "startOffset": 41, "endOffset": 47}, {"referenceID": 23, "context": "One of the first attempt is the bAbI [24] dataset which is a synthetic dataset containing elementary tasks such as selecting an answer between one or more candidate facts, answering yes/no questions, counting operations over lists and sets and basic induction and deduction tasks.", "startOffset": 37, "endOffset": 41}, {"referenceID": 6, "context": "Another relevant benchmark is the one described in [7], which provides CNN/Daily Mail datasets consisting of document-query-answer triples where an entity in the query is replaced by a placeholder and the system should identify the correct entity by", "startOffset": 51, "endOffset": 54}, {"referenceID": 15, "context": "MCTest [16] requires machines to answer multiplechoice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension.", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "Finally, SQuAD [15] consists in a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "One of the earlier approaches used to solve these tasks is given by the general Memory Network [23, 21] framework which is one of the first neural network models able to access external memories to extract relevant information through an attention mechanism and to use them to provide the correct answer.", "startOffset": 95, "endOffset": 103}, {"referenceID": 20, "context": "One of the earlier approaches used to solve these tasks is given by the general Memory Network [23, 21] framework which is one of the first neural network models able to access external memories to extract relevant information through an attention mechanism and to use them to provide the correct answer.", "startOffset": 95, "endOffset": 103}, {"referenceID": 6, "context": "A deep Recurrent Neural Network with Long Short-Term Memory units is presented in [7], which solves CNN/Daily Mail datasets by designing two different attention mechanisms called Impatient Reader and Attentive Reader.", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "Another way to incorporate attention in neural network models is proposed in [8] which defines a pointer-sum loss whose aim is to maximize the attention weights which lead to the correct answer.", "startOffset": 77, "endOffset": 80}, {"referenceID": 18, "context": "Differently from [19], our model can consider multiple documents as a source of information in order to generate multiple answers which may not belong to the documents.", "startOffset": 17, "endOffset": 21}], "year": 2017, "abstractText": "People have information needs of varying complexity, which can be solved by an intelligent agent able to answer questions formulated in a proper way, eventually considering user context and preferences. In a scenario in which the user profile can be considered as a question, intelligent agents able to answer questions can be used to find the most relevant answers for a given user. In this work we propose a novel model based on Artificial Neural Networks to answer questions with multiple answers by exploiting multiple facts retrieved from a knowledge base. The model is evaluated on the factoid Question Answering and top-n recommendation tasks of the bAbI Movie Dialog dataset. After assessing the performance of the model on both tasks, we try to define the long-term goal of a conversational recommender system able to interact using natural language and to support users in their information seeking processes in a personalized way. 1 Motivation and Background We are surrounded by a huge variety of technological artifacts which \u201clive\u201d with us today. These artifacts can help us in several ways because they have the power to accomplish complex and timeconsuming tasks. Unfortunately, common software systems can do for us only specific types of tasks, in a strictly algorithmic way which is pre-defined by the software designer. Machine Learning (ML), a branch of Artificial Intelligence (AI), gives machines the ability to learn to complete tasks without being explicitly programmed. People have information needs of varying complexity, ranging from simple questions about common facts which can be found in encyclopedias, to more sophisticated cases in which they need to know what movie to watch during a romantic evening. These tasks can be solved by an intelligent agent able to answer questions formulated in a proper way, eventually considering user context and preferences. Question Answering (QA) emerged in the last decade as one of the most promising fields in AI, since it allows to design intelligent systems which are able to give correct answers to user questions expressed in natural language. Whereas, recommender systems produce individualized recommendations as output and have the effect of guiding the user in a personalized way to interesting or useful objects in a large space of possible options. In a scenario in which the user profile (the set of user preferences) can be represented by a question, intelligent agents able to answer questions can be used to find the most appealing items for a given user, which is the classical task that recommender systems can solve. Despite the efficacy of classical recommender systems, generally they are not able to handle a conversation with the user so they miss the possibility of understanding his contextual information, emotions and feedback to refine the user profile and provide enhanced suggestions. Conversational recommender systems assist online users in their information-seeking and decision making tasks by supporting an interactive process [10] which could be goal oriented with the task of starting general and, through a series of interaction cycles, narrowing down the user interests until the desired item is obtained [17]. In this work we propose a novel model based on Artificial Neural Networks to answer questions exploiting multiple facts retrieved from a knowledge base and evaluate it on a QA task. Moreover, the effectiveness of the model is evaluated on the top-n recommendation task, where the aim of the system is to produce a list of suggestions ranked according to the user preferences. After having assessed the performance of the model on both tasks, we try to define the long-term goal of a conversational recommender system able to interact with the user using natural language and to support him in the information seeking process in a personalized way. In order to fulfill our long-term goal of building a conversational recommender system we need to assess the performance of our model on specific tasks involved in this scenario. A recent work which goes in this direction is reported in [6], which presents the bAbI Movie Dialog dataset, composed by different tasks such as factoid QA, top-n recommendation and two more complex tasks, one which mixes QA and recommendation and one which contains turns of dialogs taken from Reddit. Having more specific tasks like QA and recommendation, and a more complex one which mixes both tasks gives us the possibility to evaluate our model on different levels of granularity. Moreover, the subdivision in turns of the more complex task provides a proper benchmark of the model capability to handle an effective dialog with the user. For the task related to QA, a lot of datasets have been released in order to assess the machine reading and comprehension capabilities and a lot of neural network-based models have been proposed. Our model takes inspiration from [19], which is able to answer Cloze-style [22] questions repeating an attention mechanism over the query and the documents multiple times. Despite the effectiveness on the Cloze-style task, the original model does not consider multiple documents as a source of information to answer questions, which is fundamental in order to extract the answer from different relevant facts. The restricted assumption that the answer is contained in the given document does not allow the model to provide an answer which does not belong to the document. Moreover, this kind of task does not expect multiple answers for a given question, which is important for the complex information needs required for a conversational recommender system. According to our vision, the main outcomes of our work can be considered as building blocks for a conversational recommender system and can be summarized as follows: 1. we extend the model reported in [19] to let the inference process exploit evidences observed in multiple documents coming from an external knowledge base represented as a collection of textual documents; 2. we design a model able to leverage the attention weights generated by the inference process to provide multiple answers which does not necessarily belong to the documents through a multi-layer neural network which may uncover possible relationships between the most relevant evidences; 3. we assess the efficacy of our model through an experimental evaluation on factoid QA and top-n recommendation tasks supporting our hypothesis that a QA model can be used to solve top-n recommendation, too. The paper is organized as follows: Section 2 describes our model, while Section 3 summarizes the evaluation of the model on the two above-mentioned tasks and the comparison with respect to stateof-the-art approaches. Section 4 gives an overview of the literature of both QA and recommender systems, while final remarks and our long-term vision are reported in Section 5.", "creator": "LaTeX with hyperref package"}}}