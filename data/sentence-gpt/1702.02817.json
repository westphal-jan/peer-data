{"id": "1702.02817", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2017", "title": "Graph Based Relational Features for Collective Classification", "abstract": "Statistical Relational Learning (SRL) methods have shown that classification accuracy can be improved by integrating relations between samples. Techniques such as iterative classification or relaxation labeling achieve this by propagating information between related samples during the inference process. When only a few samples are labeled and connections between samples are sparse, collective inference methods have shown large improvements over standard feature-based ML methods, with significant performance gains in all measures.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 9 Feb 2017 12:58:23 GMT  (765kb,D)", "http://arxiv.org/abs/1702.02817v1", "Pacific-Asia Conference on Knowledge Discovery and Data Mining"]], "COMMENTS": "Pacific-Asia Conference on Knowledge Discovery and Data Mining", "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.LG", "authors": ["immanuel bayer", "uwe nagel", "steffen rendle"], "accepted": false, "id": "1702.02817"}, "pdf": {"name": "1702.02817.pdf", "metadata": {"source": "CRF", "title": "Graph Based Relational Features for Collective Classification", "authors": ["Immanuel Bayer", "Uwe Nagel", "Steffen Rendle"], "emails": ["immanuel.bayer@uni-konstanz.de", "uwe.nagel@uni-konstanz.de", "steffen.rendle@uni-konstanz.de"], "sections": [{"heading": "1 Introduction", "text": "Statistical relational learning (SRL) methods are used when samples are connected by one or more relation. These relations are helpful in tasks like scientific article classification where patterns such as \u201cconnected samples have similar labels\u201d are very predictive. Feature based ML methods in contrast often assume that samples are independently, identically distributed (iid). This approach is well established, allows for efficient parameter estimation and simplified prediction but ignores the relational information available in SRL settings. Recently, a number of methods have been proposed [11, 13, 18, 31] that significantly improves over classical methods by using joint inference. Exact joint inference has high runtime complexity which often requires approximate solutions [30]. These approximated joint inference techniques introduce new difficulties such as the need for specialized implementations that are expensive to run and difficult to tune [30].\nIn this paper we propose to transfer the relational information into classical features. This allows a straightforward combination of relational and classical\n? Current affiliation: Google Inc.\nar X\niv :1\n70 2.\n02 81\n7v 1\n[ cs\n.I R\n] 9\nF eb\n2 01\n(attribute) information. It also renders traditional, feature based ML methods competitive in settings where relational information is available and allows to leverage the large body of classical ML methods and their scalable algorithms.\nWe use three standard collective classification (CC) benchmark datasets to show that classical ML with relational features are strong competitors for state of the art SRL methods on this task. Note that on these datasets, collective classification are considered the best performing methods in the current literature [7, 14]. In particular, we make the following contributions:\n\u2013 We discuss how joint inference could be avoided by extending the sample description with relational features (Section 2.1). \u2013 We extend relational features to indirect relations (Section 2.2). This is new and crucial to achive high accuracy when only few samples are labeled (Section 4.3). \u2013 We introduce a new cluster based relational feature (Section 2.2) that provides strong results and is cheap to compute. \u2013 We show that our approach improves state of the art collective classification even in network only settings (Section 4.2)."}, {"heading": "2 Problem Setting", "text": "We start by giving the necessary definitions with the traditional setting of samples D = {(xi, yi)}Ni=1 where yi is the class label and xi is a feature vector describing sample i. We assume that for the first u samples (i \u2208 {1, . . . , u}), the class label yi is known and for the samples with index i > u, the class label is unknown. Relations among samples are represented by weighted, symmetric adjacency matrices Rk \u2208 Rn\u00d7n and the complete relational information is denoted as R = {Rk}Kk=1. While in the general case, each of the k relations could be complete, i.e. provide some similarity between each pair of samples, we explicitly consider the case of sparse, unweighted relations where only a minority of node pairs are connected by an edge. We start with a statistical argument to motivate the representation of relational information in a way that is compatible with the iid assumption."}, {"heading": "2.1 Preserving IID", "text": "Many machine learning algorithms are based on the maximum likelihood principal to learn the optimal value \u03b8\u2217 for model parameters given a dataset D (c.f. [17])\n\u03b8\u2217 := arg max \u03b8 p(\u03b8|D) = arg max \u03b8 p(D|\u03b8)\nwhere l(\u03b8) := p(D|\u03b8) is called the likelihood. A very common assumption in many ML approaches is that the samples in the dataset D are independent and identically distributed (iid). This assumption simplifies the likelihood to\np(D|\u03b8) iid\u221d N\u220f i=1 p(yi|xi, \u03b8).\nOne of the central arguments of relational learning is that examples are not iid. In particular for any pair of examples (xi, yi) and (xj , yj) conditional independence does not hold\np((xi, yi), (xj , yj)|\u03b8) 6\u221d p(yi|xi, \u03b8) p(yj |xj , \u03b8).\nNote that in this formulation the relational information R is completely neglected. However if R is used, it can render the probabilities independent\np((xi, yi), (xj , yj),R|\u03b8) \u221d p(yi|xi,R, \u03b8) p(yj |xj ,R, \u03b8).\n(1)\nThis formulation is close to standard non-relational ML with iid of samples. Standard ML algorithms assume that all information about a sample can be encoded in a (usually real-valued) feature vector x. Let us assume that the influence of R on sample i can be described through a finite number of real valued variables xri . We call x r i relational features. To simplify notation, we can define x\u0303i as the extended feature vector of an example i that combines both nonrelational features xi and relational features x r i . In total, this allows to rewrite Equation 1\np(D|\u03b8) iid\u221d N\u220f i=1 p(yi|x\u0303i, \u03b8).\nNote that due to the relational information in x\u0303i, the iid assumption can be preserved. In the remainder of this section, we discuss several ways to generate relational features."}, {"heading": "2.2 Graph Based Relational Features", "text": "Samples can be linked through multiple relations each of which can be described as a graph. Representing each relation by an independent set of features allows us to integrate an arbitrary number of relations per problem into a standard feature matrix. All of the proposed features have in common that the encoded relational information does not only consist of direct relations but in addition captures indirect relations which we found to be the key to their performance.\nNeighbor Ids Encoding the direct neighbors of a sample i in relation Rk can be achieved by treating each sample as a categorical variable which is true when samples are connected and false otherwise [1,24,25]. As this information is very local and yields limited information about i\u2019s position in Rk, we extend it by additionally including indirect neighbors at various distances to i. Distance refers to the number of edges dk(i, j) on a shortest path connecting i and j in Rk. In particular, for a (small) set of distances d \u2265 1 we describe each relation Rk by the distance-d neighborhood matrices Ddk \u2208 {0, 1}n\u00d7n with ( Ddk ) i,j = 1, if dk(i, j) = d and 0 otherwise. We illustrate the derivation of this feature in Figure 1.\nAggregated Neighbor Attributes Relational position can also be described by individual features of direct and indirect neighbors [20, 21]. As before we extend the idea by calculating individual features at various distances. For a categorical attribute with categories 1, . . . , c, we define an n \u00d7 c matrix L with Li,j = 1 if xi is labeled as j and Li,j = 0 otherwise. Then the count matrix Cdk =: D d kL can be derived as the projection of the corresponding neighborhood matrix to the label matrix L such that (Cdk)i,j yields the number of category j nodes in distance d of sample i. We denote this feature by neighbor class counts (NCC) and provide an illustration in Figure 2. An additional row normalization yields a probability matrix for the class labels in distance d, which we will denote by neighbor class probabilities (NCP).\nRandom Walk Similarity While the features described above are based strictly on shortest paths, random walks with restart (rwr) incorporate a different notion of connectivity. They have been proposed as a similarity measure in the context of auto-captioning of images [22].\nThe similarity between two nodes is measured as the probability of a random walk connecting them, i.e. the probability of the random walk process visiting one node when started from the other. To control for locality this includes a restart probability: in each step the walk will jump back to the starting point with probability r. This can be modeled as\npi = (1\u2212 r)Wpi + rei (2)\nwhere the column vector pi of matrix P describes the steady-state probability distribution over all samples for walks starting at i. W is the matrix encoding\ntransition probabilities, i.e. a L1 row normalized version of Rk, ei is the vector of zeros and unit value at position i and the parameter r is the restart probability. P can be determined as the solution of a linear system or approximated efficiently [32], leaving r as free parameter. We derive P\u0302 from P by column wise L2 normalization and use analogous to the neighbor features row i of P\u0302 as features for sample i.\nClustering Memberships Clustering methods can be used to identify groups of similar samples. Clustering features encode this information by representing this group membership. Given a clustering of the graph representing relation Rk into c clusters, we obtain an n \u00d7 c feature matrix M ck with (M ck)i,m = 1 if sample i belongs to cluster m and zero otherwise. Since a single clustering yields limited information about the dense groups in the graph, we create features for various clusterings, i.e. different c. We limit c to c = 2j subject to 2 \u2264 c \u2264 n which limits the number of clusterings to blog2(n)c while also providing a wide range of cluster sizes. This results in O(n) features per relation that are very sparse with only O(log(n)) non-zero features for per sample. The clusters can be calculated with negligible runtime using the METIS clustering framework1 [8]. Note that the dense subgroups identified in the clusterings can be directly related to the homophily assumption often exploited in relational learning."}, {"heading": "3 Related Work", "text": "We build on two main categories of related work. The first, in Section 3.1 uses features derived from the network structure to improve iid based inference. The second, discussed in Section 3.2 is work that views collective classification as a joint inference problem, simultaneously inferring the class label on every instance. The challenges specific to problems with few labeled data points have received special attention [4, 5, 14, 29] and helped us to understand the importance of indirect relations."}, {"heading": "3.1 Relational Features", "text": "Relational features can be combined with collective inference or directly used with standard ML methods as we argue in Section 2.1. Models such as Relational\n1 We used the implementation available at http://glaros.dtc.umn.edu/gkhome/ views/metis with default parameters.\nProbabilistic Trees [20], Relational Bayes Classifier [21] and the Link Based Classifier (LBC, [11]) concentrate primarily on the aggregation of attributes of connected samples. Others use rows from the (weighted) adjacency matrix as basis for feature construction [1, 24, 25]. We were especially inspired from the suggestion to extend the neighborhood of samples with few neighbors with distance-two neighborhoods [26] or ghost edges [5]. In contrast to previous work we keep the information from various neighborhood distances separated and introduce the concept of multiple indirect relations."}, {"heading": "3.2 Collective Inference", "text": "Full relational models such as Markov Logic Networks (MLN, [28]) or the Probabilistic Relational Model [31] can be used for CC [3]. We refer to Sen et al. [30] for a comprehensive overview of collective inference based CC algorithms. Their strength in high label autocorrelation settings and the problem of error propagation has been examined [7, 33] and improved inference schemes have been proposed [15]. Recently, stacking of non relational model has been introduced [9] as a fast approximation of Relational Dependency Networks [19]."}, {"heading": "4 Evaluation", "text": "In our experiments2 we investigate the following three questions:\n1. Are classical ML methods with relational features competitive to MLN and Collective Classification approaches. 2. What are the main ingredients that make relational features effective. 3. Does the combination of relational and attribute information improve re-\nsults?"}, {"heading": "4.1 Experimental Setup", "text": "Datasets We use three standard benchmark SRL datasets. The Cora and CiteSeer scientific paper collections have been used in different versions, we chose the versions3 presented in [30] and the IMDb dataset4. Both, Cora and CiteSeer include text features in form of bag of words (bow) representations. We give some statistics of these datasets in Table 1. Benchmark Models As baseline models we use the well established relational learning methods wvRN [12, 13], nLB [11] and MLN [3]. We chose relaxation labeling [2] as the collective inference method for wvRN and nLB as it has been shown to outperform Gibbs sampling and iterative classification on our datasets [14]. For MLN we used the rules HasWord(p,+w)=>Topic(p,+t) and\n2 Our code is available from https://github.com/ibayer/PAKDD2015 3 http://linqs.cs.umd.edu/projects/projects/lbc/index.html 4 http://netkit-srl.sourceforge.net/data.html\nTopic(p,t)^Link(p,p\u2019)=>Topic(p\u2019,t) together with discriminative weight learning and MC-SAT inference as recommended for Cora and Citeseer in a previous study [3]. Measures & Protocol We follow [14] and remove samples that are not connected to any other sample (singletons) in all experiments. Each experiment is repeated 10 times with class-balanced splits into ratios of 0.1, 0.2, . . . , 0.9 for the train-test set (shown as percentage in the figures). The MLN experiments are only repeated 3 times due to their extremely high evaluation time. We calculate multi-class accuracies by micro averaging and plot them on the y-axis of each figure in this section. We used the netkit-srl framework [14] in version (1.4.0)5 to evaluate the wvRN and nLB classifiers and the Alchemy 2.0 framework6 to evaluate the MLN model. The graph clusterings were obtained using METIS [8] version 5.1.0. Relational features are learned with an L2 penalized logistic regression model7 included in scikit-learn [23] version 0.14.01. The penalty hyperparameter C is optimized via grid search over {0.001, 0.01, 0.1, 1, 10, 100, 1000} on the training set."}, {"heading": "4.2 Comparing Relational Features to SRL", "text": "This section examines whether feature based relational models (without collective inference) are able to compete with the prediction quality of specialized relational learning methods. Consequently, the benchmark is a task where only relational data is available. In the first experiment, we compare wvRN and nLB with two logistic regression models that use only our relational features. The first relational feature model (rwr) is based on a random walk. The second model uses both, neighborhood and aggregated (NCP) features with distances 1,2,3. We exclude distances higher than three, since almost every node can be reached over three edges from every other node and therefore further distances do not provide additional information.\nFigure 4 illustrates two problems of SRL models: (i) nLB performs poorly8 when labels are sparse and (ii) wvRN is sensitive to violations of its built in assumptions \u2013 i.e. if label consistency among neighbors is not met, as with the IMDb dataset.\n5 http://netkit-srl.sourceforge.net/ 6 https://code.google.com/p/alchemy-2/ 7 We use a one-vs-all scheme for multiclass classification. 8 This has been attributed to a lack of training data [14].\nThe relational feature based models show a very consistent performance not much affected by the number of labeled samples. The results of the neighbor and NCP feature combination on IMDb illustrate the flexibility of relational features."}, {"heading": "4.3 Engineering Relational Features", "text": "We now examine the different relational features and the influence of their parameters. Two questions will be addressed: (i) How important are indirect relations? (ii) Which of the proposed relational features lead to the best results? All relational features that we consider can incorporate information about indirect neighbors. Each method has parameters that adjusts the locality of the resulting features. We first examine the effect of including indirect neighbors (Figure 5) and the importance of unlabeled neighbors (Figure 6) for relational neighbor features. The influence of the restart parameter r (c.f. Equation 2) for the rwr features can be seen in Figure 7 and an informative subset of results for various numbers of clusters is shown in Figure 8. The results suggest that the inclusion of indirect neighbors in the relational features is beneficial independently of whether they are used directly or for aggregation. Figure 6 shows that unlabeled neighbors contribute significantly to the overall performance. Together this answers our first question: unlabeled samples and indirect neighborhood relations are essential ingredients for relational features."}, {"heading": "4.4 Combining Relational and Local Information", "text": "In the following, we examine the effect of adding local attributes. Figure 9 shows results with neighborhood count features (NCC) of distances 1,2,3. Interestingly, the bag of words model performs better than network only models on Citeseer but worse on Cora. Combining relational and local attributes on the other hand, improves results in both cases. The figure further shows that our features outperform MLN on both datasets. In summary, our experiments suggest that the combination of relational features and attributes is beneficial even with a simple model such as logistic regression."}, {"heading": "4.5 Discussion", "text": "Our experiments indicate that relational feature based models compare well to specialized relational learners even in network only and sparse labeling settings. This has been verified on three standard SRL benchmark datasets and with three state of the art SRL methods for comparison. The inclusion of indirect neighbors has proven extremely important, especially in sparse label settings. We have further shown that the combination of relational features and local attributes is both straightforward and has the potential to improve considerably over both, feature only and network only models.\nNote, that our relational features can lead to very high dimensional representations. Such feature spaces are, however, common in recommender systems, click-through rate prediction and websearch where regularized logistic regression has been shown to be very effective [6,10]. In addition, we use a standard implementation of logistic regression and can consequently employ scalable versions that can be trained with billions of samples of high dimensions [16, 27]. We are further not committed to the logistic regression model as our features could be used as input for arbitrary vector space models."}, {"heading": "5 Conclusion", "text": "We have shown that dependencies between samples can be exploited using relational feature engineering. Our method allows to combine relational information from various sources with attributes attached to individual samples. We tested this on standard SRL benchmark datasets, showing that even on network only data our features are competitive to specialized relational learning models. In addition, our features can outperform them when additional information is available. Note that in contrast to the SRL methods, our proposal achieves these results without collective inference. While we restricted our experiments to logistic regression as prediction model, the proposed features could be used as input to any other feature based learning algorithm such as SVM, neural networks or random forests. Extending the use of relational features to multi relational datasets would be straight forward and a interesting direction for further research.\nAcknowledgments. This work was supported by the DFG under grants Re 3311/2-1 and Br 2158/6-1.\n\u25cf\n\u25cf \u25cf \u25cf\n\u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf \u25cf\n\u25cf \u25cf\nCiteseer Cora\n0.5\n0.6\n0.7\n0.65\n0.70\n0.75\n0.80\n0.85\nA ve\nra ge\nA cc\nur ac\ny"}], "references": [{"title": "The relational vector-space model and industry classification", "author": ["A. Bernstein", "S. Clearwater", "F. Provost"], "venue": "IJCAI workshop. Volume 266.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Enhanced hypertext categorization using hyperlinks", "author": ["S. Chakrabarti", "B. Dom", "P. Indyk"], "venue": "ACM SIGMOD Record 27(2)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Investigating markov logic networks for collective classification", "author": ["R. Crane", "L. McDowell"], "venue": "ICAART (1).", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Leveraging label-independent features for classification in sparsely labeled networks: An empirical study", "author": ["B. Gallagher", "T. Eliassi-Rad"], "venue": "In Giles, L., Smith, M., Yen, J., Zhang, H., eds.: ASONAM. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Using ghost edges for classification in sparsely labeled networks", "author": ["B. Gallagher", "H. Tong", "T. Eliassi-Rad", "C. Faloutsos"], "venue": "KDD.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Web-scale bayesian clickthrough rate prediction for sponsored search advertising in microsoft\u2019s bing search engine", "author": ["T. Graepel", "J.Q. Candela", "T. Borchert", "R. Herbrich"], "venue": "ICML.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Why collective inference improves relational classification", "author": ["D. Jensen", "J. Neville", "B. Gallagher"], "venue": "KDD.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "A fast and high quality multilevel scheme for partitioning irregular graphs", "author": ["G. Karypis", "V. Kumar"], "venue": "SIAM Journal on Scientific Computing 20(1)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Stacked graphical models for efficient inference in markov random fields", "author": ["Z. Kou", "W.W. Cohen"], "venue": "SDM, SIAM", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Large-scale sparse logistic regression", "author": ["J. Liu", "J. Chen", "J. Ye"], "venue": "KDD, ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Link-based classification", "author": ["Q. Lu", "L. Getoor"], "venue": "ICML. Volume 3.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Improving learning in networked data by combining explicit and mined links", "author": ["S.A. Macskassy"], "venue": "AAAI. Volume 22.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "A simple relational classifier", "author": ["S.A. Macskassy", "F. Provost"], "venue": "KDD-Workshop.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Classification in networked data: A toolkit and a univariate case study", "author": ["S.A. Macskassy", "F. Provost"], "venue": "JMLR 8", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Cautious collective classification", "author": ["L.K. McDowell", "K.M. Gupta", "D.W. Aha"], "venue": "JMLR 10", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Parallel boosting with momentum", "author": ["I. Mukherjee", "K. Canini", "R. Frongillo", "Y. Singer"], "venue": "ECML PKDD. Springer", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Machine learning: A probabilistic perspective", "author": ["K.P. Murphy"], "venue": "The MIT Press", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Iterative classification in relational data", "author": ["J. Neville", "D. Jensen"], "venue": "Proc. AAAI2000 Workshop on Learning Statistical Models from Relational Data.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Collective classification with relational dependency networks", "author": ["J. Neville", "D. Jensen"], "venue": "UAI.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning relational probability trees", "author": ["J. Neville", "D. Jensen", "L. Friedland", "M. Hay"], "venue": "KDD.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Simple estimators for relational bayesian classifiers", "author": ["J. Neville", "D. Jensen", "B. Gallagher"], "venue": "ICDM.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Automatic multimedia crossmodal correlation discovery", "author": ["J.Y. Pan", "H.J. Yang", "C. Faloutsos", "P. Duygulu"], "venue": "KDD, ACM", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "JMLR 12", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Distribution-based aggregation for relational learning with identifier attributes", "author": ["C. Perlich", "F. Provost"], "venue": "Machine Learning 62(1)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Aggregation-based feature invention and relational concept classes", "author": ["C. Perlich", "F. Provost"], "venue": "KDD, ACM", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Relational ensemble classification", "author": ["C. Preisach", "L. Schmidt-Thieme"], "venue": "ICDM.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Scaling factorization machines to relational data", "author": ["S. Rendle"], "venue": "VLDB. Volume 6., VLDB Endowment", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine Learning 62", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Handling missing values when applying classification models", "author": ["M. Saar-Tsechansky", "F. Provost"], "venue": "JMLR", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Collective classification in network data", "author": ["P. Sen", "G.M. Namata", "M. Bilgic", "L. Getoor", "B. Gallagher", "T. Eliassi-Rad"], "venue": "AI Magazine 29(3)", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}, {"title": "Probabilistic classification and clustering in relational data", "author": ["B. Taskar", "E. Segal", "D. Koller"], "venue": "IJCAI. Volume 17.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "Fast random walk with restart and its applications", "author": ["H. Tong", "C. Faloutsos", "J.Y. Pan"], "venue": "ICDM.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Understanding propagation error and its effect on collective classification", "author": ["R. Xiang", "J. Neville"], "venue": "ICDM, IEEE", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 10, "context": "Recently, a number of methods have been proposed [11, 13, 18, 31] that significantly improves over classical methods by using joint inference.", "startOffset": 49, "endOffset": 65}, {"referenceID": 12, "context": "Recently, a number of methods have been proposed [11, 13, 18, 31] that significantly improves over classical methods by using joint inference.", "startOffset": 49, "endOffset": 65}, {"referenceID": 17, "context": "Recently, a number of methods have been proposed [11, 13, 18, 31] that significantly improves over classical methods by using joint inference.", "startOffset": 49, "endOffset": 65}, {"referenceID": 30, "context": "Recently, a number of methods have been proposed [11, 13, 18, 31] that significantly improves over classical methods by using joint inference.", "startOffset": 49, "endOffset": 65}, {"referenceID": 29, "context": "Exact joint inference has high runtime complexity which often requires approximate solutions [30].", "startOffset": 93, "endOffset": 97}, {"referenceID": 29, "context": "These approximated joint inference techniques introduce new difficulties such as the need for specialized implementations that are expensive to run and difficult to tune [30].", "startOffset": 170, "endOffset": 174}, {"referenceID": 6, "context": "Note that on these datasets, collective classification are considered the best performing methods in the current literature [7, 14].", "startOffset": 124, "endOffset": 131}, {"referenceID": 13, "context": "Note that on these datasets, collective classification are considered the best performing methods in the current literature [7, 14].", "startOffset": 124, "endOffset": 131}, {"referenceID": 16, "context": "[17]) \u03b8\u2217 := arg max \u03b8 p(\u03b8|D) = arg max \u03b8 p(D|\u03b8) where l(\u03b8) := p(D|\u03b8) is called the likelihood.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Neighbor Ids Encoding the direct neighbors of a sample i in relation Rk can be achieved by treating each sample as a categorical variable which is true when samples are connected and false otherwise [1,24,25].", "startOffset": 199, "endOffset": 208}, {"referenceID": 23, "context": "Neighbor Ids Encoding the direct neighbors of a sample i in relation Rk can be achieved by treating each sample as a categorical variable which is true when samples are connected and false otherwise [1,24,25].", "startOffset": 199, "endOffset": 208}, {"referenceID": 24, "context": "Neighbor Ids Encoding the direct neighbors of a sample i in relation Rk can be achieved by treating each sample as a categorical variable which is true when samples are connected and false otherwise [1,24,25].", "startOffset": 199, "endOffset": 208}, {"referenceID": 19, "context": "Aggregated Neighbor Attributes Relational position can also be described by individual features of direct and indirect neighbors [20, 21].", "startOffset": 129, "endOffset": 137}, {"referenceID": 20, "context": "Aggregated Neighbor Attributes Relational position can also be described by individual features of direct and indirect neighbors [20, 21].", "startOffset": 129, "endOffset": 137}, {"referenceID": 21, "context": "They have been proposed as a similarity measure in the context of auto-captioning of images [22].", "startOffset": 92, "endOffset": 96}, {"referenceID": 31, "context": "P can be determined as the solution of a linear system or approximated efficiently [32], leaving r as free parameter.", "startOffset": 83, "endOffset": 87}, {"referenceID": 7, "context": "The clusters can be calculated with negligible runtime using the METIS clustering framework [8].", "startOffset": 92, "endOffset": 95}, {"referenceID": 3, "context": "The challenges specific to problems with few labeled data points have received special attention [4, 5, 14, 29] and helped us to understand the importance of indirect relations.", "startOffset": 97, "endOffset": 111}, {"referenceID": 4, "context": "The challenges specific to problems with few labeled data points have received special attention [4, 5, 14, 29] and helped us to understand the importance of indirect relations.", "startOffset": 97, "endOffset": 111}, {"referenceID": 13, "context": "The challenges specific to problems with few labeled data points have received special attention [4, 5, 14, 29] and helped us to understand the importance of indirect relations.", "startOffset": 97, "endOffset": 111}, {"referenceID": 28, "context": "The challenges specific to problems with few labeled data points have received special attention [4, 5, 14, 29] and helped us to understand the importance of indirect relations.", "startOffset": 97, "endOffset": 111}, {"referenceID": 19, "context": "Probabilistic Trees [20], Relational Bayes Classifier [21] and the Link Based Classifier (LBC, [11]) concentrate primarily on the aggregation of attributes of connected samples.", "startOffset": 20, "endOffset": 24}, {"referenceID": 20, "context": "Probabilistic Trees [20], Relational Bayes Classifier [21] and the Link Based Classifier (LBC, [11]) concentrate primarily on the aggregation of attributes of connected samples.", "startOffset": 54, "endOffset": 58}, {"referenceID": 10, "context": "Probabilistic Trees [20], Relational Bayes Classifier [21] and the Link Based Classifier (LBC, [11]) concentrate primarily on the aggregation of attributes of connected samples.", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "Others use rows from the (weighted) adjacency matrix as basis for feature construction [1, 24, 25].", "startOffset": 87, "endOffset": 98}, {"referenceID": 23, "context": "Others use rows from the (weighted) adjacency matrix as basis for feature construction [1, 24, 25].", "startOffset": 87, "endOffset": 98}, {"referenceID": 24, "context": "Others use rows from the (weighted) adjacency matrix as basis for feature construction [1, 24, 25].", "startOffset": 87, "endOffset": 98}, {"referenceID": 25, "context": "We were especially inspired from the suggestion to extend the neighborhood of samples with few neighbors with distance-two neighborhoods [26] or ghost edges [5].", "startOffset": 137, "endOffset": 141}, {"referenceID": 4, "context": "We were especially inspired from the suggestion to extend the neighborhood of samples with few neighbors with distance-two neighborhoods [26] or ghost edges [5].", "startOffset": 157, "endOffset": 160}, {"referenceID": 27, "context": "Full relational models such as Markov Logic Networks (MLN, [28]) or the Probabilistic Relational Model [31] can be used for CC [3].", "startOffset": 59, "endOffset": 63}, {"referenceID": 30, "context": "Full relational models such as Markov Logic Networks (MLN, [28]) or the Probabilistic Relational Model [31] can be used for CC [3].", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "Full relational models such as Markov Logic Networks (MLN, [28]) or the Probabilistic Relational Model [31] can be used for CC [3].", "startOffset": 127, "endOffset": 130}, {"referenceID": 29, "context": "[30] for a comprehensive overview of collective inference based CC algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Their strength in high label autocorrelation settings and the problem of error propagation has been examined [7, 33] and improved inference schemes have been proposed [15].", "startOffset": 109, "endOffset": 116}, {"referenceID": 32, "context": "Their strength in high label autocorrelation settings and the problem of error propagation has been examined [7, 33] and improved inference schemes have been proposed [15].", "startOffset": 109, "endOffset": 116}, {"referenceID": 14, "context": "Their strength in high label autocorrelation settings and the problem of error propagation has been examined [7, 33] and improved inference schemes have been proposed [15].", "startOffset": 167, "endOffset": 171}, {"referenceID": 8, "context": "Recently, stacking of non relational model has been introduced [9] as a fast approximation of Relational Dependency Networks [19].", "startOffset": 63, "endOffset": 66}, {"referenceID": 18, "context": "Recently, stacking of non relational model has been introduced [9] as a fast approximation of Relational Dependency Networks [19].", "startOffset": 125, "endOffset": 129}, {"referenceID": 29, "context": "The Cora and CiteSeer scientific paper collections have been used in different versions, we chose the versions presented in [30] and the IMDb dataset.", "startOffset": 124, "endOffset": 128}, {"referenceID": 11, "context": "Benchmark Models As baseline models we use the well established relational learning methods wvRN [12, 13], nLB [11] and MLN [3].", "startOffset": 97, "endOffset": 105}, {"referenceID": 12, "context": "Benchmark Models As baseline models we use the well established relational learning methods wvRN [12, 13], nLB [11] and MLN [3].", "startOffset": 97, "endOffset": 105}, {"referenceID": 10, "context": "Benchmark Models As baseline models we use the well established relational learning methods wvRN [12, 13], nLB [11] and MLN [3].", "startOffset": 111, "endOffset": 115}, {"referenceID": 2, "context": "Benchmark Models As baseline models we use the well established relational learning methods wvRN [12, 13], nLB [11] and MLN [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "We chose relaxation labeling [2] as the collective inference method for wvRN and nLB as it has been shown to outperform Gibbs sampling and iterative classification on our datasets [14].", "startOffset": 29, "endOffset": 32}, {"referenceID": 13, "context": "We chose relaxation labeling [2] as the collective inference method for wvRN and nLB as it has been shown to outperform Gibbs sampling and iterative classification on our datasets [14].", "startOffset": 180, "endOffset": 184}, {"referenceID": 2, "context": "Topic(p,t)^Link(p,p\u2019)=>Topic(p\u2019,t) together with discriminative weight learning and MC-SAT inference as recommended for Cora and Citeseer in a previous study [3].", "startOffset": 158, "endOffset": 161}, {"referenceID": 13, "context": "Measures & Protocol We follow [14] and remove samples that are not connected to any other sample (singletons) in all experiments.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "We used the netkit-srl framework [14] in version (1.", "startOffset": 33, "endOffset": 37}, {"referenceID": 7, "context": "The graph clusterings were obtained using METIS [8] version 5.", "startOffset": 48, "endOffset": 51}, {"referenceID": 22, "context": "Relational features are learned with an L2 penalized logistic regression model included in scikit-learn [23] version 0.", "startOffset": 104, "endOffset": 108}, {"referenceID": 13, "context": "8 This has been attributed to a lack of training data [14].", "startOffset": 54, "endOffset": 58}, {"referenceID": 5, "context": "Such feature spaces are, however, common in recommender systems, click-through rate prediction and websearch where regularized logistic regression has been shown to be very effective [6,10].", "startOffset": 183, "endOffset": 189}, {"referenceID": 9, "context": "Such feature spaces are, however, common in recommender systems, click-through rate prediction and websearch where regularized logistic regression has been shown to be very effective [6,10].", "startOffset": 183, "endOffset": 189}, {"referenceID": 15, "context": "In addition, we use a standard implementation of logistic regression and can consequently employ scalable versions that can be trained with billions of samples of high dimensions [16, 27].", "startOffset": 179, "endOffset": 187}, {"referenceID": 26, "context": "In addition, we use a standard implementation of logistic regression and can consequently employ scalable versions that can be trained with billions of samples of high dimensions [16, 27].", "startOffset": 179, "endOffset": 187}], "year": 2017, "abstractText": "Statistical Relational Learning (SRL) methods have shown that classification accuracy can be improved by integrating relations between samples. Techniques such as iterative classification or relaxation labeling achieve this by propagating information between related samples during the inference process. When only a few samples are labeled and connections between samples are sparse, collective inference methods have shown large improvements over standard feature-based ML methods. However, in contrast to feature based ML, collective inference methods require complex inference procedures and often depend on the strong assumption of label consistency among related samples. In this paper, we introduce new relational features for standard ML methods by extracting information from direct and indirect relations. We show empirically on three standard benchmark datasets that our relational features yield results comparable to collective inference methods. Finally we show that our proposal outperforms these methods when additional information is available.", "creator": "LaTeX with hyperref package"}}}