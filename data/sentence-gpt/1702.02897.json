{"id": "1702.02897", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2017", "title": "Online and Offline Domain Adaptation for Reducing BCI Calibration Effort", "abstract": "Many real-world brain-computer interface (BCI) applications rely on single-trial classification of event-related potentials (ERPs) in EEG signals. However, because different subjects have different neural responses to even the same stimulus, it is very difficult to build a generic ERP classifier whose parameters fit all subjects. The classifier needs to be calibrated for each individual subject, using some labeled subject-specific data. This paper proposes both online and offline weighted adaptation regularization (wAR) algorithms to reduce this calibration effort, i.e., to minimize the amount of labeled subject-specific EEG data required in BCI calibration, and hence to increase the utility of the BCI system. We demonstrate using a visually-evoked potential oddball task and three different EEG headsets that both online and offline wAR algorithms significantly outperform several other algorithms. Moreover, through source domain selection, we can reduce their computational cost by about 50%, making them more suitable for real-time applications. We conclude that each participant will benefit from a wide variety of applications from a variety of fields.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 9 Feb 2017 17:03:12 GMT  (807kb)", "http://arxiv.org/abs/1702.02897v1", "in press"]], "COMMENTS": "in press", "reviews": [], "SUBJECTS": "cs.LG cs.HC", "authors": ["dongrui wu"], "accepted": false, "id": "1702.02897"}, "pdf": {"name": "1702.02897.pdf", "metadata": {"source": "CRF", "title": "Online and Offline Domain Adaptation for Reducing BCI Calibration Effort", "authors": ["Dongrui Wu"], "emails": ["drwu09@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n02 89\n7v 1\n[ cs\n.L G\n] 9\nF eb\n2 01\n7 1\nIndex Terms\u2014Brain-computer interface, event-related potential, EEG, domain adaptation, transfer learning\nI. INTRODUCTION\nMany real-world brain-computer interface (BCI) applications rely on single-trial classification of event-related potentials (ERPs) in EEG signals [5], [39]. For example, in a rapid serial visual presentation (RSVP) based BCI system, a sequence of images are shown to the subject rapidly (e.g. 2- 10 Hz) [10], [36], and the subject needs to detect some target images in them. The target images are much less frequent than the non-target ones, so that they can elicit P300 ERPs in the oddball paradigm. The P300 ERPs can be detected by a BCI system [35], and the corresponding images are then triaged for further inspection. Research [33], [39], [54] has shown that these BCI systems enable the subject to detect targets in large aerial photographs faster and more accurately than traditional standard searches.\nUnfortunately, because different subjects have different neural responses to even the same visual stimulus [6], [7], [21], it is very difficult, if not impossible, to build a generic ERP classifier whose parameters fit all subjects. So, we need to calibrate the classifier for each individual subject, using some labeled subject-specific data. Reducing this calibration effort, i.e., minimizing the amount of labeled subject-specific data required in the calibration, would greatly increase the utility of the BCI system. This is the research problem tackled in this paper.\nMore specifically, we distinguish between two types of calibration in BCI:\n1) Offline calibration, in which a pool of unlabeled EEG epochs have been obtained a priori, and a subject is queried to label some of these epochs, which are then used to train a classifier to label the remaining epochs in that pool. A potential application of offline calibration is personalized automatic game highlight detection, e.g., a subject\u2019s EEG signals are recorded continuously while watching a football game; after the game, the subject manually labels a few highlights, which are then used to train an ERP classifier to detect more highlights. 2) Online calibration, in which some labeled EEG epochs are obtained on-the-fly, and then a classifier is trained from them to classify future EEG epochs. A potential application of online calibration is the afore-mentioned RSVP image tagging problem: at the beginning of the task, the subject is asked to explicitly indicate it (e.g., press a button) every time he/she detects a target image, and that information is used to train a P300 ERP classifier. After a certain number of calibration epochs, the performance of the classifier can become reliable enough so that it can label further images using EEG epochs only.\nOne major difference between offline calibration and online calibration is that, in offline calibration, the unlabeled EEG epochs can be used to help design the ERP classifier, whereas in online calibration there are no unlabeled EEG epochs. Additionally, in offline calibration we can query any epoch in the pool for the label (an optimal query strategy can hence be designed by using machine learning methods such as active learning [42], [48]), but in online calibration usually the sequence of the epochs is pre-determined and the subject has no control over which epoch he/she will see next.\nMany signal processing and machine learning approaches have been proposed to reduce the BCI calibration effort [29], [30]. They may be grouped into five categories [29]:\n1) Regularization, which is a very effective machine learning approach for constructing robust models [41], especially when the training data size is small. A popular regularization approach in BCI calibration is shrinkage [27], which gives a regularized estimate of the covariance matrices. 2) Transfer/multi-task learning, which uses relevant data from other subjects to help the current subject [19], [46]. The transfer learning (TL) [32] based approaches are particularly popular [1], [19], [20], [40], [46], [48],\n2 [49], [52], [53], because in many BCI applications we can easily find legacy data from the same subject in the same task or similar tasks, or legacy data from different subjects in the same task or similar tasks. These data, which will be called auxiliary data in this paper, can be used to improve the learning performance of a new subject, or for a new task.\n3) Adaptive learning, which refines the machine learning model as new (labeled or unlabeled) subject-specific data are available [23], [52], [53]. The main approach in this category is semi-supervised learning [9], which is often used for offline BCI calibration where unlabeled data are available. Semi-supervised learning first constructs an initial model from the labeled training data and then applies it to the unlabeled test data. The newly labeled test data are then integrated with the groundtruth training data to retrain the model, and hence to improve it iteratively. 4) Active learning, which optimally selects the most informative unlabeled samples to label [22], [31], [48], [50]. There are many criteria to determine which unlabeled samples are the most informative [42]. The most popular, and probably also the simplest, approach for classification is to select the samples that are closest to the current decision boundary, because the classifier is most uncertain about them. Active learning has been mainly used for offline BCI calibration, where unlabeled samples are available. However, a closely related technique, active class selection [24], can be used for online BCI calibration [49]. Its idea is to optimize the classes from which the new training samples are generated. 5) A priori physiological information, which can be used to construct the most useful EEG features. For example, prior information on which EEG channels are the most likely to be useful was used in [28] as a regularizer to optimize spatial filters, and beamforming has been used in [16] to find relevant features from prior regions of interest to reduce the calibration data requirement.\nInterestingly, these five categories of approaches are not mutually exclusive: in fact they can be freely combined to further reduce the amount of subject-specific calibration data. An optimal spatial filters was designed in [28] for efficient subjectto-subject transfer by combining regularization and a prior information on which channels are the most likely to be useful. A collaborative filtering approach was developed in [49], which combined TL and active class selection to minimize the online calibration effort. An active TL approach was proposed in [48] for offline calibration, which combined TL and active learning to minimize the offline calibration effort. An active weighted adaptation regularization approach, which combines active learning, TL, regularization and semi-supervised learning, was proposed in [51] to facilitate the switching between different EEG headsets. All these approaches are for BCI classification problems. However, recently researchers have also started to apply these techniques for regression problems in BCI calibration. For example, a domain adaptation with model fusion approach, which combines regularization and\nTL, was developed in [47] to estimate driver\u2019s drowsiness online continuously.\nThis paper presents a comprehensive overview and comparison of the offline and online weighted adaptation regularization with source domain selection (wARSDS) algorithms, which we proposed recently [52], [53]. The offline wARSDS algorithm, which combined TL, regularization, and semisupervised learning, was first developed in [53] for offline single-trial classification of ERPs in a visually-evoked potential (VEP) oddball task. It was later extended to online calibration in a RSVP task [52], which still includes TL and regularization but not semi-supervised learning because unlabeled samples are not available in online calibration. In this paper we use a VEP oddball task and three different EEG headsets to show that they have consistently good performance across subjects and headsets. We also compare the performances of the offline and online wARSDS algorithms in identical experimental settings to investigate the effect of semi-supervised learning, and show that it can indeed help improve the calibration performance.\nThe remainder of the paper is organized as follows: Section II introduces the details of the offline wARSDS algorithm. Section III introduces the online wARSDS (OwARSDS) algorithm. Section IV describes the experiment setup that is used to evaluate the performances of different algorithms. Section V presents performance comparison of different offline calibration algorithms. Section VI presents performance comparison of different online calibration algorithms. Section VII compares the performances of offline and online algorithms. Finally, Section VIII draws conclusions."}, {"heading": "II. WEIGHTED ADAPTATION REGULARIZATION WITH SOURCE DOMAIN SELECTION (WARSDS)", "text": "This section describes the offline wARSDS algorithm [51], [53], which originates from the adaptation regularization \u2013 regularized least squares (ARRLS) algorithm in [25]. We made several major enhancements to ARRLS to better handle classimbalance and multiple source domains, and also to make use of labeled samples in the target domain. wARSDS first uses source domain selection (SDS) to select the closest source domains for a given target domain, then uses weighted adaptation regularization (wAR) for each selected source domain to build individual classifiers, and finally performs model fusion. For simplicity, we only consider 2-class classification."}, {"heading": "A. wAR: Problem Definition", "text": "A domain [25], [32] D in TL consists of a multi-dimensional feature space X and a marginal probability distribution P (x), i.e., D = {X , P (x)}, where x \u2208 X . Two domains Ds and Dt are different if Xs 6= Xt, and/or Ps(x) 6= Pt(x).\nA task [25], [32] T in TL consists of a label space Y and a conditional probability distribution Q(y|x). Two tasks Ts and Tt are different if Ys 6= Yt, or Qs(y|x) 6= Qt(y|x).\nGiven a source domain Ds with n labeled samples, {(x1, y1), ..., (xn, yn)}, and a target domain Dt with ml labeled samples {(xn+1, yn+1), ..., (xn+ml , yn+ml)} and mu\n3 unlabeled samples {xn+ml+1, ...,xn+ml+mu}, domain adaptation (DA) TL learns a target prediction function f : xt 7\u2192 yt with low expected error on Dt, under the assumptions Xs = Xt, Ys = Yt, Ps(x) 6= Pt(x), and Qs(y|x) 6= Qt(y|x).\nFor example, in single-trial classification of VEPs, EEG epochs from the new subject are in the target domain, while EEG epochs from an existing subject (usually different from the new subject) are in the source domain. When there are multiple source domains, we perform DA for each of them separately and then aggregate the classifiers. A sample consists of the feature vector for an EEG epoch from a subject in either domain, collected as a response to a specific visual stimulus. Though usually the source and target domains employ the same feature extraction method, generally their marginal and conditional probability distributions are different, i.e., Ps(x) 6= Pt(x) and Qs(y|x) 6= Qt(y|x), because the two subjects usually have different neural responses to the same visual stimulus [6], [7], [21]. As a result, the auxiliary data from a source domain cannot represent the primary data in the target domain accurately, and must be integrated with some labeled target domain data to induce an accurate target domain classifier."}, {"heading": "B. wAR: The Learning Framework", "text": "Because\nf(x) = Q(y|x) = P (x, y)\nP (x) =\nQ(x|y)P (y)\nP (x) , (1)\nto use the source domain data in the target domain, we need to make sure Ps(xs) is close to Pt(xt), Qs(xs|ys) is close to Qt(xt|yt), and Ps(y) is also close to Pt(y). However, in this paper we focus only on the first two requirements by assuming all subjects conduct similar VEP tasks [so Ps(y) and Pt(y) are intrinsically close]. Our future research will consider the more general case that Ps(y) and Pt(y) are different.\nLet the classifier be f(x) = wT\u03c6(x), where w is the classifier parameters, and \u03c6 : X 7\u2192 H is the feature mapping function that projects the original feature vector to a Hilbert space H. As in [51], [53], the learning framework of wAR is formulated as:\nf =argmin f\u2208HK\nn \u2211\ni=1\nws,i\u2113(f(xi), yi) + wt\nn+ml \u2211\ni=n+1\nwt,i\u2113(f(xi), yi)\n+ \u03c3\u2016f\u20162K + \u03bb[Df,K(Ps, Pt) +Df,K(Qs, Qt)] (2)\nwhere \u2113 is the loss function, K \u2208 R(n+ml+mu)\u00d7(n+ml+mu) is the kernel matrix with K(xi,xj) = \u3008\u03c6(xi), \u03c6(xj)\u3009, and \u03c3 and \u03bb are non-negative regularization parameters. wt is the overall weight for target domain samples, which should be larger than 1 so that more emphasis is given to target domain samples than source domain samples1. ws,i and wt,i are the\n1Generally the number of labeled samples in the source domain is much larger than that in the target domain in both offline and online calibration scenarios, i.e., n \u226b ml . However, eventually the learned classifier will be applied to the target domain. So, the target domain should be emphasized. We choose wt > 1 so that the ml labeled target domain samples are less overwhelmed by the n source domain samples.\nweight for the ith sample in the source domain and target domain, respectively, i.e.,\nws,i =\n{\n1, xi \u2208 Ds,1 n1/(n\u2212 n1), xi \u2208 Ds,2\n(3)\nwt,i =\n{\n1, xi \u2208 Dt,1 m1/(ml \u2212m1), xi \u2208 Dt,2\n(4)\nin which Ds,c = {xi|xi \u2208 Ds \u2227 yi = c, i = 1, ..., n} is the set of samples in Class c of the source domain, Dt,c = {xj |xj \u2208 Dt \u2227 yj = c, j = n + 1, ..., n + ml} is the set of samples in Class c of the target domain, nc is the number of elements in Ds,c, and mc is the number of elements in Dt,c. The goal of ws,i (wt,i) is to balance the number of samples from difference classes in the source (target) domain. This is very important, because class imbalance is intrinsic to many applications [18], particularly BCI applications. In many cases the minority class is the one of particular interest (e.g., the VEP experiment presented in this paper), but it can be easily overwhelmed by the majority class if not properly weighted. Of course, there are many other approaches for handling class imbalance [18], [26], [37]. We used the weighting approach for its simplicity.\nBriefly speaking, the 1st term in (2) minimizes the loss on fitting the labeled samples in the source domain, the 2nd term minimizes the loss on fitting the labeled samples in the target domain, the 3rd term minimizes the structural risk of the classifier, and the 4th term minimizes the distance between the marginal probability distributions Ps(xs) and Pt(xt), and also the distance between the conditional probability distributions Qs(xs|ys) and Qt(xt|yt).\nAccording to the Representer Theorem [3], [25], the solution of (2) can be expressed as:\nf(x) =\nn+ml+mu \u2211\ni=1\n\u03b1iK(xi,x) = \u03b1 TK(X,x) (5)\nwhere\nX = [x1, ...,xn+ml+mu ] T (6)\nand \u03b1 = [\u03b11, ..., \u03b1n+ml+mu ] T are coefficients to be computed."}, {"heading": "C. wAR: Loss Functions Minimization", "text": "Let\ny = [y1, ..., yn+ml+mu ] T (7)\nwhere {y1, ..., yn} are known labels in the source domain, {yn+1, ..., yn+ml} are known labels in the target domain, and {yn+ml+1, ..., yn+ml+mu} are pseudo labels for the unlabeled target domain samples, i.e., labels estimated using available sample information in both source and target domains.\nDefine E \u2208 R(n+ml+mu)\u00d7(n+ml+mu) as a diagonal matrix with\nEii =\n\n\n ws,i, 1 \u2264 i \u2264 n wtwt,i, n+ 1 \u2264 i \u2264 n+ml 0, otherwise\n(8)\n4 We use the squared loss in this paper:\n\u2113(f(xi), yi) = (yi \u2212 f(xi)) 2 (9)\nSubstituting (5) and (9) into the first two terms in (2), we have\nn \u2211\ni=1\nws,i\u2113(f(xi), yi) + wt\nn+ml \u2211\ni=n+1\nwt,i\u2113(f(xi), yi)\n=\nn \u2211\ni=1\nws,i(yi \u2212 f(xi)) 2 + wt\nn+ml \u2211\ni=n+1\nwt,i(yi \u2212 f(xi)) 2\n=\nn+ml+mu \u2211\ni=1\nEii(yi \u2212 f(xi)) 2\n=(yT \u2212\u03b1TK)E(y \u2212K\u03b1) (10)"}, {"heading": "D. wAR: Structural Risk Minimization", "text": "As in [25], we define the structural risk as the squared norm of f in HK , i.e.,\n\u2016f\u20162K =\nn+ml+mu \u2211\ni=1\nn+ml+mu \u2211\nj=1\n\u03b1i\u03b1jK(xi,xj) = \u03b1 TK\u03b1 (11)"}, {"heading": "E. wAR: Marginal Probability Distribution Adaptation", "text": "As in [25], we compute Df,K(Ps, Pt) using the projected maximum mean discrepancy (MMD) between the source and target domains:\nDf,K(Ps, Pt) =\n[\n1\nn\nn \u2211\ni=1\nf(xi)\u2212 1\nml +mu\nn+ml+mu \u2211\ni=n+1\nf(xi)\n]2\n= \u03b1TKM0K\u03b1 (12)\nwhere M0 \u2208 R(n+ml+mu)\u00d7(n+ml+mu) is the MMD matrix:\n(M0)ij =\n\n  \n  \n1 n2 , 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 n 1 (ml+mu)2 , n+ 1 \u2264 i \u2264 n+ml +mu,\nn+ 1 \u2264 j \u2264 n+ml +mu \u22121\nn(ml+mu) , otherwise\n(13)"}, {"heading": "F. wAR: Conditional Probability Distribution Adaptation", "text": "As in [25], we first compute pseudo labels for the unlabeled target domain samples and construct the label vector y in (7). These pseudo labels can be computed using the classifier built in the previous iteration if wAR is used iteratively, or estimated using another classifier, e.g., a support vector machine (SVM) [45]. We then compute the projected MMD with respect to each class.\nLet Ds,c = {xi|xi \u2208 Ds \u2227 yi = c, i = 1, ..., n} be the set of samples in Class c of the source domain, Dt,c = {xj |xj \u2208 Dt\u2227yj = c, j = n+1, ..., n+ml+mu} be the set of samples in Class c of the target domain, nc be the number of elements in Ds,c, and mc be the number of elements in Dt,c. Then,\nthe distance between the conditional probability distributions in the two domains is computed as:\nDf,K(Qs, Qt) =\n2 \u2211\nc=1\n\n\n1\nnc\n\u2211\nxi\u2208Ds,c\nf(xi)\u2212 1\nmc\n\u2211\nxj\u2208Dt,c\nf(xj)\n\n\n2\n(14)\nSubstituting (5) into (14), it follows that\nDf,K(Qs, Qt)\n=\n2 \u2211\nc=1\n\n\n1\nnc\n\u2211\nxi\u2208Ds,c\n\u03b1 TK(X,x)\u2212\n1\nmc\n\u2211\nxj\u2208Dt,c\n\u03b1 TK(X,x)\n\n\n2\n=\n2 \u2211\nc=1\n\u03b1 TKMcK\u03b1 = \u03b1 TKMK\u03b1 (15)\nwhere\nM = M1 +M2 (16)\nin which M1 and M2 are MMD matrices computed as:\n(Mc)ij =\n\n    \n    \n1/n2c, xi,xj \u2208 Ds,c 1/m2c, xi,xj \u2208 Dt,c \u22121/(ncmc), xi \u2208 Ds,c,xj \u2208 Dt,c, or xj \u2208 Ds,c,xi \u2208 Dt,c 0, otherwise (17)"}, {"heading": "G. wAR: The Closed-Form Solution", "text": "Substituting (10), (11), (12) and (15) into (2), we have\nf = argmin f\u2208HK\n(yT \u2212\u03b1TK)E(y \u2212K\u03b1)\n+ \u03c3\u03b1TK\u03b1+ \u03bb\u03b1TK(M0 +M)K\u03b1 (18)\nSetting the derivative of the objective function above to 0 leads to the following closed-form solution for \u03b1:\n\u03b1 = [(E + \u03bbM0 + \u03bbM)K + \u03c3I] \u22121Ey (19)"}, {"heading": "H. Source Domain Selection (SDS)", "text": "When there are multiple source domains, it is very timeconsuming to perform wAR for each source domain and then aggregate the results. Additionally, aggregating results from source domains that are outliers or very different from the target domain may also hurt the classification performance. So, we introduce a source domain selection approach [53], which selects the closest source domains to reduce the computational cost, and also to (potentially) improve the classification performance.\nAssume there are Z different source domains. For the zth source domain, we first compute mz,c (c = 1, 2), the mean feature vector of each class. Then, we also compute mt,c, the mean feature vector of each target domain class, by making use of the ml known labels and the mu pseudo-labels. The distance between the two domains is then computed as:\nd(z, t) =\n2 \u2211\nc=1\n\u2016mz,c \u2212mt,c\u2016 (20)\n5 We next cluster these Z distances, {d(z, t)}z=1,...,Z, by kmeans clustering, and finally choose the cluster that has the smallest centroid, i.e., the source domains that are closest to the target domain. In this way, on average we only need to perform wAR for Z/k (k is the number of clusters in kmeans clustering) source domains, corresponding to a 50% computational cost saving if k = 2. A larger k will result in a larger saving; however, when k is too large, there may not be enough source domains selected for wAR, and hence the classification performance may be unstable. So, there is a trade-off between computational cost saving and classification performance. k = 2 was used in this paper, and it demonstrated satisfactory performance."}, {"heading": "I. The Complete wARSDS Algorithm", "text": "The pseudo code for the complete wARSDS algorithm is shown in Algorithm 1. It first uses SDS to select the closest source domains, then performs wAR for each of them separately to build individual classifiers, and finally aggregates them using a weighted average, where the weights are the corresponding training accuracies."}, {"heading": "J. Discussions", "text": "As mentioned at the beginning of this section, the formulation and derivation of wAR closely resemble the ARRLS algorithm in [25]; however, there are several major differences:\n1) wAR assumes a subject or an oracle is available to label a small number of samples in the target domain, whereas ARRLS assumes all target domain samples are unlabeled. As a result, wAR can be iterative, and the classifier can be updated every time new labeled target domain samples are available. 2) wAR explicitly considers the class imbalance problem in both source and target domains by introducing the classdependent weights on samples. As it will be shown in Section IV, this makes a huge difference in the balanced classification accuracy for the class imbalance problem, which is intrinsic in ERP-based BCI systems. 3) ARRLS also includes manifold regularization [3]. We investigated it but was not able to observe improved performance in our application, so it is not included in this paper.\nFinally, when combined with SDS, wARSDS can effectively handle multiple source domains, whereas ARRLS only considers one source domain."}, {"heading": "III. ONLINE WEIGHTED ADAPTATION REGULARIZATION WITH SOURCE DOMAIN SELECTION (OWARSDS)", "text": "This section introduces the OwARSDS algorithm [52], which extends the offline wARSDS algorithm to online BCI calibration. OwARSDS first uses SDS to select the closest source domains, then performs online weighted adaptation regularization (OwAR) for each selected source domain to build individual classifiers, and finally aggregates them."}, {"heading": "A. OwAR: The Learning Framework", "text": "Using the notations introduced in the previous section, the learning framework of OwAR can still be formulated as (2). However, because in online calibration there are no unlabeled target domain samples, the kernel matrix Ko has dimensionality (n+ml)\u00d7(n+ml), instead of (n+ml+mu)\u00d7(n+ml+mu) in offline calibration. As a result, the solution of (2) admits a different expression:\nfo(x) =\nn+ml \u2211\ni=1\n\u03b1oiK o(xi,x) = (\u03b1 o)TKo(Xo,x) (21)\nwhere\nXo = [x1, ...,xn+ml ] T (22)\nand \u03b1o = [\u03b11, ..., \u03b1n+ml ] T are coefficients to be computed.\nIt has been shown [52] that the closed-form solution for \u03b1o\nis:\n\u03b1 o = [(Eo + \u03bbMo0 + \u03bbM o)Ko + \u03c3I]\u22121Eoyo (23)\nNext we briefly introduce how the various terms in (23) are derived."}, {"heading": "B. OwAR: Loss Functions Minimization", "text": "Define\ny o = [y1, ..., yn+ml ] T (24)\nwhere {y1, ..., yn} are known labels in the source domain, and {yn+1, ..., yn+ml} are known labels in the target domain. Define also Eo \u2208 R(n+ml)\u00d7(n+ml) as a diagonal matrix with\nEoii =\n{\nws,i, 1 \u2264 i \u2264 n wtwt,i, n+ 1 \u2264 i \u2264 n+ml\n(25)\nThen, following the derivation in (10), we now have n \u2211\ni=1\nws,i\u2113(f o(xi), yi) + wt\nn+ml \u2211\ni=n+1\nwt,i\u2113(f o(xi), yi)\n=[(yo)T \u2212 (\u03b1o)TKo]Eo(yo \u2212Ko\u03b1o) (26)"}, {"heading": "C. OwAR: Structural Risk Minimization", "text": "Again, we define the structural risk as the squared norm of fo in HK , i.e.,\n\u2016fo\u20162K = (\u03b1 o)TKo\u03b1o (27)"}, {"heading": "D. OwAR: Marginal Probability Distribution Adaptation", "text": "We compute Dfo,Ko(Ps, Pt) using the projected MMD between the source and target domains:\nDfo,Ko(Ps, Pt) =\n[\n1\nn\nn \u2211\ni=1\nfo(xi)\u2212 1\nml\nn+ml \u2211\ni=n+1\nfo(xi)\n]2\n= (\u03b1o)TKoMo0K o \u03b1 o (28)\nwhere Mo0 \u2208 R (n+ml)\u00d7(n+ml) is the MMD matrix:\n(Mo0 )ij =\n\n  \n  \n1 n2 , 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 n\n1 m2\nl\n, n+ 1 \u2264 i \u2264 n+ml,\nn+ 1 \u2264 j \u2264 n+ml \u22121 nml , otherwise\n(29)\n6 Algorithm 1. The offline wARSDS algorithm [51], [53].\nInput: Z source domains, where the zth (z = 1, ..., Z) domain has nz labeled samples {xzi , y z i }i=1,...,nz ;\nml labeled target domain samples, {xtj , y t j}j=1,...,ml ; mu unlabeled target domain samples, {xtj}j=ml+1,...,ml+mu ; Parameters wt, \u03c3 and \u03bb in (2); Parameter k in k-means clustering of SDS.\nOutput: The wARSDS classifier f(x). \u22b2 SDS starts\nif ml == 0 then Retain all Z source domains; Compute pseudo-labels {yj}j=nz+ml+1,...,nz+ml+mu using another classifier, e.g., an SVM; Go to wAR. else Compute pseudo-labels {yj}j=nz+ml+1,...,nz+ml+mu using the wARSDS classifier built from the previous iteration; for z = 1, 2, ..., Z do Compute d(z, t), the distance between the target domain and the zth source domain, by (20).\nend for Cluster {d(z, t)}z=1,...,Z by k-means clustering; Retain the Z\u2032 source domains that belong to the cluster with\nthe smallest centroid. end if \u22b2 SDS ends; wAR starts Choose a kernel function K(xi,xj); for z = 1, 2, ..., Z\u2032 do\nConstruct the feature matrix X in (6); Compute the kernel matrix Kz from X; Construct y in (7), E in (8), M0 in (13), and M in (16); Compute \u03b1 by (19) and record it as \u03b1z ; Use \u03b1 to classify the nz+ml labeled samples and record the\naccuracy, wz ; end for \u22b2 wAR ends; Aggregation starts return f(x) = \u2211Z\u2032 z=1 wz\u03b1zKz(X,x).\nAlgorithm 2. The online OwARSDS algorithm [52].\nInput: Z source domains, where the zth (z = 1, ..., Z) domain has nz labeled samples {xzi , y z i }i=1,...,nz ;\nml labeled target domain samples, {xtj , y t j}j=1,...,ml ;\nParameters wt, \u03c3 and \u03bb in (2); Parameter k in k-means clustering of SDS.\nOutput: The OwARSDS classifier fo(x). \u22b2 SDS starts\nif ml == 0 then Retain all Z source domains;\nGo to OwAR. else\nfor z = 1, 2, ..., Z do Compute d(z, t), the distance between the target domain and the zth source domain, by (20). end for Cluster {d(z, t)}z=1,...,Z by k-means clustering; Retain the Z\u2032 source domains that belong to the cluster with the smallest centroid. end if \u22b2 SDS ends; OwAR starts Choose a kernel function Ko(xi,xj) ; for z = 1, 2, ..., Z\u2032 do\nConstruct the feature matrix Xo in (22); Compute the kernel matrix Koz from X\no; Construct yo in (24), Eo in (25), Mo0 in (29), M\no in (30); Compute \u03b1o by (19) and record it as \u03b1oz ; Use \u03b1oz to classify the nz + ml labeled samples and record\nthe accuracy, woz ; end for \u22b2 OwAR ends; Aggregation starts return fo(x) = \u2211Z\u2032 z=1 woz\u03b1 o zK o z (X o,x)."}, {"heading": "E. OwAR: Conditional Probability Distribution Adaptation", "text": "In offline calibration, to minimize the discrepancy between the conditional probability distributions in the source and target domains, we need to first compute the pseudo-labels for the mu unlabeled target domain samples. In online calibration, because there are no unlabeled target domain samples, this step is skipped. Following the derivation of (15), we still have:\nDfo,Ko(Qs, Qt) = (\u03b1 o)TKoMoKo\u03b1o (30)\nwhere Mo is still computed by (16), but using only the n source domain samples and ml target domain samples."}, {"heading": "F. Source Domain Selection (SDS)", "text": "The SDS procedure in OwARSDS is almost identical to that in wARSDS. The only difference is that the latter also makes use of the mu unlabeled target domain samples in computing mt,c in (20), whereas the former only uses the ml labeled target domain samples, because there are no unlabeled target domain samples in online calibration."}, {"heading": "G. The Complete OwARSDS Algorithm", "text": "The pseudo code for the complete OwARSDS algorithm is described in Algorithm 2. It first uses SDS to select the\nclosest source domains, then performs OwAR for each of them separately to build individual classifiers, and finally aggregates them using a weighted average, where the weights are the corresponding training accuracies. Observe that OwARSDS is very similar to wARSDS, the major difference being that no unlabeled target domain samples are available for use in OwARSDS."}, {"heading": "IV. THE VEP ODDBALL EXPERIMENT", "text": "This section describes the setup of the VEP oddball experiment, which is used in the following three sections to evaluate the performances of different algorithms."}, {"heading": "A. Experiment Setup", "text": "A two-stimulus VEP oddball task was used [38]. In this task, participants were seated in a sound-attenuated recording chamber, and image stimuli were presented to them at a rate of 0.5 Hz (one image every two seconds). The images (152\u00d7375 pixels), presented for 150 ms at the center of a 24 inch Dell P2410 monitor at a distance of approximately 70 cm, were either an enemy combatant [target; an example is shown in Fig. 1(a)] or a U.S. Soldier [non-target; an example is shown\n7 in Fig. 1(b)]. The subjects were instructed to maintain fixation on the center of the screen and identify each image as being target or non-target with a unique button press as quickly and accurately as possible2. A total of 270 images were presented to each subject, among which 34 were targets. The experiments were approved by U.S. Army Research Laboratory (ARL) Institutional Review Board. The voluntary, fully informed consent of the persons used in this research was obtained as required by federal and Army regulations [43], [44]. The investigator has adhered to Army policies for the protection of human subjects.\n18 subjects participated the experiments, which lasted on average 15 minutes. Signals for each subject were recorded with three different EEG headsets, including a 64-channel 512Hz BioSemi ActiveTwo system, a 9-channel 256Hz Advanced Brain Monitoring (ABM) X10 system, and a 14- channel 128Hz Emotiv EPOC headset. However, due to some exceptions at the experiment, data were correctly recorded for only 16 subjects for ABM, 15 subjects for BioSemi, and 15 subjects for Emotiv. There were 14 subjects whose data were correctly recorded for all three headsets, so we used only these 14 subjects in this study."}, {"heading": "B. Preprocessing and Feature Extraction", "text": "The preprocessing and feature extraction method for all three headsets was the same, except that for ABM and Emotiv headsets we used all the channels, but for the BioSemi headset we only used 21 channels (Cz, Fz, P1, P3, P5, P7, P9, PO7, PO3, O1, Oz, POz, Pz, P2, P4, P6, P8, P10, PO8, PO4, O2) mainly in the parietal and occipital areas, as in [48].\nEEGLAB [12] was used for EEG signal preprocessing and feature extraction. For each headset, we first band-passed the EEG signals to [1, 50] Hz, then downsampled them to 64 Hz, performed average reference, and next epoched them to the [0, 0.7] second interval timelocked to stimulus onset. We removed mean baseline from each channel in each epoch and\n2In the traditional oddball paradigm, subjects are only asked to respond to the target (oddball) stimuli. In our experiment we asked the subjects to respond to both types of stimuli so that we can remove epochs with incorrect responses from our analysis. Additionally, the experimental data will enable other analyses including the response time to different types of stimuli. Similar experimental settings have been used in [11], [17].\nremoved epochs with incorrect button press responses3. The final numbers of epochs from the 14 subjects are shown in Table I. Observe that there is significant class imbalance for every subject; that\u2019s why we need to use ws,i and wt,i in (2) to balance the two classes in both domains.\nEach [0, 0.7] second epoch contains hundreds of raw EEG magnitude samples (e.g., 64\u00d7 0.7 \u00d7 21 = 924 for BioSemi). To reduce the dimensionality, we performed a simple principal component analysis (PCA) to take the scores on the first 20 principal components as features. We then normalized each feature dimension separately to [0, 1]."}, {"heading": "C. Performance Measure", "text": "Let m+ and m\u2212 be the true number of epochs from the target and non-target class, respectively. Let m\u0302+ and m\u0302\u2212 be the number of epochs that are correctly classified by an algorithm as target and non-target, respectively. Then, we compute\na+ = m\u0302+ m+ , a\u2212 = m\u0302\u2212 m\u2212\nwhere a+ is the classification accuracy on the target class, and a\u2212 is the classification accuracy on the non-target class.\nThe following balanced classification accuracy (BCA) was then used as the performance measure in this paper:\nBCA = a+ + a\u2212\n2 . (31)"}, {"heading": "V. PERFORMANCE EVALUATION OF OFFLINE CALIBRATION ALGORITHMS", "text": "This selection presents performance comparison of wARSDS with several other offline calibration algorithms."}, {"heading": "A. Calibration Scenario", "text": "Although we knew the labels of all EEG epochs for all 14 subjects in the VEP experiment, we simulated a realistic offline calibration scenario: we had labeled EEG epochs from 13 subjects, and also all epochs from the 14th subject, but initially none of them was labeled. Our goal was to iteratively label epochs from the 14th subject and build a classifier so that his/her remaining unlabeled epochs can be reliably classified.\nThe flowchart for the simulated offline calibration scenario is shown in Fig. 2(a). Assume the 14th subject has m sequential epochs in the VEP experiment, and we want to label p epochs in each iteration, starting from zero. We first generate a random number m0 \u2208 [1,m], representing the starting position in the VEP sequence. Then, in the first iteration, we use the m unlabeled epochs from the 14th subject and all labeled epochs from the other 13 subjects to build different classifiers and compute their BCAs on the m unlabeled epochs. In the second iteration, we obtain labels for Epochs4 m0, m0+1, ...,\n3Button press responses were not recorded for most subjects using the ABM headset, so we used all 270 epochs for them.\n4For offline calibration, the labeled epochs need not to be sequential: they can be randomly selected from the m epochs. However, the labeled epochs are always sequential in online calibration. To facilitate the comparison between offline and online algorithms, we used sequential sampling in both offline and online calibrations. But note that there is no statistic difference between random sampling and sequential sampling in offline calibration.\n8\nm0 + p \u2212 1 from the 14th subject, build different classifiers, and compute their BCAs on the remaining m \u2212 p unlabeled epochs. We iterate until the maximum number of iterations is reached. When the end of the VEP sequence is reached during the iteration, we rewind to the beginning of the sequence, e.g., if m0 = m, then Epoch m0 + 1 is the 1st epoch in the VEP sequence, Epoch m0 + 2 is the 2nd, and so on.\nTo obtain statistically meaningful results, the above process was repeated 30 times for each subject, each time with a random starting point m0. We repeated this procedure 14 times so that each subject had a chance to be the \u201c14th\u201d subject."}, {"heading": "B. Algorithms", "text": "We compared the performance of wARSDS with six other algorithms [53]:\n1) BL1, a baseline approach in which we assume we know labels of all samples from the new subject, and use 5- fold cross-validation and SVM to find the highest BCA. This represents an upper bound of the BCA we can get, by using the data from the new subject only. 2) BL2, which is a simple iterative procedure: in each iteration we randomly select five unlabeled samples from the new subject to label, and then train an SVM classifier by 5-fold cross-validation. We iterate until the maximum number of iterations is reached. 3) TL, which is the TL algorithm introduced in [48]. It simply combines the labeled samples from the new subject with samples from each existing subject and train an SVM classifier. The final classifier is a weighted average of all individual classifiers, and the weights are the corresponding cross-validation BCAs. Note that this algorithm can be used both online and offline, because it does not use any information from the unlabeled epochs.\n4) TLSDS, which also performs SDS before the above TL algorithm. 5) ARRLS, which was proposed in [25] (manifold regularization was removed), and is also the wAR algorithm introduced in Algorithm 1, by setting wt = ws,i = wt,i = 1.\n6) wAR, which excludes the SDS part in Algorithm 1. Weighted libSVM [8] with RBF kernel was used as the classifier in BL1, BL2, TL and TLSDS. The optimal RBF parameter was found by cross-validation. We chose wt = 2, \u03c3 = 0.1, and \u03bb = 10, following the practice in [25], [51], [53]."}, {"heading": "C. Experimental Results", "text": "The BCAs of the seven algorithms, averaged over the 30 runs and across the 14 subjects, are shown in Fig. 3 for the three headsets. Observe that:\n1) Generally the performances of all algorithms (except BL1, which is not iterative) increased as more labeled subject-specific samples were available, which is intuitive. 2) BL2 cannot build a classifier when there were no labeled subject-specific samples at all (observe that the BCA for ml = 0 on the BL2 curve in Fig. 3 was always 0.5, representing random guess), but all TL/DA based algorithms can, because they can make use of information from other subjects. Moreover, without any labeled\n9 subject-specific samples, wAR and wARSDS can build a classifier with a BCA of 68.20% for BioSemi, 61.45% for ABM, and 64.17% for Emotiv, much better than random guess.\n3) Generally the performance of TL was worse than BL2, suggesting that it cannot cope well with large individual differences among the subjects5. 4) TLSDS always outperformed TL. This is because TL used a very simple way to combine the labeled samples from the new and existing subjects, and hence an existing subject whose ERPs are significantly different from the new subject\u2019s would have a negative impact on the final BCA. SDS can identify and remove (some of) such subjects, and hence benefited the BCA. 5) ARRLS demonstrated the worst BCA, because all other algorithms explicitly handled class-imbalance using weights, whereas ARRLS did not. For our dataset, the non-target class had seven times more samples than the target class, so many times ARRLS simply classified all samples as non-target, resulting in a BCA of 0.5. 6) wAR and wARSDS significantly outperformed BL2, TL, TLSDS and ARRLS. This is because a sophisticated DA approach was used in wAR and wARSDS, which explicitly considered class imbalance, and was optimized not only for high classification accuracy, but also for small structural risk and close feature similarity between the two domains. 7) wARSDS and wAR had very similar performance, but instead of using 13 auxiliary subjects, wARSDS only used on average 6.84 subjects for BioSemi, 6.03 subjects for ABM, and 6.85 subjects for Emotiv, corresponding to 47.38%, 53.62% and 47.31% computational cost saving, respectively.\nAs in [51], [53], we also performed comprehensive statistical tests to check if the performance differences among the six algorithms (BL1 was not included because it is not iterative) were statistically significant. We used the area-underperformance-curve (AUPC) [31], [51], [53] to assess overall performance differences among these algorithms. The AUPC is the area under the curve of the BCAs obtained at each of the 30 runs, and is normalized to [0, 1]. A larger AUPC value indicates a better overall classification performance.\nFirst, we checked the normality of our data to see if parametric ANOVA tests can be used. The histograms of the 30 \u00d7 14 = 420 AUPCs for each of the six algorithms on the three headsets are shown in Fig. 4. Observe that most of them are not even close to normal. So, parametric ANOVA tests cannot be applied.\nAs a result, we used Friedman\u2019s test [15], a two-way non-parametric ANOVA where column effects are tested for significant differences after adjusting for possible row effects.\n5Note that this does not conflict with the observation in [48], which said TL was better than BL2. This is because different datasets were used in the two studies: [48] downsampled the non-target class to balance the two classes before testing the performances of different algorithms, whereas classimbalance was preserved in this paper. Moreover, [48] only considered the BioSemi headset, and showed that TL outperformed BL2, but the performance difference between TL and BL2 decreased as ml increases. This is also the case in Fig. 3(a), when ml is small.\n0.6 0.8 0\n20\nBL2\n0.6 0.8 0\n20\nTL\n0.6 0.7 0.8 0\n20\n40 TLSDS\n0.5 0.6 0\n100\n200 ARRLS\n0.6 0.8 0\n20\nwAR\n0.6 0.8 0\n20\n40 wARSDS\n0.5 0.6 0.7 0\n50 BL2\n0.5 0.6 0.7 0\n50\nTL\n0.5 0.6 0.7 0\n20\n40\nTLSDS\n0.5 0.55 0\n100\nARRLS\n0.4 0.6 0.8 0\n20\n40 wAR\n0.4 0.6 0.8 0\n50 wARSDS\n0.6 0.8 0\n20\nBL2\n0.6 0.8 0\n20\nTL\n0.5 0.6 0.7 0\n20\nTLSDS\n0.5 0.51 0.52 0\n50\n100\nARRLS\n0.6 0.8 0\n10\n20\nwAR\n0.6 0.8 0\n10\n20\nwARSDS\nFig. 4. Histograms of the AUPCs of the six algorithms on the three headsets. Top: BioSemi; middle: ABM; bottom: Emotiv.\nWe treated the algorithm type (BL2, TL, TLSDS, ARRLS, wAR, wARSDS) as the column effects, with subjects as the row effects. Each combination of algorithm and subject had 30 values corresponding to 30 runs performed. Friedman\u2019s test showed statistically significant differences among the six algorithms for each headset (df = 5, p = 0.00).\nThen, non-parametric multiple comparison tests using Dunn\u2019s procedure [13], [14] was used to determine if the difference between any pair of algorithms was statistically significant, with a p-value correction using the false discovery rate method [4]. The results showed that the performances of wAR and wARSDS were statistically significantly different from BL2, TL, TLSDS and ARRLS for each headset (p = 0.0000 for all cases, except p = 0.0031 for ABM wAR vs BL2, and p = 0.0004 for ABM wARSDS vs BL2). There was no statistically significant performance difference between wAR and wARSDS (p = 0.2602 for BioSemi, p = 0.2734 for ABM, and p = 0.4365 for Emotiv).\nIn summary, we have demonstrated that given the same number of labeled subject-specific training samples, wAR and wARSDS can significantly improve the offline calibration performance. In other words, given a desired classification accuracy, wAR and wARSDS can significantly reduce the number of labeled subject-specific training samples. For example, in Fig. 3(a), the average BCA of BL2 is 71.14%, given 100 labeled subject-specific training samples. However, to achieve that BCA, on average wAR and wARSDS only need 5 samples, corresponding to 95% saving of the labeling effort. Moreover, Fig. 3(a) also shows that, without using any labeled subject-specific samples, wAR and wARSDS can achieve similar performance as BL2 which uses 65 samples. Similar observations can also be made for the ABM and Emotiv headsets."}, {"heading": "D. Parameter Sensitivity Analysis", "text": "In this subsection we study the sensitivity of wAR and wARSDS to parameters \u03c3 and \u03bbP (\u03bbQ). To save space, we only show the BCA results for the BioSemi headset. Similar results were obtained from the other two headsets.\nThe average BCAs of wAR and wARSDS for different \u03c3 (\u03bbP and \u03bbQ were fixed at 10) are shown in Fig. 5(a), and for different \u03bbP and6 \u03bbQ (\u03c3 was fixed at 0.1) are shown\n6We always assigned \u03bbP and \u03bbQ identical value because they are conceptually close.\n10\nin Fig. 5(b). Observe from Fig. 5(a) that both wAR and wARSDS achieved good BCAs for \u03c3 \u2208 [0.0001, 1], and from Fig. 5(b) that both wAR and wARSDS achieved good BCAs for \u03bbP \u2208 [10, 100] and \u03bbQ \u2208 [10, 100]. Moreover, \u03c3, \u03bbP and \u03bbQ have more impact to the BCA when ml is small. As ml increases, the impact diminishes. This is intuitive, as the need for transfer diminishes as the amount of labeled target domain data increases."}, {"heading": "VI. PERFORMANCE EVALUATION OF ONLINE CALIBRATION ALGORITHMS", "text": "This selection compares the performance of OwARSDS with several other online calibration algorithms."}, {"heading": "A. Online Calibration Scenario", "text": "Although we knew the labels of all EEG epochs for all 14 subjects in the experiment, we simulated a realistic online calibration scenario: we had labeled EEG epochs from 13 subjects, but initially no epoch from the 14th subject; we generated labeled epochs from the 14th subject iteratively and sequentially on-the-fly, which were used to train a classifier to label the remaining epochs from that subject.\nThe flowchart for the simulated online calibration scenario is shown in Fig. 2(b). Compared with the offline calibration scenario in Fig. 2(a), the main difference is that offline calibration has access to all m unlabeled samples from the 14th subject, but online calibration does not.\nMore specifically, assume the 14th subject has m sequential epochs in the VEP experiment, and we want to label p epochs in each iteration, starting from zero. We first generate a random number m0 \u2208 [1,m], representing the starting position in the VEP sequence. Then, in the first iteration, we use all labeled epochs from the other 13 subjects to build different classifiers, and compute their BCAs on the m unlabeled epochs. In the second iteration, we generated labeled Epochs m0, m0+1, ..., m0 + p \u2212 1 from the 14th subject, build different classifiers,\nand compute their BCAs on the remaining m \u2212 p unlabeled epochs. We iterate until the maximum number of iterations is reached. To obtain statistically meaningful results, the above process was repeated 30 times for each subject, each time with a random starting point m0. The whole process was repeated 14 times so that each subject had a chance to be the \u201c14th\u201d subject."}, {"heading": "B. Online Calibration Algorithms", "text": "We compared the performances of OwARSDS with five other algorithms:\n1) BL1 in Section V-B. 2) BL2 in Section V-B, using different PCA features. 3) TL in Section V-B, using different PCA features. 4) TLSDS, which is the above TL algorithm with SDS. 5) OwAR, which uses all existing subjects, instead of\nperforming SDS. Again, weighted libSVM [8] with RBF kernel was used as the classifier in BL1, BL2, TL and TLSDS. We chose wt = 2, \u03c3 = 0.1, and \u03bb = 10.\nNote that the online algorithms still used PCA features, but they were computed differently from those in offline calibration. In offline calibration we had access to the ml labeled samples plus the mu unlabeled samples, so the PCA bases can be pre-computed from all ml+mu samples and kept fixed in each iteration. However, in online calibration, we only had access to the ml labeled samples, so the PCA bases were computed from the ml samples only, and we updated them in each iteration as ml changed."}, {"heading": "C. Experimental Results", "text": "The BCAs of the six algorithms, averaged over the 30 runs and across the 14 subjects, are shown in Fig. 6 for different EEG headsets. The observations made in Section V-C for offline calibration still hold here, except that ARRLS was not included in online calibration. Particularly, both OwAR and OwARSDS achieved much better performance than BL2, TL and TLSDS. However, instead of using 13 auxiliary subjects in OwAR, OwARSDS only used on average 6.51 subjects for BioSemi, 6.01 subjects for ABM, and 7.09 subjects for Emotiv, corresponding to 49.92%, 53.77% and 45.46% computational cost saving, respectively.\nFriedman\u2019s test showed statistically significant performance differences among the five algorithms (excluding BL1, which is not iterative) for each headset (df = 4, p = 0.00). Dunn\u2019s procedure showed that the BCAs of OwAR and OwARSDS are statistically significantly different from BL2, TL, and TLSDS for each headset (p = 0.0002 for ABM OwARSDS vs BL2, p = 0.0001 for ABM OwARSDS vs TLSDS, and p = 0.0000 in all other cases). There was no statistically significant performance difference between OwAR and OwARSDS (p = 0.0682 for BioSemi, p = 0.1929 for ABM, and p = 0.3554 for Emotiv).\nIn summary, we have demonstrated that given the same number of labeled subject-specific training samples, OwAR and OwARSDS can significantly improve the online calibration performance. In other words, given a desired classification\n11\naccuracy, OwAR and OwARSDS can significantly reduce the number of labeled subject-specific samples. For example, in Fig. 6(a), the average BCA of BL2 was 72.34%, given 100 labeled subject-specific training samples. However, to achieve that BCA, on average OwAR only needed 15 samples, and OwARSDS only needed 20 samples, corresponding to 85% and 80% saving of labeling effort, respectively. Moreover, Fig. 6(a) also shows that, without using any labeled subjectspecific samples, OwAR and OwARSDS can achieve similar BCA as BL2 which used 60 labeled subject-specific samples. Similar observations can also be made for the ABM and Emotiv headsets."}, {"heading": "VII. COMPARISON OF OFFLINE AND ONLINE ALGORITHMS", "text": "This section compares the performances of wARSDS and OwARSDS (wAR and OwAR). Intuitively, we expect the performances of the offline calibration algorithms to be better than their online counterparts, because: 1) offline calibration uses all ml + mu EEG epochs to compute the PCA bases, whereas online calibration only uses ml epochs, so the PCA bases in offline calibration should be more representative; and, 2) offline calibration also uses the mu unlabeled epochs in the optimization, whereas online calibration does not, so offline calibration makes use of more information. In other words, offline calibration makes use of semi-supervised learning whereas online calibration does not.\nThe average performances of wAR, wARSDS, OwAR and OwARSDS across the 14 subjects are shown in Fig. 7. Observe that the results were consistent with our expectation: for all three headsets, the offline algorithms (wAR and wARSDS) achieved better BCAs than their online counterparts (OwAR and OwARSDS). Additionally, Fig. 7 shows that the algorithms had best performance using the BioSemi headset, and worst performance using the ABM headset. This is not surprising, as BioSemi used the most number of channels,\nand it was wired, which means better signal quality. The ABM headset had the least number of channels, and was wireless. Moreover, epochs with incorrect button presses were filtered out for BioSemi and Emotiv headsets, but not for most subjects for the ABM headset. So, the epochs in ABM were noisier.\nWe also performed statistical tests to check if the performance differences among the four algorithms were statistically significant. Friedman\u2019s test showed statistically significant differences among the four algorithms for BioSemi (df = 3, p = 0.00) and Emotiv (df = 3, p = 0.04), but not ABM (df = 3, p = 0.38). Dunn\u2019s procedure showed that for BioSemi the BCAs of wAR and OwAR were statistically significantly different (p = 0.0043), so were BCAs of wARSDS and OwARSDS (p = 0.0000). For ABM and Emotiv the performance differences between online and offline algorithms were not statistically significant (p = 0.5043 for ABM wAR vs OwAR, p = 0.1959 for ABM wARSDS vs OwARSDS, p = 0.0838 for Emotiv wAR vs OwAR, and p = 0.0514 for Emotiv wARSDS vs OwARSDS).\nIn conclusion, we have shown that generally the offline wAR and wARSDS algorithms, which include a semisupervised learning component, can achieve better calibration performance than the corresponding online OwAR and OwARSDS algorithms, i.e., semi-supervised learning is effective."}, {"heading": "VIII. CONCLUSIONS", "text": "Single-trial classification of ERPs in EEG signals is used in many BCI applications. However, because different subjects have different neural responses to even the same stimulus, it is very difficult to build a generic ERP classifier whose parameters fit all subjects. So, the classifier needs to be calibrated for each individual subject, using some labeled subjectspecific data. Reducing this calibration effort, i.e., minimizing\n12\nthe number of labeled subject-specific data required in the calibration, would greatly increase the utility of a BCI system. This paper introduced both online and offline wAR algorithms for this purpose. We have demonstrated using a VEP oddball task and three different EEG headsets that both algorithms can cope well with the class-imbalance problem, which is intrinsic in many real-world BCI applications, and they also significantly outperformed several other algorithms. We also compared the performances of the online and offline wAR algorithms in identical experimental settings and showed that the offline wAR algorithm, which includes an extra semisupervised component than the online wAR algorithm, can achieve better calibration performance, i.e., semi-supervised learning is effective in BCI calibration. Moreover, we further proposed a source domain selection approach, which can reduce the computational cost of both online and offline wAR algorithms by about 50%.\nWe expect our algorithms to find broad applications in various BCI calibration scenarios, and beyond. The most intuitive BCI calibration scenario, as described in this paper, is to reduce the subject-specific calibration data requirement by making use of relevant data from other subjects. Another scenario is to make use of the same subject\u2019s data from previous usages to facilitate a new calibration. For example, the subject may need to work on the same BCI task at different locations using different EEG headsets (office, home, etc.), or may upgrade a BCI game with a new EEG headset. In such applications, wAR can be used to make use of the data obtained from a previous EEG headset to facilitate the calibration for the new headset, as introduced in [51]. Of course, the above two scenarios can also be combined: auxiliary data from other subjects and from the subject himself/herself can be integrated to expedite the calibration. Furthermore, because of human-machine mutual adaptation and non-stationarity, a well-calibrated BCI system may degrade gradually. The proposed wAR algorithms can be used to re-calibrate it from time to time. Additionally, EEGs, together with many other body signals (facial expressions, speech, gesture, galvanic skin response, etc.), are also frequently used in affective computing [34], \u201ccomputing that relates to, arises from, or deliberately influences emotion or other affective phenomena.\u201d The wAR algorithms can also be used to handle individual differences and non-stationarity in such applications.\nFinally, we need to point out that the current wAR algorithms still have some limitations, which will be improved in our future research. First, we will develop incremental updating rules to reduce their computational cost. Second, we will develop criteria to determine when a negative transfer may occur, and hence use subject-only calibration data in such cases. Third, although wAR can map the features to a new kernel space to make them more consistent across the source and target domains, it still relies on good initial features. Simple PCA features were used in this paper. In our future research we will consider more sophisticated and robust features, e.g., the information geometry [2]."}, {"heading": "ACKNOWLEDGEMENT", "text": "The author would like to thank Scott Kerick, Jean Vettel, Anthony Ries, and David W. Hairston at the U.S. Army Research Laboratory (ARL) for designing the experiment and collecting the data, and Brent J. Lance and Vernon J. Lawhern from the ARL for helpful discussions.\nResearch was sponsored by the U.S. Army Research Laboratory and was accomplished under Cooperative Agreement Numbers W911NF-10-2-0022 and W911NF-10-D-0002/TO 0023. The views and the conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Army Research Laboratory or the U.S Government."}], "references": [{"title": "Multitask learning for brain-computer interfaces", "author": ["M. Alamgir", "M. Grosse-Wentrup", "Y. Altun"], "venue": "Proc. 13th Int\u2019l Conf. on Artificial Intelligence and Statistics (AISTATS), Sardinia, Italy, May 2010, pp. 17\u201324.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiclass braincomputer interface classification by Riemannian geometry", "author": ["A. Barachant", "S. Bonnet", "M. Congedo", "C. Jutten"], "venue": "IEEE Trans. on Biomedical Engineering, vol. 59, no. 4, pp. 920\u2013928, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning Research, vol. 7, pp. 2399\u20132434, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Controlling the false discovery rate: A practical and powerful approach to multiple testing", "author": ["Y. Benjamini", "Y. Hochberg"], "venue": "Journal of the Royal Statistical Society, Series B (Methodological), vol. 57, pp. 289\u2013 300, 1995.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Brain activity-based image classification from rapid serial visual presentation", "author": ["N. Bigdely-Shamlo", "A. Vankov", "R. Ramirez", "S. Makeig"], "venue": "IEEE Trans. on Neural Systems and Rehabilitation Engineering, vol. 16, no. 5, pp. 432\u2013441, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Amplitude differences of evoked alpha and gamma oscillations in two different age groups", "author": ["D. Bottger", "C.S. Herrmann", "D.Y. von Cramon"], "venue": "International Journal of Psychophysiology, vol. 45, pp. 245\u2013251, 2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Visual evoked potentials: Phenotypic and genotypic variability", "author": ["K.B. Bulayeva", "T.A. Pavlova", "G.G. Guseynov"], "venue": "Behavior Genetics, vol. 23, no. 5, pp. 443\u2013447, 1993.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1993}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Trans. on Intelligent Systems and Technology, vol. 2, no. 3, pp. 27:1\u201327:27, 2011, software available at http://www.csie.ntu.edu.tw/$\\sim$cjlin/libsvm.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "A two-stage model for multiple target detection in rapid serial visual presentation", "author": ["M.M. Chun", "M.C. Potter"], "venue": "Journal of Experimental Psychology: Human Perception and Performance, vol. 21, no. 1, pp. 109\u2013127, 1995.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Where and when the anterior cingulate cortex modulates attentional response: Combined fMRI and ERP evidence", "author": ["S. Crottaz-Herbette", "V. Menon"], "venue": "Journal of Cognitive Neuroscience, vol. 18, no. 5, pp. 766\u2013780, 2006.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis", "author": ["A. Delorme", "S. Makeig"], "venue": "Journal of Neuroscience Methods, vol. 134, pp. 9\u201321, 2004.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Multiple comparisons among means", "author": ["O. Dunn"], "venue": "Journal of the American Statistical Association, vol. 56, pp. 62\u201364, 1961.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1961}, {"title": "Multiple comparisons using rank sums", "author": ["O. Dunn"], "venue": "Technometrics, vol. 6, pp. 214\u2013252, 1964.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1964}, {"title": "A comparison of alternative tests of significance for the problem of m rankings", "author": ["M. Friedman"], "venue": "The Annals of Mathematical Statistics, vol. 11, no. 1, pp. 86\u201392, 1940.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1940}, {"title": "Beamforming in noninvasive brain computer interfaces", "author": ["M. Grosse-Wentrup", "C. Liefhold", "K. Gramann", "M. Buss"], "venue": "IEEE Trans. on Biomedical Engineering, vol. 56, no. 4, pp. 1209\u20131219, 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "What is odd in the oddball task? Prefrontal cortex is activated by dynamic changes in response strategy", "author": ["S.A. Huettel", "G. McCarthy"], "venue": "Neuropsychologia, vol. 42, pp. 379\u2013386, 2004.  13", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "The class imbalance problem: A systematic study", "author": ["N. Japkowicz", "S. Stephen"], "venue": "Intelligent Data Analysis, vol. 6, no. 5, pp. 429\u2013449, 2002.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Transfer learning in brain-computer interfaces", "author": ["V. Jayaram", "M. Alamgir", "Y. Altun", "B. Scholkopf", "M. Grosse- Wentrup"], "venue": "IEEE Computational Intelligence Magazine, vol. 11, no. 1, pp. 20\u201331, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "A P300 BCI for the masses: Prior information enables instant unsupervised spelling", "author": ["P.-J. Kindermans", "H. Verschore", "D. Verstraeten", "B. Schrauwen"], "venue": "Proc. Neural Information Processing Systems (NIPS), Lake Tahoe, NV, December 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Aging effect in pattern, motion and cognitive visual evoked potentials", "author": ["M. Kuba", "J. Kremlacek", "J. Langrova", "Z. Kubova", "J. Szanyi", "F. Vt"], "venue": "Vision Research, vol. 62, no. 1, pp. 9\u201316, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient labeling of EEG signal artifacts using active learning", "author": ["V.J. Lawhern", "D.J. Slayback", "D. Wu", "B.J. Lance"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man and Cybernetics, Hong Kong, October 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint feature re-extraction and classification using an iterative semi-supervised support vector machine algorithm", "author": ["Y. Li", "C. Guan"], "venue": "Machine Learning, vol. 71, no. 1, pp. 33\u201353, 2008.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Active class selection", "author": ["R. Lomasky", "C.E. Brodley", "M. Aernecke", "D. Walt", "M. Friedl"], "venue": "Proc. 18th European Conference on Machine Learning, Warsaw, Poland, September 2007, pp. 640\u2013647.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Adaptation regularization: A general framework for transfer learning", "author": ["M. Long", "J. Wang", "G. Ding", "S.J. Pan", "P.S. Yu"], "venue": "IEEE Trans. on Knowledge and Data Engineering, vol. 26, no. 5, pp. 1076\u20131089, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Class imbalance problem in data mining: Review", "author": ["R. Longadge", "S.S. Dongre", "L. Malik"], "venue": "Int\u2019l J. of Computer Science and Network, vol. 2, no. 1, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning from other subjects helps reducing brain-computer interface calibration time", "author": ["F. Lotte", "C. Guan"], "venue": "Proc. IEEE Int\u2019l. Conf. on Acoustics Speech and Signal Processing (ICASSP), Dallas, TX, March 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Regularizing common spatial patterns to improve BCI designs: Unified theory and new algorithms", "author": ["F. Lotte", "C. Guan"], "venue": "IEEE Trans. on Biomedical Engineering, vol. 58, no. 2, pp. 355\u2013362, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Signal processing approaches to minimize or suppress calibration time in oscillatory activity-based brain-computer interfaces", "author": ["F. Lotte"], "venue": "Proc. of the IEEE, vol. 103, no. 6, pp. 871\u2013890, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Evolving signal processing for brain-computer interfaces", "author": ["S. Makeig", "C. Kothe", "T. Mullen", "N. Bigdely-Shamlo", "Z. Zhang", "K. Kreutz-Delgado"], "venue": "Proc. of the IEEE, vol. 100, no. 3, pp. 1567\u20131584, 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Improved neural signal classification in a rapid serial visual presentation task using active learning", "author": ["A. Marathe", "V. Lawhern", "D. Wu", "D. Slayback", "B. Lance"], "venue": "IEEE Trans. on Neural Systems and Rehabilitation Engineering, vol. 24, no. 3, pp. 333\u2013343, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Trans. on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345\u20131359, 2010.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Spatiotemporal linear decoding of brain state", "author": ["L. Parra", "C. Christoforou", "A. Gerson", "M. Dyrholm", "A. Luo", "M. Wagner", "M. Philiastides", "P. Sajda"], "venue": "IEEE Signal Processing Magazine, vol. 25, no. 1, pp. 107\u2013115, 2008.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Affective Computing", "author": ["R. Picard"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1997}, {"title": "Closing the loop in cortically-coupled computer vision: a brain-computer interface for searching image databases", "author": ["E.A. Pohlmeyer", "J. Wang", "D.C. Jangraw", "B. Lou", "S.-F. Chang", "P. Sajda"], "venue": "Journal of Neural Engineering, vol. 8, no. 3, 2011.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Short-term conceptual memory for pictures", "author": ["M.C. Potter"], "venue": "Journal of Experimental Psychology: Human Learning and Memory, vol. 2, no. 5, pp. 509\u2013522, 1976.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1976}, {"title": "Machine learning from imbalanced data sets 101", "author": ["F. Provost"], "venue": "Tech. Rep. AAAI Technical Report WS-00-05, 2000.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2000}, {"title": "A comparison of electroencephalography signals acquired from conventional and mobile systems", "author": ["A.J. Ries", "J. Touryan", "J. Vettel", "K. McDowell", "W.D. Hairston"], "venue": "Journal of Neuroscience and Neuroengineering, vol. 3, no. 1, pp. 10\u201320, 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "In a blink of an eye and a switch of a transistor: Cortically coupled computer vision", "author": ["P. Sajda", "E. Pohlmeyer", "J. Wang", "L. Parra", "C. Christoforou", "J. Dmochowski", "B. Hanna", "C. Bahlmann", "M. Singh", "S.-F. Chang"], "venue": "Proc. of the IEEE, vol. 98, no. 3, pp. 462\u2013478, 2010.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "Transferring subspaces between subjects in brain-computer interfacing", "author": ["W. Samek", "F. Meinecke", "K.-R. Muller"], "venue": "IEEE Trans. on Biomedical Engineering, vol. 60, no. 8, pp. 2289\u20132298, 2013.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning with kernels: support vector machines, regularization, optimization, and beyond", "author": ["B. Scholkopf", "A.J. Smola"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2001}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "University of Wisconsin\u2013 Madison, Computer Sciences Technical Report 1648, 2009.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2009}, {"title": "Code of federal regulations protection of human subjects", "author": ["US Department of Defense Office of the Secretary of Defense"], "venue": "Government Printing Office, no. 32 CFR 19, 1999.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1999}, {"title": "Use of volunteers as subjects of research", "author": ["US Department of the Army"], "venue": "Government Printing Office, no. AR 70-25, 1990.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1990}, {"title": "Statistical Learning Theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1998}, {"title": "A review on transfer learning for brain-computer interface classification", "author": ["P. Wang", "J. Lu", "B. Zhang", "Z. Tang"], "venue": "Prof. 5th Int\u2019l Conf. on Information Science and Technology (IC1ST), Changsha, China, April 2015.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Online driver\u2019s drowsiness estimation using domain adaptation with model fusion", "author": ["D. Wu", "C.-H. Chuang", "C.-T. Lin"], "venue": "Proc. Int\u2019l Conf. on Affective Computing and Intelligent Interaction, Xi\u2019an, China, September 2015.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Active transfer learning for reducing calibration data in single-trial classification of visually-evoked potentials", "author": ["D. Wu", "B.J. Lance", "V.J. Lawhern"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man, and Cybernetics, San Diego, CA, October 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Collaborative filtering for braincomputer interaction using transfer learning and active class selection", "author": ["D. Wu", "B.J. Lance", "T.D. Parsons"], "venue": "PLoS ONE, 2013.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}, {"title": "Offline EEG-based driver drowsiness estimation using enhanced batch-mode active learning (EBMAL) for regression", "author": ["D. Wu", "V.J. Lawhern", "S. Gordon", "B.J. Lance", "C.-T. Lin"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man and Cybernetics, Budapest, Hungary, October 2016.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "Switching EEG headsets made easy: Reducing offline calibration effort using active weighted adaptation regularization", "author": ["D. Wu", "V.J. Lawhern", "W.D. Hairston", "B.J. Lance"], "venue": "IEEE Trans. on Neural Systems and Rehabilitation Engineering, 2016, in press.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2016}, {"title": "Reducing BCI calibration effort in RSVP tasks using online weighted adaptation regularization with source domain selection", "author": ["D. Wu", "V.J. Lawhern", "B.J. Lance"], "venue": "Proc. Int\u2019l Conf. on Affective Computing and Intelligent Interaction, Xi\u2019an, China, September 2015.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Reducing offline BCI calibration effort using weighted adaptation regularization with source domain selection", "author": ["D. Wu", "V.J. Lawhern", "B.J. Lance"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man and Cybernetics, Hong Kong, October 2015.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards passive brain-computer interfaces: applying brain-computer interface technology to human-machine systems in general", "author": ["T.O. Zander", "C. Kothe"], "venue": "Journal of Neural Engineering, vol. 8, no. 2, 2011.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 4, "context": "Many real-world brain-computer interface (BCI) applications rely on single-trial classification of event-related potentials (ERPs) in EEG signals [5], [39].", "startOffset": 146, "endOffset": 149}, {"referenceID": 37, "context": "Many real-world brain-computer interface (BCI) applications rely on single-trial classification of event-related potentials (ERPs) in EEG signals [5], [39].", "startOffset": 151, "endOffset": 155}, {"referenceID": 8, "context": "210 Hz) [10], [36], and the subject needs to detect some target images in them.", "startOffset": 8, "endOffset": 12}, {"referenceID": 34, "context": "210 Hz) [10], [36], and the subject needs to detect some target images in them.", "startOffset": 14, "endOffset": 18}, {"referenceID": 33, "context": "The P300 ERPs can be detected by a BCI system [35], and the corresponding images are then triaged for further inspection.", "startOffset": 46, "endOffset": 50}, {"referenceID": 31, "context": "Research [33], [39], [54] has shown that these BCI systems enable the subject to detect targets in large aerial photographs faster and more accurately than traditional standard searches.", "startOffset": 9, "endOffset": 13}, {"referenceID": 37, "context": "Research [33], [39], [54] has shown that these BCI systems enable the subject to detect targets in large aerial photographs faster and more accurately than traditional standard searches.", "startOffset": 15, "endOffset": 19}, {"referenceID": 52, "context": "Research [33], [39], [54] has shown that these BCI systems enable the subject to detect targets in large aerial photographs faster and more accurately than traditional standard searches.", "startOffset": 21, "endOffset": 25}, {"referenceID": 5, "context": "Unfortunately, because different subjects have different neural responses to even the same visual stimulus [6], [7], [21], it is very difficult, if not impossible, to build a generic ERP classifier whose parameters fit all subjects.", "startOffset": 107, "endOffset": 110}, {"referenceID": 6, "context": "Unfortunately, because different subjects have different neural responses to even the same visual stimulus [6], [7], [21], it is very difficult, if not impossible, to build a generic ERP classifier whose parameters fit all subjects.", "startOffset": 112, "endOffset": 115}, {"referenceID": 19, "context": "Unfortunately, because different subjects have different neural responses to even the same visual stimulus [6], [7], [21], it is very difficult, if not impossible, to build a generic ERP classifier whose parameters fit all subjects.", "startOffset": 117, "endOffset": 121}, {"referenceID": 40, "context": "Additionally, in offline calibration we can query any epoch in the pool for the label (an optimal query strategy can hence be designed by using machine learning methods such as active learning [42], [48]), but in online calibration usually the sequence of the epochs is pre-determined and the subject has no control over which epoch he/she will see next.", "startOffset": 193, "endOffset": 197}, {"referenceID": 46, "context": "Additionally, in offline calibration we can query any epoch in the pool for the label (an optimal query strategy can hence be designed by using machine learning methods such as active learning [42], [48]), but in online calibration usually the sequence of the epochs is pre-determined and the subject has no control over which epoch he/she will see next.", "startOffset": 199, "endOffset": 203}, {"referenceID": 27, "context": "Many signal processing and machine learning approaches have been proposed to reduce the BCI calibration effort [29], [30].", "startOffset": 111, "endOffset": 115}, {"referenceID": 28, "context": "Many signal processing and machine learning approaches have been proposed to reduce the BCI calibration effort [29], [30].", "startOffset": 117, "endOffset": 121}, {"referenceID": 27, "context": "They may be grouped into five categories [29]:", "startOffset": 41, "endOffset": 45}, {"referenceID": 39, "context": "1) Regularization, which is a very effective machine learning approach for constructing robust models [41], especially when the training data size is small.", "startOffset": 102, "endOffset": 106}, {"referenceID": 25, "context": "A popular regularization approach in BCI calibration is shrinkage [27], which gives a regularized estimate of the covariance matrices.", "startOffset": 66, "endOffset": 70}, {"referenceID": 17, "context": "2) Transfer/multi-task learning, which uses relevant data from other subjects to help the current subject [19], [46].", "startOffset": 106, "endOffset": 110}, {"referenceID": 44, "context": "2) Transfer/multi-task learning, which uses relevant data from other subjects to help the current subject [19], [46].", "startOffset": 112, "endOffset": 116}, {"referenceID": 30, "context": "The transfer learning (TL) [32] based approaches are particularly popular [1], [19], [20], [40], [46], [48],", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "The transfer learning (TL) [32] based approaches are particularly popular [1], [19], [20], [40], [46], [48],", "startOffset": 74, "endOffset": 77}, {"referenceID": 17, "context": "The transfer learning (TL) [32] based approaches are particularly popular [1], [19], [20], [40], [46], [48],", "startOffset": 79, "endOffset": 83}, {"referenceID": 18, "context": "The transfer learning (TL) [32] based approaches are particularly popular [1], [19], [20], [40], [46], [48],", "startOffset": 85, "endOffset": 89}, {"referenceID": 38, "context": "The transfer learning (TL) [32] based approaches are particularly popular [1], [19], [20], [40], [46], [48],", "startOffset": 91, "endOffset": 95}, {"referenceID": 44, "context": "The transfer learning (TL) [32] based approaches are particularly popular [1], [19], [20], [40], [46], [48],", "startOffset": 97, "endOffset": 101}, {"referenceID": 46, "context": "The transfer learning (TL) [32] based approaches are particularly popular [1], [19], [20], [40], [46], [48],", "startOffset": 103, "endOffset": 107}, {"referenceID": 47, "context": "[49], [52], [53], because in many BCI applications we can easily find legacy data from the same subject in the same task or similar tasks, or legacy data from different subjects in the same task or similar tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[49], [52], [53], because in many BCI applications we can easily find legacy data from the same subject in the same task or similar tasks, or legacy data from different subjects in the same task or similar tasks.", "startOffset": 6, "endOffset": 10}, {"referenceID": 51, "context": "[49], [52], [53], because in many BCI applications we can easily find legacy data from the same subject in the same task or similar tasks, or legacy data from different subjects in the same task or similar tasks.", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "3) Adaptive learning, which refines the machine learning model as new (labeled or unlabeled) subject-specific data are available [23], [52], [53].", "startOffset": 129, "endOffset": 133}, {"referenceID": 50, "context": "3) Adaptive learning, which refines the machine learning model as new (labeled or unlabeled) subject-specific data are available [23], [52], [53].", "startOffset": 135, "endOffset": 139}, {"referenceID": 51, "context": "3) Adaptive learning, which refines the machine learning model as new (labeled or unlabeled) subject-specific data are available [23], [52], [53].", "startOffset": 141, "endOffset": 145}, {"referenceID": 20, "context": "4) Active learning, which optimally selects the most informative unlabeled samples to label [22], [31], [48], [50].", "startOffset": 92, "endOffset": 96}, {"referenceID": 29, "context": "4) Active learning, which optimally selects the most informative unlabeled samples to label [22], [31], [48], [50].", "startOffset": 98, "endOffset": 102}, {"referenceID": 46, "context": "4) Active learning, which optimally selects the most informative unlabeled samples to label [22], [31], [48], [50].", "startOffset": 104, "endOffset": 108}, {"referenceID": 48, "context": "4) Active learning, which optimally selects the most informative unlabeled samples to label [22], [31], [48], [50].", "startOffset": 110, "endOffset": 114}, {"referenceID": 40, "context": "There are many criteria to determine which unlabeled samples are the most informative [42].", "startOffset": 86, "endOffset": 90}, {"referenceID": 22, "context": "However, a closely related technique, active class selection [24], can be used for online BCI calibration [49].", "startOffset": 61, "endOffset": 65}, {"referenceID": 47, "context": "However, a closely related technique, active class selection [24], can be used for online BCI calibration [49].", "startOffset": 106, "endOffset": 110}, {"referenceID": 26, "context": "For example, prior information on which EEG channels are the most likely to be useful was used in [28] as a regularizer to optimize spatial filters, and beamforming has been used in [16] to find relevant features from prior regions of interest to reduce the calibration data requirement.", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "For example, prior information on which EEG channels are the most likely to be useful was used in [28] as a regularizer to optimize spatial filters, and beamforming has been used in [16] to find relevant features from prior regions of interest to reduce the calibration data requirement.", "startOffset": 182, "endOffset": 186}, {"referenceID": 26, "context": "An optimal spatial filters was designed in [28] for efficient subjectto-subject transfer by combining regularization and a prior information on which channels are the most likely to be useful.", "startOffset": 43, "endOffset": 47}, {"referenceID": 47, "context": "A collaborative filtering approach was developed in [49], which combined TL and active class selection to minimize the online calibration effort.", "startOffset": 52, "endOffset": 56}, {"referenceID": 46, "context": "An active TL approach was proposed in [48] for offline calibration, which combined TL and active learning to minimize the offline calibration effort.", "startOffset": 38, "endOffset": 42}, {"referenceID": 49, "context": "An active weighted adaptation regularization approach, which combines active learning, TL, regularization and semi-supervised learning, was proposed in [51] to facilitate the switching between different EEG headsets.", "startOffset": 152, "endOffset": 156}, {"referenceID": 45, "context": "For example, a domain adaptation with model fusion approach, which combines regularization and TL, was developed in [47] to estimate driver\u2019s drowsiness online continuously.", "startOffset": 116, "endOffset": 120}, {"referenceID": 50, "context": "This paper presents a comprehensive overview and comparison of the offline and online weighted adaptation regularization with source domain selection (wARSDS) algorithms, which we proposed recently [52], [53].", "startOffset": 198, "endOffset": 202}, {"referenceID": 51, "context": "This paper presents a comprehensive overview and comparison of the offline and online weighted adaptation regularization with source domain selection (wARSDS) algorithms, which we proposed recently [52], [53].", "startOffset": 204, "endOffset": 208}, {"referenceID": 51, "context": "The offline wARSDS algorithm, which combined TL, regularization, and semisupervised learning, was first developed in [53] for offline single-trial classification of ERPs in a visually-evoked potential (VEP) oddball task.", "startOffset": 117, "endOffset": 121}, {"referenceID": 50, "context": "It was later extended to online calibration in a RSVP task [52], which still includes TL and regularization but not semi-supervised learning because unlabeled samples are not available in online calibration.", "startOffset": 59, "endOffset": 63}, {"referenceID": 49, "context": "This section describes the offline wARSDS algorithm [51], [53], which originates from the adaptation regularization \u2013 regularized least squares (ARRLS) algorithm in [25].", "startOffset": 52, "endOffset": 56}, {"referenceID": 51, "context": "This section describes the offline wARSDS algorithm [51], [53], which originates from the adaptation regularization \u2013 regularized least squares (ARRLS) algorithm in [25].", "startOffset": 58, "endOffset": 62}, {"referenceID": 23, "context": "This section describes the offline wARSDS algorithm [51], [53], which originates from the adaptation regularization \u2013 regularized least squares (ARRLS) algorithm in [25].", "startOffset": 165, "endOffset": 169}, {"referenceID": 23, "context": "A domain [25], [32] D in TL consists of a multi-dimensional feature space X and a marginal probability distribution P (x), i.", "startOffset": 9, "endOffset": 13}, {"referenceID": 30, "context": "A domain [25], [32] D in TL consists of a multi-dimensional feature space X and a marginal probability distribution P (x), i.", "startOffset": 15, "endOffset": 19}, {"referenceID": 23, "context": "A task [25], [32] T in TL consists of a label space Y and a conditional probability distribution Q(y|x).", "startOffset": 7, "endOffset": 11}, {"referenceID": 30, "context": "A task [25], [32] T in TL consists of a label space Y and a conditional probability distribution Q(y|x).", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": ", Ps(x) 6= Pt(x) and Qs(y|x) 6= Qt(y|x), because the two subjects usually have different neural responses to the same visual stimulus [6], [7], [21].", "startOffset": 134, "endOffset": 137}, {"referenceID": 6, "context": ", Ps(x) 6= Pt(x) and Qs(y|x) 6= Qt(y|x), because the two subjects usually have different neural responses to the same visual stimulus [6], [7], [21].", "startOffset": 139, "endOffset": 142}, {"referenceID": 19, "context": ", Ps(x) 6= Pt(x) and Qs(y|x) 6= Qt(y|x), because the two subjects usually have different neural responses to the same visual stimulus [6], [7], [21].", "startOffset": 144, "endOffset": 148}, {"referenceID": 49, "context": "As in [51], [53], the learning framework of wAR is formulated as:", "startOffset": 6, "endOffset": 10}, {"referenceID": 51, "context": "As in [51], [53], the learning framework of wAR is formulated as:", "startOffset": 12, "endOffset": 16}, {"referenceID": 16, "context": "This is very important, because class imbalance is intrinsic to many applications [18], particularly BCI applications.", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "Of course, there are many other approaches for handling class imbalance [18], [26], [37].", "startOffset": 72, "endOffset": 76}, {"referenceID": 24, "context": "Of course, there are many other approaches for handling class imbalance [18], [26], [37].", "startOffset": 78, "endOffset": 82}, {"referenceID": 35, "context": "Of course, there are many other approaches for handling class imbalance [18], [26], [37].", "startOffset": 84, "endOffset": 88}, {"referenceID": 2, "context": "According to the Representer Theorem [3], [25], the solution of (2) can be expressed as:", "startOffset": 37, "endOffset": 40}, {"referenceID": 23, "context": "According to the Representer Theorem [3], [25], the solution of (2) can be expressed as:", "startOffset": 42, "endOffset": 46}, {"referenceID": 23, "context": "As in [25], we define the structural risk as the squared norm of f in HK , i.", "startOffset": 6, "endOffset": 10}, {"referenceID": 23, "context": "As in [25], we compute Df,K(Ps, Pt) using the projected maximum mean discrepancy (MMD) between the source and target domains:", "startOffset": 6, "endOffset": 10}, {"referenceID": 23, "context": "As in [25], we first compute pseudo labels for the unlabeled target domain samples and construct the label vector y in (7).", "startOffset": 6, "endOffset": 10}, {"referenceID": 43, "context": ", a support vector machine (SVM) [45].", "startOffset": 33, "endOffset": 37}, {"referenceID": 51, "context": "So, we introduce a source domain selection approach [53], which selects the closest source domains to reduce the computational cost, and also to (potentially) improve the classification performance.", "startOffset": 52, "endOffset": 56}, {"referenceID": 23, "context": "As mentioned at the beginning of this section, the formulation and derivation of wAR closely resemble the ARRLS algorithm in [25]; however, there are several major differences:", "startOffset": 125, "endOffset": 129}, {"referenceID": 2, "context": "3) ARRLS also includes manifold regularization [3].", "startOffset": 47, "endOffset": 50}, {"referenceID": 50, "context": "This section introduces the OwARSDS algorithm [52], which extends the offline wARSDS algorithm to online BCI calibration.", "startOffset": 46, "endOffset": 50}, {"referenceID": 50, "context": "It has been shown [52] that the closed-form solution for \u03b1 is:", "startOffset": 18, "endOffset": 22}, {"referenceID": 49, "context": "The offline wARSDS algorithm [51], [53].", "startOffset": 29, "endOffset": 33}, {"referenceID": 51, "context": "The offline wARSDS algorithm [51], [53].", "startOffset": 35, "endOffset": 39}, {"referenceID": 50, "context": "The online OwARSDS algorithm [52].", "startOffset": 29, "endOffset": 33}, {"referenceID": 36, "context": "A two-stimulus VEP oddball task was used [38].", "startOffset": 41, "endOffset": 45}, {"referenceID": 41, "context": "The voluntary, fully informed consent of the persons used in this research was obtained as required by federal and Army regulations [43], [44].", "startOffset": 132, "endOffset": 136}, {"referenceID": 42, "context": "The voluntary, fully informed consent of the persons used in this research was obtained as required by federal and Army regulations [43], [44].", "startOffset": 138, "endOffset": 142}, {"referenceID": 46, "context": "The preprocessing and feature extraction method for all three headsets was the same, except that for ABM and Emotiv headsets we used all the channels, but for the BioSemi headset we only used 21 channels (Cz, Fz, P1, P3, P5, P7, P9, PO7, PO3, O1, Oz, POz, Pz, P2, P4, P6, P8, P10, PO8, PO4, O2) mainly in the parietal and occipital areas, as in [48].", "startOffset": 345, "endOffset": 349}, {"referenceID": 10, "context": "EEGLAB [12] was used for EEG signal preprocessing and feature extraction.", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "For each headset, we first band-passed the EEG signals to [1, 50] Hz, then downsampled them to 64 Hz, performed average reference, and next epoched them to the [0, 0.", "startOffset": 58, "endOffset": 65}, {"referenceID": 48, "context": "For each headset, we first band-passed the EEG signals to [1, 50] Hz, then downsampled them to 64 Hz, performed average reference, and next epoched them to the [0, 0.", "startOffset": 58, "endOffset": 65}, {"referenceID": 9, "context": "Similar experimental settings have been used in [11], [17].", "startOffset": 48, "endOffset": 52}, {"referenceID": 15, "context": "Similar experimental settings have been used in [11], [17].", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "We then normalized each feature dimension separately to [0, 1].", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "from the 14th subject; Set ; Generate a random number 0 [1, ] m m \u2208", "startOffset": 56, "endOffset": 61}, {"referenceID": 0, "context": "from the 14th subject; Set ; Generate a random number 0 [1, ] m m \u2208", "startOffset": 56, "endOffset": 61}, {"referenceID": 51, "context": "Algorithms We compared the performance of wARSDS with six other algorithms [53]: 1) BL1, a baseline approach in which we assume we know labels of all samples from the new subject, and use 5fold cross-validation and SVM to find the highest BCA.", "startOffset": 75, "endOffset": 79}, {"referenceID": 46, "context": "3) TL, which is the TL algorithm introduced in [48].", "startOffset": 47, "endOffset": 51}, {"referenceID": 23, "context": "5) ARRLS, which was proposed in [25] (manifold regularization was removed), and is also the wAR algorithm introduced in Algorithm 1, by setting wt = ws,i = wt,i = 1.", "startOffset": 32, "endOffset": 36}, {"referenceID": 7, "context": "Weighted libSVM [8] with RBF kernel was used as the classifier in BL1, BL2, TL and TLSDS.", "startOffset": 16, "endOffset": 19}, {"referenceID": 23, "context": "1, and \u03bb = 10, following the practice in [25], [51], [53].", "startOffset": 41, "endOffset": 45}, {"referenceID": 49, "context": "1, and \u03bb = 10, following the practice in [25], [51], [53].", "startOffset": 47, "endOffset": 51}, {"referenceID": 51, "context": "1, and \u03bb = 10, following the practice in [25], [51], [53].", "startOffset": 53, "endOffset": 57}, {"referenceID": 49, "context": "As in [51], [53], we also performed comprehensive statistical tests to check if the performance differences among the six algorithms (BL1 was not included because it is not iterative) were statistically significant.", "startOffset": 6, "endOffset": 10}, {"referenceID": 51, "context": "As in [51], [53], we also performed comprehensive statistical tests to check if the performance differences among the six algorithms (BL1 was not included because it is not iterative) were statistically significant.", "startOffset": 12, "endOffset": 16}, {"referenceID": 29, "context": "We used the area-underperformance-curve (AUPC) [31], [51], [53] to assess overall performance differences among these algorithms.", "startOffset": 47, "endOffset": 51}, {"referenceID": 49, "context": "We used the area-underperformance-curve (AUPC) [31], [51], [53] to assess overall performance differences among these algorithms.", "startOffset": 53, "endOffset": 57}, {"referenceID": 51, "context": "We used the area-underperformance-curve (AUPC) [31], [51], [53] to assess overall performance differences among these algorithms.", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "The AUPC is the area under the curve of the BCAs obtained at each of the 30 runs, and is normalized to [0, 1].", "startOffset": 103, "endOffset": 109}, {"referenceID": 13, "context": "As a result, we used Friedman\u2019s test [15], a two-way non-parametric ANOVA where column effects are tested for significant differences after adjusting for possible row effects.", "startOffset": 37, "endOffset": 41}, {"referenceID": 46, "context": "5Note that this does not conflict with the observation in [48], which said TL was better than BL2.", "startOffset": 58, "endOffset": 62}, {"referenceID": 46, "context": "This is because different datasets were used in the two studies: [48] downsampled the non-target class to balance the two classes before testing the performances of different algorithms, whereas classimbalance was preserved in this paper.", "startOffset": 65, "endOffset": 69}, {"referenceID": 46, "context": "Moreover, [48] only considered the BioSemi headset, and showed that TL outperformed BL2, but the performance difference between TL and BL2 decreased as ml increases.", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "Then, non-parametric multiple comparison tests using Dunn\u2019s procedure [13], [14] was used to determine if the difference between any pair of algorithms was statistically significant, with a p-value correction using the false discovery rate method [4].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "Then, non-parametric multiple comparison tests using Dunn\u2019s procedure [13], [14] was used to determine if the difference between any pair of algorithms was statistically significant, with a p-value correction using the false discovery rate method [4].", "startOffset": 76, "endOffset": 80}, {"referenceID": 3, "context": "Then, non-parametric multiple comparison tests using Dunn\u2019s procedure [13], [14] was used to determine if the difference between any pair of algorithms was statistically significant, with a p-value correction using the false discovery rate method [4].", "startOffset": 247, "endOffset": 250}, {"referenceID": 8, "context": "5(b) that both wAR and wARSDS achieved good BCAs for \u03bbP \u2208 [10, 100] and \u03bbQ \u2208 [10, 100].", "startOffset": 58, "endOffset": 67}, {"referenceID": 8, "context": "5(b) that both wAR and wARSDS achieved good BCAs for \u03bbP \u2208 [10, 100] and \u03bbQ \u2208 [10, 100].", "startOffset": 77, "endOffset": 86}, {"referenceID": 7, "context": "Again, weighted libSVM [8] with RBF kernel was used as the classifier in BL1, BL2, TL and TLSDS.", "startOffset": 23, "endOffset": 26}, {"referenceID": 49, "context": "In such applications, wAR can be used to make use of the data obtained from a previous EEG headset to facilitate the calibration for the new headset, as introduced in [51].", "startOffset": 167, "endOffset": 171}, {"referenceID": 32, "context": "), are also frequently used in affective computing [34], \u201ccomputing that relates to, arises from, or deliberately influences emotion or other affective phenomena.", "startOffset": 51, "endOffset": 55}, {"referenceID": 1, "context": ", the information geometry [2].", "startOffset": 27, "endOffset": 30}], "year": 2017, "abstractText": "Many real-world brain-computer interface (BCI) applications rely on single-trial classification of event-related potentials (ERPs) in EEG signals. However, because different subjects have different neural responses to even the same stimulus, it is very difficult to build a generic ERP classifier whose parameters fit all subjects. The classifier needs to be calibrated for each individual subject, using some labeled subject-specific data. This paper proposes both online and offline weighted adaptation regularization (wAR) algorithms to reduce this calibration effort, i.e., to minimize the amount of labeled subjectspecific EEG data required in BCI calibration, and hence to increase the utility of the BCI system. We demonstrate using a visually-evoked potential oddball task and three different EEG headsets that both online and offline wAR algorithms significantly outperform several other algorithms. Moreover, through source domain selection, we can reduce their computational cost by about 50%, making them more suitable for real-time applications.", "creator": "LaTeX with hyperref package"}}}