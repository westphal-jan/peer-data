{"id": "1702.03082", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2017", "title": "UsingWord Embedding for Cross-Language Plagiarism Detection", "abstract": "This paper proposes to use distributed representation of words (word embeddings) in cross-language textual similarity detection. The main contributions of this paper are the following: (a) we introduce new cross-language similarity detection methods based on distributed representation of words; (b) we combine the different methods proposed to verify their complementarity and finally obtain an overall F1 score of 89.15% for English-French similarity detection at chunk level (88.14%).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 10 Feb 2017 07:22:08 GMT  (287kb,D)", "http://arxiv.org/abs/1702.03082v1", "Accepted to EACL 2017 (short)"]], "COMMENTS": "Accepted to EACL 2017 (short)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["j ferrero", "f agnes", "l besacier", "d schwab"], "accepted": false, "id": "1702.03082"}, "pdf": {"name": "1702.03082.pdf", "metadata": {"source": "CRF", "title": "Using Word Embedding for Cross-Language Plagiarism Detection", "authors": ["J\u00e9r\u00e9my Ferrero", "Didier Schwab"], "emails": ["jeremy.ferrero@imag.fr", "frederic@compilatio.net", "laurent.besacier@imag.fr", "didier.schwab@imag.fr"], "sections": [{"heading": "1 Introduction", "text": "Plagiarism is a very significant problem nowadays, specifically in higher education institutions. In monolingual context, this problem is rather well treated by several recent researches (Potthast et al., 2014). Nevertheless, the expansion of the Internet, which facilitates access to documents throughout the world and to increasingly efficient (freely available) machine translation tools, helps to spread cross-language plagiarism. Cross-language plagiarism means plagiarism by translation, i.e. a text has been plagiarized while being translated (manually or automatically). The challenge in detecting this kind of plagiarism is that the suspicious document is no longer in the same language of its source. We investigate how distributed representations of words can help to\npropose new cross-lingual similarity measures, helpful for plagiarism detection. We use word embeddings (Mikolov et al., 2013) that have shown promising performances for all kinds of NLP tasks, as shown in Upadhyay et al. (2016), Ammar et al. (2016) and Ghannay et al. (2016), for instance.\nContributions. The main contributions of this paper are the following:\n\u2022 we augment some state-of-the-art methods with the use of word embeddings instead of lexical resources;\n\u2022 we introduce a syntax weighting in distributed representations of sentences, and prove its usefulness for textual similarity detection;\n\u2022 we combine our methods to verify their complementarity and finally obtain an overall F1 score of 89.15% for English-French similarity detection at chunk level (88.5% at sentence level) on a very challenging corpus (mix of Wikipedia, conference papers, product reviews, Europarl and JRC) while the best method alone hardly reaches F1 score higher than 50%."}, {"heading": "2 Evaluation Conditions", "text": ""}, {"heading": "2.1 Dataset", "text": "The reference dataset used during our study is the new dataset recently introduced by Ferrero et al.\nar X\niv :1\n70 2.\n03 08\n2v 1\n[ cs\n.C L\n] 1\n0 Fe\n(2016)1. The dataset was specially designed for a rigorous evaluation of cross-language textual similarity detection.\nMore precisely, the characteristics of the dataset are the following:\n\u2022 it is multilingual: it contains French, English and Spanish texts; \u2022 it proposes cross-language alignment infor-\nmation at different granularities: document level, sentence level and chunk level; \u2022 it is based on both parallel and comparable\ncorpora (mix of Wikipedia, conference papers, product reviews, Europarl and JRC); \u2022 it contains both human and machine trans-\nlated texts; \u2022 it contains different percentages of named en-\ntities; \u2022 part of it has been obfuscated (to make\nthe cross-language similarity detection more complicated) while the rest remains without noise; \u2022 the documents were written and translated by\nmultiple types of authors (from average to professionals) and cover various fields.\nIn this paper, we only use the French and English sub-corpora."}, {"heading": "2.2 Overview of State-of-the-Art Methods", "text": "Plagiarism is a statement that someone copied text deliberately without attribution, while these methods only detect textual similarities. However, textual similarity detection can be used to detect plagiarism.\nThe aim of cross-language textual similarity detection is to estimate if two textual units in different languages express the same or not. We quickly review below the state-of-the-art methods used in this paper, for more details, see Ferrero et al. (2016).\nCross-Language Character N-Gram (CL-CnG) is based on Mcnamee and Mayfield (2004) model. We use the Potthast et al. (2011) implementation which compares two textual units under their 3-grams vectors representation.\nCross-Language Conceptual Thesaurus-based Similarity (CL-CTS) (Pataki, 2012) aims to measure the semantic similarity using abstract con-\n1https://github.com/FerreroJeremy/ Cross-Language-Dataset\ncepts from words in textual units. In our implementation, these concepts are given by a linked lexical resource called DBNary (Se\u0301rasset, 2015).\nCross-Language Alignment-based Similarity Analysis (CL-ASA) aims to determinate how a textual unit is potentially the translation of another textual unit using bilingual unigram dictionary which contains translations pairs (and their probabilities) extracted from a parallel corpus (Barro\u0301nCeden\u0303o et al. (2008), Pinto et al. (2009)).\nCross-Language Explicit Semantic Analysis (CL-ESA) is based on the explicit semantic analysis model (Gabrilovich and Markovitch, 2007), which represents the meaning of a document by a vector based on concepts derived from Wikipedia. It was reused by Potthast et al. (2008) in the context of cross-language document retrieval.\nTranslation + Monolingual Analysis (T+MA) consists in translating the two units into the same language, in order to operate a monolingual comparison between them (Barro\u0301n-Ceden\u0303o, 2012). We use the Muhr et al. (2010) approach using DBNary (Se\u0301rasset, 2015), followed by monolingual matching based on bags of words."}, {"heading": "2.3 Evaluation Protocol", "text": "We apply the same evaluation protocol as in Ferrero et al. (2016)\u2019s paper. We build a distance matrix of size N xM , with M = 1,000 and N = |S| where S is the evaluated sub-corpus. Each textual unit of S is compared to itself (to its corresponding unit in the target language, since this is cross-lingual similarity detection) and to M -1 other units randomly selected from S. The same unit may be selected several times. Then, a matching score for each comparison performed is obtained, leading to the distance matrix. Thresholding on the matrix is applied to find the threshold giving the best F1 score. The F1 score is the harmonic mean of precision and recall. Precision is defined as the proportion of relevant matches (similar cross-language units) retrieved among all the matches retrieved. Recall is the proportion of relevant matches retrieved among all the relevant matches to retrieve. Each method is applied on each EN-FR sub-corpus for chunk and sentence granularities. For each configuration (i.e. a particular method applied on a particular sub-corpus considering a particular granularity), 10 folds are carried out by changing the M selected units."}, {"heading": "3 Proposed Methods", "text": "The main idea of word embeddings is that their representation is obtained according to the context (the words around it). The words are projected on a continuous space and those with similar context should be close in this multi-dimensional space. A similarity between two word vectors can be measured by cosine similarity. So using wordembeddings for plagiarism detection is appealing since they can be used to calculate similarity between sentences in the same or in two different languages (they capture intrinsically synonymy and morphological closeness). We use the MultiVec (Berard et al., 2016) toolkit for computing and managing the continuous representations of the texts. It includes word2vec (Mikolov et al., 2013), paragraph vector (Le and Mikolov, 2014) and bilingual distributed representations (Luong et al., 2015) features. The corpus used to build the vectors is the News Commentary2 parallel corpus. For training our embeddings, we use CBOW model with a vector size of 100, a window size of 5, a negative sampling parameter of 5, and an alpha of 0.02."}, {"heading": "3.1 Improving Textual Similarity Using", "text": "Word Embeddings (CL-CTS-WE and CL-WES)\nWe introduce two new methods. First, we propose to replace the lexical resource used in CL-CTS (i.e. DBNary) by distributed representation of words. We call this new implementation CL-CTS-WE. More precisely, CL-CTS-WE uses the top 10 closest words in the embeddings model to build the BOW of a word. Secondly, we implement a more straightforward method (CL-WES), which performs a direct comparison between two sentences in different languages, through the use of word embeddings. It consists in a cosine similarity on distributed representations of the sentences, which are the summation of the embeddings vectors of each word of the sentences.\nLet U a textual unit, the n words of the unit are represented by ui as:\nU = {u1, u2, u3, ..., un} (1)\nIf Ux and Uy are two textual units in two different languages, CL-WES builds their (bilingual)\n2http://www.statmt.org/wmt14/ translation-task.html\ncommon representation vectors Vx and Vy and applies a cosine similarity between them.\nA distributed representation V of a textual unit U is calculated as follows:\nV = n\u2211 i=1 (vector(ui)) (2)\nwhere ui is the ith word of the textual unit and vector is the function which gives the word embedding vector of a word. This feature is available in MultiVec3 (Berard et al., 2016)."}, {"heading": "3.2 Cross-Language Word Embedding-based Syntax Similarity (CL-WESS)", "text": "Our next innovation is the improvement of CL-WES by introducing a syntax flavour in it. Let U a textual unit, the n words of the unit are represented by ui as expressed in the formula (1). First, we syntactically tag U with a part-of-speech tagger (TreeTagger (Schmid, 1994)) and we normalize the tags with Universal Tagset of Petrov et al. (2012). Then, we assign a weight to each type of tag: this weight will be used to compute the final vector representation of the unit. Finally, we optimize the weights with the help of Condor (Berghen and Bersini, 2005). Condor applies a Newton\u2019s method with a trust region algorithm to determinate the weights that optimize the F1 score. We use the first two folds of each sub-corpus to determinate the optimal weights.\nThe formula of the syntactic aggregation is:\nV = n\u2211 i=1 (weight(pos(ui)).vector(ui)) (3)\nwhere ui is the ith word of the textual unit, pos is the function which gives the universal part-ofspeech tag of a word, weight is the function which gives the weight of a part-of-speech, vector is the function which gives the word embedding vector of a word and . is the scalar product.\nIf Ux and Uy are two textual units in two different languages, we build their representation vectors Vx and Vy following the formula (3) instead of (2), and apply a cosine similarity between them. We call this method CL-WESS and we have implemented it in MultiVec (Berard et al., 2016).\nIt is important to note that, contrarily to what is done in other tasks such as neural parsing (Chen\n3https://github.com/eske/multivec\nand Manning, 2014), we did not use POS information as an additional vector input because we considered it would be more useful to use it to weight the contribution of each word to the sentence representation, according to its morpho-syntactic category."}, {"heading": "4 Combining multiple methods", "text": ""}, {"heading": "4.1 Weighted Fusion", "text": "We try to combine our methods to improve crosslanguage similarity detection performance. During weighted fusion, we assign one weight to the similarity score of each method and we calculate a (weighted) composite score. We optimize the distribution of the weights with Condor (Berghen and Bersini, 2005). We use the first two folds of each sub-corpus to determinate the optimal weights, while the other eight folds evaluate the fusion. We also try an average fusion, i.e. a weighted fusion where all the weights are equal."}, {"heading": "4.2 Decision Tree Fusion", "text": "Regardless of their capacity to predict a (mis)match, an interesting feature of the methods is their clustering capacity, i.e. their ability to correctly separate the positives (similar units) and the negatives (different units) in order to minimize the doubts on the classification. Distribution histograms on Figure 1 highlight the fact that each method has its own fingerprint. Even if two methods look equivalent in term of final performance, their distribution can be different. One explanation is that the methods do not process on the same way. Some methods are lexical-syntax-based, others process by aligning concepts (more semantic) and still others capture context with word vectors. For instance, CL-C3G has a narrow distribution of negatives and a broad distribution for positives (Figure 1 (a)), whereas the opposite is true for CL-ASA (Figure 1 (b)). We try to exploit this complementarity using decision tree based fusion. We use the C4.5 algorithm (Quinlan, 1993) implemented in Weka 3.8.0 (Hall et al., 2009). The first two folds of each sub-corpus are used to determinate the optimal decision tree and the other eight folds to evaluate the fusion (same protocol as weighted fusion). While analyzing the trained decision tree, we see that CL-C3G, CL-WESS and CL-CTS-WE are the closest to the root. This confirms their relevance for similarity detection, as well as their complementarity."}, {"heading": "5 Results and Discussion", "text": "Use of word embeddings. We can see in Table 1 that the use of distributed representation of words instead of lexical resources improves CL-CTS (CL-CTS-WE obtains overall performance gain of +3.83% on chunks and +3.19% on sentences). Despite this improvement, CL-CTS-WE remains less efficient than CL-C3G. While the use of bilingual sentence vector (CL-WES) is simple and elegant, its performance is lower than three state-of-the-art methods. However, its syntactically weighted version (CL-WESS) looks very promising and boosts the CL-WES overall performance by +11.78% on chunks and +14.92% on sentences. Thanks to this improvement, CL-WESS is significantly better than CL-C3G (+2.97% on chunks and +7.01% on sentences) and is the best single method evaluated so far on our corpus.\nFusion. Results of the decision tree fusion are reported at both chunk and sentence level in Table 1. Weighted and average fusion are only re-\nported at chunk level. In each case, we combine the 8 previously presented methods (the 5 state-of-the-art and the 3 new methods). Weighted fusion outperforms the state-of-the-art and the embedding-based methods in any case. Nevertheless, fusion based on a decision tree looks much more efficient. At chunk level, decision tree fusion leads to an overall F1 score of 89.15% while the precedent best weighted fusion obtains 80.01% and the best single method only obtains 53.73%. The trend is the same at the sentence level where decision tree fusion largely overpasses any other method (88.50% against 56.35% for the best single method). In our evaluation, the best decision tree, for an overall higher than 85% of correct classification on both levels, involves at a minimum CL-C3G, CL-WESS and CL-CTS-WE. These results confirm that different methods proposed complement each other, and that embeddings are useful for cross-language textual similarity detection."}, {"heading": "6 Conclusion and Perspectives", "text": "We have augmented several baseline approaches using word embeddings. The most promising approach is a cosine similarity on syntactically weighted distributed representation of sentence (CL-WESS), which beats in overall the precedent\nbest state-of-the-art method. Finally, we have also demonstrated that all methods are complementary and their fusion significantly helps crosslanguage textual similarity detection performance. At chunk level, decision tree fusion leads to an overall F1 score of 89.15% while the precedent best weighted fusion obtains 80.01% and the best single method only obtains 53.73%. The trend is the same at the sentence level where decision tree fusion largely overpasses any other method.\nOur future short term goal is to work on the improvement of CL-WESS by analyzing the syntactic weights or even adapt them according to the plagiarist\u2019s stylometry. We have also made a submission at the SemEval-2017 Task 1, i.e. the task on Semantic Textual Similarity detection."}], "references": [{"title": "Massively Multilingual Word Embeddings", "author": ["Waleed Ammar", "George Mulcaire", "Yulia Tsvetkov", "Guillaume Lample", "Chris Dyer", "Noah A. Smith."], "venue": "arXiv.org: http://arxiv.org/pdf/1602.01925v2.pdf. Computing Research Repository.", "citeRegEx": "Ammar et al\\.,? 2016", "shortCiteRegEx": "Ammar et al\\.", "year": 2016}, {"title": "On Cross-lingual Plagiarism Analysis using a Statistical Model", "author": ["Alberto Barr\u00f3n-Cede\u00f1o", "Paolo Rosso", "David Pinto", "Alfons Juan."], "venue": "Benno Stein and Efstathios Stamatatos and Moshe Koppel, editor, Proceedings of the ECAI\u201908 PAN Workshop:", "citeRegEx": "Barr\u00f3n.Cede\u00f1o et al\\.,? 2008", "shortCiteRegEx": "Barr\u00f3n.Cede\u00f1o et al\\.", "year": 2008}, {"title": "On the Mono- and Cross-Language Detection of Text Re-Use and Plagiarism", "author": ["Alberto Barr\u00f3n-Cede\u00f1o."], "venue": "PhD thesis, Val\u00e8ncia, Spain.", "citeRegEx": "Barr\u00f3n.Cede\u00f1o.,? 2012", "shortCiteRegEx": "Barr\u00f3n.Cede\u00f1o.", "year": 2012}, {"title": "MultiVec: a Multilingual and Multilevel Representation Learning Toolkit for NLP", "author": ["Alexandre Berard", "Christophe Servan", "Olivier Pietquin", "Laurent Besacier."], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evalua-", "citeRegEx": "Berard et al\\.,? 2016", "shortCiteRegEx": "Berard et al\\.", "year": 2016}, {"title": "CONDOR, a new parallel, constrained extension of Powell\u2019s UOBYQA algorithm: Experimental results and comparison with the DFO algorithm", "author": ["Frank Vanden Berghen", "Hugues Bersini."], "venue": "Journal of Computational and Applied Mathematics, 181:157\u2013", "citeRegEx": "Berghen and Bersini.,? 2005", "shortCiteRegEx": "Berghen and Bersini.", "year": 2005}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["Danqi Chen", "Christopher D. Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, pages 740\u2013750, Doha, Qatar.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "A Multilingual, Multi-style and Multi-granularity Dataset for Cross-language Textual Similarity Detection", "author": ["J\u00e9r\u00e9my Ferrero", "Fr\u00e9d\u00e9ric Agn\u00e8s", "Laurent Besacier", "Didier Schwab."], "venue": "Proceedings of the Tenth International Conference on Language", "citeRegEx": "Ferrero et al\\.,? 2016", "shortCiteRegEx": "Ferrero et al\\.", "year": 2016}, {"title": "Computing Semantic Relatedness using Wikipediabased Explicit Semantic Analysis", "author": ["Evgeniy Gabrilovich", "Shaul Markovitch."], "venue": "Proceedings of the 20th International Joint Conference on Artifical Intelligence (IJCAI\u201907), pages 1606\u20131611, Hyder-", "citeRegEx": "Gabrilovich and Markovitch.,? 2007", "shortCiteRegEx": "Gabrilovich and Markovitch.", "year": 2007}, {"title": "Word Embedding Evaluation and Combination", "author": ["Sahar Ghannay", "Benoit Favre", "Yannick Est\u00e8ve", "Nathalie Camelin."], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\u201916), Portoroz, Slovenia,", "citeRegEx": "Ghannay et al\\.,? 2016", "shortCiteRegEx": "Ghannay et al\\.", "year": 2016}, {"title": "The WEKA Data Mining Software: An Update", "author": ["Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H. Witten."], "venue": "SIGKDD Explorations, volume 11, pages 10\u201318, July.", "citeRegEx": "Hall et al\\.,? 2009", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Quoc V. Le", "Tomas Mikolov."], "venue": "Proceedings of the 31th International Conference on Machine Learning (ICML\u201914), volume 32, pages 1188\u20131196, Beijing, China, June. JMLR Proceed-", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Bilingual Word Representations with Monolingual Quality in Mind", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 1st NAACL Workshop on Vector Space Modeling for Natural Language Processing, Denver, Col-", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Character N-Gram Tokenization for European Language Text Retrieval", "author": ["Paul Mcnamee", "James Mayfield."], "venue": "Information Retrieval Proceedings, volume 7, pages 73\u201397. Kluwer Academic Publishers.", "citeRegEx": "Mcnamee and Mayfield.,? 2004", "shortCiteRegEx": "Mcnamee and Mayfield.", "year": 2004}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "Proceedings of the 27th Annual Conference on Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "External and Intrinsic Plagiarism Detection Using a Cross-Lingual Retrieval and Segmentation System - Lab Report for PAN at CLEF 2010", "author": ["Markus Muhr", "Roman Kern", "Mario Zechner", "Michael Granitzer."], "venue": "Martin Braschler, Donna Har-", "citeRegEx": "Muhr et al\\.,? 2010", "shortCiteRegEx": "Muhr et al\\.", "year": 2010}, {"title": "A New Approach for Searching Translated Plagiarism", "author": ["M\u00e0t\u00e9 Pataki."], "venue": "Proceedings of the 5th International Plagiarism Conference, Newcastle, UK.", "citeRegEx": "Pataki.,? 2012", "shortCiteRegEx": "Pataki.", "year": 2012}, {"title": "A universal part-of-speech tagset", "author": ["Slav Petrov", "Dipanjan Das", "Ryan McDonald."], "venue": "Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC\u201912), Istanbul, Turkey, May. European Language Resources", "citeRegEx": "Petrov et al\\.,? 2012", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "A Statistical Approach to Crosslingual Natural Language Tasks", "author": ["David Pinto", "Jorge Civera", "Alfons Juan", "Paolo Rosso", "Alberto Barr\u00f3n-Cede\u00f1o."], "venue": "CEUR Workshop Proceedings, volume 64 of Journal of Algorithms, pages 51\u201360, January.", "citeRegEx": "Pinto et al\\.,? 2009", "shortCiteRegEx": "Pinto et al\\.", "year": 2009}, {"title": "A Wikipedia-Based Multilingual Retrieval Model", "author": ["Martin Potthast", "Benno Stein", "Maik Anderka."], "venue": "30th European Conference on IR Research (ECIR\u201908), volume 4956 of LNCS of Lecture Notes in Computer Science, pages 522\u2013530, Glas-", "citeRegEx": "Potthast et al\\.,? 2008", "shortCiteRegEx": "Potthast et al\\.", "year": 2008}, {"title": "Cross-Language Plagiarism Detection", "author": ["Martin Potthast", "Alberto Barr\u00f3n-Cede\u00f1o", "Benno Stein", "Paolo Rosso."], "venue": "Language Resources and Evaluation, volume 45, pages 45\u201362.", "citeRegEx": "Potthast et al\\.,? 2011", "shortCiteRegEx": "Potthast et al\\.", "year": 2011}, {"title": "Overview of the 6th International Competition on Plagiarism Detection", "author": ["Martin Potthast", "Matthias Hagen", "Anna Beyer", "Matthias Busse", "Martin Tippmann", "Paolo Rosso", "Benno Stein."], "venue": "PAN at CLEF 2014, Sheffield, UK, September.", "citeRegEx": "Potthast et al\\.,? 2014", "shortCiteRegEx": "Potthast et al\\.", "year": 2014}, {"title": "C4.5: Programs for Machine Learning. The Morgan Kaufmann series in machine learning", "author": ["J. Ross Quinlan"], "venue": null, "citeRegEx": "Quinlan.,? \\Q1993\\E", "shortCiteRegEx": "Quinlan.", "year": 1993}, {"title": "Probabilistic Part-of-Speech Tagging Using Decision Trees", "author": ["Helmut Schmid."], "venue": "Proceedings of the International Conference on New Methods in Language Processing, Manchester, UK.", "citeRegEx": "Schmid.,? 1994", "shortCiteRegEx": "Schmid.", "year": 1994}, {"title": "DBnary: Wiktionary as a Lemon-Based Multilingual Lexical Resource in RDF", "author": ["Gilles S\u00e9rasset."], "venue": "Semantic Web Journal (special issue on Multilingual Linked Open Data), volume 6, pages 355\u2013361.", "citeRegEx": "S\u00e9rasset.,? 2015", "shortCiteRegEx": "S\u00e9rasset.", "year": 2015}, {"title": "Cross-lingual Models of Word Embeddings: An Empirical Comparison", "author": ["Shyam Upadhyay", "Manaal Faruqui", "Chris Dyer", "Dan Roth."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL\u201916),", "citeRegEx": "Upadhyay et al\\.,? 2016", "shortCiteRegEx": "Upadhyay et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 20, "context": "In monolingual context, this problem is rather well treated by several recent researches (Potthast et al., 2014).", "startOffset": 89, "endOffset": 112}, {"referenceID": 13, "context": "We use word embeddings (Mikolov et al., 2013) that have shown promising performances for all kinds of NLP tasks, as shown in Upadhyay et al.", "startOffset": 23, "endOffset": 45}, {"referenceID": 11, "context": "We use word embeddings (Mikolov et al., 2013) that have shown promising performances for all kinds of NLP tasks, as shown in Upadhyay et al. (2016), Ammar et al.", "startOffset": 24, "endOffset": 148}, {"referenceID": 0, "context": "(2016), Ammar et al. (2016) and Ghannay et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 0, "context": "(2016), Ammar et al. (2016) and Ghannay et al. (2016), for instance.", "startOffset": 8, "endOffset": 54}, {"referenceID": 6, "context": "We quickly review below the state-of-the-art methods used in this paper, for more details, see Ferrero et al. (2016).", "startOffset": 95, "endOffset": 117}, {"referenceID": 12, "context": "is based on Mcnamee and Mayfield (2004) model.", "startOffset": 12, "endOffset": 40}, {"referenceID": 12, "context": "is based on Mcnamee and Mayfield (2004) model. We use the Potthast et al. (2011) implementation which compares two textual units under their 3-grams vectors representation.", "startOffset": 12, "endOffset": 81}, {"referenceID": 15, "context": "Similarity (CL-CTS) (Pataki, 2012) aims to measure the semantic similarity using abstract con-", "startOffset": 20, "endOffset": 34}, {"referenceID": 23, "context": "In our implementation, these concepts are given by a linked lexical resource called DBNary (S\u00e9rasset, 2015).", "startOffset": 91, "endOffset": 107}, {"referenceID": 17, "context": "(2008), Pinto et al. (2009)).", "startOffset": 8, "endOffset": 28}, {"referenceID": 7, "context": "(CL-ESA) is based on the explicit semantic analysis model (Gabrilovich and Markovitch, 2007), which represents the meaning of a document by a vector based on concepts derived from Wikipedia.", "startOffset": 58, "endOffset": 92}, {"referenceID": 7, "context": "(CL-ESA) is based on the explicit semantic analysis model (Gabrilovich and Markovitch, 2007), which represents the meaning of a document by a vector based on concepts derived from Wikipedia. It was reused by Potthast et al. (2008) in the context of cross-language document retrieval.", "startOffset": 59, "endOffset": 231}, {"referenceID": 2, "context": "consists in translating the two units into the same language, in order to operate a monolingual comparison between them (Barr\u00f3n-Cede\u00f1o, 2012).", "startOffset": 120, "endOffset": 141}, {"referenceID": 23, "context": "(2010) approach using DBNary (S\u00e9rasset, 2015), followed by monolingual matching based on bags of words.", "startOffset": 29, "endOffset": 45}, {"referenceID": 2, "context": "consists in translating the two units into the same language, in order to operate a monolingual comparison between them (Barr\u00f3n-Cede\u00f1o, 2012). We use the Muhr et al. (2010) approach using DBNary (S\u00e9rasset, 2015), followed by monolingual matching based on bags of words.", "startOffset": 121, "endOffset": 173}, {"referenceID": 6, "context": "We apply the same evaluation protocol as in Ferrero et al. (2016)\u2019s paper.", "startOffset": 44, "endOffset": 66}, {"referenceID": 3, "context": "We use the MultiVec (Berard et al., 2016) toolkit for computing and managing the continuous representations of the texts.", "startOffset": 20, "endOffset": 41}, {"referenceID": 13, "context": "It includes word2vec (Mikolov et al., 2013), paragraph vector (Le and Mikolov, 2014) and bilingual distributed representations (Luong et al.", "startOffset": 21, "endOffset": 43}, {"referenceID": 10, "context": ", 2013), paragraph vector (Le and Mikolov, 2014) and bilingual distributed representations (Luong et al.", "startOffset": 26, "endOffset": 48}, {"referenceID": 11, "context": ", 2013), paragraph vector (Le and Mikolov, 2014) and bilingual distributed representations (Luong et al., 2015) features.", "startOffset": 91, "endOffset": 111}, {"referenceID": 3, "context": "This feature is available in MultiVec3 (Berard et al., 2016).", "startOffset": 39, "endOffset": 60}, {"referenceID": 22, "context": "First, we syntactically tag U with a part-of-speech tagger (TreeTagger (Schmid, 1994)) and we normalize the tags with Universal Tagset of Petrov et al.", "startOffset": 71, "endOffset": 85}, {"referenceID": 4, "context": "Finally, we optimize the weights with the help of Condor (Berghen and Bersini, 2005).", "startOffset": 57, "endOffset": 84}, {"referenceID": 15, "context": "First, we syntactically tag U with a part-of-speech tagger (TreeTagger (Schmid, 1994)) and we normalize the tags with Universal Tagset of Petrov et al. (2012). Then, we assign a weight to each type of tag: this weight will be used to compute the final vector representation of the unit.", "startOffset": 138, "endOffset": 159}, {"referenceID": 3, "context": "We call this method CL-WESS and we have implemented it in MultiVec (Berard et al., 2016).", "startOffset": 67, "endOffset": 88}, {"referenceID": 4, "context": "We optimize the distribution of the weights with Condor (Berghen and Bersini, 2005).", "startOffset": 56, "endOffset": 83}, {"referenceID": 21, "context": "5 algorithm (Quinlan, 1993) implemented in Weka 3.", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "0 (Hall et al., 2009).", "startOffset": 2, "endOffset": 21}], "year": 2017, "abstractText": "This paper proposes to use distributed representation of words (word embeddings) in cross-language textual similarity detection. The main contributions of this paper are the following: (a) we introduce new cross-language similarity detection methods based on distributed representation of words; (b) we combine the different methods proposed to verify their complementarity and finally obtain an overall F1 score of 89.15% for English-French similarity detection at chunk level (88.5% at sentence level) on a very challenging corpus.", "creator": "TeX"}}}