{"id": "1702.03706", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2017", "title": "Multitask Learning with Deep Neural Networks for Community Question Answering", "abstract": "In this paper, we developed a deep neural network (DNN) that learns to solve simultaneously the three tasks of the cQA challenge proposed by the SemEval-2016 Task 3, i.e., question-comment similarity, question-question similarity and new question-comment similarity. The latter is the main task, which can exploit the previous two for achieving better results. Our DNN is trained jointly on all the three cQA tasks and learns to encode questions and comments into a single vector representation shared across the multiple tasks. The results on the official challenge test set show that our approach produces higher accuracy and faster convergence rates than the individual neural networks. Additionally, our method, which does not use any manual feature engineering, approaches the state of the art established with methods that make heavy use of it. The results can be seen in the Figure 6. The training results demonstrate the use of our technique for improving the results of the CQA challenge. The data are provided by the SemEval-2016 Task 3 task on which we learned the results.\n\n\n\nIn the previous paper, we present a technique that helps to solve problems in the real world, and provides some examples of our method.\nAn algorithm is a class of task which is written in an online language called SemEval-2016, which has two main concepts. The first concept is that each task must be performed in a sequence of five tasks and the second concept is that each task must be performed in a single vector representation with a different dimension.\nWhen a task is performed and the task is not performed, the other part of the task is how the task is written and is performed. As the data in the second class are used, the task is done in the context of the original task and is shown as the result of the second and third tasks.\nFor example, we used a neural network based on the following methods:\nIn order to perform our task, we first need to create a new task, and in order to find a new task, we can create a new task. Once a new task is created, we can retrieve a new task, and it is returned, from the dataset. The data in the new task will be extracted by a new task called a new task called a new task called a new task called a new task called a new task called a new task called a new task called a new task called a new task called a new task called a new task called a new task called a new task called a", "histories": [["v1", "Mon, 13 Feb 2017 10:24:55 GMT  (85kb,D)", "http://arxiv.org/abs/1702.03706v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["daniele bonadiman", "antonio uva", "alessandro moschitti"], "accepted": false, "id": "1702.03706"}, "pdf": {"name": "1702.03706.pdf", "metadata": {"source": "CRF", "title": "Multitask Learning with Deep Neural Networks for Community Question Answering", "authors": ["Daniele Bonadiman", "Antonio Uva", "Alessandro Moschitti"], "emails": ["d.bonadiman@unitn.it", "antonio.uva@unitn.it", "amoschitti@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Community Question Answering (cQA) websites enable users to freely ask questions in web forums and expect some good answers in the form of comments from the other users. Given the large number of question/answer pairs available on cQA sites, researchers started to investigate the possibility to exploit user-generated content for training automatic QA systems. Unfortunately, the text involved in the cQA scenario is rather noisy, therefore, providing models that outperform the simple bag-of-words representation can result rather difficult. The challenge, SemEval-2016 Task 3 \u201cCommunity Question Answering\u201d, has been designed to study the above problems: the participants were supposed to build a fully automatic\nsystem for cQA. In particular, given a fresh user question, qnew, and a set of forum questions, Q, answered by a comment set, C, the main task consists of determining whether a comment c \u2208 C is a pertinent answer of qnew or not. This task can be divided into three sub-tasks:\n(A) predict if a comment produced in response to a question contains a valid answer;\n(B) re-rank a set of questions according to their relevancy with respect to the original question; and\n(C) predict if a comment produced in response to a previous question posed on the cQA forum represents a valid answer to a fresh question.\nTraditionally, these tasks have been tackled by designing systems/classifiers that target each task separately. Each classifier accepts in input a vector encoding a text pair (e.g., a question/question or a question/answer pair) by using many complex lexical syntactic or semantic features and, then, computing similarity between these representations. However, this approach suffers from the drawbacks of requiring a \u201ccustomized\u201d set of features for each task being solved.\nRecent work on deep neural networks (DNNs) for Multitask Learning (MTL) (Collobert and Weston, 2008; Liu et al., 2015) showed that is possible to jointly train a general system that solves different tasks simultaneously. Inspired by the success of MTL, in this paper, we propose a DNN model that leverages the data from the three cQA tasks of SemEval. Indeed, as the three tasks are highly related, we claim that cQA can highly benefit from this approach. We show that our DNN, despite the fact that does not require any feature engineering, approaches the performance of the best systems, which use heavy feature engineering. Additionally, we are going to make the corar X\niv :1\n70 2.\n03 70\n6v 1\n[ cs\n.C L\n] 1\n3 Fe\nb 20\n17\npora for studying MTL on this interesting challenge available to the research community."}, {"heading": "2 cQA Tasks at SemEval", "text": "The research problem issued by SemEval-2016 Task 3 is exemplified by Fig. 2: given a new question qnew, Task C is about directly retrieving a relevant comment from the entire community. This can also be achieved by solving Task B, which finds a similar question, qrel, and then executing Task A, which selects good comments, crel, for qrel. It should be noted that Task A classifies comments, specifically written by the users for qrel, whereas Task C classifies comments written by the users for other, sometimes, similar questions. This means, it needs to filter out comments that can be partially related to qnew (because they correctly answer the related question, qrel) but still not correctly answering qnew. Clearly, Task C classifier needs to tackle a much more semantically challenging task. Thus, tasks A and C are semantically and computationally rather different and together with Task B, they constitute an interesting MTL problem since differences and correlations are played at a very high semantic level."}, {"heading": "2.1 Task A: Question-Comment Similarity", "text": "Given a question, qrel, and its first 10 comments, crel, in the question threads, rerank the comments according to their relevance to qrel. Relevancy is defined according to three classes: (i) good : the comment is definitively relevant; (ii) potentially useful : the comment is not good, but it still contains related information worth checking; and (iii) bad : the comment is irrelevant (e.g., it is part of a dialogue or unrelated to the topic). For evaluation pur-\nposes, both potentially useful and bad comments were considered as bad ."}, {"heading": "2.2 Task B: Question-Question Similarity", "text": "Given a new question, qnew, and its first 10 related questions (retrieved by a search engine), qrel, rerank them according to their similarity with respect to qnew. Relevancy is expressed by three classes: (i) perfect match : the new and forum questions request roughly the same information, (ii) relevant : the new and forum questions ask for similar information, or (iii) irrelevant : the new and forum questions are completely unrelated. For evaluation purposes, both perfect match and relevant forum questions are considered as relevant ."}, {"heading": "2.3 Task C: New Question-Comment Similarity", "text": "Given a new question, qnew, and its first 10 related questions (retrieved by a search engine), qrel, each associated with its first 10 comments, crel, appearing in its thread, rerank the 100 comments (10 questions \u00d7 10 comments) according to their relevance with respect to qnew. Relevancy is defined similarly to task A."}, {"heading": "2.4 Dataset", "text": "The data for the above-mentioned tasks is distributed in three datasets: train, dev and test sets. The distribution of questions and comments in each dataset varies across the different tasks: Task A contains 6,938 related questions and 40,288 comments. Each comment in the dataset was annotated with a label indicating its relevancy with respect to the related question. Task B contains 317 original questions. For each original question, 10 related questions were retrieved, summing to 3,169 related questions. Also in this case, the related questions were annotated with a relevancy label, which tells if they are relevant with respect to the user original question. Task C contains 317 original question, together with 3,169 related questions (same as in Task B) and 31,690 comments. Each comment was labelled with its relevancy with respect to the original question."}, {"heading": "3 A General Deep Architecture for cQA", "text": "All the previous tasks are about reranking questions or comments with respect to an original question. In the following, we describe a general architecture for solving them."}, {"heading": "3.1 Deep Architecture for relational learning from pairs of text", "text": "A traditional approach to cQA is to learn a different classifiers for solving each of these three tasks, independently. For example, first a classifier can be trained to rerank a set of related questions retrieved by a search engine, using their similarity with respect to the user question (Task B). Then, another classifier can be trained to rerank the list of comments appearing in the threads of similar questions (Task A). Each of these classifiers uses a different set of task-dependent features. In this work, we use a neural network architecture for classifying text pairs. The network is fed using the different pairs, (qrel, crel), (qnew, qrel) and (qnew, crel), for learning the tasks A, B and C, respectively, and produces a similarity score that can be used for reranking questions or comments.\nIt is composed of two main components: (i) two sentence encoders that map input sentences i into fixed size vectors xsi \u2208 Rm, and (ii) a feed forward neural network that computes the similarity between these two sentence vectors.\nThe sentence encoders are composed of (i) a sentence matrix si \u2208 Rd\u00d7|i|, where d is the size of the word embeddings, obtained by concatenating the word vector of the corresponding word in the input sentence wj \u2208 si, and (ii) a sentence model f : Rd\u00d7|i| \u2192 Rm, which maps the sentence matrix to a fixed size sentence embedding xsi \u2208 Rm.\nThe choice of the sentence model plays a crucial role as the resulting intermediate representation of the input sentences affects the successive steps of computing their similarity. Previous work in this direction uses different types of sentence models such as LSTM, distributional sentence model (average of word vectors), and convolutional sentence model. In particular, the latter is composed of a sequence of convolution and pooling feature maps have achieved the state of the art in various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014).\nIn this paper, we used a CNN sentence model that is a convolutional operation followed by a kmax pooling layer with k = 1, since it provides comparable performance to the LSTM on the task of new question-comment similarity, as shown in Table 2. The sentence encoder, xsi = f(si), outputs a fixed-size embedding of the input sentence si. The sentence vectors, xsi , are concatenated together and given in input to a Multi-Layer Perceptron, which is constituted by a non-linear hidden\nlayer and an sigmoid output layer."}, {"heading": "3.2 Injecting Relational Information", "text": "All the tasks we consider require to model relations between words present in the two pieces of text. For this purpose, we encode the relation in forms of discrete features, as described in (Collobert et al., 2011), i.e., using an additional embedding layer. They augmented the word embedding with the corresponding feature embedding. Thus, given a word, wj , its final word embedding is defined as wj \u2208 Rd, where d = dw + dfeat, where dw is the size of the word embedding and dfeat the size of the feature embedding.\nWe use a discrete feature, represented with an embedding of 5 dimensions, to encode matches between two words in the two input piece of text. In particular, we associate each word w in the input sentences with a word overlap index o \u2208 {0, 1}, where o = 1 means that w is shared by both Q and C (or by the two questions for task B), i.e., overlaps, o = 0 otherwise. It should be noted that the embeddings described here cannot be considered as task specific features, manually handcrafted. They are part of the network, serve the purpose of injecting relational information between the representations of the two input texts and can be generally applied to different domains, data and tasks."}, {"heading": "3.3 Adding the rank features", "text": "The SemEval problems concern reranking text initially ranked by Google and made available to the participants for tasks B and C. Considering that the Google rank is computed using powerful algorithms and a lot of resources, it is essential to encode it in our networks. There are several methods for achieving this. After some experiments, we opted for discretizing the rank values in 5 different bins of different sizes, i.e. [1\u22122], [2\u22125], [5\u221210], [10\u221225], [25\u2212\u221e]. The rank feature is added to the joint layer, where the output of the sentence model is concatenated, using a table lookup operation. It should be noted that for each task, we use a different relation feature (overlap embeddings) between each pair of text."}, {"heading": "4 MTL for cQA", "text": "MTL aims at learning several related tasks at the same time for improving some (or possibly all) tasks using joint information (Caruana, 1997).\nMTL is particularly well suited for modeling Task C as it is a composition of tasks A and B, thus, it can benefit from having both questions qnew and qrel as input to better model the interaction between the new question and the comment. More precisely, it can use the triplet \u3008qnew, qrel, crel\u3009 in the learning process, where the interaction between the triplet members is exploited during the joint training of the three models for tasks, A, B and C. In fact, an improvement on questioncomment similarity or on question-question similarity can lead to an improvement in the task of new question-comment similarity (Task C).\nAdditionally, each thread in the in the SemEval dataset is annotated with the labels for all the three tasks and therefore it is possible to apply joint learning directly."}, {"heading": "4.1 Joint Learning Architecture", "text": "Our Joint learning architecture is depicted in Figure 2, it is a direct extension of the architecture proposed for Task C (Section 3.3). It takes the three sentences as input, i.e, a new question, qnew, the related question, qrel, and its comment, crel, and produces three fixed size representations,\nxqnew , xqrel and xcrel , respectively. These three representations are then concatenated (hj = [xqnew , xqrel , xrrel ]) and fed to a hidden layer to create a shared representation of the input for the three tasks, hs =Whj .\nThe output of this layer, hs is then fed to three independent Multilayer Perceptrons (MLP) that produce the scores for the three tasks. To directly apply MTL, we use the binary cross-entropy instead of the max margin loss as our objective function. The main motivation is that such function is computed based on pairs of positive-negative examples that cannot be created with multiple labels. At training time, for each example, the loss is calculated on the three outputs of the network. The final loss is then the sum of the individual losses for the three tasks."}, {"heading": "4.2 Shared Sentence Models", "text": "The SemEval dataset contains ten times less new questions than related questions by construction. However, all questions, qnew included, are supposed to be of the same nature. Thus we can certainly use a shared text model for modeling better representations for both new and related questions.\nFormally, let xd = f(d, \u03b8) be a sentence model for document dwith parameters \u03b8, i.e., the embedding weights and the convolutional filters. In our original formulation, each sentence model uses a different set of parameters \u03b8qnew , \u03b8qrel and \u03b8crel . We used the same set of parameters \u03b8q. The shared sentence model is depicted in Figure 2 as\u2194."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Setup", "text": "We encode input sentences with fixed-sized vectors using a convolutional operation of size 5 and a k-max pooling operation with k = 1, i.e., similarly to (Severyn and Moschitti, 2015). We use two non-linear hidden layers (with hyperbolic tangent activation, Tanh), whose size is equal to the size of the previous layer, i.e., the join layer. We include information such as word overlaps and rank position as embedding with an additional lookup table with vectors of size dfeat = 5.\nPre-processing: both questions and comments are tokenized and lowercased (to reduce the dimensionality of the dictionary and therefore of the embedding matrix). Moreover, question subject and body are concatenated to create a unique question. For computational reasons, we opted to limit the size of the input documents at 100 words: we did not observe any degradation in performance.\nWord Embeddings: for all the proposed models, we pre-initialize the word embedding matrices with standard skipgram embedding of dimensionality 50 trained on the English Wikipedia dump using word2vec toolkit (Mikolov et al., 2013).\nTraining: The network is trained using SGD with shuffled mini-batches using the rmsprop update rule (Tieleman and Hinton, 2012). The model learns until the validation loss stops improving, with patience p = 10, i.e., the number of epochs to wait before early stopping, if no progress on the validation set is obtained. In fact, early stopping (Prechelt, 1998) allows us to avoid overfitting and improving the generalization capabilities of the network. For the MTL architecture, we employed two different stopping criteria. The first is to stop training when the global validation loss\ndoes not improve anymore (the sum of the individual losses of the three tasks). The second, instead, saves three different models and evaluates them when the individual losses stop improving. Since the three tasks converge at different epochs, the first method may lead to sub-optimal results for the individual tasks, but only one model is needed at test time.\nTo improve generalization and avoid coadaptation of features, we opted for adding dropout (Srivastava et al., 2014) between all the layers of the network. We experimented with different dropout rates (0.2, 0.4) for the inputs and (0.3, 0.5, 0.7) for the hidden layers obtaining better results with the highest values, i.e., 0.4 and 0.7.\nDataset: Table 1 reports the labels distributions on the train dataset. It is important to note that the dataset for Task C presents a higher number of negative than positive examples. For this reason, we automatically extended the training dataset (ED) with new positive matches for Task B and C, respectively. This process is done by creating the (qrel, crel) pairs for each qrel from the training set for Task A and creating triples of the form (qrel, qrel, crel), where the label for questionquestion similarity is obviously positive and the labels for Task C are inherited from those of Task A. The resulting dataset contains 34, 100 triples and its relevance label distribution is presented in the last row of Table 1. The extended version of the dataset with the annotation for MTL is made available for download for comparison purposes 1."}, {"heading": "5.2 Impact of the sentence models", "text": "Table 2 shows a comparison between CNN and LSTM sentence models when used in our general architecture (see Sec. 3) for solving Task C. We derived the results from the development set 3. We observe that the two sentence models show\n1Download link to be defined. 2Extended Dataset for Task C computed using questions from Task A. 3In this work, the dataset Train-part2 were used as development set.\ncomparable results. For the rest of the experiments, we used the CNN sentence model, since it shows faster convergence rate and more stable results with respect to the LSTM sentence model. In the second part of Table 2, we demonstrate that using the extended dataset for solving Task C leads to higher results than the original one. In particular, we noted that there is an improvement of 3 points in MRR."}, {"heading": "5.3 Results of individual models", "text": "Table 3 shows the results of our individual and MTL models, in comparison with the Random and Information Retrieval baselines of the challenge (first grouped row), and the three-top systems of SemEval 2016, Kelp, UH-PRHLT, SUperteam (second grouped row).\nThe third grouped row shows the performance of the individual models when trained on input pairs, \u3008qrel, crel\u3009, \u3008qnew, qrel\u3009 and \u3008qnew, crel\u3009 for task A, B and C, respectively. The model for the three tasks is the same (described in Sec. 3). These results show that the individual models can generalize well enough on all tasks. In particular, on Task B, they achieve the best results of our proposed model (the numbers in bold indicate the best results among the proposed models).\nThe fourth grouped row illustrates the models exploiting the joint input, \u3008qnew, qrel, crel\u3009, but no joint learning is carried out, i.e., the networks for the different tasks are trained individually. The results show that a small degradation of performance happens in Task B, while Task A slightly improves. These variations may be due to the fact that tasks A and B can be efficiently solved us-\ning the standard pairwise approach, thus the extra text introduced in the model may just add some noise. However, using the shared sentence model for qnew and qrel of the tasks B and C (indicated with\u2194) improves the overall performance."}, {"heading": "5.4 Results of MTL models", "text": "The shared input representation shows good results on all tasks, thus, in the last set of experiments, we jointly trained (i) tasks B and C, (ii) tasks A and C and finally (iii) the three tasks together.\nThe results are reported in the fifth grouped row. It is interesting to note that the major boost in terms of performance is obtained when we jointly train all the three tasks. In fact, the MTL architecture improves the individual model in terms of MAP by about 2 absolute points on the DEV set and by 3 absolute points on the TEST set for Task C, while the performance on the other tasks tends to degrade. However, if the three different models are evaluated at different epochs of training, e.g., see MTL(ABC)*, it is possible to obtain accuracy comparable to the individual models for all the three tasks. As previously explained, when applying MTL, the individual objective functions converge at different epochs. Therefore, when the global loss reaches the minimum, it is possible that individual models are sub-optimal.\nIndeed, the comparison between the learning curves (on the development set) for Task B (Figure 4) and Task C (Figure 5) shows that for the former, models achieve earlier convergence rate (epoch 2) while for the latter they converge later (epoch 16). Moreover, Figure 3 shows that the re-\nsults on Task A are not badly affected by jointly training models with the other two tasks.\nFinally, the learning curves show that our networks trained in MTL tend to have faster convergence rate than the individual models: this is a very interesting result.\nWe also experimented with shallower networks and SVMs using the prediction scores from the different classifiers in a stacking approach, and obtained results far below the baselines4.\nComparison with the state of the art Our models would have ranked 4th on Task C of the Semeval 2016 competition 5, i.e., the main task of the challenge. In contrast, our models for the other two tasks, which do not benefit from the overall MTL architecture would have achieved a middle position (8th). These results are important since our proposed MTL architecture obtains a placement very close to the top system, without requiring task-specific features, which in cQA are extremely important, e.g., the thread-level features.\nFinally, one reason of why we do not achieve the state of the art on Task C is due to the difference between training and test data. Several challenge participants solved this problem by using a weighted sum between the score of the Task A classifier and the Google rank as a strong features for modeling Task C. We followed a similar approach estimating the weight MTL on the dev set and using the computed score to rank the comments of the test set. This improved the MAP of our MTL by about 2.8 absolute points on the test set, obtaining a result comparable with the model ranked 2nd on Task C at the Semeval 2016 competition."}, {"heading": "6 Related Work", "text": "Question-Question Similarity. Determining question similarity remains one of the main tasks need to be solved in cQA due to difficult problems such as \u201clexical gap\u201d. Early approaches on question similarity used statistical machine translation techniques to measure similarity between questions. For example, Jeon et al. (2005) and Zhou et al. (2011) used a language models based on word or phrase translation probabilities for estimating similarity between questions. However, effective\n4We did not include these results as they do not provide interesting findings.\n5http://alt.qcri.org/semeval2016/task3/index.php?id=results\napproaches based on statistical machine translation require lots of data for estimating word probabilities. Language models for question-question similarity were also explored by Cao et al. (2009). These models exploit information from the category structure of Yahoo! Answers when computing similarity between two questions. Instead, Duan et al. (2008) propose an approach that identifies the topic and focus in a text and compute similarity between two input questions by matching the extracted topic and focus information. A different approach to question-question similarity is provided by Ji et al. (2012) and Zhang et al. (2014). They use LDA to learn the probability distribution over the topics that generate the question/answers pairs. Later, this distribution is used to measure similarity between questions.\nQuestion-Answer Similarity. In recent years, many models have been proposed for computing similarity of an answer with respect to a question. For example, Yao et al. (2013) trained a conditional random field based on a set of powerful features, such as tree-edit distance between question and answer trees: these also enable the extraction of answers from pre-retrieved sentences. Heilman and Smith (2010) use a linear classifier using syntactic features to solve different tasks such as recognizing textual entailment, paraphrases and answer selection. Wang et al. (2007) propose the use of Quasi-synchronous grammars to select short answers for TREC questions. This is done by learning syntactic and semantic transformation from the question to the answer trees. Wang and Manning (2010) propose a probabilistic Tree-Edit model with structured latent variables for solving textual entailment and question answering. An advanced model based on structural representations was proposed by Severyn and Moschitti (2012). This model uses SVM with kernels to learn structural patterns between questions and answers encoded in form of shallow syntactic parse trees.\nFinally, Wang and Nyberg (2015) trained a long short-term memory model for selecting answers to TREC questions. Their model takes words from question and answer sentences as input and returns a score measuring the relevancy of an answer with respect to a given question. A recent work close to ours is (Guzma\u0301n et al., 2016), where the authors build a neural network for solving Task A of SemEval. However, this does not approach the problem as MTL.\n0 2 4 6 8 10 12 14 16 18 20\n0.65\n0.66\n0.67\n0.68\n0.69\n0.7\n0.71\nEpochs\nM A\nP\nMTL Train+ED\nMTL Train\nNo MTL Train + ED\nNo MTL Train\nFigure 3: Learning curves for Task A on the dev. set; dotted and solid lines represent the individual and multi-task models, respectively.\n0 2 4 6 8 10 12 14 16 18 20\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\nEpochs\nM A\nP\nMTL Train+ED\nMTL Train\nNo MTL Train + ED\nNo MTL Train\nFigure 4: Learning curves for Task B on the development set; dotted lines represent the individual models, while the solid lines the multi-task ones.\nRelated work on MTL. A good overview on MTL, i.e., learning to solve multiple tasks by using a shared representation with mutual benefit, is given in (Caruana, 1997). Collobert and Weston (2008) trained a convolutional NN with MTL which, given an input sentence, performs many sequence labeling tasks. They showed that jointly training their system on different tasks, such as speech tagging, named entity recognition, etc., significantly improves the performance on the main task, i.e., semantic role labeling, without requiring hand-engineered features.\nLiu et al. (2015) is the most close work to ours. They used multi-task deep neural networks to map queries and documents into semantic vector rep-\n0 2 4 6 8 10 12 14 16 18 20\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nEpochs\nM A\nP\nMTL Train+ED\nMTL Train\nNo MTL Train + ED\nNo MTL Train\nFigure 5: Learning curves for Task C on the development set; dotted lines represent the individual models, while the solid lines the multi-task ones.\nresentations. This representation is later used into two tasks: query classification and questionanswer reranking. Results showed a competitive gain over strong baselines. In our work, we have presented an architecture that can also exploit joint representation of question and comments, given the strong interdependencies among the different SemEval Tasks."}, {"heading": "7 Conclusion", "text": "In this paper we proposed several Deep Neural Networks for the task of automatic cQA. Our main result is a network that can effectively exploit the characteristics of the cQA task for carrying out interesting MTL. Our network designed and trained in an MTL setting shows better results and a higher convergence rate than individual models that are trained independently. These results are competitive with those of the models participating at the SemEval 2016 competition for cQA. It should be noted that all the other challenge systems use domain specific features, which are both very important but also rather costly to engineer.\nIn the future, we would like to use more effective features and combine them with other machine learning methods."}, {"heading": "Acknowledgements", "text": "This work has been partially supported by the EC project CogNet, 671625 (H2020-ICT-2014-2, Research and Innovation action) and by the IYAS project, Interactive sYstems for Answer Search."}], "references": [{"title": "The use of categorization information in language models for question retrieval", "author": ["Cao et al.2009] Xin Cao", "Gao Cong", "Bin Cui", "Christian S\u00f8ndergaard Jensen", "Ce Zhang"], "venue": "In CIKM", "citeRegEx": "Cao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2009}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Searching questions by identifying question topic and question focus", "author": ["Duan et al.2008] Huizhong Duan", "Yunbo Cao", "ChinYew Lin", "Yong Yu"], "venue": null, "citeRegEx": "Duan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2008}, {"title": "Machine translation evaluation meets community question answering", "author": ["Llu\u0131\u0301s M\u00e0rquez", "Preslav Nakov"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Guzm\u00e1n et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Guzm\u00e1n et al\\.", "year": 2016}, {"title": "Tree edit models for recognizing textual entailments, paraphrases, and answers to questions", "author": ["Heilman", "Smith2010] Michael Heilman", "Noah A Smith"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American", "citeRegEx": "Heilman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Heilman et al\\.", "year": 2010}, {"title": "Finding similar questions in large question and answer archives", "author": ["Jeon et al.2005] Jiwoon Jeon", "W Bruce Croft", "Joon Ho Lee"], "venue": "In CIKM", "citeRegEx": "Jeon et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Jeon et al\\.", "year": 2005}, {"title": "Question-answer topic model for question retrieval in community question answering", "author": ["Ji et al.2012] Zongcheng Ji", "Fei Xu", "Bin Wang", "Ben He"], "venue": "In CIKM", "citeRegEx": "Ji et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2012}, {"title": "A convolutional neural network for modelling", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Representation learning using multi-task deep neural networks for semantic classification and information", "author": ["Liu et al.2015] Xiaodong Liu", "Jianfeng Gao", "Xiaodong He", "Li Deng", "Kevin Duh", "Ye-Yi Wang"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Early stoppingbut when", "author": ["Lutz Prechelt"], "venue": "In Neural Networks: Tricks of the trade,", "citeRegEx": "Prechelt.,? \\Q1998\\E", "shortCiteRegEx": "Prechelt.", "year": 1998}, {"title": "Structural relationships for large-scale learning of answer re-ranking", "author": ["Severyn", "Moschitti2012] Aliaksei Severyn", "Alessandro Moschitti"], "venue": "In Proceedings of the 35th international ACM SIGIR conference on Research and development", "citeRegEx": "Severyn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Severyn et al\\.", "year": 2012}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["Severyn", "Moschitti2015] Aliaksei Severyn", "Alessandro Moschitti"], "venue": "In Proceedings of the 38th International ACM SIGIR Conference on Research and Development", "citeRegEx": "Severyn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Severyn et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Hinton2012] Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Probabilistic tree-edit models with structured latent variables for textual entailment and question answering", "author": ["Wang", "Manning2010] Mengqiu Wang", "Christopher D. Manning"], "venue": "In Proceedings of the 23rd International Conference on Computa-", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "A long short-term memory model for answer sentence selection in question answering", "author": ["Wang", "Nyberg2015] Di Wang", "Eric Nyberg"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "What is the jeopardy model? a quasi-synchronous grammar for qa", "author": ["Wang et al.2007] Mengqiu Wang", "Noah A Smith", "Teruko Mitamura"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "Wang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Answer extraction as sequence tagging with tree edit distance", "author": ["Yao et al.2013] Xuchen Yao", "Benjamin Van Durme", "Chris Callison-Burch", "Peter Clark"], "venue": "In HLT-NAACL,", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Question retrieval with high quality answers in community question answering", "author": ["Zhang et al.2014] Kai Zhang", "Wei Wu", "Haocheng Wu", "Zhoujun Li", "Ming Zhou"], "venue": "In CIKM", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Phrase-based translation model for question retrieval in community question answer archives", "author": ["Zhou et al.2011] Guangyou Zhou", "Li Cai", "Jun Zhao", "Kang Liu"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 10, "context": "Recent work on deep neural networks (DNNs) for Multitask Learning (MTL) (Collobert and Weston, 2008; Liu et al., 2015) showed that is possible to jointly train a general system that solves different tasks simultaneously.", "startOffset": 72, "endOffset": 118}, {"referenceID": 8, "context": "In particular, the latter is composed of a sequence of convolution and pooling feature maps have achieved the state of the art in various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014).", "startOffset": 148, "endOffset": 186}, {"referenceID": 9, "context": "In particular, the latter is composed of a sequence of convolution and pooling feature maps have achieved the state of the art in various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014).", "startOffset": 148, "endOffset": 186}, {"referenceID": 2, "context": "For this purpose, we encode the relation in forms of discrete features, as described in (Collobert et al., 2011), i.", "startOffset": 88, "endOffset": 112}, {"referenceID": 11, "context": "Word Embeddings: for all the proposed models, we pre-initialize the word embedding matrices with standard skipgram embedding of dimensionality 50 trained on the English Wikipedia dump using word2vec toolkit (Mikolov et al., 2013).", "startOffset": 207, "endOffset": 229}, {"referenceID": 12, "context": "In fact, early stopping (Prechelt, 1998) allows us to avoid overfitting and improving the generalization capabilities of the network.", "startOffset": 24, "endOffset": 40}, {"referenceID": 15, "context": "To improve generalization and avoid coadaptation of features, we opted for adding dropout (Srivastava et al., 2014) between all the", "startOffset": 90, "endOffset": 115}, {"referenceID": 6, "context": "For example, Jeon et al. (2005) and Zhou et al.", "startOffset": 13, "endOffset": 32}, {"referenceID": 6, "context": "For example, Jeon et al. (2005) and Zhou et al. (2011) used a language models based on word or phrase translation probabilities for estimating similarity between questions.", "startOffset": 13, "endOffset": 55}, {"referenceID": 0, "context": "Language models for question-question similarity were also explored by Cao et al. (2009). These models exploit information from the category structure of Yahoo! Answers when computing similarity between two questions.", "startOffset": 71, "endOffset": 89}, {"referenceID": 0, "context": "Language models for question-question similarity were also explored by Cao et al. (2009). These models exploit information from the category structure of Yahoo! Answers when computing similarity between two questions. Instead, Duan et al. (2008) propose an approach that identifies the topic and focus in a text and compute similarity between two input questions by matching the extracted topic and focus information.", "startOffset": 71, "endOffset": 246}, {"referenceID": 0, "context": "Language models for question-question similarity were also explored by Cao et al. (2009). These models exploit information from the category structure of Yahoo! Answers when computing similarity between two questions. Instead, Duan et al. (2008) propose an approach that identifies the topic and focus in a text and compute similarity between two input questions by matching the extracted topic and focus information. A different approach to question-question similarity is provided by Ji et al. (2012) and Zhang et al.", "startOffset": 71, "endOffset": 503}, {"referenceID": 0, "context": "Language models for question-question similarity were also explored by Cao et al. (2009). These models exploit information from the category structure of Yahoo! Answers when computing similarity between two questions. Instead, Duan et al. (2008) propose an approach that identifies the topic and focus in a text and compute similarity between two input questions by matching the extracted topic and focus information. A different approach to question-question similarity is provided by Ji et al. (2012) and Zhang et al. (2014). They use LDA to learn the probability distribution over the topics that generate the question/answers pairs.", "startOffset": 71, "endOffset": 527}, {"referenceID": 20, "context": "For example, Yao et al. (2013) trained a conditional random field based on a set of powerful features, such as tree-edit distance between question and answer trees: these also enable the extraction of answers from pre-retrieved sentences.", "startOffset": 13, "endOffset": 31}, {"referenceID": 17, "context": "Wang et al. (2007) propose the use of Quasi-synchronous grammars to select", "startOffset": 0, "endOffset": 19}, {"referenceID": 4, "context": "A recent work close to ours is (Guzm\u00e1n et al., 2016), where the authors build a neural network for solving Task A of SemEval.", "startOffset": 31, "endOffset": 52}], "year": 2017, "abstractText": "In this paper, we developed a deep neural network (DNN) that learns to solve simultaneously the three tasks of the cQA challenge proposed by the SemEval-2016 Task 3, i.e., question-comment similarity, question-question similarity and new question-comment similarity. The latter is the main task, which can exploit the previous two for achieving better results. Our DNN is trained jointly on all the three cQA tasks and learns to encode questions and comments into a single vector representation shared across the multiple tasks. The results on the official challenge test set show that our approach produces higher accuracy and faster convergence rates than the individual neural networks. Additionally, our method, which does not use any manual feature engineering, approaches the state of the art established with methods that make heavy use of it.", "creator": "LaTeX with hyperref package"}}}