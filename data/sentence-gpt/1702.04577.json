{"id": "1702.04577", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2017", "title": "On the Discrepancy Between Kleinberg's Clustering Axioms and $k$-Means Clustering Algorithm Behavior", "abstract": "This paper investigates the validity of Kleinberg's axioms for clustering functions with respect to the quite popular clustering algorithm called $k$-means. While Kleinberg's axioms have been discussed heavily in the past, we concentrate here on the case predominantly relevant for $k$-means algorithm, that is behavior embedded in Euclidean space. We point at some contradictions and counter intuitiveness aspects of this axiomatic set within $\\mathbb{R}^m$ that were evidently not discussed so far. Our results suggest that apparently without defining clearly what kind of clusters we expect we will not be able to construct a valid axiomatic system. In particular we look at the shape and the gaps between the clusters. Finally we demonstrate that there exist several ways to reconcile the formulation of the axioms with their intended meaning and that under this reformulation the axioms stop to be contradictory and the real-world $k$-means algorithm conforms to this axiomatic system. To address this discrepancy we introduce the following axiomatic axioms: The axiomatic $k$-means algorithm is built by following two criteria: 1) the assumption that there is no single single, or single-dimensional, point of the axiomatic system, and 2) that this axiomatic system is invariant. The theorem that an axiomatic system is invariant in all three conditions is satisfied by the assumption that there is a single-dimensional or single-dimensional point of the axiomatic system and 2) that all three conditions are invariant in all three conditions are satisfied by the assumption that there is no single-dimensional or single-dimensional point of the axiomatic system.\n\n\n\n2) We find that $k$-means is indeed the best answer to this question, given that if there is a single-dimensional or single-dimensional point of the axiomatic system, if there is a single-dimensional or single-dimensional point of the axiomatic system, then $k$-means becomes the only value for which $k$-means is the value of the axiomatic system. To make sense of this, we introduce the following axiomatic axioms:\n2) In the first place, $k$-means is the least-known value that exists in $\\mathbb{R}^m$, and $k$-means is not the same as $k$-", "histories": [["v1", "Wed, 15 Feb 2017 12:25:28 GMT  (103kb,D)", "https://arxiv.org/abs/1702.04577v1", null], ["v2", "Mon, 24 Apr 2017 06:48:26 GMT  (105kb,D)", "http://arxiv.org/abs/1702.04577v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["robert k{\\l}opotek", "mieczys{\\l}aw k{\\l}opotek"], "accepted": false, "id": "1702.04577"}, "pdf": {"name": "1702.04577.pdf", "metadata": {"source": "CRF", "title": "On the Discrepancy Between Kleinberg\u2019s Clustering Axioms and k-Means Clustering Algorithm Behavior", "authors": ["Robert K LOPOTEK"], "emails": ["robert@klopotek.com.pl", "klopotek@ipipan.waw.pl"], "sections": [{"heading": null, "text": "Index Terms\u2014 clusterability, learnability, Kleinberg axioms, k-means\nar X\niv :1\n70 2.\n04 57\n7v 2"}, {"heading": "1 Introduction", "text": "One of important application areas of machine learning is the so-called cluster analysis or clustering, referred to also as unsupervised learning or learning without a teacher. It seeks to split a set of items into subsets (usually disjoint, though not necessarily, possibly with the subsets forming a hierarchy) called clusters or groups that should be \u201dsimilar\u201d within the clusters and \u201ddissimilar\u201d between them. Additional criteria like group balancing, group size limits from below and above etc. may be also taken into account. Subsequently let us restrict somehow the meaning of these terms. By partition we will understand the output of the process of cluster analysis. So the partition would be an object - a set of objects called clusters that are sets of original items (called elements).\nAs the diversity of clustering methods grows, there exists a strong pressure for finding some formal framework to get a systematic overview of the expected properties of the partitions obtained.\nA number of axiomatic frameworks have been devised for methods of clustering, the most cited probably the Kleinberg\u2019s system [20]1. Kleinberg defines [20] clustering function as\nDefinition 1. Clustering function is \u201da function f that takes a distance function d on [set] S [of size n \u2265 2] and returns a partition \u0393 of S. The sets in \u0393 will be called its clusters.\u201d We are interested only in such partitions \u0393 of S that \u222aC\u2208\u0393C = S, Ci 6= \u2205 and for any two distinct Ci, Cj \u2208 \u0393 Ci \u2229 Cj = \u2205.\nAdditionally, he defines the distance as\nDefinition 2. \u201dwith the set S = {1, 2, . . . , n} [...] we define a distance function to be any function d : S \u00d7S \u2192 R such that for distinct i, j \u2208 S we have di, j) > 0, d(i, j) = d(j, i) and d(i, i) = 0. One can optionally restrict attention to distance functions that are metrics by imposing the triangle inequality: d(i, k) \u2264 d(i, j)+d(j, k), for all i, j, k \u2208 S. We will not require the triangle inequality [...], but the results to follow both negative and positive still hold if one does require\u201d\nJon Kleinberg [20] claims that a good partition may only be a result of a reasonable method of clustering and he formulated axioms, for distance-based cluster analysis, that need to be met by the clustering method itself. He postulated that some quite \u201dnatural\u201d axioms need to be met, when we manipulate the distances between objects. As, however, the axioms proved to be not applicable to all clustering algorithms, we will rather speak about properties that Kleinberg expects of clustering functions, following e.g. Ackerman et al. [4]. These are:\nProperty 1. The method should allow to obtain any partition of the objects (so-called richness property)2,\n1Google Scholar lists about 400 citations. 2 \u201dlet Range(f) denote the set of all partitions \u0393 such that f(d) = \u0393 for some distance\nfunction d. Range(f) is equal to the set of all partitions of S.\u201d [20]\nProperty 2. The method should deliver partitions invariant with respect to distance scale (so-called scale-invariance property)3,\nProperty 3. The method should deliver the same partition if we move elements within a cluster closer to one another and elements from different clusters further away (so-called consistency property)4.\nNote that invariance and consistency properties assume a transformation on the clusters. With respect to this transformations we will speak about invariance transform(ation) and consistency transform(ation).\nSubsequently, while referring to Kleinberg\u2019s axiomatic systems, we will use the term \u201daxioms\u201d, but keeping in mind, that researchers treat them rather as properties that some algorithms have and other don\u2019t.\nKleinberg demonstrated that the above three \u201daxioms\u201d (properties) cannot be met all at once. So Kleinberg\u2019s work points at an important issue that we shall first of all revise our expectations towards the obtained partition, as the seemingly obvious axiom set is apparently not sound. In particular he stated the Impossibility Theorem.\nTheorem 1. [20, Theorem 2.1] For each n \u2265 2, there is no clustering function f that satisfies Scale-Invariance, Richness, and Consistency.\nKleinberg himself proved this theorem in the above-mentioned paper. Another proof can be found in a paper by Ambroszkiewicz and Koronacki [7], along with some discussion of the Kleinberg\u2019s concepts. Ackerman et al. [4] prove a bit more general impossibility theorem (engaging so called inner-consistency and outer-consistency).\nBeside providing a proof that his axioms are contradictory, Kleinberg showed that the axioms can be met pairwise. He uses for purpose of this demonstration versions of the well-known statistical single-linkage procedure. The versions differ by the stopping condition:\n\u2022 k-cluster stopping condition (which stops adding edges when the subgraph first consists of k connected components) - not \u201drich\u201d,\n\u2022 distance-r stopping condition (which adds edges of weight at most r only) - not scale-invariant,\n\u2022 scale-stopping condition (which adds edges of weight being at most some percentage of the largest distance between nodes) - not consistent5.\n3 \u201dFor any distance function d and any \u03b1 > 0, we have f(d) = f(\u03b1 \u00b7 d).\u201d [20] 4 \u201dLet \u0393 be a partition of S, and d and d\u2032 two distance functions on S. We say that d\u2032 is a \u0393-transformation of d if (a) for all i, j \u2208 S belonging to the same cluster of \u0393, we have d\u2032(i, j) \u2264 d(i, j) and (b) for all i, j \u2208 S belonging to different clusters of \u0393, we have d\u2032(i, j) \u2265 d(i, j). Let d and d\u2032 be two distance functions. If f(d) = \u0393, and d\u2032 is a \u0393transformation of d, then f(d\u2032) = \u0393\u201d [20]. This should reflect the property of reducing distance within a cluster and enlarging that between the clusters.\n5 Notice that, as demonstrated by Kleinberg in his paper, also k-median and k-means clustering do not have the consistency property.\nNote, however, that Ben-David and Ackerman [10] drew attention by an illustrative example (their Figure 2), that consistency is a problematic property by itself as it may give rise to new clusters at micro or macro-level.\nLet us draw attention to the fact that by introduction of his definition of clustering function, Kleinberg introduces implicitly two additional axioms onto the clustering function:\nProperty 4. A clustering function always returns a clustering ( nonrefutability).\nProperty 5. A clustering function works even if the distances cannot be embedded in Euclidean space ( permission of non-embeddability).\nThe well-known k-means clustering algorithm seeks to minimize the function6\nQ(\u0393) = m\u2211 i=1 k\u2211 j=1 uij\u2016xi \u2212 \u00b5j\u20162 = k\u2211 j=1 1 nj \u2211 xi,xl\u2208Cj \u2016xi \u2212 xl\u20162 (1)\nfor a dataset X under some partition \u0393 into the predefined number k of clusters, where uij is an indicator of the membership of data point xi in the cluster Cj having the center at \u00b5j .\nWe will call \u201dk-means-ideal\u201d such an algorithm that finds a \u0393opt that attains the minimum of function Q(\u0393). It is known that it is a hard task. 7 Hence in practice an algorithm is used with the following structure:\n1. Initialize k cluster centers \u00b51, . . . ,\u00b5k.\n2. Assign each data element xi to the cluster Cj identified by the closest \u00b5j .\n3. Update \u00b5j of each cluster Cj as the gravity center of the data elements in Cj .\n4. Repeat steps 2 and 3 until reaching a stop criterion (usually no change of cluster membership).\nIf step 1 is performed as random uniform sampling (without replacement), then we will speak about k-means-random algorithm. If step 1 is performed according to k-means++ heuristics proposed by Arthur and Vassilvitskii [8], then we will speak about k-means++ algorithm. Note that both attain a local minimum at worst. We will also touch the \u201dincremental\u201d k-means discussed by\n6 The considerations would apply also to kernel k-means algorithm using the quality function\nQ(\u0393) = m\u2211 i=1 k\u2211 j=1 uij\u2016\u03a6(xi \u2212 \u00b5\u03a6j \u20162\nwhere \u03a6 is a non-linear mapping from the original space to the so-called feature space. 7There exists a whole stream of research papers that attempt to approximate k-meansideal within a reasonable error bound via cleverly initiated k-means type algorithms, e.g. k-means++, like [24], but it has to be stated that at the current point these algorithms are rather of theoretical value.\nAckerman and Dasgupta [6]. This k-means version does not guarantee to reach a local minimum and has purely theoretical virtues.\nThe verification of Kleinberg\u2019s axioms for k-means is a bit difficult because even for k-means-ideal we cannot guarantee that there exists a single (global) minimum of the Q function. But if we talk instead of the set of all possible minimizing \u0393s, then it is easily seen that it is scale-invariant, but one sees immediately that it is not rich (only partitions with k clusters are considered). It has also been demonstrated by Kleinberg that it is not consistent.\nWith k-means-random and k-means++ it is even worse, as Q hits usually a local minimum there. So we can talk about a random variable assuming particular \u0393 with some probability. Under this assumption, again it is easily seen that both are scale-invariant, but one sees immediately that none is rich (only partitions with k clusters are considered). As k-means-ideal is not consistent, so neither of the realistic variants is so.\nHence the widely used algorithm violates in practice two of three Kleinberg\u2019s axioms. so that it cannot be considered to be a \u201dclustering function\u201d. We perceive this to be at least counterintuitive. Ben-David and Ackerman in [10] in section 4.2., raised also similar concern from the perspective of what an axiomatic system should accomplish. They state that one would expect, for the axiomatised set of objects, a kind of soundness and completeness. By soundness they mean that most useful clustering algorithms would fit the axioms. The completeness expresses that apparent non-clustering algorithms would fail on at least one axiom. While Kleinberg\u2019s axioms explicitly address the distance-based clustering algorithms (and not e.g. density based ones), they fall apparently short of reaching this goal. In this paper we demonstrate that even for a narrower set of algorithms, ones over data embedded in Euclidean space, the axioms fail.\nThere exist a number of open questions on why it is so. Recall that in [25] it has been observed by van Laarhoven and Marchiori that Kleinberg\u2019s proof of Impossibility Theorem stops to be valid in case of graph clustering. This raises immediately the question of its validity in Rm Euclidean space. Note that Kleinberg did not bother about embedding the distance in such a space. So one may ask whether or not k-means does not fit Kleinberg\u2019s axioms because this is a peculiar property of k-means or because any algorithm embedded in Euclidean space would fail to fit.\nPaying a special attention to k-means algorithm does not constitute a too restrictive limitation. k-means is applied in many domains, not only in its natural domains of data embedded in Rm, where clusters may be enclosed into Voronoi regions, but also to non-linearly separable clusters (via kernel functions [18]), to manifolds [28], in spectral clustering [15] and in community detection in social networks [22]. It has been demonstrated by Dhillon et al. [15] that kmeans is equivalent in some sense to normalized cut method of graph clustering, which in turn can be viewed as equivalent to balanced Newman\u2019s modularity, used in community detection, as shown by Bolla [12]. Crisp and fuzzy versions are used.\nAmong others, the equivalence results on k-means and graph clustering in\n[15] and the impossibility theorem challenge for graphs in [25] encourage to investigate Kleinberg\u2019s axioms in the context of Euclidean space.\nTherefore we made an effort to identify and overcome at least some reasons for the difficulties connected with axiomatic understanding of research area of cluster analysis and hope that this may be a guidance for further generalizations to encompass if not all then at least a considerable part of the real-life algorithms. This paper investigates why the k-means algorithm violates the Kleinberg\u2019s axioms for clustering functions. We claim that the reason is a mismatch between informal intuitions and formal formulations of these axioms. We claim also that there is a way to reconcile k-means with Kleinberg\u2019s consistency requirement via introduction of centric consistency and motion consistency which are neither a subset nor superset of Kleinberg\u2019s consistency, but rather a kmeans clustering model specific adaptation of the general idea of shrinking the cluster or moving cluster away.\nOur contribution is as follows:\n\u2022 To substantiate our claim that there is a mismatch between informal intuitions and formal formulations of Kleinberg\u2019s axioms, we present a series of carefully constructed examples.\n\u2022 We show in Section 3 that by the implicit non-embeddability axiom alone Kleinberg precludes consideration of k-means as a clustering algorithm.\n\u2022 We demonstrate in Section 2.1 that richness and scaling-invariance alone may lead to a contradiction for a special case. This denies Kleinberg\u2019s claims that his axioms can be fulfilled pair-wise.\n\u2022 In Section 4 we show that known relationships between Kleinberg\u2019s axioms and k-means apply also for Euclidean space, that is k-richness is granted, richness or near richness is not achievable, consistency is violated. We show also that the refinement consistency is violated too.\n\u2022 We show in Section 5 that in Rm scaling invariance transformations, by interference, annihilate effects of consistency transformation that is clusters being further away may get closer to one another.\n\u2022 Furthermore we show in Section 6 that consistency alone leads to contradictions. We demonstrate that in practical settings of application of many algorithms. In a metric m-dimensional space where m is the number of features, it is impossible to contract a single cluster without moving the other ones and as a consequence running at risk of moving some clusters closer together. Also we show that k-means version where we allow for k to range over a set, will change the optimal clustering k when Kleinberg\u2019s \u0393 operation (consistency transform) is applied.\n\u2022 We demonstrate in Section 7 that also the richness axiom denies common sense by itself, as it is unrealistic to be achieved by k-means-ideal, kmeans-random and k-means++.\n\u2022 We propose a reformulation of the Kleinberg\u2019s axioms in accordance with the intuitions and demonstrate that under this reformulation the axioms stop to be contradictory (Section 8). In particular we introduce the notion of centric consistency which is an adaptation of the general idea of shrinking the cluster. It relies simply on moving cluster elements towards its center. We provide an example of a clustering function that fits the axioms of near-richness, scale-invariance and possesses the property of centric consistency, so that it is clear that they are not contradictory.\n\u2022 We show that k-means is centric-consistent (Section 9). This implies that even a real-world algorithm like k-means conforms to the above-mentioned augmented axiomatic system (section 9).\n\u2022 As the centric consistency imitates only the consistency inside a cluster, we introduce also the notion of motion consistency, to approximate the consistency property outside a cluster and show that k-means, in order to be motion-consistent (Section 10), must impose the requirement of a gap between clusters. The introduction of gap requirement, on the other hand, violates Kleinberg\u2019s non-refutability axiom (Property 4).\n\u2022 We investigate the issue of gaps between clusters and show appropriately designed gaps induce local minima (section 11) for k-means and formulate conditions under which the gap leads to a global minimum for k-means (section 12).\n\u2022 Based on the above, we propose an alternative approach to reconcile Kleinberg\u2019s axioms with k-means. We demonstrate that under assumption of appropriate gaps we can either relax centric consistency to inner cluster consistency or go over from k-richness to an approximation of richness (sections 11 and 12)\nWe start this paper with a review of the previous work on development of an axiomatic system (Section 2) and round the paper up with a discussion of some open problems (Section 13)."}, {"heading": "2 Previous work", "text": "Axiomatic systems may be traced back to as early as 1973, when Wright [29] proposed axioms of clustering functions creating unsharp partitions, similar to fuzzy systems. In his framework every domain object was attached a positive real-valued weight, that could be distributed among multiple clusters.\nIn general, as exposed by van Laarhoven and Marchiori [25] and Ben-David and Ackerman [10] the clustering axiomatic frameworks address either:\n\u2022 required properties of clustering functions, or\n\u2022 required properties of the values of a clustering quality function, or\n\u2022 required properties of the relation between qualities of different partitions (ordering of partitions for a particular set of objects and distance or similarity or dissimilarity relations).\nOne of prominent axiomatic sets, that were later fiercely discussed, was that of Kleinberg, as already stated. From the point of view of the above classification, it imposes restrictions on the clustering function itself.\nWe have already discussed the Impossibility Theorem of Kleinberg that demonstrates the contradiction between the axioms of the set. However, there are further problems with this set, not covered by that Theorem. So Ben-David and Ackerman [10], as mentioned, pointed at the problems with consistency as such. They showed in an example in their Fig.2 that when moving clusters away the clusters themselves can create new groups. In this paper we repeat their findings for fix-dimensional Euclidean space, but we go beyond that. We draw attention to the fact that in Rm moving clusters away may be completely impossible without going into other dimension. Furthermore we show that also shrinking of a single cluster in a consistent way is also impossible in Rm. We demonstrate that interaction of consistency transformation and scaling-invariance transformation actually does something contrary to intuition behind consistency, that is it pulls cluster closer instead of pushing them away.\nA number of relaxations of axioms related to clustering functions have been proposed in order to overcome the Kleinberg\u2019s impossibility result. We recall several of them here, based on an overview by Ackerman [1] and tutorial by Ben-David [9].\nSo it was proposed to weaken Kleinberg\u2019s richness (by Kleinberg himself) to so-called k-richness as follows:\nProperty 6 (Zadeh and Ben-David [30]). For any partition \u0393 of the set X consisting of exactly k clusters there exists such a distance function d that the clustering function f(d) returns this partition \u0393.\nThis relaxation8 allows for some algorithms splitting the data into a fixed number of clusters, like k-means, not to be immediately discarded as \u201dclustering algorithms\u201d, given that no cluster is allowed to be empty.9\nHowever, this weakening of Kleinberg\u2019s axioms does not suffice to make kmeans a \u201dclustering function\u201d as it still violates consistency axiom.\nAckerman et al. [4] propose the concept of outer-consistency\nProperty 7. The method is said to be outer-consistent if it delivers the same partition if one increases only distances between elements from different clusters and lets the distances within clusters unchanged.\n8Still another relaxation of richness was proposed by Hopcroft and Kannan [19]: Richness II: For any set K of k distinct points in the given Euclidean space, there is an n and a set of S of n points such that the algorithm on input S produces k clusters, whose centers are the respective points in K. Here the weakness lies in the fact that the k points may be subject to clustering themselves in reasonable algorithms.\n9Even k-richness is still a problematic issue because as demonstrated by Ackerman et al. [5], a useful property of stability of clusters under malicious addition of data points holds only for balanced clusters.\nk-means algorithm is said to be in this sense outer-consistent.10 They propose also so-called inner consistency\nProperty 8. The method is said to be inner-consistent if it delivers the same partition when one decreases only distances between elements from same cluster and lets the distances between elements of different clusters unchanged.\nk-means algorithm is in this sense not inner-consistent. Later we will discuss the representation problem for this type of consistency with k-means. Let us mention here that they prove that (1) no general clustering function can simultaneously satisfy outer-consistency, scale- invariance, and richness, and (2) no general clustering function can simultaneously satisfy inner-consistency, scaleinvariance, and richness. They claim also that k -means-ideal has the properties of outer-consistency and locality11. None of these properties is claimed to be satisfied by k-means-random nor by a k-means with furthest element initialization. Furthermore, k-richness (in probabilistic sense) is not matched by k-means-random algorithm. In this paper we point at the fact that in Euclidean space even inner-consistency alone (see our Theorem 8) / outer-consistency alone (see our Theorem 10) are self-contradictory. Also the consistency alone poses problem (see our Theorem 7) So they do so for k-means-ideal. But on the other hand we show k-richness (in probabilistic sense) is matched by k-means-random algorithm (see our Theorem 5).\nStill another relaxation of the Kleinberg\u2019s consistency is called Refinement Consistency. It is a modification of the consistency axiom by replacing the requirement that f(d) = f(d\u2032) with the requirement that one of f(d), f(d\u2032) is a refinement of the other. A partition \u0393\u2032 is a refinement of a partition \u0393 if for each cluster c\u2032 \u2208 \u0393\u2032 there exists a cluster c \u2208 \u0393 such that c\u2032 \u2286 c. Obviously the replacement of the consistency requirement with refinement consistency breaks the impossibility proof of Kleinberg\u2019s axiom system. But there is a practical concern: In general, refinement consistency means that by the \u0393 transformation and scaling you may transform any clustering in any other. The usefulness of such an axiom is hence questionable. In this paper (Section 12) we show that under some circumstances unidirectional refinement consistency may be achieved, which makes much more sense.\nZadeh Ben-David [30] propose instead the order-consistency so that some versions of single-linkage algorithm can be classified as \u201dclustering algorithm\u201d. For any two distance functions d and d\u2032, if the orderings of edge lengths are the same then f(d) = f(d\u2032). k-means is not order-consistent.\nOne could also relax Scale-Invariance instead to e.g. Robustness, that is, \u201dSmall changes in distance function d should result in small changes of partition f(d)\u201d. The basic problem here is that partitions are discrete and the term \u201dsmall\u201d is hard to define reasonably. Small changes in distances may result in major changes of partitions obtained from k-means algorithm.\n10We show, however, that this is not true. 11 A clustering function clustering into k clusters has the locality property, if whenever a set S for a given k is clustered by it into the partition \u0393, and we take a subset \u0393\u2032 \u2282 \u0393 with |\u0393\u2032| = k\u2032 < k, then clustering of \u222aC\u2208\u0393\u2032 into k\u2032 clusters will yield exactly \u0393\u2032.\nLet us also mention here some works like that by Dunn [17] or Ackerman and Dasgupta [6] that seemingly have nothing to do with Kleinberg\u2019s axioms, but this is only a superficial impression. Papers discussing the issue of \u201dwell separated clusters\u201d or \u201dnicely separated\u201d, or \u201dperfectly separated \u201d point in fact at the weakness of the non-refutability axiom, because it is apparent that we do not want to get any partition but rather one that is meaningful.\nAckerman and Dasgupta [6] handle incremental clustering algorithms. They introduce an incremental version of k-means algorithm. The clusters are \u201dnicely separated\u201d, as defined by [6], if a distance between an element and any other element of the same cluster is lower than the distance from this element to an element outside of the cluster. The authors demonstrate that no incremental algorithm of space complexity linear in k can (routinely) discover the clusters that are nicely separated. This is contrary to single-link algorithm which can identify a set of 2k\u22121 candidate elements among which k are from different clusters, if a nice clustering is unique. A nice clustering can be only detected in this sense (a set of candidates) by an incremental algorithm with memory linear in 2k\u22121. But it cannot be detected by the incremental k-means even with such a large memory. However, when looking at the issue with randomly generated sequence of data, a memory linear in k suffices for incremental kmeans with some probability. Then they introduce the \u201dperfect clustering\u201d with the property that the smallest distance between elements of distinct clusters is larger than the distance between any two elements of the same cluster. They demonstrate that there exists an incremental algorithm discovering the \u201dperfect clustering\u201d that is linear in k with respect to space. But the incremental kmeans fails to do so. We will discuss this issue in section 11.\nAckerman and Ben-David [10] propose another direction of resolving the problem of Kleinberg\u2019s axiomatisation impossibility. Instead of axiomatising the clustering function, one should rather create axioms for cluster quality function.\nA number of further characterizations of clustering functions has been proposed to overcome Kleinberg axiom problems, e.g. [2] for linkage algorithms, [14] for hierarchical algorithms, [13] for multiscale clustering.\nNote that beside Kleinberg\u2019s axioms there exist other \u201dimpossible\u201d characterizations of clustering functions. Meila [23] demonstrates that one can\u2019t compare partitions in a manner that agrees with the lattice of partitions, is convexly additive and bounded.\nGeneral tendency of researchers wanting to overcome Kleinberg\u2019s contradiction was to weaken one or more axioms of Kleinberg. While in this way the contradiction was removed, the removal relied on weakening the reasoning capabilities so that no strong conclusions can be reached. In this research we try the different way - one of strengthening the assumptions so that for example a proof of k-means consistency becomes possible.\nBut before we present a consistent set of algorithm properties and show its validity for k-means algorithm, we will investigate counter-intuitiveness of Kleinberg\u2019s formalization of his axioms.\nLet us still mention briefly, that other characteristics of k-means algorithms were studied in the past, see e.g. papers by Ackerman et al. [3, 5]. [5] deals\nwith the susceptivity of among others the k-means algorithm to hostile addition of new points to the data set. It turns out that k-means is stable under such disturbances given that the clusters are well balanced (cluster sizes do not differ very much) and there are sufficient gaps between the clusters. [3] demonstrates that one can put any two data points into different clusters if one applies weighting functions to data points. Both of these papers, though not explicitly addressing the k-richness, demonstrate problems resulting from this axiom. [5] implies that too small clusters may be disintegrated by hostile new points so that for practical purposes one shall be only interested in larger clusters. [3] allows to conclude that poor estimates of densities for sparse clusters may lead to erroneous drawing of cluster boundaries."}, {"heading": "2.1 Counter-intuitiveness of Scale-invariance and Consistency Axioms", "text": "Kleinberg in his paper proved so-called anti-chain theorem that implies that by scaling and contraction (\u0393-transform) one can transform any clustering into any other.12 This fact combined with the richness axiom leads directly to contradiction in the three axioms.\nFirst of all let us state that\nTheorem 2. For n = 2, for data in Rm, there is no clustering function f that satisfies Scale-Invariance and Richness.13\nProof. Any set S = {e1, e2} consisting of only two elements has potentially two partitions: \u03931 = {{e1}, {e2}} (\u201dsingleton partition\u201d) and \u03932 = {e1, e2} (\u201dnosplit-partition\u201d). Let f(d1, S) = \u03931 and f(d2, S) = \u03932. Then f(\nd2(e1,e2) d1(e1,e2) d1, S) =\n\u03931 according to scale-invariance but by definition f( d2(e1,e2) d1(e1,e2) d1 = d2, S) = \u03932, so we have an obvious contradiction.\nNote that this theorem strengthens the result of Kleinberg stated in Theorem 1 - two kleinberg\u2019s properties/axioms already (and not three) lead to a contradition.\nAs we will demonstrate later, a function matching richness axiom of Kleinberg does not necessarily exhibit richness, if distances will be confined to Rm. But in case of the above theorem it does not matter because we talk about 2 data points only and hence automatically the validity as distance in Rm is granted.\nIt is further easy to show (also based on Kleinberg\u2019s anti-chain theorem) that14\n12 It is why Kleinberg proposed in his paper the \u201drefinement consistency\u201d. 13Contrary to Kleinberg\u2019s intuitions, scale-invariance plus richness alone lead to a contradiction. 14This shows that richness is not needed at all to get a contradiction from consistency and scale-invariance\nTheorem 3. For any n > 2, for data in Rm, no function f can produce no-split partition \u03931 under some distance function d1 and any other partition \u03932 under some distance function d2 if it satisfies both consistency and scale-invariance properties.\nBy the way this variant of Kleinberg\u2019s anti-chain theorem is a reason why he proposed to weaken richness requirement to \u201dnear-richness\u201d, omitting the \u201dall-in-one\u201d partition.15\nProof. To show this let mind = mine1,e2\u2208S d1 and maxd = maxe1,e2\u2208S d2. It is easy to see that d2 is a \u0393-transform of maxd mind d1 for partition \u03931. Therefore as f(d1, S) = \u03931, because of scale-invariance f( maxd mind d1, S) = f(d1, S) = \u03931, hence by consistency f(d2, S) = f( maxd mind d1, S) = \u03931. This contradicts the assumption that f(d2, S) = \u03932.\nAs mentioned, [25] pointed at the fact that such a construction would not be possible in the realm of graph clustering. We shall ask then: what about Rm? We provided the above proof to show that forcing data points into the Euclidean space does not invalidate the construction because the scaling operation keeps the points in the original Euclidean space.\nSo there is surely a need to redefine the richness property into a \u201dnearrichness\u201d.16\nBut \u201dnear-richness\u201d is again not enough to resolve all contradictions (as by the way is visible from the Kleinberg\u2019s anti-chain theorem [20]).\nTheorem 4. For any n > 1, for data in Rm (m > 2), no function f can produce a partition \u03931 consisting of two sets of elements \u03931 = {{1, 2, . . . , n}, {n+ 1, n+ 2}} under some distance function d1 and any other partition \u03932 consisting of three sets of elements \u03932 = {{1, 2, . . . , n}, {n+1}, {n+2}} under some distance function d2 if it satisfies both consistency and scale-invariance properties.\nProof. For n \u2265 1 take a set of n + 2 elements. The richness property implies that under two distinct distance functions d1, d2 the clustering function f may form two partitions: \u03931,\u03932, resp., as defined in the theorem. By invariance property, we can derive from d2 the distance function d4 such that no distance between the elements under d4 is lower than the biggest distance under d1. By invariance property, we can derive from d1 the distance function d3 such that the distance between elements n + 1, n + 2 is bigger than under d4. We have then f({1, . . . , n+2}; d4) = \u03932, f({1, . . . , n+2}; d3) = \u03931. Now let us apply the consistency axiom. From d4 we derive the distance function d6 such that for elements 1, . . . , n d1 and d6 are identical, the distance between n+1, n+2 is same as in d4 and the distances between any element of 1, . . . , n and any of n+1, n+2 is some l that is bigger than any distances between any elements under d1, . . . , d4. From d3 we derive the distance function d5 such that for elements 1, . . . , n d1\n15In fact the Kleinberg\u2019s anti-chain theorem implies that also a partition putting each element into a separate cluster should be excluded from \u201dnear-richness\u201d\n16A similar reasoning is possible for singleton partition, but we choose this way.\nand d5 are identical, the distance between n+ 1, n+ 2 is same as in d4 and the distances between any element of 1, . . . , n and any of n + 1, n + 2 is same l as above. We have then f({1, . . . , n+ 2}; d6) = \u03932, f({1, . . . , n+ 2}; d5) = \u03931. But then we have a contradiction because by construction d5 and d6 are identical.\nIn this proof, however, assumptions are made that may possibly be not correct if we require the distances to be distances in Euclidean space. So not for any configuration of n points a n+ 1-st point may be found to be equidistant to all the other ones. And even if it is so, it is not guaranteed that a second distinct n+ 2-nd point exists with the same property. Hence in the above construction of the proof, an initial step is needed, matching using consistency property, that will pose the points 1, . . . , n onto a sphere both for \u03931 and \u03932, and points n+1, n+2 on a line orthogonal to the subspace containing 1, . . . , n and passing through the origin of the sphere.\nIn the end, of course, the contradiction is still valid in Euclidean space, but this exercise shows that proofs of Kleinberg need to be rewritten if we deal with Euclidean spaces. But note that if we restrict ourselves to R2, posing the points onto a sphere does not work anymore. Points n+ 1 ad n+ 2 will become identical.\nSo, there is still an open question, whether or not we can have a clustering function matching Kleinberg\u2019s axioms, that is still not contradictory. We will at this issue below. in Rm."}, {"heading": "3 To embed or not to embed", "text": "Kleinberg\u2019s permission of non-embeddability axiom (Property 5) assumes that distances can be any non-negative symmetric functions over the set of pairs of objects.\nk-means normally operates in an Euclidean space, but by using so-called kernel-trick17 one can operate on the objects as if they were embedded in a (highly dimensional) space without actually finding the embedding (just working on a kernel matrix derived from distances). And one can get a clustering in that space optimizing the Q function.\nIt is well known that if there exists an embedding of a set of n points in an Euclidean space, then we do not need to consider more than n\u2212 1 dimensions.\nBut it is well known that not for each distance function in the sense of Kleinberg\u2019s definition there exists an embedding. Just consider the points in the table 1.\n17 We will not dive deeper in this paper into the discussion of properties of kernel k-means. Let us only make the remark that kernel k-means, given that there exists an embedding in Rm), is in fact k-means in the feature space. So all the findings related to k-means would apply also in the feature space. The weighted version of kernel k-means may be considered a bit tricky, but it can be \u201dapproximated\u201d by multiplying the unweighted points, under the restriction that all multiplied points will go into the same cluster, but this doss not seem to invalidate any findings. A separate question of course is whether or not we can invert the kernel function (if it is given explicitly) in order to find points transformed by e.g. centric consistency transform in the feature space."}, {"heading": "A 0 10 2.236 20 22.361 20.125", "text": "It is visible at the first glance that not even the triangle inequality holds in this data set (just look at points A,B,C alone). So no embedding in Euclidean space is possible.\nBut what if we still apply the kernel trick? One can easily find an embedding in a three-dimensional space if one allows for \u201dimaginary\u201d coordinates (allowing for square-rooting negative eigenvalues). See table 2. The distances are kept if we rigidly use the distance formula\nd(P, T ) = \u221a (xP,1 \u2212 xT,1)2 + (xP,2 \u2212 xT,3)2 + (xP,2 \u2212 xT,3)2\nA quick look into the table 1 would suggest that points A,B,C form one cluster, and D,E, F form another.\nHowever, if we take the centers of the respective clusters \u00b51 = (0.667 + 0.000i, 10.000 + 0.000i, 0.000 + 0.333i) and \u00b52 = (0.667 + 0.000i,\u221210.000 + 0.000i, 0.000 + 0.333i), then the Q function for such 2-means amounts to 100. But if we take the points S1 = (0 + 0.00i, 0 + 0.00i, 0 \u2212 10.18i), S2 = (0 + 0.000i, 0 + 0.000i, 0 + 9.198i) as cluster centers, then clusters {A,B,D,E} and {C,F, } are formed around them with Q function value equal 6 \u00b7 10\u22126.\nSo the Kleinberg\u2019s non-embeddability axiom is not suitable for clustering algorithms for which position of other points in space needs to be anticipated. Under the assumption of Euclidean embedding this problem is clearly solved.\nFrom now on we will always assume that, if not stated otherwise, we constrain the Kleinberg\u2019s consistency transform to the cases embeddable into a fixed dimensional Euclidean space.\nNote that with this result also the Kleinberg\u2019s non-refutability axiom is\nindirectly questioned.18\n4 Kleinberg\u2019s axioms and k-means \u2013 Conformance and Violations\nLet us briefly discuss here the relationship of k-means algorithm to the already mentioned axiomatic systems, keeping in mind that we apply it in Rm Euclidean space.\nScale-invariance is fulfilled because k-means qualifies objects into clusters based on relative distances to cluster centers and not their absolute values as may be easily seen from equation (1).19\nOn the other hand richness, a property denial of which has nothing to do with distances, hence with embedding in an Euclidean space, as already known from mentioned publications, e.g. [30], is obviously violated because k-means returns only partitions into k clusters.\nBut what about its relaxation that is k-richness. Let us briefly show here that\nTheorem 5. k-means algorithm is k-rich\nProof. We proceed by constructing a data set for each required partition. Let us consider n data points arranged on a straight line and we want to split them into k clusters fitting a concrete partition \u03930. For this purpose arrange the clusters on the line (left to right) in non-increasing order of their cardinality. Each cluster shall occupy (uniformly) a unit length. The space between the clusters (distance between closest elements of ith and (i+ 1)st cluster) should be set as follows: For i = 1, . . . , k \u2212 1 let dce(j, i) denote the distance between the most extreme data points of clusters j and i, cardc(j, i) shall denote the combined cardinality of clusters j, j + 1, . . . , i. The distance between closest elements of clusters i and i+ 1 shall be then set to 2 \u2217 dce(1, i) cardc(1,i)+cardc(i+1,i+1)cardc(i+1,i+1) . In this case application of k-means algorithm (k-means-ideal, k-meansrandom, k-means++) will lead the desired partition. The reasons are as follows: In case of k-means-ideal, let A be the most right cluster of a partition \u0393 different from \u03930, containing the \u201dspace between clusters\u201d. The definition of this distance is chosen in such a way that if we split A into two parts along this \u201dspace between clusters\u201d and attach the left and the right part to the neighboring clusters, and splitting any cluster if the number of clusters falls below k in this way, then the resulting new partition will be more optimal. Hence the optimal k-means-ideal clustering will not contain any \u201dspaces between\n18It does not mean that there do not exist versions of k-means for distances other than Euclidean distance. What we wanted to demonstrate here is that the notion of embedding is needed if we want to look at k-means from Kleinberg\u2019s axioms perspective.\n19However, this quality function fails on the axiom of Function Scale Invariance, proposed in [10].\nclusters\u201d within the clusters and be identical with the intended \u03930. This implies we can construct any partition in this way.\nIn case of k-means-random, if each of the clusters of \u03930 is seeded, the spaces between clusters of \u03930 are so large, that the clustering resulting from such a seeding is identical with \u03930 and upon subsequent steps the partition will not change any more. So consider now the case that after the random initialization (or at any later step) we get a partition \u0393 with cluster centers \u00b51, . . . \u00b5k such that there be a cluster C of \u03930 that has not been seeded (does not contain a \u00b5i in its range). No cluster of \u0393 with center to the right of C would nonetheless contain any data element from C. Consider therefore only clusters of \u03930 to the left of C and let \u00b5r be the cluster center most to the right in this set. The cluster Cr formed from elements closest to \u00b5r will contain C. Therefore during the cluster center update step of k-means-random \u00b5r will move to the right to a position, from which only C will be the set of points closest to \u00b5r. Therefore after 3 steps \u00b5r will become the center of C. Later on the same process will happen with other not seeded clusters of \u03930 to the left of it till we get the partition \u03930.\nAs k-means++ behaves similarly to k-means-random after initial seeding, the same effect will be reached. 20\n20 Let us formulate the argument more precisely. As mentioned earlier, following [4], for probabilistic algorithms we will talk about probabilistic k-richness, that is one obtainable with some probability, independent of the actual clustering that is intended to be obtained. The probability can further be increased if one wishes to.\nIn the above scheme we see that whenever during the initialization at each place more seeds are there than clusters, then they will spread to the right, given there are clusters without seeds there, ensuring that each cluster gets its cluster center. Note that if there are clusters lacking seeds to the left, there is no way to move seeds there. Hence, as the cluster s are sorted in decreasing size order from the left to right, the probability, that we have a seeding upon which by moving cluster centers to the right we can assign each cluster a cluster center amounts to at least k!/kk. This is computed as follows: The favorable seeding occurs, if the first seed is in the first cluster, and the ith seed in a cluster 1 or 2 or ... or i from the left. As the clusters are sorted non-increasingly, the probability of hitting the first cluster is at least 1 k , that of first or second 2 k , that of first, or second, or,. . . ,or ith is i k\n. This results in the aforementioned estimation.\nNote that the estimated probability is independent of the sample size and the actual distribution of sizes of clusters. It depends on k only.\nFurthermore, the targeted clustering is the absolute minimum of the k-means-ideal, hence we can run k-means-random multiple time in order to achieve the desired probability of krichness. E.g. if we need 95% certainty, we need to rerun k-means-random r times with r such that 1\u2212 (1\u2212 k!/kk)r \u2265 95%.\nThe issue with k-means++ is a bit more complex due to the way how probabilities of seeding are computed. In fact, we do not rely on the k-means iterating process, but have rather to ensure that each cluster gets a seed during the seeding phase.\nWhen the first seed is distributed, like in k-means, we have the assurance that an unhit cluster will be hit. The probability that a cluster is hit during the seeding step after the first one is proportional to the sum of squared distances of cluster elements to the closest seed assigned earlier. Consider the ith cluster (from the left) that was not hit so far. The closest hit cluster to the left can lie at least a distance\n\u03b1 \u2217 dce(1, i\u2212 1) cardc(1, i\u2212 1) + cardc(i, i)\ncardc(i, i)\nLet us stress here that there exist attempts to upgrade k-means algorithm to choose the proper k. The portion of variance explained by the clustering is used as quality criterion21. It is well known that increase of k increases the value of this criterion. The optimal k is deemed to be one when this increase\nand that to the right at\n\u03b1 \u2217 dce(1, i) cardc(1, i) + cardc(i+ 1, i+ 1)\ncardc(i+ 1, i+ 1)\n(note that the first cluster has no left neighbor, and the kth - no right neighbor). \u03b1 = 2. So the contribution of the ith cluster to the sum of squares estimation for hitting probability in a current state amounts to at least the smaller number of the following two:\ncardc(i, i) ( \u03b1 \u2217 dce(1, i\u2212 1)\ncardc(1, i\u2212 1) + cardc(i, i) cardc(i, i) )2 cardc(i, i) ( \u03b1 \u2217 dce(1, i) cardc(1, i) + cardc(i+ 1, i+ 1)\ncardc(i+ 1, i+ 1)\n)2 \u2265\n\u2265 cardc(i, i) ( \u03b1 \u2217 dce(1, i) cardc(1, i) + cardc(i+ 1, i+ 1)\ncardc(i, i) )2 Obviously the first expression is the smaller one so we will consider it only.\ncardc(i, i) ( \u03b1 \u2217 dce(1, i\u2212 1)\ncardc(1, i\u2212 1) + cardc(i, i) cardc(i, i)\n)2 = \u03b12 \u2217 dce(1, i\u2212 1)2 cardc(1, i)2\ncardc(i, i)\nNote that due to the non-increasing order of cluster sizes, cardc(1, i) \u2265 i \u00b7 cardc(i, i), and cardc(1, i) \u2265 in\nk . Therefore\n\u03b12 \u2217 dce(1, i\u2212 1)2 cardc(1, i)2\ncardc(i, i) \u2265 \u03b12 \u2217 dce(1, i\u2212 1)2i\nn\nk\nFurthermore, dce(1, 1) = 1, and dce(1, i) = dce(1, i \u2212 1) + 1 + \u03b1 \u2217 dce(1, i \u2212 1)\ncardc(1,i\u22121)+cardc(i,i) cardc(i,i)\n\u2265 dce(1, i\u2212 1) + 1 +\u03b1 \u2217 dce(1, i\u2212 1) \u00b7 i = 1 + dce(1, i\u2212 1) \u00b7 (1 + i\u03b1) \u2265 dce(1, i\u2212 1) \u00b7 (1 + i\u03b1) Hence\ndce(1, i) \u2265 (1 + 2\u03b1)i\u22121\nSo \u03b12 \u2217 dce(1, i\u2212 1)2i n\nk \u2265 \u03b12(1 + 2\u03b1)2(i\u22121)i\nn\nk After s seeds were distributed the sum of squared distances to the closest seed for hit clusters amounts to at most the combined cardinality of the clusters with seeds times 1 so this does not exceed n.\nTherefore the probability of hitting an unhit cluster after s seeds were already distributed and hit different clusters is not bigger than\n\u03b12(1 + 2\u03b1)2 n k\n+ \u2211k\u2212s\ni=2 \u03b1 2(1 + 2\u03b1)2(i\u22121)in k\nn+ \u03b12(1 + 2\u03b1)2 n k\n+ \u2211k\u2212s\ni=2 \u03b1 2(1 + 2\u03b1)2(i\u22121)in k\n= \u03b12(1 + 2\u03b1)2 +\n\u2211k\u2212s i=2 \u03b1 2(1 + 2\u03b1)2(i\u22121)i\nk + \u03b12(1 + 2\u03b1)2 + \u2211k\u2212s\ni=2 \u03b1 2(1 + 2\u03b1)2(i\u22121)i\nSo the probability that during the seeding all clusters are hit by a seed amounts to at least.\nk\u22121\u220f s=1\n\u03b12(1 + 2\u03b1)2 + \u2211k\u2212s\ni=2 \u03b1 2(1 + 2\u03b1)2(i\u22121)i k + \u03b12(1 + 2\u03b1)2 + \u2211k\u2212s\ni=2 \u03b1 2(1 + 2\u03b1)2(i\u22121)i\nIn order to increase the success probability we can now repeat the seed independently sufficiently many times, or we can increase the distances by letting \u03b1 be (much) greater than 2.\n21Such a quality function would satisfy axiom of Function Scale Invariance, proposed in [10]\nstops to be \u201dsignificant\u201d. The above construction could be extended to cover a range of k values to choose from. However, the full richness is not achievable because a split into two clusters will be better than keeping a single cluster, and the maximum is attained for this criterion if k = n. So either the clustering will be trivial or quite a large number of partitions will be excluded. However, even k-richness offers a large number of partitions to choose from.\nKleinberg himself proved via a bit artificial example (with unbalanced samples and an awkward distance function) that k-means algorithm with k=2 is not consistent. Kleinberg\u2019s counter-example would require an embedding in a very high dimensional space, non-typical for k-means applications. Also kmeans tends to produce rather balanced clusters, so Kleinberg\u2019s example could be deemed to be eccentric.\nLet us illustrate by a more realistic example (balanced, in Euclidean space) that this is a real problem. Let A,B,C,D,E, F be points in three-dimensional space with coordinates: A(1, 0, 0), B(33, 32, 0), C(33,\u221232, 0), D(\u22121, 0, 0), E(\u221233, 0,\u221232), F (\u221233, 0, 32). Let SAB , SAC , SDE , SDF be sets of say 1000 points randomly uniformly distributed over line segments (except for endpoints) AB,AC,DE,EF resp. Let X = SAB \u222a SAC \u222a SDE \u222a SEF . k-means with k = 2 applied to X yields a partition {SAB \u222aSAC , SDE \u222aSDF }. But let us perform a \u0393 transformation consisting in rotating line segments AB,BC around the point A in the plane spread by the first two coordinates towards the first coordinate axis so that the angle between this axis and AB\u2032 and AC \u2032 is say one degree. Now the k-means with k = 2 yields a different partition, splitting line segments AB\u2032 and AC \u2032.22\nWith this example not only consistency violation is shown, but also refinement-consistency violation."}, {"heading": "5 Problems with Consistency in Euclidean", "text": "Space\nHow does it happen that seemingly intuitive axioms lead to such a contradiction. We need to look more carefully at the consistency axiom in conjunction with scale-invariance. \u0393-transform does not do what Kleinberg claimed it should that is describing a situation when moving elements from distinct clusters apart and elements within a cluster closer to one another.23\nWe shall now demonstrate that application of scaling invariance axiom leads to violation of the consistency axiom of Kleinberg. More precisely:\n22In a test run with 100 restarts, in the first case we got clusters of equal sizes, with cluster centers at (17,0,0) and (-17,0,0), (between SS / total SS = 40 %) whereas after rotation we got clusters of sizes 1800, 2200 with centers at (26,0,0), (-15,0,0) (between SS / total SS = 59 %)\n23 Recall that the intuition behind clustering is to partition the data points in such a way that members of the same cluster are \u201dclose\u201d to one another, that is their distance is low, and members of two different clusters are \u201ddistant\u201d from one another, that is their distance is high. So it is intuitively obvious that moving elements from distinct clusters apart and elements within a cluster closer to one another should make a partition \u201dlook better\u201d.\nTheorem 6. For a clustering algorithm f , conforming to consistency and scaling invariance axioms, if distance d2 is derived from the distance d1 by consistency transformation, and d3 is obtained from d2 via scaling, then the existence of a d3 cannot always be obtained from d1 via consistency axiom transformation.\nProof. We prove the Theorem by finding a suitable example. let S consist of four elements e1, e2, e3, e4 and let a clustering function partition it into {e1}, {e2, e3}, {e4} under some distance function d1. One can easily construct a distance function d2 being a \u0393-transform of d1 such that d2(e2, e3) = d1(e2, e3) and d2(e1, e2)+d2(e2, e3) = d2(e1, e3) and d2(e2, e3)+d2(e3, e4) = d2(e2, e4) and d2(e1, e2) + d2(e2, e3) + d2(e3, e4) = d2(e1, e4) which implies that these points under d2 can be embedded in the space R that is the straight line. Without restricting the generality (the qualitative illustration) assume that the coordinates of these points in this space are located at points 0, 0.4, 0.6, 1 resp. Now assume we want to perform \u0393-transformation of Kleinberg (obtaining the distance function d3) in such a manner that the data points remain in R and move elements of the second set i.e. {e2, e3} (d2(e2, e3) = 0.2) closer to one another so that e2 = (0.5), e3 = (0.6) (d3(e2, e3) = 0.1). e1 may then stay where it is but e4 has to be shifted at least to (1.1) (under d3 the clustering function shall yield same clustering). Now apply rescaling into the original interval that is multiply the coordinates (and hence the distances, yielding d4) by 1/1.1. e1 stays at (0), e2 = ( 5 11 ), e3 = 6 11 , e4 = (1). e3 is now closer to e1 than before. We could have made the things still more drastic by transforming d2 to d \u2032 3 in such a way that instead of e4 going to (1.1), as under d3, we set it at (2). In this case the rescaling would result in e1 = (0), e2 = (0.25), e3 = (0.3), e4 = (1) (with the respective distances d\u20324) which means a drastic relocation of the second cluster towards the first - the distance between clusters decreases instead of increasing as claimed by Kleinberg. This is a big surprise. The \u0393 transform should have moved elements of a cluster closer together and further apart those from distinct clusters and rescaling should not disturb the proportions. It turned out to be the other way. This contradicts the consistency assumption.\nSo something is wrong either with the idea of scaling or of \u0393-transformation. We shall be reluctant to blame the scaling, except for the practical case when scaling down leads to indiscernibility between points with respect to measurement errors.\nNote that we do not observe such a clash between invariance and richness. If a set of distance functions demonstrates the richness of a clustering function conforming to richness and scaling, then after scaling all these distance functions demonstrate the richness of the same clustering function again. Scaling does not impair the richness."}, {"heading": "6 Counter-intuitiveness of Consistency Axiom", "text": "Alone\nSo we will consider counter-intuitiveness of consistency axiom. To illustrate it, recall first the fact that a large portion of known clustering algorithms uses data points embedded in anm dimensional feature space, usually Rm and the distance is the Euclidean distance therein. Now imagine that we want to perform a \u0393transform on a single cluster of a partition that is the \u0393-transform shall provide distances compatible with the situation that only elements of a single cluster change position in the embedding space.\nTheorem 7. Under the above-mentioned circumstances it is impossible to perform \u0393-transform reducing distances within a single cluster.\nProof. Assume the cluster is an \u201dinternal\u201d one that is for a point e in this cluster any hyperplane containing it has points from some other clusters on each side. Furthermore assume that other clusters contain together more than m data points, which should not be an untypical case. Here the problem starts. The position of e is determined by the distances from the elements of the other clusters in such a way that the increase of distance from one of them would necessarily decrease the distance to some other (except for strange configurations), contrary to consistency requirement. Hence the claim\nSo the \u0393-transform enforces either adding a new dimension and moving the affected single cluster along it (which does not seem to be quite natural) or to change positions of elements in at least two clusters within the embedding space. Therefore vast majority of such algorithms does not meet not only the consistency but also inner consistency requirement.\nTheorem 8. No algorithm operating in a fix-dimensional space under Euclidean distance can conform to inner-consistency axiom.24\nWhy not moving a second cluster is so problematic? Let us illustrate the difficulties with the original Kleinberg\u2019s consistency by looking at an application of the known k-means algorithm, with k being allowed to cover a range, not just a single value, to the two-dimensional data set visible in Figure 125. This example is a mixture of data points sampled from 5 normal distributions. The k-means algorithm with k = 5, as expected, separates quite well the points from various distributions. As visible from the second column of Table 3, in fact k = 5 does the best job in reducing the unexplained variance. Figure 2 illustrates a result of a \u0393-transform on the results of the former clustering. Visually we would tell that now we have two clusters. A look into the third column of the Table 3 convinces that really k = 2 is the best choice for clustering these data with k-means algorithm. This of course contradicts Kleinberg\u2019s consistency axiom. And demonstrates the weakness of outer-consistency concept as well.\n24This impossibility does not mean that there is an inner contradiction when executing the inner-consistency transform. Rather it means that considering inner-consistency is pointless\nOriginal data\nKleinberg's Gamma Transformation\nTheorem 9. k-means with k allowed to range over a set of values (approximating richness) with limited variance increase criterion for choice of k operating in a fix-dimensional space under Euclidean distance cannot conform to outerconsistency axiom.\nAnd finally have a look at Figure 1 once again. If we ignore the most right cluster, it turns out that each cluster has points being \u201dsurrounded\u201d by points in other clusters. Therefore, it is not possible to move a cluster by infinitely small distance without decreasing distances of some different clusters. Therefore\nTheorem 10. No algorithm operating in a fix-dimensional space under Euclidean distance can conform continuously to outer-consistency axiom.\nThis theorem contradicts apparently [4] claim that k-means possesses the property of outer-consistency. The key word in this theorem is however \u201dcontinuously\u201d in strict conjunction with \u201dEuclidean distance\u201d. It is the embedding into the Euclidean space that causes the problem."}, {"heading": "7 Problems of Richness Axiom", "text": "As already mentioned, richness or near-richness forces the introduction of \u201drefinement-consistency\u201d which is a too weak concept. But even if we allow for such a resolution of the contradiction in Kleinberg\u2019s framework, it still does not make it suitable for practical purposes. The most serious drawback of Kleinberg\u2019s axioms is the richness requirement.\nBut we may ask whether or not it is possible to have richness, that is for any partition there exists always a distance function that the clustering function will return this partition, and yet if we restrict ourselves to Rm, the very same clustering function is not rich any more, or even it is not anti-chain.\nConsider the following clustering function f(). If it takes a distance function d() that takes on only two distinct values d1 and d2 such that d1 < 0.5d2 and for any three data points a, b, c if d(a, b) = d1, d(b, c) = d1 then d(a, c) = d1, it creates clusters of points in such a way that a, b belong to the same cluster if and only if d(a, b) = d1, and otherwise they belong to distinct clusters. If on the other hand f() takes a distance function not exhibiting this property, it works like k-means. Obviously, function f() is rich, but at the same time, if confined to Rm, if n > m + 1 and k n, then it is not rich \u2013 it is in fact k-rich, and hence not anti-chain.\nCan we get around the problems of all three Kleinberg\u2019s axioms in a similar way in Rm? Regrettably,\nTheorem 11. If \u0393 is a partition of n > 2 elements returned by a clustering function f under some distance function d, and f satisfies Consistency, then there exists a distance function dE embedded in Rm for the same set of elements such that \u0393 is the partition of this set under dE.\nbecause inner-consistency transform is in general impossible. 25Already Ben-David [10] indicated problems in this direction.\nThe consequence of this theorem is of course that the constructs of contradiction of Kleinberg axioms are simply transposed from the domain of any distance functions to distance functions in Rm.\nProof. To show the validity of the theorem, we will construct the appropriate distance function dE by embedding in the Rm. Let dmax be the maximum distance between the considered elements under d. Let C1, . . . , Ck be all the clusters contained in \u0393. For each cluster Ci we construct a ball Bi with radius ri equal to ri = 1 2 minx,y\u2208Ci,x 6=y d(x, y). The ball B1 will be located in the origin of the coordinate system. B1,...,i be the ball of containing all the balls B1, . . . , Bi. Its center be at c1,...,i and radius r1,...,i. The ball Bi will be located on the surface of the ball with center at c1,...,i\u22121 and radius r1...,i\u22121 +dmax+ri. For each i = 1, . . . , k select distinct locations for elements of Ci within the ball Bi. The distance function dE define as the Euclidean distances within Rm in these constructed locations.\nApparently, dE is a \u0393-transform of d, as distances between elements of Ci are smaller than or equal to 2ri = minx,y\u2208Ci,x 6=y d(x, y), and the distances between elements of different balls exceed dmax.\nBut richness is not only a problem in conjunction with scale-invariance and consistency, but rather it is a problem by itself.\nIt has to be stated first that richness is easy to achieve. Imagine the following \u2019clustering function\u201d. You order nodes by average distance to other nodes, on tights on squared distance and so on, and if no sorting can be achieved, the unsortable points are set into one cluster. Then we create an enumeration of all clusters and map it onto unit line segment. Then we take the quotient of the lowest distance to the largest distance and state that this quotient mapped to that line segment identifies the optimal clustering of the points. Though the algorithm is simple in principle (and useless also), and meets axioms of richness and scale -invariance, we have a practical problem: As no other limitations\nare imposed, one has to check up to \u2211n k=2 1 k! \u2211k j=1(\u22121)k\u2212j ( k j ) jn possible partitions (Bell number) in order to verify which one of them is the best for a given distance function because there must exist at least one distance function suitable for each of them. This is prohibitive and cannot be done in reasonable\ntime even if each check is polynomial (even linear) in the dimensions of the task (n).\nFurthermore, most algorithms of cluster analysis are constructed in an incremental way. But this can be useless if the clustering quality function is designed in a very unfriendly way. For example as an XOR function over logical functions of class member distances and non-class member distances (e.g. being true if the distance rounded to an integer is odd between class members and divisible by a prime number for distances between class members and non-class members, or the same with respect to class center or medoid).\nJust have a look at sample data from Table 4. A cluster quality function was invented along the above line and exact quality value was computed for partitioning first n points from this data set as illustrated in Table 5. It turns out that the best partition for n points does not give any hint for the best partition for n + 1 points therefore each possible partition needs to be investigated in order to find the best one.26\nSummarizing these examples, the learnability theory points at two basic weaknesses of the richness or even near-richness axioms. On the one hand the hypothesis space is too big for learning a clustering from a sample (it grows too quickly with the sample size). On the other hand an exhaustive search in this space is prohibitive sop that some theoretical clustering functions do not make practical sense.\nThere is one more problem. If the clustering function can fit any data, we are practically unable to learn any structure of data space from data [21]. And this learning capability is necessary at least in the cases: either when the data may be only representatives of a larger population or the distances are measured with some measurement error (either systematic or random) or both. Note that we speak here about a much broader aspect than so-called cluster stability or cluster validity, pointed at by Luxburg [27, 26].\n26 Strict separation [11] mentioned earlier is another kind of a weird cluster quality function, requiring visits to all the partitions"}, {"heading": "8 Correcting Formalization of Kleinberg Ax-", "text": "It is obvious that richness axiom of Kleinberg needs to be replaced with a requirement of the space of hypotheses to be \u201dlarge enough\u201d. For k-means algorithm it has been shown via Theorem 5 that k-richness is satisfied (and the space is still large, a Bell number of partitions to choose from). k-means satisfies the scale-invariance axiom, so that only the consistency axiom needs to be adjusted to be more realistic.\nTherefore a meaningful redefinition of Kleinberg\u2019s \u0393-transform is urgently needed. It must not be annihilated by scaling and it must be executable.\nLet us create for R a working definition of the \u0393\u2217 transform as follows: Distances in only one cluster X are changed by moving a point along the axis connecting it to cluster X center reducing them within the cluster X by the same factor, the distances between any elements outside the cluster X are kept [as well as to the gravity center of the cluster X]27\nConsider the following one-dimensional clustering function: For a set of n \u2265 2 points two elements belong to the same cluster if their distance is strictly lower than 1n+1 of the largest distance between the elements. When a, b belong to the same cluster and b, c belong to the same cluster, then a, c belong to the same cluster. As a consequence, the minimum distance between elements of distinct clusters is 1n+1 of the largest distance between the elements of S. It is easily seen that the weakened richness is fulfilled. The scale-invariance is granted by the relativity of inter-cluster distance. And the consistency under redefined \u0393\n27Obviously, for any element outside the cluster X the distance to the closest element of X before the transform will not be smaller than its distance to the closest element of X after the transform. Note the shift of attention. We do not insist any longer that the distance to each element of other cluster is increased, rather only the distance to the cluster as a \u201dwhole\u201d shall increase. This is by the way a stronger version of inner-consistency which would be insufficient for our purposes.\ntransform holds also. In this way all three axioms hold. A generalization to an Euclidean space of higher dimensionality seems to be quite obvious if there are no ties on distances (the exist one pair of points the distance between which is unique and largest among distances28). We embed the points in the space, and then say that two points belong to the same cluster if the distance along each of the dimensions is lower than 1n+1 of the largest distance between the elements along the respective dimension. The distance is then understood as the maximum of distances along all dimensions.\nDefinition 3. Let \u0393 be a partition embedded in Rm. Let C \u2208 \u0393 and let \u00b5c be the center of the cluster C. We say that we execute the \u0393\u2217 transform (or a centric consistency transformation) if for some 0 < \u03bb \u2264 1 we create a set C \u2032 with cardinality identical with C such that for each element x \u2208 C there exists x\u2019 \u2208 C \u2032 such that x\u2019 = \u00b5c + \u03bb(x\u2212 \u00b5c), and then substitute C in \u0393 with C \u2032.\nProperty 9. A method matches the condition of centric consistency if after a \u0393\u2217 transform it returns the same partition.\nHence\nTheorem 12. For each n \u2265 2, there exists clustering function f that satisfies Scale-Invariance, near-Richness, and Centric-Consistency29.\nProof. The above-mentioned clustering function is the proof of validity of this theorem.\nThis way of resolving Kleinberg\u2019s contradictions differs from earlier approaches in that a realistic embedding into an Rm is considered and the distances are metric.\nWe created herewith the possibility of shrinking a single cluster without having to \u201dmove\u201d the other ones. As pointed out, this was impossible under Kleinberg\u2019s \u0393 transform, that is under increase of all distances between objects from distinct clusters. In fact intuitively we do not want the objects to be more distant but rather the clusters. We proposed to keep the cluster centroid unchanged while decreasing distances between cluster elements proportionally, insisting that no distance of other elements to the closest element of the shrunk cluster should decrease. This approach is pretty rigid. It assumes that we are capable to embed the objects into some Euclidean space so that the centroid has a meaning.\n9 k-means fitting centric-consistency axiom\nOur proposal of centric-consistency has a practical background. Kleinberg proved that k-means does not fit his consistency axiom. As shown experimen-\n28otherwise some tie breaking measures have to be taken that would break the any symmetry and allow to choose a unique direction\n29 Any algorithm being consistent is also refinement-consistent. Any algorithm being innerconsistent is also consistent. Any algorithm being outer-consistent is also consistent. But there are no such subsumptions for the centric-consistency.\nCentralised Gamma Transformation\ntally in table 3, k-means algorithm behaves properly under \u0393\u2217 transformation. Figure 3 illustrates a two-fold application of the \u0393\u2217 transform (same clusters affected as by \u0393-transform in the preceding figure). As recognizable visually and by inspecting the forth column of Table 3, here k = 5 is the best choice for k-means algorithm, so the centric-consistency axiom is followed.\nLet us now demonstrate theoretically, that k-means algorithm really fits \u201din the limit\u201d the centric-consistency axiom.\nTheorem 13. k-means algorithm satisfies centric consistency in the following way: if the partition \u0393 is a local minimum of k-means, and the partition \u0393 has been subject to centric consistency yielding \u0393\u2032, then \u0393\u2032 is also a local minimum of k-means.\nProof. The k-means algorithm minimizes the sum30 Q from equation (1). V (Cj) be the sum of squares of distances of all objects of the cluster Cj from its\ngravity center. Hence Q(\u0393) = \u2211k j=1 1 nj V (Cj). Consider moving a data point x \u2217 from the cluster Cj0 to cluster Cjl As demonstrated by [16], V (Cj0 \u2212 {x\u2217}) = V (Cj0)\u2212\nnj0 nj0\u22121 \u2016x\u2217 \u2212 \u00b5j0\u2016 2 and V (Cjl \u222a {x\u2217}) = V (Cjl) + nlnl+1\u2016x \u2217 \u2212 \u00b5jl\u2016 2 So\nit pays off to move a point from one cluster to another if nj0 nj0\u22121 \u2016x\u2217 \u2212 \u00b5j0\u2016 2 >\nnjl njl+1 \u2016x\u2217\u2212\u00b5jl\u2016 2. If we assume local optimality of \u0393, this obviously did not pay\n30We use here the symbol Q for the cluster quality function instead of J from section 2 because Q does not fit axiomatic system for J - it is not scale-invariant and in case of consistency it changes in opposite direction, and with respect of richness we can only apply k-richness.\noff. Now transform this data set to X\u2032 in that we transform elements of cluster Cj0 in such a way that it has now elements x \u2032 i = xi + \u03bb(xi \u2212 \u00b5j0) for some 0 < \u03bb < 1, see figure 4. Consider a partition \u0393\u2032 of X\u2032. All clusters are the same as in \u0393 except for the transformed elements that form now a cluster C \u2032j0 . The question is: does it pay off to move a data point x\u2019\u2217 \u2208 C \u2032j0 between the clusters? Consider the plane containing x\u2217,\u00b5j0 ,\u00b5jl . Project orthogonally the point x \u2217 onto the line \u00b5j0 ,\u00b5jl , giving a point p. Either p lies between \u00b5j0 ,\u00b5jl or \u00b5j0 lies between p,\u00b5jl . Properties of k-means exclude other possibilities. Denote distances y = \u2016x\u2217 \u2212 p\u2016, x = \u2016\u00b5j0 \u2212 p\u2016, d = \u2016\u00b5j0 \u2212 \u00b5jl\u2016 In the second case the condition that moving the point does not pay off means:\nnj0 nj0 \u2212 1 (x2 + y2) \u2264 njl njl + 1 ((d+ x)2 + y2)\nIf we multiply both sides with \u03bb2, we have:\n\u03bb2 nj0\nnj0 \u2212 1 (x2 + y2) = nj0 nj0 \u2212 1 ((\u03bbx)2 + (\u03bby)2)\n\u2264\u03bb2 njl njl + 1 ((d+ x)2 + y2) = njl\nnjl + 1 (\u03bb2d2 + \u03bb22dx+ \u03bb2x2 + \u03bb2y2)\n\u2264 njl njl + 1 (d2 + 2d\u03bbx+ \u03bb2x2 + \u03bb2y2) = njl\nnjl + 1 ((d+ \u03bbx)2 + (\u03bby)2) (2)\nwhich means that it does not payoff to move the point x\u2019\u2217 between clusters either. Consider now the first case and assume that it pays off to move x\u2019\u2217. So we would have\nnj0 nj0 \u2212 1 (x2 + y2) \u2264 njl njl + 1 ((d\u2212 x)2 + y2)\nand at the same time\nnj0 nj0 \u2212 1 \u03bb2(x2 + y2) > njl njl + 1 ((d\u2212 \u03bbx)2 + \u03bb2y2)\nSubtract now both sides:\nnj0 nj0 \u2212 1 (x2 + y2)\u2212 nj0 nj0 \u2212 1 \u03bb2(x2 + y2)\n< njl njl + 1 ((d\u2212 x)2 + y2)\u2212 njl njl + 1 ((d\u2212 \u03bbx)2 + \u03bb2y2)\nThis implies\nnj0 nj0 \u2212 1 (1\u2212 \u03bb2)(x2 + y2) < njl njl + 1 ((1\u2212 \u03bb2)(x2 + y2)\u2212 2d\u03bbx)\nIt is a contradiction because\nnj0 nj0 \u2212 1 (1\u2212\u03bb2)(x2+y2) > njl njl + 1 (1\u2212\u03bb2)(x2+y2) > njl njl + 1 ((1\u2212\u03bb2)(x2+y2)\u22122d\u03bbx)\nSo it does not pay off to move x\u2019\u2217, hence the partition \u0393\u2032 remains locally optimal for the transformed data set.\nIf the data have one stable optimum only like in case of \u201dwell separated\u201d normally distributed k real clusters, then both turn to global optima.\nHowever, it is possible to demonstrate that the newly defined transform preserves also the global optimum of k-means.\nTheorem 14. k-means algorithm satisfies centric consistency in the following way: if the partition \u0393 is a global minimum of k-means, and the partition \u0393 has been subject to centric consistency yielding \u0393\u2032, then \u0393\u2032 is also a global minimum of k-means.\nProof. Let us consider first the simple case of two clusters only (2-means). Let the optimal clustering for a given set of objects X consist of two clusters: T and Z. The subset T shall have its gravity center at the origin of the coordinate system. The quality of this partition Q({T,Z}) = nTV ar(T ) + nZV ar(Z) where nT , nZ denote the cardinalities of T,Z and V ar(T ), V ar(Z) their variances (averaged squared distances to gravity center). We will prove by contradiction that by applying our \u0393 transform we get partition that will be still optimal for the transformed data points. We shall assume the contrary that is that we can transform the set T by some 1 > \u03bb > 0 to T \u2032 in such a way that optimum of 2-means clustering is not the partition {T \u2032, Z} but another one, say {A\u2032 \u222aD,B\u2032 \u222a C} where Z = C \u222aD, A\u2032 and B\u2032 are transforms of sets A,B for which in turn A \u222aB = T . It may be easily verified that\nQ({A \u222aB,C \u222aD}) = nAV ar(A) + nAv2A + nBV ar(B) + nBv2B\n+nCV ar(C) + nDV ar(D) + nCnD nC + nD (vC \u2212 vD)2\nwhile\nQ({A \u222a C,B \u222aD}) = nAV ar(A) + nDV ar(D) + + nAnD nA + nD (vA \u2212 vD)2\n+nBV ar(B) + nCV ar(C) + + nBnC nB + nC (vB \u2212 vC)2\nand\nQ({A\u2032 \u222aB\u2032, C \u222aD}) = nA\u03bb2V ar(A) + nA\u03bb2v2A + nB\u03bb2V ar(B) + nB\u03bb2v2B\n+nCV ar(C) + nDV ar(D) + nCnD nC + nD (vC \u2212 vD)2\nwhile\nQ({A\u2032 \u222a C,B\u2032 \u222aD}) = nA\u03bb2V ar(A) + nDV ar(D) + + nAnD nA + nD (\u03bbvA \u2212 vD)2\n+nB\u03bb 2V ar(B) + nCV ar(C) + + nBnC nB + nC (\u03bbvB \u2212 vC)2\nThe following must hold:\nQ({A\u2032 \u222aB\u2032, C \u222aD}) > Q({A\u2032 \u222aD,B\u2032 \u222a C}) (3)\nand Q({A \u222aB,C \u222aD}) < Q({A \u222aD,B \u222a C}) (4)\nAdditionally also\nQ({A \u222aB,C \u222aD}) < Q({A \u222aB \u222a C,D}) (5)\nand Q({A \u222aB,C \u222aD}) < Q({A \u222aB \u222aD,C}) (6)\nThese two latter inequalities imply:\nnCnD nC + nD (vC \u2212 vD)2 < (nA + nB)nC (nA + nB) + nC v2C\nand nCnD nC + nD (vC \u2212 vD)2 < (nA + nB)nD (nA + nB) + nD v2D\nConsider now an extreme contraction (\u03bb = 0) yielding sets A\u201d, B\u201d out of A,B. Then we have\nQ({A\u201d \u222aB\u201d, C \u222aD})\u2212Q({A\u201d \u222a C,B\u201d \u222aD})\n= nCnD nC + nD (vC \u2212 vD)2 \u2212 nAnD nA + nD v2D \u2212 nBnC nB + nC v2C\n= nCnD nC + nD (vC \u2212 vD)2\n\u2212 nAnD nA + nD (nA + nB) + nD (nA + nB)nD (nA + nB)nD (nA + nB) + nD v2D\n\u2212 nBnC nB + nC (nA + nB) + nC (nA + nB)nC (nA + nB)nC (nA + nB) + nC v2C\n= nCnD nC + nD (vC \u2212 vD)2\n\u2212 nA nA + nD (nA + nB) + nD (nA + nB) (nA + nB)nD (nA + nB) + nD v2D\n\u2212 nB nB + nC (nA + nB) + nC (nA + nB) (nA + nB)nC (nA + nB) + nC v2C\n= nCnD nC + nD (vC \u2212 vD)2\n\u2212 nA nA + nB (1 + nB nA + nD ) (nA + nB)nD (nA + nB) + nD v2D\n\u2212 nB nA + nB (1 + nA nB + nC ) (nA + nB)nC (nA + nB) + nC v2C\n< nCnD nC + nD (vC \u2212 vD)2\n\u2212 nA nA + nB (nA + nB)nD (nA + nB) + nD v2D\n\u2212 nB nA + nB (nA + nB)nC (nA + nB) + nC v2C < 0\nbecause the linear combination of two numbers that are bigger than a third yields another number bigger than this. Let us define a function\nh(x) = +nAx 2v2A + nBx 2v2B + nCnD nC + nD (vC \u2212 vD)2\n\u2212 nAnD nA + nD (xvA \u2212 vD)2 \u2212 nBnC nB + nC (xvB \u2212 vC)2\nIt can be easily verified that h(x) is a quadratic polynomial with a positive coefficient at x2. Furthermore h(1) = Q({A\u222aB,C\u222aD})\u2212Q({A\u222aC,B\u222aD}) < 0, h(\u03bb) = Q({A\u2032\u222aB\u2032, C \u222aD})\u2212Q({A\u2032\u222aC,B\u2032\u222aD}) > 0, h(0) = Q({A\u201d\u222aB\u201d, C \u222a D})\u2212Q({A\u201d \u222a C,B\u201d \u222aD}) < 0. But no quadratic polynomial with a positive coefficient at x2 can be negative at the ends of an interval and positive in the middle. So we have the contradiction. This proves the thesis that the (globally) optimal 2-means clustering remains (globally) optimal after transformation.\nLet us turn to the general case of k-means. Let the optimal clustering for a given set of objects X consist of k clusters: T and Z1, . . . , Zk\u22121. The subset T shall have its gravity center at the origin of the coordinate system. The quality of this partition Q({T,Z1, . . . , Zk\u22121}) = nTV ar(T ) + \u2211k\u22121 i=1 nZiV ar(Zi), where nZi is the cardinality of the cluster Zi. We will prove by contradiction that\nby applying our \u0393 transform we get partition that will be still optimal for the transformed data points. We shall assume the contrary that is that we can transform the set T by some 1 > \u03bb > 0 to T \u2032 in such a way that optimum of k-means clustering is not the partition {T \u2032, Z1, . . . , Zk\u22121} but another one, say {T \u20321\u222aZ1,1\u222a\u00b7 \u00b7 \u00b7\u222aZk\u22121,1, T \u20322\u222aZ1,2\u222a\u00b7 \u00b7 \u00b7\u222aZk\u22121,2 . . . , T \u2032k\u222aZ1,k\u222a\u00b7 \u00b7 \u00b7\u222aZk\u22121,k} where Zi = \u222akj=1Zi,j (where Zi,j are pairwise disjoint), T \u20321, . . . , T \u2032k are transforms of disjoint sets T1, . . . , Tk for which in turn \u222akj=1Tj = T . It may be easily verified that\nQ({T,Z1, . . . , Zk\u22121}) = k\u2211 j=1 nTjV ar(Tj) + k\u2211 j=1 nTjv 2 Tj + k\u22121\u2211 i=1 nZiV ar(Zi)\nwhile (denoting Z\u2217,j = \u222ai=1k \u2212 1Z\u2217,j)\nQ({T1 \u222a Z\u2217,1, . . . , Tk \u222a Z\u2217,k}) =\n= k\u2211 j=1 ( nTjV ar(Tj) + nZ\u2217,jV ar(Z\u2217,j) + + nTjnZ\u2217,j nTj + nZ\u2217,j (vTj \u2212 vZ\u2217,j )2 )\nwhereas\nQ({T \u2032, Z1, . . . , Zk\u22121}) = k\u2211 j=1 nTj\u03bb 2V ar(Tj) + k\u2211 j=1 nTj\u03bb 2v2Tj\n+ k\u22121\u2211 i=1 nZiV ar(Zi)\nwhile Q({T \u20321 \u222a Z\u2217,1, . . . , T \u2032k \u222a Z\u2217,k}) =\n= k\u2211 j=1 ( nTj\u03bb 2V ar(Tj) + nZ\u2217,jV ar(Z\u2217,j) + + nTjnZ\u2217,j nTj + nZ\u2217,j (\u03bbvTj \u2212 vZ\u2217,j )2 )\nThe following must hold:\nQ({T \u2032, Z1, . . . , Zk\u22121}) > Q({T \u20321 \u222a Z\u2217,1, . . . , T \u2032k \u222a Z\u2217,k}) (7)\nand Q({T,Z1, . . . , Zk\u22121}) < Q({{T1 \u222a Z\u2217,1, . . . , Tk \u222a Z\u2217,k}) (8)\nAdditionally also\nQ({T,Z1, . . . , Zk\u22121}) < Q({{T \u222a Z\u2217,1, Z\u2217,2, . . . , Z\u2217,k) (9)\nand Q({T,Z1, . . . , Zk\u22121}) < Q({T \u222a Z\u2217,2, Z\u2217,1, Z\u2217,3, . . . , Z\u2217,k}) (10)\nand . . . and\nQ({T,Z1, . . . , Zk\u22121}) < Q({T \u222a Z\u2217,k, Z\u2217,1, . . . , Z\u2217,k\u22121}) (11)\nThese latter k inequalities imply that for l = 1, . . . , k:\nQ({T,Z1, . . . , Zk\u22121}) = nTV ar(T ) + k\u2211 j=1 nTjV ar(Tj) + k\u2211 j=1 nTjv 2 Tj\n+ k\u22121\u2211 i=1 nZiV ar(Zi) <\nQ({T \u222a Z\u2217,l, Z\u2217,1, . . . , Z\u2217,l\u22121, Z\u2217,l+1 . . . , Z\u2217,k}) =\n= nTV ar(T ) + k\u2211 j=1 nZ\u2217,jV ar(Z\u2217,j) + nTnZ\u2217,l nT + nZ\u2217,l (vT \u2212 vZ\u2217,l)2\n+ k\u22121\u2211 i=1 nZiV ar(Zi) <\nk\u2211 j=1 nZ\u2217,jV ar(Z\u2217,j) + nTnZ\u2217,l nT + nZ\u2217,l (vT \u2212 vZ\u2217,l)2\n+ k\u22121\u2211 i=1 nZiV ar(Zi)\u2212 k\u2211 j=1 nZ\u2217,jV ar(Z\u2217,j) <\nnTnZ\u2217,l nT + nZ\u2217,l (vZ\u2217,l) 2\nConsider now an extreme contraction (\u03bb = 0) yielding sets Tj\u201d out of Tj . Then we have\nQ({T\u201d, Z1, . . . , Zk\u22121})\u2212Q({T\u201d1 \u222a Z\u2217,1, . . . , T\u201dk \u222a Z\u2217,k})\n= k\u22121\u2211 i=1 nZiV ar(Zi)\u2212 k\u2211 j=1 ( nZ\u2217,jV ar(Z\u2217,j) + nTjnZ\u2217,j nTj + nZ\u2217,j (vZ\u2217,j ) 2 )\n= k\u22121\u2211 i=1 nZiV ar(Zi)\u2212 k\u2211 j=1 nZ\u2217,jV ar(Z\u2217,j)\n\u2212 k\u2211 j=1 nTjnZ\u2217,j nTj + nZ\u2217,j nT + nZ\u2217,j nTnZ\u2217,j nTnZ\u2217,j nT + nZ\u2217,j (vZ\u2217,j ) 2\n= k\u22121\u2211 i=1 nZiV ar(Zi)\u2212 k\u2211 j=1 nZ\u2217,jV ar(Z\u2217,j)\n\u2212 k\u2211 j=1 nTj nTj + nZ\u2217,j nT + nZ\u2217,j nT nTnZ\u2217,j nT + nZ\u2217,j (vZ\u2217,j ) 2\n\u2264 k\u22121\u2211 i=1 nZiV ar(Zi)\u2212 k\u2211 j=1 nZ\u2217,jV ar(Z\u2217,j)\u2212 k\u2211 j=1 nTj nT nTnZ\u2217,j nT + nZ\u2217,j (vZ\u2217,j ) 2 < 0\nbecause the linear combination of numbers that are bigger than a third yields another number bigger than this. Let us define a function\ng(x) = k\u2211 j=1 nTjx 2v2Tj + k\u22121\u2211 i=1 nZiV ar(Zi)\n\u2212 k\u2211 j=1 ( nZ\u2217,jV ar(Z\u2217,j) + + nTjnZ\u2217,j nTj + nZ\u2217,j (xvTj \u2212 vZ\u2217,j )2 )\nIt can be easily verified that g(x) is a quadratic polynomial with a positive coefficient at x2. Furthermore g(1) = Q({T,Z1, . . . , zk\u22121})\u2212Q({T1 \u222aZ\u2217,1, . . . , Tk \u222a Z\u2217,k}) < 0, g(\u03bb) = Q({T \u2032, Z1, . . . , Zk\u22121}) \u2212 Q({T \u20321 \u222a Z\u2217,1, . . . , T \u2032k \u222a Z\u2217,k}) > 0, g(0) = Q({T\u201d, Z1, . . . , Zk\u22121}) \u2212 Q({T\u201d1 \u222a Z\u2217,1, . . . , T\u201dk \u222a Z\u2217,k}) < 0. But no quadratic polynomial with a positive coefficient at x2 can be negative at the ends of an interval and positive in the middle. So we have the contradiction. This proves the thesis that the (globally) optimal k-means clustering remains (globally) optimal after transformation.\nSo summarizing the new \u0393 transformation preserves local and global optima of k-means for a fixed k. Therefore k-means algorithm is consistent under this transformation.\nHence\nTheorem 15. k-means algorithm satisfies Scale-Invariance, k-Richness, and centric Consistency.\nNote that (\u0393\u2217 based) centric Consistency is not a specialization of Kleinberg\u2019s consistency as the requirement of increased distance between all elements of different clusters is not required in \u0393\u2217 based Consistency. Note also that the decrease of distance does not need to be equal for all elements as long as the gravity center does not relocate. Also a limited rotation of the cluster may be allowed for."}, {"heading": "10 Moving clusters - motion consistency", "text": "As we have stated already, in the Rn it is actually impossible to move clusters in such a way as to increase distances to all the other elements of all the other clusters (see Theorem 10). However, we shall ask ourselves if we may possibly move away clusters as whole, via increasing the distance between cluster centers and not overlapping cluster regions, which, in case of k-means, represent Voronoi-regions.\nProperty 10. A clustering method conforms to motion consistency, if it returns the same clustering when the distances of cluster centers are increased by moving each point of a cluster by the same vector without leading to overlapping of the convex regions of clusters.\nLet us concentrate on the k-means case and let us look at two neighboring clusters. The Voronoi regions, associated with k-means clusters, are in fact polyhedrons, such that the \u201douter\u201d polyhedrons (at least one of them) can be moved away from the rest without overlapping any other region.\nSo is such an operation on regions permissible without changing the cluster structure? A closer look at the issue tells us that it is not. As k-means terminates, the neighboring clusters\u2019 polyhedrons touch each other via a hyperplane such that the straight line connecting centers of the clusters is orthogonal to this hyperplane. This causes that points on the one side of this hyperplane lie more closely to the one center, and on the other to the other one. But if we move the clusters in such a way that both touch each other along the same hyperplane, then it happens that some points within the first cluster will become closer to the center of the other cluster and vice versa.31 So moving the clusters generally will change their structure (points switch clusters) unless the points lie actually not within the polyhedrons but rather within \u201dparaboloids\u201d with appropriate equations. Then moving along the border hyperplane will not change cluster membership (locally). But the intrinsic cluster borders are now \u201dparaboloids\u201d. What would happen if we relocate the clusters allowing for touching along the \u201dparaboloids\u201d? The problem will occur again.\nHence the question can be raised: What shape should have the k-means clusters in order to be (locally) immune to movement of whole clusters?\nLet us consider the problem of susceptibility to class membership change within a 2D plane containing the two cluster centers. Let the one cluster center be located at a point (0,0) in this plane and the other at (2x0, 2y0). Let further the border of the first cluster be characterized by a (symmetric) function f(x) and le the shape of the border of the other one g(x) be the same, but properly rotated: g(x) = 2y0 \u2212 f(x\u2212 2x0) so that the cluster center is in the same. Let both have a touching point (we excluded already a straight line and want to have convex smooth borders). From the symmetry conditions one easily sees that the touching point must be (x0, y0). As this point lies on the surface of f(), y0 = f(x0) must hold. For any point (x, f(x) of the border of the first cluster with center (0, 0) the following must hold:\n(x\u2212 2x0)2 + (f(x)\u2212 2f(x0))2 \u2212 x2 \u2212 f2(x) \u2265 0 (12)\nThat is \u22122x0(2x\u2212 2x0)\u2212 2f(x0) (2f(x)\u2212 2f(x0)) \u2265 0\n\u2212f(x0) (f(x)\u2212 f(x0)) \u2265 x0(x\u2212 x0) 31This is by the way the nice trick behind the claim in [6] that incremental k-means does not identify perfectly separated clusters. Clusters in k-means are not the points, they are polyhedrons, contrary to the assumptions in [6].\nLet us consider only positions of the center of the second cluster below the X axis. In this case f(x0) < 0. Further let us concentrate on x lower than x0. We get\n\u2212f(x)\u2212 f(x0) x\u2212 x0 \u2265 x0 \u2212f(x0)\nIn the limit, when x approaches x0.\n\u2212f \u2032(x0) \u2265 x0\n\u2212f(x0)\nNow turn to x greater than x0. We get\n\u2212f(x)\u2212 f(x0) x\u2212 x0 \u2264 x0 \u2212f(x0)\nIn the limit, when x approaches x0.\n\u2212f \u2032(x0) \u2264 x0\n\u2212f(x0)\nThis implies\n\u2212 f \u2032(x0) = \u22121 f(x0) x0\n(13)\nNote that f(x0)x0 is the directional tangent of the straight line connecting both cluster centers. As well as it is the directional tangent of the line connecting the center of the first cluster to its surface. f \u2032(x0) is the tangential of the borderline of the first cluster at the touching point of both clusters. The equation above means both are orthogonal. But this property implies that f(x) must be definition (of a part) a circle centered at (0, 0). As the same reasoning applies at any touching point of the clusters, a k-means cluster would have to be (hyper)ball-shaped in order to allow the movement of the clusters without elements switching cluster membership.\nThe tendency of k-means to recognize best ball-shaped clusters has been known long ago, but we are not aware of presenting such an argument for this tendency.\nIt has to be stated however that clusters, even if enclosed in a ball-shaped region, need to be separated sufficiently to be properly recognized. Let us consider, under which circumstances a cluster C1 of radius r1 containing n1 elements would take over n21 elements (subcluster C21) of a cluster C2 of radius r2 of cardinality n2. Let n22 = n2\u2212n21 be the number of the remaining elements (subcluster C22 of the second cluster. Let the enclosing balls of both clusters be separated by the distance (gap) g. Let us consider the worst case that is that the center of the C21 subcluster lies on a straight line segment connecting both cluster centers. The center of the remaining C22 subcluster would lie on the same line but on the other side of the second cluster center. Let r21, r22 be distances of centers of n21 and n22 from the center of the second cluster. The relations\nn21 \u00b7 r21 = n22 \u00b7 r22, r21 \u2264 r2, r22 \u2264 r2\nmust hold. Let denote with SSC(C) the sum of squared distances of elements of the set C to the center of this set.\nSo in order for the clusters to be stable\nSSC(C1) + SSC(C2) \u2264 SSC(C1 \u222a C21) + SSC(C22)\nmust hold. But\nSSC(C2) = SSC(C21) + SSC(C22) + n21 \u00b7 r221 + n22 \u00b7 r222\nSSC(C1 \u222a C21) = SSC(C1) + SSC(C21) + n1n21 n1 + n21 (r1 + r2 + g \u2212 r21)2\nHence\nSSC(C1) + SSC(C21) + SSC(C22) + n21 \u00b7 r221 + n22 \u00b7 r222\n\u2264 SSC(C1) + SSC(C21) + n1n21 n1 + n21 (r1 + r2 + g \u2212 r21)2 + SSC(C22)\nn21 \u00b7 r221 + n22 \u00b7 r222 \u2264 n1n21 n1 + n21 (r1 + r2 + g \u2212 r21)2\nn21 \u00b7 r221 + n22 \u00b7 r222 n1n21 n1+n21\n\u2264 (r1 + r2 + g \u2212 r21)2\u221a n21 \u00b7 r221 + n22 \u00b7 r222\nn1n21 n1+n21\n\u2264 r1 + r2 + g \u2212 r21\n\u221a n21 \u00b7 r221 + n22 \u00b7 r222\nn1n21 n1+n21\n\u2212 r1 \u2212 r2 + r21 \u2264 g\n\u221a n21 \u00b7 r221 + n21 \u00b7 r21 \u00b7 r22\n1 1/n1+1/n21\n\u2212 r1 \u2212 r2 + r21 \u2264 g\n\u221a (n21 \u00b7 r221 + n21 \u00b7 r21 \u00b7 r22)(1/n1 + 1/n21)\u2212 r1 \u2212 r2 + r21 \u2264 g\u221a\n(r221 + r21 \u00b7 r22)(n21/n1 + 1)\u2212 r1 \u2212 r2 + r21 \u2264 g\nAs r22 = r21n21 n2\u2212n21\u221a (r221 + r21 \u00b7\nr21n21 n2 \u2212 n21 )(n21/n1 + 1)\u2212 r1 \u2212 r2 + r21 \u2264 g\nr21\n\u221a (1 +\nn21 n2 \u2212 n21 )(n21/n1 + 1)\u2212 r1 \u2212 r2 + r21 \u2264 g\nr21\n\u221a n2\nn2 \u2212 n21 n1 + n21 n1 \u2212 r1 \u2212 r2 + r21 \u2264 g\nr21 \u221a n2 n1 n1 + n21 n2 \u2212 n21 \u2212 r1 \u2212 r2 + r21 \u2264 g\nLet us consider the worst case when the elements to be taken over are at the \u201dedge\u201d of the cluster region (r21 = r2). Then\nr2 \u221a n2 n1 n1 + n21 n2 \u2212 n21 \u2212 r1 \u2264 g\nThe lower limit on g will grow with n21, but n21 \u2264 0.5n2, because otherwise r22 would exceed r2. Hence in the worst case\nr2 \u221a n2 n1 n1 + n2/2 n2/2 \u2212 r1 \u2264 g\nr2 \u221a\n2(1 + 0.5n2/n1)\u2212 r1 \u2264 g (14) In case of clusters with equal sizes and equal radius this amounts to\ng \u2265 r1( \u221a 3\u2212 1) \u2248 0.7r1\nSo we can conclude\nTheorem 16. k-means algorithm conforms (locally) to Motion Consistency axiom.\nNote that the motion consistency axiom is a substitute for outer-consistency which is impossible continuously in Euclidean space. It is to be underlined that we speak here about local optimum of k-means. With the abovementioned gap size the global k-means minimum may lie elsewhere, in a clustering possibly without gaps. Also the motion consistency transformation preserves as local minimum the partition it is applied to. Other local minima and global minimum can change.\nNote that compared to inner-consistency the centric consistency is quite rigid. And so is motion consistency compared to outer-consistency.\nIn two subsequent sections we will investigate if the rigidity of these transformations can be weakened under appropriate width of the gaps, and if we can grant these properties under global minimum. In particular, we shall study, how well the clusters need to be separated so that it is enough to find the global optimum."}, {"heading": "11 Cluster separation versus Kleinberg\u2019s axioms", "text": "of k-richness, consistency, scaling invariance"}, {"heading": "11.1 Perfect ball clusterings", "text": "The problem with k-means (-random and ++) is the discrepancy between the theoretically optimized function ()k-means-ideal) and the actual approximation of this value. It appears to be problematic even for \u201dwell-separated\u201d clusters.\nFirst let us point to the fact that \u201dwell-separatedness\u201d may keep the algorithm in a local minimum.\nIt is commonly assumed that a good initialization of a k-means clustering is one where the seeds hit different clusters. It is well known, that under some circumstances the k-means does not recover from poor initialization and as a consequence a natural cluster may be split even for \u201dwell-separated\u201d data.\nBut hitting each cluster may be not sufficient as neighboring clusters may be able to shift the cluster center away from its cluster.\nHence let us investigate what kind of well-separability would be sufficient to ensure that once clusters are hit by one seed each, would never loose the cluster center.\nLet us investigate the working hypothesis that two clusters are well separated if we can draw a ball of some radius \u03c1 around true cluster center of each of them and there is a gap between these balls. We claim that\nTheorem 17. If the distance between any the cluster centers A,B is at least 4\u03c1AB, where \u03c1AB is the radius of a ball centered at A and enclosing its cluster (that is cluster lies in the interior of the ball) and it also is the radius of a ball centered at B and enclosing its cluster, then once each cluster is seeded the clusters cannot loose their cluster elements for each other during k-meansrandom and k-means++ iterations.\nBefore starting the proof, let us introduce related definitions.\nDefinition 4. We shall say that clusters centered at A and B and enclosed in balls centered at A,B and with radius \u03c1AB each are nicely ball-separated, if the distance between A,B is at least 4\u03c1AB. If all pairs of clusters are nicely ball separated with the same ball radius, then we shall say that they are perfectly ball-separated.\nProof. For the illustration of the proof see Figure 5. Consider the two points A,B being the two ball centers and two points, X,Y , one being in each ball (presumably the cluster centers at some stage of the k-means algorithm). To represent their distances faithfully, we need at most a 3D space.\nLet us consider the plane established by the line AB and parallel to the line XY . Let X \u2032 and Y \u2032 be projections of X,Y onto this plane. Now let us establish that the hyperplane \u03c0 orthogonal to X,Y , and passing through the middle of the line segment XY , that is the hyperplane containing the boundary between clusters centered at X and Y does not cut any of the balls centered at A and B. This hyperplane will be orthogonal to the plane of the Figure 5 and so it will manifest itself as an intersecting line l that should not cross circles around A and B, being projections of the respective balls. Let us draw two solid lines k,m between circles O(A, \u03c1) and O(B, \u03c1) tangential to each of them. Line l should lie between these lines, in which case the cluster center will not jump to the other ball.\nLet the line X \u2032Y \u2032 intersect with the circles O(A, \u03c1) and O(B, \u03c1) at points C,D,E, F as in the figure.\nseparation\nIt is obvious that the line l would get closer to circle A, if the points X\u2019, Y\u2019 would lie closer to C and E, or closer to circle B if they would be closer to D and F .\nTherefore, to show that the line l does not cut the circle O(A, \u03c1) it is sufficient to consider X \u2032 = C and Y \u2032 = E. (The case with ball Ball(B, \u03c1) is symmetrical).\nLet O be the center of the line segment AB. Let us draw through this point a line parallel to CE that cuts the circles at points C \u2032, D\u2032, E\u2032 and F \u2032. Now notice that centric symmetry through point O transforms the circles O(A, \u03c1),O(B, \u03c1) into one another, and point C \u2032 into F \u2032 and D\u2032 into E\u2032. Let E\u2217 and F \u2217 be images of points E and F under this symmetry.\nIn order for the line l to lie between m and k, the middle point of the line segment CE shall lie between these lines.\nLet us introduce a planar coordinate system centered at O with X axis parallel to lines m, k, such that A has both coordinates non-negative, and B non-positive. Let us denote with \u03b1 the angle between the lines AB and k. As we assume that the distance between A and B equals 4\u03c1, then the distance between lines k and m amounts to 2\u03c1(2 sin(\u03b1)\u2212 1). Hence the Y coordinate of line k equals \u03c1(2 sin(\u03b1)\u2212 1).\nSo the Y coordinate of the center of line segment CE shall be not higher than this. Let us express this in vector calculus:\n4(yOC + yOE)/2 \u2264 \u03c1(2 sin(\u03b1)\u2212 1)\nNote, however that\nyOC +yOE = yOA+yAC +yOB+yBE = yAC +yBE = yAC\u2212yAE\u2217 = yAC +yE\u2217A\nSo let us examine the circle with center at A. Note that the lines CD and E\u2217F \u2217 are at the same distance from the line C\u2019D\u2019. Note also that the absolute values of direction coefficients of tangentials of circle A at C\u2019 and D\u2019 are identical. The more distant these lines are, as line CD gets closer to A, the yAC gets bigger, and yE\u2217A becomes smaller. But from the properties of the circle we see that yAC increases at a decreasing rate, while yE\u2217A decreases at an increasing rate. So the sum yAC + yE\u2217A has the biggest value when C is identical with C\n\u2032 and we need hence to prove only that\n(yAC\u2032 + yD\u2032A)/2 = yAC\u2032 \u2264 \u03c1(2 sin(\u03b1)\u2212 1)\nLet M denote the middle point of the line segment C \u2032D\u2032. As point A has the coordinates (2\u03c1 cos(\u03b1), 2\u03c1 sin(\u03b1)), the point M is at distance of 2\u03c1 cos(\u03b1) from A. But C \u2032M2 = \u03c12 \u2212 (2\u03c1 cos(\u03b1))2.\nSo we need to show that\n\u03c12 \u2212 (2\u03c1 cos(\u03b1))2 \u2264 (\u03c1(2 sin(\u03b1)\u2212 1))2\nIn fact we get from the above\n\u03c12 \u2212 4\u03c12 cos(\u03b1)2 \u2264 \u03c12(2 sin(\u03b1)\u2212 1)2\nDividing by \u03c12\n1\u2212 4 cos(\u03b1)2 \u2264 (2 sin(\u03b1)\u2212 1)2\n1\u2212 4 cos(\u03b1)2 \u2264 4 sin(\u03b1)2 \u2212 4 sin(\u03b1) + 1\nAdding 4 cos(\u03b1)2 to both sides and subtracting 1 we get\n0 \u2264 4\u2212 4 sin(\u03b1)\nDividing by 4 0 \u2264 1\u2212 sin(\u03b1)\nwhich is a known trigonometric relation. This means in practice that whatever point from the one and the other cluster is picked randomly as cluster center, then the Voronoi tessellation of the space will contain only points from a single cluster.\nLet us discuss at this point a bit the notions of \u201dperfect separation\u201d as introduced in [6]. In their Theorem 4.4. Ackerman and Dasgupta [6] show that the incremental k-means algorithm, as introduced in their Algorithm 2.2 , is not able to cluster correctly data that is \u201dperfectly clusterable\u201d (their Definition 4.1). However, it is obvious that under the \u201dperfect-ball-separation\u201d as introduced here their incremental k-means algorithm32 will discover the structure of the clusters. The reason is as follows. Perfect ball separation ensures that there exists an r of the enclosing ball such that the distance between any two points within the same ball is lower than 2r and between them is bigger than 2r. So whenever Ackerman\u2019s incremental k-mean merges two points, they are the points of the same ball. And upon merging the resulting point lies again within the ball. So we can conclude\nTheorem 18. The incremental k-means algorithm will discover the structure of perfect-ball-clustering.\nLet us note at this point, however, that the incremental k-means algorithm would return only a set of cluster centers without stating whether or not we got a perfect ball clustering. But it is important to know if this is the case because otherwise the resulting set of cluster centers may be arbitrary and under unfavorable conditions it may not correspond to a local minimum of k-means ideal at all. However, if we are allowed to inspect the data for the second time,\n32 Algorithm 2.2. (Sequential k-means) should be slightly modified: Set T = (t1, . . . , tk) to the first k data points Initialize the counts n1, n2, . . . , nk to 1 Repeat:\nAcquire the next example, tk+1. Set nk+1 = 1 If ti is the closest center to tj , j 6= i,\nReplace ti = (tini + tjnj)/(ni + nj), thereafter ni = ni + nj If j 6= k + 1 then replace tj = tk+1, nj = nk+1.\nsuch an information can be provided.33 A second pass for other algorithms from their section 2 would not yield such a decision.\nThe difference between our and their definition of well separatedness lies essentially in their understanding of clustering as a partition of data points, while in fact the user is interested in partition of the sample space (in terms of learnability theory of Valiant). Hence also a further correction of Kleinberg\u2019s axiomatic framework should take this flaw into account.\nLet us further turn to their concept of \u201dnice clustering\u201d (their Def. 3.1.). As they show in their Theorem 3.8., nice clustering cannot be discovered by an incremental algorithm with memory linear in k. In Theorem 5.3 they show that their incremental algorithm 5.2. with up to 2k\u22121 cluster centers can detect points from each of nice clusters. Again it is not the incremental k-means that may achieve it (see their theorem 5.7.) even under \u201dnice convex\u201d conditions. Surely our concept of nice-ball-clustering is even more restrictive than their \u201dnice-convex\u201d clustering. But if we upgrade their CANDIDATES(S) algorithm so that it behaves like k-means that is if we replace the step \u201dMoving bottom-up, assign each internal node the data point in one of its children\u201d with the assignment to the internal node the properly weighted (with respective cardinalities of leaves) average, then the algorithm 5.2. upgraded to incremental k-means version will in fact return the \u201drefinement\u201d of the clustering.34 What is more, if we are allowed to have a second pass through the data, then we can pick out the real cluster centers using an upgrade of the CANDIDATES(S) algorithm. The other algorithms considered in their section 5 will fail to do this on the second pass through the data (because of deviations from true cluster center).35\n33 One shall proceed as follows on the second pass: Let T = (t1, . . . , tk) be the resulting set of cluster centers from the first pass. Initialize the furthest neighbors f1, f2, . . . , fk with t1, t2 . . . , tk respectively. Repeat:\nAcquire the next example, x. If ti is the closest center to x,\nif x is further away from ti than fi then replace fi with X. Compute distances between corresponding ti and fi, pick the highest one, compute distances between each pair ti, tj and pick the lowest one. If the latter is 4 times or more higher than the former one, we got a perfect ball clustering.\n34The modified algorithm would look like: CANDIDATES(S) Run single linkage on S to get a tree (distances between t are used) Assign each leaf node the corresponding data point Moving bottom-up, assign each internal node the n = nL + nR, t = (tLnL + tRnR)/n, L,R indicating left and right child. Return all points at distance < k from the root\n35The needed algorithm would look like: Take the tree from the first pass with t values assigned in the first pass. Assign each node an f value identical to t value. Repeat:\nAcquire the next example, x. Find the leaf with t closest to x. Update its f value with x if it is further away from t than f . Pass x to all direct and indirect ancestors (internal) nodes of this leaf\nwhere in each of these nodes update its f value with x\nLet us discuss Kleinberg axioms for perfectly ball-separated clusters. It is clear that if k-means random or k-means++ gets initiated in such a way that each initial cluster center hits a different cluster, then upon subsequent steps the cluster centers will not leave the clusters. One gets stuck in a minimum, not necessarily the global one. Let us understand the Kleinberg\u2019s phrase \u201dthe function returns the clustering\u201d as one of possible (local) minima of the clustering functions. k-richness is trivially granted if we restrict ourselves to perfectlyball-separated clusters. If one performs the scaling on perfectly ball separated clusters, they will remain perfectly ball separated (scale invariance). If one applies moving-consistency transformation (keeping inner distances and relative positions to the cluster fixed coordinate systems, not bothering about distances between elements in distinct clusters) then the clusters will remain perfectly ball separated. Also a centric-consistency transformation will keep the partition in the realm of perfect-ball-clusterings. Hence\nTheorem 19. k-means, if restricted to perfectly ball separated clusterings, conforms (locally) to k-richness, scale-invariance, motion consistency and centric consistency.\nBut we gain still something more.\nProperty 11. A clustering method conforms to inner cluster consistency, if it returns the same clustering when the positions distances of cluster centers are kept, while the distances within each cluster are decreased\nNote that inner cluster consistency, as compared to inner-consistency, is less restrictive as one does not need to care about distances between elements in different clusters.\nIf one performs an inner cluster consistency transformation, the clusters will remain perfectly ball separated (a kind of inner-consistency). So we get\nTheorem 20. k-means, if restricted to perfectly ball separated clusterings, conforms (locally) to k-richness, scale-invariance, motion consistency and inner cluster consistency.\nAs with perfect clustering (see [6]), also if there exists a perfect ball clustering into k clusters, then there exists only one such clustering. Regrettably, via an inner cluster consistency transformation for a data set with perfect ball kclustering one can obtain a data set for which perfect ball k + l clustering is possible for an l > 0 even if it was impossible prior to transformation. Albeit only nested clusters will emerge. If one would choose to have the largest number of clusters with cluster cardinality \u2265 2, then one can speak about \u201drefinement inner cluster consistency\u201d, with the direction of the refinement towards smaller clusters.\nif it is further away from t than f . For each cut of the tree engaging exactly k nodes check if the nice ball clustering condition is fulfilled for balls rooted at t with radii \u2016f \u2212 t\u2016. If for any such a cut the condition holds, the nice ball clustering is found, otherwise it is not.\nSimilarly, if we move away cluster centers (motion consistency transformation), we can obtain a new perfect ball k \u2212 l clustering even if it did not exist prior to the transformation. Again, cluster nesting occurs. So if one would choose to have the lowest number of clusters k \u2265 2, then one can speak about \u201drefinement motion consistency\u201d, with the direction of the refinement towards larger clusters.\nThe very same statements can be made about Kleinberg\u2019s axioms for nice ball clustering and k-means. Except that for a given k the clustering, if exists, does not need to be unique.\nLast not least let us make the remark that even if the perfect-ball-clustering exists, it does not need to be the global optimum of k-means ideal, because of possible different cardinalities of these clusters. So in fact the global optimum may be one that is imperfect, even if the perfect clustering exists.\nBut let us state one more thing. Assume that we allow for a broader range of k values with k-means. Note that with centric consistency, contrary to inner cluster consistency transform, no new perfect ball structures will emerge. Therefore:\nTheorem 21. k-means, with k ranging over a set of values, if we assume that it returns the perfectly/nicely ball separated clusterings for the largest possible k (excluding too small clusters, we call it max-k-means algorithm), then it conforms (locally) to richness, scale-invariance, motion consistency and centric consistency."}, {"heading": "11.2 Core-based clusterings", "text": "But as we have seen in the previous section, for various purposes the distance between the balls enclosing clusters may be smaller. So let us discuss what happens if the distances (gaps) between clusters are smaller.\nWe claim that\nTheorem 22. Let A,B be cluster centers. Let \u03c1AB be the radius of a ball centered at A and enclosing its cluster and it also is the radius of a ball centered at B and enclosing its cluster. If the distance between the cluster centers A,B amounts to 2\u03c1AB + g, g > 0 (g being the \u201dgap\u201d between clusters), if we pick any two points, X from the cluster of A and Y from the cluster of B, then the new clusters will preserve the balls centered at A and B of radius g/2 (called subsequently \u201dcores\u201d) each (X the core of A, Y the core of B).\nDefinition 5. If the gap between each pair of clusters fulfills the condition of the above theorem, then we say that we have core-clustering.\nProof. For the illustration of the proof see Figure 6. The proof does not differ too much from the previous one and in fact the previous theorem is a special case when g = 2\u03c1. Consider the two points A,B being the two centers of double balls. The inner call represents the core of radius g/2, the outer ball of radius \u03c1 (\u03c1 = \u03c1AB).\ngap\nConsider two points, X,Y , one being in each outer ball (presumably the cluster centers at some stage of the k-means algorithm). To represent their distances faithfully, we need at most a 3D space.\nLet us consider the plane established by the line AB and parallel to the line XY . Let X \u2032 and Y \u2032 be projections of X,Y onto this plane. Now let us establish that the hyperplane \u03c0 orthogonal to X,Y , and passing through the middle of the line segment XY , that is the hyperplane containing the boundary between clusters centered at X and Y does not cut any of the balls centered at A and B. This hyperplane will be orthogonal to the plane of the Figure 6 and so it will manifest itself as an intersecting line l that should not cross inner circles around A and B, being projections of the respective balls. Let us draw two solid lines k,m between circles O(A, g/2) and O(B, g/2) tangential to each of them. Line l should lie between these lines, in which case the cluster center will not jump to the other ball.\nLet the line X \u2032Y \u2032 intersect with the circles O(A, \u03c1) and O(B, \u03c1) at points C,D,E, F as in the figure.\nIt is obvious that the line l would get closer to circle A, if the points X \u2032, Y \u2032 would lie closer to C and E, or closer to circle B if they would be closer to D and F .\nTherefore, to show that it does not cut the circle O(A, g/2) it is sufficient to consider X \u2032 = C and Y \u2032 = E. (The case with ball Ball(B, g/2) is symmetrical).\nLet O be the center of the line segment AB. Let us draw through this point a line parallel to CE that cuts the circles at points C \u2032, D\u2032, E\u2032 and F \u2032. Now notice that centric symmetry through point O transforms the circles O(A, \u03c1),O(B, \u03c1) into one another, and point C \u2032inF \u2032 and D\u2032inE\u2032. Let E\u2217 and F \u2217 be images of points E and F under this symmetry.\nIn order for the line l to lie between m and k, the middle point of the line segment CE shall lie between these lines.\nLet us introduce a planar coordinate system centered at O with X axis parallel to lines m, k, such that A has both coordinates non-negative, and B non-positive. Let us denote with \u03b1 the angle between the lines AB and k. As we assume that the distance between A and B equals 2\u03c1+ g, then the distance between lines k and m amounts to 2((\u03c1 + g/2) sin(\u03b1) \u2212 g/2). Hence the Y coordinate of line k equals ((\u03c1+ g/2) sin(\u03b1)\u2212 g/2).\nSo the Y coordinate of the center of line segment CE shall be not higher than this. Let us express this in vector calculus:\n4(yOC + yOE)/2 \u2264 ((\u03c1+ g/2) sin(\u03b1)\u2212 g/2)\nNote, however that\nyOC +yOE = yOA+yAC +yOB+yBE = yAC +yBE = yAC\u2212yAE\u2217 = yAC +yE\u2217A\nSo let us examine the circle with center at A. Note that the lines CD and E\u2217F \u2217 are at the same distance from the line C\u2019D\u2019. Note also that the absolute values of direction coefficients of tangentials of circle A at C\u2019 and D\u2019 are identical. The more distant these lines are, as line CD gets closer to A, the yAC gets bigger,\nand yE\u2217A becomes smaller. But from the properties of the circle we see that yAC increases at a decreasing rate, while yE\u2217A decreases at an increasing rate. So the sum yAC + yE\u2217A has the biggest value when C is identical with C\n\u2032 and we need hence to prove only that\n(yAC\u2032 + yD\u2032A)/2 = yAC\u2032 \u2264 ((\u03c1+ g/2) sin(\u03b1)\u2212 g/2)\nLet M denote the middle point of the line segment C \u2032D\u2032. As point A has the coordinates ((\u03c1 + g/2) cos(\u03b1), (\u03c1 + g/2) sin(\u03b1)), the point M is at distance of (\u03c1+ g/2) cos(\u03b1) from A. But C \u2032M2 = \u03c12 \u2212 ((\u03c1+ g/2) cos(\u03b1))2.\nSo we need to show that\n\u03c12 \u2212 ((\u03c1+ g/2) cos(\u03b1))2 \u2264 ((\u03c1+ g/2) sin(\u03b1)\u2212 g/2)2\nIn fact we get from the above\n\u03c12 \u2212 ((\u03c1+ g/2) cos(\u03b1))2 \u2264 ((\u03c1+ g/2) sin(\u03b1))2 + (g/2)2 \u2212 2(\u03c1+ g/2)(g/2) sin(\u03b1)\n\u03c12 \u2264 (\u03c1+ g/2)2 + (g/2)2 \u2212 2(\u03c1+ g/2)(g/2) sin(\u03b1) 0 \u2264 2(\u03c1+ g/2)(g/2)\u2212 2(\u03c1+ g/2)(g/2) sin(\u03b1)\n0 \u2264 2(\u03c1+ g/2)(g/2)(1\u2212 sin(\u03b1)) which is obviously true, as sin never exceeds 1.\nBut we have still to ask what is the gain of having an untouched core. Consider a cluster C of a clustering C and let it have the share p of its mass at its core of radius (g/2) and the remaining 1 \u2212 p in the ball of radius \u03c1 (all identical for each cluster from the clustering) and that the gaps between clusters amount to at least g. Let X be a randomly picked point from this cluster to be used as an initial cluster center for k-means. If it happens that each initial cluster center lies in the appropriate core, then in the first iteration of k-means all clusters are properly formed.\nIf however cluster centers lie off core then you have a chance that in the first iteration some clusters possess stranger cluster elements, but these strangers come not from the cores of other clusters. Hence we would be interested in getting the cluster centers into the cores in the next iteration. In the worst case a cluster C may lose all its off-core elements to other clusters and obtain all the other off-core elements.\nThe question is now: what portion (1\u2212 p) shall be allowed to lie off-core to ensure the convergence of iteration step. The answer is:\n(g/2/\u03c1) \u2217 nc/((g/2/\u03c1) \u2217 nc \u2212 (g/2/\u03c1) \u2217 (n\u2212 nc) + n) where n is the total number of elements, nc is the cardinality of the cluster.\nClearly, with this core separation incremental k-means will fail usually to recover the clustering. But if either of the well-separatedness criterion of core-clustering, perfect-ball-clustering or nice-ball-clustering applies, k-meansrandom and k-means++ will find the appropriate clusters, if it is seeded with one representative of each cluster. The theorems 20 and 21 when substituting \u201dperfect\u201d with \u201dcore\u201d clustering, apply.\n11.3 k-richness and the problems with realistic k-means algorithms\nBut what is the probability of such a seeding of the k-means that each cluster has a seed? Let us consider the k-means-random. If the share of elements in each cluster amounts to p1, . . . , pk, pi \u2265 p respectively, then the probability of appropriate seeding in a single run amounts to at least q = \u220fk\u22121 j=1 (1\u2212 (k\u2212 j)p). After say m runs, we can increase the probability of appropriate seeding to 1\u2212 (1\u2212 q)m, and reach the required success probability of e.g. 96%.\nUnder k-means++, in case of at least 4\u03c1 distances between clusters (perfect ball clustering) these probabilities amount to\nq = k\u22121\u220f j=1 (3\u03c1)2(k \u2212 j)p (3\u03c1)2(k \u2212 j)p+ (2\u03c1)2(1\u2212 (k \u2212 j)p)\nNow it becomes obvious why the k-richness axiom does not make much sense. Even if the clusters should turn out to be well separated (perfect ball clustering existent), the probability of hitting a cluster with 1 element out of n with growing sample size n is prohibitively small. Under k-means random for l such small clusters it is lower than 1\nnl . So the number of required restarts\nof k-means will grow approximately linearly with nk\u22121, which is better than the exhaustive search with at least kn\u2212k possibilities, but it is still prohibitive. This would render k-means useless. Respective retrial counts look significantly better for k-means++ but are still unacceptable.\n11.4 k-means++ with dispersion off-core elements\nAlternatively we can consider the off-core elements as noise that does not need to be bounded by any ball. The cores then are parts of the cluster such that they are enclosed into balls centered at cluster center where the distance to the other ball centers is four times the own radius of the core. In this case we can apply k-means++ with the provision of rejecting p \u00b7 n most distant elements upon initialization. p must be surely lower than the core of the smallest cluster. By rejecting p share of elements we run at risk of removing parts of most distant cluster. So to keep it to be likely included in seeding we must keep bounded the ration of noise contribution and cluster contribution. Noise would be at distance 4\u03c1 while the cluster at 2.5\u03c1 in unfavorable case. So to balance the contribution the noise to cluster minus noise ratio should be 2.52/42 = 1/2.56 So that the noise to smallest cluster ration should be 1:3.56.\nThis speaks again against the k-richness. Again theorems analogous to 20 and 21 apply, but now limited to the cores and not entire clusters. The noise allowed should not push cluster centers off core if other clusters are seeded in cores.\n12 k-richness versus global minimum of k-means\nLast not least let us discuss the issue whether or not we can tell that the wellseparated clusters constitute the global minimum of k-means (recall that perfect ball clustering did not).\nWe will investigate below under what circumstances it is possible to tell, without exhaustive check that the well separated clusters are the global minimum of k-means. We will see that the ratio between the largest and the smallest cluster cardinality plays here an important role. Therefore k-richness is in fact not welcome.\nIn particular, let us consider the set of k clusters C = {C1, . . . , Ck} of cardinalities n1, . . . , nk and with radii of balls enclosing the clusters (with centers located at cluster centers) r1, . . . , rk.\nWe are interested in a gap g between clusters such that it does not make sense to split each cluster Ci into subclusters Ci1, . . . , Cik and to combine them into a set of new clusters S = {S1, . . . , Sk} such that Sj = \u222aki=1Cij .\nWe seek a g such that the highest possible central sum of squares combined over the clusters Ci would be lower than the lowest conceivable combined sums of squares around respective centers of clusters Sj . Let V ar(C) be the variance of the cluster C (average squared distance to cluster gravity center). Let rij be the distance of the center of subcluster Cij to the center of cluster Ci. Let vilj be the distance of the center of subcluster Cij to the center of subcluster Clj . So the total k-means function for the set of clusters (C1, . . . , Ck) will amount to:\nQ(C) = k\u2211 i=1 k\u2211 j=1 (nijV ar(Cij) + nijr 2 ij) (15)\nAnd the total k-means function for the set of clusters (S1, . . . , Sk) will amount to: Q(S) = k\u2211 j=1 (( k\u2211 i=1 nijV ar(Cij) ) + ( k\u2211 i=1 nij) ( k\u22121\u2211 i=1 k\u2211 l=i+1 nij\u2211k i=1 nij nlj\u2211k i=1 nij v2ilj )) (16)\nShould (C1, . . . , Ck) constitute the absolute minimum of the k-means target function, then Q(S) \u2265 Q(C) should hold, that is:\nk\u2211 j=1 (( k\u2211 i=1 nijV ar(Cij) ) + ( k\u2211 i=1 nij) ( k\u22121\u2211 i=1 k\u2211 l=i+1 nij\u2211k i=1 nij nlj\u2211k i=1 nij v2ilj ))\n\u2265 k\u2211 i=1 k\u2211 j=1 (nijV ar(Cij) + nijr 2 ij)\nThis implies:\nk\u2211 j=1 ( k\u22121\u2211 i=1 k\u2211 l=i+1 nijnlj\u2211k i=1 nij v2ilj ) \u2265 k\u2211 i=1 k\u2211 j=1 nijr 2 ij (17)\nTo maximize \u2211k j=1 nijr 2 ij for a single cluster Ci of enclosing ball radius ri, note that you should set rij to ri. Let mj = arg maxj\u2208{1,...,k} nij . If we set rij = ri for all j except mj , then the maximal rimj is delimited by the relation\u2211k j=1;j 6=mj nijrij \u2265 nimjrimj . So\nk\u2211 j=1 nijr 2 ij \u2264 ( k\u2211 j=1;j 6=mj nij)r 2 i min(2, (1 +\n\u2211k j=1;j 6=mj nij\nnimj )) (18)\n\u22642( k\u2211\nj=1;j 6=mj\nnij)r 2 i\nSo if we can guarantee that the gap between cluster balls (of clusters from C) amounts to g then surely\nk\u2211 j=1 ( k\u22121\u2211 i=1 k\u2211 l=i+1 nijnlj\u2211k i=1 nij v2ilj ) \u2265 g2 k\u2211 j=1 ( k\u22121\u2211 i=1 k\u2211 l=i+1 nijnlj\u2211k i=1 nij ) (19)\nbecause in such case g \u2264 vilj for all i, l, j. By combining inequalities (17), (18) and (19) we see that the global minimum is granted if the following holds:\ng2 k\u2211 j=1 ( k\u22121\u2211 i=1 k\u2211 l=i+1 nijnlj\u2211k i=1 nij ) \u2265 2 k\u2211 i=1 ( k\u2211 j=1;j 6=mj nij)r 2 i (20)\nOne can distinguish two cases: either (1) there exists a cluster St containing two subclusters Cpt, Cqt such that t = arg maxj |Cpj | and t = arg maxj |Cqj | (maximum cardinality subclasses of their respective original clusters Cp, Cq or (2) not.\nConsider the first case. Let Cp, Cq be the two clusters where Cpt and Cqt be two subclusters of highest cardinality within Cp, Cq resp. This implies that npt \u2265 1knp, nqt \u2265 1 knq. Also this implies that for i 6= p, i 6= q nit \u2264 ni/2.\nk\u2211 j=1 k\u22121\u2211 i=1 k\u2211 l=i+1 nijnlj\u2211k i=1 nij\n\u2265 k\u22121\u2211 i=1 k\u2211 l=i+1 nitnlt\u2211k i=1 nit\n\u2265 nptnqt\u2211k i=1 nit\n\u2265 nptnqt np/2 + nq/2 + \u2211k i=1 ni/2 = nptnqt np/2 + nq/2 + n/2\n\u2265 1 k2 npnq np/2 + nq/2 + n/2\nNote that\n2 k\u2211 i=1 ( k\u2211 j=1;j 6=mj nij)r 2 i \u2264 2 k\u2211 i=1 nir 2 i\nSo, in order to fulfill inequality (20), it is sufficient to require that\ng \u2265 \u221a\u221a\u221a\u221a 2\u2211ki=1 nir2i 1 k2 npnq np/2+nq/2+n/2 = k \u221a np/2 + nq/2 + n/2 \u221a 2 \u2211k i=1 nir 2 i npnq = k \u221a np + nq + n \u221a\u2211k i=1 nir 2 i npnq (21) This of course maximized over all combinations of p, q.\nLet us proceed to the second case. Here each cluster Sj contains a subcluster of maximum cardinality of a different cluster Ci. As the relation between Sj and Ci is unique, we can reindex Sj in such a way that actually Cj contains its maximum cardinality subcluster Cjj . Let us rewrite the inequality (20).\ng2 k\u2211 j=1 ( k\u22121\u2211 i=1 k\u2211 l=i+1 nijnlj\u2211k i=1 nij ) \u2212 2 k\u2211 i=1 ( k\u2211 j=1;j 6=mj nij)r 2 i \u2265 0\nThis is met if\ng2 k\u2211 j=1 j\u22121\u2211 i=1 nijnjj\u2211k i=1 nij + k\u2211 l=j+1 njjnlj\u2211k i=1 nij \u2212 2 k\u2211 i=1 (ni \u2212 nii)r2i \u2265 0\nThis is the same as:\ng2 k\u2211 j=1  \u2211 i=1,...,j\u22121,j+1,...,k nijnjj\u2211k i=1 nij \u2212 2 k\u2211 i=1 (ni \u2212 nii)r2i \u2265 0\nThis is fulfilled if:\ng2 k\u2211 j=1  \u2211 i=1,...,j\u22121,j+1,...,k\nnijnj/k nj/2 + \u2211k i=1 ni/2 \u2212 2 k\u2211 i=1 (ni \u2212 nii)r2i \u2265 0\nLet M be the maximum over n1, . . . , nk. The above holds if\ng2 k\u2211 j=1  \u2211 i=1,...,j\u22121,j+1,...,k nijnj/k M/2 + n/2 \u2212 2 k\u2211 i=1 (ni \u2212 nii)r2i \u2265 0\nLet m be the minimum over n1, . . . , nk. The above holds if\ng2 k\u2211 j=1  \u2211 i=1,...,j\u22121,j+1,...,k nijm/k M/2 + n/2 \u2212 2 k\u2211 i=1 (ni \u2212 nii)r2i \u2265 0\nThis is the same as\ng2 m/k\nM/2 + n/2  k\u2211 j=1 \u2211 i=1,...,j\u22121,j+1,...,k nij \u2212 2 k\u2211 i=1 (ni \u2212 nii)r2i \u2265 0\ng2 m/k\nM/2 + n/2  k\u2211 j=1 (( k\u2211 i=1 nij ) \u2212 njj ) \u2212 2 k\u2211 i=1 (ni \u2212 nii)r2i  \u2265 0\ng2 m/k\nM/2 + n/2  k\u2211 j=1 k\u2211 i=1 nij \u2212 ( k\u2211 j=1 njj) \u2212 2( k\u2211 i=1 (ni \u2212 nii)r2i ) \u2265 0\ng2 m/k\nM/2 + n/2 ( k\u2211 i=1 ni ) \u2212 ( k\u2211 j=1 njj) \u2212 2 k\u2211 i=1 (ni \u2212 nii)r2i \u2265 0\ng2 m/k\nM/2 + n/2 ( k\u2211 i=1 (ni \u2212 nii) ) \u2212 2 k\u2211 i=1 (ni \u2212 nii)r2i \u2265 0\nk\u2211 i=1 (ni \u2212 nii) ( g2\nm/k\nM/2 + n/2 \u2212 2r2i\n) \u2265 0\nThe above will hold, if for every i = 1, . . . , k g \u2265 ri \u221a 2 m/k\nM/2+n/2\ng \u2265 ri \u221a k M + n\nm (22)\nSo the inequality (20) is fulfilled, if both inequality (21) and inequality (22) are held by an appropriately chosen g.\nSo we may call the above-mentioned \u201dwell-separatedness\u201d as \u201dabsolute clustering\u201d. One sees immediately that inner cluster consistency is kept, this time in terms of global optimum, under the restraint to k clusters.\nTheorem 23. k-means, if restricted to absolute clusterings, conforms globally to k-richness, scale-invariance, motion consistency and inner cluster consistency.\nRegrettably, a structure may emerge upon such consistency and therefore the maximal number of possible absolute clusters is not kept. However, if we apply centric consistency, the max-k-means[absolute] keeps the richness/invariance/motion consistency axioms.\nTheorem 24. k-means, with k ranging over a set of values, if we assume that it returns the absolutely separated clusterings for the largest possible k (excluding cluster sizes considered as too small), then it conforms globally to richness, scale-invariance, motion consistency and centric consistency.\nIn the end let us make a remark on Theorem 23. If one applies Kleinberg\u2019s consistency transformation in Euclidean space, not continuously of course, because it is not possible, as already shown, but in a discrete manner, with \u201djumping\u201d clusters, then this transform can be represented as (again in discrete manner) a superposition of motion consistency transform and inner cluster consistency transform. The reason is as follows: Consider a cluster C and a point x from another cluster C2. Let us compute the distance between x and the cluster center \u00b5C of the cluster C prior and after Kleinberg\u2019s consistency transformation to see that it increases. Consider the distance \u2016x\u2212\u00b5C\u20162. It may be expressed as a multiple (factor |C|+1|C| ) of the distance between x and the center\n\u00b5 of the data set C \u222a {x}. And \u2016x\u2212 \u00b5\u20162 = 1|C|+1 \u2211\ny\u2208C \u2016x\u2212 y\u20162. Hence it is obvious that increasing distance between x and elements of C, we increase also the distance of x to the cluster center of C.\nSo one can generalize that also the distances between clusters C,C2 increase under Kleinberg\u2019s consistency transformation. Hence in fact any Kleinberg\u2019s consistency transformation can be represented as a superposition of the mentioned transforms.\nThis means that\nTheorem 25. k-means, if restricted to absolute clusterings, conforms globally to k-richness, invariance and consistency axioms.\nFurthermore, let us relax a bit the centric consistency.\nProperty 12. A method matches the condition of inner cluster proportional consistency if after decreasing distances within a cluster by the same factor, specific to each cluster, while keeping the position of cluster center in space, it returns the same partition.\nTheorem 26. k-means, with k ranging over a set of values, if we assume that it returns the absolutely separated clusterings for the largest possible k (excluding cluster sizes considered as too small), then it conforms globally to richness, scaleinvariance, motion consistency and inner cluster proportional consistency.\nNote that motion consistency and inner cluster proportional consistency include as a special case the outer-consistency. So in this way we denied the theorem from [4] that \u201dno general clustering function can simultaneously satisfy outer-consistency, scale- invariance, and richness\u201d.\nLet us make at this point a remark why we insist on inner cluster proportional consistency. A reasonable assumption for consistency transformation would be that no possible partition of a given cluster being subject to consistency transformation would take advantage of the consistency transformation, so that no new substructures would occur in the cluster. In the context of k-means this\nwould mean the following. Consider a cluster C of a partition \u0393 of a whole set, say S, a distance d prior to a consistency transformation and a distance d\u0393 after the consistency transformation. Consider alternative partitions \u03931 and \u03932 of C. Let Q(\u0393, d) denote the quality function Q(\u0393) under the distance d. So we would expect that Q(\u03931,d\u0393)Q(\u03932,d) = Q(\u03932,d\u0393) Q(\u03932,d) unless we have a trivial partition such that each element is in separate cluster. This should hold for any pair of partitions of C, including the following ones: \u03931 puts all points into separate clusters except for x,y which go into a single cluster, \u03932 puts all points into separate clusters except for x, z. Let \u03bb1, \u03bb2 \u2208 (0, 1] be coefficients by which distances \u2016x\u2212y\u2016, \u2016x\u2212z\u2016 are shortened respectively under consistency transformation. So we will have the requirement \u2016\u03bb1(x\u2212y)\u2016 2/2\n\u2016x\u2212y\u20162/2 = \u03bb 2 1 = \u2016\u03bb2(x\u2212z)\u20162/2 \u2016x\u2212y\u20162/2 = \u03bb 2 2 which means that \u03bb1 = \u03bb2.\nBy induction over the whole set C we see that consistency transformation would need to shorten all distances within C by the same factor. This result justifies inner cluster proportional consistency concept, with a special case of centric consistency.\nWith respect to Kleinberg\u2019s consistency, we can say\nTheorem 27. k-means, with k ranging over a set of values, if we assume that it returns the absolutely separated clusterings for the largest possible k (excluding cluster sizes considered as too small), then it conforms globally to richness, scale-invariance and unidirectional refinement consistency.\nLet us inspect the effect of k-richness in both described cases. From inequality (22) we see that a large discrepancy between the maximum and minimum cluster size implies that the gap g between clusters needs to grow to get absolute clustering. From inequality (21) we see something similar, but this time the relation between the smallest cluster and the overall number of elements in the sample play the dominant role. Additionally, the gap size is impacted by the number of clusters.\nSo once again it is visible that k-richness is unfavorable for clustering process."}, {"heading": "13 Conclusions and future work", "text": "In this paper, contrary to results of former researchers, we reached the conclusion, that k-means algorithm can comply simultaneously to Kleinberg\u2019s krichness, scale-invariance and consistency axioms. A variant of k-means can comply simultaneously to Kleinberg\u2019s richness, scale-invariance and refinement consistency axioms. The same variant of k-means can even comply to richness, scale-invariance and motion plus inner proportional consistency axioms. where the last two axioms pretty well approximate Kleinberg\u2019s consistency without creating a risk of emergence of new structures within a cluster.\nThese new results emerged from the insight that our understanding of clustering process is to separate clusters with gaps.\nAs has been pointed at in earlier work of other researchers. k-means, like many other algorithms, is appropriately described neither by the richness-axiom nor by the consistency axiom of Kleinberg.\nAs richness is concerned, already Ackerman [4] showed that properties like stability against malicious attacks requires balanced clusters, hence k-richness is counterproductive when seeking stable clusterings.\nIn this paper we pointed at a number of further problems with the richness or near-richness axiom by itself. The major ones are: (a) the huge space to search through under \u201dhostile\u201d clustering criterion, (b) problems with ensuring learnability of the concept of a clustering for the population, (c) richness and scaling-invariance alone may lead to a contradiction for a special case.\nBut we showed also that resorting to k-richness, which was deemed as a remedy to Kleinberg\u2019s Impossibility Theorem, does not resolve all problems:\n\u2022 The initial seeding of cluster centers becomes extremely difficult both for k-means-random and k-means++ given that the cluster sizes differ extremely.\n\u2022 Even if we restrict ourselves to perfect ball clusterings realm, large differences in cluster sizes are prohibitive for a successful seeding.\n\u2022 For perfect ball clusterings with noise, even the smallest clusters require a high cluster size to noise size ratio.\n\u2022 In the realm of absolute clusterings, a high ratio between the lowest and the largest cluster result in high required gaps between clusters.\nWe showed also that the consistency axiom constitutes a problem: neither consistency, nor inner-consistency nor outer-consistency can be executed continuously in Euclidean space of limited dimension. Therefore, as a substitute of the inner-consistency, we proposed centric consistency and showed that k-means has the property of centric consistency.\nWhen investigating a substitute for outer-consistency, the motion consistency, we showed that (a) a gap between clusters is necessary for them to have a motion consistency with k-means, (b) the shape of the cluster counts - it has to be enclosed in a ball for k-means.\nTherefore we investigated further the impact of the gap on the behavior of the k-means in the light of Kleinberg\u2019s axioms. We showed that perfect ball clustering is a local minimum for k-means function so that for perfect ball clusterings axioms of invariance, k-richness, inner cluster consistency and motion consistency hold (the last pair as a fair substitute of the consistency). If we consider a variant of k-means with varying k over a broad spectrum of k, and take as the final clustering the perfect ball clustering into the largest number of clusters possible, and instead of inner cluster consistency the centric consistency is used then an approximation to near-richness can be achieved.36\n36We would exclude clusters with several cluster members on the grounds of the fact that statistically speaking we want to be sure that the probability of an element occurring in the gap should be smaller than in a cluster, say p times. So if we have n elements in a cluster\nand none in the gap. then we should have (\np p+1\n)n \u2264 0.05 for example. With p=10, we need\na minimal cluster size of at least n=32.\nAgain for k-means-random and k-means++ the k-richness (big variation of cluster sizes) constitutes a problem for appropriate seeding. Seeding becomes more important with gaps because gaps may prohibit recovery from inappropriate seeding.\nWe investigated absolute clustering realm that is space where perfect ball clusterings turn to global minimum for k-means. k-richness requirement widens the gaps between clusters that are necessary. Axiomatic behavior does not differ much from that of perfect ball clusterings except for the fact that after the transformations we remain in the real of absolute clusterings.\nThe introduction of gaps draws our attention to one important issue: the broader the gaps the more are the clustering properties close to Kleinberg\u2019s axioms. But this happens at a price of violating some Kleinberg\u2019s implicit assumptions: that the clustering function always returns a clustering. Let us illustrate the point with the incremental clustering algorithms of Ackerman and Dasgupta. They prove theorems of the form: \u201dIf a perfect clustering exists, then the algorithm returns it\u201d. But the question is not raised: what does the algorithm return if the clustering is not perfect? Their algorithms return \u201dsomething\u201d. We do not agree with such an approach. If the clustering type the algorithm is good looking at may not exist, the algorithm should state: \u201dI found the clustering of this type / I did not find the clustering of this type / The clustering of the given type does not exist\u201d. This would be a response from an ideal algorithm. A worse one, but still usable, would give one of the first two answers. In this investigation we show that a post-processing for kmeans would be capable to answer the question, whether the found clustering is a nice-ball-clustering, perfect-ball-clustering, or absolute-clustering, or none of them.\nBetter types of algorithms should provide with more diagnostics, concerning violations of shape, gap sizes, risks resulting from unbalanced cluster sizes and/or radii.\nSo the first conclusion is that the clustering algorithm should respond that either a clustering of required type was found or not found (along with the clustering).\nThe second question is what is the type of clustering we are looking for? It is a bad habit to run k-means over and over again and stop when the lowest value of the quality function was reached. But this clustering may be worse than ones generated in-between, e.g. if a perfect-ball-clustering exists, it may become a victim of the unbalanced cluster sizes.\nBut what we are looking for may become also a victim of the transformations Kleinberg is proposing. As the natural clusters returned by k-means are preferably ball-shaped, centric consistency transformation and the motion consistency transformation and Kleinberg consistency transform preserve them, when the gaps conform to perfect-ball or absolute separation. If Voronoi diagrams are to be shapes, then motion consistency transformation and Kleinberg\u2019s consistency transformations are destructive. If, however, any connected, well separated area would be deemed a good cluster, then even centric consistency transformation may turn out to be disastrous. Kleinberg\u2019s consistency transformation is dis-\nastrous by itself (especially under richness expectation), as it can create new cluster like structures not present in the original data.\nA similar statement may be made about the richness or any related axioms. The requirement of a too rich space of hypotheses imposes a too heavy burden on the clustering algorithms. One shall instead envision hypotheses spaces that are just rich enough and are still learnable, and where the decision is possible if we are still in the hypotheses space with our solution.\nSo, we disagree to some extent with the opinion expressed in [25, 10] that axiomatic systems deal with either clustering functions, or clustering quality function, or relations of quality of partitions. The particular formulations of axiomatic systems state rather the equivalence relations between the clusterings themselves. Hence we must first have an imagination what kind of clusters we are looking for and only then formulate the axioms with transformations that are reasonable within the target class of clusterings, do not lead outside of this class and equivalence or other relations between clusterings makes sense within this class and does not need to be defined outside.\nHence there is still much space for research on clustering axiomatization, especially for clarification, what types of clusters are of real interest and whether or not all of them can be axiomatised in the same way. Kleinberg pointed at the problem and is was a good starting point."}, {"heading": "Acknowledgments", "text": "The authors wish to thank to the Institute of Computer Science of Polish Academy of Sciences for promoting and financing this research. Research done by Robert A. K lopotek was financed by research fellowship within Project \u2019Information technologies: research and their interdisciplinary applications\u2019, agreement number UDA-POKL.04.01.01-00-051/10-00."}], "references": [{"title": "Towards theoretical foundations of clustering", "author": ["M. Ackerman"], "venue": "University of Waterloo, PhD Thesis", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Characterization of linkagebased clustering", "author": ["M. Ackerman", "S. Ben-David", "D. Loker"], "venue": "COLT 2010 - The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, pages 270\u2013281,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Weighted clustering", "author": ["Margareta Ackerman", "Shai Ben-David", "Simina Br\u00e2nzei", "David Loker"], "venue": "In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, July 22-26,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Towards property-based classification of clustering paradigms", "author": ["Margareta Ackerman", "Shai Ben-David", "David Loker"], "venue": "In J.D. Lafferty,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Clustering oligarchies", "author": ["Margareta Ackerman", "Shai Ben-David", "David Loker", "Sivan Sabato"], "venue": "In Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Incremental clustering: The case for extra clusters", "author": ["Margareta Ackerman", "Sanjoy Dasgupta"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "O pewqnym wyniku orzekajcym niemonoc przeprowadzenia analizy skupie", "author": ["S. Ambroszkiewicz", "J. Koronacki"], "venue": "krtki dowd z komentarzem. decision support systems conferernce, zakopane, poland, 2010, lecture,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "k-means++: the advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "N. Bansal, K. Pruhs, and C. Stein, editors, Proc. of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2007, pages 1027\u20131035, New Orleans, Louisiana, USA, 7-9 Jan.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Attempts to axiomatize clustering, 2005", "author": ["S. Ben-David"], "venue": "NIPS Workshop, December", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Measures of clustering quality: A working set of axioms for clustering", "author": ["S. Ben-David", "M. Ackerman"], "venue": "D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 121\u2013128. Curran Associates, Inc.,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Thoughts on clustering", "author": ["A. Blum"], "venue": "essay for the 2009 nips workshop \u201dclustering: Science or art?\u201d,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Penalized versions of the newman-girvan modularity and their relation to normalized cuts and k-means clustering", "author": ["Marianna Bolla"], "venue": "Phys Rev E Stat Nonlin Soft Matter Phys", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Persistent clustering and a theorem of j", "author": ["G. Carlsson", "F. M\u00e9moli"], "venue": "kleinberg. arXiv preprint arXiv:0808.2241,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Characterization, stability and convergence of hierarchical clustering methods", "author": ["G. Carlsson", "F. M\u00e9moli"], "venue": "J. Mach. Learn. Res., 11:1425\u20131470, August", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Kernel k-means, spectral clustering and normalized cuts", "author": ["Inderjit S. Dhillon", "Yuqiang Guan", "Brian Kulis"], "venue": "In KDD\u201904, August 22\u201325,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Pattern Classification", "author": ["R.O. Duda", "P.E. Hart", "G. Stork"], "venue": "J. Wiley & Sons, New York, 2nd edition,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "A fuzzy relative of the isodata process and its use in detecting compact well-separated clusters", "author": ["J.C. Dunn"], "venue": "J. Cyber., 3(3):32\u201357,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1974}, {"title": "Intelligent kernel k-means for clustering gene expression", "author": ["Teny Handhayania", "Lely Hiryantob"], "venue": "In International Conference on Computer Science and Computational Intelligence (ICCSCI", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Computer science theory for the information age, 2012. chapter 8.13.2", "author": ["J. Hopcroft", "R. Kannan"], "venue": "A Satisfiable Set of Axioms. page 272ff", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "An impossibility theorem for clustering", "author": ["J. Kleinberg"], "venue": "Proc. NIPS 2002, pages 446\u2013453,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "On the phenomenon of flattening \u2019flexible prediction", "author": ["M.A. K  lopotek"], "venue": "Fundamentals of Artificial Intelligence Research. International Workshop,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1991}, {"title": "Comparative analysis for k-means algorithms in network community detection", "author": ["Jian Liu"], "venue": "Advances in Computation and Intelligence: 5th International Symposium,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Comparing clusterings: An axiomatic view", "author": ["Marina Meil\u01ce"], "venue": "In Proceedings of the 22Nd International Conference on Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Fast algorithms for constant approximation k-means clustering", "author": ["Mingjun Song", "Sanguthevar Rajasekaran"], "venue": "Trans. MLDM,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Axioms for graph clustering quality functions", "author": ["T. van Laarhoven", "E. Marchiori"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Clustering stability: An overview", "author": ["U. von Luxburg"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Clustering: Science or art? initially, opinion paper for the NIPS Workshop \u201dClustering: Science or Art", "author": ["U. von Luxburg", "R. C Williamson", "I. Guyon"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "K-means clustering with manifold", "author": ["Lai Wei", "Weiming Zeng", "Hong Wang"], "venue": "In Seventh International Conference on Fuzzy Systems and Knowledge Discovery,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "A formalization of cluster analysis", "author": ["W.E. Wright"], "venue": "Pattern Recognition, 5(3):273\u2013282,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1973}, {"title": "A uniqueness theorem for clustering", "author": ["R.B. Zadeh", "S. Ben-David"], "venue": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI \u201909, pages 639\u2013646, Arlington, Virginia, United States,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 19, "context": "A number of axiomatic frameworks have been devised for methods of clustering, the most cited probably the Kleinberg\u2019s system [20].", "startOffset": 125, "endOffset": 129}, {"referenceID": 19, "context": "Kleinberg defines [20] clustering function as Definition 1.", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "], but the results to follow both negative and positive still hold if one does require\u201d Jon Kleinberg [20] claims that a good partition may only be a result of a reasonable method of clustering and he formulated axioms, for distance-based cluster analysis, that need to be met by the clustering method itself.", "startOffset": 102, "endOffset": 106}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "\u201d [20]", "startOffset": 2, "endOffset": 6}, {"referenceID": 6, "context": "Another proof can be found in a paper by Ambroszkiewicz and Koronacki [7], along with some discussion of the Kleinberg\u2019s concepts.", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "[4] prove a bit more general impossibility theorem (engaging so called inner-consistency and outer-consistency).", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "\u201d [20] 4 \u201dLet \u0393 be a partition of S, and d and d\u2032 two distance functions on S.", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": "If f(d) = \u0393, and d\u2032 is a \u0393transformation of d, then f(d\u2032) = \u0393\u201d [20].", "startOffset": 63, "endOffset": 67}, {"referenceID": 9, "context": "Note, however, that Ben-David and Ackerman [10] drew attention by an illustrative example (their Figure 2), that consistency is a problematic property by itself as it may give rise to new clusters at micro or macro-level.", "startOffset": 43, "endOffset": 47}, {"referenceID": 7, "context": "If step 1 is performed according to k-means++ heuristics proposed by Arthur and Vassilvitskii [8], then we will speak about k-means++ algorithm.", "startOffset": 94, "endOffset": 97}, {"referenceID": 23, "context": "k-means++, like [24], but it has to be stated that at the current point these algorithms are rather of theoretical value.", "startOffset": 16, "endOffset": 20}, {"referenceID": 5, "context": "Ackerman and Dasgupta [6].", "startOffset": 22, "endOffset": 25}, {"referenceID": 9, "context": "Ben-David and Ackerman in [10] in section 4.", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "Recall that in [25] it has been observed by van Laarhoven and Marchiori that Kleinberg\u2019s proof of Impossibility Theorem stops to be valid in case of graph clustering.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "k-means is applied in many domains, not only in its natural domains of data embedded in R, where clusters may be enclosed into Voronoi regions, but also to non-linearly separable clusters (via kernel functions [18]), to manifolds [28], in spectral clustering [15] and in community detection in social networks [22].", "startOffset": 210, "endOffset": 214}, {"referenceID": 27, "context": "k-means is applied in many domains, not only in its natural domains of data embedded in R, where clusters may be enclosed into Voronoi regions, but also to non-linearly separable clusters (via kernel functions [18]), to manifolds [28], in spectral clustering [15] and in community detection in social networks [22].", "startOffset": 230, "endOffset": 234}, {"referenceID": 14, "context": "k-means is applied in many domains, not only in its natural domains of data embedded in R, where clusters may be enclosed into Voronoi regions, but also to non-linearly separable clusters (via kernel functions [18]), to manifolds [28], in spectral clustering [15] and in community detection in social networks [22].", "startOffset": 259, "endOffset": 263}, {"referenceID": 21, "context": "k-means is applied in many domains, not only in its natural domains of data embedded in R, where clusters may be enclosed into Voronoi regions, but also to non-linearly separable clusters (via kernel functions [18]), to manifolds [28], in spectral clustering [15] and in community detection in social networks [22].", "startOffset": 310, "endOffset": 314}, {"referenceID": 14, "context": "[15] that kmeans is equivalent in some sense to normalized cut method of graph clustering, which in turn can be viewed as equivalent to balanced Newman\u2019s modularity, used in community detection, as shown by Bolla [12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[15] that kmeans is equivalent in some sense to normalized cut method of graph clustering, which in turn can be viewed as equivalent to balanced Newman\u2019s modularity, used in community detection, as shown by Bolla [12].", "startOffset": 213, "endOffset": 217}, {"referenceID": 14, "context": "[15] and the impossibility theorem challenge for graphs in [25] encourage to investigate Kleinberg\u2019s axioms in the context of Euclidean space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[15] and the impossibility theorem challenge for graphs in [25] encourage to investigate Kleinberg\u2019s axioms in the context of Euclidean space.", "startOffset": 59, "endOffset": 63}, {"referenceID": 28, "context": "2 Previous work Axiomatic systems may be traced back to as early as 1973, when Wright [29] proposed axioms of clustering functions creating unsharp partitions, similar to fuzzy systems.", "startOffset": 86, "endOffset": 90}, {"referenceID": 24, "context": "In general, as exposed by van Laarhoven and Marchiori [25] and Ben-David and Ackerman [10] the clustering axiomatic frameworks address either: \u2022 required properties of clustering functions, or \u2022 required properties of the values of a clustering quality function, or", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "In general, as exposed by van Laarhoven and Marchiori [25] and Ben-David and Ackerman [10] the clustering axiomatic frameworks address either: \u2022 required properties of clustering functions, or \u2022 required properties of the values of a clustering quality function, or", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": "So Ben-David and Ackerman [10], as mentioned, pointed at the problems with consistency as such.", "startOffset": 26, "endOffset": 30}, {"referenceID": 0, "context": "We recall several of them here, based on an overview by Ackerman [1] and tutorial by Ben-David [9].", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "We recall several of them here, based on an overview by Ackerman [1] and tutorial by Ben-David [9].", "startOffset": 95, "endOffset": 98}, {"referenceID": 29, "context": "So it was proposed to weaken Kleinberg\u2019s richness (by Kleinberg himself) to so-called k-richness as follows: Property 6 (Zadeh and Ben-David [30]).", "startOffset": 141, "endOffset": 145}, {"referenceID": 3, "context": "[4] propose the concept of outer-consistency Property 7.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "8Still another relaxation of richness was proposed by Hopcroft and Kannan [19]: Richness II: For any set K of k distinct points in the given Euclidean space, there is an n and a set of S of n points such that the algorithm on input S produces k clusters, whose centers are the respective points in K.", "startOffset": 74, "endOffset": 78}, {"referenceID": 4, "context": "[5], a useful property of stability of clusters under malicious addition of data points holds only for balanced clusters.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "Zadeh Ben-David [30] propose instead the order-consistency so that some versions of single-linkage algorithm can be classified as \u201dclustering algorithm\u201d.", "startOffset": 16, "endOffset": 20}, {"referenceID": 16, "context": "Let us also mention here some works like that by Dunn [17] or Ackerman and Dasgupta [6] that seemingly have nothing to do with Kleinberg\u2019s axioms, but this is only a superficial impression.", "startOffset": 54, "endOffset": 58}, {"referenceID": 5, "context": "Let us also mention here some works like that by Dunn [17] or Ackerman and Dasgupta [6] that seemingly have nothing to do with Kleinberg\u2019s axioms, but this is only a superficial impression.", "startOffset": 84, "endOffset": 87}, {"referenceID": 5, "context": "Ackerman and Dasgupta [6] handle incremental clustering algorithms.", "startOffset": 22, "endOffset": 25}, {"referenceID": 5, "context": "The clusters are \u201dnicely separated\u201d, as defined by [6], if a distance between an element and any other element of the same cluster is lower than the distance from this element to an element outside of the cluster.", "startOffset": 51, "endOffset": 54}, {"referenceID": 9, "context": "Ackerman and Ben-David [10] propose another direction of resolving the problem of Kleinberg\u2019s axiomatisation impossibility.", "startOffset": 23, "endOffset": 27}, {"referenceID": 1, "context": "[2] for linkage algorithms, [14] for hierarchical algorithms, [13] for multiscale clustering.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[2] for linkage algorithms, [14] for hierarchical algorithms, [13] for multiscale clustering.", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "[2] for linkage algorithms, [14] for hierarchical algorithms, [13] for multiscale clustering.", "startOffset": 62, "endOffset": 66}, {"referenceID": 22, "context": "Meila [23] demonstrates that one can\u2019t compare partitions in a manner that agrees with the lattice of partitions, is convexly additive and bounded.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "[3, 5].", "startOffset": 0, "endOffset": 6}, {"referenceID": 4, "context": "[3, 5].", "startOffset": 0, "endOffset": 6}, {"referenceID": 4, "context": "[5] deals", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] demonstrates that one can put any two data points into different clusters if one applies weighting functions to data points.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] implies that too small clusters may be disintegrated by hostile new points so that for practical purposes one shall be only interested in larger clusters.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] allows to conclude that poor estimates of densities for sparse clusters may lead to erroneous drawing of cluster boundaries.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "As mentioned, [25] pointed at the fact that such a construction would not be possible in the realm of graph clustering.", "startOffset": 14, "endOffset": 18}, {"referenceID": 19, "context": "But \u201dnear-richness\u201d is again not enough to resolve all contradictions (as by the way is visible from the Kleinberg\u2019s anti-chain theorem [20]).", "startOffset": 136, "endOffset": 140}, {"referenceID": 29, "context": "[30], is obviously violated because k-means returns only partitions into k clusters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "19However, this quality function fails on the axiom of Function Scale Invariance, proposed in [10].", "startOffset": 94, "endOffset": 98}, {"referenceID": 3, "context": "As mentioned earlier, following [4], for probabilistic algorithms we will talk about probabilistic k-richness, that is one obtainable with some probability, independent of the actual clustering that is intended to be obtained.", "startOffset": 32, "endOffset": 35}, {"referenceID": 9, "context": "21Such a quality function would satisfy axiom of Function Scale Invariance, proposed in [10]", "startOffset": 88, "endOffset": 92}, {"referenceID": 3, "context": "This theorem contradicts apparently [4] claim that k-means possesses the property of outer-consistency.", "startOffset": 36, "endOffset": 39}, {"referenceID": 9, "context": "25Already Ben-David [10] indicated problems in this direction.", "startOffset": 20, "endOffset": 24}, {"referenceID": 20, "context": "If the clustering function can fit any data, we are practically unable to learn any structure of data space from data [21].", "startOffset": 118, "endOffset": 122}, {"referenceID": 26, "context": "Note that we speak here about a much broader aspect than so-called cluster stability or cluster validity, pointed at by Luxburg [27, 26].", "startOffset": 128, "endOffset": 136}, {"referenceID": 25, "context": "Note that we speak here about a much broader aspect than so-called cluster stability or cluster validity, pointed at by Luxburg [27, 26].", "startOffset": 128, "endOffset": 136}, {"referenceID": 10, "context": "26 Strict separation [11] mentioned earlier is another kind of a weird cluster quality function, requiring visits to all the partitions", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "Consider moving a data point x \u2217 from the cluster Cj0 to cluster Cjl As demonstrated by [16], V (Cj0 \u2212 {x\u2217}) = V (Cj0)\u2212 nj0 nj0\u22121 \u2016x\u2217 \u2212 \u03bcj0\u2016 2 and V (Cjl \u222a {x\u2217}) = V (Cjl) + nl nl+1\u2016x \u2217 \u2212 \u03bcjl\u2016 2 So it pays off to move a point from one cluster to another if nj0 nj0\u22121 \u2016x\u2217 \u2212 \u03bcj0\u2016 2 > njl njl+1 \u2016x\u2212\u03bcjl\u2016 .", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "For any point (x, f(x) of the border of the first cluster with center (0, 0) the following must hold: (x\u2212 2x0) + (f(x)\u2212 2f(x0)) \u2212 x \u2212 f(x) \u2265 0 (12) That is \u22122x0(2x\u2212 2x0)\u2212 2f(x0) (2f(x)\u2212 2f(x0)) \u2265 0 \u2212f(x0) (f(x)\u2212 f(x0)) \u2265 x0(x\u2212 x0) 31This is by the way the nice trick behind the claim in [6] that incremental k-means does not identify perfectly separated clusters.", "startOffset": 287, "endOffset": 290}, {"referenceID": 5, "context": "Clusters in k-means are not the points, they are polyhedrons, contrary to the assumptions in [6].", "startOffset": 93, "endOffset": 96}, {"referenceID": 5, "context": "Let us discuss at this point a bit the notions of \u201dperfect separation\u201d as introduced in [6].", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "Ackerman and Dasgupta [6] show that the incremental k-means algorithm, as introduced in their Algorithm 2.", "startOffset": 22, "endOffset": 25}, {"referenceID": 5, "context": "As with perfect clustering (see [6]), also if there exists a perfect ball clustering into k clusters, then there exists only one such clustering.", "startOffset": 32, "endOffset": 35}, {"referenceID": 3, "context": "So in this way we denied the theorem from [4] that \u201dno general clustering function can simultaneously satisfy outer-consistency, scale- invariance, and richness\u201d.", "startOffset": 42, "endOffset": 45}, {"referenceID": 3, "context": "As richness is concerned, already Ackerman [4] showed that properties like stability against malicious attacks requires balanced clusters, hence k-richness is counterproductive when seeking stable clusterings.", "startOffset": 43, "endOffset": 46}, {"referenceID": 24, "context": "So, we disagree to some extent with the opinion expressed in [25, 10] that axiomatic systems deal with either clustering functions, or clustering quality function, or relations of quality of partitions.", "startOffset": 61, "endOffset": 69}, {"referenceID": 9, "context": "So, we disagree to some extent with the opinion expressed in [25, 10] that axiomatic systems deal with either clustering functions, or clustering quality function, or relations of quality of partitions.", "startOffset": 61, "endOffset": 69}], "year": 2017, "abstractText": "This paper investigates the validity of Kleinberg\u2019s axioms for clustering functions with respect to the quite popular clustering algorithm called k-means.We suggest that the reason why this algorithm does not fit Kleinberg\u2019s axiomatic system stems from missing match between informal intuitions and formal formulations of the axioms. While Kleinberg\u2019s axioms have been discussed heavily in the past, we concentrate here on the case predominantly relevant for k-means algorithm, that is behavior embedded in Euclidean space. We point at some contradictions and counter intuitiveness aspects of this axiomatic set within R that were evidently not discussed so far. Our results suggest that apparently without defining clearly what kind of clusters we expect we will not be able to construct a valid axiomatic system. In particular we look at the shape and the gaps between the clusters. Finally we demonstrate that there exist several ways to reconcile the formulation of the axioms with their intended meaning and that under this reformulation the axioms stop to be contradictory and the real-world k-means algorithm conforms to this axiomatic system.", "creator": "LaTeX with hyperref package"}}}