{"id": "1702.05270", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2017", "title": "Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers from Vision", "abstract": "People can refer to quantities in a visual scene by using either exact cardinals (e.g. one, two, three) or natural language quantifiers (e.g. few, most, all). In humans, these two processes underlie fairly different cognitive and neural mechanisms. Inspired by this evidence, the present study proposes two models for learning the objective meaning of cardinals and quantifiers from visual scenes containing multiple objects. We show that a model capitalizing on a 'fuzzy' measure of similarity is effective for learning quantifiers, whereas the learning of exact cardinals is better accomplished when information about number is provided.\n\n\n\n\n\n\n\n\n\n\n\nWe present a conceptual model (a neural network of brain neurons) that combines a set of numerical inputs, then maps and model and model and model-based models to identify the correct data. We explore how the representations of cardinals and quantifiers could be integrated into various brain networks and also explain the neural circuits underlying the different types of memory.\n\nThis paper is an early introduction to the neural circuits used in the study of cardinals. It shows that there is an important advantage to using a neural network of neurons as a general framework for learning the objective meaning of cardinals and quantifiers from visual scenes.\nThe neural circuit networks, which control the activity of the brain regions in the brain in visual scenes, provide useful insights into neural circuits that can be used to interpret information about visual tasks.\nOur results demonstrate a neural network of neurons which are capable of performing complex, task-solving tasks.\nIn addition to these neural circuits, we show that, in the short term, learning information about the visual part of the brain is often not able to be integrated with other learning parts.\nA network that relies primarily on mathematical equations, or numerical equations, is a non-mathematical network of neurons that integrates various cognitive tasks. It can be constructed using various mathematical equations, or numerical equations, to perform different learning tasks. We show that, in the short term, learning information about the visual part of the brain is often not able to be integrated with other learning parts.\nOur results show that, in the short term, learning information about the visual part of the brain is often not able to be integrated with other learning parts.\nThe neural circuit networks, which control the activity of the brain regions in the brain in visual scenes, provide useful insights into neural circuits that can be used to interpret information about visual tasks.\nOur results", "histories": [["v1", "Fri, 17 Feb 2017 09:26:10 GMT  (762kb,D)", "http://arxiv.org/abs/1702.05270v1", "Accepted at EACL2017. 7 pages"]], "COMMENTS": "Accepted at EACL2017. 7 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["sandro pezzelle", "marco marelli", "raffaella bernardi"], "accepted": false, "id": "1702.05270"}, "pdf": {"name": "1702.05270.pdf", "metadata": {"source": "CRF", "title": "Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers from Vision", "authors": ["Sandro Pezzelle", "Marco Marelli", "Raffaella Bernardi"], "emails": ["sandro.pezzelle@unitn.it", "marco.marelli@ugent.be", "raffaella.bernardi@unitn.it"], "sections": [{"heading": "1 Introduction", "text": "In everyday life, people can refer to quantities by using either cardinals (e.g. one, two, three) or natural language quantifiers (e.g. few, most, all). Although they share a number of syntactic, semantic and pragmatic properties (Hurewitz et al., 2006), and they are both learned in a fairly stable order of acquisition across languages (Wynn, 1992; Katsos et al., 2016), these quantity expressions underlie fairly different cognitive and neural mechanisms. First, they are handled differently by the language acquisition system, with children recognizing their disparate characteristics since early development, even before becoming \u2018full-counters\u2019 (Hurewitz et al., 2006; Sarnecka and Gelman, 2004; Barner et al., 2009). Second, while the neural processing of cardinals relies on the brain region devoted to the representation of quantities, quantifiers rather elicit regions for general semantic processing (Wei et al., 2014). Intuitively, cardinals and quantifiers refer to quantities in a different way, with the former representing a mapping between a word and the exact cardinality of a set, the latter expressing a \u2018fuzzy\u2019 numerical concept denoting set relations\nor proportions of sets (Barner et al., 2009). As a consequence, speakers can reliably answer questions involving quantifiers even in contexts that preclude counting (Pietroski et al., 2009), as well as children lacking exact cardinality concepts can understand and appropriately use quantifiers in grounded contexts (Halberda et al., 2008; Barner et al., 2009). That is, knowledge about (large) precise numbers is neither necessary nor sufficient for learning the meaning of quantifiers.\nInspired by this evidence, the present study proposes two computational models for learning the meaning of cardinals and quantifiers from visual scenes. Our hypothesis is that learning cardinals requires taking into account the number of instances of the target object in the scene (e.g. number of dogs in Figure 1). Learning quantifiers, instead, would be better accomplished by a model capitalizing on a measure evaluating the \u2018fuzzy\u2019 amount of target objects in the scene (e.g. proportion of \u2018dogness\u2019 in Figure 1). In particular, we focus on those cases where both quantification strategies might be used, namely scenes containing target (dogs) and distractor objects (cats). Our approach is thus different from salient objects detection, where the distinction targets/distractors is missing (Borji et al., 2015; Zhang et al., 2015; Zhang et al., 2016). With respect to cardinals, our approach is similar to (Segu\u00ed et al., 2015), who propose a model for counting people in natural ar X iv :1 70 2.\n05 27\n0v 1\n[ cs\n.C L\n] 1\n7 Fe\nb 20\n17\nscenes, and to more recent work aimed at counting either everyday objects in natural images (Chattopadhyay et al., 2016) or geometrical objects with attributes in synthetic scenes (Johnson et al., 2016). With respect to quantifiers, our approach is similar to (Sorodoc et al., 2016), who use quantifiers no, some, and all to quantify over sets of colored dots. Differently from ours, however, all these works tackle the issue as either a classification problem or a Visual Question Answering task, with less focus on learning the meaning representation of each cardinal/quantifier. To our knowledge, this is the first attempt to jointly investigate both mechanisms and to obtain the meaning representaton of each cardinal/quantifier as resulting from a language-to-vision mapping.\nBased on their geometric intepretation, we propose to use cosine and dot product similarity between the target object and the scene as our measures for quantifiers and cardinals, respectively. The former, ranging from -1 to 1, evaluates the similarity between two vectors with respect to their orientation and irrespectively of their magnitudes. That is, the more two vectors are overall similar, the closer they are. Ideally, cosine similarity between an image depicting a dog and a scene containing either 3 or 10 dogs without distractors (hence, \u2018all\u2019) should be equal to 1. Therefore, it would indicate that the proportion of \u2018dogness\u2019 in the scene is highest. Dot product, on the other hand, is defined as the product of the cosine between two vectors and their Euclidean magnitudes. By taking into account the magnitudes, this measure ideally encodes information regarding the number of times a target object is repeated in the scene. In the above-mentioned example, indeed, dot product would be 3 and 10, respectively. In this simplified setting, thus, it would be equal to the number of dogs.\nFurthermore, we propose that the \u2018objective\u2019 meaning of each cardinal/quantifier can be learned by means of a cross-modal mapping (see Figure 4) between the linguistic representation of the target object and its quantity (either exact or fuzzy) in a visual scene. To test our hypotheses, we carry out a proof-of-concept on the synthetic datasets we describe in Section 2. First, we explore our visual data by means of the two proposed similarity measures (\u00a7 3.1). Second, we learn the meaning representations of cardinals and quantifiers and evaluate them in the task of retrieving unseen combinations\nof targets/distractors (\u00a7 3.2). As hypothesized, the two quantification mechanisms turn out to be better accounted for by models capitalizing on the expected similarity measures."}, {"heading": "2 Data", "text": "In order to test our hypothesis, we need a dataset of visual scenes which crucially include multiple objects. Moreover, some objects in the scene should be repeated, so that we might say, for instance, that out of 5 objects \u2018three\u2019/\u2018most\u2019 are dogs. Although a large number of image datasets are currently available (see Lin et al. (2014) among many others), no one fully satisfies these requirements. Typically, images depict one salient object and even when multiple salient objects are present, only a handful of cases contain both targets and distractors (Zhang et al., 2015; Zhang et al., 2016). To bypass these issues, in the present work we experiment with synthetic visual scenes (hence, scenarios) that are made up by at most 9 images each representing one object. The choice of using a \u2018patchwork\u2019 of object-depicting images is motivated by the need of representing a reasonably large variability (e.g. \u2018few\u2019 refer to scenes containing 2 target objects out of 7 as well as 1/5, 4/9, etc.). This way, we avoid matching a quantifier always with the same number of target objects (except no, that is always represented by 0 targets), and allow cardinals to be represented by scenes with different numbers of distractors. At the same time, we get rid of any issues related to object localization.\nWe experiment with quantifiers (hence, Qs) no, few, most, and all, which we defined a priori by ratios 0%, 1-49%, 51-99% and 100%, respectively. Consistently with our goals, this arguably simplified setting does neither take into account pragmatic uses of Qs (i.e. we treat them as lying on an ordered scale) nor reflect possible overlappings. For these reasons, we avoid using quantifiers as some whose meaning overlaps with the meaning of many others. As far as cardinals (hence, Cs) are concerned, we experiment with scenarios in which the cardinality of the targets ranges from 1 to 4. Cs up to 4 are acquired by children incrementally at subsequent stages of their development, with higher numbers being learned upon this knowledge with the ability of counting (Barner et al., 2009). Also, Cs ranging from 1 to 3-4 are widely known to exhibit some peculiar properties\n(i.e. their exact number can be immediately and effortlessly grasped) due to which they are usually referred to as \u2018subitizing\u2019 range (Piazza et al., 2011; Railo et al., 2016)."}, {"heading": "2.1 Building the scenarios", "text": "We use images from ImageNet (Deng et al., 2009). Starting from the full list of 203 concepts and corresponding images extracted by Cassani (2014), we discarded those concepts whose corresponding word had low/null frequency in the large corpus used in (Baroni et al., 2014). To get rid of issues related to concept identification, we used a single representation for each of the 188 selected concepts. Technically, we computed a centroid vector by averaging the 4096-dimension visual features of the corresponding images, which were extracted from the fc7 of a CNN (Simonyan and Zisserman, 2014). We used the VGG-19 model pretrained on the ImageNet ILSVRC data (Russakovsky et al., 2015) implemented in the MatConvNet toolbox (Vedaldi and Lenc, 2015). Centroid vectors were reduced to 100-d via PCA and further normalized to length 1 before being used to build the scenarios. When building the scenarios, we put the constraint that distractors have to be different from each other. Moreover, only distractors whose visual cosine similarity with respect to the target is lower than the average are selected. For each scenario, target and distractor vectors are summed together. As a result, each scenario is represented by a 100-d vector.\nWe also experimented with scenarios where vectors are concatenated to obtain a 900-d vector (empty \u2018cells\u2019 are filled with 0s vectors) and further reduced to 100-d via PCA. Since the pattern of results in the only-vision evaluation (see \u00a7 3.1) turned out to be similar to the results obtained in the \u2018summed\u2019 setting, due to space limitations we will only focus on the \u2018summed\u2019 setting."}, {"heading": "2.2 Datasets", "text": "We built one dataset for Cs and one for Qs, each containing 4512 scenarios.1 We then split each of the two in one 3008-datapoint Training Dataset (Train) for training and validation and one 1504- datapoint Testing Dataset (Test) for testing. The two datasets were split according to their \u2018combinations\u2019, that is the mixture of targets and distractors in the scenario. As reported in Table 1, we kept 4 different combinations for each C/Q in Train and 2 in Test. Note that the numerator refers to the number of targets, the denominator to the total number of objects. The number of distractors is thus given by the difference between the two values. To illustrate, in Train-q \u2018few\u2019 is represented by scenarios 1/6, 2/5, 2/7, and 3/8, whereas in Test-q \u2018few\u2019 is represented by scenarios 1/7 and 4/9. The initial 4512 scenarios have been obtained by building a total of 24 different scenarios (6 combinations * 4 C/Q classes) for each of the 188 objects. A particular effort has been paid in making the datasets as balanced as possible. When designing the combinations for \u2018few\u2019 and \u2018most\u2019, for example, we controlled for the proportion of targets in the scene, in order to avoid making one of the two easier to learn. Also, combinations were thought to avoid biasing cardinals toward fixed proportions of targets/distractors."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Only-vision evaluation", "text": "As a first step, we carry out a preliminary evaluation aimed at exploring our visual data. If our intuition about the information encoded by the two similarity measures is correct (see \u00a7 1), we\n1A visual representation of our scenarios is provided in the rightmost side of Figure 4, while Figure 1 is only intended to provide a more intuitive overview of the task.\nshould observe that cosine is more effective than dot product in distinguishing between different Qs, while the latter should be better than cosine for Cs. Moreover, Qs/Cs should lie on an ordered scale. To test our hypothesis, we compute cosine distances (i.e. 1\u2212cosine, to avoid negative values) and dot product similarity for each target-scenario pair in both Train and Test (e.g. dog vs 2/5 dogs). Figure 2 reports the distribution of Qs with respect to cosine (left) and Cs with respect to dot product (right) in Train. As can be seen from the boxplots, both Qs and Cs are ordered on a scale. In particular, cosine distance is highest in no scenarios (where the target is not present), lowest in all scenarios. For Cs, dot product is highest in four scenarios, lowest in one scenarios.\nOur intuition is further confirmed by the results of a radial-kernel SVM classifier fed with either cosine or dot product similarities as predictors.2 Qs are better predicted by cosine than dot product (78.6% vs 63.8%), whereas dot product is a better predictor of Cs than cosine (68.7% vs 44.7%). As shown in Figure 3, the ordered scale is indeed represented to a much lesser extent when Qs are plotted against dot product (left) and Cs against cosine (right). A similar pattern of SVM results and similar plots emerged when experimenting with Test."}, {"heading": "3.2 Cross-modal mapping", "text": "Our core proposal is that the meaning of each C/Q can be learned by means of a cross-modal mapping between the linguistic representation of the target object (e.g. dog, mug, etc.) and a number of scenarios representing the target object in a given C/Q setting (e.g. \u2018two\u2019/\u2018few\u2019 dogs). In our approach, each word (e.g. dog) is represented by\n2We experimented with linear, polynomial, and radial kernels. We only report results obtained with default radial kernel, that turned out to be the overall best model.\na 400-d embedding built with the CBOW architecture of word2vec (Mikolov et al., 2013) and the best-predictive parameters of Baroni et al. (2014) on a 2.8B tokens corpus. The original 400-d vectors are further reduced to 100-d via PCA before being fed into the model.\nFigure 4 reports a single learning event of our proposed model. Each C/Q (e.g. two, few) is learned as a separate function that maps each of the 188 words representing our selected concepts to its corresponding 4 scenarios in Train (see \u00a7 2.2). To illustrate, the meaning of few is learned by mapping each word into the 4 visual scenes where the amount of \u2018targetness\u2019 is less than 50% (see \u00a7 2), whereas two is learned by mapping each word to the scenarios where the number of targets is 2, and so on. This mapping, we conjecture, would mimic the multimodal mechanism by which children acquire the meaning of both Cs and Qs (see Halberda et al. (2008)). Once learned, the function representing each C/Q can be evaluated against scenarios containing an unseen mixture of (known) target objects and distractors. If it has encoded the correct meaning of the quantified expression, the function will retrieve the unseen scenarios containing the correct quantity (either exact or fuzzy) of target objects.\nWe experiment with three different models: linear (lin), cosine neural network (nn-cos), dotproduct neural network (nn-dot). The first model is a simple linear mapping. The second is a singlelayer neural network (activation function ReLU) that maximizes the cosine similarity between input (linguistic) and output vector (visual). The third is a similar neural network that approximates to 1 the\ndot product between input and output. We evaluate the mapping functions by means of a retrieval task aimed at picking up the correct scenarios from Test among the set of 8 scenarios built upon the same target object. Recall that in Test there are 2 combinations * 4 C/Q classes for each concept.\nResults As reported in Table 2, nn-cos is overall the best model for Qs, whereas nn-dot is the best model for Cs. In particular, mean average precision (mAP) is higher in nn-cos for 3 out of 4 Qs, with only most reaching slightly better mAP in Q nn-dot due to the high number of cases confounded with all by the Q nn-cos model (see Table 3). Conversely, both mAP and precision at top2 positions (P2) for Cs are always higher in nn-dot compared to the other models. From a qualitative analysis of the results, it emerges that both the best-predictive models make \u2018plausible\u2019 errors, i.e. they confound Cs/Qs that are close to each other in the ordered scale. Table 3 reports the confusion matrices for the best performing models. Besides retrieving more cases of all instead of (correct) most, the Q nn-cos model often confounds few with no. Similarly, the C nn-dot model often confounds three with four, one with two, two with three, and so on. Overall, both models pick up very few or no responses that are on the opposite end of the \u2018scale\u2019, thus suggesting that the meaning representation they learn encodes, to a certain extent, information about the ordered position of the quantified expressions."}, {"heading": "4 Discussion", "text": "We propose that the meaning of Cs and Qs can be learned by means of a language-to-vision mapping, and we show that two models capitalizing on dot product and cosine better account for Cs and Qs, respectively. In future research, we plan to further investigate this issue by using real-scene images to avoid constraining the visual data. Moreover, we plan to experiment with a broader set of\nquantifiers (e.g. some, almost all, etc.) and higher cardinals. The latter investigation, in particular, would allow us to verify whether our approach is suitable for the (potentially infinite) set of \u2018cardinal functions\u2019 beyond the subitizing range. If so, we might observe that the models keep making cognitively plausible errors, picking items that are close to the target one in the ordered scale. This evidence, we believe, would further motivate our \u2018one quantifed expression, one function\u2019 approach, which is partially inspired by the evidence that, in human brain, so-called number neurons are tuned to preferred numbers (Nieder, 2016). Simplifying somewhat, each number would activate specific neurons. Finally, we believe that taking into account speakers\u2019 uses of Cs and Qs would constitute the natural next step toward a complete modelling of the meaning of quantified expressions."}, {"heading": "Acknowledgments", "text": "We are very grateful to Germ\u00e1n Kruszewski for the invaluable contribution in developing and discussing the intuitions behind this work. We are also grateful to Marco Baroni, Aur\u00e9lie Herbelot, Gemma Boleda and Ravi Shekhar for their advice and feedback. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used in our research, and the iV&L Net (ICT COST Action IC1307) for funding the second author\u2019s research visit aimed at working on this project."}], "references": [{"title": "Crosslinguistic relations between quantifiers and numerals in language acquisition: Evidence from Japanese", "author": ["Barner et al.2009] David Barner", "Amanda Libenson", "Pierina Cheung", "Mayu Takasaki"], "venue": "Journal of experimental child psychology,", "citeRegEx": "Barner et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Barner et al\\.", "year": 2009}, {"title": "Don\u2019t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni et al.2014] Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": null, "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Salient object detection: A benchmark", "author": ["Borji et al.2015] Ali Borji", "Ming-Ming Cheng", "Huaizu Jiang", "Jia Li"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Borji et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Borji et al\\.", "year": 2015}, {"title": "Distributional semantics for child directed speech: A multimodal approach", "author": ["Giovanni Cassani"], "venue": null, "citeRegEx": "Cassani.,? \\Q2014\\E", "shortCiteRegEx": "Cassani.", "year": 2014}, {"title": "Counting everyday objects in everyday scenes. arXiv preprint arXiv:1604.03505", "author": ["Ramakrishna Vedantam", "Ramprasaath RS", "Dhruv Batra", "Devi Parikh"], "venue": null, "citeRegEx": "Chattopadhyay et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chattopadhyay et al\\.", "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng et al.2009] Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "The development of \u2018most\u2019 comprehension and its potential dependence on counting ability in preschoolers", "author": ["Len Taing", "Jeffrey Lidz"], "venue": "Language Learning and Development,", "citeRegEx": "Halberda et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Halberda et al\\.", "year": 2008}, {"title": "Asymmetries in the acquisition of numbers and quantifiers", "author": ["Anna Papafragou", "Lila Gleitman", "Rochel Gelman"], "venue": "Language learning and development,", "citeRegEx": "Hurewitz et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hurewitz et al\\.", "year": 2006}, {"title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning", "author": ["Bharath Hariharan", "Laurens van der Maaten", "Li Fei-Fei", "C Lawrence Zitnick", "Ross Girshick"], "venue": null, "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Microsoft COCO: Common objects in context", "author": ["Lin et al.2014] Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "The neuronal code for number", "author": ["Andreas Nieder"], "venue": "Nature Reviews Neuroscience", "citeRegEx": "Nieder.,? \\Q2016\\E", "shortCiteRegEx": "Nieder.", "year": 2016}, {"title": "Subitizing reflects visuo-spatial object individuation capacity", "author": ["Antonia Fumarola", "Alessandro Chinello", "David Melcher"], "venue": null, "citeRegEx": "Piazza et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Piazza et al\\.", "year": 2011}, {"title": "The meaning of \u2018most\u2019: Semantics, numerosity and psychology", "author": ["Jeffrey Lidz", "Tim Hunter", "Justin Halberda"], "venue": "Mind & Language,", "citeRegEx": "Pietroski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pietroski et al\\.", "year": 2009}, {"title": "Rapid and accurate processing of multiple objects in briefly presented scenes", "author": ["Railo et al.2016] Henry Railo", "Veli-Matti Karhu", "Jeremy Mast", "Henri Pesonen", "Mika Koivisto"], "venue": "Journal of vision,", "citeRegEx": "Railo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Railo et al\\.", "year": 2016}, {"title": "ImageNet large scale visual recognition challenge", "author": ["Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Six does not just mean a lot: Preschoolers see number words as specific", "author": ["Sarnecka", "Gelman2004] Barbara W Sarnecka", "Susan A Gelman"], "venue": null, "citeRegEx": "Sarnecka et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sarnecka et al\\.", "year": 2004}, {"title": "Learning to count with deep object features", "author": ["Segu\u00ed et al.2015] Santi Segu\u00ed", "Oriol Pujol", "Jordi Vitria"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,", "citeRegEx": "Segu\u00ed et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Segu\u00ed et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Zisserman2014] Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Look, some green circles!\u2019: Learning to quantify from images", "author": ["Angeliki Lazaridou", "Gemma Boleda", "Aur\u00e9lie Herbelot", "Sandro Pezzelle", "Raffaella Bernardi"], "venue": "In Proceedings of the 5th Workshop on Vision", "citeRegEx": "Sorodoc et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sorodoc et al\\.", "year": 2016}, {"title": "MatConvNet \u2013 Convolutional Neural Networks for MATLAB", "author": ["Vedaldi", "Lenc2015] Andrea Vedaldi", "Karel Lenc"], "venue": "Proceeding of the ACM Int. Conf. on Multimedia", "citeRegEx": "Vedaldi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vedaldi et al\\.", "year": 2015}, {"title": "Dissociated neural correlates of quantity processing of quantifiers, numbers, and numerosities", "author": ["Wei et al.2014] Wei Wei", "Chuansheng Chen", "Tao Yang", "Han Zhang", "Xinlin Zhou"], "venue": "Human brain mapping,", "citeRegEx": "Wei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2014}, {"title": "Children\u2019s acquisition of the number words and the counting system", "author": ["Karen Wynn"], "venue": "Cognitive psychology,", "citeRegEx": "Wynn.,? \\Q1992\\E", "shortCiteRegEx": "Wynn.", "year": 1992}, {"title": "Salient object subitizing", "author": ["Zhang et al.2015] Jianming Zhang", "Shugao Ma", "Mehrnoosh Sameki", "Stan Sclaroff", "Margrit Betke", "Zhe Lin", "Xiaohui Shen", "Brian Price", "Radomir Mech"], "venue": "In Proceedings of the IEEE Conference on Computer Vision", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Salient object subitizing", "author": ["Zhang et al.2016] Jianming Zhang", "Shuga Ma", "Mehrnoosh Sameki", "Stan Sclaroff", "Margrit Betke", "Zhe Lin", "Xiaohui Shen", "Brian Price", "Radom\u00edr M\u011bch"], "venue": "arXiv preprint arXiv:1607.07525", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "Although they share a number of syntactic, semantic and pragmatic properties (Hurewitz et al., 2006), and they are both learned in a fairly stable order of acquisition across languages (Wynn, 1992; Katsos et al.", "startOffset": 77, "endOffset": 100}, {"referenceID": 22, "context": ", 2006), and they are both learned in a fairly stable order of acquisition across languages (Wynn, 1992; Katsos et al., 2016), these quantity expressions underlie fairly different cognitive and neural mechanisms.", "startOffset": 92, "endOffset": 125}, {"referenceID": 7, "context": "First, they are handled differently by the language acquisition system, with children recognizing their disparate characteristics since early development, even before becoming \u2018full-counters\u2019 (Hurewitz et al., 2006; Sarnecka and Gelman, 2004; Barner et al., 2009).", "startOffset": 192, "endOffset": 263}, {"referenceID": 0, "context": "First, they are handled differently by the language acquisition system, with children recognizing their disparate characteristics since early development, even before becoming \u2018full-counters\u2019 (Hurewitz et al., 2006; Sarnecka and Gelman, 2004; Barner et al., 2009).", "startOffset": 192, "endOffset": 263}, {"referenceID": 21, "context": "Second, while the neural processing of cardinals relies on the brain region devoted to the representation of quantities, quantifiers rather elicit regions for general semantic processing (Wei et al., 2014).", "startOffset": 187, "endOffset": 205}, {"referenceID": 0, "context": "or proportions of sets (Barner et al., 2009).", "startOffset": 23, "endOffset": 44}, {"referenceID": 13, "context": "As a consequence, speakers can reliably answer questions involving quantifiers even in contexts that preclude counting (Pietroski et al., 2009), as well as children lacking exact cardinality concepts can understand and appropriately use quantifiers in grounded contexts (Halberda et al.", "startOffset": 119, "endOffset": 143}, {"referenceID": 6, "context": ", 2009), as well as children lacking exact cardinality concepts can understand and appropriately use quantifiers in grounded contexts (Halberda et al., 2008; Barner et al., 2009).", "startOffset": 134, "endOffset": 178}, {"referenceID": 0, "context": ", 2009), as well as children lacking exact cardinality concepts can understand and appropriately use quantifiers in grounded contexts (Halberda et al., 2008; Barner et al., 2009).", "startOffset": 134, "endOffset": 178}, {"referenceID": 2, "context": "Our approach is thus different from salient objects detection, where the distinction targets/distractors is missing (Borji et al., 2015; Zhang et al., 2015; Zhang et al., 2016).", "startOffset": 116, "endOffset": 176}, {"referenceID": 23, "context": "Our approach is thus different from salient objects detection, where the distinction targets/distractors is missing (Borji et al., 2015; Zhang et al., 2015; Zhang et al., 2016).", "startOffset": 116, "endOffset": 176}, {"referenceID": 24, "context": "Our approach is thus different from salient objects detection, where the distinction targets/distractors is missing (Borji et al., 2015; Zhang et al., 2015; Zhang et al., 2016).", "startOffset": 116, "endOffset": 176}, {"referenceID": 17, "context": "With respect to cardinals, our approach is similar to (Segu\u00ed et al., 2015), who propose a model for counting people in natural ar X iv :1 70 2.", "startOffset": 54, "endOffset": 74}, {"referenceID": 4, "context": "scenes, and to more recent work aimed at counting either everyday objects in natural images (Chattopadhyay et al., 2016) or geometrical objects with attributes in synthetic scenes (Johnson et al.", "startOffset": 92, "endOffset": 120}, {"referenceID": 8, "context": ", 2016) or geometrical objects with attributes in synthetic scenes (Johnson et al., 2016).", "startOffset": 67, "endOffset": 89}, {"referenceID": 19, "context": "With respect to quantifiers, our approach is similar to (Sorodoc et al., 2016), who use quantifiers no, some, and all to quantify over sets of colored dots.", "startOffset": 56, "endOffset": 78}, {"referenceID": 23, "context": "Typically, images depict one salient object and even when multiple salient objects are present, only a handful of cases contain both targets and distractors (Zhang et al., 2015; Zhang et al., 2016).", "startOffset": 157, "endOffset": 197}, {"referenceID": 24, "context": "Typically, images depict one salient object and even when multiple salient objects are present, only a handful of cases contain both targets and distractors (Zhang et al., 2015; Zhang et al., 2016).", "startOffset": 157, "endOffset": 197}, {"referenceID": 9, "context": "Although a large number of image datasets are currently available (see Lin et al. (2014) among many others), no one fully satisfies these requirements.", "startOffset": 71, "endOffset": 89}, {"referenceID": 0, "context": "Cs up to 4 are acquired by children incrementally at subsequent stages of their development, with higher numbers being learned upon this knowledge with the ability of counting (Barner et al., 2009).", "startOffset": 176, "endOffset": 197}, {"referenceID": 12, "context": "their exact number can be immediately and effortlessly grasped) due to which they are usually referred to as \u2018subitizing\u2019 range (Piazza et al., 2011; Railo et al., 2016).", "startOffset": 128, "endOffset": 169}, {"referenceID": 14, "context": "their exact number can be immediately and effortlessly grasped) due to which they are usually referred to as \u2018subitizing\u2019 range (Piazza et al., 2011; Railo et al., 2016).", "startOffset": 128, "endOffset": 169}, {"referenceID": 5, "context": "We use images from ImageNet (Deng et al., 2009).", "startOffset": 28, "endOffset": 47}, {"referenceID": 1, "context": "Starting from the full list of 203 concepts and corresponding images extracted by Cassani (2014), we discarded those concepts whose corresponding word had low/null frequency in the large corpus used in (Baroni et al., 2014).", "startOffset": 202, "endOffset": 223}, {"referenceID": 15, "context": "We used the VGG-19 model pretrained on the ImageNet ILSVRC data (Russakovsky et al., 2015) implemented in the MatConvNet toolbox (Vedaldi and Lenc, 2015).", "startOffset": 64, "endOffset": 90}, {"referenceID": 2, "context": "Starting from the full list of 203 concepts and corresponding images extracted by Cassani (2014), we discarded those concepts whose corresponding word had low/null frequency in the large corpus used in (Baroni et al.", "startOffset": 82, "endOffset": 97}, {"referenceID": 10, "context": "a 400-d embedding built with the CBOW architecture of word2vec (Mikolov et al., 2013) and the best-predictive parameters of Baroni et al.", "startOffset": 63, "endOffset": 85}, {"referenceID": 1, "context": ", 2013) and the best-predictive parameters of Baroni et al. (2014) on a 2.", "startOffset": 46, "endOffset": 67}, {"referenceID": 6, "context": "This mapping, we conjecture, would mimic the multimodal mechanism by which children acquire the meaning of both Cs and Qs (see Halberda et al. (2008)).", "startOffset": 127, "endOffset": 150}, {"referenceID": 11, "context": "This evidence, we believe, would further motivate our \u2018one quantifed expression, one function\u2019 approach, which is partially inspired by the evidence that, in human brain, so-called number neurons are tuned to preferred numbers (Nieder, 2016).", "startOffset": 227, "endOffset": 241}], "year": 2017, "abstractText": "People can refer to quantities in a visual scene by using either exact cardinals (e.g. one, two, three) or natural language quantifiers (e.g. few, most, all). In humans, these two processes underlie fairly different cognitive and neural mechanisms. Inspired by this evidence, the present study proposes two models for learning the objective meaning of cardinals and quantifiers from visual scenes containing multiple objects. We show that a model capitalizing on a \u2018fuzzy\u2019 measure of similarity is effective for learning quantifiers, whereas the learning of exact cardinals is better accomplished when information about number is provided.", "creator": "LaTeX with hyperref package"}}}