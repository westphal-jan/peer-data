{"id": "1702.06269", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "Memory and Communication Efficient Distributed Stochastic Optimization with Minibatch-Prox", "abstract": "We present and analyze statistically optimal, communication and memory efficient distributed stochastic optimization algorithms with near-linear speedups (up to $\\log$-factors). This improves over prior work which includes methods with near-linear speedups but polynomial communication requirements (accelerated minibatch SGD) and communication efficient methods which do not exhibit any runtime speedups over a naive single-machine approach. We first analyze a distributed SVRG variant as a distributed stochastic optimization method and show that it can achieve near-linear speedups with logarithmic rounds of communication, at the cost of high memory requirements. This feature also allows for low-cost, high-efficiency SVRG variants that are easy to learn, and can be used to reduce memory costs. The method is supported by a network of SVRG variants, with the SVRG variant able to learn the most optimal SVRG.\n\n\n\n\n\nThe LRT-LRT algorithm is presented in the Open Data Framework (Open Data Framework), Open-Source.\n\n\nThe LRT-LRT variant in which the LRT-LRT is implemented relies on the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT-LRT algorithms used in the LRT", "histories": [["v1", "Tue, 21 Feb 2017 05:19:23 GMT  (619kb)", "https://arxiv.org/abs/1702.06269v1", null], ["v2", "Fri, 9 Jun 2017 16:14:48 GMT  (735kb)", "http://arxiv.org/abs/1702.06269v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jialei wang", "weiran wang", "nathan srebro"], "accepted": false, "id": "1702.06269"}, "pdf": {"name": "1702.06269.pdf", "metadata": {"source": "CRF", "title": "Memory and Communication Efficient Distributed Stochastic Optimization with Minibatch-Prox", "authors": ["Jialei Wang", "Weiran Wang", "N. Srebro", "WANG WANG SREBRO"], "emails": ["JIALEI@UCHICAGO.EDU", "WEIRANWANG@TTIC.EDU", "NATI@TTIC.EDU"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n06 26\n9v 2\n[ cs\n.L G\n] 9\noptimal and achieves near-linear speedups (up to logarithmic factors). Our approach allows a communication-memory tradeoff, with either logarithmic communication but linear memory, or polynomial communication and a corresponding polynomial reduction in required memory. This communication-memory tradeoff is achieved throughminibatch-prox iterations (minibatch passiveaggressive updates), where a subproblem on a minibatch is solved at each iteration. We provide a novel analysis for such a minibatch-prox procedure which achieves the statistical optimal rate regardless of minibatch size and smoothness, thus significantly improving on prior work."}, {"heading": "1. Introduction", "text": "Consider the stochastic convex optimization (generalized learning) problem (Nemirovskii and Yudin, 1983; Vapnik, 1995; Shalev-Shwartz et al., 2009):\nmin w\u2208\u2126 \u03c6(w) := E\u03be\u223cD [\u2113(w, \u03be)] (1)\nwhere our goal is to learn a predictor w from the convex domain \u2126 given the convex instantaneous (loss) function \u2113(w, \u03be) and i.i.d. samples \u03be1, \u03be2, . . . from some unknown data distribution D. When optimizing on a single machine, stochastic approximation methods such as stochastic gradient descent (SGD) or more generally stochastic mirror descent, are ideally suited for the problem as they typically have optimal sample complexity requirements, and run in linear time in the number of samples, and thus also have optimal runtime. Focusing on an \u21132 bounded domain with B = sup\nw\u2208\u2126 \u2016w\u2016 and L-Lipschitz loss, the min-max optimal sample complexity is n(\u03b5) = O(L2B2/\u03b52), and this is achieved by SGD using O(n(\u01eb)) vector operations. Furthermore, if examples are obtained one at a time (in a streaming setting or through access to a \u201cbutton\u201d generating examples), we only need to store O(1) vectors in memory. The situation is more complex in the distributed setting where no single method is known that is optimal with respect to sample complexity, runtime, memory and communication. Specifically, consider m machines where each machine i = 1, ...,m receives samples \u03bei1, \u03bei2, ... drawn from the\n\u2217 Equal contributions.\nc\u00a9 2017 J. Wang, W. Wang & N. Srebro.\nsame distribution D. This can equivalently be thought of as randomly distributing samples across m servers. We also assume the objective is \u03b2-smooth, taking L, \u03b2 = O(1) in our presentation of results. The goal is to find a predictor w\u0302 \u2208 \u2126 satisfying E [\u03c6(w\u0302)\u2212minw\u2208\u2126 \u03c6(w)] \u2264 \u03b5 using the smallest possible number of samples per machine, the minimal elapsed runtime, and the smallest amount of communication, and also minimal memory on each machine (again, when examples are received or generated one at a time). Ideally, we could hope for a method with linear speedup, i.e. O(n(\u01eb)/m) runtime, using the statistically optimal number of samples O(n(\u01eb)) and constant or near-constant communication and memory. Throughout we measure runtime in terms of vector operations, memory in terms of number of vectors that need to be stored on each machine and communication in terms of number of vectors sent per machine1. These resource requirements are summarized in Table 1.\nOne simple approach for distributed stochastic optimization is minibatch SGD (Cotter et al., 2011; Dekel et al., 2012), where in each update we use a gradient estimate based on mb examples: b examples from each of the m machines. Distributed minibatch SGD attains optimal statistical performance with O (n(\u03b5)/m) runtime, as long as the minibatch size is not too large: Dekel et al. (2012) showed that the minibatch size can be as large as bm = O( \u221a n(\u03b5)), and Cotter et al. (2011) showed that with acceleration this can be increased to bm = O(n(\u03b5)3/4). Using this maximal minibatch size for accelerated minibatch SGD thus yields a statistically optimal method with linear speedup in runtime, O(1) memory usage, and O(n(\u03b5)1/4) rounds of communication\u2013see Table 1. This is the most communication-efficient method with true linear speedup we are aware of.\nAn alternative approach is to use distributed optimization to optimize the regularized empirical\nobjective:\nmin w\n\u03c6S(w) + \u03bd\n2 \u2016w\u20162 , (2)\nwhere \u03c6S is the empirical objective on n(\u01eb) i.i.d. samples, distributed across the machines and \u03bd = O(L/(B \u221a n(\u03b5))). A naive approach here is to use accelerate gradient descent, distributing the gradient computations, but this, as well as approaches based on ADMM (Boyd et al., 2011), are dominated by minibatch SGD (Shamir and Srebro 2014 and see also Table 1). Better alternatives take advantage of the stochastic nature of the problem: DANE (Shamir et al., 2014) requires only O(B2m) rounds of communication for squared loss problems, while DiSCO (Zhang and Lin, 2015) and AIDE (Reddi et al., 2016)) reduce this further to O(B1/2m1/4) rounds of communication. However, these communication-efficient methods usually require expensive computation on each local machine, solving an optimization problem on all local data at each iteration. Even if this can be done in near-linear time, it is still difficult to obtain computational speedup compared with single machine solution, and certainly not linear speedups\u2014see Table 1. Furthermore, since each round of these methods involves optimization over a fixed training set, this training set must be stored thus requiring n(\u03b5)/m memory per machine. Designing stochastic distributed optimization problems with linear, or near-linear, speedups, and low communication and memory requirements is thus still an open problem. We make progress in this paper analyzing and presenting methods with near-linear speedups and better communication and memory requirements. As with the analysis of DANE, DiSCO and AIDE, our analysis is rigorous only for least squared problems, and so all results should be taken in that context (the methods themselves are applicable to any distributed stochastic convex optimization problem).\n1. In all methods involved, communication is used to average vectors across machines and make the result known to\none or all machines. We are actually counting the number of such operations.\nC o m m u n ic at io n\nMemory\nAcc. Mini. SGD DSVRG MP-DSVRG"}, {"heading": "Our contributions", "text": "batch proximal updates may be of independent interest and useful in other contexts and as a basis for other methods.\nNotations We denote by w\u2217 = argminw\u2208\u2126 \u03c6(w) the optimal solution to (1). Throughout the paper, we assume the instantaneous function \u2113(w, \u03be) is L-Lipschitz and \u03bb-strongly convex inw for some \u03bb \u2265 0 on the domain \u2126:\n\u2223\u2223\u2113(w, \u03be)\u2212 \u2113(w\u2032, \u03be) \u2223\u2223 \u2264 L \u2225\u2225w \u2212w\u2032 \u2225\u2225 ,\n\u2113(w, \u03be)\u2212 \u2113(w\u2032, \u03be) \u2265 \u2329 \u2207\u2113(w\u2032, \u03be), w \u2212w\u2032 \u232a + \u03bb\n2\n\u2225\u2225w \u2212w\u2032 \u2225\u22252 , \u2200w,w\u2032 \u2208 \u2126.\nSometimes we also assume \u2113(w, \u03be) is \u03b2-smooth inw:\n\u2113(w, \u03be)\u2212 \u2113(w\u2032, \u03be) \u2264 \u2329 \u2207\u2113(w\u2032, \u03be), w \u2212w\u2032 \u232a + \u03b2\n2\n\u2225\u2225w \u2212w\u2032 \u2225\u22252 , \u2200w,w\u2032 \u2208 \u2126.\nFor distributed stochastic optimization, our analysis focuses on the least squares loss \u2113(w, \u03be) = 1 2(w \u22a4x\u2212 y)2 where \u03be = (x, y)."}, {"heading": "2. Distributed SVRG for stochastic convex optimization", "text": "Recently, Lee et al. (2015) suggested using fast randomized optimization algorithms for finite-sums, and in particular the SVRG algorithm, as a distributed optimization approach for (2). The authors noted that, for SVRG, when the the sample size n(\u03b5) dominates the problem\u2019s condition number \u03b2/\u03bd where \u03b2 is the smoothness parameter of \u2113(w, \u03be), the time complexity is dominated by computing the batch gradients. This operation can be trivially parallelized. The stochastic updates, on the other hand, can be implemented on a single machine while the other machines wait, with the only caveat being that only sampling-without-replacement can be implemented this way. The use of without-replacement sampling was theoretically justified in a recent analysis by Shamir (2016).\nIn the distributed stochastic convex optimization setting considered here, DSVRG in fact achieves\nlinear speedup in certain regime as follows. In each iteration of the algorithm, each machine first computes its local gradient and average them with one communication round to obtain the global batch gradient, and then a single machine performs the SVRG stochastic updates by processing its local data once (sampling the n(\u03b5)/m examples without replacement). By the linear convergence of SVRG, as long as the number of stochastic updates n(\u03b5)/m is larger than \u03b2/\u03bd = O(\u03b2B \u221a n(\u03b5)/L), the algorithm converges to O(\u01eb)-suboptimality (in both the empirical and stochastic objective) in O(log 1/\u03b5) = O (log n(\u03b5)) iterations; and this condition is satisfied2 for n(\u03b5) & m2.\nClearly, in the above regime, each iteration of DSVRG uses two rounds of communications and the total communication complexity is O (n(\u03b5)). On the other hand, the computation for each machine is compute the local gradient (in time O(n(\u03b5)/m)) in each iteration, resulting in a total time complexity of O(n(\u03b5) log n(\u03b5)/m). This explains the DSVRG entry in Table 1.\nBeing communication- and computation-efficient, DSVRG requires each machine to store a portion of the sample set for ERM to make multiple passes over them, and is therefore not memoryefficient. In fact, this disadvantage is shared by previously known communication-efficient distributed optimization algorithms, including DANE, DiSCO, and AIDE. In order to develop amemoryand communication-efficient algorithm for distributed stochastic optimization, we need to bypass the ERM setting and this is enabled by the following minibatch-prox algorithm."}, {"heading": "3. The minibatch-prox algorithm for stochastic optimization", "text": "In this section, we describe and analyze the minibatch-prox algorithm for stochastic optimization, which allows us to use arbitrarily large minibatch size without slowing down the convergence rate. We first present the basic version where each proximal objective is solved exactly for each minibatch, which achieves the optimal convergence rate. Then, we show that if each minibatch objective is solved accurately enough, the algorithm still converges at the optimal rate, opening the opportunity for efficient implementations."}, {"heading": "3.1. Exact minibatch-prox", "text": "The \u201cexact\u201d minibatch-prox is defined by the following iterates: for t = 1, . . . ,\nwt = argmin w\u2208\u2126 ft(w),\nwhere ft(w) := \u03c6It(w) + \u03b3t 2 \u2016w \u2212wt\u22121\u20162 = 1 b\n\u2211 \u03be\u2208It \u2113(w, \u03be) + \u03b3t 2 \u2016w \u2212wt\u22121\u20162 , (3)\n\u03b3t > 0 is the (inverse) stepsize parameter at time t, and It is a set of a b samples from the unknown distribution D. To understand the updates in (3), we first observe by the first order optimality condition for ft(w) that\n\u2207\u03c6It(wt) + \u03b3t(wt \u2212wt\u22121) \u2208 \u2212N\u2126(wt), (4)\n2. If n(\u03b5) & m2 does not hold, we can use a \u201chot-potato\u201d style algorithm where we process all data once on machine i and pass the predictor to machine i+1 until we obtain sufficiently many stochastic updates. But then the computation efficiency deteriorates and we no longer have linear speedup in runtime.\nwhere\u2207\u03c6It(wt) is some subgradient of \u03c6It(w) atwt, andN\u2126(wt) = {y| \u3008w \u2212wt, y\u3009 \u2264 0, \u2200w \u2208 \u2126} is the normal cone of \u2126 at wt. Equivalently, the above condition implies\nwt = P\u2126 ( wt\u22121 \u2212 1\n\u03b3t \u2207\u03c6It(wt)\n) , (5)\nwhere P\u2126(w) denotes the projection ofw onto \u2126. The update rule (5) resembles that of the standard minibatch gradient descent, except the gradient is evaluated at the \u201cfuture\u201d iterate.\nProximal steps, of the form (3) or equivalently (5), are trickier to implement compared to (stochastic) gradient steps, as they involve optimization of a subproblem, instead of merely computing and adding gradients. Nevertheless, they have been suggested, used and studied in several contexts. Crammer et al. (2006) proposed the \u201cpassive aggressive\u201d update rule, where a margin-based loss from a single example with a quadratic penalty is minimized\u2014this corresponds to (3) with a \u201cbatch size\u201d of one. More general loss functions, still for \u201cbatch sizes\u201d of one, were also analyzed in the online learning setting (Cheng et al., 2006; Kulis and Bartlett, 2010). For finite-sum objectives, methods based on incremental/stochastic proximal updates were studied by Bertsekas (2011, 2015); Defazio (2016). Needell and Tropp (2014) analyzed a randomized block Kaczmarz method in the context of solving linear systems, which also minimizes the empirical loss on a randomly sampled minibatch. To the best of our knowledge, no prior work has analyzed the general minibatch variant of proximal updates for stochastic optimization except Li et al. (2014). However, the analysis of Li et al. (2014) assumes a stringent condition which is hard to verify (and is often violated) in practice, which we will discuss in detail in this section.\nThe following lemma provides the basic property of the update at each iteration.\nLemma 1 For any w \u2208 \u2126, we have\n\u03bb+ \u03b3t \u03b3t \u2016wt \u2212w\u20162 \u2264 \u2016wt\u22121 \u2212w\u20162 \u2212 \u2016wt\u22121 \u2212wt\u20162 \u2212 2 \u03b3t (\u03c6It(wt)\u2212 \u03c6It(w)) . (6)\nTo derive the convergence guarantee, we need to relate \u03c6It(wt) to \u03c6(w). The analysis of Li et al. (2014) for minibatch-prox made the assumption that for all t \u2265 1:\nEIt [D\u03c6(wt;wt\u22121)] \u2264 EIt [ D\u03c6It (wt;wt\u22121) ] + \u03b3t 2 \u2016wt \u2212wt\u22121\u20162 , (7)\nwhereDf (w,w \u2032) = f(w)\u2212 f(w\u2032)\u2212\u3008\u2207f(w\u2032), w \u2212w\u2032\u3009 denotes the Bregman divergence defined by the potential function f . This condition is hard to verify, and may constrain the stepsize to be very small. For example, as the authors argued, if \u2113(w, \u03be) is \u03b2-smooth with respect to w, we have\nD\u03c6(wt;wt\u22121) \u2264 \u03b2\n2 \u2016wt \u2212wt\u22121\u20162 ,\nand combined with the fact that D\u03c6It (wt;wt\u22121) \u2265 0, one can guarantee (7) by setting \u03b3t \u2265 \u03b2. However, to obtain the optimal convergence rate, Li et al. (2014) needed to set \u03b3t = O( \u221a T/b) which would imply b = O(T ) in order to have \u03b3t \u2265 \u03b2. In view of this implicit constraint that the minibatch size b can not be too large, the analysis of Li et al. (2014) does not really show advantage of minibatch-prox over minibatch SGD, whose optimal minibatch size is precisely b = O(T ).\nOur analysis is free of any additional assumptions. The key observation is that, when b is large, we expect \u03c6It(w) to be close to \u03c6(w). Define the stochastic objective\nFt(w) := EIt [ft(w)] = \u03c6(w) + \u03b3t 2 \u2016w \u2212wt\u22121\u20162 . (8)\nThen wt is the \u201cempirical risk minimizer\u201d of Ft(w) as it solves the empirical version ft(w) with b samples. Using a stability argument (Shalev-Shwartz et al., 2009), we can establish the \u201cgeneralization\u201d performance for the (inexact) minimizer of the minibatch objective.\nLemma 2 For the minibatch-prox algorithm,we have\n|EIt [\u03c6(wt)\u2212 \u03c6It(wt)]| \u2264 4L2\n(\u03bb+ \u03b3t)b .\nMoreover, if a possibly randomized algorithm Aminimizes ft(w) up to an error of \u03b7t, i.e.,A returns an approximate solution w\u0303t such that EA [ft(w\u0303t)\u2212 ft(wt)] \u2264 \u03b7t, we have\n|EIt,A [\u03c6(w\u0303t)\u2212 \u03c6It(wt)]| \u2264 4L2\n(\u03bb+ \u03b3t)b + \u221a 2L2\u03b7t \u03bb+ \u03b3t .\nCombining Lemma 1 and Lemma 2, we obtain the following key lemma regarding the progress on the stochastic objective at each iteration of minibatch-prox.\nLemma 3 For iteration t of exact minibatch-prox, we have for any w \u2208 \u2126 that\n\u03bb+ \u03b3t \u03b3t EIt \u2016wt \u2212w\u20162 \u2264 \u2016wt\u22121 \u2212w\u20162 \u2212 2 \u03b3t EIt [\u03c6(wt)\u2212 \u03c6(w)] +\n8L2\n\u03b3t(\u03bb+ \u03b3t)b . (9)\nWe are now ready to bound the overall convergence rates of minibatch-prox.\nTheorem 4 (Convergence of exact minibatch-prox \u2014 weakly convex \u2113(w, \u03be)) For L-Lipschitz instantaneous function \u2113(w, \u03be), set \u03b3 = \u221a\n8T b \u00b7 L\u2016w0\u2212w\u2217\u2016 for t = 1, . . . , T in minibatch-prox. Then\nfor w\u0302T = 1 T \u2211T t=1 wt, we have\nE [\u03c6(w\u0302T )\u2212 \u03c6(w\u2217)] \u2264 \u221a 8L\u221a bT \u2016w0 \u2212w\u2217\u2016 .\nTheorem 5 (Convergence of exact minibatch-prox \u2014 strongly convex \u2113(w, \u03be)) ForL-Lipschitz and \u03bb-strongly convex instantaneous function \u2113(w, \u03be), set \u03b3t = \u03bb(t\u22121)\n2 for t = 1, . . . , T in minibatch-\nprox. Then for w\u0302T = 2\nT (T+1) \u2211T t=1 twt, we have\nE [\u03c6(w\u0302T )\u2212 \u03c6(w\u2217)] \u2264 16L2\n\u03bbb(T + 1) ."}, {"heading": "3.2. Inexact minibatch-prox", "text": "We now study the case where instead of solving the subproblems ft(w) exactly, we only solve it approximately to sufficient accuracy. The \u201cinexact\u201d minibatch-prox uses a possibly randomized algorithm A for approximately solving one subproblem on a minibatch in each iteration, and generates the following iterates: for t = 1, . . . ,\nw\u0303t \u2248 w\u0304t := argmin w\u2208\u2126 f\u0303t(w) where f\u0303t(w) := \u03c6It(w) + \u03b3t 2 \u2016w \u2212 w\u0303t\u22121\u20162 , (10)\nand EA [ f\u0303t(w\u0303t)\u2212 f\u0303t(w\u0304t) ] \u2264 \u03b7t.\nAnalogous to Lemma 3, we can derive the following lemma using stability of inexact minimizers.\nLemma 6 Fix any w \u2208 \u2126. For iteration t of inexact minibatch-prox, we have\nEIt,A [\u03c6(w\u0303t)\u2212 \u03c6(w)] \u2264 \u03b3t 2 EIt,A \u2016w\u0303t\u22121 \u2212w\u20162 \u2212 \u03bb+ \u03b3t 2 EIt,A \u2016w\u0303t \u2212w\u20162 + 4L2\n(\u03bb+ \u03b3t)b\n+ \u221a 2L2\u03b7t \u03bb+ \u03b3t + \u221a 2(\u03bb+ \u03b3t)\u03b7t \u00b7 \u221a EIt,A \u2016w\u0303t \u2212w\u20162. (11)\nNote that when \u03b7t = 0, the above guarantee reduces to that of exact minibatch-prox. We now show that when the minibatch subproblems are solved sufficiently accurately, we still\nobtain the O(1/ \u221a bT ) rate for weakly-convex loss and O(1/(\u03bbbT )) rate for strongly-convex loss.\nTheorem 7 (Convergence of inexact minibatch-prox \u2014 weakly convex \u2113(w, \u03be)) ForL-Lipschitz instantaneous function \u2113(w, \u03be), set \u03b3t = \u03b3 = \u221a\n8T b \u00b7 L\u2016w0\u2212w\u2217\u2016 for all t \u2265 1 in inexact minibatch-\nprox. Assume that for all t \u2265 1, the error in minimizing f\u0303t(w) satisfies for some \u03b4 > 0 that\nEA [ f\u0303t(w\u0303t)\u2212min\nw\nf\u0303t(w) ] \u2264 min ( c1 ( T\nb\n) 1 2\n, c2\n( T\nb\n) 3 2 ) \u00b7 L \u2016w\u03030 \u2212w\u2217\u2016\nt2+2\u03b4 .\nThen for w\u0302T = 1 T \u2211T t=1 w\u0303t, we have E [\u03c6(w\u0302T )\u2212 \u03c6(w\u2217)] \u2264 c3L\u2016w0\u2212w\u2217\u2016\u221a bT , where c3 only depends on c1, c2 and \u03b4. For example, by setting c1 = 10 \u22124, c2 = 10\u22124, \u03b4 = 1/2, we have\nE [\u03c6(w\u0302T )\u2212 \u03c6(w\u2217)] \u2264 \u221a 10L \u2016w0 \u2212w\u2217\u2016\u221a\nbT .\nTheorem 8 (Convergence of inexact minibatch-prox \u2014 strongly convex \u2113(w, \u03be)) ForL-Lipschitz and \u03bb-strongly convex instantaneous function \u2113(w, \u03be), set \u03b3t = \u03bb(t\u22121)\n2 for t = 1, . . . in inexact\nminibatch-prox. Assume that for all t \u2265 1, the error in minimizing f\u0303t(w) satisfies for some \u03b4 > 0 that\nEA [ f\u0303t(w\u0303t)\u2212min\nw\nf\u0303t(w) ] \u2264 min ( c1 ( T\nb\n) , c2 ( T\nb\n)2) \u00b7 L 2\nt3+2\u03b4\u03bb .\nThen for w\u0302T = 2\nT (T+1) \u2211T t=1 tw\u0303t, we have E [\u03c6(w\u0302T )\u2212 \u03c6(w\u2217)] \u2264 c3L 2\n\u03bbbT , where c3 only depends on c1, c2 and \u03b4.\nRemark 9 The final inequalities in Theorem 4 and 7 actually apply more generally to all predictors in the domain. That is, our proofs still hold withw\u2217 replaced by any w \u2208 \u2126:\nE [\u03c6(w\u0302T )\u2212 \u03c6(w)] \u2264 O ( L \u2016w0 \u2212w\u2016\u221a\nbT\n) , w \u2208 \u2126.\nThis allows us to compete with any predictor in the domain (other than the minimizer). For example, in order to compete on \u03c6(w) with the set of predictors with small norm {w : \u2016w\u2016 \u2264 B}, we can set the domain \u2126 = Rd and initialize with w0 = 0. In view of the above inequality, we still obtain the\noptimal rateO (\nLB\u221a bT\n) from minibatch-prox by solving simpler, unconstrained subproblems (though\nwe might have \u2016w\u0302T \u2016 > B)."}, {"heading": "4. Communication-efficient distributed minibatch-prox with SVRG", "text": "We now apply the theoretical results of minibatch-prox to the distributed stochastic learning setting, and propose a novel algorithm that is both communication and computation efficient, and being able to explore trade-offs between memory and communication efficiency.\nSuppose we have m machines in a distributed environment. For each outer loop of our algorithm, each machine i draws a minibatch I (i) t of b samples independently from other machines, and denote It = \u222ami=1I (i) t which contains bm samples. To apply the minibatch-prox algorithm from the previous section, we need to find an approximate solution to the following problem:\nmin w\nf\u0303t(w) := \u03c6It(w) + \u03b3\n2 \u2016w \u2212wt\u22121\u20162 . (12)\nSince the objective (12) involves functions from different machines, we use distributed optimization algorithms for solving it. In Li et al. (2014), the authors proposed a simple algorithm EMSO to approximately solve (12), where each machine first solve its own local objective, i.e.,\nw (i) t = argmin\nw\n\u03c6 I (i) t\n+ \u03b3\n2 \u2016w \u2212wt\u22121\u20162 , (13)\nand then all machines average their local solutions via one round of communication: wt = 1 m \u2211m i=1w (i) t .\nWe note that this can be considered as the \u201cone-shot-averaging\u201d approach (Zhang et al., 2012) for solving (12). Although this approach was shown to work well empirically, no convergence guarantee for the original stochastic objective (1) was provided by Li et al. (2014). Here we instead use the distributed SVRG (DSVRG) algorithm (Lee et al., 2015; Shamir, 2016) to approximately solve (12), as DSVRG enjoys excellent communication and computation cost when the problem is well conditioned (cf. Table 1).3\nWe detail our algorithm, named MP-DSVRG (minibatch-prox with DSVRG), in Algorithm 1. The algorithm consists of two nested loops, where t, k are iteration counters for minibatch-prox (the outer for-loop), and DSVRG (the inner for-loop) respectively. In each outer loop, each machine draws a minibatch I (i) t to form the objective (12), which will be solved approximately by the inner loops. Moreover, each machine splits its local dataset into pi batches: I (i) = \u222apij=1B (i) j . In\n3. It is also possible to equip minibatch-prox with other communication-efficient distributed optimization algorithms,\nfor example in Appendix D, we present a minibatch-prox DANE (MP-DANE) algorithm which uses the accelerated DANE method for solving (12).\nAlgorithm 1Minibatch-prox with DSVRG for distributed stochastic convex optimization.\nInitialize w0 = 0. for t = 1, 2, . . . , T do\n% Outer loop performs minibatch-prox. Each machine i draws a minibatch I (i) t of b samples from the underlying data distribution, and split I (i) t to pi batches of size b/pi: B (i) 1 , B (i) 2 , ..., B (i) pi Initialize z0 \u2190 wt\u22121, x0 \u2190 wt\u22121, j \u2190 1, s \u2190 1 for k = 1, 2, . . . ,K do\n1. All machines perform one round of communication to compute the average gradient:\n\u2207\u03c6It(zk\u22121) \u2190 1\nm\nm\u2211\ni=1\n\u2207\u03c6 I (i) t (zk\u22121)\n2. Machine j performs stochastic updates by going through B (j) s once without replacement:\nxr \u2190 xr\u22121 \u2212 \u03b7 (\u2207\u2113(xr\u22121, \u03bel)\u2212\u2207\u2113(zk\u22121, \u03bel) +\u2207\u03c6It(zk\u22121) + \u03b3(xr\u22121 \u2212wt\u22121))\nfor \u03bel \u2208 B(j)s . 3. Machine j update zk:\nzk \u2190 1\n|B(j)s |\n|B(j)s |\u2211\nr=0\nxr,\nand broadcast zk to other machines. 4. Update indices: s \u2190 s+ 1, if s > pj then\ns \u2190 1, j \u2190 j + 1. end if\nend for\nUpdate wt \u2190 zK . end for\nOutput: wT is the approximate solution.\neach inner loop, all machines communicate to calculate the global gradient (averaged local gradients) of (12), and then one of the machines j picks a local batch B (j) s to perform the stochastic updates, where the local batch contains enough samples such that one pass of stochastic updates on B (j) s decrease the objective quickly. We perform two rounds of communication in each inner loop, one for computing the global gradient, and one for broadcasting the new predictor obtained by a machine j. As we will show in the next section, by carefully choosing the parameters, we will obtain a convergent algorithm for distributed stochastic convex optimization with better efficiency guarantees than previous methods.\nWe now present detailed analysis for the computation/communication complexity of Algorithm 1 for stochastic quadratic problems, and compare it with related methods in the literature. Throughout this section, we have \u2113(w, \u03be) = 12(w \u22a4x \u2212 y)2 where \u03be = (x, y). We assume that\n\u2113(w, \u03be) is \u03b2-smooth and L-Lipschitz in w,4 and we would like to learn a predictor that is competitive to all predictors with norm at most B. Note that each \u2113(w, \u03be) is only weakly convex."}, {"heading": "4.1. Efficiency of MP-DSVRG", "text": "For the distributed stochastic convex optimization problems, we are concerned with efficiency in terms of sample, communication, computation and memory. Recall that for convex L-Lipshitz, Bbounded problems, to learn a predictor w\u0302 with \u03b5-generalization error, i.e., E [\u03c6(w\u0302)\u2212 \u03c6(w\u2217)] \u2264 \u03b5, we require the sample size to be at least n(\u03b5) = O(L2B2/\u03b52). This sample complexity matches the worst case lower bound, and can be achieved by vanilla SGD.\nThe theorem below shows that with careful choices of parameters in the outer and inner loops, MP-DSVRG achieves both communication and computation efficiency with the optimal sample complexity.\nTheorem 10 (Efficiency of MP-DSVRG) Set the parameters in Algorithm 1 as follows:\n(outer loop) T = n(\u03b5)\nbm , \u03b3 =\n\u221a 8n(\u03b5)L\nbmB , pi = O\n(\u221a n(\u03b5)L\n\u03b2mB\n)\n(inner loop) K = O (log n(\u03b5)) .\nThen we have E [ \u03c6 (\n1 T \u2211T t=1 wt ) \u2212 \u03c6(w\u2217) ] \u2264 \u221a 40BL\u221a n(\u03b5) = O (\u03b5) .\nMoreover, Algorithm 1 can be implemented with O ( n(\u03b5) bm log n(\u03b5) ) rounds of communication,\nand each machine performs O ( n(\u03b5) m log n(\u03b5) ) vector operations in total.\nWe comment on the choice of parameters. For sample efficiency, we fix the sample size n(\u03b5) and number of machines m, and so we can tradeoff the local minibatch size b and the total number of outer iterations T , maintaining bT = n(\u03b5)m . For any b, the regularization parameters in the \u201clarge\nminibatch\u201d problem is set to \u03b3 = \u221a\n8T bm \u00b7 LB =\n\u221a 8n(\u03b5)L\nbmB according to Theorem 7. Moreover,\nwe choose the number of batches pi in each local machine in a way that performing one pass of stochastic updates over a single batch by without-replacement sampling is sufficient to reduce the objective by a constant factor."}, {"heading": "5. Discussion and conclusion", "text": "In this paper, we made progress toward linear speedup, communication and memory efficient methods for distributed stochastic optimization, although we still do not have an algorithm that obtains the \u201cideal\u201d distributed stochastic optimization performance of linear speedup with constant or nearconstant communication and memory. There is also no single known algorithm that dominates all others, with different methods being preferable in terms of different resources. These tradeoffs, up to log-factors, are given in Table 1 and the memory, communication and runtime requirements are also schematically depicted in Figure 2. In the figure, the horizontal axis corresponds to the \u201cminibatch\u201d size, which can be controlled with accelerated minibatch SGD and MP-DSVRG, while other methods are batch methods which consider the entire data set.\n4. We can equivalently assume \u2016x\u20162 \u2264 \u03b2 and y is bounded.\nFrom Figure 2 we can see that DSVRG (equivalent to MP-DSVRG when b = n(\u03b5)/m) dominates the other methods (up to log-factors) in terms of runtime and communication\u2014it has smaller communication requirements than DiSCO/AIDE (and better than DANE) with nearly the same optimal runtime of accelerated minibatch SGD. But like other batch methods, it requires storing and re-accessing the entire data set. Accelerated minibatch SGD is the only one of these methods requiring only O(1) memory per machine, and it achieves true linear speedup, but due to the limit on the maximal allowed minibatch size, has relatively high communication cost. MPDSVRG allows bridging these two extremes of memory and communication, trading off between memory usage and communication. The trade-off is almost an extrapolation, except that in the lowmemory high-communication extreme, MP-DSVRG still requires (small) polynomial memory, not minibatch-SGD\u2019s O(1) memory, and its runtime still involve a logarithmic factor while minibatchSGD achieves true linear speedup.\nInstead of using DSVRG to solve each proximal subproblem in a minibatch-prox iteration, we can also use any other distributed optimization approach. For example, we can consider using DiSCO or DANE. This is depicted as \u201cMP-DANE\u201d in Figure 2. Again, an external minibatch-prox loop allows trading off memory for communication. For small minibatch sizes, up to a critical value of bmp-dane = \u0398(n(\u03b5)/(m 2B2)), MP-DANE enjoys the same guarantees as MP-DSVRG. But for larger minibatch sizes, such an approach starts suffering from DANE/DiSCO\u2019s inferior runtime and communication requirements compared to DSVRG.\nWe emphasize that the above discussion is based on guarantees established only for least square problems and ignores log-factors. We are unfortunately not aware of distributed stochastic optimization guarantees that improve over minibatch SGD (i.e., achieve even near-linear speedup with lower communication requirements) for general smooth objectives, or achieve true linear speedup (and improved communication guarantees) even for least-square problems."}, {"heading": "Acknowledgement", "text": "Research was partially supported by an Intel ICRI-CI award and NSF awards IIS 1302662 and BIGDATA 1546500. We would like to thank Ohad Shamir for discussions about Distributed SVRG and Tong Zhang for discussions about minibatch-prox."}, {"heading": "Appendix A. Analysis of exact minibatch-prox", "text": ""}, {"heading": "A.1. Proof of Lemma 1", "text": "Proof Observe that (4) implies \u03b3t(wt\u22121 \u2212wt) is a subgradient at wt of the sum of \u03c6It(w) and the indicator function of \u2126 (which has value 0 in \u2126 and\u221e otherwise), and thus we have for anyw \u2208 \u2126 that\n\u03c6It(w)\u2212 \u03c6It(wt) \u2265 \u03b3t \u3008wt\u22121 \u2212wt, w \u2212wt\u3009+ \u03bb\n2 \u2016w \u2212wt\u20162 . (14)\nFor anyw \u2208 \u2126, we can bound its distance to wt\u22121 as\n\u2016wt\u22121 \u2212w\u20162 = \u2016wt\u22121 \u2212wt +wt \u2212w\u20162\n= \u2016wt\u22121 \u2212wt\u20162 + 2 \u3008wt\u22121 \u2212wt, wt \u2212w\u3009+ \u2016wt \u2212w\u20162 \u2265 \u2016wt\u22121 \u2212wt\u20162 + 2\n\u03b3t (\u03c6It(wt)\u2212 \u03c6It(w)) +\n\u03bb \u03b3t \u2016w \u2212wt\u20162 + \u2016wt \u2212w\u20162\n= \u03bb+ \u03b3t \u03b3t \u2016wt \u2212w\u20162 + 2 \u03b3t (\u03c6It(wt)\u2212 \u03c6It(w)) + \u2016wt\u22121 \u2212wt\u20162\nwhere we have used (14) in the first inequality. Rearranging the terms yields the desired result."}, {"heading": "A.2. Proof of Lemma 2", "text": "The following lemma, which is essentially shown by Shalev-Shwartz et al. (2009, Theorem 6), characterizes the convergence of the empirical loss to the population counterpart for the (approximate) regularized empirical risk minimizer.\nLemma 11 Let the instantaneous function \u2113(w, \u03be) be L-Lipschitz and \u03bb-strongly convex in w. Consider the following regularized ERM problem with sample set Z = {\u03be1, . . . , \u03ben}:\nw\u0302 = argmin w\u2208\u2126\nF\u0302 (w) where F\u0302 (w) := 1\nn\nn\u2211\ni=1\n\u2113(w, \u03bei) + r(w),\nand the regularizer r(w) is \u03b3-strongly convex. Denote by G(w) = E\u03be [\u2113(w, \u03be)] and G\u0302(w) = 1 n \u2211n i=1 \u2113(w, \u03bei) the expected and the empirical losses respectively.\n1. For the regularized empirical risk minimizer w\u0302, we have\n\u2223\u2223\u2223EZ [ G(w\u0302)\u2212 G\u0302(w\u0302) ]\u2223\u2223\u2223 \u2264 4L 2\n(\u03bb+ \u03b3)n .\n2. If for any given dataset Z , a possibly randomized algorithmAminimizes F\u0302 (w) up to an error of \u03b7, i.e., A returns an approximate solution w\u0303 such that EA [ F\u0302 (w\u0303)\u2212 F\u0302 (w\u0302) ] \u2264 \u03b7, we have\n\u2223\u2223\u2223EZ,A [ G(w\u0303)\u2212 G\u0302(w\u0302) ]\u2223\u2223\u2223 \u2264 4L 2\n(\u03bb+ \u03b3)n +\n\u221a 2L2\u03b7\n\u03bb+ \u03b3 .\nProof We prove the lemma by a stability argument.\nExact ERM Denote by Z(i) the sample set that is identical to Z except that the i-th sample \u03bei is replaced by another random sample \u03be\u2032i, by F\u0302 (i)(w) the empirical objective defined using Z(i), i.e.,\nF\u0302 (i)(w) := 1\nn\n  \u2211\nj 6=i \u2113(w, \u03bei) + \u2113(w, \u03be\n\u2032 i)\n + r(w),\nand by w\u0302(i) = argmin w\u2208\u2126 F\u0302 (i)(w) the empirical risk minimizer of F\u0302 (i)(w). By the definition of the empirical objectives, we have\nF\u0302 (w\u0302(i))\u2212 F\u0302 (w\u0302) = \u2113(w\u0302 (i), \u03bei)\u2212 \u2113(w\u0302, \u03bei)\nn +\n\u2211 j 6=i \u2113(w\u0302\n(i), \u03bei)\u2212 \u2113(w\u0302, \u03bei) n + r(w\u0302(i))\u2212 r(w\u0302)\n= \u2113(w\u0302(i), \u03bei)\u2212 \u2113(w\u0302, \u03bei)\nn + \u2113(w\u0302, \u03be\u2032i)\u2212 \u2113(w\u0302(i), \u03be\u2032i) n + ( F\u0302 (i)(w\u0302(i))\u2212 F\u0302 (i)(w\u0302) )\n\u2264 \u2223\u2223\u2113(w\u0302(i), \u03bei)\u2212 \u2113(w\u0302, \u03bei) \u2223\u2223 n + \u2223\u2223\u2113(w\u0302, \u03be\u2032i)\u2212 \u2113(w\u0302(i), \u03be\u2032i) \u2223\u2223 n \u2264 2L n \u2225\u2225\u2225w\u0302(i) \u2212 w\u0302 \u2225\u2225\u2225 (15)\nwhere we have used the fact that w\u0302(i) is the minimizer of F\u0302 (i)(w) in the first inequality, and the L-Lipschitz continuity of \u2113(w, \u03be) in the second inequality.\nOn the other hand, it follows from the (\u03bb+ \u03b3)-strong convexity of F\u0302 (w) that\nF\u0302 (w\u0302(i))\u2212 F\u0302 (w\u0302) \u2265 (\u03bb+ \u03b3) 2\n\u2225\u2225\u2225w\u0302(i) \u2212 w\u0302 \u2225\u2225\u2225 2 . (16)\nCombining (15) and (16) yields \u2225\u2225w\u0302(i) \u2212 w\u0302 \u2225\u2225 \u2264 4L(\u03bb+\u03b3)n . Again, by the L-Lipschitz continuity of \u2113(w, \u03be), we have that for any sample \u03be that\n\u2223\u2223\u2223\u2113(w\u0302, \u03be)\u2212 \u2113(w\u0302(i), \u03be) \u2223\u2223\u2223 \u2264 L \u2225\u2225\u2225w\u0302(i) \u2212 w\u0302 \u2225\u2225\u2225 \u2264 4L 2\n(\u03bb+ \u03b3)n . (17)\nSince Z and Z(i) are both i.i.d. sample sets, we have\nEZ [G(w\u0302)] = EZ(i) [ G(w\u0302(i)) ] = EZ(i)\u222a{\u03bei} [ \u2113(w\u0302(i), \u03bei) ] .\nAs this holds for all i = 1, . . . , n, we can also write\nEZ [G(w\u0302)] = 1\nn\nn\u2211\ni=1\nEZ(i)\u222a{\u03bei}\n[ \u2113(w\u0302(i), \u03bei) ] . (18)\nOn the other hand, we have\nEZ\n[ G\u0302(w\u0302) ] = EZ\n[ 1\nn\nn\u2211\ni=1\n\u2113(w\u0302, \u03bei)\n] = 1\nn\nn\u2211\ni=1\nEZ [\u2113(w\u0302, \u03bei)] . (19)\nCombining (18) and (19) and using the stability (17), we obtain\nEZ\n[ G(w\u0302)\u2212 G\u0302(w\u0302) ] = 1\nn\nn\u2211\ni=1\nEZ\u222a{\u03be\u2032i} [ \u2113(w\u0302(i), \u03bei)\u2212 \u2113(w\u0302, \u03bei) ] \u2208 [ \u2212 4L 2 (\u03bb+ \u03b3)n ,\n4L2\n(\u03bb+ \u03b3)n\n] .\nInexact ERM For the approximate solution w\u0303, due to the (\u03bb+ \u03b3)-strong convexity of F\u0302 (w), we have\nEA \u2016w\u0303 \u2212 w\u0302\u20162 \u2264 2\n\u03bb+ \u03b3 EA\n[ F\u0302 (w\u0303)\u2212 F\u0302 (w\u0302) ] \u2264 2\u03b7\n\u03bb+ \u03b3 ,\nand thus EA \u2016w\u0303 \u2212 w\u0302\u2016 \u2264 \u221a 2\u03b7 \u03bb+\u03b3 by the fact that Ex\n2 \u2265 (Ex)2 for any random variable x. It then follows from the Lipschitz continuity of G(w) that\nEA |G(w\u0303)\u2212G(w\u0302)| \u2264 L \u00b7 EA \u2016w\u0303 \u2212 w\u0302\u2016 \u2264 \u221a 2L2\u03b7\n\u03bb+ \u03b3 .\nFinally, we have by the triangle inequality and the stability of exact ERM that\n\u2223\u2223\u2223EZ,A [ G(w\u0303)\u2212 G\u0302(w\u0302) ]\u2223\u2223\u2223 \u2264 EZ [EA |G(w\u0303)\u2212G(w\u0302)|] + \u2223\u2223\u2223EZ [ G(w\u0302)\u2212 G\u0302(w\u0302) ]\u2223\u2223\u2223\n\u2264 \u221a 2L2\u03b7\n\u03bb+ \u03b3 +\n4L2\n(\u03bb+ \u03b3)n .\nThen Lemma 2 follows from the fact that that our stochastic objective (8) is equipped with\nL-Lipschitz, \u03bb-strongly convex loss \u03c6(w) and \u03b3t-strongly convex regularizer \u03b3t 2 \u2016w \u2212wt\u22121\u2016 2 ."}, {"heading": "A.3. Proof of Lemma 3", "text": "Proof We have by Lemma 2 that\n|EIt [\u03c6It(wt)\u2212 \u03c6(wt)]| \u2264 4L2\n(\u03bb+ \u03b3t)b .\nTake expectation of (6) over the random sampling of It and we obtain\n\u03bb+ \u03b3t \u03b3t EIt \u2016wt \u2212w\u20162 \u2264 \u2016wt\u22121 \u2212w\u20162 \u2212 2 \u03b3t (EIt [\u03c6It(wt)]\u2212 \u03c6(w))\n= \u2016wt\u22121 \u2212w\u20162 \u2212 2\n\u03b3t (EIt [\u03c6It(wt)\u2212 \u03c6(wt)] + EIt [\u03c6(wt)\u2212 \u03c6(w)])\n\u2264 \u2016wt\u22121 \u2212w\u20162 \u2212 2\n\u03b3t EIt [\u03c6(wt)\u2212 \u03c6(w)] +\n2 \u03b3t |EIt [\u03c6It(wt)\u2212 \u03c6(wt)]|\n\u2264 \u2016wt\u22121 \u2212w\u20162 \u2212 2\n\u03b3t EIt [\u03c6(wt)\u2212 \u03c6(w)] +\n8L2\n\u03b3t(\u03bb+ \u03b3t)b ."}, {"heading": "A.4. Proof of Theorem 4", "text": "Proof When \u2113(w, \u03be) is weakly convex (i.e., \u03bb = 0), we further set \u03b3t = \u03b3 for all t \u2265 1. Applying Lemma 3 withw = w\u2217 yields\nEIt [\u03c6(wt)\u2212 \u03c6(w\u2217)] \u2264 \u03b3\n2\n( \u2016wt\u22121 \u2212w\u2217\u20162 \u2212 EIt \u2016wt \u2212w\u2217\u20162 ) + 4L2\n\u03b3b . (20)\nSumming (20) for t = 1, . . . , T yields\nT\u2211\nt=1\nE [\u03c6(wt)\u2212 \u03c6(w\u2217)] \u2264 \u03b3\n2 \u2016w0 \u2212w\u2217\u20162 +\n4L2T\n\u03b3b .\nMinimizing the RHS over \u03b3 gives the optimal choice\n\u03b3 =\n\u221a 8T\nb \u00b7 L\u2016w0 \u2212w\u2217\u2016 ,\nwith a corresponding regret\n1\nT\nT\u2211\nt=1\nE [\u03c6(wt)\u2212 \u03c6(w\u2217)] \u2264 \u221a 8L\u221a bT \u2016w0 \u2212w\u2217\u2016 .\nAs a result, by returning the uniform average w\u0302T = 1 T \u2211T t=1 wt, we have due to the convexity of \u03c6(w) that\nE [\u03c6(w\u0302T )\u2212 \u03c6(w\u2217)] \u2264 \u221a 8L\u221a bT \u2016w0 \u2212w\u2217\u2016 ."}, {"heading": "A.5. Proof of Theorem 5", "text": "Proof Let \u2113(w, \u03be) be \u03bb-strongly convex for some \u03bb > 0. Applying Lemma 3 withw = w\u2217 yields\nEIt [\u03c6(wt)\u2212 \u03c6(w\u2217)] \u2264 ( \u03b3t 2 \u2016wt\u22121 \u2212w\u2217\u20162 \u2212 \u03bb+ \u03b3t 2 EIt \u2016wt \u2212w\u2217\u20162 ) +\n4L2\n(\u03bb+ \u03b3t)b . (21)\nSetting \u03b3t = \u03bb(t\u22121) 2 for t = 1, . . . , 5, the above inequality becomes\nEIt [\u03c6(wt)\u2212 \u03c6(w\u2217)] \u2264 ( \u03bb(t\u2212 1)\n4 \u2016wt\u22121 \u2212w\u2217\u20162 \u2212\n\u03bb(t+ 1)\n4 EIt \u2016wt \u2212w\u2217\u20162\n) +\n8L2\n\u03bbb(t+ 1)\n\u2264 ( \u03bb(t\u2212 1)\n4 \u2016wt\u22121 \u2212w\u2217\u20162 \u2212\n\u03bb(t+ 1)\n4 EIt \u2016wt \u2212w\u2217\u20162\n) + 8L2\n\u03bbbt ,\nand therefore\nt \u00b7 EIt [\u03c6(wt)\u2212 \u03c6(w\u2217)] \u2264 \u03bb\n4\n( (t\u2212 1)t \u2016wt\u22121 \u2212w\u2217\u20162 \u2212 t(t+ 1)EIt \u2016wt \u2212w\u2217\u20162 ) + 8L2\n\u03bbb .\nSumming this inequality for t = 1, . . . , T yields\nT\u2211\nt=1\nt \u00b7 E [\u03c6(wt)\u2212 \u03c6(w\u2217)] \u2264 8L2T\n\u03bbb .\nAs a result, by returning the weighted average w\u0302T = 2\nT (T+1) \u2211T t=1 twt, we have due to the con-\nvexity of \u03c6(w) that \u03c6(w\u0302T ) \u2264 2T (T+1) \u2211T t=1 t \u00b7 \u03c6(wt) and\nE [\u03c6(w\u0302T )\u2212 \u03c6(w\u2217)] \u2264 2\nT (T + 1)\nT\u2211\nt=1\nt \u00b7 E [\u03c6(wt)\u2212 \u03c6(w\u2217)] \u2264 16L2\n\u03bbb(T + 1) ."}, {"heading": "Appendix B. Analysis of inexact minibatch-prox", "text": ""}, {"heading": "B.1. Proof of Lemma 6", "text": "Proof Due to the (\u03bb+ \u03b3t)-strong convexity of f\u0303t(w), we have\nEA \u2016w\u0303t \u2212 w\u0304t\u20162 \u2264 2\n\u03bb+ \u03b3t EA\n[ f\u0303t(w\u0303t)\u2212 f\u0303t(w\u0304t) ] \u2264 2\u03b7t\n\u03bb+ \u03b3t .\nApplying Lemma 1 to the exact minimizer w\u0304t yields\n\u03c6It(w\u0304t)\u2212 \u03c6It(w) \u2264 \u03b3t 2 \u2016w\u0303t\u22121 \u2212w\u20162 \u2212 \u03bb+ \u03b3t 2 \u2016w\u0304t \u2212w\u20162 .\n5. This choice is inspired by the stepsize rule of Lacoste-Julien et al. (2012) for stochastic gradient descent.\nTherefore, for the t-th iteration, we have\nEIt,A [\u03c6(w\u0303t)\u2212 \u03c6(w)] = EIt,A [\u03c6(w\u0303t)\u2212 \u03c6It(w\u0304t)] + EIt [\u03c6It(w\u0304t)\u2212 \u03c6It(w)]\n\u2264 4L 2\n(\u03bb+ \u03b3t)b + \u221a 2L2\u03b7t \u03bb+ \u03b3t + \u03b3t 2 \u2016w\u0303t\u22121 \u2212w\u20162 \u2212 \u03bb+ \u03b3t 2 EIt \u2016w\u0304t \u2212w\u20162\n\u2264 4L 2\n(\u03bb+ \u03b3t)b + \u221a 2L2\u03b7t \u03bb+ \u03b3t + \u03b3t 2 \u2016w\u0303t\u22121 \u2212w\u20162 \u2212 \u03bb+ \u03b3t 2 EIt,A (\u2016w\u0303t \u2212w\u2016 \u2212 \u2016w\u0303t \u2212 w\u0304t\u2016)2\n\u2264 4L 2\n(\u03bb+ \u03b3t)b + \u221a 2L2\u03b7t \u03bb+ \u03b3t + \u03b3t 2 \u2016w\u0303t\u22121 \u2212w\u20162 \u2212 \u03bb+ \u03b3t 2 EIt,A \u2016w\u0303t \u2212w\u20162\n+ (\u03bb+ \u03b3t) \u00b7 EIt,A [\u2016w\u0303t \u2212 w\u0304t\u2016 \u00b7 \u2016w\u0303t \u2212w\u2016]\n\u2264 4L 2\n(\u03bb+ \u03b3t)b + \u221a 2L2\u03b7t \u03bb+ \u03b3t + \u03b3t 2 \u2016w\u0303t\u22121 \u2212w\u20162 \u2212 \u03bb+ \u03b3t 2 EIt,A \u2016w\u0303t \u2212w\u20162\n+ (\u03bb+ \u03b3t) \u221a EIt,A \u2016w\u0303t \u2212 w\u0304t\u20162 \u00b7 \u221a EIt,A \u2016w\u0303t \u2212w\u20162\n\u2264 4L 2\n(\u03bb+ \u03b3t)b + \u221a 2L2\u03b7t \u03bb+ \u03b3t + \u03b3t 2 \u2016w\u0303t\u22121 \u2212w\u20162 \u2212 \u03bb+ \u03b3t 2 EIt,A \u2016w\u0303t \u2212w\u20162\n+ \u221a 2(\u03bb+ \u03b3t)\u03b7t \u00b7 \u221a EIt,A \u2016w\u0303t \u2212w\u20162\nwhere we have applied Lemma 11 to the approximate minimizer w\u0303t in the first inequality, used the triangle inequality \u2016w\u0304t \u2212w\u2016 \u2265 |\u2016w\u0303t \u2212w\u2016 \u2212 \u2016w\u0303t \u2212 w\u0304t\u2016| in the second inequality, dropped a negative term in the third inequality, and used the Cauchy-Schwarz inequality for random variables in the fourth inequality."}, {"heading": "B.2. Proof of Theorem 7", "text": "When \u2113(w, \u03be) is weakly convex (i.e., \u03bb = 0), set \u03b3t = \u03b3 for all t \u2265 1 as in exact minibatch-prox. Then summing (11) for t = 1, . . . , T yields\nT\u2211\nt=1\nE [\u03c6(w\u0303t)\u2212 \u03c6(w\u2217)] + \u03b3\n2 E \u2016w\u0303T \u2212w\u2217\u20162 \u2264\n\u03b3 2 \u2016w\u03030 \u2212w\u2217\u20162 +\n4L2T\n\u03b3b +\nT\u2211\nt=1\n\u221a 2L2\u03b7t \u03b3\n+ T\u2211\nt=1\n\u221a 2\u03b3\u03b7t \u00b7 \u221a E \u2016w\u0303t \u2212w\u2217\u20162 (22)\nwhere the expectation is taken over random sampling and the randomness of A in the first T iterations. To resolve the recursion, we need the following lemma by Schmidt et al. (2011).\nLemma 12 Assume that the non-negative sequence {uT } satisfies the following recursion for all T \u2265 1:\nu2T \u2264 ST + T\u2211\nt=1\n\u03bbtut,\nwith ST an increasing sequence, S0 \u2265 u20 and \u03bbt \u2265 0 for all t. Then, for all T \u2265 1, we have\nuT \u2264 1\n2\nT\u2211\nt=1\n\u03bbt +  ST + ( 1\n2\nT\u2211\nt=1\n\u03bbt\n)2  1 2 \u2264 \u221a\nST +\nT\u2211\nt=1\n\u03bbt.\nWe are now ready to prove Theorem 7.\nProof Bounding \u221a E \u2016w\u0303t \u2212w\u2217\u20162. Dropping the \u2211T t=1 E [\u03c6(w\u0303t)\u2212 \u03c6(w\u2217)] term from (22) which is non-negative due to the optimality of w\u2217, we obtain\nE \u2016w\u0303T \u2212w\u2217\u20162 \u2264 \u2016w\u03030 \u2212w\u2217\u20162 + 8L2T\n\u03b32b +\nT\u2211\nt=1\n\u221a 8L2\u03b7t \u03b33 + T\u2211\nt=1\n\u221a 8\u03b7t \u03b3 \u00b7 \u221a E \u2016w\u0303t \u2212w\u2217\u20162.\nNow apply Lemma 12 (using uT = \u221a E \u2016w\u0303T \u2212w\u2217\u20162, ST = \u2016w\u03030 \u2212w\u2217\u20162+ 8L 2T \u03b32b + \u2211T t=1 \u221a 8L2\u03b7t \u03b33 , and \u03bbt = \u221a 8\u03b7t \u03b3 ) and the fact that \u221a x+ y \u2264 \u221ax+\u221ay for x, y \u2265 0, we have\n\u221a E \u2016w\u0303T \u2212w\u2217\u20162 \u2264 \u2016w\u03030 \u2212w\u2217\u2016+\n\u221a 8L2T\n\u03b32b +\nT\u2211\nt=1\n\u221a 8\u03b7t \u03b3 +\n\u221a\u221a\u221a\u221a T\u2211\nt=1\n\u221a 8L2\u03b7t \u03b33\nWe have thus bounded the sequence of \u221a E \u2016w\u0303T \u2212w\u2217\u20162 by a non-negative increasing sequence.\nBounding function values. Dropping theE \u2016w\u0303T \u2212w\u2217\u20162 term from (22) which is non-negative, we obtain\nT\u2211\nt=1\nE [\u03c6(w\u0303t)\u2212 \u03c6(w\u2217)]\n\u2264 \u03b3 2 \u2016w\u03030 \u2212w\u2217\u20162 +\n4L2T\n\u03b3b +\nT\u2211\nt=1\n\u221a 2L2\u03b7t \u03b3 + T\u2211\nt=1\n\u221a 2\u03b7t\u03b3 \u00b7 \u221a E \u2016w\u0303t \u2212w\u2217\u20162\n\u2264 \u03b3 2 \u2016w\u03030 \u2212w\u2217\u20162 +\n4L2T\n\u03b3b +\nT\u2211\nt=1\n\u221a 2L2\u03b7t \u03b3 + ( T\u2211\nt=1\n\u221a 2\u03b7t\u03b3 ) \u00b7 max 1\u2264t\u2264T \u221a E \u2016w\u0303t \u2212w\u2217\u20162\n\u2264 \u03b3 2 \u2016w\u03030 \u2212w\u2217\u20162 +\n4L2T\n\u03b3b +\nT\u2211\nt=1\n\u221a 2L2\u03b7t \u03b3\n+\n( T\u2211\nt=1\n\u221a 2\u03b7t\u03b3 ) \u00b7  \u2016w\u03030 \u2212w\u2217\u2016+ \u221a 8L2T\n\u03b32b +\nT\u2211\nt=1\n\u221a 8\u03b7t \u03b3 +\n\u221a\u221a\u221a\u221a T\u2211\nt=1\n\u221a 8L2\u03b7t \u03b33   . (23)\nTo achieve the same order of regret as in exact minibatch-prox, we require that \u03b7t decays with t, and in particular\n\u03b7t \u2264 min ( c1 ( T\nb\n) 1 2\n, c2\n( T\nb\n) 3 2 ) \u00b7 L \u2016w\u03030 \u2212w\u2217\u2016\nt2+2\u03b4 (24)\nfor some \u03b4 > 0. Note that \u03b7t has the unit of function value. Let c := \u2211\u221e i=1 1 i1+\u03b4 \u2264 1+\u03b4\u03b4 which only depends on \u03b4 (as a concrete example, we have c = \u03c0 2\n6 when \u03b4 = 2).\nUsing the choice of \u03b3 = \u221a\n8T b \u00b7 L\u2016w0\u2212w\u2217\u2016 , we obtain from (24) that\nT\u2211\nt=1\n\u221a 8\u03b7t \u03b3 = T\u2211\nt=1\n\u221a\u221a 8b\nT \u00b7 \u2016w0 \u2212w\u2217\u2016 L \u00b7 \u03b7t \u2264 8 1 4 c 1 2 1 \u2016w\u03030 \u2212w\u2217\u2016\nT\u2211\nt=1\n1\nt1+\u03b4\n\u2264 8 14 c 1 2 1 c \u2016w\u03030 \u2212w\u2217\u2016 ,\nT\u2211\nt=1\n\u221a 8L2\u03b7t \u03b33 = T\u2211\nt=1\n\u221a\u221a b3\n8T 3 \u00b7 \u2016w0 \u2212w\u2217\u2016 3 L \u00b7 \u03b7t \u2264 8\u2212 1 4 c 1 2 2 \u2016w\u03030 \u2212w\u2217\u20162\nT\u2211\nt=1\n1\nt1+\u03b4\n\u2264 8\u2212 14 c 1 2 2 c \u2016w\u03030 \u2212w\u2217\u20162 .\nContinuing from (23) and substituting in the value of \u03b3, we have\nT\u2211\nt=1\nE [\u03c6(w\u0303t)\u2212 \u03c6(w\u2217)] \u2264 \u221a 8T\nb \u00b7 L \u2016w\u03030 \u2212w\u2217\u2016+\n\u03b3\n2\nT\u2211\nt=1\n\u221a 8L2\u03b7t \u03b33\n+ \u03b3\n2\n( T\u2211\nt=1\n\u221a 8\u03b7t \u03b3 ) \u00b7  2 \u2016w\u03030 \u2212w\u2217\u2016+ T\u2211\nt=1\n\u221a 8\u03b7t \u03b3 +\n\u221a\u221a\u221a\u221a T\u2211\nt=1\n\u221a 8L2\u03b7t \u03b33  \n=\n\u221a 8T\nb \u00b7 L \u2016w\u03030 \u2212w\u2217\u2016+\n\u221a 2T\nb \u00b7 L\u2016w0 \u2212w\u2217\u2016 \u00b7 8\u2212 14 c 1 2 2 c \u2016w\u03030 \u2212w\u2217\u20162\n+\n\u221a 2T\nb \u00b7 L\u2016w0 \u2212w\u2217\u2016 \u00b7 8 14 c 1 2 1 c \u2016w\u03030 \u2212w\u2217\u2016\u00d7\n( 2 \u2016w\u03030 \u2212w\u2217\u2016+ 8 1 4 c 1 2 1 c \u2016w\u03030 \u2212w\u2217\u2016+ \u221a 8\u2212 1 4 c 1 2 2 c \u2016w\u03030 \u2212w\u2217\u20162 )\n= c3\n\u221a T\nb \u00b7 L \u2016w\u03030 \u2212w\u2217\u2016 .\nThe suboptimality of w\u0302T is then due to the convexity of \u03c6(w):\nE [\u03c6(w\u0302T )\u2212 \u03c6(w\u2217)] \u2264 1\nT\nT\u2211\nt=1\nE [\u03c6(w\u0303t)\u2212 \u03c6(w\u2217)] = c3L \u2016w\u03030 \u2212w\u2217\u2016\u221a\nbT ."}, {"heading": "B.3. Proof of Theorem 8", "text": "Proof We have by Lemma 6 that\nEIt,A [\u03c6(w\u0303t)\u2212 \u03c6(w\u2217)] \u2264 \u03bb(t\u2212 1)\n4 \u2016w\u0303t\u22121 \u2212w\u2217\u20162 \u2212\n\u03bb(t+ 1)\n4 EIt,A \u2016w\u0303t \u2212w\u2217\u20162\n+ 8L2\n\u03bbb(t+ 1) + \u221a 4L2\u03b7t \u03bb(t+ 1) + \u221a \u03bb(t+ 1)\u03b7t \u00b7 \u221a EIt,A \u2016w\u0303t \u2212w\u2217\u20162.\nRelaxing the 1t+1 to 1 t on the RHS, and multiplying both sides by t, we further obtain\nt \u00b7 EIt,A [\u03c6(w\u0303t)\u2212 \u03c6(w\u2217)] \u2264 \u03bb(t\u2212 1)t\n4 \u2016w\u0303t\u22121 \u2212w\u2217\u20162 \u2212\n\u03bbt(t+ 1)\n4 EIt,A \u2016w\u0303t \u2212w\u2217\u20162\n+ 8L2\n\u03bbb +\n\u221a 4L2t\u03b7t\n\u03bb +\n\u221a \u03bbt\u03b7t \u00b7 \u221a EIt,A [ t(t+ 1) \u2016w\u0303t \u2212w\u2217\u20162 ] .\nSumming this inequality for t = 1, . . . , T yields\nT\u2211\nt=1\nt \u00b7 E [\u03c6(w\u0303t)\u2212 \u03c6(w\u2217)] + \u03bbT (T + 1)\n4 E \u2016w\u0303T \u2212w\u2217\u20162\n\u2264 8L 2T\n\u03bbb +\nT\u2211\nt=1\n\u221a 4L2t\u03b7t\n\u03bb +\nT\u2211\nt=1\n\u221a \u03bbt\u03b7t \u00b7 \u221a E [ t(t+ 1) \u2016w\u0303t \u2212w\u2217\u20162 ] . (25)\nBounding \u221a E \u2016w\u0303t \u2212w\u2217\u20162. Dropping the \u2211T t=1 t \u00b7 E [\u03c6(w\u0303t)\u2212 \u03c6(w\u2217)] term from (25) which\nis non-negative due to the optimality of w\u2217, we obtain\nE [ T (T + 1) \u2016w\u0303T \u2212w\u2217\u20162 ] \u226432L 2T\n\u03bb2b +\nT\u2211\nt=1\n\u221a 64L2t\u03b7t\n\u03bb3 +\nT\u2211\nt=1\n\u221a 16t\u03b7t \u03bb \u00b7 \u221a E [ t(t+ 1) \u2016w\u0303t \u2212w\u2217\u20162 ] .\nApplying Lemma 12 (using uT =\n\u221a E [ T (T + 1) \u2016w\u0303T \u2212w\u2217\u20162 ] , ST = 32L2T \u03bb2b + \u2211T t=1 \u221a 64L2t\u03b7t \u03bb3 ,\nand \u03bbt = \u221a 16t\u03b7t \u03bb ), we have\n\u221a E [ T (T + 1) \u2016w\u0303T \u2212w\u2217\u20162 ] \u2264 \u221a 32L2T\n\u03bb2b +\nT\u2211\nt=1\n\u221a 16t\u03b7t \u03bb +\n\u221a\u221a\u221a\u221a T\u2211\nt=1\n\u221a 64L2t\u03b7t\n\u03bb3 .\nBounding function values. Dropping theE \u2016w\u0303T \u2212w\u2217\u20162 term from (25) which is non-negative, we obtain\nT\u2211\nt=1\nt \u00b7 E [\u03c6(w\u0303t)\u2212 \u03c6(w\u2217)] \u2264 8L2T\n\u03bbb +\nT\u2211\nt=1\n\u221a 4L2t\u03b7t\n\u03bb\n+\n( T\u2211\nt=1\n\u221a \u03bbt\u03b7t ) \u00b7   \u221a 32L2T\n\u03bb2b +\nT\u2211\nt=1\n\u221a 16t\u03b7t \u03bb +\n\u221a\u221a\u221a\u221a T\u2211\nt=1\n\u221a 64L2t\u03b7t\n\u03bb3\n  . (26)\nTo achieve the same order of regret as in exact minibatch-prox, we require that \u03b7t decays with t, and in particular\n\u03b7t \u2264 min ( c1 ( T\nb\n) , c2 ( T\nb\n)2) \u00b7 L 2\nt3+2\u03b4\u03bb (27)\nfor some \u03b4 > 0. Note that \u03b7t has the unit of function value. Let c := \u2211\u221e i=1 1 i1+\u03b4 \u2264 1+\u03b4\u03b4 . Then (27) ensures that\nT\u2211\nt=1\n\u221a t\u03b7t \u03bb \u2264 c\u221ac1 \u221a L2T \u03bb2b , and\nT\u2211\nt=1\n\u221a L2t\u03b7t \u03bb3 \u2264 c\u221ac2 \u00b7 L2T \u03bb2b .\nContinuing from (26), we have\nT\u2211\nt=1\nt \u00b7 E [\u03c6(w\u0303t)\u2212 \u03c6(w\u2217)] \u2264 8L2T\n\u03bbb + 2c\n\u221a c2 \u00b7 L2T\n\u03bbb\n+ c \u221a c1\n\u221a L2T\nb\n(\u221a 32L2T\n\u03bb2b + 4c\n\u221a c1\n\u221a L2T\n\u03bb2b +\n4 \u221a\n64c2c2\n\u221a L2T\n\u03bb2b\n)\n= c3 2 \u00b7 L 2T \u03bbb .\nIn view of the convexity of \u03c6(w), by returning the weighted average w\u0302T = 2\nT (T+1) \u2211T t=1 tw\u0303t, we\nhave\nE [\u03c6(w\u0302T )\u2212 \u03c6(w\u2217)] \u2264 2\nT (T + 1)\nT\u2211\nt=1\nt \u00b7 E [\u03c6(w\u0303t)\u2212 \u03c6(w\u2217)] \u2264 c3L\n2\n\u03bbb(T + 1) .\nB.4. Connection to minibatch stochastic gradient descent\nTo see the connection between minibatch-prox and minibatch SGD, note that if we solve the linearized minibatch problem exactly, we obtain the minibatch stochastic gradient descent algorithm:\nw\u0303t = argmin w\u2208\u2126 \u03c6It(w\u0303t\u22121) +\u2207\u3008\u03c6It(w\u0303t\u22121), w \u2212 w\u0303t\u22121\u3009+ \u03b3t 2 \u2016w \u2212 w\u0303t\u22121\u20162 .\nFollowing Cotter et al. (2011), we assume that \u2113(w, \u03be) is \u03b2-smooth: \u2225\u2225\u2207\u2113(w, \u03be)\u2212\u2207\u2113(w\u2032, \u03be) \u2225\u2225 \u2264 \u03b2 \u2225\u2225w \u2212w\u2032 \u2225\u2225 , \u2200w,w\u2032 \u2208 \u2126.\nWe then have the following guarantee for each iterate of minbatch SGD.\nProposition 13 For iteration t of minibatch SGD, we have\nEIt [\u03c6(w\u0303t)\u2212 \u03c6(w\u2217)] \u2264 2L2\n(\u03b3t \u2212 \u03b2)b + \u03b3t \u2212 \u03bb 2 \u2016w\u2217 \u2212 w\u0303t\u22121\u20162 \u2212 \u03b3t 2 EIt \u2016w\u2217 \u2212 w\u0303t\u20162 . (28)\nProof Our proof closely follows that of Cotter et al. (2011).\nDue to the smoothness of \u03c6, we have that\n\u03c6(w\u0303t) \u2264 \u03c6(w\u0303t\u22121) + \u3008\u2207\u03c6(w\u0303t\u22121), w\u0303t \u2212 w\u0303t\u22121\u3009+ \u03b2\n2 \u2016w\u0303t \u2212 w\u0303t\u22121\u20162\n\u2264 \u03c6(w\u0303t\u22121) + \u3008\u2207\u03c6(w\u0303t\u22121)\u2212\u2207\u03c6It(w\u0303t\u22121), w\u0303t \u2212 w\u0303t\u22121\u3009+ \u03b2\n2 \u2016w\u0303t \u2212 w\u0303t\u22121\u20162\n+ \u3008\u2207\u03c6It(w\u0303t\u22121), w\u0303t \u2212 w\u0303t\u22121\u3009\n= \u03c6(w\u0303t\u22121) + \u2016\u2207\u03c6(w\u0303t\u22121)\u2212\u2207\u03c6It(w\u0303t\u22121)\u2016 \u00b7 \u2016w\u0303t \u2212 w\u0303t\u22121\u2016+ \u03b2\n2 \u2016w\u0303t \u2212 w\u0303t\u22121\u20162\n+ \u3008\u2207\u03c6It(w\u0303t\u22121), w\u0303t \u2212 w\u0303t\u22121\u3009\n\u2264 \u03c6(w\u0303t\u22121) + 1\n2(\u03b3t \u2212 \u03b2) \u2016\u2207\u03c6(w\u0303t\u22121)\u2212\u2207\u03c6It(w\u0303t\u22121)\u20162 + \u03b3t \u2212 \u03b2 2 \u2016w\u0303t \u2212 w\u0303t\u22121\u20162\n+ \u03b2\n2 \u2016w\u0303t \u2212 w\u0303t\u22121\u20162 + \u3008\u2207\u03c6It(w\u0303t\u22121), w\u0303t \u2212 w\u0303t\u22121\u3009\n= \u03c6(w\u0303t\u22121) + 1\n2(\u03b3t \u2212 \u03b2) \u2016\u2207\u03c6(w\u0303t\u22121)\u2212\u2207\u03c6It(w\u0303t\u22121)\u20162 + \u03b3t 2 \u2016w\u0303t \u2212 w\u0303t\u22121\u20162\n+ \u3008\u2207\u03c6It(w\u0303t\u22121), w\u0303t \u2212 w\u0303t\u22121\u3009 (29)\nwhere we have used the Cauchy-Schwarz inequality in the second inequality, and the inequality xy \u2264 x22\u03b1 + \u03b1y2 2 in the third inequality.\nNow, since w\u0303t is the minimizer of the \u03b3t-strongly convex function\n\u03b3t 2 \u2016w \u2212 w\u0303t\u22121\u20162 + \u3008\u2207\u03c6It(w\u0303t\u22121), w \u2212 w\u0303t\u22121\u3009\nin \u2126, we have according to Lemma 1 (replacing the local objective with its linear approximation) that\n\u03b3t 2 \u2016w\u2217 \u2212 w\u0303t\u22121\u20162 + \u3008\u2207\u03c6It(w\u0303t\u22121), w\u2217 \u2212 w\u0303t\u22121\u3009\n\u2265 \u03b3t 2 \u2016w\u0303t \u2212 w\u0303t\u22121\u20162 + \u3008\u2207\u03c6It(w\u0303t\u22121), w\u0303t \u2212 w\u0303t\u22121\u3009+ \u03b3t 2 \u2016w\u2217 \u2212 w\u0303t\u20162 .\nSubstituting this into (29) gives\n\u03c6(w\u0303t) \u2264 \u03c6(w\u0303t\u22121) + 1\n2(\u03b3t \u2212 \u03b2) \u2016\u2207\u03c6(w\u0303t\u22121)\u2212\u2207\u03c6It(w\u0303t\u22121)\u20162 + \u03b3t 2 \u2016w\u2217 \u2212 w\u0303t\u22121\u20162\n+ \u3008\u2207\u03c6It(w\u0303t\u22121), w\u2217 \u2212 w\u0303t\u22121\u3009 \u2212 \u03b3t 2 \u2016w\u2217 \u2212 w\u0303t\u20162 .\nTaking expectation of this inequality over the random sampling of It further leads to\nEIt [\u03c6(w\u0303t)] \u2264 \u03c6(w\u0303t\u22121) + 1\n2(\u03b3t \u2212 \u03b2) EIt \u2016\u2207\u03c6(w\u0303t\u22121)\u2212\u2207\u03c6It(w\u0303t\u22121)\u20162 + \u03b3t 2 \u2016w\u2217 \u2212 w\u0303t\u22121\u20162\n+ \u3008\u2207\u03c6(w\u0303t\u22121), w\u2217 \u2212 w\u0303t\u22121\u3009 \u2212 \u03b3t 2 EIt \u2016w\u2217 \u2212 w\u0303t\u20162\n\u2264 \u03c6(w\u2217) + 1\n2(\u03b3t \u2212 \u03b2) EIt \u2016\u2207\u03c6(w\u0303t\u22121)\u2212\u2207\u03c6It(w\u0303t\u22121)\u20162\n+ \u03b3t \u2212 \u03bb\n2 \u2016w\u2217 \u2212 w\u0303t\u22121\u20162 \u2212 \u03b3t 2 EIt \u2016w\u2217 \u2212 w\u0303t\u20162 (30)\nwhere in the second inequality we have used the fact that\n\u03c6(w\u2217) \u2265 \u03c6(w\u0303t\u22121) + \u3008\u2207\u03c6(w\u0303t\u22121), w\u2217 \u2212 w\u0303t\u22121\u3009+ \u03bb\n2 \u2016w\u2217 \u2212 w\u0303t\u22121\u20162\ndue to the convexity of \u03c6(w). On the other hand, let It = {\u03be1, . . . , \u03beb}, we have\nEIt \u2016\u2207\u03c6(w)\u2212\u2207\u03c6It(w)\u20162\n= EIt \u2225\u2225\u2225\u2225\u2225\u2207\u03c6(w)\u2212 1 b b\u2211\ni=1\n\u2207\u2113(w, \u03bei) \u2225\u2225\u2225\u2225\u2225 2\n= EIt \u2225\u2225\u2225\u2225\u2225 1 b b\u2211\ni=1\n(\u2207\u03c6(w)\u2212\u2207\u2113(w, \u03bei)) \u2225\u2225\u2225\u2225\u2225 2\n= 1\nb2\nb\u2211\ni=1\nE\u03bei \u2016\u2207\u03c6(w)\u2212\u2207\u2113(w, \u03bei)\u20162 + 1\nb2\n\u2211 i 6=j EIt \u3008\u2207\u03c6(w)\u2212\u2207\u2113(w, \u03bei), \u2207\u03c6(w)\u2212\u2207\u2113(w, \u03bej)\u3009\n= 1\nb \u00b7 E\u03be \u2016\u2207\u03c6(w)\u2212\u2207\u2113(w, \u03be)\u20162\n\u2264 4L 2\nb\nwhere we used the fact that the samples are i.i.d. in the fourth equality, and that \u2016\u2207\u03c6(w)\u2016 , \u2016\u2207\u2113(w, \u03be)\u2016 \u2264 L in the last inequality. Continuing from (30) yields the desired result.\nComparing this result to (20) and (21), we observe that minibatch SGD has a similar recursion to that exact minibatch-prox, except the appearance of \u03b2 in the denominator of the \u201cstability\u201d term. We now show that this difference leads to significant difference in convergence rate.\nLet \u2113(w, \u03be) be weakly convex (\u03bb = 0), and \u03b3t = \u03b3 for all t \u2265 1. Summing (28) over t = 1, . . . , T gives\nT\u2211\nt=1\nE [\u03c6(w\u0303t)\u2212 \u03c6(w\u2217)] \u2264 2L2T (\u03b3 \u2212 \u03b2)b + \u03b3 2 \u2016w\u2217 \u2212 w\u03030\u20162 .\nMinimizing the RHS over \u03b3 gives\n\u03b3 = \u03b2 +\n\u221a 4T\nb \u00b7 L\u2016w\u2217 \u2212 w\u03030\u2016 ,\nwhich leads to\n1\nT\nT\u2211\nt=1\nE [\u03c6(w\u0303t)\u2212 \u03c6(w\u2217)] \u2264 2L \u2016w\u2217 \u2212 w\u03030\u2016\u221a\nbT + \u03b2 \u2016w\u2217 \u2212 w\u03030\u20162 2T .\nSo we obtain the familiar O (\n1\u221a bT + 1T\n) rate for minibatch SGD."}, {"heading": "Appendix C. Proof of Theorem 10", "text": "Proof On the one hand, as we choose \u03b3 as Theorem 7 suggested, we just need to verify that the inexactness conditions in Theorem 7 is satisfied, i.e., for t = 1, . . . , T , we require (recall that w\u2217t = argminw f\u0303t(w))\nf\u0303t(wt)\u2212 f\u0303t(w\u2217t ) \u2264 1\n104 \u00b7min\n(( T\nbm\n)1/2 , ( T\nbm )3/2) \u00b7 LB t3 .\nOn the other hand, we can bound the initial suboptimality of f\u0303t(w) when initializing from wt\u22121. This is because, by the optimality of w\u2217t , we have \u2016w\u2217t \u2212wt\u22121\u2016 = \u2225\u2225\u2225 1\u03b3\u2207\u03c6It(w\u2217t ) \u2225\u2225\u2225 \u2264 L/\u03b3, and\nf\u0303t(wt\u22121)\u2212 f\u0303t(w\u2217t ) = 0 + \u03c6It(wt\u22121)\u2212 \u03b3 2 \u2016w\u2217t \u2212wt\u22121\u20162 \u2212 \u03c6It(w\u2217t )\n\u2264 \u03c6It(wt\u22121)\u2212 \u03c6It(w\u2217t ) \u2264 L \u2016w\u2217t \u2212wt\u22121\u2016 \u2264 L2/\u03b3. (31)\nCombining the above two inequalities, the initial versus final error for the K DSVRG iterations is bounded by\n104 \u00b7max (( bm\nT\n)1/2 , ( bm\nT\n)3/2) \u00b7 t3 \u00b7 L\nB\u03b3\n= 104 \u00b7max (( bm\nT\n)1/2 , ( bm\nT\n)3/2) \u00b7 T 3 \u00b7 L\nB \u00b7 bmB\u221a\n8n(\u03b5)L\n= O ( max ( n(\u03b5)2\nbm , bm \u00b7 n(\u03b5)\n))\n= O ( n2(\u03b5) )\nwhere we have used the definition of \u03b3 and T = n(\u03b5)bm in the first and second step respectively. By the iteration complexity results for sampling without-replacement DSVRG (Shamir, 2016, Theorem 4), we have the desired suboptimality in f\u0303t(w) using\nK = O (log n(\u03b5)) (32)\niterations, as long as the batch size b/pi is larger than the problem condition number. Now, the condition number of f\u0303(w) is\n\u03b2 + \u03b3\n\u03b3 = O ( \u03b2bmB\u221a n(\u03b5)L ) .\nEquating this to the batch size b/pi yields the pi specified in the theorem. It is also easy to check that K \u03b3 = O(bm), i.e., the total number of stochastic updates is less than the total number of samples, as required by Shamir (2016, Theorem 4).\nCommunication: the total rounds of communication required by Algorithm 1 is\nKT = O ( n(\u03b5)\nmb log n(\u03b5)\n) .\nComputation: For each communication round, each machine need to compute the local full gradient, which can be done in parallel, and then one of the machines perform b/pi steps of stochastic update. So the computation cost is\nKT ( b+ b\npi\n) = O ( n(\u03b5)\nm log n(\u03b5)\n) .\nMemory: It is straightforward to see each machine only need to maintain b samples."}, {"heading": "Appendix D. Communication-efficient distributed minibatch-prox with DANE", "text": "As discussed in Section 4, it is also possible to use other efficient distributed optimization solver for minibatch-prox. Here we present a novel method that use the distributed optimization algorithm DANE (Shamir et al., 2014) and its accelerated variant AIDE (Reddi et al., 2016) for solving (12), which define better local objectives than EMSO and take into consideration the similarity between local objectives.\nWe detail our algorithm, named MP-DANE, in Algorithm 2. The algorithm consists of three nested loops, where t, r and k are iteration counters for minibatch-prox (the outer for-loop), AIDE (the intermediate for-loop) and DANE (the inner for-loop) respectively. Compared to EMSO, DANE adds a gradient correction term to (13) which can be compute efficiently with one round of communication. On top of that, AIDE uses the idea of universal catalyst (Lin et al., 2015) and adds an extra quadratic term to improve the strong-convexity of the objective for faster convergence, i.e., in order to solve (12), AIDE solves multiple instances of the \u201caugmented large minibatch\u201d problems of the form\nmin w\u2208\u2126\nf\u0304t,r(w) := \u03c6It(w) + \u03b3\n2 \u2016w \u2212wt\u22121\u20162 +\n\u03ba 2 \u2016w \u2212 yr\u22121\u20162 (36)\nwith carefully chosen extrapolation points yr\u22121. At each DANE iteration, we perform two rounds of communication, one for averaging the local gradients, and one for averaging the local updates, and the amount of data we communicate per round has the same size of the predictor.\nTo sum up, in Algorithm 2, we have introduced two levels of inexactness. First, we only approximately solve the \u201clarge minibatch\u201d subproblem (12) in each outer loop; results from the previous section guarantee the convergence of this approach. Second, we only approximately solve the local subproblems (33) to sufficient accuracy in each inner loop; the analysis of \u201cinexact DANE\u201d (for the non-stochastic setting) provides guarantee for this approach (Reddi et al., 2016), and enables us to use state-of-the-art SGD methods (e.g., SVRG Johnson and Zhang, 2013; Xiao and Zhang, 2014) for solving local subproblems. Overall, we obtain a convergent algorithm for distributed stochastic convex optimization.\nWe now present detailed analysis for the computation/communication complexity of Algo-\nrithm 2 for stochastic quadratic problems, and compare it with related methods in the literature."}, {"heading": "D.1. Efficiency of MP-DANE", "text": "We present the main results of this section (full analysis is deferred to Appendix D.3), which show that with careful choices of the minibatch size and the desired accuracy in each level of approxi-\nAlgorithm 2MP-DANE for distributed stochastic convex optimization.\nInitialize w0. for t = 1, 2, . . . , T do\nEach machine i draws a minibatch I (i) t of b samples from the underlying data distribution. Initialize y0 \u2190 wt\u22121, x0 \u2190 wt\u22121. for r = 1, 2, . . . , R do\nInitialize z0 \u2190 yr\u22121, \u03b10 = \u221a \u03b3/(\u03b3 + \u03ba). for k = 1, 2, . . . ,K do 1. All machines perform one round of communication to compute the average gradient\n\u2207\u03c6It(zk\u22121) \u2190 1\nm\nm\u2211\ni=1\n\u2207\u03c6 I (i) t (zk\u22121).\n2. Each machine i approximately solves the local objective to \u03b8-accuracy:\napply prox-SVRG to find z (i) k s.t. \u2225\u2225\u2225z(i)k \u2212 z (i)\u2217 k \u2225\u2225\u2225 \u2264 \u03b8 \u2225\u2225\u2225zk\u22121 \u2212 z(i)\u2217k \u2225\u2225\u2225\nwhere z (i)\u2217\nk = argmin z\u2208\u2126 \u03c6 I (i) t (z) + \u2329 \u2207\u03c6It(zk\u22121)\u2212\u2207\u03c6I(i)t (zk\u22121), z \u232a + \u03b3 2 \u2016z\u2212wt\u22121\u20162\n+ \u03ba\n2 \u2016z\u2212 yr\u22121\u20162 . (33)\n3. All machines reach consensus by averaging local updates through another round of communication:\nzk \u2190 1\nm\nm\u2211\ni=1\nz (i) k . (34)\nend for Update xr \u2190 zK . Compute \u03b1r \u2208 (0, 1) such that \u03b12r = (1\u2212 \u03b1r)\u03b12r\u22121 + \u03b3\u03b1k/(\u03b3 + \u03ba), and compute\nyr = xr +\n( \u03b1r\u22121(1\u2212 \u03b1r\u22121)\n\u03b1r + \u03b1 2 r\u22121\n) (xr \u2212 xr\u22121). (35)\nend for\nUpdate wt \u2190 xr. end for\nOutput: wT is the approximate solution.\nmate solution, MP-DANE achieves both communication and computation efficiency with the optimal sample complexity. Interestingly, the choices of parameters differ in two regimes which are separated by an \u201coptimal\u201d minibatch size (also denoted as bmp-dane in the main text)\nb\u2217 = n(\u03b5)L2\n32m2\u03b22B2 log(md) .\nTheorem 14 (Efficiency of MP-DANE for b \u2264 b\u2217) Set the parameters in Algorithm 2 as follows:\n(outer loop) b \u2264 b\u2217 = n(\u03b5)L 2\n32m2\u03b22B2 log(md) , T =\nn(\u03b5)\nbm , \u03b3 =\n\u221a 8n(\u03b5)L\nbmB ,\n(intermediate loop) \u03ba = 0, R = 1,\n(inner loop) \u03b8 = 1\n6 , K = O (log n(\u03b5)) .\nThen we have E [ \u03c6 (\n1 T \u2211T t=1 wt ) \u2212 \u03c6(w\u2217) ] \u2264 \u221a 40BL\u221a n(\u03b5) = O (\u03b5) .\nMoreover, Algorithm 2 can be implemented with O\u0303 ( n(\u03b5) bm ) rounds of communication, and each\nmachine performs O\u0303 ( n(\u03b5) m ) vector operations in total, where the notation O\u0303(\u00b7) hides poly-logarithmic dependences on n(\u03b5).\nWhen we choose b = b\u2217, Algorithm 1 can be implemented with O\u0303 ( m\u03b22B2\nL2\n) rounds of commu-\nnication, O\u0303 ( n(\u03b5) bm ) vector operations, and O ( n(\u03b5)L2 m2\u03b22B2 ) memory for each machine.\nWe comment on the choice of parameters. For sample efficiency, we fix the sample size n(\u03b5) and number of machines m, and so we can tradeoff the local minibatch size b and the total number of outer iterations T , maintaining bT = n(\u03b5)m . For any b, the regularization parameters in the \u201clarge\nminibatch\u201d problem is set to \u03b3 = \u221a\n8T bm \u00b7 LB =\n\u221a 8n(\u03b5)L\nbmB according to Theorem 7. When b \u2264 b\u2217, we note that (37) can be satisfied with \u03ba = 0 and there is no need for acceleration by AIDE (R = 1). Then the values of \u03b8 and K follow from Lemma 18.\nRemark 15 The above theorem suggests that in the regime of b \u2264 b\u2217, we only need to have logarithmic number of DANE iterations for solving each \u201clarge minibatch\u201d problem, and logarithmic number of passes over the local data during each DANE iteration. We present experimental results validating our theory in Appendix E.\nThe next theorem shows that when we use a large minibatch size b in Algorithm 2, we can still satisfy the condition (37) by adding extra regularization (\u03ba > 0), and then apply accelerated DANE.\nTheorem 16 (Efficiency of MP-DANE for b \u2265 b\u2217) Set the parameters in Algorithm 2 as follows:\n(outer loop) b \u2265 b\u2217 = n(\u03b5)L 2\n32m2\u03b22B2 log(md) , T =\nn(\u03b5)\nbm , \u03b3 =\n\u221a 8n(\u03b5)L\nbmB ,\n(intermediate loop) \u03ba = 16\u03b2\n\u221a log(dm)\nb \u2212 \u03b3, R = O\n( b1/4m1/2 \u00b7 \u03b21/2B1/2 n(\u03b5)1/4 \u00b7 L1/2 log n(\u03b5) ) ,\n(inner loop) \u03b8 = 1\n6 , K = O (log n(\u03b5)) .\nThen we have E [ \u03c6 (\n1 T \u2211T t=1 wt ) \u2212 \u03c6(w\u2217) ] \u2264 \u221a 40BL\u221a n(\u03b5) = O (\u03b5) .\nMoreover, Algorithm 2 can be implemented with O\u0303 ( n(\u03b5)3/4\u00b7\u03b21/2B1/2 b3/4m1/2\u00b7L1/2 ) rounds of communication,\nand each machine performs O\u0303 ( b1/4n(\u03b5)3/4\u00b7\u03b21/2B1/2 m1/2\u00b7L1/2 ) vector operations in total, where the notation O\u0303(\u00b7) hides poly-logarithmic dependences on n(\u03b5).\nD.2. Two regimes of multiple resource tradeoffs\nFrom the above analysis, we summarized in Table 2 the resources required by MP-DANE. We observe two interesting regimes, separated by the minibatch size b\u2217 \u224d n(\u03b5)/(m2B2), that present different tradeoffs between communication, computation and memory.\n\u2022 When 1 \u2264 b \u2264 b\u2217, the computation complexity remains O\u0303 (n(\u03b5)/m) which is independent of b. This means we always achieve near-linear speedup in this regime. Moreover, there is a tradeoff between communication and memory: the communication complexity decreases,\nwhile the memory cost increases as the minibatch size b increases, both at the linear rate. Thus in this regime, we can trade communication for memory without affecting computation.\n\u2022 When b\u2217 < b \u2264 bmax, the computation starts to increase with b at the rate b1/4 which is slower than linear, while the communication cost continues to decrease at the rate b3/4 which is also slower than linear. Thus in this regime, we can trade communication for computation\nand memory."}, {"heading": "D.3. Analysis of MP-DANE", "text": "In order to fully analyze Algorithm 2, we need several auxiliary lemmas that characterize the iteration complexity of solving the local problem (33) by prox-SVRG (Xiao and Zhang, 2014), the large minibatch problem (12) by DANE (Shamir et al., 2014) and AIDE (Reddi et al., 2016)."}, {"heading": "D.3.1. SOME AUXILIARY LEMMAS", "text": "First, we apply prox-SVRG to the local problem (33), pushing all terms but \u03c6 I (i) t (z) in to the proximal operator. The benefit of this approach (as opposed to using plain SVRG Johnson and Zhang, 2013) is that the smoothness parameter that determines the iteration complexity is simply \u03b2, same results hold when applying prox-SAGA (Defazio et al., 2014) as well. For sampling without replacement SVRG, the current analysis works only for plain SVRG, so we quote the results from (Shamir, 2016).\nLemma 17 (Iteration complexity of SVRG for (33)) For any target accuracy \u03b8 > 0, with initialization zk\u22121, prox-SVRG outputs z (i) k such that \u2225\u2225\u2225z(i)k \u2212 z (i)\u2217 k \u2225\u2225\u2225 \u2264 \u03b8 \u2225\u2225\u2225zk\u22121 \u2212 z(i)\u2217k \u2225\u2225\u2225 after\nO (( b+ \u03b2\n\u03b3 + \u03ba\n) \u00b7 log (\u03b2 + \u03b3 + \u03ba)\n(\u03b3 + \u03ba)\u03b82\n)\nvector operations, and sampling without replacement SVRG outputs z (i) k such that \u2225\u2225\u2225z(i)k \u2212 z (i)\u2217 k \u2225\u2225\u2225 \u2264 \u03b8 \u2225\u2225\u2225zk\u22121 \u2212 z(i)\u2217k \u2225\u2225\u2225 after\nO (( b+ \u03b2 + \u03ba\n\u03b3 + \u03ba\n) \u00b7 log (\u03b2 + \u03b3 + \u03ba)\n(\u03b3 + \u03ba)\u03b82\n)\nvector operations.\nProof Observe that the objective (33) by f (i) k (z), which is an quadratic function of z with the Hessian matrix Hi = \u22072\u03c6I(i)t (z) + (\u03b3 + \u03ba)I (\u03b3 + \u03ba)I. As a result, the suboptimality of z (i) k is\n\u01ebfinal = f (i) k (z (i) k )\u2212 f (i) k (z (i)\u2217 k ) =\n1\n2\n( z (i) k \u2212 z (i)\u2217 k )\u22a4 Hi ( z (i) k \u2212 z (i)\u2217 k ) \u2265 \u03b3 + \u03ba\n2\n\u2225\u2225\u2225z(i)k \u2212 z (i)\u2217 k \u2225\u2225\u2225 2 .\nTo satisfy the requirement of \u2225\u2225\u2225z(i)k \u2212 z (i)\u2217 k \u2225\u2225\u2225 \u2264 \u03b8 \u2225\u2225\u2225zk\u22121 \u2212 z(i)\u2217k \u2225\u2225\u2225, we require\n\u01ebfinal \u2264 (\u03b3 + \u03ba)\u03b82\n2\n\u2225\u2225\u2225zk\u22121 \u2212 z(i)\u2217k \u2225\u2225\u2225 2 .\nOn the other hand, when initializing from zk\u22121, the initial suboptimality is\n\u01ebinit = f (i) k (zk\u22121)\u2212 f (i) k (z (i)\u2217 k ) \u2264 \u03c3max(Hi)\n2\n\u2225\u2225\u2225zk\u22121 \u2212 z(i)\u2217k \u2225\u2225\u2225 2 \u2264 \u03b2 + \u03b3 + \u03ba\n2\n\u2225\u2225\u2225zk\u22121 \u2212 z(i)\u2217k \u2225\u2225\u2225 2 .\nTherefore, it suffices to have\n\u01ebinit \u01ebfinal = (\u03b2 + \u03b3 + \u03ba) (\u03b3 + \u03ba)\u03b82 .\nNoting that \u03c6 I (i) t (z) is the sum of b components, and each component is \u03b2-smooth while the overall function f (i) k is (\u03b3 + \u03ba)-strongly convex, the lemma follows directly from the convergence guarantee of prox-SVRG (Xiao and Zhang, 2014, Corollary 1), and sampling without replacement SVRG (Shamir, 2016, Theorem 4).\nNext, we state the convergence rates of \u201cinexact DANE\u201d and AIDE, which can be easily derived from Reddi et al. (2016). At the outer loop t and intermediate loop r, let x\u2217r = argminw f\u0304t,r(w) be the exact minimizer of the \u201caugmented large minibatch\u201d problem (36), which is approximately solved by the inner DANE iterations.\nLemma 18 (Iteration Complexity of inexact DANE) Let \u03b8 = 16 , and assume that\nb(\u03b3 + \u03ba)2 \u2265 256\u03b22 log(dm/\u03b4). (37)"}, {"heading": "By initializing from yr\u22121, and setting the number of inner iterations in Algorithm 2 to be", "text": "K = \u23081 2 log4/3 (\u03b2 + \u03b3 + \u03ba) (\u03b3 + \u03ba)\u03b7 \u2309 ,\nwe have with probability 1\u2212 \u03b4 over the sample set It that\nf\u0304t,r(xr)\u2212 f\u0304t,r(x\u2217r) \u2264 \u03b7 ( f\u0304t,r(yr\u22121)\u2212 f\u0304t,r(x\u2217r) ) .\nProof Denote by Hi = \u22072\u03c6I(i)t (z) + (\u03b3 + \u03ba)I the Hessian matrix of the local objective (33) for machine i. Let H = 1m \u2211m i=1Hi be the Hessian matrix of the global objective (36), and\nH\u0303\u22121 = 1m \u2211m i=1 H \u22121 i . As our objective is quadratic, Hi,H, H\u0303\n\u22121 remain unchanged during the inner iterations. By Reddi et al. (2016, Theorem 1), we have\n\u2016zk \u2212 x\u2217r\u2016 \u2264 (\u2225\u2225\u2225H\u0303\u22121H \u2212 I \u2225\u2225\u2225+ \u03b8 m m\u2211\ni=1\n\u2225\u2225H\u22121i H \u2225\u2225 ) \u2016zk\u22121 \u2212 x\u2217r\u2016 . (38)\nSince\u22072\u2113(w, \u03be) \u2264 \u03b2, by Shamir et al. (2014, Lemma 2), we have with probability at least 1\u2212\u03b4 over the sample set It that\n\u2016Hi \u2212H\u2016 \u2264 \u221a 32\u03b22 log(dm/\u03b4)\nb =: \u03c1, i = 1, . . . ,m.\nOn the other hand, we have Hi (\u03b3 + \u03ba)I and\n4\u03c12\n(\u03b3 + \u03ba)2 =\n128\u03b22 log(dm/\u03b4)\nb(\u03b3 + \u03ba)2 \u2264 1 2\nby our assumption (37). By Shamir et al. (2014, Lemma 1), we have\n\u2225\u2225\u2225H\u0303\u22121H \u2212 I \u2225\u2225\u2225 \u2264 1\n2 . (39)\nMoreover, we have\n\u03b8\nm\nm\u2211\ni=1\n\u2225\u2225H\u22121i H \u2225\u2225 \u2264 \u03b8\nm\nm\u2211\ni=1\n(1 + \u2225\u2225H\u22121i H \u2212 I \u2225\u2225)\n\u2264 \u03b8 m\nm\u2211\ni=1\n(1 + \u2225\u2225H\u22121i \u2225\u2225 \u2225\u2225H \u2212H\u22121i \u2225\u2225)\n\u2264 \u03b8 m\nm\u2211\ni=1\n( 1 + \u03c1\n\u03b3 + \u03ba\n)\n\u2264 \u03b8 m\nm\u2211\ni=1\n( 1 + 1\n2 \u221a 2\n)\n\u2264 3\u03b8 2 \u2264 1 4 . (40)\nPlugging (39) and (40) into (38) yields\n\u2016zk \u2212 x\u2217r\u2016 \u2264 3 4 \u2016zk\u22121 \u2212 x\u2217r\u2016 ,\nand thus \u2016zK \u2212 x\u2217r\u2016 \u2264 (3/4)K \u2016yr\u22121 \u2212 x\u2217r\u2016. To guarantee the suboptimality in the objective f\u0304t,r(w), we note that\nf\u0304t,r(zK)\u2212 f\u0304t,r(x\u2217r) = 1 2 (zK \u2212 x\u2217r)\u22a4H(zK \u2212 x\u2217r) \u2264 \u03b2 + \u03b3 + \u03ba 2 \u2016zK \u2212 x\u2217r\u20162\n\u2264 ( 3\n4 )2K \u03b2 + \u03b3 + \u03ba 2 \u2016yr\u22121 \u2212 x\u2217r\u20162\n\u2264 ( 3\n4 )2K \u03b2 + \u03b3 + \u03ba \u03b3 + \u03ba ( f\u0304t,r(yr\u22121)\u2212 f\u0304t,r(x\u2217r) )\nwhere we have used the fact that ft,r(w) is (\u03b3 + \u03ba)-strongly convex in the last inequality. Setting( 3 4 )2K \u03b2+\u03b3+\u03ba \u03b3+\u03ba = \u03b7, and noting xr = zK , we obtain the desired iteration complexity.\nAt the outer iteration t of Algorithm 2, we are trying to approximately minimize the objective (12) by iteratively (approximately) solving R instances of the \u201caugmented\u201d problem (36). Let w\u2217t be the exact minimizer of the \u201clarge minibatch\u201d subproblem (12):\nw\u2217t = argmin w f\u0303t(w).\nThe following lemma characterizes the accelerated convergence rate.\nLemma 19 (Acceleration by universal catalyst, Theorem 3.1 of Lin et al. (2015)) Assume that for all r \u2265 1, we have\nf\u0304t,r(xr)\u2212 f\u0304t,r(x\u2217r) \u2264 2\n9\n( 1\u2212 9\n10\n\u221a \u03b3\n\u03b3 + \u03ba\n)R \u00b7 ( f\u0303t(x0)\u2212 f\u0303t(w\u2217t ) ) ,\nthen\nf\u0303t(xR)\u2212 f\u0303t(w\u2217t ) \u2264 800(\u03b3 + \u03ba)\n\u03b3\n( 1\u2212 9\n10\n\u221a \u03b3\n\u03b3 + \u03ba\n)R+1 ( f\u0303t(x0)\u2212 f\u0303t(w\u2217t ) ) ."}, {"heading": "D.3.2. PROOF OF THEOREM 14", "text": "Proof First of all, because R = 1, our algorithm collapses into two nested loops. On the one hand, as we choose \u03b3 as Theorem 7 suggested, we just need to verify the inexactness conditions in Theorem 7 is satisfied, i.e., for t = 1, . . . , T , we require (recall that w\u2217t = argminw f\u0303t(w))\nf\u0303t(wt)\u2212 f\u0303t(w\u2217t ) \u2264 1\n104 \u00b7min\n(( T\nbm\n)1/2 , ( T\nbm\n)3/2) \u00b7 2LB\nt3 .\nOn the other hand, we can bound the initial suboptimality f\u0303t(w) (cf. derivation for (31)):\nf\u0303t(w\u0303t\u22121)\u2212 f\u0303t(w\u2217t ) \u2264 L2/\u03b3.\nUsing Lemma 18, we know as long as the inequality (37) is satisfied, we have the desired\nsuboptimality in f\u0303t(w) using (cf. the derivation for (32))\nK = O (log n(\u03b5))\nrounds of communication, where we have plugged in the value of \u03b3 in the second step. It remains to verify the condition (37), by our choice of \u03b3 and b, we have\nb\u03b32 = 8n(\u03b5)L2 bm2B2 \u2265 8n(\u03b5)L 2 b\u2217m2B2 = 256\u03b22 log(md), (41)\nas desired.\nNext we summarize the communication, computation, and memory efficiency. Communication: the total rounds of communication required by Algorithm 2 is\nKRT = O ( n(\u03b5)\nmb log n(\u03b5)\n) .\nComputation: For each communication round, we need to solve the local problem (33) using prox-SVRG. Now, in view of (41), we have \u03b2 = O( \u221a b\u03b3). This implies that \u03b2\u03b3 = O( \u221a b) and thus by Lemma 17, the dominant term of the iteration complexity of prox-SVRG is\nO ( b log \u03b2 + \u03b3\n\u03b3\n) = O (b log n(\u03b5)) .\nMultiplying this with the number of communication rounds yields the desired computation complexity.\nMemory: It is straightforward to see each machine only need to maintain b samples."}, {"heading": "D.3.3. PROOF OF THEOREM 16", "text": "Proof First, it is straightforward to verify the condition (37):\nb(\u03b3 + \u03ba)2 = 256\u03b22 log(dm).\nSimilarly to Theorem 14, we need the ratio between final versus initial error for the R AIDE iterations to be\nratio = O(n(\u03b5)).\nEquating this ratio to be 800(\u03b3+\u03ba)\n\u03b3\n( 1\u2212 910 \u221a \u03b3 \u03b3+\u03ba )R+1 , we have\nR = 10\n9\n\u221a \u03b3 + \u03ba\n\u03b3 log\n( 800(\u03b3 + \u03ba)\n\u03b3 \u00b7 1 ratio\n)\n= O ( b1/4m1/2 \u00b7 \u03b21/2B1/2 n(\u03b5)1/4 \u00b7 L1/2 log n(\u03b5) ) .\nNow according to Lemma 19, the final suboptimality for f\u0304t,r(w) need to be\n\u01ebfinal = 2\n9\n( 1\u2212 9\n10\n\u221a \u03b3\n\u03b3 + \u03ba\n)R \u00b7 ( f\u0303t(x0)\u2212 f\u0303t(w\u2217t ) ) .\nLet us initialize minw f\u0304t,r(w) by x0. By definition, we have f\u0304t,r(w) \u2265 f\u0303t(w) and thus\n\u01ebinit = f\u0304t,r(x0)\u2212 f\u0304t,r(x\u2217r) \u2264 f\u0303t(x0)\u2212 f\u0303t(xr\u2217) \u2264 f\u0303t(x0)\u2212 f\u0303t(w\u2217t )\nwhere we have used the fact that w\u2217t is the minimizer of f\u0303t(w) in the second inequality. This means we only need the initial versus final suboptimality of solving f\u0304t,r(w) to be\n1 \u03b7 = \u01ebinit \u01ebfinal = 9 2\n( 1\u2212 9\n10\n\u221a \u03b3\n\u03b3 + \u03ba\n)\u2212R ,\nwhich, according to Lemma 18, is achieved by inexact DANE with\nK = O ( log 1\n\u03b7 + log\n\u03b2 + \u03b3 + \u03ba\n\u03b3 + \u03ba\n)\n= O ( R \u221a \u03b3\n\u03b3 + \u03ba\n)\n= O (log n(\u03b5)) .\niterations.\nNext we analyze the communication and computation efficiency of our algorithm. Communication: The total rounds of communication is\nKRT = O ( log n(\u03b5) \u00b7 b\n1/4m1/2 \u00b7 \u03b21/2B1/2 n(\u03b5)1/4 \u00b7 L1/2 log n(\u03b5) \u00b7 n(\u03b5) bm\n)\n= O ( n(\u03b5)3/4 \u00b7 \u03b21/2B1/2 b3/4m1/2 \u00b7 L1/2 log 2 n(\u03b5) ) .\nComputation: Similar to the case of b \u2264 b\u2217, for each DANE local subproblem (33), the sample size b is larger than its condition number. Therefore, the total computational cost is\nO(bKRT ) = O ( b1/4n(\u03b5)3/4 \u00b7 \u03b21/2B1/2\nm1/2 \u00b7 L1/2 log 2 n(\u03b5)\n) ."}, {"heading": "Appendix E. Experiments", "text": "In this section we present empirical results to support our theoretical analysis of MP-DANE. We perform least squares regression and classification on several publicly available datasets6; the statistics of these datasets and the corresponding losses are summarized in Table 3. For each dataset, we\n6. https://www.csie.ntu.edu.tw/\u02dccjlin/libsvm/\nrandomly select half of the samples for training, and the remaining samples are used for estimating the stochastic objective.\nFor MP-DANE, we use SAGA (Defazio et al., 2014) to solve each local DANE subproblem (33) and fix the number of SAGA steps to b (i.e., we just make one pass over the local data), while varying the number of DANE rounds K over {1, 2, 4, 8, 16}. For simplicity, we do not use catalyst acceleration and set R = 1 and \u03ba = 0 in all experiments. Our experiments simulate a distributed environment with m machines, for m = 4, 8, 16. We conduct a simple comparison with minibatch SGD. Stepsizes for SAGA and minibatch SGD are set based on the smoothness parameter of the loss.\nWe plot in Figure 3 the estimated population objective vs. minibatch size b for different parameters. We make the following observations.\n\u2022 For minibatch SGD, as b increases, the objective often increases quickly, this is because minibatch SGD can not uses large minibatch sizes while preserving sample efficiency.\n\u2022 For MP-DANE, the objective increases much more slowly as b increases. This demonstrates the effectiveness of minibatch-prox for using large minibatch sizes.\n\u2022 Running more iterations of DANE often helps, but with diminishing returns. This validates our theory that only a near-constant number of DANE iterations is needed for solving the\nlarge minibatch objective, without affecting the sample efficiency.\nm = 4 m = 8 m = 16"}], "references": [{"title": "Incremental proximal methods for large scale convex optimization", "author": ["Dimitri P. Bertsekas"], "venue": "Mathematical programming,", "citeRegEx": "Bertsekas.,? \\Q2011\\E", "shortCiteRegEx": "Bertsekas.", "year": 2011}, {"title": "Incremental aggregated proximal and augmented Lagrangian algorithms", "author": ["Dimitri P. Bertsekas"], "venue": "[cs.SY], November", "citeRegEx": "Bertsekas.,? \\Q2015\\E", "shortCiteRegEx": "Bertsekas.", "year": 2015}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Boyd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "Implicit online learning with kernels", "author": ["Li Cheng", "S.V.N. Vishwanathan", "Dale Schuurmans", "Shaojun Wang", "Terry Caelli"], "venue": "In Proceedings of the 19th International Conference on Neural Information Processing Systems,", "citeRegEx": "Cheng et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2006}, {"title": "Better mini-batch algorithms via accelerated gradient methods", "author": ["Andrew Cotter", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Cotter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cotter et al\\.", "year": 2011}, {"title": "Online passive-aggressive algorithms", "author": ["Koby Crammer", "Ofer Dekel", "Joseph Keshet", "Shai Shalev-Shwartz", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Crammer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2006}, {"title": "A simple practical accelerated method for finite sums", "author": ["Aaron Defazio"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Defazio.,? \\Q2016\\E", "shortCiteRegEx": "Defazio.", "year": 2016}, {"title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Defazio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Defazio et al\\.", "year": 2014}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dekel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2012}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Johnson and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2013}, {"title": "Implicit online learning", "author": ["Brian Kulis", "Peter L. Bartlett"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Kulis and Bartlett.,? \\Q2010\\E", "shortCiteRegEx": "Kulis and Bartlett.", "year": 2010}, {"title": "A simpler approach to obtaining an o(1/t) convergence rate for the projected stochastic subgradient method", "author": ["Simon Lacoste-Julien", "Mark Schmidt", "Francis Bach"], "venue": "[cs.LG],", "citeRegEx": "Lacoste.Julien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lacoste.Julien et al\\.", "year": 2012}, {"title": "Distributed stochastic variance reduced gradient methods and a lower bound for communication complexity", "author": ["Jason D Lee", "Qihang Lin", "Tengyu Ma", "Tianbao Yang"], "venue": "arXiv preprint arXiv:1507.07595,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Efficient mini-batch training for stochastic optimization", "author": ["Mu Li", "Tong Zhang", "Yuqiang Chen", "Alexander J. Smola"], "venue": "In Proc. of the 20th ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining (SIGKDD", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "A universal catalyst for first-order optimization", "author": ["Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Paved with good intentions: Analysis of a randomized block kaczmarz method", "author": ["Deanna Needell", "Joel A. Tropp"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Needell and Tropp.,? \\Q2014\\E", "shortCiteRegEx": "Needell and Tropp.", "year": 2014}, {"title": "Problem complexity and method efficiency", "author": ["A. Nemirovskii", "D.B. Yudin"], "venue": "in optimization,", "citeRegEx": "Nemirovskii and Yudin.,? \\Q1983\\E", "shortCiteRegEx": "Nemirovskii and Yudin.", "year": 1983}, {"title": "Aide: Fast and communication efficient distributed optimization", "author": ["Sashank J Reddi", "Jakub Kone\u010dn\u1ef3", "Peter Richt\u00e1rik", "Barnab\u00e1s P\u00f3cz\u00f3s", "Alex Smola"], "venue": "arXiv preprint arXiv:1608.06879,", "citeRegEx": "Reddi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reddi et al\\.", "year": 2016}, {"title": "Convergence rates of inexact proximal-gradient methods for convex optimization", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Schmidt et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2011}, {"title": "Stochastic convex optimization", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "In Proc. of the 22th Annual Conference on Learning Theory (COLT\u201909),", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2009}, {"title": "Without-replacement sampling for stochastic gradient methods: Convergence results and application to distributed optimization", "author": ["Ohad Shamir"], "venue": "arXiv preprint arXiv:1603.00570,", "citeRegEx": "Shamir.,? \\Q2016\\E", "shortCiteRegEx": "Shamir.", "year": 2016}, {"title": "Distributed stochastic optimization and learning", "author": ["Ohad Shamir", "Nathan Srebro"], "venue": "In 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton),", "citeRegEx": "Shamir and Srebro.,? \\Q2014\\E", "shortCiteRegEx": "Shamir and Srebro.", "year": 2014}, {"title": "Communication-efficient distributed optimization using an approximate Newton-type method", "author": ["Ohad Shamir", "Nathan Srebro", "Tong Zhang"], "venue": "In Proc. of the 31st Int. Conf. Machine Learning (ICML", "citeRegEx": "Shamir et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2014}, {"title": "Analysis of MP-DANE In order to fully analyze Algorithm 2, we need several auxiliary lemmas that characterize the iteration complexity of solving the local problem (33) by prox-SVRG (Xiao and Zhang, 2014), the large minibatch problem (12) by DANE (Shamir et al., 2014) and AIDE (Reddi", "author": ["memory. D"], "venue": null, "citeRegEx": "D.3.,? \\Q2016\\E", "shortCiteRegEx": "D.3.", "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "Introduction Consider the stochastic convex optimization (generalized learning) problem (Nemirovskii and Yudin, 1983; Vapnik, 1995; Shalev-Shwartz et al., 2009):", "startOffset": 88, "endOffset": 160}, {"referenceID": 19, "context": "Introduction Consider the stochastic convex optimization (generalized learning) problem (Nemirovskii and Yudin, 1983; Vapnik, 1995; Shalev-Shwartz et al., 2009):", "startOffset": 88, "endOffset": 160}, {"referenceID": 4, "context": "One simple approach for distributed stochastic optimization is minibatch SGD (Cotter et al., 2011; Dekel et al., 2012), where in each update we use a gradient estimate based on mb examples: b examples from each of the m machines.", "startOffset": 77, "endOffset": 118}, {"referenceID": 8, "context": "One simple approach for distributed stochastic optimization is minibatch SGD (Cotter et al., 2011; Dekel et al., 2012), where in each update we use a gradient estimate based on mb examples: b examples from each of the m machines.", "startOffset": 77, "endOffset": 118}, {"referenceID": 2, "context": "A naive approach here is to use accelerate gradient descent, distributing the gradient computations, but this, as well as approaches based on ADMM (Boyd et al., 2011), are dominated by minibatch SGD (Shamir and Srebro 2014 and see also Table 1).", "startOffset": 147, "endOffset": 166}, {"referenceID": 22, "context": "Better alternatives take advantage of the stochastic nature of the problem: DANE (Shamir et al., 2014) requires only O(B2m) rounds of communication for squared loss problems, while DiSCO (Zhang and Lin, 2015) and AIDE (Reddi et al.", "startOffset": 81, "endOffset": 102}, {"referenceID": 17, "context": ", 2014) requires only O(B2m) rounds of communication for squared loss problems, while DiSCO (Zhang and Lin, 2015) and AIDE (Reddi et al., 2016)) reduce this further to O(B1/2m1/4) rounds of communication.", "startOffset": 123, "endOffset": 143}, {"referenceID": 3, "context": "One simple approach for distributed stochastic optimization is minibatch SGD (Cotter et al., 2011; Dekel et al., 2012), where in each update we use a gradient estimate based on mb examples: b examples from each of the m machines. Distributed minibatch SGD attains optimal statistical performance with O (n(\u03b5)/m) runtime, as long as the minibatch size is not too large: Dekel et al. (2012) showed that the minibatch size can be as large as bm = O( \u221a n(\u03b5)), and Cotter et al.", "startOffset": 78, "endOffset": 389}, {"referenceID": 3, "context": "One simple approach for distributed stochastic optimization is minibatch SGD (Cotter et al., 2011; Dekel et al., 2012), where in each update we use a gradient estimate based on mb examples: b examples from each of the m machines. Distributed minibatch SGD attains optimal statistical performance with O (n(\u03b5)/m) runtime, as long as the minibatch size is not too large: Dekel et al. (2012) showed that the minibatch size can be as large as bm = O( \u221a n(\u03b5)), and Cotter et al. (2011) showed that with acceleration this can be increased to bm = O(n(\u03b5)3/4).", "startOffset": 78, "endOffset": 481}, {"referenceID": 5, "context": "This can be viewed as a minibatch generalization to the passive-aggressive algorithm (Crammer et al., 2006) and has been considered in various contexts (Kulis and Bartlett, 2010; Toulis and Airoldi, 2014).", "startOffset": 85, "endOffset": 107}, {"referenceID": 10, "context": ", 2006) and has been considered in various contexts (Kulis and Bartlett, 2010; Toulis and Airoldi, 2014).", "startOffset": 52, "endOffset": 104}, {"referenceID": 5, "context": "This can be viewed as a minibatch generalization to the passive-aggressive algorithm (Crammer et al., 2006) and has been considered in various contexts (Kulis and Bartlett, 2010; Toulis and Airoldi, 2014). We show that such an approach achieves the optimal statistical rate in terms of the number of samples used independent of the number of iterations, i.e. with anyminibatch size. This significantly improves over the previous analysis of Li et al. (2014), as the guarantee is better, it entirely avoid the dependence on the minibatch size and does not rely on additional assumptions as in Li et al.", "startOffset": 86, "endOffset": 458}, {"referenceID": 5, "context": "This can be viewed as a minibatch generalization to the passive-aggressive algorithm (Crammer et al., 2006) and has been considered in various contexts (Kulis and Bartlett, 2010; Toulis and Airoldi, 2014). We show that such an approach achieves the optimal statistical rate in terms of the number of samples used independent of the number of iterations, i.e. with anyminibatch size. This significantly improves over the previous analysis of Li et al. (2014), as the guarantee is better, it entirely avoid the dependence on the minibatch size and does not rely on additional assumptions as in Li et al. (2014). The guarantee holds for any Lipschitz (even non-smooth) objective.", "startOffset": 86, "endOffset": 609}, {"referenceID": 12, "context": "Distributed SVRG for stochastic convex optimization Recently, Lee et al. (2015) suggested using fast randomized optimization algorithms for finite-sums, and in particular the SVRG algorithm, as a distributed optimization approach for (2).", "startOffset": 62, "endOffset": 80}, {"referenceID": 12, "context": "Distributed SVRG for stochastic convex optimization Recently, Lee et al. (2015) suggested using fast randomized optimization algorithms for finite-sums, and in particular the SVRG algorithm, as a distributed optimization approach for (2). The authors noted that, for SVRG, when the the sample size n(\u03b5) dominates the problem\u2019s condition number \u03b2/\u03bd where \u03b2 is the smoothness parameter of l(w, \u03be), the time complexity is dominated by computing the batch gradients. This operation can be trivially parallelized. The stochastic updates, on the other hand, can be implemented on a single machine while the other machines wait, with the only caveat being that only sampling-without-replacement can be implemented this way. The use of without-replacement sampling was theoretically justified in a recent analysis by Shamir (2016).", "startOffset": 62, "endOffset": 823}, {"referenceID": 3, "context": "More general loss functions, still for \u201cbatch sizes\u201d of one, were also analyzed in the online learning setting (Cheng et al., 2006; Kulis and Bartlett, 2010).", "startOffset": 111, "endOffset": 157}, {"referenceID": 10, "context": "More general loss functions, still for \u201cbatch sizes\u201d of one, were also analyzed in the online learning setting (Cheng et al., 2006; Kulis and Bartlett, 2010).", "startOffset": 111, "endOffset": 157}, {"referenceID": 2, "context": "Crammer et al. (2006) proposed the \u201cpassive aggressive\u201d update rule, where a margin-based loss from a single example with a quadratic penalty is minimized\u2014this corresponds to (3) with a \u201cbatch size\u201d of one.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "For finite-sum objectives, methods based on incremental/stochastic proximal updates were studied by Bertsekas (2011, 2015); Defazio (2016). Needell and Tropp (2014) analyzed a randomized block Kaczmarz method in the context of solving linear systems, which also minimizes the empirical loss on a randomly sampled minibatch.", "startOffset": 100, "endOffset": 139}, {"referenceID": 0, "context": "For finite-sum objectives, methods based on incremental/stochastic proximal updates were studied by Bertsekas (2011, 2015); Defazio (2016). Needell and Tropp (2014) analyzed a randomized block Kaczmarz method in the context of solving linear systems, which also minimizes the empirical loss on a randomly sampled minibatch.", "startOffset": 100, "endOffset": 165}, {"referenceID": 0, "context": "For finite-sum objectives, methods based on incremental/stochastic proximal updates were studied by Bertsekas (2011, 2015); Defazio (2016). Needell and Tropp (2014) analyzed a randomized block Kaczmarz method in the context of solving linear systems, which also minimizes the empirical loss on a randomly sampled minibatch. To the best of our knowledge, no prior work has analyzed the general minibatch variant of proximal updates for stochastic optimization except Li et al. (2014). However, the analysis of Li et al.", "startOffset": 100, "endOffset": 483}, {"referenceID": 0, "context": "For finite-sum objectives, methods based on incremental/stochastic proximal updates were studied by Bertsekas (2011, 2015); Defazio (2016). Needell and Tropp (2014) analyzed a randomized block Kaczmarz method in the context of solving linear systems, which also minimizes the empirical loss on a randomly sampled minibatch. To the best of our knowledge, no prior work has analyzed the general minibatch variant of proximal updates for stochastic optimization except Li et al. (2014). However, the analysis of Li et al. (2014) assumes a stringent condition which is hard to verify (and is often violated) in practice, which we will discuss in detail in this section.", "startOffset": 100, "endOffset": 526}, {"referenceID": 0, "context": "For finite-sum objectives, methods based on incremental/stochastic proximal updates were studied by Bertsekas (2011, 2015); Defazio (2016). Needell and Tropp (2014) analyzed a randomized block Kaczmarz method in the context of solving linear systems, which also minimizes the empirical loss on a randomly sampled minibatch. To the best of our knowledge, no prior work has analyzed the general minibatch variant of proximal updates for stochastic optimization except Li et al. (2014). However, the analysis of Li et al. (2014) assumes a stringent condition which is hard to verify (and is often violated) in practice, which we will discuss in detail in this section. The following lemma provides the basic property of the update at each iteration. Lemma 1 For any w \u2208 \u03a9, we have \u03bb+ \u03b3t \u03b3t \u2016wt \u2212w\u2016 \u2264 \u2016wt\u22121 \u2212w\u2016 \u2212 \u2016wt\u22121 \u2212wt\u2016 \u2212 2 \u03b3t (\u03c6It(wt)\u2212 \u03c6It(w)) . (6) To derive the convergence guarantee, we need to relate \u03c6It(wt) to \u03c6(w). The analysis of Li et al. (2014) for minibatch-prox made the assumption that for all t \u2265 1: EIt [D\u03c6(wt;wt\u22121)] \u2264 EIt [ D\u03c6It (wt;wt\u22121) ] + \u03b3t 2 \u2016wt \u2212wt\u22121\u2016 , (7)", "startOffset": 100, "endOffset": 956}, {"referenceID": 13, "context": "However, to obtain the optimal convergence rate, Li et al. (2014) needed to set \u03b3t = O( \u221a T/b) which would imply b = O(T ) in order to have \u03b3t \u2265 \u03b2.", "startOffset": 49, "endOffset": 66}, {"referenceID": 13, "context": "However, to obtain the optimal convergence rate, Li et al. (2014) needed to set \u03b3t = O( \u221a T/b) which would imply b = O(T ) in order to have \u03b3t \u2265 \u03b2. In view of this implicit constraint that the minibatch size b can not be too large, the analysis of Li et al. (2014) does not really show advantage of minibatch-prox over minibatch SGD, whose optimal minibatch size is precisely b = O(T ).", "startOffset": 49, "endOffset": 265}, {"referenceID": 19, "context": "Using a stability argument (Shalev-Shwartz et al., 2009), we can establish the \u201cgeneralization\u201d performance for the (inexact) minimizer of the minibatch objective.", "startOffset": 27, "endOffset": 56}, {"referenceID": 13, "context": "In Li et al. (2014), the authors proposed a simple algorithm EMSO to approximately solve (12), where each machine first solve its own local objective, i.", "startOffset": 3, "endOffset": 20}, {"referenceID": 12, "context": "Here we instead use the distributed SVRG (DSVRG) algorithm (Lee et al., 2015; Shamir, 2016) to approximately solve (12), as DSVRG enjoys excellent communication and computation cost when the problem is well conditioned (cf.", "startOffset": 59, "endOffset": 91}, {"referenceID": 20, "context": "Here we instead use the distributed SVRG (DSVRG) algorithm (Lee et al., 2015; Shamir, 2016) to approximately solve (12), as DSVRG enjoys excellent communication and computation cost when the problem is well conditioned (cf.", "startOffset": 59, "endOffset": 91}, {"referenceID": 12, "context": "Although this approach was shown to work well empirically, no convergence guarantee for the original stochastic objective (1) was provided by Li et al. (2014). Here we instead use the distributed SVRG (DSVRG) algorithm (Lee et al.", "startOffset": 142, "endOffset": 159}, {"referenceID": 4, "context": "Figure 2: Illustration of theoretical guarantees for MP-DSVRG and the comparison with accelerated minibatch SGD (Cotter et al., 2011), DiSCO (Zhang and Lin, 2015), AIDE (Reddi et al.", "startOffset": 112, "endOffset": 133}, {"referenceID": 17, "context": ", 2011), DiSCO (Zhang and Lin, 2015), AIDE (Reddi et al., 2016), DSVRG (Lee et al.", "startOffset": 43, "endOffset": 63}, {"referenceID": 12, "context": ", 2016), DSVRG (Lee et al., 2015), and MP-DANE (proposed and analyzed in Appendix D).", "startOffset": 15, "endOffset": 33}, {"referenceID": 11, "context": "This choice is inspired by the stepsize rule of Lacoste-Julien et al. (2012) for stochastic gradient descent.", "startOffset": 48, "endOffset": 77}, {"referenceID": 18, "context": "To resolve the recursion, we need the following lemma by Schmidt et al. (2011).", "startOffset": 57, "endOffset": 79}, {"referenceID": 4, "context": "Following Cotter et al. (2011), we assume that l(w, \u03be) is \u03b2-smooth: \u2225\u2207l(w, \u03be)\u2212\u2207l(w\u2032, \u03be) \u2225\u2225 \u2264 \u03b2 \u2225w \u2212w\u2032 \u2225\u2225 , \u2200w,w\u2032 \u2208 \u03a9.", "startOffset": 10, "endOffset": 31}, {"referenceID": 4, "context": "Proof Our proof closely follows that of Cotter et al. (2011). Due to the smoothness of \u03c6, we have that \u03c6(w\u0303t) \u2264 \u03c6(w\u0303t\u22121) + \u3008\u2207\u03c6(w\u0303t\u22121), w\u0303t \u2212 w\u0303t\u22121\u3009+ \u03b2 2 \u2016w\u0303t \u2212 w\u0303t\u22121\u2016 \u2264 \u03c6(w\u0303t\u22121) + \u3008\u2207\u03c6(w\u0303t\u22121)\u2212\u2207\u03c6It(w\u0303t\u22121), w\u0303t \u2212 w\u0303t\u22121\u3009+ \u03b2 2 \u2016w\u0303t \u2212 w\u0303t\u22121\u2016 + \u3008\u2207\u03c6It(w\u0303t\u22121), w\u0303t \u2212 w\u0303t\u22121\u3009 = \u03c6(w\u0303t\u22121) + \u2016\u2207\u03c6(w\u0303t\u22121)\u2212\u2207\u03c6It(w\u0303t\u22121)\u2016 \u00b7 \u2016w\u0303t \u2212 w\u0303t\u22121\u2016+ \u03b2 2 \u2016w\u0303t \u2212 w\u0303t\u22121\u2016 + \u3008\u2207\u03c6It(w\u0303t\u22121), w\u0303t \u2212 w\u0303t\u22121\u3009 \u2264 \u03c6(w\u0303t\u22121) + 1 2(\u03b3t \u2212 \u03b2) \u2016\u2207\u03c6(w\u0303t\u22121)\u2212\u2207\u03c6It(w\u0303t\u22121)\u2016 + \u03b3t \u2212 \u03b2 2 \u2016w\u0303t \u2212 w\u0303t\u22121\u2016", "startOffset": 40, "endOffset": 61}, {"referenceID": 22, "context": "Here we present a novel method that use the distributed optimization algorithm DANE (Shamir et al., 2014) and its accelerated variant AIDE (Reddi et al.", "startOffset": 84, "endOffset": 105}, {"referenceID": 17, "context": ", 2014) and its accelerated variant AIDE (Reddi et al., 2016) for solving (12), which define better local objectives than EMSO and take into consideration the similarity between local objectives.", "startOffset": 41, "endOffset": 61}, {"referenceID": 14, "context": "On top of that, AIDE uses the idea of universal catalyst (Lin et al., 2015) and adds an extra quadratic term to improve the strong-convexity of the objective for faster convergence, i.", "startOffset": 57, "endOffset": 75}, {"referenceID": 17, "context": "Second, we only approximately solve the local subproblems (33) to sufficient accuracy in each inner loop; the analysis of \u201cinexact DANE\u201d (for the non-stochastic setting) provides guarantee for this approach (Reddi et al., 2016), and enables us to use state-of-the-art SGD methods (e.", "startOffset": 207, "endOffset": 227}, {"referenceID": 22, "context": "Analysis of MP-DANE In order to fully analyze Algorithm 2, we need several auxiliary lemmas that characterize the iteration complexity of solving the local problem (33) by prox-SVRG (Xiao and Zhang, 2014), the large minibatch problem (12) by DANE (Shamir et al., 2014) and AIDE (Reddi et al.", "startOffset": 247, "endOffset": 268}, {"referenceID": 17, "context": ", 2014) and AIDE (Reddi et al., 2016).", "startOffset": 17, "endOffset": 37}, {"referenceID": 7, "context": "The benefit of this approach (as opposed to using plain SVRG Johnson and Zhang, 2013) is that the smoothness parameter that determines the iteration complexity is simply \u03b2, same results hold when applying prox-SAGA (Defazio et al., 2014) as well.", "startOffset": 215, "endOffset": 237}, {"referenceID": 20, "context": "For sampling without replacement SVRG, the current analysis works only for plain SVRG, so we quote the results from (Shamir, 2016).", "startOffset": 116, "endOffset": 130}, {"referenceID": 17, "context": "Next, we state the convergence rates of \u201cinexact DANE\u201d and AIDE, which can be easily derived from Reddi et al. (2016). At the outer loop t and intermediate loop r, let xr = argminw f\u0304t,r(w) be the exact minimizer of the \u201caugmented large minibatch\u201d problem (36), which is approximately solved by the inner DANE iterations.", "startOffset": 98, "endOffset": 118}, {"referenceID": 14, "context": "1 of Lin et al. (2015)) Assume that for all r \u2265 1, we have f\u0304t,r(xr)\u2212 f\u0304t,r(xr) \u2264 2 9 ( 1\u2212 9 10 \u221a \u03b3 \u03b3 + \u03ba )R \u00b7 ( f\u0303t(x0)\u2212 f\u0303t(w t ) ) ,", "startOffset": 5, "endOffset": 23}, {"referenceID": 7, "context": "For MP-DANE, we use SAGA (Defazio et al., 2014) to solve each local DANE subproblem (33) and fix the number of SAGA steps to b (i.", "startOffset": 25, "endOffset": 47}], "year": 2017, "abstractText": "We present and analyze an approach for distributed stochastic optimization which is statistically optimal and achieves near-linear speedups (up to logarithmic factors). Our approach allows a communication-memory tradeoff, with either logarithmic communication but linear memory, or polynomial communication and a corresponding polynomial reduction in required memory. This communication-memory tradeoff is achieved throughminibatch-prox iterations (minibatch passiveaggressive updates), where a subproblem on a minibatch is solved at each iteration. We provide a novel analysis for such a minibatch-prox procedure which achieves the statistical optimal rate regardless of minibatch size and smoothness, thus significantly improving on prior work.", "creator": "LaTeX with hyperref package"}}}