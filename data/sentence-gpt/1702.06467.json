{"id": "1702.06467", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "Efficient Social Network Multilingual Classification using Character, POS n-grams and Dynamic Normalization", "abstract": "In this paper we describe a dynamic normalization process applied to social network multilingual documents (Facebook and Twitter) to improve the performance of the Author profiling task for short texts. After the normalization process, $n$-grams of characters and n-grams of POS tags are obtained to extract all the possible stylistic information encoded in the documents (emoticons, character flooding, capital letters, references to other users, hyperlinks, hashtags, etc.) from the public library of Facebook.", "histories": [["v1", "Tue, 21 Feb 2017 16:26:54 GMT  (146kb,D)", "http://arxiv.org/abs/1702.06467v1", "8 pages, 6 figures, Conference paper"]], "COMMENTS": "8 pages, 6 figures, Conference paper", "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.SI", "authors": ["carlos-emiliano gonz\\'alez-gallardo", "juan-manuel torres-moreno", "azucena montes rend\\'on", "gerardo sierra"], "accepted": false, "id": "1702.06467"}, "pdf": {"name": "1702.06467.pdf", "metadata": {"source": "CRF", "title": "Efficient Social Network Multilingual Classification using Character, POS n-grams and Dynamic Normalization", "authors": ["Juan-Manuel Torres-Moreno", "Azucena Montes Rend\u00f3n", "Gerardo Sierra"], "emails": ["carlos.gonzalez-gallardo@alumni.univ-avignon.fr,", "juan-manuel.torres@univ-avignon.fr,", "amr@cenidet.edu.mx,", "gsierram@iingen.unam.mx"], "sections": [{"heading": "1 INTRODUCTION", "text": "Social networks are a resource that millions of people use to support their relationships with friends and family (Chang et al., 2015). In June 2016, Facebook1 reported to have approximately 1.13 billion active users per day; while in June, Twitter2 reported an average of 313 million active users per month.\nDue to the large amount of information that is continuously produced in social networks, to identify certain individual characteristics of users has been a problem of growing importance for different areas like forensics, security and marketing (Rangel et al., 2015); problem that Author profiling aims from an automatic classification approach with the premise that depending of the individual characteristics of each person; such as age, gender or personality, the way of communicating will be different.\nAuthor profiling is traditionally applied in texts like literature, documentaries or essays (Argamon et al., 2003; Argamon et al., 2009); these kind of texts have a relative long length (Peersman et al., 2011) and a standard language. By the other hand, documents produced by social networks users have some characteristics that differ from regular texts and prevents that\n1Facebook website: http://www.facebook.com 2Twitter website: http://www.twitter.com\nthey can be analyzed in a similar way (Peersman et al., 2011). Some characteristics that social networks texts share are their length (significantly shorter that traditional texts) (Peersman et al., 2011), the large number of misspellings, the use of non-standard capitalization and punctuation.\nSocial networks like Twitter have their own rules and features that users use to express themselves and communicate with each other. It is possible to take advantage of these rules and extract a greater amount of stylistic information. (Gimpel et al., 2011) introduce this idea to create a Part of Speech (POS) tagger for Twitter. In our case, we chose to perform a dynamic context-dependent normalization. This normalization allows to group those elements that are capable of providing stylistic information regardless of its lexical variability; this phase helps to improve the performance of the classification process. In (Gonza\u0301lezGallardo et al., 2016) we presented some preliminary results of this work, that we will extend with more experiments and further description.\nThe paper is organized as follows: in section 2 we provide an overview of Author profiling applied to age, gender and personality traits prediction. In section 3, a brief presentation of character and POS ngrams is presented. In section 4 we detail the methodology used in the dynamic context-dependent normal-\nar X\niv :1\n70 2.\n06 46\n7v 1\n[ cs\n.I R\n] 2\n1 Fe\nb 20\nization. Section 5 presents the datasets used in the study. The learning model is detailed in section 6. The different experiments and results are presented in section 7. Finally, in section 8 we present the conclusions and some future job prospects."}, {"heading": "2 AGE, GENDER AND PERSONALITY TRAITS PREDICTION", "text": "(Koppel et al., 2002) performed a research with a corpus made of 566 documents of the British National Corpus (BNC)3 in which they identified automatically the gender of the author using function words and POS n-grams, obtaining a precision of 80% with a linear separator.\n(Doyle and Kes\u030celj, 2005) constructed a simple classifier to predict gender in a collection of 495 essays of the BAWE corpus4. The classifier consisted in measuring the distance between the text to classify and two different profiles (man and woman), being the smallest one the class that was assign to the text. A precision of 81% was reported using character, word and POS n-grams as features.\nIn (Peersman et al., 2011), the authors collected 1.5 millions of samples of the Netlog social network to predict the age and gender of the authors. They used a model created with support vector machines (SVM) (Vapnik, 1998) taking into account n-grams of words and characters as features. The reported precision was of 64.2%.\n(Carmona et al., 2015) predicted age, gender and personality traits of tweets authors using a high lever representation of features. This representation is composed of discriminatory features (Second Order Atribbutes) (Lopez-Monroy et al., 2013) and descriptive features (Latent Semantic Analysis) (WiemerHastings et al., 2004). For age and gender prediction in Spanish tweets they obtained a precision of 77.27% and for personality traits a RMSE value of 0.1297.\nIn (Grivas et al., 2015), a classification model with SVM was used to predict age, gender and personality traits of Spanish tweets while a SVR model was used to predict personality traits of tweets authors. In this approach two types of feature groups where considered: structural and stylometric. Structural features were extracted form the unprocessed tweets while stylometric features were obtained after html tags, urls, hashtags and references to other users are removed.\n3BNC website http://www.natcorp.ox.ac.uk/ 4BAWE corpus website: http://www2.warwick.ac.\nuk/fac/soc/al/research/collect/bawe/\nA precision of 72.73% was reported for age and gender prediction, while a RMSE value of 0.1495 was obtained for personality traits prediction."}, {"heading": "3 CHARACTER AND POS", "text": "N-GRAMS\nn-grams are sequences of elements of a selected textual information unit (Manning and Schu\u0308tze, 1999) that allow the extraction of content and stylistic features from text; features that can be used in tasks such as Automatic Summarization, Automatic Translation and Text Classification.\nThe information unit to use changes depending on the task and the type of features that will be extracted. For example, in Automatic Translation and Summarization is common to use word and phrase ngrams (Torres-Moreno, 2014; Giannakopoulos et al., 2008; Koehn, 2010). Within Text Classification, for Plagiarism Detection, Author Identification and Author Profiling; character, word and POS n-grams are used (Doyle and Kes\u030celj, 2005; Stamatatos et al., 2015; Oberreuter and Vela\u0301squez, 2013).\nThe information units selected in this research are characters and POS tags. With the character n-grams the goal is to extract as many stylistic elements as possible: characters frequency, use of suffixes (gender, number, tense, diminutives, superlatives, etc.), use of punctuation, use of emoticons, etc (Stamatatos, 2006; Stamatatos, 2009).\nPOS n-grams provide information about the structure of the text: frequency of grammatical elements, diversity of grammatical structures and the interaction between grammatical elements. The POS tags were obtained using the Freeling5 POS tagger, which follows the POS tags proposed by EAGLES6. To fully control the standardization process and make it independent of a detector of names, we preferred to perform a specific normalization for both datasets instead of using the Freeling functionalities (Padro\u0301 and Stanilovsky, 2012).\nThe POS tags provided by Freeling have several levels of detail that provide insight into the different attributes of a grammatical category depending of the language that is being analyzed. In our case we used only the first level of detail that refers to the category itself. The example in Table 1 shows the Spanish word \u201despecializacio\u0301n\u201d (specialization), all its at-\n5Freeling website: http://nlp.lsi.upc.edu/ freeling/node/1\n6EAGLES website: http://www.ilc.cnr.it/ EAGLES96/annotate/node9.html\ntributes and the selected tag."}, {"heading": "4 DYNAMIC CONTEXT-DEPENDENT NORMALIZATION", "text": "The freedom that social networks users have to encode their messages derives in a varied lexicon. To minimize this variations it is necessary to normalize those elements that are able to provide stylistic information regardless of its lexical variability (references to other users, hyperlinks, hashtags, emoticons, etc). This is what we call Dynamic Contextdependent Normalization which is separated in two phases: Text normalization and POS relabeling. \u2022 Text normalization\nThe goal of this phase is to avoid the lexical variability that is present when a user tends to do certain actions in their texts; for example tagging another user or creating a link to a website. In the Twitter case, references to other users are defined as\n@{username}\nThe amount of values that can be assigned to the label username is potentially infinite (depending on the number of users available to the social network). To avoid such variability, the Dynamic Context-dependent Normalization standardize this element in order to highlight the intention: make a reference to a user. So, the tweet\nI was just watching \u2018\u2018update 10.\u2019\u2019 @MKBHD http://t.co/P9Dn7t8zSl\nwill be normalized to I was just watching \u2018\u2018update 10.\u2019\u2019 @us http://t.co/P9Dn7t8zSl\nHyperlinks have a similar behavior: the number of links to Internet sites is also potentially infinite. The important fact is that a reference to an external site is being implemented; so all text strings that match the pattern:\nhttp[s]://{external_site}\nare normalized. Following with the previous example, the tweet\nI was just watching \u2018\u2018update 10.\u2019\u2019 @us http://t.co/P9Dn7t8zSl\nwill be normalized to\nI was just watching \u2018\u2018update 10.\u2019\u2019 @us htt\n\u2022 POS relabeling All these lexical variations provide important grammatical information that must be preserved, but conventional POS taggers are unable to maintain. Therefore, it is necessary to relabel certain elements to keep their interaction with the rest of the POS tags of the text. Using Freeling to tag the previous example and taking into account only the categories of the POS tag, the sequence obtained is\nP V R V N Z F N N\nAs it can be seen, the reference to the user and the link to the site is lost and it is no possible to know how those elements interact with the rest of the tags. To overcome this limitation, references to other users, hyperlinks and hashtags are relabeled so that they have their own special tag dealing to the next sequence:\nP V R V N Z REF@USERNAME REF#LINK\nFigure 1 shows a general architecture of the system."}, {"heading": "5 DATASETS", "text": "In order to test various contexts, we used corpora from two social networks: Twitter and Facebook.\nThe multilingual corpus PAN-CLEF2015 (Twitter) is labeled by gender, age and personality traits; whereas the multilingual corpus PAN-CLEF2016 (Twitter) is just labeled by gender and age. The Mexican Spanish corpus \u201cComments of Mexico City through time\u201d (Facebook) is only labeled by gender."}, {"heading": "5.1 PAN-CLEF2015", "text": "The PAN-CLEF2015 corpus7 (Rangel et al., 2005) is conformed by 324 samples distributed in four languages: Spanish, English, Italian and Dutch. A larger description of the corpus can be seen in (Gonza\u0301lezGallardo et al., 2016)."}, {"heading": "5.2 PAN-CLEF2016 s", "text": "The multilingual PAN-CLEF2016 s corpus is a set of the training corpus for the Author Profiling task of PAN 2016.8 It is conformed by 935 samples labeled by age and gender which are distributed in three languages: Spanish, English and Dutch. For our experiments, only the gender label was considered.\nTable 2 shows the distribution of gender samples per language."}, {"heading": "5.3 Comments of Mexico City through time (CCDMX)", "text": "The CCDMX corpus consists of 5 979 Mexican Spanish comments from the Facebook page Mexico City through time9. In this Facebook page, pictures of Mexico City are posted so that people can share their memories and anecdotes. The average length of each comment is 110 characters.\nThe CCDMX corpus was manually annotated by bachelor\u2019s linguistic students of the Group of Linguistic Engineering (GIL) of the UNAM in 201410. It is only labeled by gender, being slightly higher the number of comments that belong to the \u201cMen\u201d class (see table 3)."}, {"heading": "6 LEARNING MODEL", "text": "For the experiments we used support vector machines (SVM) (Vapnik, 1998), a classical model of supervised learning, which has proven to be robust and efficient in various NLP tasks.\nIn particular, to perform experiments we use the Python package Scikit-learn11(Pedregosa et al., 2011) using a linear kernel (LinearSVC), which produced empirically the best results."}, {"heading": "6.1 Features", "text": "The character and POS n-gram windows used were generated with a unit length of 1 to 3.\nThus, for example, the word \u201cself-defense\u201d is represented by the following character n-grams:\n{s, e, l, f, -, d, e, f, e, n, s, e, s, se, el, lf, f-, -d, de, ef, fe, en, ns, se, e , se, sel, elf, lf-, f-d, -de, def, efe, fen, ens, nse, se }\nAnd the normalize tweet \u201c@us @us You owe me one, Cam!\u201d which POS sequence is \u201cREF@USERNAME\n9Website of the blog: http://www.facebook.com/ laciudaddemexicoeneltiempo\n10Website of the corpus: http://corpus.unam.mx 11Scikit-learn is downloadable at the Website: http://\nscikit-learn.org\nREF@USERNAME N V P N F N F\u201d, is represented by the POS n-grams:\n{REF@USERNAME, REF@USERNAME, N, V, P,"}, {"heading": "N, F, N, F,REF@USERNAME REF@USERNAME, REF@USERNAME N, N V, V P, P N, N F, F", "text": ""}, {"heading": "N, N F, REF@USERNAME REF@USERNAME N, REF@USERNAME N V, N V P, V P N, P N F, N", "text": "F N, F N F}\nThe best results were obtained using a linear frequency scale in all cases except from the POS ngrams for Spanish texts, in which the logarithmic function\nlog2(1+number o f occurrences)\nis applied. In figure 2 is possible to see the linearized POS 2-grams values of the PAN-CLEF2015 training corpus."}, {"heading": "6.2 Experimental Protocol", "text": "Four experiments were performed with the PANCLEF2015 corpus; one for each language. 70% of the samples was used for training the learning model and the remaining 30% was used during evaluation time.\nRegard to the PAN-CLEF2016 s corpus,three experiments were performed; one for Dutch, one for English and another one for Spanish. We used the training model created with the PAN-CLEF2015 corpus and applied it to this corpus. All the samples available in the PAN-CLEF2016 s corpus were used as test samples for gender classification.\nThree experiments were performed with the CCDMX corpus.\n\u2022 First, all the comments were used as test samples using the learning model generated with the Spanish training samples of the PAN-CLEF2015 corpus.\n\u2022 For the second experiment, samples of 50 comments were created. Thus 121 samples were tested using the same learning model of the first experiment.\n\u2022 Finally, the third experiment was a sum of various micro experiments. For each micro experiment a different sample size was tested: 1, 2, 4, 8, 16, 24, 32, 41, 50, 57 and 64 comments per sample. 70% of the samples were used to train the learning model and 30% to test it."}, {"heading": "7 RESULTS", "text": "In order to evaluate the performance of the model in all the datasets, a group of classical measures was implemented. Accuracy (A), Precision (P), Recall (R) and F-score (F) (Manning and Schu\u0308tze, 1999) were used to evaluate gender and age prediction. In relation to the personality traits prediction in the PANCLEF2015 corpus, the Root Mean Squared Error (RMSE) measure was used."}, {"heading": "7.1 PAN-CLEF2015 Corpus", "text": "Tables 4 to 7 show the results obtained from the PANCLEF2015 corpus for Spanish and English.\nEvaluation measures reports practically 100% in gender classification for Italian and Dutch (Gonza\u0301lezGallardo et al., 2016). This is probably because the number of existing samples for both languages was very small. We think it would be worth trying with a larger amount of data to validate the results in these two languages.\n\u2022 Spanish."}, {"heading": "7.2 PAN-CLEF2016 s Corpus", "text": "As it can be see in tables 8 to 10, the performance of the experiments is low in all three languages. We think this is caused by the difference that exists in the size of the samples used to train the model (PAN-CLEF2015 corpus) and the size of the PAN-CLEF2016 s corpus samples. Another possible reason is that the quantity of noise in the PANCLEF2016 s corpus was to much to be handled by the trained models; thus creating mistaken predictions.\n\u2022 Spanish.\n\u2022 English."}, {"heading": "7.3 CCDMX Corpus", "text": "The first experiment (E1) performed with this corpus aimed to discover how much impact had the difference between the training and test samples. The training phase was done with 70% of the samples from the PAN -CLEF2015 corpus (Spanish). Remember that a sample of this corpus is a group of about 100 tweets.\nTable 11 shows the results of the 5 979 samples that were tested.\nIn the second experiment (E2) we chose to generate samples of 50 comments, size that represent a reasonable compromise between number of samples and number of characters per sample (about 5K characters).\nA total of 121 samples were tested with the learning model of E1. The results are slightly better that E1 but the domain difference seems to affect greatly the system performance (Table 12).\nA third experiment (E3) was done with this corpus; eleven micro experiments with different number of comments per sample were performed: 1, 2, 4, 8, 16, 24, 32, 41, 50, 57, 64. This was done to measure the impact of the sample size variation in the performance of the algorithm.\nFigures 3 to 6 show the performance of E3 (accuracy, precision, recall and F-score). As it can be seen, something curious happens when the sample is of 50 comments length. At that point the general performance drops about 8%. This is due to an statistical effect that the data shows with the residual sample. For both male and female, a residual sample of 22 comments is present. It is necessary to mention that this effect does not affect the general results of E3, which shows to be consistent with the rest of the experiments."}, {"heading": "8 CONCLUSIONS AND FUTURE WORK", "text": "Character and POS n-grams have shown to be a useful resource for the extraction of stylistic features in short texts. With character n-grams it was possible to\nextract emoticons, punctuation exaggeration (character flooding), use of capital letters and different kinds of emotional information encoded in tweets and comments. The interaction between the special elements of the social networks, like references to users, hashtags or links, and the rest of the POS tags were captured with the POS n-grams after. Also, for Spanish and English it was possible to capture the most representative series of two and three grammatical elements; for the Italian and Dutch we were able to capture the most common grammatical elements. The Dynamic Context-dependent Normalization showed to be effective in the different domains (Facebook and Twitter), but also showed to have problems when a cross domain evaluation was performed. The poor results obtained in E1 reflect that it is important to keep the ratio size between train and test samples. E2 results show that the change of domain greatly affects the prediction capacity of the system; fact that E3 and the results obtained with the PAN-CLEF2016 s corpus reinforce. A proposal for future work is to repeat E3 applying cross validation. This will show a better\ndistribution of the training and test samples eliminating the possibility of a biased result. Also, this probably balance the 50 comments samples and improves its performance. An interesting development for future work would be to apply the algorithm presented in this article to comments of multimedia sources to separate in an automatic way the different sectors of opinion makers."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This project was partially financed by the project: CONACyT-Me\u0301xico No. 215179 Caracterizacio\u0301n de huellas textuales para el ana\u0301lisis forense. We also appreciate the financing of the european project CHISTERA CALL - ANR: Access Multilingual Information opinionS (AMIS), (France - Europe)."}], "references": [{"title": "Gender, genre, and writing style in formal written texts", "author": ["S. Argamon", "M. Koppel", "J. Fine", "A.R. Shimoni"], "venue": "Text, 23(3):321\u2013346.", "citeRegEx": "Argamon et al\\.,? 2003", "shortCiteRegEx": "Argamon et al\\.", "year": 2003}, {"title": "Automatically profiling the author of an anonymous text", "author": ["S. Argamon", "M. Koppel", "J.W. Pennebaker", "J. Schler"], "venue": "Communications of the ACM, 52(2):119\u2013123.", "citeRegEx": "Argamon et al\\.,? 2009", "shortCiteRegEx": "Argamon et al\\.", "year": 2009}, {"title": "Inaoe\u2019s participation at pan\u201915: Author profiling task", "author": ["M.\u00c1.\u00c1. Carmona", "A.P. L\u00f3pez-Monroy", "M. Montes-yG\u00f3mez", "L.V. Pineda", "H.J. Escalante"], "venue": "Working Notes of CLEF 2015, Toulouse, France.", "citeRegEx": "Carmona et al\\.,? 2015", "shortCiteRegEx": "Carmona et al\\.", "year": 2015}, {"title": "Age differences in online social networking: Extending socioemotional selectivity theory to social network sites", "author": ["P.F. Chang", "Y.H. Choi", "N.N. Bazarova", "C.E. L\u00f6ckenhoff"], "venue": "Journal of Broadcasting & Electronic Media, 59(2):221\u2013239.", "citeRegEx": "Chang et al\\.,? 2015", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "Automatic categorization of author gender via n-gram analysis", "author": ["J. Doyle", "V. Ke\u0161elj"], "venue": "6th Symposium on Natural Language Processing, SNLP.", "citeRegEx": "Doyle and Ke\u0161elj,? 2005", "shortCiteRegEx": "Doyle and Ke\u0161elj", "year": 2005}, {"title": "Testing the use of n-gram graphs in summarization sub-tasks", "author": ["G. Giannakopoulos", "V. Karkaletsis", "G. Vouros"], "venue": "Text Analysis Conference (TAC).", "citeRegEx": "Giannakopoulos et al\\.,? 2008", "shortCiteRegEx": "Giannakopoulos et al\\.", "year": 2008}, {"title": "Part-of-speech tagging for twitter: Annotation, features, and experiments", "author": ["K. Gimpel", "N. Schneider", "B. O\u2019Connor", "D. Das", "D. Mills", "J. Eisenstein", "M. Heilman", "D. Yogatama", "J. Flanigan", "N.A. Smith"], "venue": "In 49th ACL HLT: Short Papers", "citeRegEx": "Gimpel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2011}, {"title": "Perfilado de autor multiling\u00fce en redes sociales a partir de n-gramas de caracteres y de etiquetas gramaticales", "author": ["Gonz\u00e1lez-Gallardo", "C.-E.", "Torres-Moreno", "J.-M.", "A. Montes Rend\u00f3n", "G. Sierra"], "venue": "Linguam\u00e1tica, 8(1):21\u201329.", "citeRegEx": "Gonz\u00e1lez.Gallardo et al\\.,? 2016", "shortCiteRegEx": "Gonz\u00e1lez.Gallardo et al\\.", "year": 2016}, {"title": "Author profiling using stylometric and structural", "author": ["A. Grivas", "A. Krithara", "G. Giannakopoulos"], "venue": null, "citeRegEx": "Grivas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grivas et al\\.", "year": 2015}, {"title": "Statistical Machine Translation", "author": ["P. Koehn"], "venue": "Cambridge University Press, NY, USA, 1st edition.", "citeRegEx": "Koehn,? 2010", "shortCiteRegEx": "Koehn", "year": 2010}, {"title": "Automatically categorizing written texts by author gender", "author": ["M. Koppel", "S. Argamon", "A.R. Shimoni"], "venue": "Literary and Linguistic Computing, 17(4):401\u2013412.", "citeRegEx": "Koppel et al\\.,? 2002", "shortCiteRegEx": "Koppel et al\\.", "year": 2002}, {"title": "Inaoe\u2019s participation at pan\u201913: Author profiling task", "author": ["A.P. Lopez-Monroy", "Gomez", "M.M.-y.", "H.J. Escalante", "L. Villasenor-Pineda", "E. Villatoro-Tello"], "venue": "CLEF 2013 Evaluation Labs and Workshop.", "citeRegEx": "Lopez.Monroy et al\\.,? 2013", "shortCiteRegEx": "Lopez.Monroy et al\\.", "year": 2013}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["C.D. Manning", "H. Sch\u00fctze"], "venue": "MIT Press, Cambridge.", "citeRegEx": "Manning and Sch\u00fctze,? 1999", "shortCiteRegEx": "Manning and Sch\u00fctze", "year": 1999}, {"title": "Text mining applied to plagiarism detection: The use of words for detecting deviations in the writing style", "author": ["G. Oberreuter", "J.D. Vel\u00e1squez"], "venue": "Expert Systems with Applications, 40(9):3756\u20133763.", "citeRegEx": "Oberreuter and Vel\u00e1squez,? 2013", "shortCiteRegEx": "Oberreuter and Vel\u00e1squez", "year": 2013}, {"title": "Scikit-learn: Machine Learning", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "\u00c9. Duchesnay"], "venue": null, "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Predicting age and gender in online social networks", "author": ["C. Peersman", "W. Daelemans", "L. Van Vaerenbergh"], "venue": "3rd Int. Workshop on Search and mining user-generated contents, pages 37\u201344. ACM.", "citeRegEx": "Peersman et al\\.,? 2011", "shortCiteRegEx": "Peersman et al\\.", "year": 2011}, {"title": "Overview of the 3rd Author Profiling Task at PAN 2015", "author": ["F. Rangel", "P. Rosso", "M. Potthast", "B. Stein", "W. Daelemans"], "venue": "CLEF 2015 Labs and Workshops, Notebook Papers. Cappellato L. and Ferro N. and Gareth J. and San Juan E. (Eds).: CEUR-WS.org.", "citeRegEx": "Rangel et al\\.,? 2005", "shortCiteRegEx": "Rangel et al\\.", "year": 2005}, {"title": "Overview of the 3rd author profiling task at pan 2015", "author": ["F. Rangel", "P. Rosso", "M. Potthast", "B. Stein", "W. Daelemans"], "venue": "CLEF.", "citeRegEx": "Rangel et al\\.,? 2015", "shortCiteRegEx": "Rangel et al\\.", "year": 2015}, {"title": "Ensemble-based Author Identification Using Character N-grams", "author": ["E. Stamatatos"], "venue": "3rd Int. Workshop on Text-based Information Retrieval, pages 41\u201346.", "citeRegEx": "Stamatatos,? 2006", "shortCiteRegEx": "Stamatatos", "year": 2006}, {"title": "A Survey of Modern Authorship Attribution Methods", "author": ["E. Stamatatos"], "venue": "American Society for information Science and Technology, 60(3):538\u2013556.", "citeRegEx": "Stamatatos,? 2009", "shortCiteRegEx": "Stamatatos", "year": 2009}, {"title": "Overview of the pan/clef 2015 evaluation lab", "author": ["E. Stamatatos", "M. Potthast", "F. Rangel", "P. Rosso", "B. Stein"], "venue": "Experimental IR Meets Multilinguality, Multimodality, and Interaction, pages 518\u2013538.", "citeRegEx": "Stamatatos et al\\.,? 2015", "shortCiteRegEx": "Stamatatos et al\\.", "year": 2015}, {"title": "Automatic Text Summarization", "author": ["Torres-Moreno", "J.-M."], "venue": "Wiley-Sons, London.", "citeRegEx": "Torres.Moreno and J..M.,? 2014", "shortCiteRegEx": "Torres.Moreno and J..M.", "year": 2014}, {"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "WileyInterscience, New York.", "citeRegEx": "Vapnik,? 1998", "shortCiteRegEx": "Vapnik", "year": 1998}, {"title": "Latent semantic analysis", "author": ["P. Wiemer-Hastings", "K. Wiemer-Hastings", "A. Graesser"], "venue": "16th Int. joint conference on Artificial intelligence, pages 1\u201314.", "citeRegEx": "Wiemer.Hastings et al\\.,? 2004", "shortCiteRegEx": "Wiemer.Hastings et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 3, "context": "Social networks are a resource that millions of people use to support their relationships with friends and family (Chang et al., 2015).", "startOffset": 114, "endOffset": 134}, {"referenceID": 17, "context": "Due to the large amount of information that is continuously produced in social networks, to identify certain individual characteristics of users has been a problem of growing importance for different areas like forensics, security and marketing (Rangel et al., 2015); problem that Author profiling aims from an automatic classification approach with the premise that depending of the individual characteristics of each person; such as age, gender or personality, the way of communicating will be different.", "startOffset": 245, "endOffset": 266}, {"referenceID": 0, "context": "Author profiling is traditionally applied in texts like literature, documentaries or essays (Argamon et al., 2003; Argamon et al., 2009); these kind of texts have a relative long length (Peersman et al.", "startOffset": 92, "endOffset": 136}, {"referenceID": 1, "context": "Author profiling is traditionally applied in texts like literature, documentaries or essays (Argamon et al., 2003; Argamon et al., 2009); these kind of texts have a relative long length (Peersman et al.", "startOffset": 92, "endOffset": 136}, {"referenceID": 15, "context": ", 2009); these kind of texts have a relative long length (Peersman et al., 2011) and a standard language.", "startOffset": 57, "endOffset": 80}, {"referenceID": 15, "context": "com they can be analyzed in a similar way (Peersman et al., 2011).", "startOffset": 42, "endOffset": 65}, {"referenceID": 15, "context": "Some characteristics that social networks texts share are their length (significantly shorter that traditional texts) (Peersman et al., 2011), the large number of misspellings, the use of non-standard capitalization and punctuation.", "startOffset": 118, "endOffset": 141}, {"referenceID": 6, "context": "(Gimpel et al., 2011) introduce this idea to create a Part of Speech (POS) tagger for Twitter.", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "(Koppel et al., 2002) performed a research with a corpus made of 566 documents of the British National Corpus (BNC)3 in which they identified automatically the gender of the author using function words and POS n-grams, obtaining a precision of 80% with a linear separator.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "(Doyle and Ke\u0161elj, 2005) constructed a simple classifier to predict gender in a collection of 495 essays of the BAWE corpus4.", "startOffset": 0, "endOffset": 24}, {"referenceID": 15, "context": "In (Peersman et al., 2011), the authors collected 1.", "startOffset": 3, "endOffset": 26}, {"referenceID": 22, "context": "They used a model created with support vector machines (SVM) (Vapnik, 1998) taking into account n-grams of words and characters as features.", "startOffset": 61, "endOffset": 75}, {"referenceID": 2, "context": "(Carmona et al., 2015) predicted age, gender and personality traits of tweets authors using a high lever representation of features.", "startOffset": 0, "endOffset": 22}, {"referenceID": 11, "context": "This representation is composed of discriminatory features (Second Order Atribbutes) (Lopez-Monroy et al., 2013) and descriptive features (Latent Semantic Analysis) (WiemerHastings et al.", "startOffset": 85, "endOffset": 112}, {"referenceID": 8, "context": "In (Grivas et al., 2015), a classification model with SVM was used to predict age, gender and personality traits of Spanish tweets while a SVR model was used to predict personality traits of tweets authors.", "startOffset": 3, "endOffset": 24}, {"referenceID": 12, "context": "n-grams are sequences of elements of a selected textual information unit (Manning and Sch\u00fctze, 1999) that allow the extraction of content and stylistic features from text; features that can be used in tasks such as Automatic Summarization, Automatic Translation and Text Classification.", "startOffset": 73, "endOffset": 100}, {"referenceID": 5, "context": "For example, in Automatic Translation and Summarization is common to use word and phrase ngrams (Torres-Moreno, 2014; Giannakopoulos et al., 2008; Koehn, 2010).", "startOffset": 96, "endOffset": 159}, {"referenceID": 9, "context": "For example, in Automatic Translation and Summarization is common to use word and phrase ngrams (Torres-Moreno, 2014; Giannakopoulos et al., 2008; Koehn, 2010).", "startOffset": 96, "endOffset": 159}, {"referenceID": 4, "context": "Within Text Classification, for Plagiarism Detection, Author Identification and Author Profiling; character, word and POS n-grams are used (Doyle and Ke\u0161elj, 2005; Stamatatos et al., 2015; Oberreuter and Vel\u00e1squez, 2013).", "startOffset": 139, "endOffset": 220}, {"referenceID": 20, "context": "Within Text Classification, for Plagiarism Detection, Author Identification and Author Profiling; character, word and POS n-grams are used (Doyle and Ke\u0161elj, 2005; Stamatatos et al., 2015; Oberreuter and Vel\u00e1squez, 2013).", "startOffset": 139, "endOffset": 220}, {"referenceID": 13, "context": "Within Text Classification, for Plagiarism Detection, Author Identification and Author Profiling; character, word and POS n-grams are used (Doyle and Ke\u0161elj, 2005; Stamatatos et al., 2015; Oberreuter and Vel\u00e1squez, 2013).", "startOffset": 139, "endOffset": 220}, {"referenceID": 18, "context": "), use of punctuation, use of emoticons, etc (Stamatatos, 2006; Stamatatos, 2009).", "startOffset": 45, "endOffset": 81}, {"referenceID": 19, "context": "), use of punctuation, use of emoticons, etc (Stamatatos, 2006; Stamatatos, 2009).", "startOffset": 45, "endOffset": 81}, {"referenceID": 16, "context": "The PAN-CLEF2015 corpus7 (Rangel et al., 2005) is conformed by 324 samples distributed in four languages: Spanish, English, Italian and Dutch.", "startOffset": 25, "endOffset": 46}, {"referenceID": 22, "context": "For the experiments we used support vector machines (SVM) (Vapnik, 1998), a classical model of supervised learning, which has proven to be robust and efficient in various NLP tasks.", "startOffset": 58, "endOffset": 72}, {"referenceID": 14, "context": "In particular, to perform experiments we use the Python package Scikit-learn11(Pedregosa et al., 2011) using a linear kernel (LinearSVC), which produced empirically the best results.", "startOffset": 78, "endOffset": 102}, {"referenceID": 12, "context": "Accuracy (A), Precision (P), Recall (R) and F-score (F) (Manning and Sch\u00fctze, 1999) were used to evaluate gender and age prediction.", "startOffset": 56, "endOffset": 83}], "year": 2017, "abstractText": "In this paper we describe a dynamic normalization process applied to social network multilingual documents (Facebook and Twitter) to improve the performance of the Author profiling task for short texts. After the normalization process, n-grams of characters and n-grams of POS tags are obtained to extract all the possible stylistic information encoded in the documents (emoticons, character flooding, capital letters, references to other users, hyperlinks, hashtags, etc.). Experiments with SVM showed up to 90% of performance.", "creator": "TeX"}}}