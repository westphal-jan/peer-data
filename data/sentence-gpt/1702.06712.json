{"id": "1702.06712", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Ensembles of Randomized Time Series Shapelets Provide Improved Accuracy while Reducing Computational Costs", "abstract": "Shapelets are discriminative time series subsequences that allow generation of interpretable classification models, which provide faster and generally better classification than the nearest neighbor approach. However, the shapelet discovery process requires the evaluation of all possible subsequences of all time series in the training set, making it extremely computation intensive. Consequently, shapelet discovery for large time series datasets quickly becomes intractable. However, such algorithms may be difficult to implement in large-scale datasets. For example, if you look at the distribution of length- and length- and length- and time-series-wise, you can use a simple approach in classification learning, which also includes a deep learning algorithm. However, this approach has several disadvantages, such as not having an in-depth knowledge of classifiers, and not having the ability to predict the order of time series with a single model. For example, the difficulty in using classification data without having access to any known data in the distribution model, while using classification data to create a new classifier, you cannot do this in the training set. The key for these problems is to avoid introducing the model to a model. The solution has the following features:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 22 Feb 2017 09:07:00 GMT  (933kb,D)", "http://arxiv.org/abs/1702.06712v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["atif raza", "stefan kramer"], "accepted": false, "id": "1702.06712"}, "pdf": {"name": "1702.06712.pdf", "metadata": {"source": "CRF", "title": "Ensembles of Randomized Time Series Shapelets Provide Improved Accuracy while Reducing Computational Costs", "authors": ["Atif Raza", "Stefan Kramer"], "emails": ["atifraza@uni-mainz.de", "kramer@informatik.uni-mainz.de"], "sections": [{"heading": null, "text": "We are proposing the use of ensembles of shapelet-based classifiers obtained using random sampling of the shapelet candidates. Using random sampling reduces the number of evaluated candidates and consequently the required computational cost, while the classification accuracy of the resulting models is also not significantly different than that of the exact algorithm. The combination of randomized classifiers rectifies the inaccuracies of individual models because of the diversity of the solutions. Based on the experiments performed, it is shown that the proposed approach of using an ensemble of inexpensive classifiers provides better classification accuracy compared to the exact method at a significantly lesser computational cost."}, {"heading": "1. INTRODUCTION", "text": "Time series data mining, in general, and classification, in particular, has seen a huge interest. The most investigated approach for time series classification has been the nearest neighbor algorithm coupled with various distance measures [1,3]. The nearest neighbor approach is simple to implement and requires little to no parameter tuning.1 However, it also has a few drawbacks. It requires the storage of the entire training set with instances belonging to all the classes and the time required for classification is directly proportional to the number of instances in the training set. It also fails to provide a clear insight about why a particular instance was classified as belonging to a certain class except that it was a \u201cbest match\u201d to some instance of the assigned class.\nTime series shapelets (YK-Shapelets) were introduced to ad-\n1The distance measure used may require parameter tuning; e.g. DTW requires a window parameter for optimal results.\ndress the drawbacks of nearest neighbor based time series classification [13]. In the nearest neighbor approach, fulllength time series are compared and an instance is classified as belonging to the class of the best matching instance, or nearest neighbor, from the training set. For time series shapelets, instead of comparing entire shapes, the presence of small subsequences unique to a particular class is sought. Therefore, a shapelet is the most representative subsequence occurring in the instances of a certain class and its presence in a new instance leads to the classification of that instance as belonging to that particular class. This is also the aspect which gives time series shapelets greater insight than the nearest neighbor approach because we can state that the time series in question was classified as such because of the presence (or absence) of a particular shapelet. For example, we can differentiate between normal and abnormal patterns of an ECG time series based on the presence of certain shapelets and hence, classify the ECG data for a healthy or unhealthy person and also identify the particular heart disease. The process of shapelet discovery can be divided into (1) the enumeration of subsequences of all possible lengths for all the instances in the training set, and (2) the evaluation of all the subsequences to find the one(s) capable of dividing the dataset as best as possible and preferably, provide subsets containing only the instances of a single class.\nShapelets based classification has many advantages over the nearest neighbor approach. It provides better insight into the classification process and the results can be verified by domain experts. It can provide a better understanding of the data and may even discover unknown artifacts, providing new information and contributing fresh knowledge. It is also much faster than the nearest neighbor approach because it only searches for a single subsequence in the incoming time series whereas a (full length) comparison with all training instances is required for the nearest neighbor approach. Finally, shapelets are local features, so they can be significantly more accurate for certain problems because time series classification using global features can be highly susceptible to noise and distortions.\nDespite its many advantages, the huge computational cost of shapelets based classification is a major drawback of this technique. For a dataset with k instances of length m, the number of all possible shapelet candidates of all lengths is 1 2 km(m + 1), which is on the order of O(km2). Evaluating each candidate requires its comparison with O(km) can-\nar X\niv :1\n70 2.\n06 71\n2v 1\n[ cs\n.L G\n] 2\n2 Fe\nb 20\ndidates on average and each comparison (using Euclidean distance) on average requires O(m) operations. Therefore, the brute force approach has a time complexity of O(k2m4). Clearly, the computational complexity of training a shapelets based classifier is untenable even for very small datasets. However, a number of techniques have been proposed to reduce the time complexity of the shapelet discovery process.\nEnsembles of machine learning models have been shown to often outperform individual models, provided the models constituting the ensemble are (1) accurate, i.e. provide better results than random guessing, and (2) diverse, i.e. make different errors for an unseen problem [5]. Ensembles enable the use of a number of \u201cinexpensive\u201d models instead of a single, expensive and highly accurate model. Therefore, we are proposing the use of ensembles of inexpensive shapeletbased classifiers. This approach can provide better classification accuracy as compared to the YK-Shapelets method at a reduced computational cost. Varying the number of classifiers in the ensemble provides a mechanism to obtain highly accurate or computationally less expensive models. The averaging behavior of ensembles also reduces the variance of the individual models."}, {"heading": "2. BACKGROUND", "text": "The vast amounts of time series data are a treasure trove of information waiting to be mined and explored for the hidden insights they can provide. Time series classification using the nearest neighbor approach is a very simple and highly effective technique that has been used extensively. However, the computational complexity of the classification phase along with little to no interpretability has lead to a subsequence or shapelets based classification approach. In shapelets based classification, the presence (or absence) of a specific subsequence, or \u201cshapelet\u201d, in a time series determines its class. If the distance of a time series from the shapelet is less than a threshold, then it is said to contain that shapelet and vice versa. We will summarize the shapelet discovery algorithm and some of the proposed speed-up techniques in sections 2.1 and 2.2. For a detailed introduction, we refer the readers to the respective papers [6, 9\u201311,13].\nA time series T = t1, t2, . . . , tm, is an ordered set of m realvalued features. A subsequence S of length l is a contiguous chunk of a time series, such that S = tp, tp+1, . . . , tp+l\u22121, for 1 \u2264 p \u2264 m \u2212 l + 1. For a time series T , the set of all subsequences of length l is given as SlT = {Sl1, Sl2, . . . , Slm\u2212l+1}, where the subscript denotes the starting position of the subsequence. The number of possible subsequences of length l in a time series of length m is equal to m\u2212 l+ 1. The number of subsequences in a dataset consisting of k instances of length m and possible shapelet candidate lengths in the range [min,max] is equal to:\nmax\u2211 l=min k\u2211 i=1 (m\u2212 l + 1)\nSince the shapelet discovery process is used to split the dataset into purer splits and create a decision tree based on shapelets and their corresponding distance thresholds, the number of candidates stated above is only for the first call\nAlgorithm 1 CreateTree (D, l, u)\nInputs: D: Time Series dataset, l/u: min/max length of shapelet candidates Result: Shapelets based decision tree DT 1: if IsPure(D) then 2: return CreateLeafNode(D) 3: else 4: (S, \u03b4, dmap)\u2190 FindShapelet(D, l, u) 5: (DL, DR)\u2190 SplitData(D, \u03b4, dmap) 6: nodeL \u2190 CreateTree(DL, l, u) 7: nodeR \u2190 CreateTree(DR, l, u) 8: return (S, \u03b4, nodeL, nodeR) 9: end if\nto the shapelet discovery process. Subsequent calls further increase the number of evaluated candidates, although the number of candidates decreases at each node.\nThe distance between two subsequences of length l is defined as dist(S,R) = \u2211l i=1(si\u2212ri)\n2, which is simply the Euclidean distance without the square root. The distance between a subsequence S and a time series T is defined as the minimum distance between S and all subsequences of T having length |S| i.e. dist(S, T ) = min(dist(S, S\u2032)), \u2200S\u2032 \u2208 S|S|T ."}, {"heading": "2.1 Shapelet Discovery", "text": "The shapelet discovery algorithm aims to split the dataset into \u201cpurer\u201d subsets such that the instances with and without the shapelet form two separate splits. The purity of the obtained splits is evaluated using the information gain measure although other approaches can be used as well [8]. The shapelet discovery process is embedded in a decision tree learner. At each node, the algorithm searches for the shapelet and split distance pair, which maximizes the information gain when used to split the dataset. This shapelet and split distance pair constitutes the decision criterion for the particular node and splits the dataset for subsequent nodes of the tree. The decision tree learner initiates the shapelet discovery process or forms leaf nodes with the splits depending on their purity levels. Algorithm 1 provides the basic algorithm for learning the classification model.\nAlgorithm 2 lists the brute force shapelet discovery process. It takes the time series dataset D and the parameters minLen and maxLen as inputs and loops through the length parameters to evaluate all possible candidates (Line 4). The GetNextCandidate procedure (Line 6) provides the next shapelet candidate for evaluation or returns an empty candidate signaling candidate exhaustion for the current length. It keeps track of the current instance number and the starting point for the shapelet candidate to sequentially generate new candidates using the current candidate length, removing the requirement of creating the candidates beforehand. In Lines 7 to 10, the distance between the current candidate and each time series instance in the dataset is obtained and an order line is created. The best information gain and the corresponding splitting distance are obtained for the order line (Line 11). If the new information gain is greater than the best so far, the values for best so far information gain, split distance, shapelet and order line are updated. After evaluating all candidates, the best-\nAlgorithm 2 FindShapelet (D, minLen, maxLen)\nInputs: D: Time Series Dataset, minLen: minimum candidate length, maxLen: maximum candidate length Result: S: Shapelet, \u03b4: split distance, dmap: distance line 1: bsf InfoGain\u2190 \u2212\u221e 2: bsf SplitDist\u2190 \u2212\u221e 3: bsf OrderLine\u2190 \u2205 4: for len = maxLen to minLen do 5: order line\u2190 \u2205 6: while (cand\u2190GetNextCandidate()) 6= \u2205 do 7: for i = 1 to |D| do 8: disti \u2190 sdist(Di, cand) 9: place disti on order line 10: end for 11: IG, SD \u2190 CheckCandidate(order line) 12: if IG > bsf InfoGain then 13: bsf InfoGain\u2190 IG 14: bsf SplitDist\u2190 SD 15: bsfShapelet\u2190 cand 16: bsf OrderLine\u2190 order line 17: end if 18: end while 19: end for 20: return bsfShapelet, bsf SplitDist, bsf OrderLine\nfound shapelet along with the corresponding split distance and order line are returned. The shapelet and split distance constitute the decision criterion of the node. The dataset is split using the split distance and order line for subsequent nodes. When a split reaches the required purity level, a leaf node is created with the majority class of the instances reaching that node, otherwise the shapelet discovery process is called for the new split."}, {"heading": "2.2 Speed-up Strategies", "text": "A number of techniques have been proposed to speed-up the shapelet discovery process. The authors of the YK-Shapelets algorithm proposed early abandoning distance calculations and early candidate pruning using an upper-bound on the information gain and reported a speed-up of three orders of magnitude compared to the brute force approach [13].\nThe Logical-Shapelets algorithm reduces computational costs by reusing previously calculated distances and pruning candidates using the triangular inequality [9]. It can reduce the computational complexity to O(k2m3), however, caching the computations imposes a memory requirement on the order of O(km2) which limits the use of this algorithm for large datasets in memory constrained environments.\nThe Fast-Shapelets algorithm reduces the dimensionality of the data using SAX [7] and then performs a random projection based shapelet discovery using this lower dimensional data [10]. It uses a heuristic approach and provides a huge reduction in computational costs but requires tuning a number of parameters according to the dataset characteristics or performance requirements.\nThe Random-Shapelets algorithm performs a uniform random sampling of candidates to reduce the number of evaluated candidates [11]. The YK-Shapelets algorithm gen-\nerates candidates with a unit step size giving an almost complete overlap to subsequent candidates while uniform sampling effectively increases the step size between subsequent candidates. Figure 1 shows an example of the first ten candidates generated using the YK-Shapelets and RandomShapelets algorithms. The candidates generated using the YK-Shapelets approach have a very high overlap and only cover a small section of the time series. The candidates generated using random sampling have less overlap and also cover a greater section of the time series. This reduces the number of prospective candidates but the classification accuracy does not deteriorate because the shape of the time series is still covered by the candidates. Using an n% sampling reduces the number of possible shapelet candidates to:\nn\n100 max\u2211 l=min k\u2211 i=1 (|Ti| \u2212 l + 1)\nThis expression provides a precise number of candidates, however, the actual number of evaluated candidates can slightly vary because of the random sampling process.\nA recently published approach called Generalized Random Shapelet Forests (gRSF) also employs ensembles and a randomized candidate sampling based shapelet discovery process for improved classification accuracy and reduced runtime [6]. Each ensemble member is trained using a set of instances sampled from the training set with replacement and performing shapelet discovery on this sampled data using a candidate sampling process strikingly similar to the Random-Shapelets approach. At each node, the gRSF algorithm samples a constant number of candidates proportional\nAlgorithm 3 Bagging Ensemble (D,N)\n1: M \u2190 \u2205 2: for n = 1 to N do 3: Dn \u2190 sample |D| instances from D with replacement 4: Mn \u2190 train classifier on Dn 5: M \u2190M \u222aMn 6: end for 7: return M\nto the time series length, while the Random-Shapelets approach samples a percentage of the candidates."}, {"heading": "2.3 Ensemble Methods", "text": "Ensemble methods are based on the idea of \u201ccombining\u201d the opinions of different \u201cexperts\u201d to obtain a decision about a given problem. Members of an ensemble can have a different view of the data, or they can use different features for making their decision, or they can be totally different algorithms. This provides diversity in the ensemble decisions, which often makes them more accurate than individual models. Several studies have shown that ensembles can often be more effective than individual machine learning models.\nDue to the variety of proposed combinations in the literature regarding ensembles, it is possible to experiment with a few options. A basic ensemble of classifiers can be obtained by combining multiple diverse models all trained using the same data. Another approach for constructing ensembles is that of Bagging [2] which trains N models, each with a different bootstrap of data such that |D| instances are sampled with replacement from the original dataset. This introduces a diversification effect and a duplication of instances also allows individual models to be focused on the duplicated instances. Algorithm 3 provides the pseudo-code for bagging. Another approach called Boosting [4] relies on weighted instances. All training instances are assigned equal weights so that the weights\u2019 sum equals one. A model is trained and a classification of training data using this model identifies the misclassified instances whose weights are increased. Next, the weights of all instances are normalized to keep the sum of weights equal to one. This, in turn, decreases the weights of correctly classified instances and provides emphasis on the misclassified instances in the next iteration and in many cases leads to an improved overall accuracy of the ensemble. Algorithm 4 provides the pseudo-code for boosting."}, {"heading": "3. PROPOSED METHOD", "text": "In this section we describe the proposed method, which is a combination of the Random-Shapelets algorithm and standard ensemble approaches. The Random-Shapelets algorithm is computationally less expensive than the YK-Shapelets algorithm, however, it generates models with slightly variable results. This variability makes the Random-Shapelets algorithm a diverse algorithm and can be used to our advantage. The lesser computational cost and inherent randomization of the Random-Shapelets algorithm makes it a prime candidate for incorporation in an ensemble. Therefore, we can use the Random-Shapelets algorithm as the base classifier in an ensemble learning approach for the time series classification problem. This combines the strengths of ensemble\nAlgorithm 4 Boosting Ensemble (D,N)\n1: M \u2190 \u2205 2: w1i \u2190 1|D| , \u2200xi \u2208 D 3: for n = 1 to N do 4: Mn \u2190 train classifier on D with wn 5: calculate weighted error t 6: if n \u2265 0.5 then 7: N \u2190 n\u2212 1 8: BREAK 9: else\n10: \u03b1n \u2190 12 \u00d7 ln 1\u2212 n n 11: wi(n+1) \u2190 win 2 n\n, \u2200 misclassified xi \u2208 D 12: wj(n+1) \u2190 w j n, \u2200 correctly classified xj \u2208 D 13: normalize w(n+1) 14: end if 15: M \u2190M \u222aMn 16: end for 17: return M(x) = \u2211N t=1 \u03b1nMn\nlearning with the efficient but non-exact shapelet discovery process of Random-Shapelets to provide a highly cost effective alternative to the exact YK-Shapelets approach. Another benefit of choosing Random-Shapelets as the base classifier is that it requires a single parameter, i.e. the sampling ratio, which allows to reduce the number of evaluated candidates and directly corresponds to the amount of computation we are willing to spend for finding the shapelets. Using a small value for the sampling ratio provides speed-up while a higher value provides results which are more consistent with those of the brute force approach.\nThe method proposed in this paper, called Ensembles of Random Shapelets (EnRS), trains a set of shapelet based classifiers. The precise working of the approach can be described as follows. Depending on its incarnation, we use either bagging (Algorithm 3) or boosting (Algorithm 4) for training the individual models in EnRS-Bagging or EnRSBoosting, respectively. In the EnRS-Bagging approach, different bootstraps of data are used to train Random-Shapelets models. This incorporates dual randomization in the overall process. The input for each model is randomized while the shapelet discovery process is already randomized. This also provides instance duplication which allows the shapelet discovery process to quickly separate duplicated instances and then efficiently perform a search for shapelets in the other instances. This also provides an effective speed-up for the discovery process. In the EnRS-Boosting approach, instances are weighted and each iteration trains a model more focused on the misclassified instances from the previous iterations. This approach also modifies the calculation of information gain to use the weights instead of the class counts in the current split. In addition to the EnRS-Bagging and EnRSBoosting variants, we have also included a variant denoted by EnRS, which builds an ensemble of Random-Shapelets based trees with the original training set and does not make use of bootstrap sampling or boosting. It combines multiple Random-Shapelets models and relies only on the diversification provided by the base classifier.\nThe model generation within the ensemble methods use the\nAlgorithm 5 GetNextCandidate ()\n1: cand\u2190 \u2205 2: while CandidatesAvailable() = True do 3: update currPos, currInstance, currLen 4: rand num \u223c U(0, 1) 5: if rand num < ratio then 6: cand\u2190 create candidate 7: BREAK 8: end if 9: end while\n10: return cand\ndecision tree construction from Algorithm 1, which in turn uses the shapelet discovery process (Algorithm 2) using the randomized candidate generation approach (Algorithm 5). In contrast to the YK-Shapelets algorithm, which evaluates all possible shapelet candidates, the Random-Shapelets algorithm adopted in this paper only evaluates a small percentage of the candidates chosen at random. This is the main difference between the two algorithms and they completely share the rest of the shapelet discovery process. The candidate sampling is performed while generating shapelet candidates. The basic procedure is the same, however, a loop skips candidates based on a uniformly distributed random number and the provided selection probability. The final classification decision for an instance is based on voting."}, {"heading": "3.1 Optimizations", "text": "The extent of sampling has a direct influence on the overall reduction in the computational costs achieved by the Random-Shapelets algorithm. Smaller sampling ratios lead to higher reduction and vice versa. The authors of the Random-Shapelets algorithm only evaluated the effects of sampling the shapelet candidates, however, the approach can be further optimized by incorporating the different speed up techniques proposed in some other research efforts. First and foremost, the early abandoning of distance calculations and early candidate pruning approaches introduced in the YK-Shapelets paper can be used in the Random-Shapelets algorithm as well. Distance calculations are abandoned as soon as the distance between the candidate and the subsequence from the current window exceed the running \u201cbest so far\u201d value. The candidates themselves are pruned based on an inexpensive information gain computation. The idea is to compute an optimistic information gain value after placing each time series instance on the order line to estimate whether such a placement of the remaining instances will provide a better information gain than the \u201cbest so far\u201d information gain. If the optimistic information gain is better than the best so far value, then the current candidate can provide a better order line and, therefore, it is potentially a better candidate than the current best shapelet. So we continue the evaluation of the remaining time series instances in the dataset. However, if the optimistic \u201cinformation gain\u201d is less than the best so far value, then any further processing of the current candidate can not lead to a better result so the evaluation of the candidate shapelet can be abandoned altogether. These speed-up techniques provide an inexpensive but highly effective way of pruning distance calculations\nand the shapelet candidates."}, {"heading": "3.2 Normalization", "text": "Normalizing subsequences before distance calculations provides better overall accuracy for the shapelet discovery algorithm because time series similarity benefits from scale and offset invariance. Therefore, we need to z-normalize the subsequences before distance calculations. This requires the calculation of mean and standard deviation values for each subsequence prior to the distance calculation. Calculation of these values makes up the majority of the computation required for normalizing the subsequences and makes the computational costs untenable. Using a \u201csummary statistics\u201d [12] based approach can drastically reduce the amount of computation required to calculate the mean and standard deviation of a given subsequence. The \u201csummary statistics\u201d for each time series instance in the training set are calculated at the time of loading the dataset, while for the shapelet candidates, they are calculated at the time of shapelet candidate generation. Subsequently, a simple procedure can provide the mean and standard deviation value in almost constant time and drastically reduce the cost of z-normalizing the sequences before distance calculations. This approach for znormalization of subsequences has also been incorporated in our implementation to improve the overall accuracy of the models and also for reduced computational overhead."}, {"heading": "4. EXPERIMENTAL DESIGN", "text": "The main goal of our experimental evaluation is twofold. We want to evaluate whether (1) the proposed approach provides better or at par classification accuracy compared to the YK-Shapelets approach and (2) whether it requires less computational effort as compared to the exact method. Therefore, an extensive set of experiments has been carried out for the evaluation and comparison of the different approaches. We have set the classification accuracy and runtime of the YK-Shapelets approach as the baseline. We have also compared the results for the Fast-Shapelets algorithm [10] and the gRSF algorithm [6].\nThe YK-Shapelets, Random-Shapelets and the three ensemble approaches have been implemented in Java using a consistent program structure so that no algorithm gets an undue bias. For boosting only, the shapelet discovery procedure uses weighted instances and the weights of the instances are used instead of instance counts for calculating the entropy and information gain of the datasets and splits. Both the YK-Shapelets and Random-Shapelets algorithms share the core functionality and implementation so we can effectively compare the running times for the different approaches and determine the obtained speed up. Our implementation of the different algorithms is available from the accompanying web page for the paper.2 The Fast-Shapelets Java implementation was obtained from the UEA Time Series Repository.3 The gRSF implementation was obtained from the supporting web page for the paper.4\n2 https://dx.doi.org/10.6084/m9.figshare.4299521 3 http://www.timeseriesclassification.com 4 http://people.dsv.su.se/\u02dcisak-kar/grsf/"}, {"heading": "4.1 Datasets", "text": "The empirical evaluation has been carried out on 45 datasets publicly available from the UCR Time Series Archive.5 The datasets belong to various fields including ECG readings, image outlines, motion capture data, sensor readings, spectral analyses and synthetically generated data. The problems addressed in these datasets range from binary to multi-class problems. The original training and testing splits are used as such, using the training splits to train the classifiers and reporting the training time while using the testing set to report classification accuracy."}, {"heading": "4.2 Experiments", "text": "We have carried out experiments for the YK-Shapelets, FastShapelets, Random-Shapelets and ensembles of RandomShapelets based classifiers. Since YK-Shapelets is an exact method and its classification accuracy remains the same over different runs provided the candidate length parameters are kept the same, each dataset is evaluated once. For all the other algorithms, each dataset is evaluated 100 times with each algorithm and the mean and standard deviation of the achieved classification accuracy are reported. The number of classification models per ensemble is set to ten for each variant and voting is used for the final decision. We use fully grown decision tree models in all our experiments for all the algorithms."}, {"heading": "4.3 Parameter Settings", "text": "The main parameters required for the shapelet discovery process are minLen and maxLen, which define the range of possible shapelet candidate lengths. The shapelet discovery process searches for candidates in all possible window sizes between the provided minimum and maximum length sizes. For example, if minLen = 10 and maxLen = 20, then the shapelet discovery process will search for shapelets in all window sizes starting from 10 and ending at 20. Therefore, setting these values to the extreme cases, where minLen = 1 and maxLen = m, where m is the time series instance length, makes the algorithm search over the entire candidate set. Another approach is to set these parameters based on some assumptions about the possible shapelet lengths. However, setting these parameters incorrectly can be detrimental to the shapelet discovery process. Setting the parameters to a very small window can cause the shapelet discovery process to miss important features because they are not covered in the search window while setting the window to a very large size can cause suboptimal feature selection. A third approach is to use a parameter optimization phase before creating the complete classification model.\nThe experiments were executed using two main approaches. For the first approach, instead of setting the parameter values to the extreme cases, or making any assumptions about the possible shapelet lengths, we take a cautious approach and set the parameters to a constant fraction of the time series length for all datasets. For all experiments, we used minLen = d0.25\u00d7me and maxLen = b0.67\u00d7mc, where m is the length of time series. This allows to cover more than 66% of the time series length at the start of the discovery\n5 http://www.cs.ucr.edu/\u02dceamonn/time series data/\nprocess and narrows the search up to just a quarter of the time series length. For our second approach, we used a parameter optimization phase to search for the best shapelet candidate lengths for each dataset and then performed the experiments using these learned parameters. The parameter optimization was performed with only the training split of the datasets.\nThe Random-Shapelets algorithm evaluates a small fraction of all the possible candidates in the specified minLen and maxLen range. This fraction of candidates is determined by the sampling ratio. All the experiments involving the Random-Shapelets algorithm have been performed with a 1% sampling ratio. This includes the experiments for evaluating the Random-Shapelets algorithm itself and the variants of ensembles of Random-Shapelets."}, {"heading": "5. RESULTS", "text": "We will evaluate the competing algorithms on the basis of classification accuracy and the required computational cost. We used 45 different datasets for a thorough evaluation of the algorithms. All the experiments have been carried out on a High Performance Cluster. The maximum allowed time for evaluating a dataset with any given algorithm was set at 10 days. If the experiment for a dataset did not complete in that time frame, we report it as \u201cDNF\u201d. We will summarize the results in this section.6"}, {"heading": "5.1 Classification Accuracy", "text": "In our experiments, the ensembles of Random-Shapelets classifiers consistently outperformed the other algorithms and provided better classification accuracy. This observation is particularly interesting because the individual models in the ensembles were using just 1% of the possible candidates, uniformly sampled from the set of all possible candidates.\nFigures 2a and 2b show the critical differences diagram for the classification accuracies of the individual algorithms for a p = 0.05 significance level. The ensembles outperform the other algorithms. The large difference between the average ranks of the ensembles and other algorithms, especially YKShapelets, shows that the average improvement in the classification accuracy is also significant. For many datasets, the improvement in classification accuracy was as high as 20% when using ensembles as compared to the classification accuracy achieved using the YK-Shapelets algorithm. The total number of wins for ensembles against the other algorithms is 34 and 35 for fixed and tuned parameters respectively. A very peculiar thing to note is that the standalone RandomShapelets algorithm can also outperform the YK-Shapelets algorithm with a significant difference. This happens because the YK-Shapelets model overfits the data whereas the Random-Shapelets model can better generalize on the test set. The results of experiments performed with fixed parameter settings are provided in Table 1 while the results of experiments using parameter optimization are provided in Table 2."}, {"heading": "5.2 Run Time", "text": "6Detailed results can be obtained from https://dx.doi.org/ 10.6084/m9.figshare.4299479\nThe training time for shapelet-based classifiers accounts for almost the entire run time of the algorithms because the testing time is negligible compared to the training time. The time required for training classification models using the different approaches were noted for the training phase using standard Java timing utilities. The Fast-Shapelets algorithm was the fastest overall followed by the RandomShapelets algorithm and then the ensembles (Bagging, Simple combination, Boosting) and finally the YK-Shapelets. The ensembles consistently performed faster than the YKShapelets algorithm and could obtain a speed-up of more than an order of magnitude on average. Tables 3 and 4 list the average time (in seconds) for evaluating each dataset with the corresponding algorithm using fixed and optimized parameters respectively.\nThe results for parameter optimized experiments are shown in Figure 3. The figures show the accuracy (left panel) and speed-up (right panel) obtained for each dataset. The results for YK-Shapelets are plotted as a solid black line denoting the baseline while the box plots show the obtained results for the other algorithms. Any value to the left of the baseline implies that the YK-Shapelets algorithm performed better than the other algorithm. The speed-up obtained for the algorithms is plotted by dividing the time taken by YKShapelets algorithm by the time taken by the respective algorithm. Any value to the left of the baseline implies the YK-Shapelets algorithm was faster. The speed-up is plotted on a logarithmic scale so 100 or 1 means no speed-up while\n101 implies a speed-up of one order of magnitude. The red lines in the box plots show the median values for the observations while the black whiskers show the minimum and maximum values for the observations."}, {"heading": "5.3 EnRS-Bagging vs. gRSF", "text": "The Generalized Random Shapelet Forests or gRSF algorithm is very similar to the approach we call Ensembles of Random-Shapelets using Bagging or EnRS-Bagging, so we compared the classification accuracy of the two algorithms. We used the Java implementation provided by the authors of gRSF and evaluated all the datasets evaluated in our other experiments. The minLen and maxLen parameters were set to the best parameters reported in the gRSF paper. The results reported in the gRSF paper were obtained by setting the ensemble size to 500, while we performed all our experiments with merely 10 models per ensemble, so we also performed the experiments for gRSF using 10 models per ensemble to make a fair comparison between the two algorithms. Figure 4 shows the critical differences diagram and the average ranks for the classification accuracy results for the two algorithms. The bagging ensemble has a slightly better average rank than gRSF and the Nemenyi test does not find them significantly different at a p = 0.05 significance level. This slight difference can be explained due to the smaller number of candidates evaluated by the gRSF algorithm. By default, the number of candidates sampled by\nthe gRSF algorithm at each node is equal to \u221a\n1 2 m(m+ 1),\nwhere m is the length of the time series. This number turns out to be even smaller than 1% of the total shapelet candidates used in our experiments. In fact, for at least the root nodes, using this number of candidates will always be smaller than 1% of the total shapelet candidates for all the datasets in the UCR Time Series Archive. This observation points to the fact that even a lesser number of candidates can yield very promising results for the shapelet-based ensembles. Using an even smaller percentage of candidates will also lead to better run times."}, {"heading": "6. DISCUSSION", "text": "The EnRS-Bagging approach is the fastest out of the three ensemble approaches tested in our experimentation while EnRS-Boosting is the slowest. The EnRS-Bagging approach also has a higher number of overall wins head-to-head with the other two ensemble approaches. EnRS-Bagging performs better in classification accuracy and runtime because of the duplication of instances in the dataset used for training. The duplicated instances allow the candidate pruning strategy to quickly identify good or bad candidates. Therefore, bagging allows the algorithm to run faster. Using duplicated instances in the training dataset also introduces a bias towards the instances of the majority class. This leads to an early extraction of the shapelet specific to the instances of the majority class and allows the algorithm to effectively split the dataset early and then search for shapelets for the other instances. This also makes the process efficient. The EnRS-Boosting approach performs slower because the weighting of instances increases the computation required for performing data splits and hence the candidate pruning. Since the candidate pruning strategy creates optimistic\n(a) 50words (b) Adiac\n(c) ArrowHead (d) Beef\n(e) BeetleFly (f) BirdChicken\n(g) Car (h) CBF\nFigure 3: Box plots showing achieved classification accuracy (left panel) and speed-up (right panel) for all evaluated datasets. The red lines show median values while minimum and maximum values are shown by black whiskers. The black line passing through the plot shows the values for YK-Shapelets algorithm.\n(i) Coffee (j) DiatomSizeReduction\n(k) DistalPhalanxOutlineAgeGroup (l) DistalPhalanxOutlineCorrect\n(m) DistalPhalanxTW (n) ECG200\n(o) ECGFiveDays (p) FaceAll\nFigure 3: Box plots showing achieved classification accuracy (left panel) and speed-up (right panel) for all evaluated datasets. The red lines show median values while minimum and maximum values are shown by black whiskers. The black line passing through the plot shows the values for YK-Shapelets algorithm.\n(q) FaceFour (r) FacesUCR\n(s) FISH (t) Gun Point\n(u) Herring (v) InsectWingbeatSound\n(w) ItalyPowerDemand (x) Lighting2\nFigure 3: Box plots showing achieved classification accuracy (left panel) and speed-up (right panel) for all evaluated datasets. The red lines show median values while minimum and maximum values are shown by black whiskers. The black line passing through the plot shows the values for YK-Shapelets algorithm.\n(y) Lighting7 (z) MALLAT\n(aa) Meat (ab) MiddlePhalanxOutlineAgeGroup\n(ac) MiddlePhalanxOutlineCorrect (ad) MiddlePhalanxTW\n(ae) MoteStrain (af) OliveOil\nFigure 3: Box plots showing achieved classification accuracy (left panel) and speed-up (right panel) for all evaluated datasets. The red lines show median values while minimum and maximum values are shown by black whiskers. The black line passing through the plot shows the values for YK-Shapelets algorithm.\n(ag) Plane (ah) ProximalPhalanxTW\n(ai) ShapeletSim (aj) SonyAIBORobotSurface\n(ak) SonyAIBORobotSurfaceII (al) SwedishLeaf\n(am) Symbols (an) synthetic control\nFigure 3: Box plots showing achieved classification accuracy (left panel) and speed-up (right panel) for all evaluated datasets. The red lines show median values while minimum and maximum values are shown by black whiskers. The black line passing through the plot shows the values for YK-Shapelets algorithm.\n(ao) ToeSegmentation1 (ap) ToeSegmentation2\n(aq) Trace (ar) TwoLeadECG\n(as) Wine\nFigure 3: Box plots showing achieved classification accuracy (left panel) and speed-up (right panel) for all evaluated datasets. The red lines show median values while minimum and maximum values are shown by black whiskers. The black line passing through the plot shows the values for YK-Shapelets algorithm.\nsplits in each call, this becomes a limiting factor for the EnRS-Boosting approach.\nThe Fast-Shapelets algorithm is also a heuristic method and can be a possible candidate for the base learner in the ensemble learning approach. We experimented with this approach as well, however, the Fast-Shapelets algorithm does not provide much diversity in the models, which makes its use in ensembles less effective than the Random-Shapelets algorithm."}, {"heading": "7. CONCLUSION", "text": "We proposed an ensemble learning approach using RandomShapelets algorithm as a base classifier for shapelets based classification. The use of an inexpensive but reasonably accurate base learner proves to be highly beneficial. The benefits of the proposed method are twofold and include better classification accuracy and reduced computational effort. Better classification accuracy was achieved for almost all the evaluated datasets, while the run time was reduced in all cases. The simplicity and added benefits of the approach make it very suitable for shapelet discovery and classification. Using bagging can reduce the required computation, however, in some cases the classification accuracy of the obtained model is slightly worse than the ensemble of RandomShapelets classifiers trained on the original training dataset, albeit not significantly.\nCurrently, the Random-Shapelets algorithm can only evaluate candidates with a sampling ratio set at start of the process. The possibility of changing the fraction of evaluated candidates and use the results in an additive fashion to the already obtained results could prove beneficial. This would require some book keeping about the already evaluated candidates and the obtained results, but if the storage requirements can be kept low, this could prove as a refinement step to an approximate solution. Another future research avenue could be the use of Random-Shapelets based classification models trained using randomly chosen window length parameters and combining the models in an ensemble. This should, theoretically, enhance the diversity of the individual models and also remove the need to perform parameter tuning before model generation."}, {"heading": "8. REFERENCES", "text": "[1] A. Bagnall and J. Lines. An Experimental Evaluation\nof Nearest Neighbour Time Series Classification. CoRR, 1406.4757, 2014.\n[2] L. Breiman. Bagging predictors. Machine learning, 24(2):123\u2013140, 1996.\n[3] H. Ding, G. Trajcevski, P. Scheuermann, X. Wang, and E. Keogh. Querying and mining of time series data: experimental comparison of representations and distance measures. Proceedings of the VLDB Endowment, 1(2):1542\u20131552, 2008.\n[4] Y. Freund and R. E. Schapire. A desicion-theoretic generalization of on-line learning and an application to boosting. In Computational learning theory, pages 23\u201337. Springer, 1995.\n[5] L. Hansen and P. Salamon. Neural network ensembles. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(10):993\u20131001, 1990.\n[6] I. Karlsson, P. Papapetrou, and H. Bostro\u0308m. Generalized random shapelet forests. Data Mining and Knowledge Discovery, 30(5):1053\u20131085, 2016.\n[7] J. Lin, E. Keogh, L. Wei, and S. Lonardi. Experiencing SAX: a novel symbolic representation of time series. Data Mining and Knowledge Discovery, 15(2):107\u2013144, 2007.\n[8] J. Lines, L. M. Davis, J. Hills, and A. Bagnall. A shapelet transform for time series classification. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD \u201912, pages 289\u2013297, New York, New York, USA, 2012. ACM Press.\n[9] A. Mueen, E. Keogh, and N. Young. Logical-Shapelets: An Expressive Primitive for Time Series Classification. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD \u201911, page 1154, New York, New York, USA, 2011. ACM Press.\n[10] T. Rakthanmanon and E. Keogh. Fast Shapelets: A Scalable Algorithm for Discovering Time Series Shapelets. In Proceedings of the 2013 SIAM International Conference on Data Mining, pages 668\u2013676, Philadelphia, PA, may 2013. Society for Industrial and Applied Mathematics.\n[11] X. Renard, M. Rifqi, W. Erray, and M. Detyniecki. Random-shapelet: An algorithm for fast shapelet discovery. In 2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA), pages 1\u201310. IEEE, Oct 2015.\n[12] Y. Sakurai, S. Papadimitriou, and C. Faloutsos. Braid: Stream mining through group lag correlations. In Proceedings of the 2005 ACM SIGMOD international conference on Management of data, pages 599\u2013610. ACM, 2005.\n[13] L. Ye and E. Keogh. Time Series Shapelets: A New Primitive for Data Mining. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD \u201909, pages 947\u2013956, New York, New York, USA, 2009. ACM Press."}], "references": [{"title": "An Experimental Evaluation of Nearest Neighbour Time Series Classification", "author": ["A. Bagnall", "J. Lines"], "venue": "CoRR, 1406.4757", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine learning, 24(2):123\u2013140", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Querying and mining of time series data: experimental comparison of representations and distance measures", "author": ["H. Ding", "G. Trajcevski", "P. Scheuermann", "X. Wang", "E. Keogh"], "venue": "Proceedings of the VLDB Endowment, 1(2):1542\u20131552", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Computational learning theory, pages 23\u201337. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Neural network ensembles", "author": ["L. Hansen", "P. Salamon"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(10):993\u20131001", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1990}, {"title": "Generalized random shapelet forests", "author": ["I. Karlsson", "P. Papapetrou", "H. Bostr\u00f6m"], "venue": "Data Mining and Knowledge Discovery, 30(5):1053\u20131085", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Experiencing SAX: a novel symbolic representation of time series", "author": ["J. Lin", "E. Keogh", "L. Wei", "S. Lonardi"], "venue": "Data Mining and Knowledge Discovery, 15(2):107\u2013144", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "A shapelet transform for time series classification", "author": ["J. Lines", "L.M. Davis", "J. Hills", "A. Bagnall"], "venue": "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD \u201912, pages 289\u2013297, New York, New York, USA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Logical-Shapelets: An Expressive Primitive for Time Series Classification", "author": ["A. Mueen", "E. Keogh", "N. Young"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD \u201911, page 1154, New York, New York, USA", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast Shapelets: A Scalable Algorithm for Discovering Time Series Shapelets", "author": ["T. Rakthanmanon", "E. Keogh"], "venue": "In Proceedings of the 2013 SIAM International Conference on Data Mining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Random-shapelet: An algorithm for fast shapelet discovery", "author": ["X. Renard", "M. Rifqi", "W. Erray", "M. Detyniecki"], "venue": "In 2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Braid: Stream mining through group lag correlations", "author": ["Y. Sakurai", "S. Papadimitriou", "C. Faloutsos"], "venue": "Proceedings of the 2005 ACM SIGMOD international conference on Management of data, pages 599\u2013610. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Time Series Shapelets: A New Primitive for Data Mining", "author": ["L. Ye", "E. Keogh"], "venue": "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD \u201909, pages 947\u2013956, New York, New York, USA", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "The most investigated approach for time series classification has been the nearest neighbor algorithm coupled with various distance measures [1,3].", "startOffset": 141, "endOffset": 146}, {"referenceID": 2, "context": "The most investigated approach for time series classification has been the nearest neighbor algorithm coupled with various distance measures [1,3].", "startOffset": 141, "endOffset": 146}, {"referenceID": 12, "context": "dress the drawbacks of nearest neighbor based time series classification [13].", "startOffset": 73, "endOffset": 77}, {"referenceID": 4, "context": "make different errors for an unseen problem [5].", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "For a detailed introduction, we refer the readers to the respective papers [6, 9\u201311,13].", "startOffset": 75, "endOffset": 87}, {"referenceID": 8, "context": "For a detailed introduction, we refer the readers to the respective papers [6, 9\u201311,13].", "startOffset": 75, "endOffset": 87}, {"referenceID": 9, "context": "For a detailed introduction, we refer the readers to the respective papers [6, 9\u201311,13].", "startOffset": 75, "endOffset": 87}, {"referenceID": 10, "context": "For a detailed introduction, we refer the readers to the respective papers [6, 9\u201311,13].", "startOffset": 75, "endOffset": 87}, {"referenceID": 12, "context": "For a detailed introduction, we refer the readers to the respective papers [6, 9\u201311,13].", "startOffset": 75, "endOffset": 87}, {"referenceID": 7, "context": "The purity of the obtained splits is evaluated using the information gain measure although other approaches can be used as well [8].", "startOffset": 128, "endOffset": 131}, {"referenceID": 12, "context": "The authors of the YK-Shapelets algorithm proposed early abandoning distance calculations and early candidate pruning using an upper-bound on the information gain and reported a speed-up of three orders of magnitude compared to the brute force approach [13].", "startOffset": 253, "endOffset": 257}, {"referenceID": 8, "context": "The Logical-Shapelets algorithm reduces computational costs by reusing previously calculated distances and pruning candidates using the triangular inequality [9].", "startOffset": 158, "endOffset": 161}, {"referenceID": 6, "context": "The Fast-Shapelets algorithm reduces the dimensionality of the data using SAX [7] and then performs a random projection based shapelet discovery using this lower dimensional data [10].", "startOffset": 78, "endOffset": 81}, {"referenceID": 9, "context": "The Fast-Shapelets algorithm reduces the dimensionality of the data using SAX [7] and then performs a random projection based shapelet discovery using this lower dimensional data [10].", "startOffset": 179, "endOffset": 183}, {"referenceID": 10, "context": "The Random-Shapelets algorithm performs a uniform random sampling of candidates to reduce the number of evaluated candidates [11].", "startOffset": 125, "endOffset": 129}, {"referenceID": 5, "context": "A recently published approach called Generalized Random Shapelet Forests (gRSF) also employs ensembles and a randomized candidate sampling based shapelet discovery process for improved classification accuracy and reduced runtime [6].", "startOffset": 229, "endOffset": 232}, {"referenceID": 1, "context": "Another approach for constructing ensembles is that of Bagging [2] which trains N models, each with a different bootstrap of data such that |D| instances are sampled with replacement from the original dataset.", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "Another approach called Boosting [4] relies on weighted instances.", "startOffset": 33, "endOffset": 36}, {"referenceID": 11, "context": "Using a \u201csummary statistics\u201d [12] based approach can drastically reduce the amount of computation required to calculate the mean and standard deviation of a given subsequence.", "startOffset": 29, "endOffset": 33}, {"referenceID": 9, "context": "We have also compared the results for the Fast-Shapelets algorithm [10] and the gRSF algorithm [6].", "startOffset": 67, "endOffset": 71}, {"referenceID": 5, "context": "We have also compared the results for the Fast-Shapelets algorithm [10] and the gRSF algorithm [6].", "startOffset": 95, "endOffset": 98}], "year": 2017, "abstractText": "Shapelets are discriminative time series subsequences that allow generation of interpretable classification models, which provide faster and generally better classification than the nearest neighbor approach. However, the shapelet discovery process requires the evaluation of all possible subsequences of all time series in the training set, making it extremely computation intensive. Consequently, shapelet discovery for large time series datasets quickly becomes intractable. A number of improvements have been proposed to reduce the training time. These techniques use approximation or discretization and often lead to reduced classification accuracy compared to the exact method. We are proposing the use of ensembles of shapelet-based classifiers obtained using random sampling of the shapelet candidates. Using random sampling reduces the number of evaluated candidates and consequently the required computational cost, while the classification accuracy of the resulting models is also not significantly different than that of the exact algorithm. The combination of randomized classifiers rectifies the inaccuracies of individual models because of the diversity of the solutions. Based on the experiments performed, it is shown that the proposed approach of using an ensemble of inexpensive classifiers provides better classification accuracy compared to the exact method at a significantly lesser computational cost.", "creator": "LaTeX with hyperref package"}}}