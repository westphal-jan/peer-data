{"id": "1702.06740", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Improving Chinese SRL with Heterogeneous Annotations", "abstract": "Previous studies on Chinese semantic role labeling (SRL) have concentrated on single semantically annotated corpus. But the training data of single corpus is often limited. Meanwhile, there usually exists other semantically annotated corpora for Chinese SRL scattered across different annotation frameworks. Data sparsity remains a bottleneck. This situation calls for larger training datasets, or effective approaches which can take advantage of highly heterogeneous data. In these papers, we focus mainly on the latter, that is, to improve Chinese SRL by using heterogeneous corpora together. We propose a novel progressive learning model which augments the Progressive Neural Network with Gated Recurrent Adapters. The model can accommodate heterogeneous inputs and effectively transfer knowledge between them. We also release a new corpus, Chinese SemBank, for Chinese SRL. Experiments on CPB 1.0 show that ours model outperforms state-of-the-art methods. However, our results are of a different kind. We report that the model does not provide a fully trained corpus (or an unstructured data structure) for Chinese SRL. This means that the model cannot be fully trained (which is an important limitation), because the model does not provide a full-trained corpus (or an unstructured data structure) for Chinese SRL. This is in contrast to our previous model.\n\n\n\n\nThe model is able to generate and validate the training dataset using a multilingual learning model that is comparable to CCC. As an example, we use the LDP model to generate the classification system. With the model we obtain a classification system and the Gedberte model. In this case, we have two main effects: the training dataset has been optimized and a model is used.\nThe model also allows for a more explicit and better-defined classification system. In this case, our training model can handle multi-level classification, which we use to provide all the training dataset in the model (where there are more than two dimensional models) and the classification system (where there are more than two dimensional models).", "histories": [["v1", "Wed, 22 Feb 2017 10:34:47 GMT  (136kb,D)", "http://arxiv.org/abs/1702.06740v1", "9 pages, 4 figures"], ["v2", "Mon, 13 Mar 2017 06:46:12 GMT  (136kb,D)", "http://arxiv.org/abs/1702.06740v2", "9 pages, 4 figures"], ["v3", "Tue, 14 Mar 2017 13:05:23 GMT  (0kb,I)", "http://arxiv.org/abs/1702.06740v3", "This paper has been withdrawn by the author due to a crucial error in equation 10"]], "COMMENTS": "9 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["qiaolin xia", "baobao chang", "zhifang sui"], "accepted": false, "id": "1702.06740"}, "pdf": {"name": "1702.06740.pdf", "metadata": {"source": "CRF", "title": "A Progressive Learning Approach to Chinese SRL Using Heterogeneous Data", "authors": ["Qiaolin Xia", "Baobao Chang", "Zhifang Sui"], "emails": ["xql@pku.edu.cn", "chbb@pku.edu.cn", "szf@pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Semantic role labeling (SRL) is one of the fundamental tasks in natural language processing because of its important role in information extraction (Bastianelli et al., 2013), statistical machine translation (Aziz et al., 2016; Xiong et al., 2012), and so on.\nHowever, state-of-the-art performance of Chinese SRL is still far from satisfactory. And data sparsity has been a bottleneck that can not be ignored. For English, the most commonly used\nbenchmark dataset PropBank (Xue and Palmer, 2003) has about 54,900 sentences. But for Chinese, there are only 10,364 sentences in Chinese PropBank 1.0 (CPB) (with about 35,700 propositions) (Xue, 2008).\nTo mitigate the data sparsity, models incorporating heterogeneous resources have been introduced to improve Chinese SRL performance (Wang et al., 2015; Guo et al., 2016; Li et al., 2016). The heterogeneous resources introduced by these models include other semantically annotated corpora with annotation schema different to that used in PropBank, and even of a different language. The challenge here lies in the fact that those newly introduced resources are heterogeneous in nature, without sharing the same tagging schema, semantic role set, syntactic tag set and domain. For example, Wang et al. (2015) introduced a heterogeneous dataset, Chinese NetBank, by pretraining word embeddings. Specifically, they learn an LSTM RNN model based on NetBank first, then initialize a new model with the pretrained embeddings obtained from Netar X iv :1\n70 2.\n06 74\n0v 1\n[ cs\n.C L\n] 2\n2 Fe\nb 20\n17\nBank, and then train it on CPB. Chinese NetBank (Yulin, 2007) is also a corpus annotated with semantic roles, but using a very different roleset and annotation schema. Wang\u2019s method can inherit knowledge acquired from other resources conveniently, but only at word representation level, missing more generalized semantic meanings in higher hidden layers. Li (2016) proposed a twopass training approach to use corpora of two languages, but a few non-common roles are ignored in the first pass. Guo et al. (2016) proposed a unified neural network model for SRL and relation classification (RC). It can learn two tasks at the same time, but cannot filter out harmful features learned in incompatible tasks.\nRecently, Progressive Neural Networks (PNN) model was proposed by Rusu et al. (2016) to transfer learned reinforcement learning policies from one game to another, or from simulation to the real robot. PNN \u201cfreezes\u201d learned parameters once starting to learn a new task, and it uses lateral connections, namely adapter, to access previously learned features.\nInspired by the PNN model, we propose a progressive learning model to Chinese semantic role labeling in this paper. Especially, we extend the model with Gated Recurrent Adapters (GRA). Since the standard PNN takes pixels as input, policies as output, it is not suitable for SRL task we focus in this paper. Moreover, to handle long sentences in the corpus, we enhance adapters with internal memories, and gates to keep the gradient stable. The contributions of this paper are threefold:\n1. We reconstruct PNN columns with bidirectional LSTMs to introduce heterogeneous corpora to improve Chinese SRL. The architecture can also be applied to a wider range of NLP tasks, like event extraction and relation classification, etc.\n2. We further extend the model with GRA to remember and take advantage of what has been transferred, thus improve the performance on long sentences.\n3. We also release a new corpus, Chinese SemBank, which was annotated with the schema different to that used in CPB, in hope that it will be helpful for future work on SRL tasks.\nWe use our new corpus as a heterogeneous resource, and evaluate the proposed model on\nthe benchmark dataset CPB 1.0. The experiment shows that our approach achieves 79.67% F1 score, significantly outperforms existing state-ofthe-art systems by a large margin (Section 5)."}, {"heading": "2 Heterogeneous Corpora for Chinese SRL", "text": "In this paper, we provide a new SRL corpus Chinese SemBank (CSB) and use it as an example of heterogeneous data in our experiments. In this section, we first briefly introduce the corpus, then compare it to existing corpora.\nSentences in CSB are from a variety of sources including online articles and news. The goal of this project is to build a very large and complete Chinese semantic corpus in the future. Currently, it only focus on the predicate-argument structures in a sentence without annotation of the temporal relations and coreference. Compared to the most commonly used dataset CPB, CSB is different in the following aspects:\n\u2022 In terms of predicate, CSB takes wider range of predicates into account. We not only annotated common verbs, but also nominal verbs, as NomBank does, and state words as well. While CPB only annotate common verbs as predicates.\n\u2022 In terms of semantic roles, CSB has a more fined-grained semantic role set. we defined 31 roles in five types (as Table. 1 shows). While In CPB, there are totally 23 roles, including core roles and non-core roles.\n\u2022 CSB does not have any pre-defined frames for predicates because all roles are set to be non-predicate-specific. The reason for not defining frames is that frames may lead inconsistencies in labels. For example, in CPB, an agent of a verb is often marked as its Arg0, but not all Arg0 are agents, according to Chinese verb formation theory (Sun et al., 2009). Therefore, roles are defined for predicates with similar syntactic and semantic regularities, rather than single predicate.\nTwo direct benefits of using stand-alone nonpredicate-specific roles are: First, meanings of all semantic roles can be directly inferred from their labels. For instance, roles of things that people are telling (\u8c08 ) or looking (\u770b) are labeled as \u5185 \u5bb9/content, because verbs like \u8c08 and \u770b are often followed by an object. Second, we can easily annotate sentences with new predicates without defining new frame files.\nOther Corpora for Chinese SRL Other popular semantic role labeling corpora include Chinese NomBank (Xue, 2006), Peking University Chinese NetBank (Yulin, 2007). NomBank, often used as a complement to PropBank, annotates nominal predicates and semantic roles according to the similar semantic schema as PropBank does. Peking University Chinese NetBank was created by adding a semantic layer to Peking University Chinese TreeBank (Zhou et al., 1997). It only use non-predicate-specific roles as we does. But the role set is smaller, which has 20 roles."}, {"heading": "3 Challenges in Inheriting Knowledge from Heterogeneous Corpora", "text": "Although there are lots of annotated corpora for Chinese SRL as we mentioned in the previous section, most of them are quite small compared to that of English. And data sparsity remains a bottleneck. This situation calls for larger training dataset, or effective approaches which can take advantage of highly heterogeneous datasets. In this paper, we focus on the second problem, that is, to\nimprove Chinese SRL by using heterogeneous corpora together within one model.\nWe will consider the combination of the standard benchmark, CPB 1.0 dataset (Xue and Palmer, 2003), with the new corpus, CSB, because there are huge differences between them, as we discussed in Section 2. Consequently, a number of challenges arise for this task. Now we describe them as below. Inheriting from Different Schema and Role Sets. CPB was annotated with PropBank-style frames and roles, while Chinese FrameNet uses its own frames and roles. And our dataset has no frame files and use different role set. Therefore, it is hard to find explicit mapping or hierarchical relationships among their role sets, or decide which system is better, especially when there are more than two resources. Inheriting from Different Domain/Genre. The datasets mentioned above composed of sentences from various sources, including news and stories, etc. However, it is well known that adding data in very different genre to training data may hurts parser performance (Bikel, 2004). Therefore, we also need to deal with domain adaptation problem when using heterogeneous data. In other words, the proposed approach should be robust to harmful features learned on incompatible datasets, and can accommodate potentially different model structures and inputs in the procedure of knowledge fusion. Inheriting from Different Syntactic Annotation. Different from English, previous works (Ding and Chang, 2009; Sun et al., 2009) on Chinese SRL task often use both correct segmentation and part-of-speech tagging, and even treebank gold-standard parses (Xue, 2008) as their features. But some corpora like CPB and NetBank do not share the same PoS tag set, or do not have correct PoS tagging and gold treebank parses at all, like CSB. And in real application scenarios, it is more convenient to use automatic PoS tagging instead of gold-standard tagging on large datasets, as they can be obtained quickly. So to deal with the absence of syntactic features, we adopt automatic PoS tagging when training on CSB in this work.\nSome previous techniques, such as finetuning after pretraining (Wang et al., 2015; Li et al., 2016) and multi-task learning (Guo et al., 2016), have been used to deal with these challenges. Though they can also leverage knowledge from different\ndomains, they have following drawbacks: finetuning cannot avoid catastrophic forgetting because learned parameters, whether embeddings or other hidden weights, will be tuned after the model has been initialized; And multi-task learning cannot ignore previously learned harmful features because some features are learned in shared layers, although it avoids forgetting by randomly select a task to learn at each iteration. Therefore, to solve the above-mentioned challenges, we further introduce progressive learning which we believe is more suitable for the task."}, {"heading": "4 Progressive Learning Approach", "text": "We propose a progressive learning approach which is ideal for combining heterogeneous SRL data for multiple reasons. First, it can accommodate dissimilar inputs with different schema, syntactic information and domain, because it allow models for heterogeneous resources to be extremely different, such as different network structures, different width, and different learning rates, etc. Second, it is immune to forgetting by freezing learned weights and can leverage prior knowledge via lateral connections. Third, the lateral connections can be extended with recurrent structure and gate mechanism to handle with forgetting problem over long distance.\nOur model is mainly inspired by Rusu et al. (2016). They proposed progressive neural networks for a wide variety of reinforcement learning\ntasks (e.g. Atari games and robot simulation). In their cases, inputs are pixels, outputs are learned policies, and each column, consisting of simple layers and convolutional layers, is trained to solve a particular Markov Decision Process. But in our case, inputs are sentences annotated using different syntactic tagsets and outputs are semantic role sequences. So we change the structure of columns to recurrent neural networks with LSTM, similar to the model proposed by Wang et al. (2015). Below we first introduce basic progressive neural network architecture, then describe our model, PNN with gated recurrent adapters."}, {"heading": "4.1 Progressive Neural Networks", "text": "Fig. 2a is an illustration of the basic progressive neural network model. It starts with single column (a neural network), in which there are L hidden layers and the output for ith layer (i \u2264 L) with ni units is h1i \u2208 Rni . \u03981 denotes the parameters to be learned in the first column. When switching to a second corpus, it \"freezes\" the parameter \u03981 and randomly initialize a new column with parameters \u03982 and several lateral connections between two columns so that layer h2i can receive input from both h2i\u22121 and h 1 i\u22121. In this straightforward manner, progressive neural networks can make use of columns with any structures or to compile lateral connections in an ensemble setting. To be more general, we calculate the output of ith layer in kth column hki by:\nhki = f(W k i h k i\u22121 + \u2211 j<k U (k:j) i h j i\u22121) (1)\nwhere W ki \u2208 Rn k i\u00d7nki\u22121 is the weight matrix of layer i of column k, U (k:j)i \u2208 Rn k i\u00d7n j i\u22121 are the lateral connections to transfer information from layer i \u2212 1 of column j to layer i of column k, h0 is the input of the network. f can be any activation function, such as element-wise non-linearity. Bias term was omitted in the equation. Adapters. With implicit assumption that there is some \"overlap\" between the first task and the second task, pretrain-and-finetune learning paradigm is effective, as only slight adjustment to parameters is needed to learn new features. Progressive networks also have ability to transfer knowledge from previous tasks to improve convergence speed. On the one hand, the model reuse previously learned features from left columns via lateral connections (i.e., adapters). On the other\nhand, new features can be learned by adding more columns incrementally. Moreover, when the \"overlap\" between two tasks is small, lateral connections can filter out harmful features by sigmoid functions. So in practice, the output of adapters can also be calculated by\na (k:j) i = \u03c3(A (k:j) i \u03b1 j i\u22121h j i\u22121) (2)\nwhere A(k:j)i is a matrix to be learned. We treat Equation 2 as one of baseline settings in experiments."}, {"heading": "4.2 PNN with Gated Recurrent Adapter for Chinese SRL", "text": "We reconstruct PNN with bidirectional LSTM to solve SRL problems. Our model is illustrated in Fig. 3.\nFirst, each column in the PNN architecture is a stacked bidirectional LSTM RNN, rather than convolutional neural networks, because inputs are sentences not pixels, and bi-LSTM RNN has proved powerful for Chinese SRL (Wang et al., 2015).\nSecond, we enhance the adapter with recurrent structure and gate mechanism, because the simple Multi-Layer Perceptron (MLP) adapters has a limitation: their weights are learned word after word independently. For tasks like transferring reinforcement learning policies, this is enough because there are little dependencies among actions.\nBut in NLP domain, things are different. Therefore, we add internal memory to adapters to help them remember what has been inherited from heterogeneous resource.\nThird, to keep gradient stable and balance between long-term and short-term memory, we introduce gate mechanism which has been widely used in RNN models. Intuitively, we call the new adapter Gated Recurrent Adapter (GRA).\nFormally, let h(<k)i\u22121 = [h 1 i\u22121, ..., h j i\u22121, ..., h k\u22121 i\u22121 ] be the outputs of i \u2212 1 layers from the first column to the (k \u2212 1)th column. The dimensionality of them is n(<k)i\u22121 = [n 1 i\u22121, ..., n k\u22121 i\u22121 ]. a (<k) is the outputs of k \u2212 1 adapters with dimension m(<k) = [m1, ...,mk\u22121]. The output vector is multiplied by a learned matrix Wa initialized by random small values before going to GRAs. Its role is to adjust for the different scales of the different inputs and reduce the dimensionality. Formally, the candidate outputs is\n\u00e2t = f(W jah j t + U j aa j t\u22121) (3)\nwhere at\u22121 is the output of the adapter at the previous time-step. Ua is a weight matrix to learn. The output of an adapter ajt of layer i at time t can be formalized as follows,\ngi =\u03c3(W j i h j t + U j i a j t\u22121) (4)\ngf =\u03c3(W j fh j t + U j fa j t\u22121) (5)\ngo =\u03c3(W j oh j t + U j oa j t\u22121) (6) \u00e3t =gi \u00e2t + gf \u00e3jt\u22121 (7) at =go f(\u00e3t\u22121) (8)\nwhere hj \u2208 Rm j i\u22121\u00d7n j i\u22121 is the outputs of previous layers, Wf ,Wo,Wa \u2208 Rmi\u22121\u00d7ni\u22121 , Uf , Uo, Ua \u2208 Rmi\u22121\u00d7di\u22121 . di\u22121 is the dimension of the inner memory in adapters. \u00e3t represents the inner state of the adapter. f is an activation function, like tanh. The input gate and the forget gate can be coupled as a uniform gate, that is gi = 1\u2212gf to alleviate the problem of information redundancy and reduce the possibility of overfitting (Greff et al., 2015).\nFinally, we calculate the output of the next layer i of column k by\nhki = f(W k i concat[a (<k), hki\u22121]) (9)\nwhere Wi \u2208 Rn (k) i \u00d7\n\u2211 m\n(<k) i\u22121 is the parameters in\nith layer."}, {"heading": "4.3 Training Criteria", "text": "We adopt the sentence tagging approach as Wang et al. (2015) did, because words in a sentence may closely related with each other, independently labeling each word is inappropriate. Sentence tagging approach only consider valid transition paths of tags when calculating the cost. For example, when using IOBES tagging schema, tag transition from I-Arg0 to B-Arg0 is invalid, and transition from I-Arg0 to I-Arg1 is also invalid because the type of the role changed inside the semantic chunk. For each task (column), the log likelihood of sentence x and its correct path y is\nlog p(y|x,\u0398) = log exp \u2211N\nt ot,yt\u2211 z exp \u2211Ni t ot,zt\n(10)\nwhere N is the number of words, ot \u2208 RM is the output of the last layer at time t. yt = k means the tth word has the kth semantic role label. z ranges from all the valid paths of tags.\nThe negative log likelihood of the whole training set D is\nJ(\u0398) = \u2211\n(x,y)\u2208D\nlog p(y|x,\u0398) (11)\nWe minimize J(\u0398) using stochastic gradient descent to learn network parameters \u0398. When testing, the best prediction of a sentence can be found using Viterbi algorithm."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Experiment Settings", "text": "To compare our approach with others, we designed four experimental setups:\n(1) A simple LSTM setup on CSB and CPB with automatic PoS tagging. Since CPB is about two times as large as the new corpus, we need to know whether CSB can be used for training good semantic parsers and how much information can be learned from CSB by machine. So we conduct this experiment to provide two baselines for CSB and CPB respectively. In this setup we train and evaluate a one-column LSTM model on CSB.\n(2) A simple LSTM setup on CPB with pretrained word embedding on CSB (marked as biLSTM+CSB embedding). Previous work found that using pretrained word embeddings can improve performance (Wang et al., 2015) on Chinese SRL. So we conduct this experiment to compare with the method using embeddings trained\non large-scale unlabeled data like Gigaword 1, and NetBank.\n(3) A two-column finetuning setup where we pretrain the first column on CSB and finetune both two columns on CPB. Clearly, finetuning is a traditional method for continual learning scenarios. But the disadvantage of it is that learned features will be gradually forgotten when the model is adapting to new tasks. To assess this empirically, we design this experiment. The model use the same network structure as PNN does, but it does not \"freeze\" parameters in the first column when tuning two columns.\n(4) A progressive network setup where we train column 1 on CSB, then train column 2 and adapters on CPB. We conduct this experiment to evaluate the proposed model and compare it to all previous methods. To further analyze effectiveness of the new adapter structure, we also conduct an experiment for progressive nets with GRA.\nWe apply grid-search technique to explore hyper-parameters including learning rates and width of layers.\nPreprocessing. We follow the same data setting as previous work (Xue, 2008; Sun et al., 2009), which divided CPB dataset2 into three parts: 648 files, from chtb_081.fid to chtb_899.fid, are the training set; 40 files, from chtb_041.fid to chtb_080.fid, are the development set; 72 files, from chtb_001.fid to chtb_040.fid, and chtb_900.fid to chtb_931.fid, are used as the test set. We also divide shuffled CSB corpus into three sets with similar partition ratios: 8900 samples are used as training set, 500 samples as development set and the rest 965 samples as test set. We use Stanford Parser3 for PoS tagging."}, {"heading": "5.2 Results", "text": "Performance on Chinese SemBank Table 2 gives the results of Experiment 1. We see that precision on CPB with automatic PoS tagging is about 0.9 percentage point higher than that on CSB, while recall is about 0.4 percentage point lower, and the gap between F1 scores on CPB and CSB is not significant, which is only about 0.3 percentage point, although the size of CSB is smaller. We can explain this by two reasons. First, CSB\n1https://code.google.com/p/word2vec/ 2https://catalog.ldc.upenn.edu/LDC2005T23 3http://nlp.stanford.edu/software/lex-parser.shtml\ndoes not have predicate-specific roles which may leads to inconsistency, as we explained in Section 3. Thus, it might be easier to learn by machine. Second, there are underlying similarities between them: both of them annotate predicateargument structures. So when there are sufficient training data, difference between scores on testing sets is not very likely to be huge.\nOverall, the results indicated that the new annotated corpus CSB is not a bad choice for training semantic parser even when this does not involve larger training sets.\nCompare to Methods without Using Heterogeneous Data Table 3 summarizes the SRL performance of previous benchmark methods and our experiments described above. Collobert and Weston only conducted their experiments on English corpus, but we notice that their approach has been implemented and tested on CPB by Wang et al. (2015), so we also put their result here for comparison. We can make several observations from these results. Our approach significantly outperforms Sha et al. (2016) by a large margin (Wilcoxon Signed Rank Test, p < 0.05), even without using GRA. This result can prove the ability of our model to capture underlying similarities between heterogeneous SRL resources.\nCompare to Methods Using Heterogeneous Resources The results of methods using external language resources are also presented in Table 3. Not surprisingly, we see that the overall best F1 score, 79.67%, is achieved by the progressive nets with the GRAs. Furthermore, as shown in Fig. 4, PNN with GRA performs better on longer sentences, which is consistent with our expectation. Without GRA, the F1 drops 0.37% percentage point to 79.30, confirming that gated recurrent adapter structure is more suitable for our task because it can remember what has been transferred in previous time steps.\nCompared to progressive learning methods, finetuning method does not perform well even with the same network structure (Two-column finetuning), but it is still better than simply pretraining word embeddings (bi-LSTM+CSB embedding). This confirms the effectiveness of multicolumn learning structure which add capacity to the model by adding new columns. Therefore, as can be seen, our PNN model achieves 79.30% F1 score, outperforming finetuning by 0.88% percentage point, and pretraining embeddings by even larger margin.\nTo sum up, not only network structures but also learning methods (finetuning/multitask/progressive) can influence the performance of knowledge transfer. According to the results, our PNN approach is more effective than others because it is immune to forgetting and robust to harmful features, and GRA is more suitable for our task than simple adapters."}, {"heading": "6 Related Work", "text": ""}, {"heading": "6.1 Chinese Semantic Role Labeling", "text": "The concept of Semantic Role Labeling is first proposed by Gildea and Jurafsky(2002). Previous work on Chinese SRL mainly focused on how to improve SRL on single corpus. Approaches falls into two categories: feature-based machine learning approaches and neural-network-based approaches. Using feature-based method, Sun and Jurafsky (2004) did the preliminary work and achieved promising results without using any large annotated corpus. After CPB was built by Xue and Palmer (2003), more complete and systematic research on Chinese SRL were done (Xue and Palmer, 2005; Chen et al., 2006; Ding and Chang, 2009; Yang et al., 2014).\nNeural network methods not rely on hand-\ncrafted features. For Chinese SRL, Wang et al. (2015) proposed bidirectional a LSTM RNN model. And based on their work, Sha (2016) proposed quadratic optimization method as a postprocessing module and further improved the result."}, {"heading": "6.2 Learning with Heterogeneous Data", "text": "In this paper, we mainly focus on learning with heterogeneous semantic resource for Chinese SRL. Wang et al. (2015) introduced heterogeneous data by using pretrained embeddings at initialization and achieved promising results. Guo et al. (2016) proposed a multitask learning method with a unified neural network model to learn SRL and relation classification task together and also achieved improvement.\nDifferent from previous work, we proposed a progressive neural network model with gated recurrent adapters to leverage knowledge from heterogeneous semantic data. Compare with previous methods, this approach is more constructive, rather than destructive, because it uses lateral connections to access previously learned features which are fixed when learning new tasks. And by introducing gated recurrent adapters, we further enhance our model to deal with long sentences and achieve state-of-the-art performance on Chinese PropBank."}, {"heading": "7 Conclusion and Future Work", "text": "In this paper, we proposed a progressive neural network model with gated recurrent adapters to leverage heterogeneous corpus for Chinese SRL. Unlike previous methods like finetuning, ours leverage prior knowledge via lateral connections. Experiments have shown that our model yields better performance on CPB than all baseline models. Moreover, we proposed novel gated recurrent adapter to handle transfer on long sentences, The experiment has proved the effectiveness of the new adapter structure.\nWe believe that progressive learning with heterogeneous data is a promising avenue to pursue. So in the future, we might try to combine more heterogeneous semantic data for other tasks like event extraction and relation classification, etc.\nWe also release the new corpus Chinese SemBank4, for Chinese SRL in the hope that it will be helpful in providing common benchmarks for future work on Chinese SRL tasks."}], "references": [{"title": "Textual inference and meaning representation in human robot interaction", "author": ["Emanuele Bastianelli", "Giuseppe Castellucci", "Danilo Croce", "Roberto Basili."], "venue": "In Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Struc-", "citeRegEx": "Bastianelli et al\\.,? 2013", "shortCiteRegEx": "Bastianelli et al\\.", "year": 2013}, {"title": "On the parameter space of generative lexicalized statistical parsing models", "author": ["Daniel M Bikel."], "venue": "Ph.D. thesis, Citeseer.", "citeRegEx": "Bikel.,? 2004", "shortCiteRegEx": "Bikel.", "year": 2004}, {"title": "An empirical study of chinese chunking", "author": ["Wenliang Chen", "Yujie Zhang", "Hitoshi Isahara."], "venue": "Proceedings of the COLING/ACL on Main conference poster sessions. Association for Computational Linguistics, pages 97\u2013104.", "citeRegEx": "Chen et al\\.,? 2006", "shortCiteRegEx": "Chen et al\\.", "year": 2006}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning. ACM, pages 160\u2013167.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Word based chinese semantic role labeling with semantic chunking", "author": ["Weiwei Ding", "Baobao Chang."], "venue": "International Journal of Computer Processing Of Languages 22(02n03):133\u2013154.", "citeRegEx": "Ding and Chang.,? 2009", "shortCiteRegEx": "Ding and Chang.", "year": 2009}, {"title": "Automatic labeling of semantic roles", "author": ["Daniel Gildea", "Daniel Jurafsky."], "venue": "Computational linguistics 28(3):245\u2013288.", "citeRegEx": "Gildea and Jurafsky.,? 2002", "shortCiteRegEx": "Gildea and Jurafsky.", "year": 2002}, {"title": "Lstm: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u00edk", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber."], "venue": "arXiv preprint arXiv:1503.04069 .", "citeRegEx": "Greff et al\\.,? 2015", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "A unified architecture for semantic role labeling and relation classification", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu", "Jun Xu."], "venue": "Proc. of the 26th International Conference on Computational Linguistics (COLING).", "citeRegEx": "Guo et al\\.,? 2016", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Improving chinese semantic role labeling with english proposition bank", "author": ["Tianshi Li", "Qi Li", "BaoBao Chang."], "venue": "China National Conference on Chinese Computational Linguistics. Springer, pages 3\u201311.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Progressive neural networks", "author": ["Andrei A. Rusu", "Neil C. Rabinowitz", "Guillaume Desjardins", "Hubert Soyer", "James Kirkpatrick", "Koray Kavukcuoglu", "Razvan Pascanu", "Raia Hadsell."], "venue": "CoRR abs/1606.04671.", "citeRegEx": "Rusu et al\\.,? 2016", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Capturing argument relationships for chinese semantic role labeling", "author": ["Lei Sha", "Tingsong Jiang", "Sujian Li", "Baobao Chang", "Zhifang Sui"], "venue": null, "citeRegEx": "Sha et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sha et al\\.", "year": 2016}, {"title": "Shallow semantic parsing of chinese", "author": ["Honglin Sun", "Daniel Jurafsky."], "venue": "Proceedings of NAACL 2004. pages 249\u2013256.", "citeRegEx": "Sun and Jurafsky.,? 2004", "shortCiteRegEx": "Sun and Jurafsky.", "year": 2004}, {"title": "Chinese semantic role labeling with shallow", "author": ["Weiwei Sun", "Zhifang Sui", "Meng Wang", "Xin Wang"], "venue": null, "citeRegEx": "Sun et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2009}, {"title": "Chinese semantic role labeling with bidirectional recurrent neural networks", "author": ["Zhen Wang", "Tingsong Jiang", "Baobao Chang", "Zhifang Sui."], "venue": "Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 1626\u20131631.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Modeling the translation of predicate-argument structure for smt", "author": ["Deyi Xiong", "Min Zhang", "Haizhou Li."], "venue": "In Proc. of the 50th Annual Meeting of the Association for Computational Linguistics. pages 902\u2013911.", "citeRegEx": "Xiong et al\\.,? 2012", "shortCiteRegEx": "Xiong et al\\.", "year": 2012}, {"title": "Annotating the predicateargument structure of chinese nominalizations", "author": ["Nianwen Xue."], "venue": "Proceedings of the fifth international conference on Language Resources and Evaluation. pages 1382\u2013 1387.", "citeRegEx": "Xue.,? 2006", "shortCiteRegEx": "Xue.", "year": 2006}, {"title": "Labeling chinese predicates with semantic roles", "author": ["Nianwen Xue."], "venue": "Computational linguistics 34(2):225\u2013255.", "citeRegEx": "Xue.,? 2008", "shortCiteRegEx": "Xue.", "year": 2008}, {"title": "Annotating the propositions in the penn chinese treebank", "author": ["Nianwen Xue", "Martha Palmer."], "venue": "Proceedings of the second SIGHAN workshop on Chinese language processing-Volume 17. Association for Computational Linguistics, pages 47\u201354.", "citeRegEx": "Xue and Palmer.,? 2003", "shortCiteRegEx": "Xue and Palmer.", "year": 2003}, {"title": "Automatic semantic role labeling for chinese verbs", "author": ["Nianwen Xue", "Martha Palmer."], "venue": "In Proceedings of the 19th International Joint Conference on Artificial Intelligence. pages 1160\u20131165.", "citeRegEx": "Xue and Palmer.,? 2005", "shortCiteRegEx": "Xue and Palmer.", "year": 2005}, {"title": "Multipredicate semantic role labeling", "author": ["Haitong Yang", "Chengqing Zong"], "venue": "In EMNLP", "citeRegEx": "Yang and Zong,? \\Q2014\\E", "shortCiteRegEx": "Yang and Zong", "year": 2014}, {"title": "The fineness hierarchy of semantic roles and its application in nlp", "author": ["Yuan Yulin."], "venue": "Journal of Chinese Information Processing 21(4):10\u201320.", "citeRegEx": "Yulin.,? 2007", "shortCiteRegEx": "Yulin.", "year": 2007}, {"title": "Building a chinese treebank", "author": ["Qiang Zhou", "Wei Zhang", "Shiwen Yu."], "venue": "Journal of Chinese Information Processing 4.", "citeRegEx": "Zhou et al\\.,? 1997", "shortCiteRegEx": "Zhou et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "Semantic role labeling (SRL) is one of the fundamental tasks in natural language processing because of its important role in information extraction (Bastianelli et al., 2013), statistical machine translation (Aziz et al.", "startOffset": 148, "endOffset": 174}, {"referenceID": 14, "context": ", 2013), statistical machine translation (Aziz et al., 2016; Xiong et al., 2012), and so on.", "startOffset": 41, "endOffset": 80}, {"referenceID": 17, "context": "benchmark dataset PropBank (Xue and Palmer, 2003) has about 54,900 sentences.", "startOffset": 27, "endOffset": 49}, {"referenceID": 16, "context": "0 (CPB) (with about 35,700 propositions) (Xue, 2008).", "startOffset": 41, "endOffset": 52}, {"referenceID": 13, "context": "To mitigate the data sparsity, models incorporating heterogeneous resources have been introduced to improve Chinese SRL performance (Wang et al., 2015; Guo et al., 2016; Li et al., 2016).", "startOffset": 132, "endOffset": 186}, {"referenceID": 7, "context": "To mitigate the data sparsity, models incorporating heterogeneous resources have been introduced to improve Chinese SRL performance (Wang et al., 2015; Guo et al., 2016; Li et al., 2016).", "startOffset": 132, "endOffset": 186}, {"referenceID": 8, "context": "To mitigate the data sparsity, models incorporating heterogeneous resources have been introduced to improve Chinese SRL performance (Wang et al., 2015; Guo et al., 2016; Li et al., 2016).", "startOffset": 132, "endOffset": 186}, {"referenceID": 7, "context": ", 2015; Guo et al., 2016; Li et al., 2016). The heterogeneous resources introduced by these models include other semantically annotated corpora with annotation schema different to that used in PropBank, and even of a different language. The challenge here lies in the fact that those newly introduced resources are heterogeneous in nature, without sharing the same tagging schema, semantic role set, syntactic tag set and domain. For example, Wang et al. (2015) introduced a heterogeneous dataset, Chinese NetBank, by pretraining word embeddings.", "startOffset": 8, "endOffset": 462}, {"referenceID": 20, "context": "Chinese NetBank (Yulin, 2007) is also a corpus annotated with semantic roles, but using a very different roleset and annotation schema.", "startOffset": 16, "endOffset": 29}, {"referenceID": 18, "context": "Chinese NetBank (Yulin, 2007) is also a corpus annotated with semantic roles, but using a very different roleset and annotation schema. Wang\u2019s method can inherit knowledge acquired from other resources conveniently, but only at word representation level, missing more generalized semantic meanings in higher hidden layers. Li (2016) proposed a twopass training approach to use corpora of two languages, but a few non-common roles are ignored in the first pass.", "startOffset": 17, "endOffset": 333}, {"referenceID": 7, "context": "Guo et al. (2016) proposed a unified neural network model for SRL and relation classification (RC).", "startOffset": 0, "endOffset": 18}, {"referenceID": 7, "context": "Guo et al. (2016) proposed a unified neural network model for SRL and relation classification (RC). It can learn two tasks at the same time, but cannot filter out harmful features learned in incompatible tasks. Recently, Progressive Neural Networks (PNN) model was proposed by Rusu et al. (2016) to transfer learned reinforcement learning policies from one game to another, or from simulation to the real robot.", "startOffset": 0, "endOffset": 296}, {"referenceID": 12, "context": "For example, in CPB, an agent of a verb is often marked as its Arg0, but not all Arg0 are agents, according to Chinese verb formation theory (Sun et al., 2009).", "startOffset": 141, "endOffset": 159}, {"referenceID": 15, "context": "Other Corpora for Chinese SRL Other popular semantic role labeling corpora include Chinese NomBank (Xue, 2006), Peking University Chinese NetBank (Yulin, 2007).", "startOffset": 99, "endOffset": 110}, {"referenceID": 20, "context": "Other Corpora for Chinese SRL Other popular semantic role labeling corpora include Chinese NomBank (Xue, 2006), Peking University Chinese NetBank (Yulin, 2007).", "startOffset": 146, "endOffset": 159}, {"referenceID": 21, "context": "Peking University Chinese NetBank was created by adding a semantic layer to Peking University Chinese TreeBank (Zhou et al., 1997).", "startOffset": 111, "endOffset": 130}, {"referenceID": 17, "context": "0 dataset (Xue and Palmer, 2003), with the new corpus, CSB, because there are huge differences between them, as we discussed in Section 2.", "startOffset": 10, "endOffset": 32}, {"referenceID": 1, "context": "However, it is well known that adding data in very different genre to training data may hurts parser performance (Bikel, 2004).", "startOffset": 113, "endOffset": 126}, {"referenceID": 4, "context": "Different from English, previous works (Ding and Chang, 2009; Sun et al., 2009) on Chinese SRL task often use both correct segmentation and part-of-speech tagging, and even treebank gold-standard parses (Xue, 2008) as their features.", "startOffset": 39, "endOffset": 79}, {"referenceID": 12, "context": "Different from English, previous works (Ding and Chang, 2009; Sun et al., 2009) on Chinese SRL task often use both correct segmentation and part-of-speech tagging, and even treebank gold-standard parses (Xue, 2008) as their features.", "startOffset": 39, "endOffset": 79}, {"referenceID": 16, "context": ", 2009) on Chinese SRL task often use both correct segmentation and part-of-speech tagging, and even treebank gold-standard parses (Xue, 2008) as their features.", "startOffset": 131, "endOffset": 142}, {"referenceID": 13, "context": "Some previous techniques, such as finetuning after pretraining (Wang et al., 2015; Li et al., 2016) and multi-task learning (Guo et al.", "startOffset": 63, "endOffset": 99}, {"referenceID": 8, "context": "Some previous techniques, such as finetuning after pretraining (Wang et al., 2015; Li et al., 2016) and multi-task learning (Guo et al.", "startOffset": 63, "endOffset": 99}, {"referenceID": 7, "context": ", 2016) and multi-task learning (Guo et al., 2016), have been used to deal with these challenges.", "startOffset": 32, "endOffset": 50}, {"referenceID": 9, "context": "Our model is mainly inspired by Rusu et al. (2016). They proposed progressive neural networks for a wide variety of reinforcement learning tasks (e.", "startOffset": 32, "endOffset": 51}, {"referenceID": 9, "context": "Our model is mainly inspired by Rusu et al. (2016). They proposed progressive neural networks for a wide variety of reinforcement learning tasks (e.g. Atari games and robot simulation). In their cases, inputs are pixels, outputs are learned policies, and each column, consisting of simple layers and convolutional layers, is trained to solve a particular Markov Decision Process. But in our case, inputs are sentences annotated using different syntactic tagsets and outputs are semantic role sequences. So we change the structure of columns to recurrent neural networks with LSTM, similar to the model proposed by Wang et al. (2015). Below we first introduce basic progressive neural network architecture, then describe our model, PNN with gated recurrent adapters.", "startOffset": 32, "endOffset": 633}, {"referenceID": 13, "context": "First, each column in the PNN architecture is a stacked bidirectional LSTM RNN, rather than convolutional neural networks, because inputs are sentences not pixels, and bi-LSTM RNN has proved powerful for Chinese SRL (Wang et al., 2015).", "startOffset": 216, "endOffset": 235}, {"referenceID": 6, "context": "The input gate and the forget gate can be coupled as a uniform gate, that is gi = 1\u2212gf to alleviate the problem of information redundancy and reduce the possibility of overfitting (Greff et al., 2015).", "startOffset": 180, "endOffset": 200}, {"referenceID": 13, "context": "We adopt the sentence tagging approach as Wang et al. (2015) did, because words in a sentence may closely related with each other, independently labeling each word is inappropriate.", "startOffset": 42, "endOffset": 61}, {"referenceID": 13, "context": "Previous work found that using pretrained word embeddings can improve performance (Wang et al., 2015) on Chinese SRL.", "startOffset": 82, "endOffset": 101}, {"referenceID": 16, "context": "We follow the same data setting as previous work (Xue, 2008; Sun et al., 2009), which divided CPB dataset2 into three parts: 648 files, from chtb_081.", "startOffset": 49, "endOffset": 78}, {"referenceID": 12, "context": "We follow the same data setting as previous work (Xue, 2008; Sun et al., 2009), which divided CPB dataset2 into three parts: 648 files, from chtb_081.", "startOffset": 49, "endOffset": 78}, {"referenceID": 3, "context": "Collobert and Weston only conducted their experiments on English corpus, but we notice that their approach has been implemented and tested on CPB by Wang et al. (2015), so we also put their result here for comparison.", "startOffset": 0, "endOffset": 168}, {"referenceID": 3, "context": "Collobert and Weston only conducted their experiments on English corpus, but we notice that their approach has been implemented and tested on CPB by Wang et al. (2015), so we also put their result here for comparison. We can make several observations from these results. Our approach significantly outperforms Sha et al. (2016) by a large margin (Wilcoxon Signed Rank Test, p < 0.", "startOffset": 0, "endOffset": 328}, {"referenceID": 18, "context": "After CPB was built by Xue and Palmer (2003), more complete and systematic research on Chinese SRL were done (Xue and Palmer, 2005; Chen et al., 2006; Ding and Chang, 2009; Yang et al., 2014).", "startOffset": 109, "endOffset": 191}, {"referenceID": 2, "context": "After CPB was built by Xue and Palmer (2003), more complete and systematic research on Chinese SRL were done (Xue and Palmer, 2005; Chen et al., 2006; Ding and Chang, 2009; Yang et al., 2014).", "startOffset": 109, "endOffset": 191}, {"referenceID": 4, "context": "After CPB was built by Xue and Palmer (2003), more complete and systematic research on Chinese SRL were done (Xue and Palmer, 2005; Chen et al., 2006; Ding and Chang, 2009; Yang et al., 2014).", "startOffset": 109, "endOffset": 191}, {"referenceID": 3, "context": "The concept of Semantic Role Labeling is first proposed by Gildea and Jurafsky(2002). Previous work on Chinese SRL mainly focused on how to improve SRL on single corpus.", "startOffset": 59, "endOffset": 85}, {"referenceID": 3, "context": "The concept of Semantic Role Labeling is first proposed by Gildea and Jurafsky(2002). Previous work on Chinese SRL mainly focused on how to improve SRL on single corpus. Approaches falls into two categories: feature-based machine learning approaches and neural-network-based approaches. Using feature-based method, Sun and Jurafsky (2004) did the preliminary work and achieved promising results without using any large annotated corpus.", "startOffset": 59, "endOffset": 339}, {"referenceID": 3, "context": "The concept of Semantic Role Labeling is first proposed by Gildea and Jurafsky(2002). Previous work on Chinese SRL mainly focused on how to improve SRL on single corpus. Approaches falls into two categories: feature-based machine learning approaches and neural-network-based approaches. Using feature-based method, Sun and Jurafsky (2004) did the preliminary work and achieved promising results without using any large annotated corpus. After CPB was built by Xue and Palmer (2003), more complete and systematic research on Chinese SRL were done (Xue and Palmer, 2005; Chen et al.", "startOffset": 59, "endOffset": 482}, {"referenceID": 3, "context": "90 Collobert and Weston (2008) MTL 74.", "startOffset": 3, "endOffset": 31}, {"referenceID": 3, "context": "90 Collobert and Weston (2008) MTL 74.05 Ding and Chang (2009) CRF 72.", "startOffset": 3, "endOffset": 63}, {"referenceID": 3, "context": "90 Collobert and Weston (2008) MTL 74.05 Ding and Chang (2009) CRF 72.64 Yang et al. (2014) Multi-Predicate 75.", "startOffset": 3, "endOffset": 92}, {"referenceID": 3, "context": "90 Collobert and Weston (2008) MTL 74.05 Ding and Chang (2009) CRF 72.64 Yang et al. (2014) Multi-Predicate 75.31 Wang et al. (2015) bi-LSTM 77.", "startOffset": 3, "endOffset": 133}, {"referenceID": 3, "context": "90 Collobert and Weston (2008) MTL 74.05 Ding and Chang (2009) CRF 72.64 Yang et al. (2014) Multi-Predicate 75.31 Wang et al. (2015) bi-LSTM 77.09 (+0.00) Sha et al. (2016) bi-LSTM+QOM 77.", "startOffset": 3, "endOffset": 173}, {"referenceID": 3, "context": "90 Collobert and Weston (2008) MTL 74.05 Ding and Chang (2009) CRF 72.64 Yang et al. (2014) Multi-Predicate 75.31 Wang et al. (2015) bi-LSTM 77.09 (+0.00) Sha et al. (2016) bi-LSTM+QOM 77.69 With external language resources Wang et al. (2015) +Gigaword embedding 77.", "startOffset": 3, "endOffset": 243}, {"referenceID": 3, "context": "90 Collobert and Weston (2008) MTL 74.05 Ding and Chang (2009) CRF 72.64 Yang et al. (2014) Multi-Predicate 75.31 Wang et al. (2015) bi-LSTM 77.09 (+0.00) Sha et al. (2016) bi-LSTM+QOM 77.69 With external language resources Wang et al. (2015) +Gigaword embedding 77.21 Wang et al. (2015) +NetBank embedding 77.", "startOffset": 3, "endOffset": 288}, {"referenceID": 3, "context": "90 Collobert and Weston (2008) MTL 74.05 Ding and Chang (2009) CRF 72.64 Yang et al. (2014) Multi-Predicate 75.31 Wang et al. (2015) bi-LSTM 77.09 (+0.00) Sha et al. (2016) bi-LSTM+QOM 77.69 With external language resources Wang et al. (2015) +Gigaword embedding 77.21 Wang et al. (2015) +NetBank embedding 77.59 Guo et al. (2016) +Relataion Classification 75.", "startOffset": 3, "endOffset": 331}, {"referenceID": 13, "context": "For Chinese SRL, Wang et al. (2015) proposed bidirectional a LSTM RNN model.", "startOffset": 17, "endOffset": 36}, {"referenceID": 13, "context": "For Chinese SRL, Wang et al. (2015) proposed bidirectional a LSTM RNN model. And based on their work, Sha (2016) proposed quadratic optimization method as a postprocessing module and further improved the result.", "startOffset": 17, "endOffset": 113}, {"referenceID": 12, "context": "Wang et al. (2015) introduced heterogeneous data by using pretrained embeddings at initialization and achieved promising results.", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "Guo et al. (2016) proposed a multitask learning method with a unified neural network model to learn SRL and relation classification task together and also achieved improvement.", "startOffset": 0, "endOffset": 18}], "year": 2017, "abstractText": "Previous studies on Chinese semantic role labeling (SRL) have concentrated on single semantically annotated corpus. But the training data of single corpus is often limited. Meanwhile, there usually exists other semantically annotated corpora for Chinese SRL scattered across different annotation frameworks. Data sparsity remains a bottleneck. This situation calls for larger training datasets, or effective approaches which can take advantage of highly heterogeneous data. In these papers, we focus mainly on the latter, that is, to improve Chinese SRL by using heterogeneous corpora together. We propose a novel progressive learning model which augments the Progressive Neural Network with Gated Recurrent Adapters. The model can accommodate heterogeneous inputs and effectively transfer knowledge between them. We also release a new corpus, Chinese SemBank, for Chinese SRL. Experiments on CPB 1.0 show that ours model outperforms state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}