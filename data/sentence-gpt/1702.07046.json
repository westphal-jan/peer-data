{"id": "1702.07046", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Feature Generation for Robust Semantic Role Labeling", "abstract": "Hand-engineered feature sets are a well understood method for creating robust NLP models, but they require a lot of expertise and effort to create. In this work we describe how to automatically generate rich feature sets from simple units called featlets, requiring less engineering. Using information gain to guide the generation process, we train models which rival the state of the art on two standard Semantic Role Labeling datasets with almost no task or linguistic insight.\n\n\n\nIn this work we use three methods to create efficient NLP models using these simple units:\nTo generate an NLP model, we first import the following data:\nThe dataset will be generated using the following format:\nAs we can see in this article we need to generate the following data:\nThe dataset will look like this:\nThe dataset will look like this:\nTo generate a NLP model we need to have a simple and easy to follow approach:\nThis data will be generated using the following format:\nThe dataset will look like this:\nIf we want to generate a very simple NLP model that we just generate in an easy to follow approach we need to have a simple and easy to follow approach. If we want to generate a very simple NLP model that we just generate in an easy to follow approach we need to have a simple and easy to follow approach. The data should be a list of four data types.\nThe data should be a list of four data types. The number of four is very limited. If we want to generate a simple NLP model that we just generate in an easy to follow approach, then we need to pass each table into a single table:\nOnce we have all our data and the number of columns in the table, we can now use the following format:\nIn this instance, the list of four records is:\nAs our output will look like this:\nEach table represents one column on the order of two:\nAnd in the above case, we can create two rows in the order of two:\nEach row represents one row on the order of two:\nFinally, we can define a unique NLP model:\nAnd if we want to generate a simple NLP model that we just generate in an easy to follow approach, then we need to pass each table into a single table:\nSo we can now produce two rows in the order of two:\nThe data should be a list of four records. The list of four records is:\nAs our output will look like this", "histories": [["v1", "Wed, 22 Feb 2017 23:39:03 GMT  (45kb,D)", "http://arxiv.org/abs/1702.07046v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["travis wolfe", "mark dredze", "benjamin van durme"], "accepted": false, "id": "1702.07046"}, "pdf": {"name": "1702.07046.pdf", "metadata": {"source": "CRF", "title": "Feature Generation for Robust Semantic Role Labeling", "authors": ["Travis Wolfe", "Mark Dredze", "Benjamin Van Durme"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Feature engineering is widely recognized as an important component of robust NLP systems, with much of this engineering done by hand. Articles describing improvements in task performance over prior work tend to be methodologically driven (for example low-regret online learning algorithms and structured regularizers), with improvements in feature design often described just briefly, and as a matter of secondary importance. While the distinctions between methods of inference are formalized in the language of mathematics, most expositions of feature design employ terse, natural language descriptions, often not sufficient for reliable reproduction of the underlying factors being extracted. This has led to stagnation in feature design, and in general an attitude in some circles that features themselves are not worth exploring; i.e., we should abandon explicit, interpretable features for neural techniques which create their own representations which may not align with our own.\nFeatures sets are constructed by authors using heuristics which are often not tested. For example it is common to coarsen a feature before using it in a product because the fine grained prod-\nuct would produce \u201ctoo many\u201d features. The author may have been correct (they ran the experiment and verified that performance went down) or not, but the reader often doesn\u2019t know which is the case, and are left with the same problem of whether to run that experiment or not. Due to the cost of running experiments, practitioners are biased towards copying the feature set verbatim.\nThis work is about removing the human from the loop of feature selection, focussing on Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002). The key challenge that we address is feature generation. Previous work has generated features by taking the Cartesian product of templates, but this is not rich enough to capture many widely used manually created features. We show that by decomposing the template even further, into atoms called featlets, we can automatically compose templates with rich, ad-hoc combinators. This process can generate many features which an expert might not consider.\nOnce we have tackled the feature generation problem, we show that we can automatically derive feature sets which match the performance of state-of-the-art feature sets created by experts. Our method uses basic statistics and requires no human expertise. We believe that models specified using featlets are easier to reproduce and offer the potential for performing feature selection with machine learning rather than domain expert knowledge, potentially at lower-cost and super-human performance.\nFeature Descriptions in the Literature For a case study on feature descriptions, consider the \u201cvoice feature\u201d for SRL. It was first motivated and described in Gildea and Jurafsky (2002). They said that they defined 10 rules for when a verb had either active or passive voice, but never said what they were. Since then, almost every prominent pa-\nar X\niv :1\n70 2.\n07 04\n6v 1\n[ cs\n.C L\n] 2\n2 Fe\nb 20\n17\nper on SRL has listed voice as a feature or template that they use, but none of the following defined their rules for the voice feature.1 Further, discrepancies between authors is not unheard of: Gildea and Jurafsky (2002) report 5% of verbs were passive in the PTB, while Pradhan et al. (2005) report 11%. Some of these papers go into great detail about other aspects like ILP constraints and and regularization constants, but this same clarity doesn\u2019t always extend to features.\nIn methods papers, math is used as a bridge between natural and programming languages. There is no equivalent for describing features, so this type of omission is understandable given space constraints and the clumsiness of natural language. However, given the importance of the underlying factors in a model, the lack of clarity diminishes the value of the work to other practitioners, especially among those less linguistically-inclined."}, {"heading": "2 Featlets", "text": "Our approach begins with the notion that features can be decomposed into smaller units called featlets. These units can be composed together to make a wide variety of features. We distinguish featlets from feature templates, or just templates, which are effectively sets of features. Featlets are not necessarily features, but are composed to produce features or feature templates.\nTo start with an example, the featlet WORD: given the index of a token, it returns the word at that position. A feature would not assume a token index is given, only that y and x are given, so WORD is not a feature. Featlets are also used to provide information to other featlets. For example, the featlet ARGHEAD takes the head token index of an argument span and passes it to WORD. The combination of the two, [ARGHEAD, WORD], is a template. Importantly featlets are interchangeable: the template [ARGHEAD, WORD] is related to [ARGHEAD, POS] and [ARGHEADPARENT, WORD]. Featlets are minimal to ensure that the trial and error of feature engineering falls to the machine rather than the expert.\nDefinition Featlets are operations performed on a context, which is a data structure with:\n1. Named fields which have types 1 Gildea and Jurafsky (2002) Xue and Palmer (2004) Pradhan et al. (2005) Toutanova et al. (2005) Johansson and Nugues (2008) Ma\u0300rquez et al. (2008) Punyakanok et al. (2008) Das et al. (2014)\n2. A list of featlets which have been applied\n3. An output buffer of features\nIn our implementation the data fields are:\n\u2022 token1 and token2: are integers \u2022 span1 and span2: are pairs of integers\n(start, end)\n\u2022 value: an untyped object \u2022 sequence: is an untyped list\nEach of the fields in a context start out as a special value NIL. Once they are set, other featlets can read from these fields and put a feature into the output buffer. If a NIL field is read, then the featlet fails and no features are output.\nLabel Extractors This group of featlets are responsible for reading an aspect of the label y and putting it into the context. These are the only taskspecific featlets which the inference algorithm has to be aware of.\n\u2022 TARGETSPAN: sets span2 to a target \u2022 TARGETHEAD: sets token2 to the head of\nthe target span\n\u2022 ARGSPAN: sets span1 to an argument span \u2022 ARGHEAD: sets token1 to the head of an\nargument span\n\u2022 ROLE: sets value to a role \u2022 FRAMEROLE: sets value to the concatena-\ntion of a frame and a role\n\u2022 FRAME: sets value to a frame\nToken Extractors These read from token1 and output a feature.\n\u2022 WORD, POS, LEMMA \u2022 WNSYNSET: reads the lemma and POS tag\nat token1, looks up the first WordNet sense, and puts its synset id onto value.\n\u2022 BROWNCLUST: looks up a un-supervised hierarchical word cluster id for token12\n\u2022 DEPREL: output the syntactic dependency edge label connecting token1 to its parent.\n\u2022 DEPTHD: compute the depth of token1 in a dependency tree\n2One featlet for a 256 and a 1000 cluster output of Liang (2005).\nBefore moving on, it will be helpful to slightly redefine the behavior of token extractors: instead of immediately outputting a string, instead they will store that string in the value field and leave it to the OUTPUT featlet to finish the job of outputting a feature. By convention, we will assume that every string of featlets ends in a (possibly implicit) OUTPUT, so the old meaning of \u201ctoken extractors output a feature\u201d is true as long as the token extractor featlet is not followed by anything.\nValue Mutators In many cases though, we will want normalized or simplified versions of other features. For example we could want to find the shape of a word, \u201cMcDonalds\u201d \u2192 \u201cCcCcccccc\u201d, or perhaps just take the first few characters, \u201cNNP\u201d\u2192 \u201cN\u201d. Value mutators read a string from value, compute a new string, and store it back to value. This enables features like [ARGHEAD, WORD, SHAPE] or [TARGETHEAD, POS, PREFIX1].\n\u2022 LC: if value is a string, output its lowercase \u2022 SHAPE: if value is a string, output its shape \u2022 PREFIXN: sets value to a prefix of length\nAn interesting special case of value mutators are ones which filter. A featlet like CLOSEDCLASS can be applied after WORD but before OUTPUT in order to have a feature only fire for closed class words. This is achieved by CLOSEDCLASS writing NIL to value so that OUTPUT fails and no features are output. This selective firing is valuable because it can lead to expressive feature semantics (e.g. \u201conly output the first word in a span if is in a closed class\u201d).\nDependency Walkers Syntax lets us jump around in a sentence where structural proximity is often a more informative measure of relevance than linear proximity. Dependency walkers3 provide one way of jumping around by reading and writing token1. These can be composed as well to form walks, e.g. \u201cgrandparent\u201d = [PARENTD, PARENTD].\n\u2022 PARENTD: set token1 to its parent in the dependency tree.\n\u2022 LEFTCHILDD: sets token1 to its left-most child in the dependency tree\n\u2022 LEFTSIBD: sets token1 to its next-to-theleft sibling in the dependency tree\n3Every time we list a LEFT featlet, we have omitted its RIGHT equivalent for space.\n\u2022 LEFTMOSTSIBD: sets token1 to its leftmost sibling in the dependency tree\nSome information is contained in the name of a dependency walker (e.g. PARENTD), other information is contained in the edge crossed (e.g. whether the parent is nsubj or dobj). To capture this information, dependency walkers also append the edge that they crossed into the sequence field. This side information can be read out later by other featlets.\nSequence Reducers Values appended to sequence are converted into features with sequence reducers.\n\u2022 NGRAMS: reads n-grams from sequence, outputs each. If value is set, prefixes every n-gram with value. Clears sequence when done.\n\u2022 BAG: special case of n-grams when n=1 \u2022 SEQN: if sequence is no longer than N,\noutput items concatenated (preserves order). Also will prepend value if set. Clears sequence regardless of length.\n\u2022 COMPRESSRUNS: Collapses X Y Y Z Z Z to X Y+ Z+, no output, doesn\u2019t clear sequence\nDependency Representers Going back to dependency walkers for a moment, the edge that they append to sequence need not be a string like nsubj which sequence reducers can operate on. Edges are represented as tuples of (parent, child, deprel)4, and we add featlets which choose a string to represent each edge, called dependency representers. We construct an edge to string function by taking the Cartesian product of the token extractors to represent the parent and the set of functions { EDGEDIRECTIOND5, EDGELABELD6, NOEDGED7 } and make a featlet to map each of these functions over sequence.\nConstituent Walkers There is an equivalent class of featlets to the dependency walkers which instead operate on span1, span2, and a constituency tree, called constituent walker. Each of\n4Sequence reducers fail when attempting to operate nonstring values in sequence, so a representer must be called first.\n5Left or right 6Taken from dependency parse, e.g. nsubj 7A constant string: \u201c*\u201d\nthese operations fail if the span they read is not a constituent.\n\u2022 PARENTC: sets span1 to its parent \u2022 LEFTCHILDC: sets span1 to the left-most\nchild node\n\u2022 LEFTSIBC: sets span1 to the next-to-theleft sibling node\n\u2022 LEFTMOSTSIBC: sets span1 to the leftmost sibling node\nConstituent Representers Constituent walkers differ from dependency walkers in the values that they append to sequence, they store grammar rules like S\u2192 NP VP. Equivalently to dependency representers, the constituency representers are { CATEGORYC8, SUBCATEGORYC9 }.\nTree Walkers Operations longer than one step typically require that a start and endpoint are known to avoid meaningless walks. Tree walkers use both dependency and constituency parses, but take shortest path walks between two endpoints, adding edges or rules to sequence.\n\u2022 TOROOTD: walks from token1 to root \u2022 COMMONPARENTD: walks from token1\nto a common parent and then token2\n\u2022 TOROOTC: walks from span1 to root \u2022 COMMONPARENTC: like COMMONPAR-\nENTD for constituency trees\n\u2022 CHILDREND: walks the children of token1, left to right\n\u2022 CHILDRENC: walks the children of span1, left to right\nLinear Walkers If syntactic trees are not available or accurate, linear walkers can provide another source of relevant information. These featlets append token indices to sequence for multi-step walks, but behave like dependency walkers otherwise, mutating a field such as token1.\n\u2022 LEFTL: moves token1 position if possible \u2022 SPAN1STARTTOENDL: walks tokens in span1\n\u2022 SPAN1LEFTTORIGHTL: like SPAN1STARTTOENDL but expands two tokens in either direction\n\u2022 HEAD1TOSPAN1STARTL\n\u2022 HEAD1TOSPAN1ENDL \u2022 SPAN1TOSPAN2L: adds any tokens between\nthe two spans to sequence\nDistance Functions Distance can be informative, but usually not clear how to represent its scale. Featlets let us address which distances to measure separately from the units used to measure them. Distance functions put a number into the value field:\n\u2022 SEQLENGTH: the number of elements in sequence10\n\u2022 DELTADEPTHD: if values in sequence are dependency nodes or token indices, put the depth of the first minus the second into value\n\u2022 DELTADEPTHC: like DELTADEPTHD for constituency nodes.\nDistance Representers Once a number has been put into value, distance representers write a string representation suitable for a feature back to value.\n\u2022 DASBUCKETS: encodes the bucket widths defined in Das et al. (2014)\n\u2022 DIRECTION: writes +1 or -1 based on the sign of the number given."}, {"heading": "2.1 Finding Legal Templates", "text": "We can figure out which strings of featlets constitute a template (most sequences don\u2019t make sense, like [ARGSPAN, SHAPE]) by brute force search with a few heuristics. We have a rules which filter out strings of featlets like:\n\u2022 nothing can come before a token extractor \u2022 if apply a featlet fails or doesn\u2019t change the\ncontext, stop there (this and all suffixes are invalid)\nWe do a breadth first search over all strings of featlets up to length 6 and collect all strings which are templates: those that produce output on at least 2 of 50 instances, producing 5241 templates.\nAt this point note that since featlets are functions from one context to another, they are closed under function composition.\n10This can be use to measure a variety of distances using linear walkers, like ArgSpan width or the distance between ArgHead and TargetHead."}, {"heading": "2.2 Frequency-based Template Transforms", "text": "For every template found, we produce 5 additional templates by appending the following featlets: TOP10, TOP100, TOP1000, CNT8 CNT16. The TOPN transforms a template by sorting its features by frequency, and only letting the template fire for values with a count at least as high as theN th most common feature. CNTC only lets through features observed at least C times in the training data.\nThese automatic transforms are useful for building products, since they control number of features created. In our experiments, we found that a TOPN template transform appeared in a little less than 50% of our final features and a CNTC feature appeared in a little less than 10%."}, {"heading": "2.3 Template Composition", "text": "To grow features larger than we can discover with brute force enumeration of featlet strings, we consider products of templates. It is common to represent this by string concatenation, but we define template products to be the same as featlet concatenation. This has one importance difference: a template may return no value, in which case the rest of the product returns no value. With string concatenation you can represent one template making another more specific by including more information. With featlet composition you can have a template modulate when another can fire. This is weaker than general featlet composition though, and order doesn\u2019t matter."}, {"heading": "2.4 Near Duplicate Removal", "text": "We will generate pairs of similar and possibly redundant templates. For example: e.g. X1 = [ARGHEAD, LEFTSIBD, WORD] and X2 = [ARGHEAD, LEFTMOSTSIBD, WORD]. Principled approaches like conditional mutual information I(Y ;X2 | X1) could be used to filter, but this would require a lot of computation. It is faster to use the type level (featlet/template names) rather than the token level (values extracted on instance data). We can construct similarity functions for each of the levels of structure we\u2019ve produced.\n1. similarity between two featlets is the normalized Levenshtein edit distance11 between their names, so LEFTSIBD is similar to LEFTMOSTSIBD but not PARENTC.\n11Operations have unit cost and we divide by the length of the longer string.\n2. similarity between two templates (strings of featlets) is again the the normalized Levenshtein edit distance, but over an alphabet of featlets, where the substitution cost is inversely related to the previous similarity function.12\n3. similarity between two features is the maxweight bipartite matching of templates, where the weight is inversely related to the previous similarity function. We don\u2019t use edit distance here since order doesn\u2019t matter.\nWe use this last similarity function to prune ranked lists of features produced in \u00a73.13"}, {"heading": "3 Feature Selection", "text": "Feature generation can lead to too many features to fit in memory. Some of the features we generate may provide no signal towards a label we are trying to predict. To filter down to a manageable set of informative features, we score each template using mutual information (sometimes referred to as information gain), between a label (Bernoulli) and a template (Multinomial). Mutual information has a natural connection to Bayesian independence testing (Minka, 2003). Since computing mutual information is just counting, this task is embarrassingly parallel and can be easily implemented in frameworks like MapReduce.\nWe select a budget of how many features to search over B, and divide that budget up amongst template products up to order n such that order i features get a proportion of the budget of \u03b3i. In these experiments we set B = 3000000 and \u03b3 = 1.5, which meant that the split between features was [21%, 32%, 47%].14 For each split, features were ranked by the max of a heuristic score for each of its templates. Each templates heuristic score was its mutual information plus a Gaussian with mean 0 and standard deviation 2. Randomness was introduced for diversity and so that templates which are more useful as filters (and have low mutual information by themselves) have some chance of being picked.\n12We convert edit distance to similarity by sim(a, b) = k\nk+dist(a,b) with k = 2.\n13We consider two features redundant if the normalized max-weight matching is greater than 0.75. To normalize we divide by the shorter length (in templates) of the two features.\n14These are really maximum proportions, filled up from lowest order to highest order, with extra slots rolled over to the remaining slices proportional to the remaining weights.\nEntropy and mutual information estimation breaks down when the cardinality of the variables is large compared to the number of observations (Paninski, 2003). We observed that entropy estimates based on the maximum likelihood estimates of p(y, x) and p(x) from counts of (y, x) yielded very biased estimates of mutual information (high for sparse features). We correct for this problem by using the BUB entropy estimation method described by Paninski (2003).\nWe produce a final ranked list of features by sorting by I(Y ;X)1+\u03b2H(X) and then applying the greedy pruning described in \u00a72.4. This expression\u2019s limit as \u03b2 \u2192 0 is mutual information and normalized mutual information as \u03b2 \u2192 \u221e. In our experiments, most features had between one and nine nats of entropy, as shown in figure 3, and we created feature sets out of \u03b2 \u2208 {0.01, 0.1, 1, 10}"}, {"heading": "4 Experiments", "text": "In our experiments we use semantic role labeling (SRL) as a case study to test whether our automatic methods can derive feature sets which work as well as hand engineered ones. SRL is a difficult prediction task with more than one structural formulation (type of label). Sometimes arguments are represented by their head token in a dependency tree (Surdeanu et al., 2008; Hajic\u030c et\nal., 2009) and sometimes they are specified by a span or constituent (Carreras and Ma\u0300rquez, 2005; Baker et al., 1998). For span-based SRL, the correspondence between the argument spans and syntactic constituents can be very tight (Kingsbury and Palmer, 2002) or not (Ruppenhofer et al., 2006). Sometimes the role labels depend on the predicate sense (Ruppenhofer et al., 2006) and sometimes they don\u2019t (Kingsbury and Palmer, 2002). These differences indicate that there may not be one \u201cSRL feature set\u201d which works best, or even well, for all variants of the task.\nWe used FrameNet 1.5 (Ruppenhofer et al., 2006) and the CoNLL 2012 data derived from the OntoNotes 5.0 corpus (Pradhan et al., 2013). We used the argument candidate selection method described in Xue and Palmer (2004) as well as the extensions in Hermann et al. (2014). Annotations are provided from the Stanford CoreNLP toolset (Manning et al., 2014). Feature selection is run first on each data set to produce a few feature sets based on \u03b2 and size, then we evaluate their performance using an averaged perceptron (Freund and Schapire, 1999; Collins, 2002). We ran the algorithm for up to 10 passes over the data, shuffling before each pass, and selected the averaged weights which yielded the best F1 on the dev set.15\nSRL Stages Most work on SRL breaks the problem into (at least) two stages: argument identification and role classification. The argument identification stage classifies whether a span is a semantic argument to a particular target, and then the classification stage chooses the role for each span which was selected by the identification stage. We adopt this standard architecture for efficiency: if there are O(s) spans and O(k) roles, it turns an O(sk) decoding problem into anO(s+k) decoding problem.\nGiven this stage-based architecture, we split our budget, half going to each stage. For both we define y to be a Bernoulli variable which is one on sub-structures which appear on the gold parse. For arg id the instances are spans for a particular target, and for role classification the instances are roles for the span chosen in the previous stage. During training we use gold arg id to train the role classifier. For arg id we only score features which contain a ARGHEAD, or ARGSPAN featlet, and for role classification we additionally require\n15For FrameNet data we took a random 10% slice of the training data as the dev set.\na ROLEARG or FRAMEROLEARG featlet appear."}, {"heading": "5 Results", "text": "Overall, our method seems to work about as well as experts manually designing features for SRL. Results in table 5 shows our approach matching the performance of Das et al. (2012) and Pradhan et al. (2013). Other systems achieve better performance, but these models all use global information, an orthogonal issue to the local feature set.\nIn looking at the feature sets generated, one major difference is the complexity parameter \u03b2. For FrameNet, the best value of \u03b2 was 10, meaning that the features with the highest normalized mutual information were chosen, whereas with Propbank and \u03b2 = 0.01, it was better to ignore the entropy of the feature. This makes sense in retrospect when you consider the size of the training sets, Propbank is about 20 times larger, but its not clear how much data is needed to justify this shift when tuning by hand.\nThis difference in selection criteria does lead to very different feature sets chosen,16 but it is another question of whether this matters towards system performance. It could be that there are many different types of feature sets which lead to good performance on either task/dataset, and only one is needed (possibly created manually). In table 5 we show the effects generating a feature set for one dataset and applying it to the other. The performance on the diagonal is considerably higher, indicating empirically that there likely isn\u2019t one\n16There are actually only two templates in common between the best FrameNet and Propbank feature sets. Both contain the COMMONPARENTD featlet.\n\u201cSRL feature set\u201d. If you weight both equally, the average increase in error due to domain shift is 7.9%. This is even the case for feature selection with FrameNet, where you might expect that selecting features on Propbank, a much larger resource, could yield gains because of much lower variance without much bias.\nSensitivity by Stage Given that we can automatically generate feature sets, we can easily determine how adding or removing features from each stage will affect performance. This is useful for choosing a feature set which balances the cost of prediction time with performance, which is labor intensive and error prone when done manually. Table 5 shows that the model is more sensitive to the removal of argument id features than role classification ones. This is not a new result (Ma\u0300rquez et al., 2008), but this work offers a way to respond by applying computational rather than human resources to the problem."}, {"heading": "6 Discussion", "text": "Limitations On limitation of the methods described here is finding symmetries. The product operator for templates is commutative, but this is\nnot the case for featlets. Some templates are equivalent and there is no easy way to check short of checking their denotations, which is expensive.\nAnother issue is that a lot of features are required. The best models we trained for FrameNet use over 2500 features, which is significantly more than Das et al. (2012), which used 34. Upon manual inspection of the feature sets we learned, we find most if not all of the features that Das et al. (2012) created,17 but precision is low."}, {"heading": "7 Related Work", "text": "Recently there has been a swell in interest in neural methods in NLP which use continuous representations rather than discrete feature weights. This work shares some motivation with neural methods, e.g. the desire to avoid domain expertderived features, but we diverge primarily for computational reasons. This work is about model generation and scoring, and it is not clear how to score neural models in ways that don\u2019t involve re-training a model. Feature based models are amenable decomposition and information theoretic analysis in ways that neural models aren\u2019t.\nWithin feature based methods, backwards selection methods are common, including a large body of work on sparsity-inducing regularization, the canonical being the lasso (Tibshirani, 1996). These methods are applicable when the entire feature set can be enumerated and scored on one machine. This is not feasible for this work, since we generate features which lie in a combinatorial space too large to fit in memory. For example, our method found the feature [ TARGETHEAD, RIGHT, WNSYNSET, ARGSPAN, SPAN1START, LEFTL, WORD, TOP10, SPAN1TOSPAN2L, SEQMAPPOS, BAG ], which is comprised of 11 featlets18.\nThe alternative are forwards selection methods which work by building bigger features from smaller ones. Bjo\u0308rkelund et al. (2009) used forwards selection for dependency-based SRL (Hajic\u030c et al., 2009) for 7 languages based on products of templates. Their experiments showed a great diversity of the features learned for different languages and they placed second in the shared\n17It is a not trivial to match our templates to theirs. For example, the template [TARGETHEAD, LEFTL, TOP10] * [TARGETHEAD, CHILDSEQUENCED, SEQMAPDEPREL, BAG] is likely close to the passive voice template used in Das et al. (2012), since \u201cwas\u201d and \u201cbe\u201d are in the top 10 words to the left of a verb.\n18Technically it is 13 featlets since an OUTPUT featlet is not written after the WNSYNSET and TOP10 featlets, \u00a72.\ntask. McCallum (2003) used forwards selection for named entity recognition, scoring new feature products using approximate model re-fitting (pseudo-likelihood), which also produced good results. In both of these works, scoring new features depended on the output of a smaller feature set. Sequential methods like this are not amenable to paralellization and take quadratic time with respect to the number of feature to be searched over.\nOur method is more similar to the work of Gormley et al. (2014) where every template is scored in parallel irrespective of a trained model.\nFuture work This work dove-tails with the approach described by Lee et al. (2007), which derives a prior or regularization constant for individual features by looking at properties of the feature (meta features). This work generates features with a lot of structure, which the learner could reflect upon to improve regularization and generalization.\nThe structure in these features can also inform parameterization. Tensor decomposition methods of fixed-order tensors have been used to great effect (Lei et al., 2014; Lei et al., 2015). Low-rank or embedding methods (e.g. RNNs) for parameterizing featlet strings, as opposed to storing a weight in a dictionary, could also improve regularization.\nStep-wise methods which select some features, fit a model, and then select more features with respect to mutual information with residuals, are another simple and promising direction."}, {"heading": "8 Conclusion", "text": "In this work we propose a general framework for generating feature sets with the goal of removing expert engineering from the machine learning loop. Our approach is based on composing units called featlets to create templates. Featlets are small functions which are task agnostic and easy to define and implement by non-experts. Featlets on one hand preserve a wide variety of nuanced feature semantics, and on the other can be enumerated automatically to derive a huge amount of novel templates and features. We validate our approach on semantic role labeling and achieve performance on par with models that had considerable expert intervention."}], "references": [{"title": "The berkeley framenet", "author": ["Baker et al.1998] Collin F Baker", "Charles J Fillmore", "John B Lowe"], "venue": null, "citeRegEx": "Baker et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Multilingual semantic role labeling", "author": ["Love Hafdell", "Pierre Nugues"], "venue": "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task,", "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2009}, {"title": "Introduction to the conll-2005 shared task: Semantic role labeling", "author": ["Carreras", "M\u00e0rquez2005] Xavier Carreras", "Llu\u0131\u0301s M\u00e0rquez"], "venue": "In Proceedings of the Ninth Conference on Computational Natural Language Learning,", "citeRegEx": "Carreras et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2005}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "An exact dual decomposition algorithm for shallow semantic parsing with constraints. In SemEval, SemEval \u201912", "author": ["Das et al.2012] Dipanjan Das", "Andr\u00e9 F.T. Martins", "Noah A. Smith"], "venue": null, "citeRegEx": "Das et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Das et al\\.", "year": 2012}, {"title": "Semantic role labelling with neural network factors", "author": ["Oscar T\u00e4ckstr\u00f6m", "Kuzman Ganchev", "Dipanjan Das"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "FitzGerald et al\\.,? \\Q2015\\E", "shortCiteRegEx": "FitzGerald et al\\.", "year": 2015}, {"title": "Large margin classification using the perceptron algorithm", "author": ["Freund", "Schapire1999] Yoav Freund", "Robert E Schapire"], "venue": "Machine learning,", "citeRegEx": "Freund et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1999}, {"title": "Automatic labeling of semantic roles", "author": ["Gildea", "Jurafsky2002] Daniel Gildea", "Daniel Jurafsky"], "venue": "Computational linguistics,", "citeRegEx": "Gildea et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gildea et al\\.", "year": 2002}, {"title": "Low-resource semantic role labeling", "author": ["Margaret Mitchell", "Benjamin Van Durme", "Mark Dredze"], "venue": "In Proceedings of ACL, June", "citeRegEx": "Gormley et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gormley et al\\.", "year": 2014}, {"title": "Semantic frame identification with distributed word representations", "author": ["Dipanjan Das", "Jason Weston", "Kuzman Ganchev"], "venue": "In Proceedings of ACL. Association for Computational Linguistics", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Dependency-based semantic role labeling of propbank", "author": ["Johansson", "Nugues2008] Richard Johansson", "Pierre Nugues"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Johansson et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Johansson et al\\.", "year": 2008}, {"title": "From treebank to propbank", "author": ["Kingsbury", "Palmer2002] Paul Kingsbury", "Martha Palmer"], "venue": "In LREC. Citeseer", "citeRegEx": "Kingsbury et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kingsbury et al\\.", "year": 2002}, {"title": "Learning a meta-level prior for feature relevance from multiple related tasks", "author": ["Lee et al.2007] Su-In Lee", "Vassil Chatalbashev", "David Vickrey", "Daphne Koller"], "venue": "In Proceedings of the 24th International Conference on Machine Learning,", "citeRegEx": "Lee et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2007}, {"title": "Low-rank tensors for scoring dependency structures", "author": ["Lei et al.2014] Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Lei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "High-order low-rank tensors for semantic role labeling", "author": ["Lei et al.2015] Tao Lei", "Yuan Zhang", "Llu\u0131\u0301s M\u00e0rquez", "Alessandro Moschitti", "Regina Barzilay"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Semi-supervised learning for natural language", "author": ["Percy Liang"], "venue": "Master\u2019s thesis, Massachusetts Institute of Technology", "citeRegEx": "Liang.,? \\Q2005\\E", "shortCiteRegEx": "Liang.", "year": 2005}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Semantic role labeling: An introduction", "author": ["Xavier Carreras", "Kenneth C. Litkowski", "Suzanne Stevenson"], "venue": null, "citeRegEx": "M\u00e0rquez et al\\.,? \\Q2008\\E", "shortCiteRegEx": "M\u00e0rquez et al\\.", "year": 2008}, {"title": "Efficiently inducing features of conditional random fields", "author": ["Andrew McCallum"], "venue": "In Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "McCallum.,? \\Q2003\\E", "shortCiteRegEx": "McCallum.", "year": 2003}, {"title": "Bayesian inference, entropy, and the multinomial distribution", "author": ["Tom Minka"], "venue": "Online tutorial", "citeRegEx": "Minka.,? \\Q2003\\E", "shortCiteRegEx": "Minka.", "year": 2003}, {"title": "Estimation of entropy and mutual information", "author": ["Liam Paninski"], "venue": "Neural computation,", "citeRegEx": "Paninski.,? \\Q2003\\E", "shortCiteRegEx": "Paninski.", "year": 2003}, {"title": "Support vector learning for semantic argument classification", "author": ["Kadri Hacioglu", "Valerie Krugler", "Wayne Ward", "James H Martin", "Daniel Jurafsky"], "venue": "Machine Learning,", "citeRegEx": "Pradhan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2005}, {"title": "Towards robust linguistic analysis using ontonotes", "author": ["Alessandro Moschitti", "Nianwen Xue", "Hwee Tou Ng", "Anders Bj\u00f6rkelund", "Olga Uryupina", "Yuchen Zhang", "Zhi Zhong"], "venue": "In Proceedings of the Seven-", "citeRegEx": "Pradhan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2013}, {"title": "The importance of syntactic parsing and inference in semantic role labeling", "author": ["Dan Roth", "Wen-tau Yih"], "venue": null, "citeRegEx": "Punyakanok et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2008}, {"title": "Framenet ii: Extended theory and practice", "author": ["Michael Ellsworth", "Miriam RL Petruck", "Christopher R Johnson", "Jan Scheffczyk"], "venue": null, "citeRegEx": "Ruppenhofer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ruppenhofer et al\\.", "year": 2006}, {"title": "The conll-2008 shared task on joint parsing of syntactic and semantic dependencies", "author": ["Richard Johansson", "Adam Meyers", "Llu\u0131\u0301s M\u00e0rquez", "Joakim Nivre"], "venue": "In Proceedings of the Twelfth Conference on Computa-", "citeRegEx": "Surdeanu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2008}, {"title": "Efficient inference and structured learning for semantic role labeling", "author": ["Kuzman Ganchev", "Dipanjan Das"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2015\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2015}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Joint learning improves semantic role labeling", "author": ["Aria Haghighi", "Christopher D. Manning"], "venue": "In Proceedings of the 43rd Annual Meeting", "citeRegEx": "Toutanova et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2005}, {"title": "Calibrating features for semantic role labeling", "author": ["Xue", "Palmer2004] Nianwen Xue", "Martha Palmer"], "venue": "In EMNLP,", "citeRegEx": "Xue et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2004}, {"title": "Endto-end learning of semantic role labeling using recurrent neural networks", "author": ["Zhou", "Xu2015] Jie Zhou", "Wei Xu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 21, "context": "1 Further, discrepancies between authors is not unheard of: Gildea and Jurafsky (2002) report 5% of verbs were passive in the PTB, while Pradhan et al. (2005) report 11%.", "startOffset": 137, "endOffset": 159}, {"referenceID": 19, "context": "1 Gildea and Jurafsky (2002) Xue and Palmer (2004) Pradhan et al. (2005) Toutanova et al.", "startOffset": 51, "endOffset": 73}, {"referenceID": 19, "context": "1 Gildea and Jurafsky (2002) Xue and Palmer (2004) Pradhan et al. (2005) Toutanova et al. (2005) Johansson and Nugues (2008) M\u00e0rquez et al.", "startOffset": 51, "endOffset": 97}, {"referenceID": 19, "context": "1 Gildea and Jurafsky (2002) Xue and Palmer (2004) Pradhan et al. (2005) Toutanova et al. (2005) Johansson and Nugues (2008) M\u00e0rquez et al.", "startOffset": 51, "endOffset": 125}, {"referenceID": 16, "context": "(2005) Johansson and Nugues (2008) M\u00e0rquez et al. (2008) Punyakanok et al.", "startOffset": 35, "endOffset": 57}, {"referenceID": 16, "context": "(2005) Johansson and Nugues (2008) M\u00e0rquez et al. (2008) Punyakanok et al. (2008) Das et al.", "startOffset": 35, "endOffset": 82}, {"referenceID": 4, "context": "(2008) Das et al. (2014) 2.", "startOffset": 7, "endOffset": 25}, {"referenceID": 15, "context": "One featlet for a 256 and a 1000 cluster output of Liang (2005).", "startOffset": 51, "endOffset": 64}, {"referenceID": 4, "context": "\u2022 DASBUCKETS: encodes the bucket widths defined in Das et al. (2014)", "startOffset": 51, "endOffset": 69}, {"referenceID": 19, "context": "Mutual information has a natural connection to Bayesian independence testing (Minka, 2003).", "startOffset": 77, "endOffset": 90}, {"referenceID": 20, "context": "Entropy and mutual information estimation breaks down when the cardinality of the variables is large compared to the number of observations (Paninski, 2003).", "startOffset": 140, "endOffset": 156}, {"referenceID": 20, "context": "Entropy and mutual information estimation breaks down when the cardinality of the variables is large compared to the number of observations (Paninski, 2003). We observed that entropy estimates based on the maximum likelihood estimates of p(y, x) and p(x) from counts of (y, x) yielded very biased estimates of mutual information (high for sparse features). We correct for this problem by using the BUB entropy estimation method described by Paninski (2003).", "startOffset": 141, "endOffset": 457}, {"referenceID": 25, "context": "Sometimes arguments are represented by their head token in a dependency tree (Surdeanu et al., 2008; Haji\u010d et al., 2009) and sometimes they are specified by a span or constituent (Carreras and M\u00e0rquez, 2005; Baker et al.", "startOffset": 77, "endOffset": 120}, {"referenceID": 0, "context": ", 2009) and sometimes they are specified by a span or constituent (Carreras and M\u00e0rquez, 2005; Baker et al., 1998).", "startOffset": 66, "endOffset": 114}, {"referenceID": 24, "context": "For span-based SRL, the correspondence between the argument spans and syntactic constituents can be very tight (Kingsbury and Palmer, 2002) or not (Ruppenhofer et al., 2006).", "startOffset": 147, "endOffset": 173}, {"referenceID": 24, "context": "Sometimes the role labels depend on the predicate sense (Ruppenhofer et al., 2006) and sometimes they don\u2019t (Kingsbury and Palmer, 2002).", "startOffset": 56, "endOffset": 82}, {"referenceID": 24, "context": "5 (Ruppenhofer et al., 2006) and the CoNLL 2012 data derived from the OntoNotes 5.", "startOffset": 2, "endOffset": 28}, {"referenceID": 22, "context": "0 corpus (Pradhan et al., 2013).", "startOffset": 9, "endOffset": 31}, {"referenceID": 16, "context": "Annotations are provided from the Stanford CoreNLP toolset (Manning et al., 2014).", "startOffset": 59, "endOffset": 81}, {"referenceID": 3, "context": "Feature selection is run first on each data set to produce a few feature sets based on \u03b2 and size, then we evaluate their performance using an averaged perceptron (Freund and Schapire, 1999; Collins, 2002).", "startOffset": 163, "endOffset": 205}, {"referenceID": 18, "context": "0 corpus (Pradhan et al., 2013). We used the argument candidate selection method described in Xue and Palmer (2004) as well as the extensions in Hermann et al.", "startOffset": 10, "endOffset": 116}, {"referenceID": 8, "context": "We used the argument candidate selection method described in Xue and Palmer (2004) as well as the extensions in Hermann et al. (2014). Annotations are provided from the Stanford CoreNLP toolset (Manning et al.", "startOffset": 112, "endOffset": 134}, {"referenceID": 4, "context": "6 Das et al. (2012) local 7 67.", "startOffset": 2, "endOffset": 20}, {"referenceID": 4, "context": "6 Das et al. (2012) local 7 67.7 59.8 63.5 Das et al. (2012) constrained 3 70.", "startOffset": 2, "endOffset": 61}, {"referenceID": 20, "context": "2 Pradhan et al. (2013) 7 81.", "startOffset": 2, "endOffset": 24}, {"referenceID": 20, "context": "2 Pradhan et al. (2013) 7 81.3 70.5 75.5 Pradhan et al. (2013) (revised) 7 78.", "startOffset": 2, "endOffset": 63}, {"referenceID": 20, "context": "2 Pradhan et al. (2013) 7 81.3 70.5 75.5 Pradhan et al. (2013) (revised) 7 78.5 76.7 77.5 T\u00e4ckstr\u00f6m et al. (2015) 3 80.", "startOffset": 2, "endOffset": 114}, {"referenceID": 5, "context": "4 FitzGerald et al. (2015) 3 80.", "startOffset": 2, "endOffset": 27}, {"referenceID": 4, "context": "Results in table 5 shows our approach matching the performance of Das et al. (2012) and Pradhan et al.", "startOffset": 66, "endOffset": 84}, {"referenceID": 4, "context": "Results in table 5 shows our approach matching the performance of Das et al. (2012) and Pradhan et al. (2013). Other systems achieve better performance, but these models all use global information, an orthogonal issue to the local feature set.", "startOffset": 66, "endOffset": 110}, {"referenceID": 17, "context": "This is not a new result (M\u00e0rquez et al., 2008), but this work offers a way to respond by applying computational rather than human resources to the problem.", "startOffset": 25, "endOffset": 47}, {"referenceID": 4, "context": "The best models we trained for FrameNet use over 2500 features, which is significantly more than Das et al. (2012), which used 34.", "startOffset": 97, "endOffset": 115}, {"referenceID": 4, "context": "The best models we trained for FrameNet use over 2500 features, which is significantly more than Das et al. (2012), which used 34. Upon manual inspection of the feature sets we learned, we find most if not all of the features that Das et al. (2012) created,17 but precision is low.", "startOffset": 97, "endOffset": 249}, {"referenceID": 27, "context": "Within feature based methods, backwards selection methods are common, including a large body of work on sparsity-inducing regularization, the canonical being the lasso (Tibshirani, 1996).", "startOffset": 168, "endOffset": 186}, {"referenceID": 1, "context": "Bj\u00f6rkelund et al. (2009) used forwards selection for dependency-based SRL (Haji\u010d et al.", "startOffset": 0, "endOffset": 25}, {"referenceID": 4, "context": "For example, the template [TARGETHEAD, LEFTL, TOP10] * [TARGETHEAD, CHILDSEQUENCED, SEQMAPDEPREL, BAG] is likely close to the passive voice template used in Das et al. (2012), since \u201cwas\u201d and \u201cbe\u201d are in the top 10 words to the left of a verb.", "startOffset": 157, "endOffset": 175}, {"referenceID": 4, "context": "For example, the template [TARGETHEAD, LEFTL, TOP10] * [TARGETHEAD, CHILDSEQUENCED, SEQMAPDEPREL, BAG] is likely close to the passive voice template used in Das et al. (2012), since \u201cwas\u201d and \u201cbe\u201d are in the top 10 words to the left of a verb. Technically it is 13 featlets since an OUTPUT featlet is not written after the WNSYNSET and TOP10 featlets, \u00a72. task. McCallum (2003) used forwards selection for named entity recognition, scoring new feature products using approximate model re-fitting (pseudo-likelihood), which also produced good results.", "startOffset": 157, "endOffset": 378}, {"referenceID": 8, "context": "Our method is more similar to the work of Gormley et al. (2014) where every template is scored in parallel irrespective of a trained model.", "startOffset": 42, "endOffset": 64}, {"referenceID": 12, "context": "Future work This work dove-tails with the approach described by Lee et al. (2007), which derives a prior or regularization constant for individual features by looking at properties of the feature (meta features).", "startOffset": 64, "endOffset": 82}, {"referenceID": 13, "context": "Tensor decomposition methods of fixed-order tensors have been used to great effect (Lei et al., 2014; Lei et al., 2015).", "startOffset": 83, "endOffset": 119}, {"referenceID": 14, "context": "Tensor decomposition methods of fixed-order tensors have been used to great effect (Lei et al., 2014; Lei et al., 2015).", "startOffset": 83, "endOffset": 119}], "year": 2017, "abstractText": "Hand-engineered feature sets are a well understood method for creating robust NLP models, but they require a lot of expertise and effort to create. In this work we describe how to automatically generate rich feature sets from simple units called featlets, requiring less engineering. Using information gain to guide the generation process, we train models which rival the state of the art on two standard Semantic Role Labeling datasets with almost no task or linguistic insight.", "creator": "LaTeX with hyperref package"}}}