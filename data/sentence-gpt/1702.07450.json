{"id": "1702.07450", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2017", "title": "Strongly-Typed Agents are Guaranteed to Interact Safely", "abstract": "As artificial agents proliferate, it is becoming increasingly important to ensure that their interactions with one another are well-behaved. In this paper, we formalize a common-sense notion of when algorithms are well-behaved: an algorithm is safe if it does no harm. Motivated by recent progress in deep learning, we focus on the specific case where agents update their actions according to gradient descent. The first result is that gradient descent converges to a Nash equilibrium in safe games. In this paper, we apply an algorithm to the gradient descent algorithm and a model of how the gradient descent algorithms converge. We compare the two algorithm parameters in a series of cases where the gradient descent algorithm and the gradient descent algorithms converge. We use a method to infer the optimal and optimal gradient descent algorithms by comparing these two algorithm parameters. We analyze the results from the performance tests, while trying to calculate the optimal and optimal gradient descent algorithms by examining the performance tests from the performance tests. We use gradient descent to calculate the optimal and optimal gradient descent algorithms by comparing these two algorithm parameters. We perform the statistical regression test in one case by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using this algorithm to predict the optimal and optimal gradient descent algorithms by using", "histories": [["v1", "Fri, 24 Feb 2017 02:30:15 GMT  (360kb,D)", "http://arxiv.org/abs/1702.07450v1", "13 pages"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.GT", "authors": ["david balduzzi"], "accepted": true, "id": "1702.07450"}, "pdf": {"name": "1702.07450.pdf", "metadata": {"source": "CRF", "title": "Strongly-Typed Agents are Guaranteed to Interact Safely", "authors": ["David Balduzzi"], "emails": ["<dbalduzzi@gmail.com>."], "sections": [{"heading": null, "text": "The paper provides sufficient conditions that guarantee safe interactions. The main contribution is to define strongly-typed agents and show they are guaranteed to interact safely. A series of examples show that strong-typing generalizes certain key features of convexity and is closely related to blind source separation. The analysis introduce a new perspective on classical multilinear games based on tensor decomposition."}, {"heading": "1. Introduction \u201cFirst, do no harm\u201d", "text": "Recent years have seen rapid progress on core problems in artificial intelligence such as object and voice recognition (Hinton & et al, 2012; Krizhevsky et al., 2012), playing video and board games (Mnih et al., 2015; Silver et al., 2016), and driving autonomous vehicles (Zhang et al., 2016). As artificial agents proliferate, it is increasingly important to ensure their interactions with one another, with humans, and with their environment are safe.\nConcretely, the number of neural networks being trained and used is growing rapidly. There are enormous and increasing economies of scale that can likely be derived from treating them as populations \u2013 rather than as isolated algorithms. How to ensure interacting neural networks cooperate effectively? When can weights trained on one problem be adapted to another without adverse effects? The problems fall under mechanism design, a branch of game theory\n1Victoria University of Wellington, New Zealand. Correspondence to: David Balduzzi <dbalduzzi@gmail.com>.\n(Nisan et al., 2007). However, neural nets differ from humans in that they optimize clear objectives using gradient descent. The setting is thus more structured than traditional mechanism design.\nSafety. The first contribution of the paper is formalize safety as a criterion on how agents interact. We propose a basic notion of safety based on the common-sense principle that agents should do no harm to one another. More formally, each agent optimizes an objective whose value depends on the actions of the agent and the actions of the rest of the population. A game is safe if the actions chosen by each agent do no (infinitesimal) harm to any other agent, where harm is measured as increased loss.\nThe key simplifying assumption in the paper is to take gradient descent as a computational primitive (Balduzzi, 2016). Questions about mechanism design are sharpened under the assumption that agents use gradient descent. The assumption holds broadly since the key driver of progress in artificial intelligence is deep learning, which uses gradient descent to optimize complicated objective functions composed from simple differentiable modules (LeCun et al., 2015).\nA weakness of the approach is that it conceives safety more narrowly than, for example, Amodei et al. (2016) which is concerned with societal risks arising from artificial intelligence. We argue that a necessary foundational component of the broader AI-safety project is to clarify exactly what safety entails when the objectives of the agents and the algorithms they employ are precisely specified.\nStrongly-typed games. The second contribution is to introduce type systems suited to multi-agent optimization problems (that is, games). We build on the typed linear algebra introduced in Balduzzi & Ghifary (2016). The nomenclature is motivated by an analogy with types in the theory of programming. Type systems are used to prevent untrapped errors (errors that go unnoticed and cause arbitrary behavior later on) when running a program (Cardelli, 1997). A program is safe if it does not cause untrapped errors. Type systems can enforce safety by statically rejecting all programs that are potentially unsafe.\nThe idea underlying types in programming is that \u201clike should interact with like\u201d. Typed linear algebra, defini-\nar X\niv :1\n70 2.\n07 45\n0v 1\n[ cs\n.L G\n] 2\n4 Fe\nb 20\n17\ntion 1, formalizes \u201clike interacts with like\u201d in the simplest possible way \u2013 by fixing an orthogonal basis. Section 2 introduces a wider class of games than in the literature and defines safety. Theorem 1 shows that gradient descent converges to a Nash equilibrium in safe games. Section 3 extracts the key ingredients required for safe gradients from two warmup examples. The ingredients are simultaneous diagonalization, i.e. the existence of a shared latent orthogonal basis, and monotonic covariation, i.e. that the derivatives of the objectives have the same sign in the latent coordinate system. The main result, theorem 2, is that strongly-typed games are guaranteed to be safe.\nImplications. Safety and strong-typing generalize key properties of convexity. Convexity is of course the gold standard for well-behaved gradients. We uncover latent types and demonstrate safety of Newton\u2019s method, natural gradient and mirror descent; see sections 3.2, A1 and A2.\nThe main theme of sections 4 and 5 is disentangling latent factors. We show that strong-typing in quadratic games is closely related to blind source separation. Section 5 analyzes classical N -player games. The analysis yields a new perspective on classical games based on tensor-SVD that is closely related to independent component analysis.\nSections 6 and A5 switch to neural networks and analyze two biologically plausible variants of backpropagation (Balduzzi et al., 2015; Lillicrap et al., 2016). We show that the main results of the papers are to prove the respective algorithms are safe.\nScope and related work. This paper lays the foundations of safety in gradient-based optimization. Applications are deferred to future work.\nThe literature on safety is mostly focused on problems arising in reinforcement learning, for example ensuring agents avoid dangerous outcomes (Turchetta et al., 2016; Amodei et al., 2016; Berkenkamp et al., 2016). Gradients are typically not available in reinforcement learning problems. We study interactions between algorithms with clearly defined objectives that utilize gradient-based optimization, which gives a more technical perspective.\nThe idea of a population of neural networks solving multiple related tasks is developed in Fernando et al. (2017), which uses genetic algorithms to adapt components to new tasks. However, they repeatedly reinitialize components to undo the damage done by the genetic algorithm. Our work is intended, ultimately, to help design algorithms that detect and avoid damaging updates. A recent survey paper argues the brain optimizes a family of complementary loss functions (Marblestone et al., 2016) without considering how the complementarity of the loss functions could be checked or enforced.\nThe idea of investigating game-theoretic and mechanism design questions specific to certain classes of algorithms is introduced in Rakhlin & Sridharan (2013); Syrgkanis et al. (2015). The papers consider how convergence in games can be accelerated if the players use variants of mirror descent.\nTerminology. If \u03b1 \u2265 0 then \u03b1 is positive; if \u03b1 > 0 then it is strictly positive. A (not necessarily square) matrix D is diagonal if dij = 0 for all i 6= j and similarly for tensors. Vectors are columns. The inner product is \u3008v,w\u3009 = v\u1d40w."}, {"heading": "2. Safety", "text": ""}, {"heading": "2.1. Types and orthogonal projections", "text": "Let us recall some basic facts about orthogonal projections. Let (V, \u3008\u2022, \u2022\u3009) be a vector space equipped with an inner product. An orthogonal projection is a linear transform \u03c0 : V \u2192 V that is\nO1. idempotent, \u03c02 = \u03c0, and\nO2. self-adjoint, \u3008\u03c0v,v\u2032\u3009 = \u3008v,\u03c0v\u2032\u3009 for any v,v\u2032 \u2208 V .\nLemma 1. Let P denote an (n \u00d7 k)-matrix with orthogonal columns p1, . . . ,pk. Then the (n\u00d7 n)-matrix PP\u1d40 =\u2211k i=1 pi\u3008pi, \u2022\u3009 = \u2211k i=1 pip \u1d40 i is an (orthogonal) projection.\nLemma 2. If two orthogonal projections\u03c0 and \u03c4 commute then their product is an orthogonal projection.\nProof. Let q := \u03c0\u03c4 . If \u03c0\u03c4 = \u03c4\u03c0 then\nq2 = \u03c0\u03c4 \u00b7 \u03c0\u03c4 = \u03c0\u03c0 \u00b7 \u03c4\u03c4 = \u03c0\u03c4 = q.\nChecking self-adjointness is an exercise. Definition 1. A type TV = ( V, \u3008\u2022, \u2022\u3009, {\u03c0r}Rr=1 ) is a Ddimensional vector space with an inner product and orthogonal projections \u03c0r : V \u2192 V such that \u03c0r\u03c0s = 0 for r 6= s and \u2211Rr=1 \u03c0r = IV is the identity. Type TV has dimension D and rank R.\nA full rank type, D = R, is equivalent to a vector space equipped with an orthogonal basis. Lower rank types are less rigid, and can be thought of as vector spaces equipped with generalized orthogonal coordinates."}, {"heading": "2.2. Safe games", "text": "Definition 2. A game consists of a type TV , feasible set H \u2282 V , players [N ] := {1, . . . , N}, losses `n : H \u2192 R, and an assignment \u03c1 : [N ]\u2192 [R] of players to projections.\nThe type structure and assignments specify the coordinates controlled by each player. On round t, player n chooses\n\u03betn \u2208 V and updates the joint action via\nwt+1 = wt \u2212 \u03c0\u03c1(n)(\u03betn) where wt,wt+1 \u2208 H.\nThe projection \u03c0\u03c1(n) specifies the coordinates of the jointaction vector that player n can modify. Example 1. In a block game actions w \u2208 V = \u220fNn=1 RDn decompose as w = (w1, . . . ,wN ) where the nth player can modify the coordinates in wn. The orthogonal projections \u03c0n(w) = (0, . . . ,wn, . . . ,0) form a rank-N type with \u03c1(n) = n for all n \u2208 [N ]. Example 2. In an open game the type has rank(TV ) = 1 so the single projection is the identity and \u03c1(n) = 1 for all n. Every player can modify all the coordinates.\nBlock games coincide with the standard definition of a game in the literature. Open games arise below when considering Newton\u2019s method, natural gradients, mirror descent and neural networks.\nThe goal of each player is to minimize its loss. Safety is the condition that no player\u2019s updates harm any other player.\nDefinition 3. It is safe for player m to choose \u03betm \u2208 V if it does no infinitesimal harm to any player\n\u2329 \u03c0\u03c1(m)(\u03be t m),\u2207 `n(wt) \u232a \u2265 0 for all n \u2208 [N ].\nA game is safe if it is safe for players to use gradient descent: i.e. if choosing \u03betm := \u2207 `m(wt) is safe for all m.\nIt is worth getting a degenerate case out of the way. A block game is decomposable if player m\u2019s loss only depends on the actions it controls. Intuitively, a decomposable game is N independent optimization problems. More formally:\nLemma 3. A block game is decomposable if `m(w) = `m(\u03c0mw) for all w andm. Decomposable games are safe.\nProof. Since \u03c0m is self-adjoint, we have that \u3008\u03c0m\u03be,\u03b7\u3009 = \u3008\u03c0m\u03be,\u03c0m\u03b7\u3009. Decomposability implies \u03c0m(\u2207 `n) = 0 when m 6= n, so\n\u2329 \u03c0m(\u2207 `m),\u03c0m(\u2207 `n) \u232a = { \u2016\u03c0m(\u2207 `m)\u201622 if m = n 0 else\nwhich is always positive."}, {"heading": "2.3. Convergence", "text": "A block game is convex if the feasible set H is compact and convex and the losses `n : H \u2192 R are convex in the coordinates controlled by the respective players. Nash equilibria are guaranteed to exist in convex block games (Nash, 1950). However, finding them is often intractable (Daskalakis et al., 2009). We show gradient descent converges to a Nash equilibrium in safe convex games.\nTheorem 1. Gradient descent converges to a Nash equilibrium in safe convex games with smooth losses.\nProof. Introduce potential function \u03a6(w) = \u2211N n=1 \u03b1n \u00b7 `n(w) where \u03b1n > 0 are strictly positive. Then\n\u3008\u03c0m(\u2207\u03a6),\u2207 `m\u3009 = N\u2211\nn=1\n\u03b1n \u2329 \u03c0m(\u2207 `n),\u2207 `m \u232a (1)\n\u2265 \u03b1m \u00b7 \u2016\u03c0m(\u2207 `m)\u201622 \u2265 0 since safety implies the cross-terms are nonnegative. The players\u2019 updates therefore converge to either a critical point of \u03a6 or to the boundary of the feasible set. Suppose gradient descent converges to the interior of H. Eq (1) implies that if\u2207\u03a6 = 0 then \u03c0m(\u2207 `m) = 0 for all m. By convexity of the losses, the critical point is a minimizer of each loss with respect to that player\u2019s actions, implying it is a Nash equilibrium. A similar argument holds if gradient descent converges to the boundary.\nExample 3 (convergence in a safe constrained game). Consider a two-player block game with `1(x, y) = x+ 2y and `2(x, y) = 2x + y where player-1 controls x and player2 controls y. Introduce feasible set H = {(x, y) \u2208 R2 : x2 + y2 \u2264 1}. The game is convex and safe. The set of Nash equilibria is the bottom-left quadrant of the boundary {(x, y) \u2208 H : x, y \u2264 0 and x2 + y2 = 1}. Gradient descent with positive combinations of \u03c01(\u2207 `1) = \u2202\u2202x and \u03c02(\u2207 `2) = \u2202\u2202y always converges to a Nash equilibrium. A simple game that does not converge is the following. Example 4 (convergence requires positivity). Consider the two-player block game `1(x, y) = xy and `2(x, y) = \u2212xy where player-1 controls x and player-2 controls y. The Nash equilibrium is the origin (x, y) = (0, 0). However, gradient descent does not converge. Observe that \u2207 `1 = y \u2202\u2202x + x \u2202 \u2202y and \u2207 `2 = \u2212y \u2202\u2202x \u2212 x \u2202\u2202y so \u03c01(\u2207 `1) = y \u2202\u2202x and \u03c02(\u2207 `2) = \u2212x \u2202\u2202y . The flow \u03c01(\u2207 `1) + \u03c02(\u2207 `2) rotates around the origin. No positive combination of \u03c01(\u2207 `1) and \u03c02(\u2207 `2) converges to the origin."}, {"heading": "3. Strongly-Typed Games", "text": "Strong-typing is based two key ideas: diagonalization and positivity. Diagonalization is an important tool in applied mathematics. The Fourier transform simultaneously diagonalizes differentiation and convolution:\nF ( df dx ) = 2\u03c0i\u03c9F(f) and F(f \u2217 g) = F(f) \u00b7 F(g)\nThe SVD diagonalizes any matrix: Q\u1d40MP = D. Finally, the Legendre transform f\u2217(\u03b7) = max\u03b8{\u03b7\u1d40\u03b8 \u2212 f(\u03b8)} diagonalizes the infimal convolution\n(f g)\u2217 = f\u2217+g\u2217 for (f g)(\u03b8) = min \u03d1 {f(\u03d1)+g(\u03b8\u2212\u03d1)}.\nDiagonalization finds a latent orthogonal basis that is more mathematically amenable than the naturally occurring coordinate system. Strong-typing is based on an extension of diagonalization to nonlinear functions. Before diving in, we recall the basics of simultaneous diagonalization.\nSymmetric matrices. Any symmetric matrix A factorizes as A = P\u1d40DP where P is orthogonal and D is diagonal. A collection A1, . . . ,AN of symmetric matrices is simultaneously diagonalizable iff the matrices commute, in which case Ai = P\u1d40DiP where Di is diagonal and P determines a common latent coordinate system (or type).\nArbitrary matrices. The diagonalization of an (m\u00d7n)matrix A is A = PDQ\u1d40 where P and Q are orthogonal (m\u00d7m) and (n\u00d7 n) matrices and D is positive diagonal. A collection of matrices is simultaneously diagonalizable if Ai = PDiQ\u1d40 for all i. A necessary condition for simultaneous diagonalizability is that\nA\u1d40i Aj and AiA \u1d40 j are symmetric for all i, j. (2)\nNext, we work through two examples where diagonalization and a positivity condition imply safety."}, {"heading": "3.1. Warmup: When are two-player games safe?", "text": "To orient the reader, we consider a minimal example which illustrates most of the main ideas of the paper: two-player bilinear games (von Neumann & Morgenstern, 1944). Consider a two-player block game with loss functions\n`1(v,w) = v \u1d40Aw and `2(v,w) = v\u1d40Bw\nand projections \u03c01/2(v,w) = (v,0) and (0,w). The gradients are \u2207 `1 = \u2211 ij(wjAij \u2202 \u2202vi + viAij \u2202 \u2202wj ) = (w\u1d40A\u1d40,v\u1d40A) and \u2207 `2 = (w\u1d40B\u1d40,v\u1d40B). The game is safe if\n\u3008\u03c01(\u2207 `1),\u2207 `2\u3009 = w\u1d40A\u1d40Bw \u2265 0 and \u3008\u2207 `1,\u03c02(\u2207 `2)\u3009 = v\u1d40BA\u1d40v \u2265 0 for all v and w.\nSafety requires that A\u1d40B and BA\u1d40 are positive semidefinite. Any square matrix decomposes into symmetric and antisymmetric components M = Ms + Ma = 12 (M + M\u1d40) + 12 (M\u2212M\u1d40) where w\u1d40Maw = 0 for all w. Thus, a square matrix is positive semidefinite iff its symmetric component is positive semidefinite.\nWe therefore restrict to when A\u1d40B and BA\u1d40 are symmetric. Recalling (2), we further suppose that A and B are simultaneously diagonalizable and obtain: Lemma 4. A two-player game is safe if A = PDQ\u1d40 and B = PEQ\u1d40 where P and Q are orthogonal matrices, D and E are diagonal, and DE \u2265 0.\nProof. The assumptions imply that\n\u3008\u03c01\u2207 `1,\u2207 `2\u3009 = w\u1d40A\u1d40Bw = w\u1d40Q(D\u1d40E)Q\u1d40w \u2265 0\nand \u3008\u2207 `1,\u03c02\u2207 `2\u3009 = w\u1d40P(E\u1d40D)P\u1d40w \u2265 0."}, {"heading": "3.2. Warmup: When is Newton\u2019s method safe?", "text": "It was observed in Dauphin et al. (2014) that applying Newton\u2019s method to neural networks is problematic because it is attracted to saddle points and can increase the loss on nonconvex problems. We reformulate their observation in the language of safety.\nConsider a single player open game with twice differentiable loss ` : V \u2192 R and projection \u03c0 = IV . Newton\u2019s method optimizes ` via weight updates\nwt+1 = wt\u2212 \u03bet with \u03bet = \u03b7t \u00b7H\u22121(wt) \u00b7 \u2207 `(wt),\nwhere Hij(w) = \u2202 2`\n\u2202wi\u2202wj (w) is the Hessian and \u03b7t > 0.\nLemma 5. If ` is strictly convex then Newton\u2019s method is safe, i.e. \u3008H\u22121\u2207 `,\u2207 `\u3009 \u2265 0 for all w.\nProof. Factorize the Hessian at wt as H = PDP\u1d40. If ` is strictly convex then D > 0 and so\n\u3008H\u22121\u2207 `,\u2207 `\u3009 = \u3008D\u22121P\u1d40\u2207 `,P\u1d40\u2207 `\u3009 \u2265 0\nas required.\nTwo features are noteworthy: (i) the transform \u03b7 = P\u1d40\u03be diagonalizes the second-order Taylor expansion of `,\ncompare `(w + \u03be) = `(w) + \u03be\u1d40 \u00b7 \u2207 `+ 1 2 \u03be\u1d40H\u03be\nwith `(w + P\u03b7) = `(w) + \u03b7\u1d40(P\u1d40\u2207 `) + 1 2 \u03b7\u1d40D\u03b7,\nand (ii) the proof hinges on the positivity of D. Sections A1 and A2 extend the approach to show the natural gradient (Amari, 1998) and mirror descent (Raskutti & Mukherjee, 2015) are safe using the Legendre transform."}, {"heading": "3.3. Strongly-typed games are safe", "text": "We apply the lessons from the warmups to define a factorization of nonlinear functions.\nDefinition 4. The functions {`n : V \u2192 R}Nn=1 simultaneously factorize if there is a triple ( {Pl}Ll=1, {fl : Rpl \u2192 R}Ll=1, {gn : RL \u2192 R}Nn=1 ) ,\nsatisfying\n`n(w) = gn ( f1(P \u1d40 1w), . . . , fL(P \u1d40 Lw) ) for all n\nwhere Pl are (D\u00d7pl)-matrices whose columns jointly form an orthogonal basis of V and fl and gn are differentiable, and the gn\u2019s co-vary monotonically: \u2202gm\u2202fl \u2202gn \u2202fl \u2265 0.\nThe projections \u03c4l = PlP \u1d40 l define a type structure on V . Intuitively, the outputs zl = fl(P \u1d40 l w) are latent factors computed from the inputs w such that each zl is independent of the others \u2013 independence is enforced by the projections \u03c4l. Monotonic covariation of the functions gn with respect to the factors zl plays the same role as positivity in two-player games and Newton\u2019s method.\nDefinition 5. Game (TV , {`n}Nn=1) is strongly-typed if the loss functions admit a simultaneous factorization whose projections {\u03c4 l = PlP\u1d40l }Ll=1 commute with {\u03c0n}Nn=1. Theorem 2. Strongly-typed games are safe.\nProof. Commutativity implies there is a basis {ei}Di=1 for V that simultaneously diagonalizes the projections {\u03c0n} and {\u03c4 l}. Express elements of V as (v1, . . . , vD) in the basis. Safety then reduces to showing\n\u3008\u03c0m(\u2207 `m),\u2207 `n\u3009 = \u2211\n{i:\u03c0m(ei)6=0}\n\u2202gm \u2202vi \u00b7 \u2202gn \u2202vi \u2265 0.\nObserve that \u2202fk\u2202vi \u2202fl \u2202vi = 0 if k 6= l since fk and fl are functions of orthogonal coordinates. It follows that\n\u2202gm \u2202vi \u00b7 \u2202gn \u2202vi =\n( L\u2211\nk=1\n\u2202gm \u2202fk \u2202fk \u2202vi\n) \u00b7 ( L\u2211\nl=1\n\u2202gn \u2202fl \u2202fl \u2202vi\n)\n=\nL\u2211\nl=1\n\u2202gm \u2202fl \u2202gn \u2202fl \u00b7 ( \u2202fl \u2202vi )2 \u2265 0\nsince the gn\u2019s co-vary monotonically.\nStrong-typing is a sufficient but not necessary condition for safety. More general definitions can be proposed according to taste. Definition 5 is easy to check, covers the basic examples below, and incorporates the concrete intuition developed in the warmups."}, {"heading": "3.4. Comparison with potential games", "text": "The proof of theorem 1 suggests that safe games are related to potential games (Monderer & Shapley, 1996). In our notation, a block game is a weighted potential game if there exists a potential function \u03a6 and scalar weights \u03b1n > 0 satisfying\n`n(w)\u2212 `n(w +\u03c0nv) = \u03b1n \u00b7 ( \u03a6(w)\u2212\u03a6(w +\u03c0nv) )\nfor all w,v \u2208 V and n \u2208 [N ]. We provide two counter-examples to show that stronglytyped games are distinct from potential games.\nExample 5 (a strongly-typed game that is not a potential game). Let `1(x,y) = x1y1 + 2x2y2 and `2(x,y) = 3x1y1 + 4x2y2. The block game with projections onto x and y is strongly-typed but is not a weighted potential game. Example 6 (a potential game that is not safe). Let `1(x, y) = xy and `2(x, y) = xy \u2212 9x, with projections onto x and y. The game is a potential game but is not safe because\n\u3008\u03c01(\u2207 `1),\u2207 `2\u3009 = \u3008(y, 0), (y \u2212 9, x)\u3009 = y2 \u2212 9y\ncan be negative."}, {"heading": "4. Quadratic Games", "text": "Given a collection of (D\u00d7D)-matrices {A(n)}Nn=1 andDvectors {b(n)} the corresponding quadratic game has loss functions\n`n(w) = 1 2 w\u1d40A(n)w + w\u1d40b(n).\nWe assume the matrices A(n) are symmetric without loss of generality."}, {"heading": "4.1. Open quadratic games", "text": "In an open quadratic game, each player updates the entire joint action.\nCorollary 1. An open quadratic game is safe if there is an orthogonal (D\u00d7D)-matrix P, diagonal matrices D(n) such that D(m)D(n) \u2265 0, and D-vector b such that\nA(n) = PD(n)P\u1d40 and b(n) = A(n)b.\nWe derive corollaries 1 and 2 from theorem 2. Alternate, direct proofs are provided in appendix A3.\nProof. Let fi(x) = x(x2 \u2212 bi) and gn(z) = \u2211D i=1 d (n) i \u00b7 zi. Then\n`n(w) = gn ( f1(p \u1d40 1w), . . . , fD(p \u1d40 Dw) ) ,\nwhere pi are the columns of P, is strongly-typed.\nThe Hessian of `n is H`n = A (n). The conditions of corollary 1 can be reformulated as (i) the Hessians of the losses commute H`mH`n = H`nH`m for all m and n, and (ii) the Newton steps for the losses coincide (when the Hessians are nonsingular):\nH\u22121`n (\u2207 `n)\ufe38 \ufe37\ufe37 \ufe38 Newton step = (A(n))\u22121A(n)(w \u2212 b) = w \u2212 b.\nExample: Disentangling latent factors. An important problem in machine learning is disentangling latent factors (Bengio, 2013). Basic methods for tackling the problem include PCA, canonical correlation analysis (CCA) and independent component analysis (ICA). We show how the factorization in corollary 1 can arise \u201cin nature\u201d as a variant of blind source separation.\nSuppose a signal on D channels is recorded for T timepoints giving (D\u00d7T )-matrix X. Assume the observations combine L independent latent signals: X = MS where S is an (L \u00d7 T )-matrix representing the latent signal and M is a mixing matrix.\nBlind source separation is concerned with recovering the latent signals. The covariance of the signal is A = XX\u1d40. Factorize A = PDP\u1d40 and let S\u0303 = P\u1d40X. Although this may not recover the original signal, i.e. S\u0303 6= S in general, it does disentangle X into uncorrelated factors:\nS\u0303S\u0303\u1d40 = P\u1d40XX\u1d40P = D.\nFinally, recall that finding the first principal component can be formulated as the constrained maximization problem:\nargmax {w:\u2016w\u20162=1}\nw\u1d40Aw.\nNow suppose there are N sets of observations X(1), . . . ,X(N) generated by a single orthogonal mixing matrix acting on different sets of (potentially rescaled) latent signals: X(n) = PS(n). Finding the first principle components of the signals reduces to solving the constrained optimization problems\n{ argmax {w:\u2016w\u20162=1} w\u1d40X(n)(X(n))\u1d40w }N\nn=1\n(3)\nCorollary 1 implies that (3) is a safe. Note the corollary implies the optimization problems have compatible gradients, not that they share a common solution. In general there are many Nash equilibria, analogous to example 3.\nQuadratic games and linear regression. The blind source separation example assumes that the linear terms b(n) in the loss are zero. If the linear term is nonzero then linear regression is a special case of minimizing the quadratic loss. Safety then relates to searching for weights that simultaneously solve linear regression problems on multiple datasets."}, {"heading": "4.2. Block Quadratic Games", "text": "The block quadratic game has losses as above; however the action space decomposes into (w1, . . . ,wN ) with corresponding projections. Block decompose the components\nof the loss as\nA(n) =   A (n) 11 \u00b7 \u00b7 \u00b7 A (n) 1N ... ...\nA (n) N1 \u00b7 \u00b7 \u00b7 A (n) NN\n  and b (n) =   b (n) 1 ...\nb (n) N\n  .\nCorollary 2. A block quadratic game is safe if there are:\n(i) (D \u00d7D)-orthogonal P with Pmn = 0 for m 6= n; (ii) (D \u00d7 L) matrix R with Rn\u2022 diagonal for all n; (iii) diagonal (L\u00d7L)-matrices D(n) with D(m)D(n) \u2265 0; (iv) and a D-vector b\nsuch that A(n) = PRD(n)R\u1d40P\u1d40 and\nb(n) = A(n)b for all n.\nThe notation Pmn and R\u2022n refers to blocks in the rows and columns of P and columns of R.\nProof. Let pi denote the columns of P and gn(z) =\u2211L l=1 d (n) l \u00b7 zl. Given l, construct Pl by concatenating the columns pi of P for which the corresponding entries of Ril are nonzero and let rl be the vector containing the nonzero entries of R\u2022l. Define fl(xl) = r \u1d40 l ( xl 2 \u2212bl) \u00b7 (r \u1d40 l xl). Then\n`n(w) = gn ( f1(P \u1d40 1w), . . . , fL(P \u1d40 Lw) ) .\nIt is an exercise to check the game is strongly-typed.\nExample: Disentangling latent factors. We continue the discussion of blind source separation and safety. Suppose that the mixing matrix decomposes into blocks\nM =   M1\u2022 ...\nMN\u2022\n \nThe blocks can be thought of as generating multiple views on a single latent signal, (Kakade & Foster, 2007; McWilliams et al., 2013; Benton et al., 2017). The nth view is Mn\u2022S.\nAs in the example in section 4.1, now suppose there are N sets of observed signals generated fromN sets of latent signals. Each agent attempts to find the principal component specific to its view on its set of observations. Corollary 2 implies that the problems\n{ argmax\n{wn:\u2016wn\u20162=1} w\u1d40X(n)(X(n))\u1d40w\n}N\nn=1\ncan be safely optimized using gradient descent if the mixing matrix has the block form\nMn\u2022 = Pnn \u00b7Rn\u2022\nwhere Pnn is orthogonal and Rn\u2022 is diagonal. In other words, if the views are generated by rescaling and changing-the-basis of the latent signals.\nThe open and block settings share a common theme: Safe disentangling requires observed signals that are generated by a single (structured) mixing process applied to (arbitrary) sets of independent latent signals. The same phenomenon arises in multi-player games, resulting in tensor decompositions that generalize ICA."}, {"heading": "5. Multi-Player Games and Tensor-SVD", "text": "A classic N -player strategic game consists in finite actionsets An and losses `n : A = \u220fN n=1An \u2192 R. Enumerate the elements of each set as An = [Dn], and encode the losses as (D1, . . . , DN )-tensors\nAn[\u03b11, . . . , \u03b1N ] := `n(\u03b11, . . . , \u03b1N ) where \u03b1n \u2208 [Dn].\nGiven a collection of N such tensors, define the corresponding multilinear game1 as\n`n(w1, . . . ,wN ) = An \u00d71 w1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7N wN\n:=\nD1,...,DN\u2211\n\u03b11,...,\u03b1N=1\nA[\u03b11, . . . , \u03b1N ] \u00b7w1[\u03b11] \u00b7 \u00b7 \u00b7wN [\u03b1N ].\nThe classic example is when actions are drawn from the Dn-simplex 4Dn = {wn \u2208 RDn : \u2211Dn \u03b1=1 wn[\u03b1] = 1 and wn[\u03b1] \u2265 0 for all \u03b1}. We now recall the orthogonal tensor decomposition or tensor SVD (Zhang & Golub, 2001; Chen & Saad, 2009). A tensor admits a tensor-SVD if it can be written in the form\nA = L\u2211\nl=1\ndl \u00b7 u1l \u2297 \u00b7 \u00b7 \u00b7 \u2297 uNl = D\u00d71 U1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7N UN\nwhere Un is a (Dn \u00d7 L)-matrix with orthogonal columns and D is a diagonal (L, . . . , L)-tensor. Corollary 3. A multilinear game is safe if it admits a simultaneous tensor-SVD\nA(n) = D(n) \u00d71 U1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7N UN\nwhere the diagonals have the same sign coordinatewise.\nProof. Let gn(z) = \u2211L l=1 d (n) l zl and fl(x) = \u220f n xn. Define Pl as the (D \u00d7N)-matrix whose nth column is unl in the block of rows corresponding to wn and zero elsewhere. Then\n`n(w) = gn ( f1(P \u1d40 1w), . . . , fL(P \u1d40 Lw) )\nand the game is strongly-typed.\n1We use the n-mode product notation \u00d7n, see de Lathauwer et al. (2000).\nNot all tensors admit a tensor-SVD. However, all tensors do admit a higher-order SVD (de Lathauwer et al., 2000). Section A4 explains why simultaneous HOSVD does not guarantee safety and the stronger tensor-SVD is required.\nExample: Disentangling latent factors Suppose S is a latent signal with independent non-Gaussian coordinates. We observe X = PS + where P is a (D \u00d7 L) mixing matrix and is Gaussian noise. By whitening the signal as a preprocessing step, one can ensure the columns of P are orthogonal. ICA recovers S from the cumulants of X, see Hyva\u0308rinen et al. (2001). The main insight is that the 4th-order cumulant tensor admits a tensor-SVD:\nA[i, j, k, l] = cum(xi, xj , xk, xl) = \u2211\no,p,q,r\nPioPjpPkqPlr \u00b7 cum(so, sp, sq, sr)\n= \u2211\nr\nPirPjrPkrPlr \u00b7 kurt(sr)\nsince cum(so, sp, sq, sr) = 0 unless o = p = s = r because the signals are independent. The expression can be written A = K\u00d71P\u00d72P\u00d73P\u00d74P where diagonal tensor K specifies the kurtosis of the latent signal. In other words, computing the tensor-SVD recovers the mixing matrix and allows to recover the latent signal up to basic symmetries.\nFollowing the same prescription as the examples above, if there are N sets of observations generated from N latent signals by the same mixing matrix, then the resulting cumulant tensors satisfy corollary 3."}, {"heading": "6. Biologically Plausible Backpropagation", "text": "Our ultimate goal is to apply strong-typing to safely optimize neural nets with multiple loss functions (Marblestone et al., 2016). Doing so requires constructing variants of backprop that allow the propagation of multiple error signals. First steps in this direction have been taken with biologically plausible models of backprop that introduce additional degrees of freedom into the algorithm.\nFeedback alignment is a recently proposed algorithm with comparable empirical performance to backprop. It is also more biologically plausible since it loosens backprop\u2019s requirement that the weights used for forward- and back- propagation are symmetric (Lillicrap et al., 2016). The main theoretical result of the paper, see their supplementary information, is Theorem. Let \u03b4BP = W\u1d40e denote the error backpropagated one layer of the neural network. Under certain conditions, the error signal computed by feedback alignment, \u03b4FA = Be, satisfies\n\u03b4FA = \u03b1 \u00b7W\u2020e where \u03b1 > 0\nand W\u2020 is the pseudoinverse of W.\nProof. See theorem 2 of Lillicrap et al. (2016).\nCorollary 4. Under the same conditions, feedback alignment is safe.\nProof. We require to check \u3008\u03b4FA, \u03b4BP \u3009 \u2265 0. Applying the theorem obtains\n\u3008\u03b4FA, \u03b4BP \u3009 = \u03b1 \u00b7 \u3008W\u2020e,W\u1d40e\u3009 = \u03b1 \u00b7 \u3008WW\u2020e, e\u3009.\nObserve that WW\u2020 is an orthogonal projection by standard properties of the pseudoinverse so\n\u3008\u03b4FA, \u03b4BP \u3009 = \u03b1 \u00b7 \u3008WW\u2020e,WW\u2020e\u3009 \u2265 0\nas required.\nIn fact, Lillicrap et al. (2016) provide experimental and theoretical evidence that feedback alignment learns to align the feedforward weights with the pseudoinverse of the backconnections. In other words, they argue that feedback alignment learns safe gradients.\nAnother variant of backprop is kickback, which loosens backprop\u2019s requirement that there are distinct forward- and backward signals (Balduzzi et al., 2015). Kickback truncates backprop\u2019s error signals so that the network learns from just the feedforward sweep together with scalar error signals. One of the main results of Balduzzi et al. (2015) is that kickback is safe, see section A5."}, {"heading": "7. Conclusion", "text": "Backprop provides a general-purpose tool for training configurations of differentiable modules so long as they share a single objective. However, effectively training populations of neural networks on multiple, potentially conflicting tasks, such that they automatically exploit synergies and avoid damaging incompatibilities (such as unlearning old features because they are not useful on a new task) requires fundamentally new ideas.\nA key piece of the puzzle is to develop type systems that can be used to (i) guarantee when certain optimizations can be safely performed jointly and (ii) flag potential conflicts so that the incompatible optimization problems can be separated. The paper provides a first step in this direction.\nFrom a different perspective, convex methods have played an enormous role in optimization yet their relevance to deep learning is limited. The approach to strong-typing developed here is inspired by and extends certain features of convexity. One of the goals of this paper is to carve out some of the key concepts underlying convex geometry\nand reassemble them into a more flexible, but still powerful framework. The proposed definition of strong-typing should be considered a first and far from final attempt.\nA large class of natural examples is generated by imposing strong-typing on simple quadratic and multilinear games. It turns out that, in these settings, strong-typing yields the same matrix and tensor decompositions that arise in blind source separation and independent component analysis, where multiple latent signals are mixed by the same structured process. An important future direction is to disentangle nonlinear latent factors.\nStrong-typing and safety in neural nets. We conclude by discussing the relevance of the framework to neural networks. Firstly, neural nets and strong-typing have many of the same ingredients: neural nets combine linear algebra (matrix multiplications and convolutions) with monotonic functions (sigmoids, tanhs, rectifiers, and max-pooling amongst others). Rectifiers and sigmoids have the additional feature that their outputs are always positive.\nSecondly, there is a deeper connection between rectifiers and strong-typing. Rectifiers are orthogonal projections on weights: \u03c1(W\u1d40x) zeroes out the columns wl of W for which w\u1d40l x \u2264 0. Rectifiers are more sophisticated projections than we have previously considered because they are context-dependent. The columns that are zeroed out depend on W and x: the rectifier-projection takes W and x as parameters, compare remarks 1 and 2 in the appendix. Representation learning in rectifier networks can thus be recast as learning parameterized type structures. An interesting future direction is to consider tensor-switching networks (Tsai et al., 2016), which decouple a neuron\u2019s decision to activate from the information it passes along (for a rectifier, both depend on W\u1d40x).\nFinally, it has long been known that the brain does not use backprop (Crick, 1989). One possibility is that backprop is the optimal deep learning algorithm which, unfortunately, evolution failed to stumble upon. Another is that there are evolutionary advantages to not using backpropagation. For example, it has been argued that the brain optimizes multiple loss functions (Marblestone et al., 2016). Does jointly optimizing or satisficing multiple objectives require learning mechanisms with more degrees of freedom than backprop (Balduzzi et al., 2015; Lillicrap et al., 2016)? Safety and strong-typing provide the tools needed to frame and investigate the question."}, {"heading": "A1. The natural gradient is safe", "text": "The natural gradient was introduced in Amari (1998) and is widely used in machine learning.\nTheorem. The direction of steepest descent at `(wt) on Riemannian manifold (Rn,G) is\n\u03bet \u221d G\u22121(wt) \u00b7 \u2207 `(wt).\nThe natural gradient is the direction G\u22121(wt) \u00b7 \u2207 `(wt).\nProof. We follow Amari (1998). The problem reduces to the constrained minimization\nargmin {\u03be:\u2211Gij(wt)\u03bei\u03bej=1}\n[ `(wt) + \u2207 `(wt)\u1d40\u03be ]\nwhich can be rewritten as\nargmin \u03be\n[ \u2207 `(wt)\u1d40\u03be \u2212 \u03bb\u03be\u1d40G(wt)\u03be ]\nwith solution\n\u03bet \u221d G\u22121(wt)\u2207 `(wt)\nas required.\nCorollary A1. The natural gradient is safe.\nThe proof is obvious. We include it to highlight the role of latent types, diagonalization and positivity.\nProof. We are required to show that\n\u3008G\u22121(w)\u2207 `(w),\u2207 `(w)\u3009 \u2265 0 for all w. (A1)\nThe metric is symmetric positive definite, and so admits factorization\nG(wt) = P(w) \u00b7D(w) \u00b7P\u1d40(w)\nwhere P(w) is an orthogonal matrix and D(w) strictly positive diagonal for all w. After setting \u2207\u0303`(w) := P\u1d40(w)\u2207 `(w), the condition in (A1) can be rewritten as\n\u3008D\u22121(w)\u2207\u0303`(w), \u2207\u0303`(w)\u3009 \u2265 0\nwhich clearly holds.\nRemark 1 (parametric types). The columns pi(w) of P(w) define a parametric family of latent type systems \u03c4 i(w) = pi(w) \u00b7 p\u1d40i (w) that is parametrized by w."}, {"heading": "A2. Mirror descent is safe", "text": "We show that mirror descent is safe. The fact that mirror descent is well-behaved is far from new; there is an extensive literature analyzing its convergence properties (Bubeck, 2015). Our purpose is to highlight the type structures underlying convex duality and mirror descent.\nThe approach generalizes the analysis of Newton\u2019s method. The role played by transform that diagonalizes the Hessian in section 3.2 is taken over by the Legendre transform here.\nConvex duality. The exposition closely follows Amari (2009). Consider the manifold M = RD with coordinate system w = (w1, . . . , wD). Let \u03c8 : M \u2192 R be a twicedifferentiable strictly convex function. It follows that the Hessian of \u03c8,\ngij(w) = \u2202i\u2202j\u03c8(w)\nis a positive definite matrix for all w which defines a Riemannian metric on the manifold M . Concretely, the distance between infinitesimally close points w and w + dw is\nds2 = \u2211\nij\ngij(w)dw idwj .\nDefine the dual coordinate system\n\u03b8 = \u2207\u03c8(w), i.e. \u03b8i = \u2202\u03c8\n\u2202wi (w).\nRecall that the Legendre transform of \u03c8,\n\u03c8\u2217(\u03b8) = max w {w\u1d40\u03b8 \u2212 \u03c8(w)},\nsatisfies the relation\n\u03c8(w) + \u03c8\u2217(\u03b8)\u2212w\u1d40\u03b8 = 0,\nwhich allows to recover the original coordinates from the dual system:\nw = \u2207\u03c8\u2217(\u03b8) i.e. wi = \u2202\u03c8\u2217\n\u2202\u03b8i (\u03b8).\nDefine the dual metric\ngij(\u03b8) = \u22022\n\u2202\u03b8i\u2202\u03b8j \u03c8\u2217(\u03b8).\nTheorem. The metrics gij(w) and gij(\u03b8) are inverse. That is\nd\u03b8 = G(w)dw and dw = G\u22121(\u03b8)d\u03b8.\nProof. See Amari (2009).\nRemark 2 (parametric types). Convex duality can thus be rephrased as a relationship between two parametric families of types on the tangent spaces to the manifold that is encoded in the Riemannian metric and its inverse.\nMirror descent is safe. We show that mirror descent is safe by applying convex duality following Raskutti & Mukherjee (2015). Let \u03c8 be a strictly convex twice differentiable function. The Bregman divergence of \u03c8 is D\u03c8(v,w) = \u03c8(v)\u2212 \u03c8(w)\u2212 \u3008\u2207\u03c8(w),v \u2212w\u3009. Theorem. Given loss function ` and strictly convex twice differentiable \u03c8, both defined on M = RD, the mirror descent step\nwt+1 = argmin w\n{ w\u1d40\u2207 `(wt) + 1\n\u03b7t D\u03c8(w,w\nt) } ,\nis equivalent to the natural gradient descent step\n\u03b8t+1 = \u03b8t \u2212 \u03b7t \u00b7 [ \u22072 \u03c8\u2217(\u03b8t) ]\u22121 \u00b7 \u2207 `(\u03b8t).\nin the dual coordinate system.\nProof. We sketch the proof in Raskutti & Mukherjee (2015), which should be consulted for details. Computing the minimizer in mirror descent by differentiating shows that the mirror descent update is equivalent, in dual coordinates, to\n\u03b8t+1 = \u03b8t \u2212 \u03b7t \u00b7 \u2207w `(\u2207\u03c8\u2217(\u03b8t))\nApplying the chain rule obtains\n\u2207w `(\u2207\u03c8\u2217(\u03b8t)) = [ \u22072 \u03c8\u2217(\u03b8t) ]\u22121 \u00b7 \u2207\u03b8 `(\u2207\u03c8\u2217(\u03b8t))\nso that mirror descent can be rewritten as\n\u03b8t+1 = \u03b8t \u2212 \u03b7t \u00b7 [ \u22072 \u03c8\u2217(\u03b8t) ]\u22121 \u00b7 \u2207\u03b8 `(\u2207\u03c8\u2217(\u03b8t))\nas required.\nCorollary A2. If \u03c8 is twice-differentiable and strictly convex then mirror descent is safe.\nProof. Combine corollary A1 with the equivalence of mirror descent and the natural gradient."}, {"heading": "A3. Direct proofs of corollaries 1 and 2", "text": "Direct proof of corollary 1.\nProof. Safety requires\n\u3008\u2207 `n,\u2207 `m\u3009 = (A(n)w\u2212b(n))\u1d40(A(m)w\u2212b(m)) \u2265 0.\nPlugging in the assumptions yields\n\u3008\u2207 `n,\u2207 `m\u3009 = (w \u2212 b)\u1d40PD(n)D(m)P\u1d40(w \u2212 b) \u2265 0\nwhich is positive by inspection.\nDirect proof of corollary 2.\nProof. The gradient and projected gradient are\n\u2207 `n = w\u1d40A(n) + b(n) and \u03c0n(\u2207 `n) = w\u1d40A(n)\u2022n + b(n)n\nAfter imposing b(n) = A(n)b, safety requires that\n(w \u2212 b)\u1d40A(m)\u2022m A(n)m\u2022(w \u2212 b) \u2265 0 for all w, m and n.\nApplying the remaining conditions and setting x = R\u1d40P\u1d40(w \u2212 b) obtains\nx\u1d40D(m)R\u1d40\u2022mRm\u2022D (n)x \u2265 0\nSince Rm\u2022 is diagonal, it follows that D(m)R \u1d40 \u2022mRm\u2022D(n) is a product of diagonal matrices. The expression is positive since D(m)D(n) \u2265 0 by assumption."}, {"heading": "A4. Higher-order SVD", "text": "Any N -tensor admits a higher-order SVD (HOSVD), whereas not every tensor admits a tensor-SVD. In this section we recall the concept of HOSVD and show why simultaneous HOSVD is not sufficient to guarantee safety. The section relies heavily on notation taken from de Lathauwer et al. (2000) which the reader is encouraged to consult for details and context.\nDefinition A1 (matricization). The matricization of tensor A over its nth mode, denoted A(n) is an Dn \u00d7 (Dn+1Dn+2 \u00b7 \u00b7 \u00b7DND1D2 \u00b7 \u00b7 \u00b7Dn\u22121)-matrix that contains the element A[\u03b11, . . . , \u03b1N ] at the position with row number \u03b1n and column number\n(\u03b1n+1 \u2212 1)Dn+2Dn+3 \u00b7 \u00b7 \u00b7DND1 \u00b7 \u00b7 \u00b7Dn\u22121 +(\u03b1n+2 \u2212 1)Dn+3Dn+4 \u00b7 \u00b7 \u00b7DND1 \u00b7 \u00b7 \u00b7Dn\u22121 + \u00b7 \u00b7 \u00b7\n+(\u03b1N \u2212 1)D1D2 \u00b7 \u00b7 \u00b7Dn\u22121 + (\u03b11 \u2212 1)D2D3 \u00b7 \u00b7 \u00b7Dn\u22121 +(\u03b12 \u2212 1)D3D4 \u00b7 \u00b7 \u00b7Dn\u22121 + \u00b7 \u00b7 \u00b7+ \u03b1n\u22121\nLet us recall the notion of higher-order SVD (HOSVD) from de Lathauwer et al. (2000).\nTheorem. Every (I1, . . . , IN )-tensor A can be written as a product\nA = S \u00d71 U1 \u00d72 U2 \u00b7 \u00b7 \u00b7 \u00d7N UN\nin which\n1. Ui is a unitary (Ii \u00d7 Ii) matrix\n2. S is a (I1 \u00d7 I2 \u00b7 \u00b7 \u00b7 IN )-tensor of which the subtensors Sin=\u03b1, obtained by fixing the nth index to \u03b1 have the properties of\n(a) all-orthogonality: two subtensors Sin=\u03b1 and Sin=\u03b2 are orthogonal for all possible values of n, \u03b1 and \u03b2 subject to \u03b1 6= \u03b2:\n\u3008Sin=\u03b1,Sin=\u03b2\u3009 = 0 when \u03b1 6= \u03b2 (b) ordering:\n\u2016Sin=1\u2016 \u2265 \u2016Sin=2\u2016 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u2016Sin=In\u2016 \u2265 0 for all possible values of n.\nThe Frobenius-norms \u2016Sin=i\u2016, symbolized by \u03c3(n)i are n-mode singular values of A and the vector U(n)i is an ith n-mode singular vector.\nProof. See de Lathauwer et al. (2000).\nAn important property of the HOSVD is as follows. Let A = S \u00d71 U1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7N UN be the HOSVD of A. Then the matricization is\nA(n) = U n\u03a3n(Vn)\u1d40\nis an SVD of A(n), where the diagonal matrix \u03a3 n \u2208 RDn\u00d7Dn and the columnwise orthonormal matrix Vn \u2208 RDn+1Dn+2\u00b7\u00b7\u00b7DND1D2\u00b7\u00b7\u00b7Dn\u22121\u00d7Dn are defined as\n\u03a3n := diag(\u03c3n1 , . . . , \u03c3 n Dn),\n(Vn)\u1d40 := S\u0303(n) ( Un+1 \u2297 \u00b7 \u00b7 \u00b7UN \u2297U1 \u2297 \u00b7 \u00b7 \u00b7Un\u22121 )\u1d40\nwhere S\u0303n is a normalized version of the matricization S(n) of S, with rows rescaled to unit-length: S(n) = \u03a3n \u00b7 S\u0303n.\nA4.1. Discussion of safety and HOSVD\nTo somewhat reduce the proliferation of subscripts and superscripts, we work with two tensors denoted A and B, which stand in for the tensors of two players A(m) and A(n) in an N -player game. Lemma A1. Let A[wn\u0302] be the Dn-vector A[wn\u0302] := A\u00d71w1\u00d7\u00b7 \u00b7 \u00b7\u00d7n\u22121wn\u22121\u00d7n+1wn+1\u00d7\u00b7 \u00b7 \u00b7\u00d7NwN\nwhere note the term wn is omitted from the expression, and similarly for B. Then\n\u3008\u03c0n\u2207 `A,\u2207 `B\u3009 = A[wn\u0302]\u1d40 \u00b7B[wn\u0302]\nProof. Direct computation.\nLemma A2. Let vec(wn\u0302) denote the (D1D2 \u00b7 \u00b7 \u00b7Dn\u22121Dn+1 \u00b7 \u00b7 \u00b7Dn)-vector given by vec(wn\u0302) = vec(w1\u2297\u00b7 \u00b7 \u00b7\u2297wn\u22121\u2297wn+1\u2297\u00b7 \u00b7 \u00b7\u2297wN )\nand suppose that the matricization has SVD A(n) = Un\u03a3n(Vn)\u1d40. Then A[wn\u0302] is the Dn-vector\nA[wn\u0302] = Un\u03a3n(Vn)\u1d40 vec(wn\u0302).\nProof. Direct computation.\nLemma A3.\n\u3008\u03c0n\u2207 `A,\u2207 `B\u3009 = vec(wn\u0302) \u1d40V(n)A \u03a3 n A\u03a3 n B(V n B) \u1d40 vec(wn\u0302).\nProof. By the above working,\n\u3008\u03c0n\u2207 `A,\u2207 `B\u3009 = vec(wn\u0302) \u1d40V(n)A \u03a3 n A(U n A) \u1d40UnB\u03a3 n B(V n B) \u1d40 vec(wn\u0302).\nRecall that\n(Vn)\u1d40 := S\u0303(n) ( Un+1 \u2297 \u00b7 \u00b7 \u00b7UN \u2297U1 \u2297 \u00b7 \u00b7 \u00b7Un\u22121 )\u1d40\nso that\n\u03a3n(Vn)\u1d40 = S(n) ( Un+1 \u2297 \u00b7 \u00b7 \u00b7UN \u2297U1 \u2297 \u00b7 \u00b7 \u00b7Un\u22121 )\u1d40\nwhich we shorten to\n\u03a3n(Vn)\u1d40 = S(n) ( U\u0303n\u0302 )\u1d40\nIt follows that \u3008\u03c0n\u2207 `A,\u2207 `B\u3009 equals\nvec(wn\u0302) \u1d40U\u0303n\u0302AS \u1d40 (n)T(n)(U n\u0302 B) \u1d40 vec(wn\u0302).\nand the result follows.\nWe are finally in a position to explain why simultaneous HOSVD is insufficient to guarantee safety. That is, we are in a position to explain why\nA = S \u00d71 U1 \u00d72 U2 \u00b7 \u00b7 \u00b7 \u00d7N UN\nB = T \u00d71 U1 \u00d72 U2 \u00b7 \u00b7 \u00b7 \u00d7N UN\ndoes not guarantee \u3008\u03c01(\u2207 `A),\u2207 `B\u3009 \u2265 0. Our strategy for guaranteeing safety hinges on simultaneous diagonalization. However, HOSVD does not guarantee that the core S is diagonal. Instead, it introduces the condition of simultaneous orthogonality on subtensors of S. Unfortunately, there is nothing to guarantee that the cores S and T of two different tensors are \u201csimultaneously allorthogonal\u201d across the two tensors. In other words, there is nothing to guarantee that the product of their matricizations\nS\u1d40(n)T(n)\nis diagonal in general. A specific setting where this holds is when the two tensors A and B admits a simultaneous tensor-SVD. It is an open question whether the condition can be guaranteed in a naturally-occurring more general setting."}, {"heading": "A5. Kickback is safe", "text": "Kickback is a complementary algorithm to feedback alignment that is motivated by the observation that neurons communicate via a single kind of signal, spikes, rather than using the two kinds of signals (the feedforward sweep and backpropagated errors) required by backprop (Balduzzi et al., 2015).\nKickback is a truncated version of backprop that computes gradient-estimates using the feedforward sweep together with global error/reward signals. The error signals take the form of a single scalar value broadcast to the entire network, for example via neuromodulators. One of the main results of the paper is that\nTheorem. If neurons are coherent then kickback is safe.\nProof. See theorem 4 of Balduzzi et al. (2015).\nCoherence is, essentially, a positivity condition on synaptic weights that ensures the gradient-estimates computed by kickback have positive inner product with the gradients computed by backprop."}, {"heading": "A6. Comparison with Strongly-Typed RNNs", "text": "Typed linear algebra was proposed in Balduzzi & Ghifary (2016) (STNN), which applied the framework to analyze and simplify recurrent neural networks. The definition of typed vector space in this paper is more general than in STNN \u2013 it replaces an orthogonal basis with orthogonal projections.\nNo formal definition of strong-typing was provided in STNN. Informally, STNN stated: \u201cWe refer to architectures as strongly-typed when they both (i) preserve the type structure of their features and (ii) separate learned parameters from state-dependence\u201d. The definition of strongtyping in the main text, definition 5 is different from strongtyping in STNN, although it draws on similar intuitions. The settings are sufficiently disparate that no confusion should result."}], "references": [{"title": "Concrete Problems in AI Safety", "author": ["Amodei", "Dario", "Olah", "Chris", "Steinhardt", "Jacob", "Christiano", "Paul", "Schulman", "John", "Man\u00e9", "Dan"], "venue": "In arXiv:1606.06565,", "citeRegEx": "Amodei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Amodei et al\\.", "year": 2016}, {"title": "Kickback cuts Backprop\u2019s red-tape: Biologically plausible credit assignment in neural networks", "author": ["D Balduzzi", "H Vanchinathan", "J. Buhmann"], "venue": "In AAAI,", "citeRegEx": "Balduzzi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Balduzzi et al\\.", "year": 2015}, {"title": "Grammars for Games: A Gradient-Based, Game-Theoretic Framework for Optimization in Deep Learning", "author": ["Balduzzi", "David"], "venue": "Frontiers in Robotics and AI,", "citeRegEx": "Balduzzi and David.,? \\Q2016\\E", "shortCiteRegEx": "Balduzzi and David.", "year": 2016}, {"title": "Strongly-Typed Recurrent Neural Networks", "author": ["Balduzzi", "David", "Ghifary", "Muhammad"], "venue": "In ICML,", "citeRegEx": "Balduzzi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Balduzzi et al\\.", "year": 2016}, {"title": "Deep Learning of Representations: Looking Forward", "author": ["Bengio", "Yoshua"], "venue": "Statistical Language and Speech Processing. Springer,", "citeRegEx": "Bengio and Yoshua.,? \\Q2013\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2013}, {"title": "Deep Generalized Canonical Correlation Analysis", "author": ["Benton", "Adrian", "Khayrallah", "Huda", "Gujral", "Biman", "Reisinger", "Drew", "Zhang", "Sheng", "Arora", "Raman"], "venue": "In arXiv:1702.02519,", "citeRegEx": "Benton et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Benton et al\\.", "year": 2017}, {"title": "Safe learning of regions of attraction for uncertain, nonlinear systems with gaussian processes", "author": ["F Berkenkamp", "R Moriconi", "A Schoellig", "A. Krause"], "venue": "In IEEE CDC,", "citeRegEx": "Berkenkamp et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Berkenkamp et al\\.", "year": 2016}, {"title": "Convex Optimization: Algorithms and Complexity", "author": ["Bubeck", "S\u00e9bastien"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and S\u00e9bastien.,? \\Q2015\\E", "shortCiteRegEx": "Bubeck and S\u00e9bastien.", "year": 2015}, {"title": "On the tensor SVD and the optimal low rank orthogonal approximation of tensors", "author": ["Chen", "Jie", "Saad", "Yousef"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "Chen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "The recent excitement about neural networks", "author": ["Crick", "Francis"], "venue": "Nature, 337(12):129\u2013132,", "citeRegEx": "Crick and Francis.,? \\Q1989\\E", "shortCiteRegEx": "Crick and Francis.", "year": 1989}, {"title": "The Complexity of Computing a Nash Equilibrium", "author": ["Daskalakis", "Constantinos", "Goldberg", "Paul W", "Papadimitriou", "Christos"], "venue": "SIAM J. Computing,", "citeRegEx": "Daskalakis et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Daskalakis et al\\.", "year": 2009}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Dauphin", "Yann", "Pascanu", "Razvan", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Ganguli", "Surya", "Bengio", "Yoshua"], "venue": "In NIPS,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "A multilinear singular value decomposition", "author": ["de Lathauwer", "Lieven", "de Moor", "Bart", "Vandewalle", "Joos"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "Lathauwer et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lathauwer et al\\.", "year": 2000}, {"title": "PathNet: Evolution Channels Gradient Descent in Super Neural Networks", "author": ["C Fernando", "D Banarse", "C Blundell", "Y Zwols", "D Ha", "A Rusu", "A Pritzel", "D. Wierstra"], "venue": null, "citeRegEx": "Fernando et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Fernando et al\\.", "year": 2017}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups", "author": ["G Hinton"], "venue": "IEEE Signal Proc Magazine,", "citeRegEx": "Hinton,? \\Q2012\\E", "shortCiteRegEx": "Hinton", "year": 2012}, {"title": "Independent Component Analysis", "author": ["Hyv\u00e4rinen", "Aapo", "Karhunen", "Juha", "Oja", "Erkki"], "venue": null, "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2001}, {"title": "Multi-view Regression Via Canonical Correlation Analysis", "author": ["Kakade", "Sham", "Foster", "Dean P"], "venue": "In COLT,", "citeRegEx": "Kakade et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2007}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A Krizhevsky", "I Sutskever", "Hinton", "G E"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Random feedback weights support error backpropagation for deep learning", "author": ["Lillicrap", "Timothy P", "Cownden", "Daniel", "Tweed", "Douglas B", "Ackerman", "Colin J"], "venue": "Nature Communications,", "citeRegEx": "Lillicrap et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2016}, {"title": "Towards an Integration of Deep Learning and Neuroscience", "author": ["Marblestone", "Adam H", "Wayne", "Greg", "Kording", "Konrad P"], "venue": "Front. Comput. Neurosci.,", "citeRegEx": "Marblestone et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Marblestone et al\\.", "year": 2016}, {"title": "Correlated random features for fast semi-supervised learning", "author": ["McWilliams", "Brian", "Balduzzi", "David", "Buhmann", "Joachim"], "venue": "In NIPS,", "citeRegEx": "McWilliams et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McWilliams et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Equilibrium Points in n-Person Games", "author": ["Nash", "John F"], "venue": "Proc Natl Acad Sci U S A,", "citeRegEx": "Nash and F.,? \\Q1950\\E", "shortCiteRegEx": "Nash and F.", "year": 1950}, {"title": "Optimization, learning, and games with predictable sequences", "author": ["Rakhlin", "Alexander", "Sridharan", "Karthik"], "venue": "In NIPS,", "citeRegEx": "Rakhlin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2013}, {"title": "The Information Geometry of Mirror Descent", "author": ["G Raskutti", "S. Mukherjee"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "Raskutti and Mukherjee,? \\Q2015\\E", "shortCiteRegEx": "Raskutti and Mukherjee", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["Silver", "David", "Huang", "Aja"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Fast Convergence of Regularized Learning in Games", "author": ["Syrgkanis", "Vasilis", "Agarwal", "Alekh", "Luo", "Haipeng", "Schapire", "Robert"], "venue": "In NIPS,", "citeRegEx": "Syrgkanis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Syrgkanis et al\\.", "year": 2015}, {"title": "Tensor Switching Networks", "author": ["Tsai", "Chuan-Yung", "Saxe", "Andrew", "Cox", "David"], "venue": "In NIPS,", "citeRegEx": "Tsai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tsai et al\\.", "year": 2016}, {"title": "Safe Exploration in Finite Markov Decision Processes with Gaussian Processes", "author": ["Turchetta", "Matteo", "Berkenkamp", "Felix", "Krause", "Andreas"], "venue": "In NIPS,", "citeRegEx": "Turchetta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Turchetta et al\\.", "year": 2016}, {"title": "Theory of Games and Economic Behavior", "author": ["von Neumann", "John", "Morgenstern", "Oskar"], "venue": null, "citeRegEx": "Neumann et al\\.,? \\Q1944\\E", "shortCiteRegEx": "Neumann et al\\.", "year": 1944}, {"title": "Learning deep control policies for autonomous aerial vehicles with mpc-guided policy search", "author": ["T Zhang", "G Kahn", "S Levine", "P. Abbeel"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Rank-one approximation to higher order tensors", "author": ["Zhang", "Tong", "Golub", "Gene H"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "Zhang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 17, "context": "Recent years have seen rapid progress on core problems in artificial intelligence such as object and voice recognition (Hinton & et al, 2012; Krizhevsky et al., 2012), playing video and board games (Mnih et al.", "startOffset": 119, "endOffset": 166}, {"referenceID": 21, "context": ", 2012), playing video and board games (Mnih et al., 2015; Silver et al., 2016), and driving autonomous vehicles (Zhang et al.", "startOffset": 39, "endOffset": 79}, {"referenceID": 25, "context": ", 2012), playing video and board games (Mnih et al., 2015; Silver et al., 2016), and driving autonomous vehicles (Zhang et al.", "startOffset": 39, "endOffset": 79}, {"referenceID": 30, "context": ", 2016), and driving autonomous vehicles (Zhang et al., 2016).", "startOffset": 41, "endOffset": 61}, {"referenceID": 0, "context": "A weakness of the approach is that it conceives safety more narrowly than, for example, Amodei et al. (2016) which is concerned with societal risks arising from artificial intelligence.", "startOffset": 88, "endOffset": 109}, {"referenceID": 1, "context": "Sections 6 and A5 switch to neural networks and analyze two biologically plausible variants of backpropagation (Balduzzi et al., 2015; Lillicrap et al., 2016).", "startOffset": 111, "endOffset": 158}, {"referenceID": 18, "context": "Sections 6 and A5 switch to neural networks and analyze two biologically plausible variants of backpropagation (Balduzzi et al., 2015; Lillicrap et al., 2016).", "startOffset": 111, "endOffset": 158}, {"referenceID": 28, "context": "The literature on safety is mostly focused on problems arising in reinforcement learning, for example ensuring agents avoid dangerous outcomes (Turchetta et al., 2016; Amodei et al., 2016; Berkenkamp et al., 2016).", "startOffset": 143, "endOffset": 213}, {"referenceID": 0, "context": "The literature on safety is mostly focused on problems arising in reinforcement learning, for example ensuring agents avoid dangerous outcomes (Turchetta et al., 2016; Amodei et al., 2016; Berkenkamp et al., 2016).", "startOffset": 143, "endOffset": 213}, {"referenceID": 6, "context": "The literature on safety is mostly focused on problems arising in reinforcement learning, for example ensuring agents avoid dangerous outcomes (Turchetta et al., 2016; Amodei et al., 2016; Berkenkamp et al., 2016).", "startOffset": 143, "endOffset": 213}, {"referenceID": 19, "context": "A recent survey paper argues the brain optimizes a family of complementary loss functions (Marblestone et al., 2016) without considering how the complementarity of the loss functions could be checked or enforced.", "startOffset": 90, "endOffset": 116}, {"referenceID": 0, "context": ", 2016; Amodei et al., 2016; Berkenkamp et al., 2016). Gradients are typically not available in reinforcement learning problems. We study interactions between algorithms with clearly defined objectives that utilize gradient-based optimization, which gives a more technical perspective. The idea of a population of neural networks solving multiple related tasks is developed in Fernando et al. (2017), which uses genetic algorithms to adapt components to new tasks.", "startOffset": 8, "endOffset": 400}, {"referenceID": 0, "context": ", 2016; Amodei et al., 2016; Berkenkamp et al., 2016). Gradients are typically not available in reinforcement learning problems. We study interactions between algorithms with clearly defined objectives that utilize gradient-based optimization, which gives a more technical perspective. The idea of a population of neural networks solving multiple related tasks is developed in Fernando et al. (2017), which uses genetic algorithms to adapt components to new tasks. However, they repeatedly reinitialize components to undo the damage done by the genetic algorithm. Our work is intended, ultimately, to help design algorithms that detect and avoid damaging updates. A recent survey paper argues the brain optimizes a family of complementary loss functions (Marblestone et al., 2016) without considering how the complementarity of the loss functions could be checked or enforced. The idea of investigating game-theoretic and mechanism design questions specific to certain classes of algorithms is introduced in Rakhlin & Sridharan (2013); Syrgkanis et al.", "startOffset": 8, "endOffset": 1035}, {"referenceID": 0, "context": ", 2016; Amodei et al., 2016; Berkenkamp et al., 2016). Gradients are typically not available in reinforcement learning problems. We study interactions between algorithms with clearly defined objectives that utilize gradient-based optimization, which gives a more technical perspective. The idea of a population of neural networks solving multiple related tasks is developed in Fernando et al. (2017), which uses genetic algorithms to adapt components to new tasks. However, they repeatedly reinitialize components to undo the damage done by the genetic algorithm. Our work is intended, ultimately, to help design algorithms that detect and avoid damaging updates. A recent survey paper argues the brain optimizes a family of complementary loss functions (Marblestone et al., 2016) without considering how the complementarity of the loss functions could be checked or enforced. The idea of investigating game-theoretic and mechanism design questions specific to certain classes of algorithms is introduced in Rakhlin & Sridharan (2013); Syrgkanis et al. (2015). The papers consider how convergence in games can be accelerated if the players use variants of mirror descent.", "startOffset": 8, "endOffset": 1060}, {"referenceID": 10, "context": "However, finding them is often intractable (Daskalakis et al., 2009).", "startOffset": 43, "endOffset": 68}, {"referenceID": 11, "context": "Warmup: When is Newton\u2019s method safe? It was observed in Dauphin et al. (2014) that applying Newton\u2019s method to neural networks is problematic because it is attracted to saddle points and can increase the loss on nonconvex problems.", "startOffset": 57, "endOffset": 79}, {"referenceID": 20, "context": "The blocks can be thought of as generating multiple views on a single latent signal, (Kakade & Foster, 2007; McWilliams et al., 2013; Benton et al., 2017).", "startOffset": 85, "endOffset": 154}, {"referenceID": 5, "context": "The blocks can be thought of as generating multiple views on a single latent signal, (Kakade & Foster, 2007; McWilliams et al., 2013; Benton et al., 2017).", "startOffset": 85, "endOffset": 154}, {"referenceID": 12, "context": "We use the n-mode product notation \u00d7n, see de Lathauwer et al. (2000). Not all tensors admit a tensor-SVD.", "startOffset": 46, "endOffset": 70}, {"referenceID": 15, "context": "ICA recovers S from the cumulants of X, see Hyv\u00e4rinen et al. (2001). The main insight is that the 4th-order cumulant tensor admits a tensor-SVD: A[i, j, k, l] = cum(xi, xj , xk, xl)", "startOffset": 44, "endOffset": 68}, {"referenceID": 19, "context": "Our ultimate goal is to apply strong-typing to safely optimize neural nets with multiple loss functions (Marblestone et al., 2016).", "startOffset": 104, "endOffset": 130}, {"referenceID": 18, "context": "It is also more biologically plausible since it loosens backprop\u2019s requirement that the weights used for forward- and back- propagation are symmetric (Lillicrap et al., 2016).", "startOffset": 150, "endOffset": 174}, {"referenceID": 18, "context": "See theorem 2 of Lillicrap et al. (2016).", "startOffset": 17, "endOffset": 41}, {"referenceID": 1, "context": "Another variant of backprop is kickback, which loosens backprop\u2019s requirement that there are distinct forward- and backward signals (Balduzzi et al., 2015).", "startOffset": 132, "endOffset": 155}, {"referenceID": 16, "context": "In fact, Lillicrap et al. (2016) provide experimental and theoretical evidence that feedback alignment learns to align the feedforward weights with the pseudoinverse of the backconnections.", "startOffset": 9, "endOffset": 33}, {"referenceID": 1, "context": "Another variant of backprop is kickback, which loosens backprop\u2019s requirement that there are distinct forward- and backward signals (Balduzzi et al., 2015). Kickback truncates backprop\u2019s error signals so that the network learns from just the feedforward sweep together with scalar error signals. One of the main results of Balduzzi et al. (2015) is that kickback is safe, see section A5.", "startOffset": 133, "endOffset": 346}], "year": 2017, "abstractText": "As artificial agents proliferate, it is becoming increasingly important to ensure that their interactions with one another are well-behaved. In this paper, we formalize a common-sense notion of when algorithms are well-behaved: an algorithm is safe if it does no harm. Motivated by recent progress in deep learning, we focus on the specific case where agents update their actions according to gradient descent. The first result is that gradient descent converges to a Nash equilibrium in safe games. The paper provides sufficient conditions that guarantee safe interactions. The main contribution is to define strongly-typed agents and show they are guaranteed to interact safely. A series of examples show that strong-typing generalizes certain key features of convexity and is closely related to blind source separation. The analysis introduce a new perspective on classical multilinear games based on tensor decomposition.", "creator": "LaTeX with hyperref package"}}}