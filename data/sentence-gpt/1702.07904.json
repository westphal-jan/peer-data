{"id": "1702.07904", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2017", "title": "Coarse Grained Exponential Variational Autoencoders", "abstract": "Variational autoencoders (VAE) often use Gaussian or category distribution to model the inference process. This puts a limit on variational learning because this simplified assumption does not match the true posterior distribution, which is usually much more sophisticated. To break this limitation and apply arbitrary parametric distribution during inference, this paper derives a \\emph{semi-continuous} latent representation, which approximates a continuous density up to a prescribed precision, and is much easier to analyze than its continuous counterpart because it is fundamentally discrete, and thus more easily computable, than it does in a variational model.\n\n\n\nThis paper attempts to solve the problem by using discrete distributions to model the inference process. In particular, the Gaussian method used by this paper is used to determine the maximum posterior distribution for the inference process, using convolutional and parametric inference (which is used to approximate the posterior distribution) and the Gaussian method used by this paper.\nA variant of Gaussian is that it is used to perform the inference process in the model. In addition, it can be used to perform a convolutional or parametric function to estimate the posterior distribution, by using convolutional and parametric inference (which is used to estimate the posterior distribution).\nThe main advantage of Gaussian is that the Gaussian method used by this paper is very accurate and has not been used to calculate the posterior distribution. In particular, the Gaussian method used by this paper is very accurate and has not been used to calculate the posterior distribution. In particular, the Gaussian method used by this paper is very accurate and has not been used to estimate the posterior distribution. In particular, the Gaussian method used by this paper is very accurate and has not been used to calculate the posterior distribution. In particular, the Gaussian method used by this paper is very accurate and has not been used to estimate the posterior distribution. In particular, the Gaussian method used by this paper is very accurate and has not been used to estimate the posterior distribution. In particular, the Gaussian method used by this paper is very accurate and has not been used to estimate the posterior distribution. In particular, the Gaussian method used by this paper is very accurate and has not been used to estimate the posterior distribution. In particular, the Gaussian method used by this paper is very accurate and has not been used to estimate the posterior distribution. In particular, the Gaussian method used by this paper is very accurate and has not been used to estimate the", "histories": [["v1", "Sat, 25 Feb 2017 15:08:53 GMT  (3140kb,D)", "http://arxiv.org/abs/1702.07904v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ke sun", "xiangliang zhang"], "accepted": false, "id": "1702.07904"}, "pdf": {"name": "1702.07904.pdf", "metadata": {"source": "CRF", "title": "Coarse Grained Exponential Variational Autoencoders", "authors": ["Ke Sun", "Xiangliang Zhang"], "emails": ["sunk@ieee.org", "xiangliang.zhang@kaust.edu.sa"], "sections": [{"heading": "1 Introduction", "text": "Variational autoencoders (Kingma & Welling, 2014) and its variants (Rezende et al., 2014; Sohn et al., 2015; Salimans et al., 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al., 1999) with strong generalization and a standard learning objective, and deep learning with flexible and scalable representations. They are attracting decent attentions, producing state-of-the-art performance in semi-supervised learning (Kingma et al., 2014) and image generation (Gregor et al., 2015), and are getting applied in diverse areas such as deep generative modeling (Rezende et al., 2014), image segmentation (Sohn et al., 2015), clustering (Dilokthanakul et al., 2017), and future prediction from images (Walker et al., 2016).\nThis paper discusses unsupervised learning with VAE which pipes an inference model q(z |x) with a generative model p(x | z), where x and z are observed and latent variables, respectively. A simple parameter-free prior p(z) combined with p(x | z) parameterized by a deep neural network results in arbitrarily flexible representations. However, its (very complex) posterior p(z |x) must be within the representation power of the inference machine q(z |x), so that the variational bound is tight and variational learning is effective.\nIn the original VAE (Kingma & Welling, 2014), q(z |x) obeys a Gaussian distribution with a diagonal covariance matrix. This is a very simplified assumption, because Gaussian is the maximum entropy (least informative) distribution with respect to prescribed mean and variance and has one single mode, while human inference can be ambiguous and can have a bounded support when we exclude very unlikely cases (de Haan & Ferreira, 2006).\nMany recent works try to tackle this limitation. Jang et al. (2017) extended VAE to effectively use a discrete latent z following a category distribution (e.g. Bernoulli distribution). Kingma et al. (2014) extended the latent structure with a combination of continuous and discrete latent variables (class labels)\nar X\niv :1\n70 2.\n07 90\n4v 1\n[ cs\n.L G\n] 2\n5 Fe\nb 20\n17\nand applied the model into semi-supervised learning. Similarly, Shu et al. (2016) and Dilokthanakul et al. (2017) proposed to use a Gaussian mixture latent model in VAE. Serban et al. (2016) applied a piecewise constant distribution on z.\nThis work contributes a new ingredient in VAE model construction. To tackle the difficulty in dealing with complex probability density function (pdf) p(z) (z \u2208 Z), we generate instead a semi-continuous z \u2208 Z , by first discretizing the support Z into a grid, then drawing a discrete sample y based on the corresponding probability mass function (pmf), and then reconstruct y into z \u2208 Z . This coarse grain (CG) technique can help apply any pdf into VAE. Hence we apply a bounded polynomial exponential family (BPEF) as the underlying p(z), which is a universal pdf generator. This fits in the spirit of neural networks because the prior and posterior are not hand-crafted but learned by themselves.\nThis contribution blends theoretical insights with empirical developments. We present CG, BPEF, information monotonicity, etc., that are useful ingredients for general VAE modeling. Notably, we present a novel application scenario with new analysis on the Gumbel softmax trick (Jang et al., 2017; Maddison et al., 2016). We assemble these components into a machine CG-BPEF-VAE and present empirical results on unsupervised density estimation, showing improvements over vanilla VAE (Kingma & Welling, 2014) and category VAE (Jang et al., 2017). We present a novel perspective with theoretical analysis of VAE learning, with guaranteed bounds derived from information geometry (Amari, 2016).\nThis paper is organized as follows. Sec. 2 reviews the basics of VAE. Sec. 3 introduces CG-VAE and its implementation CG-BPEF-VAE. Sec. 4 performs an empirical study on two different datasets. Sec. 5 gives a theoretical analysis on VAE learning. Sec. 6 states our concluding remarks."}, {"heading": "2 Prerequisites: Variational Autoencoders", "text": "This section covers the basics from a brief introduction of variational Bayes to previous works on VAE. A generative model can be specified by a joint distribution between the observables x and the hidden variables z, that is, p(x, z |\u03b8) = p(z |\u03b8z)p(x | z,\u03b8x|z) where \u03b8 = (\u03b8z,\u03b8x|z). By Jensen\u2019s inequality,\n\u2212 log p(x |\u03b8) = \u2212 log \u222b q(z |x,\u03d5) p(x, z |\u03b8)\nq(z |x,\u03d5) dz \u2264 \u222b q(z |x,\u03d5) log q(z |x,\u03d5) p(x, z |\u03b8) dz ( def = L(\u03b8,\u03d5) ) , (1)\nfor any q(z |x,\u03d5). The upper bound L(\u03b8,\u03d5) on the RHS is known as the \u201cvariational free energy\u201d. We have\nL(\u03b8,\u03d5) = KL(q(z |x,\u03d5) : p(z |\u03b8z))\ufe38 \ufe37\ufe37 \ufe38 term1\n\u2212 \u222b q(z |x,\u03d5) log p(x | z,\u03b8x|z)dz\ufe38 \ufe37\ufe37 \ufe38\nterm2\n,\nwhere KL(\u00b7 : \u00b7) denotes the Kullback-Leibler (KL) divergence. (We will use term1 and term2 as shorthands for the two terms whose sum is L(\u03b8,\u03d5). One has to remember that they are functions of \u03b8 and \u03d5.) We therefore minimize the free energy with respect to both \u03b8 and \u03d5 so as to minimize \u2212 log p(x |\u03b8). The gap of the bound in eq. (1) is L(\u03b8,\u03d5) \u2212 (\u2212 log p(x |\u03b8)) = KL (q(z |x,\u03d5) : p(z |x,\u03b8)), which can be small as long as the parameter manifold of q(z |x,\u03d5) (e.g. constructed based on the mean field technique, Jordan et al. 1999) encompasses a good estimation of the true posterior.\nVAE (Kingma & Welling, 2014) assume the following generative process. The prior p(z |\u03b8z) = G(z |0, I) is parameter free, where G(\u00b7 |\u00b5,\u03a3) denotes a Gaussian distribution with mean \u00b5 and covariance matrix \u03a3. Denote dimx = D and dim z = d. The conditional mapping p(x | z,\u03b8) =\n\u220fD i=1 p (xi | f(z,\u03b8)) is parametrized by a neural network f(z,\u03b8) with input z and parameters \u03b8. For binary x, p(xi | \u00b7) is a Bernoulli distribution; for continuous x, p(xi | \u00b7) can be univariate Gaussian. This gives a very flexible p(x |\u03b8) to adapt the complex data manifold.\nIn this case, it is hard to select the parameter form of q(z |x,\u03d5), as the posterior p(z |x,\u03d5) has no closed form solution. VAE borrows again the representation power of neural networks and lets q(z |x,\u03d5) = G(z |\u00b5(x,\u03d5), diag(\u03bb(x,\u03d5))), where \u00b5(x,\u03d5) and \u03bb(x,\u03d5) are both neural networks with input x and parameters \u03d5, and diag(\u00b7) means a diagonal matrix constructed with a given diagonal vector. The assumption of a diagonal covariance is for reducing the network size so as to be efficient and to control overfitting.\nSince the KL of Gaussians is available in closed form, term1 has an analytical solution. In order to solve the integration in term2, VAE employs a reparameterization trick. It draws L i.i.d. samples 1, \u00b7 \u00b7 \u00b7 , L \u223c G( |0, I), where I is the identity matrix. Let zl = \u00b5(x,\u03d5) + \u03bb(x,\u03d5) \u25e6 l, where \u201c\u25e6\u201d denotes element-wise product. Then zl \u223c G(z |\u00b5(x,\u03d5), diag(\u03bb(x,\u03d5))). Hence\nterm2 \u2248 \u2212 1\nL L\u2211 l=1 log p ( x |\u00b5(x,\u03d5) + \u03bb(x,\u03d5) \u25e6 l,\u03b8 ) .\nThis trick allows error to backpropagate through the random mapping (\u00b5,\u03bb) z. Then L(\u03b8,\u03d5) = term1 + term2 can be expressed as simple arithmetic operations of the outputs of the hidden layer and the last layer. It can therefore be optimized e.g. with stochastic gradient descent. The optimization technique is called stochastic gradient variational Bayes (SGVB). The resulting architecture is presented in fig. 1a."}, {"heading": "3 CG-BPEF-VAE", "text": "We would like to extend VAE to incorporate a general inference process, where the model can learn by itself a proper p(z |x,\u03d5) within a flexible family of distributions, which is not limited to Gaussian or category distributions and can capture higher order moments of the posterior. We will therefore derive in this section a variation of VAE called CG-BPEF-VAE for Coarse-Grained Bounded Polynomial Exponential Family VAE."}, {"heading": "3.1 Bounded Polynomial Exponential Family", "text": "We try to model the latent z with a factorable polynomial exponential family (PEF) (Cobb et al., 1983; Nielsen & Nock, 2016) probability density function:\np(z) = d\u220f j=1 exp ( M\u2211 m=1 cjmz m j \u2212 \u03c8(cj) ) , (2)\nwhereM is the polynomial order,C = (cjm)d\u00d7M denotes the polynomial coefficients, and \u03c8 is a convex cumulant generating function (Amari, 2016). This PEF family can be regarded as the most general parameterization, because with large enough M it can approximate arbitrary finely any given p(z) satisfying weak regularity conditions (Cobb et al., 1983).\nFurthermore, we constrain z to have a bounded support so that z \u2208 [\u22121, 1]d, a hypercube. This gives z a focused density that is not wasted on unlikely cases, which is in contrast to Gaussian distribution with non-zero probability on the whole real line. This also allows one to easily explore extreme cases by setting zj to \u00b11 or beyond.\nFor example, if M = 2, then the resulting p(zj) \u221d exp ( cj1zj + cj2z 2 j ) includes the truncated Gaussian distribution (with one mode) as a special case when cj2 < 0. Moreover, the setting cj2 \u2265 0 encompasses more general cases and can have at most two modes.\nThe two important elements in constructing a VAE model are \u00c0 the KL divergence between q(z |x,\u03d5) and p(z |\u03b8z) must have a closed form; \u00c1 a random sample of q(z |x,\u03d5) can be expressed as a simple function between its parameters and some parameter-free random variables. Neither of these conditions are met for BPEF. We will address these difficulties in the remainder of this section."}, {"heading": "3.2 Coarse Grain", "text": "Our basic idea is to reduce the BPEF pdf into a discrete distribution, then draw samples based on the pmf, then reconstruct the continuous sample.\nWe sample R points uniformly on the interval [\u22121, 1]:\n\u03b6 = ( \u22121,\u22121 + 2\nR\u2212 1 , \u00b7 \u00b7 \u00b7 , 1\u2212 2 R\u2212 1 , 1\n)\u1d40 ,\nwhere the r\u2019th discrete value is \u03b6r = 2r\u2212(R+1) R\u22121 . For example, choosing R = 21 results in a precision of 0.1. In correspondence to these R locations, we assume for the j\u2019th latent dimension a random yj in \u2206R\u22121, the (R\u2212 1)-dimensional probability simplex, so that \u2211R r=1 yjr = 1, \u2200r, yjr \u2265 0. This yjr means the likelihood for zj taking the value \u03b6r. Intuitively, if we constrain yj to be one-hot (with probability mass only on vertices of \u2206R\u22121), and let P (yjr = 1) \u221d exp( \u2211M m=1 cjm\u03b6\nm r ), then the expectation zj =\u2211R\nr=1 yjr\u03b6r \u2208 [\u22121, 1] will be distributed like the BPEF in eq. (2). However, to apply the reparameterization trick, it is not known how to express a random one-hot sample yj as a simple function of the activation probabilities. Nor does Dirichlet distribution as a commonlyused density on \u2206R\u22121 can do the trick.\nThis reparameterization problem of category distribution is studied recently (Jang et al., 2017; Maddison et al., 2016) following earlier developments (Kuzmin & Warmuth, 2005; Maddison et al., 2014) on applying extreme value distributions (de Haan & Ferreira, 2006) to machine learning. Based on these previous studies, we let yj follow a Concrete distribution (Maddison et al., 2016), which is a continuous relaxation of the category distribution, with the key advantage that Concrete samples can be easily drawn to be applied to VAE. Details are explained as follows.\nThe standard Gumbel distribution (Gumbel, 1954) is defined on the support g \u2208 < with the cumulative distribution function P (g \u2264 x) = e\u2212e\u2212x . Therefore Gumbel samples can be easily obtained by inversion sampling g = \u2212 log(\u2212 logU), where U is uniform on (0, 1). Let gjr follows standard Gumbel distribution, then the random variable yj \u2208 \u2206R\u22121 defined by\nyjr = exp ((gjr + \u03c6jr)/T )\u2211R r=1 exp ((gjr + \u03c6jr)/T )\nis said to follow a Concrete distribution with location parameter \u03c6j and temperature parameter T : yj \u223c Con(\u03c6j , T ). This distribution has a closed-form probability density function (see Maddison et al. 2016)\nand has the following fundamental property \u2200r, P (\nlim T\u21920+ yjr = 1\n) = P (yjr > yjo,\u2200o 6= r)\n= exp(\u03c6jr)/ R\u2211 r=1 exp(\u03c6jr) ( def = \u03b1jr ) . (3)\nBasically, at the limit T \u2192 0+, the density will be pushed to the vertices of \u2206R\u22121, and Concrete random vectors yj tend to be onehot, with activation probability of the r\u2019th bit defined by \u03b1jr. Hence it can be considered as a relaxation (Maddison et al., 2016) of the category distribution. See fig. 2 for an intuitive view of the Concrete distribution. There are heavy volumes of densities around the vertices.\nIn our case, let \u03c6jr = \u2211M m=1 cjm\u03b6 m r , then the odds for yjr activated (i.e., the probability for zj taking\nthe value \u03b6r) will be proportional to exp (\u2211M m=1 cjm\u03b6 m r ) at the limit T \u2192 0+. This provides a way to simulate the BPEF density."}, {"heading": "3.3 The Model", "text": "Based on previous subsections, we assume the following generation process\n\u03b1jr = M\u2211 m=1 ajm\u03b6 m r , p(y |a) = d\u220f j=1 Con (yj |\u03b1j , T ) ,\nzj(a) = y \u1d40 j \u03b6, p(x | z,\u03b8) = D\u220f i=1 p (xi | f (z,\u03b8)) ,\nwhere A = (ajm)d\u00d7M is the parameters of the prior1, and f is defined by a neural network. One should always chooseM < R\u22121, because the polynomial \u2211R\u22121 m=1 cjm\u03b6 m r withR\u22121 free parameters can already represent any distribution in \u2206R\u22121. The setting M \u2265 R\u2212 1 makes the polynomial structure redundant. The corresponding inference process is given by\n\u03b2jr = M\u2211 m=1 bjm(x,\u03d5)\u03b6 m r ,\n1Strictly speaking the prior distribution only contains hyper-parameters that are set a priori. Here the term \u201cprior\u201d is more like a prior structure with learned parameters.\nq(y |x,\u03d5) = d\u220f j=1 Con (yj |\u03b2j , T ) ,\nzj(x,\u03d5) = y \u1d40 j \u03b6,\nwhere B(x,\u03d5) = (bjm(x,\u03d5))d\u00d7M is defined by a neural network. By Monte Carlo integration, it is straightforward that\nterm2 \u2248 1\nL L\u2211 l=1 D\u2211 i=1 log p\n( xi | f ( R\u2211 r=1 yl\u2022r\u03b6r,\u03b8 )) ,\nyljr = exp\n( (gljr + \u2211M m=1 bjm(x,\u03d5)\u03b6 m r )/T ) \u2211R r=1 exp ( (gljr + \u2211M m=1 bjm(x,\u03d5)\u03b6 m r )/T\n) , where (gljr) is a 3D tensor of independent Gumbel variables, and the approximation becomes accurate when L\u2192\u221e.\nFor simplicity, we assume T to be the same scalar during generation and inference. We adopt a simple annealing process of T , starting from Tmax, exponentially decaying to Tmin in the first half of training epochs, then keeping Tmin. The study (Jang et al., 2017) implies that Tmin = 0.5 \u223c 1 could be small enough to make the Concrete distribution approximate well a category distribution. The setting of Tmin will affect the computation of term1, which will be explained in the following subsection."}, {"heading": "3.4 Information Mononicity", "text": "We need to compute term1 which is the KL divergence between the posterior p(z |x,\u03d5) the prior p(z |a). This is the most complex part because these pdfs are not in closed form. However, we know that as T \u2192 0+ they converge to categories distributions over R evenly spanned positions on [\u22121, 1] (the vector \u03b6). Therefore we approximate term1 with the KL divergence between the corresponding category distributions, that is,\nterm1 \u2248 d\u2211 j=1 R\u2211 r=1 [ exp(\u03b2jr)\u2211R r=1 exp(\u03b2jr)\n\u00d7 log exp(\u03b2jr)/(\n\u2211R r=1 exp(\u03b2jr))\nexp(\u03b1jr)/( \u2211R r=1 exp(\u03b1jr))\n]\n= d\u2211 j=1 [\u2211R r=1 exp(\u03b2jr)(\u03b2jr \u2212 \u03b1jr)\u2211R r=1 exp(\u03b2jr)\n+ log R\u2211 r=1 exp(\u03b1jr)\u2212 log R\u2211 r=1 exp(\u03b2jr) ] . (4)\nIn the rest of this subsection we give theoretical and empirical justifications of this approximation. KL divergence belongs to Csisza\u0301r\u2019s f -divergence family and therefore satisfy the well-known information monotonicity (Amari, 2016). Basically, the support V can be partitioned into subregions {Vr} with zero volume overlap, so that V = ]Vr. Denote by p1(Vr) = \u222b x\u2208Vr p1(x)dx the probability mass of Vr, then\u2211\nr p1(Vr) = 1 and the pmf {p1(Vr)} is a coarse grained version of p1(x). The information monotonicity principle states that KL(p1 : p2) \u2265 \u2211 r p1(Vr) log p1(Vr) p2(Vr)\n. See (Nielsen & Sun, 2016) for an analysis. Based on this principle, we have the following result.\nTheorem 1.\n\u00c0 KL(q(y |x,\u03d5) : p(y |a)) \u2265 KL(q(z |x,\u03d5) : p(z |a)) = term1;\n\u00c1 KL(q(y |x,\u03d5) : p(y |a)) is also lower bounded by the discrete KL given by the right hand side of eq. (4).\nBy theorem 1, the KL between two Concrete distributions are lower bounded by \u00c0 KL between the dimension reduced z (the exact value of term1); \u00c1 KL between the corresponding category distributions (our approximation of term1). If one uses Concrete latent variable and uses the category KL as term1 (e.g. in Category VAE, see fig. 1b), this is equivalent to minimizing a lower bound of the free energy, which is not ideal because such learning has less control over the free energy. In contrast, CG-BPEFVAE has a reconstruction layer y \u2192 z (see fig. 1c), which reduces the number of dimensions by a factor of R (e.g. in our experiments R \u2248 100). By theorem 1 \u00c0, this effectively reduces the KL divergence between the latent posterior and the latent prior. Intuitively, we can expect term1 to be much smaller than KL(q(y |x,\u03d5) : p(y |a)) and by minimizing the category KL, we have more faith to bring down term1 rather than KL(q(y |x,\u03d5) : p(y |a)).\nHow good is our approximation in eq. (4)? Unfortunately we do not have theoretically guaranteed bounds. Therefore we fall back to an empirical study. We generate category samples \u03b1 \u2208 \u220699, then generate the corresponding Gumbel distribution y, then reduce the dimensionality by z = y\u1d40\u03b6. Figure 3 shows the KL(\u03b1 : Uniform) (our approximation) and KL(p(z) : Uniform) (the true latent KL). We repeat 100 experiments for each of two different \u03b1 generator: a high entropy uniform generator over \u220699, and a low entropy generator based on a Dirichlet distribution with shape parameter \u03b1 = 0.5. (In practice we expect a low entropy posterior which is close to the latter case). The results suggest that our approximation is roughly an upper bound of the true KL divergence between latent distributions on small temperatures. Therefore we can expect that minimizing L(\u03b8,\u03d5) based on eq. (4) will bring down the free energy. See the appendix for more empirical study. A theoretical analysis is left to future work.\nEssentially term1 serves as a regularizor, constraining p(z |x,\u03d5) to have enough entropy to respect a common p(z) that does not vary with different samples. An approximated term1 is acceptable in many cases, because one can add a regularization strength parameter to tune the model (e.g. based on validation)."}, {"heading": "4 Experimental Results", "text": "We implemented the proposed method using TensorFlow (Abadi, Mart\u0131\u0301n et al., 2015) and tested it on two different datasets. The MNIST dataset (LeCun et al.) consists of 70,000 gray scale images of handwritten digits, each of size 28 \u00d7 28. The training/validation/testing sets are split according to the ratio 11 : 1 : 2. The SVHN dataset (Netzer et al., 2011) has around 100,000 gray-scale pictures (for simplicity the original 32\u00d7 32\u00d7 3 RGB images are reduced into 32\u00d7 32\u00d7 1 by averaging the 3 channels) of door numbers with a train/valid/test split of 10 : 1 : 3.5. These pictures are centered by cropping from real street view images.\nWe only investigate unsupervised density estimation. It is nevertheless meaningful to have unsupervised VAE results on the selected datasets for future references. We compare the proposed CG-BPEFVAE with vanilla VAE (Gauss-VAE) and Category VAE (Cat-VAE) (Jang et al., 2017). For MNIST, the candidate network shapes are 784-400-(10,20,\u00b7 \u00b7 \u00b7 ,80)-400-784 and 784-400-400-(10,20,\u00b7 \u00b7 \u00b7 ,80)-400- 400-784, equipped with densely connected layers and RELU activations (Nair & Hinton, 2010). For SVHN, the encoder network has 5 convolutional layers with fixed size, reducing the images into a 128- dimensional feature space, and a bottleneck layer of size (10,20,30,40,50) (5 different configurations). The decoder network has one RELU layer of size 128, followed by 4 transposed convolutional layers with fixed size. See the appendix for the detailed configurations.\nThe learning rate is \u03b3 \u2208 {10\u22124, 5 \u00d7 10\u22124, 10\u22123, 5 \u00d7 10\u22123}. For Cat-VAE and CG-BPEF-VAE, the initial and final temperature are Tmax \u2208 {1, Tmin} and Tmin \u2208 {0.5, 0.8}, respectively, with a simple exponential annealing scheme. For Cat-VAE, the number of categories is C \u2208 {5, 10, 15, 20}. For CGBPEF-VAE, we set the polynomial order M \u2208 {5, 10, 15, 20}, and the precision R \u2208 {51, 101}. The mini-batch size is fixed to 100. The maximum number of mini-batch iterations is 10,000. For all methods we adopt the Adam optimizer (Kingma & Ba, 2014) and the Xavier initialization (Glorot & Bengio, 2010), which are commonly recognized to bring improvements.\nThe performance is measured by the per-sample average free energyL(\u03d5,\u03b8). The best model with the\nsmallest validated L is selected. Then the we report its L on the testing set, along with the reconstruction error term2 so that one can tell its trade-off between model complexity (term1) and fitness to the data (term2). See table 1 for the results on two different latent sample size L and two different temperatures Tmin.\nWe clearly see that CG-BPEF-VAE shows the best results. Essentially, Gauss-VAE can be considered as a special case of CG-BEPF-VAE when M = 2 therefore cannot model higher order moments. CatVAE has neither a polynomial exponential structure to regulate the discrete variables, nor a dimensionality reduction layer to reduce the free energy. The good results of CG-BPEF-VAE are expected.\nNotice that as we increase the final temperature Tmin, both Cat-VAE and CG-BPEF-VAE will show \u201cbetter\u201d results. However, the estimation of the free energy will become more and more inaccurate especially for Cat-VAE, whose estimation is a lower bound of the actual free energy by theorem 1 (2). In high temperature, the free energy can be well above its reported L. In contrast, for CG-BPEF-VAE, its estimated L is an empirical upper bound of the free energy, as long as Tmin is set reasonably small (T = 0.5 \u223c 1, see fig. 3).\nAll models prefer deep architectures over shallow ones. There is a significant improvement of CatVAE and CG-BPEF-VAE when L is increased from 1 to 10, when Cat-VAE starts to prefer larger category numbers. A large sample size L is required to model complex multimodal distributions and is recommended for Cat-VAE and CG-BPEF-VAE. As the size of the decoder network scales linearly with L, one will face significantly higher computation cost during increasing L.\nAs compared to MNIST, SVHN is more difficult to get improved over the baseline results by GaussVAE, because its data manifold is much more complex. One has to incorporate supervised information (Kingma et al., 2014) to achieve better results.\nCat-VAE and CG-BPEF-VAE are more computational costly than Gauss-VAE. In Cat-VAE, the tensor z has a size of batch size\u00d7 L\u00d7 d\u00d7 C. In CG-BPEF-VAE, the tensor y have a size of batch size\u00d7 L\u00d7 d\u00d7R, although this is immediately reduced to batch size\u00d7L\u00d7 d by the mapping y \u2192 z. A high precision CG-BPEF-VAE or a Cat-VAE with a large category number will multiply the computational time. Our implementation is available at https://github.com/sunk/cgvae."}, {"heading": "5 Information Geometry of VAE", "text": "This is a relatively separate section. We present a geometric theory which can be useful to uncover the intrinsics of general VAE modeling not limited to the proposed CG-BPEF-VAE, so that one can architect useful VAE models not only based on variational inference, but also along another geometric axis. We also use this geometry to discuss advantages of the proposed CG-BPEF-VAE.\nNotice, this geometry is not about the input feature space or the latent space (space of x and z), but about the models (space of \u03b8 and \u03d5) or information geometry (Amari, 2016).\nWe will consider the cost function L(\u03b8,\u03d5) averaged with respect to i.i.d. observations {xk}nk=1. term1 is the average KL divergence between q(z |xk) and p(z). Assume that both p(z) and q(z |xk) are in the same exponential family M(\u03d5) so that p(z) = exp(t\u1d40(z)\u03d5z \u2212 F (\u03d5z)) and q(z |xk) = exp(t\u1d40(z)\u03d5k \u2212 F (\u03d5k)), where t(z) is a vector of sufficient statistics (for example in CG-BPEF-VAE, t(z) = (z, z2, z3, \u00b7 \u00b7 \u00b7 )), and F (\u03d5) is a convex cumulant generating function2. This M(\u03d5) is a statistical manifold, i.e., space of probability distributions where \u03d5 serves as a coordinate system. The dual parameters (Amari, 2016) ofM(\u03d5), which form another coordinate system, are defined by the moments \u03b7 = E(t(z)) = \u222b p(z)t(z)dz. These two coordinate systems can be transformed back and forth by the Legendre transformations \u03b7 = F \u2032(\u03d5), \u03d5 = I \u2032(\u03b7), where I is Shannon\u2019s information (negative entropy). 2 In this section, we will denote p(z |\u03d5z) instead of p(z |\u03b8z) (as in previous sections) to emphasize that p(z) is in the same statistical manifold with q(z |xk).\nBy straightforward derivations,\nterm1 = 1\nn n\u2211 k=1 [ I(\u03b7k)\u2212 (\u03b7k)\u1d40\u03d5z ] + F (\u03d5z).\nNotice that the prior p(z) only appears in term1 but not in term2. We therefore consider a free p(z) which minimizes term1 with {\u03d5k}nk=1 fixed. We have\n\u2202term1 \u2202\u03d5z = \u2212 1 n n\u2211 k=1 \u03b7k + \u2202F (\u03d5z) \u2202\u03d5z = \u03b7z \u2212 1 n n\u2211 k=1 \u03b7k.\nTherefore the optimal (\u03b7z)? = 1n \u2211n k=1 \u03b7\nk is the Bregman centroid (Nielsen & Nock, 2009) of {\u03d5k}nk=1. Geometrically, term1 is the average divergence between \u03d5k and the Bregman centroid and therefore measures the n-body compactness of {\u03d5k}nk=1. We can therefore have a lower bound of term1.\nTheorem 2. Given q(z |xk) in an exponential familyM(\u03d5), if p(z) is in the same exponential family, then\nterm1 \u2265 1\nn n\u2211 k=1 I ( \u03b7k ) \u2212 I\n( 1\nn n\u2211 k=1 \u03b7k\n) \u2265 0, (5)\nwhere the first \u201c=\u201d holds if and only if \u03b7z = 1n \u2211n k=1 \u03b7\nk. If p(z) is non-parametric (not constrained by any parametric structure), then\nterm1 \u2265 1\nn n\u2211 k=1 I(\u03b7k)\u2212 I(m) \u2265 0, (6)\nwhere m(z) = 1n \u2211n k=1 q(z |xk) is a mixture model which is outsideM(\u03d5).\nComparatively, the non-parametric lower bound eq. (6) is smaller than the parametric bound eq. (5). However it needs to compute the entropy of mixture models (Nielsen & Sun, 2016), which does not have an analytic solution. Essentially, term1 is related to the convexity of Shannon information. In standard VAE, p(z) is fixed to the standard Gaussian distribution, which is not guaranteed to be the Bregman centroid, and does not activate the lower bound in theorem 2. In CG-BPEF-VAE, term1 is closer to this bound because p(z) is set free in our modeling. This hints that as a future work one can directly replace term1 with the lower bound stated in theorem 2 to avoid the model selection of p(z) and to achieve better performance.\nLet \u00b5k = \u00b5(xk,\u03d5) and V k = V (xk,\u03d5) 0 be the mean and covariance matrix of q(z |xk,\u03d5), respectively. A Taylor expansion of log p(xk | z,\u03b8) at z = \u00b5k gives\nterm2 \u2248 1\nn n\u2211 k=1 \u222b q(z |xk,\u03d5) [ \u2212 log p(xk |\u00b5k,\u03b8)\n\u2212 (z \u2212 \u00b5k)\u1d40 log p(x k |z,\u03b8)\n\u2202z +\n1 2 (z \u2212 \u00b5k)\u1d40G\u03b8(\u00b5k)(z \u2212 \u00b5k)\n] dz\n= 1\nn n\u2211 k=1 [ \u2212 log p(xk |\u00b5k,\u03b8) + 1 2 tr ( G\u03b8(\u00b5k)V k )] ,\nwhere\nG\u03b8(\u00b5k) = \u2212 \u22022 log p(xk | z,\u03b8)\n\u2202z2\n\u2223\u2223\u2223\u2223 z=\u00b5k\nis the observed Fisher information metric (FIM)3 (Amari, 2016) wrt z depending on \u03b8. The approximation is accurate when q(z |xk,\u03d5) is Gaussian with vanishing centered-moments of order 3 or above.\nAssuming the inference network is flexible enough, minimizing term2 alone gives \u00b5k = (zk)?, V k = 0, where (zk)? = arg maxz log p(x\nk | z, \u03b8) is the maximum likelihood estimation wrt xk. This (zk)? is the latent z learned by a plain autoencoder. Hence term2 measures a dissimilarity between q(z |xk,\u03d5) and the Dirac delta distribution \u03b4((zk)?). By theorem 2, we get the following approximation of the variational bound.\nCorollary 3. Assume the inference network is flexible enough. Consider a variation of Gaussian VAE, where both p(z) and q(z |xk) are free Gaussian distributions. The optimal L? is given by\nL? = min {\u00b5k,V k,\u03b8}\n1\nn n\u2211 k=1 [ \u2212 log p(xk |\u00b5k,\u03b8) + 1 2 tr ( G\u03b8(\u00b5k)V k ) \u2212 1\n2 log |V k|\n] + 1 2 log \u2223\u2223\u2223V k + \u00b5k(\u00b5k)\u1d40 \u2212 \u00b5k (\u00b5k)\u1d40\u2223\u2223\u2223 ,\nwhere \u201c \u00b7\u201d means averaging over k = 1, \u00b7 \u00b7 \u00b7 , n. Remark 4. Consider roughly V k \u2248 V and V z = \u00b5k(\u00b5k)\u1d40 \u2212 \u00b5k (\u00b5k)\u1d40. The term 12 tr ( G\u03b8(\u00b5k)V ) helps to shrink V towards 0 and lets V respect the data manifold encoded in the spectrum of G\u03b8(\u00b5k). The term \u2212 12 log |V |+ 1 2 log |V + Vz| enlarges V and lets V respect the latent manifold of {\u00b5\nk}. This reveals a fundamental trade-off between fitting the input data and generalizing.\nRemark 5. In Gaussian VAE with L = 1, the term 12 tr ( G\u03b8(\u00b5k)V k ) is inaccurate. Approximating this term can potentially give more effective implementations of VAE.\nRemark 6. Using BPEF for q(z |xk,\u03d5) has the advantage that the information preserved in higher order differentiations \u2202\nd log p(x | z, \u03b8) \u2202zd (d \u2265 3) is captured.\nIn summary, fig. 4 presents the information geometric background of VAE on the statistical manifold M(\u03d5) which includes p(z), q(z |xk) and \u03b4((zk)?). Note that \u03b4((zk)?) is along the boundary ofM(\u03d5), where the variance is 0. For example, a Gaussian distribution G(\u00b5, \u03c3) with \u03c3 \u2192 0 becomes \u03b4(\u00b5). {\u03b4((zk)?)} vary according to maximum likelihood learning along another statistical manifold M(\u03b8). The cost function L is interpreted as the geometric compactness of those three sets of distributions. This\n3The FIM is mostly computed for parameters on a statistical manifold to describe parameter sensitivity. In contrast, we compute the FIM with respect to the hidden variable z.\nis essentially related to the theory of minimum description length (Hinton & Zemel, 1994; Sun et al., 2015). G\u03b8(z) also gives a lower bound (Crame\u0301r-Rao bound, Crame\u0301r 1946) on the variance of z, which is given by G\u22121\u03b8 (z). In other words, there is a minimum precision (or maximum accuracy) that one can achieve in the inference of z given xk. Although it is hard to compute exactly, it is important to realize the existence of this bound. We give the following rough estimation. Because each z has only single observation, the FIM of z does not scale with n. On the other hand, \u03b8 has n repeated observations. Therefore its FIM G(\u03b8) = \u2211n k=1Ep ( \u2212\u22022 log p(xk | z,\u03b8)/\u2202\u03b82 ) scales linearly with n, meaning that the\nprecision of \u03b8 scales with 1/ \u221a n. Therefore the precision of z is roughly \u221a n times the precision of \u03b8. Hence the estimation of z should indeed be inaccurate as compared to \u03b8. This means that the proposed coarse grain technique is not only for computational convenience, but also has a theoretical background."}, {"heading": "6 Concluding Remarks", "text": "Within the variational auto-encoding framework (Kingma & Welling, 2014), this paper proposed a new method CG-BPEF-VAE. Among numerous variations of VAE (Burda et al., 2016; Jang et al., 2017), CG-BPEF-VAE is featured by using a universal BPEF density generator in the inference model, and providing a principled way to simulate continuous densities using discrete latent variables. For example, to apply another sophisticated distribution on the latent variable z, one can employ our CG technique so as to use the reparameterization trick. This study touches a fundamental problem in unsupervised learning: how to build a discrete latent structure to factor information in a continuous representation? We provide preliminary results on unsupervised density estimation, showing performance improvements over the original VAE and category VAE (Jang et al., 2017). An empirical study can be extended to semi-supervised learning. This is ongoing work.\nWe try to picture an information geometric background of VAE. Essentially VAE learns on two manifoldsM(\u03b8) andM(\u03d5), where the cost function can be geometrically interpreted as a sum of divergences within a n-body system. This potentially leads to new implementations based on information geometry, e.g., using alternative divergences.\nBPEF uses a linear combination of basis distributions in the \u03b8-coordinates (natural parameters). Another basic way to define probability distributions is mixture modeling, or linear combination in the \u03b7-coordinates (moment parameters). The coarse grained technique can be extended to a mixture of BPEF densities, which could more effectively model multi-modal distributions."}, {"heading": "Acknowledgments", "text": "This research is funded by King Abdullah University of Science and Technology. The experiments are conducted on the Manda cluster provided by Computational Bioscience Research Center at KAUST."}, {"heading": "A Proof of Theorem 1", "text": "Proof. We first prove (1). The mapping z = y\u1d40\u03b6 is from the simplex\n\u2206R\u22121 = { y \u2208 <R :\nR\u2211 i=1 yi = 1; \u2200i, yi \u2265 0\n}\nto the line segment [-1,1]. We therefore define the following subset\nSz = { y \u2208 \u2206R\u22121 : y\u1d40\u03b6 = z } \u2282 \u2206R\u22121,\nwhere \u22121 \u2264 z \u2264 1. There we have the following partition scheme\n\u2206R\u22121 = 1\u228e z=\u22121 Sz.\nNote KL(q(y |x,\u03d5) : p(y |a)) is the KL divergence between two distributions on \u2206R\u22121. By information monotonicity,\nKL(q(y |x,\u03d5) : p(y |a)) \u2265 KL(q(Sz |x,\u03d5) : p(Sz |a)).\nwhere q(Sz |x,\u03d5) = \u222b y\u2208Sz q(y |x,\u03d5)dy and p(Sz |a) = \u222b y\u2208Sz p(y |a)dy are coarse grained distributions defined on [\u22121, 1], that is, q(z |x,\u03d5) and p(z |a). To prove (2), we partition \u2206R\u22121 based on a Voronoi diagram. Let\nVr = {y \u2208 \u2206R\u22121 : \u2016y \u2212 er\u20162 \u2264 \u2016y \u2212 eo\u2016,\u2200o 6= r}.\nwhere er \u2208 \u2206R\u22121 has the r\u2019th bit set to 1 and the rest bits set to 0. By the basic property of Concrete distribution (Eq.(8) in the paper),\u222b\ny\u2208Vr q(y |x,\u03d5)dy = P (yr \u2265 yo,\u2200o 6= r) = exp(\u03b2jr)\u2211R r=1 exp(\u03b2jr)\n,\u222b y\u2208Vr p(y |a)dy = exp(\u03b1jr)\u2211R r=1 exp(\u03b1jr) .\nThen (2) follows immediately from information monotonicity."}, {"heading": "B Proof of Theorem 2", "text": "Lemma 1. Let p(z) = exp(\u03d5\u1d40t(z) \u2212 F (\u03d5)) be a distribution in an exponential family , then we have I(\u03b7)\u2212 \u03b7\u1d40\u03d5+ F (\u03d5) = 0.\nProof. By definition, I(\u03b7) = \u222b p(z) log p(z)dz = \u222b p(z) (\u03d5\u1d40t(z)\u2212 F (\u03d5)) dz\n= \u03d5\u1d40 \u222b p(z)t(z)dz \u2212 F (\u03d5) = \u03d5\u1d40\u03b7 \u2212 F (\u03d5).\nProof. If both q(z |xk) and p(z) are in the same exponential family, we have\np(z) = exp (t\u1d40(z)\u03d5z \u2212 F (\u03d5z)) , q(z |xk) = exp ( t\u1d40(z)\u03d5k \u2212 F (\u03d5k) ) .\nTherefore\nterm1 = 1\nn n\u2211 k=1 KL ( q(z |xk,\u03d5) : p(z |\u03d5z) ) = 1\nn n\u2211 k=1 \u222b q(z |xk,\u03d5) log q(z |x k,\u03d5) p(z |\u03d5z) dz\n= 1\nn n\u2211 k=1 [ I(\u03b7k)\u2212 \u222b q(z |xk,\u03d5)(t\u1d40(z)\u03d5z \u2212 F (\u03d5z))dz ]\n= 1\nn n\u2211 k=1 [ I(\u03b7k)\u2212 (\u03b7k)\u1d40(\u03d5z) + F (\u03d5z) ] . (7)\nBecause F (\u03d5z) is convex with respect to \u03d5z , setting its derivative\n\u2202term1 \u2202\u03d5z = 1 n n\u2211 k=1 [ \u2212\u03b7k + \u2202F \u2202\u03d5z ] = 1 n n\u2211 k=1 [ \u2212\u03b7k + \u03b7z ] (Legendre transformation)\nto zero gives the unique minimizer of term1:\n(\u03b7z)? = 1\nn n\u2211 k=1 \u03b7k.\nPlugging this into Eq. 7, we get\nterm?1 = 1\nn n\u2211 k=1 [ I(\u03b7k)\u2212 (\u03b7k)\u1d40(\u03d5z)? + F ((\u03d5z)?) ] = 1\nn n\u2211 k=1 [ I(\u03b7k)\u2212 ((\u03d5z)?)\u1d40\u03b7k + ((\u03d5z)?)\u1d40(\u03b7z)? \u2212 I((\u03b7z)?) ] (by the Lemma)\n= 1\nn n\u2211 k=1 I(\u03b7k)\u2212 ((\u03d5z)?)\u1d40 1 n n\u2211 k=1 \u03b7k + ((\u03d5z)?)\u1d40(\u03b7z)? \u2212 I((\u03b7z)?)\n= 1\nn n\u2211 k=1 I(\u03b7k)\u2212 I\n( 1\nn n\u2211 k=1 \u03b7k\n) .\nBy the above analysis, term1 \u2265 term?1, and the \u201c=\u201d holds if and only if \u03b7z = (\u03b7z)?. The second \u201c\u2265\u201d is straightforward from the fact that I is a convex function in the coordinate system \u03b7.\nIf p(z) is non-parametric, then\nterm1 = 1\nn n\u2211 k=1 \u222b q(z |xk,\u03d5) log q(z |x k,\u03d5) p(z |\u03d5z) dz\n= 1\nn n\u2211 k=1 [ I(\u03b7k)\u2212 \u222b q(z |xk,\u03d5) log p(z |\u03d5z)dz ]\n= 1\nn n\u2211 k=1 I(\u03b7k)\u2212 \u222b 1 n n\u2211 k=1 q(z |xk,\u03d5) log p(z |\u03d5z)dz. (8)\nTherefore, term1 is minimized at p(z | (\u03d5z)?) = 1n \u2211n k=1 q(z |xk,\u03d5). Plugging this minimizer into the above Eq. 8, we get\nterm?1 = 1\nn n\u2211 k=1 I(\u03b7k)\u2212 \u222b 1 n n\u2211 k=1 q(z |xk,\u03d5) log [ 1 n n\u2211 k=1 q(z |xk,\u03d5) ] dz.\nNote that the mixture model 1n \u2211n k=1 q(z |xk,\u03d5) is outside the exponential familyM(\u03d5)."}, {"heading": "C The Effect of the Dimensionality Reduction Layer of CG-BPEFVAE", "text": "Fig. 5 shows the KL(p(z) : Uniform) (KL(z)) and KL(\u03b1 : Uniform) (KL(category)) when \u03b1 is generated by Dirichlet distributions with different configurations. In all cases, KL(\u03b1 : Uniform) is lower bounded by KL(p(z) : Uniform) for small temperature.\nD Visualization of the Concrete Distribution Fig. 6, Fig. 7 and Fig. 8 show Concrete densities generated by random sampling. For each experiment (sub-figure), we generate 106 Concrete samples and plot the resulting density. There are very high density regions near the corner (the red region), which are cropped so that the visualization is clear.\nAn interesting observation is that the density will \u201cleak\u201d to the simplex faces if T is small, although in this case the density will concentrate on the corners. Therefore it may not always be good to choose a small T . This is ongoing study."}, {"heading": "E Details of the Convolutional Layers", "text": "We used convolutional layers on the SVHN dataset. The encoder is specified by\n\u2022 Input: 1\u00d7 32\u00d7 32 (RGB is averaged into 1 channel)\n\u2022 Convolutional layer: 32 (5\u00d7 5) filters, with ReLU activation and no padding (\u2192 32\u00d7 28\u00d7 28)\n\u2022 Pooling layer: 2\u00d7 2 filter with a stride of 2 and no padding zeros (\u2192 32\u00d7 14\u00d7 14)\n\u2022 Convolutional layer: 64 (5\u00d7 5) filters, with ReLU activation and no padding (\u2192 64\u00d7 10\u00d7 10)\n\u2022 Pooling layer: 2\u00d7 2 filter with a stride of 2 and no padding (\u2192 64\u00d7 5\u00d7 5)\n\u2022 Convolutional layer: 128 (5\u00d7 5) filters, with RELU activation and no padding (\u2192 128\u00d7 1\u00d7 1)\nThe decoder is specified by\n\u2022 A dense linear layer with RELU activation to transform the dimension to 128\n\u2022 Transposed convolutional layer: 64 (5\u00d7 5) filters with stride 4; with RELU activation (\u2192 64\u00d7 4\u00d7 4)\n\u2022 Transposed convolutional layer: 32 (5\u00d7 5) filters with stride 2; with RELU activation (\u2192 32\u00d7 8\u00d7 8)\n\u2022 Transposed convolutional layer: 16 (5\u00d7 5) filters with stride 2; with RELU activation (\u2192 16\u00d7 16\u00d7 16)\n\u2022 Transposed convolutional layer: 1 (5\u00d7 5) filters with stride 2; without non-linear activation (\u2192 1\u00d7 32\u00d7 32)"}], "references": [{"title": "TensorFlow: Large-scale machine learning", "author": ["Abadi", "Mart\u0131\u0301n"], "venue": "on heterogeneous systems,", "citeRegEx": "Abadi and Mart\u0131\u0301n,? \\Q2015\\E", "shortCiteRegEx": "Abadi and Mart\u0131\u0301n", "year": 2015}, {"title": "Information Geometry and its Applications, volume 194 of Applied Mathematical Sciences", "author": ["Amari", "Shun-ichi"], "venue": null, "citeRegEx": "Amari and Shun.ichi.,? \\Q2016\\E", "shortCiteRegEx": "Amari and Shun.ichi.", "year": 2016}, {"title": "Importance weighted autoencoders", "author": ["Burda", "Yuri", "Grosse", "Roger", "Salakhutdinov", "Ruslan"], "venue": "In ICLR,", "citeRegEx": "Burda et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2016}, {"title": "Estimation and moment recursion relations for multimodal distributions of the exponential family", "author": ["Cobb", "Loren", "Koppstein", "Peter", "Chen", "Neng Hsin"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Cobb et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Cobb et al\\.", "year": 1983}, {"title": "Mathematical Methods of Statistics, volume 9 of Princeton Mathematical Series", "author": ["Cram\u00e9r", "Harald"], "venue": null, "citeRegEx": "Cram\u00e9r and Harald.,? \\Q1946\\E", "shortCiteRegEx": "Cram\u00e9r and Harald.", "year": 1946}, {"title": "Extreme Value Theory: An Introduction. Springer Series in Operations Research and Financial Engineering", "author": ["de Haan", "Laurens", "Ferreira", "Ana"], "venue": null, "citeRegEx": "Haan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Haan et al\\.", "year": 2006}, {"title": "Deep unsupervised clustering with Gaussian mixture variational autoencoders", "author": ["Dilokthanakul", "Nat", "Mediano", "Pedro A.M", "Garnelo", "Marta", "Lee", "Matthew C.H", "Salimbeni", "Hugh", "Arulkumaran", "Kai", "Shanahan", "Murray"], "venue": "In ICLR,", "citeRegEx": "Dilokthanakul et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Dilokthanakul et al\\.", "year": 2017}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In AISTATS; JMLR W&CP", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Rezende", "Danilo Jimenez", "Wierstra", "Daan"], "venue": "In ICML; JMLR W & CP", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Statistical theory of extreme values and some practical applications: a series of lectures", "author": ["Gumbel", "Emil Julius"], "venue": "Applied mathematics series. U. S. Govt. Print. Office,", "citeRegEx": "Gumbel and Julius.,? \\Q1954\\E", "shortCiteRegEx": "Gumbel and Julius.", "year": 1954}, {"title": "Autoencoders, minimum description length and Helmholtz free energy", "author": ["Hinton", "Geoffrey E", "Zemel", "Richard S"], "venue": "In NIPS", "citeRegEx": "Hinton et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1994}, {"title": "Categorical reparameterization with Gumbel-softmax", "author": ["Jang", "Eric", "Gu", "Shixiang", "Poole", "Ben"], "venue": "In ICLR,", "citeRegEx": "Jang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jang et al\\.", "year": 2017}, {"title": "An introduction to variational methods for graphical models", "author": ["Jordan", "Michael I", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawrence K"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational Bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Jimenez Rezende", "Danilo", "Welling", "Max"], "venue": "In NIPS", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Optimum follow the leader algorithm", "author": ["Kuzmin", "Dima", "Warmuth", "Manfred K"], "venue": "In COLT, pp", "citeRegEx": "Kuzmin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kuzmin et al\\.", "year": 2005}, {"title": "The MNIST database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher J.C"], "venue": "In NIPS", "citeRegEx": "LeCun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2014}, {"title": "The Concrete distribution: A continuous relaxation of discrete random variables", "author": ["Maddison", "Chris J", "Mnih", "Andriy", "Teh", "Yee Whye"], "venue": "CoRR, abs/1611.00712,", "citeRegEx": "Maddison et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Maddison et al\\.", "year": 2016}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In ICML, pp", "citeRegEx": "Nair and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Sided and symmetrized Bregman centroids", "author": ["Nielsen", "Frank", "Nock", "Richard"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Nielsen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nielsen et al\\.", "year": 2009}, {"title": "Patch matching with polynomial exponential families and projective divergences", "author": ["Nielsen", "Frank", "Nock", "Richard"], "venue": "In International Conference on Similarity Search and Applications (SISAP),", "citeRegEx": "Nielsen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nielsen et al\\.", "year": 2016}, {"title": "Guaranteed bounds on information-theoretic measures of univariate mixtures using piecewise log-sum-exp", "author": ["Nielsen", "Frank", "Sun", "Ke"], "venue": "inequalities. Entropy,", "citeRegEx": "Nielsen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nielsen et al\\.", "year": 2016}, {"title": "Markov chain Monte Carlo and variational inference: Bridging the gap", "author": ["Salimans", "Tim", "Kingma", "Diederik", "Welling", "Max"], "venue": "In ICML; JMLR W&CP", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "Multi-modal variational encoder-decoders", "author": ["Serban", "II Iulian Vlad", "Alexander G. Ororbia", "Pineau", "Joelle", "Courville", "Aaron C"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Stochastic video prediction with conditional density estimation", "author": ["Shu", "Rui"], "venue": "In ECCV Workshop on Action and Anticipation for Visual Learning,", "citeRegEx": "Shu and Rui,? \\Q2016\\E", "shortCiteRegEx": "Shu and Rui", "year": 2016}, {"title": "Learning structured output representation using deep conditional generative models", "author": ["Sohn", "Kihyuk", "Lee", "Honglak", "Yan", "Xinchen"], "venue": "In NIPS", "citeRegEx": "Sohn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sohn et al\\.", "year": 2015}, {"title": "Information geometry and minimum description length networks", "author": ["Sun", "Ke", "Wang", "Jun", "Kalousis", "Alexandros", "Marchand-Maillet", "St\u00e9phane"], "venue": "In ICML; JMLR W&CP", "citeRegEx": "Sun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "An uncertain future: Forecasting from static images using variational autoencoders", "author": ["Walker", "Jacob", "Doersch", "Carl", "Gupta", "Abhinav", "Hebert", "Martial"], "venue": "In ECCV; LNCS 9911,", "citeRegEx": "Walker et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 27, "context": "1 Introduction Variational autoencoders (Kingma & Welling, 2014) and its variants (Rezende et al., 2014; Sohn et al., 2015; Salimans et al., 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al.", "startOffset": 82, "endOffset": 187}, {"referenceID": 24, "context": "1 Introduction Variational autoencoders (Kingma & Welling, 2014) and its variants (Rezende et al., 2014; Sohn et al., 2015; Salimans et al., 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al.", "startOffset": 82, "endOffset": 187}, {"referenceID": 2, "context": "1 Introduction Variational autoencoders (Kingma & Welling, 2014) and its variants (Rezende et al., 2014; Sohn et al., 2015; Salimans et al., 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al.", "startOffset": 82, "endOffset": 187}, {"referenceID": 25, "context": "1 Introduction Variational autoencoders (Kingma & Welling, 2014) and its variants (Rezende et al., 2014; Sohn et al., 2015; Salimans et al., 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al.", "startOffset": 82, "endOffset": 187}, {"referenceID": 12, "context": ", 2016) combine the two powers of variational Bayesian learning (Jordan et al., 1999) with strong generalization and a standard learning objective, and deep learning with flexible and scalable representations.", "startOffset": 64, "endOffset": 85}, {"referenceID": 13, "context": "They are attracting decent attentions, producing state-of-the-art performance in semi-supervised learning (Kingma et al., 2014) and image generation (Gregor et al.", "startOffset": 106, "endOffset": 127}, {"referenceID": 8, "context": ", 2014) and image generation (Gregor et al., 2015), and are getting applied in diverse areas such as deep generative modeling (Rezende et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 27, "context": ", 2014), image segmentation (Sohn et al., 2015), clustering (Dilokthanakul et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 6, "context": ", 2015), clustering (Dilokthanakul et al., 2017), and future prediction from images (Walker et al.", "startOffset": 20, "endOffset": 48}, {"referenceID": 29, "context": ", 2017), and future prediction from images (Walker et al., 2016).", "startOffset": 43, "endOffset": 64}, {"referenceID": 2, "context": ", 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al., 1999) with strong generalization and a standard learning objective, and deep learning with flexible and scalable representations. They are attracting decent attentions, producing state-of-the-art performance in semi-supervised learning (Kingma et al., 2014) and image generation (Gregor et al., 2015), and are getting applied in diverse areas such as deep generative modeling (Rezende et al., 2014), image segmentation (Sohn et al., 2015), clustering (Dilokthanakul et al., 2017), and future prediction from images (Walker et al., 2016). This paper discusses unsupervised learning with VAE which pipes an inference model q(z |x) with a generative model p(x | z), where x and z are observed and latent variables, respectively. A simple parameter-free prior p(z) combined with p(x | z) parameterized by a deep neural network results in arbitrarily flexible representations. However, its (very complex) posterior p(z |x) must be within the representation power of the inference machine q(z |x), so that the variational bound is tight and variational learning is effective. In the original VAE (Kingma & Welling, 2014), q(z |x) obeys a Gaussian distribution with a diagonal covariance matrix. This is a very simplified assumption, because Gaussian is the maximum entropy (least informative) distribution with respect to prescribed mean and variance and has one single mode, while human inference can be ambiguous and can have a bounded support when we exclude very unlikely cases (de Haan & Ferreira, 2006). Many recent works try to tackle this limitation. Jang et al. (2017) extended VAE to effectively use a discrete latent z following a category distribution (e.", "startOffset": 8, "endOffset": 1693}, {"referenceID": 2, "context": ", 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al., 1999) with strong generalization and a standard learning objective, and deep learning with flexible and scalable representations. They are attracting decent attentions, producing state-of-the-art performance in semi-supervised learning (Kingma et al., 2014) and image generation (Gregor et al., 2015), and are getting applied in diverse areas such as deep generative modeling (Rezende et al., 2014), image segmentation (Sohn et al., 2015), clustering (Dilokthanakul et al., 2017), and future prediction from images (Walker et al., 2016). This paper discusses unsupervised learning with VAE which pipes an inference model q(z |x) with a generative model p(x | z), where x and z are observed and latent variables, respectively. A simple parameter-free prior p(z) combined with p(x | z) parameterized by a deep neural network results in arbitrarily flexible representations. However, its (very complex) posterior p(z |x) must be within the representation power of the inference machine q(z |x), so that the variational bound is tight and variational learning is effective. In the original VAE (Kingma & Welling, 2014), q(z |x) obeys a Gaussian distribution with a diagonal covariance matrix. This is a very simplified assumption, because Gaussian is the maximum entropy (least informative) distribution with respect to prescribed mean and variance and has one single mode, while human inference can be ambiguous and can have a bounded support when we exclude very unlikely cases (de Haan & Ferreira, 2006). Many recent works try to tackle this limitation. Jang et al. (2017) extended VAE to effectively use a discrete latent z following a category distribution (e.g. Bernoulli distribution). Kingma et al. (2014) extended the latent structure with a combination of continuous and discrete latent variables (class labels)", "startOffset": 8, "endOffset": 1831}, {"referenceID": 11, "context": "Notably, we present a novel application scenario with new analysis on the Gumbel softmax trick (Jang et al., 2017; Maddison et al., 2016).", "startOffset": 95, "endOffset": 137}, {"referenceID": 18, "context": "Notably, we present a novel application scenario with new analysis on the Gumbel softmax trick (Jang et al., 2017; Maddison et al., 2016).", "startOffset": 95, "endOffset": 137}, {"referenceID": 11, "context": "We assemble these components into a machine CG-BPEF-VAE and present empirical results on unsupervised density estimation, showing improvements over vanilla VAE (Kingma & Welling, 2014) and category VAE (Jang et al., 2017).", "startOffset": 202, "endOffset": 221}, {"referenceID": 6, "context": "(2016) and Dilokthanakul et al. (2017) proposed to use a Gaussian mixture latent model in VAE.", "startOffset": 11, "endOffset": 39}, {"referenceID": 6, "context": "(2016) and Dilokthanakul et al. (2017) proposed to use a Gaussian mixture latent model in VAE. Serban et al. (2016) applied a piecewise constant distribution on z.", "startOffset": 11, "endOffset": 116}, {"referenceID": 3, "context": "1 Bounded Polynomial Exponential Family We try to model the latent z with a factorable polynomial exponential family (PEF) (Cobb et al., 1983; Nielsen & Nock, 2016) probability density function:", "startOffset": 123, "endOffset": 164}, {"referenceID": 3, "context": "This PEF family can be regarded as the most general parameterization, because with large enough M it can approximate arbitrary finely any given p(z) satisfying weak regularity conditions (Cobb et al., 1983).", "startOffset": 187, "endOffset": 206}, {"referenceID": 11, "context": "This reparameterization problem of category distribution is studied recently (Jang et al., 2017; Maddison et al., 2016) following earlier developments (Kuzmin & Warmuth, 2005; Maddison et al.", "startOffset": 77, "endOffset": 119}, {"referenceID": 18, "context": "This reparameterization problem of category distribution is studied recently (Jang et al., 2017; Maddison et al., 2016) following earlier developments (Kuzmin & Warmuth, 2005; Maddison et al.", "startOffset": 77, "endOffset": 119}, {"referenceID": 18, "context": "Based on these previous studies, we let yj follow a Concrete distribution (Maddison et al., 2016), which is a continuous relaxation of the category distribution, with the key advantage that Concrete samples can be easily drawn to be applied to VAE.", "startOffset": 74, "endOffset": 97}, {"referenceID": 18, "context": "Hence it can be considered as a relaxation (Maddison et al., 2016) of the category distribution.", "startOffset": 43, "endOffset": 66}, {"referenceID": 11, "context": "The study (Jang et al., 2017) implies that Tmin = 0.", "startOffset": 10, "endOffset": 29}, {"referenceID": 20, "context": "The SVHN dataset (Netzer et al., 2011) has around 100,000 gray-scale pictures (for simplicity the original 32\u00d7 32\u00d7 3 RGB images are reduced into 32\u00d7 32\u00d7 1 by averaging the 3 channels) of door numbers with a train/valid/test split of 10 : 1 : 3.", "startOffset": 17, "endOffset": 38}, {"referenceID": 11, "context": "We compare the proposed CG-BPEFVAE with vanilla VAE (Gauss-VAE) and Category VAE (Cat-VAE) (Jang et al., 2017).", "startOffset": 91, "endOffset": 110}, {"referenceID": 13, "context": "One has to incorporate supervised information (Kingma et al., 2014) to achieve better results.", "startOffset": 46, "endOffset": 67}, {"referenceID": 28, "context": "is essentially related to the theory of minimum description length (Hinton & Zemel, 1994; Sun et al., 2015).", "startOffset": 67, "endOffset": 107}, {"referenceID": 2, "context": "Among numerous variations of VAE (Burda et al., 2016; Jang et al., 2017), CG-BPEF-VAE is featured by using a universal BPEF density generator in the inference model, and providing a principled way to simulate continuous densities using discrete latent variables.", "startOffset": 33, "endOffset": 72}, {"referenceID": 11, "context": "Among numerous variations of VAE (Burda et al., 2016; Jang et al., 2017), CG-BPEF-VAE is featured by using a universal BPEF density generator in the inference model, and providing a principled way to simulate continuous densities using discrete latent variables.", "startOffset": 33, "endOffset": 72}, {"referenceID": 11, "context": "This study touches a fundamental problem in unsupervised learning: how to build a discrete latent structure to factor information in a continuous representation? We provide preliminary results on unsupervised density estimation, showing performance improvements over the original VAE and category VAE (Jang et al., 2017).", "startOffset": 301, "endOffset": 320}], "year": 2017, "abstractText": "Variational autoencoders (VAE) often use Gaussian or category distribution to model the inference process. This puts a limit on variational learning because this simplified assumption does not match the true posterior distribution, which is usually much more sophisticated. To break this limitation and apply arbitrary parametric distribution during inference, this paper derives a semi-continuous latent representation, which approximates a continuous density up to a prescribed precision, and is much easier to analyze than its continuous counterpart because it is fundamentally discrete. We showcase the proposition by applying polynomial exponential family distributions as the posterior, which are universal probability density function generators. Our experimental results show consistent improvements over commonly used VAE models.", "creator": "LaTeX with hyperref package"}}}