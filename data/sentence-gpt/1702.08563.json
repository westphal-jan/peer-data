{"id": "1702.08563", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "CIFT: Crowd-Informed Fine-Tuning to Improve Machine Learning Ability", "abstract": "Item Response Theory (IRT) allows for measuring ability of Machine Learning models as compared to a human population. However, it is difficult to create a large dataset to train the ability of deep neural network models (DNNs). We propose fine-tuning as a new training process, where a model pre-trained on a large dataset is fine-tuned with a small supplemental training set. Our results show that fine-tuning can improve the ability of a state-of-the-art DNN model for Recognizing Textual Entailment tasks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 27 Feb 2017 22:25:45 GMT  (385kb,D)", "https://arxiv.org/abs/1702.08563v1", "10 pages, 2 tables, 2 figures"], ["v2", "Wed, 28 Jun 2017 23:59:15 GMT  (382kb,D)", "http://arxiv.org/abs/1702.08563v2", "8 pages plus references, 3 tables, 2 figures"]], "COMMENTS": "10 pages, 2 tables, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["john p lalor", "hao wu", "hong yu"], "accepted": false, "id": "1702.08563"}, "pdf": {"name": "1702.08563.pdf", "metadata": {"source": "CRF", "title": "CIFT: Crowd-Informed Fine-Tuning to Improve Machine Learning Ability", "authors": ["John P. Lalor", "Hao Wu", "Hong Yu"], "emails": ["lalor@cs.umass.edu,", "hao.wu.5@bc.edu,", "hong.yu@umassmed.edu"], "sections": [{"heading": "Introduction", "text": "Machine Learning (ML) models require task-specific data from which the models can learn. Traditionally for supervised ML a model is trained on a set of labeled data and the model parameters are updated in order to minimize a predefined loss function that compares model outputs to the true label. This training process assumes that each gold standard label is correct and additionally, that each item is independently and equally important for updating model parameters.\nRecent work has shown that ML models can be directly compared to the latent ability of a human population using Item Response Theory (IRT). By fitting a model of ability using a large set of human-generated labels for a small dataset, IRT models characteristics of individual examples (called \u201citems\u201d) such as difficulty and discriminatory ability to estimate ability as a function of correctly answered items. IRT estimated ability relies on which items in a test set are answered correctly, not just the total number of correct items, and compares NLP models to human ability directly. High performance in terms of traditional evaluation metrics (e.g. recall, precision, accuracy) does not mean that the ML model has high ability (Lalor, Wu, and Yu 2016), due to characteristics of the test set and parameters of the items that are answered correctly. To achieve high performance in terms of ability, it is therefore important to train ML models by updating parameters based on ability.\nIdeally ML models could be trained to maximize ability directly using a large set of items with IRT estimated parameters. However IRT datasets require a large number of human-generated response patterns, and the sets of items to be labeled need to be kept short to avoid drops in annotator performance due to boredom, fatigue, etc. Therefore, the size of the IRT datasets is small, making it impossible to be used directly for training. This is especially true for DNN models which requires a large amount of data for training.\nIn this paper we propose Crowd-Informed Fine-Tuning (CIFT), an innovative fine-tuning approach to training that incorporates a specialized subset of data as a supplemental training set for a pre-trained learning model. The fine-tuning data models item characteristics as estimated according to human response data. Our hypothesis is that for a pre-trained model that has already learned a representation of a particular NLP task, introducing additional carefully selected supplemental training data via CIFT can improve performance. CIFT uses fine-tuning to minimize one of two loss functions: (i) Categorical Cross Entropy (CCE) where the probability of the correct label is 1, which represents memorizing the fine-tuning data, and (ii) Mean-Squared Error (MSE), where CIFT minimizes the difference between the model output probabilities and the empirical probability distribution of the crowdsourced responses over the labels. CIFT is \u201ccrowdinformed\u201d in two respects: (i) the data used for fine-tuning comes from IRT models that were fit using a large number of human-generated response patterns and (ii) when trained a model using MSE, CIFT updates parameters to be more closely aligned with the human distribution over labels.\nUsing MSE as the loss function attempts to prevent overfitting by learning a distribution over labels that does force overfitting by pushing probabilities to 1 for items where the empirical distribution os more spread out. In this study, we show that CIFT improves the performance of an NLP model in terms of IRT ability without negatively affecting accuracy and hurting generalizing capacity of the model. CIFT influences the parameters learned on the original full data set with a small amount of data. Our results show that CIFT is a simple and effective way to improve model ability without requiring a large amount of additional data for training.\nWe evaluate our approach on the Recognizing Textual Entailment (RTE) task with the Stanford Natural Language Inference (SNLI) data set (Bowman et al. 2015) as training\nar X\niv :1\n70 2.\n08 56\n3v 2\n[ cs\n.C L\n] 2\n8 Ju\nn 20\n17\ndata. For fine-tuning data we use the IRT evaluation scales for RTE data (Lalor, Wu, and Yu 2016). By supplementing training with the IRT data, we can improve model performance with regards to a human population, while avoiding overfitting normally associated with updating parameters on a small data set. Evaluation includes overall accuracy on the original SNLI test set as well as using IRT to model ability in terms of a human population.\nOur contributions are as follows: (i) we introduce CIFT and show that for a memory-augmented neural network (Munkhdalai and Yu 2017) CIFT improves model performance for RTE when compared with a human population, (ii) CIFT does not overfit on the supplemental data and therefore does not negatively affect generalization in terms of accuracy, and (iii) we motivate the use of CIFT as a way to improve performance in terms of ability with regards to a human population by leveraging specialized data."}, {"heading": "Background", "text": ""}, {"heading": "Item Response Theory", "text": "Item Response Theory is a psychometric methodology for scale construction and evaluation (Baker and Kim 2004). IRT is widely used in educational testing both for designing tests and analyzing human responses (graded as right or wrong) to a set of questions (called \u201citems\u201d).\nIRT jointly models an individual\u2019s ability and item characteristics to predict performance. IRT models make the following assumptions: (i) individual performance can be modeled as an unobserved latent trait dimension (called \u201cability\u201d or \u201cfactor\u201d), (ii) the probability of correctly answering an item is a function of the person\u2019s ability, (iii) responses to different items are independent of each other for a given ability level (local independence), and (iv) responses from different individuals are independent of each other (Lalor, Wu, and Yu 2016). With IRT, one models the ability of a respondent as a function of the characteristics of the individual items in a data set. IRT fits a model of ability according to response patterns to items provided by a human population.\nAccording to the two parameter logistic (2PL) singlefactor model, the probability for an individual j with latent ability \u03b8j to answer item i correctly can be modeled as:\npij(\u03b8j) = 1\n1 + e\u2212ai(\u03b8j\u2212bi) (1)\nwhere pij is the probability of individual j answering item i correctly, and \u03b8j is the estimated latent ability of individual j. In the 2PL model, ai is the discriminatory parameter for item i, which corresponds to the maximum slope of the resulting curve, and bi is the difficulty parameter, the ability level at which an individual has equal probability of answering the item correctly or incorrectly.\nThe goal of fitting an IRT model is to identify a set of items that can measure the latent ability trait \u03b8 across a range of ability levels (usually from \u22123 to 3 when measuring human ability). To do this you have to start with a set of candidate items. For these items you gather a large number of responses, known as \u201cresponse patterns.\u201d The IRT model is\nthen fit to these response patterns using Expectation Maximization to maximize the likelihood of the response patterns given the item parameters (Bock and Aitkin 1981). Ability estimates are then obtained using the estimated item parameters according to a Gaussian prior distribution.\nItem removal is an important part of the IRT model-fitting process. Items can be screened for removal according to a sequential process: local dependence and item fit are tested, and items that either do not fit the model well or have local dependence with other items are removed. This results in a calibrated set of examples that constitute a test set for measuring the specified latent ability factor.\nWhen selecting the items, it is beneficial to consider items that can discriminate according to ability at different levels. For this reason the set of items in the the calibrated IRT set are interesting to consider in the context of machine learning. Each item in the set has a specific difficulty and discriminatory parameter, such that individuals at different ability levels have different probabilities of answering the items correctly. This is in contrast to the large data sets that are usually used for training in ML models. Those training sets, particularly in classification tasks, do not consider different parameters of the items, and treat the examples in aggregate.\nIt is important to note that IRT ability is associated with a single dimension related to the items in the specific IRT test set. Items retained for an IRT analysis constitute a set of items that are locally independent in the sense that their interdependence is only due to the common latent ability dimension. Taken as a whole these items can reliably measure a latent ability trait.\nTerminology In keeping with previous work using IRT, we here define common IRT terms as they are applied to RTE. An item refers to a Premise-Hypothesis pair and its correct label (entailment, contradiction, or neutral). A vector of responses to some set of items (each graded as correct or incorrect) is a response pattern for that set of items. An evaluation scale is a specific set of items that has been calibrated by fitting an IRT model to some large set of response patterns. An evaluation scale can be administered as a test set to measure the latent ability common to the items in the evaluation scale. New respondents (either human or ML model) supplies a response pattern for the evaluation set (e.g. an output of predicted labels for the items), which is then used to estimate an ability score (or theta score) to the respondent. This ability score indicates how far from average performance the respondent is in terms of the original population. For this work we report ability as the percentage of the original population the respondent outperformed."}, {"heading": "Related Work", "text": "Recognizing Textual Entailment Recognizing Textual Entailment (RTE) or Natural Language Inference (NLI) refers to the task of classifying the semantic concepts of entailment and contradiction using NLP systems (Dagan, Glickman, and Magnini 2006; Bowman et al. 2015). In RTE, for a given Premise (P) and Hypothesis (H) sentence pair, a model must classify the relationship between the pair as entailment, contradiction, or neutral. If H can be inferred to\nbe true given that P is true, then P entails H. If H is false given that P is true, then H contradicts P. The relationship is neutral if the sentences are unrelated.\nIRT in Machine Learning (Lalor, Wu, and Yu 2016) introduced IRT for the evaluation of NLP models, showing that evaluation performance with respect to a human population provided more information than classification accuracy. For example, a high performing model in terms of accuracy can exhibit ability similar to that of average human performance if the test set is inherently easy. A DNN model that answered 96% of the IRT test items correctly for identifying the entailment label scored in the 44th percentile in terms of ability because the set of items was very easy for the human population. The same model scored in the 96th percentile of ability for a more difficult test set to identify contradiction with only 79% accuracy (Lalor, Wu, and Yu 2016).\n(Mart\u0131\u0301nez-Plumed et al. 2016) used IRT for ML models\u2019 responses to classification tasks and found that many assumptions that are made in IRT translate well to the analysis of machine learning models. They did not attempt to use IRT for building an evaluation scale or evaluating ML models, and did not remove items with poor fit to the model.\nLearning Techniques There have been a number of approaches to organizing training data to improve classification performance and generalization. Boosting is a learning procedure that attempts to reduce classification error in a training set by iteratively focusing more on misclassified items in a training set (Freund and Schapire 1997; Schapire and Singer 1999). More weight is assigned to the incorrectly classified items, such that later classifiers have a higher probability of training on these misclassified examples. Boosting in neural networks has also shown promise (Schwenk and Bengio 2000).\nOur approach differs from boosting in that we do not assume that misclassified items are inherently more difficult for a classifier. We model difficulty explicitly using IRT. Item difficulty parameters are modeled from a set of human responses, and are interpretable in the sense that they have a direct relation to the ability level of a test-taker.\n(Bengio et al. 2009) introduced curriculum learning as a training procedure. Training with simple concepts and gradually introducing more complex concepts when training a\nneural network can improve generalization and speed up convergence. They demonstrate the effectiveness of curriculum learning on several toy tasks and draw a comparison with boosting and active learning as well. Here we have an explicit and theoretically-grounded measure of difficulty in examples as modeled by IRT. Instead of selecting examples in order of difficulty, we use specialized data with difficulty parameters modeled by IRT to refine pre-trained models.\nFinally, this work is related to transfer learning and domain adaptation (Caruana 1995; Bengio et al. 2011; Bengio 2012; Yosinski et al. 2014), but with an important distinction. Transfer learning and domain adaptation focus on repurposing representations learned for a source domain to facilitate learning in a target domain. In this paper we want to improve performance in the source domain by fine-tuning with a specialized set of supplemental data. The fine-tuning data is associated with the source domain, and comes from a specific subset of the domain dataset that has been identified through IRT modeling of human responses.\nModeling Item and User Parameters The DARE model (Bachrach et al. 2012) jointly infers item difficulty and individual ability using a graphical model framework (Koller and Friedman 2009) in the context of crowdsourcing. DARE is able to infer correct answers for unknown labels given the item\u2019s difficulty and the ability estimates for individuals that have provided possible labels. In this work we use IRT test sets to improve model performance.\n(Bruce and Wiebe 1999) modeled latent traits of data points to identify a correct label. There has also been work in modeling individuals to identify poor annotators (Hovy et al. 2013), but neither jointly model the ability of individuals and data points, or apply the resulting metrics to NLP models. (Passonneau and Carpenter 2014) model the probability a label is correct along with the probability of an annotator to label an item correctly according to the (Dawid and Skene 1979) model, but do not consider difficulty or discriminatory ability of the data points.\nThere has been much work in probabilistic modeling of crowdsourcing, most frequently to identify a correct label given noisy crowdsourced labels, for example (Dawid and Skene 1979; Kim and Ghahramani 2012; Kamar, Kapoor, and Horvitz 2015; Nguyen et al. 2016). In contrast to this work, our goal is to use models of item characteristics to in-\nform training of ML models to leverage item-specific characteristics such as difficulty and discriminatory ability."}, {"heading": "Crowd-Informed Fine-Tuning", "text": "We would like to understand the effect of a carefully selected set of items on a pre-trained learning model. Can a small set of examples that have been selected for a particular purpose be used to improve overall performance in a model that has already been trained with a large training set? In our experiment we fine-tune a high-performing neural network model with a selection of supplemental data sets in order to identify where and how additional training data can improve performance, both in terms of accuracy and latent ability. We experiment with two loss rates that are grounded in expected model performance with respect to the fine-tuning data."}, {"heading": "Model and Data", "text": "Neural Semantic Encoders For this experiment we use a Neural Semantic Encoder (NSE) (Munkhdalai and Yu 2017), a memory augmented neural network. NSE uses read, compute, and write operations to maintain and update an external memory during training. The model was selected as it has one of the highest test set accuracies at the time of writing on the SNLI test set. 1\nStanford Natural Language Inference Corpus The model is trained on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al. 2015). SNLI is an order of magnitude larger than previously available RTE data sets, and it consists entirely of human-generated P-H pairs. There are 500k training items, 10k development items, and 10k test items in the corpus. Table 1 shows examples of P-H pairs in the IRT test sets, their corresponding labels, and the difficulty value as modeled with IRT. The first pair in Table 1, for example, is of a difficulty that an individual with latent ability of -1.92 has a 50% chance of correctly labeling the pair as neutral. Most items in the IRT test sets have difficulty parameters below 0 (or close to 0), which is appropriate for evaluating ML models (Lalor, Wu, and Yu 2016).\nFine-Tuning Data For the IRT data, we use the evaluation sets created by (Lalor, Wu, and Yu 2016). They obtained 1000 labels from a set of Amazon Mechanical Turk (AMT) annotators for a random sample of items from SNLI, and used IRT to select items and construct evaluation scales to measure latent ability. Their selected items were in five groups according to the correct label and the number of quality-control annotators who agreed on the label: entailment with 5 of 5 annotator agreement (G5Entailment), contradiction with 5 of 5 annotator agreement (G5-Contradiction), neutral with 5 of 5 annotator agreement (G5-Neutral), contradiction with 4 of 5 annotator agreement (G4-Contradiction), and neutral with 4 of 5 annotator agreement (G4-Neutral). Each test set consists of 20 to 30 P-H pairs, and each set is an independent scale for evaluating latent ability with regards to the ability to identify the relevant RTE label. For example, the G4-Contradiction IRT group is\n1For additional model details please refer to the original paper.\na test set that measures a human\u2019s (or ML model\u2019s) ability to recognize contradiction when presented with a P-H pair.\nThe supplemental IRT training data has two qualities that make it desirable as training data: (i) the data were identified because of their local independence from each other in the process of fitting an evaluation metric to measure latent ability, and (ii) each example in the data has a large number (approx. 1000) of human-annotated responses which can be interpreted as a probability distribution over the potential labels. The supplemental SICK data was generated entirely independently of the source data set and can be considered an alternative to the original training set."}, {"heading": "Experiments", "text": "To test our hypothesis we performed the following experiment. We trained the NSE DNN on a randomly selected subset of the SNLI training set (100, 1k, 2k, 10k, 100k, 200k, and the full 550k training set). We followed the original NSE training parameters and hyperparameters (Munkhdalai and Yu 2017). The model is trained for 40 epochs on the training data and evaluated on the development set at each epoch. NSE is initialized with the 300-D GloVe 840B word embeddings (Pennington, Socher, and Manning 2014). Model parameters where the development set accuracy is highest are retained and used for CIFT.\nThese models were then fine-tuned using CIFT with 4 of the 5 IRT test sets with both categorical cross entropy and mean squared error as loss functions. The fine-tuned models were then tested on the original SNLI test set (10k items) for accuracy and the held-out IRT test set for latent ability.\nOur goal with this experiment is to understand what effect, if any, a specialized training set has on a fully trained model in terms of performance. To evaluate the hypothesis, we report the mean accuracy averaged across the results for the 5 held out test sets at each training set size. In addition, we report IRT ability percentile scores, which represent the percentage of the human population that the model outperformed with respect to ability for the held out test set.\nWe compare our results to the following baseline methods: (i) pre-trained models with no fine-tuning, and (ii) a more traditional transfer learning scenario using a random selection of 100 items from another RTE corpus, SICK (Marelli et al. 2014) (iii) transfer learning using the entire SICK training set (5k items). This way we can compare performance between CIFT and traditional transfer learning. For SICK baselines training was done with categorical cross-entropy as the loss function, as there is not a large set of human annotations for this data.\nWe now explicitly define the training procedure for CIFT. Let XNtrain be a random selection of N examples from the SNLI training set, and let Xtest be the 10k example SNLI test set. Let IRT = {4N, 4C, 5N, 5C, 5E} be the 5 IRT evaluation sets. IRTtest is the held-out IRT set used for testing. CIFTtrain is the CIFT training set, consisting of 4 of the 5 subsets in IRT, CIFTtrain = IRT \\ IRTtest. For the transfer learning baseline, CIFTtrain is a random sample of M examples from the SICK training set, SICKMtrain where M \u2208 {100, 5000}. If M = 5000 we use the entire SICK training set. The CIFT procedure is defined in Algorithm 1.\nAlgorithm 1 CIFT Algorithm Input: NumEpochs e, XNtrain, Xtest, CIFTtrain, IRTtest, Loss Function l for i = 1 to e do\nTrain NSE with XNtrain with loss function CCE end for for i = 1 to e do\nTrain NSE with CIFTtrain with loss function l end for calculate accuracy for Xtest calculate ability for IRTtest\nWe now elaborate on the motivation for the choices of loss functions in our experiments.\nOverfitting on a Selection of Items The IRT test sets consist of a selection of items that were identified as being a good set of items for evaluating the latent ability trait associated with Recognizing Entailment. Our hypothesis here can then be thought of as \u201cstudying just for the test,\u201d where the pre-trained model (which already has a good representation of entailment) is fine tuned according to this specific subset of items. We expect that overfitting on these items will improve performance as the model learns specific information.\nOur loss function in this case is Categorical CrossEntropy (CCE). If a model outputs a length N vector of probabilities y\u0302 for a particular training example where the correct output vector is yi, CCE is calculated as:\nLCCEi = \u2212 \u2211 j\u2208N yij log p(y\u0302ij) (2)\nIn the case where a single element in the output vector yi has probability 1 (a common case in ML), CCE loss is:\nLCCEi = \u2212 log p(y\u0302i) (3) Loss is summed over all of the training examples:\nLCCE = N\u2211 i LCCEi (4)\nBy fine-tuning on the IRT sets using categorical cross entropy as the loss function, we are effectively asking the network to memorize this small subset of items.\nPushing the probability of the correct label towards 1 can be considered a proxy for training to increase ability score for those answers. Number correct is positively correlated with ability score, but it is not an exact approximation, as different response patterns with the same number of correct answers can result in different estimates of ability (Lalor, Wu, and Yu 2016). If the pre-trained network learns the supplemental training data such that it is able to classify most (or all) of the examples correctly, this implies a high ability score for those factors. The held out test set does not necessarily align with the supplemental training set in terms of the latent factor that is being evaluated, but because all of the examples deal with recognizing entailment in some fashion,\nwe hypothesize that the supplemental training set has some positive effect on performance.\nOne distinction to be made with regards to the IRT sets is the difficulty of items in each set. (Lalor, Wu, and Yu 2016) showed that the G4 groups are more difficult on average than the G5 groups. This is true with regards to the difficulty parameters as modeled by IRT, but it also follows when you consider what G4 and G5 represent. If 5 out of 5 annotators agree on a gold standard label, there is little disagreement about which label is correct. However if only 4 of 5 annotators agreed, there was some additional discrepancy which led to one of the annotators choosing a different label. In this sense, the G4 items are more difficult than the G5 items.\nWith this in mind, we can think about the fine-tuning process in the context of the held-out IRT test set. If a G4 set is held out for testing, we know that these items are more difficult than the items in the G5 sets, which are being used for training (along with one other G4 set). Therefore one hypothesis is that performance in terms of ability with regards to the held out set would not improve, given that learning easy items may not necessarily help if testing on hard items.\nThe opposite hypothesis is relevant when we hold out one of the G5 groups for testing. If we train with the more difficult examples (in this case the G4 items), then we would expect that the fine-tuned models will perform better in terms of ability on the held out set, as learning the more difficult items would be beneficial when tested on an easier test set.\nLearning from the Crowd This hypothesis is similar to the previous one, but in this case we do not want to completely overfit on the IRT items. Instead, we treat the human responses from Amazon Mechanical Turk (AMT) as an estimate of the probability distribution over labels for the items:\np(Y = y) = Ny N\n(5)\nWhere Ny is the count of occurrences where Y is labeled y andN is the total number of labels from the AMT workers.\nWe now use Mean Squared Error (MSE) instead of crossentropy as our loss function, where we minimize the difference between the estimated probabilities learned by the model and the empirical distributions obtained from AMT.\nLMSE = 1\nN N\u2211 i=1 (p\u0302(yi)\u2212 p(yi)) (6)\nIn this case, we are attempting to move the NSE model predictions closer to the AMT distribution of responses. We are not necessarily trying to push predicted probability values to 1, which is a departure from the standard understanding of single-label classification in ML. In our case, we hypothesize that updating weights according to differences in the observed probability distributions will improve the model by preventing it from updating too much for more difficult items (that is, examples where the empirical distribution is more evenly spread across the three labels).\nTraining with MSE in this scenario assumes that the crowdsourced distribution of responses is a better measure of correctness than a single gold-standard label for the set of\nitems. The crowd distribution over labels gives a fuller understanding of the items being used for training. CIFT can update parameters to move closer to this distribution without making large parameter updates under the assumption that a single correct label should have probability 1.\nIf we assume that ML performance is not at the level of an average human (which is reasonable in many cases), then fine-tuning with MSE as a loss function can help pull models towards average human behavior. If the model updates parameters to minimize the difference between its predictions and the distribution of responses provided by AMT workers, then the model predictions should look like that of the average AMT user. Therefore the resulting response patterns should score close to 50% in terms of IRT percentile.\nWhen ML model performance is better than the average AMT user, there is a risk that performance in terms of IRT percentile may suffer. The model may have learned a set of parameters that better models the data than the human population, and updating parameters to reflect the human distribution could lead to a drop in performance."}, {"heading": "Results", "text": ""}, {"heading": "IRT Ability", "text": "We first report theta scores for CIFT. Figure 1 plots theta percentile scores for each IRT test set as a function of train-\ning size (log scale). Theta scores are similar (or worse) for very small training set sizes, however at the large training set sizes (200K and 500K), there is more separation between the theta scores between the fine-tuning approaches.\nFor the G4 test sets (Contradiction and Neutral), finetuning with the rest of the IRT data does not improve performance. This is expected, as the G4 groups are more difficult than the G5 groups. By training on additional easy items, the model is not able to improve performance over the baseline model. Baseline results using a sample of examples from the SICK data set does lead to a higher theta percentile score for the G4 contradiction set, while using the full SICK training set is worse than the original baseline but outperforms CIFT. CIFT does not improve ability when the fine-tuning data is easier than the test data, but the introduction of a sample of data from an unseen dataset has a positive affect on ability. Interestingly the more traditional transfer learning baseline (using the full SICK training set) does not outperform the baseline for G4 test sets.\nFor the easier test sets (G5), fine-tuning does improve ability performance. Both IRT and SICK fine-tuning lead to improvements. For IRT, one would expect that fine-tuning with examples that are of a comparable difficulty (the other G5 groups) and items that are more difficult (the G4 items) would lead to improved performance, which is what we see. For G5-Contradiction and G5-Neutral, CIFT with CCE or\nMSE with the IRT data leads to similar performance.\nFor G5-Entailment, training with MSE as the loss function shows substantially higher performance. The G5Entailment group is very \u201ceasy\u201d in the sense that most of the examples\u2019 difficulty parameters are very low. High performance on this subset of data in terms of accuracy does not imply a high theta score, as most of the AMT population labeled most of these items correctly (Lalor, Wu, and Yu 2016). Because initial performance was so low to start, CIFT with MSE allows the model to significantly improve performance by moving closer to the empirical distribution as estimated by the AMT population.\nNote the improvement for the G5-Contradiction group when fine-tuning on MSE. Our original hypothesis was that using MSE as a loss function would pull the model performance towards the average human performance. However, performance in terms of theta was originally above the average human (about 60%), and after fine-tuning performance jumped to 70%. This result is unexpected given what the loss function states, so we must think about why this is the case. One explanation is that the original learned parameters are situated such that in order to move closer to the crowd distribution the model can only get to a point that is actually better than the crowd before settling to a new local minimum. In this case the fact that performance in terms of theta improved is a surprising result.\nIn terms of the transfer learning baselines, using the full SICK training set negatively impacts performance in all cases, while fine-tuning with a random sample from SICK improves performance over the original model in all cases except G4-Neutral, where the original model performed best. By introducing a subset of new data to the model in the fine-tuning phase, the model can update parameters without completely re-learning based on the new data. It is able to pick up patterns in the new data that it can apply to its existing representations instead of replacing them, as seems to be the case when fine-tuned with the entire SICK data set.\nTable 2 shows the accuracy scores after CIFT on the NSE model pre-trained with the full SNLI training set. Each IRT test set is short (between 20-30 items), so accuracies are sensitive to the response patterns. However we can see that in several cases, multiple procedures result in the same accuracy, but Figure 1 shows that the models have different ability scores. Even though the number of correct answers were the same, the specific response patterns result in different estimates of ability, which is consistent with previous results (Lalor, Wu, and Yu 2016)."}, {"heading": "Generalization with Fine-Tuning", "text": "We next plot accuracy results for the baseline models and both fine-tuned models at each training set size. We report mean accuracy results for each fine-tuning loss function used with CIFT (MSE and CCE) as well as baseline results. Figure 2 plots accuracy as a function of training size (log scale). As Figure 2 shows, accuracy generally does not improve after CIFT. However, performance in terms of accuracy after CIFT is very close to the pre-CIFT baseline. One explanation is that the models have already reached a strong point in terms of a local minimum, so there is only so much negative impact the additional overfitting can have in terms of reducing accuracy. Indeed, most results were obtained very early in the training process (around epoch 4 or so), which indicates that there was not too much overfitting.\nIn particular, CIFT with MSE tracks very closely with the original model even with very small training set sizes. This shows that using CIFT with MSE can improve ability while still being robust to overfitting.\nOne exception is the baseline where the full SICK training set is used for fine-tuning. In this case test accuracy is significantly lower than the other models, which was a surprising result given that it most closely resembles a traditional transfer learning scenario. The introduction of a new, large set of data may have led to the NSE model learning patterns in the SICK data that do not translate well to the SNLI test set. The other fine-tuning sets are small enough that they do not\nsubstantially alter the initial learned representations. Finally, we re-trained the NSE models after CIFT to see if fine-tuning was able to move the representations out of a local minimum and improve performance. Table 3 shows the accuracy on the full SNLI training (550k items) and test (10k items) sets for NSE models after a full training-CIFTfull training run. Each CIFT model is able to generalize well to the full test set after fine-tuning, and in the case of the CIFT-MSE model where fine-tuning was performed with all but the G5-Entailment set, outperforms the original baseline in terms of test set accuracy. In this case CIFT used the most difficult IRT items for fine-tuning according to MSE loss, which led to updated parameters according to the crowdsourced knowledge. This crowdsourced-informed representation leads to improved performance in the SNLI test set."}, {"heading": "Discussion", "text": "By using CIFT on a high-performing model, we can improve performance in terms of latent ability without decreasing accuracy due to overfitting. In different subsets of test data, different fine-tuning methods lead to better performance.\nOur results show performance improvements with respect to ability as estimated using IRT. IRT ability measures how well an individual (or in our case, an ML model) performs with respect to some human population. While IRT performance is positively correlated with accuracy, it is not always true that higher accuracy implies a higher theta score. Items in an IRT test set have varying difficulty and discriminatory parameters which affect an ability estimate. Which items are answered correctly is more important than how many.\nTraining a model to directly maximize ability is a difficult proposition due to the small size of IRT data. However we have shown that using CIFT with IRT data on a pre-trained DNN model can significantly improve performance in terms of ability. At the same time, using this fine-tuning approach does not have a significant negative effect on generalization, and only small decreases in test set accuracy. In fact in one case test-time accuracy is improved after running CIFT and a subsequent full training pass, indicating that CIFT can positively affect accuracy as well as ability.\nMost surprising is the fact that CIFT with MSE shows significant IRT performance improvement for a model that was already above the average human performance. Our original assumption was that there was a risk that using MSE might pull the model\u2019s performance towards the average human, meaning theta score would decrease. However we found that theta percentile actually improved in the G5-Contradiction case. One explanation is that the local minimum that the NSE model settled on in the original training was such that moving towards the average via CIFT with MSE actually leads to a better position in terms of model weights.\nUsing IRT as an evaluation metric allows us to directly compare ML performance to a human population. This is an important prerequisite to being able to compare human and machine performance directly and build machines that learn in a similar fashion to human learning (Lake et al. 2016). There have been ML systems that have surpassed human performance (Campbell, Hoane, and Hsu 2002; Ferrucci et al. 2010; Stadie, Levine, and Abbeel 2015;\nSilver et al. 2016), but for most tasks and with regards to general intelligence ML systems are still not performing as well as humans."}, {"heading": "Conclusion and Future Work", "text": "In this paper we have introduced CIFT, a novel fine-tuning approach to training that can improve performance for a state of the art DNN by leveraging item parameters as modeled from a large collection of human response data. CIFT improves performance in terms of IRT ability scores, without negatively impacting generalization due to overfitting.\nBy introducing specialized supplemental data the model is able to update its representations to boost performance in model ability. CIFT outperforms a standard transfer learning baseline in terms of ability when training on items that are more difficult than the test set. When testing on more difficult items, it is not helpful to fine-tune with easier items as identified by IRT. Interestingly, our results show that by introducing a small set of examples from a separate dataset the model can improve performance in certain situations while learning from a large supplemental training set has a negative effect on performance. However, introducing examples from a new data set does improve performance.\nThere are limitations to this work. IRT models require human expertise in deciding which items to retain and remove in the model fitting process. However IRT is a well studied area of psychometrics and open-source tools exist to fit IRT models according to standard parameters (Chalmers et al. 2015). IRT test sets are small because a large number of human annotators are necessary to sufficiently model the item and ability parameters. Crowdsourcing platforms make recruiting annotators relatively inexpensive compared to expert annotators, but asking crowd workers to label a large set of examples can lead to deteriorating performance as the time required increases. In addition, our fine-tuning performance relies on the initial pre-trained model, and therefore performance improvements are tied to the representations learned by the original model.\nFuture work includes investigation into datasets that can be used with CIFT and why certain fine-tuning sets lead to better performance in certain scenarios. Experiments with different loss functions and different data can help to understand how CIFT affects the representations learned by a model. With regards to the IRT items themselves, if a larger set of IRT items was fit to model ability, these items could incorporate a wider variety that could further improve performance as a fine-tuning set. The improvements using MSE suggest that future work training DNNs to learn a distribution over labels can lead to further performance improvements. Being able to directly optimize a model for ability should further improve DNN performance and improve generalizing capacity of DNNs. This requires a large amount of labeled data from a large human population in order to fit the IRT models, which is difficult. Future work to estimate item parameters from small sets of human response patterns can reduce the cost involved in generating large-scale IRT sets."}], "references": [{"title": "How to grade a test without knowing the answers \u2014 a bayesian graphical model for adaptive crowdsourcing and aptitude testing", "author": ["Bachrach"], "venue": "Proceedings of the 29th International Con-", "citeRegEx": "Bachrach,? \\Q2012\\E", "shortCiteRegEx": "Bachrach", "year": 2012}, {"title": "Item Response Theory: Parameter Estimation Techniques, Second Edition", "author": ["Baker", "F.B. Kim 2004] Baker", "Kim", "S.-H"], "venue": null, "citeRegEx": "Baker et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Baker et al\\.", "year": 2004}, {"title": "Curriculum learning", "author": ["Bengio"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Bengio,? \\Q2009\\E", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Deep learners benefit more from out-of", "author": ["Bengio"], "venue": null, "citeRegEx": "Bengio,? \\Q2011\\E", "shortCiteRegEx": "Bengio", "year": 2011}, {"title": "Marginal maximum likelihood estimation of item parameters: Application of an em algorithm", "author": ["Bock", "R.D. Aitkin 1981] Bock", "M. Aitkin"], "venue": null, "citeRegEx": "Bock et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Bock et al\\.", "year": 1981}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Bowman"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Bowman,? \\Q2015\\E", "shortCiteRegEx": "Bowman", "year": 2015}, {"title": "Recognizing subjectivity: A case study in manual tagging", "author": ["Bruce", "R.F. Wiebe 1999] Bruce", "J.M. Wiebe"], "venue": null, "citeRegEx": "Bruce et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Bruce et al\\.", "year": 1999}, {"title": "The PASCAL Recognising Textual Entailment Challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment", "author": ["Glickman Dagan", "I. Magnini 2006] Dagan", "O. Glickman", "B. Magnini"], "venue": null, "citeRegEx": "Dagan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Maximum likelihood estimation of observer errorrates using the em algorithm", "author": ["Dawid", "A.P. Skene 1979] Dawid", "A.M. Skene"], "venue": "Journal of the Royal Statistical Society. Series C (Applied", "citeRegEx": "Dawid et al\\.,? \\Q1979\\E", "shortCiteRegEx": "Dawid et al\\.", "year": 1979}, {"title": "Building watson: An overview of the deepqa project. AI magazine 31(3):59\u201379", "author": ["Ferrucci"], "venue": null, "citeRegEx": "Ferrucci,? \\Q2010\\E", "shortCiteRegEx": "Ferrucci", "year": 2010}, {"title": "A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting", "author": ["Freund", "Y. Schapire 1997] Freund", "R.E. Schapire"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "Freund et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "Learning whom to trust with mace", "author": ["Hovy"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Hovy,? \\Q2013\\E", "shortCiteRegEx": "Hovy", "year": 2013}, {"title": "Identifying and accounting for taskdependent bias in crowdsourcing", "author": ["Kapoor Kamar", "E. Horvitz 2015] Kamar", "A. Kapoor", "E. Horvitz"], "venue": "In Third AAAI Conference on Human Computation and Crowdsourcing", "citeRegEx": "Kamar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kamar et al\\.", "year": 2015}, {"title": "Bayesian classifier combination", "author": ["Kim", "Ghahramani 2012] Kim", "H.-C", "Z. Ghahramani"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2012}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["Koller", "D. Friedman 2009] Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2009}, {"title": "Building machines that learn and think like people", "author": ["Lake"], "venue": "Behavioral and Brain Sciences", "citeRegEx": "Lake,? \\Q2016\\E", "shortCiteRegEx": "Lake", "year": 2016}, {"title": "Building an evaluation scale using item response theory", "author": ["Wu Lalor", "J.P. Yu 2016] Lalor", "H. Wu", "H. Yu"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lalor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lalor et al\\.", "year": 2016}, {"title": "A sick cure for the evaluation of compositional distributional semantic models", "author": ["Marelli"], "venue": null, "citeRegEx": "Marelli,? \\Q2014\\E", "shortCiteRegEx": "Marelli", "year": 2014}, {"title": "Making sense of item response theory in machine learning", "author": ["Mart\u0131\u0301nez-Plumed"], "venue": "In ECAI,", "citeRegEx": "Mart\u0131\u0301nez.Plumed,? \\Q2016\\E", "shortCiteRegEx": "Mart\u0131\u0301nez.Plumed", "year": 2016}, {"title": "Neural semantic encoders", "author": ["Munkhdalai", "T. Yu 2017] Munkhdalai", "H. Yu"], "venue": "In Proceedings of the 15th Conference of the European Chapter of the Association", "citeRegEx": "Munkhdalai et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Munkhdalai et al\\.", "year": 2017}, {"title": "Probabilistic modeling for crowdsourcing partially-subjective ratings", "author": ["Nguyen"], "venue": "In Proceedings of The Conference on Human Computation and Crowdsourcing (HCOMP),", "citeRegEx": "Nguyen,? \\Q2016\\E", "shortCiteRegEx": "Nguyen", "year": 2016}, {"title": "The benefits of a model of annotation", "author": ["Passonneau", "R.J. Carpenter 2014] Passonneau", "B. Carpenter"], "venue": null, "citeRegEx": "Passonneau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Passonneau et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Socher Pennington", "J. Manning 2014] Pennington", "R. Socher", "C.D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Improved Boosting Algorithms Using Confidencerated Predictions", "author": ["Schapire", "R.E. Singer 1999] Schapire", "Y. Singer"], "venue": "Machine Learning", "citeRegEx": "Schapire et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Schapire et al\\.", "year": 1999}, {"title": "Boosting neural networks. Neural Computation 12(8):1869\u20131887", "author": ["Schwenk", "H. Bengio 2000] Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "Schwenk et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Schwenk et al\\.", "year": 2000}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. Hassabis"], "venue": "Nature", "citeRegEx": "Hassabis,? \\Q2016\\E", "shortCiteRegEx": "Hassabis", "year": 2016}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["Levine Stadie", "B.C. Abbeel 2015] Stadie", "S. Levine", "P. Abbeel"], "venue": "CoRR abs/1507.00814", "citeRegEx": "Stadie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2015}, {"title": "How transferable are features in deep neural networks", "author": ["Yosinski"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Yosinski,? \\Q2014\\E", "shortCiteRegEx": "Yosinski", "year": 2014}], "referenceMentions": [], "year": 2017, "abstractText": "Item Response Theory (IRT) allows for measuring ability of Machine Learning models as compared to a human population. However, it is difficult to create a large dataset to train the ability of deep neural network models (DNNs). We propose Crowd-Informed Fine-Tuning (CIFT) as a new training process, where a pre-trained model is fine-tuned with a specialized supplemental training set obtained via IRT modelfitting on a large set of crowdsourced response patterns. With CIFT we can leverage the specialized set of data obtained through IRT to inform parameter tuning in DNNs. We experiment with two loss functions in CIFT to represent (i) memorization of fine-tuning items and (ii) learning a probability distribution over potential labels that is similar to the crowdsourced distribution over labels to simulate crowd knowledge. Our results show that CIFT improves ability for a state-of-theart DNN model for Recognizing Textual Entailment (RTE) tasks and is generalizable to a large-scale RTE test set.", "creator": "LaTeX with hyperref package"}}}