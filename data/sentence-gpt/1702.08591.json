{"id": "1702.08591", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "The Shattered Gradients Problem: If resnets are the answer, then what is the question?", "abstract": "A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. The problem has largely been overcome through the introduction of carefully constructed initializations and batch normalization. Nevertheless, architectures incorporating skip-connections such as resnets perform much better than standard feedforward architectures despite well-chosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise. In contrast, the gradients in architectures with skip-connections are far more resistant to shattering decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new \"looks linear\" (LL) initialization that prevents shattering. Preliminary experiments show the new initialization allows to train very deep networks without the addition of skip-connections. The main goal of this new LLL is to improve the performance of the existing LLL in non-linear networks. We show how we can train LLLs with no extra steps to get LLLs out of the way.\n\n\n\n\n\n\n\nThe model for LLLs is derived from the fact that all the gradients with skip-connections do not exist. We conclude that the first step in our study of the non-linear LLL is to identify an initialization that produces \"zero\" gradients with a number of parameters. In fact, the model for LLLs only works with an additional parameter: the \"zero\" gradient, the \"zero\" gradient, the \"zero\" gradient, the \"zero\" gradient, the \"zero\" gradient, the \"zero\" gradient, and the \"zero\" gradient.\nWe demonstrate that the first step in our LLL process is a one-off \"low-layer\" gradient that can be trained to train the LLLs by training a new LLL with a number of parameters, without the addition of a drop-layer gradient. The gradient also works in the context of a LLL in general: if the gradient is low-layer, it can be trained to train a new LLL with a number of parameters in one-off \"low-layer\" gradient. The gradient also is not a \"low-layer\" gradient, although that is usually the case for high-layer LLLs.\nIn our research, the LLLs are", "histories": [["v1", "Tue, 28 Feb 2017 01:06:13 GMT  (1304kb,D)", "http://arxiv.org/abs/1702.08591v1", "14 pages, 6 figures"]], "COMMENTS": "14 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG stat.ML", "authors": ["david balduzzi", "marcus frean", "lennox leary", "j p lewis", "kurt wan-duo ma", "brian mcwilliams"], "accepted": true, "id": "1702.08591"}, "pdf": {"name": "1702.08591.pdf", "metadata": {"source": "CRF", "title": "The Shattered Gradients Problem: If resnets are the answer, then what is the question?", "authors": ["David Balduzzi", "Marcus Frean", "Lennox Leary", "JP Lewis", "Kurt Wan-Duo", "Brian McWilliams"], "emails": ["<dbalduzzi@gmail.com>,", "<brian@disneyresearch.com>."], "sections": [{"heading": null, "text": "In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise. In contrast, the gradients in architectures with skipconnections are far more resistant to shattering decaying sublinearly.\nDetailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new \u201clooks linear\u201d (LL) initialization that prevents shattering. Preliminary experiments show the new initialization allows to train very deep networks without the addition of skip-connections."}, {"heading": "1. Introduction", "text": "Deep neural networks have achieved outstanding performance (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2016b). Reducing the tendency of gradients to vanish or explode with depth (Hochreiter, 1991; Bengio et al., 1994) has been essential to this progress.\nCombining careful initialization (Glorot & Bengio, 2010; He et al., 2015) with batch normalization (Ioffe & Szegedy,\n*Authors listed alphabetically 1Victoria University of Wellington, New Zealand 2Disney Research, Zu\u0308rich, Switzerland. Correspondence to: David Balduzzi <dbalduzzi@gmail.com>, Brian McWilliams <brian@disneyresearch.com>.\n2015) bakes two solutions to the vanishing/exploding gradient problem into a single architecture. The He initialization ensures variance is preserved across rectifier layers, and batch normalization ensures that backpropagation through layers is unaffected by the scale of the weights (Ioffe & Szegedy, 2015).\nIt is perhaps surprising then that residual networks (resnets) still perform so much better than standard architectures when networks are sufficiently deep (He et al., 2016a;b). This raises the question: If resnets are the solution, then what is the problem? We identify the shattered gradient problem: a previously unnoticed difficulty with gradients in deep rectifier networks that is orthogonal to vanishing and exploding gradients. The shattering gradients problem is that, as depth increases, gradients in standard feedforward networks increasingly resemble white noise. Resnets dramatically reduce the tendency of gradients to shatter.\nTerminology. We refer to networks without skip connections as feedforward nets\u2014in contrast to residual nets (resnets) and highway nets. We distinguish between the real-valued output of a rectifier and its binary activation: the activation is 1 if the output is positive and 0 otherwise."}, {"heading": "1.1. The Shattered Gradients Problem", "text": "The first step is to simply look at the gradients of neural networks. Gradients are averaged over minibatches, depend on both the loss and the random sample from the data, and are extremely high-dimensional, which introduces multiple confounding factors and makes visualization difficult (but see section 4). We therefore construct a minimal model designed to eliminate these confounding factors. The minimal model is a neural network fW : R \u2192 R taking scalars to scalars; each hidden layer contains N = 200 rectifier neurons. The model is not intended to be applied to real data. Rather, it is a laboratory where gradients can be isolated and investigated.\nWe are interested in how the gradient varies, at initializa-\nar X\niv :1\n70 2.\n08 59\n1v 1\n[ cs\n.N E\n] 2\n8 Fe\nb 20\n17\ntion, as a function of the input:\ndfW dx\n(x(i)) where x(i) \u2208 [\u22122, 2] is in a (1) 1-dim grid of M = 256 \u201cdata points\u201d.\nUpdates during training depend on derivatives with respect to weights, not inputs. Our results are relevant because, by the chain rule, \u2202fW\u2202wij = \u2202fW \u2202nj \u2202nj \u2202wij . Weight updates thus depend on \u2202fW\u2202nj \u2014i.e. how the output of the network varies with the output of neurons in one layer (which are just inputs to the next layer).\nThe top row of figure 1 plots dfWdx (x (i)) for each point x(i) in the 1-dim grid. The bottom row shows the (absolute value) of the covariance matrix: |(g \u2212 g\u0304)(g \u2212 g\u0304)>|/\u03c32g where g is the 256-vector of gradients, g\u0304 the mean, and \u03c32g the variance.\nIf all the neurons were linear then the gradient would be a horizontal line (i.e. the gradient would be constant as a function of x). Rectifiers are not smooth, so the gradients are discontinuous.\nGradients of shallow networks resemble brown noise. Suppose the network has a single hidden layer: fw,b(x) = w>\u03c1(x \u00b7 v \u2212 b). Following Glorot & Bengio (2010), weights w and biases b are sampled from N (0, \u03c32) with \u03c32 = 1N . Set v = (1, . . . , 1).\nFigure 1a shows the gradient of the network for inputs x \u2208 [\u22122, 2] and its covariance matrix. Figure 1d shows a discrete approximation to brownian motion: BN (t) =\u2211t s=1Ws where Ws \u223c N (0, 1N ). The plots are strikingly similar: both clearly exhibit spatial covariance structure. The resemblance is not coincidental: section A1 applies\nDonsker\u2019s theorem to show the gradient converges to brownian motion as N \u2192\u221e.\nGradients of deep networks resemble white noise. Figure 1b shows the gradient of a 24-layer fully-connected rectifier network. Figure 1e shows white noise given by samples Wk \u223c N (0, 1). Again, the plots are strikingly similar. Since the inputs lie on a 1-dim grid, it makes sense to compute the autocorrelation function (ACF) of the gradient. Figures 2a and 2d compare this function for feedforward networks of different depth with white and brown noise. The ACF for shallow networks resembles the ACF of brown noise. As the network gets deeper, the ACF quickly comes to resemble that of white noise.\nTheorem 1 explains this phenomenon. We show that correlations between gradients decrease exponentially 1\n2L with\ndepth in feedforward rectifier networks.\nTraining is difficult when gradients behave like white noise. The shattered gradient problem is that the spatial structure of gradients is progressively obliterated as neural nets deepen. The problem is clearly visible when inputs are taken from a one-dimensional grid, but is difficult to observe when inputs are randomly sampled from a highdimensional dataset.\nShattered gradients undermine the effectiveness of algorithms that assume gradients at nearby points are similar such as momentum-based and accelerated methods (Sutskever et al., 2013; Balduzzi et al., 2016). If dfWdnj behaves like white noise, then a neuron\u2019s effect on the output of the network (whether increasing weights causes the network to output more or less) becomes extremely unstable\nmaking learning difficult.\nGradients of deep resnets lie in between brown and white noise. Introducing skip-connections allows much deeper networks to be trained (Srivastava et al., 2015; He et al., 2016b;a; Greff et al., 2017). Skip-connections significantly change the correlation structure of gradients. Figure 1c shows the concrete example of a 50-layer resnet which has markedly more structure than the equivalent feedforward net (figure 1b). Figure 2b shows the ACF of resnets of different depths. Although the gradients become progressively less structured, they do not whiten to the extent of the gradients in standard feedforward networks\u2014 there are still correlations in the 50-layer resnet whereas in the equivalent feedforward net, the gradients are indistinguishable from white noise. Figure 2c shows the dramatic effect of recently proposed \u03b2-rescaling (Szegedy et al., 2016): the ACF of even the 50 layer network resemble brown-noise.\nTheorem 3 shows that correlations between gradients decay sublinearly with depth 1\u221a\nL for resnets with batch normal-\nization. We also show, corollary 1, that modified highway networks (where the gates are scalars) can achieve a depth independent correlation structure on gradients. The analysis explains why skip-connections, combined with suitable rescaling, preserve the structure of gradients."}, {"heading": "1.2. Outline", "text": "Section 2 shows that batch normalization increases neural efficiency. We explore how batch normalization behaves differently in feedforward and resnets, and draw out facts that are relevant to the main results.\nThe main results are in section 3. They explain why gradients shatter and how skip-connections reduce shattering. The proofs are for a mathematically amenable model: fully-connected rectifier networks with the same number of hidden neurons in each layer. Section 4 presents empirical results which show gradients similarly shatter in convnets for real data. It also shows that shattering causes average\ngradients over minibatches to decrease with depth (relative to the average variance of gradients).\nFinally, section 5 proposes the LL-init (\u201clooks linear initialization\u201d) which eliminates shattering. Preliminary experiments show the LL-init allows training of extremely deep networks (\u223c200 layers) without skip-connections."}, {"heading": "1.3. Related work", "text": "Carefully initializing neural networks has led to a series of performance breakthroughs dating back (at least) to the unsupervised pretraining in Hinton et al. (2006); Bengio et al. (2006). The insight of Glorot & Bengio (2010) is that controlling the variance of the distributions from which weights are sampled allows to control how layers progressively amplify or dampen the variance of activations and error signals. More recently, He et al. (2015) refined the approach to take rectifiers into account. Rectifiers effectively halve the variance since, at initialization and on average, they are active for half their inputs. Orthogonalizing weight matrices can yield further improvements albeit at a computational cost (Saxe et al., 2014; Mishkin & Matas, 2016). The observation that the norms of weights form a random walk was used by Sussillo & Abbott (2015) to tune the gains of neurons.\nIn short, it has proven useful to treat weights and gradients as random variables, and carefully examine their effect on the variance of the signals propagated through the network. This paper presents a more detailed analysis that considers correlations between gradients at different datapoints.\nThe closest work to ours is Veit et al. (2016), which shows resnets behave like ensembles of shallow networks. We provide a more detailed analysis of the effect of skipconnections on gradients. A recent paper showed resnets have universal finite-sample expressivity and may lack spurious local optima (Hardt & Ma, 2017) but this does not explain why deep feedforward networks are harder to train than resnets. An interesting hypothesis is that skipconnections improve performance by breaking symmetries (Orhan, 2017).\nThe Shattered Gradients Problem"}, {"heading": "2. Observations on batch normalization", "text": "Batch normalization was introduced to reduce covariate shift (Ioffe & Szegedy, 2015). However, it has other effects that are less well-known. We investigate the impact of batch normalization at initialization (i.e. when it meancenters and rescales to unit variance).\nWe first investigate batch normalization\u2019s effect on neural activations. Neurons are active for half their inputs on average, figure 3, with or without batch normalization. Figure 3 also shows how often neurons are co-active for two inputs. With batch normalization, neurons are co-active for 1 4 of distinct pairs of inputs, which is what would happen if activations were decided by unbiased coin flips. Without batch normalization, the co-active proportion climbs with depth, suggesting neuronal responses are increasingly redundant. Resnets with batch normalization behave the same as feeforward nets (not shown).\nFigure 4 takes a closer look. It turns out that computing the proportion of inputs causing neurons to be active on average is misleading. The distribution becomes increasingly bimodal with depth. In particular, neurons are either always active or always inactive for layer 50 in the feedforward net without batch normalization (blue histogram in figure 4a). Batch normalization causes most neurons to be active for half the inputs, blue histograms in figures 4b,c.\nNeurons that are always active may as well be linear. Neurons that are always inactive may as well not exist. It follows that batch normalization increases the efficiency with which rectifier nonlinearities are utilized.\nThe increased efficiency comes at a price. The raster plot\nfor feedforward networks resembles static television noise: the spatial structure is obliterated. Resnets (Figure 4c) exhibit a compromise where neurons are utilized efficiently but the spatial structure is also somewhat preserved. The preservation of spatial structure is quantified via the contiguity histograms which counts long runs of consistent activation. Resnets maintain a broad distribution of contiguity even with deep networks whereas batch normalization on feedforward nets shatters these into small sections."}, {"heading": "3. Analysis", "text": "This section analyzes the correlation structure of gradients in neural nets. The main ideas and results are presented, with the details provided in section A3.\nPerhaps the simplest way to probe the structure of a random process is to measure the first few moments: the mean, variance and covariance. We investigate how the correlation between typical datapoints (defined below) changes with network structure and depth. Weaker correlations correspond to whiter gradients.\nThe analysis is for fully-connected networks. Extending to convnets involves (significant) additional bookkeeping.\nProof strategy. The covariance defines an inner product on the vector space of real-valued random variables with mean zero and finite second moment. It was shown in Balduzzi et al. (2015); Balduzzi (2016) that the gradients in neural nets are sums of path-weights over active paths. The first step in the proof is to observe that path-weights are orthogonal with respect to the variance inner product. To express gradients as linear combinations of path-weights is thus to express them over an orthogonal basis.\nWorking in the path-weight basis reduces computing the covariance between gradients at different datapoints to counting the number of paths through the network that are co-active. The second step of the proof is to count co-active paths and adjust for rescaling factors (for example due to batch normalization).\nThe following assumption simplifies the analysis:\nAssumption 1 (typical datapoints). We say x(i) and x(j) are typical datapoints if half of neurons per layer are active for each and a quarter per layer are co-active for both. We assume all pairs of datapoints are typical.\nFigure 3 shows the assumption holds under batch normalization for both activations and coactivations. The initialization in He et al. (2015) assumes datapoints activate half the neurons per layer. The assumption on co-activations is implied by (and so weaker than) the assumption in Choromanska et al. (2015) that activations are Bernoulli random variables independent of the inputs.\nCorrelations between gradients. Weight updates in a neural network are proportional to\n\u2206wjk \u221d #mb\u2211\ni=1\nP\u2211\np=1\n\u2202`\n\u2202fp \u2202fp \u2202nk \u2202nk \u2202wjk\n( x(i) ) .\nThe derivatives \u2202`\u2202fp and \u2202nk \u2202wjk do not depend on the network\u2019s internal structure. We are interested in the middle term \u2202fp\u2202nk , which does. Without loss of generality, pick a neuron n separated from the output by L layers.\nIt is mathematically convenient to work with the sum\u2211P p=1 fp over output coordinates of the network. We do so to highlight the contribution of the network\u2019s internal structure to the gradients. Section 4 shows that our results hold for convnets on real-data with the cross-entropy loss. See also remark A2. Definition 1. Let\u2207i := \u2211P p=1 \u2202fp \u2202n (x\n(i)) be the derivative with respect to neuron n given input x(i) \u2208 D. For each input x(i), the derivative \u2207i is a real-valued random variable. It has mean zero since weights are sampled from distributions with mean zero. Let\nC(i, j) = E[\u2207i\u2207j ] and R(i, j) = E[\u2207i\u2207j ]\u221a E[\u22072i ] \u00b7 E[\u22072j ]\ndenote the covariance and correlation of gradients."}, {"heading": "3.1. Feedforward networks", "text": "The first major result is Theorem 1 (covariance of gradients in feedforward nets). Suppose weights are initialized with variance \u03c32 = 2N following He et al. (2015). Then\na) The variance of the gradient at x(i) is C fnn(i) = 1.\nb) The covariance is C fnn(i, j) = 1 2L .\nPart (a) recovers the observation in He et al. (2015) that setting \u03c32 = 2N preserves the variance across layers in rectifier networks. Part (b) is new. It explains the empirical observation, figure 2a, that gradients in feedforward nets whiten with depth. Intuitively, gradients whiten because the number of paths through the network grows exponentially faster with depth than the fraction of co-active paths, see section A3 for details."}, {"heading": "3.2. Residual networks", "text": "The residual modules introduced in He et al. (2016a) are\nxl = xl\u22121 + W l\u03c1BN ( Vl\u03c1BN (xl\u22121) )\nwhere \u03c1BN (a) = \u03c1(BN(a)). We analyse the strippeddown variant\nxl = \u03b1 \u00b7 ( xl\u22121 + \u03b2 \u00b7Wl\u03c1BN (xl\u22121) ) (2)\nwhere \u03b1 and \u03b2 are rescaling factors. Dropping Vl\u03c1BN makes no essential difference to the analysis. The \u03b2rescaling was introduced in Szegedy et al. (2016) where it was observed setting \u03b2 \u2208 [0.1, 0.3] reduces instability. We include \u03b1 for reasons of symmetry.\nTheorem 2 (covariance of gradients in resnets). Consider a resnet with batch normalization disabled and \u03b1 = \u03b2 = 1. Suppose \u03c32 = 2N as above. Then\na) The variance of the gradient at x(i) is Cres(i) = 2L.\nThe Shattered Gradients Problem\nb) The covariance is Cres(i, j) = ( 3 2 )L .\nThe correlation isRres(i, j) = ( 3 4 )L .\nThe theorem implies there are two problems in resnets without batch normalization: (i) the variance of gradients grows and (ii) their correlation decays exponentially with depth. Both problems are visible empirically."}, {"heading": "3.3. Rescaling in Resnets", "text": "A solution to the exploding variance of resnets is to rescale layers by \u03b1 = 1\u221a\n2 which yields\nCres \u03b1= \u221a 2 (i) = 1 and Rres \u03b1= \u221a 2 (i, j) =\n( 3\n4\n)L\nand so controls the variance but the correlation between gradients still decays exponentially with depth. Both theoretical predictions hold empirically.\nIn practice, \u03b1-rescaling is not used. Instead, activations are rescaled by batch normalization (Ioffe & Szegedy, 2015) and, more recently, setting \u03b2 \u2208 [0.1, 0.3] per Szegedy et al. (2016). The effect is dramatic:\nTheorem 3 (covariance of gradients in resnets with BN and rescaling). Under the assumptions above, for resnets with batch normalization and \u03b2-rescaling,\na) the variance is Cres\u03b2,BN(i) = \u03b22(L\u2212 1) + 1;\nb) the covariance1 is Cres\u03b2,BN(i, j) \u223c \u03b2 \u221a L; and\nthe correlation isRres\u03b2,BN(i, j) \u223c 1\u03b2\u221aL .\nThe theorem explains the empirical observation, figure 2a, that gradients in resnets whiten much more slowly with depth than feedforward nets. It also explains why setting \u03b2 near zero further reduces whitening.\nBatch normalization changes the decay of the correlations from 1\n2L to 1\u221a L . Intuitively, the reason is that the variance\nof the outputs of layers grows linearly, so batch normalization rescales them by different amounts. Rescaling by \u03b2 introduces a constant factor. Concretely, the model predicts using batch normalization with \u03b2 = 0.1 on a 100- layer resnet gives typical correlation Rres0.1,BN(i, j) = 0.7. Setting \u03b2 = 1.0 gives Rres1.0,BN(i, j) = 0.1. By contrast, a 100-layer feedforward net has correlation indistinguishable from zero.\n1See section A3.4 for exact computations."}, {"heading": "3.4. Highway networks", "text": "The standard highway network (Srivastava et al., 2015; Greff et al., 2017) has layers of the form\nxl = ( 1\u2212 T (xl\u22121) ) \u00b7 xl\u22121 + T (xl\u22121) \u00b7H(xl\u22121)\nConsider the following modification where \u03b31 and \u03b32 are scalars satisfying \u03b321 + \u03b3 2 2 = 1:\nxl = \u03b31 \u00b7 xl\u22121 + \u03b32 \u00b7Wl\u03c1(xl\u22121)\nThe module can be recovered by judiciously choosing \u03b1 and \u03b2 in equation (2). However, it is worth studying in its own right:\nCorollary 1 (covariance of gradients in highway networks). Under the assumptions above, for modified highway networks with \u03b3-rescaling,\na) the variance of gradients is CHN\u03b3 (i) = 1; and b) the correlation isRHN\u03b3 (i, j) = ( \u03b321 + 1 2\u03b3 2 2 )L .\nIn particular, if \u03b31 = \u221a 1\u2212 1L and \u03b32 = \u221a 1 L then the correlation between gradients does not decay with depth\nlim L\u2192\u221e RHN\u03b3 (i, j) = 1\u221a e .\nThe tradeoff is that the contributions of the layers becomes increasingly trivial (i.e. close to the identity) as L\u2192\u221e."}, {"heading": "4. Gradients shatter in convnets", "text": "In this section we provide empirical evidence that the main results also hold for deep convnets using the CIFAR-10 dataset. We instantiate feedforward and resnets with 2, 4, 10, 24 and 50 layers of equivalent size. Using a slight modification of the \u201cbottleneck\u201d architecture in He et al. (2016a), we introduce one skip-connection for every two convolutional layers and both network architectures use batch normalization.\nFigures 5a and b compare the covariance of gradients in the first layer of feedforward and resnets (\u03b2 = 0.1) with a minibatch of 256 random samples from CIFAR-10 for networks of depth 2 and 50. To highlight the spatial structure of the gradients, the indices of the minibatches were reordered according to a k-means clustering (k = 10) applied to the gradients of the two-layer networks. The same permutation is used for all networks within a row. The spatial structure is visible in both two-layer networks, although it is more apparent in the resnet. In the feedforward network the structure quickly disappears with depth. In the resnet, the structure remains apparent at 50 layers.\nTo quantify this effect we consider the \u201cwhiteness\u201d of the gradient using relative effective rank. Let \u2206 be the matrix whose columns are the gradients with respect to the input, for each datapoint x(i) in a minibatch. The effective rank is r(\u2206) = tr(\u2206>\u2206)/\u2016\u2206\u201622 and measures the intrinsic dimension of a matrix (Vershynin, 2012). It is bounded above by the rank of \u2206\u2014a matrix with highly correlated columns and therefore more structure will have a lower effective rank. We are interested in the effective rank of the covariance matrix of the gradients relative to a \u201cwhite\u201d matrix Y of the same dimensions with i.i.d. Gaussian entries. The relative effective rank r(\u2206)/r(Y) measures the similarity between the second moments of \u2206 and Y.\nFigure 5c shows that the relative effective rank (averaged over 30 minibatches) grows much faster as a function of depth for networks without skip-connections. For resnets, the parameter \u03b2 slows down the rate of growth of the effective rank as predicted by theorem 3.\nFigure 5d shows the average `2-norm of the gradient in each coordinate (normalized by the standard deviation computed per minibatch). We observe that this quantity decays much more rapidly as a function of depth for feedforward networks. This is due to the effect of averaging increasingly whitening gradients within each minibatch. In other words, the noise within minibatches overwhelms the signal. The phenomenon is much less pronounced in resnets.\nTaken together these results confirm the results in section 3 for networks with convolutional layers and show that the gradients in resnets are indeed more structured than those in feedforward nets and therefore do not vanish when averaged within a minibatch. This phenomena allows for the training of very deep resnets."}, {"heading": "5. The \u201clooks linear\u201d initialization", "text": "Shattering gradients are not a problem for linear networks, see remark after equation (1). Unfortunately, linear networks are not useful since they lack expressivity.\nThe LL-init combines the best of linear and rectifier nets by initializing rectifiers to look linear. Several implementations are possible. We use concatenated rectifiers or CReLUs (Shang et al., 2016):\nx 7\u2192 ( \u03c1(x) \u03c1(\u2212x) )\nThe key observation is that initializing weights with a mirrored block structure yields linear outputs\n( W \u2212W ) \u00b7 ( \u03c1(x) \u03c1(\u2212x) ) = W\u03c1(x)\u2212W\u03c1(\u2212x) = Wx.\nThe output will cease to be linear as soon as weight updates cause the two blocks to diverge.\nAn alternative architecture is based on the PReLU introduced in He et al. (2015):\nPReLU: \u03c1p(x) = { x if x > 0 ax else.\nSetting a = 1 at initialization obtains a different kind of LL-init. Preliminary experiments, not shown, suggest that the LL-init is more effective on the CReLU-based architecture than PReLU. The reason is unclear.\nOrthogonal convolutions. A detailed analysis of learning in linear neural networks by Saxe et al. (2014) showed, theoretically and experimentally, that arbitrarily deep linear networks can be trained when initialized with orthogonal weights. Motivated by these results, we use the LL-init in conjunction with orthogonal weights.\nThe Shattered Gradients Problem\nWe briefly describe how we orthogonally initialize a kernel K of size A \u00d7 B \u00d7 3 \u00d7 3 where A \u2265 B. First, set all the entries of K to zero. Second, sample a random matrix W of size (A \u00d7 B) with orthonormal columns. Finally, set K[:, :, 2, 2] := W. The kernel is used in conjunction with strides of one and zero-padding."}, {"heading": "5.1. Experiments", "text": "We investigated the empirical performance of the LL-init on very deep networks. Performance was evaluated on CIFAR-10. We conducted a set of proof-of-concept experiments. The aim was not to match the state-of-the-art, but rather to investigate whether the LL-init allows training of deeper networks than standard initializations.\nWe compared a CReLU architecture with an orthogonal LL-init against an equivalent CReLU network, resnet, and a standard feedforward ReLU network. The other networks were initialized according to He et al. (2015). The architectures are thin with the number of filters per layer in the ReLU networks ranging from 8 at the input layer to 64, see section A4. Doubling with each spatial extent reduction. The thinness of the architecture makes it particularly difficult for gradients to propagate at high depth. The reduction is performed by convolutional layers with strides of 2, and following the last reduction the representation is passed to a fully connected layer with 10 neurons for classification. The numbers of filters per layer of the CReLU models were adjusted by a factor of 1/ \u221a 2 to achieve parameter parity with the ReLU models. The Resnet version of the model is the same as the basic ReLU model with skip-connections after every two modules following He et al. (2016a).\nUpdates were performed with Adam (Kingma & Ba, 2015).\nTraining schedules were automatically determined by an auto-scheduler that measures how quickly the loss on the training set has been decreasing over the last ten epochs, and drops the learning rate if a threshold remains crossed for five measurements in a row. Standard data augmentation was performed; translating up to 4 pixels in any direction and flipping horizontally with p = 0.5.\nResults are shown in figure 6, each point being the mean performance of 10 trained models. The ReLU and CReLU nets performed steadily worse with depth; the ReLU net performing worse than the linear baseline of 40% at the maximum depth of 198. The feedforward net with LL-init performs comparably to a resnet, suggesting that shattered gradients are a large part of the problem in training very deep architectures."}, {"heading": "6. Conclusion", "text": "The representational power of rectifier networks depends on the number of linear regions into which it splits the input space. It was shown in Montufar et al. (2014) that the number of linear regions can grow exponentially with depth (but only polynomially with width). Hence deep neural networks are capable of far richer mappings than shallow ones (Telgarsky, 2016). An underappreciated consequence of the exponential growth in linear regions is the proliferation of discontinuities in the gradients of rectifier nets.\nThis paper has identified and analyzed a previously unnoticed problem with gradients in deep networks: in a randomly initialized network, the gradients of deeper layers are increasingly uncorrelated. Shattered gradients play havoc with the optimization methods currently in use2 and may explain the difficulty in training deep feedforward networks even when effective initialization and batch normalization are employed. Averaging gradients over minibatches becomes analogous to integrating over white noise \u2013 there is no clear trend that can be summarized in a single average direction. Shattered gradients can also introduce numerical instabilities, since small differences in the input can lead to large differences in gradients.\nSkip-connections in combination with suitable rescaling reduce shattering. Specifically, we show that the rate at which correlations between gradients decays changes from exponential for feedforward architectures to sublinear for resnets. The analysis uncovers a surprising and (to us at least) unexpected side-effect of batch normalization. An alternate solution to the shattering gradient problem is to design initializations that do not shatter such as the LLinit. An interesting future direction is to investigate hybrid architectures combining the LL-init with skip connections.\n2Note that even the choice of a step size in SGD typically reflects an assumption about the correlation scale of the gradients.\nThe Shattered Gradients Problem"}, {"heading": "A1. Backprop and Brownian Motion", "text": "Brownian motion is a stochastic process {Bt : t \u2265 0} such that\n\u2022 B0 = 0\n\u2022 (Bt2 \u2212Bt1) \u223c N (0, t2 \u2212 t1) for any 0 \u2264 t1 < t2.\n\u2022 (Bt2 \u2212Bt1) and (Bt4 \u2212Bt3) are independent for any 0 \u2264 t1 < t2 \u2264 t3 < t4.\nThe Shattered Gradients Problem\n\u2022 the sample function t 7\u2192 Bt(\u03c9) is continuous for almost all \u03c9.\nSome important properties of Brownian motion are that\n\u2022 Bt \u223c N (0, t). In particular E[Bt] = 0 and Var[Bt] = t.\n\u2022 E[BtBs] = min(t, s) for any 0 \u2264 s, t.\nThe following well known theorem shows how Brownian motion arises as an infinite limit of discrete random walks: Theorem (Donsker). Let X1, . . . , be i.i.d. random variables with mean 0 and variance 1. Let SN = \u2211N i=1Xi. Then the rescaled random walk\nB (N) t = SbNtc\u221a N for t \u2208 [0, 1] (A1)\nconverges weakly limn\u2192\u221eB(N) = B to Brownian motion Bt\u2208[0,1] on the interval [0, 1].\nWe are now in a position to demonstrate the connection between the gradients and Brownian motion. Proposition A1. Suppose weights are sampled from a distribution with mean zero and variance \u03c32 = 1N per Glorot & Bengio (2010). Then the derivative of fW,b, suitably reparametrized, converges weakly to Brownian motion as N \u2192\u221e.\nProof. The derivative of the neural net with respect to its input is:\nd\ndx fw,b(x) =\n\u2211\nx>bi\nwi. (A2)\nIf we vary x, then the (A2) is a random walk that jumps at points sampled from a Gaussian. In contrast, discrete Brownian motion, (A1), jumps at uniformly spaced points in the unit interval.\nRelabel the neurons so the biases are ordered\nb1 \u2264 b2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 bN without loss of generality. A rectifier is active if its output is nonzero. Let A(x) = {i : x > bi} denote the vector of hidden neurons that are active for input x. Ordering the neurons by their bias terms means the derivative only depends on |A(x)|, the number of active neurons:\nd\ndx fw,b(x) =\n|A(x)|\u2211\ni=1\nwi.\nFinally, we can write the derivative as a function of the fraction t \u2208 [0, 1] of neurons that are active:\nd\ndx fw,b(t) =\nbNtc\u2211\ni=1\nwi.\nThe result follows by Donsker\u2019s theorem since the weights are sampled from N (0, 1N )."}, {"heading": "A2. The Karhunen-Loeve theorem", "text": "Let {Xt : t \u2208 [0, 1]} be a stochastic process with E[Xt] = 0 for all t. The covariance function is\nK(s, t) = Cov(Xs, Xt) = E[XsXt].\nDefine the associated integral operator TK : L2(R) \u2192 L2(R) as\nTK(\u03c6)(t) =\n\u222b 1\n0\nK(t, s)\u03c6(t) ds\nIf K(t, s) is continuous in t and s then, by Mercer\u2019s theorem, the operator TK has orthonormal basis of eigenvectors ei(t) with associated eigenvalues \u03bbi.\nTheorem (Karhunen-Loeve). Let\nFi =\n\u222b 1\n0\nXtei(t) dt\nThen E[Fi] = 0, E[FiFj ] = 0 for i 6= j, Var[Fi] = \u03bbi, and\nXt = \u221e\u2211\ni=1\nFiei(t)\nunder uniform convergence in the mean with respect to t.\nFor example, the eigenvectors and eigenfunctions of Brownian motion, with K(s, t) = min(s, t), are\nek(t) = \u221a 2 sin ( (k \u2212 1\n2 )\u03c0t\n) and \u03bbk =\n1\n(k \u2212 12 )2\u03c02 ."}, {"heading": "A3. Details of the Analysis", "text": "Neural functional analysis. Functional analysis studies functions and families of functions in vector spaces equipped with a topological structure such as a metric. A fundamental tool is to expand a function in terms of an orthonormal basis f(x) = \u2211 k \u03b1kek(x) where the basis satisfies \u3008ej(x), ek(x)\u3009 = 1j=k. A classical example is the Fourier expansion; a more recent example is wavelets.\nA powerful tool for analyzing random processes based on the same philosophy is the Karhunen-Loeve transform. The idea is to represent random processes as linear combinations of orthogonal vectors or functions. For example, principal component analysis is a special case of the KarhunenLoeve transform.\nThe weights of a neural network at initialization are random variables. We can therefore model the output of the neural\nThe Shattered Gradients Problem\nnetwork as a random process indexed by datapoints. That is, for each x(i) \u2208 D, the output fW(x(i)) is a random variable. Similarly, the gradients of the neural network form a random process indexed by the data.\nThe main technical insight underlying the analysis below is that path-weights (Balduzzi et al., 2015; Balduzzi, 2016) provide an orthogonal basis relative to the inner product on random variables given by the covariance. The role played by path-weights in the analysis of gradients is thus analogous to the role of sin, cos and exp in Fourier analysis.\nA3.1. Covariance structure of path-sums\nLemma A2 below shows that gradients are sums over products of weights, where the products are \u201czeroed out\u201d if any neuron along the path is inactive. In this section we develop a minimal mathematical model of path-sums that allows us to compute covariances and correlations between gradients in rectifier networks.\nTo keep things simple, the model is of a network of L layers each of which contains N neurons. Let W be a random (N,N,L\u22121)-tensor with entries given by independent random variables with mean zero and variance \u03c32. A path is a sequence of numbers\u03b1 = (\u03b11, . . . \u03b1L) \u2208 [N ]L. The pathweight \u03b1 is W\u03b1 := \u220fL\u22121 l=1 W[\u03b1l, \u03b1l+1, l], the product of the weights along the path.\nPath-weights are random variables. The expected weight of a path is zero. Paths are uncorrelated unless they coincide exactly:\nE[W\u03b1] = 0 and E[W\u03b1W\u03b2] = { \u03c32(L\u22121) if \u03b1 = \u03b2 0 else.\n(A3) Remark A1. Equation (A3) implies that path-weights are orthogonal under the inner product given by covariance.\nAn activation configuration A is a binary N \u00d7 L-matrix. Path \u03b1 is active under configurationA if all neurons along the path are active, i.e. if A\u03b1 = \u220fL l=1A[\u03b1l, l] = 1, otherwise the path is inactive. The number of active paths in configurationA is\n|A| := \u2211\n\u03b1\u2208[N ]L A\u03b1.\nThe number of co-active paths in configurationsA andB is\n|A \u2229B| := \u2211\n\u03b1\u2208[N ]L A\u03b1 \u00b7B\u03b1.\nFinally, the path-sum under configuration A is the sum of the weights of all active paths:\npW(A) = \u2211\n\u03b1\u2208[N ]L W\u03b1 \u00b7A\u03b1.\nLemma A1. Path-sums have mean zero, E[pW(A)] = 0, and covariance\nE[pW(A) \u00b7 pW(B)] = |A \u2229B| \u00b7 \u03c32(L\u22121)."}, {"heading": "A special case is the variance:", "text": "E[pW(A)2] = |A| \u00b7 \u03c32(L\u22121).\nProof. The mean is zero since E[W\u03b1] = 0. The crossterms E[W\u03b11 \u00b7W\u03b2] vanish for \u03b1 6= \u03b2 by Eq. (A3), so the covariance simplifies as\nE[pW(A)pW(B)] = \u2211\n\u03b1,\u03b2\u2208[N ]L E[W\u03b1W\u03b2] \u00b7A\u03b1B\u03b2\n= \u2211\n\u03b1\u2208[N ]L E[W2\u03b1] \u00b7A\u03b1B\u03b1\nand the result follows.\nA3.2. Gradients are path-sums\nConsider a network of L + 1 layers number 0, 1, . . . L, where each layer contains N neurons. Let\nsL = xL,1 + \u00b7 \u00b7 \u00b7+ xL,N\nbe the sum of the outputs of the neurons in the last layer.\nLemma A2. The derivative\n\u2202sL \u2202x0,i\n= \u2211\n\u03b1=(i,\u03b11,...,\u03b1L)\u2208{i}\u00d7[N ]L W\u03b1 \u00b7A(x)\u03b1.\nis the sum of the weights of all active paths from neuron i to the last layer.\nProof. Direct computation.\nRemark A2. The setup of lemma A2 is quite specific. It is chosen for mathematical convenience. In particular, the numerical coincidence that multiplying the number of paths by the variance of the paths yields exactly one, when weights are initialized according to (He et al., 2015), makes the formulas easier on the eye.\nThe theorems are concerned with the large-scale behavior of the variance and covariance as a function of the number of layers (e.g. exponential versus sublinear decay). Their broader implications\u2014but not the precise quantities\u2014are robust to substantial changes to the setup.\nProof of theorem 1.\nProof. Lemma A2 implies that gradients are sums over path-weights.\na) By lemma A1 the gradient decomposes as a sum over active paths. There are NL paths through the network.\nThe Shattered Gradients Problem\nIf half the neurons per layer are active, then there are |A(xi)| = (N2 )L active paths. Each path is a product of L weights and so has covariance \u03c32L = ( 2N )\nL. Thus, by lemma A1\n( N\n2\n)L \u00b7 ( 2\nN\n)L = L\u220f\nl=1\n(1) = 1.\nb) The number of coactive neurons per layer is N4 and so there are |A(xi) \u2229 A(xj)| = (N4 )L coactive paths. By lemma A1 the covariance is\n( 1\n2\nN\n2\n)L \u00b7 ( 2\nN\n)L = L\u220f\nl=1\n( 1\n2\n) = 1\n2L\nas required.\nA3.3. Covariance structure of residual path-sums\nDerivatives in resnets are path-sums as before. However, skip-connections complicate their structure. We adapt the minimal model in section A3.1 to resnets as follows.\nA residual path is a pair \u03b1\u0303 = (F,\u03b1) where F \u2282 [L] and \u03b1 \u2208 [N ]|F |. The subset F specifies the layers that are not skipped; \u03b1 specifies the neurons in those layers. The length of the path is l(\u03b1\u0303) = |F |. Let P res denote the set of all residual paths. Let Fi denote the ith element of F , listed from smallest to largest. Given weight tensor W as in section A3.1, the weight of path \u03b1\u0303 is\nW\u0303\u03b1\u0303 = { 1 if F = \u2205\u220f|F |\u22121 i=1 W[\u03b1i, \u03b1i+1, Fi] else.\nRemark A3. We adopt the convention that products over empty index sets equal one.\nPath \u03b1\u0303 is active under configurationA if A\u0303\u03b1\u0303 = 1 where\nA\u0303\u03b1\u0303 =\n|F |\u220f\ni=1\nA[\u03b1i, Fi]\nThe residual path-sum under configurationA is\nrW(A) = \u2211\n\u03b1\u0303\u2208P res W\u0303\u03b1\u0303 \u00b7 A\u0303\u03b1\u0303.\nRestricting to F = [L] recovers the definitions for standard feedforward networks in section A3.1.\nThe number of co-active paths shared by configurations A andB on the layers in F is\n|A \u2229B|F = \u2211\n\u03b1\u2208[N ]|F | A[\u03b1i, Fi] \u00b7B[\u03b1i, Fi].\nLemma A3. The covariance between two residual pathsums is\nE[rW(A) \u00b7 rW(B)] = \u2211\nF\u2282[L+1] |A \u2229B|F \u00b7 \u03c32(|F |\u22121)\nProof. Direct computation.\nA3.4. Residual gradients\nProof of theorem 2. The theorem is proved in the setting of lemma A2, see remark A2 for justification.\nProof. a) By lemma A3 the variance is\nE[rW(A)2] = \u03c32 \u00b7 \u2211\nF\u2282[L]\n( N\n2\n)|F | \u03c32(|F |\u22121)\n=\n( 2\nN\n) \u00b7 \u2211\nF\u2282[L]\n( N\n2\n)|F | \u00b7 ( 2\nN\n)|F |\u22121\n=\nL\u2211\nl=0\n( L\nl\n) = 2L\nwhere the final equality follows from the binomial theorem.\nb) For the covariance we obtain\nE[rW(A) \u00b7 rW(B)] = \u03c32 \u00b7 \u2211\nF\u2282[L]\n( N\n4\n)|F | \u03c32(|F |\u22121)\n= \u2211\nF\u2282[L]\n( 1\n2\n)|F |\n=\nL\u2211\nl=0\n( L\nl\n)( 1\n2\n)l\n= ( 1 + 1\n2\n)L\nby the binomial theorem.\nA convenient way to intuit the computations is to think of each layer as contributing (1+1) to the variance and (1+ 12 ) to the covariance:\nCres(i) = L\u220f\nl=1\n(1 + 1) = 2L and\nCres(i, j) = L\u220f\nl=1\n( 1 + 1\n2\n) = ( 3\n2\n)L\nProof of theorem 3 when \u03b2 = 1. The theorem is proved in the setting of lemma A2, see remark A2 for justification.\nThe Shattered Gradients Problem\nProof. a) Theorem 2 implies that each additional layer (without batch normalization) doubles the contribution to the variance of gradients, which we write schematically as vl+1 = 2vl = 2\nl, the variance of the (l+1)st layer is double the lth layer.\nBatch normalization changes the schema to\nvl+1 = vl + 1\nwhere vl is the variance of the previous layer and +1 is added to account for additional variance generated by the non-skip connection (which is renormalized to have unitvariance). The variance of active path sums through (l+ 1) layers is therefore\nvl+1 = l + 1. (A4)\nFinally the variance of gradient CresBN (xi) = vL = L. b) The above schema for batch normalization can be written\nvl+1 = vl + vl l = vl\n( 1 + 1\nl\n)\nwhere the rescaling factor 1l is the expected variance of the previous layer per Eq. (A4). Unrolling yields\nvL =\nL\u22121\u220f\nl=1\n( 1 + 1\nl\n) = L.\nTaking into account the fact that applying batchnormalization to the lth-layer rescales by 1\u221a\nl , the resnet\nmodule can be written in expectation as\nxl+1 = xl + \u03c1BN (W l+1xl) = xl + \u03c1(Wl+1xl)\u221a l .\nThe contribution of each (non-skip) layer to the covariance is half its contribution to the variance since we assume the two inputs are co-active on a quarter of the neurons per layer. The covariance is therefore given by\nL\u22121\u220f\nl=1\n( 1 + 1\n2 \u00b7 1 l\n) \u223c \u221a 2L\nas required.\nTo intuit the approximation, observe that\nL\u22121\u220f\nl=1\n( 1 + 1\n2l\n) \u00b7 ( 1 + 1\n2l \u2212 1\n) = 2L\u22122\u220f\nl=1\n( 1 + 1\nl\n) = 2L\u22121\nSince ( 1 + 12l ) \u223c ( 1 + 12l\u22121 ) , rewrite as\nL\u22121\u220f\nl=1\n( 1 + 1\n2l\n)2 \u223c 2L\u22122\u220f\nl=1\n( 1 + 1\nl\n) = 2L\u2212 1\nand so \u220fL\u22121 l=1 ( 1 + 12l ) \u223c \u221a L. Numerically we find \u220fL\u22121 l=1 ( 1 + 12l ) \u223c \u221a 4 \u03c0 (L+ 1) to be a good approximation for large L.\nProof of theorem 3 for general \u03b2. The theorem is proved in the setting of lemma A2, see remark A2 for justification.\nProof. a) The introduction of \u03b2-rescaling changes the schema to\nvl+1 = vl\n( 1 +\n\u03b22\n\u03b22(l \u2212 1) + 1\n) .\nThe proof then follows from the observation that\nL\u22121\u220f\nl=1\n( 1 +\n\u03b22\n\u03b22(l \u2212 1) + 1\n) = \u03b22(L\u2212 1) + 1.\nb) The covariance is given by\nL\u22121\u220f\nl=1\n( 1 + 1\n2\n\u03b22\n\u03b22(l \u2212 1) + 1\n) \u223c \u03b2 \u221a L\nby similar working to when \u03b2 = 1.\nA3.5. Highway gradients\nProof of corollary 1. The theorem is proved in the setting of lemma A2, see remark A2 for justification.\nProof. The variance is given by\nL\u220f\nl=1\n( \u03b321 + \u03b3 2 2 ) = 1\nand the covariance by\nL\u220f\nl=1\n( \u03b31 + 1\n2 \u03b32\n) = ( \u03b321 + 1\n2 \u03b322\n)L\nby analogous working to the previous theorems.\nSetting \u03b31 = \u221a 1\u2212 1L and \u03b32 = \u221a 1 L obtains\nCHN\u03b3 (xi,xj) = (\n1\u2212 1 L + 1 2 1 L\n)L\n= ( 1\u2212 1\n2\n1\nL\n)L\nL\u2212\u2192 \u221e e\u2212 1 2\nby standard properties of the constant e.\nThe Shattered Gradients Problem"}, {"heading": "A4. Details on architecture for figure 6", "text": "r modules with 8 filters each Downsampling module with 16 filters r \u2212 1 modules with 16 filters each Downsampling module with 32 filters r \u2212 1 modules with 32 filters each Downsampling module with 64 filters r \u2212 1 modules with 64 filters each Downsampling module with 64 filters Flattening layer\nFC layer to output (width 10)"}], "references": [{"title": "Deep Online Convex Optimization with Gated Games", "author": ["Balduzzi", "David"], "venue": "In arXiv:1604.01952,", "citeRegEx": "Balduzzi and David.,? \\Q2016\\E", "shortCiteRegEx": "Balduzzi and David.", "year": 2016}, {"title": "Kickback cuts Backprop\u2019s red-tape: Biologically plausible credit assignment in neural networks", "author": ["Balduzzi", "David", "Vanchinathan", "Hastagiri", "Buhmann", "Joachim"], "venue": "In AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Balduzzi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Balduzzi et al\\.", "year": 2015}, {"title": "Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks", "author": ["Balduzzi", "David", "McWilliams", "Brian", "Butler-Yeoman", "Tony"], "venue": "In arXiv:1611.02345,", "citeRegEx": "Balduzzi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Balduzzi et al\\.", "year": 2016}, {"title": "Greedy Layer-Wise Training of Deep Networks", "author": ["Y Bengio", "P Lamblin", "D. Popovici", "H. Larochelle"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "P Simard", "P. Frasconi"], "venue": "IEEE Trans. Neur. Net.,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "The loss surface of multilayer networks", "author": ["A Choromanska", "M Henaff", "M Mathieu", "G B Arous", "Y. LeCun"], "venue": "In Journal of Machine Learning Research: Workshop and Conference Proceeedings,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Highway and Residual Networks learn Unrolled Iterative Estimation", "author": ["Greff", "Klaus", "Srivastava", "Rupesh Kumar", "Schmidhuber", "Juergen"], "venue": "In ICLR,", "citeRegEx": "Greff et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2017}, {"title": "Identity Matters in Deep Learning", "author": ["Hardt", "Moritz", "Ma", "Tengyu"], "venue": "In ICLR,", "citeRegEx": "Hardt et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2017}, {"title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In ICCV,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Identity Mappings in Deep Residual Networks", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In ECCV,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Deep Residual Learning for Image Recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In CVPR,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "A Fast Learning Algorithm for Deep Belief Nets", "author": ["GE Hinton", "S Osindero", "Teh", "Y W"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen", "author": ["Hochreiter", "Sepp"], "venue": "Master\u2019s thesis, Technische Universita\u0308t Mu\u0308nchen,", "citeRegEx": "Hochreiter and Sepp.,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter and Sepp.", "year": 1991}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In ICML,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy Lei"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A Krizhevsky", "I Sutskever", "Hinton", "G E"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "All you need is a good init", "author": ["D Mishkin", "J. Matas"], "venue": "In ICLR,", "citeRegEx": "Mishkin and Matas,? \\Q2016\\E", "shortCiteRegEx": "Mishkin and Matas", "year": 2016}, {"title": "On the number of linear regions of deep neural networks", "author": ["Montufar", "Guido F", "Pascanu", "Razvan", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Skip Connections as Effective SymmetryBreaking", "author": ["Orhan", "A Emin"], "venue": "In arXiv:1701.09175,", "citeRegEx": "Orhan and Emin.,? \\Q2017\\E", "shortCiteRegEx": "Orhan and Emin.", "year": 2017}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "In ICLR,", "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units", "author": ["Shang", "Wenling", "Sohn", "Kihyuk", "Almeida", "Diogo", "Lee", "Honglak"], "venue": null, "citeRegEx": "Shang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2016}, {"title": "Random Walk Initialization for Training Very Deep Feedforward Networks", "author": ["Sussillo", "David", "Abbott", "L F"], "venue": "In ICLR,", "citeRegEx": "Sussillo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sussillo et al\\.", "year": 2015}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Going Deeper With Convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In CVPR,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning", "author": ["Szegedy", "Christian", "Ioffe", "Sergey", "Vanhoucke", "Vincent"], "venue": "In arXiv:1602.07261,", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Benefits of depth in neural networks", "author": ["Telgarsky", "Matus"], "venue": "In COLT,", "citeRegEx": "Telgarsky and Matus.,? \\Q2016\\E", "shortCiteRegEx": "Telgarsky and Matus.", "year": 2016}, {"title": "Residual Networks Behave Like Ensembles of Relatively Shallow Networks", "author": ["Veit", "Andreas", "Wilber", "Michael J", "Belongie", "Serge"], "venue": "In NIPS,", "citeRegEx": "Veit et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Veit et al\\.", "year": 2016}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Vershynin", "Roman"], "venue": "In Compressed sensing,", "citeRegEx": "Vershynin and Roman.,? \\Q2012\\E", "shortCiteRegEx": "Vershynin and Roman.", "year": 2012}], "referenceMentions": [{"referenceID": 16, "context": "Introduction Deep neural networks have achieved outstanding performance (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2016b).", "startOffset": 72, "endOffset": 137}, {"referenceID": 24, "context": "Introduction Deep neural networks have achieved outstanding performance (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2016b).", "startOffset": 72, "endOffset": 137}, {"referenceID": 4, "context": "Reducing the tendency of gradients to vanish or explode with depth (Hochreiter, 1991; Bengio et al., 1994) has been essential to this progress.", "startOffset": 67, "endOffset": 106}, {"referenceID": 9, "context": "Combining careful initialization (Glorot & Bengio, 2010; He et al., 2015) with batch normalization (Ioffe & Szegedy, Authors listed alphabetically Victoria University of Wellington, New Zealand Disney Research, Z\u00fcrich, Switzerland.", "startOffset": 33, "endOffset": 73}, {"referenceID": 23, "context": "Shattered gradients undermine the effectiveness of algorithms that assume gradients at nearby points are similar such as momentum-based and accelerated methods (Sutskever et al., 2013; Balduzzi et al., 2016).", "startOffset": 160, "endOffset": 207}, {"referenceID": 2, "context": "Shattered gradients undermine the effectiveness of algorithms that assume gradients at nearby points are similar such as momentum-based and accelerated methods (Sutskever et al., 2013; Balduzzi et al., 2016).", "startOffset": 160, "endOffset": 207}, {"referenceID": 7, "context": "Introducing skip-connections allows much deeper networks to be trained (Srivastava et al., 2015; He et al., 2016b;a; Greff et al., 2017).", "startOffset": 71, "endOffset": 136}, {"referenceID": 25, "context": "Figure 2c shows the dramatic effect of recently proposed \u03b2-rescaling (Szegedy et al., 2016): the ACF of even the 50 layer network resemble brown-noise.", "startOffset": 69, "endOffset": 91}, {"referenceID": 20, "context": "Orthogonalizing weight matrices can yield further improvements albeit at a computational cost (Saxe et al., 2014; Mishkin & Matas, 2016).", "startOffset": 94, "endOffset": 136}, {"referenceID": 7, "context": "Related work Carefully initializing neural networks has led to a series of performance breakthroughs dating back (at least) to the unsupervised pretraining in Hinton et al. (2006); Bengio et al.", "startOffset": 159, "endOffset": 180}, {"referenceID": 3, "context": "(2006); Bengio et al. (2006). The insight of Glorot & Bengio (2010) is that controlling the variance of the distributions from which weights are sampled allows to control how layers progressively amplify or dampen the variance of activations and error signals.", "startOffset": 8, "endOffset": 29}, {"referenceID": 3, "context": "(2006); Bengio et al. (2006). The insight of Glorot & Bengio (2010) is that controlling the variance of the distributions from which weights are sampled allows to control how layers progressively amplify or dampen the variance of activations and error signals.", "startOffset": 8, "endOffset": 68}, {"referenceID": 3, "context": "(2006); Bengio et al. (2006). The insight of Glorot & Bengio (2010) is that controlling the variance of the distributions from which weights are sampled allows to control how layers progressively amplify or dampen the variance of activations and error signals. More recently, He et al. (2015) refined the approach to take rectifiers into account.", "startOffset": 8, "endOffset": 293}, {"referenceID": 3, "context": "(2006); Bengio et al. (2006). The insight of Glorot & Bengio (2010) is that controlling the variance of the distributions from which weights are sampled allows to control how layers progressively amplify or dampen the variance of activations and error signals. More recently, He et al. (2015) refined the approach to take rectifiers into account. Rectifiers effectively halve the variance since, at initialization and on average, they are active for half their inputs. Orthogonalizing weight matrices can yield further improvements albeit at a computational cost (Saxe et al., 2014; Mishkin & Matas, 2016). The observation that the norms of weights form a random walk was used by Sussillo & Abbott (2015) to tune the gains of neurons.", "startOffset": 8, "endOffset": 705}, {"referenceID": 3, "context": "(2006); Bengio et al. (2006). The insight of Glorot & Bengio (2010) is that controlling the variance of the distributions from which weights are sampled allows to control how layers progressively amplify or dampen the variance of activations and error signals. More recently, He et al. (2015) refined the approach to take rectifiers into account. Rectifiers effectively halve the variance since, at initialization and on average, they are active for half their inputs. Orthogonalizing weight matrices can yield further improvements albeit at a computational cost (Saxe et al., 2014; Mishkin & Matas, 2016). The observation that the norms of weights form a random walk was used by Sussillo & Abbott (2015) to tune the gains of neurons. In short, it has proven useful to treat weights and gradients as random variables, and carefully examine their effect on the variance of the signals propagated through the network. This paper presents a more detailed analysis that considers correlations between gradients at different datapoints. The closest work to ours is Veit et al. (2016), which shows resnets behave like ensembles of shallow networks.", "startOffset": 8, "endOffset": 1079}, {"referenceID": 1, "context": "It was shown in Balduzzi et al. (2015); Balduzzi (2016) that the gradients in neural nets are sums of path-weights over active paths.", "startOffset": 16, "endOffset": 39}, {"referenceID": 1, "context": "It was shown in Balduzzi et al. (2015); Balduzzi (2016) that the gradients in neural nets are sums of path-weights over active paths.", "startOffset": 16, "endOffset": 56}, {"referenceID": 8, "context": "The initialization in He et al. (2015) assumes datapoints activate half the neurons per layer.", "startOffset": 22, "endOffset": 39}, {"referenceID": 5, "context": "The assumption on co-activations is implied by (and so weaker than) the assumption in Choromanska et al. (2015) that activations are Bernoulli random variables independent of the inputs.", "startOffset": 86, "endOffset": 112}, {"referenceID": 9, "context": "Suppose weights are initialized with variance \u03c3 = 2 N following He et al. (2015). Then a) The variance of the gradient at x is C fnn(i) = 1.", "startOffset": 64, "endOffset": 81}, {"referenceID": 9, "context": "Part (a) recovers the observation in He et al. (2015) that setting \u03c3 = 2 N preserves the variance across layers in rectifier networks.", "startOffset": 37, "endOffset": 54}, {"referenceID": 9, "context": "Residual networks The residual modules introduced in He et al. (2016a) are", "startOffset": 53, "endOffset": 71}, {"referenceID": 24, "context": "The \u03b2rescaling was introduced in Szegedy et al. (2016) where it was observed setting \u03b2 \u2208 [0.", "startOffset": 33, "endOffset": 55}, {"referenceID": 24, "context": "3] per Szegedy et al. (2016). The effect is dramatic: Theorem 3 (covariance of gradients in resnets with BN and rescaling).", "startOffset": 7, "endOffset": 29}, {"referenceID": 7, "context": "Highway networks The standard highway network (Srivastava et al., 2015; Greff et al., 2017) has layers of the form", "startOffset": 46, "endOffset": 91}, {"referenceID": 9, "context": "Using a slight modification of the \u201cbottleneck\u201d architecture in He et al. (2016a), we introduce one skip-connection for every two convolutional layers and both network architectures use batch normalization.", "startOffset": 64, "endOffset": 82}, {"referenceID": 21, "context": "We use concatenated rectifiers or CReLUs (Shang et al., 2016):", "startOffset": 41, "endOffset": 61}, {"referenceID": 9, "context": "An alternative architecture is based on the PReLU introduced in He et al. (2015):", "startOffset": 64, "endOffset": 81}, {"referenceID": 20, "context": "A detailed analysis of learning in linear neural networks by Saxe et al. (2014) showed, theoretically and experimentally, that arbitrarily deep linear networks can be trained when initialized with orthogonal weights.", "startOffset": 61, "endOffset": 80}, {"referenceID": 9, "context": "The other networks were initialized according to He et al. (2015). The architectures are thin with the number of filters per layer in the ReLU networks ranging from 8 at the input layer to 64, see section A4.", "startOffset": 49, "endOffset": 66}, {"referenceID": 9, "context": "The other networks were initialized according to He et al. (2015). The architectures are thin with the number of filters per layer in the ReLU networks ranging from 8 at the input layer to 64, see section A4. Doubling with each spatial extent reduction. The thinness of the architecture makes it particularly difficult for gradients to propagate at high depth. The reduction is performed by convolutional layers with strides of 2, and following the last reduction the representation is passed to a fully connected layer with 10 neurons for classification. The numbers of filters per layer of the CReLU models were adjusted by a factor of 1/ \u221a 2 to achieve parameter parity with the ReLU models. The Resnet version of the model is the same as the basic ReLU model with skip-connections after every two modules following He et al. (2016a). Updates were performed with Adam (Kingma & Ba, 2015).", "startOffset": 49, "endOffset": 837}, {"referenceID": 18, "context": "It was shown in Montufar et al. (2014) that the number of linear regions can grow exponentially with depth (but only polynomially with width).", "startOffset": 16, "endOffset": 39}], "year": 2017, "abstractText": "A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. The problem has largely been overcome through the introduction of carefully constructed initializations and batch normalization. Nevertheless, architectures incorporating skip-connections such as resnets perform much better than standard feedforward architectures despite well-chosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise. In contrast, the gradients in architectures with skipconnections are far more resistant to shattering decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new \u201clooks linear\u201d (LL) initialization that prevents shattering. Preliminary experiments show the new initialization allows to train very deep networks without the addition of skip-connections.", "creator": "LaTeX with hyperref package"}}}