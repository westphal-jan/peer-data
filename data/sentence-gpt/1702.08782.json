{"id": "1702.08782", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "ShaResNet: reducing residual network parameter number by sharing weights", "abstract": "Deep Residual Networks have reached the state of the art in many image processing tasks such image classification. However, the cost for a gain in accuracy in terms of depth and memory is prohibitive as it requires a higher number of residual blocks, up to double the initial value. To tackle this problem, we propose in this paper a way to reduce the redundant information of the networks. We share the weights of convolutional layers between residual blocks operating at the same spatial scale. The signal flows multiple times in the same convolutional layer. The resulting architecture, called ShaResNet, contains block specific layers and shared layers. These ShaResNet are trained exactly in the same fashion as the commonly used residual networks. We show, on the one hand, that they are almost as efficient as their sequential counterparts while involving less parameters, and on the other hand that they are more efficient than a residual network with the same number of parameters. For example, a 152-layer-deep residual network can be reduced to 106 convolutional layers, i.e. a parameter gain of 39\\%, while loosing less than 0.2\\% accuracy on ImageNet. The resulting architecture will take an extra 3 layers for each convolutional layer (see Fig. 1 ). This is particularly useful in image processing tasks such as deep learning in image processing tasks such as image classification.\n\n\n\n\nFor further information about our approach, see the following paper:\nAffecting Residual Networks is an optimization process by which each layer of a convolutional layer (and its components) has a minimum of one layer per layer (Figure 1 ). To overcome this difficulty, we used a Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Ga", "histories": [["v1", "Tue, 28 Feb 2017 13:37:59 GMT  (2124kb,D)", "http://arxiv.org/abs/1702.08782v1", null], ["v2", "Mon, 6 Mar 2017 13:49:15 GMT  (2124kb,D)", "http://arxiv.org/abs/1702.08782v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["alexandre boulch"], "accepted": false, "id": "1702.08782"}, "pdf": {"name": "1702.08782.pdf", "metadata": {"source": "CRF", "title": "ShaResNet: reducing residual network parameter number by sharingweights", "authors": ["Alexandre Boulch"], "emails": [], "sections": [{"heading": null, "text": "Deep Residual Networks have reached the state of the art in many image processing tasks such image classification. However, the cost for a gain in accuracy in terms of depth and memory is prohibitive as it requires a higher number of residual blocks, up to double the initial value. To tackle this problem, we propose in this paper a way to reduce the redundant information of the networks. We share the weights of convolutional layers between residual blocks operating at the same spatial scale. The signal flows multiple times in the same convolutional layer. The resulting architecture, called ShaResNet, containsblockspecific layersandshared layers. These ShaResNet are trained exactly in the same fashion as the commonly used residual networks. We show, on the one hand, that they are almost as efficient as their sequential counterparts while involving less parameters, and on the other hand that they are more efficient than a residual network with the same number of parameters. For example, a 152-layer-deep residual network can be reduced to 106 convolutional layers, i.e. a parameter gain of 39%, while loosing less than 0.2% accuracy on ImageNet."}, {"heading": "1 Introduction", "text": "Convolutional Neural Networks (CNNs) are now widely used for image processing tasks from clas-\nsification [LBBH98] and object detection [GDDM14, RHGS15] to semantic segmentation [BKC15, ASL16]. Their utilisation even generalizes to other fields where data can be represented as tensors like in point cloud processing [BM16] or 3D shape style identification [LGK16]. Today's network architectures still carry a strong inheritanceof theCNNearly stage designs. They are based on stacking convolutional, activation and dimensionality reduction layers. Over the past years, the progress in image processing tasks went together with a gradual increase in the number of layers, from AlexNet [KSH12] to Residual networks [HZRS16] (ResNets) that may contain up to hundreds of convolutions.\nPractical use of such networks may be challenging when using low memory system, such as autonomous vehicles, both for optimization and inference. Moreoever, from a biological point of view, a higher number of stacked layers leads to networks further from the original underlying idea of neural networks: biological brain mimicry. According to the current knowledge of the brain, cerebral cortex is composed of a low number of layers where the neurons are highly connected. Moreover the signal is also allowed to recursively go through the same neurons. In that sense recurrentneuralnetworksare much closer to the brain structure butmore difficult to optimize [BSF94, HS97].\nLooking more closely at the repetition of residual blocks in ResNets, it could somehow be interpreted as an unwrapped recurrent neural networks. This constatation raises questions such as \"how similar are the weights of the blocks ?\", \"do the same parts of the blocks operate similar operations ?\" and in the later case \"is it possible to reduce the parameter number of a residual network ?\". Drivenby theseobservations and\nar X\niv :1\n70 2.\n08 78\n2v 1\n[ cs\n.C V\n] 2\n8 Fe\nb 20\nStage 1 Data\npreparation\nStage 2 Residual\nblocks with sharing\nStage 3 Residual\nblocks with sharing\nStage 4 Residual\nblocks with sharing\nStage 5 Residual\nblocks with sharing\nStage 6 Classification\nColor code: green convolutions are shared between several residual blocks, red blocks are spatial\nquestions, we present a new network architecture basedon residual networkswherepart of the convolutions shareweights, called ShaResNets. It results in a great decrease of the number of network parameters, from 25% to 45% depending on the size of the original architecture. Our networks also present a better ratio performances over parameter number while downgrading the absolute performance by less than 1%.\nThe paper is organized as follow: section 2 presents the related work on convolutional neural networks (CNNs) ; the ShaResNets are presented in section 3 and finally, in section 4, we expose our experimentations on classification datasets CIFAR 10 and 100 and ILSVRC Imagenet."}, {"heading": "2 Related work", "text": "CNNswere introduced in [LBD+89] for handwritten digits recognition. They became over the past years one of the most enthusiastic field of deep learning [LBH15]. TheCNNsareusuallybuilt usinga common framework. They containsmany convolutional layers and operate a gradual spatial dimension reduction using convolutional strides or pooling layers [CMS12, JKL+09]. This structure naturally integrates low/mid/high level features along with a dimension compression before ending with a classifier, commonly a perceptron [Ros57], multi-layered or not, i.e. one ormore fully connected layers.\nLooking at the evolution CNNs, the depth appears to be a key feature. On challenging image processing tasks, an increase of performance is often related to a deeper network. As an example, AlexNet [KSH12] has 5 convolutions while VGG16 and VGG19 [SZ14] have respectively 16 and 19 convolutional layers andmore recently, in [HSL+16], the authors train a 1200 layer deep network.\nVariations in the LeNet structure have also been used to improve convergence. In a Network in Network (NiN) [LCY14], convolutionsaremappedwitha multilayerperceptron (1x1 convolutions),whichprevent overfitting and improved accuracy on datasets such as CIFAR [KH09]. GoogLeNet [SLJ+15] introduced a multiscale approach using the inception module, composedofparallel convolutionswithdif-\nferent kernel sizes.\nOptimizingsuchdeeparchitectures can facepractical problems such as overfitting or vanishing or explodinggradients. Toovercomethese issues, several solutions havebeenproposed such as enhance optimizers [SMDH13],dropout [SHK+14]applyingarandom reduction of the number of connection in fully connected layers or on convolutional layers [ZK16], intelligent initialization strategies [GB10] or training sub-networks with stochastic depth [HSL+16].\nResidual networks [HZRS16] achieved the state of the art in many recognition tasks including Imagenet [RDS+15] and COCO [LMB+14]. They proved to be easier to optimize. One of the particularities of these networks is to be very deep, up to hundreds of residual layers. More recently, the authors of [ZK16] introduced wide residual networks which reduce the depth compared to usual resnets by usingwider convolutional blocks.\nReducing network size has been the object of several works. More compact layers are also used, like the replacement of the fully connected layers by average pooling [SLJ+15, HZRS16]. In [RORF16, CB16], theweights are constrained be binary, reducing considerably thememory consumption. [Pra89] prune weights in pretrained networks, modfiying the structure, to create a lighter network. [HMD15] remove redundant connections and allow weight sharing. Similarly, in this work, we use weight sharing to reduce the network size, except that we enforceapredefinedsharingstructureat trainingtime. We do not need post processing of the network."}, {"heading": "3 Sharing Residual Networks", "text": "ShaResNets are based on residual networks architectures in which we force the residual blocks in the same stage, i.e. between two spatial dimension reduction, to share the weights of one convolution. In this section, we first present the residual architectures we based our work on, and then, detail the sharing process."}, {"heading": "3.1 Residual networks.", "text": "Theresidualnetworksbasic [HZRS16]orwide [ZK16] areasequential stackof residualblocks,withseveral convolution layers bypassed by parallel branch. The output of block k,xk+1 can be represented as:\nxk+1 = xk + F(xk,Wk) (1)\nwhere xk is the input (output of block k \u2212 1), F is the residual function andWk are the parameters of the residual unit. Among themtwo types of residual blocks are used in this paper.\n\u2022 basic composed of two consecutive 3x3 convolutions.\n\u2022 bottleneck composed of one 3x3 convolution surrounded by two 1x1 convolutions for reducing and then expanding the dimensionality.\nAcommonelementof theconvolutional structure in the residual blocks is at least a 3x3 convolution. This convolution allows a neighborhood connection so that the final decision is taken using neighborhood relations between pixels and not only independent pixel values.\nFigure 2 describes the blocks composing the networks presented in this paper. We used two implementations, depending on the datasets (CIFAR 10- 100 and ImageNet) to fit the original network structure we will compare to (section 4). They differ in the position of batch normalization and ReLU, before convolutions for CIFAR datasets and after for ImageNet."}, {"heading": "3.2 Sharing weights", "text": "The underlying idea of our approach is that it is possible to somehow distinguish two types of mechanisms in a residual block. The first, specific to the block, is the abstraction of the residual block. From block to block, it created higher level features. The second, redundant in theblocksof thesamestage, is the spatial connection relations between neighboring tensor cells, between pixels. The equation 1 becomes:\nxs,k+1 = xs,k + F(xs,k,Ws,k,Ws) (2)\nwhere xs,k (resp. xs,k+1) is the input (resp. the output) of block (k, s),k-th residual unit of stage s.Wk of equation 1 is split intoWk,s, the parameters specific to the residual block and Ws the parameters shared at the stage level.\nIn ResNets, the spatial information is taken into account in the 3x3 convolutions and in the layers with dimension reduction (convolution with stride or pooling layers). We first look at the bottleneck block, composedof 3 convolutions (1x1, 3x3 and 1x1). The fictive separation between spatial connection andspecific operations is easyas the 1x1 convolution do not connect neighboring cells. This is the top line of figure 3(a). Then, for all spatial connections in the same stage, i.e. for all the blocks between two pooling layers (or convolution with stride), we share the weights. By using a unique 3x3 convolution, we consider that all the spatial connections of a given stage can be explained by a common set of kernels (figure 3(b)).\nWe adapt the approach to the basic residual block. As it is composed of two 3x3 convolutions, extracting the spatial component is not possible. Still, we adopt a similar approach, the first convolution is considered as specific and the second is sharedwith the blocks of the same stage (figure 3, bottom line).\nIn the two cases, we obtain a similar global architecture represented in figure 1. For each stage, the green convolution is common to all block while the specific items are blue (the number of specific items depends on the architecture choice). The red blocks are the dimensionality reduction layers and the yellow onewould be either amulti-layer perceptron or an average pooling."}, {"heading": "3.3 Gradient propagation", "text": "The gradient propagation in the block specific convolutions is similar to the usual stochastic gradient descent with momentum. The corresponding up-\ndate rule for convolution k of stage s is:\nvs,k = \u03b3vs,k + \u03b1\u2207Ws,kJ(Ws,k) (3) Ws,k = Ws,k \u2212 vs,k (4)\nwhere \u03b3 is the momentum, \u03b1 is the learning rate, J(Ws,k) is the objective function and v is the velocity vector.\nIn the case of shared convolution the gradients are accumulated before weight update. The update rule becomes:\nvs = \u03b3vs + \u03b1 \u2211 i\u2208S \u2207WsJ(Ws)(i) (5)\nWs = Ws \u2212 vs (6)\nwhere i stands for the index of the block of stage s."}, {"heading": "4 Experimental results", "text": ""}, {"heading": "4.1 Datasets and architectures", "text": "We experiment on three dataset, CIFAR 10, CIFAR 100and ImageNet. Wepropose evaluations consisting into a comparison between the ShaResNets and their ResNet counterpart.\nCIFAR 10 and 100 are two datasets containing 50000 images for training and 10000 for test. In order to show that our sharing process can be generalized to different residual architectures, we use ResNets andWide ResNets:\n\u2022 The ResNets implementation (ResNet-164) with 164 convolutions is a good example of very deep network for CIFAR 10 dataset, based on basic residual blocks. (figure 2 second column).\n\u2022 Wide residual networks are not as deep as the previous but are composed of wider convolutions (more convolutional planes). We present results with two depth: 40 (WRN-40-4) for CIFAR 10 and 28 (WRN-28-10) for CIFAR 100. Thesearebasedonthewide residualblock (figure 2 first column). The dropout is only activated for CIFAR 100.\nImagenet is a much bigger dataset, with more than one million training images and 1000 classes. We experiment with residual networks of different depth: 34 (ResNet-34) with basic block (figure 2 third column), 50, 101 and 152 (ResNet-50 and ResNet-152) with bottleneck block (figure 2 last column)."}, {"heading": "4.2 Parameter number reduction", "text": "This section deals with the consequences of sharing convolutional weights on the network parameter number. Using one convolution for the spatial relations per stage instead of one per block reduce significantly the size of the network. Comparatively, thedeeper theResNet is the thebigger thegain is for its ShaResNet version.\nTable 1 shows the figures for the different architectures and datasets. As expected, the gain is substantial, from 20% for ResNet-50 (ImageNet) to 45% for ResNet-164 (CIFAR). The convolution number in the table expresses the number of independant convolutionnal layers, the shared convolutional layers are counted once.\nTraining loss on CIFAR 10 (left) and CIFAR 100 (right).\nTest accuracy on CIFAR 10 (left) and CIFAR 100 (right).\nFigure 4: Training loss and test accuracy on CIFAR"}, {"heading": "4.3 Training", "text": "ShaResNet are trained using the same training process as their non-shared counterpart. We used a stochastic gradient descent with momentum and step dropping learning rate policy. CIFAR models were trained with whitened data from PyLearn2 [GWFL+13] and we applied a random horizontal flip on the input image to simulate a larger training dataset and avoid over-fitting. For ImageNet, we adopted a similar approach, we apply on the input image a random crop, random contrast, lighting and color normalization as well as horizontal flip. According to our experiments, the behaviors of our networks are very similar to the original ones. Figure 4 presents the training loss and testing accuracy plots obtained on the CIFAR datasets. The gradient accumulation at shared convolutions does not induces instabilities at both training and testing time."}, {"heading": "4.4 Accuracy", "text": "Quantitative evaluation of the networks are presentedon table2. Onall theseexperiments, the top1 decrease by less than 1% when using the ShaResNet version of the algorithm. The gapbetweenoriginal and shared version is lower on ImageNet and CIFAR 100. This is to be related to the sizes of the networks. The CIFAR 10 networks are smaller than\nthe others (less than 10M parameters). To our understanding, smaller residual networks induces less redundancy, so that reducing the number of parameters only reduce the learning capacity. On the contrary, large networks such as wide residual network with large widening factor or residual networks for ImageNet are more subject to redundant parameters. In that case, sharing weights makes more sense, like for CIFAR 100 where the accuracy gap is only of 0.2%with parameters reduced by 26%.\nCompared to sequential networks, sharing spatial connections at stage level induces a loss of accuracy at test time. We now compare our ShaResNets to less deep networks with a similar number of parameters. Table 3 shows these results for CIFAR. For each shared architecture, we compare its sequential counterpart with reduced depth. We can draw similar conclusion as in the first paragraph. Sharing weights on relatively small architectures (CIFAR 10) is notmore efficient thanusing a less deepnetwork. On the contrary,Wide ResNetwith awiden factor of 10 (CIFAR 100) gets a boost in accuracy using shared convolutions. Deeper networks benefit frommutualisation of spatial relations, the weights are better used, i.e. the ratio accuracy over network size gets better.\nOn Imagenet, table 2 underlines that the sharing is more efficient as the network goes larger. We even reach similar accuracy (less than 0.2% drop) for the 152 layer architecture. The figure 5 presents the top-1 error function of the weight on ImageNet. The shared architectures plot (green curve) is situated under the blue curve (residual networks). It illustrates that for large networks, shared networks are more efficient than their sequential peers with similar number of parameters."}, {"heading": "4.5 Limitations and perspectives", "text": "Wehave shown in the previous section that residual networkswith sharedspatial connectionsareparticularly efficient on large networks: given a number of parameters, ShaResNets are more efficient (figure 5). However, they induce a loss in terms of accuracy sometimes even leading to performances similar to networks with reduced depth (CIFAR 10 networks). The conclusions we can draw is that, first,\nDataset Model Param. Acc.\nCIFAR 10\nResNet-164 Share 0.93M 93.8 ResNet-92 0.96M 93.9 WRN-40-4 Share 5.85M 94.9 WRN-28-4 5.85M 95.0\nCIFAR 100 WRN-28-10 Share 26.86M 79.8WRN-22-10 26.85M 79.55\nRed line: SRN 50 performs better than RN 34 with less parameters.\nMagenta line: SRN 152 performs as well as RN 152 with less parameters.\nFigure 5: Top 1 error (%) of ShaResNet and ResNets function of themodel size (millions of parameters).\nin relatively small networks parameters are used to their potential or at least that redundant spatial connectivities are fewer in number than for large networks.\nSecond, we chose an arbitrary shared structure.\nWe considered the 3x3 convolutions to operate similar operations for all blocks in the same stage. This assumption may be too restrictive. In our future work we will investigate flexible shared networks, where the sharing rate would be adaptative, function of the noise, the position in the stage and classification dataset (image size, class number). Moreover, we would also investigate other possible splits between spatial relations and block specific operations that would require to modify the basic or bottleneck residual block structure, for example using channelwiseconvolutionas spatial relationsand1x1 convolution for information abstraction."}, {"heading": "5 Conclusion", "text": "In this paper, we introduced the ShaResNet, a new convolutional neural network architecture based on residual networks. By sharing convolutions between residual blocks, we create neural architectures lighter than their sequential residual counterpart by 25% to 45% in terms of number of parameters. The training of such network is as easy as with common residual networks. We experimented on three classification datasets. Shared residual architecture proved to be efficient for large networks. We observed an accuracy gap to the corresponding residual architecture of less than 1% for a substantial size reduction. By exploiting the redundant relations of 3x3 convolutions between residual blocks, the ShaResNets make better use of the optimizable weights. With an equivalent parameter number, we obtain better results. We hope that these findings\nwill help further investigation in image processing andmore generally in deep learning research.\nImplementation and hardware details\nThe experiments uses Torch7 with neural network package. Our code for CIFAR 10 and 100 experiments is based on the original implementation of Wide Residual Networks (github.com/szagoruyko/wide-residualnetworks). Similarly for Imagenet, we used the code from github.com/facebook/fb.resnet.torch.\nExperiments were operated using Titan X (Maxwell) GPUs, one for CIFAR and two for ImageNet.\nCode\nThe code is available online at github.com/ aboulch/sharesnet."}, {"heading": "Acknowledgements", "text": "This work is part of the DeLTA project at ONERA delta-onera.github.io. This projects aims at developing innovative machine learning approaches for aerospace applications."}], "references": [{"title": "Semantic Segmentation of Earth Observation Data Using Multimodal and Multi-scale Deep Networks", "author": ["Nicolas Audebert", "Bertrand Le Saux", "S\u00e9bastien Lefevre"], "venue": "Asian Conference in Computer Vision,", "citeRegEx": "Audebert et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Audebert et al\\.", "year": 2016}, {"title": "Segnet: A deep convolutional encoderdecoder architecture for image segmentation", "author": ["Vijay Badrinarayanan", "Alex Kendall", "Roberto Cipolla"], "venue": "arXiv preprint arXiv:1511.00561,", "citeRegEx": "Badrinarayanan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Badrinarayanan et al\\.", "year": 2015}, {"title": "Deep learning for robust normal estimation in unstructured point clouds", "author": ["Alexandre Boulch", "Renaud Marlet"], "venue": "In Computer Graphics Forum,", "citeRegEx": "Boulch and Marlet.,? \\Q2016\\E", "shortCiteRegEx": "Boulch and Marlet.", "year": 2016}, {"title": "Learning long-termdependencieswith gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1", "author": ["Matthieu Courbariaux", "Yoshua Bengio"], "venue": null, "citeRegEx": "Courbariaux and Bengio.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux and Bengio.", "year": 2016}, {"title": "Multi-column deep neural networks for image classification", "author": ["Dan Ciregan", "Ueli Meier", "J\u00fcrgen Schmidhuber"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Ciregan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ciregan et al\\.", "year": 2012}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "andWilliam J Dally"], "venue": "CoRR, abs/1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Deep networks with stochastic depth. NIPS 2016", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger"], "venue": "arXiv preprint arXiv:1603.09382,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Kevin Jarrett", "Koray Kavukcuoglu", "Yann Lecun"], "venue": "IEEE 12th International Conference on Computer Vision,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "PatrickHaffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Yann LeCun", "BernhardBoser", "JohnSDenker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel"], "venue": "Neural computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "IdentifyingStyle of 3DShapesusingDeepMetric Learning", "author": ["Isaak Lim", "Anne Gehre", "Leif Kobbelt"], "venue": "Computer Graphics Forum,", "citeRegEx": "Lim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lim et al\\.", "year": 2016}, {"title": "Comparing biases for minimal network construction with back-propagation, volume 1", "author": ["Lorien Y Pratt"], "venue": null, "citeRegEx": "Pratt.,? \\Q1989\\E", "shortCiteRegEx": "Pratt.", "year": 1989}, {"title": "Faster R-CNN: Towards real-time object detection with region proposal networks", "author": ["Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["Mohammad Rastegari", "Vicente Ordonez", "Joseph Redmon", "Ali Farhadi"], "venue": null, "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "The perceptron, a perceiving and recognizing automaton Project Para", "author": ["Frank Rosenblatt"], "venue": "Cornell Aeronautical Laboratory,", "citeRegEx": "Rosenblatt.,? \\Q1957\\E", "shortCiteRegEx": "Rosenblatt.", "year": 1957}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["IlyaSutskever", "JamesMartens", "GeorgeEDahl", "Geoffrey E Hinton"], "venue": "ICML (3),", "citeRegEx": "IlyaSutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "IlyaSutskever et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}], "referenceMentions": [], "year": 2017, "abstractText": "Deep Residual Networks have reached the state of the art in many image processing tasks such image classification. However, the cost for a gain in accuracy in terms of depth and memory is prohibitive as it requires a higher number of residual blocks, up to double the initial value. To tackle this problem, we propose in this paper a way to reduce the redundant information of the networks. We share the weights of convolutional layers between residual blocks operating at the same spatial scale. The signal flows multiple times in the same convolutional layer. The resulting architecture, called ShaResNet, containsblockspecific layersandshared layers. These ShaResNet are trained exactly in the same fashion as the commonly used residual networks. We show, on the one hand, that they are almost as efficient as their sequential counterparts while involving less parameters, and on the other hand that they are more efficient than a residual network with the same number of parameters. For example, a 152-layer-deep residual network can be reduced to 106 convolutional layers, i.e. a parameter gain of 39%, while loosing less than 0.2% accuracy on ImageNet.", "creator": "LaTeX with hyperref package"}}}