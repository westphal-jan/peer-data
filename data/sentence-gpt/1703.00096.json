{"id": "1703.00096", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling", "abstract": "Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units. These methods suffer from two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 2) the decomposition of target sequences is fixed. These drawbacks usually result in sub-optimal performance of modeling sequences.\n\n\n\n\n\nThe problem is that the data available to build a new algorithm is often sparse (the average estimate for the number of basic units is just under 10 billion units). The problem is that it may not be enough to estimate the number of basic units needed to build a new algorithm, but it is probably enough to figure out how to do it. The problem is that the data available to build a new algorithm is often sparse (the average estimate for the number of basic units is just under 10 billion units). The problem is that it may not be enough to estimate the number of basic units needed to build a new algorithm, but it is probably enough to figure out how to do it. The problem is that the data available to build a new algorithm is often sparse (the average estimate for the number of basic units is just under 10 billion units). The problem is that it may not be enough to estimate the number of basic units needed to build a new algorithm, but it is probably enough to figure out how to do it. The problem is that the data available to build a new algorithm is often sparse (the average estimate for the number of basic units is just under 10 billion units). The problem is that it may not be enough to estimate the number of basic units needed to build a new algorithm, but it is probably enough to figure out how to do it. The problem is that the data available to build a new algorithm is often sparse (the average estimate for the number of basic units is just under 10 billion units). The problem is that it may not be enough to estimate the number of basic units needed to build a new algorithm, but it is probably enough to figure out how to do it. The problem is that it may not be enough to estimate the number of basic units needed to build a new algorithm, but it is probably enough to figure out how to do it. The problem is that it may not be enough to estimate the number of basic units needed to build a new algorithm, but it is probably enough to figure out how to do it. The problem is that it may not be enough to estimate", "histories": [["v1", "Wed, 1 Mar 2017 00:59:17 GMT  (1555kb,D)", "http://arxiv.org/abs/1703.00096v1", null], ["v2", "Sat, 12 Aug 2017 00:02:26 GMT  (1445kb,D)", "http://arxiv.org/abs/1703.00096v2", "Published at ICML 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["hairong liu", "zhenyao zhu", "xiangang li", "sanjeev satheesh"], "accepted": true, "id": "1703.00096"}, "pdf": {"name": "1703.00096.pdf", "metadata": {"source": "META", "title": "Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling", "authors": ["Hairong Liu", "Zhenyao Zhu", "Xiangang Li", "Sanjeev Satheesh"], "emails": ["LIUHAIRONG@BAIDU.COM", "ZHENYAOZHU@BAIDU.COM", "LIXIANGANG@BAIDU.COM", "SANJEEVSATHEESH@BAIDU.COM"], "sections": [{"heading": "1. Introduction", "text": "In recent years, there has been an explosion of interest in sequence labelling tasks. Connectionist Temporal Classification (CTC) loss (Graves et al., 2006) and Sequenceto-sequence (seq2seq) models (Cho et al., 2014; Sutskever et al., 2014) present powerful approaches to multiple applications, such as Automatic Speech Recognition (ASR)\nBaidu Silicon Valley AI Lab Tech Report \u2217Equal contribution.\n(Chan et al., 2016a; Hannun et al., 2014; Bahdanau et al., 2016), machine translation (S\u00e9bastien et al., 2015), and parsing (Vinyals et al., 2015). These methods are based on 1) a fixed and carefully chosen set of basic units, such as words (Sutskever et al., 2014), phonemes (Chorowski et al., 2015) or characters (Chan et al., 2016a), and 2) a fixed and pre-determined decomposition of target sequences into these basic units. While these two preconditions greatly simplify the problems, especially the training processes, they are also strict and unnecessary constraints, which usually lead to suboptimal solutions. CTC models are especially harmed by fixed basic units in target space, because they build on the independence assumption between successive outputs in that space - an assumption which is often violated in practice.\nThe problem with fixed set of basic units is obvious: it is really hard, if not impossible, to determine the optimal set of basic units beforehand. For example, in English ASR, if we use words as basic units, we will need to deal with the large vocabulary-sized softmax, as well as rare words and data sparsity problem. On the other hand, if we use characters as basic units, the model is forced to learn the complex rules of English spelling and pronunciation. For example, the \"oh\" sound can be spelled in any of following ways, depending on the word it occurs in - { o, oa, oe, ow, ough, eau, oo, ew }. While CTC can easily model commonly co-occuring grams together, it is impossible to give roughly equal probability to many possible spellings when transcribing unseen words. Most speech recognition systems model phonemes, sub-phoneme units and senones e.g, (Xiong et al., 2016a) to get around these problems. Similarly, state-of-the-art neural machine translation systems use pre-segmented word pieces e.g, (Wu et al., 2016a) aiming to find the best of both worlds.\nIn reality, groups of characters are typically cohesive units for many tasks. For the ASR task, words can be decomposed into groups of characters that can be associated with sound (such as \u2018tion\u2019 and \u2018eaux\u2019). For the machine translation task, there may be values in decomposing words as\nar X\niv :1\n70 3.\n00 09\n6v 1\n[ cs\n.C L\n] 1\nM ar\n2 01\nroot words and extensions (so that meaning may be shared explicitly between \u2018paternal\u2019 and \u2018paternity\u2019). Since this information is already available in the training data, it is perhaps, better to let the model figure it out by itself. At the same time, it raises another import question: how to decompose a target sequence into basic units? This is coupled with the problem of automatic selection of basic units, thus also better to let the model determine. Recently, there are some interesting attempts in these directions in the seq2seq framework. For example, Chan et al (Chan et al., 2016b) proposed the Latent Sequence Decomposition to decompose target sequences with variable length units as a function of both input sequence and the output sequence.\nIn this work, we propose Gram-CTC - a strictly more general version of CTC - to automatically seek the best set of basic units from the training data, called grams, and automatically decompose target sequences into sequences of grams. Just as sequence prediction with cross entropy training can be seen as special case of the CTC loss with a fixed alignment, CTC can be seen as a special case of Gram-CTC with a fixed decomposition of target sequences. Since it is a loss function, it can be applied to many seq2seq tasks to enable automatic selection of grams and decomposition of target sequences without modifying the underlying networks. Extensive experiments on multiple scales of data validate that Gram-CTC can improve CTC in terms of both performance and efficiency, and that using Gram-CTC the models outperform state-of-the-arts on standard speech benchmarks."}, {"heading": "2. Related Work", "text": "The basic text units that previous works utilized for text prediction tasks (e.g,, automatic speech recognition, handwriting recognition, machine translation and image captioning) can be generally divided into two categories: handcrafted ones and learning-based ones.\nHand-crafted Basic Units. Fixed sets of characters (graphemes) (Graves et al., 2006; Amodei et al., 2015), word-pieces (Wu et al., 2016b; Collobert et al., 2016; Zweig et al., 2016a), words (Soltau et al., 2016; S\u00e9bastien et al., 2015), and phonemes (Lee and Hon, 1988; Sercu and Goel, 2016; Xiong et al., 2016b) have been widely used as basic units for text prediction, but all of them have drawbacks.\n\u2022 Word-segmented models remove the component of learning to spell and thus enable direct optimization towards reducing Word Error Rate (WER). However, these models suffer from having to handle a large vocabulary (1.7 million in (Soltau et al., 2016)), out-ofvocabulary words (Soltau et al., 2016; S\u00e9bastien et al., 2015) and data sparsity problems (Soltau et al., 2016).\n\u2022 Using characters results in much smaller vocabularies (e.g, 26 for English and thousands for Chinese),\nbut it requires much longer contexts compared to using words or word-pieces and poses the challenge of composing characters to words (Graves et al., 2006; Chan et al., 2015), which is very noisy for languages like English.\n\u2022 Word-pieces lie at the middle-ground of words and characters, providing a good trade-off between vocabulary size and context size, while the performance of using word pieces is sensitive to the choice of the word-piece set and its decomposition.\n\u2022 For the ASR task, the use of phonemes was popular in the past few decades as it eases acoustic modeling (Lee and Hon, 1988) and good results were reported with phonemic models (Xiong et al., 2016b; Sercu and Goel, 2016). However, it introduces the uncertainties of mapping phonemes to words during decoding (Doss et al., 2003), which becomes less robust especially for accented speech data.\nUsing these aforementioned fixed deterministic decompositions of text sequences defines a prior, which is not necessarily optimal for end-to-end learning.\nLearning-based Basic Units. More recently, attempts have been made to learn basic unit sets along with the selection and decomposition of them automatically. (Chung et al., 2016) leveraged a multiscale RNN architecture to model the characters and words hierarchically, by building word-level representations on top of character-level representations. (Luong and Manning, 2016) proposed a hybrid Word-Character model what translates mostly at the word level and consults the character components for rare words. Chan et al (Chan et al., 2016b) proposed the Latent Sequence Decompositions framework to decomposes target sequences with variable length-ed basic units as a function of both input sequence and the output sequence."}, {"heading": "3. Gram-CTC", "text": "In this section, we will describe the proposed Gram-CTC algorithm. Since it is very similar to the CTC criteria (Graves et al., 2006), we put our emphasis on the differences between them."}, {"heading": "3.1. Probability of a Target Sequence", "text": "Let G be a set of n-grams of the set of basic units C of the target sequence, and \u03c4 be the length of the longest gram in G. A Gram-CTC network has a softmax output layer with |G| + 1 units, that is, the probability over all grams in G and one additional symbol, blank.\nTo simplify the problem, we also assume C \u2286 G. This is because there may be no valid decompositions for some target sequences if C 6\u2286 G. Since Gram-CTC will figure out the ideal decomposition of target sequences into grams during training, this condition is harmless, and only guaran-\ntees that there is at least one valid decomposition for every target sequence.\nFor an input sequence x of length T , let y = Nw(x) be the sequence of network outputs, and denote by ytk as the probability of the k-th gram at time t, where k is the index of grams in G\u2032 = G \u222a {blank}, then we have\np(\u03c0|x) = T\u220f t=1 yt\u03c0t ,\u2200\u03c0 \u2208 G \u2032T (1)\nJust as in the case of CTC, here we refer to the elements of G\u2032\nT as paths, and denote them by \u03c0. The difference is that for each word in the target sequence, it may be decomposed into different sequences of grams. For example, the word hello can only be decomposed into the sequence [\u2018h\u2019, \u2018e\u2019, \u2018l\u2019, \u2018l\u2019, \u2018o\u2019] for CTC (assume uni-gram CTC here), but it also can be decomposed into the sequence [\u2018he\u2019, \u2018ll\u2019, \u2018o\u2019] if \u2019he\u2019 and \u2019ll\u2019 are in G.\nFor each \u03c0, we map it into a target sequence the same way as CTC using the collapsing function that 1) removes all repeated labels from the path and then 2) removes all blanks. Note that essentially it is these rules which determine the transitions between the states of adjacent time steps in Figure 1. This is a many-to-one mapping and we denote it by B. Thus, for a target sequence l, B\u22121(l) represents all paths mapped to l. Then, we have\np(l|x) = \u2211\n\u03c0\u2208B\u22121(l)\np(\u03c0|x) (2)\nThis equation allows us train sequence labeling models without any alignment information using CTC loss, be-\ncause it marginalizes over all possible alignments during training. However, in order to maximize the p(l|x), the model learns to concentrate all the probability mass on one path, such that Equation 2 may be approximated as\np(l|x) \u2248 max \u03c0\u2208B\u22121(l) p(\u03c0|x) (3)\nThis selection of a single path, enables easy approximate decoding of CTC trained models. Gram-CTC uses the same effect to enable the model to select the most likely path, which defines in this case, not only an alignment, but also a decomposition of the target sequence.\nNote that for each target sequence l, the set B\u22121(l) has O(\u03c42) more paths than it does in vanilla CTC. This is because there are O(\u03c4) times more valid states per time step, and each state may have a valid transition from O(\u03c4) states in the previous time step. The original CTC method is thus, a special case of Gram-CTC when G = C and \u03c4 = 1. While the quadratic increase in the complexity of the algorithm is non trivial, we assert that it is a trivial increase in the overall training time of typical neural networks, where the computation time is dominated by the neural networks themselves. Additionally, the algorithm extends generally to any arbitrary G and need not have all possible n-grams up to length \u03c4 ."}, {"heading": "3.2. The Forward-Backward Algorithm", "text": "To efficiently compute p(l|x), we also adopt the dynamic programming algorithm. The essence of all dynamic programming is in identifying the states of the problem, so that we may solve future states by reusing solutions to earlier\nstates. In our case, the state must contain all the information required to identify all valid extensions of an incomplete path \u03c0 such that the collapsing function will eventually collapse the complete \u03c0 back to l. For Gram-CTC, this can be done by collapsing all but the last element of the path \u03c0. Therefore, the state is a tuple (l1:i, j), where the first item is a collapsed path, representing a prefix of the target label sequence, and j \u2208 {0, . . . , \u03c4} is the length of the last gram (li\u2212j+1:i) used for making the prefix. j = 0 is valid and means that blank was used. We denote the gram li\u2212j+1:i by g j i (l), and the state (l1:i, j) as s j i (l). For readability, we will further shorten sji (l) to s j i and g j i (l) to gji . For a state s, the positions of its first character and last character in l are denoted by b(s) and e(s), respectively.\nFigure 1 illustrates partially the dynamic programming process for the target sequence \u2018CAT\u2019. Here we suppose G contains all possible uni-grams and bi-grams. Thus, for each character in \u2018CAT\u2019, there are three possible states associated with it: 1) the current character, 2) the bi-gram ending in current character, and 3) the blank after current character. There is also one blank at beginning. In total we have 10 states.\nSuppose the maximum length of grams in G is \u03c4 , we first scan l to get the set S of all possible states, such that for all sji \u2208 S, its corresponding g j i \u2208 G\u2032. i \u2208 {0, . . . , |l|} and j \u2208 {0, . . . , \u03c4}.\nFor a target sequence l, define the forward variable \u03b1t(s) for any s \u2208 S to the total probability of all valid paths prefixes that end at state s at time t.\n\u03b1t(s) def = \u2211 \u03c0|B(\u03c01:t)=l1:e(s),\u03c0t=s t\u220f t\u2032=1 yt \u2032 \u03c0t\u2032 (4)\nFollowing this definition, we have the following rules for initialization\n\u03b11(s) =  y1b s = s 0 0 y1 sii\ns = sii \u2200i \u2208 {1, . . . , \u03c4} 0 otherwise\n(5)\nand recursion\n\u03b1t(s) =  \u03b1\u0302it\u22121 \u2217 ytb when s = s0i , [\u03b1\u0302i\u2212jt\u22121 + \u03b1t\u22121(s)] \u2217 ytsji when s = sji and s j i 6= s j i\u2212j , [\u03b1\u0302i\u2212jt\u22121 + \u03b1t\u22121(s)\u2212 \u03b1t\u22121(s j i\u2212j)] \u2217 ytsji\nwhen s = sji and s j i = s j i\u2212j\n(6) where \u03b1\u0302it = \u2211\u03c4 j=0 \u03b1t(s j i ) and y t b is the probability of blank at time t.\nThe total probability of the target sequence l is then ex-\npressed in the following way:\np(l|x) = \u03c4\u2211 j=0 \u03b1T (s j |l|) (7)\nsimilarly, we can define the backward variable \u03b2t(s) as:\n\u03b2t(s) def = \u2211 \u03c0|B(\u03c0t:T )=lb(s):l,\u03c0t=s T\u220f t\u2032=t yt \u2032 \u03c0t\u2032 (8)\nFor the initialization and recursion of \u03b2t(s), we have\n\u03b2T (s) =  yTb s = s 0 T yT siT\ns = siT \u2200i \u2208 {1, . . . , \u03c4} 0 otherwise\n(9)\nand\n\u03b2t(s) =  \u03b2\u0302it+1 \u2217 ytb when s = s0i , [\u03b2\u0302i+jt+1 + \u03b2t+1(s)] \u2217 ytsji when s = sji and s j i 6= s j i+j , [\u03b2\u0302i+jt+1 + \u03b2t+1(s)\u2212 \u03b2t+1(s j i+j)] \u2217 ytsji\nwhen s = sji and s j i = s j i+j\n(10) where \u03b2\u0302it = \u2211\u03c4 j=0 \u03b2t(s j i+j)"}, {"heading": "3.3. BackPropagation", "text": "Similar to CTC, we have the following expression:\np(l|x) = \u2211 s\u2208S \u03b1t(s)\u03b2t(s) yts \u2200t \u2208 {1, . . . ,T} (11)\nThe derivative with regards to ytk is:\n\u2202p(l|x) \u2202ytk = 1 ytk 2 \u2211 s\u2208lab(l,k) \u03b1t(s)\u03b2t(s) (12)\nwhere lab(l, k) is the set of states in S whose corresponding gram is k. This is because there may be multiple states corresponding to the same gram.\nFor the backpropagation, the most important formula is the partial derivative of loss with regard to the unnormalized output utk.\n\u2202 ln p(l|x) \u2202utk = ytk \u2212 1 ytkZt \u2211 s\u2208lab(l,k) \u03b1t(s)\u03b2t(s) (13)\nwhere Zt def = \u2211 s\u2208S\n\u03b1t(s)\u03b2t(s) ytsg\n(a) Training curves before (blue) and after (orange) auto-refinement of grams. (b) Training curves without (blue) and with (orange) joint-training\nGram-CTC C _ AT\nCTC _ C _ A T _\nGram-CTC C - AT\nCTC - C - A T -\n(c) Joint-training Architecture\nFigure 2: (Figure 2a) compares the training curves before (blue) and after (orange) auto-refinement of grams. They look very similar, although the number of grams is greatly reduced after refinement, which makes training faster and potentially more robust due to less gram sparsity. Figure (2b) Training curve of model with and without joint-training. The model corresponding to the orange training curve is jointly trained together with vanilla CTC, such models are often more stable during training. Figure (2c) Typical joint-training model architecture - vanilla CTC loss is best applied a few levels lower than the Gram-CTC loss."}, {"heading": "4. Methodology", "text": "Here we describe additional techniques we found useful in practice to enable the Gram-CTC to work efficiently as well as effectively."}, {"heading": "4.1. Iterative Gram Selection", "text": "Although Gram-CTC can automatically select useful grams, it is challenging to train with a large G. The total number of possible grams is usually huge. For example, in English, we have 26 characters, then the total number of bi-grams is 262 = 676, the total number of tri-grams are 263 = 17576, . . . , which grows exponentially and quickly becomes intractable. However, it is unnecessary to consider many grams, such as \u2018aaaa\u2019, which are obviously useless.\nIn our experiments, we first eliminate most of useless grams from the statistics of a huge corpus, that is, we count the frequency of each gram in the corpus and drop these grams with rare frequencies. Then, we train a model with Gram-CTC on all the remaining grams. By applying (decoding) the trained model on a large speech dataset, we get the real statistics of gram\u2019s usage. Ultimately, we choose high frequency grams together with all uni-grams as our final gram set G. Table 1 shows the impact of iterative gram selection on WSJ (without LM). Figure 2a shows its corresponding training curve. For details, please refer to Section 5.2."}, {"heading": "4.2. Joint Training with Vanilla CTC", "text": "Gram-CTC needs to solve both decomposition and alignment tasks, which is a harder task for a model to learn than CTC. This is often manifested in unstable training curves,\nforcing us to lower the learning rate which in turn results in models converging to a worse optima. To overcome this difficulty, we found it beneficial to train a model with both the Gram-CTC, as well as the vanilla CTC loss (similar to joint-training CTC together with CE loss as mentioned in (Sak et al., 2015)). Joint training of multiple objectives for sequence labelling has also been explored in previous works (Kim et al., 2016; Kim and Rush, 2016).\nA typical joint-training model looks like Figure 2c, and the corresponding training curve is shown in Figure 2b. The effect of joint-training are shown in Table 4 and Table 5 in the experiments."}, {"heading": "5. Experiments", "text": "We test the Gram-CTC loss on the ASR task, while both CTC and the introduced Gram-CTC are generic techniques for other sequence labelling tasks. For all of the experiments, the model specification and training procedure are the same as in (Amodei et al., 2015) - The model is a recurrent neural network (RNN) with 2 two-dimensional convolutional input layers, followed by K forward (Fwd) or bidirectional (Bidi) Gated Recurrent layers, N cells each, and one fully connected layer before a softmax layer. In short hand, such a model is written as \u20182x2D Conv - KxN GRU\u2019. The network is trained end-to-end with the CTC, GramCTC or a weighted combination of both. This combination is described in the earlier section.\nIn all experiments, audio data is is sampled at 16kHz. Linear FFT features are extracted with a hop size of 10ms and window size of 20ms, and are normalized so that each input feature has zero mean and unit variance. The network inputs are thus spectral magnitude maps ranging\nfrom 0-8kHz with 161 features per 10ms frame. At each epoch, 40% of the utterances are randomly selected to add background noise to. The optimization method we use is stochastic gradient descent with Nesterov momentum. Learning hyperparameters (batch-size, learning-rate, momentum, and etc.) vary across different datasets and are tuned for each model by optimizing a hold-out set. Typical values are a learning rate of 10\u22123 and momentum of 0.99."}, {"heading": "5.1. Data and Setup", "text": "Wall Street Journal (WSJ). This corpora consists primarily of read speech with texts drawn from a machinereadable corpus of Wall Street Journal news text, and contains about 80 hours speech data. We used the standard configuration of train si284 dataset for training, dev93 for validation and eval92 for testing. This is a relatively \u2018clean\u2019 task and often used for model prototyping (Miao et al., 2015; Bahdanau et al., 2016; Zhang et al., 2016; Chan et al., 2016b).\nFisher-Switchboard. This is a commonly used English conversational telephone speech (CTS) corpora, which contains 2300 hours CTS data. Following the previous works (Zweig et al., 2016b; Povey et al., 2016; Xiong et al., 2016b; Sercu and Goel, 2016), evaluation is carried out on the NIST 2000 CTS test set, which comprises both Switchboard (SWB) and CallHome (CH) subsets.\n10K Speech Dataset. We conduct large scale ASR experiments on a noisy internal dataset of 10,000 hours. This dataset contains speech collected from various scenarios, such as different background noises, far-field, different accents, and so on. Due to its inherent complexities, it is a very challenging task, and can thus validate the effectiveness of the proposed method for real-world application."}, {"heading": "5.2. Gram Selection", "text": "We employ the WSJ dataset for demonstrating different strategies of selecting grams for Gram-CTC, since it is a widely used dataset and also small enough for rapid idea verification. However, because it is small, we cannot use large grams here due to data sparsity problem. Thus, the auto-refined gram set on WSJ is not optimal for other larger datasets, where larger grams could be effectively used, but\nthe procedure of refinement is the same for them.\nWe first train a model using all uni-grams and bi-grams (29 uni-grams and 262 = 676 bi-grams, in total 705 grams), and then do decoding with the obtained model on another speech dataset to get the statistics of the usage of grams. Top 100 bi-grams together with all 29 uni-grams (autorefined grams) are used for the second round of training. For comparison, we also present the result of the best handpicked grams, as well as the results on uni-grams. All the results are shown in Table 1.\nSome interesting observations can be found in Table 1. First, the performance of auto-refined grams is only slightly better than the combination of all uni-grams and all bigrams. This is probably because WSJ is so small that gram learning suffers from the data sparsity problem here (similar to word-segmented models). The auto-refined gram set contains only a small subset of bi-grams, thus more robust. This is also why we only try bi-grams, not including higher-order grams. Second, the performance of best handpicked grams is worse than auto-refined grams. This is desirable. It is time-consuming to handpick grams, especially when you consider high-order grams. The method of iterative gram selection is not only fast, but usually better. Third, the performance of Gram-CTC on auto-refined grams is only slightly better than CTC on uni-grams. This is because Gram-CTC is inherently difficult to train, since it needs to learn both decomposition and alignment. WSJ is too small to provide enough data to train Gram-CTC."}, {"heading": "5.3. Sequence Labelling in Large Stride", "text": "Using a large time stride for sequence labelling with RNNs can greatly boost the overall computation efficiency, since it effectively reduces the time steps for recurrent computation, thus speeds up the process of both forward inference and backward propagation. However, the largest stride that can be used is limited by the gram set we use. The (unigram) CTC has to work in a high time resolution (small stride) in order to have enough number of frames to output every character. This is very inefficient as we know the same acoustic feature could correspond to several grams of different lengths (e.g., {\u2019i\u2019, \u2019igh\u2019, \u2019eye\u2019}) . The larger the grams are, the larger stride we are potentially able to use.\nDS2 (Amodei et al., 2015) employed non-overlapping bigram outputs to allow for a larger stride. This imposes an\nartificial constraint forcing the model to learn, not only the spelling of each word, but also how to split words into bigrams. For example, part is split as [pa, rt] but the word apart is forced to be decomposed as [ap, ar, t]. GramCTC removes this constraint by allowing the model to decompose words into larger units into the most convenient or sensible decomposition. Comparison results show this change enables Gram-CTC to work much better than bigram CTC, as in Table 2.\nIn Table 2, we compare the performance of trained model and training efficiency on two strides, 2 and 4. For GramCTC, we use the auto-refined gram set from previous section. As expected, using stride 4 almost cuts the training time per epoch into half, compared to stride 2. From stride 2 to stride 4, the performance of uni-gram CTC drops quickly. This is because small grams inherently need higher time resolutions. As for Gram-CTC, from stride 2 to stride 4, its performance decreases a little bit, while in experiments on the other datasets, Gram-CTC constantly works better in stride 4. One possible explanation is that WSJ is too small for Gram-CTC to learn large grams well. In contrast, the performance of bi-gram CTC is not as good as that of Gram-CTC in either stride."}, {"heading": "5.4. Decoding Examples", "text": "Figure 3 illustrates the max-decoding results of both CTC and Gram-CTC on nine utterances. Here the label set for CTC is the set of all characters, and the label set for GramCTC is an auto-refined gram set containing all uni-grams and some high-frequency high-order grams. Here GramCTC uses stride 4 while CTC uses stride 2.\nFrom Figure 3, we can find that: 1) Gram-CTC does automatically find many intuitive and meaningful grams, such as \u2018the\u2019, \u2018ng\u2019, and \u2018are\u2019. 2) It also decomposes the sentences into segments which are meaningful in term of pronunciation. This decomposition resembles the phonetic decomposition, but in larger granularity and arguably more natural. 3) Since Gram-CTC predicts a chunk of characters (a gram) each time, each prediction utilizes larger context and these characters in the same predicted chunk are dependent, thus potentially more robust. One example is the word \u2018will\u2019 in the last sentence in Figure 3. 4) Since the output of network is the probability over all grams, the decoding process is almost the same as CTC, still end-toend. This makes such decomposition superior to phonetic decomposition. In summary, Gram-CTC combines the advantages of both CTC on characters and CTC on phonemes."}, {"heading": "5.5. Comparison with Other Methods", "text": ""}, {"heading": "5.5.1. WSJ DATASET", "text": "The model used here is [2x2D conv, 3x1280 Bidi GRU] with a CTC or Gram-CTC loss. The results are shown in Table 3. For all models we trained, language model can greatly improve their performances, in term of WER. Though this dataset contains very limited amount of text data for learning gram selection and decomposition, GramCTC can still improve the vanilla CTC notably."}, {"heading": "5.5.2. FISHER-SWITCHBOARD", "text": "The acoustic model trained here is composed of two 2D convolutions and six bi-directional GRU layer in 2048 dimension. The corresponding labels are used for training N-gram language models.\n\u2022 Switchboard English speech 97S62 \u2022 Fisher English speech Part 1 - 2004S13, 2004T19 \u2022 Fisher English speech Part 2 - 2005S13, 2005T19\nWe use a sample of the Switchboard-1 portion of the NIST 2002 dataset (2004S11 RT-02) for tuning language model hyper-parameters. The evaluation is done on the NIST 2000 set. This configuration forms a standard benchmark for evaluating ASR models. Results are in Table 4.\nWe compare our model against best published results on in-domain data. These results can often be improved using out-of-domain data for training the language model, and sometimes the acoustic model as well. Together these techniques allow (Xiong et al., 2016b) to reach a WER of 5.9 on the SWBD set."}, {"heading": "5.5.3. 10K SPEECH DATASET", "text": "Finally, we experiment on a large noisy dataset collected by ourself for building large-vocabulary Continuous Speech Recognition (LVCSR) systems. This dataset contains about 10000 hours speech in a diversity of scenarios, such as farfield, background noises, accents. In all cases, the model\nis [2x2D Conv, 3x2560 Fwd GRU, LA Conv] with only a change in the loss function. \u2018LA Conv\u2019 refers to a look ahead convolution layer as seen in (Amodei et al., 2015) which works together with forward-only RNNs for deployment purpose.\nAs with the Fisher-Switchboard dataset, the optimal stride is 4 for Gram-CTC and 2 for vanilla CTC on this dataset. Thus, in both experiments, both Gram-CTC and vanilla CTC + Gram-CTC are trained mush faster than vanilla CTC itself. The result is shown in Table 5. Gram-CTC performs better than CTC. After joint-training with vanilla CTC and alignment information through a CE loss, its performance is further boosted, which verifies joint-training helps training. In fact, with only a small additional cost of time, it effectively reduces the WER from 27.56% to 25.59% (without language model)."}, {"heading": "6. Conclusions and Future Work", "text": "In this paper, we have proposed the Gram-CTC loss to enable automatic decomposition of target sequences into learned grams. We also present techniques to train the Gram-CTC in a clean and stable way. Our extensive experiments demonstrate the proposed Gram-CTC enables the models to run more efficiently than the vanilla CTC, by using larger stride, while obtaining better performance of sequence labelling. Comparison experiments on multiplescale datasets show the proposed Gram-CTC obtains stateof-the-art results on various ASR tasks. We will continue investigating techniques of improving the optimization of Gram-CTC loss, as well as the applications of Gram-CTC on other sequence labelling tasks."}], "references": [{"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["William Chan", "Navdeep Jaitly", "Quoc Le", "Oriol Vinyals"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Chan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2016}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Awni Y. Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates", "Andrew Y. Ng"], "venue": "CoRR, abs/1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Yoshua Bengio"], "venue": "In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Bahdanau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2016}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean S\u00e9bastien", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": null, "citeRegEx": "S\u00e9bastien et al\\.,? \\Q2015\\E", "shortCiteRegEx": "S\u00e9bastien et al\\.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Attention-based models for speech recognition", "author": ["Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "The microsoft 2016 conversational speech recognition system", "author": ["W Xiong", "J Droppo", "X Huang", "F Seide", "M Seltzer", "A Stolcke", "D Yu", "G Zweig"], "venue": "arXiv preprint arXiv:1609.03528,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Latent sequence decompositions", "author": ["William Chan", "Yu Zhang", "Quoc Le", "Navdeep Jaitly"], "venue": "In Arxiv,", "citeRegEx": "Chan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2016}, {"title": "Wav2letter: an end-to-end convnet-based speech recognition system", "author": ["Ronan Collobert", "Christian Puhrsch", "Gabriel Synnaeve"], "venue": "arXiv preprint arXiv:1609.03193,", "citeRegEx": "Collobert et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2016}, {"title": "Advances in all-neural speech recognition", "author": ["Geoffrey Zweig", "Chengzhu Yu", "Jasha Droppo", "Andreas Stolcke"], "venue": "arXiv preprint arXiv:1609.05935,", "citeRegEx": "Zweig et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zweig et al\\.", "year": 2016}, {"title": "Neural speech recognizer: Acoustic-to-word lstm model for large vocabulary speech recognition", "author": ["Hagen Soltau", "Hank Liao", "Hasim Sak"], "venue": "arXiv preprint arXiv:1610.09975,", "citeRegEx": "Soltau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Soltau et al\\.", "year": 2016}, {"title": "Large-vocabulary speakerindependent continuous speech recognition using hmm", "author": ["K-F Lee", "H-W Hon"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Lee and Hon.,? \\Q1988\\E", "shortCiteRegEx": "Lee and Hon.", "year": 1988}, {"title": "Dense prediction on sequences with time-dilated convolutions for speech recognition", "author": ["Tom Sercu", "Vaibhava Goel"], "venue": "arXiv preprint arXiv:1611.09288,", "citeRegEx": "Sercu and Goel.,? \\Q2016\\E", "shortCiteRegEx": "Sercu and Goel.", "year": 2016}, {"title": "Achieving human parity in conversational speech recognition", "author": ["Wayne Xiong", "Jasha Droppo", "Xuedong Huang", "Frank Seide", "Mike Seltzer", "Andreas Stolcke", "Dong Yu", "Geoffrey Zweig"], "venue": "arXiv preprint arXiv:1610.05256,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Phoneme-grapheme based speech recognition system", "author": ["Mathew Magimai Doss", "Todd A Stephenson", "Herv\u00e9 Bourlard", "Samy Bengio"], "venue": "In Automatic Speech Recognition and Understanding,", "citeRegEx": "Doss et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Doss et al\\.", "year": 2003}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.01704,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1604.00788,", "citeRegEx": "Luong and Manning.,? \\Q2016\\E", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Hasim Sak", "Andrew W. Senior", "Kanishka Rao", "Fran\u00c3\u011foise Beaufays"], "venue": "CoRR, abs/1507.06947,", "citeRegEx": "Sak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2015}, {"title": "Joint ctc-attention based end-to-end speech recognition using multi-task learning", "author": ["Suyoun Kim", "Takaaki Hori", "Shinji Watanabe"], "venue": "arXiv preprint arXiv:1609.06773,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Sequence-level knowledge distillation", "author": ["Yoon Kim", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1606.07947,", "citeRegEx": "Kim and Rush.,? \\Q2016\\E", "shortCiteRegEx": "Kim and Rush.", "year": 2016}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Yajie Miao", "Mohammad Gowayyed", "Florian Metze"], "venue": "In Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Miao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for end-to-end speech recognition", "author": ["Yu Zhang", "William Chan", "Navdeep Jaitly"], "venue": "arXiv preprint arXiv:1610.03022,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Advances in all-neural speech recognition", "author": ["Geoffery Zweig", "Ghengzhu Yu", "Jasha Droppo", "Andreas Stolcke"], "venue": "arXiv preprint arXiv:1609.05935,", "citeRegEx": "Zweig et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zweig et al\\.", "year": 2016}, {"title": "Purely sequence-trained neural networks for asr based on lattice-free mmi", "author": ["Daniel Povey", "Vijayaditya Peddinti", "Daniel Galvez", "Pegah Ghahrmani", "Vimal Manohar", "Xingyu Na", "Yiming Wang", "Sanjeev Khudanpur"], "venue": null, "citeRegEx": "Povey et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": ", 2006) and Sequenceto-sequence (seq2seq) models (Cho et al., 2014; Sutskever et al., 2014) present powerful approaches to multiple applications, such as Automatic Speech Recognition (ASR)", "startOffset": 49, "endOffset": 91}, {"referenceID": 1, "context": ", 2006) and Sequenceto-sequence (seq2seq) models (Cho et al., 2014; Sutskever et al., 2014) present powerful approaches to multiple applications, such as Automatic Speech Recognition (ASR)", "startOffset": 49, "endOffset": 91}, {"referenceID": 3, "context": "(Chan et al., 2016a; Hannun et al., 2014; Bahdanau et al., 2016), machine translation (S\u00e9bastien et al.", "startOffset": 0, "endOffset": 64}, {"referenceID": 4, "context": "(Chan et al., 2016a; Hannun et al., 2014; Bahdanau et al., 2016), machine translation (S\u00e9bastien et al.", "startOffset": 0, "endOffset": 64}, {"referenceID": 5, "context": ", 2016), machine translation (S\u00e9bastien et al., 2015), and parsing (Vinyals et al.", "startOffset": 29, "endOffset": 53}, {"referenceID": 6, "context": ", 2015), and parsing (Vinyals et al., 2015).", "startOffset": 21, "endOffset": 43}, {"referenceID": 1, "context": "These methods are based on 1) a fixed and carefully chosen set of basic units, such as words (Sutskever et al., 2014), phonemes (Chorowski et al.", "startOffset": 93, "endOffset": 117}, {"referenceID": 7, "context": ", 2014), phonemes (Chorowski et al., 2015) or characters (Chan et al.", "startOffset": 18, "endOffset": 42}, {"referenceID": 10, "context": ", 2015), word-pieces (Wu et al., 2016b; Collobert et al., 2016; Zweig et al., 2016a), words (Soltau et al.", "startOffset": 21, "endOffset": 84}, {"referenceID": 12, "context": ", 2016a), words (Soltau et al., 2016; S\u00e9bastien et al., 2015), and phonemes (Lee and Hon, 1988; Sercu and Goel, 2016; Xiong et al.", "startOffset": 16, "endOffset": 61}, {"referenceID": 5, "context": ", 2016a), words (Soltau et al., 2016; S\u00e9bastien et al., 2015), and phonemes (Lee and Hon, 1988; Sercu and Goel, 2016; Xiong et al.", "startOffset": 16, "endOffset": 61}, {"referenceID": 13, "context": ", 2015), and phonemes (Lee and Hon, 1988; Sercu and Goel, 2016; Xiong et al., 2016b) have been widely used as basic units for text prediction, but all of them have drawbacks.", "startOffset": 22, "endOffset": 84}, {"referenceID": 14, "context": ", 2015), and phonemes (Lee and Hon, 1988; Sercu and Goel, 2016; Xiong et al., 2016b) have been widely used as basic units for text prediction, but all of them have drawbacks.", "startOffset": 22, "endOffset": 84}, {"referenceID": 12, "context": "7 million in (Soltau et al., 2016)), out-ofvocabulary words (Soltau et al.", "startOffset": 13, "endOffset": 34}, {"referenceID": 12, "context": ", 2016)), out-ofvocabulary words (Soltau et al., 2016; S\u00e9bastien et al., 2015) and data sparsity problems (Soltau et al.", "startOffset": 33, "endOffset": 78}, {"referenceID": 5, "context": ", 2016)), out-ofvocabulary words (Soltau et al., 2016; S\u00e9bastien et al., 2015) and data sparsity problems (Soltau et al.", "startOffset": 33, "endOffset": 78}, {"referenceID": 12, "context": ", 2015) and data sparsity problems (Soltau et al., 2016).", "startOffset": 35, "endOffset": 56}, {"referenceID": 16, "context": "g, 26 for English and thousands for Chinese), but it requires much longer contexts compared to using words or word-pieces and poses the challenge of composing characters to words (Graves et al., 2006; Chan et al., 2015), which is very noisy for languages like English.", "startOffset": 179, "endOffset": 219}, {"referenceID": 13, "context": "\u2022 For the ASR task, the use of phonemes was popular in the past few decades as it eases acoustic modeling (Lee and Hon, 1988) and good results were reported with phonemic models (Xiong et al.", "startOffset": 106, "endOffset": 125}, {"referenceID": 14, "context": "\u2022 For the ASR task, the use of phonemes was popular in the past few decades as it eases acoustic modeling (Lee and Hon, 1988) and good results were reported with phonemic models (Xiong et al., 2016b; Sercu and Goel, 2016).", "startOffset": 178, "endOffset": 221}, {"referenceID": 17, "context": "However, it introduces the uncertainties of mapping phonemes to words during decoding (Doss et al., 2003), which becomes less robust especially for accented speech data.", "startOffset": 86, "endOffset": 105}, {"referenceID": 18, "context": "(Chung et al., 2016) leveraged a multiscale RNN architecture to model the characters and words hierarchically, by building word-level representations on top of character-level representations.", "startOffset": 0, "endOffset": 20}, {"referenceID": 19, "context": "(Luong and Manning, 2016) proposed a hybrid Word-Character model what translates mostly at the word level and consults the character components for rare words.", "startOffset": 0, "endOffset": 25}, {"referenceID": 20, "context": "To overcome this difficulty, we found it beneficial to train a model with both the Gram-CTC, as well as the vanilla CTC loss (similar to joint-training CTC together with CE loss as mentioned in (Sak et al., 2015)).", "startOffset": 194, "endOffset": 212}, {"referenceID": 21, "context": "Joint training of multiple objectives for sequence labelling has also been explored in previous works (Kim et al., 2016; Kim and Rush, 2016).", "startOffset": 102, "endOffset": 140}, {"referenceID": 22, "context": "Joint training of multiple objectives for sequence labelling has also been explored in previous works (Kim et al., 2016; Kim and Rush, 2016).", "startOffset": 102, "endOffset": 140}, {"referenceID": 23, "context": "This is a relatively \u2018clean\u2019 task and often used for model prototyping (Miao et al., 2015; Bahdanau et al., 2016; Zhang et al., 2016; Chan et al., 2016b).", "startOffset": 71, "endOffset": 153}, {"referenceID": 4, "context": "This is a relatively \u2018clean\u2019 task and often used for model prototyping (Miao et al., 2015; Bahdanau et al., 2016; Zhang et al., 2016; Chan et al., 2016b).", "startOffset": 71, "endOffset": 153}, {"referenceID": 24, "context": "This is a relatively \u2018clean\u2019 task and often used for model prototyping (Miao et al., 2015; Bahdanau et al., 2016; Zhang et al., 2016; Chan et al., 2016b).", "startOffset": 71, "endOffset": 153}, {"referenceID": 26, "context": "Following the previous works (Zweig et al., 2016b; Povey et al., 2016; Xiong et al., 2016b; Sercu and Goel, 2016), evaluation is carried out on the NIST 2000 CTS test set, which comprises both Switchboard (SWB) and CallHome (CH) subsets.", "startOffset": 29, "endOffset": 113}, {"referenceID": 14, "context": "Following the previous works (Zweig et al., 2016b; Povey et al., 2016; Xiong et al., 2016b; Sercu and Goel, 2016), evaluation is carried out on the NIST 2000 CTS test set, which comprises both Switchboard (SWB) and CallHome (CH) subsets.", "startOffset": 29, "endOffset": 113}, {"referenceID": 23, "context": "Phoneme CTC + trigram LM (Miao et al., 2015) 7.", "startOffset": 25, "endOffset": 44}, {"referenceID": 23, "context": "3 Grapheme CTC + trigram LM (Miao et al., 2015) 9.", "startOffset": 28, "endOffset": 47}, {"referenceID": 4, "context": "0 Attention + trigram LM (Bahdanau et al., 2016) 9.", "startOffset": 25, "endOffset": 48}, {"referenceID": 24, "context": "3 DeepConv LAS + no LM (Zhang et al., 2016) 10.", "startOffset": 23, "endOffset": 43}, {"referenceID": 26, "context": "7 BLSTM + LF MMI (Povey et al., 2016) 8.", "startOffset": 17, "endOffset": 37}, {"referenceID": 14, "context": "8 Dilated convolutions (Sercu and Goel, 2016) 7.", "startOffset": 23, "endOffset": 45}], "year": 2017, "abstractText": "Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units. These methods suffer from two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 2) the decomposition of target sequences is fixed. These drawbacks usually result in sub-optimal performance of modeling sequences. In this paper, we extend the popular CTC loss criterion to alleviate these limitations, and propose a new loss function called Gram-CTC. While preserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomposition of target sequences. Unlike CTC, Gram-CTC allows the model to output variable number of characters at each time step, which enables the model to capture longer term dependency and improves the computational efficiency. We demonstrate that the proposed Gram-CTC improves CTC in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data, and that with Gram-CTC we can outperform the state-of-the-art on a standard speech benchmark.", "creator": "LaTeX with hyperref package"}}}