{"id": "1703.00144", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank", "abstract": "Recently low displacement rank (LDR) matrices, or so-called structured matrices, have been proposed to compress large-scale neural networks. Empirical results have shown that neural networks with weight matrices of LDR matrices, referred as LDR neural networks, can achieve significant reduction in space and computational complexity while retaining high accuracy. We formally study LDR matrices in deep learning. First, we prove the universal approximation property of LDR neural networks with a mild condition on the displacement operators. We then show that the error bounds of LDR neural networks are as efficient as general neural networks with both single-layer and multiple-layer structure. Finally, we propose back-propagation based training algorithm for general LDR neural networks. To improve this, we describe an initial estimation of LDR neural networks. We first test that general LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of capacity on LDR neural networks with loss of", "histories": [["v1", "Wed, 1 Mar 2017 05:38:16 GMT  (152kb,D)", "https://arxiv.org/abs/1703.00144v1", "13 pages, 1 figure"], ["v2", "Mon, 1 May 2017 16:15:40 GMT  (312kb,D)", "http://arxiv.org/abs/1703.00144v2", "13 pages, 1 figure"], ["v3", "Fri, 19 May 2017 15:57:19 GMT  (195kb,D)", "http://arxiv.org/abs/1703.00144v3", "13 pages, 1 figure"], ["v4", "Fri, 22 Sep 2017 01:53:39 GMT  (152kb,D)", "http://arxiv.org/abs/1703.00144v4", "13 pages, 1 figure"]], "COMMENTS": "13 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["liang zhao", "siyu liao", "yanzhi wang", "zhe li 0001", "jian tang", "bo yuan"], "accepted": true, "id": "1703.00144"}, "pdf": {"name": "1703.00144.pdf", "metadata": {"source": "CRF", "title": "Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank", "authors": ["Liang Zhao", "Siyu Liao", "Yanzhi Wang", "Zhe Li", "Jian Tang", "Victor Pan", "Bo Yuan"], "emails": ["lzhao1@gradcenter.cuny.edu", "sliao2@gradcenter.cuny.edu", "ywang393@syr.edu", "zli89@syr.edu", "jtang02@syr.edu", "victor.pan@lehman.cuny.edu", "byuan@ccny.cuny.edu"], "sections": [{"heading": null, "text": "Keywords: Deep learning, Matrix displacement, Structured matrices"}, {"heading": "1 Introduction", "text": "Neural networks, especially large-scale deep neural networks, have made remarkable success in various applications such as computer vision, natural language processing, etc. [14][21]. However, large-scale neural networks are both memory-intensive and computation-intensive, thereby posing severe challenges when deploying those large-scale neural network models on memory-constrained and energy-constrained embedded devices. To overcome these limitations, many studies and approaches, such as connection pruning [9][8], low rank approximation [7][12], sparsity regularization [23][16] etc., have been proposed to reduce the model size of large-scale (deep) neural networks.\nLDR Construction and LDR Neural Networks: Among those efforts, low displacement rank (LDR) construction is a type of structure-imposing technique for network model reduction and computational complexity reduction. By regularizing the weight matrices of neural networks using the format of LDR matrices (when weight matrices are square) or the composition of multiple LDR matrices (when weight matrices are non-square), a strong structure is naturally imposed to the construction of neural networks. Since an LDR matrix typically requires O(n) independent parameters and exhibits fast matrix operation algorithms [18], an immense space for network model and computational complexity reduction can be enabled. Pioneering work in this direction [3][20] applied special types of LDR matrices (structured matrices), such as circulant\nar X\niv :1\n70 3.\n00 14\n4v 4\n[ cs\nmatrices and Toeplitz matrices, for weight representation. Other types of LDR matrices exist such as Cauchy matrices, Vandermonde matrices, etc., as shown in Figure 1.\nBenefits of LDR Neural Networks: Compared with other types of network compression approaches, the LDR construction shows several unique advantages. First, unlike heuristic weight-pruning methods [9][8] that produce irregular pruned networks, the LDR construction approach always guarantees the strong structure of the trained network, thereby avoiding the storage space and computation time overhead incurred by the complicated indexing process. Second, as a \u201ctrain from scratch\u201d technique, LDR construction does not need extra re-training, and hence eliminating the additional complexity to the training process. Third, the reduction in space complexity and computational complexity by using the structured weight matrices are significant. Different from other network compression approaches that can only provide a heuristic compression factor, the LDR construction can enable the model reduction and computational complexity reduction in Big-O complexity: The storage requirement is reduced fromO(n2) toO(n), and the computational complexity can be reduced from O(n2) to O(n log n) or O(n log2 n) because of the existence of fast matrix-vector multiplication algorithm [18][2] for LDR matrices. For example, when applying structured matrices to the fully-connected layers of AlexNet using ImageNet dataset [6], the storage requirement can be reduced by more than 4,000X while incurring negligible degradation in overall accuracy [3].\nMotivation of This Work: Because of its inherent structure-imposing characteristic, convenient retraining-free training process and unique capability of simultaneous Big-O complexity reduction in storage and computation, LDR construction is a promising approach to achieve high compression ratio and high speedup for a broad category of network models. However, since imposing the structure to weight matrices results in substantial reduction of weight storage from O(n2) to O(n), cautious researchers need to know whether the neural networks with LDR construction, referred to as LDR neural networks, will consistently yield the similar accuracy as compared with the uncompressed networks. Although [3][20] have already shown that using LDR construction still results the same accuracy or minor degradation on various datasets, such as ImageNet [6], CIFAR [13] etc., the theoretical analysis, which can provide the mathematically solid proofs that the LDR neural networks can converge to the same \u201ceffectiveness\u201d as the uncompressed neural networks, is still very necessary in order to promote the wide application of LDR neural networks for emerging and larger-scale applications.\nTechnical Preview and Contributions: To address the above necessity, in this paper we study and provide a solid theoretical foundation of LDR neural networks on the ability to approximate an arbitrary continuous function, the error bound for function approximation, applications on shallow and deep neural networks, etc. More specifically, the main contributions of this paper include:\n\u2022 We prove the universal approximation property for LDR neural networks, which states that the LDR neural networks could approximate an arbitrary continuous function with arbitrary accuracy given enough parameters/neurons. In other words, the LDR neural network will have the same \u201ceffectiveness\u201d of classical neural networks without compression. This property serves as the theoretical foundation of the potential broad applications of LDR neural networks.\n\u2022 We show that, for LDR matrices defined by O(n) parameters, the corresponding LDR neural networks are still capable of achieving integrated squared error of order O(1/n), which is identical to the error\nbound of unstructured weight matrices-based neural networks, thereby indicating that there is essentially no loss for restricting to the weight matrices to LDR matrices.\n\u2022 We develop a universal training process for LDR neural networks with computational complexity reduction compared with backward propagation process for classical neural networks. The proposed algorithm is the generalization of the training process in [3][20] that restricts the structure of weight matrices to circulant matrices or Toeplitz matrices.\nOutline: The paper is outlined as follows. In Section 2 we review the related work on this topic. Section 3 presents necessary definitions and properties of matrix displacement and LDR neural networks. The problem statement is also presented in this section. In Section 4 we prove the universal approximation property for a broad family of LDR neural networks. Section 5 addresses the approximation potential (error bounds) with a limited amount of neurons on shallow LDR neural networks and deep LDR neural networks, respectively. The proposed detailed procedure for training general LDR neural networks are derived in Section 6. Section 7 concludes the article."}, {"heading": "2 Related Work", "text": "Universal Approximation & Error Bound Analysis: For feedforward neural networks with one hidden layer, [4] and [11] proved separately the universal approximation property, which guarantees that for any given continuous function or decision function and any error bound > 0, there always exists a single-hidden layer neural network that approximates the function within integrated error. However, this property does not specify the number of neurons needed to construct such a neural network. In practice, there must be a limit on the maximum amount of neurons due to the computational limit. Moreover, the magnitude of the coefficients can be neither too large nor too small. To address these issues for general neural networks, [11] proved that it is sufficient to approximate functions with weights and biases whose absolute values are bounded by a constant (depending on the activation function). [10] further extended this result to an arbitrarily small bound. [1] showed that feedforward networks with one layer of sigmoidal nonlinearities achieve an integrated squared error with order of O(1/n), where n is the number of neurons.\nMore recently, several interesting results were published on the approximation capabilities of deep neural networks. [5] have shown that there exist certain functions that can be approximated by three-layer neural networks with a polynomial amount of neurons, while two-layer neural networks require exponentially larger amount to achieve the same error. [17] and [22] have shown the exponential increase of linear regions as neural networks grow deeper. [15] proved that with log(1/ ) layers, the neural network can achieve the error bound for any continuous function with O(polylog( )) parameters in each layer.\nLDR Matrices in Neural Networks: [3] have analyzed the effectiveness of replacing conventional weight matrices in fully-connected layers with circulant matrices, which can reduce the time complexity fromO(n2) to O(n log n), and the space complexity from O(n2) to O(n), respectively. [20] have demonstrated significant benefits of using Toeplitz-like matrices to tackle the issue of large space and computation requirement for neural networks training and inference. Experiments show that the use of matrices with low displacement rank offers superior tradeoffs between accuracy and time/space complexity."}, {"heading": "3 Preliminaries on LDR Matrices and Neural Networks", "text": ""}, {"heading": "3.1 Matrix Displacement", "text": "An n\u00d7n matrix M is called a structured matrix when it has a low displacement rank \u03b3 [18]. More precisely, with the proper choice of operator matrices A and B, if the Sylvester displacement\n\u2207A,B(M) := AM\u2212MB (1)\nand the Stein displacement \u2206A,B(M) := M\u2212 AMB (2)\nof matrix M have a rank \u03b3 bounded by a value that is independent of the size of M, then matrix M is referred to as a matrix with a low displacement rank [18]. In this paper we will call these matrices as LDR matrices. Even a full-rank matrix may have small displacement rank with appropriate choice of displacement operators (A,B). Figure 1 illustrates a series of commonly used structured matrices, including a circulant matrix, a Cauchy matrix, a Toeplitz matrix, a Hankel matrix, and a Vandermonde matrix, and Table 1 summarizes their displacement ranks and corresponding displacement operators.\nThe general procedure of handling LDR matrices generally takes three steps: Compression, Computation with Displacements, Decompression. Here compression means to obtain a low-rank displacement of the matrices, and decompression means to converting the results from displacement computations to the answer to the original computational problem. In particular, if one of the displacement operator has the property that its power equals the identity matrix, then one can use the following method to decompress directly:\nLemma 3.1. If A is an a-potent matrix (i.e., Aq = aI for some positive integer q \u2264 n), then\nM = [ q\u22121\u2211 k=0 Ak\u2206A,B(M)Bk ] (I\u2212 aBq)\u22121. (3)\nProof. See Corollary 4.3.7 in [18].\nOne of the most important characteristics of structured matrices is their low number of independent variables. The number of independent parameters is O(n) for an n-by-n structured matrix instead of the order of n2, which indicates that the storage complexity can be potentially reduced to O(n). Besides, the computational complexity for many matrix operations, such as matrix-vector multiplication, matrix inversion, etc., can be significantly reduced when operating on the structured ones. The definition and analysis of structured matrices have been generalized to the case of n-by-m matrices where m 6= n, e.g., the block-circulant matrices [19]. Our application of LDR matrices to neural networks would be the general n-by-m weight matrices. For certain lemmas and theorems such as Lemma 3.1, only the form on n \u00d7 n square matrices is needed for the derivation procedure in this paper. So we omit the generalized form of such statements unless necessary."}, {"heading": "3.2 LDR Neural Networks", "text": "In this paper we study the viability of applying LDR matrices in neural networks. Without loss of generality, we focus on a feed-forward neural network with one fully-connected (hidden) layer, which is similar network setup as [4]. Here the input layer (with n neurons) and the hidden layer (with kn neurons)1 are assumed to be fully connected with a weight matrix W \u2208 Rn\u00d7kn of displacement rank at most r corresponding to displacement operators (A,B), where r n. The domain for the input vector x is the n-dimensional hypercube In := [0, 1]n, and the output layer only contains one neuron. The neural network can be expressed as:\ny = GW,\u03b8(x) = kn\u2211 j=1 \u03b1j\u03c3(wjT x + \u03b8j). (4)\n1Please note that this assumption does not sacrifice any generality because the n-by-m case can be transformed to n-by-kn format with the nearest k using zero padding [3].\nHere \u03c3(\u00b7) is the activation function, wj \u2208 Rn denotes the j-th column of the weight matrix W, and \u03b1j , \u03b8j \u2208 R for j = 1, ..., kn. When the weight matrix W = [w1|w2| \u00b7 \u00b7 \u00b7 |wkn] has a low-rank displacement, we call it an LDR neural network. Matrix displacement techniques ensure that LDR neural network has much lower space requirement and higher computational speed comparing to classical neural networks of the similar size."}, {"heading": "3.3 Problem Statement", "text": "In this paper, we aim at providing theoretical support on the accuracy of function approximation using LDR neural networks, which represents the \u201ceffectiveness\u201d of LDR neural networks compared with the original neural networks. Given a continuous function f(x) defined on [0, 1]n, we study the following tasks:\n\u2022 For any > 0, find an LDR weight matrix W so that the function defined by equation (4) satisfies\nmax x\u2208[0,1]n\n|f(x)\u2212GW,\u03b8(x)| < . (5)\n\u2022 Fix a positive integer n, find an upper bound so that for any continuous function f(x) there exists a bias vector \u03b8 and an LDR matrix with at most n rows satisfying equation (5).\n\u2022 Find a multi-layer LDR neural network that achieves error bound (5) but with fewer parameters.\nThe first task is handled in Section 4, which is the universal approximation property of LDR neural networks. It states that the LDR neural networks could approximate an arbitrary continuous function arbitrarily well and is the underpinning of the widespread applications. The error bounds for shallow and deep neural networks are derived in Section 5. In addition, we derived explicit back-propagation expressions for LDR neural networks in Section 6."}, {"heading": "4 The Universal Approximation Property of LDR Neural Networks", "text": "In this section we will first prove a theorem for matrix displacements. Based on the theorem, we prove the universal approximation property of neural networks utilizing only LDR matrices.\nTheorem 4.1. Let A, B be two n\u00d7 n non-singular diagonalizable matrices satisfying: i) Aq = aI for some positive integer q \u2264 n and a scalar a 6= 0; ii) (I \u2212 aBq) is nonsingular; iii) the eigenvalues of B have distinguishable absolute values. Define S as the set of matrices M such that \u2206A,B(M) has rank 1, i.e.,\nSA,B = {M \u2208 Rn\u00d7n|\u2203g,h \u2208 Rn,\u2206A,B(M) = ghT }. (6)\nThen for any vector v \u2208 Rn, there exists a matrix M \u2208 SA,B and an index v \u2208 {1, ..., n} such that the i-th column of M equals vector v.\nProof. By the property of Stein displacement, any matrix M \u2208 S can be expressed in terms of A, B, and its displacement as follows:\nM = q\u22121\u2211 k=0 Ak\u2206A,B(M)Bk(I\u2212 aBq)\u22121. (7)\nHere we use the property that \u2206A,B(M) has rank 1, and thus it can be written as g \u00b7 hT . Since A is diagonalizable, one can write its eigen-decomposition as\nA = Q\u22121\u039bQ, (8)\nwhere \u039b = diag(\u03bb1, ..., \u03bbn) is a diagonal matrix generated by the eigenvalues of A. Now define ej to be the j-th unit column vector for j = 1, ..., n. Write\nQMej =Q q\u22121\u2211 k=0 Ak\u2206A,B(M)B k(I\u2212 aBq)\u22121ej\n=Q q\u22121\u2211 k=0 (Q\u22121\u039bQ)kghTBk(I\u2212 aBq)\u22121ej\n= ( q\u22121\u2211 k=0 sh,j\u039b k ) Qg.\n(9)\nHere we use sh,j to denote the resulting scalar from matrix product hTBk(I \u2212 aBq)\u22121ej for k = 1, ..., n. Define T := (I\u2212 aBq)\u22121. In order to prove the theorem, we need to show that there exists a vector h and an index k such that the matrix \u2211q\u22121 k=0 sh,j\u039b\nk is nonsingular. In order to distinguish scalar multiplication from matrix multiplication, we use notation a \u25e6M to denote the multiplication of a scalar value and a matrices whenever necessary. Rewrite the expression as\nq\u22121\u2211 k=0 sh,j\u039b k\n= q\u22121\u2211 k=0 hT \u00b7 ( BkTej \u25e6 diag(\u03bbk1 , ..., \u03bbkn) ) =\nq\u22121\u2211 k=0 diag(hT \u00b7Bk \u00b7T \u00b7 [\u03bbk1ej | \u00b7 \u00b7 \u00b7 |\u03bbknej ])\n=diag ( hT \u00b7 ( q\u22121\u2211 k=0 BkT\u03bbk1ej ) , ...,hT \u00b7 ( q\u22121\u2211 k=0 BkT\u03bbknej )) .\nThe diagonal matrix \u2211q\u22121 k=0 sh,j\u039b\nk is nonsingular if and only if all of its diagonal entries are nonzero. Let bij denote the column vector \u2211q\u22121 k=0 BT\nk\u03bbki ej . Unless for every j there is an index ij such that bijj = 0, we can always choose an appropriate vector h so that the resulting diagonal matrix is nonsingular. Next we will show that the former case is not possible using proof by contradiction. Assume that there is a column bijj = 0 for every j = 1, 2, \u00b7 \u00b7 \u00b7 , n, we must have:\n0 =[bi11|bi22| \u00b7 \u00b7 \u00b7 |binn]\n= [ q\u22121\u2211 k=0 BkT\u03bbki1e1| \u00b7 \u00b7 \u00b7 | q\u22121\u2211 k=0 BkT\u03bbkinen ] =\nq\u22121\u2211 k=0 BkT \u00b7 diag(\u03bbki1 , ..., \u03bb k in).\nSince B is diagonalizable, we write B = P\u22121\u03a0P, where \u03a0 = diag(\u03b71, ..., \u03b7n). Also we have T =\n(I\u2212 aBq)\u22121 = P\u22121(I\u2212 a\u03a0q)\u22121P. Then\n0 = q\u22121\u2211 k=0 BTkdiag(\u03bbki1 , ..., \u03bb k in)\n= P\u22121 [ q\u22121\u2211 k=0 \u03a0k(I\u2212 a\u03a0q)\u22121diag(\u03bbki1 , ..., \u03bb k in) ] P\n= P\u22121 q\u22121\u2211 k=0 diag ( (\u03bbi1\u03b71) k, ..., (\u03bbin\u03b7n) k ) (I\u2212 a\u03a0q)\u22121P\n= P\u22121diag ( q\u22121\u2211 k=0 (\u03bbi1\u03b71) k, ..., q\u22121\u2211 k=0 (\u03bbin\u03b7n) k ) (I\u2212 a\u03a0q)\u22121P.\nThis implies that \u03bbi1\u03b71, ..., \u03bbin\u03b7n are solutions to the equation\n1 + x+ x2 + \u00b7 \u00b7 \u00b7+ xq\u22121 = 0. (10)\nBy assumption of matrix B, \u03b71, ..., \u03b7k have different absolute values, and so are \u03bbi1\u03b71, ..., \u03bbi1\u03b71, since all \u03bbk have the same absolute value because Aq = aI. This fact suggests that there are q distinguished solutions of equation (10), which contradicts the fundamental theorem of algebra. Thus it is incorrect to assume that matrix \u2211q\u22121 k=0 sh,j\u039b\nk is singular for all h \u2208 Rn. With this property proven, given any vector v \u2208 Rn, one can take the following procedure to find a matrix M \u2208 S and a index j such that the j-th column of M equals v:\ni) Find a vector h and a index j such that matrix \u2211q\u22121 k=0 sh,j\u039b\nk is non-singular; ii) By equation (9), find\ng :=Q\u22121 ( q\u22121\u2211 k=0 sh,j\u039b k )\u22121 QTv;\niii) Construct M \u2208 S with g and h by equation (7). Then its j-th column will equal to v. With the above construction, we have shown that for any vector v \u2208 Rn one can find a matrix M \u2208 S and a index j such that the j-th column of M equals v, thus the theorem is proved.\nOur main goal of this section is to show that neural networks with many types of LDR matrices (LDR neural networks) can approximate continuous functions arbitrarily well. In particular, we are going to show that Toeplitz matrices and circulant matrices, as specific cases of LDR matrices, have the same property. In order to do so, we need to introduce the following definition of a discriminatory function and one key property. (cf. [4])\nDefinition 4.1. A function \u03c3(u) : R\u2192 R is called as discriminatory if the zero measure is the only measure \u00b5 that satisfies the following property:\u222b\nIn \u03c3(wTx + \u03b8)d\u00b5(x) = 0,\u2200w \u2208 Rn, \u03b8 \u2208 R. (11)\nLemma 4.1. Any bounded, measurable sigmoidal function is discriminatory.\nProof. The statement of this lemma and its proof is included in [4].\nNow we are ready to present the universal approximation theorem of LDR neural networks with n-by-kn weight matrix W:\nTheorem 4.2 (Universal Approximation Theorem for LDR Neural Networks). Let \u03c3 be any continuous discriminatory function. For any continuous function f(x) defined on In, > 0, and any A,B \u2208 Rn\u00d7n, satisfying assumptions in Theorem 4.1, then there exists a function G(x) in the form of equation (4) so that its weight matrix consists of k submatrices with displacement rank of 1 and\nmax x\u2208In\n|G(x)\u2212 f(x)| < . (12)\nProof. Denote the i-th n\u00d7 n submatrix of W as Wi. Then W can be written as W = [ W1|W2|...|Wk ] . (13)\nLet M be any of submatirx Wi with displacement rank 1. M can be written as\n\u2206A,B(M) = M\u2212AMB = g \u00b7 hT, (14)\nwhere g,h \u2208 Rn. Since Aq = I, follow Lemma 3.1 and we obtain\nM = [ q\u22121\u2211 k=0 Ak\u2206A,B(M)B k ] (I\u2212 aBq)\u22121. (15)\nLet SIn denote the set of all continuous functions defined on In. Let UIn be the linear subspace of SIn that can be expressed in form of equation (4) where W consists of k sub-matrices with displacement rank 1. We want to show that UIn is dense in the set of all continuous functions SIn .\nSuppose not, by Hahn-Banach Theorem, there exists a bounded linear functional L 6= 0 such that L(U\u0304(In)) = 0. Moreover, By Riesz Representation Theorem, L can be written as\nL(h) = \u222b In h(x)d\u00b5(x),\u2200h \u2208 S(In),\nfor some measure \u00b5. Next we show that for any y \u2208 Rn and \u03b8 \u2208 R, the function \u03c3(yTx + \u03b8) belongs to the set UIn , and thus\nwe must have \u222b In \u03c3(yTx + \u03b8)d\u00b5(x) = 0. (16) For any vector y \u2208 Rn, Theorem 4.1 guarantees that there exists an n \u00d7 n LDR matrix M = [b1| \u00b7 \u00b7 \u00b7 |bn] and an index j such that bj = y. Now define a vector (\u03b11, ..., \u03b1n) such that \u03b1j = 1 and \u03b11 = \u00b7 \u00b7 \u00b7 = \u03b1n = 0. Also let the value of all bias be \u03b8. Then the LDR neural network function becomes\nG(x) = n\u2211 i=1 \u03b1i\u03c3(b T i x + \u03b8)\n=\u03b1j\u03c3(b T j x + \u03b8) = \u03c3(y Tx + \u03b8).\n(17)\nFrom the fact that L(G(x)) = 0, we derive that\n0 =L(G(x))\n= \u222b In n\u2211 i=1 \u03b1i\u03c3(b T i x + \u03b8) = \u222b In \u03c3(yTx + \u03b8)d\u00b5(x).\nSince \u03c3(t) is a discriminatory function by Lemma 4.1. We can conclude that \u00b5 is the zero measure. As a result, the function defined as an integral with measure \u00b5 must be zero for any input function h \u2208 S(In). The last statement contradicts the property that L 6= 0 from the Hahn-Banach Theorem, which is obtained based on the assumption that the set UIn of LDR neural network functions are not dense in SIn . As this assumption is not true, we have the universal approximation property of LDR neural networks.\nReference work [3], [20] have utilized a circulant matrix or a Toeplitz matrix for weight representation in deep neural networks. Please note that for the general case of n-by-m weight matrices, either the more general Block-circulant matrices should be utilized or padding extra columns or rows of zeroes are needed [3]. Circulant matrices and Topelitz matrices are both special form of LDR matrices, and thus we could apply the above universal approximation property of LDR neural networks and provide theoretical support for the use of circulant and Toeplitz matrices in [3], [20]. Although circulant and Toeplitz matrices have displacement rank of 2 instead of 1, the property of Theorem 4.1 still holds, as a Toeplitz matrix is completely determined by its first row and its first column (and a circulant matrix is completely determined by its first row.) Therefore we arrive at the following corollary.\nCorollary 4.1. Any continuous function can be arbitrarily approximated by neural networks constructed with Toeplitz matrices or circulant matrices (with padding or using Block-circulant matrices)."}, {"heading": "5 Error Bounds on LDR Neural Networks", "text": "With the universal approximation property proved, naturally we seek ways to provide error bound estimates for LDR neural networks. We are able to prove that for LDR matrices defined by O(n) parameters (n represents the number of rows and has the same order as the number of columns), the corresponding structured neural network is capable of achieving integrated squared error of order O(1/n), where n is the number of parameters. This result is asymptotically equivalent to Barron\u2019s aforementioned result on general neural networks, indicating that there is essentially no loss for restricting to LDR matrices.\nThe functions we would like to approximate are those who are defined on a n-dimensional ball Br = {x \u2208 Rn : |x| \u2264 r} such that \u222b Br |x||f(x)|\u00b5(dx) \u2264 C, where \u00b5 is an arbitrary measure normalized so that \u00b5(Br) = 1. Let\u2019s call this set \u0393C,Br . [1] considered the following set of bounded multiples of a sigmoidal function composed with linear functions: G\u03c3 = {\u03b1\u03c3(yTx + \u03b8) : |\u03b1| \u2264 2C,y \u2208 Rn, \u03b8 \u2208 R}. (18)\nHe proved the following theorem:\nTheorem 5.1 ([1]). For every function in \u0393C,Br , every sigmoidal function \u03c3, every probability measure, and every k \u2265 1, there exists a linear combination of sigmoidal functions fk(x) of the form\nfk(x) = k\u2211 j=1 \u03b1j\u03c3(y T j x + \u03b8j), (19)\nsuch that \u222b Br (f(x)\u2212 fk(x))2\u00b5(dx) \u2264 4r2C k . (20) Here yj \u2208 Rn and \u03b8j \u2208 R for every j = 1, 2, ..., N , Moreover, the coefficients of the linear combination may be restricted to satisfy \u2211k j=1 |cj | \u2264 2rC.\nNow we will show how to obtain a similar result for LDR matrices. Fix operator (A,B) and define\nSkn\u03c3 = { kn\u2211 j=1 \u03b1j\u03c3(y T j x + \u03b8j) : |\u03b1j | \u2264 2C,yj \u2208 Rn,\n\u03b8j \u2208 R, j = 1, 2, ..., N, and [y(i\u22121)n+1|y(i\u22121)n+2| \u00b7 \u00b7 \u00b7 |yin]\nis an LDR matrix, \u2200i = 1, ..., k } .\n(21)\nMoreover, let Gk\u03c3 be the set of function that can be expressed as a sum of no more than k terms from G\u03c3 . Define the metric ||f \u2212 g||\u00b5 = \u221a\u222b\nBr (f(x)\u2212 g(x))2\u00b5(dx). Theorem 5.1 essentially states that the minimal\ndistance between a function f \u2208 \u0393C,B and Gm\u03c3 is asymptotically O(1/n). The following lemma proves that Gk\u03c3 is in fact contained in S kn \u03c3 .\nLemma 5.1. For any k \u2265 1, Gk\u03c3 \u2282 Skn\u03c3 .\nProof. Any function fk(x) \u2208 Gk\u03c3 can be written in the form\nfk(x) = k\u2211 j=1 \u03b1j\u03c3(y T j x + \u03b8j). (22)\nFor each j = 1, ..., k, define a n \u00d7 n LDR matrix Wj such that one of its column is yj . Let tij be the i-th column of Wj . Let ij correspond to the column index such that tij = yj for all j. Now consider the following function\nG(x) := k\u2211 j=1 n\u2211 i=1 \u03b2ij\u03c3(t T ijx + \u03b8j), (23)\nwhere \u03b2ijj equals \u03b1j , and \u03b2ij = 0 if i 6= ij . Notice that we have the following equality\nG(x) := k\u2211 j=1 n\u2211 i=1 \u03b2ij\u03c3(t T ijx + \u03b8j)\n= k\u2211 j=1 \u03b2ijj\u03c3(t T ijx + \u03b8j)\n= k\u2211 j=1 \u03b1j\u03c3(y T j x + \u03b8j) = fk(x).\nNotice that the matrix W = [W1|W2| \u00b7 \u00b7 \u00b7 |Wk] consists k LDR submatrices. Thus fk(x) belongs to the set Skn\u03c3 .\nBy Lemma 5.1, we can replace Gk\u03c3 with S kn \u03c3 in Theorem 5.1 and obtain the following error bound\nestimates on LDR neural networks:\nTheorem 5.2. For every disk Br \u2282 Rn, every function in \u0393C,Br , every sigmoidal function \u03c3, every normalized measure \u00b5, and every k \u2265 1, there exists neural network defined by a weight matrix consists of k LDR submatrices such that \u222b\nBr\n(f(x)\u2212 fkn(x))2\u00b5(dx) \u2264 4r2C\nk . (24)\nMoreover, the coefficients of the linear combination may be restricted to satisfy \u2211N k=1 |ck| \u2264 2rC.\nThe next theorem naturally extended the result from [15] to LDR neural networks, indicating that LDR neural networks can also benefit a parameter reduction if one uses more than one layers. More precisely, we have the following statement:\nTheorem 5.3. Let f be a continuous function on [0, 1] and is 2n + 1 times differentiable in (0, 1) for n = dlog 1 + 1]e. If |f (k)(x)| \u2264 k! holds for all x \u2208 (0, 1) and k \u2208 [ 2n + 1 ] , then for any n \u00d7 n matrices A and B satisfying the conditions of Theorem 4.1, there exists a LDR neural network GA,B(x) with O(log 1 ) layers, O(log 1 n) binary step units, O(log 2 1 n) rectifier linear units such that\nmax x\u2208(0,1)\n|f(x)\u2212GA,B(x)| < .\nProof. The theorem with better bounds and without assumption of being LDR neural network is proved in [15] as Theorem 4. For each binary step unit or rectifier linear unit in the construction of the general neural network, attach (n\u22121) dummy units, and expand the weights associated to this unit from a vector to an LDR matrix based on Theorem 4.1. By doing so we need to add a factor n to the original amount of units, and the asymptotic bounds are relaxed accordingly."}, {"heading": "6 Training LDR Neural Networks", "text": "In this section, we reformulate the gradient computation of LDR neural networks. The computation for propagating through a fully-connected layer can be written as\ny = \u03c3(WTx + \u03b8), (25)\nwhere \u03c3(\u00b7) is the activation function, W \u2208 Rn\u00d7kn is the weight matrix, x \u2208 Rn is input vector and \u03b8 \u2208 Rkn is bias vector. According to Equation (7), if Wi is an LDR matrix with operators (Ai,Bi) satisfying conditions of Theorem 4.1, then it is essentially determined by two matrices Gi \u2208 Rn\u00d7r,Hi \u2208 Rn\u00d7r as\nWi = [ q\u22121\u2211 k=0 AkiGiH T i B k i ] (I\u2212 aBqi ) \u22121. (26)\nTo fit the back-propagation algorithm, our goal is to compute derivatives \u2202O\u2202Gi , \u2202O \u2202Hi and \u2202O\u2202x for any objective function O = O(W1, . . . ,Wk).\nIn general, given that a := WTx + \u03b8, we can have:\n\u2202O\n\u2202W = x(\n\u2202O \u2202a )T , \u2202O \u2202x = W \u2202O \u2202a , \u2202O \u2202\u03b8 = \u2202O \u2202a 1. (27)\nwhere 1 is a column vector full of ones. Let G\u0302ik := AkiGi, H\u0302ik := H T i B k i (I \u2212 aB q i ) \u22121, and Wik :=\nG\u0302ikH\u0302ik. The derivatives of \u2202O\u2202Wik can be computed as following:\n\u2202O\n\u2202Wik =\n\u2202O\n\u2202Wi . (28)\nAccording to Equation (27), if we let a = Wik, W = G\u0302Tik and x = H\u0302ik, then \u2202O \u2202G\u0302ik and \u2202O \u2202H\u0302ik can be derived as:\n\u2202O \u2202G\u0302ik = [ \u2202O \u2202G\u0302Tik ]T = [ H\u0302ik \u2202O \u2202Wik ]T = ( \u2202O \u2202Wik )T H\u0302Tik, (29)\n\u2202O\n\u2202H\u0302ik = G\u0302Tik\n\u2202O\n\u2202Wik . (30)\nSimilarly, let a = G\u0302ik, W = (Aki ) T and x = Gi, then \u2202O\u2202Gi can be derived as:\n\u2202O\n\u2202Gi = q\u22121\u2211 k=0 (Aki ) T ( \u2202O \u2202G\u0302ik )\n= q\u22121\u2211 k=0 (Aki ) T ( \u2202O \u2202Wik )T H\u0302Tik.\n(31)\nSubstituting with a = H\u0302ik, W = HTi and x = B k i (I\u2212 aB q i ) \u22121, we have \u2202O\u2202Hi derived as:\n\u2202O\n\u2202Hi = q\u22121\u2211 k=0 Bki (I\u2212 aB q i ) \u22121( \u2202O \u2202H\u0302ik )T\n= q\u22121\u2211 k=0 Bki (I\u2212 aB q i ) \u22121( \u2202O \u2202Wik )T G\u0302ik.\n(32)\nIn this way, derivatives \u2202O\u2202Gi and \u2202O \u2202Hi can be computed given \u2202O\u2202Wik which is equal to \u2202O \u2202Wi . The essence of back-propagation algorithm is to propagate gradients backward from the layer with objective function to the input layer. \u2202O\u2202Wi can be calculated from previous layer and \u2202O \u2202x will be propagated to the next layer if necessary. For practical use one may want to choose matrices Ai and Bi with fast multiplication method such as diagonal matrices, permutation matrices, banded matrices, etc. Then the space complexity (the number of parameters for storage) of Wi can be O(2n+ 2nr) rather than O(n2) of traditional dense matrix. The 2n is for Ai and Bi and 2nr is for Gi and Hi. The time complexity of WTi x will be O(q(3n+ 2nr)) compared with O(n2) of dense matrix. Particularly, when Wi is a structured matrix like the Toeplitz matrix, the space complexity will be O(2n). This is because the Toeplitz matrix is defined by 2n parameters. Moreover, its matrix-vector multiplication can be accelerated by using Fast Fourier Transform (for Toeplitz and circulant matrices), resulting in time complexity O(n log n). In this way the back-propagation computation for the layer can be done with near-linear time."}, {"heading": "7 Conclusion", "text": "In this paper, we have proven that the universal approximation property of LDR neural networks. In addition, we also theoretically show that the error bounds of LDR neural networks are at least as efficient as general unstructured neural network. Besides, we also develop the back-propagation based training algorithm for universal LDR neural networks. Our study provides the theoretical foundation of the empirical success of LDR neural networks."}], "references": [{"title": "Universal approximation bounds for superpositions of a sigmoidal function", "author": ["Andrew R Barron"], "venue": "IEEE Transactions on Information theory,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1993}, {"title": "Polynomial and matrix computations volume 1: Fundamental algorithms", "author": ["Dario Bini", "Victor Pan", "Wayne Eberly"], "venue": "SIAM Review,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Yu Cheng", "Felix X Yu", "Rogerio S Feris", "Sanjiv Kumar", "Alok Choudhary", "Shi-Fu Chang"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["George Cybenko"], "venue": "Mathematics of Control, Signals, and Systems (MCSS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1989}, {"title": "Shallow vs. deep sum-product networks", "author": ["Olivier Delalleau", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Emily L Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["Kurt Hornik"], "venue": "Neural networks,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1991}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Kurt Hornik", "Maxwell Stinchcombe", "Halbert White"], "venue": "Neural networks,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1989}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1405.3866,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Why deep neural networks", "author": ["Shiyu Liang", "R Srikant"], "venue": "arXiv preprint arXiv:1610.04161,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Sparse convolutional neural networks", "author": ["Baoyuan Liu", "Min Wang", "Hassan Foroosh", "Marshall Tappen", "Marianna Pensky"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Structured matrices and polynomials: unified superfast algorithms", "author": ["Victor Pan"], "venue": "Springer Science & Business Media,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Estimating the norms of random circulant and toeplitz matrices and their inverses", "author": ["Victor Y Pan", "John Svadlenka", "Liang Zhao"], "venue": "Linear algebra and its applications,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Structured transforms for small-footprint deep learning", "author": ["Vikas Sindhwani", "Tara Sainath", "Sanjiv Kumar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Benefits of depth in neural networks", "author": ["Matus Telgarsky"], "venue": "arXiv preprint arXiv:1602.04485,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Learning structured sparsity in deep neural networks", "author": ["Wei Wen", "Chunpeng Wu", "Yandan Wang", "Yiran Chen", "Hai Li"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank Liang Zhao, Siyu Liao, Yanzhi Wang, Zhe Li, Jian Tang, Victor Pan , and Bo Yuan [5],[g] [1] Department of Mathematics, Graduate Center of the City University of New York [2] Department of Computer Science, Graduate Center of the City University of New York [3] Department of Electrical Engineering and Computer Science, Syracuse Univ.", "startOffset": 169, "endOffset": 172}, {"referenceID": 0, "context": "Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank Liang Zhao, Siyu Liao, Yanzhi Wang, Zhe Li, Jian Tang, Victor Pan , and Bo Yuan [5],[g] [1] Department of Mathematics, Graduate Center of the City University of New York [2] Department of Computer Science, Graduate Center of the City University of New York [3] Department of Electrical Engineering and Computer Science, Syracuse Univ.", "startOffset": 177, "endOffset": 180}, {"referenceID": 1, "context": "Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank Liang Zhao, Siyu Liao, Yanzhi Wang, Zhe Li, Jian Tang, Victor Pan , and Bo Yuan [5],[g] [1] Department of Mathematics, Graduate Center of the City University of New York [2] Department of Computer Science, Graduate Center of the City University of New York [3] Department of Electrical Engineering and Computer Science, Syracuse Univ.", "startOffset": 259, "endOffset": 262}, {"referenceID": 2, "context": "Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank Liang Zhao, Siyu Liao, Yanzhi Wang, Zhe Li, Jian Tang, Victor Pan , and Bo Yuan [5],[g] [1] Department of Mathematics, Graduate Center of the City University of New York [2] Department of Computer Science, Graduate Center of the City University of New York [3] Department of Electrical Engineering and Computer Science, Syracuse Univ.", "startOffset": 346, "endOffset": 349}, {"referenceID": 3, "context": "[4] Departments of Mathematics and Computer Science, Lehman College and the Graduate Center of the City University of New York [5] Department of Electrical Engineering, City College of the City University of New York [a] lzhao1@gradcenter.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[4] Departments of Mathematics and Computer Science, Lehman College and the Graduate Center of the City University of New York [5] Department of Electrical Engineering, City College of the City University of New York [a] lzhao1@gradcenter.", "startOffset": 127, "endOffset": 130}, {"referenceID": 13, "context": "[14][21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[14][21].", "startOffset": 4, "endOffset": 8}, {"referenceID": 8, "context": "To overcome these limitations, many studies and approaches, such as connection pruning [9][8], low rank approximation [7][12], sparsity regularization [23][16] etc.", "startOffset": 87, "endOffset": 90}, {"referenceID": 7, "context": "To overcome these limitations, many studies and approaches, such as connection pruning [9][8], low rank approximation [7][12], sparsity regularization [23][16] etc.", "startOffset": 90, "endOffset": 93}, {"referenceID": 6, "context": "To overcome these limitations, many studies and approaches, such as connection pruning [9][8], low rank approximation [7][12], sparsity regularization [23][16] etc.", "startOffset": 118, "endOffset": 121}, {"referenceID": 11, "context": "To overcome these limitations, many studies and approaches, such as connection pruning [9][8], low rank approximation [7][12], sparsity regularization [23][16] etc.", "startOffset": 121, "endOffset": 125}, {"referenceID": 22, "context": "To overcome these limitations, many studies and approaches, such as connection pruning [9][8], low rank approximation [7][12], sparsity regularization [23][16] etc.", "startOffset": 151, "endOffset": 155}, {"referenceID": 15, "context": "To overcome these limitations, many studies and approaches, such as connection pruning [9][8], low rank approximation [7][12], sparsity regularization [23][16] etc.", "startOffset": 155, "endOffset": 159}, {"referenceID": 17, "context": "Since an LDR matrix typically requires O(n) independent parameters and exhibits fast matrix operation algorithms [18], an immense space for network model and computational complexity reduction can be enabled.", "startOffset": 113, "endOffset": 117}, {"referenceID": 2, "context": "Pioneering work in this direction [3][20] applied special types of LDR matrices (structured matrices), such as circulant", "startOffset": 34, "endOffset": 37}, {"referenceID": 19, "context": "Pioneering work in this direction [3][20] applied special types of LDR matrices (structured matrices), such as circulant", "startOffset": 37, "endOffset": 41}, {"referenceID": 8, "context": "First, unlike heuristic weight-pruning methods [9][8] that produce irregular pruned networks, the LDR construction approach always guarantees the strong structure of the trained network, thereby avoiding the storage space and computation time overhead incurred by the complicated indexing process.", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "First, unlike heuristic weight-pruning methods [9][8] that produce irregular pruned networks, the LDR construction approach always guarantees the strong structure of the trained network, thereby avoiding the storage space and computation time overhead incurred by the complicated indexing process.", "startOffset": 50, "endOffset": 53}, {"referenceID": 17, "context": "Different from other network compression approaches that can only provide a heuristic compression factor, the LDR construction can enable the model reduction and computational complexity reduction in Big-O complexity: The storage requirement is reduced fromO(n) toO(n), and the computational complexity can be reduced from O(n) to O(n log n) or O(n log n) because of the existence of fast matrix-vector multiplication algorithm [18][2] for LDR matrices.", "startOffset": 428, "endOffset": 432}, {"referenceID": 1, "context": "Different from other network compression approaches that can only provide a heuristic compression factor, the LDR construction can enable the model reduction and computational complexity reduction in Big-O complexity: The storage requirement is reduced fromO(n) toO(n), and the computational complexity can be reduced from O(n) to O(n log n) or O(n log n) because of the existence of fast matrix-vector multiplication algorithm [18][2] for LDR matrices.", "startOffset": 432, "endOffset": 435}, {"referenceID": 5, "context": "For example, when applying structured matrices to the fully-connected layers of AlexNet using ImageNet dataset [6], the storage requirement can be reduced by more than 4,000X while incurring negligible degradation in overall accuracy [3].", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "For example, when applying structured matrices to the fully-connected layers of AlexNet using ImageNet dataset [6], the storage requirement can be reduced by more than 4,000X while incurring negligible degradation in overall accuracy [3].", "startOffset": 234, "endOffset": 237}, {"referenceID": 2, "context": "Although [3][20] have already shown that using LDR construction still results the same accuracy or minor degradation on various datasets, such as ImageNet [6], CIFAR [13] etc.", "startOffset": 9, "endOffset": 12}, {"referenceID": 19, "context": "Although [3][20] have already shown that using LDR construction still results the same accuracy or minor degradation on various datasets, such as ImageNet [6], CIFAR [13] etc.", "startOffset": 12, "endOffset": 16}, {"referenceID": 5, "context": "Although [3][20] have already shown that using LDR construction still results the same accuracy or minor degradation on various datasets, such as ImageNet [6], CIFAR [13] etc.", "startOffset": 155, "endOffset": 158}, {"referenceID": 12, "context": "Although [3][20] have already shown that using LDR construction still results the same accuracy or minor degradation on various datasets, such as ImageNet [6], CIFAR [13] etc.", "startOffset": 166, "endOffset": 170}, {"referenceID": 2, "context": "The proposed algorithm is the generalization of the training process in [3][20] that restricts the structure of weight matrices to circulant matrices or Toeplitz matrices.", "startOffset": 72, "endOffset": 75}, {"referenceID": 19, "context": "The proposed algorithm is the generalization of the training process in [3][20] that restricts the structure of weight matrices to circulant matrices or Toeplitz matrices.", "startOffset": 75, "endOffset": 79}, {"referenceID": 3, "context": "2 Related Work Universal Approximation & Error Bound Analysis: For feedforward neural networks with one hidden layer, [4] and [11] proved separately the universal approximation property, which guarantees that for any given continuous function or decision function and any error bound > 0, there always exists a single-hidden layer neural network that approximates the function within integrated error.", "startOffset": 118, "endOffset": 121}, {"referenceID": 10, "context": "2 Related Work Universal Approximation & Error Bound Analysis: For feedforward neural networks with one hidden layer, [4] and [11] proved separately the universal approximation property, which guarantees that for any given continuous function or decision function and any error bound > 0, there always exists a single-hidden layer neural network that approximates the function within integrated error.", "startOffset": 126, "endOffset": 130}, {"referenceID": 10, "context": "To address these issues for general neural networks, [11] proved that it is sufficient to approximate functions with weights and biases whose absolute values are bounded by a constant (depending on the activation function).", "startOffset": 53, "endOffset": 57}, {"referenceID": 9, "context": "[10] further extended this result to an arbitrarily small bound.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] showed that feedforward networks with one layer of sigmoidal nonlinearities achieve an integrated squared error with order of O(1/n), where n is the number of neurons.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] have shown that there exist certain functions that can be approximated by three-layer neural networks with a polynomial amount of neurons, while two-layer neural networks require exponentially larger amount to achieve the same error.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] and [22] have shown the exponential increase of linear regions as neural networks grow deeper.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[17] and [22] have shown the exponential increase of linear regions as neural networks grow deeper.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "[15] proved that with log(1/ ) layers, the neural network can achieve the error bound for any continuous function with O(polylog( )) parameters in each layer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "LDR Matrices in Neural Networks: [3] have analyzed the effectiveness of replacing conventional weight matrices in fully-connected layers with circulant matrices, which can reduce the time complexity fromO(n) to O(n log n), and the space complexity from O(n) to O(n), respectively.", "startOffset": 33, "endOffset": 36}, {"referenceID": 19, "context": "[20] have demonstrated significant benefits of using Toeplitz-like matrices to tackle the issue of large space and computation requirement for neural networks training and inference.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "1 Matrix Displacement An n\u00d7n matrix M is called a structured matrix when it has a low displacement rank \u03b3 [18].", "startOffset": 106, "endOffset": 110}, {"referenceID": 19, "context": "[20]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "of matrix M have a rank \u03b3 bounded by a value that is independent of the size of M, then matrix M is referred to as a matrix with a low displacement rank [18].", "startOffset": 153, "endOffset": 157}, {"referenceID": 17, "context": "7 in [18].", "startOffset": 5, "endOffset": 9}, {"referenceID": 18, "context": ", the block-circulant matrices [19].", "startOffset": 31, "endOffset": 35}, {"referenceID": 3, "context": "Without loss of generality, we focus on a feed-forward neural network with one fully-connected (hidden) layer, which is similar network setup as [4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 0, "context": "The domain for the input vector x is the n-dimensional hypercube I := [0, 1], and the output layer only contains one neuron.", "startOffset": 70, "endOffset": 76}, {"referenceID": 2, "context": "1Please note that this assumption does not sacrifice any generality because the n-by-m case can be transformed to n-by-kn format with the nearest k using zero padding [3].", "startOffset": 167, "endOffset": 170}, {"referenceID": 0, "context": "Given a continuous function f(x) defined on [0, 1], we study the following tasks: \u2022 For any > 0, find an LDR weight matrix W so that the function defined by equation (4) satisfies", "startOffset": 44, "endOffset": 50}, {"referenceID": 0, "context": "max x\u2208[0,1]n |f(x)\u2212GW,\u03b8(x)| < .", "startOffset": 6, "endOffset": 11}, {"referenceID": 3, "context": "[4]) Definition 4.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "The statement of this lemma and its proof is included in [4].", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "Reference work [3], [20] have utilized a circulant matrix or a Toeplitz matrix for weight representation in deep neural networks.", "startOffset": 15, "endOffset": 18}, {"referenceID": 19, "context": "Reference work [3], [20] have utilized a circulant matrix or a Toeplitz matrix for weight representation in deep neural networks.", "startOffset": 20, "endOffset": 24}, {"referenceID": 2, "context": "Please note that for the general case of n-by-m weight matrices, either the more general Block-circulant matrices should be utilized or padding extra columns or rows of zeroes are needed [3].", "startOffset": 187, "endOffset": 190}, {"referenceID": 2, "context": "Circulant matrices and Topelitz matrices are both special form of LDR matrices, and thus we could apply the above universal approximation property of LDR neural networks and provide theoretical support for the use of circulant and Toeplitz matrices in [3], [20].", "startOffset": 252, "endOffset": 255}, {"referenceID": 19, "context": "Circulant matrices and Topelitz matrices are both special form of LDR matrices, and thus we could apply the above universal approximation property of LDR neural networks and provide theoretical support for the use of circulant and Toeplitz matrices in [3], [20].", "startOffset": 257, "endOffset": 261}, {"referenceID": 0, "context": "[1] considered the following set of bounded multiples of a sigmoidal function composed with linear functions: G\u03c3 = {\u03b1\u03c3(yx + \u03b8) : |\u03b1| \u2264 2C,y \u2208 R, \u03b8 \u2208 R}.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "1 ([1]).", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "The next theorem naturally extended the result from [15] to LDR neural networks, indicating that LDR neural networks can also benefit a parameter reduction if one uses more than one layers.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "Let f be a continuous function on [0, 1] and is 2n + 1 times differentiable in (0, 1) for n = dlog 1 + 1]e.", "startOffset": 34, "endOffset": 40}, {"referenceID": 14, "context": "The theorem with better bounds and without assumption of being LDR neural network is proved in [15] as Theorem 4.", "startOffset": 95, "endOffset": 99}], "year": 2017, "abstractText": "Recently low displacement rank (LDR) matrices, or so-called structured matrices, have been proposed to compress large-scale neural networks. Empirical results have shown that neural networks with weight matrices of LDR matrices, referred as LDR neural networks, can achieve significant reduction in space and computational complexity while retaining high accuracy. We formally study LDR matrices in deep learning. First, we prove the universal approximation property of LDR neural networks with a mild condition on the displacement operators. We then show that the error bounds of LDR neural networks are as efficient as general neural networks with both single-layer and multiple-layer structure. Finally, we propose backpropagation based training algorithm for general LDR neural networks.", "creator": "LaTeX with hyperref package"}}}