{"id": "1703.00356", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "Graph-based Isometry Invariant Representation Learning", "abstract": "Learning transformation invariant representations of visual data is an important problem in computer vision. Deep convolutional networks have demonstrated remarkable results for image and video classification tasks. However, they have achieved only limited success in the classification of images that undergo geometric transformations. In this work we present a novel Transformation Invariant Graph-based Network (TIGraNet), which learns graph-based features that are inherently invariant to isometric transformations such as rotation and translation of input images. In particular, images are represented as signals on graphs, which permits to replace classical convolution and pooling layers in deep networks with graph spectral convolution and dynamic graph pooling layers that together contribute to invariance to isometric transformations. Our experiments show high performance on rotated and translated images from the test set compared to classical architectures that are very sensitive to transformations in the data. The inherent invariance properties of our framework provide key advantages, such as increased resiliency to data variability and sustained performance with limited training sets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 1 Mar 2017 15:51:13 GMT  (3851kb,D)", "http://arxiv.org/abs/1703.00356v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["renata khasanova", "pascal frossard"], "accepted": true, "id": "1703.00356"}, "pdf": {"name": "1703.00356.pdf", "metadata": {"source": "META", "title": "Graph-based Isometry Invariant Representation Learning", "authors": ["Renata Khasanova", "Pascal Frossard"], "emails": ["RENATA.KHASANOVA@EPFL.CH", "PASCAL.FROSSARD@EPFL.CH", "nata.khasanova@epfl.ch>."], "sections": [{"heading": "1. Introduction", "text": "Deep convolutional networks (ConvNets) have achieved impressive results for various computer vision tasks, such as image classification (Krizhevsky et al., 2012) and segmentation (Ronneberger et al., 2015). However, they still suffer from the potentially high variability of data in highdimensional image spaces. In particular, ConvNets that are\n. Correspondence to: Renata Khasanova <renata.khasanova@epfl.ch>.\nC on v ST\nN T IG ra N et\nFigure 1. Illustrative transformation-invariant handwritten digit classification task. Rotated test images, along with their classification label obtained from ConvNets (Conv) (Boureau et al., 2010), Spatial-Transformer Network (STN) (Jaderberg et al., 2015), and our method. (best seen in color)\ntrained to recognize an object from a given perspective or camera viewpoint, will likely fail when the viewpoint is changed or the image of the object is simply rotated. In order to overcome this issue the most natural step is to extend the training dataset with images of the same objects but seen from different perspectives. This however increases the complexity of data collection and more importantly leads to the growth of the training dataset when the variability of the data is high.\nInstead of simply augmenting the training set, which may not always be feasible, one can try to solve the aforementioned problem by making the classification architecture invariant to transformations of the input signal as illustrated in Fig. 1. In that perspective, we propose to represent input images as signals on the grid graph instead of simple matrices of pixel intensities. The benefits of this representation is that graph signals do not carry a strict notion of orientation, while at the same time, signals on a grid graph stay invariant to translation. We exploit these properties to create features that are invariant to isometric transformations and we design new graph-based convolutional and pooling layers, which replace their counterparts used in the classical deep learning settings. This permits preserving the trans-\nar X\niv :1\n70 3.\n00 35\n6v 1\n[ cs\n.C V\n] 1\nM ar\n2 01\nformation equivariance of each intermediate feature representation under both translation and rotation of the input signals. Specifically, our convolutional layer relies on filters that are polynomials of the graph Laplacian for effective signal representation without computing eigendecompositions of the graph signals. We further introduce a new statistical layer that is placed right before the first fullyconnected layer of the network prior to the classification. This layer is specific to our graph signal representation, and in turn permits combining the rotation and translation invariance features along with the power of fully-connected layers that are essential for solving the classification task. We finally design a complete architecture for a deep neural network called TIGraNet, which efficiently combines spectral convolutional, dynamic pooling, statistical and fullyconnected layers to process images represented on grid graphs. We train our network in order to learn isometric transformation invariant features. These features are used in sample transformation-invariant image classification tasks, where our solution outperforms the state-of-theart algorithms for handwritten digit recognition and classification of objects seen from different viewpoints.\nIn summary, we propose the following contributions in this paper:\n\u2022 We design a new graph-based deep learning framework that learns isometric invariant features; \u2022 We propose a new statistical layer that leads to effective transformation-invariant classification of images described by graph-based features; \u2022 Through experiments, we show that our method is able to correctly classify rotated or translated images even if such deformations are not present in the training data.\nThe remainder of the paper is organized as follows. In Section 2 we describe the related work. Section 3 reviews elements of graph signal processing, which are later used to design graph filters. Our new graph-based architecture is presented in details in Section 4. Finally, our experiments and their analysis are presented in Section 5 and we conclude in Section 6."}, {"heading": "2. Related work", "text": "Most of the recent architectures (LeCun et al., 2001; Krizhevsky et al., 2012) have been very successful in processing natural images, but not necessarily in properly handling geometric transformations in the data. We describe below some of the recent attempts that have been proposed to construct transformation-invariant architectures. We further review quickly the recent works that extend deep learning data represented on graphs or networks."}, {"heading": "2.1. Transformation-invariant deep learning", "text": "One intuitive way to make the classification architectures more robust to isometric transformations is to augment the training set with transformed data (e.g., (Dyk & Meng, 2012)), which however, increases both the training set and training time. Alternatively, there have been works that incorporate sort of data augmentation inside the network learning framework. The authors in (Fasel & Gatica-Perez, 2006) construct deep neural networks that operate in parallel on the original and transformed images simultaneously with weight-shared convolutional filters. Then, the authors in (Laptev et al., 2016) extend this multi-column deep neural networks with averaging the output of all the columns to provide the final classification label. A different approach was proposed in (Jaderberg et al., 2015), where the authors introduce a new spatial transformer layer that deforms images according to a predefined transformation class. Then, the work in (Marcos et al., 2016) suggests using rotated filter banks and a special max pooling operation to combine their outcomes and improve invariance to transformations. The authors in (Cohen & Welling, 2016) propose a generalization of the ConvNets and introduce equivariance to 90\u25e6 rotations and flips by exploiting Group Theory arguments in their architecture and introducing a novel Gconvolutional layer. Finally, the authors in (Dieleman et al., 2015) exploit rotation symmetry in the Convolutional Network for the specific problem of galaxy morphology prediction. This work has been extended in (Dieleman et al., 2016) which introduces an additional layer that makes the network to be partially invariant to rotations. All the above methods, however, still need to be trained on a large dataset of randomly rotated images in order to be rotation invariant and achieve effective performance.\nContrary to the previous methods, we propose to directly learn feature representations that are invariant to isometric data transformations. With such features, our architecture preserves all the advantages of deep networks, but additionally provides invariance to isometric geometric transformations. The methods in (Oyallon & Mallat, 2015; Bruna & Mallat, 2013; Worrall et al., 2016) are the closest in spirit to ours. In order to be invariant to local transformation, the works in (Oyallon & Mallat, 2015; Bruna & Mallat, 2013) propose to replace the classical convolutional layers with wavelets, which are stable to some deformations. The latter achieves high performance on texture classification task, however it does not improve the performance of supervised ConvNets on natural images, due to the fact that the final feature representations are too rigid and unable to adapt to a specific task. Finally, a very recent work (Worrall et al., 2016) proposes a so called Harmonic Network, which uses specifically designed complex valued filters to make feature representations equivariant to rotations. This method, however, still requires the training dataset to contain exam-\nples of rotated images to achieve its full potential. On the other hand, we propose building features that are inherently invariant to isometric transformations, which allows us to train more compact networks and achieve state-of-the-art results."}, {"heading": "2.2. Deep learning and graph signal processing", "text": "While there has been a lot of research efforts related to the application of deep learning methods to traditional data like 1-D speech signals or 2-D images, it is only recently that researchers have started to consider the analysis of network or graph data with such architectures (Kipf & Welling, 2016; Henaff et al., 2015; Duvenaud et al., 2015; Jain et al., 2015). The work in (Bruna et al., 2014) has been among the pioneering efforts in trying to bridge the gap between graph-based learning and deep learning methods. The authors calculate the projection of graph signals onto the space defined by the eigenvectors of the Laplacian matrix of the input graph, which itself describes the geometry of the data. It however requires an expensive calculation of the graph eigendecomposition, which can be a strong limitation for large graphs, as it requires O(N3) operations with N being the number of nodes in the graph. The authors in (Defferrard et al., 2016) later propose an alternative to analyse network data, which is built on a vertex domain feature representation and on fast spectral convolutional filters. Both methods directly integrate the graph features into a fully-connected layer similarly to classical ConvNets, which is however not directly amenable to transformation-invariant image classification.\nSome recent works further apply deep networks to particular graph data analysis tasks. For example, the authors in (Masci et al., 2015) generalize the ConvNets paradigm to the extraction of feature descriptors for 3D shapes that are defined on different graphs. The work in (Duvenaud et al., 2015) further applies deep architectures to train descriptors of chemical molecules, which can be used to predict properties of novel molecules. In (Perozzi et al., 2014), the authors introduce deep networks to analyze web-scale graphs using random walks, which can be used for social network classification tasks. The above algorithms are however specifically developed for a particular task, therefore their generalization to other problems is often difficult.\nTo the best of our knowledge, the current approaches to deep learning on graphs do not provide transformationinvariance in image classification. At the same time, the methods that specifically target transformation invariance in image datasets mostly rely on data augmentation, which largely remains an art. We propose to bridge this gap and present a novel method that uses the power of graph signal processing to add translation and rotation invariance to the image feature representation learned by deep networks."}, {"heading": "3. Graph signal processing elements", "text": "We now briefly review some elements of graph signal processing that are important in the construction of our novel framework. We represent an input image as a signal y(vn) on the nodes {vn} of the grid graph G. In more details, G = {V, E , A} is an undirected, weighted and connected graph, where V is a set of N vertices (i.e., the image pixels), E is a set of edges and A is a weighted adjacency matrix. An edge e(vi, vj) that connects two nodes vi and vj is associated with the weight aij = aji, which is usually chosen to capture the distance between both vertices. The edge weight is set to zero for pairs of nodes that are not connected, and all the edge weights together build the adjacency matrix A. Every vertex vn of G carries the luminance value of the corresponding image pixel. Altogether, the valued vertices define a graph signal y(vn) : V \u2192 R.\nSimilarly to regular 1-D or 2-D signals, the graph signals can be efficiently analysed via harmonic analysis and processed in the spectral domain (Shuman et al., 2013). In that respect, we first consider the normalized graph Laplacian operator of the graph G, defined as\nL = I \u2212D\u22121/2AD\u22121/2,\nwhere D is a diagonal degree matrix with elements di =\u2211N n=0,n6=iAni. The Laplacian operator is a real symmetric and positive semidefinite matrix, which has a set of orthonormal eigenvectors and corresponding eigenvalues. Let \u03c7 = [\u03c70, \u03c71, . . . , \u03c7N\u22121] denote these eigenvectors and {0 = \u03bb0 \u2264 \u03bb1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbN\u22121} denote the corresponding eigenvalues with \u03bbN\u22121 = \u03bbmax = 2 for the normalized Laplacian L. The eigenvectors form a Fourier basis and the eigenvalues carry a notion of frequencies as in the classical Fourier analysis. The Graph Fourier Transform y\u0302(\u03bbi) at frequency \u03bbi for signal y and respectively the inverse graph Fourier transform for the vertex vn \u2208 V are thus defined as:\ny\u0302(\u03bbi) = N\u2211 n=1 y(vn)\u03c7 \u2217 i (vn), (1)\nand\ny(vn) = N\u22121\u2211 i=0 y\u0302(\u03bbi)\u03c7i(vn). (2)\nEquipped with the above notion of Graph Fourier Transform, we can denote the generalized convolution of two graph signals y1 and y2 with help of the graph Laplacian eigenvectors as\n(y1 \u2217 y2)(vn) = N\u22121\u2211 i=0 y\u03021(\u03bbi)y\u03022(\u03bbi)\u03c7i(vn). (3)\nBy comparing the previous relations, we can see that the convolution in the vertex domain is equivalent to the mul-\ntiplication in the graph spectral domain. Graph spectral filtering can further be defined as\ny\u0302f (\u03bbi) = y\u0302(\u03bbi)h\u0302(\u03bbi), (4)\nwhere h\u0302(\u03bbi) is the spectral representation of the graph filter h(vn) and y\u0302f (\u03bbi) is the Graph Fourier Transform of the filtered signal yf . In a matrix form, the graph filter can be denoted by H \u2208 RN\u00d7N : H = \u03c7H\u0302\u03c7T , where H\u0302 is a diagonal matrix constructed on the spectral representation of the graph filter:\nH\u0302 = diag(h\u0302(\u03bb0), . . . , h\u0302(\u03bbN\u22121)). (5)\nThe graph filtering process becomes yf = Hy, with the vectors y and yf being the graph signal and its filtered version in the vertex domain. Finally, we can define the generalized translation operator Tvn for a graph signal y as the convolution of y with a delta function \u03b4vn centered at vertex vn (Thanou et al., 2014):\nTvny = \u221a N(y \u2217 \u03b4vn) = \u221a N \u2211N\u22121\ni=0 y\u0302(\u03bbi)\u03c7 \u2217 i (vn)\u03c7i.\n(6)\nMore details about the above graph signal processing operators can be found in (Shuman et al., 2013)."}, {"heading": "4. Graph-based convolutional network", "text": "We now present the overview of our new architecture, which is illustrated in Fig. 2. The input to our system can be characterized by a normalized Laplacian matrix L computed on the grid graph G and the signal y0 = (y0(v1), . . . , y0(vN )), where y0(vj) is the intensity of the pixel j in the input image and N is the number of pixels in the images. Our network eventually returns a class label for each input signal.\nIn more details, our deep learning architecture consists of an alternation of spectral convolution layers F l and dynamic pooling layers P l. They are followed by a statistical layerH and a sequence of fully-connected layers (FC) that precedes a softmax operator (SM) that produces a categorical distribution over labels to classify the input data. Both the spectral convolution and the dynamic pooling layers contain Kl operators denoted by F li and P li , i = 1, . . . ,Kl, respectively. Each convolutional layer F li is specifically designed to compute transformation-invariant features on grid graphs. The dynamic pooling layer follows the same principles as the classical ConvNet\u2019s max-pooling operation but preserves the graph structure in the signal representation. Finally, the statistical layer H is a new layer designed specifically to achieve invariance to isometric transformations on grid graphs. It does not have any correspondent in the classical ConvNets architectures. We discuss more thoroughly each of these layers in the remainder of this section."}, {"heading": "4.1. Spectral convolutional layer", "text": "Similarly to the convolutional layers in classical architectures, the spectral convolutional layer l in our network consists of Kl convolutional filters F li , as illustrated in Fig. 3. However, each filter i operates in the graph spectral domain. In order to avoid computing the graph eigendecomposition that is required to perform filtering through Eq. (3), we choose to design our graph filters as smooth polynomial filters of order M (Thanou et al., 2014), which can be written as\nh\u0302(\u03bbl) = M\u2211 m=0 \u03b1m\u03bb m l . (7)\nFollowing the notation of Eq. (5), each filter operator in the spectral convolutional layer l can be written as\nF li = M\u2211\nm=0\n\u03b1li,mLm, (8)\nwhere Lm denotes the Laplacian matrix of power m. The polynomial coefficients {\u03b1li,m} have to be learned during the training of the network, for each spectral convolutional layer l. Each column of this N \u00d7 N operator corresponds to an instance of the graph filter centered at a different vertex of the graph (Thanou et al., 2014). The support of each graph filter is directly controlled by the degree M of the polynomial kernel, as the filter takes values only on vertices that are less than M-hop away from the filter center. Larger values of M require more parameters but allow training more complex filters. Therefore, M can be seen as a counterpart of the filter\u2019s size parameter in the classical ConvNets.\nThe filtering operation then simply consists in multiplying the graph signal by the transpose of the operator defined in Eq. (8), namely\ny\u0303li,k = [ F li |N l\u22121i ]T ylk, (9)\nwhere ylk and y\u0303 l i,k are the graph signals at the input and respectively the output of the lth spectral convolutional layer (see Fig. 3). In particular, y(1)k = y0 is the input image for the first level filter, while at the next levels of the network ylk is rather one of the feature maps output by the lower layers. We finally use the notation A|N li to represent an operator that preserves the columns of the matrix A, which have an index in the setN li , and set all the other columns to zero. This operator permits computing the filtering operations only on specific vertices of the graphs. It is important to note that the spectral graph convolutional filter permits equivariance to isometric transformations, which is a key property for designing a classifier that is invariant to rotation and translation.\nFinally, the output of the lth spectral convolutional layer is a set of Kl feature maps zli. Each i\nth feature map is computed as a linear combination of the outputs of the corresponding polynomial filter as follows:\nzli = Kl\u22121\u2211 k=1 \u03b2lk y\u0303 l i,k, (10)\nwhere the set of signals y\u0303li,k are the outputs of the i th polynomial filter applied on the Kl\u22121 input signals of the spectral convolutional layer with Eq. (9). The vector of parameters {\u03b2lk}, for each spectral convolutional layer l is learned during the training of the network. The operations in the spectral convolutional layer are illustrated in Fig. 3. Lastly, the complexity of spectral filtering can be computed based on the fact that L and thus the filters are sparse matrices. Then, the complexity is O(|EM |N) where |EM | is a maximum number of nonzero elements in the columns of F li ."}, {"heading": "4.2. Dynamic pooling layer", "text": "In classical ConvNets the goal of pooling layers is to summarize the outputs of filters for each operator at the previous convolutional layer. Inspired by (Kalchbrenner et al., 2014) we introduce a novel layer that we refer to as dynamic pooling layer, which basically consists in preserving only the most important features at each level of the network.\nIn more details, we perform a dynamic pooling operation, which is essentially driven by the set of graph vertices of\ninterest \u2126l. This set is initialised to include all the nodes of graph, i.e., \u2126(1) = V . It is then successively refined along the progression through the multiple layers of the network. More particularly, for each dynamic pooling layer l, we select the Jl vertices that are part of \u2126l\u22121 and that have the highest values in zli. The indexes of these largest valued vertices form a set of nodesN li . The union of these sets for the different features maps zli form the new set \u2126 l, i.e.,\n\u2126l = Kl\u22c3 i=1 N li . (11)\nThe sets \u2126l drives the pooling operations at the next dynamic pooling layer P l+1. We note that, by construction, the different sets \u2126l are embedded, namely we have \u2126l \u2287 \u2126l+1, \u2200l \u2208 [1..L]. The Algorithm 1 summarize our approach, and Fig. 4 illustrates the effect of the pooling process through the different network levels.\nAlgorithm 1 Dynamic pooling layer at layer l. 1: Input: Feature maps zli, i \u2208 [1,Kl] 2: Set of nodes of interest \u2126l\u22121\n3: Number of active nodes, Jl\n4: for i \u2208 [1,Kl] do 5: N li = V 6: N li = \u2205 7: for j \u2208 [1,max ( Jl, |\u2126l\u22121| ) ] do 8: \u03bd = arg max v\u2208 ( \u2126l\u22121\u2229N li\n) zli(v) 9: N li = N li \\ {\u03bd}\n10: N li = N li \u222a {\u03bd} 11: end for 12: end for\n13: \u2126l = Kl\u22c3 i=1 N li\nThe sets N li are used to control the filtering process at the next layer. The spectral convolutional filters F l+1i compute the output of filters centred on the nodes in N li that are selected by the dynamic pooling layer, and not necessarily for all the nodes in the graph. The filtering operation is given by Eq. (9).\nFinally, we note that one of the major differences with the classical max-pooling operator is that our dynamic pooling layer is not limited to a small neighbourhood around each node. Instead, it considers the set of nodes of interest \u2126l which is selected over all graph\u2019s nodes. The dynamic pooling operator P l is thus equivariant to the isometric transformations R, similarly to the spectral convolutional layers, which is a key property in building a transformationinvariant classification architecture. The complexity of P l is comparable with the classical pooling operator as the task ofP l is equivalent to finding Jl highest statistics. Using the selection algorithm (Knuth, 1998) we can reach the average computational complexity of O(N)."}, {"heading": "4.3. Upper layers", "text": "After the series of alternating spectral convolutional and dynamic pooling layers, we add output layers that compute the label probability distributions for the input images. Instead of connecting directly a fully-connected layer as in classical ConvNet architectures, we first insert a new statistical layer, whose output is then fed into fully-connected layers (see Fig. 2).\nThe main motivation for the statistical layer resides in our objective of designing a transformation-invariant classification architecture. If fully-connected layers are added directly on top of the last dynamic pooling layers, their neurons would have to memorize large amounts of information corresponding to the different positions and rotation of the visual objects. Instead, we propose to insert a new statistical layer, which computes transformation-invariant statistics of the input signal distributions.\nIn more details, the statistical layer estimates the distribution of values on the active nodes after the last pooling layer. The inputs of the statistical layer j are denoted as z\u0303i, which correspond to the outputs z j\u22121 i of the last pooling layer Pj\u22121 where the values on non-active nodes (i.e.,\nthe nodes in N li ) are set to zero. We then calculate multiscale statistics of these input features maps using Chebyshev polynomials of the graph Laplacian. These polynomials have the advantage of a fast computation due to their iterative construction, and they can be adapted to distributed implementations (Shuman et al., 2011). In order to construct these polynomials, we first shift the spectrum of the Laplacian L to the interval [\u22121, 1], which is the original support of Chebyshev polynomials. Equivalently, we set L\u0303 = L \u2212 I .\nAs suggested in (Defferrard et al., 2016), for each input feature map z\u0303i we iteratively construct a set of signals ti,k using graph Chebyshev polynomials of order k, with k \u2264 Kmax, as\nti,k = 2L\u0303ti,k\u22121 \u2212 ti,k\u22122, (12)\nwith ti,0 = z\u0303i and ti,1 = L\u0303z\u0303i. We finally compute a feature vector that gathers the first order statistics of the magnitude of these signals, namely the mean \u00b5i,k and variance \u03c32i,k for each signal |ti,k|. This forms a feature vector \u03c6i of 2Kmax + 2 elements, i.e., \u03c6i = [\u00b5i,0, \u03c3 2 i,0, . . . , \u00b5i,Kmax , \u03c3 2 i,Kmax\n]. We choose these particular statistics as they are prone to efficient gradient computation, which is important during back propagation. Furthermore, we note that such feature vectors are inherently invariant to transformation such as translation or rotation.\nThe feature vectors \u03c6i\u2019s are eventually sent to a series of fully-connected layers similarly to classical ConvNet architectures. However, since our feature vectors are transformation invariant, the fully-connected layers will also benefit from these properties. This is in opposition to their counterparts in classical ConvNet systems, which need to compute position-dependent parameters. The details about fully-connected layer parameters are given in the Section 5. The output of the fully-connected layers is then fed to a softmax layer (Bishop, 2006), which finally returns the probability distribution of a given input sample to belong to a given set of classes."}, {"heading": "4.4. Training", "text": "We use supervised learning and train our network so that it maximizes the log-probability of estimating the correct class of training samples via logistic regression. Overall, we need to compute the values of the parameters in each convolutional and in fully-connected layers. The other layers do not have any parameter to be estimated. We train the network using a classical back-propagation algorithm and learn the parameters using ADAM stochastic optimization (Kingma & Ba, 2014).\nWe provide more details here about the computation that are specific to our new architecture. We refer the reader to (Rumelhart et al., 1988) for more details about the over-\nall training procedure. The back-propagation in the spectral convolutional layer is performed by evaluating the partial derivatives with respect to the parameters \u03b1 : \u03b1 \u2208 RKl\u22121\u00d7M of the spectral filters, and to the parameters \u03b2 : \u03b2 \u2208 RKl\u22121 of the feature map construction. The partial derivatives read\n\u2202E\n\u2202\u03b1li,m = Kl\u22121\u2211 k=0 \u03b2lk [ Lm|N l\u22121i ] yl\u22121k \u2202E \u2202zli , (13)\n\u2202E \u2202\u03b2lj = M\u2211 m=0 \u03b1li,m [ Lm|N l\u22121i ] yl\u22121j \u2202E \u2202zli , (14)\nwhere E is the negative log-likelihood cost function, zli = yli is the output feature map of layer l, Kl\u22121 denotes the number of feature maps at the previous layer of the network, M is the polynomial degree of the convolutional filter and L is the Laplacian matrix. Then, we further need to compute the partial derivatives with respect to the previous feature maps as follows\n\u2202E\n\u2202yl\u22121j = \u03b2lj M\u2211 m=0 \u03b1li,m [ Lm|N l\u22121i ] \u2202E \u2202zli . (15)\nOur new dynamic pooling layers, as well as our statistical layer do not have parameters to be trained. Similarly to the max-pooling operator our dynamic pooling layer permits back-propagation through the active nodes since the gradient is 0 for the non-selected nodes and not zero for the chosen ones. Further, the statistical layer back-propagates the gradients as follows:\n\u2202E\n\u2202ti,k =\n1\nN\n\u2202E\n\u2202\u00b5i,k , (16)\n\u2202E\n\u2202ti,k = 2(N \u2212 1) N2 N\u2211 i=1 (ti,k \u2212 \u00b5i,k) \u2202E \u2202\u03c32i,k , (17)\nwhere \u00b5i,k, \u03c32i,k are the inputs to the first fully-connected layer and the outputs of the statistical layer. The derivatives \u2202E/\u2202z\u0303i are then computed as:\n\u2202E \u2202z\u0303i = Kmax\u2211 k=0 \u2202E \u2202ti,k \u2202ti,k \u2202z\u0303i , (18)\nwhere \u2202ti,k/\u2202z\u0303i are simply the derivatives of Chebyshev polynomials (Shuman et al., 2011) with maximum order Kmax. Please note that we use the non-linear absolute function |ti,k| before statistical layer, therefore, the gradient at ti,k = 0 is not defined. In practice, however, we set it to 0, which gives us a nice property of encouraging some feature map values to be 0 and favors sparsity.\nFinally, the parameters of the fully-connected layers are trained in a classical way, similarly to the training of fully-connected layers in ConvNet architectures (Rumelhart et al., 1988)."}, {"heading": "5. Experiments", "text": "In this section we analyze the results and compare our network to the state-of-the-art transformation-invariant classification algorithms. We first describe the experimental settings. We then analyze our architecture and the influence of the different design parameters. Finally we compare our network to the state-of-the-art transformation-invariant classification algorithms."}, {"heading": "5.1. Experimental settings", "text": "The initialization of the system may have some influence on the actual values of the parameters after training. We have chosen to initialize the parameters \u03b1li,m (Eq. 8) of our spectral convolutional filters so that the different filters uniformly cover the full spectral domain. We first create a set of Z overlapping rectangular functions w(\u03bb, ai, bi)\nw(\u03bb, ai, bi) = { 1 if ai < \u03bb < bi, 0 otherwise.\n(19)\nThe non-zero regions for all functions have the same size, and the set of functions covers the full spectrum of the normalized laplacian L, i.e., [0, 2]. We finally approximate each of these rectangular functions by a M -order polynomial, which produces a set of initial coefficients \u03b1li,m that are used to define the initial version of the spectral filterF li . Then, the initial values of the parameters \u03b2 in the spectral convolutional layer are distributed uniformly in [0, 1] and those of the parameters in the fully-connected layers are selected uniformly in [\u22121, 1].\nWe run experiments with different numbers of layers and parameters. For each architecture, the network is trained using back-propagation with Adam (Kingma & Ba, 2014) optimization. The exact formulas of the partial derivatives are provided in the supplementary material.\nOur architecture has been trained and tested on different datasets, namely:\n\u2022 MNIST-012. This is a small subset of the MNIST dataset (LeCun & Cortes, 2010). It includes 500 training, 100 validation and 100 test images selected randomly from the MNIST images with labels \u20180\u2019, \u20181\u2019 and \u20182\u2019. This small dataset permits studying the behavior of our network in detail and to analyze the influence of each of the layers on the performance. \u2022 Rotated and translated MNIST. To test the invariance to rotation and translation of the objects in an image we create MNIST-rot and MNIST-trans datasets respectively. Both of these datasets contain 50k training, 3k validation and \u223c9k test images. We use all MNIST digits (LeCun & Cortes, 2010) except \u20189\u2019 as\nit is rotated version resembles \u20186\u2019. In order to be able to apply transformation to the digits, we resize the MNIST-rot to the size 26 \u00d7 26 and MNIST-trans to the 34\u00d7 34. The training and validation data of these datasets contain images of digits without any transformation. However, the testing set of MNIST-rot contains randomly rotated digits by angles in range [0\u25e6, 360\u25e6], while the testing set of MNIST-trans comprises randomly translated MNIST examples up to\u00b16 pixels in both vertical and horizontal directions. \u2022 ETH-80. This dataset (Leibe & Schiele, 2003) con-\ntains images of 80 objects that belong to 8 classes. Each object is represented by 41 images captured from different viewpoints located on a hemisphere. The dataset shows a real life example where isometric transformation invariant features are useful for the object classification. We resize the images to [50 \u00d7 50] and randomly select 2300 and 300 of them as the training and validation sets and we use the rest of the images for testing.\nFor all these datasets, we define G as a grid graph where each node corresponds to a pixel location and is connected with 8 its nearest neighbors with a weight that is equal to 1. The pixel luminance values finally define the signal y on the graph G for each image."}, {"heading": "5.2. TIGraNet Analysis", "text": "We analyze the performance of our new architecture on the MNIST-012 dataset. We first give some examples of feature maps that are produced by our network. We then illustrate the spectral kernels learned by our system, and discuss\nthe influence of dynamic pooling operator.\nWe first confirm the transformation invariant properties of our architecture. Even though our classifier is trained on images without any transformations, it is able to correctly classify rotated images in the test set, since our spectral convolutional layer learns filters that are equivariant to isometric transformations. We illustrate this in Fig. 5, which depicts several examples of feature maps y2i from the second spectral convolutional layer for randomly rotated input digits in the test set. Each row of Fig. 5 corresponds to images of a different digit, and we see that the corresponding feature maps are very close to each other (up to the image rotation) even when the rotation angle is quite large. This confirms that our architecture is able to learn features that are preserved with rotation, even if the training has been performed on non-transformed images. Despite important similarities in feature maps of rotated digits, one may however observe some slightly different values for the intensity. This can be explained by the fact that rotated versions of the input images may differ a bit from the original images due to interpolation artifacts.\nFig. 6 then shows the spectral representation of the kernels learned for the first two spectral convolutional layers of our network. As expected, the network learns filters that are quite different from each other in the spectral domain but that altogether cover the full spectrum. They permit to efficiently combine information in the different bands of frequency in the spectral representation of the input signal. Generally, the filters in the upper spectral convolutional layers are more diverse and represent more complicated features than those for the lower ones.\nFinally, we look at the influence of the new dynamic pooling layers in our architecture. Recall that dynamic pooling is used to reduce the network complexity and to focus on the representative parts of the input signal. Fig. 7 depicts the intermediate feature maps of the network for sample test images. We can see that after each pooling operation the signal is getting more and more sparse, while structure of the data that is important for discriminating images in different classes is preserved. That shows that our dynamic pooling operator is able to retain the important information in the feature maps constructed by the spectral convolutional layers."}, {"heading": "5.3. Performance evaluation", "text": "Here, we compare TIGraNet to state-of-the art algorithms for transformation-invariant image classification tasks, i.e., ConvNet (Boureau et al., 2010), Spatial Transformer Network (STN) (Jaderberg et al., 2015), Deep Scattering (DeepScat) (Oyallon & Mallat, 2015) and Harmonic Networks (HarmNet) (Worrall et al., 2016). Briefly, ConvNet is a classical convolutional deep network that is invariant to small image translations. STN compensates for image transformations by learning the affine transformation matrix. Further, DeepScat uses filters based on rich wavelet representation to achieve transformation invariance, however, it does not contain any parameters for the convolutional layers. Finally, HarmNet trains complex valued filters that are equivariant to signal rotations. For the sake of fairness in our comparisons, we use versions of these architectures that have roughly the same number of parameters, which means that each of the approaches learns features with a comparable complexity. For the DeepScat we use the default architecture. Further for the HarmNet we preserve the default network structure, keeping the same number of complex harmonic filters, as the number of spectral convolutional filters that we have in TIGraNet.\nWe first compare the performance of our algorithm to the ones of ConvNet and STN for the small digit dataset MNIST-012. The specific architectures used in this experiments are given in Table 1.\nThe results of this first experiment are presented in Table. 2. We can see that if we train the methods on the dataset that does not contain rotated images and test on the rotated im-\nages of digits, our approach achieves a significant increase in performance (i.e., 86%), due to its inherent transformation invariant characteristics. We further run experiments where a simple augmentation of the training set is implemented with randomly rotated each image of digits. This permits increasing the performance of all algorithms, as expected, possibly at the price of more complex training. Still, due to the rotation invariant nature of its features, TIGraNet is still able to achieve higher classification accuracy than all its competitors.\nWe then run experiments on the MNIST-rot and MNISTtrans datasets. Note that both of them do not contain any isometric transformation in training and validation sets, but the test set contains transformed images. For all the methods we have used the architectures defined in Table 1. Table 3 shows that our algorithm significantly outperforms the competitor methods on both datasets due to its transformation invariant features. The other architectures have only limited capabilities with respect to such transformation as rotation.\nTo further analyze the performance of our network we illustrate several sample feature maps for the different filters of the first two spectral convolutional layers of TIGraNet in Fig. 8, for the MNIST-rot and MNIST-trans datasets. We can see a few examples of misclassification of our network; for example, the algorithm predicts label \u20185\u2019 for the digit \u20186\u2019. This mostly happens due to the border artifacts; if the digit is shifted too close to the border due to an isomet-\nric transformation, then the neighborhood of some nodes may change. This problem can be solved by increasing the image borders or applying filters only to the central pixel locations.\nFinally, we evaluate the performance of our algorithm in more realistic settings where the objective is to classify images of objects that are captured from different viewpoints. This task requires having a classifier that is invariant to isometric transformations of the input signal. We therefore run experiments on the ETH-80 dataset and compare the classification performance of TIGraNet to those of ConvNet, STN, DeepScat and HarmNet. The architectures of the different methods are described in Table 1.\nTable 4 shows the classification results in this experiment. We can see that our approach outperforms the state-of-theart methods due to its transformation invariant features. The closest performance is achieved by Harmonic Networks, since this architecture also learns equivariant features. It is important to note that the ETH-80 dataset contains less training examples than other publicly available datasets that are commonly used for the training of deep neural networks. This likely results in decrease of accuracy for methods such as ConvNets and STN. On the contrary, our method is able to achieve good accuracy even with small amounts of training data, due to its inherent invariance to isometric transformations.\nOverall, all the above experiments confirm the benefit of our transformation invariant classification architecture, which learns features that are invariant to transformation by construction. Classification performance improves with these features, such that the algorithm is able to reach sustained performance even if the training set is relatively small, or does not contain similar transformed images as in the test set. These are very important advantages in practice."}, {"heading": "6. Conclusion", "text": "In this paper we present a new transformation invariant classification architecture, which combines the power of\nGraph-based Isometry Invariant Representation Learning fir st la ye r\nse co\nnd la\nye r\nFigure 8. Network feature maps visualization. Each row shows the feature maps of different digits after the first and the second spectral convolutional layers. The misclassified images are marked by red bounding boxes. (best seen in color)\ndeep networks and graph signal processing, which allows developing filters that are equivariant to translation and rotation. A novel statistical layer further renders our full network invariant to the isometric transformations. This permits outperforming state-of-the-art algorithms on various illustrative benchmarks. Our new method is able to correctly classify rotated and translated images even if such transformed images do not appear in the training set. This confirms its high potential in practical settings where the training sets are limited but where the data is expected to present high variability."}, {"heading": "7. Acknowledgment", "text": "The authors thank Dr Dorina Thanou, Damian Foucard and Dr Andreas Loukas for comments that help to improve the paper and we gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research."}], "references": [{"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop,? \\Q2006\\E", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "A Theoretical Analysis of Feature Pooling in Visual Recognition", "author": ["Y.L. Boureau", "J. Ponce", "Y. LeCun"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Boureau et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2010}, {"title": "Invariant scattering convolution networks", "author": ["J. Bruna", "S. Mallat"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Bruna and Mallat,? \\Q2013\\E", "shortCiteRegEx": "Bruna and Mallat", "year": 2013}, {"title": "Spectral Networks and Locally Connected Networks on Graphs", "author": ["J. Bruna", "W. Zaremba", "A. Szlam", "Y. LeCun"], "venue": "In International Conference for Learning Representations,", "citeRegEx": "Bruna et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2014}, {"title": "Group equivariant convolutional networks", "author": ["T.S. Cohen", "M. Welling"], "venue": "arXiv preprint,", "citeRegEx": "Cohen and Welling,? \\Q2016\\E", "shortCiteRegEx": "Cohen and Welling", "year": 2016}, {"title": "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering", "author": ["M. Defferrard", "X. Bresson", "P. Vandergheynst"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Defferrard et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Defferrard et al\\.", "year": 2016}, {"title": "Rotationinvariant convolutional neural networks for galaxy morphology prediction", "author": ["S. Dieleman", "K.W. Willett", "J. Dambre"], "venue": "Monthly notices of the royal astronomical society,", "citeRegEx": "Dieleman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dieleman et al\\.", "year": 2015}, {"title": "Exploiting cyclic symmetry in convolutional neural networks", "author": ["S. Dieleman", "J.D. Fauw", "K. Kavukcuoglu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Dieleman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dieleman et al\\.", "year": 2016}, {"title": "Convolutional networks on graphs for learning molecular fingerprints", "author": ["P. R"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "R.,? \\Q2015\\E", "shortCiteRegEx": "R.", "year": 2015}, {"title": "The art of data augmentation", "author": ["D.A. Dyk", "Meng", "X.-L"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Dyk et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dyk et al\\.", "year": 2012}, {"title": "Rotation-invariant neoperceptron", "author": ["B. Fasel", "D. Gatica-Perez"], "venue": "In International Conference on Pattern Recognition,", "citeRegEx": "Fasel and Gatica.Perez,? \\Q2006\\E", "shortCiteRegEx": "Fasel and Gatica.Perez", "year": 2006}, {"title": "Deep convolutional networks on graph-structured data", "author": ["M. Henaff", "J. Bruna", "Y. LeCun"], "venue": "arXiv preprint,", "citeRegEx": "Henaff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Henaff et al\\.", "year": 2015}, {"title": "A Convolutional Neural Network for Modelling Sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "arXiv Preprint,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv Preprint,", "citeRegEx": "Kingma and Ba,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Semi-supervised classification with graph convolutional networks", "author": ["T.N. Kipf", "M. Welling"], "venue": "arXiv preprint,", "citeRegEx": "Kipf and Welling,? \\Q2016\\E", "shortCiteRegEx": "Kipf and Welling", "year": 2016}, {"title": "The Art of Computer Programming: Sorting and Searching", "author": ["D.E. Knuth"], "venue": null, "citeRegEx": "Knuth,? \\Q1998\\E", "shortCiteRegEx": "Knuth", "year": 1998}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "E.H. Geoffrey"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "TI-Pooling: Transformation-Invariant Pooling for Feature Learning in Convolutional Neural Networks", "author": ["D. Laptev", "N. Savinov", "J.M. Buhmann", "M. Pollefeys"], "venue": "In Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Laptev et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Laptev et al\\.", "year": 2016}, {"title": "GradientBased Learning Applied to Document Recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In Intelligent Signal Processing,", "citeRegEx": "LeCun et al\\.,? \\Q2001\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2001}, {"title": "Analyzing appearance and contour based methods for object categorization", "author": ["B. Leibe", "B. Schiele"], "venue": "In Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Leibe and Schiele,? \\Q2003\\E", "shortCiteRegEx": "Leibe and Schiele", "year": 2003}, {"title": "Learning rotation invariant convolutional filters for texture classification", "author": ["D. Marcos", "M. Volpi", "D. Tuia"], "venue": "arXiv preprint,", "citeRegEx": "Marcos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Marcos et al\\.", "year": 2016}, {"title": "Geodesic Convolutional Neural Networks on Riemannian Manifolds", "author": ["J. Masci", "D. Boscaini", "M.M. Bronstein", "P. Vandergheynst"], "venue": "In International Conference on Computer Vision Workshops,", "citeRegEx": "Masci et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Masci et al\\.", "year": 2015}, {"title": "Deep roto-translation scattering for object classification", "author": ["E. Oyallon", "S. Mallat"], "venue": "In Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Oyallon and Mallat,? \\Q2015\\E", "shortCiteRegEx": "Oyallon and Mallat", "year": 2015}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "In International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Perozzi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Perozzi et al\\.", "year": 2014}, {"title": "U-net: Convolutional networks for biomedical image segmentation", "author": ["O. Ronneberger", "P. Fischer", "T. Brox"], "venue": "In Medical Image Computing and Computer-Assisted Intervention,", "citeRegEx": "Ronneberger et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ronneberger et al\\.", "year": 2015}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Chebyshev polynomial approximation for distributed signal processing", "author": ["D.I. Shuman", "P. Vandergheynst", "P. Frossard"], "venue": "In IEEE International Conference on Distributed Computing in Sensor Systems,", "citeRegEx": "Shuman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shuman et al\\.", "year": 2011}, {"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "author": ["D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Shuman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shuman et al\\.", "year": 2013}, {"title": "Learning parametric dictionaries for signals on graphs", "author": ["D. Thanou", "D.I. Shuman", "P. Frossard"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Thanou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Thanou et al\\.", "year": 2014}, {"title": "Harmonic networks: Deep translation and rotation equivariance", "author": ["D.E. Worrall", "S.J. Garbin", "D. Turmukhambetov", "G.J. Brostow"], "venue": "arXiv Preprint,", "citeRegEx": "Worrall et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Worrall et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "Deep convolutional networks (ConvNets) have achieved impressive results for various computer vision tasks, such as image classification (Krizhevsky et al., 2012) and segmentation (Ronneberger et al.", "startOffset": 136, "endOffset": 161}, {"referenceID": 24, "context": ", 2012) and segmentation (Ronneberger et al., 2015).", "startOffset": 25, "endOffset": 51}, {"referenceID": 1, "context": "Rotated test images, along with their classification label obtained from ConvNets (Conv) (Boureau et al., 2010), Spatial-Transformer Network (STN) (Jaderberg et al.", "startOffset": 89, "endOffset": 111}, {"referenceID": 18, "context": "Most of the recent architectures (LeCun et al., 2001; Krizhevsky et al., 2012) have been very successful in processing natural images, but not necessarily in properly handling geometric transformations in the data.", "startOffset": 33, "endOffset": 78}, {"referenceID": 16, "context": "Most of the recent architectures (LeCun et al., 2001; Krizhevsky et al., 2012) have been very successful in processing natural images, but not necessarily in properly handling geometric transformations in the data.", "startOffset": 33, "endOffset": 78}, {"referenceID": 17, "context": "Then, the authors in (Laptev et al., 2016) extend this multi-column deep neural networks with averaging the output of all the columns to provide the final classification label.", "startOffset": 21, "endOffset": 42}, {"referenceID": 20, "context": "Then, the work in (Marcos et al., 2016) suggests using rotated filter banks and a special max pooling operation to combine their outcomes and improve invariance to transformations.", "startOffset": 18, "endOffset": 39}, {"referenceID": 6, "context": "Finally, the authors in (Dieleman et al., 2015) exploit rotation symmetry in the Convolutional Network for the specific problem of galaxy morphology prediction.", "startOffset": 24, "endOffset": 47}, {"referenceID": 7, "context": "This work has been extended in (Dieleman et al., 2016) which introduces an additional layer that makes the network to be partially invariant to rotations.", "startOffset": 31, "endOffset": 54}, {"referenceID": 29, "context": "The methods in (Oyallon & Mallat, 2015; Bruna & Mallat, 2013; Worrall et al., 2016) are the closest in spirit to ours.", "startOffset": 15, "endOffset": 83}, {"referenceID": 29, "context": "Finally, a very recent work (Worrall et al., 2016) proposes a so called Harmonic Network, which uses specifically designed complex valued filters to make feature representations equivariant to rotations.", "startOffset": 28, "endOffset": 50}, {"referenceID": 11, "context": "While there has been a lot of research efforts related to the application of deep learning methods to traditional data like 1-D speech signals or 2-D images, it is only recently that researchers have started to consider the analysis of network or graph data with such architectures (Kipf & Welling, 2016; Henaff et al., 2015; Duvenaud et al., 2015; Jain et al., 2015).", "startOffset": 282, "endOffset": 367}, {"referenceID": 3, "context": "The work in (Bruna et al., 2014) has been among the pioneering efforts in trying to bridge the gap between graph-based learning and deep learning methods.", "startOffset": 12, "endOffset": 32}, {"referenceID": 5, "context": "The authors in (Defferrard et al., 2016) later propose an alternative to analyse network data, which is built on a vertex domain feature representation and on fast spectral convolutional filters.", "startOffset": 15, "endOffset": 40}, {"referenceID": 21, "context": "For example, the authors in (Masci et al., 2015) generalize the ConvNets paradigm to the extraction of feature descriptors for 3D shapes that are defined on different graphs.", "startOffset": 28, "endOffset": 48}, {"referenceID": 23, "context": "In (Perozzi et al., 2014), the authors introduce deep networks to analyze web-scale graphs using random walks, which can be used for social network classification tasks.", "startOffset": 3, "endOffset": 25}, {"referenceID": 27, "context": "Similarly to regular 1-D or 2-D signals, the graph signals can be efficiently analysed via harmonic analysis and processed in the spectral domain (Shuman et al., 2013).", "startOffset": 146, "endOffset": 167}, {"referenceID": 28, "context": "Finally, we can define the generalized translation operator Tvn for a graph signal y as the convolution of y with a delta function \u03b4vn centered at vertex vn (Thanou et al., 2014):", "startOffset": 157, "endOffset": 178}, {"referenceID": 27, "context": "More details about the above graph signal processing operators can be found in (Shuman et al., 2013).", "startOffset": 79, "endOffset": 100}, {"referenceID": 28, "context": "(3), we choose to design our graph filters as smooth polynomial filters of order M (Thanou et al., 2014), which can be written as", "startOffset": 83, "endOffset": 104}, {"referenceID": 28, "context": "Each column of this N \u00d7 N operator corresponds to an instance of the graph filter centered at a different vertex of the graph (Thanou et al., 2014).", "startOffset": 126, "endOffset": 147}, {"referenceID": 12, "context": "Inspired by (Kalchbrenner et al., 2014) we introduce a novel layer that we refer to as dynamic pooling layer, which basically consists in preserving only the most important features at each level of the network.", "startOffset": 12, "endOffset": 39}, {"referenceID": 15, "context": "Using the selection algorithm (Knuth, 1998) we can reach the average computational complexity of O(N).", "startOffset": 30, "endOffset": 43}, {"referenceID": 26, "context": "These polynomials have the advantage of a fast computation due to their iterative construction, and they can be adapted to distributed implementations (Shuman et al., 2011).", "startOffset": 151, "endOffset": 172}, {"referenceID": 5, "context": "As suggested in (Defferrard et al., 2016), for each input feature map z\u0303i we iteratively construct a set of signals ti,k using graph Chebyshev polynomials of order k, with k \u2264 Kmax, as", "startOffset": 16, "endOffset": 41}, {"referenceID": 0, "context": "The output of the fully-connected layers is then fed to a softmax layer (Bishop, 2006), which finally returns the probability distribution of a given input sample to belong to a given set of classes.", "startOffset": 72, "endOffset": 86}, {"referenceID": 25, "context": "We refer the reader to (Rumelhart et al., 1988) for more details about the over-", "startOffset": 23, "endOffset": 47}, {"referenceID": 26, "context": "where \u2202ti,k/\u2202z\u0303i are simply the derivatives of Chebyshev polynomials (Shuman et al., 2011) with maximum order Kmax.", "startOffset": 69, "endOffset": 90}, {"referenceID": 25, "context": "Finally, the parameters of the fully-connected layers are trained in a classical way, similarly to the training of fully-connected layers in ConvNet architectures (Rumelhart et al., 1988).", "startOffset": 163, "endOffset": 187}, {"referenceID": 1, "context": "Experiments on MNIST-012 ConvNet (Boureau et al., 2010) C[3]-P[2]-C[6]-P[2]-FC[50]-FC[30]-FC[10] STN (Jaderberg et al.", "startOffset": 33, "endOffset": 55}, {"referenceID": 1, "context": "Other experiments ConvNet (Boureau et al., 2010) C[10]-P[2]-C[20]-P[2]-FC[500]-FC[300]-FC[100] STN (Jaderberg et al.", "startOffset": 26, "endOffset": 48}, {"referenceID": 29, "context": ", 2015) C[10]-ST[6]-C[20]-ST[6]-FC[500]-FC[300]-FC[100] DeepScat (Oyallon & Mallat, 2015) W[2, 5]-PCA[20] HarmNet (Worrall et al., 2016) HRC[1, 10]-HCN[10]-HRC[10, 10]-HRC[10, 20]-HCN[20]-HRC[20, 20] TIGraNet SC[10, 4]-DP[600]-SC[20, 4]-DP[300]-S[12]-FC[500]-FC[300]-FC[100]", "startOffset": 114, "endOffset": 136}, {"referenceID": 1, "context": ", ConvNet (Boureau et al., 2010), Spatial Transformer Network (STN) (Jaderberg et al.", "startOffset": 10, "endOffset": 32}, {"referenceID": 29, "context": ", 2015), Deep Scattering (DeepScat) (Oyallon & Mallat, 2015) and Harmonic Networks (HarmNet) (Worrall et al., 2016).", "startOffset": 93, "endOffset": 115}, {"referenceID": 1, "context": "1 ConvNet (Boureau et al., 2010) 80.", "startOffset": 10, "endOffset": 32}, {"referenceID": 29, "context": "3 HarmNet (Worrall et al., 2016) 94.", "startOffset": 10, "endOffset": 32}], "year": 2017, "abstractText": "Learning transformation invariant representations of visual data is an important problem in computer vision. Deep convolutional networks have demonstrated remarkable results for image and video classification tasks. However, they have achieved only limited success in the classification of images that undergo geometric transformations. In this work we present a novel Transformation Invariant Graph-based Network (TIGraNet), which learns graph-based features that are inherently invariant to isometric transformations such as rotation and translation of input images. In particular, images are represented as signals on graphs, which permits to replace classical convolution and pooling layers in deep networks with graph spectral convolution and dynamic graph pooling layers that together contribute to invariance to isometric transformations. Our experiments show high performance on rotated and translated images from the test set compared to classical architectures that are very sensitive to transformations in the data. The inherent invariance properties of our framework provide key advantages, such as increased resiliency to data variability and sustained performance with limited training sets.", "creator": "LaTeX with hyperref package"}}}