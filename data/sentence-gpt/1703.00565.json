{"id": "1703.00565", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ", "abstract": "Scattertext is an open source tool for visualizing linguistic variation between document categories in a language-independent way. The tool presents a scatterplot, where each axis corresponds to the rank-frequency a term occurs in a category of documents. Through a tie-breaking strategy, the tool is able to display thousands of visible term-representing points and find space to legibly label hundreds of them.\n\n\n\n\nThere is one possible solution to this problem:\nIf the authors of the tool have their ideas and the data collected, they would be able to use their data to classify, track, and classify words, and in that case, the words, the numbers, and the meanings. This is by default a word. In fact, this is the standard way in which word recognition is done: the authors write the standard definition of word recognition using words like \"Schnir\" or \"Dvorak\" with a different value: The word recognition algorithm is written in an extremely precise, straightforward manner that will be useful in future iterations of the tool, and will allow users to quickly identify and identify the word names of the word names.\nHowever, the tools themselves do not work in any way at all: the language language can be configured automatically on a new page, and the algorithm automatically makes the changes to the text that will be found. Furthermore, a more powerful and efficient way of interpreting Word-Markers can be done.\nFor example, the term \"Lectured Words\" is currently not currently available as an open source tool, but a new version is coming to the Web soon. This project is part of a project called the \u202aA Tool for Visualizing Language Differences\u202c and aims to help developers and users get better understanding of the language and their specific set of words and sentences. The project was founded as an effort to encourage learners to understand what is and is not spoken in English.\nAt the start of the project, the first issue of the software was the language definition. In this paper, we discussed how to define \"Lectured Words\" (LDR). When a word is in a LDR context, and the word contains a tag to indicate the word, the text is displayed as a text with a different value. This feature can be described as a simple way of categorizing words using words like \"Dvorak\" or \"Dvorak\" with a different value:\nThe word is a semantic structure that can only be translated into two languages and can be classified as", "histories": [["v1", "Thu, 2 Mar 2017 00:48:15 GMT  (1627kb,D)", "http://arxiv.org/abs/1703.00565v1", "6 pages, 5 figures. Seethis https URLfor source code and documentation"], ["v2", "Mon, 6 Mar 2017 19:15:04 GMT  (3923kb,D)", "http://arxiv.org/abs/1703.00565v2", "6 pages, 5 figures. See this Githup repothis https URLfor source code and documentation"], ["v3", "Thu, 20 Apr 2017 21:39:34 GMT  (3924kb,D)", "http://arxiv.org/abs/1703.00565v3", "ACL 2017 Demos. 6 pages, 5 figures. See the Githup repothis https URLfor source code and documentation"]], "COMMENTS": "6 pages, 5 figures. Seethis https URLfor source code and documentation", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["jason s kessler"], "accepted": true, "id": "1703.00565"}, "pdf": {"name": "1703.00565.pdf", "metadata": {"source": "CRF", "title": "Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ", "authors": ["Jason S. Kessler"], "emails": ["jason.kessler@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Finding words and phrases that discriminate categories of text is a common application of statistical NLP. For example, finding words that are most characteristic of a political party in congressional speeches can help political scientists identify means of partisan framing (Monroe et al., 2008; Grimmer, 2010), while identifying differences in word usage between male and female characters in films can highlight narrative archetypes (Schofield and Mehr, 2016). Language use in social media can inform understanding of personality types (Schwartz et al., 2013), and provides insights into customers\u2019 evaluations of restaurants (Jurafsky et al., 2014).\nA wide range of visualizations have been used to highlight discriminating words\u2013 simple ranked lists of words, word clouds, word bubbles, and word-based scatter plots. These techniques have a number of limitations. For example, the difficulty\nin comparing the relative frequencies of two terms in a word cloud, or in legibly displaying term labels in scatterplots. Scattertext1 is an interactive, scalable tool which overcomes many of these limitations. It is built around a scatterplot which displays a high number of words and phrases used in a corpus. Points representing terms are positioned to allow a high number of unobstructed labels and to indicate category association. The coordinates of a point indicate how frequently the word is used in each category.\nFigure 1 shows an example of a Scattertext plot comparing Republican and Democratic political speeches. The higher up a point is on the y-axis, the more it was used by Democrats, and similarly, the further right on the x-axis a point appears, the more its corresponding word was used by Republicans. Highly associated terms fall closer to the upper left and lower right-hand corners of the chart, while stop words fall in the far upper right-hand corner. Words occurring infrequently in both classes fall closer to the lower left-hand corner. When used interactively, mousing-over a point shows statistics about a term\u2019s relative use in the two contrasting categories, and clicking on a term shows excerpts from convention speeches used.\nThe point placement, intelligent word-labeling, and auxiliary term-lists ensure a low-whitespace, legible plot. These are issues which have plagued other scatterplot visualizations showing discriminative language. \u00a72 discusses different views of term-category association that make up the basis of visualizations. In \u00a73, the objectives, strengths, and weaknesses of existing visualization techniques. \u00a74 presents the technical details behind Scattertext. \u00a75 discusses how Scattertext can be used to iden-\n1github.com/JasonKessler/scattertext\nar X\niv :1\n70 3.\n00 56\n5v 1\n[ cs\n.C L\n] 2\nM ar\n2 01\n7\nInfrequent Average Frequent\nRepublican Frequency\nIn fre\nqu en\nt Av\ner ag\ne Fr\neq ue\nnt\nD em\noc ra\ntic F\nre qu\nen cy Top Democratic\nauto insurance companies auto industry pell last week pell grants affordable grants platform reduce access fair clean coverage\nTop Republican unemployment liberty olympics reagan ann founding constitution church free enterprise federal government enterprise sons boy greatness Characteristic obama romney barack mitt obamacare biden romneys hardworking bain grandkids billionaires millionaires ledbetter buenas pell noches bless dreamers congresswoman bipartisan wealthiest risked trillion republicans recession insurmountable gentlemen electing pelosi understands obama romney barack millionaires pell wealthiest trillion gentlemen fought president americans medicare prosper reagan grandparents blaming founding seniors unemployment wealthy veterans blessed rhetoric fathers restore hector olympics stake cares\nstimulus\nbacks\nfighting\ncuts\nfights\nliberty\nbridges\ninspire\nfreedom\nblame\nroosevelt\nwe\nballot\nfailing\nmichelle\ngreatest\ndeeply\nhopes\nheal\ncharlotteaffordable\ncaring\nsuccess\nbelief\nvote\nloyal\nwalks\nmayor\nbetter\nvoting\nworkers\ndetroit\ncents\nbroke\nlaid ran\nopponent\nclass\nmyth\nfair\ngrants\ndrill\nkept\npushed\nshared\njill refuse\npay\nspoke\nworry\nsolve\nloved\nprinciples\nnotion\nmove\nwon\nled\nstory\nrid\nadministration\nfiscal\ntried\npray\nthinks\ndoors\nisrael\nbreaks\ni\nliberal\nminds\niraq\neight\nmate\nthreats\njustice\nshut\nregulations\nbanks\nsupreme\ntells\nlucky\nfix forever\nlie\ntrust\naside\ngm\nyounger\nrise\nera\nwisdom\nkate\nflying\nfacts\nfear\nbegin\nque\nwalk\nhey\nauto\nescape\nhole\nhit\nmean\nhelps\nlay\ncapital\npass\nfelt\ncarefully\nlater\nplayed\ngas\nact\nownerscast\ncancer\nnations\ndefense\nsafe\ninsurance\nsimple\nfail\nbus\ntea\nfall\nedge\nwall\nloans\nchurch rich\nneither\nbeyond math con\nfill\nskills\nfive train\nfirm\nearly\nfuel\nfront\nrace\nleading\nbit\njoin street\nsent district\nre\noh hours\nfeet\naccess\nfine\ntest\nlos\npoint\ntop\nloss\nbaby\nfree\n5\nre elect\n30\n20 1\n29\n4\n42\nroll back\nok.\n2\n8\nturned around\n3 bain capital\n16\ninsurance companies\nDemocratic\u00a0document\u00a0count:\u00a0123\u037e\u00a0word\u00a0count:\u00a076,864 Republican\u00a0document\u00a0count:\u00a066\u037e\u00a0word\u00a0count:\u00a058,138\nSearch for term \u00a0 Type a word or two\u2026 \u00a0\nDownload\u00a0SVG Figure 1: Scattertext visualization of words and phrases used in the 2012 Political Conventions. 2,202 points are colored r d or blue based on the association of their corresponding terms with Democrats or Republicans, 215 of which were labeled. The corpus consists of 123 speeches by Democrats (76,864 words) and 66 by Republicans (58,138 words). The most associated terms are listed under \u201cTop Democrat\u201d and \u201cTop Republican\u201d headings. Interactive version: https://jasonkessler.github.io/st-main.html\ntify category-discriminating terms that are semantically similar to a query."}, {"heading": "2 On text visualization", "text": "The simplest visualization, a list of words ranked by their scores, is easy to produce, interpret and is thus very common in the literature. There are numerous ways of producing word scores for ranking which are thoroughly covered in previous work. The reader is directed to Monroe et al. (2008) (subsequently referred to as MCQ) for an overview of model-based term scoring algorithms. Also of interest, Bitvai and Cohn (2015) present a method for finding sparse words and phrase scores from a trained ANN (with bag-of-words features) and its training data.\nRegardless of how complex the calculation, word scores capture a number of different measures of word-association, which can be interesting when viewed independently instead of as part of a unitary score. These loosely defined measures include: Precision A word\u2019s discriminative power regardless of its frequency. A term that appears once in the categorized corpus will have perfect precision. This (and subsequent metrics) presuppose a balanced class distribution. Words close to the x and\ny-axis in Scattertext have high precision.\nRecall The frequency a word appears in a particular class, or P (word|class). The variance of precision tends to decrease as recall increases. Extremely high recall words tend to be stop-words. High recall words occur close to the top and right sides of Scattertext plots.\nNon-redundancy The level of a word\u2019s discriminative power given other words that co-occur with it. If a word wa always co-occurs with wb and word wb has a higher precision and recall, wa would have a high level of redundancy. Measuring redundancy is non-trivial, and has traditionally been approached through penalized logistic regression (Joshi et al., 2010), as well as through other feature selection techniques. In configurations of Scattertext such as the one discussed at the end of \u00a74, terms can be colored based on their regression coefficients that indicate nonredundancy.\nCharacteristicness How much more does a word occur in than the categories examined than in background in-domain text? For example, if comparing positive and negative reviews of a single movie, a logical background corpus may be reviews of other movies. Highly associated terms\ntend to be characteristic because they frequently appear in one category and not the other. Some visualizations explicitly highlight these, ex. (Coppersmith and Kelly, 2014)."}, {"heading": "3 Past work and design motivation", "text": "Text visualizations manipulate the position and appearance of words or points representing them to indicate their relative scores in these measures. For example, in Schwartz et al. (2013), two word clouds are given, one per each category of text being compared. Words (and selected ngrams) are sized by their linear regression coefficients (a composite metric of precision, recall, and redundancy) and colored by frequency. Only words occurring in geq1% of documents and having Bonferroni-corrected coefficient p-values of <0.001 were shown. Given that these words are highly correlated to their class of interest, the frequency of use is likely a good proxy for recall.\nCoppersmith and Kelly (2014) also describe a word-cloud based visualization for discriminating terms, but intend it for categories which are both small subsets of a much larger corpus. They include a third, middle cloud for terms that appear characteristic.\nWord clouds can be difficult to interpret. It is difficult to compare the sizes of two nonhorizontally adjacent words, as well as the relative color intensities of any two words. By virtue of being larger, longer words can unintentionally appear more important. Sizing of words can be a source of confusion when used to represent precision, since a larger word may naturally be seen as more frequent.\nBostock et al. (2012)2 employed an interesting, interactive word-bubble visualization for exploring different word usage among Republicans and Democrats in the 2012 American Political Conventions. A bubble represents each term, and its size increases with its frequency of use. Its precision is represented by its coloring\u2013 each bubble is colored blue and red, with the blue portion proportionally colored to a term\u2019s relative use by Democrats. Terms were manually chosen, and arranged along the x-axis based on their discriminative power. When clicked, sentences from speeches containing the word used are listed below the visualization. This bubble approach to word clouds inspired Nelson et al. (2015). 2nytimes.com/interactive/2012/09/06/us/politics/conventionword-counts.html\nThe dataset used in Bostock et al. (2012) is used to demonstrate the capabilities of Scattertext in each of these figures. The dataset is accessible the Scattertext Github.\n3.1 Scatterplot visualizations\na\u00f0i\u00dekw 5 a \u00f0i\u00de k0p\u0302 MLE 5 y # a0 n\n\u00f023\u00de\nwhere a\u00f0i\u00dek0 determines the implied amount of information in the prior. This prior shrinks the p\u00f0i\u00dekw and X \u00f0i\u00de kw to the global values, and shrinks the feature evaluation measures, the f \u00f0i\u00de kw and the f\u00f0i2 j\u00dekw , toward zero. The greater a \u00f0i\u00de k0 is, the more shrinkage we will have.\nFig. 5 Feature evaluation and selection based on f\u0302 \u00f0D2R\u00de kw . Plot size is proportional to evaluation weight, f\u0302 \u00f0D2R\u00de\nkw ; those with !!!f\u0302 \u00f0D2R\u00de kw\n!!!<1:96 are gray. The top 20 Democratic and Republican words are\nlabeled and listed in rank order to the right.\n388 Burt L. Monroe, Michael P. Colaresi, and Kevin M. Quinn\nat U niversity of Pennsylvania Library on June 3, 2013\nhttp://pan.oxfordjournals.org/ D ow nloaded from\n's\n@b0rk @benmarwick @coursera @gshotwell\nauto @seanhacks @daattali @aalear gganimate android\n@onlybluefeet\n@juliasilge\n@bigkage\naccepted hop\napplied created\n@flowingdata gif\nadvantage stats sql base\nbroom\npit algebra conf pipe\ncolumns tool knitr\nstack\n@youtube\n8th\nad\ntalks btw\n2x blank bob lazy notice linear\n@jc4p\n@jasonpunyon\nslides\ntraffic\nalive\naloud 90s chunk hurt rule user\n@kara_woo\ntweets\nball\nwars april train ha\nchoice file\narticle\nsupport\niron\nroll\ntech\nposts\nfail ran\n@noamross dplyr\nview @jhollist link\neye\njoin\nadd\nbring\nart\nmath\nbug\ndr\nline\nanalysis\nfeet\num\nyep\nlaws\nrest\npublic\nreason\nsit\nfloor\ntable\ncode\nfan\n@hspter\nfigure\nhotel 3rd\nquestion\ncooking\nstore\nfine word wrong\nmiss\n@hadleywickham\ndog play\ntalk\nslow\ntiny\nnormal\nwatch\nrunning times\n@qu minus\nblog\nfamily\npackage\nbuy\nbought\nhours\nfound start\nfavorite\nidea\nhot\nhard bit\npost\nlol\nstuff\nfood\nd ta\namazing\n#rstats\nlife\nnice\nkids\nhome\ndinner tonight\nweek\nlot\nschool\npretty\npeople\nmorning\nhouse baby\nday\ntime\n0.01%\n0.10%\n1.00%\n0.01% 1.00% Julia\nD av id\nFigure 2: A sample of existing scatterplot visualizati ns. MCQ\u2019s is at the top. Ti ytext (Silge a Robinson, 2016) is below.\nMCQ present a visualization to illustrate the use of their proposed word score, log-odds-ratio with an informative Dirichlet prior (top of Figure 2). This visualization plots word-representing points along two axes. The axes are log10 recall vs. the difference in word scores z-scores. Points with a zscore difference below 1.96 are grayed-out, while the top and bottom 20 are labeled, both by each point and on the right-hand side. The side-labeling is necessary because labels are permitted to overlap, hindering their on-plot readability. The sizes of points and labels are increased proportionally to the word score. This word score encompasses precision, recall, and characteristicness since it penalizes scores of terms used more frequently in the background corpus.\nMCQ used this type of plot to illustrate the different effects of various scoring techniques intro-\nduced in the paper. However, the small number of points which are possible to label limit its utility for in-depth corpus analysis.\nSchofield and Mehr (2016) use essentially the same visualization, but plot over 100 corresponding n-grams next to an unlabeled frequency/zscore plot. While this is appropriate for publication, displaying associated terms and the shape of the score distribution, it is impossible to align all but the highest scoring points to their labels.\nThe tidytext R-package (Silge and Robinson, 2016) documentation includes a non-interactive ggplot2-based scatter plot that is very similar to Scattertext. The x and y-axes both, like in Scattertext, correspond to word frequencies in the two contrasting categories, with jitter added.3 In the example in Figure 2 (bottom), the contrasting categories are tweets from two different accounts. The red diagonal line separates words based on their odds-ratio. Importantly, compared to MCQ, less of this chart\u2019s area is occupied by whitespace.\nWhile tidytext\u2019s labels do not overlap each other (in contrast to MCQ) they do overlap points. The points\u2019 semi-transparency makes labels in lessdense areas legible, the dense interior of the chart is nearly illegible, with both points and labels obscured.\n's\n@b0rk\n@benmarwick\n@coursera\n@gshotwell\nblast auto @sf99\n@seanhacks probability\ncc @daattali\n@nssdeviations @aalear\n@gaborcsardi gganimat\n@simplystats android @fody\n@nick_craver\n@onlybluefeet\n@juliasilge\n@bigkage\naccepted\nclinton's\n@hairboat\ncreated\narticles @flowingdata distribution compare simulation network advantage\nlog\nstats\nsql users\nbase\noverflow\nbroom\niffy\npit\nfacet\neffect\nyork\nfalse\ntidying df\nbaseball\nknitr beta\ndatas t\nstack\naired\n8th\ncmd\n@alexwhan\nad chapter\ncomparing adding gonna\nregression talks\nbtw\n@kwbroman\nvi\n2x\neasily\nbeat\nbc\n@jc4p\n@jasonpunyon\nchia\nfish\naccess\nform\napi\n@timelyportfolio\ntraffic\nchunk\nhurt\nclosed\nuser\nfuture\n@kara_woo\narms\nbar\nfill\nmelt\nsentiment\nframe\nscale matter\ntitle\ntweets\napril\navoid\nha\nsave\nexamples\npackages article\ncup\nfear\ncount\nanswer\nbiggest\nlines\ntech\nadvice aw\nchance\nresults\nposts\ngithub\nsell\nsan\nfail\nran\nhold\n@noamross\ndplyr\n@jhollist\nlink\nstatistics\ntidy\njoin\nadd\ndry\nhat\nvoice\nshiny\ntidytext\nggplot2\nart\nmath\nbug\nfunction\neasier\noptions line\nbased\nanalysis\nnom\nguy\ndue\num\nyep\nrest\npublic\neasy\nloved\nideas\nla\ntalking\nemail\ntable\nwrite\ncode\njam\nfan\nlearning\nnames\nsocial\nfit\n@hspter\nclose\nlist\ntweet\n3rd\nrealized\nquestion\ncooking\nage\nstore\nfine\nword\ncall\nliterally\nlive\nhope\nlibrary\nwrong\nclass\n@hadleywickham\ndog\nplay\nstop\ntalk\nhalf cool\ntaking\nscience\ntiny\nmeet\nnormal\ntrip\nperson\nwatch\nawesome\n@quominus\nwearing\nred wait\nblog\nlate\ncoming\npackage\nbuy\nsad\nhours\nwear\nfound\nstarted\nstart\nago\nsleep\nfavorite\nidea\nhot\ntomorrow\nthinking bad makes\ntotally\nyeah\nhard\nread\npost\nlol\nstuff\nreal\ndata\nsick\namazing\n#rstats\nlife\nnice\nkids\n@drob\nhappy\nhome\nbirthday\ndinner\ntonight\nnight\nweek\nlot\nschool\npretty feel\npeople\nhouse\nbaby\nday\ntime\n0.01%\n0.10%\n1.00%\n0.01% 1.00% Julia\nD av id\nFigure 3: A small cropping from an un-jittered version a the bottom of Figure 2. The dark, opaque points indicate stacks of points.\nFigure 3 shows an excerpt of the same plot, but with no jitter. Words appearing with the same frequency in both categories all become stacked atop each other, however, this provides more interior space for labeling.\nThe next section presents Scattertext and how its approach to word ordering solves the problems discussed above."}, {"heading": "4 Scattertext", "text": "Scattertext builds on tinytext and Rudder (2014). It plots a set of unigrams and bigrams (we will refer to these as \u201cterms\u201d) found in a corpus of docu-\n3This type of visualization may have first been introduced in Rudder (2014).\nments assigned to one of two categories on a twodimensional scatterplot.\nIn the following notation, user-supplied parameters are in bold typeface.\nConsider a corpus of documents C with disjoint subsets A and B s.t. A \u222aB \u2261 C. Let \u03c6T (t, C) be the number of times term t occurs in C, \u03c6T (t, A) be the the number of times t occurs in A. Let \u03c6D(t, . . .) refer to the number of documents containing t. Let tij be the jth word in term ti. In practice, j \u2208 {1, 2}. The parameter \u03c6 may be \u03c6T or \u03c6D.4 Other feature representations (ex., tf.idf) may be used for \u03c6.\nPr[ti] = \u03c6(ti, C)\u2211\nt\u2208C\u2227|t|\u2261|ti|\u03c6(t, C) . (1)\nThe construction of the set of terms included in the visualization V is a two-step process. Terms must occur at least m times, and if bigrams, appear to be phrases. In order to keep the approach language neutral, we follow Schartz et al. (2013), and use a pointwise mutual information score to filter out bigrams that do not occur far more frequently than would be expected. Let\nPMI(ti) = log Pr[ti]\u220f\ntij\u2208tiPr[Tij ] . (2)\nThe minimum PMI accepted is p. Now, we can define V as\n{t|\u03c6(t, C) \u2265m\u2227(|t| \u2261 1\u2228PMI(t) > p)} (3)\nLet a term t\u2019s coordinates on the scatterplot be (xAt , x B t ), where A and B are the two document categories. Although xKt is proportional to \u03c6(t,K), ma y ter s will have identical \u03c6(t,K) values. To break ties the word that appears last alphabetically will have a larg r xKt .\nWe define rKt s.t. t \u2208 V andK \u2208 {A,B} as the ranks of \u03c6(t,K), sorted in ascending order, where ties are broken by terms\u2019 alphabetical order. This allows us to define\nxKt = rKt\nargmax rK (4)\nThis limits x values to [0, 1], ensuring both axes are scaled identically. This keeps the chart from becoming lopsided toward the corpus that had a larger number of terms. 4\u03c6D is useful when documents contain unique, characteristic, highly frequent terms. For example, names of movies can have high \u03c6T when finding differences in positive and negative film reviews. The may lead to them receiving higher scores than sentiment terms.\nThe charts in Figures 1, 4, and 5, were made with parametersm=5, p=8, and \u03c6=\u03c6T .\nBreaking ties alphabetically is a simple but important alternative to jitter. While jitter (i.e., randomly perturbing xAt and x B t ) breaks up the stacked points shown in Figure 3, it eliminates empty space to legibly label points. Jitter can make it seem like identically frequent points are closer to an upper left or lower right corner. Alphabetic tie-breaking makes identical adjustments to both axes, leading to the horizontal (lower-left to upper-right) alignments of identically frequent points. This angle does not cause one point to be substantially closer to either of the category associated corners (the upper-left and lower-right).\nThese alignments provide two advantages. First, they open up point-free tracts in the center of the chart which allow for unobstructed interior labels. Second, they arrange points in a way that it is easy to hover a mouse over all of them, to indicate what term they correspond to, and be clicked to see excerpts of that term.\nIn the running example, 154 points were labeled when a jitter of 10% of each axis and no tie-breaking was applied. 210 points (a 36% lift) were labeled when no jitter was applied. 140 were labeled if no tie breaking was used.\nRudder (2014) observed terms closer to the lower-right corner were used frequently in A and infrequently in B, indicating they have both high recall and precision wrt category A. Symmetrically, the same relationship exists for B and the upper-right corner. We can formalize this score between a point\u2019s coordinates and it\u2019s respective corner. This intuition is represented by a score function sK(t) (K \u2208 {A,B} and t \u2208 V ) where\nsK(t) = { \u2016\u30081\u2212 xAt , xBt \u3009\u2016 if K = A, \u2016\u3008xAt , 1\u2212 xBt \u3009\u2016 if K = B . (5)\nOther term scoring methods (e.g., regression weights or a weighted log-odd-ratio with a prior) may be used in place of Formula 5.\nMaximal non-overlapping labeling of scatterplots is NP-hard (Been et al., 2007). Scattertex\u2019s heuristic is labeling points if space is available in one of 12 places around a point. This is performed iteratively, beginning with points having the highest score (regardless of category) and proceeding downward in score. An optimized data structure automatically constructed using Cozy (Loncaric\net al., 2016) holds the locations of drawn points and labels.\nThe top scoring terms in classes B and A (Democrats and Republicans in Figure 1) are listed to the right of the chart. Hovering over points and terms highlights the point and displays frequency statistics.\nPoint colors are determined by their scores on s. Those corresponding to terms with a high sB colored in progressively darker shades of blue, while those with a higher sA are colored in progressively darker shades of red. When both scores are about equal, the point colors become more yellow, which creates a visual divide between the two classes. The colors are provided by D3\u2019s \u201cRdYlBu\u201d diverging color scheme from Colorbrewer5 via d36.\nOther point colors (and scorings) can be used. For example, Figure 4 shows coefficients of an `1 penalized log. reg. classifier on V features. Scattertext, in this example, is set to color 0-scoring coefficients light gray. Terms\u2019 univariate predictive power are still evident by their chart position. See below7 for an interactive version.\nDemocratic frequency: 8 per 25,000 terms\nSome of the 25 mentions:\nRepublican frequency: 26 per 25,000 terms\nSome of the 62 mentions: BARACK OBAMA They\u00a0knew\u00a0they\u00a0were\u00a0part\u00a0of\u00a0something\u00a0larger\u00a0\u2014\u00a0a\u00a0nation\u00a0that\u00a0triumphed\u00a0over\u00a0fascism\u00a0and\u00a0 depression,\u00a0a\u00a0nation\u00a0where\u00a0the\u00a0most\u00a0innovative\u00a0businesses\u00a0turn\u00a0out\u00a0the\u00a0world's\u00a0best\u00a0products,\u00a0and\u00a0 everyone\u00a0shared\u00a0in\u00a0that\u00a0pride\u00a0and\u00a0success\u00a0from\u00a0the\u00a0corner\u00a0office\u00a0to\u00a0the\u00a0factory\u00a0floor. MITT ROMNEY That\u00a0business\u00a0we\u00a0started\u00a0with\u00a010\u00a0people\u00a0has\u00a0now\u00a0grown\u00a0into\u00a0a\u00a0great\u00a0American\u00a0success\u00a0story. MARCO RUBIO\nInfrequent Average Frequent\nRepublican Frequency\nIn fre\nqu en\nt Av\ner ag\ne Fr\neq ue\nnt\nD em\noc ra\ntic F\nre qu\nen cy Top Democratic\npresident obama barack 'm here for forward class obama not charlotte boston medicare education she make\nTop Republican government mitt administration success leadership\nstory\nbusiness\nto be\npaul\ni think\nthis\nbuilt it is and\nobama romney barack millionaires pell wealthiest trillion gentlemen fought americans medicare repeal prosper\nreagan\namerica\nblaming\nfounding\nseniors\nunemployment\nwealthy veterans blessed\ninsult\nfathers\nolympics\nstake\nstimulus\ndemocratic stronger fighting rebuild cuts ours forward\nliberty\nstrongest\nfreedom\nblame\nstaples\nroosevelt we michelle greatest deeply\nstark\nhopes charlotte affordable\ncaring\nsuccess belief uniform\nloyal\nwalks\nloving\nfulfill troops mayor\nbetter voting committed\npursuit detroit\ncents ran class fair drill kept jill farmers hands pay\ncoal\nloved\nprinciples\nnotion move led son story rid administration vision lady\nwealth\ndoors israel breaks minds\niraq\nmate visited threats\nregulations hundreds\npulled\napart\nplatform\nfix\nlie\ntrust hunt\nyounger\nrise\nera\nflying\nfacts birth fear\nenemy\nbegin\ncoverage que rules auto\npara\nhit\nlay\nfederal pass felt deep chief twice solar act ownerscast\nlesson\nlast\nnations\nsafe signed insurance simple\nfail\nbus fall\nbattle edge\nloans\nchurch\ndeal bad\nplanet\nif paid\nfill six\ntradefuel\nbank\nbit\nlots\njoin hall save sent none girl self re until field hours ok access line point top\nvisit\ngirls age free low\nstore\nthe story 1 roll back\n23\nhis dream\nstory of\nbig government\nok.\n16\nmitt was\n42\nnatural gas 3 take away re elect that has\nit has\ninsurance companies 18\nd.c.\n2 a business\n8 built it\ni think 'm here\nDemocratic\u00a0document\u00a0count:\u00a0123\u037e\u00a0word\u00a0count:\u00a076,864 Republican\u00a0document\u00a0count:\u00a066\u037e\u00a0word\u00a0count:\u00a058,138\nSearch for term \u00a0 Type a word or two\u2026 \u00a0\nTerm: success Download\u00a0SVG"}, {"heading": "5 Topical category discriminators", "text": "In 2012, how did Republicans and Democrats use language relating to \u201cjobs\u201d, \u201chealthcare\u201d, or \u201cmilitary\u201d differently? Figure 5 shows, in our running example, words similar to \u201cjobs\u201d that were characteristic of the political parties.\nThere has been much work in visualizing word vectors8 and none to my knowledge has involved category-association.\n5colorbrewer2.org 6github.com/d3/d3-scale-chromatic 7jasonkessler.github.io/sparseviz.html 8E.g., projector.tensorflow.org\nInfrequent Average Frequent\nRepublican Frequency\nIn fre\nqu en\nt Av\ner ag\ne Fr\neq ue\nnt\nD em\noc ra\ntic F\nre qu\nen cy Top Democratic\nworkers insurance companies companies education families millionaires medicare pell grants pay seniors insurance affordable industry auto industry\nTop Republican job creators unemployment business small business problems mitt romney enterprise mitt leadership free enterprise federal government fathers olympics reagan Most similar jobs jobs overseas create jobs creating jobs job job growth job creators job creation opportunities businesses responsibilities paycheck employees workers investments insurance companies small businesses budgets homes moms labor programs lobbyists customers factories industries business owners solutions schools raise taxes jobs jobs overseas workers investments insurance companies moms lobbyists loans skills unemployment business role small business problems burdens costs resources business owner people duty medicare pell grants pay seniorsinsurance cents staples affordable doctors invest hours years later tax cut nights grandparents women coal owners deal grantspell experts fathers jill hundreds olympics16 trillion reagan they paid iraq veterans girls roosevelt banks bain capital administration iowa hire water action ballot government york we fix texas second term regulations firm detroit ok. wealthy freedom lake city 60 others early childhood clean fill fuel race cuts quality game 40 america 8 girl boys tea kate buy 30 18 my grandfather pays simple care liberty running mate feet team 15 doors bain hunt street decent 5 talks hands bus earth code prosper fulfill law risk begin 42 line gas ceo decade move story bit five fine fiscal israel 16 term fair shot wall rural 29 coverage 4 federal lift wait edge principles ship district act d.c. fought ok guarantee $ 1 mate class teaching steel2 capital thanks pursuit better wisdom con six arms save core notion fighting join re elect church bad era auto age birth lie trust learn hero hit gone charlotte fair visitlay son table point greatest felt allow short led until free fall blessed fear tells kept deny signed el re top\nque Figure 5: Words and phrases that are se antically similar to the word \u201cjobs\u201d are colored darker on a gray-to-purple scale, and general a d category-specific related terms are listed to the right. Note that this is a cropping of the upper left-hand corner of the plot. Interactive version: jasonkessler.github.io/st-sim.html.\nIn this configuration of Scattertext, words are colored by their similarity to a query phrase. This is done using spaCy9-provided GloVe (Pennington et al., 2014) word vectors (trained on the Common Crawl corpus). The cosine distance between vectors is used, with mean vectors used for phrases.\nThe calculation of the most similar terms associated with each category is a simple heuristic. First, sets of terms closely associated with a category are found. Second, these terms are ranked based on their similarity to the query, and the top rank terms are displayed to the right of the scatterplot (Figure 5).\nA term is considered associated if its p-value is less than 0.05. P-values are determined using MCQ\u2019s difference in the weighted log-odds-ratio with an uninformative Dirichlet prior. This is the only model-based method discussed in Monroe et al. that does not rely on a large, in-domain background corpus. Since we are scoring bigrams in addition to the unigrams scored by MCQ, the size of the corpus would have to be larger to have high enough bigram counts for proper penalization.\nThis function relies the Dirichlet distribution\u2019s parameter \u03b1, a real, positive vector of size |V |. Following MCQ, \u03b1t = 0.01. Formulas 16, 18 and 22 are used to compute z-scores, which are then converted to p-values using the Normal CDF of \u03b6\u0302A\u2212Bw , letting y (K) t = \u03c6(t,K) st K \u2208 {A,B} and t \u2208 V . As seen in Figure 5, the top Republican word related to \u201cjobs\u201d is \u201cjob creators\u201d, while \u201cworkers\u201d is the top Democratic term.\n9spacy.io"}, {"heading": "6 Conclusion", "text": "We have described Scattertext, a tool to make legible comprehensive visualizations of labeled corpora. We have demonstrated how it can be used to examine importance scores of bag-of-words features and visualize word representations through a category-associated lens."}], "references": [{"title": "Dynamic map labeling", "author": ["Ken Been", "Eli Daiches", "Chee Yap."], "venue": "IEEE-VCG .", "citeRegEx": "Been et al\\.,? 2007", "shortCiteRegEx": "Been et al\\.", "year": 2007}, {"title": "Non-linear text regression with a deep convolutional neural network", "author": ["Zsolt Bitvai", "Trevor Cohn."], "venue": "ACL.", "citeRegEx": "Bitvai and Cohn.,? 2015", "shortCiteRegEx": "Bitvai and Cohn.", "year": 2015}, {"title": "At the national conventions, the words they used", "author": ["Mike Bostock", "Shan Carter", "Matthew Ericson."], "venue": "The New York Times.", "citeRegEx": "Bostock et al\\.,? 2012", "shortCiteRegEx": "Bostock et al\\.", "year": 2012}, {"title": "Dynamic wordclouds and vennclouds for exploratory data analysis", "author": ["Glen Coppersmith", "Erin Kelly."], "venue": "ACL-ILLVI.", "citeRegEx": "Coppersmith and Kelly.,? 2014", "shortCiteRegEx": "Coppersmith and Kelly.", "year": 2014}, {"title": "Representational Style: The Central Role of Communication in Representation", "author": ["Justin Ryan Grimmer."], "venue": "Ph.D. thesis, Harvard University.", "citeRegEx": "Grimmer.,? 2010", "shortCiteRegEx": "Grimmer.", "year": 2010}, {"title": "Movie reviews and revenues: An experiment in text regression", "author": ["Mahesh Joshi", "Dipanjan Das", "Kevin Gimpel", "Noah A. Smith."], "venue": "HLT-NAACL.", "citeRegEx": "Joshi et al\\.,? 2010", "shortCiteRegEx": "Joshi et al\\.", "year": 2010}, {"title": "Narrative framing of consumer sentiment in online restaurant reviews", "author": ["Dan Jurafsky", "Victor Chahuneau", "Bryan Routledge", "Noah Smith."], "venue": "First Monday .", "citeRegEx": "Jurafsky et al\\.,? 2014", "shortCiteRegEx": "Jurafsky et al\\.", "year": 2014}, {"title": "Fast synthesis of fast collections", "author": ["Calvin Loncaric", "Emina Torlak", "Michael D. Ernst."], "venue": "PLDI.", "citeRegEx": "Loncaric et al\\.,? 2016", "shortCiteRegEx": "Loncaric et al\\.", "year": 2016}, {"title": "Fightin\u2019 words: Lexical feature selection and evaluation for identifying the content of political conflict", "author": ["Burt L. Monroe", "Michael P. Colaresi", "Kevin M. Quinn."], "venue": "Political Analysis .", "citeRegEx": "Monroe et al\\.,? 2008", "shortCiteRegEx": "Monroe et al\\.", "year": 2008}, {"title": "Geovisual Analytics Approach to Exploring Public Political Discourse on Twitter", "author": ["J. Nelson", "S. Quinn", "B. Swedberg", "W. Chu", "A. MacEachren."], "venue": "ISPRS-IJGI .", "citeRegEx": "Nelson et al\\.,? 2015", "shortCiteRegEx": "Nelson et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Dataclysm: Who We Are (When We Think No One\u2019s Looking)", "author": ["Christian Rudder."], "venue": "Crown Publishing Group.", "citeRegEx": "Rudder.,? 2014", "shortCiteRegEx": "Rudder.", "year": 2014}, {"title": "Genderdistinguishing features in film dialogue", "author": ["Alexandra Schofield", "Leo Mehr."], "venue": "NAACLCLfL .", "citeRegEx": "Schofield and Mehr.,? 2016", "shortCiteRegEx": "Schofield and Mehr.", "year": 2016}, {"title": "Personality, gender, and age in the language of social media: The openvocabulary approach", "author": ["H. Andrew Schwartz", "Johannes C. Eichstaedt", "Margaret L. et al. Kern."], "venue": "PLOS ONE .", "citeRegEx": "Schwartz et al\\.,? 2013", "shortCiteRegEx": "Schwartz et al\\.", "year": 2013}, {"title": "tidytext: Text mining and analysis using tidy data principles in r", "author": ["Julia Silge", "David Robinson."], "venue": "JOSS .", "citeRegEx": "Silge and Robinson.,? 2016", "shortCiteRegEx": "Silge and Robinson.", "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "For example, finding words that are most characteristic of a political party in congressional speeches can help political scientists identify means of partisan framing (Monroe et al., 2008; Grimmer, 2010), while identifying differences in word usage between male and female characters in films can highlight narrative archetypes (Schofield and Mehr, 2016).", "startOffset": 168, "endOffset": 204}, {"referenceID": 4, "context": "For example, finding words that are most characteristic of a political party in congressional speeches can help political scientists identify means of partisan framing (Monroe et al., 2008; Grimmer, 2010), while identifying differences in word usage between male and female characters in films can highlight narrative archetypes (Schofield and Mehr, 2016).", "startOffset": 168, "endOffset": 204}, {"referenceID": 12, "context": ", 2008; Grimmer, 2010), while identifying differences in word usage between male and female characters in films can highlight narrative archetypes (Schofield and Mehr, 2016).", "startOffset": 147, "endOffset": 173}, {"referenceID": 13, "context": "Language use in social media can inform understanding of personality types (Schwartz et al., 2013), and provides insights into customers\u2019 evaluations of restaurants (Jurafsky et al.", "startOffset": 75, "endOffset": 98}, {"referenceID": 6, "context": ", 2013), and provides insights into customers\u2019 evaluations of restaurants (Jurafsky et al., 2014).", "startOffset": 74, "endOffset": 97}, {"referenceID": 5, "context": "Measuring redundancy is non-trivial, and has traditionally been approached through penalized logistic regression (Joshi et al., 2010), as well as through other feature selection techniques.", "startOffset": 113, "endOffset": 133}, {"referenceID": 6, "context": "The reader is directed to Monroe et al. (2008) (subsequently referred to as MCQ) for an overview of model-based term scoring algorithms.", "startOffset": 26, "endOffset": 47}, {"referenceID": 1, "context": "Also of interest, Bitvai and Cohn (2015) present a method for finding sparse words and phrase scores from a trained ANN (with bag-of-words features) and its training data.", "startOffset": 18, "endOffset": 41}, {"referenceID": 3, "context": "(Coppersmith and Kelly, 2014).", "startOffset": 0, "endOffset": 29}, {"referenceID": 2, "context": "(Coppersmith and Kelly, 2014). 3 Past work and design motivation Text visualizations manipulate the position and appearance of words or points representing them to indicate their relative scores in these measures. For example, in Schwartz et al. (2013), two word clouds are given, one per each category of text being compared.", "startOffset": 1, "endOffset": 253}, {"referenceID": 2, "context": "(Coppersmith and Kelly, 2014). 3 Past work and design motivation Text visualizations manipulate the position and appearance of words or points representing them to indicate their relative scores in these measures. For example, in Schwartz et al. (2013), two word clouds are given, one per each category of text being compared. Words (and selected ngrams) are sized by their linear regression coefficients (a composite metric of precision, recall, and redundancy) and colored by frequency. Only words occurring in geq1% of documents and having Bonferroni-corrected coefficient p-values of <0.001 were shown. Given that these words are highly correlated to their class of interest, the frequency of use is likely a good proxy for recall. Coppersmith and Kelly (2014) also describe a word-cloud based visualization for discriminating terms, but intend it for categories which are both small subsets of a much larger corpus.", "startOffset": 1, "endOffset": 765}, {"referenceID": 2, "context": "Bostock et al. (2012)2 employed an interesting, interactive word-bubble visualization for exploring different word usage among Republicans and Democrats in the 2012 American Political Conventions.", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "Bostock et al. (2012)2 employed an interesting, interactive word-bubble visualization for exploring different word usage among Republicans and Democrats in the 2012 American Political Conventions. A bubble represents each term, and its size increases with its frequency of use. Its precision is represented by its coloring\u2013 each bubble is colored blue and red, with the blue portion proportionally colored to a term\u2019s relative use by Democrats. Terms were manually chosen, and arranged along the x-axis based on their discriminative power. When clicked, sentences from speeches containing the word used are listed below the visualization. This bubble approach to word clouds inspired Nelson et al. (2015). nytimes.", "startOffset": 0, "endOffset": 705}, {"referenceID": 2, "context": "Bostock et al. (2012)2 employed an interesting, interactive word-bubble visualization for exploring different word usage among Republicans and Democrats in the 2012 American Political Conventions. A bubble represents each term, and its size increases with its frequency of use. Its precision is represented by its coloring\u2013 each bubble is colored blue and red, with the blue portion proportionally colored to a term\u2019s relative use by Democrats. Terms were manually chosen, and arranged along the x-axis based on their discriminative power. When clicked, sentences from speeches containing the word used are listed below the visualization. This bubble approach to word clouds inspired Nelson et al. (2015). nytimes.com/interactive/2012/09/06/us/politics/conventionword-counts.html The dataset used in Bostock et al. (2012) is used to demonstrate the capabilities of Scattertext in each of these figures.", "startOffset": 0, "endOffset": 822}, {"referenceID": 14, "context": "The tidytext R-package (Silge and Robinson, 2016) documentation includes a non-interactive ggplot2-based scatter plot that is very similar to Scattertext.", "startOffset": 23, "endOffset": 49}, {"referenceID": 12, "context": "Schofield and Mehr (2016) use essentially the same visualization, but plot over 100 corresponding n-grams next to an unlabeled frequency/zscore plot.", "startOffset": 0, "endOffset": 26}, {"referenceID": 11, "context": "4 Scattertext Scattertext builds on tinytext and Rudder (2014). It plots a set of unigrams and bigrams (we will refer to these as \u201cterms\u201d) found in a corpus of docuThis type of visualization may have first been introduced in Rudder (2014).", "startOffset": 49, "endOffset": 63}, {"referenceID": 11, "context": "4 Scattertext Scattertext builds on tinytext and Rudder (2014). It plots a set of unigrams and bigrams (we will refer to these as \u201cterms\u201d) found in a corpus of docuThis type of visualization may have first been introduced in Rudder (2014). ments assigned to one of two categories on a twodimensional scatterplot.", "startOffset": 49, "endOffset": 239}, {"referenceID": 11, "context": "4 Scattertext Scattertext builds on tinytext and Rudder (2014). It plots a set of unigrams and bigrams (we will refer to these as \u201cterms\u201d) found in a corpus of docuThis type of visualization may have first been introduced in Rudder (2014). ments assigned to one of two categories on a twodimensional scatterplot. In the following notation, user-supplied parameters are in bold typeface. Consider a corpus of documents C with disjoint subsets A and B s.t. A \u222aB \u2261 C. Let \u03c6T (t, C) be the number of times term t occurs in C, \u03c6T (t, A) be the the number of times t occurs in A. Let \u03c6D(t, . . .) refer to the number of documents containing t. Let tij be the jth word in term ti. In practice, j \u2208 {1, 2}. The parameter \u03c6 may be \u03c6T or \u03c6D.4 Other feature representations (ex., tf.idf) may be used for \u03c6. Pr[ti] = \u03c6(ti, C) \u2211 t\u2208C\u2227|t|\u2261|ti|\u03c6(t, C) . (1) The construction of the set of terms included in the visualization V is a two-step process. Terms must occur at least m times, and if bigrams, appear to be phrases. In order to keep the approach language neutral, we follow Schartz et al. (2013), and use a pointwise mutual information score to filter out bigrams that do not occur far more frequently than would be expected.", "startOffset": 49, "endOffset": 1087}, {"referenceID": 0, "context": "Maximal non-overlapping labeling of scatterplots is NP-hard (Been et al., 2007).", "startOffset": 60, "endOffset": 79}, {"referenceID": 7, "context": "An optimized data structure automatically constructed using Cozy (Loncaric et al., 2016) holds the locations of drawn points and labels.", "startOffset": 65, "endOffset": 88}, {"referenceID": 9, "context": "Rudder (2014) observed terms closer to the lower-right corner were used frequently in A and infrequently in B, indicating they have both high recall and precision wrt category A.", "startOffset": 0, "endOffset": 14}], "year": 2017, "abstractText": "Scattertext is an open source tool for visualizing linguistic variation between document categories in a language-independent way. The tool presents a scatterplot, where each axis corresponds to the rankfrequency a term occurs in a category of documents. Through a tie-breaking strategy, the tool is able to display thousands of visible term-representing points and find space to legibly label hundreds of them. Scattertext also lends itself to a query-based visualization of how the use of terms with similar embeddings differs between document categories, as well as a visualization for comparing the importance scores of bag-of-words features to univariate metrics.", "creator": "LaTeX with hyperref package"}}}