{"id": "1703.00830", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "General and Robust Communication-Efficient Algorithms for Distributed Clustering", "abstract": "As datasets become larger and more distributed, algorithms for distributed clustering have become more and more important. In this work, we present a general framework for designing distributed clustering algorithms that are robust to outliers. Using our framework, we give a distributed approximation algorithm for k-means, k-median, or generally any L_p objective, with z outliers and/or balance constraints, using O(m(k+z)(d+log n)) bits of communication, where m is the number of machines, n is the size of the point set, and d is the dimension. This generalizes and improves over previous work of Bateni et al. and Malkomes et al. As a special case, we achieve the first distributed algorithm for k-median with outliers, answering an open question posed by Malkomes et al. For distributed k-means clustering, we provide the first dimension-dependent communication complexity lower bound for finding the optimal clustering. This improves over the lower bound from Chen et al. which is dimension-agnostic. The second dimension-dependent communication complexity higher bound for finding the optimal clustering. Finally, we have a way to reduce the number of machine-specific machines in a distributed algorithm by simplifying the number of machine-specific machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying the number of machines in a distributed algorithm by simplifying", "histories": [["v1", "Thu, 2 Mar 2017 15:27:14 GMT  (167kb,D)", "http://arxiv.org/abs/1703.00830v1", null], ["v2", "Thu, 12 Oct 2017 19:08:04 GMT  (167kb,D)", "http://arxiv.org/abs/1703.00830v2", null]], "reviews": [], "SUBJECTS": "cs.DS cs.DC cs.LG", "authors": ["pranjal awasthi", "maria-florina balcan", "colin white"], "accepted": false, "id": "1703.00830"}, "pdf": {"name": "1703.00830.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Pranjal Awasthi", "Maria-Florina Balcan", "Colin White"], "emails": ["pranjal.awasthi@rutgers.edu,", "ninamf@cs.cmu.edu,", "crwhite@cs.cmu.edu."], "sections": [{"heading": null, "text": "Furthermore, we give distributed clustering algorithms which return nearly optimal solutions, provided the data satisfies the approximation stability condition of Balcan et al. [8] or the spectral stability condition of Kumar and Kannan [27]. In certain clustering applications where each machine only needs to find a clustering consistent with the global optimum, we show that no communication is necessary if the data satisfies approximation stability.\n\u2217Authors\u2019 addresses: pranjal.awasthi@rutgers.edu, ninamf@cs.cmu.edu, crwhite@cs.cmu.edu. This work was supported in part by NSF grants CCF-1422910, CCF-1535967, IIS-1618714, a Sloan Research Fellowship, a Microsoft Research Faculty Fellowship, a Google Research Award, and a National Defense Science & Engineering Graduate (NDSEG) fellowship.\nar X\niv :1\n70 3.\n00 83\n0v 1\n[ cs\n.D S]\n2 M\nar 2\n01 7"}, {"heading": "1 Introduction", "text": "Clustering is a fundamental problem in machine learning with applications in many areas including computer vision, text analysis, bioinformatics, and so on. The underlying goal is to group a given set of points to maximize similarity inside a group and dissimilarity among groups. A common approach to clustering is to set up an objective function and then approximately find the optimal solution according to the objective. Examples of these objective functions include k-means, k-median, and k-center, and more generally any `p objective, in which the goal is to find k centers to minimize the sum of the `p distances from each point to its closest center. Motivated by real-world constraints, further variants of clustering have been studied. For instance, in k-clustering with outliers, the goal is to find the best clustering (according to one of the above objectives) after removing a specified number of datapoints, which is useful for noisy data or outliers. The capacitated clustering variant adds the constraint that no individual cluster can be larger than a certain size, which is useful for load-balancing. Finding approximation algorithms to different clustering objectives and variants has attracted significant attention in the computer science community [3, 15, 16, 17, 19, 23, 30].\nAs datasets become larger, sequential algorithms designed to run on a single machine are no longer feasible for real-world applications. Additionally, in may cases data is naturally spread out among multiple locations. For example, hospitals may keep records of their patients locally, but may want to cluster the entire spread of patients across all hospitals in order to do better data analysis and inference. Therefore, distributed clustering algorithms have gained popularity over the past few years [11, 12, 31]. In the distributed setting, it is assumed that the data is partitioned arbitrarily across m machines, and the goal is to find a clustering which approximates the optimal solution over the entire dataset while minimizing communication among machines. Recent work in the theoretical machine learning community establishes guarantees on the clusterings produced in distributed settings for certain problems [11, 12, 31]. For example, Malkomes et al. provide distributed algorithms for k-center and k-center with outliers [31], and Bateni et al. introduce distributed algorithms for capacitated k-clustering under any `p objective [12]. A key algorithmic idea common among both of these works is the following: each machine locally constructs an approximate size O\u0303(k) summary of its data; the summaries are collected on a central machine which then runs a sequential clustering algorithm. A natural question that arises is whether there is a unifying theory for all distributed clustering variants. In the current work, we answer this question by providing a general distributed algorithm for clustering under any `p objective with or without outliers, and with or without capacity constraints, thereby generalizing and improving over recent results. We complement our algorithm by giving the first dimension-dependent lower bound on the level of communication needed to find the optimal distributed clustering, improving over the recent dimension-agnostic lower bound of Chen et al. [18].\nCertain real-world applications might require nearly optimal clusterings, closer than the constantfactor worst-case approximation ratios mentioned in the previous paragraph. However, these approximation ratios are unavoidable in the worst case due to existing lower bounds, for example, k-center cannot be approximated to a factor smaller than 2\u2212 even on a single machine [23]. To go beyond these worst-case results, many recent works have studied natural structure that exists in real-world instances, and shown that algorithms can output a clustering very close to optimal under these natural stability assumptions [5, 7, 8, 10, 13, 21, 27, 37]. For example, the (c, )-approximation stability condition defined by Balcan et al. states that any c-approximation to the clustering objective is -close to the target clustering [1, 8, 25], and the spectral stability condition of Kumar and Kannan is a deterministic generalization of many generative clustering models [27, 34, 35]. In the current work, we study these conditions in the distributed setting, and design algorithms with low communication that have guarantees just as strong as in the sequential setting. Finally, motivated\nby several examples, we initiate the study of a relaxed version of distributed clustering in which each machine needs to find a clustering consistent with the global optimal, which we call a locally consistent clustering. Surprisingly, we show that if the data satisfies approximation stability, no communication is necessary for each machine to find a locally consistent clustering."}, {"heading": "1.1 Our contributions", "text": "In this paper, we provide conceptually simple distributed clustering algorithms, combined with a new analysis, to paint a unifying picture for distributed clustering that generalizes and improves over several recent results. A distributed clustering instance consists of a set of n points partitioned arbitrarily across m machines, with a distance metric (if points lie in Rd, the metric is Euclidean distance) and the problem is to approximate the `p clustering objective while minimizing the amount of communication across machines. For clustering with capacities or outliers, the number of outliers z and the capacity constraint L are inputs to the algorithm (as is k).\nGeneral Robust Distributed Clustering. In Section 3, we show a general distributed algorithm for balanced k-clustering in `p in d dimensions with z outliers, using O(m(k + z)(d+ log n)) communication. The algorithm can be summarized as follows. Each machine performs a kclustering on its own data, and then sends the centers, along with the sizes of their corresponding clusters, to a central machine. The central machine then runs a weighted clustering algorithm on the mk centers. We add necessary changes to handle capacities and outliers, for instance, each (non-central) machine runs a k + z clustering algorithm in the case of k-clustering with z outliers. Given a sequential \u03b1-approximation algorithm and a bicriteria \u03b2-approximation algorithm which opens up O\u0303(k) centers, in Theorem 3 we prove our distributed algorithm returns an O(\u03b1\u03b2) approximation and improves over the approximation guarantees of Bateni et al. [12]. For example, for k-median, we achieve a (6\u03b1 + 2 + )-approximation by plugging in the bicriteria algorithm of Lin and Vitter [29] (adding an O(log n) factor to the communication cost), as opposed to a 32\u03b1approximation from Bateni et al. [12]. By plugging in the approximation algorithm for k-median with outliers [19], our algorithm achieves the first constant approximation for distributed k-median with outliers, answering an open question posed by Malkomes et al. [31]. We achieve these improvements by using a refined analysis that applies the triangle inequality in clever ways to prove a strong bound on the distance between each local center and its closest center in the solution outputted by the algorithm. We show how to carefully reason about the optimal clustering with subsets of outliers removed to preserve the constant factor approximation guarantee for k-median with outliers.\nCommunication Complexity Lower Bounds. In Section 4, we consider the communication complexity of distributed clustering \u2013 the minimum number of bits that need to be communicated among the machines by any algorithm which can solve the problem on all inputs with high probability. Recently, Chen et al. gave a communication complexity lower bound of \u2126(mk) for obtaining any c-approximation for distributed clustering [18]. When the dimension d is constant, this matches the upper bounds on the communication complexity of many distributed algorithms in the literature. However, for d \u2208 \u03c9(1), there is a gap of \u0398(d). We close this gap using a directsum reduction from a recent result on distributed mean estimation [22], proving an \u2126\u0303(mkd) lower bound for finding the optimal k-means centers. We show this lower bound holds even when the data satisfies approximation stability or spectral stability, and even when the optimal partition is known up front.\nDistributed Clustering under Stability. In the second part of the paper, we improve over the results from Section 3, and in some cases we bypass the hardness result from Section 4, by considering clustering under natural stability notions. Specifically, in Section 5, we provide an algorithm for distributed clustering under (1 + \u03b1, )-approximation stability using communication O(mk(d+log n)). The high-level structure is similar to the algorithm from Section 3. For k-median and k-means, we show the clustering outputted has O( (1+ 1\u03b1)) error. If we further assume that all optimal clusters are size \u2126( n(1+ 1\u03b1)), and increase communication by a factor of m, then the error drops to O( ) for k-median. For `p clustering when p < log n, we show the error is O( (1 + ( 18 \u03b1 )\np)). This is the first result for `p clustering under approximation stability when 2 < p < log n, even for a single machine, therefore we improve over Balcan et al. For p = \u221e (k-center), we provide an algorithm which outputs the optimal clustering under (2, 0)-approximation stability with O(mkd) communication. Due to the lower bound of Balcan et al. [9], this result is optimal with respect to the value of \u03b1. In Section 6, we give an algorithm for distributed clustering under spectral stability. We introduce a distributed algorithm which outputs k centers very close to the optimal centers, using communication O\u0303(mk(d+ log n)).\nLocally Consistent Clustering. Finally, in Section 7, we show that no communication is necessary in certain settings. We introduce the notion of a locally consistent clustering, motivated by several examples, in which the goal for each machine is to create a k-clustering such that each cluster is a subset of a global optimal cluster. If the data globally satisfies approximation stability, we show that no communication is necessary for each machine to output a locally consistent clustering with small total error."}, {"heading": "1.2 Related Work", "text": "Centralized Clustering. The first constant-factor approximation algorithm for k-median was given by Charikar et al. [16], and the current best approximation ratio is 2.675 from Byrka et al. [15]. For k-center, there is a tight 2-approximation algorithm [23]. For k-means, the best approximation ratio is 6.357 [2], and Makarychev et al. recently showed a bicriteria algorithm with strong guarantees [30]. For clustering with outliers, there is a 3-approximation algorithm for kcenter with z outliers, as well as a bicriteria 4(1 + 1/ )-approximation algorithm for k-median that picks (1+ )z outliers [17]. Chen found a true constant factor approximation algorithm for k-median (the constant is not explicitly computed) [19].\nDistributed Clustering. Balcan et al. showed a coreset construction for k-median and k-means, which leads to a clustering algorithm with O\u0303(mkd) communication, and also studied more general graph topologies for distributed computing [11]. Bateni et al. indroduced a construction for mapping coresets, which admits a distributed clustering algorithm that can handle balance constraints with communication cost O\u0303(mk) [12]. Malkomes et al. showed a distributed 13- and 4- approximation algorithm for k-center with and without outliers, respectively [31]. Chen et al. studied clustering under the broadcast model of distributed computing, and also proved a communication complexity lower bound of \u2126(mk) for distributed clustering [18], building on a recent lower bound for setdisjointness in the message-passing model [14]. Garg et al. showed a communication complexity lower bound for computing the mean of d-dimensional points [22].\nClustering under stability. The notion of approximation stability was defined by Balcan et al. who showed an algorithm which utilizes the structure to output a nearly optimal clustering [8]. Balcan et al. showed an algorithm to exactly cluster k-center instances satisfying (2, )-approximation\nstability, and they proved a matching lower bound, namely that (2\u2212 \u03b4, 0)-approximation stability is NP-hard for any \u03b4 unless NP = RP [9]. Kumar and Kannan introduced a spectral stability condition which generalized many generative models [27], including Gaussian mixture-models [20, 26], the Planted Partition model [32], as well as deterministic conditions [33]. This work was later improved along several axes, including the dependence on k in the condition, by Awasthi and Sheffet [7]. Chen et al. study distributed algorithms for graph partitioning when the graphs satisfy a notion of stability relating internal expansion of the k pieces to the external expansion [18]. It is known that such graphs also satisfy spectral stability under a suitable Euclidean embedding [6]. See Section 6 for a more detailed comparison of the two notions of stability."}, {"heading": "2 Preliminaries", "text": "Clustering. Given a set of points V of size n and a distance metric d, let C denote a clustering of V , which we define as a partition of V into k subsets X1, . . . , Xk. Each cluster Xi contains a center xi. When d is an arbitrary distance metric, we must choose the centers from the point set. If V \u2286 Rd and the distance metric is the standard Euclidean distance, then the centers can be any k points in Rd. In fact, this distinction only changes the cost of the optimal clustering by at most a factor of 2 when p = 1, 2, or \u221e [4]. The `p cost of C is\ncost(C) = \u2211 i \u2211 v\u2208Xi d(v, xi) p  1p . We will denote the optimal clustering of a point set V in `p with z outliers as OPT k,z,p(V ). V , p, k, and/or z will often be clear from context, so we may drop some or all of these parameters. OPT (A,B) will denote the optimal clustering for a point set A \u2286 V , using centers from a different point set B \u2286 V . We often overload notation and let OPT denote the objective value of the optimal clustering as well. In our proofs, we make use of the triangle inequality generalized for `p, d(u, v)\np \u2264 2p\u22121(d(u,w)p + d(w, v)p), for any points u, v, w. We denote the optimal clusters as C1, . . . , Ck, with centers c1, . . . , ck. We say a bicriteria clustering algorithm A is a (\u03b3, \u03b1)approximation algorithm if it returns \u03b3 \u00b7 k centers which define a clustering whose cost is at most an \u03b1-factor from the optimal clustering with k centers. Throughout the paper, unless otherwise noted, we assume any d-dimensional datapoint can be expressed using O(d) bits.\nDistributed computing. We use a common, general theoretical framework for distributed computing called the coordinator model. There are m machines, and machine 1 is designated as the coordinator. Each machine can send messages back and forth with machine 1. This model is very similar to the message-passing model, also known as the point-to-point model, in which any pair of machines can send messages back and forth. In fact, the two models are equivalent up to small factors in the communication complexity [14]. We assume the data is arbitrarily partitioned across the m machines, and it is the coordinator\u2019s job to output the answer. Most of our algorithms can be applied to the mapreduce framework with a constant number of rounds. For more details, see [12, 31].\nCommunication Complexity. One of our main goals when designing distributed algorithms is to minimize the communication. For an input X and a protocol \u03a0, the communication cost is the total number of bits in the messages sent to and from the coordinator. When designing algorithms, we wish to minimize the communication complexity, or the maximum communication\ncost over all possible inputs X. When proving a lower bound for a problem A, we define the \u03b4-error communication complexity as the minimum communication complexity of any randomized protocol \u03a0, such that for all inputs X, the probability that \u03a0 outputs an incorrect answer is at most \u03b4. For brevity, we use communication complexity to mean .99-error communication complexity.\nApproximation Stability. In Sections 4 and 5, we consider clustering under a natural property of real-world instances called approximation stability. Intuitively, a clustering instance satisfies this assumption if all clusterings close in value to OPT are also close in terms of the clusters themselves. This is a desirable property when running an approximation algorithm, since in many applications, the k-means or k-median costs are proxies for the final goal of recovering a clustering that is close to the desired \u201ctarget\u201d clustering. Approximation stability makes this assumption explicit. First we define two clusterings C and C\u2032 as -close, if only an -fraction of the input points are clustered differently in the two clusterings, formally, there exists a permutation \u03c3 of [k] such that \u2211k i=1 |Ci \\ C \u2032\u03c3(i)| \u2264 n.\nDefinition 1. A clustering instance (V, d) satisfies (1 +\u03b1, )-approximation stability if all clusterings C with cost(C) \u2264 (1 + \u03b1) \u00b7 OPT are -close to C.\nSpectral Stability. In Sections 4 and 6, we consider clustering under spectral stability. This is a deterministic condition on a clustering dataset that generalizes many assumptions that exist in the study of generative models such as Gaussian Mixture Models. Let A be an n\u00d7 d data matrix consisting of n points in a d-dimensional Euclidean space. Let C be an n \u00d7 d rank k matrix with each row consisting of the center of the corresponding datapoint.\nDefinition 2. We say that the matrix A satisfies \u03b3-spectral stability for some constant \u03b3 > 0, if for every Ai \u2208 Cr and for every r 6= s, the projection of Ai onto the line joining cr and cs is closer to cr than to cs by an additive factor of ( \u03b3k |Cr| + \u03b3k |Cs| ) \u2016A\u2212 C\u2016."}, {"heading": "3 General Robust Distributed Clustering", "text": "In this section, we give a general algorithm for distributed clustering with the `p objective, with or without balance constraints and with or without outliers. This generalizes previous distributed clustering results [12, 31], and answers an open question of Malkomes et al. [31]. We give a simple algorithmic framework, together with a careful analysis, to prove strong guarantees in various settings. Each machine performs a k-clustering on its own data, and the centers, along with the size of their corresponding clusters, are sent to a central machine, which then runs a weighted clustering algorithm on the mk centers (see Figure 1). For the case of clustering with outliers, each machine runs a (k+z)-clustering, and the central machine runs a clustering algorithm that handles outliers.\nTheorem 3. Given a sequential (\u03b4, \u03b1)-approximation algorithm A 1 for balanced k-clustering with the `p objective with z outliers, and given a sequential (\u03b3, \u03b2)-approximation algorithm B for kclustering with the `p objective, then Algorithm 1 is a distributed algorithm for clustering in `p with z outliers, with communication cost O(m(k + z)(d+ log n)\u03b3). The number of centers opened is \u03b4k and the approximation ratio is (23p\u22121\u03b1p\u03b2p + 22p\u22121(\u03b1p + \u03b2p))1/p. For k-median and k-center, this ratio simplifies to 4\u03b1\u03b2 + 2\u03b1+ 2\u03b2.\n1 We note that A must be able to handle weighted points. It has been pointed out that every clustering algorithm we are aware of has this property [12].\nAlgorithm 1 Distributed balanced clustering with outliers\nInput: Distributed points V = V1 \u222a \u00b7 \u00b7 \u00b7 \u222a Vm, algorithms A and B 1: For each machine i,\n\u2022 Run B for (k + z)-clustering on Vi, outputting Ai = {ai1, . . . , aik+z}. \u2022 Set wij = |{p \u2208 Vj | aij = argmina\u2208Aid(p, a)}|. \u2022 Send Ai and all weights to machine 1.\n2: Run A on \u22c3 iAi using the corresponding weights, outputting X = {x1, . . . , xk}.\nOutput: Centers X = x1, . . . , xk\nSetting B to be the (8 logn , 1 + )-bicriteria approximation algorithm for k-median [29], the approximation ratio becomes 6\u03b1+ 2 + , which improves over the 32\u03b1 approximation ratio of Bateni et al. [12]. If we set A as the current best k-median algorithm [15], we achieve a distributed (18.05 + )-approximation algorithm for k-median. If instead we plug in the sequential approximation algorithm for k-median with z outliers [19], we obtain the first constant-factor approximation algorithm for k-median with outliers, answering an open question from Malkomes et al. [31]. We can also use the results from Gupta and Tangwongsan [24] to obtain an O(1)-approximation algorithm for 1 < p < log n.\nOur proof of Theorem 3 carefully reasons about the optimal clustering in certain settings where subsets of the outliers are removed, to ensure the constant approximation guarantee carries through to the final result. First we bound the sum of the local optimal (k+ z)-clustering on each machine by the global clustering with outliers in the following lemma (a non-outlier version of this lemma appears in [12]).\nLemma 4. For a partition V1, . . . , Vk of V and 1 \u2264 p <\u221e, \u2211m i=1OPT k+z(Vi)p \u2264 2pOPT p k,z.\nProof. Given a machine with datapoints Vi \u2286 V , we will first show that\nOPT (Vi, Vi)p \u2264 2pOPT (Vi, V )p.\nLet c1, . . . , ck be the optimal centers for OPT (Vi, V ). Given cj , let c\u2032j be the closest point in Vi to cj . Note that there may be one point c\n\u2032 which is the closest point in Vi to two different centers, but this just means we will end up with \u2264 k centers total, which is okay. Then we have the following:\nOPT (Vi, Vi)p \u2264 \u2211 v\u2208Vi d(v, c\u2032v) p\n\u2264 2p\u22121 \u2211 v\u2208Vi (d(v, cv) p + d(cv, c \u2032 v) p)\n\u2264 2p \u2211 v\u2208Vi d(v, cv) p \u2264 2pOPT (Vi, V )p\nThe third inequality follows because c\u2032v was defined as the closest point in Vi to cv. By choosing k\u2032 = k + z, we have that OPT k+z(Vi, Vi)p \u2264 2pOPT k+z(Vi, V )p for all i.\nLet the centers in OPT k,z be c1, . . . , ck, and for v \u2208 V , let cv denote the closest of these centers to v. Given the outliers Z from OPT k,z, let V \u2032i = Vi\\Z. Then OPT k+z(Vi, V )p \u2264 OPT k(V \u2032i , V )p \u2264\u2211\nv\u2208V \u2032i d(v, cv)\np. The second inequality follows because with k + z centers, we can make all points\nin Vi \u2229 Z a center and also use the centers in OPT k(V \u2032i , V ). Summing over all i, we arrive at \u2211 iOPT k+z(Vi, V )p \u2264 OPT p k,z, and the lemma follows.\nNow we prove Theorem 3.\nProof. (Theorem 3) Given a (\u03b4, \u03b1)-approximation algorithm A for balanced clustering in `p with z outliers and a (\u03b3, \u03b2)-approximation algorithm B for `p clustering, we show that Algorithm 1 outputs a set X of centers with provable approximation guarantees. First we consider the case where p <\u221e.\nWe start by defining all the notation we need for the proof. Let Z denote the set of outliers returned by Algorithm 1 when running B, let Z\u2217 denote the outliers in OPT k,z(A,A), where A = \u22c3 tAt (defined in Algorithm 1), and let Z\n\u2032 denote the outliers in OPT k,z. Denote the centers in OPT k,z(A,A) by x\u2217, denote the centers in OPT k,z by cj , and let c\u2032j denote the closest point in A to cj . Given a point v \u2208 Vt, let av,, xv, and cv denote the closest point to v in At, X, or OPT (V ), respectively. Finally, let c\u2032v denote the closest point to cv in A.\nUsing the triangle inequality and the fact that for all v, d(v, xv) \u2264 d(v, xav),\n\u2211 v\u2208V \\Z d(v, xv) p \u2264 \u2211 v\u2208V \\Z d(v, xav) p \u2264 2p\u22121 \u2211 v\u2208V \\Z (d(v, av) p + d(av, xav) p) (1)\n\u2264 2p\u22121 \u2211 v\u2208V d(v, av) p + 2p\u22121 \u2211 v\u2208V \\Z d(av, xav) p (2)\nWe can bound the first summation in Expression 2 as follows.\u2211 v\u2208V d(v, av) p = \u2211 i \u2211 v\u2208Vi d(v, av) p (3)\n\u2264 \u2211 i \u03b2pOPT k+z(Vi, Vi)p [by definition of B] (4) \u2264 2p\u03b2pOPT pk,z [by Lemma 4.] (5)\nNow we show how to bound the second summation.\n\u2211 v\u2208V \\Z d(av, xav) p \u2264 \u03b1p \u2211 v\u2208V \\Z\u2217 d(av, x \u2217 v) p [by def\u2019n of A]\n\u2264 \u03b1p \u2211\nv\u2208V \\Z\u2032 d(av, c\n\u2032 v) p [by def\u2019n of OPT k,z(A,A)]\n\u2264 2p\u22121\u03b1p \u2211\nv\u2208V \\Z\u2032 (d(av, cv)\np + d(cv, c \u2032 v) p) [by triangle ineq.]\n\u2264 2p\u03b1p \u2211\nv\u2208V \\Z\u2032 d(av, cv)\np [by definition of c\u2032v]\n\u2264 22p\u22121\u03b1p \u2211\nv\u2208V \\Z\u2032 (d(v, av)\np + d(v, cv) p) [by triangle ineq.]\n\u2264 22p\u22121\u03b1p \u2211\nv\u2208V \\Z\u2032 d(v, av)\np + 22p\u22121\u03b1p \u2211\nv\u2208V \\Z\u2032 d(v, cv)\np [expanding]\n\u2264 23p\u22121\u03b1p\u03b2pOPT pk,z + 2 2p\u22121\u03b1pOPT pk,z [by Expression 5] \u2264 (23p\u22121\u03b1p\u03b2p + 22p\u22121\u03b1p)OPT pk,z [arithmetic]\nTherefore, our final result is \u2211 v\u2208V \\Z d(v, xv) p  1p \u2264 (23p\u22121\u03b1p\u03b2p + 22p\u22121(\u03b1p + \u03b2p)) 1p OPT k,z. For the communication complexity, it is clear that we communicate \u2264 (k+z)m\u03b3 total points and weights, and the weights are numbers \u2264 n, so the communication cost is O(m(k + z)(d+ log n)\u03b3). The balance constraints follow from the guarantees of Algorithm A, and the fact that the points in A are weighted. We also note that for all i, machine 1 can send the center assignments of Ai to machine i, so that each datapoint knows its global center (and this does not increase the communication cost above O(m(k + z)(d+ log n)\u03b3)).\nFor k-center, we can derive the same result as k-median by using the same analysis, but replacing every summation with a maximum. For instance, we use a modified Lemma 4 to show maxmi=1OPT k+z(Vi) \u2264 2OPT k,z. Our final result for k-center is maxv\u2208V \\Z d(v, xv) \u2264 (4\u03b1\u03b2 + 2\u03b1+ 2\u03b2)OPT k,z."}, {"heading": "4 Communication Complexity Lower Bounds", "text": "In this section, we give a dimension-dependent lower bound on the communication complexity for distributed k-means clustering. This generalizes the result of Chen et al. [18], who showed a lower bound independent of the dimension. Our lower bound holds even when the data satisfies (1+\u03b1, )- approximation stability or \u03b3-spectral stability for arbitrary \u03b1, , or \u03b3. We also note a corollary from Chen et al. [18].\nCorollary 5. For any c \u2265 1, p \u2208 N, and z \u2265 0, the communication complexity of computing a c-approximation for k-clustering in `p with z outliers is \u2126(m(k + z)).\nThis follows from Theorem 4.1 in [18] because k-clustering with z outliers is a k + z eligible function: it evaluates to 0 if there are at most k + z points, otherwise it is greater than 0. This shows that for constant dimension, the communication complexity of our Theorem 3 for clustering with outliers is tight up to logarithmic factors.\nNow we move on to a dimension-dependent lower bound. Specifically, we lower bound the communication complexity to compute the optimal centers (or the optimal cost) for distributed k-means clustering. Interestingly, this lower bound holds even if the coordinator knows the optimal clusters up front, and just needs to calculate the k different means. Indeed, our method will use a direct-sum theorem on computing the mean of m different data points.\nThe communication complexity needed to compute the sum of m numbers, each on a different machine, is \u2126(m) [36]. Clearly, the same result holds for averaging m numbers. Now we use a direct-sum theorem to generalize this result to d-dimensional numbers in Euclidean space [22]. The full details are in Appendix A.\nTheorem 6. The communication complexity to compute the optimal clustering for mk points in d dimensions, where each machine contains k points, is \u2126 (\nmkd log(md)\n) even if the clustering is promised\nto satisfy (1 + \u03b1, )-approximation stability for any \u03b1, , or \u03b3-spectral stability for any \u03b3.\nProof sketch We reduce the problem of computing the optimal clustering across m machines, to the mean estimation problem with m machines, each with one kd-dimensional datapoint. Given such a mean estimation instance, we break up each data point X \u2208 [\u22121, 1]kd into k \u201cchunks\u201d of d-dimensional data points: for 1 \u2264 i \u2264 k, let Xi = X[1+k(i\u22121),ki] \u2208 [\u22121, 1]d. Then we add offsets 4i \u00b7 [1]d to each vector Xi.\nNow we have a set of mk d-dimensional data points, k per machine, defining a clustering instance. We show that any protocol which solves this clustering instance can solve the original mean estimation instance. First we show that the optimal clusters are exactly the k sets of data points Y i corresponding to all ith chunks of the original data points, which follows because of the added offsets. Therefore, if the coordinator correctly computes the mean of each cluster, it knows the mean of each length-d chunk of dimensions from the original data points. The coordinator can subtract the offsets from the centers and concatenate them, to find the mean of the original data points. This follows because the `2 mean functions is linear across dimensions. We obtain a\ncommunication complexity of \u2126 (\nmkd log(md)\n) by plugging the results of Garg et al. [22] (included in\nAppendix A) and Viola [36]. To show this result holds under stability, we increase the offsets by factors of (2(1 + \u03b1)) or 2\u03b3 log k, from which it follows that the clustering instance is stable."}, {"heading": "5 Distributed Clustering under Approximation Stability", "text": "In this section, we improve over the results from Section 3 if it is known up front that the data satisfies approximation stability. We give modified algorithms which leverage this structure to output a clustering very close to optimal, using no more communication than the algorithm from Section 3. We focus on k-median, but we give the details for k-center and `p for p < log n in the appendix.\nWe give a two-phase algorithm for k-median under approximation stability. The high level structure of the algorithm is similar to Algorithm 1: first each machine clusters its local point set, and sends the weighted centers to the coordinator. Then the coordinator runs a weighted clustering algorithm to output the solution. The difference lies in the algorithm that is run by the machines and the coordinator, which will now take advantage of the approximation stability assumption. We obtain the following result.\nTheorem 7. Algorithm 3 outputs a set of centers defining a clustering that is O( (1 + 1\u03b1))-close to OPT for k-median under (1 +\u03b1, )-approximation stability with O(mk(d+ log n)) communication.\nWe achieve a similar result for k-means. For clustering in `p when p < log n, the error of the outputted clustering increases to O( (1 + (18\u03b1 )\np)) (Theorem 15). This is the first result for `p clustering under approximation stability when 2 < p < log n, even for a single machine, thereby improving over Balcan et al. We also show that if the optimal clusters are not too small, the error of the outputted clustering can be pushed even lower.\nAlgorithm 2 Iterative greedy procedure\nInput: Graph G = (V,E), parameter k 1: Initialize A = \u2205, V \u2032 = V . 2: While |A| < k, set v\u2032 = maxv\u2208V \u2032 N(v) \u2229 V \u2032, C(v\u2032) = N(v\u2032) \u2229 V \u2032. \u2022 Add (v\u2032, C(v\u2032)) to A, and remove N(v\u2032) from V \u2032. Output: Center and cluster pairs A = {(v1, C(v1)), . . . , (vk, C(vk))}\nAlgorithm 3 Distributed k-median clustering under (1 + \u03b1, )-approximation stability\nInput: Distributed points V = V1 \u222a \u00b7 \u00b7 \u00b7 \u222a Vm, average k-median cost wavg 1: Set t =\n\u03b1wavg 18\n2: For each machine i,\n\u2022 Create the threshold graph Gi2t using distance 2t. \u2022 Run Algorithm 2 with input (Gi2t, k), outputting A\u2032i = {(vi1, C(vi1)), . . . , (vik, C(vik))}. \u2022 Send Ai = {(vi1, |C(vi1)|), . . . , (vik, |C(vik)|)} to Machine 1.\n3: Given the set of weighted points received, A = \u222aiAi, create the threshold graph G6t using points A and distance 6t. 4: Run Algorithm 2 with graph G6t (using weighted points) and parameter k, outputting X \u2032 =\n{(x1, C(x1)), . . . , (xk, C(xk))}. Output: Centers X = {x1, . . . , xk}\nTheorem 8. There exists an algorithm which outputs a clustering that is O( )-close to OPT for k-median under (1 + \u03b1, )-approximation stability with O(m2kd+mk log n) communication if each optimal cluster Ci has size \u2126( n(1 + 1 \u03b1)).\nWe start by stating two properties which demonstrate the power of approximation stability. Let wavg denote the average distance from each point to its optimal center, so wavg \u00b7 n = OPT .\nLemma 9. [8] Given a (1+\u03b1, )-approximation stable clustering instance (V, d) for k-median, then Property 1: For all x, for \u2264 x n\u03b1 points v, d(v, cv) \u2265 \u03b1wavg/(x ). Property 2: For < 6 n points v, \u2203ci 6= cv such that d(v, ci)\u2212 d(v, cv) \u2264 \u03b1wavg/(2 ).\nProperty 1 follows from Markov\u2019s inequality. Property 2 follows from the (1 +\u03b1, )- approximation stability condition: If more than 6 n points are almost the same distance to their second-closest center as their closest center, then we can assign these points to their second-closest center, achieving a low-cost clustering that is not -close to OPT , contradicting approximation stability.\nNow we define a point as bad if it falls into the bad case of either Property 1 or Property 2 with x = 36. Formally, B = {v | d(v, cv) \u2265 \u03b1wavg36 or \u2203ci 6= cv s.t. d(v, ci) \u2212 d(v, cv) \u2264 \u03b1wavg 2 }. From Lemma 9, |B| \u2264 (6 + 36\u03b1 ) n. Otherwise, a point is good. Given an optimal cluster Ci, define Hi = Ci \\ B, the set of good points in Ci. Given a set of points V with a distance metric d and t > 0, we define the threshold graph Gt as a graph on V , where there is an edge between each pair u, v \u2208 V if and only if d(u, v) \u2264 t. We use this concept in our algorithm.\nWe prove the following about Algorithm 2 (formally, Lemma 14 in Appendix B): Given the input graph G contains a partition A1, . . . , Ak, A\n\u2032 with the following guarantee: Condition (1) for all i, for all u, v \u2208 Ai, (u, v) \u2208 E(G), and Condition (2) for all i 6= j, u \u2208 Ai, and v \u2208 Aj , then (u, v) /\u2208 E(G), moreover, u and v do not share a common neighbor in G. Then Algorithm 2 outputs\na clustering that is 3|A\u2032|-close to A1, . . . , Ak. Now we can prove Theorems 7 and 8. The full details are in Appendix B. Proof sketch (Theorem 7) We assume Algorithm 3 knows the value wavg, but in the appendix, we show how to relax this condition. First, given machine i, let {H i1, . . . ,H ik} denote the good clusters, and let Bi denote the set of bad points on machine i. We use Lemma 9 to show that in graph Gi2t, the good clusters satisfy Conditions (1) and (2). Therefore, by Lemma 14, the clustering outputted in Step 2 is 3|Bi|-close to the good clusters {H i1, . . . ,H ik}. The total error over all machines is < 3|B|, and it follows that all but < 3|B| good points are within 2t of some point in A.\nNow, we partition A into sets HA1 , . . . ,H A k , B \u2032, where HAj denotes points which are distance 2t to good points from Hi, and B\n\u2032 contains points far from all good points. This partition is well-defined because any pair of good points from different clusters are far apart. From the previous paragraph, |B\u2032| \u2264 3|B| (we let |B\u2032| denote the sum of the weights of all points in B\u2032). Again using Lemma 9, we show that HA1 , . . . ,H A k , B\n\u2032 in graph G6t satisfy Conditions (1) and (2). Given two points u, v \u2208 HAj , then there exist u\u2032, v\u2032 \u2208 Hj such that d(u, v) \u2264 d(u, u\u2032) + d(u\u2032, cj) + d(cj , v\u2032) + d(v\u2032, v) \u2264 6t. Given u \u2208 HAj and w \u2208 HAj\u2032 , there exist u\u2032 \u2208 Hj , w\u2032 \u2208 Hj\u2032 such that d(u\u2032, cj\u2032) > 18t\u2212 d(cj , u\u2032), which we use to show u and w cannot have a common neighbor in G6t (see Figure 2).\nTherefore, by Lemma 14, the clustering outputted in Step 4 is 3|B\u2032|-close to the good clusters {HA1 , . . . ,HAk }. It follows that there exists a bijection between each center outputted xj , and each good cluster HAj , and all but 3|B\u2032| \u2264 9|B| good points v \u2208 Hj are distance 2t to a point in A which is distance 6t to xj , and v must be distance > 8t to any other outputted center from Lemma 9. So the error over all points, good and bad, is 9|B| + |B| = 10|B| \u2208 O( n(1 + 1\u03b1)), so the algorithm achieves the desired error bound. The total communication is mk points and mk weights, or O(mk(d+ log n)) bits. Proof sketch (Theorem 8) The algorithm is as follows. First, run Algorithm 3. Then send X \u2032 to each machine i, incurring a communication cost of O(m2kd). For each point v \u2208 V , calculate the median distance from v to each cluster C(xj) (using the weights), and assign v to the index j with the minimum median distance. Call the new clusters X1, . . . , Xk, according to the indices. We will prove this clustering is O( )-close to the optimal clustering. Specifically, we will show that all points are correct except for the 6 n points in the bad case of Property 2 from Lemma 9.\nFrom the proof of Theorem 7, we know that at most 3|B| points from the clustering {HA1 , . . . ,HAk } are misclassified with respect to H1, . . . ,Hk, so all but 3|B| \u2208 O( n(1+ 1\u03b1)) weighted points v \u2208 H A j are within 2t of a point in Hj . By assumption, all clusters H A j consist of more than half of these good (proxy) points. Given a point v \u2208 Cj satisfying the good case in Property 2, we use the triangle inequality to show d(v, u) \u2264 d(v, cj) + 3t for all u \u2208 HAj , and d(v, u) > d(v, cj) + 15t for all u \u2208 HAj\u2032 , therefore, v will be assigned to the correct cluster.\nFor k-center, we show an algorithm that outputs the exact solution under (2, 0)-approximation stability using O(mkd) communication. Even in the single machine setting, it is NP-hard to find the exact solution to k-center under (2 \u2212 , 0)-approximation stability unless NP = RP [9], therefore our result is tight with respect to the level of stability. We include all details in Appendix B."}, {"heading": "6 Distributed Clustering under Spectral Stability", "text": "In this section, we give a distributed clustering algorithm under spectral stability. Recall the definition of matrices A and C from Section 2. Our distributed algorithm for spectral stability works as follows. We first compute a k-SVD of the data matrix and project the data onto the span of the top k singular vectors. This can be done in a distributed manner [28]. We then run a distributed constant factor approximation algorithm for k-means developed in Section 3. Finally, we run a natural distributed version of the popular Lloyd\u2019s heuristic for a few rounds to converge to nearly optimal centers. We achieve the following theorem.\nTheorem 10. Let A be a data matrix satisfying \u03b3-spectral stability. Then, Algorithm 4 on A outputs centers \u03bd1, \u03bd2, . . . \u03bdk on machine 1 such that \u2016\u03bdi\u2212 ci\u2016 \u2264 . The total communication cost is O(mk(d+ log n) log(k )).\nProof. At each step of the for loop, every machine is computing a local weighted mean for each of its k clusters. Given such information from each machine, machine 1 can easily update the means to the new values exactly as in the Lloyd\u2019s step. The correctness of the algorithm then follows from previous work [7, 27] that shows that in T = log(\u2016A\u2016 ) steps the cluster centers will be recovered to accuracy. To bound the communication cost, the result of Balcan et al. [28] bounds the communication cost of computing k-SVD to be O(mkd). From Theorem 3, Algorithm 1 uses O(mk(d+ log n)) bits of communication. Finally, in each iteration of the distributed Lloyd\u2019s algorithm, each machine receives k data points and sends out k data points along with k weight values. Hence, the communication cost per iteration is O(mk(d+ log n)). Combining, and because log \u2016A\u2016 \u2208 O(log k), we get that the overall communication cost is O(mk(d+ log n) log(k )).\nAlgorithm 4 Distributed Spectral Clustering\nInput: n\u00d7 d data matrix A distributed over m machines , parameter k, accuracy . 1: Run the distributed algorithm from [28] to compute A\u0302i\u2019s, i.e., the projection of Ai onto the top k singular vectors of A.\n2: Run Algorithm 1 for k-means to compute initial centers \u03bd01 , . . . , \u03bd 0 k . Set T = log(\n\u2016A\u2016 ).\n3: For t = 1 to T ,\n\u2022 Machine 1 sends \u03bdt\u221211 , . . . \u03bd t\u22121 k to every other machine. \u2022 For each machine i, \u2013 Compute local clustering using \u03bdt\u221211 , . . . \u03bd t\u22121 k\n\u2013 Compute the mean \u00b5t\u22121i,j and the weight w t\u22121 i,j for each cluster \u2013 Send all \u00b5t\u22121i,j and w t\u22121 i,j to machine 1.\n\u2022 Machine 1 updates \u03bdtj = \u2211m i=1 w t\u22121 i,j \u00b5 t\u22121 i,j\u2211k\ni=1 w t\u22121 i,j\nfor j = 1, 2, . . . k.\n4: Output \u03bdT1 , . . . \u03bd T k .\nOutput: Centers \u03bd1, \u03bd2, . . . , \u03bdk\nA notion related to spectral stability has been recently studied [18] for distributed graph partitioning. Given a graph on n vertices with the ground truth partitioning as V1, V2, . . . , Vk, let \u03c1(k) be the maximum edge expansion of any piece, and let \u03bbk+1(G) be the k+1th smallest eigenvalue of the normalized graph Laplacian. Then the graph is stable if \u03bbk+1(LG)\n\u03c1k = \u2126(k3). Chen et al. design\ncommunication efficient distributed algorithms to cluster such stable graphs [18]. These graphs are intimately connected to spectral stability. In fact, it was shown that stable graphs in the above sense also satisfy spectral stability under an appropriate Euclidean embedding of the nodes of the graph [6]. Hence, in principle, our distributed algorithm for spectral stability can also be applicable to cluster stable graphs. However, Chen et al. study a setting where the edges are partitioned across machines and hence their result is formally incomparable to ours [18]."}, {"heading": "7 Locally Consistent Clustering", "text": "In this section, we show how to output a nearly optimal local clustering with no communication, provided the data satisfies approximation stability globally. First we provide a few motivating examples. Given m different hospitals, each of which have data on their own patients, each patient is at low or high risk for certain diseases, and there exists an (unknown) ground truth k-clustering of the patients corresponding to their risks for the diseases. We want to conduct a representative survey, with at least one patient from each cluster and each hospital. One solution is to run a distributed clustering algorithm to approximate the ground truth clustering using \u2126(mkd) communication, and then randomly sample m patients from each cluster. However, in Theorem 12 we show that if the data is globally structured, we can run a clustering algorithm on each individual hospital, then sample one patient from each cluster at each hospital.\nAnother example is when each hospital wants to decide, for each pair of patients, whether one can donate blood to the other. In this application, the global clustering is irrelevant \u2013 the only important information is pairwise information. Again, assume there is a ground truth clustering, and one patient can donate to another if and only if they are in the same cluster. If the data satisfies approximation stability, we can cluster locally, requiring no communication.\nDefinition 11. Given a clustering instance (V, d) with optimal clustering OPT , assuming the data is distributed across m machines, an -locally consistent clustering is a k-clustering Ci1, . . . , C i k for\neach machine i, such that the global clustering C = { \u22c3k j=1C i j | i \u2208 [m]} is -close to OPT .\nIn words, an -locally consistent clustering is a clustering for each machine, such that for each machine i and optimal cluster j, machine i contains a cluster that is nearly a subset of Cj \u2013 the total error across all pairs 1 \u2264 i \u2264 m and 1 \u2264 j \u2264 k is .\nTheorem 12. Given a k-median instance satisfying (1 + \u03b1, )-approximation stability, then there is an efficient algorithm which outputs an O( (1 + 1\u03b1))-locally consistent clustering. The communication complexity is 0.\nProof. Each machine runs steps 1 and 2 from Algorithm 3, i.e. the algorithm from Balcan et al. [8]. We will refer to the details of Theorem 7. Recall in Theorem 7, for each machine i, and 1 \u2264 j \u2264 k, let H ij denote the set of good points from cluster Cj on machine i. Let Bi denote the set of bad points on machine i. Then after Steps 1 and 2 from Algorithm 3, machine i outputs k clusters C(vj) such that there exists a bijection \u03c3 : [k] \u2192 [k] between the clusters C(vj) and the good clusters H` such that \u2211 j |H i\u03c3(j) \\C(vj)| \u2264 4|Bi|. For ease of notation, we now assume that \u03c3 is the identity bijetion for all machines.\nSince Hj = \u222aiH ij , then for the clusters Xj = \u222aiC(vj), we have \u2211 j |Hj \\ Xj | = \u2211\nj | \u222ai H ij \\ \u222aiC(vj)| = \u2211 j | \u222ai (H ij \\ C(vj))| = \u2211 i \u2211 j |H ij \\ C(vj)| \u2264 \u2211 i |Bi| \u2264 |B| \u2208 O( n(1 + 1 \u03b1)). This completes the proof."}, {"heading": "8 Conclusion", "text": "We present a simple and general framework for distributed clustering with outliers. We give an algorithm for k-clustering for any `p objective with z outliers using O\u0303(m(k + z)(d + log n)) bits of communication, answering an open question [31] and improving over the previous best approximation ratio [12]. For distributed k-means clustering, we give the first dimension-dependent communication complexity lower bound for finding the optimal clustering, improving over the lower bound of Chen et al. [18]. Our lower bound holds even when the data is stable. An interesting open question is to extend this result to any `p objective.\nWe show how to improve the quality of the clustering produced, provided the data satisfies certain natural notions of stability, specifically, approximation stability and spectral stability [8, 27]. In certain applications that only require locally consistent clusterings, we show that no communication is necessary if the data satisfies approximation stability. It is an interesting question to study the locally consistent clustering problem in different settings."}, {"heading": "A Proof from Section 4", "text": "To prove Theorem 6, we utilize the following direct-sum theorem.\nTheorem 13. [22] 2 Suppose \u03a0 computes the mean of m datapoints in X \u2208 [\u22121, 1]d distributed across m machines. Then the communication cost is \u2126 ( dm\nlogm\n) .\nTheorem 6 (restated). The communication complexity to compute the optimal clustering for mk points in d dimensions, where each machine contains k points, is \u2126 (\nmkd log(md)\n) even if the clustering\nis promised to satisfy (1 + \u03b1, )-approximation stability for any \u03b1, , or \u03b3-spectral stability for any \u03b3.\nProof. Given a protocol \u03a0 for computing the optimal clustering over a distributed clustering instance with B bits of communication, and an instance X of the mean estimation problem with m machines each with one kd-dimensional datapoint, then we will solve the mean estimation problem using \u03a0.\nFor each sample point X \u2208 [\u22121, 1]kd, we break it up into k \u201cchunks\u201d X1, . . . , Xk \u2208 [\u22121, 1]d, where Xi = X[1+k(i\u22121),ki]. Then we add offsets to each of the new vectors, Y\ni = Xi + 4i \u00b7 [1]d. Denote the set of all datapoints created from chunk i to be Y i.\nDefine the new set Y = \u222aiY i of km datapoints in d dimensions as a clustering input to protocol \u03a0. By assumption, \u03a0 returns the optimal centers c1, . . . , ck with B bits of communication. First, we show that the optimal clusters are exactly the k sets of datapoints Y1, . . . ,Yk. This is because of the offset we added, which implies each point p \u2208 Y i is closer to all other points in Y i (distance \u2264 2 \u221a d) than to any point from a set j 6= i (distance \u2265 2 \u221a d). Therefore, the optimal centers c1, . . . , ck must be equal to the means of the sets Y1, . . . ,Yk. The coordinator knows the centers at the end of \u03a0. Now, for each center ci, the coordinator subtracts the offset, c\u2032i = ci \u2212 4i \u00b7 [1]d, to obtain the means for the original data chunks Xi. Finally, the coordinator concatenates the new centers together into a single vector of size kd, C = [c1 c2 . . . ck]. This is equal to the mean of the original mean estimation problem, since the\nmean function is linear over dimensions. Then we obtain a communication complexity of \u2126 (\nmkd logmd ) by plugging in Theorem 13 and the result of Viola [36].\nTo obtain the result for clustering under (1 + \u03b1, )-approximation stability, we increase the offsets by a factor of (2\u03b1), to 8\u03b1i \u00b7 [1]d. This ensures that each datapoint is an \u03b1-factor closer to its center than to any other point from another cluster, so the data easily satisfies (\u03b1, 0)-approximation stability. Similarly, for spectral stability, we increase the offset to 4\u2016A\u2016 \u00b7 i, which easily satisfies the condition."}, {"heading": "B Proofs from Section 5", "text": "Lemma 14. [8] 3 Given a graph G over good clusters X1, . . . Xk and bad points B, with the following properties:\n1. For all u, v in the same Xi, edge (u, v) is in E(G).\n2 The theorem is in terms of computing the true mean of a Gaussian distribution, but the proof applies for any function f : [\u22121, 1]d \u2192 R which is linear over the dimensions (such as the mean function).\n3This lemma is obtained by merging Lemma 3.6 and Theorem 3.9 [8].\n2. For u \u2208 Xi, v \u2208 Xj such that i 6= j, then (u, v) /\u2208 E(G), moreover, u and v do not share a common neighbor in G.\nThen let C(v1), . . . , C(vk) denote the output of running Algorithm 2 on G with parameter k. There exists a bijection \u03c3 : [k]\u2192 [k] between the clusters C(vi) and Xj such that \u2211 i |X\u03c3(i)\\C(vi)| \u2264 3|B|.\nProof. From the first assumption, each good cluster Xi is a clique in G. Initially, let each clique Xi be \u201cunmarked\u201d, and then we \u201cmark\u201d it the first time the algorithm picks a C(vj) that intersects Xi. A cluster C(vj) can intersect at most one Xi because of the second assumption. During the algorithm, there will be two cases to consider. If the cluster C(vj) intersects an unmarked clique Xi, then set \u03c3(j) = i. Denote |Xi \\ C(Vj)| = rj . Since the algorithm chose the maximum degree node and Xi is a clique, then there must be at least rj points from B in C(Vj). So for all cliques Xi corresponding to the first case, we have \u2211 j |X\u03c3(j) \\ C(vj)| \u2264 \u2211 j rj \u2264 |B|.\nIf the cluster C(vj) intersects a marked clique, then assign \u03c3(j) to an arbitrary Xi\u2032 that is not marked by the end of the algorithm. The total number of points in all such C(vj)\u2019s is at most the number of points remaining from the marked cliques, which we previously bounded by |B|, plus up to |B| more points from the bad points. Because the algorithm chose the highest degree nodes in each step, each Xi\u2032 has size at most the size of its corresponding C(vj). Therefore, for all cliques Xi\u2032 corresponding to the second case, we have \u2211 j |X\u03c3(j) \\ C(vj)| \u2264 \u2211 j |X\u03c3(j)| \u2264 2|B|. Thus, over both cases, we reach a total error of 3|B|.\nTheorem 7 (restated). Algorithm 3 outputs a set of centers defining a clustering that is O( (1 + 1\u03b1))-close to OPT for k-median under (1 + \u03b1, )-approximation stability with O(mk(d + log n)) communication.\nProof. The proof is split into two parts, both of which utilize Lemma 14. First, given machine i and 1 \u2264 j \u2264 k, let H ij denote the set of good points from cluster Cj on machine i. Let Bi denote the set of bad points on machine i. Given u, v \u2208 H ij , d(u, v) \u2264 d(u, cj) + d(cj , v) \u2264 2t, so H ij is a clique in Gi2t. Given u \u2208 H ij and v \u2208 H ij\u2032 such that j 6= j\u2032, then\nd(u, v) > d(u, cj\u2032)\u2212 d(cj\u2032 , v) \u2265 18t\u2212 d(u, cj)\u2212 d(cj\u2032 , v) > 16t.\nTherefore, if u and v had a common neighbor w in Gi2t, 16t < d(u, v) \u2264 d(u,w) + d(v, w) \u2264 4t, causing a contradiction. Since Gi2t satisfies the conditions of Lemma 14, it follows that there exists a bijection \u03c3 : [k] \u2192 [k] between the clusters C(vj) and the good clusters H` such that\u2211\nj |H i\u03c3(j) \\C(vj)| \u2264 3|Bi|. Therefore, all but 3|Bi| good points on machine i are within 2t of some point in Ai. Across all machines, \u2211 i |Bi| \u2264 |B|, so there are less than 4|B| good points which are not distance 2t to some point in A. Since two points u \u2208 Hi, v \u2208 Hj for i 6= j are distance > 16t, then each point in A is distance \u2264 2t from good points in at most one setHi. Then we can partition A into setsHA1 , . . . ,HAk , B\u2032, such that for each point u \u2208 HAi , there exists a point v \u2208 Hi such that d(u, v) \u2264 2t. The set B\u2032 consists of points which are not 2t from any good point. From the previous paragraph, |B\u2032| \u2264 3|B|, where |B\u2032| denotes the sum of the weights of all points in B\u2032. Now, given u, v \u2208 HAi , there exist u\u2032, v\u2032 \u2208 Hi such that d(u, u\u2032) \u2264 2t and d(v, v\u2032) \u2264 2t, and d(u, v) \u2264 d(u, u\u2032) + d(u\u2032ci) + d(ci, v\u2032) + d(v\u2032, v) \u2264 6t. Given u \u2208 HAi and w \u2208 HAj for i 6= j, there exist u\u2032 \u2208 Hi, w\u2032 \u2208 Hj such that d(u, u\u2032) \u2264 2t and d(w,w\u2032) \u2264 2t.\nd(u,w) \u2265 d(u\u2032, cj)\u2212 d(u, u\u2032)\u2212 d(cj , w\u2032)\u2212 d(w,w\u2032) > (18t\u2212 d(u, ci))\u2212 2t\u2212 t\u2212 2t \u2265 12t.\nSee Figure 2. Therefore, if u and w had a common neighbor w in G6t, then 12t < d(u, v) \u2264 d(u,w) + d(v, w) \u2264 12t, causing a contradiction. Since G6t satisfies the conditions of Lemma 14 it follows that there exists a bijection \u03c3 : [k] \u2192 [k] between the clusters C(vi) and the good clusters HAj such that \u2211 j |HA\u03c3(j) \\C(vj)| \u2264 3|B\n\u2032|. Recall the centers chosen by the algorithm are labeled as the set X. Let xi \u2208 X denote the center for the cluster Hi according to \u03c3. Then all but 3|B\u2032| good points u \u2208 Hi are distance 2t to a point in A which is distance 6t to xi. u must be distance > 8t to all other points in X because they are distance 2t from good points in other clusters. Therefore, all but 3|B\u2032| \u2264 12|B| good points are correctly clustered. The total error over good and bad points is then 12|B|+ |B| = 13|B| \u2264 (48 + 468\u03b1 ) n so the algorithm achieves error O( (1 + 1 \u03b1)). There are mk points communicated to M1, and the weights have log n bits, so the total communication is O(mk(d+ log n)). This completes the proof for k-median when the algorithm knows wavg up front.\nWhen Algorithm 3 does not know wavg, then it first runs Algorithm 1 to obtain an estimate w\u0302 \u2208 [wavg, \u03b2wavg] for \u03b2 \u2208 O(1). As mentioned in Section 3, \u03b2 can be as low as 18.05 + . Now we reset t in Algorithm 3 to be t\u0302 =\n\u03b1\u03b2wavg 18 . Then the set of bad points grows by a factor of \u03b2, but the\nsame analysis still holds, in particular, Lemma 14 and the above paragraphs go through, adding a factor of \u03b2 to the error.\nAs in Theorem 3, machine 1 can send the center assignments of Ai to machine i, so that each datapoint knows its global center, without increasing the bound on the communication cost.\nNow we show how to generalize Theorem 7 to any `p objective for p < log n.\nTheorem 15. Given p < log n, there exists an algorithm which outputs a set of centers defining a clustering that is O( (1 + (18\u03b1 )\np))-close to OPT for clustering in `p under (1 + \u03b1, )-approximation stability with O(mk log n) communication.\nProof. The proof closely follows the previous proof, with necessary modifications. The properties in Lemma 9 are changed to the following.\n\u2022 Property 1: For all x, for \u2264 x n\u03b1p points v, d(v, cv) p \u2265 \u03b1pOPT px n .\n\u2022 Property 2: For < 6 n points v, d(v, ci)p \u2212 d(v, cv)p \u2264 \u03b1 pOPT p 2 n for some ci 6= cv.\nThen the proof for Lemma 9 remains unchanged. Now Algorithm 3 runs Algorithm 1 using a sequential \u03b2-approximation algorithm. When p < log n, \u03b2 \u2208 \u0398(1) [24]. Then Algorithm 3 sets\nt = ( \u03b1pOPT p 2\u00b718p\u03b2p n ) 1 p = 118 ( \u03b1pOPT p 2\u03b2p n ) 1 p . We define good points as d(v, cv) < t and d(v, cv,2) > 17t, where cv,2 denotes the second-closest optimal center to v. As before, B denotes the set of bad points.\nNow we claim that |B| < (6+ 2\u00b718 p\u03b2p \u03b1p ) n. From the two properties, all but (6+ 2\u00b718p\u03b2p \u03b1p ) n points\nhave d(v, cv) p < \u03b1 pOPT p 2\u00b718p\u03b2p n and d(v, cv,2) p\u2212d(v, cv)p > \u03b1 pOPT p 2\u03b2p n from which it follows that d(v, cv) > t and d(v, cv,2) > 17t. Now we have the same structure as in Theorem 7, where the good points are at distance t to their center, and at distance 17t to their next-closest center. The analysis carries through, and the error grows by a factor of (18\u03b1 ) p.\nTheorem 8 (restated). There exists an algorithm which outputs a clustering that is O( )-close to OPT for k-median under (1+\u03b1, )-approximation stability with O(m2kd+mk log n) communication if each optimal cluster Ci has size \u2126( n(1 + 1 \u03b1)).\nAlgorithm 5 Distributed k-center clustering under 2-approximation stability Input: Distributed points V = V1 \u222a \u00b7 \u00b7 \u00b7 \u222a Vm, optimal cost r\u2217. 1: For each machine t,\nThreshold using distance 2r\u2217. Send one point per connected component to machine 1. 2: Given the set of points received, A, threshold using distance 2r\u2217 again. 3: Pick one point per connected component, X = {x1, . . . , xk}.\nOutput: Centers X = {x1, . . . , xk}\nProof. The algorithm is as follows. First, run Algorithm 3. Then send X \u2032 to each machine i, incurring a communication cost of O(m2kd). For each machine i, for every point v \u2208 Vi, calculate the median distance from v to each cluster C(xj) (using the weights). Assign v to the index j with the minimum median distance. Once every point undergoes this procedure, call the new clusters X1, . . . , Xk, where Xj consists of all points assigned to index j. Now we will prove the clustering {X1, . . . , Xk} is O( )-close to the optimal clustering. Specifically, we will show that all are classified correctly except for the 6 n points in the bad case of Property 2 from Lemma 9.\nAssume each cluster C(xj) contains a majority of points that are 2t to a point in Hj (we will prove this at the end). Given a point v \u2208 Cj such that d(v, ci) \u2212 d(v, cj) > \u03b1wavg2 for all ci 6= cj (Property 2 from Lemma 9), and given a point u \u2208 C(xj) that is at distance 2t to a point u\u2032 \u2208 Hj , then d(v, u) \u2264 d(v, cj)+d(cj , u\u2032)+d(u\u2032, u) \u2264 d(v, cj)+3t. On the other hand, given u \u2208 C(xj\u2032) that is at distance 2t to a point u\u2032 \u2208 Hj\u2032 , then d(v, u) \u2265 d(v, cj\u2032)\u2212d(cj\u2032 , u\u2032)\u2212d(u\u2032, u) > 18t+d(v, cj)\u22123t \u2265 d(v, cj) + 15t. Then v\u2019s median distance to C(xj) is \u2264 d(v, cj) + 3t, and v\u2019s median distance to any other cluster is \u2265 d(v, cj) + 15t, so v will be assigned to the correct cluster.\nNow we will prove each cluster C(xj) contains a majority of points that are 2t to a point in Hj . Assume for all j, |Cj | > 16|B|. It follows that for all j |Hj | > 15|B|. From the proof of Theorem 7, we know that ( \u2211 j Hj \\ ( \u2211 iC(v i j))) \u2264 3|B|, therefore, for all j, HAj > 12|B|, since HAj represents the points in A which are 2t to a point in Hj . Again from the proof of Theorem 7, the clustering {HA1 , . . . ,HAk } is 9|B|-close to X \u2032 = {C(x1), . . . , C(xk)}. Then even if C(xj) is missing 9|B| good points, and contains 3|B| bad points, it will still have a majority of points that are within 2t of a point in Hj . This completes the proof.\nB.1 k-center\nIn this section, we show a simple distributed algorithm to achieve the exact solution for k-center under (2, 0)-approximation stability. Even in the single machine setting, it is NP-hard to find the exact solution to k-center under (2 \u2212 , 0)-approximation stability unless NP = RP [9], therefore our result is tight with respect to the level of stability.\nGiven a 2-approximation stable k-center instance S, we assume our algorithm knows the value of OPT . Since this value is some distance between two points in S, we can use binary search to efficiently guess this value; call it r\u2217.\nTheorem 16. Algorithm 5 outputs the optimal solution for k-center under 2-approximation stability, with O(mkd) communication.\nProof. First we prove the following: u, v \u2208 V are from the same optimal cluster iff d(u, v) \u2264 2r\u2217. The forward direction is straightforward from the triangle inequality: if u, v \u2208 Ci, then d(u, v) \u2264 d(u, ci)+d(ci, v) \u2264 2r\u2217. For the reverse direction, assume there exist i 6= j, u \u2208 Ci, v \u2208 Cj such that d(u, v) \u2264 2r\u2217. Then consider the set of optimal centers {cl}kl=1, but replacing ci with u. Then for all\nu\u2032 \u2208 Ci, d(u, u\u2032) \u2264 d(u, ci) + d(ci, u\u2032) \u2264 2r\u2217. Furthermore, d(u, v) \u2264 2r\u2217. So the optimal clustering, but with v moved from Cj to Ci, achieves cost 2r\n\u2217. This contradicts (2, 0)-approximation stability. Therefore, given any subset V \u2032 \u2286 V of the point set, thresholding using distance 2r\u2217 will exactly\ncluster the points into C \u20321, . . . , C \u2032 k such that C \u2032 i \u2286 Ci for all i. It follows that there is exactly one point per cluster in the outputted centers X. Since a point u \u2208 X \u2229 Ci is distance 2r\u2217 from every point in Ci, X is a 2-approximation. Then by the definition of approximation stability, X defines the optimal clustering."}], "references": [{"title": "k-means++ under approximation stability", "author": ["Manu Agarwal", "Ragesh Jaiswal", "Arindam Pal"], "venue": "Theoretical Computer Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Better guarantees for k-means and euclidean k-median by primal-dual algorithms", "author": ["Sara Ahmadian", "Ashkan Norouzi-Fard", "Ola Svensson", "Justin Ward"], "venue": "arXiv preprint arXiv:1612.07925,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Local search heuristics for k-median and facility location problems", "author": ["Vijay Arya", "Naveen Garg", "Rohit Khandekar", "Adam Meyerson", "Kamesh Munagala", "Vinayaka Pandit"], "venue": "SIAM Journal on Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Center based clustering: A foundational perspective", "author": ["Pranjal Awasthi", "Maria-Florina Balcan"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Center-based clustering under perturbation stability", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "Information Processing Letters,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Spectral embedding of k-cliques, graph partitioning and k-means", "author": ["Pranjal Awasthi", "Moses Charikar", "Ravishankar Krishnaswamy", "Ali Kemal Sinop"], "venue": "In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Improved spectral-norm bounds for clustering. In Approximation, Randomization, and Combinatorial Optimization", "author": ["Pranjal Awasthi", "Or Sheffet"], "venue": "Algorithms and Techniques,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Clustering under approximation stability", "author": ["Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "k-center clustering under perturbation resilience", "author": ["Maria-Florina Balcan", "Nika Haghtalab", "Colin White"], "venue": "In Proceedings of the Annual International Colloquium on Automata, Languages, and Programming (ICALP),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Clustering under perturbation resilience", "author": ["Maria Florina Balcan", "Yingyu Liang"], "venue": "SIAM Journal on Computing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Distributed k-means and kmedian clustering on general topologies", "author": ["Maria-Florina F Balcan", "Steven Ehrlich", "Yingyu Liang"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "Distributed balanced clustering via mapping coresets", "author": ["Mohammadhossein Bateni", "Aditya Bhaskara", "Silvio Lattanzi", "Vahab Mirrokni"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Are stable instances easy? Combinatorics", "author": ["Yonatan Bilu", "Nathan Linial"], "venue": "Probability and Computing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "A tight bound for set disjointness in the message-passing model", "author": ["Mark Braverman", "Faith Ellen", "Rotem Oshman", "Toniann Pitassi", "Vinod Vaikuntanathan"], "venue": "In Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "An improved approximation for k-median, and positive correlation in budgeted optimization", "author": ["Jaros  law Byrka", "Thomas Pensyl", "Bartosz Rybicki", "Aravind Srinivasan", "Khoa Trinh"], "venue": "In Proceedings of the Annual Symposium on Discrete Algorithms (SODA),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "A constant-factor approximation algorithm for the k-median problem", "author": ["Moses Charikar", "Sudipto Guha", "\u00c9va Tardos", "David B Shmoys"], "venue": "In Proceedings of the Annual Symposium on Theory of Computing (STOC),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Algorithms for facility location problems with outliers", "author": ["Moses Charikar", "Samir Khuller", "David M Mount", "Giri Narasimhan"], "venue": "In Proceedings of the Annual Symposium on Discrete Algorithms (SODA),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Communication-optimal distributed clustering", "author": ["Jiecao Chen", "He Sun", "David Woodruff", "Qin Zhang"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "A constant factor approximation algorithm for k-median clustering with outliers", "author": ["Ke Chen"], "venue": "In Proceedings of the Annual Symposium on Discrete Algorithms (SODA),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "A probabilistic analysis of em for mixtures of separated, spherical gaussians", "author": ["Sanjoy Dasgupta", "Leonard Schulman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Data driven resource allocation for distributed learning", "author": ["Travis Dick", "Mu Li", "Venkata Krishna Pillutla", "Colin White", "Maria Florina Balcan", "Alex Smola"], "venue": "In \u201dProceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)\u201d,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2017}, {"title": "On communication cost of distributed statistical estimation and dimensionality", "author": ["Ankit Garg", "Tengyu Ma", "Huy Nguyen"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Clustering to minimize the maximum intercluster distance", "author": ["Teofilo F Gonzalez"], "venue": "Theoretical Computer Science,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1985}, {"title": "Simpler analyses of local search algorithms for facility location", "author": ["Anupam Gupta", "Kanat Tangwongsan"], "venue": "arXiv preprint arXiv:0809.2554,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Decompositions of triangle-dense graphs", "author": ["Rishi Gupta", "Tim Roughgarden", "C Seshadhri"], "venue": "In Proceedings of the 5th conference on Innovations in theoretical computer science,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "The spectral method for general mixture models", "author": ["Ravindran Kannan", "Hadi Salmasian", "Santosh Vempala"], "venue": "SIAM Journal on Computing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Clustering with spectral norm and the k-means algorithm", "author": ["Amit Kumar", "Ravindran Kannan"], "venue": "In Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Improved distributed principal component analysis", "author": ["Yingyu Liang", "Maria-Florina F Balcan", "Vandana Kanchanapally", "David Woodruff"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Approximation algorithms for geometric median problems", "author": ["Jyh-Han Lin", "Jeffrey Scott Vitter"], "venue": "Information Processing Letters,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1992}, {"title": "A bi-criteria approximation algorithm for k means", "author": ["Konstantin Makarychev", "Yury Makarychev", "Maxim Sviridenko", "Justin Ward"], "venue": "In APPROX,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Fast distributed k-center clustering with outliers on massive data", "author": ["Gustavo Malkomes", "Matt J Kusner", "Wenlin Chen", "Kilian Q Weinberger", "Benjamin Moseley"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Spectral partitioning of random graphs", "author": ["Frank McSherry"], "venue": "In Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2001}, {"title": "The effectiveness of lloyd-type methods for the k-means problem", "author": ["Rafail Ostrovsky", "Yuval Rabani", "Leonard J Schulman", "Chaitanya Swamy"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "How to round subspaces: A new spectral clustering algorithm", "author": ["Ali Kemal Sinop"], "venue": "In Proceedings of the Annual Symposium on Discrete Algorithms (SODA),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "On lloyds algorithm: new theoretical insights for clustering in practice", "author": ["Cheng Tang", "Claire Monteleoni"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "The communication complexity of addition", "author": ["Emanuele Viola"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "[12] and Malkomes et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "which is dimension-agnostic [18].", "startOffset": 28, "endOffset": 32}, {"referenceID": 7, "context": "[8] or the spectral stability condition of Kumar and Kannan [27].", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "[8] or the spectral stability condition of Kumar and Kannan [27].", "startOffset": 60, "endOffset": 64}, {"referenceID": 2, "context": "Finding approximation algorithms to different clustering objectives and variants has attracted significant attention in the computer science community [3, 15, 16, 17, 19, 23, 30].", "startOffset": 151, "endOffset": 178}, {"referenceID": 14, "context": "Finding approximation algorithms to different clustering objectives and variants has attracted significant attention in the computer science community [3, 15, 16, 17, 19, 23, 30].", "startOffset": 151, "endOffset": 178}, {"referenceID": 15, "context": "Finding approximation algorithms to different clustering objectives and variants has attracted significant attention in the computer science community [3, 15, 16, 17, 19, 23, 30].", "startOffset": 151, "endOffset": 178}, {"referenceID": 16, "context": "Finding approximation algorithms to different clustering objectives and variants has attracted significant attention in the computer science community [3, 15, 16, 17, 19, 23, 30].", "startOffset": 151, "endOffset": 178}, {"referenceID": 18, "context": "Finding approximation algorithms to different clustering objectives and variants has attracted significant attention in the computer science community [3, 15, 16, 17, 19, 23, 30].", "startOffset": 151, "endOffset": 178}, {"referenceID": 22, "context": "Finding approximation algorithms to different clustering objectives and variants has attracted significant attention in the computer science community [3, 15, 16, 17, 19, 23, 30].", "startOffset": 151, "endOffset": 178}, {"referenceID": 29, "context": "Finding approximation algorithms to different clustering objectives and variants has attracted significant attention in the computer science community [3, 15, 16, 17, 19, 23, 30].", "startOffset": 151, "endOffset": 178}, {"referenceID": 10, "context": "Therefore, distributed clustering algorithms have gained popularity over the past few years [11, 12, 31].", "startOffset": 92, "endOffset": 104}, {"referenceID": 11, "context": "Therefore, distributed clustering algorithms have gained popularity over the past few years [11, 12, 31].", "startOffset": 92, "endOffset": 104}, {"referenceID": 30, "context": "Therefore, distributed clustering algorithms have gained popularity over the past few years [11, 12, 31].", "startOffset": 92, "endOffset": 104}, {"referenceID": 10, "context": "Recent work in the theoretical machine learning community establishes guarantees on the clusterings produced in distributed settings for certain problems [11, 12, 31].", "startOffset": 154, "endOffset": 166}, {"referenceID": 11, "context": "Recent work in the theoretical machine learning community establishes guarantees on the clusterings produced in distributed settings for certain problems [11, 12, 31].", "startOffset": 154, "endOffset": 166}, {"referenceID": 30, "context": "Recent work in the theoretical machine learning community establishes guarantees on the clusterings produced in distributed settings for certain problems [11, 12, 31].", "startOffset": 154, "endOffset": 166}, {"referenceID": 30, "context": "provide distributed algorithms for k-center and k-center with outliers [31], and Bateni et al.", "startOffset": 71, "endOffset": 75}, {"referenceID": 11, "context": "introduce distributed algorithms for capacitated k-clustering under any `p objective [12].", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "However, these approximation ratios are unavoidable in the worst case due to existing lower bounds, for example, k-center cannot be approximated to a factor smaller than 2\u2212 even on a single machine [23].", "startOffset": 198, "endOffset": 202}, {"referenceID": 4, "context": "To go beyond these worst-case results, many recent works have studied natural structure that exists in real-world instances, and shown that algorithms can output a clustering very close to optimal under these natural stability assumptions [5, 7, 8, 10, 13, 21, 27, 37].", "startOffset": 239, "endOffset": 268}, {"referenceID": 6, "context": "To go beyond these worst-case results, many recent works have studied natural structure that exists in real-world instances, and shown that algorithms can output a clustering very close to optimal under these natural stability assumptions [5, 7, 8, 10, 13, 21, 27, 37].", "startOffset": 239, "endOffset": 268}, {"referenceID": 7, "context": "To go beyond these worst-case results, many recent works have studied natural structure that exists in real-world instances, and shown that algorithms can output a clustering very close to optimal under these natural stability assumptions [5, 7, 8, 10, 13, 21, 27, 37].", "startOffset": 239, "endOffset": 268}, {"referenceID": 9, "context": "To go beyond these worst-case results, many recent works have studied natural structure that exists in real-world instances, and shown that algorithms can output a clustering very close to optimal under these natural stability assumptions [5, 7, 8, 10, 13, 21, 27, 37].", "startOffset": 239, "endOffset": 268}, {"referenceID": 12, "context": "To go beyond these worst-case results, many recent works have studied natural structure that exists in real-world instances, and shown that algorithms can output a clustering very close to optimal under these natural stability assumptions [5, 7, 8, 10, 13, 21, 27, 37].", "startOffset": 239, "endOffset": 268}, {"referenceID": 20, "context": "To go beyond these worst-case results, many recent works have studied natural structure that exists in real-world instances, and shown that algorithms can output a clustering very close to optimal under these natural stability assumptions [5, 7, 8, 10, 13, 21, 27, 37].", "startOffset": 239, "endOffset": 268}, {"referenceID": 26, "context": "To go beyond these worst-case results, many recent works have studied natural structure that exists in real-world instances, and shown that algorithms can output a clustering very close to optimal under these natural stability assumptions [5, 7, 8, 10, 13, 21, 27, 37].", "startOffset": 239, "endOffset": 268}, {"referenceID": 0, "context": "states that any c-approximation to the clustering objective is -close to the target clustering [1, 8, 25], and the spectral stability condition of Kumar and Kannan is a deterministic generalization of many generative clustering models [27, 34, 35].", "startOffset": 95, "endOffset": 105}, {"referenceID": 7, "context": "states that any c-approximation to the clustering objective is -close to the target clustering [1, 8, 25], and the spectral stability condition of Kumar and Kannan is a deterministic generalization of many generative clustering models [27, 34, 35].", "startOffset": 95, "endOffset": 105}, {"referenceID": 24, "context": "states that any c-approximation to the clustering objective is -close to the target clustering [1, 8, 25], and the spectral stability condition of Kumar and Kannan is a deterministic generalization of many generative clustering models [27, 34, 35].", "startOffset": 95, "endOffset": 105}, {"referenceID": 26, "context": "states that any c-approximation to the clustering objective is -close to the target clustering [1, 8, 25], and the spectral stability condition of Kumar and Kannan is a deterministic generalization of many generative clustering models [27, 34, 35].", "startOffset": 235, "endOffset": 247}, {"referenceID": 33, "context": "states that any c-approximation to the clustering objective is -close to the target clustering [1, 8, 25], and the spectral stability condition of Kumar and Kannan is a deterministic generalization of many generative clustering models [27, 34, 35].", "startOffset": 235, "endOffset": 247}, {"referenceID": 34, "context": "states that any c-approximation to the clustering objective is -close to the target clustering [1, 8, 25], and the spectral stability condition of Kumar and Kannan is a deterministic generalization of many generative clustering models [27, 34, 35].", "startOffset": 235, "endOffset": 247}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "For example, for k-median, we achieve a (6\u03b1 + 2 + )-approximation by plugging in the bicriteria algorithm of Lin and Vitter [29] (adding an O(log n) factor to the communication cost), as opposed to a 32\u03b1approximation from Bateni et al.", "startOffset": 124, "endOffset": 128}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "By plugging in the approximation algorithm for k-median with outliers [19], our algorithm achieves the first constant approximation for distributed k-median with outliers, answering an open question posed by Malkomes et al.", "startOffset": 70, "endOffset": 74}, {"referenceID": 30, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "gave a communication complexity lower bound of \u03a9(mk) for obtaining any c-approximation for distributed clustering [18].", "startOffset": 114, "endOffset": 118}, {"referenceID": 21, "context": "We close this gap using a directsum reduction from a recent result on distributed mean estimation [22], proving an \u03a9\u0303(mkd) lower bound for finding the optimal k-means centers.", "startOffset": 98, "endOffset": 102}, {"referenceID": 8, "context": "[9], this result is optimal with respect to the value of \u03b1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16], and the current best approximation ratio is 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "For k-center, there is a tight 2-approximation algorithm [23].", "startOffset": 57, "endOffset": 61}, {"referenceID": 1, "context": "357 [2], and Makarychev et al.", "startOffset": 4, "endOffset": 7}, {"referenceID": 29, "context": "recently showed a bicriteria algorithm with strong guarantees [30].", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "For clustering with outliers, there is a 3-approximation algorithm for kcenter with z outliers, as well as a bicriteria 4(1 + 1/ )-approximation algorithm for k-median that picks (1+ )z outliers [17].", "startOffset": 195, "endOffset": 199}, {"referenceID": 18, "context": "Chen found a true constant factor approximation algorithm for k-median (the constant is not explicitly computed) [19].", "startOffset": 113, "endOffset": 117}, {"referenceID": 10, "context": "showed a coreset construction for k-median and k-means, which leads to a clustering algorithm with \u00d5(mkd) communication, and also studied more general graph topologies for distributed computing [11].", "startOffset": 194, "endOffset": 198}, {"referenceID": 11, "context": "indroduced a construction for mapping coresets, which admits a distributed clustering algorithm that can handle balance constraints with communication cost \u00d5(mk) [12].", "startOffset": 162, "endOffset": 166}, {"referenceID": 30, "context": "showed a distributed 13- and 4- approximation algorithm for k-center with and without outliers, respectively [31].", "startOffset": 109, "endOffset": 113}, {"referenceID": 17, "context": "studied clustering under the broadcast model of distributed computing, and also proved a communication complexity lower bound of \u03a9(mk) for distributed clustering [18], building on a recent lower bound for setdisjointness in the message-passing model [14].", "startOffset": 162, "endOffset": 166}, {"referenceID": 13, "context": "studied clustering under the broadcast model of distributed computing, and also proved a communication complexity lower bound of \u03a9(mk) for distributed clustering [18], building on a recent lower bound for setdisjointness in the message-passing model [14].", "startOffset": 250, "endOffset": 254}, {"referenceID": 21, "context": "showed a communication complexity lower bound for computing the mean of d-dimensional points [22].", "startOffset": 93, "endOffset": 97}, {"referenceID": 7, "context": "who showed an algorithm which utilizes the structure to output a nearly optimal clustering [8].", "startOffset": 91, "endOffset": 94}, {"referenceID": 8, "context": "stability, and they proved a matching lower bound, namely that (2\u2212 \u03b4, 0)-approximation stability is NP-hard for any \u03b4 unless NP = RP [9].", "startOffset": 133, "endOffset": 136}, {"referenceID": 26, "context": "Kumar and Kannan introduced a spectral stability condition which generalized many generative models [27], including Gaussian mixture-models [20, 26], the Planted Partition model [32], as well as deterministic conditions [33].", "startOffset": 100, "endOffset": 104}, {"referenceID": 19, "context": "Kumar and Kannan introduced a spectral stability condition which generalized many generative models [27], including Gaussian mixture-models [20, 26], the Planted Partition model [32], as well as deterministic conditions [33].", "startOffset": 140, "endOffset": 148}, {"referenceID": 25, "context": "Kumar and Kannan introduced a spectral stability condition which generalized many generative models [27], including Gaussian mixture-models [20, 26], the Planted Partition model [32], as well as deterministic conditions [33].", "startOffset": 140, "endOffset": 148}, {"referenceID": 31, "context": "Kumar and Kannan introduced a spectral stability condition which generalized many generative models [27], including Gaussian mixture-models [20, 26], the Planted Partition model [32], as well as deterministic conditions [33].", "startOffset": 178, "endOffset": 182}, {"referenceID": 32, "context": "Kumar and Kannan introduced a spectral stability condition which generalized many generative models [27], including Gaussian mixture-models [20, 26], the Planted Partition model [32], as well as deterministic conditions [33].", "startOffset": 220, "endOffset": 224}, {"referenceID": 6, "context": "This work was later improved along several axes, including the dependence on k in the condition, by Awasthi and Sheffet [7].", "startOffset": 120, "endOffset": 123}, {"referenceID": 17, "context": "study distributed algorithms for graph partitioning when the graphs satisfy a notion of stability relating internal expansion of the k pieces to the external expansion [18].", "startOffset": 168, "endOffset": 172}, {"referenceID": 5, "context": "It is known that such graphs also satisfy spectral stability under a suitable Euclidean embedding [6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "In fact, this distinction only changes the cost of the optimal clustering by at most a factor of 2 when p = 1, 2, or \u221e [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 13, "context": "In fact, the two models are equivalent up to small factors in the communication complexity [14].", "startOffset": 91, "endOffset": 95}, {"referenceID": 11, "context": "For more details, see [12, 31].", "startOffset": 22, "endOffset": 30}, {"referenceID": 30, "context": "For more details, see [12, 31].", "startOffset": 22, "endOffset": 30}, {"referenceID": 11, "context": "This generalizes previous distributed clustering results [12, 31], and answers an open question of Malkomes et al.", "startOffset": 57, "endOffset": 65}, {"referenceID": 30, "context": "This generalizes previous distributed clustering results [12, 31], and answers an open question of Malkomes et al.", "startOffset": 57, "endOffset": 65}, {"referenceID": 30, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "It has been pointed out that every clustering algorithm we are aware of has this property [12].", "startOffset": 90, "endOffset": 94}, {"referenceID": 28, "context": "Setting B to be the ( logn , 1 + )-bicriteria approximation algorithm for k-median [29], the approximation ratio becomes 6\u03b1+ 2 + , which improves over the 32\u03b1 approximation ratio of Bateni et al.", "startOffset": 83, "endOffset": 87}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "If we set A as the current best k-median algorithm [15], we achieve a distributed (18.", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "If instead we plug in the sequential approximation algorithm for k-median with z outliers [19], we obtain the first constant-factor approximation algorithm for k-median with outliers, answering an open question from Malkomes et al.", "startOffset": 90, "endOffset": 94}, {"referenceID": 30, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "We can also use the results from Gupta and Tangwongsan [24] to obtain an O(1)-approximation algorithm for 1 < p < log n.", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "First we bound the sum of the local optimal (k+ z)-clustering on each machine by the global clustering with outliers in the following lemma (a non-outlier version of this lemma appears in [12]).", "startOffset": 188, "endOffset": 192}, {"referenceID": 17, "context": "[18], who showed a lower bound independent of the dimension.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "1 in [18] because k-clustering with z outliers is a k + z eligible function: it evaluates to 0 if there are at most k + z points, otherwise it is greater than 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 35, "context": "The communication complexity needed to compute the sum of m numbers, each on a different machine, is \u03a9(m) [36].", "startOffset": 106, "endOffset": 110}, {"referenceID": 21, "context": "Now we use a direct-sum theorem to generalize this result to d-dimensional numbers in Euclidean space [22].", "startOffset": 102, "endOffset": 106}, {"referenceID": 0, "context": "Then we add offsets 4i \u00b7 [1]d to each vector Xi.", "startOffset": 25, "endOffset": 28}, {"referenceID": 21, "context": "[22] (included in", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Appendix A) and Viola [36].", "startOffset": 22, "endOffset": 26}, {"referenceID": 7, "context": "[8] Given a (1+\u03b1, )-approximation stable clustering instance (V, d) for k-median, then Property 1: For all x, for \u2264 x n \u03b1 points v, d(v, cv) \u2265 \u03b1wavg/(x ).", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Even in the single machine setting, it is NP-hard to find the exact solution to k-center under (2 \u2212 , 0)-approximation stability unless NP = RP [9], therefore our result is tight with respect to the level of stability.", "startOffset": 144, "endOffset": 147}, {"referenceID": 27, "context": "This can be done in a distributed manner [28].", "startOffset": 41, "endOffset": 45}, {"referenceID": 6, "context": "The correctness of the algorithm then follows from previous work [7, 27] that shows that in T = log(\u2016A\u2016 ) steps the cluster centers will be recovered to accuracy.", "startOffset": 65, "endOffset": 72}, {"referenceID": 26, "context": "The correctness of the algorithm then follows from previous work [7, 27] that shows that in T = log(\u2016A\u2016 ) steps the cluster centers will be recovered to accuracy.", "startOffset": 65, "endOffset": 72}, {"referenceID": 27, "context": "[28] bounds the communication cost of computing k-SVD to be O(mkd).", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "1: Run the distributed algorithm from [28] to compute \u00c2i\u2019s, i.", "startOffset": 38, "endOffset": 42}, {"referenceID": 17, "context": "A notion related to spectral stability has been recently studied [18] for distributed graph partitioning.", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "design communication efficient distributed algorithms to cluster such stable graphs [18].", "startOffset": 84, "endOffset": 88}, {"referenceID": 5, "context": "In fact, it was shown that stable graphs in the above sense also satisfy spectral stability under an appropriate Euclidean embedding of the nodes of the graph [6].", "startOffset": 159, "endOffset": 162}, {"referenceID": 17, "context": "study a setting where the edges are partitioned across machines and hence their result is formally incomparable to ours [18].", "startOffset": 120, "endOffset": 124}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "We give an algorithm for k-clustering for any `p objective with z outliers using \u00d5(m(k + z)(d + log n)) bits of communication, answering an open question [31] and improving over the previous best approximation ratio [12].", "startOffset": 154, "endOffset": 158}, {"referenceID": 11, "context": "We give an algorithm for k-clustering for any `p objective with z outliers using \u00d5(m(k + z)(d + log n)) bits of communication, answering an open question [31] and improving over the previous best approximation ratio [12].", "startOffset": 216, "endOffset": 220}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "We show how to improve the quality of the clustering produced, provided the data satisfies certain natural notions of stability, specifically, approximation stability and spectral stability [8, 27].", "startOffset": 190, "endOffset": 197}, {"referenceID": 26, "context": "We show how to improve the quality of the clustering produced, provided the data satisfies certain natural notions of stability, specifically, approximation stability and spectral stability [8, 27].", "startOffset": 190, "endOffset": 197}], "year": 2017, "abstractText": "As datasets become larger and more distributed, algorithms for distributed clustering have become more and more important. In this work, we present a general framework for designing distributed clustering algorithms that are robust to outliers. Using our framework, we give a distributed approximation algorithm for k-means, k-median, or generally any `p objective, with z outliers and/or balance constraints, using O(m(k+ z)(d+ log n)) bits of communication, where m is the number of machines, n is the size of the point set, and d is the dimension. This generalizes and improves over the previous work of Bateni et al. [12] and Malkomes et al. [31]. As a special case, we achieve the first distributed algorithm for k-median with outliers, answering an open question posed by Malkomes et al. [31]. For distributed k-means clustering, we provide the first dimension-dependent communication complexity lower bound for finding the optimal clustering. This improves over the lower bound of Chen et al. which is dimension-agnostic [18]. Furthermore, we give distributed clustering algorithms which return nearly optimal solutions, provided the data satisfies the approximation stability condition of Balcan et al. [8] or the spectral stability condition of Kumar and Kannan [27]. In certain clustering applications where each machine only needs to find a clustering consistent with the global optimum, we show that no communication is necessary if the data satisfies approximation stability. \u2217Authors\u2019 addresses: pranjal.awasthi@rutgers.edu, ninamf@cs.cmu.edu, crwhite@cs.cmu.edu. This work was supported in part by NSF grants CCF-1422910, CCF-1535967, IIS-1618714, a Sloan Research Fellowship, a Microsoft Research Faculty Fellowship, a Google Research Award, and a National Defense Science & Engineering Graduate (NDSEG) fellowship. ar X iv :1 70 3. 00 83 0v 1 [ cs .D S] 2 M ar 2 01 7", "creator": "LaTeX with hyperref package"}}}