{"id": "1703.01732", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning", "abstract": "Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as $\\epsilon$-greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent's surprise about its experiences via intrinsic motivation. We propose to learn a model of the MDP transition probabilities concurrently with the policy, and to form intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. One of our approximations results in using surprisal as intrinsic motivation, while the other gives the $k$-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques. We have proposed the hypothesis that, in order to capture the KL-divergence of the true transition probabilities, agents must have intrinsic motivation when doing deep reinforcement learning. This model is highly complex and requires an understanding of the behavior and the behavioral parameters that drive behavior. As these dynamics change, incentives can change the K-divergence of the true transition probabilities in the K-divergence of the true transition probabilities. We also propose a general strategy for estimating K-divergence when the K-divergence of the true transition probabilities is greater than expected. For example, we propose that K-divergence is greater than expected if K-divergence is greater than expected if K-divergence is greater than expected. Our model reveals a high-value K-divergence of the true transition probabilities. Using our models, we perform a systematic search for potential outcomes by choosing an agent's surprise over the K-divergence of the true transition probabilities (Fig. 1a). We propose to obtain a new model of K-divergence in which agents use natural selection in an optimal manner. To estimate K-divergence from the predicted K-divergence of the true transition probabilities, we perform a systematic search for potential outcomes using random-effects and random-effects of the same experiment. The model is well understood by many of the investigators for many of the possible consequences of learning learning the K-d", "histories": [["v1", "Mon, 6 Mar 2017 05:51:42 GMT  (673kb,D)", "http://arxiv.org/abs/1703.01732v1", "Appeared in Deep RL Workshop at NIPS 2016"]], "COMMENTS": "Appeared in Deep RL Workshop at NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["joshua achiam", "shankar sastry"], "accepted": false, "id": "1703.01732"}, "pdf": {"name": "1703.01732.pdf", "metadata": {"source": "CRF", "title": "SURPRISE-BASED INTRINSIC MOTIVATION FOR DEEP REINFORCEMENT LEARNING", "authors": ["Joshua Achiam"], "emails": ["jachiam@berkeley.edu,", "sastry@coe.berkeley.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "A reinforcement learning agent uses experiences obtained from interacting with an unknown environment to learn behavior that maximizes a reward signal. The optimality of the learned behavior is strongly dependent on how the agent approaches the exploration/exploitation trade-off in that environment. If it explores poorly or too little, it may never find rewards from which to learn, and its behavior will always remain suboptimal; if it does find rewards but exploits them too intensely, it may wind up prematurely converging to suboptimal behaviors, and fail to discover more rewarding opportunities. Although substantial theoretical work has been done on optimal exploration strategies for environments with finite state and action spaces, we are here concerned with problems that have continuous state and/or action spaces, where algorithms with theoretical guarantees admit no obvious generalization or are prohibitively impractical to implement.\nSimple heuristic methods of exploring such as -greedy action selection and Gaussian control noise have been successful on a wide range of tasks, but are inadequate when rewards are especially sparse. For example, the Deep Q-Network approach of Mnih et al. [13] used -greedy exploration in training deep neural networks to play Atari games directly from raw pixels. On many games, the algorithm resulted in superhuman play; however, on games like Montezuma\u2019s Revenge, where rewards are extremely sparse, DQN (and its variants [25], [26], [15], [12]) with -greedy exploration failed to achieve scores even at the level of a novice human. Similarly, in benchmarking deep reinforcement learning for continuous control, Duan et al.[5] found that policy optimization algorithms that explored by acting according to the current stochastic policy, including REINFORCE and Trust Region Policy Optimization (TRPO), could succeed across a diverse slate of simulated robotics control tasks with well-defined, non-sparse reward signals (like rewards proportional to the forward velocity of the robot). Yet, when tested in environments with sparse rewards\u2014where the agent would only be able to attain rewards after first figuring out complex motion primitives without reinforcement\u2014every algorithm failed to attain scores better than random agents. The failure modes in all of these cases pertained to the nature of the exploration: the agents encountered reward signals so infrequently that they were never able to learn reward-seeking behavior.\nar X\niv :1\n70 3.\n01 73\n2v 1\n[ cs\n.L G\n] 6\nM ar\n2 01\n7\nOne approach to encourage better exploration is via intrinsic motivation, where an agent has a task-independent, often information-theoretic intrinsic reward function which it seeks to maximize in addition to the reward from the environment. Examples of intrinsic motivation include empowerment, where the agent enjoys the level of control it has about its future; surprise, where the agent is excited to see outcomes that run contrary to its understanding of the world; and novelty, where the agent is excited to see new states (which is tightly connected to surprise, as shown in [2]). For in-depth reviews of the different types of intrinsic motivation, we direct the reader to [1] and [17].\nRecently, several applications of intrinsic motivation to the deep reinforcement learning setting (such as [2], [7], [22]) have found promising success. In this work, we build on that success by exploring scalable measures of surprise for intrinsic motivation in deep reinforcement learning. We formulate surprise as the KL-divergence of the true transition probability distribution from a transition model which is learned concurrently with the policy, and consider two approximations to this divergence which are easy to compute in practice. One of these approximations results in using the surprisal of a transition as an intrinsic reward; the other results in using a measure of learning progress which is closer to a Bayesian concept of surprise. Our contributions are as follows:\n1. we investigate surprisal and learning progress as intrinsic rewards across a wide range of environments in the deep reinforcement learning setting, and demonstrate empirically that the incentives (especially surprisal) result in efficient exploration,\n2. we evaluate the difficulty of the slate of sparse reward continuous control tasks introduced by Houthooft et al. [7] to benchmark exploration incentives, and introduce a new task to complement the slate,\n3. and we present an efficient method for learning the dynamics model (transition probabilities) concurrently with a policy.\nWe distinguish our work from prior work in a number of implementation details: unlike Bellemare et al. [2], we learn a transition model as opposed to a state-action occupancy density; unlike Stadie et al. [22], our formulation naturally encompasses environments with stochastic dynamics; unlike Houthooft et al. [7], we avoid the overhead of maintaining a distribution over possible dynamics models, and learn a single deep dynamics model.\nIn our empirical evaluations, we compare the performance of our proposed intrinsic rewards with other heuristic intrinsic reward schemes and to recent results from the literature. In particular, we compare to Variational Information Maximizing Exploration (VIME) [7], a method which approximately maximizes Bayesian surprise and currently achieves state-of-the-art performance on continuous control with sparse rewards. We show that our incentives can perform on the level of VIME at a lower computational cost."}, {"heading": "2 PRELIMINARIES", "text": "We begin by introducing notation which we will use throughout the paper. A Markov decision process (MDP) is a tuple, (S,A,R, P, \u00b5), where S is the set of states, A is the set of actions, R : S \u00d7 A \u00d7 S \u2192 R is the reward function, P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the transition probability function (where P (s\u2032|s, a) is the probability of transitioning to state s\u2032 given that the previous state was s and the agent took action a in s), and \u00b5 : S \u2192 [0, 1] is the starting state distribution. A policy \u03c0 : S \u00d7 A \u2192 [0, 1] is a distribution over actions per state, with \u03c0(a|s) the probability of selecting a in state s. We aim to select a policy \u03c0 which maximizes a performance measure, L(\u03c0), which usually takes the form of expected finite-horizon total return (sum of rewards in a fixed time period), or expected infinite-horizon discounted total return (discounted sum of all rewards forever). In this paper, we use the finite-horizon total return formulation."}, {"heading": "3 SURPRISE INCENTIVES", "text": "To train an agent with surprise-based exploration, we alternate between making an update step to a dynamics model (an approximator of the MDP\u2019s transition probability function), and making a policy update step that maximizes a trade-off between policy performance and a surprise measure.\nThe dynamics model step makes progress on the optimization problem\nmin \u03c6 \u2212 1 |D| \u2211 (s,a,s\u2032)\u2208D logP\u03c6(s \u2032|s, a) + \u03b1f(\u03c6), (1)\nwhere D is is a dataset of transition tuples from the environment, P\u03c6 is the model we are learning, f is a regularization function, and \u03b1 > 0 is a regularization trade-off coefficient. The policy update step makes progress on an approximation to the optimization problem\nmax \u03c0 L(\u03c0) + \u03b7 E s,a\u223c\u03c0\n[DKL(P ||P\u03c6)[s, a]] , (2)\nwhere \u03b7 > 0 is an explore-exploit trade-off coefficient. The exploration incentive in (2), which we select to be the on-policy average KL-divergence of P\u03c6 from P , is intended to capture the agent\u2019s surprise about its experience. The dynamics model P\u03c6 should only be close to P on regions of the transition state space that the agent has already visited (because those transitions will appear in D and thus the model will be fit to them), and as a result, the KL divergence of P\u03c6 and P will be higher in unfamiliar places. Essentially, this exploits the generalization in the model to encourage the agent to go where it has not gone before. The surprise incentive in (2) gives the net effect of performing a reward shaping of the form\nr\u2032(s, a, s\u2032) = r(s, a, s\u2032) + \u03b7 (logP (s\u2032|s, a)\u2212 logP\u03c6(s\u2032|s, a)) , (3) where r(s, a, s\u2032) is the original reward and r\u2032(s, a, s\u2032) is the transformed reward, so ideally we could solve (2) by applying any reinforcement learning algorithm with these reshaped rewards. In practice, we cannot directly implement this reward reshaping because P is unknown. Instead, we consider two ways of finding an approximate solution to (2).\nIn one method, we approximate the KL-divergence by the cross-entropy, which is reasonable when H(P ) is finite (and small) and P\u03c6 is sufficiently far from P 1; that is, denoting the cross-entropy by H(P, P\u03c6)[s, a] . = Es\u2032\u223cP (\u00b7|s,a)[\u2212 logP\u03c6(s\u2032|s, a)], we assume\nDKL(P ||P\u03c6)[s, a] = H(P, P\u03c6)[s, a]\u2212H(P )[s, a] \u2248 H(P, P\u03c6)[s, a].\n(4)\nThis approximation results in a reward shaping of the form\nr\u2032(s, a, s\u2032) = r(s, a, s\u2032)\u2212 \u03b7 logP\u03c6(s\u2032|s, a); (5) here, the intrinsic reward is the surprisal of s\u2032 given the model P\u03c6 and the context (s, a).\nIn the other method, we maximize a lower bound on the objective in (2) by lower bounding the surprise term:\nDKL(P ||P\u03c6)[s, a] = DKL(P ||P\u03c6\u2032)[s, a] + E s\u2032\u223cP\n[ log P\u03c6\u2032(s \u2032|s, a)\nP\u03c6(s\u2032|s, a) ] \u2265 E s\u2032\u223cP [ log P\u03c6\u2032(s \u2032|s, a)\nP\u03c6(s\u2032|s, a)\n] .\n(6)\nThe bound (6) results in a reward shaping of the form\nr\u2032(s, a, s\u2032) = r(s, a, s\u2032) + \u03b7 (logP\u03c6\u2032(s \u2032|s, a)\u2212 logP\u03c6(s\u2032|s, a)) , (7)\nwhich requires a choice of \u03c6\u2032. From (6), we can see that the bound becomes tighter by minimizing DKL(P ||P\u03c6\u2032). As a result, we choose \u03c6\u2032 to be the parameters of the dynamics model after k updates based on (1), and \u03c6 to be the parameters from before the updates. Thus, at iteration t, the reshaped rewards are\nr\u2032(s, a, s\u2032) = r(s, a, s\u2032) + \u03b7 ( logP\u03c6t(s \u2032|s, a)\u2212 logP\u03c6t\u2212k(s\u2032|s, a) ) ; (8)\nhere, the intrinsic reward is the k-step learning progress at (s, a, s\u2032). It also bears a resemblance to Bayesian surprise; we expand on this similarity in the next section.\nIn our experiments, we investigate both the surprisal bonus (5) and the k-step learning progress bonus (8) (with varying values of k).\n1On the other hand, if H(P )[s, a] is non-finite everywhere\u2014for instance if the MDP has continuous states and deterministic transitions\u2014then as long as it has the same sign everywhere, Es,a\u223c\u03c0[H(P )[s, a]] is a constant with respect to \u03c0 and we can drop it from the optimization problem anyway."}, {"heading": "3.1 DISCUSSION", "text": "Ideally, we would like the intrinsic rewards to vanish in the limit as P\u03c6 \u2192 P , because in this case, the agent should have sufficiently explored the state space, and should primarily learn from extrinsic rewards. For the proposed intrinsic reward in (5), this is not the case, and it may result in poor performance in that limit. The thinking goes that when P\u03c6 = P , the agent will be incentivized to seek out states with the noisiest transitions. However, we argue that this may not be an issue, because the intrinsic motivation seems mostly useful long before the dynamics model is fully learned. As long as the agent is able to find the extrinsic rewards before the intrinsic reward is just the entropy in P , the pathological noise-seeking behavior should not happen. On the other hand, the intrinsic reward in (8) should not suffer from this pathology, because in the limit, as the dynamics model converges, we should have P\u03c6t \u2248 P\u03c6t\u2212k . Then the intrinsic reward will vanish as desired. Next, we relate (8) to Bayesian surprise. The Bayesian surprise associated with a transition is the reduction in uncertainty over possibly dynamics models from observing it ([1],[8]):\nDKL (P (\u03c6|ht, at, st+1)||P (\u03c6|ht)) . Here, P (\u03c6|ht) is meant to represent a distribution over possible dynamics models parametrized by \u03c6 given the preceding history of observed states and actions ht (so ht includes st), and P (\u03c6|ht, at, st+1) is the posterior distribution over dynamics models after observing (at, st+1). By Bayes\u2019 rule, the dynamics prior and posterior are related to the model-based transition probabilities by\nP (\u03c6|ht, at, st+1) = P (\u03c6|ht)P (st+1|ht, at, \u03c6)\nE\u03c6\u223cP (\u00b7|ht) [P (st+1|ht, at, \u03c6)] ,\nso the Bayesian surprise can be expressed as E\n\u03c6\u223cPt+1 [logP (st+1|ht, at, \u03c6)]\u2212 log E \u03c6\u223cPt [P (st+1|ht, at, \u03c6)] , (9)\nwhere Pt+1 = P (\u00b7|ht, at, st+1) is the posterior and Pt = P (\u00b7|ht) is the prior. In this form, the resemblance between (9) and (8) is clarified. Although the update from \u03c6t\u2212k to \u03c6t is not Bayesian\u2014 and is performed in batch, instead of per transition sample\u2014we can imagine (8) might contain similar information to (9)."}, {"heading": "3.2 IMPLEMENTATION DETAILS", "text": "Our implementation usesL2 regularization in the dynamics model fitting, and we impose an additional constraint to keep model iterates close in the KL-divergence sense. Denoting the average divergence as\nD\u0304KL(P\u03c6\u2032 ||P\u03c6) = 1 |D| \u2211\n(s,a)\u2208D\nDKL(P\u03c6\u2032 ||P\u03c6)[s, a], (10)\nour dynamics model update is\n\u03c6i+1 = arg min \u03c6 \u2212 1 |D| \u2211 (s,a,s\u2032)\u2208D logP\u03c6(s \u2032|s, a) + \u03b1\u2016\u03c6\u201622 : D\u0304KL(P\u03c6||P\u03c6i) \u2264 \u03ba. (11)\nThe constraint value \u03ba is a hyper-parameter of the algorithm. We solve this optimization problem approximately using a single second-order step with a line search, as described by [20]; full details are given in supplementary material. D is a FIFO replay memory, and at each iteration, instead of using the entirety of D for the update step we sub-sample a batch d \u2282 D. Also, similarly to [7], we adjust the bonus coefficient \u03b7 at each iteration, to keep the average bonus magnitude upper-bounded (and usually fixed). Let \u03b70 denote the desired average bonus, and r+(s, a, s\u2032) denote the intrinsic reward; then, at each iteration, we set\n\u03b7 = \u03b70 max (\n1, 1|B| \u2223\u2223\u2223\u2211(s,a,s\u2032)\u2208B r+(s, a, s\u2032)\u2223\u2223\u2223) , where B is the batch of data used for the policy update step. This normalization improves the stability of the algorithm by keeping the scale of the bonuses fixed with respect to the scale of the extrinsic rewards. Also, in environments where the agent can die, we avoid the possibility of the intrinsic rewards becoming a living cost by translating all bonuses so that the mean is nonnegative. The basic outline of the algorithm is given as Algorithm 1. In all experiments, we use fully-factored Gaussian distributions for the dynamics models, where the means and variances are the outputs of neural networks.\nAlgorithm 1 Reinforcement Learning with Surprise Incentive Input: Initial policy \u03c00, dynamics model P\u03c60 repeat\ncollect rollouts on current policy \u03c0i add rollout (s, a, s\u2032) tuples to replay memory D compute reshaped rewards using (5) or (8) with dynamics model P\u03c6i normalize \u03b7 by the average intrinsic reward of the current batch of data update policy to \u03c0i+1 using any RL algorithm with the reshaped rewards update the dynamics model to P\u03c6i+1 according to (11)\nuntil training is completed"}, {"heading": "4 EXPERIMENTS", "text": "We evaluate our proposed surprise incentives on a wide range of benchmarks that are challenging for naive exploration methods, including continuous control and discrete control tasks. Our continuous control tasks include the slate of sparse reward tasks introduced by Houthooft et al. [7]: sparse MountainCar, sparse CartPoleSwingup, and sparse HalfCheetah, as well as a new sparse reward task that we introduce here: sparse Swimmer. (We refer to these environments with the prefix \u2018sparse\u2019 to differentiate them from other versions which appear in the literature, where agents receive non-sparse reward signals.) Additionally, we evaluate performance on a highly-challenging hierarchical sparse reward task introduced by Duan et al [5], SwimmerGather. The discrete action tasks are several games from the Atari RAM domain of the OpenAI Gym [4]: Pong, BankHeist, Freeway, and Venture.\nEnvironments with deterministic and stochastic dynamics are represented in our benchmarks: the continuous control domains have deterministic dynamics, while the Gym Atari RAM games have stochastic dynamics. (In the Atari games, actions are repeated for a random number of frames.)\nWe use Trust Region Policy Optimization (TRPO) [20], a state-of-the-art policy gradient method, as our base reinforcement learning algorithm throughout our experiments, and we use the rllab implementations of TRPO and the continuous control tasks [5]. Full details for the experimental set-up are included in the appendix.\nOn all tasks, we compare against TRPO without intrinsic rewards, which we refer to as using naive exploration (in contrast to intrinsically motivated exploration). For the continuous control tasks, we also compare against intrinsic motivation using the L2 model prediction error,\nr+(s, a, s \u2032) = \u2016s\u2032 \u2212 \u00b5\u03c6(s, a)\u20162, (12)\nwhere \u00b5\u03c6 is the mean of the learned Gaussian distribution P\u03c6. The model prediction error was investigated as intrinsic motivation for deep reinforcement learning by Stadie et al [22], although they used a different method for learning the model \u00b5\u03c6. This comparison helps us verify whether or not our proposed form of surprise, as a KL-divergence from the true dynamics model, is useful. Additionally, we compare our performance against the performance reported by Houthooft et al. [7] for Variational Information Maximizing Exploration (VIME), a method where the intrinsic reward associated with a transition approximates its Bayesian surprise using variational methods. Currently, VIME has achieved state-of-the-art results on intrinsic motivation for continuous control.\nAs a final check for the continuous control tasks, we benchmark the tasks themselves, by measuring the performance of the surprisal bonus without any dynamics learning: r+(s, a, s\u2032) = \u2212 logP\u03c60(s\u2032|s, a), where \u03c60 are the original random parameters of P\u03c6. This allows us to verify whether our benchmark tasks actually require surprise to solve at all, or if random exploration strategies successfully solve them."}, {"heading": "4.1 CONTINUOUS CONTROL RESULTS", "text": "Median performance curves are shown in Figure 1 with interquartile ranges shown in shaded areas. Note that TRPO without intrinsic motivation failed on all tasks: the median score and upper quartile range for naive exploration were zero everywhere. Also note that TRPO with random exploration bonuses failed on most tasks, as shown separately in Figure 2. We found that surprise was not needed to solve MountainCar, but was necessary to perform well on the other tasks.\nThe surprisal bonus was especially robust across tasks, achieving good results in all domains and substantially exceeding the other baselines on the more challenging ones. The learning progress bonus for k = 1 was successful on CartpoleSwingup and HalfCheetah but it faltered in the others. Its weak performance in MountainCar was due to premature convergence of the dynamics model, which resulted in the agent receiving intrinsic rewards that were identically zero. (Given the simplicity of the environment, it is not surprising that the dynamics model converged so quickly.) In Swimmer, however, it seems that the learning progress bonuses did not inspire sufficient exploration. Because the Swimmer environment is effectively a stepping stone to the harder SwimmerGather, where the agent has to learn a motion primitive and collect target pellets, on SwimmerGather, we only evaluated the intrinsic rewards that had been successful on Swimmer.\nBoth surprisal and learning progress (with k = 1) exceeded the reported performance of VIME on HalfCheetah by learning to solve the task more quickly. On CartpoleSwingup, however, both were more susceptible to getting stuck in locally optimal policies, resulting in lower median scores than VIME. Surprisal performed comparably to VIME on SwimmerGather, the hardest task in the slate\u2014in the sense that after 1000 iterations, they both reached approximately the same median score\u2014although with greater variance than VIME.\nOur results suggest that surprisal is a viable alternative to VIME in terms of performance, and is highly favorable in terms of computational cost. In VIME, a backwards pass through the dynamics model must be computed for every transition tuple separately to compute the intrinsic rewards, whereas our surprisal bonus only requires forward passes through the dynamics model for intrinsic\nreward computation. (Limitations of current deep learning tool kits make it difficult to efficiently compute separate backwards passes, whereas almost all of them support highly parallel forward computations.) Furthermore, our dynamics model is substantially simpler than the Bayesian neural network dynamics model of VIME. To illustrate this point, in Figure 3 we show the results of a speed comparison making use of the open-source VIME code [6], with the settings described in the VIME paper. In our speed test, our bonus had a per-iteration speedup of a factor of 3 over VIME.2 We give a full analysis of the potential speedup in Appendix C."}, {"heading": "4.2 ATARI RAM DOMAIN RESULTS", "text": "Median performance curves are shown in Figure 4, with tasks arranged from (a) to (d) roughly in order of increasing difficulty.\nIn Pong, naive exploration naturally succeeds, so we are not surprised to see that intrinsic motivation does not improve performance. However, this serves as a sanity check to verify that our intrinsic rewards do not degrade performance. (As an aside, we note that the performance here falls short of the standard score of 20 for this domain because we truncate play at 5000 timesteps.)\nIn BankHeist, we find that intrinsic motivation accelerates the learning significantly. The agents with surprisal incentives reached high levels of performance (scores > 1000) 10% sooner than naive exploration, while agents with learning progress incentives reached high levels almost 20% sooner.\nIn Freeway, the median performance for TRPO without intrinsic motivation was adequate, but the lower quartile range was quite poor\u2014only 6 out of 10 runs ever found rewards. With the learning progress incentives, 8 out of 10 runs found rewards; with the surprisal incentive, all 10 did. Freeway is a game with very sparse rewards, where the agent effectively has to cross a long hallway before it can score a point, so naive exploration tends to exhibit random walk behavior and only rarely reaches the reward state. The intrinsic motivation helps the agent explore more purposefully.\n2We compute this by comparing the marginal time cost incurred just by the bonus in each case: that is, if Tvime, Tsurprisal, and Tnobonus denote the times to 15 iterations, we obtain the speedup as\nTvime \u2212 Tnobonus Tsurprisal \u2212 Tnobonus .\nIn Venture, we obtain our strongest results in the Atari domain. Venture is extremely difficult because the agent has to navigate a large map to find very sparse rewards, and the agent can be killed by enemies interspersed throughout. We found that our intrinsic rewards were able to substantially improve performance over naive exploration in this challenging environment. Here, the best performance was again obtained by the surprisal incentive, which usually inspired the agent to reach scores greater than 500."}, {"heading": "4.3 COMPARING INCENTIVES", "text": "Among our proposed incentives, we found that surprisal worked the best overall, achieving the most consistent performance across tasks. The learning progress-based incentives worked well on some domains, but generally not as well as surprisal. Interestingly, learning progress with k = 10 performed much worse on the continuous control tasks than with k = 1, but we observed virtually no difference in their performance on the Atari games; it is unclear why this should be the case.\nSurprisal strongly outperformed the L2 error based incentive on the harder continuous control tasks, learning to solve them more quickly and without forgetting. Because we used fully-factored Gaussians for all of our dyanmics models, the surprisal had the form\n\u2212 logP\u03c6(s\u2032|s, a) = n\u2211 i=1\n( (s\u2032i \u2212 \u00b5\u03c6,i(s, a))2\n2\u03c32\u03c6,i(s, a) + log \u03c3\u03c6,i(s, a)\n) + k\n2 log 2\u03c0,\nwhich essentially includes the L2-squared error norm as a sub-expression. The relative difference in performance suggests that the variance terms confer additional useful information about the novelty of a state-action pair."}, {"heading": "5 RELATED WORK", "text": "Substantial theoretical work has been done on optimal exploration in finite MDPs, resulting in algorithms such as E3 [10], R-max [3], and UCRL [9], which scale polynomially with MDP size. However, these works do not permit obvious generalizations to MDPs with continuous state and action spaces. C-PACE [18] provides a theoretical foundation for PAC-optimal exploration in MDPs with continuous state spaces, but it requires a metric on state spaces. Lopes et al. [11] investigated exploration driven by learning progress and proved theoretical guarantees for their approach in the finite MDP case, but they did not address the question of scaling their approach to continuous or high-dimensional MDPs. Also, although they formulated learning progress in the same way as (8), they formed intrinsic rewards differently. Conceptually and mathematically, our work is closest to prior work on curiosity and surprise [8, 19, 23, 24], although these works focus mainly on small finite MDPs.\nRecently, several intrinsic motivation strategies that deal specifically with deep reinforcement learning have been proposed. Stadie et al. [22] learn deterministic dynamics models by minimizing Euclidean loss\u2014whereas in our work, we learn stochastic dynamics with cross entropy loss\u2014and use L2 prediction errors for intrinsic motivation. Houthooft et al. [7] train Bayesian neural networks to approximate posterior distributions over dynamics models given observed data, by maximizing a variational lower bound; they then use second-order approximations of the Bayesian surprise as intrinsic motivation. Bellemare et al. [2] derived pseudo-counts from CTS density models over states and used those to form intrinsic rewards, notably resulting in dramatic performance improvement on Montezuma\u2019s Revenge, one of the hardest games in the Atari domain. Mohamed and Rezende [14] developed a scalable method of approximating empowerment, the mutual information between an agent\u2019s actions and the future state of the environment, using variational methods. Oh et al. [16] estimated state visit frequency using Gaussian kernels to compare against a replay memory, and used these estimates for directed exploration."}, {"heading": "6 CONCLUSIONS", "text": "In this work, we formulated surprise for intrinsic motivation as the KL-divergence of the true transition probabilities from learned model probabilities, and derived two approximations\u2014surprisal and k-step\nlearning progress\u2014that are scalable, computationally inexpensive, and suitable for application to high-dimensional and continuous control tasks. We showed that empirically, motivation by surprisal and 1-step learning progress resulted in efficient exploration on several hard deep reinforcement learning benchmarks. In particular, we found that surprisal was a robust and effective intrinsic motivator, outperforming other heuristics on a wide range of tasks, and competitive with the current state-of-the-art for intrinsic motivation in continuous control."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We thank Rein Houthooft for interesting discussions and for sharing data from the original VIME experiments. We also thank Rocky Duan, Carlos Florensa, Vicenc Rubies-Royo, Dexter Scobee, and Eric Mazumdar for insightful discussions and reviews of the preliminary manuscript.\nThis work is supported by TRUST (Team for Research in Ubiquitous Secure Technology) which receives support from NSF (award number CCF-0424422)."}, {"heading": "A SINGLE STEP SECOND-ORDER OPTIMIZATION", "text": "In our experiments, we approximately solve several optimization problems by using a single secondorder step with a line search. This section will describe the exact methodology, which was originally given by Schulman et al. [20].\nWe consider the optimization problem\np\u2217 = max \u03b8 L(\u03b8) : D(\u03b8) \u2264 \u03b4, (13)\nwhere \u03b8 \u2208 Rn, and for some \u03b8old we have D(\u03b8old) = 0,\u2207\u03b8D(\u03b8old) = 0, and\u22072\u03b8D(\u03b8old) 0; also, \u2200\u03b8,D(\u03b8) \u2265 0. We suppose that \u03b4 is small, so the optimal point will be close to \u03b8old. We also suppose that the curvature of the constraint is much greater than the curvature of the objective. As a result, we feel justified in approximating the objective to linear order and the constraint to quadratic order:\nL(\u03b8) \u2248 L(\u03b8old) + gT (\u03b8 \u2212 \u03b8old) g . = \u2207\u03b8L(\u03b8old)\nD(\u03b8) \u2248 1 2 (\u03b8 \u2212 \u03b8old)TA(\u03b8 \u2212 \u03b8old) A . = \u22072\u03b8D(\u03b8old).\nWe now consider the approximate optimization problem,\np\u2217 \u2248 max \u03b8 gT (\u03b8 \u2212 \u03b8old) :\n1 2 (\u03b8 \u2212 \u03b8old)TA(\u03b8 \u2212 \u03b8old) \u2264 \u03b4.\nThis optimization problem is convex as long as A 0, which is an assumption that we make. (If this assumption seems to be empirically invalid, then we repair the issue by using the substitution A\u2192 A+ I , where I is the identity matrix, and > 0 is a small constant chosen so that we usually have A+ I 0.) This problem can be solved analytically by applying methods of duality, and its optimal point is\n\u03b8\u2217 = \u03b8old +\n\u221a 2\u03b4\ngTA\u22121g A\u22121g. (14)\nIt is possible that the parameter update step given by (14) may not exactly solve the original optimization problem (13)\u2014in fact, it may not even satisfy the constraint\u2014so we perform a line search between \u03b8old and \u03b8\u2217. Our update with the line search included is given by\n\u03b8 = \u03b8old + s k\n\u221a 2\u03b4\ngTA\u22121g A\u22121g, (15)\nwhere s \u2208 (0, 1) is a backtracking coefficient, and k is the smallest integer for which L(\u03b8) \u2265 L(\u03b8old) and D(\u03b8) \u2264 \u03b4. We select k by checking each of k = 1, 2, ...,K, where K is the maximum number of backtracks. If there is no value of k in that range which satisfies the conditions, no update is performed.\nBecause the optimization problems we solve with this method tend to involve thousands of parameters, inverting A is prohibitively computationally expensive. Thus in the implementation of this algorithm that we use, the search direction x = A\u22121g is found by using the conjugate gradient method to solve Ax = g; this avoids the need to invert A.\nWhen A and g are sample averages meant to stand in for expectations, we employ an additional trick to reduce the total number of computations necessary to solve Ax = g. The computation of A is more expensive than g, and so we use a smaller fraction of the population to estimate it quickly. Concretely, suppose that the original optimization problem\u2019s objective is Ez\u223cP [L(\u03b8, z)], and the constraint is Ez\u223cP [D(\u03b8, z)] \u2264 \u03b4, where z is some random variable and P is its distribution; furthermore, suppose that we have a dataset of samples D = {zi}i=1,...,N drawn on P , and we form an approximate optimization problem using these samples. Defining g(z) .= \u2207\u03b8L(\u03b8old, z) and A(z)\n. = \u22072\u03b8D(\u03b8old, z), we would need to solve(\n1 |D| \u2211 z\u2208D A(z)\n) x = 1\n|D| \u2211 z\u2208D g(z)\nto obtain the search direction x. However, because the computation of the average Hessian is expensive, we sub-sample a batch b \u2282 D to form it. As long as b is a large enough set, then the approximation\n1 |b| \u2211 z\u2208b A(z) \u2248 1 |D| \u2211 z\u2208D A(z) \u2248 E z\u223cP [A(z)]\nis good, and the search direction we obtain by solving( 1\n|b| \u2211 z\u2208b A(z)\n) x = 1\n|D| \u2211 z\u2208D g(z)\nis reasonable. The sub-sample ratio |b|/|D| is a hyperparameter of the algorithm."}, {"heading": "B EXPERIMENT DETAILS", "text": "B.1 ENVIRONMENTS\nThe environments have the following state and action spaces: for the sparse MountainCar environment, S \u2286 R2, A \u2286 R1; for the sparse CartpoleSwingup task, S \u2286 R4, A \u2286 R1; for the sparse HalfCheetah\ntask, S \u2282 R20, A \u2286 R6; for the sparse Swimmer task, S \u2286 R13, A \u2286 R2; for the SwimmerGather task, S \u2286 R33, A \u2286 R2; for the Atari RAM domain, S \u2286 R128, A \u2286 {1, ..., 18}. For the sparse MountainCar task, the agent receives a reward of 1 only when it escapes the valley. For the sparse CartpoleSwingup task, the agent receives a reward of 1 only when cos(\u03b2) > 0.8, with \u03b2 the pole angle. For the sparse HalfCheetah task, the agent receives a reward of 1 when xbody \u2265 5. For the sparse Swimmer task, the agent receives a reward of 1 + |vbody| when |xbody| \u2265 2. Atari RAM states, by default, take on values from 0 to 256 in integer intervals. We use a simple preprocessing step to map them onto values in (\u22121/3, 1/3). Let x denote the raw RAM state, and s the preprocessed RAM state:\ns = 1\n3 ( x 128 \u2212 1 ) .\nB.2 POLICY AND VALUE FUNCTIONS\nFor all continuous control tasks we used fully-factored Gaussian policies, where the means of the action distributions were the outputs of neural networks, and the variances were separate trainable parameters. For the sparse MountainCar and sparse CartpoleSwingup tasks, the policy mean networks had a single hidden layer of 32 units. For sparse HalfCheetah, sparse Swimmer, and SwimmerGather, the policy mean networks were of size (64, 32). For the Atari RAM tasks, we used categorical distributions over actions, produced by neural networks of size (64, 32).\nThe value functions used for the sparse MountainCar and sparse CartpoleSwingup tasks were neural networks with a single hidden layer of 32 units. For sparse HalfCheetah, sparse Swimmer, and SwimmerGather, time-varying linear value functions were used, as described by Duan et al. [5]. For the Atari RAM tasks, the value functions were neural networks of size (64, 32). The neural network value functions were learned via single second-order step optimization; the linear baselines were obtained by least-squares fit at each iteration.\nAll neural networks were feed-forward, fully-connected networks with tanh activation units.\nB.3 TRPO HYPERPARAMETERS\nFor all tasks, the MDP discount factor \u03b3 was fixed to 0.995, and generalized advantage estimators (GAE) [21] were used, with the GAE \u03bb parameter fixed to 0.95.\nIn the table below, we show several other TRPO hyperparameters. Batch size refers to steps of experience collected at each iteration. The sub-sample factor is for the second-order optimization step, as detailed in Appendix A.\nB.4 EXPLORATION HYPERPARAMETERS\nFor all tasks, fully-factored Gaussian distributions were used as dynamics models, where the means and variances of the distributions were the outputs of neural networks.\nFor the sparse MountainCar and sparse CartpoleSwingup tasks, the means and variances were parametrized by single hidden layer neural networks with 32 units. For all other tasks, the means and variances were parametrized by neural networks with two hidden layers of size 64 units each. All networks used tanh activation functions.\nFor all continuous control tasks except SwimmerGather, we used replay memories of size 5, 000, 000, and a KL-divergence step size of \u03ba = 0.001. For SwimmerGather, the replay memory was the same size, but we set the KL-divergence size to \u03ba = 0.005. For the Atari RAM domain tasks, we used replay memories of size 1, 000, 000, and a KL-divergence step size of \u03ba = 0.01.\nFor all tasks except SwimmerGather and Venture, 5000 time steps of experience were sampled from the replay memory at each iteration of dynamics model learning to take a stochastic step on (11), and a sub-sample factor of 1 was used in the second-order step optimizer. For SwimmerGather and Venture, 10, 000 time steps of experience were sampled at each iteration, and a sub-sample factor of 0.5 was used in the optimizer.\nFor all continuous control tasks, the L2 penalty coefficient was set to \u03b1 = 1. For the Atari RAM tasks except for Venture, it was set to \u03b1 = 0.01. For Venture, it was set to \u03b1 = 0.1.\nFor all continuous control tasks except SwimmerGather, \u03b70 = 0.001. For SwimmerGather, \u03b70 = 0.0001. For the Atari RAM tasks, \u03b70 = 0.005."}, {"heading": "C ANALYSIS OF SPEEDUP COMPARED TO VIME", "text": "In this section, we provide an analysis of the time cost incurred by using VIME or our bonuses, and derive the potential magnitude of speedup attained by our bonuses versus VIME.\nAt each iteration, bonuses based on learned dynamics models incur two primary costs:\n\u2022 the time cost of fitting the dynamics model, \u2022 and the time cost of computing the rewards.\nWe denote the dynamics fitting costs for VIME and our methods as T fitvime and T fit ours. Although the Bayesian neural network dynamics model for VIME is more complex than our model, the fit times can work out to be similar depending on the choice of fitting algorithm. In our speed test, the fit times were nearly equivalent, but used different algorithms.\nFor the time cost of computing rewards, we first introduce the following quantities:\n\u2022 n: the number of CPU threads available, \u2022 tf : time for a forward pass through the model, \u2022 tb: time for a backward pass through the model, \u2022 N : batch size (number of samples per iteration), \u2022 k: the number of forward passes that can be performed simultaneously.\nFor our method, the time cost of computing rewards is\nT rewours = Ntf kn .\nFor VIME, things are more complex. Each reward requires the computation of a gradient through its model, which necessitates a forward and a backward pass. Because gradient calculations cannot be efficiently parallelized by any deep learning toolkits currently available3, each (s, a, s\u2032) tuple requires its own forward/backward pass. As a result, the time cost of computing rewards for VIME is:\nT rewvime = N(tf + tb)\nn .\nThe speedup of our method over VIME is therefore\nT fitvime + N(tf+tb) n\nT fitours + Ntf kn\n.\nIn the limit of large N , and with the approximation that tf \u2248 tb, the speedup is a factor of \u223c 2k. 3If this is not correct, please contact the authors so that we can issue a correction! But to the best of our knowledge, this is currently true, at time of publication."}], "references": [{"title": "Novelty or Surprise", "author": ["Andrew Barto", "Marco Mirolli", "Gianluca Baldassarre"], "venue": "Frontiers in Psychology,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Unifying Count-Based Exploration and Intrinsic Motivation", "author": ["Marc G Bellemare", "Sriram Srinivasan", "Georg Ostrovski", "Tom Schaul", "David Saxton", "Google Deepmind", "R\u00e9mi Munos"], "venue": "arXiv, (Im),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "R-max \u2013 A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning", "author": ["Ronen I Brafman", "Moshe Tennenholtz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Benchmarking Deep Reinforcement Learning for Continuous Control", "author": ["Yan Duan", "Xi Chen", "John Schulman", "Pieter Abbeel"], "venue": "The 33rd International Conference on Machine Learning", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "VIME Open-Source Code", "author": ["Rein Houthooft"], "venue": "https://github.com/openai/vime,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Variational Information Maximizing Exploration", "author": ["Rein Houthooft", "Xi Chen", "Yan Duan", "John Schulman", "Filip De Turck", "Pieter Abbeel"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Bayesian surprise attracts human attention", "author": ["Laurent Itti", "Pierre Baldi"], "venue": "Vision Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Near-optimal Regret Bounds for Reinforcement Learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Near Optimal Reinforcement Learning in Polynomial Time", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Proceedings of the 15th International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Exploration in model-based reinforcement learning by empirically estimating learning progress", "author": ["Manuel Lopes", "Tobias Lang", "Marc Toussaint", "Py Oudeyer"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Asynchronous Methods for Deep Reinforcement Learning", "author": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P. Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "In ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei a Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning", "author": ["Shakir Mohamed", "Danilo J Rezende"], "venue": "In Proceedings of the 29th Conference on Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Massively Parallel Methods for Deep Reinforcement Learning", "author": ["Arun Nair", "Praveen Srinivasan", "Sam Blackwell", "Cagdas Alcicek", "Rory Fearon", "Alessandro De Maria", "Mustafa Suleyman", "Charles Beattie", "Stig Petersen", "Shane Legg", "Volodymyr Mnih", "David Silver"], "venue": "ICML Deep Learning Workshop 2015,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Action- Conditional Video Prediction using Deep Networks in Atari Games", "author": ["Junhyuk Oh", "Guo Xiaoxiao", "Lee Honglak", "Lewis Richard", "Singh Satinder"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "How can we define intrinsic motivation", "author": ["Pierre-Yves Oudeyer", "Frederic Kaplan"], "venue": "In 8th International Conference on Epigenetic Robotics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "PAC Optimal Exploration in Continuous Space Markov Decision Processes", "author": ["Jason Pazis", "Ronald Parr"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Curious Model-Building Control Systems", "author": ["J\u00fcrgen Schmidhuber"], "venue": "International Joint Conference on Neural Networks,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1991}, {"title": "Trust Region Policy Optimization", "author": ["John Schulman", "Philipp Moritz", "Michael Jordan", "Pieter Abbeel"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "High- Dimensional Continuous Control Using Generalized Advantage Estimation", "author": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel"], "venue": "In ICLR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Incentivizing Exploration In Reinforcement Learning", "author": ["Bradly C. Stadie", "Sergey Levine", "Pieter Abbeel"], "venue": "With Deep Predictive Models. arXiv,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Reinforcement driven information acquisition in non-deterministic environments", "author": ["Jan Storck", "Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of the International . . . ,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1995}, {"title": "Planning to be surprised: Optimal Bayesian exploration in dynamic environments", "author": ["Yi Sun", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In International Conference on Artificial General Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Deep Reinforcement Learning with Double Q-learning", "author": ["Hado van Hasselt", "Arthur Guez", "David Silver"], "venue": "In AAAI 2016,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}], "referenceMentions": [{"referenceID": 11, "context": "[13] used -greedy exploration in training deep neural networks to play Atari games directly from raw pixels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "On many games, the algorithm resulted in superhuman play; however, on games like Montezuma\u2019s Revenge, where rewards are extremely sparse, DQN (and its variants [25], [26], [15], [12]) with -greedy exploration failed to achieve scores even at the level of a novice human.", "startOffset": 160, "endOffset": 164}, {"referenceID": 13, "context": "On many games, the algorithm resulted in superhuman play; however, on games like Montezuma\u2019s Revenge, where rewards are extremely sparse, DQN (and its variants [25], [26], [15], [12]) with -greedy exploration failed to achieve scores even at the level of a novice human.", "startOffset": 172, "endOffset": 176}, {"referenceID": 10, "context": "On many games, the algorithm resulted in superhuman play; however, on games like Montezuma\u2019s Revenge, where rewards are extremely sparse, DQN (and its variants [25], [26], [15], [12]) with -greedy exploration failed to achieve scores even at the level of a novice human.", "startOffset": 178, "endOffset": 182}, {"referenceID": 3, "context": "[5] found that policy optimization algorithms that explored by acting according to the current stochastic policy, including REINFORCE and Trust Region Policy Optimization (TRPO), could succeed across a diverse slate of simulated robotics control tasks with well-defined, non-sparse reward signals (like rewards proportional to the forward velocity of the robot).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Examples of intrinsic motivation include empowerment, where the agent enjoys the level of control it has about its future; surprise, where the agent is excited to see outcomes that run contrary to its understanding of the world; and novelty, where the agent is excited to see new states (which is tightly connected to surprise, as shown in [2]).", "startOffset": 340, "endOffset": 343}, {"referenceID": 0, "context": "For in-depth reviews of the different types of intrinsic motivation, we direct the reader to [1] and [17].", "startOffset": 93, "endOffset": 96}, {"referenceID": 15, "context": "For in-depth reviews of the different types of intrinsic motivation, we direct the reader to [1] and [17].", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "Recently, several applications of intrinsic motivation to the deep reinforcement learning setting (such as [2], [7], [22]) have found promising success.", "startOffset": 107, "endOffset": 110}, {"referenceID": 5, "context": "Recently, several applications of intrinsic motivation to the deep reinforcement learning setting (such as [2], [7], [22]) have found promising success.", "startOffset": 112, "endOffset": 115}, {"referenceID": 20, "context": "Recently, several applications of intrinsic motivation to the deep reinforcement learning setting (such as [2], [7], [22]) have found promising success.", "startOffset": 117, "endOffset": 121}, {"referenceID": 5, "context": "[7] to benchmark exploration incentives, and introduce a new task to complement the slate, 3.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2], we learn a transition model as opposed to a state-action occupancy density; unlike Stadie et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[22], our formulation naturally encompasses environments with stochastic dynamics; unlike Houthooft et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[7], we avoid the overhead of maintaining a distribution over possible dynamics models, and learn a single deep dynamics model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "In particular, we compare to Variational Information Maximizing Exploration (VIME) [7], a method which approximately maximizes Bayesian surprise and currently achieves state-of-the-art performance on continuous control with sparse rewards.", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "A Markov decision process (MDP) is a tuple, (S,A,R, P, \u03bc), where S is the set of states, A is the set of actions, R : S \u00d7 A \u00d7 S \u2192 R is the reward function, P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the transition probability function (where P (s\u2032|s, a) is the probability of transitioning to state s\u2032 given that the previous state was s and the agent took action a in s), and \u03bc : S \u2192 [0, 1] is the starting state distribution.", "startOffset": 172, "endOffset": 178}, {"referenceID": 0, "context": "A Markov decision process (MDP) is a tuple, (S,A,R, P, \u03bc), where S is the set of states, A is the set of actions, R : S \u00d7 A \u00d7 S \u2192 R is the reward function, P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the transition probability function (where P (s\u2032|s, a) is the probability of transitioning to state s\u2032 given that the previous state was s and the agent took action a in s), and \u03bc : S \u2192 [0, 1] is the starting state distribution.", "startOffset": 368, "endOffset": 374}, {"referenceID": 0, "context": "A policy \u03c0 : S \u00d7 A \u2192 [0, 1] is a distribution over actions per state, with \u03c0(a|s) the probability of selecting a in state s.", "startOffset": 21, "endOffset": 27}, {"referenceID": 0, "context": "The Bayesian surprise associated with a transition is the reduction in uncertainty over possibly dynamics models from observing it ([1],[8]): DKL (P (\u03c6|ht, at, st+1)||P (\u03c6|ht)) .", "startOffset": 132, "endOffset": 135}, {"referenceID": 6, "context": "The Bayesian surprise associated with a transition is the reduction in uncertainty over possibly dynamics models from observing it ([1],[8]): DKL (P (\u03c6|ht, at, st+1)||P (\u03c6|ht)) .", "startOffset": 136, "endOffset": 139}, {"referenceID": 18, "context": "We solve this optimization problem approximately using a single second-order step with a line search, as described by [20]; full details are given in supplementary material.", "startOffset": 118, "endOffset": 122}, {"referenceID": 5, "context": "Also, similarly to [7], we adjust the bonus coefficient \u03b7 at each iteration, to keep the average bonus magnitude upper-bounded (and usually fixed).", "startOffset": 19, "endOffset": 22}, {"referenceID": 5, "context": "[7]: sparse MountainCar, sparse CartPoleSwingup, and sparse HalfCheetah, as well as a new sparse reward task that we introduce here: sparse Swimmer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": ") Additionally, we evaluate performance on a highly-challenging hierarchical sparse reward task introduced by Duan et al [5], SwimmerGather.", "startOffset": 121, "endOffset": 124}, {"referenceID": 18, "context": ") We use Trust Region Policy Optimization (TRPO) [20], a state-of-the-art policy gradient method, as our base reinforcement learning algorithm throughout our experiments, and we use the rllab implementations of TRPO and the continuous control tasks [5].", "startOffset": 49, "endOffset": 53}, {"referenceID": 3, "context": ") We use Trust Region Policy Optimization (TRPO) [20], a state-of-the-art policy gradient method, as our base reinforcement learning algorithm throughout our experiments, and we use the rllab implementations of TRPO and the continuous control tasks [5].", "startOffset": 249, "endOffset": 252}, {"referenceID": 20, "context": "The model prediction error was investigated as intrinsic motivation for deep reinforcement learning by Stadie et al [22], although they used a different method for learning the model \u03bc\u03c6.", "startOffset": 116, "endOffset": 120}, {"referenceID": 5, "context": "[7] for Variational Information Maximizing Exploration (VIME), a method where the intrinsic reward associated with a transition approximates its Bayesian surprise using variational methods.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7], reproduced here with permission.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "To illustrate this point, in Figure 3 we show the results of a speed comparison making use of the open-source VIME code [6], with the settings described in the VIME paper.", "startOffset": 120, "endOffset": 123}, {"referenceID": 8, "context": "Substantial theoretical work has been done on optimal exploration in finite MDPs, resulting in algorithms such as E [10], R-max [3], and UCRL [9], which scale polynomially with MDP size.", "startOffset": 116, "endOffset": 120}, {"referenceID": 2, "context": "Substantial theoretical work has been done on optimal exploration in finite MDPs, resulting in algorithms such as E [10], R-max [3], and UCRL [9], which scale polynomially with MDP size.", "startOffset": 128, "endOffset": 131}, {"referenceID": 7, "context": "Substantial theoretical work has been done on optimal exploration in finite MDPs, resulting in algorithms such as E [10], R-max [3], and UCRL [9], which scale polynomially with MDP size.", "startOffset": 142, "endOffset": 145}, {"referenceID": 16, "context": "C-PACE [18] provides a theoretical foundation for PAC-optimal exploration in MDPs with continuous state spaces, but it requires a metric on state spaces.", "startOffset": 7, "endOffset": 11}, {"referenceID": 9, "context": "[11] investigated exploration driven by learning progress and proved theoretical guarantees for their approach in the finite MDP case, but they did not address the question of scaling their approach to continuous or high-dimensional MDPs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Conceptually and mathematically, our work is closest to prior work on curiosity and surprise [8, 19, 23, 24], although these works focus mainly on small finite MDPs.", "startOffset": 93, "endOffset": 108}, {"referenceID": 17, "context": "Conceptually and mathematically, our work is closest to prior work on curiosity and surprise [8, 19, 23, 24], although these works focus mainly on small finite MDPs.", "startOffset": 93, "endOffset": 108}, {"referenceID": 21, "context": "Conceptually and mathematically, our work is closest to prior work on curiosity and surprise [8, 19, 23, 24], although these works focus mainly on small finite MDPs.", "startOffset": 93, "endOffset": 108}, {"referenceID": 22, "context": "Conceptually and mathematically, our work is closest to prior work on curiosity and surprise [8, 19, 23, 24], although these works focus mainly on small finite MDPs.", "startOffset": 93, "endOffset": 108}, {"referenceID": 20, "context": "[22] learn deterministic dynamics models by minimizing Euclidean loss\u2014whereas in our work, we learn stochastic dynamics with cross entropy loss\u2014and use L2 prediction errors for intrinsic motivation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[7] train Bayesian neural networks to approximate posterior distributions over dynamics models given observed data, by maximizing a variational lower bound; they then use second-order approximations of the Bayesian surprise as intrinsic motivation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] derived pseudo-counts from CTS density models over states and used those to form intrinsic rewards, notably resulting in dramatic performance improvement on Montezuma\u2019s Revenge, one of the hardest games in the Atari domain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Mohamed and Rezende [14] developed a scalable method of approximating empowerment, the mutual information between an agent\u2019s actions and the future state of the environment, using variational methods.", "startOffset": 20, "endOffset": 24}, {"referenceID": 14, "context": "[16] estimated state visit frequency using Gaussian kernels to compare against a replay memory, and used these estimates for directed exploration.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as -greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent\u2019s surprise about its experiences via intrinsic motivation. We propose to learn a model of the MDP transition probabilities concurrently with the policy, and to form intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. One of our approximations results in using surprisal as intrinsic motivation, while the other gives the k-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques.", "creator": "LaTeX with hyperref package"}}}