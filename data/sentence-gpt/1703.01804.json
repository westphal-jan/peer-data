{"id": "1703.01804", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Orthogonalized ALS: A Theoretically Principled Tensor Decomposition Algorithm for Practical Use", "abstract": "The popular Alternating Least Squares (ALS) algorithm for tensor decomposition is extremely efficient, but often converges to poor local optima, particularly when the weights of the factors are non-uniform. We propose a modification of the ALS approach that is as efficient as standard ALS, but provably recovers the true factors with random initialization under standard incoherence assumptions on the factors of the tensor. We demonstrate the significant practical superiority of our approach over traditional ALS (with both random initialization and SVD-based initialization) for a variety of tasks on synthetic data - including tensor factorization on exact, noisy and over-complete tensors, as well as tensor completion - and for computing word embeddings from a third-order word tri-occurrence tensor. We also demonstrate that the ALS approach is the most efficient for large datasets in general, with an approximate average of 30% of all datasets in all cases, compared with 32% in all cases. We propose a modification of the ALS approach, that is as efficient as standard ALS, but provably recovers the true factors with random initialization under standard incoherence assumptions on the factors of the tensor.\n\n\n\n\n\n\nWe have already developed a number of tools for generating large datasets. We recommend an introductory tutorial on the following topics:\nFor an example of the best and most efficient way to generate large datasets in general, consider one of our current favorite methods of generating large datasets:\nTo build up a good model, we use a model based on the following techniques:\nOur method, using the following techniques, combines an alternative algorithm to the normal models. The original method uses a randomization method that can be used by either one or both of the original models, using an unsupervised model such as Algorithm 3. This algorithm is also used by multiple algorithms such as C, D, or M. A. A. B. [see also our previous paper], including the algorithm for building a high-level real-time ML analysis using the algorithm for building a model of tensor-specific linear regressions. We then apply an alternative model in the current model that is better suited for large datasets:\nOur algorithm is based on the following methods and uses a randomization algorithm. The original method uses a randomization algorithm that can be used by either one or both of the original models, using an unsupervised model such as Algorithm 3. This algorithm is also used by multiple algorithms such as C, D, or M", "histories": [["v1", "Mon, 6 Mar 2017 10:31:00 GMT  (1958kb,D)", "http://arxiv.org/abs/1703.01804v1", "47 pages, 6 figures"], ["v2", "Sat, 23 Sep 2017 21:15:50 GMT  (1466kb,D)", "http://arxiv.org/abs/1703.01804v2", "Minor updates to presentation. Appears in ICML'17"]], "COMMENTS": "47 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["vatsal sharan", "gregory valiant"], "accepted": true, "id": "1703.01804"}, "pdf": {"name": "1703.01804.pdf", "metadata": {"source": "CRF", "title": "Orthogonalized ALS: A Theoretically Principled Tensor Decomposition Algorithm for Practical Use", "authors": ["Vatsal Sharan"], "emails": ["vsharan@stanford.edu", "valiant@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "From a theoretical perspective, tensor methods have become an incredibly useful and versatile tool for learning a wide array of popular models, including topic modeling (Anandkumar et al., 2012), mixtures of Gaussians (Ge et al., 2015), community detection (Anandkumar et al., 2014a), learning graphical models with guarantees via the method of moments (Anandkumar et al., 2014b; Chaganty and Liang, 2014) and reinforcement learning (Azizzadenesheli et al., 2016). The key property of tensors that enables these applications is that tensors have a unique decomposition (decomposition here refers to the most commonly used CANDECOMP/PARAFAC or CP decomposition), under mild conditions on the factor matrices (Kruskal, 1977); for example, tensors have a unique decomposition whenever the factor matrices are full rank. As tensor methods naturally model three-way (or higher-order) relationships, it is not too optimistic to hope that their practical utility will only increase, with the rise of multi-modal measurements (e.g. measurements taken by \u201cInternet of Things\u201d devices) and the numerous practical applications involving high order dependencies, such as those encountered in natural language processing or genomic settings. In fact, we are already seeing exciting applications of tensor methods for analysis of high-order spatiotemporal data (Yu and Liu, 2016), health data analysis (Wang et al., 2015a) and bioinformatics (Colombo and Vlassis, 2015). Nevertheless, to truly realize the practical impact that the current theory of tensor methods portends, we require better algorithms for computing decompositions\u2014practically efficient algorithms that are both capable of scaling to large (and possibly sparse) tensors, and are robust to noise and deviations from the idealized \u201clow-rank\u201d assumptions.\nAs tensor decomposition is NP-Hard in the worst-case (Hillar and Lim, 2013; H\u030aastad, 1990), one cannot hope for algorithms which always produce the correct factorization. Despite this worstcase impossibility, accurate decompositions can be efficiently computed in many practical settings. Early work from the 1970\u2019s (Leurgans et al., 1993; Harshman, 1970) established a simple algorithm\nar X\niv :1\n70 3.\n01 80\n4v 1\n[ cs\n.L G\n] 6\nM ar\n2 01\nfor computing the tensor decomposition (in the noiseless setting) provided that the factor matrices are full rank. This approach, based on an eigendecomposition, is very sensitive to noise in the tensor (as we also show in our experiments), and does not scale well for large, sparse tensors.\nSince this early work, much progress has been made. Nevertheless, many of the tensor decomposition algorithms hitherto proposed and employed have strong provable success guarantees but are computationally expensive (though still polynomial time)\u2014either requiring an expensive initialization phase, being unable to leverage the sparsity of the input tensor, or not being efficiently parallelizable. On the other hand, there are also approaches which are efficient to implement, but which fail to compute an accurate decomposition in many natural settings. The Alternating Least Squares (ALS) algorithm (either with random initialization or more complicated initializations) falls in this latter category and is, by far, the most widely employed decomposition algorithm despite its often poor performance and propensity for getting stuck in local optima (which we demonstrate on both synthetic data and real NLP data).\nIn this paper we propose an alternative decomposition algorithm, \u201cOrthogonalized Alternating Least Squares\u201d (Orth-ALS) which has strong theoretical guarantees, and seems to significantly outperform the most commonly used existing approaches in practice on both real and synthetic data, for a number of tasks related to tensor decomposition. This algorithm is a simple modification of the ALS algorithm to periodically \u201corthogonalize\u201d the estimates of the factors. Intuitively, this periodic orthogonalization prevents multiple recovered factors from \u201cchasing after\u201d the same true factors, allowing for the avoidance of local optima and more rapid convergence to the true factors.\nFrom the practical side, our algorithm enjoys all the benefits of standard ALS, namely simplicity and computational efficiency/scalability, particularly for very large yet sparse tensors, and noise robustness. Additionally, the speed of convergence and quality of the recovered factors are substantially better than standard ALS, even when ALS is initialized using the more expensive SVD initialization. As we show, on synthetic low-rank tensors, our algorithm consistently recovers the true factors, while standard ALS often falters in local optima and fails both in recovering the true factors and in recovering an accurate low-rank approximation to the original tensor. We also applied Orth-ALS to a large 3-tensor of word co-occurrences to compute \u201cword embeddings\u201d.1 The embedding produced by our Orth-ALS algorithm is significantly better than that produced by standard ALS, as we quantify via a near 30% better performance of the resulting word embeddings across standard NLP datasets that test the ability of the embeddings to answer basic analogy tasks (i.e. \u201cpuppy is to dog as kitten is to ?\u201d) and semantic word-similarity tasks. Together, these results support our optimism that with better decomposition algorithms, tensor methods will become an indispensable, widely-used data analysis tool in the near future.\nBeyond the practical benefits of Orth-ALS, we also consider its theoretical properties. We show that Orth-ALS provably recovers all factors under random initialization for worst-case tensors as long as the tensor satisfies an incoherence property (which translates to the factors of the tensors having small correlation with each other), which is satisfied by random tensors with rank k = o(d0.25) where d is the dimension of the tensor. This requirement that k = o(d0.25) is significantly worse than the best known provable recovery guarantees for polynomial-time algorithms on random tensors\u2014the recent work Ma et al. (2016) succeeds even in the over-complete setting with k = o(d1.5). Nevertheless, our experiments support our belief that this shortcoming is more a property of our analysis than the algorithm itself. Additionally, for many practical settings, particularly natural language tasks, the rank of the recovered tensor is typically significantly sublinear in the dimensionality of the space, and the benefits of an extremely efficient and simple algorithm might\n1Word embeddings are vector representations of words, which can then be used as features for higher-level machine learning. Word embeddings have rapidly become the backbone of many downstream natural language processing tasks (see e.g. (Mikolov et al., 2013b)).\noutweigh limitations on the required rank for provable recovery. Finally, as a consequence of our analysis technique for proving convergence of Orth-ALS, we also improve the known guarantees for another popular tensor decomposition algorithm\u2014 the tensor power method. We show that the tensor power method with random initialization converges to one of the factors with small residual error for rank k = o(d), where d is the dimension. We also show that the convergence rate is quadratic in the dimension. Anandkumar et al. (2014c) had previously shown local convergence of the tensor power method with a linear convergence rate (and also showed global convergence via a SVD-based initialization scheme, obtaining the first guarantees for the tensor power method in non-orthogonal settings). Our new results, particularly global convergence from random initialization, provide some deeper insights into the behavior of this popular algorithm.\nThe rest of the paper is organized as follows\u2013 in Section 2 we discuss related work, describe the ALS algorithm and tensor power method, and discuss the shortcomings of both algorithms, particularly for tensors with non-uniform factor weights. Section 3 states the notation. Section 4 introduces and motivates Orth-ALS, and states the convergence guarantees. We state our convergence results for the tensor power method in Section 4.2. The experimental results, on both synthetic data and the NLP tasks are discussed in Section 5. In Section 6 we illustrate our proof techniques for the special case of orthogonal tensors. Proof details have been deferred to the Appendix."}, {"heading": "2 Background and Related Work", "text": "We begin the section with a brief discussion of related work on tensor decomposition. We then review the ALS algorithm and the tensor power method and discuss their basic properties. Our proposed tensor decomposition algorithm, Orth-ALS, builds on these algorithms."}, {"heading": "2.1 Related Work on Tensor Decomposition", "text": "Though it is not possible for us to do justice to the substantial body of work on tensor decomposition, we will review three families of algorithms which are distinct from alternating minimization approaches such as ALS and the tensor power method. Many algorithms have been proposed for guaranteed decomposition of orthogonal tensors, we refer the reader to Anandkumar et al. (2014b); Kolda and Mayo (2011); Comon et al. (2009); Zhang and Golub (2001). However, obtaining guaranteed recovery of non-orthogonal tensors using algorithms for orthogonal tensors requires converting the tensor into an orthogonal form (known as whitening) which is ill conditioned in high dimensions (Le et al., 2011; Souloumiac, 2009), and is computationally the most expensive step (Huang et al., 2013). Another very interesting line of work on tensor decompositions is to use simultaneous diagonalization and higher order SVD (Colombo and Vlassis, 2016; Kuleshov et al., 2015; De Lathauwer, 2006) but the algorithms typically have global convergence guarantees only for orthogonal tensors, and are not as computationally efficient as alternating minimization2. Recently, there has been intriguing work on provably decomposing random tensors using the sum-of-squares approach (Ma et al., 2016; Hopkins et al., 2016; Tang and Shah, 2015; Ge and Ma, 2015). Ma et al. (2016) show that a sum-of-squares based relaxation can decompose highly overcomplete random tensors of rank\n2De Lathauwer (2006) prove unique recovery under very general conditions, but their algorithm is quite complex and requires solving a linear system of size O(d4), which is prohibitive for large tensors. We ran the simultaneous diagonalization algorithm of Kuleshov et al. (2015) on a dimension 100, rank 30 tensor; and the algorithm needed around 30 minutes to run, whereas Orth-ALS converges in less than 5 seconds.\nup to o(d1.5). Though these results establish the polynomial learnability of the problem, they are unfortunately not practical.\nVery recently, there has been exciting work on scalable tensor decomposition algorithms using ideas such as sketching (Song et al., 2016; Wang et al., 2015b) and contraction of tensor problems to matrix problems (Shah et al., 2015). Also worth noting are recent approaches to speedup ALS via sampling and randomized least squares (Battaglino et al., 2017; Cheng et al., 2016; Papalexakis et al., 2012)."}, {"heading": "2.2 Alternating Least Squares (ALS)", "text": "ALS is the most widely used algorithm for tensor decomposition and has been described as the \u201cworkhorse\u201d for tensor decomposition (Kolda and Bader, 2009). The algorithm is conceptually very simple: fixing two of the modes, the optimization problem of finding the value of the third mode that minimizes the squared error of the resulting tensor can be expressed as a linear least-squares regression problem (and hence can be efficiently solved). As its name suggests, ALS iteratively fixes two of the three modes, and solves the least squares problem on the remaining mode. These updates continue until some stopping condition is satisfied\u2014typically when the squared error of the approximation is no longer decreasing, or when a fixed number of iterations have elapsed. The factors used in ALS are either chosen uniformly at random, or via a more expensive initialization scheme such as SVD based initialization (Anandkumar et al., 2014c). In the SVD based scheme, the factors are initialized to be the singular vectors of a random projection of the tensor onto a matrix.\nThe main advantages of the ALS approach, which have led to its widespread use in practice are its conceptual simplicity, noise robustness and computational efficiency given its graceful handling of sparse tensors and ease of parallelization. There are several publicly available optimized packages implementing ALS, such as (Kossaifi et al., 2016; Vervliet et al.; Bader et al., 2012; Bader and Kolda, 2007; Smith and Karypis; Huang et al., 2014; Kang et al., 2012).\nDespite the advantages, ALS does not have any global convergence guarantees and can get stuck in local optima (Comon et al., 2009; Kolda and Bader, 2009), even under very realistic settings. For example, consider a setting where the weights wi for the factors {Ai, Bi, Ci} decay according to a power-law, hence the first few factors have much larger weight than the others. As we show in the experiments (see Fig. 2), ALS fails to recover the low-weight factors. Intuitively, this is because multiple recovered factors will be chasing after the same high weight factor, leading to a bad local optima."}, {"heading": "2.3 Tensor Power Method", "text": "The tensor power method is a special case of ALS that only computes a rank-1 approximation. The procedure is then repeated multiple times to recover different factors. The factors recovered in different iterations of the algorithm are then clustered to determine the set of unique factors. Different initialization strategies have been proposed for the tensor power method. Anandkumar et al. (2014c) showed that the tensor power method converges locally (i.e. for a suitably chosen initialization) for random tensors with rank o(d1.5). They also showed that a SVD based initialization strategy gives good starting points and used this to prove global convergence for random tensors with rank O(d). However, the SVD based initialization strategy can be computationally expensive, and our experiments suggest that even SVD initialization fails in the setting where the weights decay according to a power-law (see Fig. 2).\nIn this work, we prove global convergence guarantees with random initializations for the tensor\npower method for random and worst-case incoherent tensors. Our results also demonstrate how, with random initialization, the tensor power method converges to the factor having the largest product of weight times the correlation of the factor with the random initialization vector. This explains the difficulty of using random initialization to recover factors with small weight. For example, if one factor has weight less than a 1/c fraction of the weight of, say, the heaviest k/2 factors, then with high probability we would require at least k\u0398(c\n2) random initializations to recover this factor. This is because the correlation between random vectors in high dimensions is approximately distributed as a Normal random variable and if k/2 + 1 samples are drawn from the standard Normal distribution, the probability that one particular sample is at least a factor of c larger than the other k/2 other samples scales as roughly k\u2212\u0398(c 2)."}, {"heading": "3 Notation", "text": "We state our algorithm and results for 3rd order tensors, and believe the algorithm and analysis techniques should extend easily to higher dimensions. Given a 3rd order tensor T \u2208 Rd\u00d7d\u00d7d our task is to decompose the tensor into its factor matrices A,B and C: T = \u2211 i\u2208[k]wiAi \u2297 Bi \u2297 Ci, where Ai denotes the ith column of a matrix A. Here wi \u2208 R, Ai, Bi, Ci \u2208 Rd and \u2297 denotes the tensor product: if a, b, c \u2208 Rd then a\u2297 b\u2297 c \u2208 Rd\u00d7d\u00d7d and (a\u2297 b\u2297 c)ijk = aibjck. We will refer to wi as the weight of the factor {Ai, Bi, Ci}. This is also known as CP decomposition. We refer to the dimension of the tensor by d and denote its rank by k. We refer to different dimensions of a tensor as the modes of the tensor.\nWe denote T(n) as the mode n matricization of the tensor, which is the flattening of the tensor along the nth direction obtained by stacking all the matrix slices together. For example T(1) denotes flattening of a tensor T \u2208 Rn\u00d7m\u00d7p to a (n \u00d7 mp) matrix. We denote the Khatri-Rao product of two matrices A and B as (A B)i = (Ai \u2297 Bi)(1), where (Ai \u2297 Bi)(1) denotes the flattening of the matrix Ai \u2297 Bi into a row vector. For any tensor T and vectors a, b, c, we also define T (a, b, c) = \u2211 i,j,k Tijkaibjck. Throughout, we say f(n) = O\u0303(g(n)) if f(n) = O(g(n)) up to poly-logarithmic factors. Though all algorithms in the paper extend to asymmetric tensors, we prove convergence results under the symmetric setting where A = B = C. Similar to other works (Tang and Shah, 2015; Anandkumar et al., 2014c), our guarantees for tensor decomposition depend on the incoherence of the factor matrices (cmax), defined to be the maximum correlation in absolute value between any two factors, i.e. cmax = maxi 6=j |ATi Aj |."}, {"heading": "4 The Algorithm: Orthogonalized Alternating Least Squares", "text": "In this section we introduce Orth-ALS, which combines the computational benefits of standard ALS and the provable recovery of the tensor power method, while avoiding the difficulties faced by both when the factors have different weights. Orth-ALS is a simple modification of standard ALS that adds an orthogonalization step before each set of ALS steps. For completeness, we describe the algorithm in Algorithm 1.\nTo get some intuition for why the orthogonalization makes sense, let us consider the more intuitive matrix factorization problem, where the goal is to compute the eigenvectors of a matrix. Subspace iteration is a straightforward extension of the matrix power method to recover all eigenvectors at once. In subspace iteration, the matrix of eigenvector estimates is orthogonalized before each power method step (by projecting the second eigenvector estimate orthogonal to the first one and so on), because otherwise all the vectors would converge to the dominant eigenvector. For\nAlgorithm 1 Orthogonalized ALS (Orth-ALS) for tensor decomposition\nInput: Tensor T \u2208 Rd\u00d7d\u00d7d, number of iterations N .\n1: Initialize each column of A\u0302, B\u0302 and C\u0302 \u2208 Rd\u00d7k uniformly from the unit sphere 2: for t = 1 : N do 3: Find QR decomposition of A\u0302, set A\u0302 = Q. Orthogonalize B\u0302 and C\u0302 analogously. 4: X = T(1)(C\u0302 B\u0302) 5: Y = T(1)(C\u0302 A\u0302) 6: Z = T(1)(B\u0302 A\u0302) 7: Normalize X,Y, Z and store results in A\u0302, B\u0302, C\u0302 8: end for 9: Estimate weights w\u0302i = T (A\u0302i, B\u0302i, C\u0302i),\u2200 i \u2208 [k].\n10: return A\u0302, B\u0302, C\u0302, w\u0302\nthe case of tensors, the vectors would not all necessarily converge to the dominant factor if the initialization is good, but with high probability a random initialization would drive many factors towards the larger weight factors. The orthogonalization step is a natural modification which forces the estimates to converge to different factors, even if some factors are much larger than the others. It is worth stressing that the orthogonalization step does not force the final recovered factors to be orthogonal (because the ALS step follows the orthogonalization step) and in general the factors output will not be orthogonal (which is essential for accurately recovering the factors).\nFrom a computational perspective, adding the orthogonalization step does not add to the computational cost as the least squares updates in step 4-6 of Algorithm 1 involve an extra pseudoinverse term for standard ALS, which evaluates to identity for Orth-ALS and does not have to be computed. The cost of orthogonalization is O(k2d), while the cost of computing the pseudoinverse is also O(k2d).\nWe also observe significant speedups in terms of the number of iterations required for convergence for Orth-ALS as compared to standard ALS in our simulations on random tensors (see the experiments in Section 5). Several other modifications to the simple orthogonalization step also seem natural. Particularly for low-dimensional settings, in practice we found that it is useful to carry out orthogonalization for a few steps and then continue with standard ALS updates until convergence (we call this variant Hybrid-ALS ). Hybrid-ALS also gracefully reverts to standard ALS in settings where the factors are highly correlated and orthogonalization is not helpful."}, {"heading": "4.1 Performance Guarantees", "text": "We now state the formal guarantees on the performance of Orthogonalized ALS. The specific variant of Orthogonalized ALS that our theorems apply to is a slight modification of Algorithm 1, and differs in that there is a periodic (every log k steps) re-randomization of the factors for which our analysis has not yet guaranteed convergence. In our practical implementations, we observe that all factors seem to converge within this first log k steps, and hence the subsequent re-randomization is unnecessary.\nTheorem 1. Consider a d-dimensional rank k tensor T = \u2211k\ni=1wiAi \u2297 Ai \u2297 Ai. Let cmax = maxi 6=j |ATi Aj | be the incoherence between the true factors and \u03b3 = wmaxwmin be the ratio of the largest and smallest weight. Assume \u03b3cmax \u2264 o(k\u22122), and the estimates of the factors are ini-\ntialized randomly from the unit sphere. Provided that, at the i(log k + log log d)th step of the algorithm the estimates for all but the first i factors are re-randomized, then with high probability the orthogonalized ALS updates converge to the true factors in O(k(log k + log log d)) steps, and the error at convergence satisfies (up to relabelling) \u2016 Ai \u2212 A\u0302i \u20162 \u2264 O(\u03b3kmax{c2max, 1/d2}) and |1\u2212 w\u0302iwi | \u2264 O(max{cmax, 1/d}), for all i.\nTheorem 1 immediately gives convergence guarantees for random low rank tensors. For random d dimensional tensors, cmax = O(1/ \u221a d); therefore Orth-ALS converges globally with random initialization whenever k = o(d0.25). If the tensor has rank much smaller than the dimension, then our analysis can tolerate significantly higher correlation between the factors. In Section 6, we prove Theorem 1 for the special and easy case of orthogonal tensors, which nevertheless highlights the key proof ideas."}, {"heading": "4.2 New Guarantees for the Tensor Power Method", "text": "As a consequence of our analysis of the orthogonalized ALS algorithm, we also prove new guarantees on the tensor power method. As these may be of independent interest because of the wide use of the tensor power method, we summarize them in this subsection. We show a quadratic rate of convergence (in O(log log d) steps) with random initialization for random tensors having rank k = o(d). This contrasts with the analysis of Anandkumar et al. (2014c) who showed a linear rate of convergence (O(log d) steps) for random tensors, provided an SVD based initialization is employed.\nTheorem 2. Consider a d-dimensional rank k tensor T = \u2211k\ni=1wiAi \u2297 Ai \u2297 Ai with the factors Ai sampled uniformly from the d-dimensional sphere. Define \u03b3 =\nwmax wmin to be the ratio of the largest\nand smallest weight. Assume k \u2264 o(d) and \u03b3 \u2264 polylog(d). If the initialization x0 \u2208 Rd is chosen uniformly from the unit sphere, then with high probability the tensor power method updates converge to one of the true factors (say A1) in O(log log d) steps, and the error at convergence satisfies \u2016 A1 \u2212 A\u03021 \u20162 \u2264 O\u0303(1/ \u221a d). Also, the estimate of the weight w\u03021 satisfies |1\u2212 w\u03021w1 | \u2264 O\u0303(1/ \u221a d).\nTheorem 2 provides guarantees for random tensors, but it is natural to ask if there are deterministic conditions on the tensors which guarantee global convergence of the tensor power method. Our analysis also allows us to obtain a clean characterization for global convergence of the tensor power method updates for worst-case tensors in terms of the incoherence of the factor matrix \u2013\nTheorem 3. Consider a d-dimensional rank k tensor T = \u2211k\ni=1wiAi \u2297 Ai \u2297 Ai. Let cmax = maxi 6=j |ATi Aj | and \u03b3 = wmaxwmin be the ratio of the largest and smallest weight, and assume \u03b3cmax \u2264 o(k\u22122). If the initialization x0 \u2208 Rd is chosen uniformly from the unit sphere, then with high probability the tensor power method updates converge to one of the true factors (say A1) in O(log k+ log log d) steps, and the error at convergence satisfies \u2016 A1 \u2212 A\u03021 \u20162 \u2264 O(\u03b3kmax{c2max, 1/d2}) and |1\u2212 w\u03021w1 | \u2264 O(max{cmax, 1/d})."}, {"heading": "5 Experiments", "text": "We compare the performance of Orth-ALS, standard ALS (with random and SVD initialization), the tensor power method, and the classical eigendecomposition approach, through experiments on low rank tensor recovery in a few different parameter regimes, on an overcomplete tensor decomposition task and a tensor completion task. We also compare the factorization of Orth-ALS and standard\nALS on a large real-world tensor of word tri-occurrence based on the 1.5 billion word English Wikipedia corpus. 3"}, {"heading": "5.1 Experiments on Random Tensors", "text": "Recovering low rank tensors: We explore the abilities of Orth-ALS, standard ALS, and the tensor power method (TPM), to recover a low rank (rank k) tensor that has been constructed by independently drawing each of the k factors independently and uniformly at random from the d dimensional unit spherical shell. We consider several different combinations of the dimension, d, and rank, k. We also consider both the setting where all of the factors are equally weighted, as well as the practically relevant setting where the factor weights have a geometric spacing, and consider the setting where independent Gaussian noise has been added to the low-rank tensor.\nIn addition to random initialization for standard ALS and the TPM, we also explore SVD based initialization (Anandkumar et al., 2014c) where the factors are initialized via SVD of a projection of the tensor onto a matrix. We also test the classical technique for tensor decomposition via simultaneous diagonalization (Leurgans et al., 1993; Harshman, 1970) (also known as Jennrich\u2019s algorithm, we refer to it as Sim-Diag), which first performs two random projections of the tensor, and then recovers the factors by an eigenvalue decomposition of the projected matrices. This gives guaranteed recovery when the tensors are noiseless and factors are linearly independent, but is extremely unstable to perturbations.\nWe evaluate the performance in two respects: 1) the ability of the algorithms to recover a lowrank tensor that is close to the input tensor, and 2) the ability of the algorithms to recover accurate approximations of many of the true factors. Fig. 1 depicts the performance via the first metric. We evaluate the performance in terms of the discrepancy between the input low-rank tensor, and the low-rank tensor recovered by the algorithms, quantified via the ratio of the Frobenius norm of the residual, to the Frobenius norm of the actual tensor: \u2016T\u2212T\u0302\u2016F\u2016T\u2016F , where T\u0302 is the recovered tensor. Since the true tensor has rank k, the inability of an algorithm to drive this error to zero indicates the presence of local optima. Fig. 1 depicts the performance of Orth-ALS, standard ALS with random initialization and the hybrid algorithm that performs Orth-ALS for the first five iterations before reverting to standard ALS (Hybrid-ALS). Tests are conducted in both the setting where factor weights are uniform, as well as a geometric spacing, where the ratio of the largest factor weight to the smallest is 100. Fig. 1 shows that Hybrid ALS and Orth-ALS have much faster convergence and find a significantly better fit than standard ALS.\nFig. 2 quantifies the performance of the algorithms in terms of the number of the original factors that the algorithms accurately recover. We use standard ALS, Orth-ALS (Algorithm 1), Hybrid-ALS, TPM with random initialization (TPM), ALS with SVD initialization (ALS-SVD), TPM with SVD initialization (TPM-SVD) and the simultaneous diagonalization approach (SimDiag). We run TPM and SVD-TPM with 100 different initializations and find a rank k = 30 decomposition for ALS, ALS-SVD, Orth-ALS, Hybrid-ALS and Sim-Diag. We repeat the experiment (by sampling a new tensor) 10 times. We perform this evaluation in both the setting where we receive an actual low-rank tensor as input, as well as the setting where each entry Tijk of the low-rank tensor has been perturbed by independent Gaussian noise of standard deviation equal to 0.05Tijk. We can see that Orth-ALS and Hybrid-ALS perform significantly better than the other algorithms and are able to recover all factors in the noiseless case even when the weights are highly skewed. Note that the reason the Hybrid-ALS and Orth-ALS fail to recover all factors in the noisy\n3MATLAB, Python and C code for Orth-ALS and Hybrid-ALS is available at http://web.stanford.edu/\n~vsharan/orth-als.html\ncase when the weights are highly skewed is that the magnitude of the noise essentially swamps the contribution from the smallest weight factors.\nRecovering over-complete tensors: Overcomplete tensors are tensors with rank higher than the dimension, and have found numerous theoretical applications in learning latent variable models (Anandkumar et al., 2015). Even though orthogonalization cannot be directly applied to the setting where the rank is more than the dimension (as the factors can no longer be orthogonalized), we explore a deflation based approach to decomposing tensors. Given a tensor T with dimension d = 50 and rank r > d, we find a rank d decomposition T1 of T , subtract T1 from T , and repeat the process till we have computed a rank r decomposition. Fig. 3a plots the number of factors when recovered when this deflation based approach is applied to a dimension d = 50 tensor with a mild power low distribution on weights. We can see that Hybrid-ALS is successful at recovering tensors even in the overcomplete setup, and gives an improvement over ALS.\nTensor completion: We also test the utility of orthogonalization on a tensor completion task, where the goal is to recover a large missing fraction of the entries. Each entry is sampled with a sampling probability p, hence the expected number of missing entries is (1\u2212 p)d3. Fig. 3b suggests Hybrid-ALS gives considerable improvements over standard ALS. Further examining the utility of orthogonalization in this important setting, in theory and practice, would be an interesting direction."}, {"heading": "5.2 Learning Word Embeddings via Tensor Factorization", "text": "A word embedding is a vector representation of words which preserves some of the syntactic and semantic relationships in the language. Current methods for learning word embeddings implicitly (Mikolov et al., 2013b; Levy and Goldberg, 2014) or explicitly (Pennington et al., 2014) factorize some matrix derived from the matrix of word co-occurrences M , where Mij denotes how often word i appears with word j. We explore tensor methods for learning word embeddings, and contrast the performance of standard ALS and Orthogonalized ALS on tasks which test the quality of the embeddings."}, {"heading": "5.2.1 Methodology", "text": "We used the English Wikipedia as our corpus, with 1.5 billion words. We constructed a word co-occurrence tensor T of the 10,000 most frequent words, where the entry Tijk denotes the number of times the words i, j and k appear in a sliding window of length w across the corpus. We consider two different window lengths, w = 3 and w = 5. Before factoring the tensor, we apply the non-linear element-wise scaling f(x) = log(1 +x) to the tensor of tri-occurrence counts. This scaling is known to perform well in practice for co-occurrence matrices (Pennington et al., 2014), and makes some intuitive sense in light of the Zipfian distribution of word frequencies. Following the application of this element-wise nonlinearity, we recover a rank 100 approximation of the tensor using Orth-ALS or ALS.\nWe concatenate the (three) recovered factor matrices into one matrix and normalize the rows. The ith row of this matrix is then the embedding for the ith word. We test the quality of these embeddings on two tasks aimed at measuring the syntactic and semantic structure captured by these word embeddings.\nWe also evaluated the performance of matrix SVD based methods on the task. For this, we built the co-occurrence matrix M with a sliding window of length w over the corpus. We applied the same non-linear element-wise scaling and performed a rank 100 SVD, and set the word embeddings to be the singular vectors after row normalization.\nIt is worth highlighting some implementation details for our experiments, as they indicate the practical efficiency and scalability inherited by Orth-ALS from standard ALS. Our experiments\nwere run on a cluster with 8 cores and 48 GB of RAM memory per core. Most of the runtime was spent in reading the tensor, the runtime for Orth-ALS was around 80 minutes, with 60 minutes spent in reading the tensor (the runtime for standard ALS was around 100 minutes because it took longer to converge). Since storing a dense representation of the 10,000\u00d710,000\u00d710,000 tensor is too expensive, we use an optimized ALS solver for sparse tensors (Smith and Karypis; 2015) which also has an efficient parallel implementation."}, {"heading": "5.3 Evaluation: Similarity and Analogy Tasks", "text": "We evaluated the quality of the recovered word embeddings produced by the various methods via their performance on two different NLP tasks for which standard, human-labeled data exists: estimating the similarity between a pair of words, and completing word analogies.\nThe word similarity tasks (Bruni et al., 2012; Finkelstein et al., 2001) contain word pairs along with human assigned similarity scores, and the objective is to maximize the correlation between the similarity in the embeddings of the two words (according to a similarity metric such as the dot product) and human judged similarity.\nThe word analogy tasks (Mikolov et al., 2013a;c) present questions of the form \u201ca is to a\u2217 as b is to ?\u201d (e.g. \u201cParis is to France as Rome is to ?\u201d). We find the answer to \u201ca is to a\u2217 as b is to b\u2217\u201d by finding the word whose embedding is the closest to wa\u2217 \u2212wa +wb in cosine similarity, where wa denotes the embedding of the word a. The performances are summarized in the Table 1. WordSim and MEN are the word similarity tasks, and the syntactic and mixed analogies are the word analogy tasks.\nThe use of Orth-ALS rather than standard ALS leads to significant improvement in the quality of the embeddings as judged by the similarity and analogy tasks. However, the matrix SVD method still outperforms the tensor based methods. We believe that it is possible that better tensor based approaches (e.g. using better renormalization, additional data, or some other tensor rather than the symmetric tri-occurrence tensor) or a combination of tensor and matrix based methods can actually improve the quality of word embeddings, and is an interesting research direction. Alternatively, it is possible that natural language does not contain sufficiently rich higher-order dependencies among words that appear close together, beyond the co-occurrence structure, to truly leverage the power of tensor methods. Or, perhaps, the two tasks we evaluated on\u2014similarity and analogy tasks\u2014do not require this higher order. In any case, investigating these possibilities seems worthwhile."}, {"heading": "6 Proof Overview: the Orthogonal Tensor Case", "text": "In this section, we will analyze Orthogonalized ALS for the special case when the factors matrix of the tensor is an orthogonal matrix. Although this special case is easy and numerous algorithms provably work in this setting, it will serve to highlight the high level analysis approach that we apply to the more general settings.\nThe analysis of Orth-ALS hinges on an analysis of the tensor power method. For completeness we describe the tensor power method in Algorithm 2. We will first go through some preliminaries for our analysis of the tensor power method. Let the iterate of the tensor power method at time t be Zt. The tensor power method update equations can be written as (refer to Anandkumar et al. (2014c))\nZt =\n\u2211k i=1wi\u3008Zt\u22121, Ai\u30092Ai\n\u2016 \u2211k i=1wi\u3008Zt\u22121, Ai\u30092Ai \u20162 (6.1)\nEq. 6.1 is just the tensor analog of the matrix power method updates. For tensors, the updates are quadratic in the previous inner products, in contrast to matrices where the updates are linear in the inner products in the previous step.\nAlgorithm 2 Tensor power method to recover all factors (Anandkumar et al., 2014c)\nInput: Tensor T \u2208 Rd\u00d7d\u00d7d, number of initializations L, number of iterations N .\n1: for \u03c4 = 1 : L do 2: Initialize x\n(\u03c4) 0 , y (\u03c4) 0 , z (\u03c4) 0 \u2208 Rd uniformly from the unit sphere or using the SVD based method\n3: for t = 1 : N do 4: Rank-1 ALS/Power method updates-x\n(\u03c4) t+1 = T(1)(z\u0302 (\u03c4) t y\u0302 (\u03c4) t )\n5: Rank-1 ALS/Power method updates-y (\u03c4) t+1 = T(2)(z\u0302 (\u03c4) t x\u0302 (\u03c4) t ) 6: Rank-1 ALS/Power method updates-z (\u03c4) t+1 = T(3)(y\u0302 (\u03c4) t x\u0302 (\u03c4) t ) 7: Normalize x\u03c4t+1, y \u03c4 t+1, z \u03c4 t+1 and store results in x\u0302 \u03c4 t+1, y\u0302 \u03c4 t+1, z\u0302 \u03c4 t+1. 8: end for 9: Estimate weights-w\u0302(\u03c4) = T (x\u0302\n(\u03c4) N , y\u0302 (\u03c4) N , z\u0302 (\u03c4) N )\n10: end for 11: Cluster set {(w\u0302(\u03c4), x\u0302(\u03c4)N , y\u0302 (\u03c4) N , z\u0302 (\u03c4) N ), \u03c4 \u2208 [L]} into k clusters. 12: return the centers {(w\u0302i, a\u0302i, b\u0302i, c\u0302i), i \u2208 [k]} of the k clusters as the estimates\nObserve from Algorithm 1 (Orth-ALS) that the ALS steps in step 4-6 have the same form as tensor power method updates, but on the orthogonalized factors. This is the key idea we use in our analysis of Orth-ALS. Note that the first factor estimate is never affected by the orthogonalization, hence the updates for the first estimated factor have exactly the same form as the tensor power method updates. The subsequent factors have an orthogonalization step before every tensor power method step. This ensures that they never have high correlation with the factors which have already been recovered, as they are projected orthogonal to the recovered factors before each ALS step. We then use the incoherence of the factors to argue that orthogonalization does not significantly affect the updates of the factors which have not been recovered so far, while ensuring that the factors which have already been recovered always have a small correlation.\nNote that Eq. 6.1 is invariant with respect to multiplying the weights of all the factors by some constant. Hence for ease of exposition, we assume that all the weights lie in the interval [1, \u03b3], where \u03b3 = wmaxwmin . We also define \u03b7 = max{cmax, 1/d}. Proposition 1 is a restatement of Theorem 1 for the case of orthogonal tensors.\nProposition 1. Consider a d-dimensional rank k tensor T = \u2211k\ni=1wiAi\u2297Ai\u2297Ai where the factor matrix A is orthogonal. Define \u03b3 = wmaxwmin to be the ratio of the largest and smallest weight. If the initial estimates for all the factors are initialized randomly from the unit sphere and the factors {Aj , j \u2265 i+1} are re-randomized after i(log k+log log d) steps where i is an integer, then with high probability the orthogonalized ALS updates converge to the true factors in O(k(log k + log log d)) steps, and the error at convergence satisfies \u2016 Ai \u2212 A\u0302i \u20162 \u2264 O(\u03b3k/d2) and |1\u2212 w\u0302iwi | \u2264 O(1/d) for all i.\nProof. Without loss of generality, we assume that the ith estimated factor converges to the ith true factor. As mentioned earlier, the iterations for the first factor are the usual tensor power method updates and are unaffected by the remaining factors. Therefore to show that orthogonalized ALS recovers the first factor, we only need to analyze the tensor method updates. We show that the\ntensor power method with random initialization converges in O(log k+ log log d) steps with failure probability at most O\u0303(1/k1+ ), for some > 0. Hence this implies that Orth-ALS correctly recovers the first factor in O(log k + log log d) steps with failure probability at most O\u0303(1/k1+ ), for some > 0.\nThe main idea of our proof of convergence of the tensor power method is the following \u2013 with decent probability, there is some separation between the correlations of the factors with the random initialization. By the tensor power method updates (Eq. 6.1), this gap is amplified at every stage. We analyze the updates for all the factors together by a simple recursion. We then show that this recursion converges in in O(log k + log log d) steps.\nLet Zt be the iterate of the tensor power method updates at time t. Without loss of generality, we will be proving convergence to the first factor A1. Let ai,t be the correlation of the ith factor Ai with Zt, i.e. ai,t = \u3008Ai, Zt\u3009 (note that this should technically be called the inner product, but we refer to it as the correlation). We will refer to wiai,t as the weighted correlation of the ith factor.\nThe first step of the proof is that with decent probability, there is some separation between the weighted correlation of the factors with the initial random estimate. This is Lemma 1.\nLemma 1. If \u03b3kcmax \u2264 1/k1+ for some > 0, then with probability at least ( 1 \u2212 log 5 k\nk1+\n) ,\n|wiai,0| maxi |wiai,0| \u2264 1\u2212 5/k 1+ \u2200 i 6= arg maxi |wiai,0|.\nThe proof of Lemma 1 is a bit technical, but relies on basic concentration inequalities for Gaussians. Then using Eq. 6.1 the correlation at the end of the (t+ 1)th time step is given by\nai,t+1 = wia 2 i,t/\u03bat\nwhere \u03bat = \u2016 \u2211k\ni=1wi\u3008Zt\u22121, Ai\u30092 \u20162 is the normalizing factor at the tth time step. Because the estimate is normalized at the end of the updates, we only care about the ratio of the correlations of the factors with the estimate rather than the magnitude of the correlations themselves. Hence, it is convenient to normalize all the correlations by the correlation of the largest factor and normalize all the weights by the weight of the largest factor. Therefore, let a\u0302i,t =\nai,t a1,t\nand w\u0302i = wi w1 . The new update equation for the ratio of correlations a\u0302i,t is \u2013\na\u0302i,t+1 = w\u0302ia\u0302 2 i,t (6.2)\nOur goal is to show that ai,t becomes small for all i 6= 1 in O(log k + log log d) steps. Instead of separately analyzing the different ai,t for different factors Ai, we upper bound ai,t for all i via a simple recursion. Consider the recursion,\n\u03b20 = max i 6=1 \u2223\u2223\u2223w\u0302ia\u0302i,0\u2223\u2223\u2223 (6.3) \u03b2t+1 = \u03b2 2 t (6.4)\nWe claim that |w\u0302ia\u0302i,t| \u2264 \u03b2t for all t and i 6= 1. By Eq. 6.3, this is true for t = 0 by definition. We prove our claim via induction. Assume that |w\u0302ia\u0302i,t| \u2264 \u03b2t for t = p. Note that by Eq. 6.2, w\u0302ia\u0302i,p+1 = w\u0302 2 i a\u0302 2 i,p. Therefore wia\u0302i,p+1 \u2264 \u03b22p+1 for all i 6= 1. This proves the induction argument, hence |w\u0302ia\u0302i,t| \u2264 \u03b2t for all t and i 6= 1. Note that as the weights lie in the interval [1, \u03b3], a\u0302i,t \u2264 \u03b2t. To show convergence, we will now analyze the recursion in Eq. 6.4. We will show that \u03b2t becomes sufficiently small in O(log k + log log d) steps. Note that \u03b2t = (\u03b20) 2t and \u03b20 \u2264 1\u2212 5/k1+ by Lemma 1. Therefore \u03b2t \u2264 0.1 for t = 2 log k. In another log log d steps, \u03b2t \u2264 1/d. Hence \u03b2t \u2264 1/d in O(log k+ log log d) steps. As \u03b2t is an upper bound for ratio of correlations of all factors with the first factor, hence |a\u0302i,t| \u2264 1/d for all i 6= 1 in O(log k + log log d) steps.\nTo finish the proof of convergence for the tensor power method, we need to show that the estimate Zt is close to A1 if it has small correlation with every factor other than A1. Lemma 2 shows that if the ratio of the correlation of every other factor with A1 is small, then the residual `2 error in estimating A1 is also small.\nLemma 2. Let \u03b3kcmax \u2264 1/k1+ . Without loss of generality assume convergence to the first factor A1. Define a\u0302i,t = | ai,ta1,t |- the ratio of the correlation of the ith and 1st factor with the iterate at time t. If a\u0302i,t \u2264 2\u03b7 \u2200 i 6= 1, then \u2016 A1 \u2212 A\u03021 \u20162 \u2264 10\u03b3k\u03b72 in the subsequent iteration. Also, if \u2016 A1 \u2212 A\u03021 \u20162 \u2264 O(\u03b7) then the relative error in the estimation of the weight w1 is at most O(\u03b7).\nUsing Lemma 2, it follows that the estimate A\u03021 and w\u03021 for the factor A1 satisfies \u2016 A1\u2212 A\u03021 \u201622\u2264 10\u03b3k/d2 and |1 \u2212 w\u03021w1 | \u2264 O(1/d). Hence we have shown that Orth-ALS correctly recovers the first factor.\nWe now prove that Orth-ALS also recovers the remaining factors. The proof proceeds by induction. We have already shown that the base case is correct and the algorithm recovers the first factor. We next show that if the first (m \u2212 1) factors have converged, then the mth factor converges in O(log k+ log log d) steps with failure probability at most O\u0303(1/k1+ ). Hence Orth-ALS successfully recovers all factors in O(k(log k + log log d)) steps with high probability. The main idea is that as the factors have small correlation with each other, hence orthogonalization does not affect the factors which have not been recovered but ensures that the mth estimate never has high correlation with the factors which have already been recovered. Recall that we assume without loss of generality that the ith recovered factor Xi converges to the ith true factor, hence Xi = Ai + \u2206\u0302i for i < m, where \u2016 \u2206\u0302i \u20162 \u2264 10\u03b3k/d2. This is our induction hypothesis, which is true for the base case as we just showed that the tensor power method updates converge with residual error at most 10\u03b3k/d2.\nLet Xm,t denote the mth factor estimate at time t and let Xm denote it\u2019s value at convergence. We will first calculate the effect of the orthogonalization step on the correlation between the factors and the estimate Xm,t. Let {X\u0304i, i < m} denote an orthogonal basis for {Xi, i < m}. The basis {X\u0304i, i < m} is calculated via QR decomposition, and can be recursively written down as follows,\nX\u0304i = Xi \u2212\n\u2211 j<i X\u0304 T j XiX\u0304j\n\u2016 Xi \u2212 \u2211 j<i X\u0304 T j XiX\u0304j \u20162\n(6.5)\nNote that the estimate Xm,t is projected orthogonal to this basis. Define X\u0304m,t as this orthogonal projection, which can be written down as follows \u2013\nX\u0304m,t = X\u0304m,t \u2212 \u2211 j<m X\u0304Tj Xm,tX\u0304j\nIn the QR decomposition algorithm X\u0304m,t is also normalized to have unit norm but we will ignore the normalization of Xm,t in our analysis because as before we only consider ratios of correlations of the true factors with X\u0304m,t, which is unaffected by normalization.\nWe will now analyze the orthogonal basis {X\u0304i, i < m}. The key idea is that the orthogonal basis {X\u0304i, i < m} is close to the original factors {Ai, i < m} as the factors are incoherent. Lemma 3 proves this claim.\nLemma 3. Consider a stage of the Orthogonalized ALS iterations when the first (m \u2212 1) factors have converged. Without loss of generality let Xi = Ai + \u2206\u0302i, i < m, where\u2016 \u2206\u0302i \u20162 \u2264 10\u03b3k\u03b72. Let {X\u0304i, i < m} denote an orthogonal basis for {Xi, i < m} calculated using Eq. 6.5. Then,\n1. X\u0304i = Ai + \u2206i, \u2200 i < m and \u2016 \u2206j \u20162 \u2264 10k\u03b7.\n2. |ATj \u2206i| \u2264 3\u03b7, \u2200 i < m, j < i.\n3. |ATj \u2206i| \u2264 20\u03b3k\u03b72, \u2200 i < m, j > i.\nUsing Lemma 3, we will find the effect of orthogonalization on the correlations of the factors with the estimate Xm,t. At a high level, we need to show that the iterations for the factors {Ai, i \u2265 m} are not much affected by the orthogonalization, while the correlations of the factors {Ai, i < m} with the estimate Xm,t are ensured to be small. Lemma 3 is the key tool to prove this, as it shows that the orthogonalized basis is close to the true factors.\nWe will now analyze the inner product between X\u0304m,t and factor Ai to find the effect of the orthogonalization step on the estimate Xm,t. This is given by-\nATi X\u0304m,t = A T i Xm,t \u2212 \u2211 j<m XTm,tX\u0304jA T i X\u0304j\nAs earlier, we normalize all the correlations by the correlation of the largest factor, let a\u0304i,t be the ratio of the correlations of Ai and Am with the orthogonalized estimate X\u0304m,t at time t. We can write a\u0304i,t as-\na\u0304i,t = ATi Xm,t \u2212\n\u2211 j<mX T m,tX\u0304jA T i X\u0304j\nATmXm,t \u2212 \u2211 j<mX T m,tX\u0304jA T mX\u0304j\nWe can multiply both sides by w\u0302i and substitute X\u0304j from Lemma 3 and then rewrite as follows-\nw\u0302ia\u0304i,t = w\u0302iA\nT i Xm,t \u2212 \u2211 j<m w\u0302iX T m,t(Aj + \u2206j)A T i \u2206j\nATmXm,t \u2212 \u2211 j<mX T m,t(Aj + \u2206j)A T m\u2206j\nWe divide the numerator and denominator by ATmXm,t to derive an expression in terms of the ratios of correlations. Let \u03b4i,t = XTm,t\u2206j\nXTm,tAm .\nw\u0302ia\u0304i,t = w\u0302ia\u0302i,t \u2212\n\u2211 j<m(w\u0302ia\u0302j,t + w\u0302i\u03b4i,t)A T i \u2206j\n1\u2212 \u2211\nj<m(a\u0302j,t + \u03b4i,t)A T m\u2206j\nWe now need to show w\u0302ia\u0304i,t is small for all i < m and is close to w\u0302iai,t, the weighted correlation before orthogonalization, for all i > m. Lemma 4 proves this, and shows that the weighted correlation of factors which have not yet been recovered, {Ai, i \u2265 m}, is not much affected by orthogonalization, but the factors which have already been recovered, {Ai, i < m}, are ensured to be have small correlation after the orthogonalization step.\nLemma 4. Let |w\u0302ia\u0302i,t| \u2264 \u03b2t \u2200 i 6= m at the end of the tth iteration. Let a\u0304i,t be the ratio of the correlation of the ith and the mth factor with Xm,t, the iterate at time t after the orthogonalization step. Then,\n1. |w\u0302ia\u0304i,t| \u2264 \u03b2t(1 + 1/k1+ ), \u2200 i > m.\n2. |w\u0302ia\u0304i,t| \u2264 50\u03b3k\u03b7\u03b2t, \u2200 i < m.\nWe are now ready to analyze the Orth-ALS updates for the mth factor. First, we argue about the initialization step. Lemma 4 shows that an orthogonalization step performed after a random initialization ensures that the factors which have already been recovered have small correlation with the orthogonalized initialization. This is where we need a periodic re-randomization of the factors which have not converged so far.\nLemma 5. Let Xm,0 be initialized randomly and the result be projected orthogonal to the (m\u22121) previously estimated factors, let these be {Ai, i < m} without loss of generality. Then arg maxi |wiai,0| \u2265 m with high probability. Also, with failure probability at most ( 1\u2212 log\n5 k k1+ ) ,\u2223\u2223\u2223 wia\u0304i,0\nmaxi{wia\u0304i,0} \u2223\u2223\u2223 \u2264 1\u2212 4/k1+ \u2200 i 6= arg maxi|wiai,0| after the orthogonalization step.\nLemma 5 shows that with high probability, the initialization for the mth factor estimate has the largest weighted correlation with a factor which has not been recovered so far after the orthogonalization step. It also shows that the separation condition in Lemma 1 is satisfied for all remaining factors with probability (1\u2212 log5 k/k1+ ).\nNow, we combine the effects of the tensor power method step and the orthogonalization step for subsequent iterations to show that that Xm,t converges to Am. Consider a tensor power method step followed by an orthogonalization step. By our previous argument about the convergence of the tensor power method, if |w\u0302ia\u0302i,t\u22121| \u2264 \u03b2t\u22121 i 6= m at some time (t \u2212 1), then |w\u0302ia\u0302i,t| \u2264 \u03b22t\u22121 for i 6= m after a tensor power method step. Lemma 4 shows that the correlation of all factors other than the mth factor is still small after the orthogonalization step, if it was small before. Combining the effect of the orthogonalization step via Lemma 4 to the effect of the tensor power method step, if |w\u0302ia\u0302i,t\u22121| \u2264 \u03b2t\u22121 i 6= m for some time (t \u2212 1), then |w\u0302ia\u0302i,t| \u2264 \u03b22t\u22121(1 + 1/k1+ ) for i 6= m after both the tensor power method and the orthogonalization steps. By also using Lemma 5 for the initialization, can now write the updated combined recursion analogous to Eq. 6.3 and Eq. 6.4, but which combines the effect of the tensor power method step and the orthogonalization step.\n\u03b20 = max i 6=1 \u2223\u2223\u2223w\u0302ia\u0302i,0\u2223\u2223\u2223 (6.6) \u03b2t+1 = \u03b2 2 t (1 + 1/k 1+ ) (6.7)\nBy the previous argument, |wia\u0304i,t| \u2264 \u03b2t. Note that \u03b20 \u2264 1\u22124/k1+ by Lemma 5. By expanding the recursion in Eq. 6.7, \u03b2t = (\u03b20(1 + 1/k 1+ ))2 t . Hence \u03b2t \u2264 1/d in 2 log k + log log d steps as was the case for the analysis for the tensor power method. This shows that the correlation of the estimate Xm,t with all factors other than Am becomes small in 2 log k+log log d steps. We now again use Lemma 2 to argue that this implies that the recovery error is small, i.e. \u2016 Am\u2212A\u0302m \u201622\u2264 10\u03b3k/d2 and |1\u2212 w\u0302mwm | \u2264 O(1/d).\nHence we have shown that if the first (m \u2212 1) factors have converged to Xi = Ai + \u2206\u0302i where \u2016 \u2206\u0302i \u20162 \u2264 10\u03b3k/d2, \u2200 i < m then the mth factor converges to Xm = Am + \u2206\u0302m where \u2016 \u2206\u0302m \u20162 \u2264 10\u03b3k/d2 in O(log k+log log d) steps with probability at least ( 1\u2212 log\n5 k k1+\n) . This proves the induction\nhypothesis. We can now do a union bound to argue that each factor converges with `2 error at most O(\u03b3k/d\n2) in O(k(log k+log log d)) steps with overall failure probability at most O\u0303(1/k\u2212 ), > 0. This finishes the proof of convergence of Orth-ALS for the special case of orthogonal tensors.\nThe analysis of Orth-ALS for the case of incoherent tensors rather than orthogonal tensors is similar to the analysis for orthogonal tensors, but requires a careful analysis of the perturbations caused due to the factors not being orthogonal."}, {"heading": "7 Conclusion", "text": "Our results suggest the theoretical and practical benefits of Orthogonalized ALS, versus standard ALS. An interesting direction for future work would be to more thoroughly examine the practical and theoretical utility of orthogonalization for other tensor-related tasks, such as tensor completion. Additionally, its seems worthwhile to investigate Orthogonalized ALS or Hybrid ALS in more application-specific domains, such as natural language processing."}, {"heading": "A Global convergence of the tensor power method for incoherent", "text": "tensors\nIn this section, we will analyze the tensor power method updates for worst-case incoherent tensors. This is a necessary step before analyzing Orth-ALS, because as was pointed out in the proof of convergence of Orth-ALS in the orthogonal tensor case, analyzing Orth-ALS updates reduces to analyzing a perturbed version of the tensor power method updates. Our convergence results for the tensor power method are interesting independent of Orth-ALS though, as they prove global\nconvergence under random initialization. The proof idea is similar to the proof of convergence of the tensor power method in the orthogonal case, but we now need to analyze the cross-terms which come in because the factors are no longer orthogonal.\nTheorem 3. Consider a d-dimensional rank k tensor T = \u2211k\ni=1wiAi \u2297 Ai \u2297 Ai. Let cmax = maxi 6=j |ATi Aj | and \u03b3 = wmaxwmin be the ratio of the largest and smallest weight, and assume \u03b3cmax \u2264 o(k\u22122). If the initialization x0 \u2208 Rd is chosen uniformly from the unit sphere, then with high probability the tensor power method updates converge to one of the true factors (say A1) in O(log k+ log log d) steps, and the error at convergence satisfies \u2016 A1 \u2212 A\u03021 \u20162 \u2264 O(\u03b3kmax{c2max, 1/d2}) and |1\u2212 w\u03021w1 | \u2264 O(max{cmax, 1/d}).\nProof. We assume \u03b3kcmax \u2264 1/k1+ for some > 0. Without loss of generality, we will prove convergence to the first factor A1. The proof is similar in spirit to the proof of convergence of the tensor power method in the orthogonal case in Section 6.\nAs in the orthogonal case, Lemma 1 states that with high probability there is some separation between the weighted correlation of the largest and second largest factors.\nLemma 1. If \u03b3kcmax \u2264 1/k1+ for some > 0, then with probability at least ( 1 \u2212 log 5 k\nk1+\n) ,\n|wiai,0| maxi |wiai,0| \u2264 1\u2212 5/k 1+ \u2200 i 6= arg maxi |wiai,0|.\nWe normalize all the correlations by the correlation of the largest factor, let a\u0302i,t+1 = ai,t a1,t and\nnormalize all the weights by the weight of the largest factor, w\u0302i = wi w1\n. The new update equations in terms of the ratio of correlations a\u0302i,t become-\na\u0302i,t+1 = w\u0302ia\u0302\n2 i,t + ci,1 + \u2211 j:j 6={i,1} ci,jw\u0302j a\u0302 2 j,t\n1 + \u2211\nj:j 6=1 c1,jw\u0302j a\u0302 2 j,t\n(A.1)\nNotice that we have cross terms in Eq. A.1 as compared to Eq. 6.2 in the orthogonal case, due to the correlation ci,j between the factors being non-zero. The goal of the analysis for the non-orthogonal case is to bound these cross-terms using the incoherence between the factors.\nAs in the orthogonal case, we will analyze all the correlations a\u0302i,t via a single recursion. We define \u03b2t in the non-orthogonal case keeping in mind the cross-terms because of the correlations between the factors being non-zero.\n\u03b20 = max i 6=1 \u2223\u2223\u2223wia\u0302i,0\u2223\u2223\u2223 (A.2) \u03b2t+1 = \u03b3cmax + \u03b2 2 t + 3\u03b3kcmax\u03b2 2 t (A.3)\nWe now show that |wia\u0302i,t| \u2264 \u03b2t,\u2200 i 6= 1 and all t.\nLemma 6. If |w\u0302ia\u0302i,m| \u2264 \u03b2m for some time m and for all i 6= 1, then at time (m+ 1) for all i 6= 1,\n1. |w\u0302ia\u0302i,m+1| \u2264 \u03b2m+1.\n2. |a\u0302i,m+1 \u2212 ci,1| \u2264 2\u03b22m Proof. Note that by Lemma 7, \u03b2t < 1 \u2200 t =\u21d2 w\u0302ia\u03022i,m \u2264 1. Therefore \u2211\nj |ci,jw\u0302j a\u03022j,m| \u2264 kcmax \u2264 1/k1+ \u2200 i. Hence we can write,\n1 1 + \u2211\nj:j 6=1 c1,jw\u0302j a\u0302 2 j,m = 1\u2212 \u2211 j:j 6=1 c1,jw\u0302j a\u0302 2 j,m + 1\nwhere 1 is the residual term, and | 1| \u2264 \u2223\u2223\u2223\u2211j:j 6=1 c1,jw\u0302j a\u03022j,m\u2223\u2223\u22232 \u2264 k2c2max \u2264 1/k2. We can now rewrite the updates for a\u0302i,m+1 as-\na\u0302i,m+1 = ( ci,1 + w\u0302ia\u0302 2 i,m + \u2211 j:j 6={i,1} ci,jw\u0302j a\u0302 2 j,m )( 1\u2212 \u2211 j:j 6=1 c1,jw\u0302j a\u0302 2 j,m + 1 ) Let \u03c1i,m = ci,1 + w\u0302ia\u0302 2 i,m + \u2211 j:j 6={i,1} ci,jw\u0302j a\u0302 2 j,m. We can write,\na\u0302i,m+1 = ci,1 + w\u0302ia\u0302 2 i,m + \u2211 j:j 6={i,1} ci,jw\u0302j a\u0302 2 j,m \u2212 \u03c1i,m \u2211 j:j 6=1 c1,jw\u0302j a\u0302 2 j,m + \u03c1i,m 1 (A.4)\n=\u21d2 \u2223\u2223\u2223a\u0302i,m+1\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223ci,1\u2223\u2223\u2223+ \u2223\u2223\u2223w\u0302ia\u03022i,m\u2223\u2223\u2223+ \u2223\u2223\u2223 \u2211\nj:j 6={i,1}\nci,jw\u0302j a\u0302 2 j,m \u2223\u2223\u2223+ \u2223\u2223\u2223\u03c1i,m \u2211 j:j 6=1 c1,jw\u0302j a\u0302 2 j,m \u2223\u2223\u2223+ \u2223\u2223\u2223\u03c1i,m 1\u2223\u2223\u2223 (A.5) We claim that \u03c1i,m \u2264 1. We verify this as follows,\n\u03c1i,m = ci,1 + w\u0302ia\u0302 2 i,m + \u2211 j:j 6={i,1} ci,jw\u0302j a\u0302 2 j,m\n=\u21d2 \u2223\u2223\u2223\u03c1i,m\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223ci,1\u2223\u2223\u2223+ \u2223\u2223\u2223w\u0302ia\u03022i,m\u2223\u2223\u2223+ \u2223\u2223\u2223 \u2211\nj:j 6={i,1}\nci,jw\u0302j a\u0302 2 j,m \u2223\u2223\u2223 =\u21d2\n\u2223\u2223\u2223w\u0302i\u03c1i,m\u2223\u2223\u2223 \u2264 \u03b3\u2223\u2223\u2223ci,1\u2223\u2223\u2223+ \u2223\u2223\u2223w\u03022i a\u03022i,m\u2223\u2223\u2223+ \u2223\u2223\u2223 \u2211 j:j 6={i,1} ci,jw\u0302iw\u0302j a\u0302 2 j,m \u2223\u2223\u2223 \u2264 \u03b3cmax + \u03b22m + \u03b3kcmax\u03b22m \u2264 \u03b2m+1 \u2264 1\n=\u21d2 \u2223\u2223\u2223\u03c1i,m\u2223\u2223\u2223 \u2264 1\nwhere we used the fact that the weights lie in the interval [1, \u03b3]. Hence |\u03c1i,m| \u2264 1. Therefore, by Eq. A.5, \u2223\u2223\u2223a\u0302i,m+1\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223ci,1\u2223\u2223\u2223+ \u2223\u2223\u2223w\u0302ia\u03022i,m\u2223\u2223\u2223+ \u2223\u2223\u2223 \u2211\nj:j 6={i,1}\nci,jw\u0302j a\u0302 2 j,m \u2223\u2223\u2223+ \u2223\u2223\u2223 \u2211 j:j 6=1 c1,jw\u0302j a\u0302 2 j,m \u2223\u2223\u2223+ \u2223\u2223\u2223 1\u2223\u2223\u2223 =\u21d2\n\u2223\u2223\u2223w\u0302ia\u0302i,m+1\u2223\u2223\u2223 \u2264 \u03b3\u2223\u2223\u2223ci,1\u2223\u2223\u2223+ \u2223\u2223\u2223w\u03022i a\u03022i,m\u2223\u2223\u2223+ \u2223\u2223\u2223 \u2211 j:j 6={i,1} ci,jw\u0302iw\u0302j a\u0302 2 j,m \u2223\u2223\u2223+ \u2223\u2223\u2223 \u2211 j:j 6=1 c1,jw\u0302iw\u0302j a\u0302 2 j,m \u2223\u2223\u2223+ \u03b3\u2223\u2223\u2223 1\u2223\u2223\u2223 \u2264 \u03b3cmax + \u03b22m + 3\u03b3kcmax\u03b22m = \u03b2m+1 (A.6)\nTo show that |a\u0302i,m+1 \u2212 ci,1| \u2264 2\u03b22m we use Eq. A.4 and repeat the steps used to show that |w\u0302ia\u0302i,m+1| \u2264 \u03b2m+1 \u2200 t.\nBy using induction and Lemma 6, the iterates at all time t satisfy the following properties, for all i 6= 1,\n1. |w\u0302ia\u0302i,t| \u2264 \u03b2t \u2200 t.\n2. |a\u0302i,t \u2212 ci,1| \u2264 2\u03b22t\u22121 This allows us to analyze the iterations of \u03b2t instead of keeping track of the different ai,t. We will now analyze the recursion for \u03b2t. The following Lemma shows that \u03b2t becomes sufficiently small in O(log k + log log d) steps.\nLemma 7. \u03b2t \u2264 3\u03b3\u03b7 \u2200 t \u2265 O(log k + log log d). Also \u03b2t < 1 \u2200 t.\nProof. We divide the updates into three stages.\n1. 0.1 \u2264 \u03b2t \u2264 1\u2212 5/k1+ : As \u03b2t \u2265 0.1, therefore k\u03b22t \u2265 1 in this regime and hence \u03b3cmax \u2264 \u03b3kcmax\u03b22t , and we can write-\n\u03b2t+1 = \u03b3cmax + \u03b2 2 t + 3\u03b3kcmax\u03b2 2 t \u03b2t+1 \u2264 \u03b22t + 4\u03b3kcmax\u03b22t\nWe claim that \u03b2t < 0.1 for t = O(log d). To verify, note that-\n\u03b2t \u2264 (\u03b20(1 + 4\u03b3kcmax))2 t \u2264 ( (1\u2212 5/k1+ )(1 + 1/k1+ ) )2t\n\u2264 ( 1\u2212 1/k1+ )2t\n(A.7)\nwhere we used the fact that \u03b3kcmax \u2264 1/k1+ . Note that (1\u2212 1/k1+ )2 t \u2264 0.1 for t = 2 log k and hence we stay in this regime for at most 2 log k steps.\n2. \u221a \u03b3\u03b7 \u2264 \u03b2t \u2264 0.1 :\nFor notational convenience, we restart t from 0 in this stage. Because \u03b3cmax \u2264 \u03b3\u03b7 \u2264 \u03b22t in this regime and 3\u03b3kcmax\u03b2 2 t \u2264 0.1\u03b22t as \u03b3kcmax \u2264 1/k1+ , we can write-\n\u03b2t+1 = \u03b3cmax + \u03b2 2 t + 3\u03b3kcmax\u03b2 2 t\n\u2264 \u03b22t + \u03b22t + 0.3\u03b22t \u2264 2.5\u03b22t\nWe claim that \u03b2t < \u221a \u03b3\u03b7 for t = O(log log \u03b3\u03b7\u22121). To verify, note that-\n\u03b2t \u2264 (2.5\u03b20)2 t \u2264 (0.25)2t (A.8)\nNote that (0.25)2 t \u2264 \u221a\u03b3\u03b7 for t = O(log log(\u03b3\u03b7)\u22121) and hence we stay in this stage for at most O(log log(\u03b3\u03b7)\u22121) steps. As \u03b7\u22121 = O(d), this stage continues for at most O(log log d) steps.\n3. Note that in the next step, \u03b2t \u2264 \u03b3cmax + 1.1\u03b3\u03b7 \u2264 3\u03b3\u03b7. This is again because 3\u03b3kcmax\u03b22t \u2264 0.1\u03b22t and \u03b2t \u2264 \u221a \u03b3\u03b7 at the end of the previous stage.\nThe claim that \u03b2t \u2264 1 for all t is clear from the upper bounds for \u03b2t for all the three stages.\nHence \u03b2t \u2264 3\u03b3\u03b7 for t = O(log log d + log k). By Lemma 6, |a\u0302i,t \u2212 ci,1| \u2264 18\u03b32\u03b72, i 6= 1. Hence |a\u0302i,t| \u2264 2\u03b7. By Lemma 2, the error at convergence satisfies \u2016 A1 \u2212 A\u03021 \u20162 \u2264 10\u03b3k\u03b72 and the estimate of the weight w\u03021 satisfies |1\u2212 w\u03021w1 | \u2264 O(\u03b7)."}, {"heading": "B Global convergence of the tensor power method for random", "text": "tensors\nThe previous section gives global convergence guarantees for the tensor power method for incoherent tensors. Applying Theorem 3 to a tensor whose factors are chosen uniformly at random, we can say\nthat the tensor power method converges with random initialization whenever the rank k = o(d0.25). Theorem 3 also proves a linear convergence rate. However, this is quite suboptimal for random tensors. In this section, we use the randomness in the tensor to get much stronger convergence results.\nThe techniques used in this section are very different from the rest of the paper. Instead of recursively analyzing the tensor power method updates by showing that the algorithm makes progress at every step by boosting its correlation with some fixed factor, we directly express the correlation of the factors with the estimate Z\u03c4 after a fixed number of \u03c4 = O(log log d) time steps in terms of the initial correlations of the factors with the random initialization. This allows us to then skillfully use the randomness in the factors to get strong results. The difficulty with the recursive approach is that all the randomness in the tensor is \u201clost\u201d after just one tensor power method update, i.e. the correlations of different factors with the estimate are no longer independent of each other, which makes the analysis much more difficult.\nTheorem 2. Consider a d-dimensional rank k tensor T = \u2211k\ni=1wiAi \u2297 Ai \u2297 Ai with the factors Ai sampled uniformly from the d-dimensional sphere. Define \u03b3 =\nwmax wmin to be the ratio of the largest\nand smallest weight. Assume k \u2264 o(d) and \u03b3 \u2264 polylog(d). If the initialization x0 \u2208 Rd is chosen uniformly from the unit sphere, then with high probability the tensor power method updates converge to one of the true factors (say A1) in O(log log d) steps, and the error at convergence satisfies \u2016 A1 \u2212 A\u03021 \u20162 \u2264 O\u0303(1/ \u221a d). Also, the estimate of the weight w\u03021 satisfies |1\u2212 w\u03021w1 | \u2264 O\u0303(1/ \u221a d). Proof. We assume k \u2264 d1\u2212 for some > 0. Without loss of generality, we will prove convergence to the first factor A1. Let \u03c4 = 5 log log d. As before, define ai,t = \u3008Ai, Zt\u3009 where Zt is the iterate at time t. For the analysis of the tensor power method updates for random tensors we ignore the normalization step of the updates, till the last iteration. This does not make a difference in the analysis as the final estimate is invariant to rescaling the estimate in any intermediate steps. In practice though, it is important to normalize after every step to prevent the vectors from becoming too small and causing numerical errors. Recall that the update equations for ai,t for any t are\u2013\nai,t = wia 2 i,t\u22121 + ci,1w1a 2 1,t\u22121 + \u2211 j:j 6={i,1} ci,jwja 2 j,t\u22121 (B.1)\nand the iterate Z\u03c4+1 at time \u03c4 + 1 can be written as\nZ\u03c4+1 = w1a 2 1,\u03c4A1 + \u2211 i 6=1 wia 2 i,\u03c4Ai (B.2)\nOur strategy will be to expand the a2i,\u03c4 terms in Eq. B.2 recursively using Eq. B.1. On expanding w1a 2 1,\u03c4 recursively using Eq. B.1, one of the terms that appears in the expansion is (w1a1,0) 2\u03c4 /w1. Define \u03b1\u03c4 = |w1a1,0|2 \u03c4 /w1. We can write (1/\u03b1\u03c4 )w1a 2 1,\u03c4 , the coefficient for first factor A1 normalized by \u03b1\u03c4 , as follows\nw1a 2 1,\u03c4\n\u03b1\u03c4 =\n(w1a1,0) 2\u03c4\nw1\u03b1\u03c4 +\n1\n\u03b1\u03c4\n( w1a 2 1,\u03c4 \u2212 (w1a1,0) 2\u03c4\nw1 ) = 1 + 1\n\u03b1\u03c4\n( w1a 2 1,\u03c4 \u2212 (w1a1,0) 2\u03c4\nw1 ) Let \u03c9\u03c4 =\n1 \u03b1\u03c4\n( w1a 2 1,\u03c4 \u2212 (w1a1,0)2 \u03c4\nw1\n) , \u2206\u03c4 = (1/\u03b1\u03c4 ) \u2211 i 6=1wia 2 i,\u03c4Ai and Z \u2032 \u03c4+1 = Z\u03c4+1/\u03b1\u03c4 . We can write\nZ \u2032\u03c4+1 as\nZ \u2032\u03c4+1 = (1 + \u03c9\u03c4 )A1 + \u2206\u03c4\nNote that Z\u2032\u03c4+1 \u2016Z\u2032\u03c4+1\u20162 = Z\u03c4+1\u2016Z\u03c4+1\u20162 . Let Z\u2032\u03c4+1 \u2016Z\u2032\u03c4+1\u20162\n= Z\u0303\u03c4+1. We desire to show that the residual \u2016 Z\u0303\u03c4+1 \u2212A1 \u20162 \u2264 O\u0303(1/ \u221a d). We can bound \u2016 Z\u0303\u03c4+1 \u2212A1 \u20162 as follows using the triangle inequality,\n\u2016 Z\u0303\u03c4+1 \u2212A1 \u20162 \u2264 \u2223\u2223\u2223 1 + \u03c9\u03c4\u2016 (1 + \u03c9\u03c4 )A1 + \u2206\u03c4 \u20162 \u2212 1 \u2223\u2223\u2223+ \u2016 \u2206\u03c4 \u20162\u2016 (1 + \u03c9\u03c4 )A1 + \u2206\u03c4 \u20162 If \u2016 \u2206\u03c4 \u20162 \u2264 O\u0303(1/ \u221a d) and |\u03c9\u03c4 | \u2264 d\u2212\u03b4 for some \u03b4 > 0 then,\n\u2016 Z\u0303\u03c4+1 \u2212A1 \u20162 \u2264 \u2223\u2223\u2223 1\u2016 A1 + \u2206\u03c4/(1 + \u03c9\u03c4 ) \u20162 \u2212 1 \u2223\u2223\u2223+ \u2016 \u2206\u03c4 \u20162 1\u2212 |\u03c9\u03c4 | \u2212 \u2016 \u2206\u03c4 \u20162\n\u2264 2\u2016 \u2206\u03c4 \u20162 1\u2212 |\u03c9\u03c4 | + \u2016 \u2206\u03c4 \u20162 1\u2212 |\u03c9\u03c4 | \u2212 \u2016 \u2206\u03c4 \u20162 \u2264 O\u0303(1/ \u221a d)\nIf \u2016 Z\u0303\u03c4+1 \u2212A1 \u20162 \u2264 O\u0303(1/ \u221a d) then, by Lemma 2, the estimate of the weight w\u03021 satisfies |1\u2212 w\u03021w1 | \u2264\nO\u0303(1/ \u221a d).\nHence the goal of the remainder of the proof will be to show that \u2016 \u2206\u03c4 \u20162 \u2264 O\u0303(1/ \u221a d) and |\u03c9\u03c4 | \u2264 d\u2212\u03b4 with failure probability at most log\u22121 d. Let \u03c4 =\u2016 \u2206\u03c4 \u201622 and \u03bb\u03c4 = \u03c92\u03c4 . We can write \u03c4 as\n\u03c4 =\u2016 \u2206\u03c4 \u201622 = \u2211\ni 6=1,j 6=1 (1/\u03b12\u03c4 )wiwja 2 i,\u03c4a 2 j,\u03c4ci,j\nWe can also write \u03bb\u03c4 as follows\u2013\n\u03bb2\u03c4 = (1/\u03b1 2 \u03c4 )w 2 1 ( a21,\u03c4 \u2212 (w1a1,0) 2\u03c4\nw21 )2 Note that \u03bb\u03c4 has the same form as \u03c4 with the restriction that i = j = 1 and the (w1a1,0)2 \u03c4\nw21 in\nthe expansion of a21,\u03c4 is removed.\nOur approach will be to recursively expand the a2i,\u03c4 terms to express \u03c4 and \u03bb\u03c4 only in terms of ai,0 (the initial correlations at time 0), the correlation between factors ci,j and the weights wi. We use the recursion Eq. B.1 to do this.\nWe first consider the expansion of a2i,\u03c4 for any i using recursion Eq. B.1. a 2 i,t can be written as a weighted sum of correlations of the factors with the iterate at the (t \u2212 1)st time step as follows using recursion Eq. B.1\u2013\na2i,t = ( wia 2 i,t\u22121 + \u2211 j 6=i ci,jwja 2 j,t\u22121 )2 (B.3)\n= \u2211 j,k wjwkci,jci,ka 2 j,t\u22121a 2 k,t\u22121 (B.4)\nEach term in the summation corresponds to two choices for the terms at time (t\u2212 1), the j and k variables. We continue this recursive procedure of choosing two factors at the previous time step to expand the correlation at the current time step, for \u03c4 time steps. Therefore can represent each monomial in the expansion by a complete binary tree with depth \u03c4 . We label a node of the binary tree as j if it corresponds to factor Aj . For ease of exposition, we will consider the initialization Z0 as the 0th factor for the binary tree representation, hence ci,0 = ai,0 for notational purposes. The root of the tree is labeled as i if it corresponds to the factor Ai. The descendants of the root i are\nlabelled as j and k if a2i,\u03c4 is expanded into a 2 j,\u03c4\u22121 and a 2 k,\u03c4\u22121 using recursion Eq. B.1. The process is repeated at any step of the recursion, by expanding a2j,t in terms of a 2 l,t\u22121 and a 2 m,t\u22121 for some l and m. Refer to Fig. 4 for an example of a monomial and it\u2019s binary tree representation. Given any complete binary tree B, the monomial associated with the tree can be written down recursively. We write down the procedure for finding the monomial corresponding to a binary tree B explicitly in Algorithm 3 for clarity.\nAlgorithm 3 Finding monomial f from binary tree B Input: Binary tree B, root u monomial(B, u)\n1: while u is not a leaf do 2: Set i to be the factor corresponding to u 3: Set v to be the left child of u, set j to be the factor corresponding to v 4: Set w to be the right child of u, set k to be the factor corresponding to w 5: f = fwi 6: f = fci,jmonomial(B, v) 7: f = fci,kmonomial(B,w) 8: end while 9: return f\nTherefore, by successively using Eq. B.1, we expand wia 2 i,1 in terms of the correlations of the factors with the random initialization Z0 (the a 2 j,0 factors) and define a complete binary tree Bf for every monomial f in the expansion. We also define a graph Gf for the monomial f by coalescing nodes of the binary tree having the same label and removing self-loops. We allow more than one edge between two nodes.\nFor any monomial f in the expansion of (1/\u03b12\u03c4 )wiwja 2 i,\u03c4a 2 j,\u03c4ci,j in \u03c4 , we construct two binary\ntrees corresponding to the expansion of wia 2 i,\u03c4 and wja 2 j,\u03c4 . We construct the graph Gf by adding an edge between the roots of the two binary trees (this corresponds to the ci,j term) and then coalescing nodes of the new graph having the same label and removing self-loops, while allowing multiple edges between two nodes. The same procedure is followed for the expansion of \u03bb2\u03c4 , with the difference that now i = j = 1, and the (w1a1,0)2 \u03c4\nw21 term in the expansion of a21,\u03c4 is removed.\nB.1 Choosing a suitable basis for the factors\nWe will use the fact that the factors are random to show that they can be conveniently represented in a particular basis. Without loss of generality, assume that the first (n\u2212 1) factors are present in Gf , for some n. The (n\u2212 1) vectors corresponding to the (n\u2212 1) factors and the initialization Z0 span a n dimensional subspace. We will choose a particular basis {vi}, i \u2208 [n] for the n dimensional subspace and express the factors with respect to that basis. v1 = Z0, and vi is unit vector along the projection of Ai\u22121 orthogonal to {Aj , j < i\u22121}. In terms of this basis, Z0 = (1, 0, \u00b7 \u00b7 \u00b7 , 0). Let the 1st factor A1 have component x1,1 along the first coordinate axis and u1,2 along the second coordinate axis. Note that x1,1 is distributed as x\u03031,1 r1 and u1,2 is distributed as u\u03031,2 r1 where x\u03031,1 \u223c N(0, 1/d),\nu\u03031,2 \u223c z1 \u221a\u2211d i=2 y\u0303 2 1,i and r1 = \u221a x\u030321,1 + u\u0303 2 1,2. Here y\u03031,i \u223c N(0, 1/d) and z1 is uniform on {\u22121,+1}.\nSimilarly, the 2nd factor A2 has components (x2,1, x2,2, u2,3) \u223c ( x\u03032,1 r2 , x\u03032,2 r2 , u\u03032,3 r2 ) along the first three\ncoordinate axes. Here x\u03032,1, x\u03032,2 \u223c N(0, 1/d) and u\u03032,3 \u223c z2 \u221a\u2211d i=3 y\u0303 2 2,i and r2 = \u221a x\u030322,1 + x\u0303 2 2,2 + u\u0303 2 2,3, where y\u03032,i \u223c N(0, 1/d) and z2 is uniform on {\u22121,+1}. We continue this projection for all subsequent factors.\nWe first prove a Lemma that bounds the magnitude of the projection of any factor along the basis vectors.\nLemma 8. The projection of n factors along the basis defined above has the following properties-\n1. 1\u2212 1 d0.25 \u2264 r2i \u2264 1 + 1d0.25 \u2200 i \u2208 [n] with failure probability at most 2ne \u2212 \u221a d/8. 2. |x\u0303i,j | \u2264 log5 d/ \u221a d for all valid i, j (i.e. for all j < i, i \u2208 [n]) with failure probability at most\nn(1d) log8 d.\nProof. The proof relies on basic concentration inequalities.\n1. Consider the vector (xi,1, \u00b7 \u00b7 \u00b7 , ui,i+1, 0 \u00b7 \u00b7 \u00b7 , 0) corresponding to factor i. The squared scaling factor r2i is distributed as r 2 i \u223c (x\u03032i,1 + \u00b7 \u00b7 \u00b7 + x\u03032i,i + u\u03032i,i+1), where u\u03032i,i+1 \u223c y\u03032i,i+1 + \u00b7 \u00b7 \u00b7 + y\u03032i,d,\nthe y\u0303i,j are independent N(0, 1/d) random variables. r 2 i is the sum of squares of independent zero mean Gaussian random variables each having variance 1/d, and hence xi = dr 2 i is a \u03c7 2 random variable with d degrees of freedom. We use the following tail bound on a \u03c72 random variable x with d degrees of freedom (the bound follows from the sub-exponential property of the \u03c72 random variable)\nP[|x\u2212 d| \u2265 dt] \u2264 2e\u2212dt2/8 \u2200 t \u2208 [0, 1]\nChoosing t = d\u22120.25, P[|x2i \u2212 d| \u2265 d0.75] \u2264 2e\u2212 \u221a d/8. Therefore P[|r2i \u2212 1| \u2265 d\u22120.25] \u2264 2e\u2212 \u221a d/8. By a union bound, |r2i \u2212 1| \u2264 1d0.25 \u2200 i \u2208 [n] with failure probability at most 2ne \u2212 \u221a d/8.\n2. The bound follows directly from basic Gaussian tail bounds (refer to Eq. D.1) and a union bound.\nNote that as \u03c4 = 5 log log d, the total number of nodes of the binary tree corresponding to a monomial is at most 2\u03c4+1 = 2 log5 d. As each monomial corresponds to two binary trees, the number of number in the graph Gf can be at most 4 log\n5 d. Let N = 4 log5 d. We can now use a union bound to argue that the properties of the factors in Lemma 8 hold with high probability for\nany set of N factors. We define \u03b20 = max {\u2223\u2223\u2223 wixi,1w1x1,1 \u2223\u2223\u2223, i 6= 1} and \u03b2t = \u03b22t0 for any t.\nLemma 9. Consider the projection of any set of N = 4 log5 d factors. With failure probability at most 1/dlog d, |xi,j | \u2264 2 log3 d/ \u221a d for all valid i, j (i.e. for all j < i, i \u2208 [N ]). Also, with failure probability at most 1/ log d, \u03b20 \u2264 1\u2212 1/ log5 k. Proof. Using Lemma 8 and a union bound, |x\u0303i,j | \u2264 (log d)5/ \u221a d for all valid i, j and |r2i \u2212 1| \u2264\n1 d0.25 \u2200 i \u2208 [N ] with failure probability at most N(1d) log8 d + 2Ne\u2212 \u221a d/8 \u2264 2N(1d) log8 d. As xi,j =\nx\u0303i,j/ri, therefore xi,j \u2264 2 log5 d/ \u221a d whenever x\u0303i,j \u2264 log5 d/ \u221a d and ri \u2265 1 \u2212 1d0.25 . Therefore, as the total number of sets of N factors is at most kN \u2264 dN , by doing a union bound over all possible sets of N factors, |xi,j | \u2264 2 log5 d/ \u221a d for all valid i, j with failure probability at most 2dNN(1d) log8 d \u2264 1/dlog d.\nUsing Lemma 14, with failure probability at most 1/ log d, \u2223\u2223\u2223 wix\u0303i,1w1x\u03031,1 \u2223\u2223\u2223 \u2264 1\u2212 1/ log5 d for all i 6= 1. As |r2i \u2212 1| \u2264 1d0.25 \u2200 i \u2208 [k] with failure probability at most 2ke \u2212 \u221a d/8, therefore with failure\nprobability at most 2/ log d, \u2223\u2223\u2223 wixi,1w1x1,1 \u2223\u2223\u2223 \u2264 1\u2212 0.5/ log5 d for all i 6= 1.\nLet E be the event that for any projection of up to N factors |xi,j | \u2264 2 log3 d/ \u221a d for all valid i, j (i.e. for all j < i, i \u2208 [n]) and \u03b20 \u2264 1\u2212 1/ log5 k. By Lemma 9, probability of the event E is at least (1\u2212 3/ log d). We condition on the event E for the rest of the proof. We denote E|E [x] as the expectation of the random variable x conditioned on the event E.\nB.2 Characterizing when the monomial has non-zero expectation\nLet f2 refer to the product of all a 2 i,0 terms, all the weights wi for any i appearing in f and 1/\u03b1 2 \u03c4 . Let f1 refer to all the terms in f not present in f2, hence f = f1f2. Let G \u2032 f be the graph obtained by removing the node corresponding to the initialization X0 and all it\u2019s edges from Gf . Note that G\u2032f is a connected graph, as the 0th factors only appears in the leaves of the binary tree.\nAs the ci,j terms are inner products between the factors, we can write ci,j in terms of the coordinates of the vectors Ai and Aj , with respect to the basis we described previously. Note that ai,0 = xi,1 hence there is only one term in the inner product ai,0. f1 is a product of the correlation terms ci,j , hence it can be written as the summation of a product of a choices of coordinate for every ci,j term. Let the monomials obtained on rewriting f1 in terms of the coordinates of the vectors\nbe gi, hence f1 = \u2211K\ni=1 gi, where K is the total number of monomials obtained by expanding the correlation terms ci,j , in terms of the co-ordinates with respect to the chosen basis.\nLemma 10. f has non-zero expectation only if Gf is Eulerian. Also, every term gi having non-zero expectation corresponds to choosing a split of G\u2032f into a disjoint union of cycles and then choosing a single coordinate for all inner products ci,j which are part of a particular cycle.\nProof. We claim that every node in Gf must have even degree for f to have non-zero expectation. To verify, consider any node j which has odd degree. Note that the 0th node corresponding to the initialization Z0 always has even degree, hence j 6= 0. E|E [f] is the expectation of the product of all correlation terms ci,j and ai,0 appearing in the monomial. Each inner product ci,j involves a xi,t term or ui,t term for some coordinate t. Hence if node i has odd degree, then there is at least some t such that xi,t or ui,t is raised to an odd power. Note that the sign of xi,t or ui,t is an independent zero mean random variable, hence the expectation evaluates to 0 in this case. Hence every node in Gf must have even degree for f to have non-zero expectation. By Euler\u2019s theorem every node in a graph has even degree if and only if the graph is Eulerian (there exists a trail in the graph which uses every edge exactly once and returns to its starting point). Also, an Eulerian graph can be written as a disjoint union of cycles (Veblen\u2019s theorem).\nG\u2032f is also Eulerian and can be written as a disjoint union of cycles as every node has an even number of edges to node 0 and hence removal of these edges preserves the Eulerian property.\nWe now prove the second part of the Lemma, that every term gi having non-zero expectation corresponds to choosing a split of G\u2032f into a disjoint union of cycles and then choosing a single coordinate for all inner products ci,j which are part of a particular cycle. To verify this, let\u2019s start at any node i and consider it\u2019s inner product with a neighbor j. Say we choose coordinate t for the inner product ci,j which leads to a xi,txj,t term in gi. To ensure that gi has non-zero expectation, xj,t must appear in the term an even number of times (as the sign of xj,t is an independent zero mean random variable). Hence the coordinate t must be chosen in the inner product of node j with some neighbor of j. By repeating this argument, there must exist a cycle C with node i such that the coordinate t is chosen for all correlation terms in that cycle C. We then repeat the process on the graph obtained by removing the edges corresponding to cycle C from G\u2032f . Hence every gi term having non-zero expectation corresponds to choosing a split of G\u2032f into a disjoint union of cycles and then choosing a single coordinate for all inner products ci,j which are part of a particular cycle.\nWe let f \u20321 = \u2211 i:E|E [gi] 6=0 gi and f \u2032 = f \u20321f2. We claim that E|E [f ] = E|E [f \u2032]. Consider any term gi, such that E|E [gi] = 0. We claim that E|E [gif2] also equals 0, hence E|E [f ] = E|E [f \u2032]. This is because if gi has zero expectation, then there is some xi,t term raised to an odd power, as otherwise the expectation is non-zero. But, as all terms are raised to an even power in f2, the xi,t term is also raised to an odd power in gif2, which implies that E|E [gif2] = 0. This verifies the claim that E|E [gif2] = 0 if E|E [gi] = 0.\nHence will only consider the terms gi such that E|E [gi] 6= 0. By Lemma 10, all these terms correspond to choosing a split of G\u2032f into a disjoint union of cycles and then choosing a single coordinate for all inner products ci,j which are part of a particular cycle.\nB.3 Bounding expected value of monomial\nWe are now ready to bound the expected value of f. Note that E|E [f] = 0 if Gf is not Eulerian. If Gf and hence G \u2032 f are Eulerian, split G \u2032 f into some disjoint union of cycles. Say we split G \u2032 f into p cycles {C1, C2, \u00b7 \u00b7 \u00b7 , Cp} with m1,m2, \u00b7 \u00b7 \u00b7 ,mp edges. Let D(Cj) refer to the choice of coordinate D(Cj) for cycle Cj . Let g(\u222ajCj(D(Cj))) be the term in the expansion of f corresponding to a split of Gf into cycles {C1, C2, \u00b7 \u00b7 \u00b7 , Cp} and the choice of coordinate D(Cj) for cycle Cj . We also define h(Cj(D(Cj))) as the product of terms corresponding to cycle Cj and the choice of coordinate D(Cj) for the cycle Cj . Note that g(\u222ajCj(D(Cj))) = \u03a0pj=1h(Cj(D(Cj))). We can write\ng(\u222ajCj(D(Cj)))] = \u03a0jh(Cj(D(Cj)))\nh(Cj(D(Cj))) is the product of the square of the D(Cj)-th co-ordinate of all the factors appearing in the cycle Cj . Conditioned on the event E, there is only one factor having a component greater than log5 d/ \u221a d in absolute value along the D(Cj)-th co-ordinate axis, hence\nh(Cj(D(Cj))) \u2264 (log10 d)m1\u22121\ndm1\u22121\nHence, conditioned on event E, we can bound g(\u222ajCj(D(Cj))) as\u2013\ng(\u222ajCj(D(Cj))) \u2264 (log10 d)m\ndm\u2212p\nLet c(Gf ) be the largest p such that G \u2032 f can be decomposed into a union of p disjoint cycles. There can be at most m/2 disjoint cycles in G\u2032f as there are m edges, therefore c(Gf ) \u2264 m/2. Each edge can be placed in one of the total number of possible cycles, hence the total number of ways of splitting G\u2032f into a disjoint union of cycles is at most (m/2) m. Also, there are n possible choices for a coordinate for each cycle, hence there are at most n(m/2) terms corresponding to the same split of G\u2032f into a disjoint union of cycles. Hence for any particular monomial f , the number of possible gi terms having non-zero expectation is at most (m/2) mn(m/2). Note that m \u2264 4 log5 d as the graph G\u2032f is constructed by collapsing the two binary trees corresponding to monomial f . Each binary tree has depth \u03c4 = 5 log log d, hence the number of edges in each binary tree at most 2 log5 d. Hence the total number of edges in graph G\u2032f is at most 4 log\n5 d \u2264 log6 d. Note that n \u2264 m as the graph G\u2032f is connected. Hence we can bound E|E [f] as\u2013\nE|E [f] \u2264 f \u2032 \u2264 (m/2)mn(m/2) (log10 d)m\ndm\u2212c(Gf ) f2 \u2264\n(log10 d)5m/2\ndm\u2212c(Gf ) f2 (B.5)\nWe will now bound the f2 term. We will consider the representation of the monomial f as two complete binary trees. Recall that the leaves of the binary tree correspond to the 0th factor. Each pair of leaves having the factor i as their parent corresponds to a a2i,0 term. We will pair every leaf node with it\u2019s successor, regarding the binary tree as a binary search tree. Note that the left child of any node has the same node as it\u2019s successor. Let the right child of the node with factor i have a node with factor j as it\u2019s successor. We group the wi term due to the successor of the left child and wj term due to the successor of the right child together with the a 2 i,0 term. We bound the wjwia 2 i,0 term by \u03b3(wiai,0)\n2 whenever j 6= i and by (wiai,0)2 when j = i. If all the edges from the successor to the leaf are self-loops of the form cj,j , then j = i. Note that the paths of all leaves of a binary tree to their successor are disjoint, hence each cross-correlation term ci,j , i 6= j can lead to at most one leaf with j 6= i. The number of cross-correlation terms equals m, the number of edges in the graph G\u2032f . Recall that \u03b1\u03c4 = |w1a1,0|2 \u03c4 /w1. For the binary tree rooted at node u, define Tu,1 to be the left subtree and Tu,2 to be the right subtree. Let\n\u03b8 = min { max \u2223\u2223\u2223 wixi,1w1x1,1 \u2223\u2223\u2223 \u2200 i \u2208 Tu,1,max \u2223\u2223\u2223wjxj,1w1x1,1 \u2223\u2223\u2223 \u2200 j \u2208 Tu,1}. Clearly \u03b8 \u2264 \u03b20 if node 1 is not in both Tu,1 and Tu,2, and is at most 1 otherwise. Therefore the product of all the wi and a 2 i,0 terms normalized by \u03b1\u03c4 is at most \u03b3 m\u03b82 \u03c4\u22121 . Also as \u03b20 \u2264 1 \u2212 1/ log5 d and \u03c4 = 5 log log d, therefore (\u03b20) 2\u03c4\u22121 = \u03b2\u03c4\u22121 \u2264 1/d2. We provide an example of the analysis for bounding the expected value of a monomial using our techniques below.\nAs an example, consider the monomial f = (1/\u03b122)w 2 2w 4 1(c1,2) 4(a1,0) 8. Here f1 = c 4 1,2 and\nf2 = (1/\u03b1 2 2)w 2 2w 4 1(a1,0) 8. The binary tree Bf corresponding to f is given in Fig. 5. Both binary trees are the same in this case. The graph Gf obtained by coalescing the two binary trees is given in Fig. 6.\n1. Projecting factors onto suitable basis: We can write the initialization Z0 as the vector (1, 0 \u00b7 \u00b7 \u00b7 , 0). We write the factor A1 as (x1,1, u1,2, 0, \u00b7 \u00b7 \u00b7 , 0). Similarly, the 2nd factor A2 has components (x2,1, x2,2, u2,3). Using Lemma 9, max{|x1,1|, |x2,1|, |x2,2|} \u2264 log5 d/ \u221a d.\n2. Writing expectation of f as product of expectation of cycles: Let f1 = (c1,2) 4. Let f2 =\n(a1,0) 8 = (x1,0) 8. f can be expanded by choosing a coordinate corresponding to each c1,2 term, and then summing across all choices. Let the terms obtained on rewriting f1\nin terms of the co-ordinates of the factors A1 and A2 be gi, hence f = \u2211K\nj=1 gi. By Lemma 10, every term gi having non-zero expectation corresponds to choosing a split of G\u2032f into a disjoint union of cycles and then choosing a single coordinate for all inner products ci,j which are part of a particular cycle. Say we split G \u2032 f into the union of cycles C1 and C2 where C1 and C2 are 2 edge cycles between node 1 and node 2. Say we choose the 2nd coordinate for both the cycles C1 and C2. Following the notation of subsection B.3, D(C1) = D(C2) = 2 and g(C1(2) \u222a C2(2)) is the term in the expansion of f corresponding to split of G\u2032f into cycles C1 and C2 and then choosing the second coordinate for both cycles. g(C1(2) \u222a C2(2)) = h(C1(2))h(C2(2)) where h(C1(1)) as the product of terms corresponding to cycle C1 and the choice of coordinate 2 for the cycle 1 and similarly for h(C2(2)). In our example, h(C1(1)) = h(C1(1)) = x 2 1,2u 2 1,2. Therefore g(C1(2) \u222a C2(2)) = h(C1(2))h(C2(2)) = x41,2u41,2. By the bound obtained for the coordinates of the vectors via Lemma 9, g(C1(2) \u222a C2(2)) \u2264 log20 d/d2, again following the notation of subsection B.3. Recalling the definition of c(Gf ) as the largest p such that G\u2032f can be decomposed into a union of p disjoint cycles, for our example, c(Gf ) = 2. As each edge can be placed in one of the two possible cycles and there are 4 edges, the total number of ways of splitting G\u2032f into a disjoint union of cycles is at most 2\n4. There are 2 possible choices for coordinates for each cycle as we have two factors. Hence we can bound f \u2032 and E[f ] as -\nE|E [f] \u2264 f \u2032 \u2264 2442 (log10 d)2\nd2 f2 \u2264\n(log10 d)10\nd2 f2\n3. Bounding f2: For bounding f2, note that \u03b8 = 1 as the factor A1 appears in the left and right subtree for both the binary trees. Hence f2 \u2264 \u03b34.\nFigure 5: Binary tree Bf for f = w 2 2w 4 1(c1,2) 4(a1,0) 8 (both binary trees for f are the same)\nWe are now ready to bound \u03c4 . We will divide \u03c4 into 2 sets of monomials with non-zero expectation and bound each one of them separately\u2013\n1. All monomials with root nodes i and j such that either the binary tree with the root i or the binary tree with the root j does not have the node 1 in the left subtree or the right subtree. We call this set S1.\n2. All other monomials with non-zero expectation. We call this set S2.\nNote that the number of paths between two nodes in the graph G\u2032f is always even if f has non-zero expectation, as G\u2032f is Eulerian in that case. We need to relate the number of nodes and edges of an Eulerian graph for the rest of the proof, Lemma 11 provides a simple bound.\nLemma 11. For any connected Eulerian graph G, let N be the number of nodes and M be the number of edges. Consider any decomposition of G into a edge-disjoint set of p cycles. Then, N \u2264 M \u2212 p + 1. Moreover, if G has at least four edge-disjoint paths between a pair of nodes then N \u2264M \u2212 p.\nWe first consider the set S1. Note that \u03b8 \u2264 \u03b20 for at least one of the binary trees in this case. Therefore f2 \u2264 \u03b3m\u03b22 \u03c4\u22121 0 = \u03b3\nm\u03b2\u03c4\u22121. For any graph Gf with n nodes, there can be at most kn monomials having a graph isomorphic to Gf as their representation. By Lemma 11, n \u2264 m\u2212 c(Gf ) + 1. The total number of graphs with n nodes and m edges is be at most (n2)m. As the graphG\u2032f is connected, n \u2264 m. Note that the number of edgesm can be at most 4 log\n5 d \u2264 log6 d. Hence we can bound the contribution of all monomials in the set S1 as follows\u2013\n\u2211 f :f\u2208S1 E|E [f ] \u2264 log6 d\u2211 m=0 km\u2212c(Gf )+1 (m2)m(log10 d)5m/2\u03b3m dm\u2212c(Gf ) \u03b2\u03c4\u22121\n\u2264 log6 d\u2211 m=0 km\u2212c(Gf )+1 (\u03b3 log55 d)m dm\u2212c(Gf ) 1 d2\n\u2264 1 d log6 d\u2211 m=0 (\u03b3 log55 d)m (k d )m\u2212c(Gf ) \u2264 1 d log6 d\u2211 m=0 (\u03b3 log55 d)m ( 1 d )m/2 \u2264 1 d \u221e\u2211 m=0 (\u03b3 log55 d d0.5 )m \u2264 2 d\nWe next consider the set S2. For any graph Gf with n nodes with at least one of the nodes corresponding to factor A1, there can be at most nk\nn\u22121 monomials having a graph isomorphic to Gf as their representation as there are n possible positions to place the factor A1 and at most k n\u22121 ways to label the remaining nodes. We claim that by Lemma 11, n \u2264 m\u2212 c(Gf ). This is because there are two edge-disjoint paths from node i to node 1 and two edge-disjoint paths from node j to node 1. Note that there is always an edge between nodes i and j, as we connect the roots of the binary trees by an edge. Hence there are at least three edge-disjoint paths between nodes i and j. But there cannot be an odd number of edge-disjoint paths between 2 nodes in an Eulerian graph, hence there must be at least four edge-disjoint paths between nodes i and j. Hence by Lemma 11, n \u2264 m \u2212 c(Gf ). Also, note that the number of edges m \u2265 4 for monomials in S2 as there are two paths from node i to node 1 and two paths from node j to node 1. Hence we can bound the\ncontribution of all monomials in the set S2 as follows\u2013\n\u2211 f :f\u2208S2 E|E [f ] \u2264 log6 d\u2211 m=4 km\u2212c(Gf )\u22121 m(m2)m(log10 d)5m/2\u03b3m dm\u2212c(Gf )\n\u2264 log6 d\u2211 m=4 km\u2212c(Gf )\u22121 (\u03b3 log55 d)m dm\u2212c(Gf )\n\u2264 1 d log6 d\u2211 m=4 (\u03b3 log55 d)m (k d )m\u2212c(Gf )\u22121 \u2264 1 d log6 d\u2211 m=4 (\u03b3 log55 d)m ( 1 d )m/2\u22121 \u2264 1 d log6 d\u2211 m=4 (\u03b3 log55 d)m ( 1 d\n)m/10 \u2264 1 d log6 d\u2211 m=4 (\u03b3 log55 d d0.1 )m \u2264 1 d \u221e\u2211 m=4 (\u03b3 log55 d d0.1 )m \u2264 2 d\nTherefore E|E [ \u03c4 ] \u2264 4/d. We will now bound E|E [\u03bb\u03c4 ]. \u03bb\u03c4 is composed of monomials with at least one correlation (c1,i) term for i 6= 1. Also, all graphs for monomials corresponding to the expansion of \u03bb\u03c4 must include a node with label A1. As before, for any graph Gf with n nodes with at least one of the nodes corresponding to factor A1, there can be at most nk\nn\u22121 monomials having a graph isomorphic to Gf as their representation. By Lemma 11, n \u2264 m\u2212 c(Gf ) + 1. Hence we can bound \u03bb\u03c4 as follows,\nE|E [\u03bb\u03c4 ] \u2264 log6 d\u2211 m=1 km\u2212c(Gf ) m(m2)m(log10 d)5m/2\u03b3m dm\u2212c(Gf )\n\u2264 log6 d\u2211 m=1 km\u2212c(Gf ) (\u03b3 log55 d)m dm\u2212c(Gf ) \u2264 log6 d\u2211 m=1 (\u03b3 log55 d)m (k d\n)m\u2212c(Gf ) \u2264\nlog6 d\u2211 m=1 (\u03b3 log55 d)m ( 1 d )m/2 \u2264 log6 d\u2211 m=1 (\u03b3 log55 d d0.5 )m \u2264 \u221e\u2211 m=1 (\u03b3 log55 d d0.5 )m \u2264 1 d \u2032\nfor some \u2032 > 0. We now use Markov\u2019s inequality to get high probability guarantees\nP|E [ \u03c4 \u2265 log d/d ] \u2264 4/log d\nP|E [ \u03bb\u03c4 \u2265 log d/d \u2032 ] \u2264 1/log d\nAs the probability of the event E not happening is at most O(log\u22121 d), hence we have shown that \u2016 \u2206\u03c4 \u20162 \u2264 O\u0303(1/ \u221a d) and |\u03c9\u03c4 | \u2264 d\u2212\u03b4 for some \u03b4 > 0 with failure probability at most O(log\u22121 d)."}, {"heading": "C Proof of convergence for Orth-ALS", "text": "The proof of convergence of Orth-ALS for incoherent tensors mirrors the proof for orthogonal tensors in Section 6. For clarity, we will try to stick to the proof for the orthogonal case as far possible, while also providing proofs for intermediate Lemmas which were stated without proof in Section 6.\nTheorem 1. Consider a d-dimensional rank k tensor T = \u2211k\ni=1wiAi \u2297 Ai \u2297 Ai. Let cmax = maxi 6=j |ATi Aj | be the incoherence between the true factors and \u03b3 = wmaxwmin be the ratio of the largest and smallest weight. Assume \u03b3cmax \u2264 o(k\u22122), and the estimates of the factors are initialized randomly from the unit sphere. Provided that, at the i(log k + log log d)th step of the algorithm the estimates for all but the first i factors are re-randomized, then with high probability the orthogonalized ALS updates converge to the true factors in O(k(log k + log log d)) steps, and the error at convergence satisfies (up to relabelling) \u2016 Ai \u2212 A\u0302i \u20162 \u2264 O(\u03b3kmax{c2max, 1/d2}) and |1\u2212 w\u0302iwi | \u2264 O(max{cmax, 1/d}), for all i. Proof. We assume \u03b3kcmax \u2264 1/k1+ for some > 0. Without loss of generality, we assume that the ith recovered factor converges to the ith true factor. Note that the iterations for the first factor are the usual tensor power method updates and are unaffected by the remaining factors. Hence by Theorem 3, Orth-ALS correctly recovers the first factor O(log k + log log d) steps with probability at least (1\u2212 log5 k/k1+ ), for any > 0.\nWe now prove that Orth-ALS also recovers the remaining factors. The proof proceeds by induction. We have already shown that the base case is correct and the algorithm recovers the first factor. We next show that if the first (m\u22121) factors have converged, then the mth factor converges in O(log k+log log d) steps with failure probability at most O\u0303(1/k1+ ). The main idea is that as the factors have small correlation with each other, hence orthogonalization does not affect the factors which have not been recovered but ensures that the mth estimate never has high correlation with the factors which have already been recovered. Recall that we assume without loss of generality that the ith recovered factor Xi converges to the ith true factor, hence Xi = Ai + \u2206\u0302i for i < m, where \u2016 \u2206\u0302i \u20162 \u2264 10\u03b3k\u03b72. This is our induction hypothesis, which is true for the base case as by Theorem 3 the tensor power method updates converge with residual error at most 10\u03b3k\u03b72.\nLet Xm,t denote the mth factor estimate at time t and let Xm denote it\u2019s value at convergence. We will first calculate the effect of the orthogonalization step on the correlation between the factors and the estimate Xm,t. Let {X\u0304i, i < m} denote an orthogonal basis for {Xi, i < m}. The basis {X\u0304i, i < m} is calculated via QR decomposition, and can be written down as follows,\nX\u0304i = Xi \u2212\n\u2211 j<i X\u0304 T j XiX\u0304j\n\u2016 Xi \u2212 \u2211 j<i X\u0304 T j XiX\u0304j \u20162\nNote that the estimate Xm,t is projected orthogonal to this basis. Define X\u0304m,t as this orthogonal projection, which can be written down as follows \u2013\nX\u0304m,t = X\u0304m,t \u2212 \u2211 j<m X\u0304Tj Xm,tX\u0304j\nIn the QR decomposition algorithm X\u0304m,t is also normalized to have unit norm but we will ignore the normalization of Xm,t in our analysis because as before we only consider ratios of correlations of the true factors with X\u0304m,t, which is unaffected by normalization.\nWe will now analyze the orthogonal basis {X\u0304i, i < m}. The key idea is that the orthogonal basis {X\u0304i, i < m} is close to the original factors {Ai, i < m} as the factors are incoherent. Lemma 3 proves this claim.\nLemma 3. Consider a stage of the Orthogonalized ALS iterations when the first (m \u2212 1) factors have converged. Without loss of generality let Xi = Ai + \u2206\u0302i, i < m, where\u2016 \u2206\u0302i \u20162 \u2264 10\u03b3k\u03b72. Let {X\u0304i, i < m} denote an orthogonal basis for {Xi, i < m} calculated using Eq. 6.5. Then,\n1. X\u0304i = Ai + \u2206i, \u2200 i < m and \u2016 \u2206j \u20162 \u2264 10k\u03b7.\n2. |ATj \u2206i| \u2264 3\u03b7, \u2200 i < m, j < i.\n3. |ATj \u2206i| \u2264 20\u03b3k\u03b72, \u2200 i < m, j > i.\nProof. We prove the result by induction. As the first estimate converges to A1+\u2206\u03021 where \u2016 \u2206\u03021 \u20162 \u2264 10\u03b3k\u03b72, the base case is correct. Assume that the result is true for the first p \u2212 1 vectors in the basis. After orthogonalization, the pth basis vector has the following form\u2013\nX\u0304p = 1\n\u03ba\n( (Ap + \u2206\u0302p)\u2212 \u2211 j<p ((Ap + \u2206\u0302p) T X\u0304j)X\u0304j ) where \u03ba is the normalizing factor which ensures \u2016 X\u0304p \u20162 = 1. Define \u00b5p,j = ATp (Aj + \u2206j). As |ATp \u2206j | \u2264 20\u03b3k\u03b72 by the induction hypothesis and |ATpAj | \u2264 \u03b7 by definition of \u03b7, |\u00b5p,j | \u2264 2\u03b7. Using the induction hypothesis, we can write\n\u03baX\u0304p = Ap \u2212 \u2211 j<p ( ATp (Aj + \u2206j) ) (Aj + \u2206j) + \u2206\u0302p \u2212 \u2211 k<p ( \u2206\u0302Tp (Aj + \u2206j) ) (Aj + \u2206j)\n= Ap \u2212 \u2211 j<p \u00b5p,j(Aj + \u2206j) + \u2206\u0302\nwhere \u2206\u0302 = \u2206\u0302p \u2212 \u2211\nk<p ( \u2206\u0302Tp (Aj + \u2206j) ) (Aj + \u2206j). As \u2206\u0302 is a projection of \u2206\u0302p orthogonal to the\nbasis {X\u0304i, i < p}, \u2016 \u2206\u0302 \u20162 \u2264 \u2016 \u2206\u0302p \u20162 \u2264 10\u03b3k\u03b72. We can write-\n\u03baX\u0304p = Ap \u2212 \u2211 j<p \u00b5p,jAj \u2212 \u2211 j<p \u00b5p,j\u2206j + \u2206\u0302\n= Ap + \u2206 \u2032 p\nwhere \u2206\u2032p = \u2212 \u2211 j<p \u00b5p,jAj \u2212 \u2211 j<p \u00b5p,j\u2206j + \u2206\u0302 . We bound \u2016 \u2206\u2032p \u20162 as follows-\n\u2016 \u2206\u2032p \u20162 \u2264 \u2211 j<p \u2016 \u00b5p,jAj \u20162 + \u2211 j<p \u2016 \u00b5p,j\u2206j \u20162 + \u2016 \u2206\u0302 \u20162\n\u2264 2k\u03b7 + 20k2\u03b72 + 10\u03b3k\u03b72 \u2264 3k\u03b7\nNote that \u03ba = \u2016 Ap + \u2206\u2032p \u20162 =\u21d2 1 \u2212 3k\u03b7 \u2264 \u03ba \u2264 1 + 3k\u03b7 by the triangle inequality. Hence 1\u2212 3k\u03b7 \u2264 1/\u03ba \u2264 1 + 6k\u03b7. Therefore we can rewrite X\u0304p as-\nX\u0304p = 1\n\u03ba (Ap + \u2206\n\u2032 p)\n= Ap + (1\u2212 1\n\u03ba )Ap +\n1 \u03ba \u2206\u2032p\n= Ap + c1Ap + c2\u2206 \u2032 p\n= Ap + \u2206p\nwhere c1 = (1\u2212 1\u03ba), c2 = 1 \u03ba and \u2206p = c1Ap+c2\u2206 \u2032 p. Note that |c1| \u2264 6k\u03b7 and 1\u22123k\u03b7 \u2264 c2 \u2264 1+6k\u03b7. Hence \u2016 \u2206p \u20162 \u2264 10k\u03b7.\nWe now show that |ATi \u2206p| \u2264 3\u03b7, i < p,\n\u2206p = c1Ap + c2 ( \u2212 \u2211 j<p \u00b5p,jAj \u2212 \u2211 j<p \u00b5p,j\u2206j + \u2206\u0302 ) =\u21d2\n\u2223\u2223\u2223ATi \u2206p| = \u2223\u2223\u2223c1ATi Ap\u2223\u2223\u2223+ c2\u2223\u2223\u2223\u2211 j<p \u00b5p,jA T i Aj \u2212 \u2211 j<p,j 6=i \u00b5p,jA T i \u2206j \u2212 \u00b5p,iATi \u2206i +ATi \u2206\u0302 \u2223\u2223\u2223 \u2264 6k\u03b72 + (1 + 6k\u03b7)(2\u03b7(1 + k\u03b7) + 6k\u03b72 + 20k\u03b72 + 10\u03b3k\u03b72) \u2264 3\u03b7\nFinally, we show that |ATi \u2206p| \u2264 20\u03b3k\u03b72, i > p,\u2223\u2223\u2223ATi \u2206p| = c1\u2223\u2223\u2223ATi Ap\u2223\u2223\u2223+ c2\u2223\u2223\u2223\u2211 j<p \u00b5p,jA T i Aj \u2212 \u2211 j<p \u00b5p,jA T i \u2206j +A T i \u2206\u0302 \u2223\u2223\u2223 \u2264 6k\u03b72 + (1 + 6k\u03b7)(2k\u03b72 + 40\u03b3k2\u03b73 + 10\u03b3k\u03b72) \u2264 20\u03b3k\u03b72\nUsing Lemma 3, we will find the effect of orthogonalization on the correlations of the factors with the iterate Xm,t. At a high level, we need to show that the iterations for the factors {Ai, i \u2265 m} are not much affected by the orthogonalization, while the correlations of the factors {Ai, i < m} with the estimate Xm,t are ensured to be small. Lemma 3 is the key tool to prove this, as it shows that the orthogonalized basis is close to the true factors.\nWe will now analyze the inner product between X\u0304m,t and factor Ai. This is given by-\nATi X\u0304m,t = A T i Xm,t \u2212 \u2211 j<m XTm,tX\u0304jA T i X\u0304j\nAs earlier, we normalize all the correlations by the correlation of the largest factor, let a\u0304i,t be the ratio of the correlations of Ai and Am with the orthogonalized estimate X\u0304m,t at time t. We can write a\u0304i,t as-\na\u0304i,t = ATi Xm,t \u2212\n\u2211 j<mX T m,tX\u0304jA T i X\u0304j\nATmXm,t \u2212 \u2211 j<mX T m,tX\u0304jA T mX\u0304j\nWe can multiply both sides by w\u0302i and substitute X\u0304j from Lemma 3 and then rewrite as follows-\nw\u0302ia\u0304i,t = w\u0302iA\nT i Xm,t \u2212 \u2211 j<m w\u0302iX T m,t(Aj + \u2206j)A T i \u2206j\nATmXm,t \u2212 \u2211 j<mX T m,t(Aj + \u2206j)A T m\u2206j\nWe divide the numerator and denominator by ATmXm,t to derive an expression in terms of the ratios of correlations. Let \u03b4i,t = XTm,t\u2206j\nXTm,tAm .\nw\u0302ia\u0304i,t = w\u0302ia\u0302i,t \u2212\n\u2211 j<m(w\u0302ia\u0302j,t + w\u0302i\u03b4i,t)A T i \u2206j\n1\u2212 \u2211\nj<m(a\u0302j,t + \u03b4i,t)A T m\u2206j\nLemma 12 upper bounds \u03b4i,t.\nLemma 12. Let |w\u0302ia\u0302i,t\u22121| \u2264 \u03b2t\u22121 \u2200 i 6= m and some time (t\u2212 1). Also, let \u03b3\u03b7 + \u03b22t\u22121 \u2264 \u03b2t. Then for all i < m, \u03b4i,t \u2264 40k\u03b7\u03b2t. Proof. By the power method updates Xm,t = \u2211 i wi\u03bbiAi\n\u2016 \u2211 i wi\u03bbiAi\u20162 where \u03bbi = a 2 i,t\u22121. Note that \u03b4i,j is normalized by the correlation of the largest factor Am, hence the normalizing factor \u2016 \u2211\niwi\u03bbiAi \u20162 does not matter and we will ignore it. We use Lemma 3 to bound |ATi \u2206j |. Hence,\u2223\u2223\u2223XTm,t\u2206j\nXTm,tAm \u2223\u2223\u2223 \u2264 \u2211i w\u0302ia\u03022i,t\u22121|ATi \u2206j |\u2211 i w\u0302ia\u0302 2 i,t\u22121A T i Am\n= |ATm\u2206j |+\n\u2211 i 6=j,m w\u0302ia\u0302 2 i,t\u22121|ATi \u2206i|+ w\u0302ia\u03022j,t\u22121|ATj \u2206j |\n1 + \u2211\ni 6=m ci,mw\u0302ia\u0302 2 i,t\u22121\n\u2264 20\u03b3k\u03b72 + 3k\u03b7\u03b22t\u22121 + 10k\u03b7\u03b2 2 t\u22121\n1\u2212 k\u03b7\u03b22t\u22121\n\u2264 k\u03b7(20\u03b3\u03b7 + 13\u03b22t\u22121) 1\u2212 0.5 \u2264 40k\u03b7\u03b2t\nWe now need to show w\u0302ia\u0304i,t is small for all i < m and is close to w\u0302iai,t, the weighted correlation before orthogonalization, for all i > m. Lemma 4 proves this, and shows that the weighted correlation of factors which have not yet been recovered, {Ai, i \u2265 m}, is not much affected by orthogonalization, but the factors which have already been recovered, {Ai, i < m}, are ensured to be small after the orthogonalization step.\nLemma 4. Let |w\u0302ia\u0302i,t| \u2264 \u03b2t \u2200 i 6= m at the end of the tth iteration. Let a\u0304i,t be the ratio of the correlation of the ith and the mth factor with Xm,t, the iterate at time t after the orthogonalization step. Then,\n1. |w\u0302ia\u0304i,t| \u2264 \u03b2t(1 + 1/k1+ ), \u2200 i > m.\n2. |w\u0302ia\u0304i,t| \u2264 50\u03b3k\u03b7\u03b2t, \u2200 i < m.\nProof. We can bound a\u0304i,t for all i \u2265 m as-\n\u2223\u2223\u2223w\u0302ia\u0304i,t\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223w\u0302ia\u0302i,t\u2223\u2223\u2223+ \u2223\u2223\u2223\u2211j<m(w\u0302ia\u0302j,t + w\u0302i\u03b4j,t)ATi (Aj + \u2206j)\u2223\u2223\u2223\n1\u2212 \u2223\u2223\u2223\u2211j<m(a\u0302j,t + \u03b4j,t)ATm(Aj + \u2206j)\u2223\u2223\u2223\n\u2264 \u2223\u2223\u2223w\u0302ia\u0302i,t\u2223\u2223\u2223+\u2211j<m \u2223\u2223\u2223(w\u0302ia\u0302j,t + w\u0302i\u03b4j,t)\u2223\u2223\u2223\u2223\u2223\u2223ATi (Aj + \u2206j)\u2223\u2223\u2223 1\u2212 \u2211 j<m\n\u2223\u2223\u2223(a\u0302j,t + \u03b4j,t)\u2223\u2223\u2223\u2223\u2223\u2223ATm(Aj + \u2206j)\u2223\u2223\u2223 \u2264\n\u2223\u2223\u2223w\u0302ia\u0302i,t\u2223\u2223\u2223+\u2211j<m (\u2223\u2223\u2223w\u0302ia\u0302j,t\u2223\u2223\u2223+ \u2223\u2223\u2223w\u0302i\u03b4j,t\u2223\u2223\u2223)\u2223\u2223\u2223\u2223\u2223\u2223ATi (Aj + \u2206j)\u2223\u2223\u2223 1\u2212 \u2211 j<m (\u2223\u2223\u2223a\u0302j,t\u2223\u2223\u2223+ \u2223\u2223\u2223\u03b4j,t\u2223\u2223\u2223)\u2223\u2223\u2223ATm(Aj + \u2206j)\u2223\u2223\u2223\nNote that |w\u0302ia\u0302i,t| \u2264 \u03b2t, |w\u0302ia\u0302j,t| \u2264 \u03b3\u03b2t and |w\u0302i\u03b4j,t| \u2264 40\u03b3k\u03b7\u03b2t, using Lemma 12. Also, |ATi (Aj + \u2206j)| \u2264 4\u03b7 using Lemma 3. Hence we can write,\u2223\u2223\u2223w\u0302ia\u0304i,t\u2223\u2223\u2223 \u2264 \u03b2t 1 + 8\u03b3k\u03b7\n1\u2212 4k\u03b7\u03b2t \u2264 \u03b2t(1 + 8\u03b3k\u03b7)(1 + 8k\u03b7\u03b2t) \u2264 \u03b2t(1 + 20\u03b3k\u03b7) \u2264 \u03b2t(1 + 1/k1+ )\nSimilarly, we can bound a\u0304i,t for all i < m as-\nw\u0302ia\u0304i,t = w\u0302ia\u0302i,t \u2212\n\u2211 j<m(w\u0302ia\u0302j,t + w\u0302i\u03b4j,t)A T i (Aj + \u2206j)\n1\u2212 \u2211\nj<m(a\u0302j,t + \u03b4j,t)A T m(Aj + \u2206j)\n= w\u0302ia\u0302i,t \u2212 (w\u0302ia\u0302i,t + w\u0302i\u03b4j,t)ATi (Ai + \u2206i)\u2212\n\u2211 j<m,j 6=i(w\u0302ia\u0302j,t + w\u0302i\u03b4j,t)A T i (Aj + \u2206j)\n1\u2212 \u2211\nj<m(a\u0302j,t + \u03b4j,t)A T m(Aj + \u2206j)\n= w\u0302i\u03b4j,tA\nT i (Ai + \u2206i)\u2212 \u2211 j<m,j 6=i(w\u0302ia\u0302j,t + w\u0302i\u03b4j,t)A T i (Aj + \u2206j)\n1\u2212 \u2211\nj<m(a\u0302j,t + \u03b4j,t)A T m(Aj + \u2206j)\n\u2264 \u2223\u2223\u2223w\u0302i\u03b4j,t\u2223\u2223\u2223\u2223\u2223\u2223ATi (Ai + \u2206i)\u2223\u2223\u2223+\u2211j 6=i (\u2223\u2223\u2223w\u0302ia\u0302j,t\u2223\u2223\u2223+ \u2223\u2223\u2223w\u0302i\u03b4j,t\u2223\u2223\u2223)\u2223\u2223\u2223ATi (Aj + \u2206j)\u2223\u2223\u2223 1\u2212 \u2211 j,m\n(\u2223\u2223\u2223a\u0302j,t\u2223\u2223\u2223+ \u2223\u2223\u2223\u03b4j,t\u2223\u2223\u2223)\u2223\u2223\u2223ATm(Aj + \u2206j)\u2223\u2223\u2223 \u2264 40\u03b3k\u03b7\u03b2t + 8\u03b3k\u03b7\u03b2t 1\u2212 4k\u03b7\u03b2t \u2264 50\u03b3k\u03b7\u03b2t\nwhere we have again used the relations |w\u0302ia\u0302i,t| \u2264 \u03b2t, |w\u0302ia\u0302j,t| \u2264 \u03b3\u03b2t, |w\u0302i\u03b4j,t| \u2264 40\u03b3k\u03b7\u03b2t (by Lemma 12) and |ATi (Aj + \u2206j)| \u2264 4\u03b7.\nWe are now ready to analyze the Orth-ALS updates for the mth factor. First, we argue about the initialization step. Lemma 4 shows that an orthogonalization step performed after the initialization ensures that the factors which have already been recovered have small correlation with the orthogonalized initialization \u2013\nLemma 5. Let Xm,0 be initialized randomly and the result be projected orthogonal to the (m\u22121) previously estimated factors, let these be {Ai, i < m} without loss of generality. Then arg maxi |wiai,0| \u2265 m with high probability. Also, with failure probability at most ( 1\u2212 log\n5 k k1+ ) ,\u2223\u2223\u2223 wia\u0304i,0\nmaxi{wia\u0304i,0} \u2223\u2223\u2223 \u2264 1\u2212 4/k1+ \u2200 i 6= arg maxi|wiai,0| after the orthogonalization step.\nProof. We first show that arg maxi |wiai,0| \u2265 m. From Lemma 4, the ratio of the weighted correlation of all factors {Ai, i < m} with the random initialization and the weighted correlation of all factors {Ai, i \u2265 m} with the random initialization is shrunk by a factor of O(k\u2212(1+ )) after the orthogonalization step. Hence with exponentially small failure probability, no factor {Ai, i < m} will have maximum weighted correlation after the orthogonalization step.\nLemma 1 can now be applied on all remaining factors, to get the initialization condition. Without loss of generality, assume that arg max |wiai,0| = m. Consider the set of factors {Ai,m \u2264 i \u2264 n}.\nFrom Lemma 1, with probability at least ( 1\u2212 log 5 k\nk1+ ) , \u2223\u2223\u2223 wiai,0wmam,0 \u2223\u2223\u2223 \u2264 1\u2212 5/k1+ , > 0 \u2200 i 6= 1. Ap-\nplying Lemma 4 once more, |w\u0302ia\u0304i,t| \u2264 \u03b2t(1 + 1/k1+ ), \u2200 i > m. Therefore combining Lemma 1 and Lemma 4, with failure probability at most ( 1 \u2212 log\n5 k k1+ ) , \u2223\u2223\u2223 wia\u0304i,0wma\u0304m,0 \u2223\u2223\u2223 \u2264 1\u2212 4/k1+ \u2200 i 6= m after the\northogonalization step.\nLemma 5 shows that with high probability, the initialization for the mth recovered factor has the largest weighted correlation with a factor which has not been recovered so far after the orthogonalization step. It also shows that the separation condition in Lemma 1 is satisfied for all remaining factors with probability (1\u2212 log5 k/k1+ ).\nNow, we combine the effects of the tensor power method step and the orthogonalization step for subsequent iterations to show that that Xm,t converges to Am. Consider a tensor power method step followed by an orthogonalization step. By Lemma 6, if |w\u0302ia\u0302i,t\u22121| \u2264 \u03b2t\u22121 i 6= m at some time (t\u2212 1), then |w\u0302ia\u0302i,t| \u2264 (\u03b3cmax + \u03b22t\u22121 + 3\u03b3kcmax\u03b22t\u22121) for i 6= m after a tensor power method step. Lemma 4 shows that the correlation of all factors other than the mth factors is still small after the orthogonalization step if it was small before. Combining the effect of the orthogonalization step via Lemma 4, if |w\u0302ia\u0302i,t\u22121| \u2264 \u03b2t\u22121 i 6= m for some time (t \u2212 1), then |w\u0302ia\u0302i,t| \u2264 (\u03b3cmax + \u03b22t\u22121 + 3\u03b3kcmax\u03b2 2 t\u22121)(1 + 1/k\n1+ ) for i 6= m after both the tensor power method and the orthogonalization steps. By also using Lemma 5 for the initialization, can now write the updated combined recursion analogous to Eq. A.2 and Eq. A.2, but which combines the effect of the tensor power method step and the orthogonalization step.\n\u03b20 = max i 6=1 \u2223\u2223\u2223wia\u0302i,0\u2223\u2223\u2223 (C.1) \u03b2t+1 = (\u03b3cmax + \u03b2 2 t + 3\u03b3kcmax\u03b2 2 t )(1 + 1/k 1+ ) (C.2)\nBy the previous argument, |wia\u0304i,t| \u2264 \u03b2t. Note that \u03b20 \u2264 1 \u2212 4/k1+ by Lemma 5. We will now analyze the recursion in Eq. C.1 and Eq. C.2.\nLemma 13. \u03b2t \u2264 3\u03b3\u03b7 \u2200 t \u2265 O(log k + log log d). Also \u03b2t < 1\u2212 1/k1+ \u2200 t.\nProof. The proof is very similar to the proof for Lemma 7. We divide the updates into three stages.\n1. 0.1 \u2264 \u03b2t \u2264 1\u2212 4/k1+ : As \u03b2t \u2265 0.1 therefore k\u03b22t \u2265 1 in this regime and hence \u03b3cmax \u2264 \u03b3k\u03b22t cmax, and we can write-\n\u03b2t+1 = (\u03b3cmax + \u03b2 2 t + 3\u03b3kcmax\u03b2 2 t )(1 + 1/k 1+ ) \u03b2t+1 \u2264 (\u03b22t + 4\u03b3kcmax\u03b22t )(1 + 1/k1+ )\nWe claim that \u03b2t < 0.1 for t = 2 log k. To verify, note that-\n\u03b2t \u2264 (\u03b20(1 + 4\u03b32kcmax)(1 + 1/k1+ ))2 t \u2264 ( (1\u2212 4/k1+ )(1 + 1/k1+ )(1 + 1/k1+ ) )2t\n\u2264 ( 1\u2212 1/k1+ )2t\nThis follows because \u03b3kcmax \u2264 1/k1+ . Note that (1 \u2212 1/k1+ )2 t \u2264 0.1 for t = 2 log k and hence we stay in this regime for at most 2 log k steps.\n2. \u221a \u03b3\u03b7 \u2264 \u03b2t \u2264 0.1 :\nFor notational convenience, we restart t from 0 in this stage. Because \u03b3cmax \u2264 \u03b3\u03b7 \u2264 \u03b22t in this regime and 3\u03b3k\u03b22t cmax \u2264 0.1\u03b22t as \u03b3kcmax \u2264 1/k1+ , we can write-\n\u03b2t+1 = (\u03b3cmax + \u03b2 2 t + 4\u03b3kcmax\u03b22t )(1 + 1/k 1+ )\n\u2264 (\u03b22t + \u03b22t + 0.1\u03b22t )(1 + 1/k1+ ) \u2264 2.5\u03b22t\nWe claim that \u03b2t < \u221a \u03b3\u03b7 for t = O(log log(\u03b3\u03b7)\u22121). To verify, note that-\n\u03b2t \u2264 (2.5\u03b20)2 t\n\u2264 (0.25)2t\nNote that (0.25)2 t \u2264 \u221a\u03b3\u03b7 for t = O(log log(\u03b3\u03b7)\u22121) and hence we stay in this stage for at most O(log log(\u03b3\u03b7)\u22121) steps. As \u03b7\u22121 = O(d), this stage continues for at most O(log log d) steps.\n3. Note that in the next step, \u03b2t \u2264 (\u03b3cmax + 1.1\u03b3\u03b7)(1 + 1/k1+ ) \u2264 3\u03b3\u03b7. This is again because 3\u03b3k\u03b22t \u03b7 \u2264 0.1\u03b22t and \u03b2t \u2264 \u221a \u03b3\u03b7 at the end of the previous stage.\nTherefore \u03b2t \u2264 3\u03b3\u03b7 for some t = O(log log d+ log k). By Lemma 6, |a\u0302i,t\u2212 ci,1| \u2264 18\u03b32\u03b72, i 6= 1. Hence |a\u0302i,t| \u2264 2\u03b7. By Lemma 2, the error at convergence satisfies \u2016 Am \u2212 A\u0302m \u20162 \u2264 10\u03b3k\u03b72 and the estimate of the weight w\u0304m satisfies |1\u2212 w\u0302mwm | \u2264 O(\u03b7).\nHence we have shown that if the first (m \u2212 1) factors have converged to Xi = Ai + \u2206\u0302i where \u2016 \u2206\u0302i \u20162 \u2264 10\u03b3k\u03b72, \u2200 i < m then the mth factor converges to Xm = Am + \u2206\u0302m where \u2016 \u2206\u0302m \u20162 \u2264 10\u03b3k\u03b72 in O(log k+ log log d) steps with probability at least ( 1\u2212 log\n5 k k1+\n) . This proves the induction\nhypothesis. We can now do a union bound to argue that each factor converges with `2 error at most O(\u03b3k\u03b7\n2) in O(log k + log log d) with overall failure probability at most O\u0303(1/k\u2212 ), > 0."}, {"heading": "D Proof of additional Lemmas", "text": "In this section, we will prove the initialization condition which we used at several points in the proof of convergence of the tensor power method and Orth-ALS updates. We also provide the proof for a few Lemmas whose proofs were omitted earlier.\nLemma 1. If \u03b3kcmax \u2264 1/k1+ for some > 0, then with probability at least ( 1 \u2212 log 5 k\nk1+\n) ,\n|wiai,0| maxi |wiai,0| \u2264 1\u2212 5/k 1+ \u2200 i 6= arg maxi |wiai,0|.\nProof. Without loss of generality, assume arg maxi |wiai,0| = 1. We will first express all factors in terms of a particular choice of orthonormal basis vectors {vi}, i \u2208 [k]. v1 = A1, and vi is unit vector along the projection of Ai orthogonal to {Aj}, j < i. In terms of this basis, A1 = (1, 0, \u00b7 \u00b7 \u00b7 , 0), let A2 = (x1,2, u2,2, 0 \u00b7 \u00b7 \u00b7 , 0) and in general Ai = (xi,1, xi,2, \u00b7 \u00b7 \u00b7 , xi,i\u22121, ui,i, 0, \u00b7 \u00b7 \u00b7 , 0). We will show that |xi,j | \u2264 O(cmax) for all valid i, j i.e. for all j < i, i \u2208 [k].\nWe claim that |xi,j | \u2264 cmax(1 + jcmax) for all valid i, j. We prove this via induction on j. It is clear that xi,1 \u2264 cmax(1 + cmax) for all valid i as \u3008Ai, A1\u3009 \u2264 cmax, i 6= 1. This proves the base\ncase. The induction step is that xi,j \u2264 cmax(1 + pcmax) for all valid i and j \u2264 p. We show that this implies that xi,p+1 \u2264 cmax(1 + (p + 1)cmax) for all valid i. Note that |\u3008Ai, Ap+1\u3009| \u2264 cmax \u2200 i \u2265 p therefore,\n|up+1,p+1xi,p+1| \u2264 cmax + p\u2211 i=1 c2max(1 + icmax) 2\n\u2264 cmax + c2max p\u2211 i=1 (1 + 4icmax) \u2264 cmax + pc2max + 4p2c3max\nFrom the induction hypothesis, |up+1,p+1| \u2265 1\u2212 2kc2max. This is because |xi,j | \u2264 cmax(1 + jcmax) \u2264 2cmax =\u21d2 \u2211 j x 2 i,j \u2264 4c2max =\u21d2 |up+1,p+1| \u2265 1\u2212 2kc2max. Hence,\n|xi,p+1| \u2264 (cmax + pc2max + 4p2c3max)(1\u2212 2kc2max)\u22121\n\u2264 (cmax + pc2max + 4p2c3max)(1 + 4kc2max) \u2264 cmax + pc2max + 4k2c3max + 4kc3max + 4k2c4max + 16k3c5max \u2264 cmax(1 + (p+ 1)cmax)\nTherefore |xi,j | \u2264 cmax(1 + jcmax) \u2264 2cmax for all valid i, j. Let the random initialization be (t1/r, t2/r, \u00b7 \u00b7 \u00b7 , tk/r) where ti \u223c N(0, 1/d) and r = \u2211 i t 2 i . Let ui = witi. By Lemma 14 with\nprobability at least ( 1\u2212 10 log 4 k\nk1+ ) , \u2223\u2223\u2223 witiw1t1 \u2223\u2223\u2223 \u2264 1\u2212 10/k1+ , > 0 \u2200 i 6= 1. We claim that \u2223\u2223\u2223 wiai,0w1a1,0 \u2223\u2223\u2223 \u2264\n1\u2212 5/k1+ , > 0 \u2200 i 6= 1 whenever \u2223\u2223\u2223 tit1 \u2223\u2223\u2223 \u2264 1\u2212 10/k1+ , > 0 \u2200 i 6= 1. This follows becausewiai,0 w1a1,0 = witiui,i + \u2211 j<i xi,jwjtj wi wj w1t1\n=\u21d2 \u2223\u2223\u2223 wiai,0 w1a1,0 \u2223\u2223\u2223 \u2264 \u2223\u2223\u2223 witi w1t1 \u2223\u2223\u2223+\u2211 j<i 2cmax \u2223\u2223\u2223 witi w1t1 \u2223\u2223\u2223\u03b3 \u2264 1\u2212 10/k1+ + 2\u03b3kcmax \u2264 1\u2212 10/k1+ + 1/k1+\n\u2264 1\u2212 5/k1+\nLemma 14. Let ui \u223c N(0, w2i ), i \u2208 [k] be independent Gaussian random variables. For log 4 k \u2264 h \u2264 k2, with probability at least (\n1\u2212 log 4 k h ) , |wiui|maxi |wiui| \u2264 1\u2212 1/h for all i 6= arg maxi |wiai,0|.\nProof. We refer to the pdf of ui by fi(x). Without loss of generality, assume arg maxi |wiai,0| = 1. As we are only interested in the ratio of the absolute value of random variables {ui}, we will assume without loss of generality that the standard deviations or the weights wi have been scaled such that wi \u2265 1. We will use the following tail bound on the standard Gaussian random variable x (refer to Duembgen (2010))-\ne\u2212t 2/2\n\u221a 2\u03c0 4\u221a 4 + t2 + t\n\u2264 P[|x| > t] \u2264 2e \u2212t2/2\nt \u221a 2\u03c0 (D.1)\nLet \u03ba be a constant which satisfies the following relation\u2013\n1\u2212 3 log k k\n\u2264 \u2211k\ni=1 P[|ui| \u2264 \u03ba] k \u2264 1\u2212 2 log k k\n(D.2)\nLet m = maxi ui. As the ui are independent,\nP[m > \u03ba] = 1\u2212\u03a0iP[|ui| \u2264 \u03ba]\nBy the AM-GM inequality-\n\u03a0iP[|ui| \u2264 \u03ba] \u2264 (\u2211k\ni=1 P[|ui| \u2264 \u03ba] k )k \u2264 (\n1\u2212 2 log k k )k \u2264 1 k2\nHence with failure probability at most 1/k2 the maximum is at least \u03ba. Instead of drawing k samples from the k distributions corresponding to the k factors, we first draw the maximum m from the distribution of the maximum of the k samples, and then draw the remaining samples conditioned on the maximum being m. We have shown that m > \u03ba with high probability. We condition on the maximum m being greater than \u03ba. We now show that with high probability no sample lies in the range [m(1 \u2212 1/h),m], given that the maximum is at least \u03ba. After drawing the maximum from its distribution, we will draw samples from the distributions corresponding to all the k factors even though one of the factors would already be the maximum m. Clearly this can only increase the probability of a sample lying in the interval [m(1\u2212 1/h),m], and as we only want an upper bound this is permissible. Let the conditional pdf of the ith random variable ui conditioned on the maximum being m be gi|m(x). Conditioned on the maximum being m, all remaining samples are at most m and hence gi|m(x) = fi(x)/P[|ui| \u2264 m] for all x \u2264 m and is 0 otherwise. We will now upper bound 1/P[|ui| \u2264 m]. We rely on the following observation about the distribution of a standard Normal random variable x-\nP[|x| \u2264 t] \u2265 { 0.5t t \u2208 [0, 1] 0.5 t > 1\nThe bound for t \u2208 [0, 1] follows from the concavity of the Gaussian cumulative distribution function for t > 0, the bound for t > 1 is easily verified. Using this, we can write\nP[|ui| \u2264 m] \u2265 0.5 min {m wi , 1 }\nWe will now find a upper bound on fi(m(1 \u2212 1/h)), the pdf of the samples at m(1 \u2212 1/h). Let\nti = m wi . Using Eq. D.1-\nfi(m) \u2264\n(\u221a 4 + t2i + ti ) P[|ui| \u2265 m]\n4wi\n=\u21d2 gi|m(m) \u2264 (2ti + 2)P[|ui| \u2265 \u03ba]\n4wiP[|ui| \u2264 m]\n\u2264 (2ti + 2)P[|ui| \u2265 \u03ba] 2wi min { m wi , 1 } \u2264 (ti + 1)P[|ui| \u2265 \u03ba] min{m,wi}\n=\u21d2 \u2211\ni:ti\u2264log k gi|m(m) \u2264 \u2211 i:ti\u2264log k (ti + 1)P[|ui| \u2265 \u03ba] m/ log k\n\u2264 2 log2 k \u2211 i P[|ui| \u2265 \u03ba] m \u2264 6 log3 k/m (D.3)\nwhere we used Eq. D.2 in the last step. We will now relate gi|m(m) and gi|m(m(1\u2212 1/h)). We can write,\ngi|m(m(1\u2212 1/h)) = exp\n( \u2212 m2\n2w2i (1\u2212 2/h+ 1/k2+2 ) ) \u221a\n2\u03c0wiP[|ui| \u2264 m]\n\u2264 exp\n( \u2212 t2i (1\u2212 2/h)/2 ) \u221a\n2\u03c0wiP[|ui| \u2264 m] = ( e\u2212t2i /2\u221a\n2\u03c0wiP[|ui| \u2264 m]\n) et 2 i /h\n= gi|m(m)e t2i /h =\u21d2 \u2211\ni:ti\u2264log k gi|m(m(1\u2212 1/h)) \u2264 \u2211 i:ti\u2264log k gi|m(m)e t2i /h\n\u2264 9 log3 k/m\nwhere we used Eq. D.3 in the last step and the fact that h \u2265 log4 k =\u21d2 et2i /h \u2264 1.5 for ti \u2264 log k. For all i : ti > log k, we can write,\ngi|m(m(1\u2212 1/h)) \u2264 e\u2212t 2 i (1/2\u22121/h)\n\u221a 2\u03c0wiP[|ui| \u2264 m]\n\u2264 2e \u2212t2i /3\n\u221a 2\u03c0min{m,wi}\n\u2264 2e \u2212t2i /3\n\u221a 2\u03c0m/ti\n\u2264 2tie \u2212t2i /3\n\u221a 2\u03c0m \u2264 1/(k2m)\nTherefore, \u2211 i:ti>log k gi|m(m(1\u2212 1/h)) \u2264 1/(km)\n=\u21d2 \u2211 i gi|m(m(1\u2212 1/h)) \u2264 10 log3 k/m\nHence the probability of a sample lying in the interval [m(1\u2212 1/h),m] can be bounded by-\nP[\u222a(ui \u2208 [m(1\u2212 1/h),m])] \u2264 \u2211 i P[ui \u2208 [m(1\u2212 1/h),m]]\n\u2264 \u2211 i gi|m(m(1\u2212 1/h)) m h = m\nh \u2211 i gi|m(m(1\u2212 1/h))\n\u2264 10 log 3 k\nh\nHence with probability at least (1 \u2212 1 k2\n)(1 \u2212 10 log 3 k h ) = 1 \u2212 log4 k h the maximum is greater than \u03ba\nand there are no samples in the interval [m(1\u2212 1/h),m].\nLemma 11. For any connected Eulerian graph G, let N be the number of nodes and M be the number of edges. Consider any decomposition of G into a edge-disjoint set of p cycles. Then, N \u2264 M \u2212 p + 1. Moreover, if G has at least four edge-disjoint paths between a pair of nodes then N \u2264M \u2212 p.\nProof. Consider any decomposition of G into a disjoint set of p cycles C1 \u222a C2 \u00b7 \u00b7 \u00b7 \u222a Cp. We will consider the number of unique nodes in C1 \u222a C2 \u00b7 \u00b7 \u00b7 \u222a Ct for t \u2264 p. Let N(C1 \u222a C2 \u00b7 \u00b7 \u00b7 \u222a Ct) be the number of unique nodes in C1 \u222a C2 \u00b7 \u00b7 \u00b7 \u222a Ct. Similarly, let M(C1 \u222a C2 \u00b7 \u00b7 \u00b7 \u222a Ct) be the number of edges in C1 \u222a C2 \u00b7 \u00b7 \u00b7 \u222a Ct. We will prove the argument by induction on t. The base case for t = 1 is correct as the graph is connected. Assume that statement is for some t. There must be a cycle in C1 \u00b7 \u00b7 \u00b7 \u222a Ct with at least one node common to Ct+1 as G is connected. Then, N(C1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ct \u222a Ct+1) \u2264 N(C1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ct) + N(Ct+1) \u2212 1. Note that N(Ct+1) \u2264 M(Ct+1). Also, by the induction hypothesis, N(C1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ct) \u2264 M(C1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ct) \u2212 t + 1. Therefore, N(C1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ct \u222a Ct+1) \u2264M(C1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ct+1)\u2212 (t+ 1) + 1.\nTo prove the second part of the Lemma, we claim that if N = M\u2212p+1 for some decomposition C of G into a disjoint set of p cycles, then there cannot be more than two edge-disjoint paths between any pair of nodes. By our previous argument, if N = M \u2212 p+ 1 for some decomposition C of G into p cycles, then for any union S of connected cycles in the decomposition, any cycle not in S can have at most one node common with the nodes in S. Note that the number of edge-disjoint set of paths between any pair u and v must be even as the graph is Eulerian. Assume for the sake of contradiction that there are at least four edge-disjoint paths between two nodes u and v. Consider any set of cycles S in C which cover two of the edge-disjoint paths. Say that P is some path which is not covered by S. Note that u and v are present in C but the path P is not present in C. We claim that this implies that there exists some union S \u2032 of cycles such that there is some cycle having two nodes common with S \u2032. To verify this, we simply add cycles to S to grow our subgraph from node u till it reaches node v. At some point, there must be a cycle with two nodes\ncommon to the cycles already selected, because we have to reach the node v which has already been included.\nLemma 2. Let \u03b3kcmax \u2264 1/k1+ . Without loss of generality assume convergence to the first factor A1. Define a\u0302i,t = | ai,ta1,t |- the ratio of the correlation of the ith and 1st factor with the iterate at time t. If a\u0302i,t \u2264 2\u03b7 \u2200 i 6= 1, then \u2016 A1 \u2212 A\u03021 \u20162 \u2264 10\u03b3k\u03b72 in the subsequent iteration. Also, if \u2016 A1 \u2212 A\u03021 \u20162 \u2264 O(\u03b7) then the relative error in the estimation of the weight w1 is at most O(\u03b7).\nProof. Consider any step \u03c4 of the power iterations at the end of which |a\u0302i,\u03c4 | \u2264 2\u03b7 \u2200 i 6= 1. Let the first (largest) factor have true correlation a1,\u03c4 with the iterate at this time step. Consider the next tensor power method update. From the update formula, the result Z\u03c4+1 is-\nZ\u03c4+1 =\n\u2211k i=1w1w\u0302ia 2 1,\u03c4 a\u0302 2 i,\u03c4Ai\n\u2016 \u2211k\ni=1w1w\u0302ia 2 1,\u03c4 a\u0302 2 i,\u03c4Ai \u20162\nLet \u03ba = w1a21,\u03c4 \u2016 \u2211k i=1 w1w\u0302ia 2 1,\u03c4 a\u0302 2 i,\u03c4Ai\u20162 . Hence the estimate at the end of the mth iteration is-\nX\u03c4+1 = \u03ba k\u2211 i=1 w\u0302ia\u0302 2 i,\u03c4Ai\n= \u03ba(A1 + \u2211 i 6=1 w\u0302ia\u0302 2 i,\u03c4Ai)\nDenote \u2211\ni 6=1 w\u0302ia\u0302 2 i,\u03c4Ai = \u2206\u03021. Note that \u2016 \u2206\u03021 \u20162 \u2264 4\u03b3k\u03b72 as |a\u0302i,\u03c4 | \u2264 2\u03b7 =\u21d2 w\u0302ia\u03022i,\u03c4 \u2264 4\u03b3\u03b72 and the\nfactors Ai have unit norm. As \u2016 X\u03c4+1 \u20162 = 1, \u03ba = 1/\u2016 A1 + \u2206\u03021 \u20162. From the triangle inequality,\n1\u2212 \u2016 \u2206\u03021 \u20162 \u2264 \u2016 A1 + \u2206\u03021 \u20162 \u2264 1 + \u2016 \u2206\u03021 \u20162 =\u21d2 1\u2212 4\u03b3k\u03b72 \u2264 \u2016 A1 + \u2206\u03021 \u20162 \u2264 1 + 4\u03b3k\u03b72 =\u21d2 1 1 + 4\u03b3k\u03b72 \u2264 \u03ba \u2264 1 1\u2212 4\u03b3k\u03b72 =\u21d2 1\u2212 4\u03b3k\u03b72 \u2264 \u03ba \u2264 1 + 5\u03b3k\u03b72\nWe can now write the error \u2016 A1 \u2212X\u03c4+1 \u20162 as-\n\u2016 A1 \u2212X\u03c4+1 \u20162 = \u2016 A1 \u2212 \u03ba(A1 + \u2206\u03021) \u20162 = \u2016 A1(1\u2212 \u03ba) + \u03ba\u2206\u03021 \u20162 \u2264 \u2016 A1(1\u2212 \u03ba) \u20162 + \u03ba\u2016 \u2206\u03021 \u20162 \u2264 5\u03b3k\u03b72 + 4\u03b3k\u03b72(1 + 5\u03b3k\u03b72) \u2264 10\u03b3k\u03b72\nWe also show that the error in estimating the weight w1 of factor A1 is small once we have good\nestimate of the factor.\nw\u03041 = \u2211 i wi\u3008A\u03021, Ai\u30093\n= \u2211 i wi\u3008A1 + \u2206\u03021, Ai\u30093\n=\u21d2 |w1 \u2212 w\u03041| \u2264 \u2223\u2223\u2223w1\u3008A1 + \u2206\u03021, A1\u30093 \u2212 w1\u2223\u2223\u2223+ \u2223\u2223\u2223\u2211\ni 6=1 wi\u3008A1 + \u2206\u03021, Ai\u30093 \u2223\u2223\u2223 \u2264 3w1\u03b7 + 8\n\u2211 i wi\u03b7 3 (D.4)\n\u2264 3w1\u03b7 + 8wik\u03b73 \u2264 4w1\u03b7 =\u21d2 \u2223\u2223\u22231\u2212 w\u03041\nw1 \u2223\u2223\u2223 \u2264 O(\u03b7) where Eq. D.4 follows as |ATi \u2206\u03021| \u2264 \u2016 \u2206\u03021 \u20162 \u2264 \u03b7."}], "references": [{"title": "A spectral algorithm for latent dirichlet allocation", "author": ["Animashree Anandkumar", "Yi-kai Liu", "Daniel J Hsu", "Dean P Foster", "Sham M Kakade"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "A tensor approach to learning mixed membership community models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M Kakade"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "Tensor decompositions for learning latent variable models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M Kakade", "Matus Telgarsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "Guaranteed non-orthogonal tensor decomposition via alternating rank-1 updates", "author": ["Animashree Anandkumar", "Rong Ge", "Majid Janzamin"], "venue": "arXiv preprint arXiv:1402.5180,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "Learning overcomplete latent variable models through tensor methods", "author": ["Animashree Anandkumar", "Rong Ge", "Majid Janzamin"], "venue": "In Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "Anandkumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2015}, {"title": "Reinforcement learning of POMDPs using spectral methods", "author": ["Kamyar Azizzadenesheli", "Alessandro Lazaric", "Animashree Anandkumar"], "venue": "In 29th Annual Conference on Learning Theory,", "citeRegEx": "Azizzadenesheli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Azizzadenesheli et al\\.", "year": 2016}, {"title": "Efficient MATLAB computations with sparse and factored tensors", "author": ["Brett W. Bader", "Tamara G. Kolda"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Bader and Kolda.,? \\Q2007\\E", "shortCiteRegEx": "Bader and Kolda.", "year": 2007}, {"title": "Matlab tensor toolbox version 2.5", "author": ["Brett W. Bader", "Tamara G. Kolda"], "venue": "Available online,", "citeRegEx": "Bader and Kolda,? \\Q2012\\E", "shortCiteRegEx": "Bader and Kolda", "year": 2012}, {"title": "A practical randomized CP tensor decomposition", "author": ["Casey Battaglino", "Grey Ballard", "Tamara G Kolda"], "venue": "arXiv preprint arXiv:1701.06600,", "citeRegEx": "Battaglino et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Battaglino et al\\.", "year": 2017}, {"title": "Distributional semantics in technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 136\u2013145", "author": ["Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Bruni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Estimating latent-variable graphical models using moments and likelihoods", "author": ["Arun Tejasvi Chaganty", "Percy Liang"], "venue": "In ICML,", "citeRegEx": "Chaganty and Liang.,? \\Q2014\\E", "shortCiteRegEx": "Chaganty and Liang.", "year": 2014}, {"title": "SPALS: Fast alternating least squares via implicit leverage scores sampling", "author": ["Dehua Cheng", "Richard Peng", "Yan Liu", "Ioakeim Perros"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "FastMotif: spectral sequence motif", "author": ["Nicolo Colombo", "Nikos Vlassis"], "venue": "discovery. Bioinformatics,", "citeRegEx": "Colombo and Vlassis.,? \\Q2015\\E", "shortCiteRegEx": "Colombo and Vlassis.", "year": 2015}, {"title": "Tensor decomposition via joint matrix schur decomposition", "author": ["Nicolo Colombo", "Nikos Vlassis"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Colombo and Vlassis.,? \\Q2016\\E", "shortCiteRegEx": "Colombo and Vlassis.", "year": 2016}, {"title": "Tensor decompositions, alternating least squares and other tales", "author": ["Pierre Comon", "Xavier Luciani", "Andr\u00e9 LF De Almeida"], "venue": "Journal of chemometrics,", "citeRegEx": "Comon et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Comon et al\\.", "year": 2009}, {"title": "A link between the canonical decomposition in multilinear algebra and simultaneous matrix diagonalization", "author": ["Lieven De Lathauwer"], "venue": "SIAM journal on Matrix Analysis and Applications,", "citeRegEx": "Lathauwer.,? \\Q2006\\E", "shortCiteRegEx": "Lathauwer.", "year": 2006}, {"title": "Bounding standard gaussian tail probabilities", "author": ["Lutz Duembgen"], "venue": "arXiv preprint arXiv:1012.2063,", "citeRegEx": "Duembgen.,? \\Q2010\\E", "shortCiteRegEx": "Duembgen.", "year": 2010}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Decomposing overcomplete 3rd order tensors using sum-of-squares algorithms. Approximation, Randomization, and Combinatorial Optimization", "author": ["Rong Ge", "Tengyu Ma"], "venue": "Algorithms and Techniques,", "citeRegEx": "Ge and Ma.,? \\Q2015\\E", "shortCiteRegEx": "Ge and Ma.", "year": 2015}, {"title": "Learning mixtures of gaussians in high dimensions", "author": ["Rong Ge", "Qingqing Huang", "Sham M Kakade"], "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,", "citeRegEx": "Ge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2015}, {"title": "Foundations of the parafac procedure: Models and conditions for an", "author": ["Richard A Harshman"], "venue": "explanatory\u201d multi-modal factor analysis", "citeRegEx": "Harshman.,? \\Q1970\\E", "shortCiteRegEx": "Harshman.", "year": 1970}, {"title": "Tensor rank is NP-Complete", "author": ["Johan H\u030aastad"], "venue": "Journal of Algorithms,", "citeRegEx": "H\u030aastad.,? \\Q1990\\E", "shortCiteRegEx": "H\u030aastad.", "year": 1990}, {"title": "Most tensor problems are NP-Hard", "author": ["Christopher J Hillar", "Lek-Heng Lim"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Hillar and Lim.,? \\Q2013\\E", "shortCiteRegEx": "Hillar and Lim.", "year": 2013}, {"title": "Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors", "author": ["Samuel B Hopkins", "Tselil Schramm", "Jonathan Shi", "David Steurer"], "venue": "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,", "citeRegEx": "Hopkins et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hopkins et al\\.", "year": 2016}, {"title": "Fast detection of overlapping communities via online tensor methods", "author": ["Furong Huang", "UN Niranjan", "Mohammad Umar Hakeem", "Animashree Anandkumar"], "venue": "arXiv preprint arXiv:1309.0787,", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Distributed latent dirichlet allocation via tensor factorization", "author": ["Furong Huang", "Sergiy Matusevych", "Anima Anandkumar", "Nikos Karampatziakis", "Paul Mineiro"], "venue": "In NIPS Optimization Workshop,", "citeRegEx": "Huang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2014}, {"title": "Gigatensor: scaling tensor analysis up by 100 times-algorithms and discoveries", "author": ["U Kang", "Evangelos Papalexakis", "Abhay Harpale", "Christos Faloutsos"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Kang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2012}, {"title": "Tensor decompositions and applications", "author": ["Tamara G Kolda", "Brett W Bader"], "venue": "SIAM review,", "citeRegEx": "Kolda and Bader.,? \\Q2009\\E", "shortCiteRegEx": "Kolda and Bader.", "year": 2009}, {"title": "Shifted power method for computing tensor eigenpairs", "author": ["Tamara G Kolda", "Jackson R Mayo"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Kolda and Mayo.,? \\Q2011\\E", "shortCiteRegEx": "Kolda and Mayo.", "year": 2011}, {"title": "Tensorly: Tensor learning in python", "author": ["Jean Kossaifi", "Yannis Panagakis", "Maja Pantic"], "venue": "arXiv preprint arXiv:1610.09555,", "citeRegEx": "Kossaifi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kossaifi et al\\.", "year": 2016}, {"title": "Three-way arrays: rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics", "author": ["Joseph B Kruskal"], "venue": "Linear algebra and its applications,", "citeRegEx": "Kruskal.,? \\Q1977\\E", "shortCiteRegEx": "Kruskal.", "year": 1977}, {"title": "Tensor factorization via matrix factorization", "author": ["Volodymyr Kuleshov", "Arun Tejasvi Chaganty", "Percy Liang"], "venue": "In AISTATS,", "citeRegEx": "Kuleshov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kuleshov et al\\.", "year": 2015}, {"title": "ICA with reconstruction cost for efficient overcomplete feature learning", "author": ["Quoc V Le", "Alexandre Karpenko", "Jiquan Ngiam", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Le et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Le et al\\.", "year": 2011}, {"title": "A decomposition for three-way arrays", "author": ["SE Leurgans", "RT Ross", "RB Abel"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Leurgans et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Leurgans et al\\.", "year": 1993}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levy and Goldberg.,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Polynomial-time tensor decompositions with sumof-squares", "author": ["Tengyu Ma", "Jonathan Shi", "David Steurer"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Ma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Parcube: Sparse parallelizable tensor decompositions", "author": ["Evangelos E Papalexakis", "Christos Faloutsos", "Nicholas D Sidiropoulos"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Papalexakis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Papalexakis et al\\.", "year": 2012}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Sparse and low-rank tensor decomposition", "author": ["Parikshit Shah", "Nikhil Rao", "Gongguo Tang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Shah et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shah et al\\.", "year": 2015}, {"title": "DMS: Distributed sparse tensor factorization with alternating least squares", "author": ["Shaden Smith", "George Karypis"], "venue": "Technical report,", "citeRegEx": "Smith and Karypis.,? \\Q2015\\E", "shortCiteRegEx": "Smith and Karypis.", "year": 2015}, {"title": "Sublinear time orthogonal tensor decomposition", "author": ["Zhao Song", "David Woodruff", "Huan Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Song et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Song et al\\.", "year": 2016}, {"title": "Joint diagonalization: Is non-orthogonal always preferable to orthogonal", "author": ["Antoine Souloumiac"], "venue": "In 2009 3rd IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP),", "citeRegEx": "Souloumiac.,? \\Q2009\\E", "shortCiteRegEx": "Souloumiac.", "year": 2009}, {"title": "Guaranteed tensor decomposition: A moment approach", "author": ["Gongguo Tang", "Parikshit Shah"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Tang and Shah.,? \\Q2015\\E", "shortCiteRegEx": "Tang and Shah.", "year": 2015}, {"title": "Rubik: Knowledge guided tensor factorization and completion for health data analytics", "author": ["Yichen Wang", "Robert Chen", "Joydeep Ghosh", "Joshua C Denny", "Abel Kho", "You Chen", "Bradley A Malin", "Jimeng Sun"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Fast and guaranteed tensor decomposition via sketching", "author": ["Yining Wang", "Hsiao-Yu Tung", "Alexander J Smola", "Anima Anandkumar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Learning from multiway data: Simple and efficient tensor regression", "author": ["Rose Yu", "Yan Liu"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning", "citeRegEx": "Yu and Liu.,? \\Q2016\\E", "shortCiteRegEx": "Yu and Liu.", "year": 2016}, {"title": "Rank-one approximation to high order tensors", "author": ["Tong Zhang", "Gene H Golub"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Zhang and Golub.,? \\Q2001\\E", "shortCiteRegEx": "Zhang and Golub.", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "From a theoretical perspective, tensor methods have become an incredibly useful and versatile tool for learning a wide array of popular models, including topic modeling (Anandkumar et al., 2012), mixtures of Gaussians (Ge et al.", "startOffset": 169, "endOffset": 194}, {"referenceID": 19, "context": ", 2012), mixtures of Gaussians (Ge et al., 2015), community detection (Anandkumar et al.", "startOffset": 31, "endOffset": 48}, {"referenceID": 10, "context": ", 2014a), learning graphical models with guarantees via the method of moments (Anandkumar et al., 2014b; Chaganty and Liang, 2014) and reinforcement learning (Azizzadenesheli et al.", "startOffset": 78, "endOffset": 130}, {"referenceID": 5, "context": ", 2014b; Chaganty and Liang, 2014) and reinforcement learning (Azizzadenesheli et al., 2016).", "startOffset": 62, "endOffset": 92}, {"referenceID": 30, "context": "The key property of tensors that enables these applications is that tensors have a unique decomposition (decomposition here refers to the most commonly used CANDECOMP/PARAFAC or CP decomposition), under mild conditions on the factor matrices (Kruskal, 1977); for example, tensors have a unique decomposition whenever the factor matrices are full rank.", "startOffset": 242, "endOffset": 257}, {"referenceID": 48, "context": "In fact, we are already seeing exciting applications of tensor methods for analysis of high-order spatiotemporal data (Yu and Liu, 2016), health data analysis (Wang et al.", "startOffset": 118, "endOffset": 136}, {"referenceID": 12, "context": ", 2015a) and bioinformatics (Colombo and Vlassis, 2015).", "startOffset": 28, "endOffset": 55}, {"referenceID": 22, "context": "As tensor decomposition is NP-Hard in the worst-case (Hillar and Lim, 2013; H\u030aastad, 1990), one cannot hope for algorithms which always produce the correct factorization.", "startOffset": 53, "endOffset": 90}, {"referenceID": 21, "context": "As tensor decomposition is NP-Hard in the worst-case (Hillar and Lim, 2013; H\u030aastad, 1990), one cannot hope for algorithms which always produce the correct factorization.", "startOffset": 53, "endOffset": 90}, {"referenceID": 33, "context": "Early work from the 1970\u2019s (Leurgans et al., 1993; Harshman, 1970) established a simple algorithm ar X iv :1 70 3.", "startOffset": 27, "endOffset": 66}, {"referenceID": 20, "context": "Early work from the 1970\u2019s (Leurgans et al., 1993; Harshman, 1970) established a simple algorithm ar X iv :1 70 3.", "startOffset": 27, "endOffset": 66}, {"referenceID": 35, "context": "25) is significantly worse than the best known provable recovery guarantees for polynomial-time algorithms on random tensors\u2014the recent work Ma et al. (2016) succeeds even in the over-complete setting with k = o(d1.", "startOffset": 141, "endOffset": 158}, {"referenceID": 0, "context": "Anandkumar et al. (2014c) had previously shown local convergence of the tensor power method with a linear convergence rate (and also showed global convergence via a SVD-based initialization scheme, obtaining the first guarantees for the tensor power method in non-orthogonal settings).", "startOffset": 0, "endOffset": 26}, {"referenceID": 32, "context": "However, obtaining guaranteed recovery of non-orthogonal tensors using algorithms for orthogonal tensors requires converting the tensor into an orthogonal form (known as whitening) which is ill conditioned in high dimensions (Le et al., 2011; Souloumiac, 2009), and is computationally the most expensive step (Huang et al.", "startOffset": 225, "endOffset": 260}, {"referenceID": 44, "context": "However, obtaining guaranteed recovery of non-orthogonal tensors using algorithms for orthogonal tensors requires converting the tensor into an orthogonal form (known as whitening) which is ill conditioned in high dimensions (Le et al., 2011; Souloumiac, 2009), and is computationally the most expensive step (Huang et al.", "startOffset": 225, "endOffset": 260}, {"referenceID": 24, "context": ", 2011; Souloumiac, 2009), and is computationally the most expensive step (Huang et al., 2013).", "startOffset": 74, "endOffset": 94}, {"referenceID": 13, "context": "Another very interesting line of work on tensor decompositions is to use simultaneous diagonalization and higher order SVD (Colombo and Vlassis, 2016; Kuleshov et al., 2015; De Lathauwer, 2006) but the algorithms typically have global convergence guarantees only for orthogonal tensors, and are not as computationally efficient as alternating minimization2.", "startOffset": 123, "endOffset": 193}, {"referenceID": 31, "context": "Another very interesting line of work on tensor decompositions is to use simultaneous diagonalization and higher order SVD (Colombo and Vlassis, 2016; Kuleshov et al., 2015; De Lathauwer, 2006) but the algorithms typically have global convergence guarantees only for orthogonal tensors, and are not as computationally efficient as alternating minimization2.", "startOffset": 123, "endOffset": 193}, {"referenceID": 35, "context": "Recently, there has been intriguing work on provably decomposing random tensors using the sum-of-squares approach (Ma et al., 2016; Hopkins et al., 2016; Tang and Shah, 2015; Ge and Ma, 2015).", "startOffset": 114, "endOffset": 191}, {"referenceID": 23, "context": "Recently, there has been intriguing work on provably decomposing random tensors using the sum-of-squares approach (Ma et al., 2016; Hopkins et al., 2016; Tang and Shah, 2015; Ge and Ma, 2015).", "startOffset": 114, "endOffset": 191}, {"referenceID": 45, "context": "Recently, there has been intriguing work on provably decomposing random tensors using the sum-of-squares approach (Ma et al., 2016; Hopkins et al., 2016; Tang and Shah, 2015; Ge and Ma, 2015).", "startOffset": 114, "endOffset": 191}, {"referenceID": 18, "context": "Recently, there has been intriguing work on provably decomposing random tensors using the sum-of-squares approach (Ma et al., 2016; Hopkins et al., 2016; Tang and Shah, 2015; Ge and Ma, 2015).", "startOffset": 114, "endOffset": 191}, {"referenceID": 0, "context": "Many algorithms have been proposed for guaranteed decomposition of orthogonal tensors, we refer the reader to Anandkumar et al. (2014b); Kolda and Mayo (2011); Comon et al.", "startOffset": 110, "endOffset": 136}, {"referenceID": 0, "context": "Many algorithms have been proposed for guaranteed decomposition of orthogonal tensors, we refer the reader to Anandkumar et al. (2014b); Kolda and Mayo (2011); Comon et al.", "startOffset": 110, "endOffset": 159}, {"referenceID": 0, "context": "Many algorithms have been proposed for guaranteed decomposition of orthogonal tensors, we refer the reader to Anandkumar et al. (2014b); Kolda and Mayo (2011); Comon et al. (2009); Zhang and Golub (2001).", "startOffset": 110, "endOffset": 180}, {"referenceID": 0, "context": "Many algorithms have been proposed for guaranteed decomposition of orthogonal tensors, we refer the reader to Anandkumar et al. (2014b); Kolda and Mayo (2011); Comon et al. (2009); Zhang and Golub (2001). However, obtaining guaranteed recovery of non-orthogonal tensors using algorithms for orthogonal tensors requires converting the tensor into an orthogonal form (known as whitening) which is ill conditioned in high dimensions (Le et al.", "startOffset": 110, "endOffset": 204}, {"referenceID": 0, "context": "Many algorithms have been proposed for guaranteed decomposition of orthogonal tensors, we refer the reader to Anandkumar et al. (2014b); Kolda and Mayo (2011); Comon et al. (2009); Zhang and Golub (2001). However, obtaining guaranteed recovery of non-orthogonal tensors using algorithms for orthogonal tensors requires converting the tensor into an orthogonal form (known as whitening) which is ill conditioned in high dimensions (Le et al., 2011; Souloumiac, 2009), and is computationally the most expensive step (Huang et al., 2013). Another very interesting line of work on tensor decompositions is to use simultaneous diagonalization and higher order SVD (Colombo and Vlassis, 2016; Kuleshov et al., 2015; De Lathauwer, 2006) but the algorithms typically have global convergence guarantees only for orthogonal tensors, and are not as computationally efficient as alternating minimization2. Recently, there has been intriguing work on provably decomposing random tensors using the sum-of-squares approach (Ma et al., 2016; Hopkins et al., 2016; Tang and Shah, 2015; Ge and Ma, 2015). Ma et al. (2016) show that a sum-of-squares based relaxation can decompose highly overcomplete random tensors of rank De Lathauwer (2006) prove unique recovery under very general conditions, but their algorithm is quite complex and requires solving a linear system of size O(d), which is prohibitive for large tensors.", "startOffset": 110, "endOffset": 1104}, {"referenceID": 0, "context": "Many algorithms have been proposed for guaranteed decomposition of orthogonal tensors, we refer the reader to Anandkumar et al. (2014b); Kolda and Mayo (2011); Comon et al. (2009); Zhang and Golub (2001). However, obtaining guaranteed recovery of non-orthogonal tensors using algorithms for orthogonal tensors requires converting the tensor into an orthogonal form (known as whitening) which is ill conditioned in high dimensions (Le et al., 2011; Souloumiac, 2009), and is computationally the most expensive step (Huang et al., 2013). Another very interesting line of work on tensor decompositions is to use simultaneous diagonalization and higher order SVD (Colombo and Vlassis, 2016; Kuleshov et al., 2015; De Lathauwer, 2006) but the algorithms typically have global convergence guarantees only for orthogonal tensors, and are not as computationally efficient as alternating minimization2. Recently, there has been intriguing work on provably decomposing random tensors using the sum-of-squares approach (Ma et al., 2016; Hopkins et al., 2016; Tang and Shah, 2015; Ge and Ma, 2015). Ma et al. (2016) show that a sum-of-squares based relaxation can decompose highly overcomplete random tensors of rank De Lathauwer (2006) prove unique recovery under very general conditions, but their algorithm is quite complex and requires solving a linear system of size O(d), which is prohibitive for large tensors.", "startOffset": 110, "endOffset": 1225}, {"referenceID": 0, "context": "Many algorithms have been proposed for guaranteed decomposition of orthogonal tensors, we refer the reader to Anandkumar et al. (2014b); Kolda and Mayo (2011); Comon et al. (2009); Zhang and Golub (2001). However, obtaining guaranteed recovery of non-orthogonal tensors using algorithms for orthogonal tensors requires converting the tensor into an orthogonal form (known as whitening) which is ill conditioned in high dimensions (Le et al., 2011; Souloumiac, 2009), and is computationally the most expensive step (Huang et al., 2013). Another very interesting line of work on tensor decompositions is to use simultaneous diagonalization and higher order SVD (Colombo and Vlassis, 2016; Kuleshov et al., 2015; De Lathauwer, 2006) but the algorithms typically have global convergence guarantees only for orthogonal tensors, and are not as computationally efficient as alternating minimization2. Recently, there has been intriguing work on provably decomposing random tensors using the sum-of-squares approach (Ma et al., 2016; Hopkins et al., 2016; Tang and Shah, 2015; Ge and Ma, 2015). Ma et al. (2016) show that a sum-of-squares based relaxation can decompose highly overcomplete random tensors of rank De Lathauwer (2006) prove unique recovery under very general conditions, but their algorithm is quite complex and requires solving a linear system of size O(d), which is prohibitive for large tensors. We ran the simultaneous diagonalization algorithm of Kuleshov et al. (2015) on a dimension 100, rank 30 tensor; and the algorithm needed around 30 minutes to run, whereas Orth-ALS converges in less than 5 seconds.", "startOffset": 110, "endOffset": 1482}, {"referenceID": 43, "context": "Very recently, there has been exciting work on scalable tensor decomposition algorithms using ideas such as sketching (Song et al., 2016; Wang et al., 2015b) and contraction of tensor problems to matrix problems (Shah et al.", "startOffset": 118, "endOffset": 157}, {"referenceID": 41, "context": ", 2015b) and contraction of tensor problems to matrix problems (Shah et al., 2015).", "startOffset": 63, "endOffset": 82}, {"referenceID": 8, "context": "Also worth noting are recent approaches to speedup ALS via sampling and randomized least squares (Battaglino et al., 2017; Cheng et al., 2016; Papalexakis et al., 2012).", "startOffset": 97, "endOffset": 168}, {"referenceID": 11, "context": "Also worth noting are recent approaches to speedup ALS via sampling and randomized least squares (Battaglino et al., 2017; Cheng et al., 2016; Papalexakis et al., 2012).", "startOffset": 97, "endOffset": 168}, {"referenceID": 39, "context": "Also worth noting are recent approaches to speedup ALS via sampling and randomized least squares (Battaglino et al., 2017; Cheng et al., 2016; Papalexakis et al., 2012).", "startOffset": 97, "endOffset": 168}, {"referenceID": 27, "context": "ALS is the most widely used algorithm for tensor decomposition and has been described as the \u201cworkhorse\u201d for tensor decomposition (Kolda and Bader, 2009).", "startOffset": 130, "endOffset": 153}, {"referenceID": 29, "context": "There are several publicly available optimized packages implementing ALS, such as (Kossaifi et al., 2016; Vervliet et al.; Bader et al., 2012; Bader and Kolda, 2007; Smith and Karypis; Huang et al., 2014; Kang et al., 2012).", "startOffset": 82, "endOffset": 223}, {"referenceID": 6, "context": "There are several publicly available optimized packages implementing ALS, such as (Kossaifi et al., 2016; Vervliet et al.; Bader et al., 2012; Bader and Kolda, 2007; Smith and Karypis; Huang et al., 2014; Kang et al., 2012).", "startOffset": 82, "endOffset": 223}, {"referenceID": 25, "context": "There are several publicly available optimized packages implementing ALS, such as (Kossaifi et al., 2016; Vervliet et al.; Bader et al., 2012; Bader and Kolda, 2007; Smith and Karypis; Huang et al., 2014; Kang et al., 2012).", "startOffset": 82, "endOffset": 223}, {"referenceID": 26, "context": "There are several publicly available optimized packages implementing ALS, such as (Kossaifi et al., 2016; Vervliet et al.; Bader et al., 2012; Bader and Kolda, 2007; Smith and Karypis; Huang et al., 2014; Kang et al., 2012).", "startOffset": 82, "endOffset": 223}, {"referenceID": 14, "context": "Despite the advantages, ALS does not have any global convergence guarantees and can get stuck in local optima (Comon et al., 2009; Kolda and Bader, 2009), even under very realistic settings.", "startOffset": 110, "endOffset": 153}, {"referenceID": 27, "context": "Despite the advantages, ALS does not have any global convergence guarantees and can get stuck in local optima (Comon et al., 2009; Kolda and Bader, 2009), even under very realistic settings.", "startOffset": 110, "endOffset": 153}, {"referenceID": 0, "context": "Anandkumar et al. (2014c) showed that the tensor power method converges locally (i.", "startOffset": 0, "endOffset": 26}, {"referenceID": 45, "context": "Similar to other works (Tang and Shah, 2015; Anandkumar et al., 2014c), our guarantees for tensor decomposition depend on the incoherence of the factor matrices (cmax), defined to be the maximum correlation in absolute value between any two factors, i.", "startOffset": 23, "endOffset": 70}, {"referenceID": 0, "context": "This contrasts with the analysis of Anandkumar et al. (2014c) who showed a linear rate of convergence (O(log d) steps) for random tensors, provided an SVD based initialization is employed.", "startOffset": 36, "endOffset": 62}, {"referenceID": 33, "context": "We also test the classical technique for tensor decomposition via simultaneous diagonalization (Leurgans et al., 1993; Harshman, 1970) (also known as Jennrich\u2019s algorithm, we refer to it as Sim-Diag), which first performs two random projections of the tensor, and then recovers the factors by an eigenvalue decomposition of the projected matrices.", "startOffset": 95, "endOffset": 134}, {"referenceID": 20, "context": "We also test the classical technique for tensor decomposition via simultaneous diagonalization (Leurgans et al., 1993; Harshman, 1970) (also known as Jennrich\u2019s algorithm, we refer to it as Sim-Diag), which first performs two random projections of the tensor, and then recovers the factors by an eigenvalue decomposition of the projected matrices.", "startOffset": 95, "endOffset": 134}, {"referenceID": 4, "context": "Recovering over-complete tensors: Overcomplete tensors are tensors with rank higher than the dimension, and have found numerous theoretical applications in learning latent variable models (Anandkumar et al., 2015).", "startOffset": 188, "endOffset": 213}, {"referenceID": 34, "context": "Current methods for learning word embeddings implicitly (Mikolov et al., 2013b; Levy and Goldberg, 2014) or explicitly (Pennington et al.", "startOffset": 56, "endOffset": 104}, {"referenceID": 40, "context": ", 2013b; Levy and Goldberg, 2014) or explicitly (Pennington et al., 2014) factorize some matrix derived from the matrix of word co-occurrences M , where Mij denotes how often word i appears with word j.", "startOffset": 48, "endOffset": 73}, {"referenceID": 40, "context": "This scaling is known to perform well in practice for co-occurrence matrices (Pennington et al., 2014), and makes some intuitive sense in light of the Zipfian distribution of word frequencies.", "startOffset": 77, "endOffset": 102}, {"referenceID": 9, "context": "The word similarity tasks (Bruni et al., 2012; Finkelstein et al., 2001) contain word pairs along with human assigned similarity scores, and the objective is to maximize the correlation between the similarity in the embeddings of the two words (according to a similarity metric such as the dot product) and human judged similarity.", "startOffset": 26, "endOffset": 72}, {"referenceID": 17, "context": "The word similarity tasks (Bruni et al., 2012; Finkelstein et al., 2001) contain word pairs along with human assigned similarity scores, and the objective is to maximize the correlation between the similarity in the embeddings of the two words (according to a similarity metric such as the dot product) and human judged similarity.", "startOffset": 26, "endOffset": 72}, {"referenceID": 0, "context": "The tensor power method update equations can be written as (refer to Anandkumar et al. (2014c))", "startOffset": 69, "endOffset": 95}, {"referenceID": 16, "context": "We will use the following tail bound on the standard Gaussian random variable x (refer to Duembgen (2010))-", "startOffset": 90, "endOffset": 106}], "year": 2017, "abstractText": "The popular Alternating Least Squares (ALS) algorithm for tensor decomposition is extremely efficient, but often converges to poor local optima, particularly when the weights of the factors are non-uniform. We propose a modification of the ALS approach that is as efficient as standard ALS, but provably recovers the true factors with random initialization under standard incoherence assumptions on the factors of the tensor. We demonstrate the significant practical superiority of our approach over traditional ALS (with both random initialization and SVDbased initialization) for a variety of tasks on synthetic data\u2014including tensor factorization on exact, noisy and over-complete tensors, as well as tensor completion\u2014and for computing word embeddings from a third-order word tri-occurrence tensor.", "creator": "LaTeX with hyperref package"}}}