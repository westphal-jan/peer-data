{"id": "1703.01827", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "All You Need is Beyond a Good Init: Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and Modulation", "abstract": "Deep neural network is difficult to train and this predicament becomes worse as the depth increases. The essence of this problem exists in the magnitude of backpropagated errors that will result in gradient vanishing or exploding phenomenon. We show that a variant of regularizer which utilizes orthonormality among different filter banks can alleviate this problem. The current neural network has a deep convolutional neural network and an average of about 15/100 neurons of each group to be generated. This makes the neural network better, allowing it to learn to perform in more complex ways. The problems here are an extremely deep neural network which is only partially able to learn as well as to operate correctly in the network. There is very little evidence of a neural network such as the Deep Convolutional Neural Network (DNN). The Deep Convolutional Neural Network (DNN) was developed by Dr. Poulay and his group of neurosurgeons at the National Institute of Mental Health in Boston, and consists of 20 trained neural network experts. The basic structure of the neural network is the neural network and the neural network's core features are the underlying architecture. A single set of neurons is involved in performing a task. These neurons are placed on the network and then reorganized into clusters. The neurons of a cluster are trained with the same neural network, which determines the size of the neural network. The main learning process in the neural network consists of learning to act as a feedback loop. Once the brain learns a block size (block size) of that block size it becomes more and more necessary to make the network more robust. This can only occur if the network's input size has an equal or less equal probability of learning to its maximum of about 10/100 neurons. A block size of a network of 10 or fewer is about one block size per neuron. The neural network's input size should be about 20/100 neurons per neuron.\nWe show that if the task in question is a block size of one, the neural network's input size will be around 15/100 neurons per neuron. This will not be very strong in comparison to the size in which it is expected. Since we are not able to perform the task in any way, we may need some other inputs which have the same input size. One thing we have learned is that it will not only make some blocks but it can also provide an adequate amount of data to our neural network, which is not too expensive, but will be required to perform the task in a way that is similar to that for a", "histories": [["v1", "Mon, 6 Mar 2017 11:54:43 GMT  (989kb,D)", "http://arxiv.org/abs/1703.01827v1", "accepted by CVPR2017"], ["v2", "Thu, 6 Apr 2017 08:22:09 GMT  (990kb,D)", "http://arxiv.org/abs/1703.01827v2", "Updating experiments; CVPR2017"], ["v3", "Mon, 10 Apr 2017 02:12:29 GMT  (990kb,D)", "http://arxiv.org/abs/1703.01827v3", "Updating experiments; CVPR2017"]], "COMMENTS": "accepted by CVPR2017", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["di xie", "jiang xiong", "shiliang pu"], "accepted": false, "id": "1703.01827"}, "pdf": {"name": "1703.01827.pdf", "metadata": {"source": "CRF", "title": "All You Need is Beyond a Good Init: Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and Modulation", "authors": ["Di Xie", "Jiang Xiong", "Shiliang Pu"], "emails": ["xiedi@hikvision.com", "xiongjiang@hikvision.com", "pushiliang@hikvision.com"], "sections": [{"heading": null, "text": "Besides, we propose new principles for designing network structure from the insights evoked by orthonormality. Combined with residual structure, we achieve comparative performance on the ImageNet dataset."}, {"heading": "1. Introduction", "text": "Deep convolutional neural networks have improved performance across a wider variety of computer vision tasks, especially for image classification [17, 34, 39, 31, 45], object detection [42, 26, 33] and segmentation [20, 5, 25]. Much of this improvement should give the credit to gradually deeper network architectures. In just four years, the layer number of networks escalates from several to hundreds, which learns more abstract and expressive repre-\nsentations from large amount of data, e.g. [27]. Simply stacking more layers onto current architectures is not a reasonable solution, which incurs vanishing/exploding gradients [4, 9]. To handle the relatively shallower networks, a variety of initialization and normalization methodologies are proposed [9, 30, 12, 37, 15, 22, 13, 1], while deep residual learning [11] is utilized to deal with extremely deep ones.\nThough other works, e.g. [36, 35], have also announced that they can train an extremely deep network with improved performance, deep residual network [11] is still the best and most practical solution for dealing with the degradation of training accuracy as depth increases. However, it is substantial that residual networks are exponential ensembles of relatively shallow ones (usually only 10-34 layers deep), as an interpretation by Veit et al. [41], it avoids the vanishing/exploding gradient problem instead of resolving it directly. Intrinsically, the performance gain of networks is determined by its multiplicity, not the depth. So how to train an ultra-deep network is still an open research question with which few works concern. Most researches still focus on designing more complicated structures based on residual block and its variants [18, 43]. Anyway, dose there exist an applicable methodology that can be used for training a genuinely deep network?\nIn this paper, we try to find a direct feasible solution to answer above question. We think batch normalization (BN) [13] is necessary to ensure the propagation stability in the forward pass in ultra-deep networks and the key of learning availability exists in the backward pass which propagates errors with a top-down way. We constrain the network\u2019s structure to repetitive modules consisted by Convolution, BN and ReLU [23] layers (Fig. 1) and analyze the Jacobian of the output with respect to the input between consecutive modules. We show that BN cannot guarantee the magnitude of errors to be stable in the backward pass and this amplification/attenuation effect to signal will ac-\n1\nar X\niv :1\n70 3.\n01 82\n7v 1\n[ cs\n.C V\n] 6\nM ar\ncumulate layer-wisely which results in gradients exploding/vanishing. From the view of norm-preserving, we find that keeping the orthonormality between filter banks within a layer during learning process is a sufficient and necessary condition to ensure the stability of backward errors. While this condition cannot be satisfied in nonlinear networks equipped with BN, this orthonormal constrain can mitigate backward signal\u2019s attenuation and we prove it by experiments. An orthonormal regularizer is introduced to replace traditional weight decay regularization [8]. Experiments show that there is 3% \u223c 4% gains for a 44-layer network on CIFAR-10.\nHowever, as depth increases, e.g. deeper than 100 layers, the non-orthogonal impact induced by BN, ReLU and gradients updating accumulates, which breaks the dynamic isometry [30] and makes learning unavailable. To neutralize this impact, we design a modulation mechanism based on the quasi-isometry assumption between two consecutive parametric layers. We show the quasi-isometry property with both mathematical analysis and experiments. With the modulation, a global scale factor can be applied on the magnitude of errors a little unscrupulously during the backward pass in a layer-wise fashion. Combined with orthonormality, experiments show that a plain CNN shown in Fig. 1 can be trained relatively well and match the performance of its residual counterpart.\nThe contributions of this paper are summarized as follows. 1) We demonstrate the necessity of applying BN and explain the potential reason which results in degradation problem in optimizing deep CNNs; 2) A concise methodology equipped with orthonormality and modulation is proposed to provide more insights to understand learning dynamics of CNNs; 3) Experiments and analysis exhibit inter-\nesting phenomenons and promising research directions."}, {"heading": "2. Related Work", "text": "Initialization in Neural Networks. As depth increases, Gaussian initialization cannot suffice to train a network from scratch [34]. The two most prevalent works are proposed by Glorot & Bengio [9] and He et al. [12] respectively. The core idea of their works is to keep the unit variance of each layer\u2019s output. Sussillo & Abbott [37] propose a novel random walk initialization and mainly focus on adjusting the so-called scalar factor g to make the ratio of input/output error to be constant around 1. Kra\u0308henbu\u0308hl et al. [15] introduce data-dependent initialization to ensure all layers training at an equal rate.\nOrthogonality is also in consideration. Saxe et al. [29, 30] analyse the dynamics of learning in linear deep neural networks. They find that the convergence rate of random orthogonal initialization of weights is equivalent to unsupervised pre-training, which are both superior to random Gaussian initialization. LSUV initialization method [22] is proposed which not only takes advantage of orthonormality but also makes use of the unit-variance of each layer\u2019s output.\nIn our opinion, a well-behaved initialization is not enough to resist the variation as learning progresses, which is to say, to have a good initial condition (e.g. isometry) cannot ensure the preferred condition to keep unchanged all the time, especially in extremely deep networks. This argument forms the basic idea that motivates us to explore the solutions for genuinely deep networks.\nSignal Propagation Normalization. Normalization is a common and ubiquitous technique in machine learning community. The whitening and decorrelation of input data brings benefits to both deep learning and other machine learning algorithms, which helps speeding up the training process [19]. Batch normalization [13] generalize this idea to ensure each layer\u2019s output to be identical distributions which reduce the internal covariate shift. Weight normalization [28] is inspired by BN by decoupling the norm of the weight vector from its direction while introducing independencies between the examples in a minibatch. To overcome the disadvantage of BN that dependent on minibatch size, layer normalization [2] is proposed to solve the normalization problem for recurrent neural networks. But this method cannot be applied to CNN, as the assumption violates the statistics of the hidden layers. For more applicable in CNN, Arpit et al. introduce normalization propagation [1] to reduce the internal covariate shift for convolutional layers and even rectified linear units. The idea of normalization each layers\u2019 activations is promising, but a little idealistic in practice. Since the incoherence prior of weight matrix is actually not true in the initialization phase and even worsen in iterations, the normalized magnitude of each layer\u2019s activa-\ntions cannot be guaranteed in an extremely deep network. In our implementation, it even cannot prevent the exploding activations\u2019 magnitude just after initialization.\nSignal Modulation. Few work is done in this field explicitly, but implicitly integrated the idea of modulation. In a broad sense, modulation can be viewed as a persistent process of the combination of normalization and other methodology to keep the magnitude of a variety of signals steady at learning. With this understanding, we can summarize all the methods above with a unified framework, e.g. batch normalization [13] for activation modulation, weight normalization [28] for parameter modulation, etc."}, {"heading": "3. Methodology", "text": ""}, {"heading": "3.1. Why is BN a requisite?", "text": "Since the complexity dynamics of learning in nonlinear neural networks [30], even a proven mathematical theory cannot guarantee that a variety of signals keeping isometrical at the same time in practice applications. Depth itself results in the \u201cbutterfly effect\u201d with exponential diffusion while nonlinear gives rise to indefiniteness and randomness. Recently proposed methods [1, 37, 15] which utilize isometry fail to keep the steady propagation of signals in over100-layer networks. These methods try to stabilize the magnitude of signals from one direction (forward/backward) as a substituted way to control the signals in both directions. However, since the complexity variations of signals, it is impossible to have conditions held on both ways with just one modulation method.\nAn alternative option is to simplify this problem to constrain the magnitude of signals in either direction, which we can pay the whole attention to another direction1. Batch normalization is an existed solution that satisfies our requirement. It does normalization in the forward pass to reduce internal covariate shift with a layer-wise way2, which, in our opinion, make us to focus all the analyses on the opposite direction.\nFrom [13], during the backpropagation of the gradient of loss ` through BN, we can formulate errors between adjacent layers as follow:\n\u2202 `\n\u2202 xi = 1\u221a \u03c32B + (\u03b4i \u2212 \u00b5\u03b4 \u2212 x\u0302i m m\u2211 j=1 \u03b4j x\u0302j) (1)\nwhere xi is ith sample in a mini-batch (we omit activation 1For a specified weight that connected ith neuron in lth layer and kth neuron in (l + 1)th layer, w(l)ij , its gradient can be computed as \u2207 w(l)ij = a (l) i \u00d7 \u03b4 (l+1) j . If the two variables are independent from each other, then the magnitude of gradient can be directly related with just one factor (activation/error).\n2Methods modulate signals without a layer-wise manner, e.g. [1], will accumulate the indefiniteness with a superlinear way and finally the propagated signals will be out of control.\nindex for simplicity), so \u2202 `\u2202 xi denotes output error. \u03b4i = \u2202 ` \u2202 yi \u00b7\u03b3 where \u2202 `\u2202 yi is the input error and \u03b3 is scale parameter\nof BN. \u00b5\u03b4 = 1m \u2211m i=1 \u03b4i is mean of scaled input errors, where m denotes mini-batch\u2019s size. x\u0302i = xi\u2212\u00b5B\u221a \u03c32B+ is the corresponding normalized activation. Equation 1 represents a kind of \u201cpseudo-normalization\u201d transformation for error signals \u03b4i compared with its forward operation. If the mean of distribution of input error \u03b4i is zero and symmetric, we can infer that the mean of distribution of output error is approximately zero. It centralizes the errors and the last term x\u0302im \u2211m j=1 \u03b4j x\u0302j will bias the distribution but these biases may be cancelled out from each other owing to the normalized coefficient x\u0302i which is normal distribution. Besides, errors are normalized with a mismatched variance. This type of transformation will change error signal\u2019s original distribution with a layer-wise way since the second order moment of each layer\u2019s output errors loses its isometry progressively. However, this phenomenon can be ignored when we only consider a pair of consecutive layers. In a sense, we can think the backward propagated errors are also normalized as well as its forward pass, which is why we apply \u201cConv-BN-ReLU\u201d triple instead of \u201cConv-ReLU-BN\u201d3.\nThe biased distribution effect will accumulated as depth increases and distort input signals\u2019 original distribution, which is one of several reasons that make training extreme deep neural network difficult. In next section we try to solve the problem to some extent."}, {"heading": "3.2. Orthonormality", "text": "Norm-preserving resides in the core idea of this section. A vector x \u2208 <dx is mapped by a linear transformation W \u2208 <dx\u00d7 dy to another vector y \u2208 <dy , say, y = WT x. If \u2016y\u2016 = \u2016x\u2016, then we call this transformation norm-preserving. Obviously, orthonormality, not the normalization proposed by [1] alone, is both sufficient and necessary for holding this equation, since\n\u2016y\u2016 = \u221a yT y = \u221a xTWWT x = \u221a\nxT x = \u2016x\u2016 iff.WWT = I (2)\nGiven the precondition that signals in forward pass are definitely normalized, here we can analyse the magnitude variation of errors only in backward pass. To keep the gradient with respect to the input of previous layer normpreserving, it is straightforward to conclude that we would better maintain orthonormality among columns4 of a weight matrix in a specific layer during learning process rather than at initialization according to Eq. 2, which equivalently\n3Another reason is that placing ReLU after BN guarantees approximately 50% activations to be nonzero, while the ratio may be unstable if putting it after convolution operation.\n4Beware of the direction, which results in the exchange of notations in equation 2. So the rows and columns of the matrix are also exchanged.\nmakes the Jacobian to be ideally dynamical isometry [30]. Obviously in CNN this property cannot be ensured because of 1) the gradient update which makes the correlation among different columns of weights stronger as learning proceeding; 2) nonlinear operations, such as BN and ReLU, which destroy the orthonormality. However, we think it is reasonable to force the learned parameters to be conformed with the orthogonal group as possible, which can alleviate vanishing/exploding phenomenon of the magnitude of errors and the signal distortion after accumulated nonlinear transformation. The rationality of these statements and hypotheses has been proved by experiments.\nTo adapt the orthonormality for convolutional operations, we generalize the orthogonal expression with a direct modification. Let W\u0303l \u2208 <W\u00d7 H\u00d7 C\u00d7 M denote a set of convolution kernels in lth layer, where W , H , C, M are width, height, input channel number and output channel number, respectively. We replace original weight decay regularizer with the orthonormal regularizer:\n\u03bb\n2 D\u2211 i=1 \u2016WTl Wl \u2212 I\u20162F (3)\nwhere \u03bb is the regularization coefficient as weight decay, D is total number of convolutional layers and/or fully connected layers, I is the identity matrix and Wl \u2208 <fin\u00d7 fout where fin = W \u00d7 H\u00d7 C and fout = M . \u2016 \u00b7 \u2016F represents the Frobenius norm. In other words, equation 3 constraints orthogonality among filters in one layer, which makes the learned features have minimum correlation with each other, thus implicitly reduce the redundancy and enhance the diversity among the filters, especially those from the lower layers [32].\nBesides, orthonormality constraints provide alternative solution other than L2 regularization to the exploration of weight space in learning process. It provides more probabilities by limiting set of parameters in an orthogonal space instead of inside a hypersphere."}, {"heading": "3.3. Modulation", "text": "The dynamical isometry of signal propagation in neural networks has been mentioned and underlined several times [1, 30, 13], and it amounts to maintain the singular values of Jacobian, say J = \u2202y\u2202x , to be around 1. In this section, we will analyze the variation of singular values of Jacobian through different types of layers in detail. We omit the layer index and bias term for simplicity and clarity.\nFor linear case, we have y = WT x, which shows that having dynamical isometry is equivalent to keep orthogonality since J = WT and JJT = WTW.\nNext let us consider the activations after normalization transformation, y = BN\u03b3,\u03b2(WT x), which we borrow the notation from [13]. Given the assumption that input dimen-\nsion equals output dimension and both are d-dimension vectors, the Jacobian is\nJ =  J11 0 \u00b7 \u00b7 \u00b7 0 0 J22 \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 0 0 \u00b7 \u00b7 \u00b7 Jdd  md\u00d7md\n(4)\nwhere each Jkk is a m\u00d7m square matrix, that is\nJkk =  \u2202y (k) 1 \u2202x (k) 1 \u2202y (k) 1 \u2202x (k) 2 \u00b7 \u00b7 \u00b7 \u2202y (k) 1 \u2202x (k) m \u2202y (k) 2 \u2202x (k) 1 \u2202y (k) 2 \u2202x (k) 2 \u00b7 \u00b7 \u00b7 \u2202y (k) 2 \u2202x (k) m ... ... . . . ...\n\u2202y(k)m \u2202x\n(k) 1\n\u2202y(k)m \u2202x\n(k) 2\n\u00b7 \u00b7 \u00b7 \u2202y (k) m\n\u2202x (k) m\n (5)\nHere \u2202y (k) i\n\u2202x (k) j\ndenotes partial derivative of output of ith sample\nwith respect to jth sample in kth component. The Jacobian of BN has its speciality that its partial derivatives are not only related with components of activations, but also with samples in one mini-batch. Because of each component k of activations is transformed independently by BN, J can be expressed with a blocked diagonal matrix as Eq. 4. Again since the independence among activations, we can analyse just one of d sub-Jacobians, e.g. Jkk.\nFrom equation 1 we can get the entries of Jkk, which is\n\u2202yj \u2202xi = \u03c1\n[ \u2206(i = j)\u2212 1 + x\u0302ix\u0302j\nm\n] (6)\nwhere \u03c1 = \u03b3\u221a \u03c32B+ and \u2206(\u00b7) is the indicator operator. Here we still omit index k since dropping it brings no ambiguity.\nEq. 6 concludes obviously that JJT 6= I. So the orthonormality is not held after BN operation. Now the correlation among columns of W is directly impacted by normalized activations, while the corresponding weights determine these activations in turn, which results in a complicated situation. Fortunately, we can deduce the preferred equation according to subadditivity of matrix rank [3], which is\nJ = PT \u03c1  1\u2212 \u03bb1m 0 0 \u00b7 \u00b7 \u00b7 0 0 1\u2212 \u03bb2m 0 \u00b7 \u00b7 \u00b7 0 0 0 1 \u00b7 \u00b7 \u00b7 0 ... ... ... . . . ...\n0 0 0 \u00b7 \u00b7 \u00b7 1  m\u00d7m P (7)\nwhere P is the matrix consists of eigenvectors of J. \u03bb1 and \u03bb2 are two nonzero eigenvalues of U, say Uij = 1 + x\u0302ix\u0302j , i = 1 \u00b7 \u00b7 \u00b7m, j = 1 \u00b7 \u00b7 \u00b7m.\nEq. 7 shows us that JJT \u2248 \u03c12I 5. The approximation comes from first two diagonal entries in Eq. 7 which may\n5The Jacobian after ReLU is amount to multiply a scalar with J [1], which we can merge it into \u03c1 instead.\nbe close to zero. We think it is one of reasons that violate the perfect dynamic isometry and result in the degradation problem with this kind of non-full rank. Since value of \u03c1 is determined by \u03b3 and \u03c3B , it is bounded as long as these two variables keep stable during the learning process, which achieves the so-called quasi-isometry [6].\nNotice that \u03c1 changes with \u03b3 and \u03c3B while \u03b3 and \u03c3B will change in every iteration. Based on the observation, we propose the scale factor \u03c1 should be adjusted dynamically instead of fixing it like [1, 37, 30]. According to [30], when the nonlinearity is odd, so that the mean activity in each layer is approximately 0, neural population variance, or second order moment of output errors, can capture these dynamical properties quantitatively. ReLU nonlinearity is not satisfied but owing to the pseudo-normalization we can regard the errors propagated backwardly through BN as having zero mean, which makes the second order moment statistics reasonable."}, {"heading": "4. Implementation Details", "text": "We insist to keep the orthonormality throughout the training process, so we implement this constraint both at initialization and in regularization. For a convolution parameter Wl \u2208 <fin\u00d7 fout of lth layer, we initialize subset of W, say fin-dimension vectors, on the first output channel. Then Gram-Schmidt process is applied to sequentially generate next orthogonal vectors channel by channel. Mathematically, generating n orthogonal vectors in d-dimension space which satisfies n > d is ill-posed and, hence, impossible. So one solution is to avoid the fan-ins and fan-outs of kernels violating the principle, say fin \u2265 fout, in designing structures of networks; another candidate is group-wise orthogonalization proposed by us. If fin < fout, we divide the vectors into foutfin + foutmodfin groups, orthogonalization is implemented within each group independently. We do not encourage the hybrid utilization of L2 regularization for those parameters of fin < fout and orthonormal regularization for those of fin \u2265 fout. Forcing parameters to retract into inconsistent manifolds may cause convergence problems. Details can be referred in experiments.\nFor signal modulation, we compute the second order moment statistics of output errors between consecutive parametric layers (convolutional layer in our case) in each iteration. The scale factor \u03c1 is defined as the square root of ratio of second order moment of higher layer, say ql+1, to that, say ql, of lower layer. However, if we modulate all the layers as long as ql+1 6= ql, then the magnitude of propagated signal will tend to be identical with the input error signal, which probably eliminate the variety encoded in the error signal. So we make a trade-off that the modulation only happens when the magnitudes of propagated signals of consecutive layers mismatch. Experiments show that it is a relatively reasonable and non-extreme modulation mechanism\nwhich has a capability of maintaining magnitude constancy for error signals."}, {"heading": "5. Experiments", "text": "First of all, we must demonstrate that the core idea of this paper is to show that the proposed methods can be used to train extremely deep and plain CNNs and improve the performance drastically compared against prevalent stochastic gradient descent (SGD) with L2 regularization rather than achieving state-of-the-art performance in a certain dataset by all manner of means. Moreover, we try to show that the degradation problem of training a plain network reported in [11, 10, 35] can be partially solved by our methods."}, {"heading": "5.1. Datasets and Protocols", "text": "Two representative datasets, CIFAR-10 [16] and ImageNet [27], are used in our experiments.\nCIFAR-10. CIFAR-10 consists of 60, 000 32 \u00d7 32 real world color images in 10 classes split into 50, 000 train and 10, 000 test images. All present experiments are trained on the training set and evaluated on the test set. Top-1 accuracy is evaluated.\nImageNet 2012 classification. For large-scale dataset, ImageNet 2012 classification dataset is used in our experiments. It consists of 1000 classes and there are 1.28 million training images and 50k validation images. Both top-1 and top-5 error rates are evaluated.\nProtocol of CIFAR-10. To demonstrate that our proposed method can partially solve the degradation problem and show that the gap between deeper plain network and the shallower one can be shrunk or even removed, we aim to have fair comparison6 with the plain network in [11]. So we directly adopt their proposed architectures with minor modifications for both plain networks and residual networks. Specifically, the network inputs are 32\u00d7 32 images, with the per-pixel mean subtracted and standard deviation divided. The first layer is 3 \u00d7 3 convolution and then following a stack of 6n 3 \u00d7 3 convolution layers, in which each convolution layer is accompanied by a BN layer and a ReLU layer (Fig. 1). While in the residual case, when size of feature maps doubles, e.g. 16 to 32, we use 3 \u00d7 3 projection shortcuts instead of identity ones. All the hyperparameters such as weight decay, momentum and learning rate are identical with [11]. Horizontal flip is the only data augmentation.\nProtocol of ImageNet 2012 classification. The architectures in this protocol are also with a slight variation. Detailed architectures can be referred in Table 1. The hyperparameters are identical with those of CIFAR-10 protocol.\n6 Here \u201cfair comparison\u201d means the baseline we tend to compare is not from the authors\u2019 results in their paper, while is implemented by ourselves. So having a minor different architecture does not matter.\n224\u00d7224 crops are randomly sampled on 256\u00d7256 images plus horizontal flip and color augmentation [17]. Mean of RGB is subtracted then scaling with a factor 0.017 (standard deviation of RGB). The mini-batch size is 256. Only the performances on validation set are reported."}, {"heading": "5.2. Orthonormality Regularization Enhances the", "text": "Magnitude of Signals\nIn this section, we design experiments to show that orthonormality can indeed enhance the magnitude of propagated signals in deep plain networks through decorrelating learned weights among different channels. A 44-layer plain network and CIFAR-10 dataset is adopted.\nFirst we make statistics of average correlation among different channels over all the layers between two types of methods, say \u201cmsra\u201d [12] initialization plus L2 regularization (abbr. as \u201cmsra+L2 reg\u201d) and our proposed orthonormal initialization and orthonormality regularization (abbr. as \u201cortho init+ortho reg\u201d). Cosine distance Dcos(x, y) is considered to compute this value:\ns\u0304 = 1\nN D\u2211 l=1 fout\u2211 i=1 fout\u2211 j=i Dcos(v (l) i , v (l) j ) (8)\nwhere v(l)i \u2208 <fin denotes ith kernel of Wl in lth layer and N is total computation count. From Fig. 2 we can see the variation of correlation among weights with iterations. Under the constraints of orthonormality, correlation of learned weights are forced into a consistent and relatively lower level (about 6 \u00d7 10\u22123). On the contrary, \u201cmsra+L2 reg\u201d cannot prevent from increasing correlation among weights as learning progresses. Finally, the correlation of \u201cmsra+L2 reg\u201d is about 2.5 times higher than that of \u201cortho init+ortho reg\u201d, which demonstrates the effectiveness of orthonormality constraints.\nNext we make statistics of variation of second order moments of back-propagated errors. Since the empirical risk\n0 1 2 3 4 5 6 7\nx 10 4\n\u22122\n0\n2\n4\n6\n8\n10\n12\n14\n16 x 10\n\u22123 variation of correlation of weights during learning\niterations\nav er\nag e\nco si\nne d\nis ta\nnc e\nmsra + L2 reg ortho init+ortho reg\nwill convergence as learning progresses, which results in smaller magnitude of loss value hence unscaled magnitude of error signals, we actually plot the ratio of second order moment of output error signals (input errors of first convolution layer) to that of input error signals (input errors of last convolution layer). Fig. 3 tells us that in first training phase\n(when learning rate is relatively large) the evolution of signal propagation is more insensitive than the in second and third training phases (when learning rate is small) because of mismatched order of magnitudes between learning rate and decay coefficient of regularizer (0.1 to 0.0001). However, it shows the advantage of orthonormal regularization against L2 regularization no matter in which phase, especially in later phases. The magnitude of propagated signals is enhanced one order of magnitude by orthonormality. It is important to note that we omit the ratios of first five iterations in Fig. 3 since the disproportional order of magnitude. An interesting phenomenon is that all the magnitude of error signals is vanishing, e.g. ratio is less than 1, except for the initialization phase, in which the signals are amplified. We think randomness plays the key role for this phenomenon and it also provides evidence that makes us introduce orthonormality beyond initialization in optimizing extremely deep networks."}, {"heading": "5.3. The Rationality of Modulation", "text": "In this section, we present our findings in training deep plain networks and aim to demonstrate modulation is a promising mechanism to train genuinely deep networks.\nWe find that a 44-layer network can be trained well just with orthonormality but a 110-layer one incurs seriously divergence, which states the accumulation effect mentioned in Sec. 3.1 by evidence. The proposed modulation is applied to train the 110-layer network and achieves 50% performance improvement against other one-order methods (see Table 2). The training methodology is a little tricky that we first apply with both orthonormality and modulation at the first n iterations, then the signals are regulated only through orthonormality until it converges. Keeping the magnitude of error signals to be isometric can easily be done by our modulation, but it is observed that this strategy undermines propagation of signals (81.6% vs. 73.5% on CIFAR-10). So when and how to modulation is an interesting and key research topic to totally solve the degradation problem.\nIn this paper the value of n is somewhat heuristic, which is derived from our observation to the evolution of ratios of second-order moment of output errors of each layer to the second-order moment of input errors at each iteration of training a 44-layer network. Fig. 4 reveals that it probably exists a potential evolution pattern in training deep networks. Actually we just shrink the degradation gap instead of eliminating it in training genuinely deep networks and one of our future work will focus on the methodology of modulation."}, {"heading": "5.4. Results of Plain and Residual Network Architecture", "text": "To prove our proposed method has advantage against other methods integrated with the idea of adaptivity in train-\ning extremely deep plain networks, we compare it with six prevalent one-order methods in this section. We do not compare with second-order methods in consideration of implementation and memory practicality. Table 2 shows the performances. We can see that most methods cannot handle relatively shallow networks well other than SGD and ours and all the methods except for ours cannot even converge in the deeper version. As pointed by [38], most one-order methods can only be a very effective method for optimizing certain types of deep learning architectures. So next we will focus on making comparison against more general SGD method. We also do not compare our method with other modulation methods, e.g. [1], because of they will fail convergence at the very first few iteration in such deep architecture7.\nThen we compare the performance with different regularizer in identical network architecture (ortho vs. L2 of a plain network), and further compare the performance of plain networks with residual networks have similar architectures (plain network with orthonormality vs. residual network with L2). Results are shown in Fig. 5. We can conclude that our proposed method has distinct advantage in optimizing plain networks and the orthonormality indeed can enhance magnitude of signal which alleviates gradient vanishing in training process.\nTo emphasize that orthonormality can be general to prevalent network architectures and large-scale datasets, we extend the experiments on ImageNet dataset. From Fig. 5 it shows the decreasing performance boost in ResNet-34 and\n7Actually, for NormProp [1], the magnitudes of both forward and backward signals explode in our implementation which results in NaN.\nalmost comparative performance in ResNet-110. Compared with architectures on CIFAR-10, they have more channels, e.g. 64 vs. 2048, which introduces more redundancies among intra-layer\u2019s filter banks. Fig. 6 can be used to explain above results, so it probably be difficult for orthonormality to explore in parameter space with so many redundancies. The right sub-figure in Fig. 6 shows more noise-like feature maps than the left one, which inspires us to design thinner architectures in the future work."}, {"heading": "6. Discussion and Conclusion", "text": "Recently we find that [21] has proposed similar ideas. They unify three types of kernel normalization methods into a geometric framework called kernel submanifolds, in which sphere, oblique and compact Stiefel manifolds (orthonormal kernels) are considered. The differences exists in three aspects: 1) The intrinsic explanations about the performance improvement is different, of which they mainly focus on regularization of models with data augmentation and learning of models endowed with geometric invariants; 2) The orthogonalization is different, of which they orthog-\nonalize convolutional kernels within a channel while we do this among channel; 3) As the second statement tells, we believe that their proposed method still cannot handle the extremely deep plain networks. Besides, all the details and key steps to implement their methods are ambiguous that prevents from understanding and verifying it further.\nIntrinsically, one can regard our proposed modulation as assigning each parametric layer an individual and adaptive learning rate. This kind of modulation can be more practical than local methods, e.g. second-order methods, while be more flexible than global ones, e.g. SGD. Besides, if we can approach some strategies to compensate the evanescent orthonomality as learning progresses, we believe that training a genuinely deep network will be available.\nWe propose a simple and direct method to train extremely deep plain networks with orthonormality and modulation. Furthermore, orthonormality reveals its generalization capability which can be applied in residual networks. Great performance boost is observed in experiments. However, the degradation problem is still not totally solved, which may be on condition understanding more comprehensively about the insights of signal modulation, reparametrization and novel constraints, etc. We hope our\nwork will encourage more attentions on this problem."}, {"heading": "7. Appendix: Quasi-isometry inference with", "text": "Batch Normalization\nFor batch normalization (BN) layer, its Jacobian, denoted as J, is not only related with components of activations (d components in total), but also with samples in one mini-batch (size of m).\nLet x(k)j and y (k) i be kth component of jth input sample and ith output sample respectively and given the independence between different components, \u2202y (k) i\n\u2202x (k) j\nis one of m2d\nnonzero entries of J. In fact, J is a tensor but we can express it as a blocked matrix:\nJ =  D11 D12 \u00b7 \u00b7 \u00b7 D1m D21 D22 \u00b7 \u00b7 \u00b7 D2m ... ... . . . ...\nDm1 Dm2 \u00b7 \u00b7 \u00b7 Dmm  (9) where each Dij is a d\u00d7 d diagonal matrix:\nDij =  \u2202y (1) i \u2202x (1) j \u2202y (2) i \u2202x (2) j . . . \u2202y\n(d) i \u2202x (d) j\n (10)\nSince BN is a component-wise rather than sample-wise transformation, we prefer to analyse a variant of Eq. 9 instead of Dij . Note that by elementary matrix transformation, the m2 d\u00d7 d matrices can be converted into d m\u00d7m\nmatrices:\nJ =  J11 0 \u00b7 \u00b7 \u00b7 0 0 J22 \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 0 0 \u00b7 \u00b7 \u00b7 Jdd\n (11)\nand the entries of each Jkk is\n\u2202yj \u2202xi = \u03c1\n[ \u2206(i = j)\u2212 1 + x\u0302ix\u0302j\nm\n] (12)\nThe notations of \u03c1, \u2206(\u00b7) and x\u0302k have been explained in our main paper and here we omit the component index k for clarity. Base on the observation of Eq. 12, we separate the numerator of latter part and denote it as Uij = 1 + x\u0302ix\u0302j .\nLet x\u0302 = (x\u03021, x\u03022, ..., x\u0302m)T , e = (1, 1, ..1)T , we have\nU = eeT + x\u0302x\u0302T (13)\nand\nJkk = \u03c1(I\u2212 1\nm U) (14)\nRecall that for any column vector v, rank(vvT ) = 1. According to the subadditivity of matrix rank [3], it implies that\nrank(U) = rank(eeT + x\u0302x\u0302T ) \u2264\nrank(eeT ) + rank(x\u0302x\u0302T ) = 2 (15)\nEq. 15 tells us that U actually only has two nonzero eigenvalues, say \u03bb1 and \u03bb2, and we can formulate U as follow:\nU = PT  \u03bb1 \u03bb2 0\n. . . 0\nP (16)\ncombined with Eq. 14, finally we get the equation of Jkk from the eigenvalue decomposition view, which is\nJ = PT \u03c1  1\u2212 \u03bb1m 1\u2212 \u03bb2m 1\n. . . 1\nP (17)\nTo show that Jkk probably is not full rank, we formulate\nthe relationship between U2 and U\nU2 = (eeT + x\u0302x\u0302T )(eeT + x\u0302x\u0302T ) = eeT eeT + eeT x\u0302x\u0302T\n+x\u0302x\u0302T eeT + x\u0302x\u0302T x\u0302x\u0302T = meeT + ( m\u2211 i=1 x\u0302i)ex\u0302T\n+( m\u2211 i=1 x\u0302i)x\u0302eT + ( m\u2211 i=1 x\u03022i )x\u0302x\u0302 T\n= mU + ( m\u2211 i=1 x\u0302i)ex\u0302T + ( m\u2211 i=1 x\u0302i)x\u0302eT + ( m\u2211 i=1 x\u03022i \u2212m)x\u0302x\u0302 T\n(18) Note that x\u0302i \u223c N(0, 1), so we can regard the one-order and second-order accumulated items in Eq. 18 as approximately equaling the corresponding one-order and secondorder statistical moments for relatively large mini-batch, from which we get U2 \u2248 mU.\nThe relationship implies that \u03bb21 \u2248 m\u03bb1 and \u03bb22 \u2248 m\u03bb2. Since \u03bb1 and \u03bb2 cannot be zeros, it concludes that \u03bb1 \u2248 \u03bb2 \u2248 m therefor 1 \u2212 \u03bb1m \u2248 0 and 1 \u2212 \u03bb2 m \u2248 0 if batch size is sufficient in a statistical sense."}], "references": [{"title": "Normalization propagation: A parametric technique for removing internal covariate shift in deep networks", "author": ["D. Arpit", "Y. Zhou", "B.U. Kota", "V. Govindaraju"], "venue": "ICML", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Layer normalization", "author": ["J.L. Ba", "J.R. Kiros", "G.E. Hinton"], "venue": "arXiv:1607.06450", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Linear algebra and matrix analysis for statistics", "author": ["S. Banerjee", "A. Roy"], "venue": "Crc Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks, 5(2):157\u2013166", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1994}, {"title": "Deeplab: Semantic image segmentation with deep convolutional nets", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "atrous convolution, and fully connected crfs. arXiv preprint arXiv:1606.00915", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "and G", "author": ["D.J. Collins", "R.I. Grigorchuk", "P.F. Kurchanov", "P.M. Cohn"], "venue": "(Mathematik). Combinatorial Group Theory, Applications to Geometry. Springer", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Regularization theory and neural networks architectures", "author": ["F. Girosi", "M. Jones", "T. Poggio"], "venue": "Neural Computation, 7(2):219\u2013269", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1995}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Journal of Machine Learning Research, 9:249\u2013256", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Convolutional neural networks at constrained time cost", "author": ["K. He", "J. Sun"], "venue": "CVPR, pages 5353\u20135360", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv:1512.03385", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "pages 1026\u20131034", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "ICML", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "International Conference for Learning Representations", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Datadependent initializations of convolutional neural networks", "author": ["P. Kr\u00e4henb\u00fchl", "C. Doersch", "J. Donahue", "T. Darrell"], "venue": "arXiv:1511.06856", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Tech Report", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 25(2)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient backprop", "author": ["Y. Lecun", "L. Bottou", "G.B. Orr", "K.R. Mller"], "venue": "Neural Networks Tricks of the Trade, 1524(1):9\u2013 50", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2000}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3431\u20133440", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimization on submanifolds of convolution kernels in cnns", "author": ["O. Mete", "O. Takayuki"], "venue": "arXiv:1610.07008", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "All you need is a good init", "author": ["D. Mishkin", "J. Matas"], "venue": "Tetrahedron, 69(14):3013\u20133018", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "A method of solving a convex programming problem with convergence rate O(1/  \u221a k)", "author": ["Y. Nesterov"], "venue": "Soviet Mathematics Doklady", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1983}, {"title": "Learning to refine object segments", "author": ["P.O. Pinheiro", "T.-Y. Lin", "R. Collobert", "P. Doll\u00e1r"], "venue": "arXiv preprint arXiv:1603.08695", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1\u20131", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV), 115(3):211\u2013252", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["T. Salimans", "D.P. Kingma"], "venue": "arXiv:1602.07868", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning hierarchical category structure in deep neural networks", "author": ["A. Saxe", "J. McClelland", "S. Ganguli"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["A.M. Saxe", "J.L. McClelland", "S. Ganguli"], "venue": "arXiv:1312.6120", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Overfeat: Integrated recognition", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. Lecun"], "venue": "localization and detection using convolutional networks. ICLR", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding and improving convolutional neural networks via concatenated rectified linear units", "author": ["W. Shang", "K. Sohn", "D. Almeida", "H. Lee"], "venue": "arXiv:1603.05201", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Training region-based object detectors with online hard example mining", "author": ["A. Shrivastava", "A. Gupta", "R.B. Girshick"], "venue": "CoRR, abs/1604.03540", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Highway networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "arXiv:1505.00387", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Training very deep networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "arXiv:1507.06228", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Random walk initialization for training very deep feedforward networks", "author": ["D. Sussillo", "L.F. Abbott"], "venue": "arXiv:1412.6558", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet"], "venue": "CVPR", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning. Technical report", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Residual networks are exponential ensembles of relatively shallow networks", "author": ["A. Veit", "M. Wilber", "S. Belongie"], "venue": "arXiv:1605.06431", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Craft objects from images", "author": ["B. Yang", "J. Yan", "Z. Lei", "S.Z. Li"], "venue": "arXiv preprint arXiv:1604.03239", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Adadelta: An adaptive learning rate method", "author": ["M. Zeiler"], "venue": "arXiv preprint", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "ECCV", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "Deep convolutional neural networks have improved performance across a wider variety of computer vision tasks, especially for image classification [17, 34, 39, 31, 45], object detection [42, 26, 33] and segmentation [20, 5, 25].", "startOffset": 146, "endOffset": 166}, {"referenceID": 32, "context": "Deep convolutional neural networks have improved performance across a wider variety of computer vision tasks, especially for image classification [17, 34, 39, 31, 45], object detection [42, 26, 33] and segmentation [20, 5, 25].", "startOffset": 146, "endOffset": 166}, {"referenceID": 37, "context": "Deep convolutional neural networks have improved performance across a wider variety of computer vision tasks, especially for image classification [17, 34, 39, 31, 45], object detection [42, 26, 33] and segmentation [20, 5, 25].", "startOffset": 146, "endOffset": 166}, {"referenceID": 29, "context": "Deep convolutional neural networks have improved performance across a wider variety of computer vision tasks, especially for image classification [17, 34, 39, 31, 45], object detection [42, 26, 33] and segmentation [20, 5, 25].", "startOffset": 146, "endOffset": 166}, {"referenceID": 42, "context": "Deep convolutional neural networks have improved performance across a wider variety of computer vision tasks, especially for image classification [17, 34, 39, 31, 45], object detection [42, 26, 33] and segmentation [20, 5, 25].", "startOffset": 146, "endOffset": 166}, {"referenceID": 40, "context": "Deep convolutional neural networks have improved performance across a wider variety of computer vision tasks, especially for image classification [17, 34, 39, 31, 45], object detection [42, 26, 33] and segmentation [20, 5, 25].", "startOffset": 185, "endOffset": 197}, {"referenceID": 24, "context": "Deep convolutional neural networks have improved performance across a wider variety of computer vision tasks, especially for image classification [17, 34, 39, 31, 45], object detection [42, 26, 33] and segmentation [20, 5, 25].", "startOffset": 185, "endOffset": 197}, {"referenceID": 31, "context": "Deep convolutional neural networks have improved performance across a wider variety of computer vision tasks, especially for image classification [17, 34, 39, 31, 45], object detection [42, 26, 33] and segmentation [20, 5, 25].", "startOffset": 185, "endOffset": 197}, {"referenceID": 18, "context": "Deep convolutional neural networks have improved performance across a wider variety of computer vision tasks, especially for image classification [17, 34, 39, 31, 45], object detection [42, 26, 33] and segmentation [20, 5, 25].", "startOffset": 215, "endOffset": 226}, {"referenceID": 4, "context": "Deep convolutional neural networks have improved performance across a wider variety of computer vision tasks, especially for image classification [17, 34, 39, 31, 45], object detection [42, 26, 33] and segmentation [20, 5, 25].", "startOffset": 215, "endOffset": 226}, {"referenceID": 23, "context": "Deep convolutional neural networks have improved performance across a wider variety of computer vision tasks, especially for image classification [17, 34, 39, 31, 45], object detection [42, 26, 33] and segmentation [20, 5, 25].", "startOffset": 215, "endOffset": 226}, {"referenceID": 25, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Simply stacking more layers onto current architectures is not a reasonable solution, which incurs vanishing/exploding gradients [4, 9].", "startOffset": 128, "endOffset": 134}, {"referenceID": 8, "context": "Simply stacking more layers onto current architectures is not a reasonable solution, which incurs vanishing/exploding gradients [4, 9].", "startOffset": 128, "endOffset": 134}, {"referenceID": 8, "context": "To handle the relatively shallower networks, a variety of initialization and normalization methodologies are proposed [9, 30, 12, 37, 15, 22, 13, 1], while deep residual learning [11] is utilized to deal with extremely deep ones.", "startOffset": 118, "endOffset": 148}, {"referenceID": 28, "context": "To handle the relatively shallower networks, a variety of initialization and normalization methodologies are proposed [9, 30, 12, 37, 15, 22, 13, 1], while deep residual learning [11] is utilized to deal with extremely deep ones.", "startOffset": 118, "endOffset": 148}, {"referenceID": 11, "context": "To handle the relatively shallower networks, a variety of initialization and normalization methodologies are proposed [9, 30, 12, 37, 15, 22, 13, 1], while deep residual learning [11] is utilized to deal with extremely deep ones.", "startOffset": 118, "endOffset": 148}, {"referenceID": 35, "context": "To handle the relatively shallower networks, a variety of initialization and normalization methodologies are proposed [9, 30, 12, 37, 15, 22, 13, 1], while deep residual learning [11] is utilized to deal with extremely deep ones.", "startOffset": 118, "endOffset": 148}, {"referenceID": 14, "context": "To handle the relatively shallower networks, a variety of initialization and normalization methodologies are proposed [9, 30, 12, 37, 15, 22, 13, 1], while deep residual learning [11] is utilized to deal with extremely deep ones.", "startOffset": 118, "endOffset": 148}, {"referenceID": 20, "context": "To handle the relatively shallower networks, a variety of initialization and normalization methodologies are proposed [9, 30, 12, 37, 15, 22, 13, 1], while deep residual learning [11] is utilized to deal with extremely deep ones.", "startOffset": 118, "endOffset": 148}, {"referenceID": 12, "context": "To handle the relatively shallower networks, a variety of initialization and normalization methodologies are proposed [9, 30, 12, 37, 15, 22, 13, 1], while deep residual learning [11] is utilized to deal with extremely deep ones.", "startOffset": 118, "endOffset": 148}, {"referenceID": 0, "context": "To handle the relatively shallower networks, a variety of initialization and normalization methodologies are proposed [9, 30, 12, 37, 15, 22, 13, 1], while deep residual learning [11] is utilized to deal with extremely deep ones.", "startOffset": 118, "endOffset": 148}, {"referenceID": 10, "context": "To handle the relatively shallower networks, a variety of initialization and normalization methodologies are proposed [9, 30, 12, 37, 15, 22, 13, 1], while deep residual learning [11] is utilized to deal with extremely deep ones.", "startOffset": 179, "endOffset": 183}, {"referenceID": 34, "context": "[36, 35], have also announced that they can train an extremely deep network with improved performance, deep residual network [11] is still the best and most practical solution for dealing with the degradation of training accuracy as depth increases.", "startOffset": 0, "endOffset": 8}, {"referenceID": 33, "context": "[36, 35], have also announced that they can train an extremely deep network with improved performance, deep residual network [11] is still the best and most practical solution for dealing with the degradation of training accuracy as depth increases.", "startOffset": 0, "endOffset": 8}, {"referenceID": 10, "context": "[36, 35], have also announced that they can train an extremely deep network with improved performance, deep residual network [11] is still the best and most practical solution for dealing with the degradation of training accuracy as depth increases.", "startOffset": 125, "endOffset": 129}, {"referenceID": 39, "context": "[41], it avoids the vanishing/exploding gradient problem instead of resolving it directly.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "We think batch normalization (BN) [13] is necessary to ensure the propagation stability in the forward pass in ultra-deep networks and the key of learning availability exists in the backward pass which propagates errors with a top-down way.", "startOffset": 34, "endOffset": 38}, {"referenceID": 21, "context": "We constrain the network\u2019s structure to repetitive modules consisted by Convolution, BN and ReLU [23] layers (Fig.", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "An orthonormal regularizer is introduced to replace traditional weight decay regularization [8].", "startOffset": 92, "endOffset": 95}, {"referenceID": 28, "context": "deeper than 100 layers, the non-orthogonal impact induced by BN, ReLU and gradients updating accumulates, which breaks the dynamic isometry [30] and makes learning unavailable.", "startOffset": 140, "endOffset": 144}, {"referenceID": 32, "context": "As depth increases, Gaussian initialization cannot suffice to train a network from scratch [34].", "startOffset": 91, "endOffset": 95}, {"referenceID": 8, "context": "The two most prevalent works are proposed by Glorot & Bengio [9] and He et al.", "startOffset": 61, "endOffset": 64}, {"referenceID": 11, "context": "[12] respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Sussillo & Abbott [37] propose a novel random walk initialization and mainly focus on adjusting the so-called scalar factor g to make the ratio of input/output error to be constant around 1.", "startOffset": 18, "endOffset": 22}, {"referenceID": 14, "context": "[15] introduce data-dependent initialization to ensure all layers training at an equal rate.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29, 30] analyse the dynamics of learning in linear deep neural networks.", "startOffset": 0, "endOffset": 8}, {"referenceID": 28, "context": "[29, 30] analyse the dynamics of learning in linear deep neural networks.", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "LSUV initialization method [22] is proposed which not only takes advantage of orthonormality but also makes use of the unit-variance of each layer\u2019s output.", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "The whitening and decorrelation of input data brings benefits to both deep learning and other machine learning algorithms, which helps speeding up the training process [19].", "startOffset": 168, "endOffset": 172}, {"referenceID": 12, "context": "Batch normalization [13] generalize this idea to ensure each layer\u2019s output to be identical distributions which reduce the internal covariate shift.", "startOffset": 20, "endOffset": 24}, {"referenceID": 26, "context": "Weight normalization [28] is inspired by BN by decoupling the norm of the weight vector from its direction while introducing independencies between the examples in a minibatch.", "startOffset": 21, "endOffset": 25}, {"referenceID": 1, "context": "To overcome the disadvantage of BN that dependent on minibatch size, layer normalization [2] is proposed to solve the normalization problem for recurrent neural networks.", "startOffset": 89, "endOffset": 92}, {"referenceID": 0, "context": "introduce normalization propagation [1] to reduce the internal covariate shift for convolutional layers and even rectified linear units.", "startOffset": 36, "endOffset": 39}, {"referenceID": 12, "context": "batch normalization [13] for activation modulation, weight normalization [28] for parameter modulation, etc.", "startOffset": 20, "endOffset": 24}, {"referenceID": 26, "context": "batch normalization [13] for activation modulation, weight normalization [28] for parameter modulation, etc.", "startOffset": 73, "endOffset": 77}, {"referenceID": 28, "context": "Since the complexity dynamics of learning in nonlinear neural networks [30], even a proven mathematical theory cannot guarantee that a variety of signals keeping isometrical at the same time in practice applications.", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "Recently proposed methods [1, 37, 15] which utilize isometry fail to keep the steady propagation of signals in over100-layer networks.", "startOffset": 26, "endOffset": 37}, {"referenceID": 35, "context": "Recently proposed methods [1, 37, 15] which utilize isometry fail to keep the steady propagation of signals in over100-layer networks.", "startOffset": 26, "endOffset": 37}, {"referenceID": 14, "context": "Recently proposed methods [1, 37, 15] which utilize isometry fail to keep the steady propagation of signals in over100-layer networks.", "startOffset": 26, "endOffset": 37}, {"referenceID": 12, "context": "From [13], during the backpropagation of the gradient of loss ` through BN, we can formulate errors between adjacent layers as follow:", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "[1], will accumulate the indefiniteness with a superlinear way and finally the propagated signals will be out of control.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Obviously, orthonormality, not the normalization proposed by [1] alone, is both sufficient and necessary for holding this equation, since", "startOffset": 61, "endOffset": 64}, {"referenceID": 28, "context": "makes the Jacobian to be ideally dynamical isometry [30].", "startOffset": 52, "endOffset": 56}, {"referenceID": 30, "context": "In other words, equation 3 constraints orthogonality among filters in one layer, which makes the learned features have minimum correlation with each other, thus implicitly reduce the redundancy and enhance the diversity among the filters, especially those from the lower layers [32].", "startOffset": 278, "endOffset": 282}, {"referenceID": 0, "context": "The dynamical isometry of signal propagation in neural networks has been mentioned and underlined several times [1, 30, 13], and it amounts to maintain the singular values of Jacobian, say J = \u2202y \u2202x , to be around 1.", "startOffset": 112, "endOffset": 123}, {"referenceID": 28, "context": "The dynamical isometry of signal propagation in neural networks has been mentioned and underlined several times [1, 30, 13], and it amounts to maintain the singular values of Jacobian, say J = \u2202y \u2202x , to be around 1.", "startOffset": 112, "endOffset": 123}, {"referenceID": 12, "context": "The dynamical isometry of signal propagation in neural networks has been mentioned and underlined several times [1, 30, 13], and it amounts to maintain the singular values of Jacobian, say J = \u2202y \u2202x , to be around 1.", "startOffset": 112, "endOffset": 123}, {"referenceID": 12, "context": "Next let us consider the activations after normalization transformation, y = BN\u03b3,\u03b2(W x), which we borrow the notation from [13].", "startOffset": 123, "endOffset": 127}, {"referenceID": 2, "context": "Fortunately, we can deduce the preferred equation according to subadditivity of matrix rank [3], which is", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "5The Jacobian after ReLU is amount to multiply a scalar with J [1], which we can merge it into \u03c1 instead.", "startOffset": 63, "endOffset": 66}, {"referenceID": 5, "context": "Since value of \u03c1 is determined by \u03b3 and \u03c3B , it is bounded as long as these two variables keep stable during the learning process, which achieves the so-called quasi-isometry [6].", "startOffset": 175, "endOffset": 178}, {"referenceID": 0, "context": "Based on the observation, we propose the scale factor \u03c1 should be adjusted dynamically instead of fixing it like [1, 37, 30].", "startOffset": 113, "endOffset": 124}, {"referenceID": 35, "context": "Based on the observation, we propose the scale factor \u03c1 should be adjusted dynamically instead of fixing it like [1, 37, 30].", "startOffset": 113, "endOffset": 124}, {"referenceID": 28, "context": "Based on the observation, we propose the scale factor \u03c1 should be adjusted dynamically instead of fixing it like [1, 37, 30].", "startOffset": 113, "endOffset": 124}, {"referenceID": 28, "context": "According to [30], when the nonlinearity is odd, so that the mean activity in each layer is approximately 0, neural population variance, or second order moment of output errors, can capture these dynamical properties quantitatively.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "Moreover, we try to show that the degradation problem of training a plain network reported in [11, 10, 35] can be partially solved by our methods.", "startOffset": 94, "endOffset": 106}, {"referenceID": 9, "context": "Moreover, we try to show that the degradation problem of training a plain network reported in [11, 10, 35] can be partially solved by our methods.", "startOffset": 94, "endOffset": 106}, {"referenceID": 33, "context": "Moreover, we try to show that the degradation problem of training a plain network reported in [11, 10, 35] can be partially solved by our methods.", "startOffset": 94, "endOffset": 106}, {"referenceID": 15, "context": "Two representative datasets, CIFAR-10 [16] and ImageNet [27], are used in our experiments.", "startOffset": 38, "endOffset": 42}, {"referenceID": 25, "context": "Two representative datasets, CIFAR-10 [16] and ImageNet [27], are used in our experiments.", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "To demonstrate that our proposed method can partially solve the degradation problem and show that the gap between deeper plain network and the shallower one can be shrunk or even removed, we aim to have fair comparison6 with the plain network in [11].", "startOffset": 246, "endOffset": 250}, {"referenceID": 10, "context": "All the hyperparameters such as weight decay, momentum and learning rate are identical with [11].", "startOffset": 92, "endOffset": 96}, {"referenceID": 16, "context": "224\u00d7224 crops are randomly sampled on 256\u00d7256 images plus horizontal flip and color augmentation [17].", "startOffset": 97, "endOffset": 101}, {"referenceID": 11, "context": "First we make statistics of average correlation among different channels over all the layers between two types of methods, say \u201cmsra\u201d [12] initialization plus L2 regularization (abbr.", "startOffset": 134, "endOffset": 138}, {"referenceID": 36, "context": "As pointed by [38], most one-order methods can only be a very effective method for optimizing certain types of deep learning architectures.", "startOffset": 14, "endOffset": 18}, {"referenceID": 0, "context": "[1], because of they will fail convergence at the very first few iteration in such deep architecture7.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "7Actually, for NormProp [1], the magnitudes of both forward and backward signals explode in our implementation which results in NaN.", "startOffset": 24, "endOffset": 27}, {"referenceID": 22, "context": "Method Top-1 Accuracy (%) 44-layer 110-layer Nesterov[24] 85.", "startOffset": 53, "endOffset": 57}, {"referenceID": 6, "context": "18 AdaGrad[7] 77.", "startOffset": 10, "endOffset": 13}, {"referenceID": 41, "context": "3 AdaDelta[44] 48.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "6 Adam[14] 39.", "startOffset": 6, "endOffset": 10}, {"referenceID": 38, "context": "0 RmsProp[40] 10.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "Recently we find that [21] has proposed similar ideas.", "startOffset": 22, "endOffset": 26}], "year": 2017, "abstractText": "Deep neural network is difficult to train and this predicament becomes worse as the depth increases. The essence of this problem exists in the magnitude of backpropagated errors that will result in gradient vanishing or exploding phenomenon. We show that a variant of regularizer which utilizes orthonormality among different filter banks can alleviate this problem. Moreover, we design a backward error modulation mechanism based on the quasiisometry assumption between two consecutive parametric layers. Equipped with these two ingredients, we propose several novel optimization solutions that can be utilized for training a specific-structured (repetitively triple modules of Conv-BN-ReLU) extremely deep convolutional neural network (CNN) WITHOUT any shortcuts/ identity mappings from scratch. Experiments show that our proposed solutions can achieve 4% improvement for a 44-layer plain network and almost 50% improvement for a 110-layer plain network on the CIFAR-10 dataset. Moreover, we can successfully train plain CNNs to match the performance of the residual counterparts. Besides, we propose new principles for designing network structure from the insights evoked by orthonormality. Combined with residual structure, we achieve comparative performance on the ImageNet dataset.", "creator": "LaTeX with hyperref package"}}}