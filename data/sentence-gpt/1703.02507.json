{"id": "1703.02507", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features", "abstract": "The recent tremendous success of unsupervised word embeddings in a multitude of applications raises the obvious question if similar methods could be derived to improve embeddings (i.e. semantic representations) of word sequences as well. We present a simple but efficient unsupervised objective to train distributed representations of sentences. Our method outperforms the state-of-the-art unsupervised models on most benchmark tasks, and on many tasks even beats supervised models, highlighting the robustness of the produced sentence embeddings. We present a simple yet efficient way to do this using a single-object, single-image (AED) image dataset. This method performs best when a dataset is a single-image set of images with a large number of images that include many multiple images. We present a fully effective implementation of this technique.\n\n\n\nThe performance of a distributed model, which uses multiple objects, is quite different from the performance of a single-object-image set of images that are often not directly linked to an individual task. Instead of creating a single-image set of images in a single-image set of images, we do the same for all instances of a single-image set. In this example, the image dataset is a single-image set with a large number of images, which represent many different images and can be accessed with a single-image set. To make this work much more intuitive, we used the same approach as previously outlined. The technique used to train a single-image dataset, for example, only allows an individual task to be run without a single-image set of images.\nWe present a comprehensive approach to generating image embeddings using a single-image set of images using an optimized method.\nOne advantage of the multiple-image dataset is that it is able to capture all the images. The image dataset also contains many different parameters including: number of images, a number of objects, and the presence of a single-image set of images in a single-image set of images. The image dataset is a single-image set of images, and we only have to create the image dataset in memory. Additionally, all images are contained in a single-image set of images. We then run the algorithm as described in the next section. We also use the fact that we are able to generate image embeddings for each individual task.\nTo achieve these goals, we have created an output (a,b) that captures all the images in a single-image set of images", "histories": [["v1", "Tue, 7 Mar 2017 18:19:11 GMT  (91kb,D)", "http://arxiv.org/abs/1703.02507v1", null], ["v2", "Mon, 10 Jul 2017 18:05:48 GMT  (108kb,D)", "http://arxiv.org/abs/1703.02507v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.IR", "authors": ["matteo pagliardini", "prakhar gupta", "martin jaggi"], "accepted": false, "id": "1703.02507"}, "pdf": {"name": "1703.02507.pdf", "metadata": {"source": "META", "title": "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features", "authors": ["Matteo Pagliardini", "Prakhar Gupta", "Martin Jaggi"], "emails": ["<martin.jaggi@epfl.ch>."], "sections": [{"heading": "1. Introduction", "text": "Improving unsupervised learning is of key importance for advancing machine learning methods, as to unlock access to almost unlimited amounts of data to be used as training resources. The majority of recent success stories of deep learning does not fall into this category, but instead relied on supervised training (in particular in the vision domain). A very notable exception comes from the text and natural language processing domain, in the form of semantic word embeddings trained unsupervised (Mikolov et al., 2013b;a; Pennington et al., 2014). Within only few years from their invention, such word representations \u2013 which are based on a simple matrix factorization model \u2013 are now routinely trained on very large amounts of raw text data, and have become ubiquitous building blocks of a majority of current state-of-the-art NLP applications.\nWhile very useful semantic representations are now available for words, it remains challenging to produce and learn such semantic embeddings for longer pieces of text, such as sentences, paragraphs or entire documents. Even more so, it remains a key goal to learn such representations in an unsupervised way.\nCurrently, two opposite research trends have emerged towards tackling text sequence representation learning: On\n*Equal contribution 1Iprova SA, Switzerland 2Computer and Communication Sciences, EPFL, Switzerland. Correspondence to: Martin Jaggi <martin.jaggi@epfl.ch>.\none hand, a trend in deep-learning for NLP moves towards increasingly powerful and complex models, such as recurrent neural networks (RNNs), LSTMs, attention models and even Neural Turing Machine architectures. While extremely strong in expressiveness, the increased model complexity makes such models much slower to train, which especially in the unsupervised setting is problematic. On the other end of the spectrum, we have simpler \u201cshallow\u201d models such as matrix factorizations which are very successful as they can be trained on much larger sets of data. For more successful unsupervised learning, exploring this trade-off between model complexity and ability to process huge amounts of text using scalable algorithms is a key question which we explore in this paper.\nWe will provide a more detailed overview of the existing literature in Section 3.\nContributions. The main contributions in this work can be summarized as follows:\n\u2022 Model. We propose Sent2Vec, a simple unsupervised model allowing to compose sentence embeddings using the word vectors along with n-gram embeddings.\n\u2022 Scalability. Due to the computational efficiency of the model, with a training and inference time per sentence being only linear in the sentence length, the model readily scales to extremely large datasets, which is a crucial advantage in the unsupervised setting.\n\u2022 Performance. Our method shows significant performance improvements compared to the current stateof-the-art unsupervised models. On several benchmark tasks, the proposed unsupervised model even outperforms supervised models tailored to the particular task. The resulting embeddings show strong robustness when transferred to a wide range of prediction benchmarks."}, {"heading": "2. Model", "text": "Our model is inspired by simple matrix factorization models such as recently very successfully used in unsupervised learning of word embeddings (Mikolov et al., 2013b;a;\nar X\niv :1\n70 3.\n02 50\n7v 1\n[ cs\n.C L\n] 7\nM ar\n2 01\n7\nPennington et al., 2014; Bojanowski et al., 2016) as well as supervised and unsupervised learning of sentence representations (Joulin et al., 2016).\nThese models are all described by the following matrixfactorization optimization problem\nmin U ,V \u2211 S\u2208C fS(UV \u03b9S) (1)\nfor two parameter matrices U \u2208 Rk\u00d7h and V \u2208 Rh\u00d7|V|, where V denotes the vocabulary. In all models studied, the columns of the matrix V will collect the learned wordrepresentations. For a given sentence S, which can be of arbitrary length, the indicator vector \u03b9S \u2208 {0, 1}|V| is a binary vector encoding S (bag of words encoding). The cost function fS : Rk \u2192 R is allowed to depend on the sentence or context S.\nFor all existing models of the above form (1), the crucial property enabling fast and scalable training is the fact that the objective can be formulated as a sum over a large given text corpus C, running over all sentences S. Training is performed by stochastic gradient descent (SGD) type methods over both variables U ,V , using the gradient of only a single one of the cost functions fS .\nFixed-length context windows S running over the corpus are used in word embedding methods as in CBOW (Mikolov et al., 2013b;a) and GloVe (Pennington et al., 2014). In contrast, for sentence embeddings as in the focus of our paper here, S will be entire sentences or documents (therefore variable length), just like in the supervised FastText classifier (Joulin et al., 2016). We give a more detailed overview over these variations in Sections 2.3 and 3."}, {"heading": "2.1. Proposed Unsupervised Model", "text": "We propose a new unsupervised model, Sent2Vec, for learning sentence embeddings. Conceptually, the model can be interpreted as a natural extension of the wordcontexts from CBOW (Mikolov et al., 2013b;a) to a larger sentence context, with the sentence words being specifically optimized towards additive combination over the sentence, by means of the unsupervised objective function.\nFormally, we learn source vw and target uw embeddings for each word w in the vocabulary. The sentence embedding as in (2) is defined as the average of the source word embeddings of its constituent words. We augment this model furthermore by also learning source embeddings for not only unigrams but also n-grams present in each sentence, and averaging the n-gram embeddings along with the words, i.e., the sentence embedding vS for S is modeled as\nvS := 1 |R(S)|V \u03b9R(S) = 1 |R(S)| \u2211 w\u2208R(S) vw (2)\nwhere R(S) is the list of n-grams (including unigrams) generated from sentence S. In order to predict a missing word from the context, our objective models the softmax output approximated by negative sampling following (Mikolov et al., 2013b). For the large number of output classes |V| to be predicted, negative sampling is known to significantly improve training efficiency, see also (Goldberg & Levy, 2014). Given the binary logistic loss function ` : x 7\u2192 log (1 + e\u2212x) coupled with negative sampling, our unsupervised training objective is formulated as follows:\nmin U ,V \u2211 S\u2208C \u2211 wt\u2208S ( ` ( u>wtvS\\{wt} ) + \u2211 w\u2032\u2208Nwt ` ( \u2212 u>w\u2032vS\\{wt} )) where S corresponds to the current sentence and Nwt is the set of words sampled negatively for the word wt \u2208 S. The negatives are sampled1 following a multinomial distribution where each word w is associated with a probability qn(w) := \u221a fw / (\u2211 wi\u2208V \u221a fwi ) , where fw is the normalized frequency of w in the corpus.\nTo select the possible target unigrams (positives), we use subsampling as in (Joulin et al., 2016; Bojanowski et al., 2016), each word w being discarded with probability 1 \u2212 qp(w) where qp(w) := min { 1, \u221a t/fw + t/fw } . Where t is the subsampling hyper-parameter. Subsampling prevents very frequent words of having too much influence in the learning as they would introduce strong biases in the prediction task. With positives subsampling and respecting the negative sampling distribution, the precise training objective function becomes\nmin U ,V \u2211 S\u2208C \u2211 wt\u2208S ( qp(wt)` ( u>wtvS\\{wt} ) (3)\n+ |Nwt | \u2211 w\u2032\u2208V qn(w \u2032)` ( \u2212 u>w\u2032vS\\{wt} ))"}, {"heading": "2.2. Computational Efficiency", "text": "In contrast to more complex neural network based models, one of the core advantages of the proposed technique is the low computational cost for both inference and training. Given a sentence S and a trained model U ,V , computing the resulting sentence representation vS only requires |S|\u00b7h floating point operations (or |R(S)| \u00b7 h to be precise for the n-gram case, see (2)), where h is the dimension of the embedding. The same holds for a training step using SGD on the objective (3), for any pair of sentence context and target word. Parallel training of the model is straight-forward using parallelized or distributed SGD.\n1To efficiently sample negatives, a pre-processing table is constructed, containing the words corresponding to the square root of their corpora frequency. Then, the negatives Nwt are sampled uniformly at random from the negatives table except the target wt itself, following (Joulin et al., 2016; Bojanowski et al., 2016)."}, {"heading": "2.3. Comparison to CBOW", "text": "CBOW tries to predict a chosen target word given its fixedsize context window, the context being defined by the average of the vectors associated to the words at a distance less than the window size hyper-parameter ws. If our system, when restricted to unigram features, can be seen as an extension of CBOW where the context window includes the entire sentence, in practice there are few important differences as CBOW uses important tricks to facilitate the learning of word embeddings. CBOW first uses frequent word subsampling on the sentences, deciding to discard each token w with probability qp(w) or alike (small variations exist across implementations). Subsampling prevents the generation of n-grams features, and deprives the sentence of an important part of its syntactical features. It also shortens the distance between subsampled words, implicitly increasing the span of the context window. A second trick consists in using dynamic context windows: for each subsampled word w, the size of its associated context window is sampled uniformly between 1 and ws. Using dynamic context windows is equivalent to weighing by the distance from the focus word w divided by the window size (Levy et al., 2015). This makes the prediction task local and go against our objective of creating sentence embeddings as we want to learn how to compose all n-gram features present in a sentence.\nTo better distinguish our contribution over CBOW, we compare Sent2Vec with, on one hand the CBOW model trained and evaluated in (Hill et al., 2016a), and on another hand, a CBOW model that we tuned ourself for the purpose of obtaining sentence embeddings. We report a significant improvement of our method over both models."}, {"heading": "2.4. Model Training", "text": "Two different datasets have been used to train our models, wikipedia sentences and tweets. The wikipedia sentences dataset is composed of 69 million sentences and 1.7 Billion words while the Twitter dataset contains 1.2 billion tweets and 19.7 Billion words. The wikipedia sentences have been tokenized using the Stanford NLP library (Manning et al., 2014), while the tweets have been tokenized\nusing the NLTK tweets tokenizer (Bird et al., 2009). For training, we select a sentence randomly from the dataset and then proceed to select all the possible target unigrams using subsampling. We update the weights using SGD associated with a linearly decaying learning rate.\nAlso, to prevent overfitting, for each sentence we use dropout on its list of n-grams R(S) \\ {U(S)}, where U(S) is the set of all unigrams contained by sentence S. After empirically trying multiple dropout schemes, we find that droppingK n-grams (n > 1) for each sentence is giving superior results compared to dropping each token with some fixed probability. This dropout mechanism would negatively impact shorter sentences.\nWe train two models on each dataset, one with unigrams only and one with unigrams and bigrams. All training parameters for the models are provided in Table 1. Our C++ implementation builds upon the FastText library (Joulin et al., 2016; Bojanowski et al., 2016). We will make our code and pre-trained models available open-source."}, {"heading": "3. Related Work", "text": "We discuss existing models which have been proposed to construct sentence embeddings. While some methods require ordered raw text i.e., a coherent corpus where the next sentence is a logical continuation of the previous sentence, others rely only on raw text i.e., an unordered collection of sentences. Finally we also discuss alternative models built from structured data sources."}, {"heading": "3.1. Unsupervised Models Independent of Sentence Ordering", "text": "The ParagraphVector DBOW model by (Le & Mikolov, 2014) is a log-linear model which is trained to learn sentence as well as word embeddings and then use a softmax distribution to predict words contained in the sentence given the sentence vector representation. They also propose a different model ParagraphVector DM where they use n-grams of consecutive words along with the sentence vector representation to predict the next word.\n(Hill et al., 2016a) propose a Sequential (Denoising) Autoencoder, S(D)AE. This models first introduces noise in the input data: After each word is deleted with probability p0, then for each non-overlapping bigram, words are swapped with probability px. The model then uses an LSTM-based architecture to retrieve the original sentence from the corrupted version. The model can then be used to encode new sentences into vector representations. In the case of p0 = px = 0, the model simply becomes a Sequential Autoencoder. (Hill et al., 2016a) also propose a variant (S(D)AE + embs.) in which the words are represented by fixed pre-trained word vector embeddings.\nArora et al. (2017) propose a model in which sentences are represented as a weighted average of fixed (pre-trained) word vectors, followed by post-processing step of subtracting the principal component. Using the generative model of (Arora et al., 2016), words are generated conditioned on a sentence \u201cdiscourse\u201d vector cs:\nPr[w | cs] = \u03b1fw + (1\u2212 \u03b1) exp(c\u0303>s vw)\nZc\u0303s ,\nwhere Zc\u0303s := \u2211 w\u2208V exp(c\u0303 > s vw) and c\u0303s := \u03b2c0 + (1 \u2212 \u03b2)cs and \u03b1, \u03b2 are scalars. c0 is the common discourse vector, representing a shared component among all discourses, mainly related to syntax. It allows the model to better generate syntactical features. The \u03b1fw term is here to enable the model to generate some frequent words even if their matching with the discourse vector c\u0303s is low.\nTherefore, this model tries to generate sentences as a mixture of three type of words: words matching the sentence discourse vector cs, syntactical words matching c0, and words with high fw. (Arora et al., 2017) demonstrated that for this model, the MLE of c\u0303s can be approximated by \u2211\nw\u2208S a fw+a vw, where a is a scalar. The sentence dis-\ncourse vector can hence be obtained by subtracting c0 estimated by the first principal component of c\u0303s\u2019s on a set of sentences. In other words, the sentence embeddings are obtained by a weighted average of the word vectors stripping away the syntax by subtracting the common discourse vector and down-weighting frequent tokens. They generate sentence embeddings from diverse pre-trained word embeddings among which are unsupervised word embeddings such as GloVe (Pennington et al., 2014) as well as supervised word embeddings such as paragram-SL999 (PSL) (Wieting et al., 2015) trained on the Paraphrase Database (Ganitkevitch et al., 2013).\nIn a very different line of work, CPHRASE by (Pham et al., 2015) uses a the output of a syntactic parser to produce a sentence vector representation."}, {"heading": "3.2. Unsupervised Models Depending on Sentence Ordering", "text": "The SkipThought model of (Kiros et al., 2015) combines sentence level models with recurrent neural networks. Given a sentence Si from an ordered corpus, the model is trained to predict Si\u22121 and Si+1.\nFastSent by (Hill et al., 2016a) is a sentence-level loglinear bag-of-words model. Like SkipThought, it uses adjacent sentences as the prediction target and is trained in an unsupervised fashion. Using word sequences allows the model to improve over the earlier work of paragraph2vec (Le & Mikolov, 2014). (Hill et al., 2016a) augment FastSent further by training it to predict the constituent words of the sentence as well. This model is named FastSent + AE in our comparisons.\nNote that on the character sequence level instead of word sequences, FastText by (Bojanowski et al., 2016) uses the same conceptual model to obtain better word embeddings. This is most similar to our proposed model, with two key differences: Firstly, we predict from source word sequences to target words, as opposed to character sequences to target words, and secondly, our model is averaging the source embeddings instead of just summing them."}, {"heading": "3.3. Supervised and Semi-Supervised Models", "text": "DictRep by (Hill et al., 2016b) is trained to map dictionary definitions of the words to the pre-trained word embeddings of these words. They use two different architectures, namely BOW and RNN (LSTM) with the choice of learning the input word embeddings or using them pre-trained. A similar architecture is used by the CaptionRep variant, but here the task is the mapping of given image captions to a pre-trained vector representation of these images.\nParaphrastic Sentence Embeddings by (Wieting et al., 2016) are trained on a paraphrase database in a supervised manner. The paraphrase database contains annotated pairs of phrases which are similar in meaning. The model is trained so that phrases occurring in a pair have a high cosine similarity. In this context, the paragramphrase model is trained for this target, on a representation being the averaged word embeddings of each phrase. In the paragram-phrase-P variant, this model is followed by a learned projection matrix and bias. Experiments in (Wieting et al., 2016) show that these two simple models outperforms more complex RNN as well as LSTM architectures on similarity evaluation tasks."}, {"heading": "4. Evaluation", "text": "We use both supervised as well as unsupervised benchmark tasks to evaluate our trained models. For the supervised evaluation, we use our sentence embeddings combined with logistic regression to predict the target label. In the unsupervised evaluation, we evaluate the correlation of the cosine similarity between two embeddings as compared to human annotators. The evaluation datasets cover multiple domains, allowing us to observe on one hand the generalization of our models, and on the other hand also the versatility. We use the same tasks as well as the same evaluation methodology as (Hill et al., 2016a) in order to facilitate comparison with other models. We also compare our unsupervised method to the supervised sentence embeddings of (Wieting et al., 2016) which are learnt from the Paraphrase Database (Ganitkevitch et al., 2013) as well as the more recent work of (Arora et al., 2017).\nThe CBOW model reported in (Hill et al., 2016a) has been tuned on the Book Corpus dataset 2. Only the dimension hyper-parameter has been reported, other parameters are assumed to be default parameters, fit for the generation of word embeddings. To be sure to give a fair comparison to CBOW, we also train a CBOW model on our wikipedia corpus, we used random search to tune hyper-parameters: the embedding dimensions h \u2208 {400, 500, 600, 700}, the window size ws \u2208 {3, 5, 10, 15, 20}, the number of epochs ep \u2208 {1, 3, 5, 6, 7, 8}, the initial learning rate lr \u2208 {0.1, 0.07, 0.05, 0.01}, as well as the subsampling hyper-\n2http://www.cs.toronto.edu/~mbweb/\nparameter t \u2208 { 10\u22126, 10\u22125 } . Considering large window sizes is important as it is supposed to be the closest that CBOW can be of our model. The best parameter configuration found is h = 600, ws = 10, ep = 5, lr = 0.07, and t = 10\u22125. We refer to this model as CBOW Wiki.\nSupervised evaluation. Evaluation of the the learnt sentence embeddings for supervised classification tasks is performed as follows. We evaluate on paraphrase identification (MSRP) (Dolan et al., 2004), movie review sentiment (MR) (Pang & Lee, 2005), product reviews (CR) (Hu & Liu, 2004), subjectivity classification (SUBJ)(Pang & Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and question type classification (TREC) (Voorhees, 2002). To classify, we use the code provided by (Kiros et al., 2015) in the same manner as in (Hill et al., 2016a). For the MSRP dataset, containing pairs of sentences (s1, s2) with associated paraphrase label, we generate feature vectors by concatenating their Sent2Vec representations |vs1 \u2212 vs2 | with the component-wise product vs1 \u00b7 vs2 . The predefined training split is used to tune the L2 penalty parameter using cross-validation and the accuracy and F1 scores are computed on the test set. For the remaining 5 datasets, Sent2Vec embeddings are inferred from input sentences and directly fed to a logistic regression classifier. Accuracy scores are obtained using 10-fold cross-validation for the MR, CR, SUBJ and MPQA datasets. For those datasets nested cross-validation is used to tune the L2 penalty. For the TREC dataset, as for the MRSP dataset, the L2 penalty is tuned on the predefined train split using 10-fold crossvalidation, and the accuracy is computed on the test set.\nUnsupervised evaluation. We perform unsupervised evaluation of the the learnt sentence embeddings using the sentence cosine similarity, on the STS 2014 (Agirre et al., 2014) and SICK 2014 (Marelli et al., 2014) datasets. These similarity scores are compared to the gold-standard human judgements using Pearson\u2019s r (Pearson, 1895) and Spearman\u2019s \u03c1 (Spearman, 1904) correlation scores. The SICK dataset consists of about 10,000 sentence pairs along with relatedness scores of the pairs. The STS 2014 dataset contains 3,770 pairs, divided into six different categories on the basis of origin of sentences/phrases namely Twitter, headlines, news, forum, WordNet and images. See (Agirre et al., 2014) for more precise information on how the pairs have been created."}, {"heading": "5. Results", "text": "In Tables 2 and 3, we compare our results with those obtained by (Hill et al., 2016a) on different models. Along with the models discussed in Section 3, this also includes the sentence embedding baselines obtained by simple averaging of word embeddings over the sentence, in both the CBOW and skip-gram variants. TF-IDF BOW is a representation consisting of the counts of the 200,000 most common feature-words, weighed by their TF-IDF frequencies. Finally, the comparison also includes sentence embeddings\nlearnt by neural machine translation models which use the architecture by (Cho et al., 2014) on English to German (En-De) and English to French (En-Fr) data from the 2015 Workshop on Statistical MT (WMT)3. For the methodology of training of these models, we refer the reader to (Hill et al., 2016a).\nWe also compare our results with the the similarity evaluations done by (Wieting et al., 2016) on their supervised paragram-phrase models as well as on the unsupervised and semi-supervised embeddings obtained by (Arora et al., 2017)."}, {"heading": "5.1. Supervised Evaluation Results", "text": "On running supervised evaluations and observing the results in Table 2, we find that our Sent2Vec unigrams + bigrams model is able to outperform all the previous state-ofthe-art methods in 3 of the datasets. It also performs better than all previous models except SkipThought model for the SUBJ and TREC task. Although we notice that our models are unable to perform well on the MSRP task. The models trained on Twitter perform in a similar fashion when compared to wikipedia trained models.\n3http://www.statmt.org/wmt15/translation-task.html"}, {"heading": "5.2. Unsupervised Evaluation Results", "text": "In Table 3, apart from the results compiled by (Hill et al., 2016a), we also compare our similarity evaluation results with those obtained by (Wieting et al., 2016) on the same set of tasks. We see that our Twitter models are state-ofthe-art on a majority of tasks. Also, on a majority of tasks, our Sent2Vec Twitter models are able to outperform or are on par with the supervised models of (Wieting et al., 2016). However, the DictRep models as well as CPHRASE perform much better on the WordNet dataset and CaptionRep remains the state-of-the-art model for the image dataset."}, {"heading": "5.3. Comparison with (Arora et al., 2017)", "text": "In Table 4, we report an experimental comparison to the model of (Arora et al., 2017), which is particularly tailored to sentence similarity tasks. In the table, the suffix W indicates that their down-weighting scheme has been used, while the suffix R indicates that the first principal component has been subtracted. They reported values of a between 10\u22124 and 10\u22123 as giving the best results and used a = 10\u22123 for all their experiments. Their rational and down-weighting scheme hint us to also try to reduce the importance of syntactical features. To do this we use a simple blacklist containing the 25 most frequent tokens in the Twitter corpus. Those tokens are discarded before averaging. Results are also reported in Table 4.\nWe observe that our results are competitive with the embeddings of (Arora et al., 2017) for purely unsupervised methods. We confirm their empirical finding that reducing the influence of the syntax helps performance on semantic similarity tasks, and we show that applying a simple blacklist already yields a noticeable amelioration. It is important to note that the scores obtained from supervised task-specific PSL embeddings trained for the purpose of semantic similarity outperform our method on both SICK and average STS 2014, which is expected as our model is trained purely unsupervised."}, {"heading": "6. Discussion", "text": "We see that for both supervised as well as unsupervised evaluations, our Sent2Vec models deliver state-of-the-art performance in the majority of tasks/datasets when compared to other unsupervised methods. In fact, they even perform better than or on a par with the Paraphrase based\nembeddings which are learnt in a supervised fashion, on a majority of datasets on similarity evaluation tasks.\nSupervised tasks. On supervised tasks, our unsupervised models are weaker on the MSRP task (which consists in the identification of labelled paraphrases) compared to stateof-the-art methods. However, we observe that the supervised models which perform extremely well on this task end up faring very poorly on the other tasks, indicating a lack of generalizability. For the TREC (question answering task), our models are second only to the SkipThought model although with a significant performance gap. The SkipThought model is able to outperform others on the TREC task as it is trained to predict the previous and next sentences and the TREC task is a poor measure of how one predicts the content of the sentence (the question) but a good measure of how the next sentence in the sequence (the answer) is predicted.\nUnsupervised Evaluations. Similarly, in case of the unsupervised similarity evaluations, our models lag behind the DictRep model for the STS 2014 WordNet dataset. This observation can be attributed to the fact that DictRep models are trained to map words to their definitions and wordNet dataset contains sense definition mappings and ontology notes relating different objects. On the image dataset, our Sent2Vec models also perform better than all other models except the CaptionRep BOW and RNN which are trained to map the image captions to the pre-trained vector representation of these images and hence, these two models end up overfitting the image dataset and fail to generalize well.\nThe effect of datasets, embedding dimension and ngrams. We also observe that Twitter models perform better for unsupervised evaluations but wikipedia models are more accurate and generalize well when it comes to supervised tasks. We also see that addition of bigrams to our models doesn\u2019t help much when it comes to unsupervised evaluations but gives a significant boost-up in accuracy on supervised tasks. On running experiments, we also found out that size of embeddings also plays an important role in the performance on different datasets. We find out that models with smaller embedding sizes tend to fare better on the SICK dataset with multiple models with small embedding dimensions(ranging from 200 to 300) obtaining a Pearson\u2019s similarity score of 0.73. However, these model failed to perform well on the STS 2014 datasets. These\nOn learning the importance and the direction of the word vectors. Our model \u2013 by learning how to generate and compose word vectors \u2013 has to learn both the direction of the word embeddings as well as their norm. Considering the norms of the used word vectors as by our averaging over the sentence, we observe an interesting distribution of the \u201cimportance\u201d of each word. In Figure 1 we show the profile of the norm as a function of log(fw) for each w \u2208 V , and compare it to the static down-weighting mechanism of (Arora et al., 2017). We can observe that our model is\nlearning to down-weight frequent tokens by itself. It is also down-weighting rare tokens and the norm = f(log(fw)) profile seems to roughly follow Luhn\u2019s hypothesis (Luhn, 1958), a well known information retrieval paradigm, stating that mid-rank terms are the most significant to discriminate content. Modifying the objective function would change the weighting scheme learnt. From a more semantic oriented objective it should be possible to learn to attribute lower norms for very frequent terms, to more specifically fit sentence similarity tasks.\nOn the suitability of the evaluation paradigm. We would also like to shed some light on the suitability of the evaluation tasks as well as evaluation datasets for the sentence embeddings. Generally, STS and SICK datasets are taken as the common datasets for the similarity evaluation benchmark. We think that these datasets suffer from the lack of complex sentences which one can encounter in academic, industrial and day-to-day texts. We also observe in Table 5 that the average sentence length for the evaluation datasets except the STS 2014 News dataset is quite low compared to the wikipedia and Twitter datasets. This ends up penalizing the models which try to model longer sequences of tokens and make use of the local word information in the form of n-grams. Thus, we hope that more realistic datasets divided into various domains with sentences from a diverse set of resources and with a significant variance in sentence complexity can be used in future benchmarks for unsupervised similarity evaluations."}, {"heading": "7. Conclusion", "text": "In this paper, we introduced a novel unsupervised and computationally efficient method to train and infer sentence embeddings. Our method, on a majority of tasks, achieves better performance than all other unsupervised competitors \u2013 including those trained on deep learning architectures \u2013 and even outperforms some recent supervised methods on multiple datasets. Future work could focus on augmenting the model to exploit data with ordered sentences. Furthermore, the contextual information could be used better to learn the importance of words present in a sentence. Last but not least, we would like to further investigate the models ability as giving pre-trained embeddings to enable downstream transfer learning tasks.\nAcknowledgments. We are indebted to Piotr Bojanowski and Armand Joulin for helpful discussions."}], "references": [{"title": "Semeval-2014 task 10: Multilingual semantic textual similarity", "author": ["Agirre", "Eneko", "Banea", "Carmen", "Cardie", "Claire", "Cer", "Daniel", "Diab", "Mona", "Gonzalez-Agirre", "Aitor", "Guo", "Weiwei", "Mihalcea", "Rada", "Rigau", "German", "Wiebe", "Janyce"], "venue": "In Proceedings of the 8th international workshop on semantic evaluation (SemEval", "citeRegEx": "Agirre et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2014}, {"title": "A Latent Variable Model Approach to PMIbased Word Embeddings", "author": ["Arora", "Sanjeev", "Li", "Yuanzhi", "Liang", "Yingyu", "Ma", "Tengyu", "Risteski", "Andrej"], "venue": "In Transactions of the Association for Computational Linguistics,", "citeRegEx": "Arora et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2016}, {"title": "A simple but tough-to-beat baseline for sentence embeddings", "author": ["Arora", "Sanjeev", "Liang", "Yingyu", "Ma", "Tengyu"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Arora et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2017}, {"title": "Natural language processing with Python: analyzing text with the natural language toolkit", "author": ["Bird", "Steven", "Klein", "Ewan", "Loper", "Edward"], "venue": null, "citeRegEx": "Bird et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Enriching Word Vectors with Subword Information", "author": ["Bojanowski", "Piotr", "Grave", "Edouard", "Joulin", "Armand", "Mikolov", "Tomas"], "venue": null, "citeRegEx": "Bojanowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["Dolan", "Bill", "Quirk", "Chris", "Brockett"], "venue": "In Proceedings of the 20th International Conference on Computational Linguistics,", "citeRegEx": "Dolan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "Ppdb: The paraphrase database", "author": ["Ganitkevitch", "Juri", "Van Durme", "Benjamin", "Callison-Burch", "Chris"], "venue": "In HLT-NAACL,", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "word2vec Explained: deriving Mikolov et al.\u2019s negative-sampling word-embedding method", "author": ["Goldberg", "Yoav", "Levy", "Omer"], "venue": null, "citeRegEx": "Goldberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2014}, {"title": "Learning Distributed Representations of Sentences from Unlabelled Data", "author": ["Hill", "Felix", "Cho", "Kyunghyun", "Korhonen", "Anna"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Learning to understand phrases by embedding the dictionary", "author": ["Hill", "Felix", "Cho", "KyungHyun", "Korhonen", "Anna", "Bengio", "Yoshua"], "venue": "TACL, 4:17\u201330,", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Mining and summarizing customer reviews", "author": ["Hu", "Minqing", "Liu", "Bing"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Bag of tricks for efficient text classification", "author": ["Joulin", "Armand", "Grave", "Edouard", "Bojanowski", "Piotr", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1607.01759,", "citeRegEx": "Joulin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2016}, {"title": "Skip-Thought Vectors", "author": ["Kiros", "Ryan", "Zhu", "Yukun", "Salakhutdinov", "Ruslan R", "Zemel", "Richard", "Urtasun", "Raquel", "Torralba", "Antonio", "Fidler", "Sanja"], "venue": "In NIPS 2015 - Advances in Neural Information Processing Systems", "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Le", "Quoc V", "Mikolov", "Tomas"], "venue": "In ICML 2014 - Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Levy", "Omer", "Goldberg", "Yoav", "Dagan", "Ido"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "The automatic creation of literature abstracts", "author": ["Luhn", "Hans Peter"], "venue": "IBM Journal of research and development,", "citeRegEx": "Luhn and Peter.,? \\Q1958\\E", "shortCiteRegEx": "Luhn and Peter.", "year": 1958}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Manning", "Christopher D", "Surdeanu", "Mihai", "Bauer", "John", "Finkel", "Jenny Rose", "Bethard", "Steven", "McClosky", "David"], "venue": "In ACL (System Demonstrations),", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "A sick cure for the evaluation of compositional distributional semantic models", "author": ["Marelli", "Marco", "Menini", "Stefano", "Baroni", "Bentivogli", "Luisa", "Bernardi", "Raffaella", "Zamparelli", "Roberto"], "venue": "In LREC,", "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In NIPS - Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Pang", "Bo", "Lee", "Lillian"], "venue": "In Proceedings of the 42nd annual meeting on Association for Computational Linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Pang", "Bo", "Lee", "Lillian"], "venue": "In Proceedings of the 43rd annual meeting on association for computational linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Jointly optimizing word representations for lexical and sentential tasks with the c-phrase model", "author": ["NT Pham", "G Kruszewski", "A Lazaridou", "M. Baroni"], "venue": "ACL/IJCNLP,", "citeRegEx": "Pham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2015}, {"title": "The proof and measurement of association between two things", "author": ["Spearman", "Charles"], "venue": "The American journal of psychology,", "citeRegEx": "Spearman and Charles.,? \\Q1904\\E", "shortCiteRegEx": "Spearman and Charles.", "year": 1904}, {"title": "Overview of the trec 2001 question answering track", "author": ["Voorhees", "Ellen M"], "venue": "In NIST special publication,", "citeRegEx": "Voorhees and M.,? \\Q2002\\E", "shortCiteRegEx": "Voorhees and M.", "year": 2002}, {"title": "Annotating expressions of opinions and emotions in language", "author": ["Wiebe", "Janyce", "Wilson", "Theresa", "Cardie", "Claire"], "venue": "Language resources and evaluation,", "citeRegEx": "Wiebe et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wiebe et al\\.", "year": 2005}, {"title": "From paraphrase database to compositional paraphrase model and back", "author": ["Wieting", "John", "Bansal", "Mohit", "Gimpel", "Kevin", "Livescu", "Karen", "Roth", "Dan"], "venue": null, "citeRegEx": "Wieting et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Towards universal paraphrastic sentence embeddings", "author": ["Wieting", "John", "Bansal", "Mohit", "Gimpel", "Kevin", "Livescu", "Karen"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Wieting et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 23, "context": "A very notable exception comes from the text and natural language processing domain, in the form of semantic word embeddings trained unsupervised (Mikolov et al., 2013b;a; Pennington et al., 2014).", "startOffset": 146, "endOffset": 196}, {"referenceID": 12, "context": ", 2016) as well as supervised and unsupervised learning of sentence representations (Joulin et al., 2016).", "startOffset": 84, "endOffset": 105}, {"referenceID": 23, "context": ", 2013b;a) and GloVe (Pennington et al., 2014).", "startOffset": 21, "endOffset": 46}, {"referenceID": 12, "context": "In contrast, for sentence embeddings as in the focus of our paper here, S will be entire sentences or documents (therefore variable length), just like in the supervised FastText classifier (Joulin et al., 2016).", "startOffset": 189, "endOffset": 210}, {"referenceID": 12, "context": "To select the possible target unigrams (positives), we use subsampling as in (Joulin et al., 2016; Bojanowski et al., 2016), each word w being discarded with probability 1 \u2212 qp(w) where qp(w) := min { 1, \u221a t/fw + t/fw } .", "startOffset": 77, "endOffset": 123}, {"referenceID": 4, "context": "To select the possible target unigrams (positives), we use subsampling as in (Joulin et al., 2016; Bojanowski et al., 2016), each word w being discarded with probability 1 \u2212 qp(w) where qp(w) := min { 1, \u221a t/fw + t/fw } .", "startOffset": 77, "endOffset": 123}, {"referenceID": 12, "context": "Then, the negatives Nwt are sampled uniformly at random from the negatives table except the target wt itself, following (Joulin et al., 2016; Bojanowski et al., 2016).", "startOffset": 120, "endOffset": 166}, {"referenceID": 4, "context": "Then, the negatives Nwt are sampled uniformly at random from the negatives table except the target wt itself, following (Joulin et al., 2016; Bojanowski et al., 2016).", "startOffset": 120, "endOffset": 166}, {"referenceID": 15, "context": "Using dynamic context windows is equivalent to weighing by the distance from the focus word w divided by the window size (Levy et al., 2015).", "startOffset": 121, "endOffset": 140}, {"referenceID": 3, "context": ", 2014), while the tweets have been tokenized using the NLTK tweets tokenizer (Bird et al., 2009).", "startOffset": 78, "endOffset": 97}, {"referenceID": 12, "context": "Our C++ implementation builds upon the FastText library (Joulin et al., 2016; Bojanowski et al., 2016).", "startOffset": 56, "endOffset": 102}, {"referenceID": 4, "context": "Our C++ implementation builds upon the FastText library (Joulin et al., 2016; Bojanowski et al., 2016).", "startOffset": 56, "endOffset": 102}, {"referenceID": 1, "context": "Using the generative model of (Arora et al., 2016), words are generated conditioned on a sentence \u201cdiscourse\u201d vector cs:", "startOffset": 30, "endOffset": 50}, {"referenceID": 2, "context": "(Arora et al., 2017) demonstrated that for this model, the MLE of c\u0303s can be approximated by \u2211", "startOffset": 0, "endOffset": 20}, {"referenceID": 23, "context": "They generate sentence embeddings from diverse pre-trained word embeddings among which are unsupervised word embeddings such as GloVe (Pennington et al., 2014) as well as supervised word embeddings such as paragram-SL999 (PSL)", "startOffset": 134, "endOffset": 159}, {"referenceID": 28, "context": "(Wieting et al., 2015) trained on the Paraphrase Database (Ganitkevitch et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 7, "context": ", 2015) trained on the Paraphrase Database (Ganitkevitch et al., 2013).", "startOffset": 43, "endOffset": 70}, {"referenceID": 13, "context": "The SkipThought model of (Kiros et al., 2015) combines sentence level models with recurrent neural networks.", "startOffset": 25, "endOffset": 45}, {"referenceID": 4, "context": "Note that on the character sequence level instead of word sequences, FastText by (Bojanowski et al., 2016) uses the same conceptual model to obtain better word embeddings.", "startOffset": 81, "endOffset": 106}, {"referenceID": 29, "context": "Paraphrastic Sentence Embeddings by (Wieting et al., 2016) are trained on a paraphrase database in a supervised manner.", "startOffset": 36, "endOffset": 58}, {"referenceID": 29, "context": "Experiments in (Wieting et al., 2016) show that these two simple models outperforms more complex RNN as well as LSTM architectures on similarity evaluation tasks.", "startOffset": 15, "endOffset": 37}, {"referenceID": 29, "context": "We also compare our unsupervised method to the supervised sentence embeddings of (Wieting et al., 2016) which are learnt from the Paraphrase Database (Ganitkevitch et al.", "startOffset": 81, "endOffset": 103}, {"referenceID": 7, "context": ", 2016) which are learnt from the Paraphrase Database (Ganitkevitch et al., 2013) as well as the more recent work of (Arora et al.", "startOffset": 54, "endOffset": 81}, {"referenceID": 2, "context": ", 2013) as well as the more recent work of (Arora et al., 2017).", "startOffset": 43, "endOffset": 63}, {"referenceID": 6, "context": "We evaluate on paraphrase identification (MSRP) (Dolan et al., 2004), movie review sentiment (MR) (Pang & Lee, 2005), product reviews (CR) (Hu & Liu, 2004), subjectivity classification (SUBJ)(Pang & Lee, 2004), opinion polarity (MPQA) (Wiebe et al.", "startOffset": 48, "endOffset": 68}, {"referenceID": 27, "context": ", 2004), movie review sentiment (MR) (Pang & Lee, 2005), product reviews (CR) (Hu & Liu, 2004), subjectivity classification (SUBJ)(Pang & Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and question type classification (TREC) (Voorhees, 2002).", "startOffset": 174, "endOffset": 194}, {"referenceID": 13, "context": "To classify, we use the code provided by (Kiros et al., 2015) in the same manner as in (Hill et al.", "startOffset": 41, "endOffset": 61}, {"referenceID": 0, "context": "We perform unsupervised evaluation of the the learnt sentence embeddings using the sentence cosine similarity, on the STS 2014 (Agirre et al., 2014) and SICK 2014 (Marelli et al.", "startOffset": 127, "endOffset": 148}, {"referenceID": 18, "context": ", 2014) and SICK 2014 (Marelli et al., 2014) datasets.", "startOffset": 22, "endOffset": 44}, {"referenceID": 0, "context": "See (Agirre et al., 2014) for more precise information on how the pairs have", "startOffset": 4, "endOffset": 25}, {"referenceID": 5, "context": "Finally, the comparison also includes sentence embeddings learnt by neural machine translation models which use the architecture by (Cho et al., 2014) on English to German (En-De) and English to French (En-Fr) data from the 2015 Workshop on Statistical MT (WMT)3.", "startOffset": 132, "endOffset": 150}, {"referenceID": 29, "context": "We also compare our results with the the similarity evaluations done by (Wieting et al., 2016) on their supervised paragram-phrase models as well as on the unsupervised", "startOffset": 72, "endOffset": 94}, {"referenceID": 2, "context": "and semi-supervised embeddings obtained by (Arora et al., 2017).", "startOffset": 43, "endOffset": 63}, {"referenceID": 2, "context": "Comparison of the performance of the unsupervised and semi-supervised sentence embeddings by (Arora et al., 2017) with our", "startOffset": 93, "endOffset": 113}, {"referenceID": 29, "context": ", 2016a), we also compare our similarity evaluation results with those obtained by (Wieting et al., 2016) on the same set of tasks.", "startOffset": 83, "endOffset": 105}, {"referenceID": 29, "context": "Also, on a majority of tasks, our Sent2Vec Twitter models are able to outperform or are on par with the supervised models of (Wieting et al., 2016).", "startOffset": 125, "endOffset": 147}, {"referenceID": 2, "context": "Comparison with (Arora et al., 2017)", "startOffset": 16, "endOffset": 36}, {"referenceID": 2, "context": "In Table 4, we report an experimental comparison to the model of (Arora et al., 2017), which is particularly tailored to sentence similarity tasks.", "startOffset": 65, "endOffset": 85}, {"referenceID": 2, "context": "We observe that our results are competitive with the embeddings of (Arora et al., 2017) for purely unsupervised methods.", "startOffset": 67, "endOffset": 87}, {"referenceID": 2, "context": "Bottom figure: down-weighting scheme proposed by (Arora et al., 2017): weight(w) = a a+fw .", "startOffset": 49, "endOffset": 69}, {"referenceID": 2, "context": "In Figure 1 we show the profile of the norm as a function of log(fw) for each w \u2208 V , and compare it to the static down-weighting mechanism of (Arora et al., 2017).", "startOffset": 143, "endOffset": 163}], "year": 2017, "abstractText": "The recent tremendous success of unsupervised word embeddings in a multitude of applications raises the obvious question if similar methods could be derived to improve embeddings (i.e. semantic representations) of word sequences as well. We present a simple but efficient unsupervised objective to train distributed representations of sentences. Our method outperforms the state-of-the-art unsupervised models on most benchmark tasks, and on many tasks even beats supervised models, highlighting the robustness of the produced sentence embeddings.", "creator": "LaTeX with hyperref package"}}}