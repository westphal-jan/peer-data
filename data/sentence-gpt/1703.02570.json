{"id": "1703.02570", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Regularising Non-linear Models Using Feature Side-information", "abstract": "Very often features come with their own vectorial descriptions which provide detailed information about their properties. We refer to these vectorial descriptions as feature side-information. In the standard learning scenario, input is represented as a vector of features and the feature side-information is most often ignored or used only for feature selection prior to model fitting. In contrast, you may see a vector that represents a vector of features. In these cases, a vector of features is represented as a vector of features.\n\n\n\nIn the usual practice of vectorial terms, you will be seeing a vector that represents a vector of features and a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look like a vector of features. In this case, it may look", "histories": [["v1", "Tue, 7 Mar 2017 19:47:22 GMT  (636kb,D)", "http://arxiv.org/abs/1703.02570v1", "11 page with appendix"]], "COMMENTS": "11 page with appendix", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["amina mollaysa", "pablo strasser", "alexandros kalousis"], "accepted": true, "id": "1703.02570"}, "pdf": {"name": "1703.02570.pdf", "metadata": {"source": "META", "title": "Regularising Non-linear Models Using Feature Side-information", "authors": ["Amina Mollaysa", "Pablo Strasser", "Alexandros Kalousis"], "emails": ["<maolaaisha.aminanmu@hesge.ch>,", "<pablo.strasser@hesge.ch>,", "dros.Kalousis@hesge.ch>."], "sections": [{"heading": "1. Introduction", "text": "Side-information in machine learning is a very general term used in very different learning scenarios with quite different connotations. Nevertheless, generally it is understood as any type of information, other than the learning instances, which can be used to support the learning process; typically such information will live in a different space than the learning instances. Examples include learning with privileged information (Vapnik & Izmailov, 2015) in which during training a teacher provides additional information for the learning instances; this information is not\n1HES-SO & University of Geneva, Switzerland 2HESSO & University of Geneva, Switzerland 3HES-SO & University of Geneva, Switzerland. Correspondence to: Amina Mollaysa <maolaaisha.aminanmu@hesge.ch>, Pablo Strasser <pablo.strasser@hesge.ch>, Alexandros Kalousis <Alexandros.Kalousis@hesge.ch>.\navailable in testing. In metric learning and clustering, it has been used to denote the availability of additional similarity information on instances, i.e. pairs of similar and dissimilar instances, (Xing et al., 2002). In this paper we focus on side-information describing the features. We will consider learning problems in which we have additional information describing the properties and/or the relations of the features. The features will have their own vectorial descriptions in some space in which we will describe their properties.\nReal world problems with such properties are very common. For example in drug efficiency prediction problems, and more general in chemical formulae property prediction problems, drugs/formulae are collections of molecules. Each molecule comes with its own description, for example in terms of its physio-chemical properties, and/or its molecular structure. In language modeling, words are features and the words\u2019 semantic and syntactic properties are their side-information. In image recognition, pixels are features and their position is the side-information, and so on. Similar ideas also appear in tasks such as matrix completion, robust PCA and collaborative filtering (Rao et al., 2015; Chiang et al., 2016; 2015). There one seeks low rank matrix decompositions in which the component matrices are constrained to follow relationships given by side-information matrices, typically matrices which contain user and item descriptors.\nDespite the prevalence of such problems, there has been surprisingly limited work on learning with feature sideinformation. Krupka et al., 2008, used the feature sideinformation to perform feature selection as a preprocessing step prior to any modelling or learning. More interestingly (Krupka & Tishby, 2007) exploit the feature sideinformation directly within the learning process, by forcing features that have similar side-information to have a similar weight within a SVM model. One can think of this as a model regularisation technique in which we force the model structure, i.e. the feature parameters, to reflect the feature manifold as this is given by the feature sideinformation. In the same work the authors also provide an ad-hoc way to apply the same idea for non-linear models, more precisely polynomials of low degree. However the solution that they propose requires an explicit construction of the different non-linear terms, as well as appropriate ar X\niv :1\n70 3.\n02 57\n0v 1\n[ cs\n.L G\n] 7\nM ar\n2 01\n7\ndefinitions of the feature side-information that is associated with them. These definitions are hand-crafted and depend on the specific application problem. Beyond this ad-hoc approach it is far from clear how one could regularise general non-linear models so that they follow the feature manifold.\nIn this paper we present a method for the exploitation of feature side-information in non-linear models. The main idea is that the learned model will treat in a similar manner features that are similar. Intuitively, exchanging the values of two very similar features should only have a marginal effect on the model output. This is straightforward for linear models since we have direct access to how the model treats the features, i.e. the feature weights. In such a case one can design regularisers as Krupka & Tishby, 2007, did which force the feature weights to reflect the feature manifold. An obvious choice would be to apply a Laplacian regulariser to the linear model, where the Laplacian is based on the feature similarity. Such regularisers have been previously used for parameter shrinkage but only in the setting of linear models where one has direct access to the model parameters (Huang et al., 2011). However, in general non-linear models we no longer have access to the feature weights; the model parameters are shared between the features and we cannot disentangle them.\nWe present a regulariser which forces the learned model to be invariant/symmetric to relative changes in the values of similar features. It directly reflects the intuition that small changes in the values of similar features should have a small effect on the model output. The regulariser relies on a measure of the model output sensitivity to changes in all possible pairs of features. The model sensitivity measure quantifies the norm of the change of the model output under all possible relative changes of the values of two features. We compute this norm by integrating over the relative changes and the data distribution. Integrating over the relative changes is problematic we thus give two ways to approximate the sensitivity measure. In the first approach we rely on a first order Taylor expansion of the learned model under which the sensitivity measure boils down to the squared norm of the difference of the partial derivatives of the model with respect to the input features. Under this approach the regulariser finally boils down to the application of a Laplacian regulariser on the Jacobian of the model. In the second approach we rely on sampling and data augmentation to generate instances with appropriate relative changes over different feature pairs. We approximate the value of the regulariser only on the augmented data.\nWe implement the above ideas in the context of neural networks, nevertheless it is relatively straightforward to use them in other non-linear models such as SVMs and kernels.\nWe experiment on a number of text classification datasets in which the side-information is the word2vec representation\nof the words. We compare against a number of baselines and we show significant performance improvements."}, {"heading": "2. Learning Symmetric Models with Respect to Feature Similarity", "text": "We consider supervised learning settings in which, in addition to the classical data matrix X : n \u00d7 d containing n instances and d features, and the target matrix Y : n \u00d7m, we are also given a matrix Z : d\u00d7 c, the ith row of which, denoted by zi, contains a description of the ith feature. We call Z the feature side-information matrix. Note how the Z matrix is fixed and independent of the training instances. As in the standard supervised setting, instances, xi \u2208 X \u2286 Rd, are drawn i.i.d from some non-observed probability distributions P (X ) and targets, yi \u2208 Y \u2286 Rm, are assigned according to some non-observed conditional distribution P (Y|X ),Y \u2208 Rm. In the standard setting we learn a mapping from the input to the output \u03c6 : x \u2208 Rd \u2192 y \u2208 Rm using the X,Y matrices by optimizing some loss function L. In this paper we learn the inputoutput mapping using in addition to the X,Y, matrices the feature side-information Z matrix.\nWe bring the feature side-information in the learning process through the feature similarity matrix S \u2208 Rd\u00d7d which we construct from Z as follows. Given two features i, j, with zi, zj , side-information vectors the Sij element of S contains their similarity given by some similarity function. We will denote by L = D \u2212 S the Laplacian of the similarity matrix S; D is the diagonal degree matrix with Dii = \u2211 j Sij .\nWe use the similarity and the Laplacian matrices to constraint the learned model to treat in a similar manner features that have similar side-information. This is relatively straightforward with linear models such as WXT,W \u2208 Rm\u00d7d, and can be achieved through the introduction of the Laplacian regulariser Tr (WLWT) = \u2211 ij ||W.i \u2212 W.j ||2Sij in the objective function where W.i is the ith column vector of W containing the model parameters associated with the ith feature, (Huang et al., 2011). The Laplacian regulariser forces the parameter vectors of the features to cluster according to the feature similarity.\nHowever in non-linear models such neat separation of the model parameters is not possible since these are shared between the different input features. In order to achieve the same effect we will now operate directly on the model output. We will do so by requiring that the change in the model\u2019s output is marginal if we change the relative proportion of two very similar features. Concretely, let i and j be such features, and ei, ej , be the d-dimensional unit vectors with the ith and jth dimensions respectively equal\nto one. We want that:\n\u03c6(x+ \u03bbiei + \u03bbjej) \u2248 \u03c6(x+ \u03bb\u2032iei + \u03bb\u2032jej) (1)\n\u2200\u03bbi, \u03bbj , \u03bb\u2032i, \u03bb\u2032j \u2208 R such that \u03bbi + \u03bbj = \u03bb\u2032i + \u03bb\u2032j\nEquation (1) states that as long as the total contribution of the i, j, features is kept fixed, the model\u2019s output should be left almost unchanged. The exact equality will hold when the i, j, are on the limit identical, i.e. Sij \u2192 \u221e. More general the level of the model\u2019s change should reflect the similarity of the i, j, features, thus a more accurate reformulation of equation 1 is:\n||\u03c6(x+ \u03bbiei + \u03bbjej)\u2212 \u03c6(x+ \u03bb\u2032iei + \u03bb\u2032jej)||2 \u221d 1\nSij (2)\n\u2200\u03bbi, \u03bbj , \u03bb\u2032i, \u03bb\u2032j \u2208 R such that \u03bbi + \u03bbj = \u03bb\u2032i + \u03bb\u2032j\nThus the norm of the change in the model output, that we get when we alter the relative proportion of two features i and j, while keeping their total contribution fixed, should be inversely proportional to the features similarity, i.e. large similarity, small output change. The result is that the model is symmetric to similar features and its output does not depend on the individual contributions/values of two similar features but only on their total contribution. In figure 1 we visualise the effect of the model constraint given in eq. 2. Given some instance x and two features i, j, that are on the limit identical the constraint forces the model output to be constant on the line defined by x + \u03bbiei + \u03bbjej , \u2200\u03bbi + \u03bbj = c, for some given c \u2208 R. We can think of the whole process as the model clustering together, to some latent factor, features that have very high similarity. The latent factor captures the original features total contribution leaving the model\u2019s output unaffected to relative changes in their values.\nTo unclutter notation we will define the vector \u03bb = (\u03bbi, \u03bbj , \u03bb \u2032 i, \u03bb \u2032 j). We want the constraint of eq. 2 to be valid over all instances drawn from P (x) as well as for all \u03bb vectors that satisfy the equality constraint eq 2. A natural measure of the degree to which the constraint holds for the feature pair i, j is given by:\nRij(\u03c6) = \u222b ||\u03c6(x+ \u03bbiei + \u03bbjej) (3)\n\u2212\u03c6(x+ \u03bb\u2032iei + \u03bb\u2032jej)||2\nSijI(\u03bb)P (x)d\u03bbdx\nwhere I(\u03bb) = 1 if \u03bbi + \u03bbj = \u03bb\u2032i + \u03bb \u2032 j , and 0 otherwise. Since we want to define a regulariser that accounts for all feature pairs and their similarities we simply have:\nR(\u03c6) = \u2211 ij Rdij(\u03c6) (4)\nCalculating the regularizer is problematic due to the presence of the I(\u03bb) function that selects the \u03bb subspace over which the integration is performed. In the next two sections we will give two ways to approximate it. The first one will be analytical relying on the first order Taylor expansion of \u03c6(x) and its Jacobian. The second one stochastic, essentially performing data augmentation and defining a regularisation term along the lines of eq. 2."}, {"heading": "2.1. An analytical approximation", "text": "We will use the first order Taylor expansion of \u03c6(x) to simplify the squared term in eq. 2 by removing the \u03bb variable. We will start by using the first order Taylor expansion to approximate the value of \u03c6(x+ \u03bbiei + \u03bbjej) at x\n\u03c6(x+ \u03bbiei + \u03bbjej) \u2248 \u03c6(x) + J(x)(\u03bbiei + \u03bbjej)\nJ(x) \u2208 Rm\u00d7d is the Jacobian of \u03c6(x) evaluated at x. Then plugging the Taylor expansion in eq 2 we get:\n||(\u03bbi \u2212 \u03bb\u2032i)J(x)ei \u2212 (\u03bb\u2032j \u2212 \u03bbj)J(x)ej ||2 \u221d 1\nSij (5)\nand since \u03bbi+\u03bbj = \u03bb\u2032i+\u03bb \u2032 j we have (\u03bbi\u2212\u03bb\u2032i) = (\u03bb\u2032j\u2212\u03bbj) and eq 5 becomes:\n||J(x)ei \u2212 J(x)ej ||2 = ||\u2207i\u03c6(x)\u2212\u2207j\u03c6(x)||2 \u221d 1\nSij (6)\nwhere \u2207i\u03c6(x) is the m-dimensional partial derivative of \u03c6(x) with respect to the ith input feature. Using eq 6 we can approximate Rij as follows:\nRij(\u03c6) \u2248 \u222b ||\u2207i\u03c6(x)\u2212\u2207j\u03c6(x)||2SijP (x)dx\nfrom which we get the following approximation of the R(\u03c6) regulariser:\nR(\u03c6) \u2248 \u2211 ij \u222b ||\u2207i\u03c6(x)\u2212\u2207j\u03c6(x)||2SijP (x)dx\n\u2248 \u222b \u2211\nij\n||\u2207i\u03c6(x)\u2212\u2207j\u03c6(x)||2SijP (x)dx\n\u2248 \u222b Tr[J(x)LJT(x)P (x)dx (7)\nwhich is the local linear approximation of the original regulariser eq 4 on the input instances. Since we only have access to the training sample and not to P (x) we will get the sample estimate of eq. 7 given by\nR\u0302(\u03c6) = \u2211 ij \u2211 k ||\u2207i\u03c6(xk)\u2212\u2207j\u03c6(xk)||2Sij\n= \u2211 k \u2211 ij ||\u2207i\u03c6(xk)\u2212\u2207j\u03c6(xk)||2Sij\n= \u2211 k Tr[J(xk)LJ T(xk)] (8)\nSo the sample based estimate of the regulariser is a sum of Laplacian regularisers applied on the Jacobian of each one of the training samples. It forces the partial derivatives of the model with respect to the input, or equivalently the model\u2019s sensitivity to the input features, to reflect the features similarity in the local neighborhood around each training point. Or in other words it will constrain the learned model in a small neighborhood around each training point to have similar slop in the dimensions that are associated with similar features. Note that if \u03c6(x) = Wx then J(xk) = W and Tr[J(xk)LJT(xk)] reduces to the standard Tr[WLWT] Laplacian regulariser on the columns of W associated with the input features. Adding the sample based estimate of the regulariser to the loss function we get the final objective function which we minimize with \u03c6(x) giving the following minimization problem under the analytical approximation:\nmin \u03c6 \u2211 k L(yk, \u03c6(xk)) + \u03bb \u2211 k Tr[J(xk)LJ T(xk)] (9)\nThe approximation of the requlariser is only effective locally around each training point since it relies on first order Taylor expansion. When the learned function is highly nonlinear, it can force model invariance only to small relative changes in the values of two similar features. However, as the size of the relative changes increases and we move away from the local region the approximation is no longer effective. The regulariser will not be powerful enough to make the invariance hold away from the training points. If we want a less local approximation we can either use higher order Taylor approximation which is computationally prohibitive or rely on a more global approximation through\ndata augmentation as we will see in the next section. Note also that the presence of the Jacobian in the objective function means that if we optimise it using gradient descent we will need to compute second order partial derivatives which come with an increasing computational cost."}, {"heading": "2.2. A stochastic approximation", "text": "Instead of using the first order Taylor expansion to simplify the squared term required by the regulariser we can use sampling to approximate it. Concretely for a given feature pair, i, j, and a given instance x we randomly sample p quadruples \u03bb(l)i , \u03bb (l) j , \u03bb (l)\u2032 i , \u03bb (l)\u2032\nj \u2208 R such that \u03bb (l) i + \u03bb (l) j = \u03bb (l)\u2032 i + \u03bb (l)\u2032\nj , l := 1 . . . p, which we use to generate p new instance pairs as follows:\nx\u2192\n{ x+ \u03bb\n(l) i ei + \u03bb (l) j ej\nx+ \u03bb (l)\u2032 i ei + \u03bb (l)\u2032 j ej\nWe can now use the training sample and the sampling process to get an estimate of Rij(\u03c6) by:\u2211\nk \u2211 l ||(\u03c6(xk + \u03bb(l)i ei + \u03bb (l) j ej)\n\u2212\u03c6(xk + \u03bb(l) \u2032 i ei + \u03bb (l)\u2032 j ej))|| 2Sij\nand of the final regulariser R(\u03c6) by:\nR\u0303(\u03c6) = \u2211 ij \u2211 k \u2211 l ||(\u03c6(xk + \u03bb(l)i ei + \u03bb (l) j ej)\n\u2212\u03c6(xk + \u03bb(l) \u2032 i ei + \u03bb (l)\u2032 j ej))|| 2Sij (10)\nSo the final optimization problem will now become:\nmin \u03c6 \u2211 k L(yk, \u03c6(xk)) + \u03bbR\u0303(\u03c6) (11)\nNote that the new instances appear only in the regulariser and not in the loss. The regulariser will penalise models which do not have the invariance property with respect to pairs of similar features. In practice when computing R\u0303(\u03c6) we do not want to go through all the pairs of features but only through the most similar. We do not want to spend sampling time on data augmentation for dissimilar pairs since for these there is no effective constraint on the values of the model\u2019s output. So we simplify the sum run only over the pairs of similar features. One motivation for the stochastic approach was the fact that the analytical one relies in an approximation which is only effective locally in the neighborhood of each learning instance. In the stochastic approach we have control on the size of the neighborhood over which the constraint is enforced through the Euclidean norm of the change vector (\u03bbi, \u03bbj); the larger its\nvalue the larger the neighborhood. The smaller the neighborhood the closer we are to the local behavior of the analytical approximation. We should note here that the sampling of stochastic approximation will naturally blend with the stochastic gradient optimization that we will use to optimize our objective functions."}, {"heading": "2.3. Optimization", "text": "We learn \u03c6 with a standard feed forward neural network with sigmoid activation functions applied on the hidden layers using stochastic gradient descent.\nThe objective function of the analytical approach contains the Jacobian of the model with respect to its input. Calculating the gradient over this results in the introduction of second order partial derivatives of the model with respect to the inputs and the model parameters. Bishop, 1992, gave a backpropagation algorithm for the exact calculation of the Hessian of the loss of a multi-layer perceptron. We have adapted this algorithm so that we can compute the gradient of objective functions that contain the Jacobian with respect to the input features, we give the complete gradient calculation procedure in the appendix.\nWe will give now the computational complexity of each the two methods. We will denote by l the number of layers, m the output dimension of the network, hk the number of hidden units of the kth layer and we will define hmax = max{hk|k = 1, . . . , l \u2212 1}. The computational complexity of computing the gradient for a single instance of the objective function of the analytical approach is O(m\u00d7 h1\u00d7 d2) for networks with a single hidden layer andO(l\u00d7m\u00d7h2max\u00d7d2+ l\u00d7m\u00d7h3max\u00d7d) for networks with more than one hidden layers. To reduce this computational complexity in our experiments we sparsify S by keeping only the entries correponding to top 20% biggest elements and zero out the rest. The complexity now becomes O(m \u00d7 h1 \u00d7 d) for one layer networks and O(l \u00d7 m\u00d7h2max\u00d7d+ l\u00d7m\u00d7h3max) for networks with more than one layer. The computational complexity of the stochastic approach is O(l \u00d7 h2max \u00d7 m \u00d7 p + l \u00d7 hmax \u00d7 m \u00d7 p) while the computational complexity for standard feed forward network is O(l \u00d7 h2max + l \u00d7 hmax \u00d7m)."}, {"heading": "3. Related Work", "text": "Krupka & Tishby, 2007, use feature side-information, they call it meta-features, within a linear SVM model. They force the SVM\u2019s weights to be similar for features that have similar side-information. They achieve that through the introduction of a Gaussian prior on the feature weight vector. The covariance matrix of the Gaussian is a function of the features similarity. The authors show how to extend their approach from linear to polynomial models. However,\ntheir approach requires explicit calculation of all the higher order terms limiting its applicability to low order polynomials. Very similar in spirit is all the body of work on Laplacian regularisation for feature regularisation; (Krupka & Tishby, 2007) contains an extensive review. Such regularisers constrain the feature weights to reflect relations that are given by the Laplacian. The Laplacian matrix is constructed from available domain knowledge, what here we call feature side information. However, it can also be constructed from the data; for example as a function of the feature correlation matrix.\nThe Taylor expansion we use in the analytical approximation of the regulariser results in the use of the Jacobian of the model. Regularisers that use the Jacobian have previously been successfully used to control the stability/robustness of models to noisy inputs. Relevant work includes contractive auto encoders, (Rifai et al., 2011b), and (Zhai & Zhang, 2015). (Rifai et al., 2011b) use the Frobenius of the Jacobian at the input instances to force the model to be relatively constant in small neighbors around the input instances. Such a regulariser introduces invariance to small input variations. In a different setting Rosasco et al., 2013, used the Jacobian to learn sparse nonlinear models in the context of kernels.\nOptimizing the Jacobian in networks with more than one layer is cumbersome, thus very often the stochastic approach is preferred over the analytic e.g. (Zhai & Zhang, 2015). Denoising autoencoders, (Vincent et al., 2010) follow the stochastic paradigm and require that small random variations in the inputs have only a limited effect on the model output. Zheng et al., 2016, used Gaussian perturbations to stabilise the network\u2019s output with respect to variations in the input, essentially augmenting the training data. Regularisers on higher order derivatives, Hessian, are also used, (Rifai et al., 2011a), in such cases the stochastic approach is the only choice due to the prohibitive cost of optimizing the Hessian term.\nData augmentation is a well-established approach for learning models with built-in invariance to noise and/or robustness to data perturbations. In addition it is also used when we have additional prior knowledge on the instance structures to which the models should be invariant. In imaging problems such structures include translations, rotations, scalings,etc, (Simard et al., 1991; Decoste & Scho\u0308lkopf, 2002).\nThe works that are closer to our work are (Krupka & Tishby, 2007) as well as the works that use Laplacian based regularisers for model regularisation, e.g. (Huang et al., 2011). However to the best of our knowledge all previous work was strictly limited to linear models. We are the first ones who show how such regularisers and constraints can be applied to general classes of non-linear models."}, {"heading": "4. Experiments", "text": "We will experiment and evaluate our regularisers in two settings, a synthetic and a real world one. We will compare the analytical and the stochastic regulariser, which we will denote by AN and ST respectively, against popular regularisers used with neural networks, namely `2 and Dropout (Srivastava et al., 2014), over different network architectures. In the real world datasets we also give the results of the Word Mover\u2019s Distance, WMD, (Kusner et al., 2015) which makes direct use of the side-information to compute document distances. Obviously our regularisers and WMD have an advantage over `2 and dropout since it exploit sideinformation which `2 and dropout do not.\nWe trained both the analytical and the stochastic models, as well as all baselines against which we compare, using Adam (Kingma & Ba, 2014). We used \u03b1 = 0.001, \u03b21 = 0.9, \u03b22 = 0.999 for one hidden layer networks, and \u03b1 = 0.001 for the networks with more hidden layers. We initialize all networks parameters using (Glorot & Bengio, 2010). Due to the large computational complexity of the analytical approach we set the mini-batch size m to five. For the stochastic model, as well as for all the baseline models, we set the mini-batch size to 20. For the analytical model we set the maximum number of iterations to 5000. For the stochastic model we set the maximum number of iterations to 10000 for the one layer networks and to 20000 for networks with more layers. We used early stopping where we keep 20% of the training data as the validation set. Every five parameter updates we compute the validation error. Training terminates either if we reach the maximum iteration number or the validation error keeps increasing more than ten times in a row.\nIn the stochastic approach we do the sampling for the generation of the instance pairs within the stochastic gradient descent process. Concretely for each instance x in a mini batch we randomly chose a feature pair i, j, from the set of similar feature pairs. We sample a quadruple {\u03bbi, \u03bbj , \u03bb\u2032i, \u03bb\u2032j} from R respecting the constraint: \u03bbi+\u03bbj = \u03bb\u2032i + \u03bb \u2032 j from which we generate the respective instance pairs. We repeat the process p times each time sampling a new feature pair i, j, and a new quadruple. We fix the set of similar feature pairs to be the top 20% of most similar feature pairs. Thus within each mini-batch of size m we generate m\u00d7 p instance pairs and we accumulate the norm of the respective model output differences in the objective. In the experiments we set p = 5"}, {"heading": "4.1. Artificial datasets", "text": "We design a simple data generation process in order to test the performance of our regularisers when the data generation mechanism is compatible with the assumptions of our models. We randomly generate an instance matrix\nX \u2208 Rn\u00d7d by uniformly sampling instances from Rd. We create d/2 feature clusters as follows. To each one of these clusters we initially assign one of the original input features without replacement. We assign randomly and uniformly the remaining d/2 features to the clusters. We use the feature clusters to define a latent space where every feature cluster gives rise to a latent feature. The value of each latent feature is the sum of the values of the features that belong to its cluster. On the latent space representation of the training data we apply a linear transformation that projects the latent space to a new space with lower dimensionality q. On this lower dimensionality space we apply an element-wise sigmoid and the final class assignment is given by the index of the maximum sigmoid value. The similarity Sij of the i, j, features of the original space is 1 if they ended up in the same cluster and 0 otherwise. We set d = 3000, n = 5000, q = 5. We will call this dataset A1. The generating procedure gave a very sparse S matrix with only 0.04% of its entries being non-zero. Each feature had an average of 1.3 similar features. We used 4000 instances for training and the rest for testing. During training 20% of the instances are used for the validation set. We measure performance with the classification error, i.e. percentage of wrong predictions. We train all algorithms on the original input space. For all regularisers we used a single layer with 100 hidden units. We tune the hyperparameters based on the performance on the validation set. We select the \u03bb hyperparameters of of AN, ST, and `2 from {10k|k = \u22123, . . . , 3}; we select the \u03bb of dropout from [0.1, 0.2, 0.3, 0.4, 0.5]. We set the c in the augmentation process, that controls the size of the neigborhood within which the output constraints should hold, to one.\nBoth the analytical and the stochastic regulariser bring performance improvements of roughly 10% when compared to the `2 regulariser and to Dropout, results in table 1. In figure 2 we plot the learning curves, i.e. error on the validation set for each epoch, of the four regularisers. We can see that both the analytical and the stochastic regulariser converge much faster and to significantly lower error values than either `2 or dropout.\nThe regularizer we propose constrains the model structure by forcing the model to reflect the feature similarity structures as these are given in the similarity matrix. Thus we expect the structure of the similarity matrix to have an impact on the performance of the regulariser. To see that let us consider the trivial case in which S is diagonal, i.e. there are no similar features. In this case the input and the latent spaces are equivalent. Under such setting the regulariser will have no effect since there are no similarity constraints to impose on the model. If on the other hand, all features are identical, i.e. Sij = 1,\u2200i, j, then the latent space will have a dimensionality of one, in such a case the regulariser has the strongest effect.\nTo explore this dependency we generate two additional synthetic datasets where we use the same generating mechanism as in A1 but vary the proportion of features we cluster together to generate latent factors. Concretely in the synthetic dataset we will call A2 we randomly select a set A of d/2 features over which we will perform clustering to define latent factors. We use the remaining set B of d/2 features directly as they are in the latent space. We cluster the features of the A set to d/4 clusters\u2014latent factors, making sure that as in A1 each cluster has at least one feature in it. As a result the final latent space has a dimensionality of d/4 + d/2 = 3d/4. To generate the class assignments we proceed as in A1. To generate the third dataset, A3, we select d/4 features to generate A and the remaining for B. We now cluster the features in A to d/8 clusters, again making sure that there is at least one feature pre cluster. The dimensionality of the latent space is now d/8 + 3d/4 = 7d/8; class assignments are generated as above. We used the same values for n, d, q as in A1. As we move from A1 to A3 we reduce the number of features that are similar to other features, thus we increase the sparsity of S. For A2 and A3 the percentage of non-zero elements is 0.021% and 0.011% respectively, compared to 0.04% we had in A1. So A1 is the datasets that has most constraints while A3 is the one with the least constraints. We apply the different regularisers in these two datasets using exactly the same protocol as in A1. The results are also given in table 1. As we see the classification error of both ST and AN increases as the dataset sparsity increases and it approaches that of the standard regularisers."}, {"heading": "4.2. Real world datasets", "text": "We evaluated both approaches on the eight classification datasets used in (Kusner et al., 2015). The datasets are: BBC sports articles (BBCSPORT) labeled as one of athletics, cricket, footbal, rugby, tennis; tweets labeled with sen-\ntiments positive, negative, or neutral (TWITTR); recipes labeled by their region of origin (RECIPE); of medical abstracts labeled by different cardiovascular disease groups (OHSUMED); sentences from academic papers labeled by publisher name (CLASSIC); amazon reviews labeled by product category (AMAZON); news dataset labeled by the news topics (REUTER); news articles classified into 20 different categories (20NEWS). We removed all the words in the SMART stop word list (Salton & Buckley, 1988). Documents are represented as bag of words. To speed up training, we removed words that appear very few times over all the documents of a dataset. Concretely, in 20NEWS we reduce the dictionary size by removing words with a frequency less or equal to three. In the OHSUMED and CLASSIC datasets we remove words with frequency one and the in REUTER dataset words with frequency equal or less than two. As feature side-information we use the word2vec representation of the words which have a dimensionality of 300 (Mikolov et al., 2013); other possibilities include knowledge-based side-information, e.g. based on WordNet (Miller, 1995). In table 2 we give a description of the final datasets on which we experiment including the number of classes (m) and average number of unique words per document.\nWe compute the similarity matrix S from the word2vect word representations using the heat kernel with bandwidth parameter \u03c3, i.e. the similarity of i, j, features is given by: Sij = exp(\u2212 12\u03c32 (zi \u2212 zj)\nT (zi \u2212 zj)). We select \u03c3 so that roughly 20% of the entries of the similarity matrix are in [0.8, 1] interval.\nFor those datasets that do not come with a predefined train/test split (BBCSPORT, TWITTER, CLASSIC, AMAZON, RECIPE), we use five-fold cross validation and re-\nport the average error. We compare the statistical significance of the results using the MacNemar\u2019s test with a significance level of 0.05. For hyperparameter tuning we use three-fold inner cross validation. We select the \u03bb hyperparameters of of AN, ST, and `2 from [0.001, 0.01, 0.1, 1, 10]; we select the \u03bb of dropout from [0.1, 0.2, 0.3, 0.4, 0.5]. We do a series of experiments in which we vary the number of hidden layers. Due to the computational complexity of the backprogation for the AN regulariser we only give results for the single layer architecture.\nIn the first set of experiments we use a neural network with one hidden layer and 100 hidden units, we give the results in table 3. ST is significantly better than the AN in five out of the eight datasets, significantly worse once, and equivalent in one dataset. ST is significantly better than the `2 in six out of the eight daasets, while it is equivalent in one. Compared to dropout it is four times significantly better and three times significantly worse.\nWhen we increase the number of hidden layers to two with 500 and 100 units on the first and second layer ST method is significantly better compared to `2 three times, significantly worse three times, while there is no significant difference in two datasets. A similar picture emerges with respect to Dropout with ST being significantly better three times, significantly worse twise, while in three cases there is no significant difference. We give the detailed results in table 4."}, {"heading": "5. Conclusion and Future Work", "text": "Many real world applications come with additional information describing the properties of the features. Despite that, quite limited attention has been given to such setting. In this paper we develop a regulariser that exploits exactly such information for general non-linear models. It relies on\nthe simple intuition that features which have similar properties should be treated by the learned model in a similar manner. The regulariser imposes a stability constraint over the model output. The constraint forces the model to produce similar outputs for instances the feature values of which differ only on similar features. We give two ways to approximate the value of the regulariser. An analytical one which boils down to the imposition of a Laplacian regulariser on the Jacobian of the learned model with respect to the input features and a stochastic one which relies on sampling.\nWe experiment with neural networks with the two approximations of the regulariser and compare their performance to well established model regularisers, namely `2 and dropout, on artificial and real world datasets. In the artificial datasets, for which we know that they match the assumptions of our regulariser we demonstrate significant performance improvements. In the real world datasets the performance improvements are less striking. One of the main underlying assumptions of our model is that the feature side-information is indeed relevant for the learning problem, when this is indeed the case we will have performance improvements. If it is not the case then the regulariser will not be selected, as a result of the tuning of the \u03bb parameter. Along the same lines we want to perform a more detailed study on how the structure of the similarity matrix, namely its sparsity and the underlying feature cluster structure, determines the regularisation strength of our regulariser. It is clear that a sparse similarity matrix will lead to a rather limited regularisation effect since only few features will be affected. This points to the fact that the regulariser should be used together with more traditional sparsity inducing regularisers, especially in the case of a sparse feature feature similarity matrix. Finally since we use the feature information through a similarity function it might be the case that the similarity function that we are using is not appropriate and better results can be obtained if we also learn the feature similarity. We leave this for future work."}, {"heading": "6. Appendix", "text": ""}, {"heading": "6.1. Modified Backpropogation", "text": "For notion simplicity, we consider stochastic gradient descent. The objective function we want to minimize is as following:\nE = L(y,\u03c6(x)) + \u03bb1 \u2211 ij ||\u2202\u03c6(x) \u2202xi \u2212 \u2202\u03c6(x) \u2202xj ||2Sij (12)\nNotice that the objective function includes derivative of the learned function with respect to the input features, if we use neural network to learn the model, the conventional backpropagation algorithm can\u2019t be applied directly. Therefore, we developed a modified version of the backpropagation algorithm to find the gradient of the objective. We keep the notation consistent with the notation used in the book of (Bishop, 1995). n is the total layers (including input and out put layer) number of the network, ak is the pre-activation units in layer k, k1 is the number of hidden units in hidden layer k, m is the number of output units, and h(x) stands for the non-linear activation function.\nz0 = x\nak = wkzk\u22121 + bk\nzk = h(ak)\n\u03c6(x) = zn\n(13)\nTo find the gradient of (12), we define \u03b4k as the Jacobian of the learned function with respect to pre-activations at the layer k:\n\u03b4k =  \u2202\u03c61 \u2202ak1\n\u2202\u03c62 \u2202ah1 \u00b7 \u00b7 \u00b7 . \u2202\u03c6m \u2202ak1\n\u2202\u03c61 \u2202ak2\n\u2202\u03c62 \u2202ak2\n\u00b7 \u00b7 \u00b7 \u2202\u03c6m \u2202ak2\n... ... ... \u2202\u03c61 \u2202akk1 \u2202\u03c62 \u2202akk1 \u00b7 \u00b7 \u00b7 \u2202\u03c6m \u2202akk1\n (14)\n\u03b4k for all k can be achieved by the following backpropagation equation.\n\u03b4k = ((Wk+1)T \u03b4k+1) h\u2032(ak) \u2200k = 1, 2, ..., n\u22121 (15)\nWhere stands for the element wise multiplication of a column vector to every column of the matrix.\n\u03b4n =  h\u2032(an1 ) 0 ... 0\n0 h\u2032(an2 ) ... 0 ... ... ... 0 0 ... h\u2032(anm)  (16) Defining the term \u03b4 in such a away, we can rewrite the regularizer term in equation (12) as following:\u2211\nij\n||(W1(:, i))\u2212W1(:, j))T \u03b41||2Sij (17)\nIf the network only has one hidden layer, we can derive derivative of the regularizer with respect to weights using \u03b4 and (15). When hidden layer\u2019s number is more than one, we need to introduce two more term, one to the backward path and one to the forward path: Define Gk as the jacobian of pre-activation unit at layer k with respect to preactivation at first hidden layer, note layer k = 1 corresponding to first hidden layer.\nGkmg = \u2202akm \u2202a1g \u2200k = 1, 2, 3, ..., n (18)\nWe know that:\nG1mg = \u2202a1m \u2202a1g = { 1 if m=g 0 others (19)\nAnd Gk for all k can be achieved during forward path by the following forward propagation equation and G1\nGkmg = \u2211 l W kmlG k\u22121 lg h \u2032(ak\u22121l ) \u2200k = 2, 3, ..., n (20)\nDefine Bk which gives the derivative of the \u03b4k with respect to the pre-activation units in the first hidden layers:\nBkljg = \u2202\u03b4klj \u2202a1g \u2200k = 1, 2, ..., n (21)\nWe know that:\nBnljg = \u2202\u03b4nlj \u2202a1g = h\u2032\u2032(anl )1ljG n lg (22)\nBk for all k can be obtained by the following propagating equation during backward path using Bn as following:\nBkljg = h \u2032\u2032(akl )G k lg \u2211 p \u03b4 k+1 pj W k+1 pl + h \u2032(akl ) \u2211 pW k+1 pl B k+1 pjg (23)\n\u2200k = 1, 2, ..., n\u2212 1\nFinally, the gradient of the regularizer, i.e. second term of the equation (12), can be calculated as follwoing: For k = 1, i.e. first hidden layer:\n\u2202R \u2202W 1lm\n= 4\u03bb1 \u2211 s Sms \u2211 j(W 1(:,m)\u2212W1(:, s))T \u03b41(:, j)\u03b41(lj)\n+2\u03bb1 \u2211 ks Sks \u2211 j(W 1(:, k)\u2212W 1(:, s))T \u03b41(:, j) \u2211 g(W 1(g, k)\u2212W 1(g, s))B1ljgz0m) (24)\nFor k = 2, ..., n:\n\u2202R \u2202Wklm\n= 2\u03bb1 \u2211 ks Sks \u2211 j(W\n1(:, k)\u2212W1(:, s))T \u03b41(:, j)\u2211 g(W 1(g, k)\u2212W 1(g, s))(zk\u22121m Bkljg + \u03b4kljh\u2032(ak\u22121m )Gk\u22121mg ) (25)\nGradient with respect to bias term, for all k = 1, ..., n:\n\u2202R\n\u2202bklm = 2\u03bb1 \u2211 ks Sks \u2211 j (W1(:, k)\u2212W1(:, s))T \u03b41(:, j)\n\u2211 g (W 1(g, k)\u2212W 1(g, s))Bkljg (26)\nThe gradient of the first part of the objective which is some loss function we chose, is same as in the standard Backpropagation algorithm, here we just need to rewrite it in terms of the newly defined \u03b4. For example, if we use sigmoid on all layers as activation function and cross entropy loss, we have the following:\nE = \u2212 m\u2211 i=1 (yi log \u03c6(x)i + (1\u2212 yi) log(1\u2212 \u03c6(x)i))\n(27) \u2202E\n\u2202Wk = \u03b4k \u03c6\u2212 y \u03c6(1\u2212 \u03c6) (zk\u22121)T (28)\n\u2202E \u2202bk = \u03b4k \u03c6\u2212 y \u03c6(1\u2212 \u03c6)\n(29)\nNow we can find the gradient of the loss with respect to weights in all layers. Compared to the conventional back propagation algorithm, except we have \u03b4 term which is defined differently than the conventional backprop algorithm, we have one more extra term Bk to add to the backward path and one more term Gh to the forward path."}], "references": [{"title": "Exact calculation of the hessian matrix for the multilayer perceptron", "author": ["Bishop", "Chris"], "venue": null, "citeRegEx": "Bishop and Chris.,? \\Q1992\\E", "shortCiteRegEx": "Bishop and Chris.", "year": 1992}, {"title": "Neural networks for pattern recognition", "author": ["Bishop", "Christopher M"], "venue": "Oxford university press,", "citeRegEx": "Bishop and M.,? \\Q1995\\E", "shortCiteRegEx": "Bishop and M.", "year": 1995}, {"title": "Matrix completion with noisy side information", "author": ["Chiang", "Kai-Yang", "Hsieh", "Cho-Jui", "Dhillon", "Inderjit S"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chiang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chiang et al\\.", "year": 2015}, {"title": "Robust principal component analysis with side information", "author": ["Chiang", "Kai-Yang", "Hsieh", "Cho-Jui", "Dhillon", "EDU Inderjit S"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Chiang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chiang et al\\.", "year": 2016}, {"title": "Training invariant support vector machines", "author": ["Decoste", "Dennis", "Sch\u00f6lkopf", "Bernhard"], "venue": "Machine Learning,", "citeRegEx": "Decoste et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Decoste et al\\.", "year": 2002}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In Aistats,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "The sparse laplacian shrinkage estimator for highdimensional regression", "author": ["Huang", "Jian", "Ma", "Shuangge", "Li", "Hongzhe", "Zhang", "CunHui"], "venue": "The Annals of Statistics,", "citeRegEx": "Huang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Incorporating prior knowledge on features into learning", "author": ["Krupka", "Eyal", "Tishby", "Naftali"], "venue": "In AISTATS,", "citeRegEx": "Krupka et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Krupka et al\\.", "year": 2007}, {"title": "Learning to select features using their properties", "author": ["Krupka", "Eyal", "Navot", "Amir", "Tishby", "Naftali"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Krupka et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Krupka et al\\.", "year": 2008}, {"title": "From word embeddings to document distances", "author": ["Kusner", "Matt J", "Sun", "Yu", "Kolkin", "Nicholas I", "Weinberger", "Kilian Q"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML", "citeRegEx": "Kusner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kusner et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["Miller", "George A"], "venue": "Communications of the ACM,", "citeRegEx": "Miller and A.,? \\Q1995\\E", "shortCiteRegEx": "Miller and A.", "year": 1995}, {"title": "Collaborative filtering with graph information: Consistency and scalable methods", "author": ["Rao", "Nikhil", "Yu", "Hsiang-Fu", "Ravikumar", "Pradeep K", "Dhillon", "Inderjit S"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Rao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rao et al\\.", "year": 2015}, {"title": "Higher order contractive auto-encoder", "author": ["Rifai", "Salah", "Mesnil", "Gr\u00e9goire", "Vincent", "Pascal", "Muller", "Xavier", "Bengio", "Yoshua", "Dauphin", "Yann", "Glorot"], "venue": "In Machine Learning and Knowledge Discovery in Databases - European Conference,", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["Rifai", "Salah", "Vincent", "Pascal", "Muller", "Xavier", "Glorot", "Bengio", "Yoshua"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Nonparametric sparsity and regularization", "author": ["L Rosasco", "S Villa", "S Mosci", "M Santoro", "others"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Rosasco et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rosasco et al\\.", "year": 2013}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["Salton", "Gerard", "Buckley", "Christopher"], "venue": "Information processing & management,", "citeRegEx": "Salton et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Salton et al\\.", "year": 1988}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Learning using privileged information: Similarity control and knowledge transfer", "author": ["Vapnik", "Vladimir", "Izmailov", "Rauf"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Vapnik et al\\.,? \\Q2023\\E", "shortCiteRegEx": "Vapnik et al\\.", "year": 2023}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Manifold regularized discriminative neural networks", "author": ["Zhai", "Shuangfei", "Zhang", "Zhongfei"], "venue": "arXiv preprint arXiv:1511.06328,", "citeRegEx": "Zhai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhai et al\\.", "year": 2015}, {"title": "Improving the robustness of deep neural networks via stability training", "author": ["Zheng", "Stephan", "Song", "Yang", "Leung", "Thomas", "Goodfellow", "Ian J"], "venue": "In 2016 IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Zheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "Similar ideas also appear in tasks such as matrix completion, robust PCA and collaborative filtering (Rao et al., 2015; Chiang et al., 2016; 2015).", "startOffset": 101, "endOffset": 146}, {"referenceID": 3, "context": "Similar ideas also appear in tasks such as matrix completion, robust PCA and collaborative filtering (Rao et al., 2015; Chiang et al., 2016; 2015).", "startOffset": 101, "endOffset": 146}, {"referenceID": 6, "context": "Such regularisers have been previously used for parameter shrinkage but only in the setting of linear models where one has direct access to the model parameters (Huang et al., 2011).", "startOffset": 161, "endOffset": 181}, {"referenceID": 6, "context": "i is the ith column vector of W containing the model parameters associated with the ith feature, (Huang et al., 2011).", "startOffset": 97, "endOffset": 117}, {"referenceID": 20, "context": "Denoising autoencoders, (Vincent et al., 2010) follow the stochastic paradigm and require that small random variations in the inputs have only a limited effect on the model output.", "startOffset": 24, "endOffset": 46}, {"referenceID": 6, "context": "(Huang et al., 2011).", "startOffset": 0, "endOffset": 20}, {"referenceID": 10, "context": "In the real world datasets we also give the results of the Word Mover\u2019s Distance, WMD, (Kusner et al., 2015) which makes direct use of the side-information to compute document distances.", "startOffset": 87, "endOffset": 108}, {"referenceID": 10, "context": "We evaluated both approaches on the eight classification datasets used in (Kusner et al., 2015).", "startOffset": 74, "endOffset": 95}, {"referenceID": 11, "context": "As feature side-information we use the word2vec representation of the words which have a dimensionality of 300 (Mikolov et al., 2013); other possibilities include knowledge-based side-information, e.", "startOffset": 111, "endOffset": 133}, {"referenceID": 10, "context": "WMD results are from from (Kusner et al., 2015).", "startOffset": 26, "endOffset": 47}], "year": 2017, "abstractText": "Very often features come with their own vectorial descriptions which provide detailed information about their properties. We refer to these vectorial descriptions as feature side-information. In the standard learning scenario, input is represented as a vector of features and the feature sideinformation is most often ignored or used only for feature selection prior to model fitting. We believe that feature side-information which carries information about features intrinsic property will help improve model prediction if used in a proper way during learning process. In this paper, we propose a framework that allows for the incorporation of the feature side-information during the learning of very general model families to improve the prediction performance. We control the structures of the learned models so that they reflect features\u2019 similarities as these are defined on the basis of the side-information. We perform experiments on a number of benchmark datasets which show significant predictive performance gains, over a number of baselines, as a result of the exploitation of the side-information.", "creator": "LaTeX with hyperref package"}}}