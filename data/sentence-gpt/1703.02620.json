{"id": "1703.02620", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Linguistic Knowledge as Memory for Recurrent Neural Networks", "abstract": "Training recurrent neural networks to model long term dependencies is difficult. Hence, we propose to use external linguistic knowledge as an explicit signal to inform the model which memories it should utilize. Specifically, external knowledge is used to augment a sequence with typed edges between arbitrarily distant elements, and the resulting graph is decomposed into directed acyclic subgraphs of latent memory in order to provide an alternative to that of the prior.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 7 Mar 2017 22:13:17 GMT  (1152kb,D)", "http://arxiv.org/abs/1703.02620v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bhuwan dhingra", "zhilin yang", "william w cohen", "ruslan salakhutdinov"], "accepted": false, "id": "1703.02620"}, "pdf": {"name": "1703.02620.pdf", "metadata": {"source": "META", "title": "Linguistic Knowledge as Memory for Recurrent Neural Networks", "authors": ["Bhuwan Dhingra", "Zhilin Yang", "William W. Cohen", "Ruslan Salakhutdinov"], "emails": ["gra@cs.cmu.edu>."], "sections": [{"heading": "1. Introduction", "text": "Sequential data appears in many real world applications involving natural language, videos, speech and financial markets. Predictions involving such data require accurate modeling of dependencies between elements of the sequence which may be arbitrarily far apart. Deep learning offers the promise of extracting these dependencies in a purely data driven manner, with Recurrent Neural Networks (RNNs) being the architecture of choice. RNNs show excellent performance when the dependencies of interest range short spans of the sequence, however they can be notoriously hard to train to discover longer range dependencies (Koutnik et al., 2014; Bengio et al., 1994).\nHochreiter & Schmidhuber (1997) introduced Long Short Term Memory (LSTM) networks which use a special unit called the Constant Error Carousel (CEC) to alleviate this\n1School of Computer Science, Carnegie Mellon University, Pittsburgh, USA. Correspondence to: Bhuwan Dhingra <bdhingra@cs.cmu.edu>.\nproblem. The CEC has a memory cell with a constant linear connection to itself which allows gradients to flow over long distances. Cho et al. (2014b) introduced a simplified version of LSTMs called Gated Recurrent Units (GRU) with has one less gate, and consequently fewer parameters. Both LSTMs and GRUs have been hugely popular for modeling sequence data (Sutskever et al., 2014; Kiros et al., 2015; Oord et al., 2016).\nDespite these extensions, empirical studies have shown that it is still difficult to train RNNs with long-range dependencies (see for example, (Cho et al., 2014a)). One suggested explanation for this is that the network must propagate all the information in a single fixed-size vector, which may be infeasible. This led to the introduction of the attention mechanism (Bahdanau et al., 2014) which adapts the sequence model with a more explicit form of long term memory. At each time step t, the model can perform a \u201csoft\u201dlookup over all previous outputs through a weighted average \u2211t\u22121 i=1 \u03b1ihi. The weights \u03b1i are the outputs of another network whose parameters are learned from data. Augmenting sequence models with attention has lead to significant improvements in various language modeling domains (Hermann et al., 2015). Other architectures, such as Memory Networks (Weston et al., 2014), further build on this idea by introducing a memory module for the soft-lookup operation, and a number of models allow the RNN to hold differentiable \u201cmemories\u201d of past elements to discover long range correlations (Graves et al., 2014). However, Daniluk et al. (2017) showed that even memory-augmented neural models do not look beyond the immediately preceding time steps. Clearly, training RNNs to discover long range dependencies without an explicit signal is challenging.\nIn this paper we do not attempt to solve this problem. Instead we argue that in many applications, information about long-term dependencies may be readily available in the form of symbolic knowledge. As an example, consider the sequence of text shown in Figure 1. Standard preprocessing tools can be used to extract relations such as coreference and hypernymy between pairs of tokens, which can be added as extra edges in addition to the sequential links between elements. We argue that these extra signals can be used to provide the RNN model with locations of an explicit memory of distant elements when computing the representation of the current element. The content of a memory is\nar X\niv :1\n70 3.\n02 62\n0v 1\n[ cs\n.C L\n] 7\nM ar\n2 01\n7\nthe representation of the linked element, and edge labels can distinguish different types of memories. In this manner symbolic knowledge can guide the propagation of information through a recurrent network.\nTechnically, incorporating these \u201cskip connections\u201d into the sequence converts it into a graph with cycles. Graph based neural networks (Scarselli et al., 2009) can be used to handle such data, but they are computationally expensive when the number of nodes in the graph is large. Instead, we utilize the order inherent in the the unaugmented sequence to decompose the graph into two Directed Acyclic Graphs (DAGs) with a topological ordering. We introduce the Memory as Acyclic Graph Encoding RNN (MAGERNN) framework to compute the representation of such graphs while touching every node only once, and implement a GRU version of it called MAGE-GRU. MAGERNN learns separate representations for propagation along each edge type, which leads to superior performance empirically. In cases where there is at most a single incoming edge of a particular type at a node, it reduces to a memory augmented regular RNN whose memory access is determined by a symbolic signal.\nWe use MAGE-RNN to model coreference relations for text comprehension tasks, where answers to a query have to be extracted from a context document. Tokens in a document are connected by a coreference relation if they refer to the same underlying entity. Identifying such relations is important for developing an understanding of the document, and hence we augment RNN architectures for text comprehension with an explicit memory of coreferent mentions. MAGE-GRU leads to a consistent improvement over the vanilla GRU, as well as a baseline where the coreference information is added as input features to the model. By further replacing GRU units in existing reading comprehension models with MAGE-GRUs we achieve stateof-the-art performance on three well studied benchmarks \u2013 the bAbi QA tasks, the LAMBADA dataset, and the CNN dataset. An analysis of the learned representations by the model also show its effectiveness in encoding fine-grained information about the entities in a document."}, {"heading": "2. Related Work", "text": "Augmenting the sequence with these extra links converts it from a chain to a more general graph structure. Models such as the Graph Neural Networks (Scarselli et al., 2009) and Gated Graph Sequence Neural Networks (Li et al., 2016) can be used to handle such data. The basic idea in these architectures is to update the representation of every single node at every time-step, based on the incoming representations of their neighbours. Depending on the optimization procedure, the updates are either performed till the representations converge, or for a fixed number of\ntime-steps. The resulting complexity is O(NT ), where N is the number of nodes, and T is the number of time-steps. To fully propagate information in the graph, T should be at least the width of the graph. Though in practice it is possible to obtain an approximation with a smaller T , training graph-based neural networks is computationally expensive compared to RNNs, which have complexity only O(N).\nTrees are another commonly encountered graph structure, and Tai et al. (2015) proposed Tree-Structured LSTMs for handling such data. However, that work focused solely on dependency parses of textual data and ignored its inherent sequential ordering, whereas here we argue for a more general approach which can incorporate many types of edges between tokens, including sequential ones. The resulting MAGE-RNN formulation can be viewed as an extension of Tree-Structured LSTMs.\nShuai et al. (2016) proposed a similar idea to ours, which employs RNNs on DAGs of image pixels. However, their model focuses on images and does not handle typed edges. In contrast, our work provides a novel perspective on incorporating symbolic knowledge as links to sequential data. Moreover, in terms of model architectures, our model explicitly handles typed edges by using separate parameter matrices and split hidden states, which are key to improved performance.\nThe importance of reference resolution for reading comprehension was previously studied in Wang et al. (2017). They showed that models which utilize explicit coreference information, for example via the attention sum mechanism (see Section 4.1), tend to perform better than those which do not. The suggested solution in that work was to add this information as extra features to the input of the model This approach forms our \u201cone-hot\u201d baseline. Here we take that idea further by proposing a modification to the structure of the reader itself for handling coreference, and show that doing so leads to further performance improvement, and interpretable output representations. Our model is also related to the Recurrent Entity Networks architecture (Henaff et al., 2016). In fact, with perfect coreference information, each chain corresponds to an entity in the story, plus one chain for the sequential context. However, the MAGE-RNN model allows these chains to interact with each other, unlike recurrent entity networks (for example, the local context around the mention of an entity can inform the coreference representation of that entity and vice versa). It is also possible to incorporate other types of symbolic knowledge beyond coreference in MAGE-RNNs.\nRecently, there has been interest in incorporating symbolic knowledge, such as that from a Knowledge Base or coreference information, within RNN-based language models (Yang et al., 2016; Ahn et al., 2016). However, rather than incorporating this knowledge within the structure of\nthe RNN, these works instead use the output representation learned by the RNN to model latent variables which decide when to select the next token from the full vocabulary or a restricted vocabulary from the knowledge source. This approach is specific to the task of predicting the next token in a sequence. Our work is aimed at the more general problem of learning suitable representations of text."}, {"heading": "3. Methods", "text": ""}, {"heading": "3.1. From Sequences to DAGs", "text": "Suppose that along with the input sequence x1, . . . , xT , where xi \u2208 Rdin , we are also given information about which pairs of elements connect with each other. Further, suppose that these extra \u201cedges\u201d are typed\u2014i.e., they belong to one of several different categories. Such extra information is common in Natural Language Processing (NLP). For example, one type of edge might connect multiple mentions of the same entity (coreference), while another type of edge might connect generic terms to their specific instances (hyponymy and hypernymy). Figure 1 shows a simple example. Any piece of text can be augmented in this manner by running standard preprocessing tools such as coreference taggers and entity linkers.\nLet G = (X , E) denote the resulting directed graph which includes the sequential edges between consecutive elements, as well as the extra typed edges. The nodes X = {xi}Ti=1 correspond to the elements of the original sequence, and edges E0 = {(x, x\u2032, e)} are tuples consisting of the source, target and type of the link. The graph G results from augmenting the edges in E0 with inverse edges. More formally, for each edge (x, x\u2032, e) \u2208 E0, we add an edge (x\u2032, x, e\u2032) to the graph with e\u2032 being the (artificial) inverse edge type of e. The resulting edge set with both original and inverse edges is denoted as E . By definition, the graph G is a directed cyclic graph in general, but we can use the inherent order of the original sequence to decompose this into two subgraphs in the forward and backward directions respectively. The forward subgraph can be defined as Gf = (X , Ef ), where Ef = {(xi, xj , ef ) \u2208 E : i < j}. Here i and j are indices into the original sequence. The backward graph is defined analogously, with Eb = {(xi, xj , eb) \u2208 E : i > j}. By construction, Gf and Gb are DAGs, i.e., they do not contain cycles. We denote the set of all forward edge types by Ef and all backward edge types by Eb.\nFor every DAG there exists a topological ordering of its\nnodes in a sequence such that all edges in the graph are directed from preceding nodes to succeeding nodes in the sequence. For Gf (respectively Gb) defined above, one such ordering is immediately available \u2013 the forward sequence order (1, 2, . . . , T ) (respectively the backward order (T, T \u2212 1, . . . , 1)). The existence of such an ordering makes DAGs particularly amenable to be modeled using RNNs, and below we discuss an architecture for doing so."}, {"heading": "3.2. MAGE-GRUs", "text": "We present our framework as an adaptation of Gated Recurrent Units, called MAGE-GRU; however similar extensions can be derived for any recurrent neural network. MAGEGRU uses separate networks for the forward and backward subgraphs respectively. We present the updates only for the forward subgraph, since the backward subgraph is processed analogously.\nMiller et al. (2016) and Daniluk et al. (2017) argue that overloaded use of state representations as both memory content and address makes training of the network difficult, and decompose these two functions by parameterizing them separately. Similarly, we decompose the hidden states in the GRUs and maintain a separate hidden state vector het \u2208 Rde for each edge type in Ef . The intuition behind this is that, for example, the representation flowing through the black edges in Figure 1 need not be the same as that flowing through the red or green edges.\nAs t varies from 1 to T , the hidden states are updated in the topological order defined by the sequence and, importantly, the update for each edge state depends on the current state of all incoming edges at xt. Specifically, define\nIf (xt) = {(t\u2032, e) : (xt\u2032 , xt, e) \u2208 Ef} (1)\nas the set of incoming edges at node x, along with the index of their sources. Then the next state is given by\nret = \u03c3(W e r xt + \u2211 (t\u2032,e\u2032)\u2208If (xt) Ue,e \u2032 r h e\u2032 t\u2032 + b e r)\nzet = \u03c3(W e z xt + \u2211 (t\u2032,e\u2032)\u2208If (xt) Ue,e \u2032 z h e\u2032 t\u2032 + b e z)\nh\u0303et = tanh(W e hxt + r e t \u2211 (t\u2032,e\u2032)\u2208If (xt) Ue,e \u2032 h h e\u2032 t\u2032 + b e h)\nhet = (1\u2212 zet ) het\u22121 + zet h\u0303et (2)\nfor each e \u2208 Ef . W er,z,h and U e,e\u2032\nr,z,h are parameter matrices of size de \u00d7 din and de \u00d7 de\u2032 respectively, and ber,z,h are\nparameter vectors of size de. The output representation at time step t is given by,\nht = h e1 t \u2016h e2 t \u2016 . . . \u2016h e|Ef | t , (3)\nwhere ht \u2208 R \u2211\ne de , and \u2016 denotes concatenation. Output for the forward subgraph is given by Hf = [h1, . . . , hT ]. Similarly, we can obtain the output of the backward subgraph Hb, and concatenate with Hf such that elements of the original sequence line up. The collection of all previous output representations Mt = [h0;h1; . . . ;ht\u22121] can be viewed as the memory available to the recurrent model at time-step t.\nIn the case of coreference, or any relation where there is at most one incoming edge of a particular type at any node, the DAG can be decomposed into a collection of independent chains. Then the updates for each e in (2) can be trivially combined into one regular GRU update, as depicted in Figure 2. To see this, define gt = ge1t \u2016g e2 t \u2016 . . . \u2016ge|E| , where geit = h ei t\u2032 , if \u2203(t\u2032, ei) \u2208 I(xt), else g ei t = 0. In other words, geit holds the hidden state of the node connecting to xt via edge e, if there is such a node. Then, by stacking the matrices W e\u2217 , U e,e\u2032\n\u2217 and the vector be\u2217 for all e, e\u2032, we obtain the following combined updates:\nrt = \u03c3(Wrxst + Urgt + br)\nzt = \u03c3(Wzxst + Uzgt + bz)\nh\u0303t = tanh(Whxst + rt Uhgt + bh) ht = (1\u2212 zt) ht\u22121 + zt h\u0303t, (4)\nwhich are the usual GRU updates, but with the recurrent state replaced by gt. Finally, we must slice the output vector ht back into its constituents het for all e \u2208 Ef by extracting the appropriate dimensions."}, {"heading": "3.3. Multiple Sequences", "text": "In certain applications, we have multiple sequences whose elements interact with each other via known relationships. Continuing with our motivating example, Figure 3 shows\nan example where the first sequence is a context passage and the second sequence is a question posed over the passage. The sequences are further augmented with coreference and hypernymy relations, resulting in an undirected cyclic graph.\nWe would like to decompose this graph into a collection of DAGs and use the MAGE-GRU presented above to learn representations of the elements in the sequences. Also, we would like to preserve the order of the original sequences in the decomposed DAGs. Suppose we have S sequences X1, . . . , XS . One way to do this is as follows: for each permutation of the collection of sequences (Xk1 , Xk2 , . . . , XkS ), treat it as a single long sequence and decompose into forward and backward subgraphs as described in Section 3.1. However, this results in 2S! DAGs, which is expensive except for very small S. Instead, we propose here taking one random permutation of the sequences and decomposing it into the forward and backward subgraphs. In this manner, each edge in the graph is still traversed twice (once in both directions), without incurring any additional cost compared to processing the sequences individually. Moreover, multi-layer extensions of the MAGE-GRU can allow information to flow through arbitrary paths through the graph."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Text Comprehension with Coreference", "text": "Text comprehension tasks involve tuples of the form (d, q, a), where d is a context document and q is a question whose answer a can be inferred from reading d. In the extractive setting the answer is a span of text in the context document. Often a set of candidates C is available, in which case the task becomes a classification task. Several large-scale benchmarks (Rajpurkar et al., 2016; Onishi et al., 2016) and deep learning models (Munkhdalai & Yu, 2016) have been proposed for this task recently.\nComprehending a document requires complex reasoning processes and prior knowledge on the part of the reader. One of these processes is coreference resolution, where the reader must identify expressions in text referring to the same entity (see red edges in Figures 1, 3). Chu et al. (2016) show that existing state-of-the art models have poor\nperformance in cases where some form of coreference resolution is required. A separate line of work (Clark & Manning, 2015), however, has led to the development of sophisticated coreference annotators1, and here we use these annotations over document and question pairs as an explicit memory signal for an RNN reader. The intuition is that when processing referring expressions, the model should also receive information about previous mentions of the referent entity.\nWe study the effect of adding an explicit coreference signal to RNN based models for three well studied text comprehension benchmarks. Following previous work (Hermann et al., 2015; Kadlec et al., 2016) our basic architecture consists of bidirectional GRUs to encode the document and query into a matrix Hd = [hd1, . . . , h d |d|] and vector h\nq respectively. Next the query vector is used to compute an attention distribution over the document,\n\u03b1 = softmax (hq)T Hd (5)\nFor extractive tasks, we use this attention distribution directly to predict the answer, using the attention-sum mechanism suggested by Kadlec et al. (2016). Hence, the probability of selecting token w as the answer is given by\u2211\ni\u2208I(w,d) \u03b1i, where I(w, d) is the set of positions w occurs in d. For classification tasks, we select the answer among the candidates C as follows:\nhd = \u03b1THd\npC = softmax((hd)TWC) a\u0302 = argmaxCpC (6)\nwhere WC is a lookup table of output embeddings, one for each candidate.\nTo test our contribution, we replace the pair of bi-GRUs with the single MAGE-GRU model described for multiple sequences for computing the document and query representations, and compare the final performance. As a baseline we also compare to the setting where coreference information is added as extra features at the input of the GRU. Let M be the number of coreference chains for the (d, q) pair: we append a one-hot vector ot \u2208 {0, 1}M to the input of the GRU xt indicating which coreference chain, if any, that token is a part of. Such additional features were shown to be useful by Wang et al. (2017). Henceforth, we refer to this baseline as \u201cone-hot\u201d.\nDhingra et al. (2016) presented a multi-layer architecture called GA Reader for text comprehension which achieves state of the art performance over several datasets. To further test whether our model can improve over this competitive baseline, we replace the bi-GRU at every layer of the GA Reader with our proposed MAGE-GRU.\n1http://stanfordnlp.github.io/CoreNLP/ coref.html"}, {"heading": "4.2. Performance Comparison", "text": "Story Based QA: Our first benchmark is the bAbi dataset from Weston et al. (2015), a set of 20 toy tasks aimed at measuring the ability of agents to reason about natural language. In each task the agent is presented with a simple story about entities in operating in an environment, followed by a question about the final state of that environment. Different tasks measure different reasoning abilities on the part of the agent, including chaining facts, counting, deduction, induction and others. An example2 is shown in Figure 3. The official release of the dataset3 comes with two versions, we focus on the more difficult 1K split in this work. Following Seo et al. (2017b), we ran 10 random initializations of the model and report the test set performance for the model with the best validation set performance.\nThe natural language in the stories was generated using templates, which are easy to learner. Rather, the difficulty of these tasks lies in tracking the state of multiple entities across long distances in the input sequence, which makes them particularly suitable for testing our proposed model with explicit memory. Specifically, we connect consecutive mentions of the same entity with coreference links, which are easily extracted due to the synthetic nature of the language. Table 1 shows a comparison of previous works with our proposed models and several baselines. Our model achieves new state-of-the-art results, outperforming strong baselines such as QRNs. Moreover, we observe that the proposed MAGE architecture can substantially improve the performance for both bi-GRUs and GAs. Adding the same information as one-hot features fails to improve the performance, which indicates that the inductive bias we employ on MAGE is useful. The DAG-RNN baseline from Shuai et al. (2016) and the shared version of MAGE (where edge representations are tied) also perform worse, showing that our proposed architecture is superior.\nOur motivation in incorporating the DAG structure in text comprehension models is to help the reader model long term dependencies in the input sequences. To test how well it is able to do that, we constructed a new version of the bAbi tasks, called bAbi-Mix, where each story is a mixed version of two independent stories in two different worlds. This was done by first replacing all entity mentions in one of the stories with alternates, so \u201cDaniel\u201d becomes \u201cDavid\u201d, \u201cmilk\u201d becomes \u201cjuice\u201d and so on. Then we mixed the sentences in the two stories, in random order, and asked a question about one of the stories. As a result, the stories become longer, and relevant information about entities has to be tracked over longer distances. Answering the questions is still trivial for human readers, but\n2the actual dataset only contains named mentions and not pronouns. These were introduced in the figure for exposition.\n3https://research.fb.com/downloads/babi/\nbecomes even more challenging for RNN models. Table 2 shows the performance of QRNs and MAGEs. Both variants of MAGE substantially outperform QRNs, which are the current state-of-the-art models on the bAbi dataset. As a case study, we also report the performances on Task 3 which requires reasoning over three supporting facts. We observe that MAGE increases the ability to model multiple facts and reduces the error rate from 72.8% to 0.3%.\nTo further gain insight into the representations being learned by the models, we visualize the scores assigned to candidate answers at intermediate points in the story by GA and GA+MAGE in Figure 4. We picked the document representation at sentence boundaries, and computed its inner product with the output lookup table WC followed by the softmax nonlinearity. The resulting distribution for two such stories is plotted in each row of Figure 4. In these examples, and many more that we analyzed, we observed that the output distribution remains more or less constant across the story. Hence, the model tries to learn a global representation which selects the correct answer, sacrificing the fine-grained information of what is happening in the story at intermediate points. In contrast, the output distribution for GA+MAGE accurately reflects the state of the story at the point that it is computed. Indeed, the biggest gains for GA+MAGE over GA are in tasks which require tracking the state of entities across the story, something that the learned representation seems to encode well. This could be potentially useful in a situation where the agent is required\nto answer multiple questions about a story.\nBroad Context Language Modeling: For our second benchmark we pick the LAMBADA dataset from Paperno et al. (2016), where the task is to predict the last word in a given passage. The passages are filtered such that human subjects were able to predict the last word when given a broad context of 4-5 sentences, but not when only given the immediately preceding sentence. This filtering process makes the task a challenging one \u2013 any model must understand the broader discourse to predict the correct word. The passages here are selected from fiction books, and hence\nlanguage comprehension is challenging, unlike the bAbi tasks. Standard language models only achieve about 7.3% accuracy on this dataset, but Chu et al. (2016) improved this to 49% by reformulating the task as a reading comprehension one, and training on a large corpus of automatically extracted passages. They treat the last sentence in the passage as the query and the remaining passage as context document, and extract the answer from this context instead. This limits the resulting model to only predict words when they are in the context, which is the case for 81% of passages in the test set.\nChu et al. (2016) also performed manual analysis on a subset of the test set and found that approximately 20% of the passages required coreference resolution to find the correct answer. With this motivation, we extracted coreference chains for each passage in the dataset using the Stanford CoreNLP tools4, and compare the performance of baseline models with our proposed MAGE-GRU, listed in Table 3. We keep the total hidden state size, and hence number of parameters, fixed for all models at 256, but for MAGE models part of this is allocated to sequential edges and part of it is allocated to coreference edges. We focus on the split where the answer is in context, since the architecture only attempts to solve these passages. Our implementation of GA gave higher performance than that reported by (Chu et al., 2016), without the use of linguistic features. We believe the difference is because we use a newer release of the code by the authors of GA (Dhingra et al., 2016).\nOn the simple bi-GRU architecture we see an improvement of 1.7% by incorporating coreference edges in the graph, whereas the one-hot baseline does not lead to any improvement. Hence, simply providing the model information about which tokens in text refer to which entity is not enough; modifying the structure of the network to allow it to propagate these memories is important. On the multi-layer GA architecture, the coreference edges again\n4http://stanfordnlp.github.io/CoreNLP/ coref.html\nlead to an improvement of 2%, setting a new state-of-theart on this dataset. In this case we see that the one-hot baseline also performs comparably. This suggests that for short documents (LAMBADA passages have an average size of 75 tokens), multi-layer architectures are able to propagate reference information when given an explicit input signal.\nTable 4 shows a comparison of the baseline GA architecture with the coreference augmented GA+MAGE model on the 100 manually labeled validation instances available from Chu et al. (2016). The small sample size for each category makes it hard to draw strong conclusions from these results. Nevertheless, we note that GA+MAGE consistently outperforms GA in each category, with the biggest improvements coming for the single name cue, semantic trigger, coreference and external knowledge labels.\nFigure 5 shows passages from the LAMBADA dataset along with the annotated coreferences and the predictions from GA and GA+MAGE models. In both cases GA predicts the wrong entities as the answer. Instead MAGE, which is able to track entity states with the provided coreference signals, answers both passages correctly.\nCloze-style QA: Lastly, we test our models on the CNN dataset from Hermann et al. (2015), which consists of pairs of news articles and a cloze-style question over the contents\nof the article5. Since its release, the dataset has recieved much attention and several deep learning architectures have been introduced with impressive results. An interesting property of the dataset is that in each article named entities and expressions referring to them have been anonymized by replacing with placeholders (such as @entity24) to make the task purely a comprehension one. For our purposes, without the use of any external tools, we augment the article with extra edges connecting mentions of the same entity, and compare the performance with and without these links. Table 5 shows the performance compari-\n5The queries are constructed by replacing an entity in the summary of the article with a placeholder, and the task is to find the entity\nson. Augmenting the bi-GRU model with MAGE leads to an improvement of 2.5% on the test set. The previous best results for this dataset were achieved by the GA Reader, and we see that adding MAGE to it leads to a further improvement of 0.7%, setting a new state of the art. This is an impressive improvement, given that previous works have reported that we are already close to the upper bound possible on this dataset (Chen et al., 2016). Note that we are not adding any information beyond what is already available in the dataset, since entity mentions are anonymized."}, {"heading": "5. Conclusions", "text": "We have presented a framework for incorporating symbolic knowledge such as linguistic relations between tokens in text into recurrent neural networks. We interpret these relations as an explicit memory signal and augment the chain structured RNN with edges connecting the arguments of the relations. Our model, MAGE-RNN, parameterizes each edge type separately, and also maintains a separate hidden state representation for distinct edges at every node. It can be interpreted as a memory-augmented RNN where the memory access is dictated by the graph structure. We apply the MAGE-RNN framework to model coreference for text comprehension tasks by preprocessing to extract coreference relations and replacing recurrent units in comprehension models with MAGE-RNN. We observe consistent improvements across three widely studied benchmarks, for both simple and sophisticated architectures. Our best results set a new state of the art on all three tasks.\nThe ultimate goal in machine learning is, of course, to be able to learn purely from data, without relying on external tools. However, in practice this is only feasible when the training dataset size is large, which is often not the case in NLP applications. We hypothesize, however, that the biggest benefit of explicit linguistic knowledge will be in cases where the data is scarce, for example as we observe in the bAbi tasks. Moreover, since coreference and entitylinking tools are widely available, models which exploit them are valuable. Hence, an interesting future direction of this work is to incorporate an attention mechanism over the edge types into MAGE-RNN and study its distribution over various sources as the dataset size varies. Coreference is one important type of linguistic knowledge that machine comprehension models can benefit from. Our encouraging results motivate us to explore other potentially useful sources of knowledge, which may include \u2013 dependency parses, semantic role labels, semantic frames, ontologies such as Wordnet (Miller et al., 1990), and databases such as Freebase (Bollacker et al., 2008). MAGE-RNN is a general framework capable of integrating all these sources into a single neural model, and we plan to investigate this research in future work."}, {"heading": "Acknowledgments", "text": "This work was funded by NSF under CCF1414030 and Google Research."}], "references": [{"title": "A neural knowledge language model", "author": ["References Ahn", "Sungjin", "Choi", "Heeyoul", "P\u00e4rnamaa", "Tanel", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1608.00318,", "citeRegEx": "Ahn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Bollacker", "Kurt", "Evans", "Colin", "Paritosh", "Praveen", "Sturge", "Tim", "Taylor", "Jamie"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "A thorough examination of the cnn/daily mail reading comprehension", "author": ["Chen", "Danqi", "Bolton", "Jason", "Manning", "Christopher D"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Broad context language modeling as reading comprehension", "author": ["Chu", "Zewei", "Wang", "Hai", "Gimpel", "Kevin", "McAllester", "David"], "venue": "arXiv preprint arXiv:1610.08431,", "citeRegEx": "Chu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2016}, {"title": "Frustratingly short attention spans in neural language modeling", "author": ["Daniluk", "Micha", "Rocktschel", "Tim", "Welbl", "Johannes", "Riedel", "Sebastian"], "venue": "arXiv preprint arXiv:1702.04521,", "citeRegEx": "Daniluk et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Daniluk et al\\.", "year": 2017}, {"title": "Gated-attention readers for text comprehension", "author": ["Dhingra", "Bhuwan", "Liu", "Hanxiao", "Cohen", "William W", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1606.01549,", "citeRegEx": "Dhingra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Tracking the world state with recurrent entity networks", "author": ["Henaff", "Mikael", "Weston", "Jason", "Szlam", "Arthur", "Bordes", "Antoine", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1612.03969,", "citeRegEx": "Henaff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Henaff et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Kocisky", "Tomas", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Text understanding with the attention sum reader network", "author": ["Kadlec", "Rudolf", "Schmid", "Martin", "Bajgar", "Ondrej", "Kleindienst", "Jan"], "venue": "arXiv preprint arXiv:1603.01547,", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Skip-thought vectors. In Advances in neural information processing", "author": ["Kiros", "Ryan", "Zhu", "Yukun", "Salakhutdinov", "Ruslan R", "Zemel", "Richard", "Urtasun", "Raquel", "Torralba", "Antonio", "Fidler", "Sanja"], "venue": null, "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "A clockwork rnn", "author": ["Koutnik", "Jan", "Greff", "Klaus", "Gomez", "Faustino", "Schmidhuber", "Juergen"], "venue": "arXiv preprint arXiv:1402.3511,", "citeRegEx": "Koutnik et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koutnik et al\\.", "year": 2014}, {"title": "Gated graph sequence neural networks", "author": ["Li", "Yujia", "Tarlow", "Daniel", "Brockschmidt", "Marc", "Zemel", "Richard"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Key-value memory networks for directly reading documents", "author": ["Miller", "Alexander", "Fisch", "Adam", "Dodge", "Jesse", "Karimi", "Amir-Hossein", "Bordes", "Antoine", "Weston", "Jason"], "venue": "arXiv preprint arXiv:1606.03126,", "citeRegEx": "Miller et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Introduction to wordnet: An on-line lexical database", "author": ["Miller", "George A", "Beckwith", "Richard", "Fellbaum", "Christiane", "Gross", "Derek", "Katherine J"], "venue": "International journal of lexicography,", "citeRegEx": "Miller et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1990}, {"title": "Reasoning with memory augmented neural networks for language comprehension", "author": ["Munkhdalai", "Tsendsuren", "Yu", "Hong"], "venue": "arXiv preprint arXiv:1610.06454,", "citeRegEx": "Munkhdalai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Munkhdalai et al\\.", "year": 2016}, {"title": "Who did what: A largescale person-centered cloze dataset", "author": ["Onishi", "Takeshi", "Wang", "Hai", "Bansal", "Mohit", "Gimpel", "Kevin", "McAllester", "David"], "venue": "arXiv preprint arXiv:1608.05457,", "citeRegEx": "Onishi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Onishi et al\\.", "year": 2016}, {"title": "Pixel recurrent neural networks", "author": ["Oord", "Aaron van den", "Kalchbrenner", "Nal", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Rajpurkar", "Pranav", "Zhang", "Jian", "Lopyrev", "Konstantin", "Liang", "Percy"], "venue": "arXiv preprint arXiv:1606.05250,", "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "The graph neural network model", "author": ["Scarselli", "Franco", "Gori", "Marco", "Tsoi", "Ah Chung", "Hagenbuchner", "Markus", "Monfardini", "Gabriele"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Scarselli et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Scarselli et al\\.", "year": 2009}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Seo", "Minjoon", "Kembhavi", "Aniruddha", "Farhadi", "Ali", "Hajishirzi", "Hannaneh"], "venue": null, "citeRegEx": "Seo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2017}, {"title": "Query-reduction networks for question answering", "author": ["Seo", "Minjoon", "Min", "Sewon", "Farhadi", "Ali", "Hajishirzi", "Hannaneh"], "venue": null, "citeRegEx": "Seo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2017}, {"title": "Dag-recurrent neural networks for scene labeling", "author": ["Shuai", "Bing", "Zuo", "Zhen", "Wang", "Gang"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Shuai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shuai et al\\.", "year": 2016}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Weston", "Jason", "Fergus", "Rob"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from treestructured long short-term memory networks", "author": ["Tai", "Kai Sheng", "Socher", "Richard", "Manning", "Christopher D"], "venue": "arXiv preprint arXiv:1503.00075,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Emergent logical structure in vector representations of neural readers", "author": ["Wang", "Hai", "Onishi", "Takeshi", "Gimpel", "Kevin", "McAllester", "David"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Weston", "Jason", "Bordes", "Antoine", "Chopra", "Sumit", "Rush", "Alexander M", "van Merri\u00ebnboer", "Bart", "Joulin", "Armand", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1502.05698,", "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Reference-aware language models", "author": ["Yang", "Zichao", "Blunsom", "Phil", "Dyer", "Chris", "Ling", "Wang"], "venue": "arXiv preprint arXiv:1611.01628,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "RNNs show excellent performance when the dependencies of interest range short spans of the sequence, however they can be notoriously hard to train to discover longer range dependencies (Koutnik et al., 2014; Bengio et al., 1994).", "startOffset": 185, "endOffset": 228}, {"referenceID": 2, "context": "RNNs show excellent performance when the dependencies of interest range short spans of the sequence, however they can be notoriously hard to train to discover longer range dependencies (Koutnik et al., 2014; Bengio et al., 1994).", "startOffset": 185, "endOffset": 228}, {"referenceID": 29, "context": "Both LSTMs and GRUs have been hugely popular for modeling sequence data (Sutskever et al., 2014; Kiros et al., 2015; Oord et al., 2016).", "startOffset": 72, "endOffset": 135}, {"referenceID": 15, "context": "Both LSTMs and GRUs have been hugely popular for modeling sequence data (Sutskever et al., 2014; Kiros et al., 2015; Oord et al., 2016).", "startOffset": 72, "endOffset": 135}, {"referenceID": 22, "context": "Both LSTMs and GRUs have been hugely popular for modeling sequence data (Sutskever et al., 2014; Kiros et al., 2015; Oord et al., 2016).", "startOffset": 72, "endOffset": 135}, {"referenceID": 5, "context": "Cho et al. (2014b) introduced a simplified version of LSTMs called Gated Recurrent Units (GRU) with has one less gate, and consequently fewer parameters.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "This led to the introduction of the attention mechanism (Bahdanau et al., 2014) which adapts the sequence model with a more explicit form of long term memory.", "startOffset": 56, "endOffset": 79}, {"referenceID": 12, "context": "Augmenting sequence models with attention has lead to significant improvements in various language modeling domains (Hermann et al., 2015).", "startOffset": 116, "endOffset": 138}, {"referenceID": 10, "context": ", 2014), further build on this idea by introducing a memory module for the soft-lookup operation, and a number of models allow the RNN to hold differentiable \u201cmemories\u201d of past elements to discover long range correlations (Graves et al., 2014).", "startOffset": 222, "endOffset": 243}, {"referenceID": 1, "context": "This led to the introduction of the attention mechanism (Bahdanau et al., 2014) which adapts the sequence model with a more explicit form of long term memory. At each time step t, the model can perform a \u201csoft\u201dlookup over all previous outputs through a weighted average \u2211t\u22121 i=1 \u03b1ihi. The weights \u03b1i are the outputs of another network whose parameters are learned from data. Augmenting sequence models with attention has lead to significant improvements in various language modeling domains (Hermann et al., 2015). Other architectures, such as Memory Networks (Weston et al., 2014), further build on this idea by introducing a memory module for the soft-lookup operation, and a number of models allow the RNN to hold differentiable \u201cmemories\u201d of past elements to discover long range correlations (Graves et al., 2014). However, Daniluk et al. (2017) showed that even memory-augmented neural models do not look beyond the immediately preceding time steps.", "startOffset": 57, "endOffset": 850}, {"referenceID": 24, "context": "Graph based neural networks (Scarselli et al., 2009) can be used to handle such data, but they are computationally expensive when the number of nodes in the graph is large.", "startOffset": 28, "endOffset": 52}, {"referenceID": 24, "context": "Models such as the Graph Neural Networks (Scarselli et al., 2009) and Gated Graph Sequence Neural Networks (Li et al.", "startOffset": 41, "endOffset": 65}, {"referenceID": 17, "context": ", 2009) and Gated Graph Sequence Neural Networks (Li et al., 2016) can be used to handle such data.", "startOffset": 49, "endOffset": 66}, {"referenceID": 30, "context": "Trees are another commonly encountered graph structure, and Tai et al. (2015) proposed Tree-Structured LSTMs for handling such data.", "startOffset": 60, "endOffset": 78}, {"referenceID": 11, "context": "Our model is also related to the Recurrent Entity Networks architecture (Henaff et al., 2016).", "startOffset": 72, "endOffset": 93}, {"referenceID": 30, "context": "The importance of reference resolution for reading comprehension was previously studied in Wang et al. (2017). They showed that models which utilize explicit coreference information, for example via the attention sum mechanism (see Section 4.", "startOffset": 91, "endOffset": 110}, {"referenceID": 33, "context": "Recently, there has been interest in incorporating symbolic knowledge, such as that from a Knowledge Base or coreference information, within RNN-based language models (Yang et al., 2016; Ahn et al., 2016).", "startOffset": 167, "endOffset": 204}, {"referenceID": 0, "context": "Recently, there has been interest in incorporating symbolic knowledge, such as that from a Knowledge Base or coreference information, within RNN-based language models (Yang et al., 2016; Ahn et al., 2016).", "startOffset": 167, "endOffset": 204}, {"referenceID": 8, "context": "(2016) and Daniluk et al. (2017) argue that overloaded use of state representations as both memory content and address makes training of the network difficult, and decompose these two functions by parameterizing them separately.", "startOffset": 11, "endOffset": 33}, {"referenceID": 23, "context": "Several large-scale benchmarks (Rajpurkar et al., 2016; Onishi et al., 2016) and deep learning models (Munkhdalai & Yu, 2016) have been proposed for this task recently.", "startOffset": 31, "endOffset": 76}, {"referenceID": 21, "context": "Several large-scale benchmarks (Rajpurkar et al., 2016; Onishi et al., 2016) and deep learning models (Munkhdalai & Yu, 2016) have been proposed for this task recently.", "startOffset": 31, "endOffset": 76}, {"referenceID": 7, "context": "Chu et al. (2016) show that existing state-of-the art models have poor", "startOffset": 0, "endOffset": 18}, {"referenceID": 12, "context": "Following previous work (Hermann et al., 2015; Kadlec et al., 2016) our basic architecture consists of bidirectional GRUs to encode the document and query into a matrix H = [h1, .", "startOffset": 24, "endOffset": 67}, {"referenceID": 14, "context": "Following previous work (Hermann et al., 2015; Kadlec et al., 2016) our basic architecture consists of bidirectional GRUs to encode the document and query into a matrix H = [h1, .", "startOffset": 24, "endOffset": 67}, {"referenceID": 14, "context": "For extractive tasks, we use this attention distribution directly to predict the answer, using the attention-sum mechanism suggested by Kadlec et al. (2016). Hence, the probability of selecting token w as the answer is given by \u2211 i\u2208I(w,d) \u03b1i, where I(w, d) is the set of positions w occurs in d.", "startOffset": 136, "endOffset": 157}, {"referenceID": 31, "context": "Such additional features were shown to be useful by Wang et al. (2017). Henceforth, we refer to this baseline as \u201cone-hot\u201d.", "startOffset": 52, "endOffset": 71}, {"referenceID": 30, "context": "Story Based QA: Our first benchmark is the bAbi dataset from Weston et al. (2015), a set of 20 toy tasks aimed at measuring the ability of agents to reason about natural language.", "startOffset": 61, "endOffset": 82}, {"referenceID": 25, "context": "Following Seo et al. (2017b), we ran 10 random initializations of the model and report the test set performance for the model with the best validation set performance.", "startOffset": 10, "endOffset": 29}, {"referenceID": 27, "context": "The DAG-RNN baseline from Shuai et al. (2016) and the shared version of MAGE (where edge representations are tied) also perform worse, showing that our proposed architecture is superior.", "startOffset": 26, "endOffset": 46}, {"referenceID": 28, "context": "N2N refers to End-to-end Memory Networks (Sukhbaatar et al., 2015).", "startOffset": 41, "endOffset": 66}, {"referenceID": 27, "context": "proposed by Shuai et al. (2016) incorporated with GA.", "startOffset": 12, "endOffset": 32}, {"referenceID": 7, "context": "Results marked with \u2020 are cf (Chu et al., 2016).", "startOffset": 29, "endOffset": 47}, {"referenceID": 7, "context": "3% accuracy on this dataset, but Chu et al. (2016) improved this to 49% by reformulating the task as a reading comprehension one, and training on a large corpus of automatically extracted passages.", "startOffset": 33, "endOffset": 51}, {"referenceID": 7, "context": "Our implementation of GA gave higher performance than that reported by (Chu et al., 2016), without the use of linguistic features.", "startOffset": 71, "endOffset": 89}, {"referenceID": 9, "context": "We believe the difference is because we use a newer release of the code by the authors of GA (Dhingra et al., 2016).", "startOffset": 93, "endOffset": 115}, {"referenceID": 7, "context": "Table 4 shows a comparison of the baseline GA architecture with the coreference augmented GA+MAGE model on the 100 manually labeled validation instances available from Chu et al. (2016). The small sample size for each category makes it hard to draw strong conclusions from these results.", "startOffset": 168, "endOffset": 186}, {"referenceID": 12, "context": "Cloze-style QA: Lastly, we test our models on the CNN dataset from Hermann et al. (2015), which consists of pairs of news articles and a cloze-style question over the contents", "startOffset": 67, "endOffset": 89}, {"referenceID": 9, "context": ", 2017a) and with \u2021 from (Dhingra et al., 2016).", "startOffset": 25, "endOffset": 47}, {"referenceID": 4, "context": "This is an impressive improvement, given that previous works have reported that we are already close to the upper bound possible on this dataset (Chen et al., 2016).", "startOffset": 145, "endOffset": 164}, {"referenceID": 19, "context": "Our encouraging results motivate us to explore other potentially useful sources of knowledge, which may include \u2013 dependency parses, semantic role labels, semantic frames, ontologies such as Wordnet (Miller et al., 1990), and databases such as Freebase (Bollacker et al.", "startOffset": 199, "endOffset": 220}, {"referenceID": 3, "context": ", 1990), and databases such as Freebase (Bollacker et al., 2008).", "startOffset": 40, "endOffset": 64}], "year": 2017, "abstractText": "Training recurrent neural networks to model long term dependencies is difficult. Hence, we propose to use external linguistic knowledge as an explicit signal to inform the model which memories it should utilize. Specifically, external knowledge is used to augment a sequence with typed edges between arbitrarily distant elements, and the resulting graph is decomposed into directed acyclic subgraphs. We introduce a model that encodes such graphs as explicit memory in recurrent neural networks, and use it to model coreference relations in text. We apply our model to several text comprehension tasks and achieve new state-of-the-art results on all considered benchmarks, including CNN, bAbi, and LAMBADA. On the bAbi QA tasks, our model solves 15 out of the 20 tasks with only 1000 training examples per task. Analysis of the learned representations further demonstrates the ability of our model to encode fine-grained entity information across a document.", "creator": "LaTeX with hyperref package"}}}