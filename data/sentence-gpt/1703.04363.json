{"id": "1703.04363", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Mar-2017", "title": "Deep Value Networks Learn to Evaluate and Iteratively Refine Structured Outputs", "abstract": "We approach structured output prediction by learning a deep value network (DVN) that evaluates different output structures for a given input. For example, when applied to image segmentation, the value network takes an image and a segmentation mask as inputs and predicts a scalar score evaluating the mask quality and its correspondence with the image. Once the value network is optimized, at inference, it finds output structures that maximize the score of the value net via gradient descent on continuous relaxations of structured outputs. This can be done by using different approaches: the linear-gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient", "histories": [["v1", "Mon, 13 Mar 2017 12:49:20 GMT  (769kb,D)", "http://arxiv.org/abs/1703.04363v1", "Submitted to ICML 2017"], ["v2", "Tue, 8 Aug 2017 08:10:34 GMT  (1587kb,D)", "http://arxiv.org/abs/1703.04363v2", "Published at ICML 2017"]], "COMMENTS": "Submitted to ICML 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["michael gygli", "mohammad norouzi", "anelia angelova"], "accepted": true, "id": "1703.04363"}, "pdf": {"name": "1703.04363.pdf", "metadata": {"source": "META", "title": "Deep Value Networks Learn toEvaluate and Iteratively Refine Structured Outputs", "authors": ["Michael Gygli", "Mohammad Norouzi", "Anelia Angelova"], "emails": ["gygli@vision.ee.ethz.ch", "mnorouzi@google.com", "anelia@google.com"], "sections": [{"heading": "1. Introduction", "text": "Structured output prediction is a fundamental problem in machine learning that entails learning a mapping from input objects to complex multivariate output structures. Because structured outputs live in a high-dimensional combinatorial space, one needs to design factored prediction models that are not only expressive, but also computationally tractable for both learning and inference. Due to computational considerations, a large body of previous work (e.g., Lafferty et al. (2001); Tsochantaridis et al. (2004)) has focused on relatively weak linear graphical models with\n1Work done during an internship at Google Brain.\npairwise or small clique potentials. Such models are not capable of learning complex non-linear correlations between output variables, making them not suitable for tasks requiring complicated high level reasoning to resolve ambiguity.\nBy contrast, in this work we do not restrict the expressive power of the prediction model, and we simply rely on gradient descent as our inference algorithm. Our key intuition is that learning to critique different output hypotheses is easier than learning to directly come up with optimal predictions. Accordingly, we build a deep value network (DVN) that takes an input x and a corresponding output structure y, both as inputs, and predicts a scalar score v(x,y) evaluating the quality of the hypothesis y and its correspondence with the input x. In teaching a DVN to evaluate different output hypotheses, a loss function `(y,y\u2217) that compares an output structure y against ground truth labels y\u2217, is the key learning signal. Our goal is to distill the loss function into the weights of a value network so that during inference, in the absence of a labeled output y\u2217, one can still rely on the value judgments of the neural network to rank output structures.\nTo enable effective iterative refinement of structured outputs via gradient descent on the score of a DVN, we relax our outputs to live in a continuous space instead of a discrete space, and extend the domain of loss function so the loss applies to continuous variable outputs. For example, for multi-label classification, instead of enforcing each output dimension yi to be binary, we let yi \u2208 [0, 1] be continuous, and we generalize the notion of F1 score to apply to continuous predictions. For image segmentation, we define a similar generalization of intersection over union. Then, we train a DVN on many output examples encouraging the network to predict very accurate (negative) loss scores for any output hypothesis. We generate the output hypotheses via gradient descent at training time, so that the value net\u2019s estimate around the inference trajectory is as accurate as possible. We also generate output hypotheses by finding adversarial cases (Goodfellow et al., 2015; Szegedy et al., 2013) \u2013 output structures that have a large disagreement between the value network scores and the loss function. ar X iv :1\n70 3.\n04 36\n3v 1\n[ cs\n.L G\n] 1\n3 M\nar 2\n01 7\nGiven a trained deep value network, we iteratively refine structured outputs by gradient ascent to maximize the score of the value network as depicted in Figure 1. All of the output variables are initialized at 0, but in about 20 to 30 gradient steps descent results are obtained.\nWe assess the effectiveness of the proposed deep value network, training on both multi-label classification based on text data and on image segmentation. In both cases we obtain state-of-the-art results even though the inputs are from different domains and the loss functions are different. Our algorithm implicitly learns a prior over output variables and takes advantage of the joint modeling of the inputs and outputs. Even given a small number of input-output pairs, we find that we are able to build powerful deep value networks. For example, on the Weizmann horses dataset (Borenstein & Ullman, 2004), without any form of pre-training, we are able to optimize 2.5 million network parameters only on 200 training images (with multiple crops). Our model is able to outperforms methods that are pre-trained on large datasets such as ImageNet (Deng et al., 2009) or methods that operate on larger input dimensions.\nThis paper presents a new Deep Value Network architecture paired with a gradient descent inference algorithm for structured output prediction. We propose a novel training objective inspired by value-based reinforcement learning algorithms to accurately evaluate the quality any inputoutput pair based on a notion of a loss function."}, {"heading": "2. Background", "text": "Structured output prediction is a supervised learning problem that entails learning a mapping from some input objects x \u2208 X (e.g., X \u2261 RM ) to multivariate discrete outputs y \u2208 Y (e.g., Y \u2261 {0, 1}N ). Given a training dataset of input-output pairs, D \u2261 {(x(i),y\u2217(i))}Ni=1, we aim to learn a mapping y\u0302(x) : X \u2192 Y from inputs to ground truth outputs. Because finding the exact ground truth output structures in a high-dimensional space is often infeasible, one measures the quality of a mapping via a loss function `(y,y\u2032) : Y\u00d7Y \u2192 R+ that evaluates the distance between different output structures. Given such a loss function, the quality of a mapping is measured by empirical loss over a validation dataset D\u2032,\u2211\n(x,y\u2217)\u2208D\u2032 `(y\u0302(x),y\u2217) (1)\nThis loss can take an arbitrary form and is often nondifferentiable. For multi-label classification, a common choice of the loss is F1 scores and for image segmentation, a typical loss is intersection over union (IOU).\nSome structured output prediction methods (Taskar et al., 2003; Tsochantaridis et al., 2004) learn a mapping from inputs to outputs via a score function s(x,y;\u03b8), which evaluates different joint configurations of inputs and outputs based on a linear function of some joint input-output features \u03c8(x,y),\ns(x,y;\u03b8) = \u03b8T\u03c8(x,y) . (2)\nThe goal of learning is to optimize a score function such that the model\u2019s prediction y\u0302,\ny\u0302 = argmax y s(x,y;\u03b8) , (3)\nare closely aligned with ground-truth labels y\u2217 as measured by empirical loss in (1) on the training set.\nEmpirical loss is not amenable to numerical optimization because the argmax in (3) is discontinuous. Structural SVM formulations (Taskar et al., 2003; Tsochantaridis et al., 2004) introduce a margin violation (slack) variable for each training pair, and define a continuous upper bound on the empirical loss. The upper bound on the loss for an example (x,y\u2217) and the model\u2019s prediction y\u0302 takes the form:\n`(y\u0302,y\u2217)\n\u2264 max y\n[ `(y,y\u2217)+s(x,y;\u03b8) ]\u2212 s(x, y\u0302;\u03b8) (4a)\n\u2264 max y\n[ `(y,y\u2217) + s(x,y;\u03b8) ]\u2212 s(x,y\u2217;\u03b8) . (4b)\nPrevious work (Taskar et al., 2003; Tsochantaridis et al., 2004), defines a surrogate objective on the empirical loss,\nby summing over the bound in (4b) for different training examples, plus a regularizer. This surrogate objective is convex in \u03b8, which makes optimization convenient.\nThis paper is inspired by the structural SVM formulation above, but we give up the convexity of the objective to obtain more expressive power using a multi-layer neural network. Specifically, we generalize the formulation above in three ways: 1) use a non-linear score function denoted v(x,y;\u03b8) that fuses \u03c8(\u00b7, \u00b7) and \u03b8 together and jointly learns them. 2) use gradient descend in y for iterative refinement of outputs to approximately find the best y\u0302(x). 3) optimize the score function with a regression objective so that the predicted scores closely approximate the negative loss values,\n\u2200y \u2208 Y, v(x,y;\u03b8) \u2248 \u2212`(y,y\u2217) . (5)\nThus, we call our model a deep value network (DVN) because it is a non-linear function trying to evaluate the value of any y \u2208 Y accurately. In the structural SVM\u2019s objective, the score surface can vary as long as it does not violate margin constraints in (4b). By contrast, we restrict the score surface much more by penalizing it whenever it overor underestimates the loss values. This seems to be beneficial as a neural network v(x,y;\u03b8) has a lot of flexibility, and adding more suitable constraints can benefit learning.\nOur value network and its inference strategy resembles the structured prediction energy network (SPEN) framework by Belanger & McCallum (2016). Importantly, our motivation and training objective are different. Belanger & McCallum rely on the structural SVM surrogate objective to train their neural models, whereas inspired by value based reinforcement learning, we learn an accurate estimates of the values as in (5). Empirically, we find that the DVN outperforms SPENs on multi-label classification using a similar neural network architecture."}, {"heading": "3. Learning a Deep Value Network", "text": "We propose a deep value network architecture, denoted v(x,y;\u03b8), to evaluate a joint configuration of an input and a corresponding output via a neural network. More specifically, the deep value network takes as input both x and y jointly, and after several layers followed by non-linearity, predicts a scalar v(x,y;\u03b8), which evaluates the quality of an output y and its compatibility with x. We assume that during training, one has access to an oracle value function v\u2217(y,y\u2217) = \u2212`(y,y\u2217) that positively relates to the quality of the masks. Such an oracle value function assigns optimal values to any input-output pairs given ground truth labels y\u2217. During training, the goal is to optimize the parameters of a value network, denoted \u03b8, to mimic the behavior of the oracle value function v\u2217(y,y\u2217) as much as possible.\nExample oracle value functions for image segmentation\nand multi-label classification include IOU and F1 metrics, which are both defined on (y,y\u2217) \u2208 {0, 1}M \u00d7 {0, 1}M ,\nv\u2217IOU(y,y \u2217) =\ny \u2229 y\u2217 y \u222a y\u2217 , (6)\nv\u2217F1(y,y \u2217) = 2 (y \u2229 y\u2217) (y \u2229 y\u2217) + (y \u222a y\u2217) . (7)\nHere y \u2229 y\u2217 denotes the number of dimension i where both yi and y\u2217i are active and y \u222a y\u2217 denotes the number of dimensions where at least one of yi and y\u2217i is active. Assuming that one has learned a suitable value network that attains v(x,y;\u03b8) \u2248 v\u2217(y,y\u2217) at every input-output pairs, in order to infer a prediction for an input x, which is valued highly by the value network, one needs to find y\u0302 = argmaxy v(x,y;\u03b8), which is described below."}, {"heading": "3.1. Gradient based inference", "text": "Since v(x,y;\u03b8) represents a complex non-linear function of (x,y) induced by a neural network, finding y\u0302 is not straightforward, and approximate inference algorithms based on graph-cut (Boykov et al., 2001) or loopy belief propagation (Murphy et al., 1999) are not easily applicable. Instead, we advocate using a simple gradient descent optimizer for inference. To facilitate that, we relax the structured output variables to live in a real-valued space. For example, instead of using y \u2208 {0, 1}M , we use y \u2208 [0, 1]M . The key to make this inference algorithm work is that during training we make sure that our value estimates are optimized along the inference trajectory.\nGiven a continuous variable y, to find a local optimum of v(x,y;\u03b8) w.r.t. y, we start from an initial prediction y(0) (i.e., y(0) = [0]M in all of our experiments), followed by gradient ascent for several steps,\ny(t+1) = PY ( y(t) + \u03b7 d\ndy v(x,y(t);\u03b8)\n) , (8)\nwhere PY denotes an operator that projects the predicted outputs back to the feasible set of solutions so that y(t+1) remains in Y . In the simplest case, where Y = [0, 1]M , the PY operator projects dimensions smaller than zero back to zero, and dimensions larger than one to one. After the final gradient step T , we simply round y(T ) to become discrete. Empirically, we find that for a trained DVN, the generated y(T )\u2019s tend to become nearly binary themselves."}, {"heading": "3.2. Optimization", "text": "To train a DVN using an oracle value function, first, one needs to extend the domain of v\u2217(y,y\u2217) so it applies to continuous output y\u2019s. For our IOU and F1 scores, we simply extend the notions of intersection and union by using\nelement-wise min and max operators,\ny \u2229 y\u2217 = \u2211M\ni=1 min (yi, y\n\u2217 i ) , (9)\ny \u222a y\u2217 = \u2211M\ni=1 max (yi, y\n\u2217 i ) . (10)\nSubstituting (9) and (10) into (6) and (7) provides a generalization of IOU and F1 score to [0, 1]M \u00d7 [0, 1]M .\nOur training objective aims at minimizing the discrepancy between v(x(i),y(i)) and v\u2217(i) on a training set of triplets (input, output, value\u2217) denotedD \u2261 {(x(i),y(i), v\u2217(i)}Ni=1. Very much like Q-learning (Watkins & Dayan, 1992), this training set evolves over time, and one can make use of an experience replay buffer. In Section 3.3, we discuss several strategies to generate training tuples and in our experiments we evaluate such strategies in terms of their empirical loss, once a gradient based optimizer is used to find y\u0302.\nGiven a dataset of training tuples, one can use an appropriate loss to regress v(x,y) to v\u2217 values. More specifically, since both IOU and F1 scores lie between 0 and 1, we used a cross-entropy loss between oracle values vs. our DVN values. As such, our neural network v(x,y) has a sigmoid non-linearity at the top to predict a number between 0 and 1, and the loss takes the form,\nLCE(\u03b8) = \u2211\n(x,y,v\u2217)\u2208D\n\u2212 v\u2217 log v(x,y;\u03b8)\n\u2212 (1\u2212 v\u2217) log(1\u2212 v(x,y;\u03b8)) (11)\nThe exact form of the loss does not have a significant impact on the performance and other loss functions can be used, e.g., L2. A high level overview for training a DVN is shown in Algorithm 1. For simplicity, we show the case when not using a queue and batch size = 1."}, {"heading": "3.3. Generating training tuples", "text": "Each training tuple comprises an input, an output, and a corresponding oracle value, i.e., (x,y, v\u2217). The way training tuples are generated significantly impacts the performance of our structured prediction algorithm. In particular, it is important that the tuples are chosen such that they provide a good coverage of the space of possible outputs and result in a large learning signal. There exist several ways to generate training tuples including: \u2022 running gradient based inference during training. \u2022 generating adversarial tuples that have a large dis-\ncrepancy between v(x,y;\u03b8) and v\u2217(y,y\u2217). \u2022 random samples from Y , maybe biased towards y\u2217.\nWe elaborate on these methods below, and present a comparison of their performance in Section 5.4. Our ablation experiments suggest that combining examples from gradient based inference with adversarial tuples works best.\nAlgorithm 1 Deep Value Network training 1: function TRAINEPOCH(training buffer D, initial weights \u03b8,\nlearning rate \u03bb) 2: while not converged do 3: (x,y\u2217) \u223c D . Get a training example 4: y\u2190 GENERATEOUPUT(x,\u03b8) . cf. Sec. 3.3 5: v\u2217 \u2190 v\u2217(y,y\u2217) . Get oracle value for y 6: . Compute loss based on estimation error cf. (11) 7: L \u2190 \u2212v\u2217 log v(x,y;\u03b8) \u2212(1\u2212 v\u2217) log(1\u2212 v(x,y;\u03b8)) 8: \u03b8 \u2190 \u03b8 \u2212 \u03bb d\nd\u03b8 L . Update DVN weights\n9: end while 10: end function\nGround truth. In this setup we simply add the ground truth outputs y\u2217 into training with a v\u2217 = 1 to provide some positive examples.\nInference. In this scenario, we generate samples by running a gradient based inference algorithm (Section 3.1) along our training. This procedure is useful because it helps learning a good value estimate on the output hypotheses that are generated along the inference trajectory at test time. To speed up training, we run a parallel inference job using slightly older neural network weights and accumulate the inferred examples in a queue.\nRandom samples. In this approach, we sample a solution y proportional to its exponentiated oracle value, i.e., y is sampled with probability p(y) \u221d exp{v\u2217(y,y\u2217)/\u03c4}, where \u03c4 > 0 controls the concentration of samples in the vicinity of the ground truth. At \u03c4 = 0 we recover the ground truth samples above. We follow (Norouzi et al., 2016) and sample from the exponentiated value distribution using stratified sampling, where we group y\u2019s according to their values. This approach provides a good coverage of the space of possible solutions.\nAdversarial tuples. We maximize the cross-entropy loss used to train the value network (11) to generate adversarial tuples again using a gradient based optimizer (e.g., see (Goodfellow et al., 2015; Szegedy et al., 2013). Such adversarial tuples are the outputs y for which the network over- or underestimates the oracle values the most. This strategy finds some difficult tuples that provide a useful learning signal, while ensuring that the value network has a minimum level of accuracy across all outputs y."}, {"heading": "4. Related work", "text": "Our approach is inspired in part by the success of previous work on value-based reinforcement learning (RL) such as Q-learning (Watkins, 1989; Watkins & Dayan, 1992) (see (Sutton & Barto, 1998) for an overview). The main idea is to learn an estimate of the future reward under the optimal behavior policy at any point in time. Recent RL\nalgorithms use a neural network function approximator as the model to estimate the action values (Van Hasselt et al., 2016). We adopt similar ideas for strucutred output prediction, where we use the task loss as the optimal value estimator. Unlike RL, we use a gradient based inferenece algorithm to find optimal solutions at test time.\nGradient based inference, sometimes called deep dreaming has led to impressive artwork and has been influential in designing DVN (Gatys et al., 2015; Mordvintsev et al., 2015; Nguyen et al., 2016; Dumoulin et al., 2016). Deep dreaming and style transfer methods iteratively refine the input to a neural net to optimize a prespecified objective. Such methods often use a pre-trained network to define a notion of a perceptual loss (Johnson et al., 2016). By contrast, we train a task specific value network to learn the characteristics of a task specific loss function and we learn the network\u2019s weights from scratch.\nThere has been a surge of recent interest in using neural networks for structured prediction (Zheng et al., 2015; Chen et al., 2015; Song et al., 2016). The Structured Prediction Energy Network (SPEN) of (Belanger & McCallum, 2016) based on (LeCun et al., 2006) is the one most related to the DVN architecture as SPENs also assign a non-linear score to each input-output configuration and use a gradient based inference algorithm to find a final solution. Importantly, the motivation and the objective function for SPENs and DVNs are distinct \u2013 SPENs rely on a max-margin surrogate objective whereas we directly regress the energy of an input-output pair to its corresponding loss. Unlike SPENs that only consider multi-label classification problems, we also train a deep convolutional network to successfully address complex and high-dimensional image segmentation problems.\nImage segmentation (Arbelaez et al., 2012; Carreira et al., 2012; Girshick et al., 2014; Hariharan et al., 2015), is a key problem in computer vision and a canonical example of structured prediction. Many segmentation approaches based on Convolutional Neural Networks (CNN) have been proposed (Girshick et al., 2014; Chen et al., 2014; Eigen & Fergus, 2015; Long et al., 2015; Ronneberger et al., 2015; Noh et al., 2015). Most use a deep neural network to make a per-pixel prediction, thereby modeling pairs of pixels as being conditionally independent given the input.\nTo diminish the conditional independence problem, recent techniques propose to model dependencies among output labels to refine an initial CNN-based coarse segmentation. Different ways to incorporate pairwise dependencies within a segmentation mask to obtain more expressive models are proposed in (Chen et al., 2014; 2016; Ladicky\u0300 et al., 2013; Zheng et al., 2015). Such methods perform joint inference of the segmentation mask dimensions via graph-cut (Li et al., 2015), message passing (Kra\u0308henbu\u0308hl &\nKoltun, 2011) or loopy belief propagation (Murphy et al., 1999), to name a few variants. Some methods incorporate higher order potentials in CRFs (Kohli et al., 2009) or model global shape priors with Restricted Boltzmann Machines (Li et al., 2013; Kae et al., 2013; Yang et al., 2014; Eslami et al., 2014). Other methods do not rely on a graphical model, but instead learn to iteratively refine an initial prediction with CNNs, which may just be a coarse segmentation mask (Safar & Yang, 2015; Pinheiro et al., 2016; Li et al., 2016).\nBy contrast, this paper presents a new framework for training a score function by keeping a gradient based inference algorithm in mind during training. Our deep value network framework applies to image segmentation and other structured prediction tasks. A key difference with some of the methods above is that we do not exploit complex combinatorial strcutures and special constraints such as submodularity to design specific inference algorithms. Rather, we use expressive energy models and the simplest conceivable inference algorithm of all \u2013 gradient descent."}, {"heading": "5. Experimental evaluation", "text": "We evaluate the proposed Deep Value Networks on three tasks: On multi-class text analysis (Section 5.1), on binary image segmentation (Section 5.2) and on a three-class face segmentation task (Section 5.3). Section 5.4 investigates the sampling mechanisms and Section 5.5 visualizes the prior over the label distribution that the model learned."}, {"heading": "5.1. Multi-label classification", "text": "We start by evaluating the method on the task of predicting tags from text inputs. We use standard benchmarks in multi-label classification, namely Bibtex and Bookmarks, introduced in (Katakis et al., 2008). In this task, multiple labels are possible per example, and the correct number is not known. Given the structure in the label space, methods modeling label correlations often outperform models with independent label predictions. We compare our method to standard baselines including per-label logistic regression from (Lin et al., 2014), and a two-layer neural network with cross entropy loss (Belanger & McCallum, 2016), as well as SPENs (Belanger & McCallum, 2016) and PRLR (Lin et al., 2014), which is the state-of-the-art on these datasets. To allow direct comparison with SPENs, we adopt the same architecture in this paper. Such an architecture combines local predictions that are non-linear in x, but linear in y, with a so-called global network, which scores label configuration with a non-linear function of y independent of x (see Belanger & McCallum (2016), Eqs. (3) - (5)). Both local prediction and global networks have one or two hidden layers with Softplus non-linerarities. We follow the same experimental protocol and report F1 scores on the same test\nInput size 24x24\n5x5\nstride 1\n5x5\nstride 2\n5x5\nstride 2\n5x5\n2\n24x24 12x12\n6x6\n3+k 64 128\n128\n384 192\n1\nImage+Mask Layer 1 Layer 2 Layer 3 Layer 4\nOutput\nLayer 5\nFigure 2. A deep value network with a feed-forward convolutional architecture, used for segmentation. The network takes an image and a segmentation mask as input and predicts a scalar evaluating the compatibility between the input pairs.\nsplit as (Belanger & McCallum, 2016).\nThe results are summarized in Table 1. As can be seen from the table, our method outperforms the logistic regression baselines by a large margin. It also significantly improves over SPEN, despite not using any pre-training. SPEN, on the other hand, relies on pre-training of the feature network with a logistic loss to obtain good results. Our results even outperform (Lin et al., 2014). This is encouraging, as their method is specific to classification and encourages sparse and low-rank predictions, whereas our technique does not have such dataset specific regularizers."}, {"heading": "5.2. Weizmann horses", "text": "The Weizmann horse dataset (Borenstein & Ullman, 2004) is a commonly used dataset for evaluating image segmentation algorithms (Li et al., 2013; Yang et al., 2014; Safar & Yang, 2015). The dataset consists of 328 images of left oriented horses and their binary segmentation masks. We follow (Li et al., 2013; Yang et al., 2014; Safar & Yang, 2015) and evaluate the segmentation results at 32\u00d732 dimensions. This dataset is well established in the literature for segmentation and proper segmentation of horses requires learning strong shape priors. The low resolution setting of 32\u00d732 is challenging because parts such as the legs are often barely visible in the RGB image, thus relying on a learnt, complex shape model is important. We follow the experimentation protocol and report results on the same test split as (Li et al., 2013).\nFor the deep value network we use a simple CNN archi-\ntecture consisting of 3 convolutional and 2 fully connected layers (Figure 2). We use a learning rate of 0.01 and apply dropout on the first fully connected layer with the keeping probability 0.75 as determined on the validation set. We empirically found \u03c4 = 0.05 to work best for stratified sampling. For training data augmentation purposes we randomly crop the image, similar to (Krizhevsky et al., 2012). At test time, various strategies are possible to obtain a full resolution segmentation, which we investigate in Section 5.4. For comparison we also implemented a Fully Convolutional Network (FCN) baseline (Long et al., 2015), by using the same convolutional layers as for the value network (cf. Figure 2). If not explicitly stated, masks are averaged over over 36 crops for our model and (Long et al., 2015) (see below).\nWe test and compare our model on the Weizmann horses segmentation task in Table 2. We tune the hyperparameters of the model on a validation set, and once best hyper-parameters are found, then fine tume on the combination of training and validation sets. We report the mean image IOU, as well as the IOU over the whole test set combined, as commonly done in the literature. It is clear that our approach outperforms previous methods by a significant margin on both metrics. Our model shows strong segmentation results, without relying on externally trained CNN features as (e.g., Safar & Yang (2015)). The weights of our value network are learned from scratch on crops of just 200 training images. Even though the number of examples is very small for this dataset, we did not observe overfitting during training, which we attribute to being able to generate a large set of segmentation masks for training.\nWe show qualitative results for CHOPPS (Li et al., 2013), our implementation of fully convolutional networks (FCN) (Long et al., 2015), and our DVN model in Figure 3. When comparing our model to FCN, trained on the same data and resolution, we find that the FCN has challenges correctly segmenting legs and ensuring that the segmentation masks have a single connected component (e.g., Figure 3, last two row). Indeed, the masks produced by the\nDVN correspond to much more reasonable horse shapes as opposed to those of other methods \u2013 the DVN seem capable of learning complex shape models and effectively grounding them to visual evidence. We also note that in our comparison in Table 2, most prior methods use larger inputs (e.g., 128\u00d7128), but are still able to obtain the best results."}, {"heading": "5.3. Labeled Faces in the Wild", "text": "The Labeled Faces in the Wild (LFW) introduced in (Huang et al., 2007) is a dataset designed for face recognition and contains more than 13000 images. A subset of 2927 was later annotated for segmentation by (Kae et al., 2013). The labels are annotated on a superpixel level and consist of three classes: face, hair and background. We use this data in order to test the adaption of our approach to multiclass segmentation problems. We use the same train, validation and test splits as (Kae et al., 2013; Tsogkas et al., 2015). As our method predicts labels for each pixel, we follow (Tsogkas et al., 2015) and map these to superpixels by using the most frequent label in a superpixel as its class. To train the value network we predict mean pixel accuracy\ninstead of superpixel accuracy for efficiency of computing the labels.\nTable 3 shows quantitative results. Our method performs reasonably well, but is outperformed by state of the art methods on this dataset. We attribute this to three reasons. (i) the pre-training and more direct optimization of the perpixel prediction methods of (Tsogkas et al., 2015; Long et al., 2015), (ii) the input resolution and (iii) the properties of the dataset. In contrast to the horses, faces do not have thin parts and exhibit limited deformations. Thus, a feed forward method as used in (Long et al., 2015), which produces coarser and smooth predictions is sufficient to obtain good results. Indeed, this has also been observed in the negligible improvement of refining CNN predictions with Conditional Random Fields and Restricted Boltzmann machines (cf. Table 3 last three rows). Despite this, our model is able to learn a prior on the shape and align it with the image evidence in most cases. Some failure cases include failing to recognize subtle and more rare parts such as mustaches, given their small size, and difficulties in correctly labeling blond hair. Figure 4 shows qualitative results of our segmentation method on this dataset."}, {"heading": "5.4. Ablation experiments", "text": "In this section we analyze different configurations of our method. As already mentioned, generating appropriate training data for our method is key to learning good value networks. We compare 3 main approaches: 1) inference + ground truth, 2) inference + stratified sampling, and 3) inference + adversarial training. These experiments are con-\nducted on the Weizmann dataset, described above. Table 4, top portion, reports IOU results for different approaches for training the dataset. As can be seen, including adversarial training works best, followed by stratified sampling. Both of these methods help explore the space of segmentation masks in the vicinity of ground truth masks better, as opposed to just including the ground truth masks. Adding adversarial examples works better than stratified sampling, as the adversarial examples are the masks on which the model is least accurate. Thus, these masks provide useful gradient information as to help improve the model.\nWe also investigate ways to do model averaging (Table 4, bottom portion). Averaging the segmentation masks of multiple crops leads to improved performance. When the masks are averaged na\u0131\u0308vely, the result becomes blurry, making it difficult to obtain a final segmentation. Instead, joint inference updates the complete segmentation mask in each step, using the gradients of the individual crops. This procedure leads to clean, near-binary segmentation masks. This is manifested in the performance when using the raw foreground confidence (Table 4, Mask averaging non-binary vs. Joint inference non-binary). Joint inference leads to somewhat improved segmentation results, even after binarization, in particular when using fewer crops."}, {"heading": "5.5. Visualizing the learned correlations", "text": "To visualize what the model has learned, we run our inference algorithm on the mean image of the Weizmann dataset (training split). Optionally, we perturb the mean image by adding some Gaussian noise. The masks obtained through this procedure are shown in Figure 5. As one can see, the segmentation masks found by the value network on (noisy) mean images resemble a side-view of a horse with some uncertainty on the leg and head positions. These parts have the most amount of variation in the dataset. Even though noisy images do not contain horses, the value network hallucinates proper horse silhouettes, which is what our model is trained on."}, {"heading": "6. Conclusion", "text": "This paper presents a framework for structured output prediction by learning a deep value network that predicts the quality of different output hypotheses for a given input. As the DVN learns to predict a value based on both, input and output, it implicitly learns a prior over output variables and takes advantage of the joint modelling of the inputs and outputs. By visualizing the prior for image segmentation, we indeed find that our model learns realistic shape priors. Furthermore, rather than learning a model by optimizing a surrogate loss, using DVNs allows to directly train a network to accurately predict the desired performance metric (e.g., IOU), even if it is non-differentiable. We apply our method to several standard datasets in multi-label classification and image segmentation. Our experiments show that DVNs apply to different structured prediction problems achieving state-of-the-art results with no pre-training.\nAs future work, we plan to improve the scalability and computational efficiency of our algorithm by inducing input features computed solely on x, which is going to be computed only once. The gradient based inference can improve by injecting noise to the gradient estimate, similar to Hamiltonian Monte Carlo sampling. Finally, one can explore better ways to initialize the inference process."}, {"heading": "7. Acknowledgment", "text": "We thank Kevin Murphy, Ryan & George Dahl, Vincent Vanhoucke, Zhifeng Chen, and the Google Brain team for insightful comments and discussions."}], "references": [{"title": "Semantic segmentation using regions and parts", "author": ["Arbelaez", "Pablo", "Hariharan", "Bharath", "Gu", "Chunhui", "Gupta", "Saurabh", "Bourdev", "Lubomir", "Malik", "Jitendra"], "venue": null, "citeRegEx": "Arbelaez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arbelaez et al\\.", "year": 2012}, {"title": "Structured prediction energy networks", "author": ["Belanger", "David", "McCallum", "Andrew"], "venue": null, "citeRegEx": "Belanger et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Belanger et al\\.", "year": 2016}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Boykov", "Yuri", "Veksler", "Olga", "Zabih", "Ramin"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "Boykov et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 2001}, {"title": "Semantic segmentation with second-order pooling", "author": ["Carreira", "Joao", "Caseiro", "Rui", "Batista", "Jorge", "Sminchisescu", "Cristian"], "venue": null, "citeRegEx": "Carreira et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Carreira et al\\.", "year": 2012}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["Chen", "Liang-Chieh", "Papandreou", "George", "Kokkinos", "Iasonas", "Murphy", "Kevin", "Yuille", "Alan L"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Learning deep structured models", "author": ["Chen", "Liang-Chieh", "Schwing", "Alexander", "Yuille", "Alan", "Urtasun", "Raquel"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs", "author": ["Chen", "Liang-Chieh", "Papandreou", "Iasonas", "Murphy", "Kevin", "Yuille", "Alan L"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": null, "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "A learned representation for artistic style", "author": ["Dumoulin", "Vincent", "Shlens", "Jonathon", "Kudlur", "Manjunath"], "venue": null, "citeRegEx": "Dumoulin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dumoulin et al\\.", "year": 2016}, {"title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture", "author": ["Eigen", "David", "Fergus", "Rob"], "venue": null, "citeRegEx": "Eigen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Eigen et al\\.", "year": 2015}, {"title": "The shape boltzmann machine: a strong model of object", "author": ["Eslami", "SM Ali", "Heess", "Nicolas", "Williams", "Christopher KI", "Winn", "John"], "venue": null, "citeRegEx": "Eslami et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Eslami et al\\.", "year": 2014}, {"title": "A neural algorithm of artistic style", "author": ["Gatys", "Leon A", "Ecker", "Alexander S", "Bethge", "Matthias"], "venue": null, "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"], "venue": null, "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "ICLR,", "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Hypercolumns for object segmentation and fine-grained localization", "author": ["Hariharan", "Bharath", "Arbelaez", "Pablo", "Girshick", "Ross"], "venue": null, "citeRegEx": "Hariharan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hariharan et al\\.", "year": 2015}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["Huang", "Gary B", "Ramesh", "Manu", "Berg", "Tamara", "Learned-Miller", "Erik"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "Perceptual losses for real-time style transfer and superresolution", "author": ["Johnson", "Justin", "Alahi", "Alexandre", "Fei-Fei", "Li"], "venue": null, "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Augmenting crfs with boltzmann machine shape priors for image labeling", "author": ["Kae", "Andrew", "Sohn", "Kihyuk", "Lee", "Honglak", "LearnedMiller", "Erik"], "venue": null, "citeRegEx": "Kae et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kae et al\\.", "year": 2013}, {"title": "Multilabel text classification for automated tag suggestion", "author": ["Katakis", "Ioannis", "Tsoumakas", "Grigorios", "Vlahavas"], "venue": "ECML PKDD discovery challenge,", "citeRegEx": "Katakis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Katakis et al\\.", "year": 2008}, {"title": "Robust higher order potentials for enforcing label consistency", "author": ["Kohli", "Pushmeet", "Torr", "Philip HS"], "venue": null, "citeRegEx": "Kohli et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kohli et al\\.", "year": 2009}, {"title": "Efficient inference in fully connected crfs with gaussian edge potentials", "author": ["Kr\u00e4henb\u00fchl", "Philipp", "Koltun", "Vladlen"], "venue": null, "citeRegEx": "Kr\u00e4henb\u00fchl et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kr\u00e4henb\u00fchl et al\\.", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Inference methods for crfs with cooccurrence statistics", "author": ["Ladick\u1ef3", "L\u2019ubor", "Russell", "Chris", "Kohli", "Pushmeet", "Torr", "Philip HS"], "venue": null, "citeRegEx": "Ladick\u1ef3 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ladick\u1ef3 et al\\.", "year": 2013}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Lafferty", "John", "McCallum", "Andrew", "Pereira", "Fernando"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "A tutorial on energy-based learning", "author": ["LeCun", "Yann", "Chopra", "Sumit", "Hadsell", "Raia", "M Ranzato", "F. Huang"], "venue": "Predicting structured data,", "citeRegEx": "LeCun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2006}, {"title": "Object segmentation with deep regression", "author": ["Li", "Jianchao", "Wang", "Dan", "Yan", "Canxiang", "Shan", "Shiguang"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Iterative instance segmentation", "author": ["Li", "Ke", "Hariharan", "Bharath", "Malik", "Jitendra"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Exploring compositional high order pattern potentials for structured output learning", "author": ["Li", "Yujia", "Tarlow", "Daniel", "Zemel", "Richard"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Multi-label learning with posterior regularization", "author": ["Lin", "Victoria (Xi", "Singh", "Sameer", "He", "Luheng", "Taskar", "Ben", "Zettlemoyer", "Luke"], "venue": "NIPS Workshop on Modern Machine Learning and Natural Language Processing,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"], "venue": null, "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Inceptionism: Going deeper into neural networks", "author": ["Mordvintsev", "Alexander", "Olah", "Christopher", "Tyka", "Mike"], "venue": "Google Research Blog.,", "citeRegEx": "Mordvintsev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mordvintsev et al\\.", "year": 2015}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["Murphy", "Kevin P", "Weiss", "Yair", "Jordan", "Michael I"], "venue": null, "citeRegEx": "Murphy et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Murphy et al\\.", "year": 1999}, {"title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks", "author": ["Nguyen", "Anh", "Dosovitskiy", "Alexey", "Yosinski", "Jason", "Brox", "Thomas", "Clune", "Jeff"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["Noh", "Hyeonwoo", "Hong", "Seunghoon", "Han", "Bohyung"], "venue": null, "citeRegEx": "Noh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Noh et al\\.", "year": 2015}, {"title": "Reward augmented maximum likelihood for neural structured prediction", "author": ["Norouzi", "Mohammad", "Bengio", "Samy", "Chen", "Zhifeng", "Jaitly", "Navdeep", "Schuster", "Mike", "Wu", "Yonghui", "Schuurmans", "Dale"], "venue": null, "citeRegEx": "Norouzi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2016}, {"title": "Learning to refine object segments", "author": ["P. Pinheiro", "Lin", "T.-Y", "R. Collobert", "P. Dollar"], "venue": null, "citeRegEx": "Pinheiro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pinheiro et al\\.", "year": 2016}, {"title": "Unet: Convolutional networks for biomedical image segmentation", "author": ["Ronneberger", "Olaf", "Fischer", "Philipp", "Brox", "Thomas"], "venue": null, "citeRegEx": "Ronneberger et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ronneberger et al\\.", "year": 2015}, {"title": "Learning shape priors for object segmentation via neural networks", "author": ["Safar", "Simon", "Yang", "Ming-Hsuan"], "venue": null, "citeRegEx": "Safar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Safar et al\\.", "year": 2015}, {"title": "Training deep neural networks via direct loss minimization", "author": ["Song", "Yang", "Schwing", "Alexander", "Zemel", "Richard", "Urtasun", "Raquel"], "venue": null, "citeRegEx": "Song et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Song et al\\.", "year": 2016}, {"title": "Reinforcement learning: An introduction", "author": ["Sutton", "Richard", "Barto", "Andrew"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian", "Fergus", "Rob"], "venue": null, "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": null, "citeRegEx": "Tsochantaridis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2004}, {"title": "Deep learning for semantic part segmentation with high-level guidance", "author": ["Tsogkas", "Stavros", "Kokkinos", "Iasonas", "Papandreou", "George", "Vedaldi", "Andrea"], "venue": null, "citeRegEx": "Tsogkas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tsogkas et al\\.", "year": 2015}, {"title": "Deep reinforcement learning with double q-learning", "author": ["Van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David"], "venue": null, "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Learning from delayed rewards", "author": ["Watkins", "Christopher JCH"], "venue": "PhD thesis, University of Cambridge England,", "citeRegEx": "Watkins and JCH.,? \\Q1989\\E", "shortCiteRegEx": "Watkins and JCH.", "year": 1989}, {"title": "Maxmargin boltzmann machines for object segmentation", "author": ["Yang", "Jimei", "Safar", "Simon", "Ming-Hsuan"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Conditional random fields as recurrent neural networks", "author": ["Zheng", "Shuai", "Jayasumana", "Sadeep", "Romera-Paredes", "Bernardino", "Vineet", "Vibhav", "Su", "Zhizhong", "Du", "Dalong", "Huang", "Chang", "Torr", "Philip HS"], "venue": null, "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 23, "context": ", Lafferty et al. (2001); Tsochantaridis et al.", "startOffset": 2, "endOffset": 25}, {"referenceID": 23, "context": ", Lafferty et al. (2001); Tsochantaridis et al. (2004)) has focused on relatively weak linear graphical models with", "startOffset": 2, "endOffset": 55}, {"referenceID": 13, "context": "We also generate output hypotheses by finding adversarial cases (Goodfellow et al., 2015; Szegedy et al., 2013) \u2013 output structures that have a large disagreement between the value network scores and the loss function.", "startOffset": 64, "endOffset": 111}, {"referenceID": 40, "context": "We also generate output hypotheses by finding adversarial cases (Goodfellow et al., 2015; Szegedy et al., 2013) \u2013 output structures that have a large disagreement between the value network scores and the loss function.", "startOffset": 64, "endOffset": 111}, {"referenceID": 7, "context": "Our model is able to outperforms methods that are pre-trained on large datasets such as ImageNet (Deng et al., 2009) or methods that operate on larger input dimensions.", "startOffset": 97, "endOffset": 116}, {"referenceID": 41, "context": "Some structured output prediction methods (Taskar et al., 2003; Tsochantaridis et al., 2004) learn a mapping from inputs to outputs via a score function s(x,y;\u03b8), which evaluates different joint configurations of inputs and outputs based on a linear function of some joint input-output features \u03c8(x,y),", "startOffset": 42, "endOffset": 92}, {"referenceID": 41, "context": "Structural SVM formulations (Taskar et al., 2003; Tsochantaridis et al., 2004) introduce a margin violation (slack) variable for each training pair, and define a continuous upper bound on the empirical loss.", "startOffset": 28, "endOffset": 78}, {"referenceID": 41, "context": "Previous work (Taskar et al., 2003; Tsochantaridis et al., 2004), defines a surrogate objective on the empirical loss,", "startOffset": 14, "endOffset": 64}, {"referenceID": 2, "context": "Since v(x,y;\u03b8) represents a complex non-linear function of (x,y) induced by a neural network, finding \u0177 is not straightforward, and approximate inference algorithms based on graph-cut (Boykov et al., 2001) or loopy belief propagation (Murphy et al.", "startOffset": 184, "endOffset": 205}, {"referenceID": 31, "context": ", 2001) or loopy belief propagation (Murphy et al., 1999) are not easily applicable.", "startOffset": 36, "endOffset": 57}, {"referenceID": 34, "context": "We follow (Norouzi et al., 2016) and sample from the exponentiated value distribution using stratified sampling, where we group y\u2019s according to their values.", "startOffset": 10, "endOffset": 32}, {"referenceID": 13, "context": ", see (Goodfellow et al., 2015; Szegedy et al., 2013).", "startOffset": 6, "endOffset": 53}, {"referenceID": 40, "context": ", see (Goodfellow et al., 2015; Szegedy et al., 2013).", "startOffset": 6, "endOffset": 53}, {"referenceID": 11, "context": "Gradient based inference, sometimes called deep dreaming has led to impressive artwork and has been influential in designing DVN (Gatys et al., 2015; Mordvintsev et al., 2015; Nguyen et al., 2016; Dumoulin et al., 2016).", "startOffset": 129, "endOffset": 219}, {"referenceID": 30, "context": "Gradient based inference, sometimes called deep dreaming has led to impressive artwork and has been influential in designing DVN (Gatys et al., 2015; Mordvintsev et al., 2015; Nguyen et al., 2016; Dumoulin et al., 2016).", "startOffset": 129, "endOffset": 219}, {"referenceID": 32, "context": "Gradient based inference, sometimes called deep dreaming has led to impressive artwork and has been influential in designing DVN (Gatys et al., 2015; Mordvintsev et al., 2015; Nguyen et al., 2016; Dumoulin et al., 2016).", "startOffset": 129, "endOffset": 219}, {"referenceID": 8, "context": "Gradient based inference, sometimes called deep dreaming has led to impressive artwork and has been influential in designing DVN (Gatys et al., 2015; Mordvintsev et al., 2015; Nguyen et al., 2016; Dumoulin et al., 2016).", "startOffset": 129, "endOffset": 219}, {"referenceID": 16, "context": "Such methods often use a pre-trained network to define a notion of a perceptual loss (Johnson et al., 2016).", "startOffset": 85, "endOffset": 107}, {"referenceID": 46, "context": "There has been a surge of recent interest in using neural networks for structured prediction (Zheng et al., 2015; Chen et al., 2015; Song et al., 2016).", "startOffset": 93, "endOffset": 151}, {"referenceID": 5, "context": "There has been a surge of recent interest in using neural networks for structured prediction (Zheng et al., 2015; Chen et al., 2015; Song et al., 2016).", "startOffset": 93, "endOffset": 151}, {"referenceID": 38, "context": "There has been a surge of recent interest in using neural networks for structured prediction (Zheng et al., 2015; Chen et al., 2015; Song et al., 2016).", "startOffset": 93, "endOffset": 151}, {"referenceID": 24, "context": "The Structured Prediction Energy Network (SPEN) of (Belanger & McCallum, 2016) based on (LeCun et al., 2006) is the one most related to the DVN architecture as SPENs also assign a non-linear score to each input-output configuration and use a gradient based inference algorithm to find a final solution.", "startOffset": 88, "endOffset": 108}, {"referenceID": 0, "context": "Image segmentation (Arbelaez et al., 2012; Carreira et al., 2012; Girshick et al., 2014; Hariharan et al., 2015), is a key problem in computer vision and a canonical example of structured prediction.", "startOffset": 19, "endOffset": 112}, {"referenceID": 3, "context": "Image segmentation (Arbelaez et al., 2012; Carreira et al., 2012; Girshick et al., 2014; Hariharan et al., 2015), is a key problem in computer vision and a canonical example of structured prediction.", "startOffset": 19, "endOffset": 112}, {"referenceID": 12, "context": "Image segmentation (Arbelaez et al., 2012; Carreira et al., 2012; Girshick et al., 2014; Hariharan et al., 2015), is a key problem in computer vision and a canonical example of structured prediction.", "startOffset": 19, "endOffset": 112}, {"referenceID": 14, "context": "Image segmentation (Arbelaez et al., 2012; Carreira et al., 2012; Girshick et al., 2014; Hariharan et al., 2015), is a key problem in computer vision and a canonical example of structured prediction.", "startOffset": 19, "endOffset": 112}, {"referenceID": 12, "context": "Many segmentation approaches based on Convolutional Neural Networks (CNN) have been proposed (Girshick et al., 2014; Chen et al., 2014; Eigen & Fergus, 2015; Long et al., 2015; Ronneberger et al., 2015; Noh et al., 2015).", "startOffset": 93, "endOffset": 220}, {"referenceID": 4, "context": "Many segmentation approaches based on Convolutional Neural Networks (CNN) have been proposed (Girshick et al., 2014; Chen et al., 2014; Eigen & Fergus, 2015; Long et al., 2015; Ronneberger et al., 2015; Noh et al., 2015).", "startOffset": 93, "endOffset": 220}, {"referenceID": 29, "context": "Many segmentation approaches based on Convolutional Neural Networks (CNN) have been proposed (Girshick et al., 2014; Chen et al., 2014; Eigen & Fergus, 2015; Long et al., 2015; Ronneberger et al., 2015; Noh et al., 2015).", "startOffset": 93, "endOffset": 220}, {"referenceID": 36, "context": "Many segmentation approaches based on Convolutional Neural Networks (CNN) have been proposed (Girshick et al., 2014; Chen et al., 2014; Eigen & Fergus, 2015; Long et al., 2015; Ronneberger et al., 2015; Noh et al., 2015).", "startOffset": 93, "endOffset": 220}, {"referenceID": 33, "context": "Many segmentation approaches based on Convolutional Neural Networks (CNN) have been proposed (Girshick et al., 2014; Chen et al., 2014; Eigen & Fergus, 2015; Long et al., 2015; Ronneberger et al., 2015; Noh et al., 2015).", "startOffset": 93, "endOffset": 220}, {"referenceID": 4, "context": "Different ways to incorporate pairwise dependencies within a segmentation mask to obtain more expressive models are proposed in (Chen et al., 2014; 2016; Ladick\u1ef3 et al., 2013; Zheng et al., 2015).", "startOffset": 128, "endOffset": 195}, {"referenceID": 22, "context": "Different ways to incorporate pairwise dependencies within a segmentation mask to obtain more expressive models are proposed in (Chen et al., 2014; 2016; Ladick\u1ef3 et al., 2013; Zheng et al., 2015).", "startOffset": 128, "endOffset": 195}, {"referenceID": 46, "context": "Different ways to incorporate pairwise dependencies within a segmentation mask to obtain more expressive models are proposed in (Chen et al., 2014; 2016; Ladick\u1ef3 et al., 2013; Zheng et al., 2015).", "startOffset": 128, "endOffset": 195}, {"referenceID": 25, "context": "Such methods perform joint inference of the segmentation mask dimensions via graph-cut (Li et al., 2015), message passing (Kr\u00e4henb\u00fchl & Koltun, 2011) or loopy belief propagation (Murphy et al.", "startOffset": 87, "endOffset": 104}, {"referenceID": 31, "context": ", 2015), message passing (Kr\u00e4henb\u00fchl & Koltun, 2011) or loopy belief propagation (Murphy et al., 1999), to name a few variants.", "startOffset": 81, "endOffset": 102}, {"referenceID": 19, "context": "Some methods incorporate higher order potentials in CRFs (Kohli et al., 2009) or model global shape priors with Restricted Boltzmann Machines (Li et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 27, "context": ", 2009) or model global shape priors with Restricted Boltzmann Machines (Li et al., 2013; Kae et al., 2013; Yang et al., 2014; Eslami et al., 2014).", "startOffset": 72, "endOffset": 147}, {"referenceID": 17, "context": ", 2009) or model global shape priors with Restricted Boltzmann Machines (Li et al., 2013; Kae et al., 2013; Yang et al., 2014; Eslami et al., 2014).", "startOffset": 72, "endOffset": 147}, {"referenceID": 45, "context": ", 2009) or model global shape priors with Restricted Boltzmann Machines (Li et al., 2013; Kae et al., 2013; Yang et al., 2014; Eslami et al., 2014).", "startOffset": 72, "endOffset": 147}, {"referenceID": 10, "context": ", 2009) or model global shape priors with Restricted Boltzmann Machines (Li et al., 2013; Kae et al., 2013; Yang et al., 2014; Eslami et al., 2014).", "startOffset": 72, "endOffset": 147}, {"referenceID": 35, "context": "Other methods do not rely on a graphical model, but instead learn to iteratively refine an initial prediction with CNNs, which may just be a coarse segmentation mask (Safar & Yang, 2015; Pinheiro et al., 2016; Li et al., 2016).", "startOffset": 166, "endOffset": 226}, {"referenceID": 26, "context": "Other methods do not rely on a graphical model, but instead learn to iteratively refine an initial prediction with CNNs, which may just be a coarse segmentation mask (Safar & Yang, 2015; Pinheiro et al., 2016; Li et al., 2016).", "startOffset": 166, "endOffset": 226}, {"referenceID": 18, "context": "We use standard benchmarks in multi-label classification, namely Bibtex and Bookmarks, introduced in (Katakis et al., 2008).", "startOffset": 101, "endOffset": 123}, {"referenceID": 28, "context": "We compare our method to standard baselines including per-label logistic regression from (Lin et al., 2014), and a two-layer neural network with cross entropy loss (Belanger & McCallum, 2016), as well as SPENs (Belanger & McCallum, 2016) and PRLR (Lin et al.", "startOffset": 89, "endOffset": 107}, {"referenceID": 28, "context": ", 2014), and a two-layer neural network with cross entropy loss (Belanger & McCallum, 2016), as well as SPENs (Belanger & McCallum, 2016) and PRLR (Lin et al., 2014), which is the state-of-the-art on these datasets.", "startOffset": 147, "endOffset": 165}, {"referenceID": 18, "context": "We use standard benchmarks in multi-label classification, namely Bibtex and Bookmarks, introduced in (Katakis et al., 2008). In this task, multiple labels are possible per example, and the correct number is not known. Given the structure in the label space, methods modeling label correlations often outperform models with independent label predictions. We compare our method to standard baselines including per-label logistic regression from (Lin et al., 2014), and a two-layer neural network with cross entropy loss (Belanger & McCallum, 2016), as well as SPENs (Belanger & McCallum, 2016) and PRLR (Lin et al., 2014), which is the state-of-the-art on these datasets. To allow direct comparison with SPENs, we adopt the same architecture in this paper. Such an architecture combines local predictions that are non-linear in x, but linear in y, with a so-called global network, which scores label configuration with a non-linear function of y independent of x (see Belanger & McCallum (2016), Eqs.", "startOffset": 102, "endOffset": 993}, {"referenceID": 28, "context": "Logistic regression (Lin et al., 2014) 37.", "startOffset": 20, "endOffset": 38}, {"referenceID": 28, "context": "4 PRLR (Lin et al., 2014) 44.", "startOffset": 7, "endOffset": 25}, {"referenceID": 28, "context": "All results except ours are taken from (Lin et al., 2014; Belanger & McCallum, 2016)", "startOffset": 39, "endOffset": 84}, {"referenceID": 28, "context": "Our results even outperform (Lin et al., 2014).", "startOffset": 28, "endOffset": 46}, {"referenceID": 27, "context": "The Weizmann horse dataset (Borenstein & Ullman, 2004) is a commonly used dataset for evaluating image segmentation algorithms (Li et al., 2013; Yang et al., 2014; Safar & Yang, 2015).", "startOffset": 127, "endOffset": 183}, {"referenceID": 45, "context": "The Weizmann horse dataset (Borenstein & Ullman, 2004) is a commonly used dataset for evaluating image segmentation algorithms (Li et al., 2013; Yang et al., 2014; Safar & Yang, 2015).", "startOffset": 127, "endOffset": 183}, {"referenceID": 27, "context": "We follow (Li et al., 2013; Yang et al., 2014; Safar & Yang, 2015) and evaluate the segmentation results at 32\u00d732 dimensions.", "startOffset": 10, "endOffset": 66}, {"referenceID": 45, "context": "We follow (Li et al., 2013; Yang et al., 2014; Safar & Yang, 2015) and evaluate the segmentation results at 32\u00d732 dimensions.", "startOffset": 10, "endOffset": 66}, {"referenceID": 27, "context": "We follow the experimentation protocol and report results on the same test split as (Li et al., 2013).", "startOffset": 84, "endOffset": 101}, {"referenceID": 27, "context": "In pu ts iz e 3 2 \u00d7 3 2 CHOPPS (Li et al., 2013) 69.", "startOffset": 31, "endOffset": 48}, {"referenceID": 45, "context": "1 2 8 \u00d7 1 2 8 MMBM2 (Yang et al., 2014) - 72.", "startOffset": 20, "endOffset": 39}, {"referenceID": 45, "context": "MMBM2 + GC (Yang et al., 2014) - 75.", "startOffset": 11, "endOffset": 30}, {"referenceID": 45, "context": "Our method outperforms all previous methods, despite using a much lower input resolution than (Yang et al., 2014) and (Safar & Yang, 2015).", "startOffset": 94, "endOffset": 113}, {"referenceID": 21, "context": "For training data augmentation purposes we randomly crop the image, similar to (Krizhevsky et al., 2012).", "startOffset": 79, "endOffset": 104}, {"referenceID": 29, "context": "For comparison we also implemented a Fully Convolutional Network (FCN) baseline (Long et al., 2015), by using the same convolutional layers as for the value network (cf.", "startOffset": 80, "endOffset": 99}, {"referenceID": 29, "context": "If not explicitly stated, masks are averaged over over 36 crops for our model and (Long et al., 2015) (see below).", "startOffset": 82, "endOffset": 101}, {"referenceID": 27, "context": "We show qualitative results for CHOPPS (Li et al., 2013), our implementation of fully convolutional networks (FCN) (Long et al.", "startOffset": 39, "endOffset": 56}, {"referenceID": 29, "context": ", 2013), our implementation of fully convolutional networks (FCN) (Long et al., 2015), and our DVN model in Figure 3.", "startOffset": 66, "endOffset": 85}, {"referenceID": 29, "context": "(2013) [2] Our implementation of FCN (Long et al., 2015)", "startOffset": 37, "endOffset": 56}, {"referenceID": 25, "context": "References: [1] Li et al. (2013) [2] Our implementation of FCN (Long et al.", "startOffset": 16, "endOffset": 33}, {"referenceID": 15, "context": "The Labeled Faces in the Wild (LFW) introduced in (Huang et al., 2007) is a dataset designed for face recognition and contains more than 13000 images.", "startOffset": 50, "endOffset": 70}, {"referenceID": 17, "context": "A subset of 2927 was later annotated for segmentation by (Kae et al., 2013).", "startOffset": 57, "endOffset": 75}, {"referenceID": 17, "context": "We use the same train, validation and test splits as (Kae et al., 2013; Tsogkas et al., 2015).", "startOffset": 53, "endOffset": 93}, {"referenceID": 42, "context": "We use the same train, validation and test splits as (Kae et al., 2013; Tsogkas et al., 2015).", "startOffset": 53, "endOffset": 93}, {"referenceID": 42, "context": "As our method predicts labels for each pixel, we follow (Tsogkas et al., 2015) and map these to superpixels by using the most frequent label in a superpixel as its class.", "startOffset": 56, "endOffset": 78}, {"referenceID": 17, "context": "23 GLOC (Kae et al., 2013) 94.", "startOffset": 8, "endOffset": 26}, {"referenceID": 17, "context": "2 5 0 2 CRF (as in Kae et al. (2013)) 93.", "startOffset": 19, "endOffset": 37}, {"referenceID": 42, "context": "DNN (Tsogkas et al., 2015) 96.", "startOffset": 4, "endOffset": 26}, {"referenceID": 42, "context": "DNN+CRF+SBM (Tsogkas et al., 2015) 96.", "startOffset": 12, "endOffset": 34}, {"referenceID": 42, "context": "(i) the pre-training and more direct optimization of the perpixel prediction methods of (Tsogkas et al., 2015; Long et al., 2015), (ii) the input resolution and (iii) the properties of the dataset.", "startOffset": 88, "endOffset": 129}, {"referenceID": 29, "context": "(i) the pre-training and more direct optimization of the perpixel prediction methods of (Tsogkas et al., 2015; Long et al., 2015), (ii) the input resolution and (iii) the properties of the dataset.", "startOffset": 88, "endOffset": 129}, {"referenceID": 29, "context": "Thus, a feed forward method as used in (Long et al., 2015), which produces coarser and smooth predictions is sufficient to obtain good results.", "startOffset": 39, "endOffset": 58}], "year": 2017, "abstractText": "We approach structured output prediction by learning a deep value network (DVN) that evaluates different output structures for a given input. For example, when applied to image segmentation, the value network takes an image and a segmentation mask as inputs and predicts a scalar score evaluating the mask quality and its correspondence with the image. Once the value network is optimized, at inference, it finds output structures that maximize the score of the value net via gradient descent on continuous relaxations of structured outputs. Thus DVN takes advantage of the joint modeling of the inputs and outputs. Our framework applies to a wide range of structured output prediction problems. We conduct experiments on multi-label classification based on text data and on image segmentation problems. DVN outperforms several strong baselines and the state-of-the-art results on these benchmarks. In addition, on image segmentation, the proposed deep value network learns complex shape priors and effectively combines image information with the prior to obtain competitive segmentation results.", "creator": "LaTeX with hyperref package"}}}