{"id": "1703.05376", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "Two-Timescale Stochastic Approximation Convergence Rates with Applications to Reinforcement Learning", "abstract": "Two-timescale Stochastic Approximation (SA) algorithms are widely used in Reinforcement Learning (RL). In such methods, the iterates consist of two parts that are updated using different stepsizes. We develop the first convergence rate result for these algorithms; in particular, we provide a general methodology for analyzing two-timescale linear SA. We apply our methodology to two-timescale RL algorithms such as GTD(0), GTD2, and TDC.\n\n\n\n\n\nThe LRS is a statistical method that generates the results for each factor of a given factor that is represented in the logarithm of the factor in each linear step. The LRS provides the first step for each factor of a given factor that is represented in the logarithm of the factor in each linear step. The LRS provides the first step for each factor of a given factor that is represented in the logarithm of the factor in each linear step.\n\nThe LRS also provides the first step for each factor of a given factor that is represented in the logarithm of the factor in each linear step.\n\nThe LRS also provides the first step for each factor of a given factor that is represented in the logarithm of the factor in each linear step.\nThe LRS also provides the first step for each factor of a given factor that is represented in the logarithm of the factor in each linear step.\nThe LRS also provides the first step for each factor of a given factor that is represented in the logarithm of the factor in each linear step.\nIn the LRS, the LRS algorithm is computed as follows:\nFirst-steps (i.e., a) are calculated as follows:\nFirst-steps (i.e., a) are calculated as follows:\nFirst-steps (i.e., a) are calculated as follows:\nFirst-steps (i.e., a) are calculated as follows:\nFirst-steps (i.e., a) are calculated as follows:\nFirst-steps (i.e., a) are calculated as follows:\nFirst-steps (i.e., a) are calculated as follows:\nFirst-steps (i.e., a) are calculated as follows:\nFirst-steps (i.e., a) are calculated as follows:\nFirst-steps (i.e., a) are calculated as follows:\nFirst-", "histories": [["v1", "Wed, 15 Mar 2017 20:23:45 GMT  (42kb)", "https://arxiv.org/abs/1703.05376v1", null], ["v2", "Wed, 31 May 2017 16:35:17 GMT  (59kb,D)", "http://arxiv.org/abs/1703.05376v2", null], ["v3", "Thu, 7 Sep 2017 07:12:14 GMT  (59kb,D)", "http://arxiv.org/abs/1703.05376v3", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["gal dalal", "balazs szorenyi", "gugan thoppe", "shie mannor"], "accepted": false, "id": "1703.05376"}, "pdf": {"name": "1703.05376.pdf", "metadata": {"source": "CRF", "title": "Two-Timescale Stochastic Approximation Convergence Rates with Applications to Reinforcement Learning", "authors": ["Gal Dalal", "Bal\u00e1zs Sz\u00f6r\u00e9nyi", "Gugan Thoppe", "Shie Mannor"], "emails": ["gald@tx.technion.ac.il", "szorenyi.balazs@gmail.com", "gugan.thoppe@gmail.com", "shie@ee.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "Stochastic Approximation (SA) is the subject of an enormous literature, both theoretical and applied (Kushner & Yin, 1997). It is used for finding optimal points, fixed points, or zeros of a function for which only noisy access is available. Consequently, SA lies at the core of many machine learning algorithms and in particular Reinforcement Learning (RL) algorithms, especially when function approximation is used.\nThe most powerful analysis tool for SA algorithms has been the Ordinary Differential Equation (ODE) method (Borkar & Meyn, 2000). The underlying idea of the ODE method is that, under the right conditions, noise effects average out and the SA iterates closely track the trajectory of the so called limiting ODE. Classical results give a convenient recipe for showing convergence (Borkar & Meyn, 2000). Hence, many RL analyses are given in that form, especially when the state-space is large and function approximation is used (Sutton et al., 2009a,b, 2015; Bhatnagar et al., 2009b). Concentration bounds for SA are, however, scarce; in fact, they are nonexistent in the case of two-timescale SA. This gives the motivation for our work."}, {"heading": "1.1 Related Work", "text": "Two-timescale SA methods are prominent in RL (Peters & Schaal, 2008; Bhatnagar et al., 2009b; Sutton et al., 2009b). Nonetheless, as mentioned before, there are no concentration bounds for these types of algorithms. Below we briefly survey related finite sample analyses for single-timescale SA and asymptotic convergence results for two-timescale RL algorithms.\nA broad rigorous study of SA is given in (Borkar, 2008); in particular, it contains concentration bounds for single-timescale methods. A more recent work (Thoppe & Borkar, 2015) obtains tighter concentration bounds under weaker assumptions for single-timescale SA using a variational methodology called Alekseev\u2019s Formula. In the context of RL, (Konda, 2002; Korda & Prashanth, 2015)\n\u2217Equal contribution\nar X\niv :1\n70 3.\n05 37\n6v 3\n[ cs\n.A I]\n7 S\nep 2\ndiscuss convergence rate for TD(0) when the stepsizes are set using knowledge about the system dynamics. We stress that our results are in similar flavor but for the two-timescale setup.\nNext, we relate to the relevant RL literature on two time-scale methods. We partition them into two principal classes: actor-critic and gradient Temporal Difference (TD). In an actor-critic setting, a policy is being evaluated by the critic in the fast timescale, and improved by the actor in the slow timescale (Peters & Schaal, 2008; Bhatnagar et al., 2009b). The second class, i.e., gradient TD methods, was introduced by (Sutton et al., 2009a). This work presented the GTD(0) algorithm, which is gradient descent version of TD(0); being applicable to the so called off-policy setting, it has a clear advantage over TD(0). Later variants, GTD2 and TDC, were reported to be faster than GTD(0) while enjoying its benefits. They were also shown to converge in the case of linear and non-linear function approximation (Sutton et al., 2009b; Bhatnagar et al., 2009a). In addition to convergence, there also exists a concentration result for the GTD family, though only for the single-timescale setting (Liu et al., 2015). That work introduced altered versions of GTD(0) and GTD2, presented them as gradient methods to some saddle-point optimization problem, and obtained concentration bounds using results from convex optimization. These algorithms differ from the original versions in two aspects: a projection step is used to keep the iterates in a convex set, and the learning rates are chosen to be of fixed ratio. The latter makes the altered algorithms single-timescale variants of the original ones."}, {"heading": "1.2 Our Contributions", "text": "Our main contributions are three-fold:\n\u2022 We provide the first concentration bound for two-timescale SA algorithms; specifically, we analyze the linear SA case. The analysis is provided as a general methodology that can be used in various fields as a \u201dhammer\u201d in a plug-and-play fashion.\n\u2022 Particularly, we show how to use our tool to obtain concentration bounds for the twotimescale RL algorithms in the gradient TD family: GTD(0), GTD2, and TDC. We are the first to obtain concentration bounds for the above algorithms in their original form.\n\u2022 We do away with the usual square summability assumption on stepsizes (see Remark 1). Therefore, our tool is relevant for a broader family of stepsizes."}, {"heading": "2 Preliminaries", "text": "In the following we present the generic two-timescale SA algorithm, state our goal and list our assumptions.\nA generic two-timescale linear SA is\n\u03b8n+1 = \u03b8n + \u03b1n[h1(\u03b8n, wn) +M (1) n+1], (1)\nwn+1 = wn + \u03b2n[h2(\u03b8n, wn) +M (2) n+1], (2)\nwhere \u03b1n, \u03b2n \u2208 R are stepsizes, M (i)n \u2208 Rd denotes noise, and each function hi : Rd \u00d7 Rd \u2192 Rd is of the form hi(\u03b8, w) = vi \u2212 \u0393i\u03b8 \u2212Wiw (3) for a vector vi \u2208 Rd and matrices \u0393i,Wi \u2208 Rd\u00d7d. Our aim is to obtain concentration bounds for (1) and (2) under the following assumptions.\nA1 . W2 and X1 := \u03931 \u2212W1W\u221212 \u03932 are positive definite (not necessarily symmetric). A2 . {\u03b1n} and {\u03b2n} are positive real numbers satisfying\n\u221e\u2211 n=0 \u03b1n = \u221e\u2211 n=0 \u03b2n =\u221e, sup n {\u03b1n, \u03b2n, \u03b7n} \u2264 1, lim n\u2192\u221e \u03b1n = lim n\u2192\u221e \u03b2n = lim n\u2192\u221e \u03b7n = 0,\n(4) where \u03b7n := \u03b1n/\u03b2n.\nA3 . {M (1)n }, {M (2)n } are martingale difference sequences w.r.t. the increasing family of \u03c3\u2212fields {Fn}, where Fn = \u03c3(\u03b80, w0,M (1)1 ,M (2) 1 , . . . ,M (1) n ,M (2) n ). Also, there exist positive\nconstants m1 and m2 so that, for all n \u2265 0,\u2225\u2225\u2225M (1)n+1\u2225\u2225\u2225 \u2264 m1(1 + \u2016\u03b8n\u2016+ \u2016wn\u2016), and \u2225\u2225\u2225M (2)n+1\u2225\u2225\u2225 \u2264 m2(1 + \u2016\u03b8n\u2016+ \u2016wn\u2016). Remark 1. Unlike most works, \u2211 n\u22650 \u03b1 2 n or \u2211 n\u22650 \u03b2 2 n need not be finite. Thus our analysis is applicable for a wider class of stepsizes; e.g., 1/n\u03ba with \u03ba \u2208 (0, 1/2]. In (Borkar, 2008), on which much of the existing RL literature is based on, the square summability assumption is due to the Gronwall inequality. In contrast, in our work, we use the Variation of Parameters Formula (Lakshmikantham & Deo, 1998) for comparing the SA trajectory to appropriate trajectories of the limiting ODE; it is a stronger tool than Gronwall inequality.\nWe now briefly highlight relevant ideas used in (Borkar, 2008) to establish convergence for twotimescale SA in the context of (1) and (2), and describe how our approach builds upon it. Following terminology in pp. 64-65 (Borkar, 2008), since \u03b7n \u2192 0, {wn} is the fast transient and {\u03b8n} is the slow component. Hence, we consider (2) as a noisy discretization of the ODE\nw\u0307(t) = v2 \u2212 \u03932\u03b8 \u2212W2w(t) (5) for fixed \u03b8, and view (1) as an approximation of\n\u03b8\u0307(t) = h1(\u03b8(t), \u03bb(\u03b8(t))) = b1 \u2212X1\u03b8(t), (6) where b1 := v1 \u2212W1W\u221212 v2 and \u03bb(\u03b8) := W \u22121 2 [v2 \u2212 \u03932\u03b8]. Due toA1 , b1 and the function \u03bb(\u00b7) are well defined. Moreover, \u03bb(\u03b8) and \u03b8\u2217 := X\u221211 b1 are unique globally asymptotically stable equilibrium points of (5) and (6), respectively.\nLemma 1, p. 66, (Borkar, 2008) applied to (1) and (2) gives limn\u2192\u221e \u2016wn \u2212 \u03bb(\u03b8n)\u2016 = 0 under suitable assumptions. Building upon this, for analysis, we choose to deal with {zn} instead of {wn} directly, where zn := wn \u2212 \u03bb(\u03b8n). Using (2), {zn} satisfies the iterative rule\nzn+1 = zn \u2212 \u03b2nW2zn + \u03b2nM (2)n+1 + \u03bb(\u03b8n)\u2212 \u03bb(\u03b8n+1). (7) As {\u03b8n} is the slow component, we shall consider (7) as a noisy discretization of the ODE\nz\u0307(s) = \u2212W2z(s). (8) Since W2 is positive definite fromA1 , z\u2217 := 0 \u2208 Rd is the unique globally asymptotically stable equilibrium of (8). Remark 2. We emphasize that working with {zn} instead of {wn} is the vital reason why our approach works. As \u03b8n evolves, the limiting ODE in (5) changes with it; on the other hand, (8) remains unchanged. This makes comparing (7) with (8) easier than comparing (2) with (5)."}, {"heading": "3 Main Result", "text": "Let q1, q2 > 0 be lower bounds on the real part of the eigenvalues of matricesX1 andW2, respectively. For n \u2265 0, let an := \u2211n\u22121 k=0 \u03b1 2 ke \u22122q1 \u2211n\u22121 i=k+1 \u03b1i and bn := \u2211n\u22121 k=0 \u03b2 2 ke \u22122q2 \u2211n\u22121 i=k+1 \u03b2i . These sums are obtained from the Azuma-Hoeffding concentration bound. Their behavior is dependent on stepsize choice; this is elaborated for a common stepsize family in Theorem 3.2. Theorem 3.1 (Main Result). Consider a linear two-timescale SA algorithm and stepsize sequences {\u03b1\u0302k}, {\u03b2\u0302k}. Let 1, 2 > 0 and let n0 be s.t.\nmax { sup n\u2265n0 \u03b2\u0302n, sup n\u2265n0 \u03b1\u0302n \u03b2\u0302n } \u2264 g1 min{ 1, 2} and sup n\u2265n0 \u03b2\u0302n \u2264 g2 1,\nfor constants g1 and g2, in accordance with A4 and A5 . Set {\u03b1n = \u03b1\u0302n+n0}, {\u03b2n = \u03b2\u0302n+n0} and pick n1 s.t. max{ \u2211n1 k=0 \u03b1k, \u2211n1 k=0 \u03b2k} \u2265 T1( 1, 2), where T1 = O(max{ln( 1 1\n), ln( 1 2 )}) in accordance with (22). Then\nPr{\u2016\u03b8n \u2212 \u03b8\u2217\u2016 \u2264 1, \u2016zn\u2016 \u2264 2,\u2200n \u2265 n1} \u2265 1\u2212 2d2 \u2211 n\u22650 [ exp [ \u2212c1 21 an ] + exp [ \u2212c2 21 bn ] + exp [ \u2212c3 22 bn ]] ,\nwhere c1, c2, c3 > 0 are suitable constants as defined in Theorem 4.2.\nRemark 3. The result introduces two key notions: n0 and n1.\n1. A large n0 ensures the stepsizes are small enough to mitigate the martingale noise of the SA trajectories; i.e., the additive SA noise {\u03b1nM (1)n+1} and {\u03b2nM (2) n+1} \u2200n \u2265 n0 is small w.h.p.\nThis concept is of the nature of previous single timescale finite sample analyses: Corollary 14, Chapter 4, (Borkar, 2008), and the more recent (Korda & Prashanth, 2015), Theorem 1.\n2. The quantity n1 is an intrinsic property of the limiting ODEs.\n(a) After n1 iterations, the two ODE trajectories \u03b8(tn) and z(sn) hit the -neighborhoods of their respective solutions when started in balls of radius Rin1 and R in 2 around them.\nAs shown explicitly in Theorem 3.2, n1 also depends on n0, since larger n0 implies smaller stepsizes and hence more iterations.\n(b) After n1 iterations, as they closely follow their ODE trajectories due to the n0 shift, \u03b8n and zn will also respectively reach the above -neighborhoods and remain in it thereafter.\nThe fact that n1 ensures a deterministic event hints at why the high probability bound in Theorem 3.1 does not depend on n1, but rather solely on 1, 2, and n0.\nRemark 4. As can be seen by inspecting the constants in the theorem\u2019s statement, they exhibit a relatively complex dependence on the problem-dependent parameters q1 and q2. A significant appearance of q1 and q2, however, is in the form of 1qi respectively multiplying the ln( 1 i ) terms in T1.\nTheorem 3.1 applies for a general setting, where the stepsizes are not of a specific family, and may not even be monotone. For a more explicit result, we obtain a closed-form expression of above bound for commonly used stepsizes. We demonstrate the generality of our result by choosing stepsizes that may not be square-summable, as usually was required by previous works.\nTheorem 3.2. Fix 1, 2 > 0, \u03b4 \u2208 (0, 1). Let \u03b1n = (n+n0)\u2212\u03b1 and \u03b2n = (n+n0)\u2212\u03b2 \u2200n \u2265 0, with 1 > \u03b1 > \u03b2. Then for some\nn0 = O\u0303 ( [1/min{ 1, 2}]2/min{\u03b2,2(\u03b1\u2212\u03b2)} ln(1/\u03b4) )\nand some n1 = O\u0303 ( max { n0, (ln(1/ 1)) 1/(1\u2212\u03b1) , (ln(1/ 2)) 1/(1\u2212\u03b2) }) ,\nwe have Pr{\u2016\u03b8n \u2212 \u03b8\u2217\u2016 \u2264 1, \u2016zn\u2016 \u2264 2,\u2200n \u2265 n1} \u2265 1\u2212 \u03b4.\nRemark 5. As explained in Remark 3, the role of n0 is to ensure small enough stepsizes, while the role of n1 is to ensure sufficient proximity of the SA and ODE trajectories toward convergence. Given this, Theorem 3.2 exhibits several valuable tradeoffs in the choice of \u03b1 and \u03b2:\n1. As \u03b1 or \u03b2 approach 0 (stepsizes approach constants), n0 blows up. This is since the stepsizes\u2019 slow decay rate impairs i) their ability to mitigate the martingale noise; and hence ii) the ability of the SA trajectories to follow the ODE trajectories.\n2. As \u03b1 and \u03b2 get close to each other, n0 blows up since the two-timescale nature is nullified. In a two-timescale algorithm, convergence of zn to z\u2217 should be faster relative to that of \u03b8n to \u03b8\u2217; this is ensured by the decay rate of the stepsize ratio \u03b7n.\n3. As \u03b1 or \u03b2 approach 1, which is the largest value for which (4) is still satisfied, n1 blows up. This is since the stepsizes then decay too fast, impairing the speed of the ODE convergence; more accurately, (22) moves away from exponential nature to inverse polynomial.\nIt is also worth mentioning that again, as in Remark 4, the problem-dependent parameters q1 and q2 appear in the form of 1qi , respectively multiplying the ln( 1 i ) terms in n0 and n1.\nRemark 6. One can see that for \u03b1k = (k + n0)\u2212\u03b1, we have an = O(n\u2212\u03b1). A similar behavior can be shown for bn as well."}, {"heading": "4 Linear Two Timescale SA Analysis", "text": "As a first step, we define the linearly interpolated trajectories of the iterates in each timescale. Having a continuous version of the discrete SA algorithm enables our analysis.\nAll proofs for the results in this section are given in Appendix A."}, {"heading": "4.1 Analysis Preliminaries", "text": "Let t0 = s0 = 0 and for all n \u2265 0, tn+1 = tn + \u03b1n and sn+1 = sn + \u03b2n. (9)\nLet \u03b8\u0304(\u00b7) be the linear interpolation of {\u03b8n} on {tn}; i.e., let \u03b8\u0304(tn) = \u03b8n and, for \u03c4 \u2208 (tn, tn+1), let\n\u03b8\u0304(\u03c4) = \u03b8\u0304(tn) + (\u03c4 \u2212 tn) \u03b1n [\u03b8\u0304(tn+1)\u2212 \u03b8\u0304(tn)]. (10)\nAlso, let z\u0304(\u00b7) be the linear interpolation of {zn}, but on the time steps {sn}. For \u03c4 \u2208 [tn, tn+1), let\n\u03be(\u03c4) := sn + \u03b2n \u03b1n (\u03c4 \u2212 tn). (11)\nThe mapping \u03be(\u00b7) linearly interpolates {sn} on {tn}. Let 1, 2, Rin1 , Rin2 > 0 be such that 1 < R in 1 , 2 < R in 2 , \u2016\u03b80 \u2212 \u03b8\u2217\u2016 \u2264 Rin1 , \u2016z0\u2016 \u2264 Rin2 . (12)\nFor T > 0, define the event E(T ) := { \u2225\u2225\u03b8\u0304(t)\u2212 \u03b8\u2217\u2225\u2225 \u2264 1 \u2200t \u2265 T + 1} \u2229 {\u2016z\u0304(s)\u2016 \u2264 2 \u2200s \u2265 \u03be(T ) + 1}, (13) Let \u03b8(t), t \u2265 0, be the solution to (6) satisfying \u03b8(0) = \u03b80. From (6) and standard ODE results, \u03b8(t) = \u03b8\u2217 + e\u2212X1t(\u03b80 \u2212 \u03b8\u2217), t \u2265 0. (14) Similarly define z(s). From (8), z(s) = e\u2212W2sz0, s \u2265 0. (15) We begin with an outline of the proof of Theorem (3.1)."}, {"heading": "Overall Analysis Outline", "text": "The key idea in our analysis is that we compare the SA trajectories \u03b8\u0304(t) and z\u0304(s) to their respective limiting ODE trajectories \u03b8(t), z(s). If the stepsizes are small enough, it is harder for the noise to perturb the SA trajectory \u03b8\u0304(t) away from the limiting ODE behavior. A similar relation holds between z\u0304(s) and z(s).\nProving Theorem 3.1 is done in two steps. First, we use the Variation of Constants (VoC) formula (Lakshmikantham & Deo, 1998) to quantify the distance of the perturbed trajectories \u03b8\u0304(t) and z\u0304(s) from their unperturbed trajectories \u03b8(t) and z(s); this is done by splitting the perturbations into three parts per each trajectory, as described in Section 4.3. Thus, we obtain upper bounds on\n\u2225\u2225\u03b8\u0304(t)\u2212 \u03b8(t)\u2225\u2225 and \u2016z\u0304(s)\u2212 z(s)\u2016 . Second, exploiting the fact that the stepsizes are small enough and using AzumaHoeffding Martingale concentration inequality, we show that these upper bounds are small with very high probability for all t, s \u2265 0. More explicitly, when \u03b8(t) and z(s) are sufficiently close to \u03b8\u2217 and z\u2217 respectively, the same is also true for \u03b8\u0304(t) and z\u0304(s) with high probability. A visualization of the process is given in Fig. 1."}, {"heading": "4.2 A Smart Decomposition of The Event of Interest", "text": "For an event E , let Ec be its complement. Fix sufficiently large T > 0. We will say later on how large it ought to be. Pick n1 > 0 such that\nT \u2264 tn1+1 = n1\u2211 k=0 \u03b1k \u2264 T + 1. (16)\nThis is possible as {\u03b1n} satisfies (4). Our aim here is to construct a superset for the event Ec(T ), defined in (13), which is easier for analysis. The superset additionally contains the information of what happens until time tn1 .\nRemark 7. By standard ODE literature, limt\u2192\u221e \u03b8(t) = \u03b8\u2217. As X1 is positive definite by A1 , d dt\u2016\u03b8(t) \u2212 \u03b8\n\u2217\u20162 = \u22122(\u03b8(t) \u2212 \u03b8\u2217)>X1(\u03b8(t) \u2212 \u03b8\u2217) < 0; hence, \u2016\u03b8(t)\u2212 \u03b8\u2217\u2016 \u2264 Rin1 for all t \u2265 0. Likewise, lims\u2192\u221e z(s) = z\u2217 and \u2016z(s)\u2016 \u2264 Rin2 for all s \u2265 0.\nBy Remark 7, \u03b8(t) stays in the Rin1\u2212radius ball around \u03b8\u2217 for all t \u2265 0, and z(s) stays in the Rin2\u2212radius ball around z\u2217 for all s \u2265 0. However, the same cannot be said for \u03b8\u0304(t) and z\u0304(s) due to the presence of noise. We will show instead that these lie with high probability in bigger but fixed radii balls Rout1 and R out 2 which we define below.\nFix Rout1 , R out 2 > 0 such that R out 1 > R in 1 , R out 2 > R in 2 ,\nRgap1 := R out 1 \u2212Rin1 \u2265 1, and R gap 2 := R out 2 \u2212Rin2 \u2265 2. (17)\nFor n \u2265 0, let\n\u03c1n+1 := sup \u03c4\u2208[tn,tn+1] \u2225\u2225\u03b8\u0304(\u03c4)\u2212 \u03b8(\u03c4)\u2225\u2225 , \u03c1\u2217n+1 := sup \u03c4\u2208[tn,tn+1] \u2225\u2225\u03b8\u0304(\u03c4)\u2212 \u03b8\u2217\u2225\u2225 , (18) \u03bdn+1 := sup\n\u00b5\u2208[sn,sn+1] \u2016z\u0304(\u00b5)\u2212 z(\u00b5)\u2016 , \u03bd\u2217n+1 := sup \u00b5\u2208[sn,sn+1] \u2016z\u0304(\u00b5)\u2016 , (19)\nand define the (\u201cgood\u201d) event Gn := { \u2225\u2225\u03b8\u0304(\u03c4)\u2212 \u03b8\u2217\u2225\u2225 \u2264 Rout1 \u2200\u03c4 \u2208 [0, tn]} \u2229 {\u2016z\u0304(\u00b5)\u2016 \u2264 Rout2 \u2200\u00b5 \u2208 [0, sn]}. (20)\nAdditionally, define the (\u201cbad\u201d) events Emid := {[\nsup 0\u2264n\u2264n1 \u03c1n+1\n] \u2265 Rgap1 } \u222a {[\nsup 0\u2264n\u2264n1 \u03bdn+1\n] \u2265 Rgap2 } ,\nEafter := \u22c3 n>n1 [ {\u03c1\u2217n+1 > 1} \u222a {\u03bd\u2217n+1 > 2} ] .\nThe desired superset is now ready to be given below.\nLemma 4.1 (Decomposition of Event of Interest). The following relation holds for the event of interest, defined in (13). Ec(T ) \u2286 \u22c3n1 n=0{Gn \u2229 [{\u03c1n+1 \u2265 R gap 1 } \u222a {\u03bdn+1 \u2265 R gap 2 }]} \u222a\u22c3\nn>n1\n{ Gn \u2229 [ {\u03c1\u2217n+1 \u2265 1} \u222a {\u03bd\u2217n+1 \u2265 2} ]} .\nThe key advantage of this result is that the analysis can now be broken down into an incremental union of events, enabling easier analysis. Each event has an inductive structure: good up to time n (ensured by conditioning on Gn, where the iterates remain in bounded regions) and bad in the subsequent interval (\u03b8\u0304(tn+1) and z\u0304(sn+1) leave the bounded regions). Moreover, using Lemma 4.1 to bound Pr{Ec(T )} results in a tight bound since the relation holds whenever the two-timescale iterates stay in the Rin balls. This happens w.h.p. due toA4 andA5 .\n4.3 Perturbation Bounds for {zn} and {\u03b8n} Our aim here is to use the VoC formula to bound \u2016z\u0304(s)\u2212 z(s)\u2016 and \u2225\u2225\u03b8\u0304(t)\u2212 \u03b8(t)\u2225\u2225. Then, use this to obtain bounds on \u03bdn+1 and \u03bd\u2217n+1, and \u03c1n+1 and \u03c1 \u2217 n+1 on the event Gn. This is a preparation step for applying Lemma 4.1. Through the rest of this section, the steps are described solely for the {zn} iterates, and are similarly applied for {\u03b8n}, as given in detail in Appendix A.2. When applying the VoC formula, we compare the perturbed trajectory z\u0304(s) to its unperturbed counterpart z(s). As we show below, the difference between them is due to three components, namely discretization error, martingale difference noise, and slow drift in the equilibrium of (5):\n\u03c7de(\u00b5) := W2[z\u0304(\u00b5)\u2212 zk], \u03c7md(\u00b5) := M\n(2) k+1,\n\u03c7sd(\u00b5) := [\u03bb(\u03b8k)\u2212 \u03bb(\u03b8k+1)]/\u03b2k = W\u221212 \u03932[\u03b8k+1 \u2212 \u03b8k]/\u03b2k,\nfor k \u2265 0 and \u00b5 \u2208 [sk, sk+1). To clarify on the role of \u03c7sd(\u00b5), the drift is due to the fact that \u03b8n evolves and it is slow because {\u03b8n} is updated on the slow time scale {tn} (recall \u03b7n \u2192 0). In the case of {\u03b8n} iterates, the first two terms are similar, while the third is the error in tracking the equilibrium of (5). Recall that as \u03b8n evolves, the equilibrium of (5) changes. The tracking error is a function of zn which, from (7), is the difference between wn and \u03bb(\u03b8n). Simple manipulations on (7) show that for all s \u2265 0, z\u0304(s) = z0 + \u222b s 0\n[\u2212W2z\u0304(\u00b5) + \u03c7(\u00b5)]d\u00b5, where \u03c7(\u00b5) = \u03c7de(\u00b5) + \u03c7md(\u00b5) + \u03c7sd(\u00b5). Using the VoC formula, we have\nz\u0304(s) = z(s) + E2(s), (21)\nwhere E2(s) = Ede2 (s) +E md 2 (s) +E sd 2 (s) with E de 2 (s) =\n\u222b s e\u2212W2[s\u2212\u00b5]\u03c7de(\u00b5)d\u00b5, and similarly for\nEmd2 (s) and E sd 2 (s). The exponential term e \u2212W2(s\u2212\u00b5) multiplying each perturbation is a consequence of using the VoC formula. We exploit this fact extensively in all our proofs. On Gn, to obtain bounds on \u03bdn+1 and \u03bd\u2217n+1, it thus suffices to get bounds on the perturbation errors \u2225\u2225Ede2 (\u00b7)\u2225\u2225 , \u2225\u2225Emd2 (\u00b7)\u2225\u2225 , and\n\u2225\u2225Esd2 (\u00b7)\u2225\u2225 on the interval [sn, sn+1]. Those, and consequently the bounds on \u03bdn+1 and \u03bd\u2217n+1 are obtained in Appendix A.2."}, {"heading": "4.4 Concentration Bounds for Two Time Scale SA", "text": "Summarizing Subsection 4.3, on the good event Gn, each of \u03bdn+1, \u03c1n+1, \u03bd\u2217n+1 and \u03c1 \u2217 n+1 is bounded from above by three kinds of terms: i) sum of Martingale differences, ii) exponentially decaying term and iii) stepsize based term. For large enough n, type i) terms are small with high probability due toA3 and the Azuma-Hoeffding martingale concentration inequality; type ii) terms are small for sufficiently large n; type iii) terms are small for small enough stepsizes. Based on this, we bring our main technical result in Theorem 4.2. Theorem 3.1 then follows trivially.\nSuppose the following additional assumptions on the stepsizes hold, with the constants taken from Appendix A.3.\nA4 . max { supk\u22650 \u03b2k, supk\u22650 \u03b7k } \u2264 min { 1 8 , 2 3 } /max{LzL\u03b8c , Lz}.\nA5 . supk\u22650 \u03b2k \u2264 1/(4L\u03b8b).\nThese assumptions may not hold for a general stepsize sequence. However, due to (4), they will be ensured by properly shifting the stepsize sequence as in Theorem 3.1.\nAssume that 1 \u2264 4L\u03b8a and let R gap 1 = 4L \u03b8 a. Let N1 be large enough to satisfy\nK2R in 2 e \u2212q2sN1 \u2264 2 3 and [K1Rin1 + L \u03b8 a]e \u2212qtN1 \u2264 1 4 . (22)\nTheorem 4.2 (Main Technical Result). Fix n1 \u2265 N1. Then\nPr{\u2016\u03b8n \u2212 \u03b8\u2217\u2016 \u2264 1, \u2016zn\u2016 \u2264 2,\u2200n \u2265 n1} \u2265 Pr{E(T )} \u2265 1\u2212 2d2 \u2211 n\u22650 [ exp [ \u2212c1 21 an ] + exp [ \u2212c2 21 bn ] + exp [ \u2212c3 22 bn ]] ,\nwhere c1 = (16K21d 3[Lmd1 ] 2)\u22121, c2 = (9K 2 2d 3[Lmd2 ] 2)\u22121, c3 = (64K 2 2 [L \u03b8 c ] 2d3[Lmd2 ] 2)\u22121."}, {"heading": "5 Applications to Two-timescale RL", "text": "In this section we show how our novel machinery implies concentration bounds on the standard two-timescale RL methods with linear function approximation, in a plug-and-play fashion. We consider the problem of policy evaluation and use the standard RL framework and notations, given in detail in Appendix A.4. We assume linear function approximation, i.e., V \u03c0(s) \u2248 \u03b8>\u03c6(s), where \u03c6(s) \u2208 Rd is a feature vector at state s, and \u03b8 \u2208 Rd is a parameter vector. For brevity, we denote \u03c6(sn), \u03c6(s \u2032 n) by \u03c6n, \u03c6 \u2032 n. We also relate to the matrices A = E[\u03c6(\u03c6\u2212 \u03b3\u03c6\u2032)>] and C = E[\u03c6\u03c6>], and the vector b = E[r\u03c6], where the expectations are w.r.t. the stationary distribution of the induced chain. We assume all rewards r(s) and feature vectors \u03c6(s) are bounded: |r(s)| \u2264 1, \u2016\u03c6(s)\u2016 \u2264 1 \u2200s \u2208 S. Also, it is assumed that A and C are of full rank. These assumption are standard (see (Sutton et al., 2009a,b)). It is known that A is positive definite (Bertsekas, 2012). Also, by construction, C is positive semidefinite; thus, by the full-rank assumption it is actually positive definite.\nWe now present the GTD(0) algorithm, verify its related required assumptions, and obtain the necessary relevant constants to apply Theorem 3.2 for it. The GTD(0) algorithm (Sutton et al., 2009a) is designed to minimize the objective function JNEU(\u03b8) = 12 (b\u2212A\u03b8)\n>(b\u2212A\u03b8) . The update rule of the algorithm takes the form of Equations (1) and (2) with h1(\u03b8, w) = A>w , h2(\u03b8, w) = b\u2212A\u03b8 \u2212 w , andM (1)n+1 = (\u03c6n \u2212 \u03b3\u03c6\u2032n)\u03c6>nwn\u2212A>wn ,M (2) n+1 = rn\u03c6n+\u03c6n[\u03b3\u03c6 \u2032 n\u2212\u03c6n]>\u03b8n\u2212(b\u2212A\u03b8n) . That is, in case of GTD(0), the relevant matrices in the update rules take the form \u03931 = 0,W1 = \u2212A>, v1 = 0, and \u03932 = A, W2 = I, v2 = b. Additionally, X1 = \u03931 \u2212W1W\u221212 \u03932 = A>A. By our assumption above, both W2 and X1 are symmetric positive definite matrices, and thus the real parts of their eigenvalues are also positive. It is also clear that\n\u2016M (1)n+1\u2016 \u2264(1 + \u03b3 + \u2016A\u2016)\u2016wn\u2016,\n\u2016M (2)n+1\u2016 =\u2016rn\u03c6n + \u03c6n[\u03b3\u03c6\u2032n \u2212 \u03c6n]>\u03b8n \u2212 (b\u2212A\u03b8n) \u2016 \u2264 1 + \u2016b\u2016+ (1 + \u03b3 + \u2016A\u2016)\u2016\u03b8n\u2016 .\nConsequently, Assumption A3 is satisfied with constants m1 = (1 + \u03b3 + \u2016A\u2016) and m2 = 1 + max(\u2016b\u2016, \u03b3 + \u2016A\u2016). In a similar fashion, we perform the same steps for GTD2 and TDC (Sutton et al., 2009b) and summarize the results in Table 5. The detailed derivation is provided in Appendix A.4.\nWe now apply Theorem 3.2 for a specific choice of stepsizes and give it in a diminishing bound form, in similar spirit to Theorem 1 in (Korda & Prashanth, 2015). Let 1 = 2 = > 0 and \u03b4 \u2208 (0, 1/3). Let \u03b1 = 3/4, \u03b2 = 1/2. Set n0 = C1 \u22121/4 ln 1\u03b4 , so that it satisfies the appropriate condition in Theorem 3.2. The resulting stepsizes are \u03b1n = (n+ n0)\u22123/4, \u03b2n = (n+ n0)\u22121/2. Let n1 = C2n0, satisfying the appropriate condition on n1 in Theorem 3.2. This choice is valid since( 1\n)4 ln 1\u03b4 \u2265 ( ln 1 )4 . Applying Theorem 3.2 gives\nPr { max{\u2016\u03b8n \u2212 \u03b8\u2217\u2016 , \u2016zn\u2016} \u2264 C1C2 [ ln(1/\u03b4)\nn1\n]1/4 , \u2200n \u2265 n1 } \u2265 1\u2212 \u03b4,\nwhere C1, C2 are problem-dependent constants that can be extracted using Table 5."}, {"heading": "6 Discussion", "text": "In this work we obtained the first concentration bound for two-timescale SA algorithms. We provide it as a general methodology that applies to all linear two-timescale SA algorithms. A natural extension\nto our methodology is considering the non-linear function-approximation case, in a similar fashion to (Thoppe & Borkar, 2015). Such a result can be of high interest due to the recently growing attractiveness of neural networks in the RL community. An additional direction for future research is extending to actor-critic RL algorithms, in addition to the gradient TD methods explored here."}, {"heading": "A Supplementary Material", "text": "This section contains all proofs of the lemmas and theorems presented in the paper, and provides additional technical results to support several of these proofs."}, {"heading": "A.1 Proofs from Subsection 4.2", "text": ""}, {"heading": "Let R\u2217 :=", "text": "\u2225\u2225X\u221211 \u2225\u2225 \u2016b1\u2016 so that \u2016\u03b8\u2217\u2016 \u2264 R\u2217. (23) On Gn, for k \u2208 {0, . . . , n}, \u2016wk\u2016 \u2264 \u2016zk\u2016+ \u2016\u03bb(\u03b8\u2217)\u2016+ \u2016\u03bb(\u03b8k)\u2212 \u03bb(\u03b8\u2217)\u2016 \u2264 Rout2 +\n\u2225\u2225W\u221212 \u2225\u2225 [ \u2016v2\u2016+ \u2016\u03932\u2016 [R\u2217+Rout1 ]] =: Rw2 . Proof of Lemma 4.1. By (16), as tn1+1 \u2264 T + 1, Ec(T ) \u2286 Eafter. For any two events E1 and E2, as\nE1 = [E2 \u2229 E1] \u222a [Ec2 \u2229 E1] \u2286 E2 \u222a [Ec2 \u2229 E1], we have Eafter \u2286 Emid \u222a [Ecmid \u2229 Eafter]. Using Remark 7,{[\nsup 0\u2264k<n \u03c1k+1\n] \u2264 Rgap1 } \u2229 {[\nsup 0\u2264k<n \u03bdk+1\n] \u2264 Rgap2 } \u2286 Gn.\nfor all n \u2265 0. Hence by simple manipulations, we have\nEmid \u2286 n1\u22c3 n=0 { Gn \u2229 [ {\u03c1n+1 \u2265 Rgap1 } \u222a {\u03bdn+1 \u2265 R gap 2 } ] }.\nArguing similarly, one can see that\nEcmid \u2229 Eafter \u2286 Gn1+1 \u2229 Eafter \u2286\n\u22c3 n>n1 [ Gn \u2229 [ {\u03c1\u2217n+1 \u2265 1} \u222a {\u03bd\u2217n+1 \u2265 2} ]] ,\nwhere the last inequality follows as 1 \u2264 Rout1 and 2 \u2264 Rout2 . The desired result is now easy to see."}, {"heading": "A.2 Proofs from Subsection 4.3", "text": "For obtaining the bounds in this subsection, we first show worst-case bounds on the increments. For k \u2265 0, let I\u03b8(k) := \u2016\u03b8k+1 \u2212 \u03b8k\u2016 /\u03b1k (24) and Iz(k) := \u2016zk+1 \u2212 zk\u2016 /\u03b2k. (25) Lemma A.1 (Bounded Differences). Fix n \u2265 0. Then on Gn,\nsup 0\u2264k\u2264n I\u03b8(k) \u2264 J\u03b8, sup 0\u2264k\u2264n Iz(k) \u2264 Jz. (26)\nwhere J\u03b8 = \u2016v1\u2016+ \u2016\u03931\u2016 [R\u2217 +Rout1 ] + \u2016W1\u2016Rw2 +m1[1 +R\u2217 +Rout1 +Rw2 ] (27) and Jz := \u2016W2\u2016Rout2 +\n\u2225\u2225W\u221212 \u2225\u2225 \u2016\u03932\u2016 J\u03b8 +m2(1 +R\u2217 +Rout1 +Rw2 ). (28) Proof of Lemma A.1. Fix k \u2208 {0, . . . , n}. On Gn, using (1),A3 , (23), (20), and (24), in that order,\nI\u03b8(k) \u2264 \u2016v1 \u2212 \u03931\u03b8k \u2212W1wk\u2016+ \u2225\u2225\u2225M (1)k+1\u2225\u2225\u2225\n\u2264 \u2016v1\u2016+ \u2016\u03931\u2016 (\u2016\u03b8\u2217\u2016+ \u2016\u03b8k \u2212 \u03b8\u2217\u2016) + \u2016W1\u2016 \u2016wk\u2016 +m1[1 + \u2016\u03b8\u2217\u2016+ \u2016\u03b8k \u2212 \u03b8\u2217\u2016+ \u2016wk\u2016]\n\u2264 J\u03b8. (29)\nSimilarly, on Gn, using (7),A3 , (20), (4) fromA2 , (29), (23), and (24), in that order,\nIz(k) \u2264 \u2016W2\u2016 \u2016zk\u2016+ \u2016\u03bb(\u03b8k)\u2212 \u03bb(\u03b8k+1)\u2016 /\u03b2k + \u2225\u2225\u2225M (2)k+1\u2225\u2225\u2225\n\u2264 \u2016W2\u2016 \u2016zk\u2016+ \u2225\u2225[W2]\u22121\u2225\u2225 \u2016\u03932\u2016 \u03b7kI\u03b8(k)\n+m2(1 + \u2016\u03b8\u2217\u2016+ \u2016\u03b8k \u2212 \u03b8\u2217\u2016+ \u2016wk\u2016) \u2264 Jz.\nSince k was arbitrary the result follows.\nLet q(1)(W2), . . . , q(d)(W2) be the eigenvalues of W2. Fix q2 \u2208 (0, q\u20322), where q\u20322 := mini{real(q(i)(W2))}. Then from Corollary 3.6 (Teschl, 2004), there exists K2 \u2265 1 so that\u2225\u2225\u2225e\u2212W2(s\u2212\u00b5)\u2225\u2225\u2225 \u2264 K2e\u2212q2(s\u2212\u00b5), \u2200s \u2265 \u00b5. (30) For the rest of the results in this subsection we consider intermediate intervals [sn, sn+1]. The next lemma gives bounds on the three error terms of the interpolated trajectory z\u0304(s) at the extremes {sn, sn+1}.\nLemma A.2 (Perturbation Error Bounds for zn). Fix n \u2265 0. Then on Gn,\nsup `\u2208{n,n+1} \u2225\u2225Ede2 (s`)\u2225\u2225 \u2264 Lde2 [sup k\u22650 \u03b2k ] ,\nsup `\u2208{n,n+1} \u2225\u2225Esd2 (s`)\u2225\u2225 \u2264 Lsd2 [sup k\u22650 \u03b7k ] ,\u2225\u2225Emd2 (sn+1)\u2225\u2225 \u2264 K2 \u2225\u2225Emd2 (sn)\u2225\u2225+ Lmd2 \u03b2n.\nwhere Lde2 := K2J z\u2016W2\u2016 q2 , Lsd2 := K2\u2016W\u221212 \u2016\u2016\u03932\u2016J\u03b8 q2 , Lmd2 := K2m2[1 +R \u2217 +Rout1 +R w 2 ].\nThe previous lemma shows that for \u03c4 \u2208 [sn, sn+1], z\u0304(\u03c4) cannot deviate much from the ODE trajectory z(\u03c4) if the stepsizes are small enough. In particular, it bounds the distance with decaying terms using Lemma A.2.\nLemma A.3 (ODE-SA Distance Bound for zn). Fix n \u2265 0. Then on Gn,\n\u03bdn+1 \u2264K2 \u2225\u2225Emd2 (sn)\u2225\u2225+ Lz max{sup\nk\u22650 \u03b2k, sup k\u22650 \u03b7k\n} ,\n\u03bd\u2217n+1 \u2264K2 \u2225\u2225Emd2 (sn)\u2225\u2225+K2Rin2 e\u2212q2(sn\u2212) + Lz max{sup\nk\u22650 \u03b2k, sup k\u22650 \u03b7k\n} ,\nwhere Lz = Lde2 + L md 2 + \u2016W2\u2016Rin2 + Lsd2 .\nNext, we provide a technical lemma for later usage.\nLemma A.4. Let 0 < r0 < r1 < \u00b7 \u00b7 \u00b7 < r`, let \u03b3i = ri+1 \u2212 ri for i = 0, . . . , ` \u2212 1, let U be some d\u00d7 d matrix, and let \u03c1 : R\u2192 R be some mapping. Assume that for some constant J it holds that \u2016\u03c1(\u03c3)\u2016 \u2264 \u03b3iJ for any \u03c3 \u2208 [ri, ri+1] and i = 0, . . . , ` \u2212 1. Assume, furthermore that for some constants K > 0 and q0 > 0 it holds that \u2016e\u2212U(r\u2212r0)\u2016 \u2264 Ke\u2212q0(r\u2212r0) for any r > r0. Then\u2225\u2225\u2225\u2225\u222b r`\nr0\ne\u2212U(r`\u2212\u03c3)\u03c1(\u03c3)d\u03c3 \u2225\u2225\u2225\u2225 \u2264 KJq0 [ sup i=0,...,`\u22121 \u03b3i ] .\nProof. The claim of the lemma follows easily as, due to the assumptions,\u2225\u2225\u2225\u2225\u222b r` r0 e\u2212U(r`\u2212\u03c3)\u03c1(\u03c3)d\u03c3 \u2225\u2225\u2225\u2225 \u2264\n`\u22121\u2211 i=0 \u222b ri+1 ri \u2225\u2225\u2225e\u2212U(r`\u2212\u03c3)\u2225\u2225\u2225 \u2016\u03c1(\u03c3)\u2016 d\u03c3 \u2264 KJ\n`\u22121\u2211 i=0 \u03b3j \u222b ri+1 ri e\u2212q0(r`\u2212\u03c3)d\u03c3\n\u2264 KJ [ sup\ni=0,...,`\u22121 \u03b3i ]\u222b r`\u2212r0 0 e\u2212q0\u03c3d\u03c3\n\u2264 KJ q0\n[ sup\ni=0,...,`\u22121 \u03b3i\n] .\nProof of Lemma A.2. Fix ` \u2208 {n, n+ 1}. For the first claim of the lemma note that, by Lemma A.1, on Gn,\u2225\u2225\u03c7de(\u00b5)\u2225\u2225 \u2264 \u2016W2\u2016 (\u00b5\u2212 sk)Iz(k) \u2264 \u2016W2\u2016\u03b2kJz for \u00b5 \u2208 [sk, sk+1), where Iz(k) is as in (25). The claim then follows easily by recalling (30), and applying Lemma (A.4) with ri = si, \u03b3i = \u03b2i, U = W2, \u03c1 = \u03c7de, K = K2, q0 = \u2212q2 and J = \u2016W2\u2016 Jz .\nFor the second claim, let k \u2208 {0, . . . , `\u2212 1} and \u00b5 \u2208 [sk, sk+1). With I\u03b8(k) as in (24),\u2225\u2225\u03c7sd(\u00b5)\u2225\u2225 \u2264 \u03b7k \u2225\u2225W\u221212 \u2225\u2225 \u2016\u03932\u2016 I\u03b8(k). Hence by Lemma A.1, on Gn, \u2225\u2225\u03c7sd(\u00b5)\u2225\u2225 \u2264 \u03b7k \u2225\u2225W\u221212 \u2225\u2225 \u2016\u03932\u2016 J\u03b8. The claim then follows again by (30) and Lemma (A.4).\nFor the third claim, by its definition and the triangle inequality,\u2225\u2225Emd2 (sn+1)\u2225\u2225 \u2264\n\u2225\u2225\u2225\u2225\u222b sn+1 e\u2212W2(sn+1\u2212\u00b5)\u03c7md(\u00b5)d\u00b5\u2225\u2225\u2225\u2225 \u2264\n\u2225\u2225\u2225\u2225e\u2212W2\u03b2n \u222b sn e\u2212W2(sn\u2212\u00b5)\u03c7md(\u00b5)d\u00b5\u2225\u2225\u2225\u2225 +\n\u2225\u2225\u2225\u2225\u222b sn+1 sn e\u2212W2(sn+1\u2212\u00b5)\u03c7md(\u00b5)d\u00b5 \u2225\u2225\u2225\u2225 . Applying (30) on both terms, we get that\u2225\u2225Emd2 (sn+1)\u2225\u2225 \u2264 K2 \u2225\u2225Emd2 (sn)\u2225\u2225+K2\u03b2n \u2225\u2225\u2225M (2)n+1\u2225\u2225\u2225 . OnGn, usingA3 with (20), (23), and (24), we haveK2\n\u2225\u2225\u2225M (2)n+1\u2225\u2225\u2225 \u2264 Lmd2 . The third claim follows. Proof of Lemma A.3. Let \u00b5 \u2208 [sn, sn+1]. Then there exists \u03ba \u2208 [0, 1] so that\nz\u0304(\u00b5) = (1\u2212 \u03ba)z\u0304(sn) + \u03baz\u0304(sn+1). Hence\n\u2016z\u0304(\u00b5)\u2212 z(\u00b5)\u2016 \u2264 (1\u2212 \u03ba) \u2016z\u0304(sn)\u2212 z(\u00b5)\u2016\n+\u03ba \u2016z\u0304(sn+1)\u2212 z(\u00b5)\u2016 .\nUsing (8),\nz(\u00b5) = z(sn) + \u222b \u00b5 sn [\u2212W2 z(\u00b51)]d\u00b51,\nand\nz(sn+1) = z(\u00b5) + \u222b sn+1 \u00b5 [\u2212W2 z(\u00b51)]d\u00b51.\nCombining the above three relations, we have\n\u2016z\u0304(\u00b5)\u2212 z(\u00b5)\u2016 \u2264(1\u2212 \u03ba) \u2016z\u0304(sn)\u2212 z(sn)\u2016\n+ \u03ba \u2016z\u0304(sn+1)\u2212 z(sn+1)\u2016\n+ \u222b sn+1 sn \u2016W2\u2016 \u2016z(\u00b51)\u2016d\u00b51.\nAs \u2016z0\u2016 \u2264 Rin2 , from Remark 7, \u2016z(\u00b5)\u2016 \u2264 Rin2 for all s \u2265 0. Using this with (21), (30), the facts that K2 \u2265 1 and \u03b2n \u2264 [supk\u22650 \u03b2k], and Lemma A.2, the first claim follows:\n\u03bdn+1 \u2264Lde2 [ sup k\u22650 \u03b2k ] + Lsd2 [ sup k\u22650 \u03b7k ] + \u03baLmd2 \u03b2n\n+ ((1\u2212 \u03ba) + \u03baK2) \u2225\u2225Emd2 (sn)\u2225\u2225+ \u2016W2\u2016\u03b2nRin2 .\n\u2264K2 \u2225\u2225Emd2 (sn)\u2225\u2225+ Lz max{sup\nk\u22650 \u03b2k, sup k\u22650 \u03b7k\n} . (31)\nFor the second claim observe that\n\u2016z\u0304(\u00b5)\u2016 \u2264 \u2016z\u0304(\u00b5)\u2212 z(\u00b5)\u2016 + \u2016z(\u00b5)\u2016 .\nHence \u03bd\u2217n+1 \u2264 \u03bdn+1 + sup\n\u00b5\u2208[sn,sn+1] \u2016z(\u00b5)\u2016 .\n\u2016z0\u2016 \u2264 Rin2 , and hence using (15) and (30),\n\u2016z(\u00b5)\u2016 \u2264 K2Rin2 e\u2212q2\u00b5.\nCombining the above two relations with (31), the desired result is now easy to see.\nWe now reproduce the results of Lemma A.3, this time for {\u03b8n} instead of {zn}, and obtain bounds on \u03c1n+1 and \u03c1\u2217n+1 on Gn.\nFor k \u2265 0 and \u03c4 \u2208 [tk, tk+1), let\n\u03b6de(\u03c4) := h1(\u03b8k, \u03bb(\u03b8k))\u2212 h1(\u03b8\u0304(\u03c4), \u03bb(\u03b8\u0304(\u03c4))) = X1[\u03b8\u0304(\u03c4)\u2212 \u03b8k],\n\u03b6md(\u03c4) := M (1) k+1,\n\u03b6 te(\u03c4) := h1(\u03b8k, wk)\u2212 h1(\u03b8k, \u03bb(\u03b8k)) = \u2212W1zk.\nUsing simple manipulations on (1), for any t \u2265 0,\n\u03b8\u0304(t) = \u03b80 + \u222b t 0 [ h1 ( \u03b8\u0304(\u03c4), \u03bb(\u03b8\u0304(\u03c4)) ) + \u03b6(\u03c4) ] d\u03c4,\nwhere \u03b6(\u03c4) = \u03b6de(\u03c4) + \u03b6md(\u03c4) + \u03b6 te(\u03c4). These are respectively perturbations due to discretization, martingale difference noise, and error in tracking the equilibrium of (5). Recall that as \u03b8n evolves, the equilibria of (5) moves. The tracking error is a function of the zn which, from (7), is the difference between wn and \u03bb(\u03b8n). By the VoC formula,\n\u03b8\u0304(t) = \u03b8(t) + E1(t), (32)\nwhere E1(t) = Ede1 (t) + E md 1 (t) + E te 1 (t) with\nEde1 (t) = \u222b t 0 e\u2212X1(t\u2212\u03c4)\u03b6de(\u03c4)d\u03c4,\nand similarly for Emd1 (t) and E te 1 (t). As in Subsection 4.3, to obtain bounds on \u03c1n+1 and \u03c1 \u2217 n+1, it suffices to get bounds on \u2225\u2225Ede1 (\u00b7)\u2225\u2225 , \u2225\u2225Emd1 (\u00b7)\u2225\u2225 , and \u2016Ete1 (\u00b7)\u2016 on the interval [tn, tn+1], on the event Gn.\nIn the same way as in (30), there exist q1 and K1 \u2265 1 so that\u2225\u2225\u2225e\u2212X1(t\u2212\u03c4)\u2225\u2225\u2225 \u2264 K1e\u2212q1(t\u2212\u03c4), \u2200t \u2265 \u03c4. (33) Fix q \u2208 (0, qmin), where qmin := min{q1, q2} and q2 is from (30). The next lemma gives bounds on the three components of E1(t) at the extremes {tn, tn+1}. Lemma A.5 (Perturbation Error Bounds for \u03b8n). Fix n \u2265 0. Then on Gn,\nsup `\u2208{n,n+1} \u2225\u2225Ede1 (t`)\u2225\u2225 \u2264 Lde1 [sup k\u22650 \u03b1k ] ,\nsup `\u2208{n,n+1} \u2016Ete1 (t`)\u2016 \u2264 Lte1a e\u2212qtn + Lte1b [ sup k\u22650 \u03b2k ] + Lte1c [ sup 0\u2264k\u2264n \u03bdk+1 ] ,\u2225\u2225Emd1 (tn+1)\u2225\u2225 \u2264 K1 \u2225\u2225Emd1 (tn)\u2225\u2225+ Lmd1 \u03b1n.\nwhere Lde1 := K1J \u03b8\u2016X1\u2016 q1 , Lte1a := K1 \u2016W1\u2016K2Rin2 1(qmin\u2212q)e , L te 1b := K1 \u2016W1\u2016 \u2016W2\u2016Rin2 /q1, Lte1c := K1 \u2016W1\u2016 /q1, Lmd1 := K1m1[1 +R\u2217 +Rout1 +Rw2 ].\nSimilarly to Subsection 4.3, the next lemma bounds \u03c1n+1 and \u03c1\u2217n+1 with decaying terms using Lemma A.5. Lemma A.6 (ODE-SA Distance Bound for \u03b8n). Fix n \u2265 0. Then on Gn,\n\u03c1n+1 \u2264K1 \u2225\u2225Emd1 (tn)\u2225\u2225+ L\u03b8a e\u2212qtn + L\u03b8b [sup\nk\u22650 \u03b2k\n] + L\u03b8c [ sup\n0\u2264k\u2264n \u03bdk+1\n] ,\n\u03c1\u2217n+1 \u2264K1 \u2225\u2225Emd1 (tn)\u2225\u2225+ [K1Rin1 + L\u03b8a]e\u2212qtn + L\u03b8b [sup\nk\u22650 \u03b2k\n] + L\u03b8c [ sup\n0\u2264k\u2264n \u03bdk+1\n] ,\nwhere L\u03b8a = L te 1a, L \u03b8 c = L te 1c and L \u03b8 b := L de 1 + L md 1 + \u2016X1\u2016Rin1 + Lte1b.\nWe first provide the following technical result for later usage. Lemma A.7 (Dominating Decay Rate Bound). Fix q \u2208 (0, qmin) where qmin := min{q1, q2}. Then for n \u2265 0,\nn\u22121\u2211 k=0 \u222b tk+1 tk e\u2212q1(tn\u2212\u03c4)e\u2212q2\u03be(\u03c4)d\u03c4 \u2264 1 (qmin \u2212 q)e e\u2212qtn .\nProof. From (4), \u03b2k \u2265 \u03b1k \u2200k \u2265 1. Using this and (11), \u2200k \u2265 1 and \u03c4 \u2208 [tk, tk+1], \u03be(\u03c4) \u2212 sk \u2265 \u03c4 \u2212 tk. Hence for any \u03c4 \u2208 [0, tn],\n\u2212q1(tn \u2212 \u03c4)\u2212 q2(\u03be(\u03c4)\u2212) \u2264 \u2212qmin(tn \u2212 0).\nNow, since 1\u03b1e is the maximum of xe \u2212\u03b1x,\ntne \u2212qmintn\n=tne \u2212(qmin\u2212q)tne\u2212qtn\n\u2264 1 (qmin \u2212 q)e e\u2212qtn .\nThe desired result now follows.\nProof of Lemma A.5. For the first claim of the lemma fix ` \u2208 {n, n + 1}. Let k \u2208 {0, . . . , ` \u2212 1} and \u03c4 \u2208 [tk, tk+1). With I\u03b8(k) as in (24),\u2225\u2225\u03b6de(\u03c4)\u2225\u2225 \u2264 \u2016X1\u2016 (\u03c4 \u2212 tk)I\u03b8(k) \u2264 \u03b1k \u2016X1\u2016 I\u03b8(k). So by Lemma A.1, on Gn,\n\u2225\u2225\u03b6de(\u03c4)\u2225\u2225 \u2264 \u03b1k \u2016X1\u2016 J\u03b8. The first claim now follows by (33) and Lemma (A.4).\nFor proving the second claim of the lemma let ` = n. By triangle inequality,\n\u2016Ete1 (tn)\u2016 \u2264 n\u22121\u2211 k=0 \u222b tk+1 tk \u2225\u2225\u2225e\u2212X1(tn\u2212\u03c4)\u2225\u2225\u2225\u2225\u2225\u03b6 te(\u03c4)\u2225\u2225 d\u03c4. Using (33), it follows that\n\u2016Ete1 (tn)\u2016 \u2264 K1 n\u22121\u2211 k=0 \u222b tk+1 tk e\u2212q1(tn\u2212\u03c4) \u2225\u2225\u03b6 te(\u03c4)\u2225\u2225d\u03c4.\nFix k \u2208 {0, . . . , n\u2212 1} and \u03c4 \u2208 [tk, tk+1). Then\u2225\u2225\u03b6 te(\u03c4)\u2225\u2225 \u2264 \u2016W1\u2016 \u2016zk\u2016 . Using (11) and the triangle inequality,\n\u2016zk\u2016 \u2264 \u2016z(\u03be(\u03c4))\u2016\n+ \u2016z(\u03be(\u03c4))\u2212 z(\u03be(tk))\u2016 + \u2016zk \u2212 z(\u03be(tk))\u2016 .\nAs \u2016z0\u2016 \u2264 Rin2 , by (15) and (30), \u2016z(\u03be(\u03c4))\u2016 \u2264 K2Rin2 e\u2212q2(\u03be(\u03c4)\u2212). Remark 7 also implies that, as \u2016z0\u2016 \u2264 Rin2 , \u2016z(s)\u2016 \u2264 Rin2 for all s \u2265 0. Hence by (8), \u2016z(\u03be(\u03c4))\u2212 z(\u03be(tk))\u2016\n\u2264 \u2225\u2225\u2225\u2225\u2225 \u222b \u03be(\u03c4) \u03be(tk) [\u2212W2] z(\u00b5)d\u00b5 \u2225\u2225\u2225\u2225\u2225 \u2264 \u2016W2\u2016Rin2 \u03b2k,\nwhere the last relation holds as [\u03be(\u03c4)\u2212 \u03be(tk)] \u2264 [sk+1 \u2212 sk]. Also note that, by (19), \u2016zk \u2212 z(\u03be(tk))\u2016 \u2264 \u03bdk+1. Combining the above relations,\u2225\u2225\u03b6 te(\u03c4)\u2225\u2225 \u2264 \u2016W1\u2016 [ K2R in 2 e \u2212q2(\u03be(\u03c4)\u2212)\n+ \u2016W2\u2016Rin2 \u03b2k + \u03bdk+1 ]\n\u2264 \u2016W1\u2016 [ K2R in 2 e \u2212q2(\u03be(\u03c4)\u2212)\n+ \u2016W2\u2016Rin2 [ sup k\u22650 \u03b2k ] + [ sup 0\u2264k\u2264n\u22121 \u03bdk+1 ]] By Lemma A.7 and the fact that \u222b tn 0 e\u2212q1(tn\u2212\u03c4)d\u03c4 \u2264 1/q1,\n\u2016Ete1 (tn)\u2016 \u2264 Lte1ae\u2212q(tn\u22120) + Lte1b [ sup k\u22650 \u03b2k ] + Lte1c [ sup 0\u2264k\u2264n\u22121 \u03bdk+1 ] .\nA similar bound holds for ` = n+ 1. Since e\u2212q(tn+1\u22120) \u2264 e\u2212q(tn\u22120), the second claim of the lemma follows.\nThe third claim of the lemma, bounding \u2225\u2225Emd2 (sn+1)\u2225\u2225, follows in a similar way to the third claim of Lemma A.2.\nProof of Lemma A.6. Let \u03c4 \u2208 [tn, tn+1]. Then arguing as in proof of Lemma A.3 and using (6), there exists \u03ba \u2208 [0, 1] such that \u2225\u2225\u03b8\u0304(\u03c4)\u2212 \u03b8(\u03c4)\u2225\u2225\n\u2264 (1\u2212 \u03ba) \u2225\u2225\u03b8\u0304(tn)\u2212 \u03b8(tn)\u2225\u2225\n+\u03ba \u2225\u2225\u03b8\u0304(tn+1)\u2212 \u03b8(tn+1)\u2225\u2225\n+ \u222b tn+1 tn \u2016X1\u2016 \u2016\u03b8(\u03c41)\u2212 \u03b8\u2217\u2016 d\u03c41.\nSince \u2225\u2225\u03b8\u0304(0)\u2212 \u03b8\u2217\u2225\u2225 \u2264 Rin1 , from Remark 7, \u2016\u03b8(\u03c4)\u2212 \u03b8\u2217\u2016 \u2264 Rin1 for all t \u2265 0. Using this with (32) and (30), the facts that K1 \u2265 1,\n\u03b1n \u2264 [ sup k\u22650 \u03b1k ] \u2264 [ sup k\u22650 \u03b2k ] ,\nand Lemma A.5, the first claim of the lemma follows: \u03c1n+1 \u2264Lde1 [ sup k\u22650 \u03b2k ] + Lte1ae \u2212q(tn\u22120) + Lte1b [ sup k\u22650 \u03b2k ] + Lte1c [ sup\n0\u2264k\u2264n \u03bdk+1\n] + \u03baLmd1 [ sup k\u22650 \u03b2k ] + (\u03ba+ (1\u2212 \u03ba)K1)\n\u2225\u2225Emd1 (tn)\u2225\u2225 + \u2016X1\u2016Rin1 [ sup k\u22650 \u03b2k\n] \u2264K1\n\u2225\u2225Emd1 (tn)\u2225\u2225+ L\u03b8a e\u2212q(tn\u22120) + L\u03b8b [ sup k\u22650 \u03b2k ] + L\u03b8c [ sup 0\u2264k\u2264n \u03bdk+1 ] . (34)\nFor the second claim of the lemma, notice that\u2225\u2225\u03b8\u0304(\u03c4)\u2212 \u03b8\u2217\u2225\u2225 \u2264 \u2225\u2225\u03b8\u0304(\u03c4)\u2212 \u03b8(\u03c4)\u2225\u2225 + \u2016\u03b8(\u03c4)\u2212 \u03b8\u2217\u2016 .\nThus, we have \u03c1\u2217n+1 \u2264 \u03c1n+1 + sup\n\u03c4\u2208[tn,tn+1] \u2016\u03b8(\u03c4)\u2212 \u03b8\u2217\u2016 .\u2225\u2225\u03b8\u0304(0)\u2212 \u03b8\u2217\u2225\u2225 \u2264 Rin1 , and hence using (14), \u2016\u03b8(\u03c4)\u2212 \u03b8\u2217\u2016 \u2264 K1Rin1 e\u2212q1(\u03c4\u22120). Combining the above two relations and using (34) and the fact that q < q1, the second claim of the lemma follows."}, {"heading": "A.3 Proofs from Subsection 4.4", "text": "We first bring Lemmas A.8 to A.11 for proving Lemma A.12. Lemma A.8. For n \u2265 0,\n[Gn \u2229 {\u03bdn+1 \u2265 Rgap2 }] \u2286 [ Gn \u2229 { K2 \u2225\u2225Emd2 (sn)\u2225\u2225 \u2265 23 }] .\nProof. The desired result follows from Lemma A.3, (17), (A4), and the fact that 2/3 \u2264 2/2 \u2264 Rgap2 /2.\nLemma A.9. Fix 0 \u2265 N0. Then for n \u2265 0, [Gn \u2229 {\u03c1n+1 \u2265 Rgap1 }]\n\u2286 [ Gn \u2229 { K1 \u2225\u2225Emd1 (tn)\u2225\u2225 \u2265 14 }]\u222a\nn\u22c3 k=0 [ Gk \u2229 { L\u03b8cK2 \u2225\u2225Emd2 (sk)\u2225\u2225 \u2265 18 }] .\nProof. By Lemma A.6 and since Rgap1 = 4L \u03b8 a, L \u03b8 ae \u2212q(tn\u22120) \u2264 Rgap1 /4. Combined with (22) and (A5), we get that\n[Gn \u2229 {\u03c1n+1 \u2265 Rgap1 }]\n\u2286 [ Gn \u2229 { K1 \u2225\u2225Emd1 (tn)\u2225\u2225 \u2265 Rgap14 }] \u222a [ Gn \u2229 { L\u03b8c [ sup\n0\u2264k\u2264n \u03bdk+1\n] \u2265 R gap 1\n4\n}] .\nAs Rgap1 \u2265 1 due to (17), (A4) and Gn \u2286 Gk for all 0 \u2264 k \u2264 n, the desired result follows from Lemma A.3.\nLemma A.10. Fix n1 \u2265 N1. Then for all n \u2265 n1,\n[Gn \u2229 {\u03bd\u2217n+1 \u2265 2}] \u2286 [ Gn \u2229 { K2 \u2225\u2225Emd2 (sn)\u2225\u2225 \u2265 23 }] .\nProof. The desired result follows from Lemma A.3 and (22).\nLemma A.11. Fix n1 \u2265 N1. Then for all n \u2265 n1,\n[Gn \u2229 {\u03c1\u2217n+1 \u2265 1}]\n\u2286 [ Gn \u2229 { K1 \u2225\u2225Emd1 (tn)\u2225\u2225 \u2265 14 }]\u222a\nn\u22c3 k=0 [ Gk \u2229 { L\u03b8cK2 \u2225\u2225Emd2 (sk)\u2225\u2225 \u2265 18 }] . Proof. Arguing as in the proof of Lemma A.9, the desired result follows from the second claim in Lemma A.6, first claim in Lemma A.3, (17), (A4), (22) and (A5).\nLemma A.12 (Bound Form for Event of Interest). Let n1 \u2265 N1. Then\nEc(T ) \u2286Emid \u222a Eafter \u2286 [ \u221e\u22c3 n=0 [ Gn \u2229 { K1 \u2225\u2225Emd1 (tn)\u2225\u2225 \u2265 14 }] ]\n\u222a [ \u221e\u22c3 n=0 [ Gn \u2229 { K2 \u2225\u2225Emd2 (sn)\u2225\u2225 \u2265 23 }] ] \u222a [ \u221e\u22c3 n=0 [ Gn \u2229 { L\u03b8cK2 \u2225\u2225Emd2 (sn)\u2225\u2225 \u2265 18 }] ] .\nProof of Lemma A.12. This desired result follows from Lemma 4.1 and the Lemmas A.8, A.9, A.10, and A.11 put together.\nLastly, to provide the proof of our main technical theorem, we give the two following lemmas. For n \u2265 0, let an := \u2211n\u22121 k=0 \u03b1 2 ke \u22122q1(tn\u2212tk+1).\nLemma A.13 (Azuma-Hoeffding for Emd1 ). Fix \u03b4 > 0. Then for any n \u2265 0,\nPr { Gn, \u2225\u2225Emd1 (tn)\u2225\u2225 \u2265 \u03b4} \u2264 2d2 exp(\u2212 \u03b42d3[Lmd1 ]2an ) .\nProof. Let Ak,n be the matrix \u222b tk+1 tk\ne\u2212X1(tn\u2212\u03c4)d\u03c4 with Aijk,n denoting its i, j\u2212th entry. Let M\n(1) k+1(j) denote the j\u2212th entry of M (1) k+1. On Gn, 1Gk = 1 for all 0 \u2264 k \u2264 n. So\nPr { Gn, \u2225\u2225Emd1 (tn)\u2225\u2225 \u2265 \u03b4} = Pr { Gn,\n\u2225\u2225\u2225\u2225\u2225 n\u22121\u2211 k=0 Ak,nM (1) k+11Gk \u2225\u2225\u2225\u2225\u2225 \u2265 \u03b4 }\n\u2264 Pr {\u2225\u2225\u2225\u2225\u2225 n\u22121\u2211 k=0 Ak,nM (1) k+11Gk \u2225\u2225\u2225\u2225\u2225 \u2265 \u03b4 }\n\u2264 d\u2211 i=1 d\u2211 j=1 Pr {\u2225\u2225\u2225\u2225\u2225 n\u22121\u2211 k=0 Aijk,nM (1) k+1(j)1Gk \u2225\u2225\u2225\u2225\u2225 \u2265 \u03b4d\u221ad } ,\nwhere the last relation is due to the union bound applied twice. On Gk, K1 \u2225\u2225\u2225M (1)k+1\u2225\u2225\u2225 \u2264 Lmd1 . Hence, on Gk, for any i, j \u2208 {1, . . . , d}, using (33),\n|Aijk,n| |M (1) k+1(j)| \u2264 \u2016Ak,n\u2016 \u2016Mk+1\u2016\n\u2264 K1Lmd1 \u03b1ke\u2212q1(tn\u2212tk+1).\nUsing \u2211n\u22121 k=0 \u03b1 2 ke \u22122q1(tn\u2212tk+1) \u2264 an, the desired result now follows from the Azuma-Hoeffding inequality. For n \u2265 0, let bn := \u2211n\u22121 k=0 \u03b2 2 ke \u22122q2(sn\u2212sk+1).\nLemma A.14 (Azuma-Hoeffding for Emd2 ). Fix \u03b4 > 0. Then for any n \u2265 0,\nPr { Gn, \u2225\u2225Emd2 (sn)\u2225\u2225 \u2265 \u03b4} \u2264 2d2 exp(\u2212 \u03b42d3[Lmd2 ]2bn ) .\nProof. The proof follows similarly to that of Lemma A.13.\nOur main technical result directly follows from Lemma A.12.\nProof of Theorem 4.2. The proof follows from Lemmas A.12, A.13 and A.14."}, {"heading": "A.4 Proofs from Section 5", "text": "We begin with a presentation of the RL framework in this section. A MDP is defined by the 5-tuple (S,A, P,R, \u03b3) (Sutton, 1988), where S is the set of states, A is the set actions, P = P (s\u2032|s, a) is the transition kernel, R(s, a, s\u2032) is the reward function, and \u03b3 \u2208 (0, 1) is the discount factor. In each time-step, the process is in some state sn \u2208 S, an action an \u2208 A is taken, the system transitions to a next state s\u2032n \u2208 S according to a transition kernel P (sn, an, s\u2032n), and an immediate reward rn is received according to R(sn, an, s\u2032n). Let policy \u03c0 : S \u2192 A be a stationary mapping from states to actions and V \u03c0(s) = E\u03c0[ \u2211\u221e n=0 \u03b3\nnrn|s0 = s] be the value function at state s w.r.t \u03c0. In our policy evaluation setting the goal is to estimate the MDP\u2019s value function V \u03c0(s) with respect to a given \u03c0 using linear regression, i.e., V \u03c0(s) \u2248 \u03b8>\u03c6(s), where \u03c6(s) \u2208 Rd is a feature vector at state s, and \u03b8 \u2208 Rd is a parameter vector. For brevity, we omit the notation \u03c0 and denote \u03c6(sn), \u03c6(s\u2032n) by \u03c6n, \u03c6 \u2032 n. Finally, we introduce the notation \u03b4n = rn + \u03b3\u03b8 > n \u03c6 \u2032 n \u2212 \u03b8>n \u03c6n."}, {"heading": "A.5 GTD2", "text": "The GTD2 algorithm (Sutton et al., 2009b) minimizes the objective function\nJMSPBE(\u03b8) = 12 (b\u2212A\u03b8) >C\u22121(b\u2212A\u03b8). (35)\nThe update rule of the algorithm takes the form of Equations (1) and (2) with\nh1(\u03b8, w) = A >w,\nh2(\u03b8, w) = b\u2212A\u03b8 \u2212 Cw,\nand\nM (1) n+1 = (\u03c6n \u2212 \u03b3\u03c6\u2032n)\u03c6>nwn \u2212A>wn ,\nM (2) n+1 =rn\u03c6n + \u03c6n[\u03b3\u03c6 \u2032 n \u2212 \u03c6n]>\u03b8n \u2212 \u03c6n\u03c6>nwn\n\u2212 [b\u2212A\u03b8n \u2212 Cwn] .\nThat is, in case of GTD2 the relevant matrices in the update rules take the form \u03931 = 0, W1 = \u2212A>, v1 = 0, and \u03932 = A, W2 = C, v2 = b. Additionally, X1 = \u03931 \u2212W1W\u221212 \u03932 = A>C\u22121A. By our assumptions, both W2 and X1 are symmetric positive definite matrices, and thus the real part of their eigenvalues are also positive. It is also clear that\n\u2016M (1)n+1\u2016 \u2264(1 + \u03b3 + \u2016A\u2016)\u2016wn\u2016,\n\u2016M (2)n+1\u2016 =\u2016rn\u03c6n \u2212 b+ [A+ \u03c6n(\u03b3\u03c6\u2032n \u2212 \u03c6n)>]\u03b8n \u2212 [\u03c6n\u03c6>n \u2212 C]wn\u2016\n\u22641 + \u2016b\u2016+ (1 + \u03b3 + \u2016A\u2016)\u2016\u03b8n\u2016 + (1 + \u2016C\u2016)\u2016wn\u2016.\nConsequently, Assumption A3 is satisfied with constants m1 = (1 + \u03b3 + \u2016A\u2016) and m2 = 1 + max(\u2016b\u2016, \u03b3 + \u2016A\u2016, \u2016C\u2016)."}, {"heading": "A.6 TDC", "text": "The TDC algorithm is designed to minimize (35), just like GTD2.\nThe update rule of the algorithm takes the form of Equations (1) and (2) with\nh1\u03b8(\u03b8, w) = b\u2212A\u03b8 + [A> \u2212 C]w , h2(\u03b8, w) = b\u2212A\u03b8 \u2212 Cw ,\nand\nM (1) n+1 =rn\u03c6n + \u03c6n[\u03b3\u03c6 \u2032 n \u2212 \u03c6n]>\u03b8n \u2212 \u03b3\u03c6\u2032\u03c6>wn\n\u2212 [b\u2212A\u03b8n + [A> \u2212 C]wn] ,\nM (2) n+1 =rn\u03c6n + \u03c6n[\u03b3\u03c6 \u2032 n \u2212 \u03c6n]>\u03b8n \u2212 \u03c6n\u03c6>nwn\n\u2212 [b\u2212A\u03b8n + Cwn] .\nThat is, in case of TDC, the relevant matrices in the update rules take the form \u03931 = A, W1 = [C \u2212 A>], v1 = b, and \u03932 = A, W2 = C, v2 = b. Additionally, X1 = \u03931 \u2212 W1W\u221212 \u03932 = A \u2212 [C \u2212 A>]C\u22121A = A>C\u22121A. By our assumptions, both W2 and X1 are symmetric positive definite matrices, and thus the real part of their eigenvalues are also positive. It is also clear that\n\u2016M (1)n+1\u2016 \u22642 + (1 + \u03b3 + \u2016A\u2016)\u2016\u03b8n\u2016 + (\u03b3 + \u2016A\u2016+ \u2016C\u2016)\u2016wn\u2016,\n\u2016M (2)n+1\u2016 =2 + (1 + \u03b3 + \u2016A\u2016)\u2016\u03b8n\u2016+ (1 + \u2016C\u2016)\u2016wn\u2016 .\nConsequently, AssumptionA3 is satisfied with constants m1 = (2 + \u03b3 + \u2016A\u2016 + \u2016C\u2016) and m2 = (2 + \u03b3 + \u2016A\u2016+ \u2016C\u2016)."}], "references": [{"title": "Dynamic Programming and Optimal Control", "author": ["D.P. Bertsekas"], "venue": "Vol II. Athena Scientific, fourth edition,", "citeRegEx": "Bertsekas,? \\Q2012\\E", "shortCiteRegEx": "Bertsekas", "year": 2012}, {"title": "Convergent temporal-difference learning with arbitrary smooth function approximation", "author": ["Bhatnagar", "Shalabh", "Precup", "Doina", "Silver", "David", "Sutton", "Richard S", "Maei", "Hamid R", "Szepesv\u00e1ri", "Csaba"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bhatnagar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bhatnagar et al\\.", "year": 2009}, {"title": "Stochastic approximation: a dynamical systems viewpoint", "author": ["Borkar", "Vivek S"], "venue": null, "citeRegEx": "Borkar and S.,? \\Q2008\\E", "shortCiteRegEx": "Borkar and S.", "year": 2008}, {"title": "The ode method for convergence of stochastic approximation and reinforcement learning", "author": ["Borkar", "Vivek S", "Meyn", "Sean P"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Borkar et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Borkar et al\\.", "year": 2000}, {"title": "Actor-Critic Algorithms", "author": ["Konda", "Vijaymohan"], "venue": "PhD thesis, Department of Electrical Engineering and Computer Science, MIT,", "citeRegEx": "Konda and Vijaymohan.,? \\Q2002\\E", "shortCiteRegEx": "Konda and Vijaymohan.", "year": 2002}, {"title": "On td (0) with function approximation: Concentration bounds and a centered variant with exponential convergence", "author": ["Korda", "Nathaniel", "Prashanth", "LA"], "venue": "In ICML, pp", "citeRegEx": "Korda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Korda et al\\.", "year": 2015}, {"title": "Stochastic Approximation Algorithms and Applications", "author": ["Kushner", "Harold J", "Yin", "G. George"], "venue": null, "citeRegEx": "Kushner et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Kushner et al\\.", "year": 1997}, {"title": "Method of variation of parameters for dynamic systems", "author": ["Lakshmikantham", "Vangipuram", "Deo", "Sadashiv G"], "venue": null, "citeRegEx": "Lakshmikantham et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lakshmikantham et al\\.", "year": 1998}, {"title": "Finite-sample analysis of proximal gradient td algorithms", "author": ["Liu", "Bo", "Ji", "Ghavamzadeh", "Mohammad", "Mahadevan", "Sridhar", "Petrik", "Marek"], "venue": "In UAI,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Sutton", "Richard S"], "venue": "Machine learning,", "citeRegEx": "Sutton and S.,? \\Q1988\\E", "shortCiteRegEx": "Sutton and S.", "year": 1988}, {"title": "A convergent o (n) temporal-difference algorithm for off-policy learning with linear function approximation", "author": ["Sutton", "Richard S", "Maei", "Hamid R", "Szepesv\u00e1ri", "Csaba"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sutton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["Sutton", "Richard S", "Maei", "Hamid Reza", "Precup", "Doina", "Bhatnagar", "Shalabh", "Silver", "David", "Szepesv\u00e1ri", "Csaba", "Wiewiora", "Eric"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Sutton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "An emphatic approach to the problem of off-policy temporal-difference learning", "author": ["Sutton", "Richard S", "Mahmood", "A Rupam", "White", "Martha"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Sutton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2015}, {"title": "Ordinary differential equations and dynamical systems", "author": ["Teschl", "Gerald"], "venue": null, "citeRegEx": "Teschl and Gerald.,? \\Q2004\\E", "shortCiteRegEx": "Teschl and Gerald.", "year": 2004}, {"title": "A concentration bound for stochastic approximation via alekseev\u2019s formula", "author": ["Thoppe", "Gugan", "Borkar", "Vivek S"], "venue": null, "citeRegEx": "Thoppe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Thoppe et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "In addition to convergence, there also exists a concentration result for the GTD family, though only for the single-timescale setting (Liu et al., 2015).", "startOffset": 134, "endOffset": 152}, {"referenceID": 0, "context": "It is known that A is positive definite (Bertsekas, 2012).", "startOffset": 40, "endOffset": 57}], "year": 2017, "abstractText": "Two-timescale Stochastic Approximation (SA) algorithms are widely used in Reinforcement Learning (RL). Their iterates have two parts that are updated with distinct stepsizes. In this work we provide a recipe for analyzing two-timescale SA. Using it, we develop the first convergence rate result for them. From this result we extract key insights on stepsize selection. As an application, we obtain convergence rates for two-timescale RL algorithms such as GTD(0), GTD2, and TDC.", "creator": "LaTeX with hyperref package"}}}