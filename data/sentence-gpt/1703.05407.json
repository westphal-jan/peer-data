{"id": "1703.05407", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play", "abstract": "We describe a simple scheme that allows an agent to explore its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on (nearly) reversible environments, or environments that can be reset, and Alice will \"propose\" the task by running a set of actions and then Bob must partially undo, or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When deployed on an RL task within the environment, this unsupervised training reduces the number of episodes needed to learn. For example, Alice and Bob want a video or movie. If Alice chooses a movie, they need to use Alice's manual, and Alice will only get the video, whereas Bob may also play a movie and have to complete the required training, as long as Alice's manual is in the \"good\" state. The following approach shows the following way:\n\nIn a situation where a task is a task that's in the good state of the situation, Alice can use Alice's manual, and Alice will only get the video, whereas Bob may also play a movie and have to complete the required training. However, as in a situation where the task is a task that's in the good state of the situation, Alice must still be a task that's in the bad state of the situation.\nIn a scenario where the task is a task that's in the good state of the situation, Alice may actually use Alice's manual, and Alice will only get the video, while Bob may also play a movie and have to complete the required training. However, as in a situation where a task is a task that's in the bad state of the situation, Alice will still be a task that's in the good state of the situation.\nThis is the way Alice can choose to do some experiments in the game, like picking up and running a game. If Alice chooses to explore a world without moving around, Alice will not be able to learn that, but instead to try to solve the world by creating some of the world's resources. Furthermore, as in a situation where Alice has an unsupervised knowledge of the environment, she may also be able to choose to read a paper.\nIn a situation where Alice is not able to learn that, she may also try to", "histories": [["v1", "Wed, 15 Mar 2017 22:27:43 GMT  (1257kb,D)", "http://arxiv.org/abs/1703.05407v1", null], ["v2", "Wed, 19 Apr 2017 23:32:25 GMT  (1227kb,D)", "http://arxiv.org/abs/1703.05407v2", null], ["v3", "Sun, 4 Jun 2017 12:44:45 GMT  (1430kb,D)", "http://arxiv.org/abs/1703.05407v3", null], ["v4", "Sun, 29 Oct 2017 16:02:21 GMT  (1415kb,D)", "http://arxiv.org/abs/1703.05407v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sainbayar sukhbaatar", "zeming lin", "ilya kostrikov", "gabriel synnaeve", "arthur szlam"], "accepted": false, "id": "1703.05407"}, "pdf": {"name": "1703.05407.pdf", "metadata": {"source": "META", "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play", "authors": ["Sainbayar Sukhbaatar", "Ilya Kostrikov", "Arthur Szlam", "Rob Fergus"], "emails": ["Sukhbaatar<sainbar@cs.nyu.edu>."], "sections": [{"heading": null, "text": "We describe a simple scheme that allows an agent to explore its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on (nearly) reversible environments, or environments that can be reset, and Alice will \u201cpropose\u201d the task by running a set of actions and then Bob must partially undo, or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When deployed on an RL task within the environment, this unsupervised training reduces the number of episodes needed to learn."}, {"heading": "1. Introduction", "text": "Model-free approaches to reinforcement learning are inefficient, typically requiring a huge number of episodes to learn a satisfactory policy. The lack of an explicit environment model means the agent must learn it from scratch at the same time as it tries to understand which trajectories lead to rewards. In environments where reward is sparse, only a small fraction of the agents\u2019 experience is directly used to update the policy, contributing to the inefficiency.\nIn this paper we introduce a novel form of unsupervised training for an agent that enables exploration and learning about the environment without any external reward that incentivizes the agents to learn how to transition between states as efficiently as possible. We demonstrate that this unsupervised training allows the agent to learn new tasks within the environment more quickly than starting with a random policy.\n1Dept. of Computer Science, Courant Institute, New York University 2Facebook AI Research, New York. Correspondence to: Sainbayar Sukhbaatar<sainbar@cs.nyu.edu>.\nSTOP\nAlice\nBob\nFigure 1. Illustration of self-play in a Mazebase task. Alice sets Bob a task he must complete. In this instance, Alice first picks up the key, then opens the door. After passing through, she turns off the light and then decides to perform the STOP action. This hands control of the agent over to Bob. He must now return the environment to its original state to receive the (internal) reward. Thus he must first turn the light back on, then go back through the door and drop the key at its original location before returning to Alice\u2019s start location. This forces Bob to learn the function of the various items in the environment. This task is just one of many devised by Alice, who automatically builds a curriculum of increasingly challenging tasks. When Bob is presented with a new evaluation task, e.g. go to the flag (present only in the new task) to obtain external reward, he is able to learn it relatively quickly since he already knows how the environment works."}, {"heading": "2. Approach", "text": "We consider environments with a single physical agent, but we allow it to have two separate minds: Alice and Bob, each with its own objective and parameters. At a high level, Alice\u2019s job is to propose a task for Bob to complete, and Bob\u2019s job is to complete the task.\nOur approach is restricted to two classes of environment: (i) those that are (nearly) reversible, or (ii) ones that can be reset to their initial state at least once. These restrictions allow us to sidestep complications around how to communicate the task and determine its difficulty (see Section 5 for further discussion). In these two scenarios, Alice starts at some initial state s0 and proposes a task by doing it, i.e. executing a sequence of actions that takes the agent to a state st. She then outputs a STOP action which hands control over to Bob. In reversible environments, Bob\u2019s goal is to return the agent back to state s0 (or within some margin of it, if the state is continuous), to receive reward. In partially observable environments, the objective is relaxed to Bob finding a state that returns the same observation as Alice\u2019s initial state. In environments where resets are permissible, Alice\u2019s STOP action also reinitializes\nar X\niv :1\n70 3.\n05 40\n7v 1\n[ cs\n.L G\n] 1\n5 M\nar 2\n01 7\nthe environment, thus Bob starts at s0 and now must reach st to be rewarded, thus repeating Alice\u2019s task instead of reversing it. See Fig. 1 for an example, and also Algorithm 1.\nIn both cases, this self-play between Alice and Bob only involves internal reward (detailed below), thus multiple rounds can be performed without needing any supervisory signal from the environment. As such, it comprises a form of unsupervised training where Alice and Bob explore the environment and learn how it operates. This exploration can be leveraged for some target task by using Bob\u2019s policy as the agent\u2019s initialization. Alternatively, the self-play and target task episodes can be interleaved, biasing the exploration to be in service of the target task.\nWe choose the reward structure for Alice and Bob to encourage Alice to push Bob past his comfort zone, but not give him impossible tasks. Denoting Bob\u2019s reward by Rb and Alice\u2019s reward byRa, we use\nRb=\u2212tb (1)\nwhere tb is the time taken by Bob to complete his task (and is set it set maximum value tb=tMax if Bob fails) and\nRa=max(0,tb\u2212ta) (2)\nwhere ta is the time until Alice performs the STOP action. Thus Alice is rewarded if Bob takes more time, but the negative term on her own time will encourage Alice not to take too many steps when Bob is failing. For both reversible and resettable environments, Alice must limit her steps to make Bob\u2019s task easier, thus Alice\u2019s optimal behavior is to find simplest tasks that Bob cannot complete. This eases learning for Bob since the new task will be only just beyond his current capabilities. The self-regulating feedback between Alice and Bob allows them to automatically construct a curriculum for exploration, a key contribution of our approach."}, {"heading": "2.1. Parameterizing Alice and Bob\u2019s actions", "text": "Alice and Bob each have policy functions which take as input two observations of state variables, and output a distribution over actions . In Alice\u2019s case, the function will be of the form\naAlice =fA(st,s0),\nwhere s0 is the observation of the initial state of the environment and st is the observation of the current state. In Bob\u2019s case, the function will be\naBob =fB(s \u2032 t,s \u2032 0),\nwhere s\u20320 = s0 when we have a reversible environment. In a resettable environment s\u20320 is the state where Alice executed the stop action. Note that the \u201cobservations\u201d can include a parameterized model of a raw observation. When a target task is presented, the agent\u2019s policy function is aTarget =fB(s\u2032\u2032t ,e), where e is a special observation corresponding to the target task.\nIn the experiments below, we demonstrate our approach in settings where f is tabular; where it is a neural network taking discrete inputs, and where it is a neural network taking in continuous inputs."}, {"heading": "2.2. Universal Bob in the tabular setting", "text": "We now show that in environments with finite states, tabular policies, and Markovian transitions, we can interpret the reset and reverse games as training the agents to find policies that can get from any state to any other in the least expected number of steps.\nNote that as discussed above, the policy table for both Alice and Bob is indexed by (s0,sT ), not just by si. In particular, with the assumptions above, this means that there is a policy \u03c0fast such that \u03c0fast(s0,sT ) has the smallest expected number of steps to transition from s0 to sT . Call any such policy a fast policy. It is clear that \u03c0fast is a universal policy for Bob, such that for any Alice policy \u03c0a, \u03c0fast is optimal with respect to \u03c0a. In a reset game, with deterministic transitions, \u03c0fast nets Bob a return of 0, and in the reverse game, the return of \u03c0fast against an optimal Alice also using \u03c0fast can be considered a measure of the reversibility of the environment.\nFor this discussion, assume that we are using the reset game or the reverse game in a perfectly reversible environment. If \u03c0A and \u03c0B are policies of Alice and Bob that are in equilibrium (that is, one cannot make Alice better without changing Bob, and one cannot make Bob better without changing Alice), \u03c0B is a fast policy. To see this, note that if \u03c0B is not fast, then we can replace it with \u03c0fast, and then for any challenge (s0,sT ) that Alice gives Bob with nonzero probability and for which \u03c0fast(s0,sT ) gives a smaller number of expected steps, Bob will get a higher reward. On the other hand, if Alice is not giving positive probability to some challenge (s0,sT ) (where the initial probability of Alice starting at s0 is nonzero), and if Bob\u2019s policy on (s0, sT ) is not fast, then Alice can use \u03c0fast(s0,sT ) and increase her reward.\nThus we can see that in the finite, tabular, and Markovian setting, the asymmetric self-play can be interpreted as a method for training Alice and Bob to be able to transit between pairs of states as efficiently as possible."}, {"heading": "3. Related Work", "text": "Self-play arises naturally in reinforcement learning, and has been well studied. For example, for playing checkers (Samuel, 1959), backgammon (Tesauro, 1995), and Go, (Silver et al., 2016), and in in multi-agent games such as RoboSoccer (Riedmiller et al., 2009). Here, the agents or teams of agents compete for external reward. This differs from our scheme where the reward is purely internal and the self-play is a way of motivating an agent to learn about its environment to augment sparse rewards from separate target tasks.\nfunction SELFPLAYEPISODE(REVERSE/REPEAT,tMAX) ta\u21900 s0\u2190 env.observe() sTarget=s0 while True do\n# Alice\u2019s turn ta\u2190ta+1 sta\u2190 env.observe() ata\u2190 policy.Alice(sta , s0) if ata = STOP then\nsTarget=sta env.reset() break\nenv.act(ata) tb\u21900 while True do\n# Bob\u2019s turn tb\u2190tb+1 s\u2032tb\u2190 env.observe() a\u2032tb\u2190 policy.Bob(s \u2032 tb , sTarget) env.act(a\u2032tb) if s\u2032tb =sTarget or tb>tMax then\nbreak Ra=max(0,tb\u2212ta) Rb=\u2212tb policy.Alice.update(Ra) policy.Bob.update(Rb) return\nOur approach has some relationships with Generative adversarial networks (GANs) (Goodfellow et al., 2014), which train a generative neural net by having it try to fool a discriminator network which tries to differentiate samples from the training examples. (Li et al., 2017) introduce an adversarial approach to dialogue generation, where a generator model is subjected to a form of \u201cTuring test\u201d by a discriminator network. (Mescheder et al., 2017) demonstrate how adversarial loss terms can be combined with variational auto-encoders to permit more accurate density modeling. While GAN\u2019s are often thought of as methods for training a generator, the generator can be thought of as a method for generating hard negatives for the discriminator. From this viewpoint, in our approach, Alice acts as a \u201cgenerator\u201d, finding \u201cnegatives\u201d for Bob, and Bob is the \u201cdiscriminator\u201d.\nThere is a large body of work on intrinsic motivation (Barto, 2013; Singh et al., 2004; Klyubin et al., 2005; Schmidhuber, 1991) for self-supervised learning agents. These works propose methods for training an agent to explore and become proficient at manipulating its environment without necessarily having a specific target task, and without a source of extrinsic supervision. One line in this direction is curiosity-driven exploration (Schmidhuber, 1991). These techniques can be applied in encouraging exploration in the context of reinforcement learning, for example (Bellemare et al., 2016; Strehl & Littman, 2008; Lopes et al., 2012; Tang et al., 2016); Roughly, these use some notion of the novelty of a state to give a reward. In the simplest setting, novelty can be just the\nnumber of times a state has been visited; in more complex scenarios, the agent can build a model of the world, and the novelty is the difficulty in placing the current state into the model. In our work, there is no explicit notion of novelty. Even if Bob has seen a state many times, if he has trouble getting to it, Alice should force him towards that state. Another line of work on intrinsic motivation is a formalization of the notion of empowerment (Klyubin et al., 2005), or how much control the agent has over its environment. Our work is related in the sense that it is in both Alice\u2019s and Bob\u2019s interests to have more control over the environment; but we do not explicitly measure that control except in relation to the tasks that Alice sets.\nCurriculum learning (Bengio et al., 2009) is widely used in many machine learning approaches. Typically however, the curriculum requires at least some manual specification. A key point about our work is that Alice and Bob devise their own curriculum entirely automatically. Previous automatic approaches, such as (Kumar et al., 2010), rely on monitoring training error. But since ours is unsupervised, no training labels are required either.\nOur basic paradigm of \u201cAlice proposing a task, and Bob doing it\u201d is related to the Horde architecture (Sutton et al., 2011) and (Schaul et al., 2015). In those works, instead of using a value function V =V (s) that depends on the current state, a value function that explicitly depends on state and goal V =V (s,g) is used. In our experiments, our models will be parameterized in a similar fashion. The novelty in this work is in how Alice defines the goal for Bob."}, {"heading": "4. Experiments", "text": "The following experiments explore our self-play approach on a variety of tasks, both continuous and discrete, from the Mazebase (Sukhbaatar et al., 2015) and RLLab (Duan et al., 2016) environments. The same protocol is used in all settings: self-play and target task episodes are mixed together and used to train the agent via discrete policy gradient. We evaluate both the reverse and repeat versions of self-play. We demonstrate that the self-play episodes help training, in terms of number of target task episodes needed to learn the task (where we consider the self-play episodes \u201cfree\u201d, since they make no use of environmental reward).\nIn all the experiments we use use policy gradient (Williams, 1992) with a baseline for optimizing the policies. In the tabular task below, we use a constant baseline; in all the other tasks we use a policy parameterized by a neural network, and a baseline that depends on the state. Denote the states in an episode by s(1), ..., s(T), and the actions taken at each of those states as a(1),...,a(T), where T is the length of the episode. The baseline is a scalar function of the states b(s,\u03b8), computed via an extra head on the network producing the action probabilities. Beside maximizing the expected reward with policy gradient,\nthe models are also trained to minimize the distance between the baseline value and actual reward. Thus after finishing an episode, we update the model parameters \u03b8 by\n\u2206\u03b8= T\u2211 t=1\n[ \u2202logp(a(t)|s(t),\u03b8)\n\u2202\u03b8\n( T\u2211 i=t r(i)\u2212b(s(t),\u03b8) )\n\u2212\u03bb \u2202 \u2202\u03b8 ( T\u2211 i=t r(i)\u2212b(s(t),\u03b8) )2. (3) Here r(t) is reward given at time t, and the hyperparameter \u03b3 is for balancing the reward and the baseline objectives, which set to 0.1 in all experiments."}, {"heading": "4.1. Long hallway", "text": "We first describe a simple toy designed to illustrate the function of the asymmetric self-play. The environment consists of M states {s1,...,sM} arranged in a chain. Both Alice and Bob have three possible actions, \u201cleft\u201d, \u201cright\u201d, or \u201cstop\u201d. If the agent is at si with i 6= 1, \u201cleft\u201d takes it to si\u22121; \u201cright\u201d analogously increases the state index, and \u201cstop\u201d transfers control to Bob when Alice runs it and terminates the episode when Bob runs it. We use \u201creturn to initial state\u201d as the self-play task (i.e. Reverse in Algorithm 1). For the target task, we randomly pick a starting state and target state, and the episode is considered successful if Bob moves to the target state and executes the stop action before a fixed number of maximum steps.\nIn this case, the target task is essentially the same as the self-play task, and so running it is not unsupervised learning (and in particular, on this toy example unlike the other examples below, we do not mix self-play training with target task training). However, we see that the curriculum afforded by the self-play is efficient at training the agent to do the target task at the beginning of the training, and is effective at forcing exploration of the state space as Bob gets more competent.\nIn Fig. 2 we plot the number of episodes vs rate of success at the target task with four different methods. We setM=25 and the maximum allowed steps for Alice and Bob to be 30. We use fully tabular controllers; the table is of sizeM2\u00d73, with a distribution over the three actions for each possible (start, end pair).\nThe red curve corresponds to policy gradient, with a penalty of \u22121 given upon failure to complete the task, and a penalty of\u2212s/30 for successfully completing the task in s steps. The magenta curve corresponds to taking Alice to have a random policy (1/2 probability of moving left or right, and not stopping till the maximum allowed steps). The green curve corresponds to policy gradient with an exploration bonus similar to (Strehl & Littman, 2008). That is, we keep count of the number of times Ns the agent has been in each state s, and the reward for s is adjusted by exploration bonus \u03b1/ \u221a Ns, where \u03b1 is a constant balancing the reward from completing the task with the exploration bonus. We choose the weight \u03b1 to maximize\nsuccess at 0.2M episodes from the set {0,0.1,0.2,...,1}. The blue curve corresponds to the asymmetric self-play training.\nWe can see that at the very beginning, a random policy for Alice gives some form of curriculum but eventually is harmful, because Bob never gets to see any long treks. On the other hand, policy gradient sees very few successes in the beginning, and so trains slowly. Using the self-play method, Alice gives Bob easy problems at first (she starts from random), and then builds harder and harder problems as the training progresses, finally matching the performance boost of the count based exploration. Although not shown, similar patterns are observed for a wide range of learning rates."}, {"heading": "4.2. Mazebase", "text": "We now describe experiments using the MazeBase environment (Sukhbaatar et al., 2015). These have discrete actions and states, but sufficient combinatorial complexity that tabular methods cannot be used. They consist of various items placed on a finite 2D grid; the environment is randomly generated for each episode.\nFor both self-play and the target task, we use an environment where the maze contains a light switch, a key and a wall with a door (see Fig. 1). An agent can open or close the door by toggling the key switch, and turn on or off light with the light switch. When the light is off, the agent can only see the (glowing) light switch. There is also a goal flag item in the target task.\nIn self-play, an episode starts with Alice in control, who can navigate through the maze and change the switch states until she outputs the STOP action. Then, Bob takes control and tries to return everything to its original state, restricted to visible items (e.g. if light was off initially, then Bob does not need to worry about the state of door because it was invisible) in the\nreverse self-play. In the repeat version, the maze resets back to its initial state when Bob takes the control, who tries to reach the final state of Alice.\nIn the target task, the agent and the goal are always placed on opposite sides of the wall. Also, the light and key switches are placed on the same side as the agent, but the light is always off and the door is closed initially. Therefore, in order to succeed, the agent has to turn on the light, toggle the key switch to open the door, pass through it, and reach the goal flag. Reward of -0.1 is given at every step until the agent reaches the goal or episode runs more than tMax =80 time steps.\nIn self-play, episodes are also limited to tMax = 80 time steps, and reward is only given at the end of the episode. Alice and Bob\u2019s reward from Equ. (1) and (2) is scaled by hyperparameter \u03b3=0.1 to match the target task reward.\nBoth Alice and Bob\u2019s policies are modeled by a fully-connected neural network with two hidden layers each with 100 and 50 units (with tanh non-linearities) respectively. The encoder into each of the networks takes a bag of words over (objects, locations); that is, there is a separate word in the lookup table for each (object, location) pair. As described above, f takes as input two states; these are combined after the shared encoder layer by concatenation. Action probabilities are output by a linear layer followed by a softmax. In addition, the model also outputs a baseline value using a linear layer, which is trained with mean-square loss to predict the cumulative reward. The parameters of Alice and Bob are not shared.\nTraining used RMSProp (Tieleman & Hinton, 2012) with learning rate of 0.003 and batch size 256. All parameters are randomly initialized from N (0,0.2). We also use an entropy regularization term on the softmax output, set to 0.003. During each training episode, we randomly pick between self-play and target tasks with 80% and 20% probabilities respectively unless otherwise specified. Fig. 3 shows details of a single training run, demonstrating how Alice and Bob automatically build a curriculum between themselves though self-play."}, {"heading": "4.2.1. BIASING FOR OR AGAINST SELF-PLAY", "text": "The effectiveness of our approach depends in part on the similarity between the self-play and target tasks. One way to explore this in our environment is to vary the probability of the light being off initially during self-play episodes1. Note that the light is always off in the target task; if the light is usually on at the start of Alice\u2019s turn in reverse, for example, she will learn to turn it off, and then Bob will be biased to turn it back on. On the other hand, if the light is usually off at the start of Alice\u2019s turn in reverse, Bob is strongly biased against turning the light on, and so the test task becomes especially hard. Thus changing this probability gives us some way to\n1The initial state of the light should dramatically change the behavior of the agent: if it is on then agent can directly proceed to the key.\nadjust the similarity between the two tasks.\nIn Fig. 4, we set p(Light off)=0.5 during self-play and evaluate both reverse and repeat forms of self-play, alongside two baselines: (i) target task only training (i.e. no self-play) and (ii) self-play with a random policy for Alice. We see that the repeat form of self-play succeeds quickly while target task-only training takes much longer2. The reverse form of self-play and random Alice work comparably well, being in between the other two in terms of speed.\nFig. 5 shows what happens when p(Light off)=0.3. Here reverse self-play works well, but repeat self-play does poorly. As discussed above, this flipping, relative to Fig. 4, can be explained as follows: low p(Light off) means that Bob\u2019s task in reverse self-play will typically involve returning the light to the on position (irrespective of how Alice left it), the same function that must be performed in the target task. The opposite situation applies for repeat self-play, where Bob needs to encounter the light typically in the off position to help him with the test task.\nIn Fig. 6 we systematically vary p(Light off) between 0.1 and 0.9. The y-axis shows the speed-up (reduction in target task\n2Training was stopped for all methods except target-only at 5\u00d7106 episodes.\nepisodes) relative to training purely on the target-task for runs where the reward goes above -2. Unsuccessful runs are given a unity speed-up factor. The curves show that when the self-play task not biased against the target task it can help significantly."}, {"heading": "4.3. RLLab: Mountain Car", "text": "We now apply our approach to the Mountain Car task in RLLab. Here the agent controls a car trapped in a 1-D valley. It must learn to build momentum by alternately moving to the left and right, climbing higher up the valley walls until it is able to escape. Although the problem is presented as continuous, we discretize the 1-D action space into 5 bins (uniformly sized) enabling us to use discrete policy gradient, as above. An observation of state st consists of the location and speed of the car.\nAs in (Houthooft et al., 2016; Tang et al., 2016), a reward of +1 is given only when the car succeeds in climbing the hill,\nand episodes are limited to tMax =500 steps. We use the same hyperparameters as the Mazebase experiments, except the batch size is 128 and self-play rewards are scaled by \u03b3=0.01 since episodes are much longer. During training, only 1% of episodes are from the target task. In self-play, an episode is considered completed only when \u2016sb\u2212sa\u2016< 0.2, where sa and sb are the final states of Alice and Bob respectively.\nThe nature of the environment makes it highly asymmetric from Alice and Bob\u2019s point of view, since it is far easier to coast down the hill to the starting point that it is to climb up it. Hence we exclusively use the reset form of self-play. In Fig. 7, we compare this to current state-of-the-art methods, namely VIME (Houthooft et al., 2016) and SimHash (Tang et al., 2016). Our approach (blue) performs comparably to both of these. We also tried using policy gradient directly on the target task samples, but it was unable to solve the problem. In this setting the self-play and target tasks are very similar thus our approach\u2019s success is somewhat expected."}, {"heading": "4.4. RLLab: Swimmer", "text": "Finally, we applied our approach to the SwimmerGather task in RLLab (which uses the Mujoco (Todorov et al., 2012) simulator), where the agent controls a worm with two flexible joints, swimming in a 2D viscous fluid. In the target task, the agent gets reward +1 for eating green apples and -1 for touching red bombs, which are not present during self-play. Thus the self-play task and target tasks are different: in the former, the worm just swims around but in the latter it must learn to swim towards green apples and away from the red bombs.\nThe observation state consists of a 13-dimensional vector describing location and joint angles of the worm, and a 20 dimensional vector for sensing nearby objects. The worm takes two real values as an action, each controlling one joint. We add secondary action head to our models to handle this. As in the mountain car, we discretize the output space (each joint is given 9 uniformly sized bins) to allow the use of discrete policy gradients.\nThe episode length is 500 steps for target tasks as in (Houthooft et al., 2016; Tang et al., 2016), and 600 for self-play. In our experiments we skip two frames with each action, but still count them toward the episode length. The hyperparameters are the same as MountainCar, except the entropy regularization is only applied to the self-play episodes and batch size is 256. Also, the self-play terminates when \u2016lb\u2212la\u2016<0.3 where la and lb are the final locations of Alice and Bob respectively. Target tasks constitute 10% of the training episodes. Fig. 8 shows the target task reward as a function of training iteration for our approach alongside VIME (Houthooft et al., 2016) and SimHash (Tang et al., 2016). Ours can be seen to gain reward earlier than the others and it also converges to a higher final reward than the other two methods. A video of our worm performing the test task can be found at https://goo.gl/Vsd8Js.\nIn Fig. 9 shows details of a single training run. The changes in Alice\u2019s behavior, observed in Fig. 9(c) and (d), correlate with Alice and Bob\u2019s reward (Fig. 9(b)) and, initially at least, to the reward on the test target (Fig. 9(a)). In Fig. 10 we visualize for a single training run the locations where Alice hands over to Bob at different stages of training, showing how the distribution varies."}, {"heading": "5. Discussion", "text": "In this work we described a novel method for intrinsically motivated learning which we call asymmetric self-play. Despite the method\u2019s conceptual simplicity, we have seen that it can be effective in both discrete and continuous input settings with function approximation, for encouraging exploration and automatically generating curriculums. When evaluated on challenging benchmarks, our approach is comparable or superior to current state-of-the-art RL methods that incorporate an incentive for exploration. Furthermore, it is possible show theoretically that in simple environments, using asymmetric self-play with reward functions from (1) and (2), optimal agents can transit\nbetween any pair of reachable states as efficiently as possible.\nHowever, there are limitations in the simple scheme we have described; these suggest avenues for further work:"}, {"heading": "5.1. Meta-exploration for Alice", "text": "We want Alice and Bob to explore the state (or state-action) space, and we would like Bob to be exposed to many different tasks. Because of the form of the standard reinforcement learning objective (expectation over rewards), Alice only wants to find the single hardest thing for Bob, and is not interested in the space of things that are hard for Bob. In the fully tabular setting, with fully reversible dynamics or with resetting, and without the constraints of realistic optimization strategies, we saw in section 2.2 that this ends up forcing Bob and Alice to learn to make any state transition as efficiently as possible. However, with more realistic optimization methods or environments, and with function approximation, Bob and Alice can get stuck in sub-optimal minima.\nFor example, let us follow the argument in the third paragraph of 2.2, and assume that Bob and Alice are at an equilibrium (and that we are in the tabular, finite, Markovian setting), but now we can only update Bob\u2019s and Alice\u2019s policy locally. By this we mean that in our search for a better policy for Bob or Alice, we can only make small perturbations, as in policy gradient algorithms. In this case, we can only guarantee that Bob runs a fast policy on challenges that Alice has non-zero probability of giving; but there is no guarantee that Alice will cover all possible challenges. With function approximation instead of tabular policies, we can not make any guarantees at all.\nAnother example with a similar outcome but different mechanism can occur using the reverse game in an environment\nwithout fully reversible dynamics. In that case, it could be that the shortest expected number of steps to complete a challenge (s0,sT ) is longer than the reverse, and indeed, so much longer that Alice should concentrate all her energy on this challenge to maximize her rewards. Thus there could be equilibria with Bob matching the fast policy only for a subset of challenges even if we allow non-local optimization.\nThe result is that Alice can end up in a policy that is not ideal for our purposes. In figure 10 we show the distributions of where Alice cedes control to Bob in the swimmer task. We can see that Alice has a preferred direction. Ideally, in this environment, Alice would be teaching Bob how to get from any state to any other efficiently; but instead, she is mostly teaching him how to move in one direction.\nOne possible approach to correcting this is to have multiple Alices, regularized so that they do not implement the same policy. More generally, we can investigate objectives for Alice that encourage her to cover a wider distribution of behaviors."}, {"heading": "5.2. Communicating via actions", "text": "In this work we have limited Alice to propose tasks for Bob by doing them. This limitation is practical and effective in restricted environments that allow resetting or are (nearly) reversible. It allows a solution to three of the key difficulties of implementing the basic idea of \u201cAlice proposes tasks, Bob does them\u201d: parameterizing the sampling of tasks, representing and communicating the tasks, and ensuring the appropriate level of difficulty of the tasks. Each of these is interesting in more general contexts. In this work, the tasks have incentivized efficient transitions. One can imagine other reward functions and task representations that incentivize discovering statistics of the states and state-transitions, for example models of their causality or temporal ordering, cluster structure."}], "references": [{"title": "Intrinsic Motivation and Reinforcement Learning, pp. 17\u201347", "author": ["Barto", "Andrew G"], "venue": null, "citeRegEx": "Barto and G.,? \\Q2013\\E", "shortCiteRegEx": "Barto and G.", "year": 2013}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["Bellemare", "Marc G", "Srinivasan", "Sriram", "Ostrovski", "Georg", "Schaul", "Tom", "Saxton", "David", "Munos", "R\u00e9mi"], "venue": "In NIPS,", "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Curriculum learning", "author": ["Bengio", "Yoshua", "Louradour", "J\u00e9r\u00f4me", "Collobert", "Ronan", "Weston", "Jason"], "venue": "In ICML, pp", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Duan", "Yan", "Chen", "Xi", "Houthooft", "Rein", "Schulman", "John", "Abbeel", "Pieter"], "venue": "In ICML,", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Bengio", "Yoshua"], "venue": "In NIPS, pp", "citeRegEx": "Bengio and Yoshua.,? \\Q2014\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2014}, {"title": "Curiosity-driven exploration in deep reinforcement learning via bayesian neural networks", "author": ["Houthooft", "Rein", "Chen", "Xi", "Duan", "Yan", "Schulman", "John", "Turck", "Filip De", "Abbeel", "Pieter"], "venue": null, "citeRegEx": "Houthooft et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Houthooft et al\\.", "year": 2016}, {"title": "Empowerment: a universal agent-centric measure of control", "author": ["Klyubin", "Alexander S", "Polani", "Daniel", "Nehaniv", "Chrystopher L"], "venue": "In Proceedings of the IEEE Congress on Evolutionary Computation,", "citeRegEx": "Klyubin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Klyubin et al\\.", "year": 2005}, {"title": "Selfpaced learning for latent variable models", "author": ["M.P. Kumar", "Packer", "Benjamin", "Koller", "Daphne"], "venue": "In NIPS", "citeRegEx": "Kumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2010}, {"title": "Adversarial learning for neural dialogue generation", "author": ["Li", "Jiwei", "Monroe", "Will", "Shi", "Tianlin", "Ritter", "Alan", "Jurafsky", "Dan"], "venue": "arXiv 1701.06547,", "citeRegEx": "Li et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Exploration in model-based reinforcement learning by empirically estimating learning progress", "author": ["Lopes", "Manuel", "Lang", "Tobias", "Toussaint", "Marc", "Oudeyer", "Pierre-Yves"], "venue": "In NIPS, pp", "citeRegEx": "Lopes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lopes et al\\.", "year": 2012}, {"title": "Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks", "author": ["Mescheder", "Lars M", "Nowozin", "Sebastian", "Geiger", "Andreas"], "venue": "arXiv abs/1701.04722,", "citeRegEx": "Mescheder et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mescheder et al\\.", "year": 2017}, {"title": "Reinforcement learning for robot soccer", "author": ["Riedmiller", "Martin", "Gabel", "Thomas", "Hafner", "Roland", "Lange", "Sascha"], "venue": "Autonomous Robots,", "citeRegEx": "Riedmiller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Riedmiller et al\\.", "year": 2009}, {"title": "Some studies in machine learning using the game of checkers", "author": ["Samuel", "Arthur L"], "venue": "IBM Journal of Research and Development,", "citeRegEx": "Samuel and L.,? \\Q1959\\E", "shortCiteRegEx": "Samuel and L.", "year": 1959}, {"title": "Universal value function approximators", "author": ["Schaul", "Tom", "Horgan", "Dan", "Gregor", "Karol", "Silver", "David"], "venue": "In ICML, pp", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Curious model-building control systems", "author": ["J. Schmidhuber"], "venue": "In Proc. Int. J. Conf. Neural Networks,", "citeRegEx": "Schmidhuber,? \\Q1991\\E", "shortCiteRegEx": "Schmidhuber", "year": 1991}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["Leach", "Madeleine", "Kavukcuoglu", "Koray", "Graepel", "Thore", "Hassabis", "Demis"], "venue": "search. Nature,", "citeRegEx": "Leach et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Leach et al\\.", "year": 2016}, {"title": "Intrinsically motivated reinforcement learning", "author": ["Singh", "Satinder P", "Barto", "Andrew G", "Chentanez", "Nuttapong"], "venue": "In NIPS, pp", "citeRegEx": "Singh et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2004}, {"title": "An analysis of model-based interval estimation for markov decision processes", "author": ["Strehl", "Alexander L", "Littman", "Michael L"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Strehl et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2008}, {"title": "Mazebase: A sandbox for learning from games", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Synnaeve", "Gabriel", "Chintala", "Soumith", "Fergus", "Rob"], "venue": "arXiv 1511.07401,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["Sutton", "Richard S", "Modayil", "Joseph", "Delp", "Michael", "Degris", "Thomas", "Pilarski", "Patrick M", "White", "Adam", "Precup", "Doina"], "venue": "In AAMAS", "citeRegEx": "Sutton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2011}, {"title": "exploration: A study of count-based exploration for deep reinforcement learning", "author": ["H. Tang", "R. Houthooft", "D. Foote", "A. Stooke", "X. Chen", "Y. Duan", "J. Schulman", "F. De Turck", "P. Abbeel"], "venue": "arXiv abs/1611.04717,", "citeRegEx": "Tang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2016}, {"title": "Temporal difference learning and td-gammon", "author": ["Tesauro", "Gerald"], "venue": "Commun. ACM,", "citeRegEx": "Tesauro and Gerald.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro and Gerald.", "year": 1995}, {"title": "Lecture 6.5 - rmsprop, coursera: Neural networks for machine", "author": ["T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Todorov", "Emanuel", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In IROS,", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "In Machine Learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}], "referenceMentions": [{"referenceID": 11, "context": ", 2016), and in in multi-agent games such as RoboSoccer (Riedmiller et al., 2009).", "startOffset": 56, "endOffset": 81}, {"referenceID": 8, "context": "(Li et al., 2017) introduce an adversarial approach to dialogue generation, where a generator model is subjected to a form of \u201cTuring test\u201d by a discriminator network.", "startOffset": 0, "endOffset": 17}, {"referenceID": 10, "context": "(Mescheder et al., 2017) demonstrate how adversarial loss terms can be combined with variational auto-encoders to permit more accurate density modeling.", "startOffset": 0, "endOffset": 24}, {"referenceID": 16, "context": "There is a large body of work on intrinsic motivation (Barto, 2013; Singh et al., 2004; Klyubin et al., 2005; Schmidhuber, 1991) for self-supervised learning agents.", "startOffset": 54, "endOffset": 128}, {"referenceID": 6, "context": "There is a large body of work on intrinsic motivation (Barto, 2013; Singh et al., 2004; Klyubin et al., 2005; Schmidhuber, 1991) for self-supervised learning agents.", "startOffset": 54, "endOffset": 128}, {"referenceID": 14, "context": "There is a large body of work on intrinsic motivation (Barto, 2013; Singh et al., 2004; Klyubin et al., 2005; Schmidhuber, 1991) for self-supervised learning agents.", "startOffset": 54, "endOffset": 128}, {"referenceID": 14, "context": "One line in this direction is curiosity-driven exploration (Schmidhuber, 1991).", "startOffset": 59, "endOffset": 78}, {"referenceID": 1, "context": "These techniques can be applied in encouraging exploration in the context of reinforcement learning, for example (Bellemare et al., 2016; Strehl & Littman, 2008; Lopes et al., 2012; Tang et al., 2016); Roughly, these use some notion of the novelty of a state to give a reward.", "startOffset": 113, "endOffset": 200}, {"referenceID": 9, "context": "These techniques can be applied in encouraging exploration in the context of reinforcement learning, for example (Bellemare et al., 2016; Strehl & Littman, 2008; Lopes et al., 2012; Tang et al., 2016); Roughly, these use some notion of the novelty of a state to give a reward.", "startOffset": 113, "endOffset": 200}, {"referenceID": 20, "context": "These techniques can be applied in encouraging exploration in the context of reinforcement learning, for example (Bellemare et al., 2016; Strehl & Littman, 2008; Lopes et al., 2012; Tang et al., 2016); Roughly, these use some notion of the novelty of a state to give a reward.", "startOffset": 113, "endOffset": 200}, {"referenceID": 6, "context": "Another line of work on intrinsic motivation is a formalization of the notion of empowerment (Klyubin et al., 2005), or how much control the agent has over its environment.", "startOffset": 93, "endOffset": 115}, {"referenceID": 2, "context": "Curriculum learning (Bengio et al., 2009) is widely used in many machine learning approaches.", "startOffset": 20, "endOffset": 41}, {"referenceID": 7, "context": "Previous automatic approaches, such as (Kumar et al., 2010), rely on monitoring training error.", "startOffset": 39, "endOffset": 59}, {"referenceID": 19, "context": "Our basic paradigm of \u201cAlice proposing a task, and Bob doing it\u201d is related to the Horde architecture (Sutton et al., 2011) and (Schaul et al.", "startOffset": 102, "endOffset": 123}, {"referenceID": 13, "context": ", 2011) and (Schaul et al., 2015).", "startOffset": 12, "endOffset": 33}, {"referenceID": 18, "context": "The following experiments explore our self-play approach on a variety of tasks, both continuous and discrete, from the Mazebase (Sukhbaatar et al., 2015) and RLLab (Duan et al.", "startOffset": 128, "endOffset": 153}, {"referenceID": 3, "context": ", 2015) and RLLab (Duan et al., 2016) environments.", "startOffset": 18, "endOffset": 37}, {"referenceID": 18, "context": "We now describe experiments using the MazeBase environment (Sukhbaatar et al., 2015).", "startOffset": 59, "endOffset": 84}, {"referenceID": 5, "context": "As in (Houthooft et al., 2016; Tang et al., 2016), a reward of +1 is given only when the car succeeds in climbing the hill, 0.", "startOffset": 6, "endOffset": 49}, {"referenceID": 20, "context": "As in (Houthooft et al., 2016; Tang et al., 2016), a reward of +1 is given only when the car succeeds in climbing the hill, 0.", "startOffset": 6, "endOffset": 49}, {"referenceID": 5, "context": "7, we compare this to current state-of-the-art methods, namely VIME (Houthooft et al., 2016) and SimHash (Tang et al.", "startOffset": 68, "endOffset": 92}, {"referenceID": 20, "context": ", 2016) and SimHash (Tang et al., 2016).", "startOffset": 20, "endOffset": 39}, {"referenceID": 23, "context": "Finally, we applied our approach to the SwimmerGather task in RLLab (which uses the Mujoco (Todorov et al., 2012) simulator), where the agent controls a worm with two flexible joints, swimming in a 2D viscous fluid.", "startOffset": 91, "endOffset": 113}, {"referenceID": 5, "context": "A comparison of our self-play approach on MountainCar task with VIME (Houthooft et al., 2016) and SimHash (Tang et al.", "startOffset": 69, "endOffset": 93}, {"referenceID": 20, "context": ", 2016) and SimHash (Tang et al., 2016) (figure adapted from (Tang et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 20, "context": ", 2016) (figure adapted from (Tang et al., 2016)).", "startOffset": 29, "endOffset": 48}, {"referenceID": 5, "context": "The episode length is 500 steps for target tasks as in (Houthooft et al., 2016; Tang et al., 2016), and 600 for self-play.", "startOffset": 55, "endOffset": 98}, {"referenceID": 20, "context": "The episode length is 500 steps for target tasks as in (Houthooft et al., 2016; Tang et al., 2016), and 600 for self-play.", "startOffset": 55, "endOffset": 98}, {"referenceID": 5, "context": "8 shows the target task reward as a function of training iteration for our approach alongside VIME (Houthooft et al., 2016) and SimHash (Tang et al.", "startOffset": 99, "endOffset": 123}, {"referenceID": 20, "context": ", 2016) and SimHash (Tang et al., 2016).", "startOffset": 20, "endOffset": 39}, {"referenceID": 5, "context": "Evaluation on SwimmerGather target task, comparing to VIME (Houthooft et al., 2016) and SimHash (Tang et al.", "startOffset": 59, "endOffset": 83}, {"referenceID": 20, "context": ", 2016) and SimHash (Tang et al., 2016) (figure adapted from (Tang et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 20, "context": ", 2016) (figure adapted from (Tang et al., 2016)).", "startOffset": 29, "endOffset": 48}], "year": 2017, "abstractText": "We describe a simple scheme that allows an agent to explore its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on (nearly) reversible environments, or environments that can be reset, and Alice will \u201cpropose\u201d the task by running a set of actions and then Bob must partially undo, or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When deployed on an RL task within the environment, this unsupervised training reduces the number of episodes needed to learn.", "creator": "LaTeX with hyperref package"}}}