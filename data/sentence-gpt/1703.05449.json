{"id": "1703.05449", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2017", "title": "Minimax Regret Bounds for Reinforcement Learning", "abstract": "We consider the problem of efficient exploration in finite horizon MDPs.We show that an optimistic modification to model-based value iteration, can achieve a regret bound $\\tilde{O}( \\sqrt{HSAT} + H^2S^2A+H\\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states, $A$ the number of actions and $T$ the time elapsed. This result improves over the best previous known bound $\\tilde{O}(HS \\sqrt{AT})$ achieved by the UCRL2 algorithm.The key significance of our new results is that when $T\\geq H^3S^3A$ and $SA\\geq H$, it leads to a regret of $\\tilde{O}(\\sqrt{HSAT})$ that matches the established lower bounds of $\\Omega(\\sqrt{HSAT})$ up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in $S$), and we use \"exploration bonuses\" based on Bernstein's inequality, together with using a recursive -Bellman-type- Law of Total Variance (to improve scaling in $H$).\n\n\n\n\n\nWe use a procedure that can be used to calculate our expected values in exponential time, or the total uncertainty in our estimates of the expected value function. For simplicity, we use a method called the time-varying operation to evaluate our estimates in exponential time. In this case, we use the algorithm to estimate the amount of time left for estimating the average number of actions and actions. When we calculate the estimated amount of time left for predicting the expected value function, the method is based on the inverse variance of the value function.\nThe function is called on the inverse variance of the value function. In the case of the following equation, for example, a function, for example, would not be expected to have a higher value function, but its estimate function is based on the inverse variance of the value function. In this case, for example, a function, for example, would not be expected to have a higher value function, but its estimate function is based on the inverse variance of the value function. The algorithm is not required to be optimized for this problem, because it also introduces some optimization artifacts that are", "histories": [["v1", "Thu, 16 Mar 2017 01:31:33 GMT  (33kb)", "http://arxiv.org/abs/1703.05449v1", null], ["v2", "Sat, 1 Jul 2017 13:00:06 GMT  (30kb)", "http://arxiv.org/abs/1703.05449v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["mohammad gheshlaghi azar", "ian osband", "r\u00e9mi munos"], "accepted": true, "id": "1703.05449"}, "pdf": {"name": "1703.05449.pdf", "metadata": {"source": "CRF", "title": "Minimax Regret Bounds for Reinforcement Learning", "authors": ["Mohammad Gheshlaghi Azar", "Ian Osband"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n05 44\n9v 1\n[ st\nat .M\nL ]\n1 6\nM ar\nWe consider the problem of efficient exploration in finite horizon MDPs. We show that an optimistic modification to model-\nbased value iteration, can achieve a regret bound O\u0303( \u221a HSAT+H2S2A+H \u221a T )whereH is the time horizon, S the number of states, A the number of actions and T the time elapsed. This result improves over the best previous known bound O\u0303(HS \u221a AT ) achieved by the UCRL2 algorithm Jaksch et al. (2010a). The key significance of our new results is that when T \u2265 H3S3A and SA \u2265 H , it leads to a regret of O\u0303( \u221a HSAT ) that matches the established lower bounds of \u2126( \u221a HSAT ) up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in S), and we use \"exploration bonuses\" based on Bernstein\u2019s inequality, together with using a recursive -Bellman-type- Law of Total Variance (to improve scaling in H)."}, {"heading": "1 Introduction", "text": "We consider the reinforcement learning (RL) problem of an agent interacting with an environment in order to maximize its cumulative rewards through time (Burnetas and Katehakis, 1997; Sutton and Barto, 1998). We model the environment as a Markov decision process (MDP), but the agent is uncertain of the true dynamics of the MDP. As the agent interacts with the environment it observes the states, actions and rewards generated by the system dynamics. This leads to a fundamental trade off: should the agent explore poorly-understood states and actions in order to improve future performance, or exploit its existing knowledge to better optimize short-run rewards.\nThe most common approach to this learning problem is to separate the process of estimation and optimization. In this paradigm, point estimates of the unknown quantities are used in place of the unknown parameters and a plan is made with respect to these estimates. Naive optimization with respect to these point estimates can lead to premature and sub-optimal exploitation and so may never learn the optimal policy. Dithering approaches to exploration (such as \u01eb-greedy) address this failing through additional random action selection. However, as this exploration is not directed the resultant algorithms may take exponentially long to learn (Kearns and Singh, 2002).\nIn order to learn efficiently it is necessary that the agent prioritizes potentially informative states and actions. To do this, it is important that the agent maintains some notion of its own uncertainty. In some sense, given any prior belief, the optimal solution to this exploration/exploitation dilemma is given by the dynamic programming in the extended Bayesian belief state (Bertsekas, 2007). However, the computational demands of this method become intractable for even small problems (Guez et al., 2013) while finite approximations can be arbitrarily poor (Munos, 2014).\nTo combat these failings, the majority of provably efficient learning algorithms employ a heuristic principle known as optimism in the face of uncertainty (OFU). In these algorithms, each state and action is afforded some \u201coptimism\u201d such that its imagined value is as high as statistically plausible. The agent then chooses a policy under this optimistic view of the world. This allows for efficient exploration since poorly-understood states and actions are afforded higher optimistic bonus. As the agent resolves its uncertainty, the effects of optimism will reduce and the agent\u2019s policy will approach optimality. Almost all reinforcement learning algorithms with polynomial bounds on sample complexity employ optimism to guide exploration (Kearns and Singh, 2002; Brafman and Tennenholtz, 2002; Strehl et al., 2006; Jaksch et al., 2010a).\nAn alternative principle motivated by the Thompson sampling heuristic (Thompson, 1933) has emerged as a practical competitor to optimism. The algorithm posterior sampling for reinforcement learning (PSRL) maintains a posterior distribution for MDPs and, at each episode of interaction, follows a policy which is optimal for a single random sample (Strens, 2000).\nRecent work has established Bayesian regret bounds for PSRL and even argues for the potential benefits of sampling-based methods over existing optimistic approaches (Osband et al., 2013; Osband and Van Roy, 2016b). In this paper we incorporate several of these insights and \u201cfire back\u201d to defend the optimistic principle for tabular reinforcement learning with a new and improved algorithm.\nWe present a conceptually simple and computationally efficient approach to optimistic reinforcement learning in MDPs with finite episode length and without generalization. Our algorithm, upper confidence bound value iteration (UCBVI) is similar to model-based interval estimation (MBIE-EB) (Strehl and Littman, 2005) with a delicate alteration to the form of the \u201cexploration bonus\u201d. In particular UCBVI replaces the constant \u03b2 in MBIE-EB, which scales the bonus, with the empirical variance of the value function of each state-action pair. Importantly, this algorithm addresses some of the shortcomings of existing approaches to optimistic exploration (Jaksch et al., 2010a; Dann and Brunskill, 2015) that have been highlighted relative to PSRL (Osband and Van Roy, 2016b).\nOur key contribution is to establish a high probability regret bound O\u0303( \u221a HSAT +H2S2A+H \u221a T ) where S is the number\nof states, A is the number of actions,H is the episode length and T is the time elapsed and where O\u0303 ignores logarithmic terms. Importantly, for T > H3S3A and SA \u2265 H this bound is O\u0303( \u221a HSAT ), which matches the lower bounds for this problem up to logarithmic factors (Jaksch et al., 2010a). This positive result is the first of its kind and helps to address an ongoing question about where the fundamental lower bounds lie for reinforcement learning in finite horizon MDPs (Bartlett and Tewari, 2009; Dann and Brunskill, 2015; Osband and Van Roy, 2016a). Our analysis contains two key insights that may be useful for future work in this area.\n1. Careful application of the Bernstein and Freedman inequalities (Bernstein, 1927; Freedman, 1975) to the concentration\nof the optimal value function directly, rather than building confidence sets for the transitions probabilities and rewards, like in UCRL2 (Jaksch et al., 2010a) and UCFH (Dann and Brunskill, 2015). 2. The use of exploration bonuses based on Bernstein\u2019s inequality, and a recursive Bellman-type Law of Total Variance\n(LTV) to prove tight bounds on the expected sum of the variances of the value estimates, in a similar spirit to the analysis from Azar et al. (2013); Lattimore and Hutter (2012).\nAt a high level, this work addresses the noted shortcomings of existing algorithms for optimistic RL (Osband and Van Roy, 2016b) and demonstrates (contrary to previous assertions) that it is possible to design a simple and computationally efficient optimistic algorithm that does not suffer from these flaws when T is sufficiently large. We use these insights to alter the algorithm to simultaneously address both the loose scaling in S and the loose scaling inH to obtain the first regret bounds that match the \u2126( \u221a HSAT ) lower bounds as T becomes large.\nWe should be careful to mention the current limitations of our work, each of which may provide fruitful ground for future research. First, we study the setting of episodic, finite horizon MDPs and not the more general setting of weakly communicating systems (Bartlett and Tewari, 2009; Jaksch et al., 2010a). Further, our bounds only improve over previous scalings\nO\u0303(HS \u221a AT ) for every T > H3S3A (see Jaksch et al., 2010a, for the derivations).\nWe hope that this work will serve to elucidate several of the existing shortcomings of efficient exploration in the tabular\nsetting and, hopefully, help further the direction of research towards provably optimal exploration in reinforcement learning."}, {"heading": "2 Problem formulation", "text": "In this section, we briefly review some notation, as well as some standard concepts and definitions from the theory of Markov decision processes (MDPs).\nMarkovDecision ProblemsWe consider the problem of undiscounted episodic reinforcement learning (RL) (Bertsekas and Tsitsiklis,\n1996), where an RL agent interacts with a stochastic environment and this interaction is modeled as a discrete-time MDP. An MDP is a quintuple \u3008S,A, P,R,H\u3009, where S and A are the set of states and actions, P is the state transition distribution, The function R : S \u00d7 A \u2192 \u211c is a real-valued function on the state-action space and the horizon H is the length of episode. We denote by P (\u00b7|x, a) and R(x, a) the probability distribution over the next state and the immediate reward of taking action a at state x, respectively. The agent interacts with the environment in a sequence of episodes. The interaction between the agent and environment at every episode1 k \u2208 [K] is as follows: starting from xk,1 \u2208 S which is chosen by the environment, the agent interacts with environment forH steps by following a sequence of actions chosen in A and observes a sequence of next-states and rewards until the end of episode. The initial state xk,1 may change arbitrary from one episode to another one. We also use the notation \u2016 \u00b7 \u20161 for the \u21131 norm throughout this paper.\n1we write [n] for {i \u2208 N | 1 \u2264 i \u2264 n}.\nAssumption 1 (MDP Regularity). We assume S andA are finite sets with cardinalities S, A, respectively. We also assume that the immediate reward R(x, a) is deterministic and belongs to the interval [0, 1].2\nIn this paper we focus on the setting where the reward function R is known, but extending our algorithm to unknown stochastic rewards poses no real difficulty.\nThe policy during an episode is expressed as a mappings \u03c0 : S \u00d7 [H ] \u2192 A. The value V \u03c0h : S \u2192 R denotes the value function at every step h = 1, 2, . . . , H and state x \u2208 S such that V \u03c0h (x) corresponds to the expected sum of H \u2212 h rewards received under policy \u03c0, starting from xh = x \u2208 S. Under assumption 1 there exists always a policy \u03c0\u2217 which attains the best possible values, V \u2217h (x) def = sup\u03c0 V \u03c0 h (x) \u2200x \u2208 S and h \u2265 1. The function V \u2217h is called the optimal value function. The policy \u03c0 at every step h defines the state transition kernel P \u03c0h and the reward function r \u03c0 h as P \u03c0 h (y|s) def = P (y|x, \u03c0(x, h)) and r\u03c0h(x) def = R(x, \u03c0(x, h)) for all x \u2208 S. For every V : S \u2192 R the right-linear operators P \u00b7 and P \u03c0h \u00b7 are also defined as (PV )(x, a) def = \u2211 y\u2208SP (y|x, a)V (y) for all (x, a) \u2208 S \u00d7A and (P \u03c0h V )(x) def = \u2211 y\u2208S P \u03c0 h (y|s)V (y) for all x \u2208 S, respectively.\nThe Bellman operator for the policy \u03c0, at every step h > 0, is defined, for all x \u2208 S, as\n(T \u03c0h V )(x) def = r\u03c0h(x) + (P \u03c0 h V )(x).\nWe also define the state-action Bellman operator as follows: \u2200(x, a) \u2208 S \u00d7A,\n(T V )(x, a) def= R(x, a) + (PV )(x, a), and the optimality Bellman operator: \u2200(x, a) \u2208 S \u00d7A,\n(T \u2217V )(x) def= max a\u2208A (T V )(x, a).\nFor ease of exposition, we remove the dependence on x and (x, a), e.g., writing PV for (PV )(x, a) and V for V (x), when there is no possible confusion.\nWe measure the performance of the learner over T = KH steps by the regret Regret(K), defined as\nRegret(K) def =\nK\u2211\nk=1\nV \u22171 (xk,1)\u2212 V \u03c0k1 (xk,1),\nwhere \u03c0k is the control policy followed by the learner at episode k. Thus the regret measures the expected loss of following the policy produced by the learner instead of the optimal policy. So the goal of learner is to follow a sequence \u03c01, \u03c02, . . . , \u03c0K such that Regret(K) is as small as possible."}, {"heading": "3 Upper confidence bound value iteration", "text": "In this section we introduce the two variants of the algorithm that we investigate in this paper. We call the algorithm upper confidence bound value iteration (UCBVI). UCBVI is an extension of value iteration which guarantee than the resultant value function is a (high-probability) upper confidence bound (UCB) on the optimal value function. This algorithm is very related to the model based interval estimation (MBIE-EB) algorithms (Strehl and Littman, 2005). Our key contribution is the precise design of the upper confidence sets, and the analysis which lead to tight regret bounds.\nAlgorithm 1 UCB-VI\nInitialize dataH = \u2205 for episode k = 1, 2, .. do\nQk,h = UCB\u2212 Q\u2212 values(H) for step h = 1, .., H do\nTake action ak,h = argmaxa Qk,h(xk,h, a) UpdateH = H \u222a (xk,h, ak,h, xk,h+1)\nend for\nend for\n2Our results also hold if the rewards are taken from some interval [Rmin, Rmax] instead of [0, 1], in which case the bounds scale with the factor Rmax \u2212 Rmin.\nAlgorithm 2 UCB-Q-values Require: Bonus algorithm gen_bonus, DataH Compute, for all (x,a,y)\u2208S\u00d7A\u00d7S, Nk(x,a,y)= \u2211 (x\u2032,a\u2032,y\u2032)\u2208H1{x\u2032=x,a\u2032=a,y\u2032=y}\nNk(x,a)= \u2211\ny\u2208SNk(x,a,y) N \u2032k,h(x,a)= \u2211 (xi,h,ai,h,xi,h+1)\u2208H1{xi,h=x,ai,h=a} Let K={(x,a)\u2208S \u00d7A, Nk(x,a)>0} Estimate P\u0302k(y|x,a)= Nk(x,a,y)Nk(x,a) for all (x,a)\u2208K Initialize Qk,H+1(x,a)=0 for all (x,a)\u2208S\u00d7A for h=H,H\u22121,..,1 do\nfor (x,a)\u2208S\u00d7A do if (x,a)\u2208K then\nbk,h(x,a)=bonus(P\u0302k(x,a),Vk,h+1,Nk,N \u2032 k,h)\nQk,h(x,a)=min ( Qk\u22121,h(x,a),H,\nR(x,a)+(P\u0302kVk,h+1)(x,a)+bk,h(x,a) )\nelse\nQk,h(x,a)=H end if\nend for\nend for return Q-valuesQk,h\nUCB-VI described in Algorithm 1 calls UCB-Q-values (Algorithm 2) which returns UCBs on the Q-values computed by value iteration using an empirical Bellman operator to which is added a confidence bonus bonus. We consider two variants of UCBVI depending on the structure of bonus, which we present in Algorithms 3 and 4.\nThe first of these, UCBVI_1 considers UCBVI with bonus = bonus_1. bonus_1 is a very simple bound based upon Chernoff-Hoeffding\u2019s concentration inequality for values bounded in [0, H ]. We will see in Theorem 1 that this very simple algorithm can already achieve a regret bound of O\u0303(H \u221a SAT ), thus improving the best previously known regret bounds from\na S to a \u221a S dependence. The intuition for this improved S-dependence is that our algorithm (as well as our analysis) does not consider confidence sets on the transition dynamics P (y|x, a) like UCRL2 and UCFH do, but instead directly maintains confidence intervals on the optimal value function. This is crucial as, for any given (x, a), the transition dynamics are Sdimensional whereas the Q-value function is one-dimensional.\nAlgorithm 3 bonus_1\nRequire: P\u0302k(x, a), Nk(x, a)\nb(x, a) = 7HL \u221a\n1 Nk(x,a) where L = log(5SAT/\u03b4),\nreturn b\nHowever, the loose form of UCB given by UCBVI_1 does not look at the value function of the next state, and just consider it as being bounded in [0, H ]. However, much better bounds can be obtained by looking at the variance of the next state values. Our main result relies upon UCBVI with bonus = bonus_2, which we refer to as UCBVI_2 . UCBVI_2 builds upon the intuition for UCBVI_1 , but also incorporates a variance-dependent exploration bonus. This leads to tighter exploration\nbonuses and an improved regret bound of O\u0303( \u221a HSAT ).\nThe intuition for this is relatively simple. If the agent had knowledge of the true value V \u2217, we could use the variance (over one transition) of the value function V \u2217 in place of the loose bound ofH . Of course, as the agent learns it does not have access to the true value. Instead, in UCBVI_2 , we use a bonus based upon the empirical variance of the estimated next value, given the data gathered so far and with some additional bonus to account for the estimation error we ensure that we have an upper bound on the variance of V \u2217.\nAlgorithm 4 bonus_2 Require: P\u0302k(x, a), Vk,h+1, Nk, N \u2032 k,h\nb(x,a)=\n\u221a 8LVarY\u223cP\u0302k(\u00b7|x,a)(Vk,h+1(Y ))\nNk(x,a) +\n14L\n3Nk(x,a) +\n\u221a\u221a\u221a\u221a8 \u2211 yP\u0302k(y|x,a) [ min ( 652H3S2AL2\nN \u2032 k,h+1\n(y) ,H 2 )]\nNk(x,a) ,\nwhere L=log(5SAT/\u03b4), return b\nAs more data is gathered, this variance estimate will converge to the variance of V \u2217 and the effects of the bonus are subdominant. Now, using an iterative -Bellman-type- Law of Total Variance, we have that the sum of the next-state variances of V \u2217 (overH time steps) (which is related to the sum of the exploration bonuses overH steps) is bounded by the variance of the H-steps return. Thus the size of the bonuses built by UCBVI_2 are constrained over the H steps. And we prove that the sum of those bonuses do not grow linearly in H but in \u221a H only. This is the key for our improved dependence fromH to \u221a H ."}, {"heading": "4 Main results", "text": "In this section we present the main results of the paper, which are upper bounds on the regret of UCBVI_1 and UCBVI_2 algorithms. We assume Assumption 1 holds.\nTheorem 1 (Regret bound for UCBVI_1 ). Consider a parameter \u03b4 > 0. Then the regret of UCBVI_1 is bounded w.p. at least 1\u2212 \u03b4, by\nRegret(T ) \u2264 20HL \u221a SAT + 100H2S2AL2, (1)\nwhere L = log(5HSAT/\u03b4).\nTheorem 1 is significant in that, for large T , it improves the regret dependence from S to \u221a S, compared to the best known bound of Jaksch et al. (2010b). The main intuition for this improved S-dependence is that we bound the estimation error of the next-state value function directly, instead of the transition probabilities. More precisely, instead of bounding the estimation error (P\u0302 \u03c0kk \u2212P \u03c0k)Vk,h+1 by \u2016P\u0302 \u03c0kk \u2212P \u03c0k\u20161\u2016Vk,h+1\u2016\u221e (as is done in Jaksch et al. (2010b) for example), we bound (P\u0302 \u03c0kk \u2212 P \u03c0k)V \u2217h+1 instead (for which a bound with no dependence on S can be achieved since V \u2217 is deterministic) and handle carefully the correction term (P\u0302 \u03c0kk \u2212 P \u03c0k)(Vk,h+1 \u2212 V \u2217h+1).\nOur second result, Theorem 2, demonstrates that we can improve upon the H-dependence by using a more refined, Bernstein-type, exploration bonus.\nTheorem 2 (Regret bound for UCBVI_2 ). Consider a parameter \u03b4 > 0. Then the regret of UCBVI_2 is bounded w.p. 1\u2212 \u03b4, by\nRegret(T ) \u2264 28L \u221a HSAT + 1225H2S2AL2 + 4H \u221a TL, (2)\nwhere L = log(5HSAT/\u03b4).\nThis result is particularly significant since, for T large enough (i.e., T \u2265 H3S3A), our bound is O\u0303( \u221a HSAT ) which\nmatches the established lower bound \u2126( \u221a HSAT ) of (Jaksch et al., 2010a; Osband and Van Roy, 2016a) up to a logarithmic factor.\nThe key insight is to apply concentration inequalities to bound the estimation errors and the exploration bonuses in terms of the variance of V \u2217 at the next state. We then use the fact that the sum of these variances is bounded by the variance of the return (in a spirit similar to (Munos and Moore, 1999; Azar et al., 2013; Lattimore and Hutter, 2012)), which proves that the\nestimation errors accumulate as \u221a H instead of linearly inH , thus implying the improvedH-dependence."}, {"heading": "4.1 Computational efficiency", "text": "The results of Theorems 1 and 2 provide performance guarantees on the statistical efficiency of the respective UCBVI algorithms. Importantly, both UCBVI_1 and UCBVI_2 result in computationally tractable algorithms. Each episode, both algorithms perform an optimistic value iteration at computational cost on the same order as solving a known MDP. In fact the computational cost of this algorithm is of the same order as that of standard model-based value iteration, which is of O\u0303(HSAmin(T, S)) at every update (Kearns and Singh, 1999). This amounts to the total computational cost of O\u0303(SAT min(T, S))\nas we updateK = T/H times after T steps. In fact it is possible to match the O\u0303-scalings of both Theorems 1 and 2 with variants of UCBVI that only selectively recompute optimistic value functions after sufficiently many extra visits to any state and action. This technique is common to the literature involves extending the period of fixed optimistic estimates as more data is gathered Jaksch et al. (2010b); Dann and Brunskill (2015). The computational cost of this variant of UCBVI then amounts to O\u0303(S2A2 min(T, S)) as it only needs to update the model O\u0303(SA) times (Jaksch et al., 2010a). We can use similar techniques to further improve the computational complexity of both UCBVI_1 and UCBVI_2 but omit these modifications for clarity in an (already complicated) analysis.\nIn comparison with UCRL2 both variants of our algorithms are up to S-times less computationally expensive than the previous state of the art , despite the improvement in regret scaling for large T . The reason for this improved computational efficiency comes from the structure of UCBVI. Since both algorithms design confidence sets upon the optimal value function directly, rather than the underlying estimates for rewards and transitions, they avoid the need for the computationally intensive extended value iteration Jaksch et al. (2010b)."}, {"heading": "5 Proof sketch", "text": "Here provide the sketch proof of our results. The full proof is deferred to the appendix."}, {"heading": "5.1 Sketch Proof of Theorem 1", "text": "Let\u2126 = {Vk,h \u2265 V \u2217h , \u2200k, h} be the event under which all computed Vk,h values are upper bounds on the optimal value function. Using backward induction on h (and standard concentration inequalities) we can prove that \u2126 holds with high probability. To simplify notations in this sketch of proof we will not make explicit the numerical constant, denoting by a numerical constant which can vary from line to line. The exact values of these constants are provided in the full proof. We will also make use of simplified notations, such as using L to represent the logarithmic term L = log( SAT/\u03b4).\nThe cumulative regret at time T is Regret(k) def = \u2211 k V \u2217(xk,1) \u2212 V \u03c0k(xk,1). Define R\u0303egret(k) def= \u2211\nk Vk,h(xk,1) \u2212 V \u03c0k(xk,1). Under \u2126 we have Regret(k) \u2264 R\u0303egret(k), so we now bound R\u0303egret(k). Define \u2206k,h def= V \u2217 \u2212 V \u03c0k and \u2206\u0303k,h def= Vk,h \u2212 V \u03c0k . Thus\n\u2206k,h \u2264 \u2206\u0303k,h = P\u0302 \u03c0kk Vk,h+1 + bk,h \u2212 P \u03c0kV \u03c0kh+1 = (P\u0302 \u03c0kk \u2212 P \u03c0k)Vk,h+1 + P \u03c0k\u2206\u0303k,h+1 + bk,h.\nThe difficulty in bounding (P\u0302 \u03c0kk \u2212 P \u03c0k)Vk,h+1 is that both Vk,h+1 and P\u0302 \u03c0kk are random vectors and are not independent (the value function Vk,h+1 computed at h + 1 may depend on the samples collected from state xh,k), thus a straightforward application of Chernoff-Hoeffding (CH) inequality does not work here. In Jaksch et al. (2010b), this issue is addressed by\nbounding it as \u2016P\u0302 \u03c0kk \u2212 P \u03c0k\u20161\u2016Vk,h+1\u2016\u221e at the price of an additional \u221a S.\nThe main contribution of our O\u0303(H \u221a SAT ) bound (which removes a \u221a S factor compared to the previous bound of Jaksch et al. (2010b)) is to handle this term more properly. Instead of directly bounding (P\u0302 \u03c0kk \u2212 P \u03c0k)Vk,h+1, we bound (P\u0302 \u03c0kk \u2212 P \u03c0k)V \u2217h+1, using straightforward application of CH (which removes the \u221a S factor since V \u2217h+1 is deterministic), and deal with the correction term (P\u0302 \u03c0kk \u2212 P \u03c0k)(Vk,h+1 \u2212 V \u2217h+1). We have\n\u2206\u0303k,h = (P\u0302 \u03c0k k \u2212 P \u03c0k)(Vk,h+1 \u2212 V \u2217h+1) + P \u03c0k\u2206\u0303k,h+1 + bk,h + ek,h,\nwhere ek,h def = (P\u0302 \u03c0kk \u2212 P \u03c0k)V \u2217h+1(xk,h) is the estimation error of the optimal value function at the next state. Defining \u03b4\u0303k,h def = \u2206\u0303k,h(xk,h), we have\n\u03b4\u0303k,h \u2264 (P\u0302 \u03c0kk \u2212 P \u03c0k)\u2206k,h+1(xk,h)+\u03b4\u0303k,h+1+\u01ebk,h+bk,h+ek,h,\nwhere \u01ebk,h def = P \u03c0k\u2206k,h+1(xk,h)\u2212\u2206k,h+1(xk,h+1).\nStep 1: bound on the correction term (P\u0302 \u03c0kk \u2212 P \u03c0k)\u2206k,h+1(xk,h). Using Bernstein\u2019s inequality (B), this term is bounded by\n\u2211\ny\n(P\u0302 \u03c0k(y|xk,h)\u2212 P \u03c0k(y|xk,h))\u2206k,h+1(y) (B) \u2264 \u2211\ny\nP \u03c0k(y|xk,h) \u221a\nL\nP \u03c0k(y|xk,h)nk,h \u2206k,h+1(y) +\nSHL nk,h ,\nwhere nk,h def = Nk(xk,h, \u03c0k(xk,h)). Now considering only the y such thatP \u03c0k(y|xk,h)nk,h \u2265 H2L, and since 0 \u2264 \u2206k,h+1 \u2264 \u2206\u0303k,h+1, then (P\u0302 \u03c0k k \u2212 P \u03c0k)\u2206k,h+1(xk,h) is bounded by\n\u01eb\u0304k,h +\n\u221a L\nP \u03c0k(xk,h+1|xk,h)nk,h \u03b4\u0303k,h+1 +\nSHL nk,h \u2264\u01eb\u0304k,h + 1 H \u03b4\u0303k,h+1 + SHL nk,h ,\nwhere \u01eb\u0304k,h def = \u221a L nk,h (\u2211 y P \u03c0k(y|xk,h) \u2206\u0303k,h+1(y)\u221a P\u03c0k (y|xk,h) \u2212 \u03b4\u0303k,h+1\u221a P\u03c0k (xk,h+1|xk,h) ) .\nThe sum over the neglected y such that P \u03c0k(y|xk,h)nk,h < H2L contributes to an additional term\n\u2211\ny\n\u221a P \u03c0k(y|xk,h)nk,hL\nn2k,h \u2206k,h+1(y) \u2264 SH2L/nk,h.\nNeglecting this term (and the smaller order term SHL/nk,h) for now (by the pigeon-hole principle we can prove that these terms contribute to the final regret by a constant at most S2AH2L2), we have\n\u03b4\u0303k,h \u2264 ( 1 + 1\nH\n) \u03b4\u0303k,h+1 + \u01ebk,h + \u01eb\u0304k,h + bk,h + ek,h\n\u2264 ( 1 + 1\nH\n)H\n\ufe38 \ufe37\ufe37 \ufe38 \u2264e\nH\u22121\u2211\ni=h\n( \u01ebk,i + \u01eb\u0304k,i + bk,i + ck,i ) .\nThe regret is thus bounded by\nR\u0303egret(k) \u2264 \u2211\nk,h\n(\u01ebk,h + \u01eb\u0304k,h + bk,h + ek,h). (3)\nWe now bound those 4 terms. It is easy to check that \u2211 k,h \u01ebk,h and \u2211 k,h \u01eb\u0304k,h are sums of martingale differences, which are bounded using Azuma\u2019s inequality, and lead to a regret of O\u0303(H \u221a T ) without dependence on the size of state and action space. The leading terms in the regret bound comes from the sum of the exploration bonuses \u2211\nk,h bk,h and the estimation errors ek,h.\nStep 2: Bounding the martingales \u2211 k,h \u01ebk,h and \u2211 k,h \u01eb\u0304k,h. Using Azuma\u2019s inequality we deduce\n\u2211\nk,h\n\u01ebk,h (Az) \u2264 H \u221a TL, and\n\u2211\nk,h\n\u01eb\u0304k,h (Az) \u2264 \u221a TL. (4)\nStep 3: Bounding the exploration bonuses \u2211\nk,h bk,h: Using the pigeon-hole principle, we have\n\u2211\nk,h\nbk,h = HL \u2211\nk,h\n\u221a 1\nnk,h\n= HL \u2211\nx,a\nNT (x,a)\u2211\nn=1\n\u221a 1\nn\n\u2264 HL \u221a SAT . (5)\nStep 4: Bounding on the estimation errors \u2211\nk,h ek,h. Using CH, w.h.p. we have ek,h = (P\u0302 \u03c0k k \u2212 P \u03c0k)V \u2217h+1\n(CH)\n\u2264 H \u221a L\nnk,h . Thus this bound on the estimation errors are of the same order as the exploration bonuses (which is the reason we\nchoose those bonuses...).\nPutting everything together: Plugging (4) and (5) into (3) (and adding the smaller order term) we deduce\nRegret(k) \u2264 R\u0303egret(k) \u2264 ( HL \u221a SAT +H2S2AL2 ) ."}, {"heading": "5.2 Sketch Proof of Theorem 2", "text": "The proof of Theorem 1 relied on proving by a straightforward induction over h that \u2126 = {Vk,h \u2265 V \u2217h , \u2200k, h} hold with high probability. In the case of exploration defined by the exploration bonuses:\nbk,h(x, a) =\n\u221a LVY\u223cP\u0302\u03c0k\nk (\u00b7|x,a)\n( Vk,h+1(Y ) )\nNk(x, a)\ufe38 \ufe37\ufe37 \ufe38 empirical Bernstein\n+ L\nNk(x, a)\ufe38 \ufe37\ufe37 \ufe38 empirical Bernstein\n+\n\u221a\u221a\u221a\u221amin ( H3S2AL2 \u2211 y P\u0302k(y|x,a) N \u2032\nk,h+1 (y) , H\n2 )\nNk(x, a)\ufe38 \ufe37\ufe37 \ufe38 additional bonus\n.\nThe backward induction over h is not straightforward. Indeed, if the Vk,h+1 are upper bounds on V \u2217 h+1, it is not necessarily the case that the empirical variance of Vk,h+1 are upper bound on the empirical variance of V \u2217 h+1. However we can prove by (backward) induction over h that Vk,h+1 is sufficiently close to V \u2217 h+1 to guarantee that the variance of those terms are sufficiently close to each other so that the additional bonus (additional term in the exploration bonus (6)) will make sure that Vk,h is still a bound on V \u2217h . More precisely, define the set of indices:\n[k, h]hist def = {(i, j), s.t.(1 \u2264 i \u2264 k and 1 \u2264 j \u2264 H) or (i = k and h < j \u2264 H)},\nand the event \u2126k,h def = {Vi,j \u2265 V \u2217h , (i, j) \u2208 [k, h]hist}. Our induction is the following: \u2022 Assume that \u2126k,h holds. Then we prove that (Vk,h+1 \u2212 V \u2217h+1)(y) \u2264 H \u221a SAL N \u2032\nk,h+1 (y) .\n\u2022 Deduce that V Y \u223cP\u0302k(\u00b7|x,a)\n( Vk,h+1(Y ) ) + H3S2AL2 \u2211 y P\u0302k(y|x,a) N\u2032\nk,h+1 (y)\n\u2265V Y\u223cP\u0302k(\u00b7|x,a)\n( V\n\u2217 h+1(Y )\n) , so the additional bonus com-\npensates for the possible variance difference. Thus Vk,h \u2265 V \u2217h and \u2126k,h\u22121 holds.\nSo in order to prove that all values computed by the algorithm are upper bounding V \u2217, we just need to prove that under \u2126k,h,\nwe have (Vk,h+1 \u2212 V \u2217h+1)(y) \u2264 min( H1.5SL \u221a A N \u2032\nk,h+1 (y) , H), which is obtained by deriving the following regret bound on\nR\u0303k,h(y) def =\n\u2211 i\u2264k (Vi,h+1 \u2212 V \u03c0ih+1)(xi,h+1)I{xi,h+1 = y}\n\u2264 HL \u221a SAN \u2032k,h+1(y), (6)\n(indeed, since {Vi,h}i is a decreasing sequence in i, we have that\n(Vk,h+1 \u2212 V \u2217h+1)(y) \u2264 R\u0303k,h+1(y)/N \u2032k,h+1(y)\n\u2264 HL \u221a SA/N \u2032k,h+1(y)).\nOnce we have proven that w.h.p., all computed values are upper bounds on V \u2217 (i.e. event \u2126), then we prove that under \u2126, the following regret bound holds:\nRegret(k) \u2264 R\u0303egret(k) \u2264 (L \u221a HSAT +H2S2AL2). (7)\nThe proof of (6) relies on the same derivations as those used for proving (7). The only two difference being are that (i) T is replaced by N \u2032k,h+1(y), the number of times a state y was reached at time h + 1, up to episode k, and (ii) the additional \u221a H factor which comes from the fact that at any episode, N \u2032k,h+1(y) can only tick once, whereas the total number of transitions from y during any episode can be as large as H . The full proof of (6) will be given in detail in the appendix. We now give a proof sketch of (7) under \u2126.\nSimilar steps used for proving Theorem 1 apply. The main difference compared to Theorem 1 is the bound on the sum of the exploration bonuses and the estimation errors (which we consider in Steps 3\u2019 and 4\u2019 below). This is where we can remove\nthe \u221a H factor. The use of the Bernstein inequality makes it possible to bound both of those terms in terms of the expected sum of variances (under the current policy \u03c0k at any episode k) of the next-state values (for that policy), and then using recursively the Law of Total Variance to conclude that this quantity is nothing else that the variance of the returns. This step is detailed now. For simplicity of the exposition of this sketch proof we neglect second order terms.\nStep 3\u2019: Bounding the sum of exploration bonuses bk,h. We have\n\u2211\nk,h\nbk,h = \u221a L \u2211\nk,h\n\u221a VY \u223cP\u0302\u03c0k\nh (\u00b7|xk,h)\n( Vk,h+1(Y ) )\nnk,h \ufe38 \ufe37\ufe37 \ufe38\nmain term\n+\n\u221a\u221a\u221a\u221amin ( H3S2AL2 \u2211 y P\u0302k(y|x,a) N \u2032\nk,h+1 (y) , H\n2 )\nNk(x, a)\ufe38 \ufe37\ufe37 \ufe38 second order term\n+ \u2211\nk,h\nL\nNk(x, a) \ufe38 \ufe37\ufe37 \ufe38\nsecond order term\n.\nBy Cauchy-Schwarz, the main term is bounded by (\u2211 k,h V\u0302k,h+1 \u2211 k,h 1\nnk,h\n)1/2 , where V\u0302k,h+1 def = VY\u223cP\u0302\u03c0k\nh (\u00b7|xk,h)\n( Vk,h+1(Y ) ) .\nSince \u2211\nk,h 1 nk,h \u2264 SA log(T ) by the pigeon-hole principle, we now focus on the term\u2211k,h V\u0302k,h+1.\nWe now prove that V\u0302k,h+1 is close to V \u03c0k k,h+1 def = VY\u223cP\u03c0k\nh (\u00b7|xk,h)\n( V \u03c0kh+1(Y ) ) by bounding the following quantity:\nV\u0302k,h+1 \u2212 V\u03c0kk,h+1 (i)\n\u2264 P\u0302 \u03c0kV 2k,h+1 \u2212 P \u03c0k(V \u03c0kh+1)2 (8) = (P\u0302 \u03c0k \u2212 P \u03c0k)V 2k,h\ufe38 \ufe37\ufe37 \ufe38\n(ak,h)\n+P \u03c0k(V 2k,h \u2212 (V \u03c0kh )2)\ufe38 \ufe37\ufe37 \ufe38 (a\u2032\nk,h )\n,\nwhere (i) holds since under \u2126, Vk,h+1 \u2265 V \u2217h+1 \u2265 V \u03c0kh+1.\nStep 3\u2019-a: bounding \u2211\nk,h V\u0302k,h+1 \u2212 V\u03c0kk,h+1. Using similar argument as those used in Jaksch et al. (2010b), we have that ak,h \u2264 H2\u2016P\u0302 \u03c0k \u2212 P \u03c0k\u20161 \u2264 H2 \u221a SL/nk,h (where nk,h\ndef = Nk(xk,h, \u03c0k(xk,h))). Thus from the pigeon-hole principle,\u2211\nk,h ak,h \u2264 H2S \u221a ATL.\nNow a\u2032k,h is be bounded as\na\u2032k,h \u2264 2HP \u03c0k(Vk,h \u2212 V \u03c0kh ) = 2HP \u03c0k\u2206\u0302k,h.\nThus using Azuma\u2019s inequality,\n\u2211\nk,h\na\u2032k,h (Az) \u2264 2H \u2211\nk,h\n\u03b4\u0302k,h+1 + H 2 \u221a TL\n\u2264 2H2U + H2 \u221a TL,\nwhere U is defined an upper-bound on the pseudo regret: U def = \u2211 k,h(bk,h + ek,h) + H \u221a T (an upper bound on the r.h.s. of (3)).\nStep 3\u2019-b: bounding \u2211\nk,h V \u03c0k k,h+1. This is the dominant term!\nFor any episode k, E[ \u2211\nh V \u03c0k k,h+1|Hk] is the expected sum of variances of the value function V \u03c0k (y) at the next state\ny \u223c P \u03c0k(\u00b7|xk,h) under the true transition model for the current policy. A recursive application of the law of total variance (see e.g., Munos and Moore, 1999; Azar et al., 2013; Lattimore and Hutter, 2012) shows that this quantity is nothing else than the variance of the return (sum of h rewards) under policy \u03c0k: V (\u2211 h r(xk,h, \u03c0k(xk,h)) ) , which is thus bounded by H2. Finally,\nusing Freedman\u2019s (Fr) inequality to bound \u2211\nk,h V \u03c0k k,h+1 by its expectation (see the exact derivation in the appendix), we deduce\n\u2211\nk,h\nV\u03c0kk,h+1\n(Fr) \u2264 \u2211\nk\nE [\u2211\nh\nV\u03c0kk,h+1|Hk ] + H2 \u221a TL\n\u2264 TH + H2 \u221a TL. (9)\nThus, using (9), (8) and the bounds on \u2211 ak,h and \u2211 a\u2032k,h, we deduce that\n\u2211\nk,h\nbk,h \u2264 L \u221a (TH +H2U)SA.\nStep 4\u2019: Bounding the sum of estimation errors \u2211\nk,h ek,h. We now use Bernstein inequality to bound the estimation errors\n\u2211\nk,h\nek,h = \u2211\nk,h\n(P\u0302 \u03c0kk \u2212 P \u03c0k)V \u2217h+1(xk,h)\n\u2264 \u221a\nV\u2217k,h+1 nk,h + HL nk,h ,\nwhere V\u2217k,h+1 def = VY\u223cP\u03c0k (\u00b7|xk,h) ( V \u2217h+1(Y ) ) . Now, in a very similar way as in Step 3\u2019 above, we relate V\u2217k,h+1 to V \u03c0k k,h+1 and\nuse the Law of total variance to bound \u2211\nk,h V \u03c0k k,h+1 byHT and deduce that\n\u2211\nk,h\nek,h \u2264 L \u221a (TH +H2U)SA.\nFinally we deduce from (3) that U \u2264 L \u221a (TH +H2U)SA thus U \u2264 (L \u221a HSAT + H2SAL2), which is the main\nterm of the regret, from which we deduce (7). So the reason we are able to remove the \u221a H factor from the regret bound comes from the fact that the sum, overH steps, of the variances of the next state values (which define the amplitude of the confidence intervals) is at most bounded by the variance of the return. Intuitively this means that the size of the confidence intervals do not add up linearly over H steps but grows as \u221a H only. Although the sequence of estimation errors are not independent over time, we are able to demonstrate a concentration of measure phenomenon that shows that those estimation errors concentrate as if they were independent."}, {"heading": "6 Conclusion", "text": "In this paper we present a new variant of the familiar concept of optimism in the face of uncertainty. Our key contribution is the design and analysis of the algorithm UCBVI_2 , which addresses two key shortcomings in existing algorithms for optimistic exploration in finite MDPs. First we apply a concentration to the value as a whole, rather than the transition estimates, this\nleads to a reduction from S to \u221a S. Next we apply a recursive law of total variance to couple estimates across an episode, rather\nthan at each time step individually, this leads to a reduction fromH to \u221a H .\nTheorem 2 provides the first regret bounds which, for sufficiently large T , match the lower bounds for the problem O\u0303( \u221a HSAT ) up to logarithmic factors. It remains an open problem whether for small values of T we can match the lower bound using this approach. In fact we believe that the higher order term can be improved from O\u0303(H2S2A) to O\u0303(HS2A) by a more careful analysis, i.e., a more extensive use Freedman-Bernstein inequalities. The same applies to the term of orderH \u221a T\nwhich can be improved to \u221a HT .\nThese results are particularly significant because they help to settle an ongoing debate over whether such bounds were even attainable, or whether the true lower bound should be \u2126(H \u221a SAT ) Bartlett and Tewari (2009); Osband and Van Roy (2016a).\nMoving from this big-picture insight we have stressed in this paper to an analytically rigorous bound is non-trivial. Although we push many of the technical details to the appendix, our paper also makes several contributions in terms of analytical tools that may be useful in subsequent work."}, {"heading": "Appendices", "text": "We begin by introducing some notation in Sect. A. We then provide the full analysis of UCBVI in Sect. B."}, {"heading": "A Notation", "text": "Let denote the total number of times that we visit state x while taking action a at step h of all episodes up to episode k by N \u2032k,h(x, a). We also use the notation N \u2032 k,h(x) = \u2211 a\u2208A N \u2032 k,h(x, a) for the total number of visits to state x at time step h up to episode k. Also define the empirical variance V\u0302k,h, the variance of optimal value function V \u2217 h(x, a) the empirical variance of optimal value function V\u0302\u2217k,h(x, a) and the variance of V \u03c0 h as\nV\u0302k,h(x, a) def = Vary\u223cP\u0302k(\u00b7|x,a)(Vk,h+1(y)),\nV\u2217h(x, a) def = Vary\u223cP (\u00b7|x,a)(V \u2217 h (y)),\nV\u0302\u2217k,h(x, a) def = Vary\u223cP\u0302k(\u00b7|x,a)(V \u2217 h (y)),\nV\u03c0h(x, a) def = Vary\u223cP (\u00b7|x,a)(V \u03c0 h (y)).\nfor every (x, a) \u2208 S \u00d7 A and k \u2208 [K] and h \u2208 [H ]. We also introduce some short-hand notation: we use the lower case to denote the functions evaluated at the current state-action pair, e.g., we write nk,h for Nk(xk,h, \u03c0k(xk,h, h)) and vk,h for Vk,h(xk,h). Let also denote V \u2217 k,h = V \u2217 k,h(xk,h, \u03c0k(xk,h, h)) and V \u03c0k k,h = V \u03c0k k,h(xk,h, \u03c0k(xk,h, h)) for every k \u2208 [K] and\nh \u2208 [H ]. Also define b\u2032i,j(x) = min ( 652S2H2AL2\nN \u2032 i,j+1\n(x) , H 2 ) for every x \u2208 S"}, {"heading": "A.1 \u201cTypical\u201d state-actions and steps", "text": "In our analysis we split the episodes into 2 sets: the set of \u201ctypical\u201d episodes in which the number of visits to the encountered state-actions are large and the rest of the episodes. We then prove a tight regret bound for the typical episodes. As the total count of other episodes is bounded this technique provides us with the desired result. The set of typical state-actions pairs for every episode k is defined as follows\n[(x, a)]k def = {(x, a) : (x, a) \u2208 S \u00d7A, Nh(x, a) \u2265 H,N \u2032k,h(x) \u2265 H}.\nBased on the definition of [(x, a)]typ and [k]typ we define the set of typical episodes and the set of typical state-dependent episodes as follow\n[k]typ def = {i : i \u2208 [k], \u2200h \u2208 [H ], (xi,h, \u03c0i(xi,h, h)) \u2208 [(x, a)]k, i \u2265 75HS2AL},\n[k]typ,x def = {i : i \u2208 [k], \u2200h \u2208 [H ], (xi,h, \u03c0i(xi,h, h)) \u2208 [(x, a)]k, N \u2032k,h(x) \u2265 75HS2AL}.\nAlso for every (x, a) \u2208 S \u00d7A the set of typical next states at every episode k is defined as follows:\n[y]k,x,a def = {y : y \u2208 S, Nk(x, a)P (y|x, a) \u2265 2H2L}.\nAlso let denote [y]k,h = [y]k,xk,h,\u03c0k(xxk,h ) for every k \u2208 [K] and h \u2208 [H ]."}, {"heading": "A.2 Surrogate regrets", "text": "Our ultimate goal is to prove bound on the regret Regret(k). However in our analysis we mostly focus on bounding the surrogate regrets. Let \u2206\u0303k,h(x) def = Vk,h(x) \u2212 V \u03c0kh (x) for every x \u2208 S, h \u2208 [H ] and k \u2208 [K]. Then the upper-bound regret R\u0303egret defined as follows\nR\u0303egret(k) def =\nk\u2211\ni=1\n\u03b4\u0303i,1.\nR\u0303egret(k) is useful in our analysis since it provides an upperbound on the true regret Regret(k). So one can bound\nR\u0303egret(k) as a surrogate for Regret(k). We also define the corresponding per state-step regret and upper-bound regret for every state x \u2208 X and step h \u2208 [H ], respectively, as follows\nRegret(k, x, h) def =\nk\u2211\ni=1\nI(xi,h = x)\u03b4i,h,\nR\u0303egret(k, x, h) def =\nk\u2211\ni=1\nI(xi,h = x)\u03b4\u0303i,h."}, {"heading": "A.3 Martingale difference sequences", "text": "In our analysis we rely heavily on the theory of martingale sequences to prove bound on the regret incurred due to encountering a random sequence of states. We now provide some definitions and notation in that regard.\nWe define the following martingale operator for every k \u2208 [K], h \u2208 [H ] and F : S \u2192 \u211c. Also let t = (k \u2212 1)H + h. denote the time stamp at step h of episode k then\nMtF def= P \u03c0kh F \u2212 F (xk,h+1).\nLet Ht be the history of all random events up to (and including) step h of episode k then we have that E(MtF |Ht) = 0. Thus MtF is a martingale difference w.r.t. Ht. Also let G be a real-value function depends on Ht+s for some integer s > 0. Then we generalize our definition of operatorMt as\nMtG def= E (G(Ht+s)| Ht)\u2212G(Ht+s),\nwhere E is over the randomization of the sequence of states generated by the sequence of policies \u03c0k, \u03c0k+1, . . . . Here also MtG is a martingale difference w.r.t. Ht.\nLet define\u2206typ,k,h : S \u2192 \u211c as follows for every k \u2208 [K] and h \u2208 [H ] and y \u2208 S:\n\u2206\u0303typ,k,h+1(y) def =\n\u221a Ik,h(y)\nnk,hpk,h(y) \u2206\u0303k,h+1(y),\nwhere the function pk,h : S \u2192 [0, 1] is defined as pk,h(y) = P \u03c0kh (y|xk,h) and Ik,h(y) writes for Ik,h(y) = I(y \u2208 [y]k,h) for every y \u2208 X . We also define the following martingale differences which we use frequently:\n\u03b5k,h def = Mt\u2206\u0303k,h+1, \u03b5\u0304k,h def = Mt\u2206\u0303typ,k,h+1."}, {"heading": "A.4 High probability events", "text": "We now introduce the high probability events E and \u2126k,h under which the regret is small.\nLet use the shorthand notation L def = log\n( 5SAT\n\u03b4\n) . Also for every v > 0, p \u2208 [0, 1] and n > 0 let define the confidence\nintervals c1, c2 and c3, respectively, as follow:\nc1(v, n) def = 2\n\u221a vL\nn +\n14HL\n3n ,\nc2(p, n) def = 2\n\u221a p(1\u2212 p)L\nn +\n2L 3n ,\nc3(n) def = 2\n\u221a SL\nn .\nLet P be the set of all probability distributions on S. Define the following confidence set for every k = 1, . . . ,K , n > 0 and (x, a) \u2208 S \u00d7A:\nP(k, h, n, x, a, y) def= { P\u0303 (\u00b7|x, a) \u2208 P : |(P\u0303 \u2212 P )V \u2217h (x, a)| \u2264 min ( c1 (V \u2217 h(x, a), n) , c1 ( V\u0302\u2217k,h(x, a), n ))\n|P\u0303 (y|x, a)\u2212 P (y|x, a)| \u2264 c2 (P (y|x, a), n) , \u2016P\u0303 (\u00b7|x, a) \u2212 P (\u00b7|x, a)\u20161 \u2264 c3(n) } .\nWe now define the random event EP\u0302 as follows\nEP\u0302 def = { P\u0302k(\u00b7|x, a) \u2208 P(k, h,Nk(x, a), x, a, y), \u2200k \u2208 [K], \u2200h \u2208 [H ], \u2200(y, x, a) \u2208 S \u00d7 S \u00d7A } .\nLet t be a positive integer. Let F = {fs}s\u2208[t] be a set of real-value functions on Ht+s, for some integer s > 0. We now define the following random events for every w\u0304 > 0 and u\u0304 > 0 and c\u0304 > 0:\nEaz(F , u\u0304, c\u0304) def= { t\u2211\ns=1\nMsfs \u2264 2 \u221a tu\u03042c\u0304 } ,\nEfr(F , w\u0304, u\u0304, c\u0304) def= { t\u2211\ns=1\nMsfs \u2264 4 \u221a w\u0304c+ 14u\u0304c\u0304\n3\n} .\nWe also use the short-hand notation Eaz(F , u\u0304) and Efr(F , w\u0304, u\u0304) for Eaz(F , u\u0304, L) and Efr(F , w\u0304, L), respectively. Now let define the following sets of functions for every k \u2208 [K] and h \u2208 [H ]:\nF\u2206\u0303,k,h def = { \u2206\u0303i,j : i \u2208 [k], h < j \u2208 [H \u2212 1] } ,\nF \u2032 \u2206\u0303,k,h def =\n{ \u2206\u0303typ,i,j : i \u2208 [k], h < j \u2208 [H ] } ,\nF\u2206\u0303,k,h,x def = { \u2206\u0303i,jI(xi,h = x) : i \u2208 [k], h < j \u2208 [H ] } ,\nF \u2032 \u2206\u0303,k,h,x def =\n{ \u2206\u0303typ,i,jI(xi,h = x) : i \u2208 [k], h < j \u2208 [H ] } ,\nGV,k,h def=    H\u2211\nj=h+1\nV\u03c0ij : i \u2208 [k], h < j \u2208 [H ]    ,\nGV,k,h,x def=    H\u2211\nj=h+1\nV\u03c0ij I(xi,h = x) : i \u2208 [k], h < j \u2208 [H ]    ,\nFb\u2032,k,h def= { b\u2032i,j : i \u2208 [k], h < j \u2208 [H \u2212 1] } ,\nFb\u2032,k,h,x def= { b\u2032i,jI(xi,h = x) : i \u2208 [k], h < j \u2208 [H ] } .\nWe now define the high probability event E as follows\nE def= EP\u0302 \u22c2 \u22c2\nk\u2208[K] h\u2208[H] x\u2208S\n[ Eaz(F\u2206\u0303,k,h, H) \u22c2 Eaz(F \u2032\u2206\u0303,k,h, 1/ \u221a L) \u22c2 Eaz(F\u2206\u0303,k,h,x, H) \u22c2 Eaz(F \u2032\u2206\u0303,k,x,h, 1/ \u221a L)\n\u22c2 Efr(GV,k,h, H4T,H3) \u22c2 Eaz(GV,k,h,x, H5N \u2032k,h(x), H3) \u22c2 Eaz(Fb\u2032,k,h, H2) \u22c2 Eaz(Fb\u2032,k,h,x, H2) ] .\nThe following lemma shows that the event E holds with high probability: Lemma 1. Let \u03b4 > 0 be a real scalar. Then the event E holds w.p. at least 1\u2212 \u03b4. Proof. To prove this result we need to show that a set of concentration inequalities with regard to the empirical model P\u0302k holds simultaneously. For every h \u2208 [H ] the Bernstein inequality combined with a union bound argument, to take into account that Nk(x, a) \u2208 [T ] is a random number, leads to the following inequality w.p. 1 \u2212 \u03b4 (see, e.g., Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012, for the statement of the Bernstein inequality and the application of the union bound in similar cases, respectively.)\n\u2223\u2223\u2223 [ (P \u2212 P\u0302k)V \u2217h ] (x, a) \u2223\u2223\u2223 \u2264 \u221a 2V\u2217h(x, a) log ( 2T \u03b4 )\nNk(x, a) +\n2H log ( 2T \u03b4 )\n3Nk(x, a) , (10)\nwhere we rely on the fact that V \u2217h is uniformly bounded by H . Using the same argument but this time with the Empirical Bernstein inequality (see, e.g., Maurer and Pontil, 2009), for Nk(x., a) > 1, leads to\n\u2223\u2223\u2223 [ (P \u2212 P\u0302k)V \u2217h ] (x, a) \u2223\u2223\u2223 \u2264\n\u221a 2V\u0302\u2217k,h(x, a) log ( 2T \u03b4 )\nNk(x, a) +\n7H log ( 2T \u03b4 )\n3Nk(x, a) . (11)\nThe Bernstein inequality combined with a union bound argument on Nk(x, a) also implies the following bound w.p. 1\u2212 \u03b4\n|Nk(y, x, a)\u2212Nk(x, a)P (y|x, a)| \u2264 \u221a 2Nk(x, a)Varz\u223cP (\u00b7|x,a)(1(z = y)) log ( 2T\n\u03b4\n) + 2 log ( 2T \u03b4 )\n3 ,\nwhich implies the following bound w.p. 1\u2212 \u03b4:\n\u2223\u2223\u2223P\u0302k(y|x, a)\u2212 P (y|x, a) \u2223\u2223\u2223 \u2264\n\u221a P (y|x, a)(1 \u2212 P (y|x, a)) log ( 2T \u03b4 )\nNk(x, a) +\n2 log ( 2T \u03b4 ) 3Nk(x, a) . (12)\nThe following result also holds on \u21131-normed estimation error of the transition distribution (Weissman et al., 2003), combined with a union bound on Nk(x, a) \u2208 [T ] implies w.p. 1\u2212 \u03b4\n\u2225\u2225\u2225P\u0302k(\u00b7|x, a) \u2212 P (\u00b7|x, a) \u2225\u2225\u2225 1\n\u2264 \u221a 2S log ( 2T \u03b4 )\nNk(x, a) . (13)\nWe now focus on bounding the sequence of martingales. Let n > 0 be an integer and u, \u03b4 > 0 be some real scalars. Let the sequence of random variables {X1, X2, . . . , Xn} be a sequence of martingale differences w.r.t. to some filtration Fn. Let this sequence be uniformly bounded from above and below by u. Then the Azuma\u2019s inequality (Cesa-Bianchi and Lugosi, 2006, see, e.g.,) implies that w.p. 1\u2212 \u03b4\nn\u2211\ni=1\nXi \u2264 \u221a 2nu log ( 1\n\u03b4\n) . (14)\nWhen the sum of the variances \u2211n\ni=1 Var(Xi|Fi) \u2264 w then the following sharper bound due to Freedman (1975) holds w.p. 1\u2212 \u03b4\nn\u2211\ni=1\nXi \u2264 \u221a 2w log ( 1\n\u03b4\n) + 2u log ( 1 \u03b4 )\n3 . (15)\nLet k \u2208 [K], h \u2208 [H ] and x \u2208 X . then the inequality of Eq. 14 immediately implies that the following events holds w.p. 1\u2212 \u03b4:\nEaz ( F\u2206\u0303,k,h, H, log (1/\u03b4) ) , (16) Eaz ( F \u2032\n\u2206\u0303,k,h , 1/\n\u221a L, log (1/\u03b4) ) , (17)\nEaz ( Fb\u2032,k,h, H2, log (1/\u03b4) ) . (18)\nAlso Eq. 14 combined with a union bound argument over all N \u2032k,h(x) \u2208 [T ] (see, e.g., Bubeck et al., 2011, for the full description of the application of union bound argument in the case martigale process with random stopping time) implies that the following events hold w.p. 1\u2212 \u03b4\nEaz ( F\u2206\u0303,k,h,x, H, log (T/\u03b4) ) , (19) Eaz ( F \u2032\n\u2206\u0303,k,h,x , 1/\n\u221a L, log (T/\u03b4) ) , (20)\nEaz ( Fb\u2032,k,h,x, H2, log (T/\u03b4) ) . (21)\nSimilarly the inequality of Eq. 15 leads to the following events w.p. 1\u2212 \u03b4\nEfr ( GV,k,h, w\u0304k,h, , H3, log (T/\u03b4) ) , (22) Efr ( GV,k,h,x, w\u0304k,h,x, , H3, log (1/\u03b4) ) , (23)\nwhere w\u0304k,h and w\u0304k,h,x are upper bounds onWk,h andWk,h,x, respectively, defined as\nWk,h def =\nk\u2211\ni=1\nVar\n  H\u22121\u2211\nj=h\nV\u03c0i,j+1 \u2223\u2223\u2223\u2223\u2223\u2223 Hi,1   , (24)\nWk,h,x def =\nk\u2211\ni=1\nI(xi,h = x)E\n  H\u22121\u2211\nj=h\nV\u03c0i,j+1 \u2223\u2223\u2223\u2223\u2223\u2223 Hi,1   . (25)\nSo to establish a value for w\u0304k,h and w\u0304k,h,x we need to prove bound Wk,h and Wk,h,x. Here we only prove this bound for Wk,h as the proof technique to boundWk,h,x is identical to the way we boundWk,h.\nWk,h \u2264 k\u2211\ni=1\nE\n  H\u22121\u2211\nj=h\nV\u03c0ki,j+1 \u2223\u2223\u2223\u2223\u2223\u2223 Hk   2 \u2264 H3 k\u2211\ni=1\nE\n  H\u22121\u2211\nj=h\nV\u03c0ki,j+1 \u2223\u2223\u2223\u2223\u2223\u2223 Hk   . (26)\nNow let the sequence {x1, x2, . . . , xH} be the sequence of states encountered by following some policy \u03c0 throughout an episode k. Then the recursive application of LTV leads to (see e.g., Munos and Moore, 1999; Lattimore and Hutter, 2012, for the proof.)\nE\n  H\u22121\u2211\nj=h\nV\u03c0(xh, \u03c0(xh, h))   = Var ( H\u2211\nh=1\nr\u03c0(xh) ) . (27)\nBy combining Eq. 27 into Eq. 26 we deduce\nWk,h \u2264 H3 k\u2211\ni=1\nVar\n  H\u22121\u2211\nj=h\nrk,h \u2223\u2223\u2223\u2223\u2223\u2223 Hk   \u2264 H5k = H4Tk. (28)\nSimilarly the following bound holds onWk,h,x\nWk,h,x \u2264 H5Nk,h(x). (29)\nPlugging the bounds of Eq. 28 and Eq. 29 in to the bounds of Eq. 22 and Eq. 23 and a union bound over all Nk,h(x) \u2208 [T ] leads to the following bounds w.p. 1\u2212 \u03b4:\nEfr ( GV,k,h, H4T,H3, log (1/\u03b4) ) , (30) Efr ( GV,k,h,x, H5Nk,h(x), H3, log (T/\u03b4) ) . (31)\nCombining the results of Eq. 10, Eq. 11, Eq. 12, Eq. 13, Eq. 16 Eq. 17 Eq. 18, Eq. 19, Eq. 20, Eq. 21, Eq. 30 and Eq. 31\nand taking a union bound over these random events as well as all possible k \u2208 [K], h \u2208 [H ], (s, a) \u2208 S \u00d7A proves the result."}, {"heading": "A.4.1 UCB Events", "text": "Let k \u2208 [K] and h \u2208 [H ]. Denote the set of steps for which the value functions are obtained before Vk,h as\n[k, h]hist = {(i, j) : i \u2208 [K], j \u2208 [H ], i < k \u2228 (i = k \u2227 j > h)}.\nLet \u2126k,h = {Vi,j \u2265 V \u2217h , \u2200(i, j) \u2208 [k, h]hist} be the event under whic Vi,j prior to Vk,h computation are upper bounds on the optimal value functions. Using backward induction on h (and standard concentration inequalities) we will prove that \u2126k,h holds under the event E (see Lem. 19)."}, {"heading": "A.5 Other useful notation", "text": "Here we define some other notation that we use throughout the proof. We denote the total count of steps upto episode k \u2208 [K] by Tk def = H(k \u2212 1). We first define zk,h, for every h \u2208 [H ] and k \u2208 [K], as follow\nc4,k,h = 4H2SAL\nnk,h .\nfor every k \u2208 [K] , h \u2208 [H ] and x \u2208 [x] we also introduce the following notation which we use later when we sum up the regret:\nCk,h def =\nk\u2211\ni=1\nI(i \u2208 [k]typ) H\u22121\u2211\nj=h\nc1,i,j ,\nBk,h def =\nk\u2211\ni=1\nI(i \u2208 [k]typ) H\u22121\u2211\nj=h\nbi,j ,\nCk,h,x def =\nk\u2211\ni=1\nI(i \u2208 [k]typ,x, xk,h = x) H\u22121\u2211\nj=h\nc1,i,j ,\nBk,h,x def =\nk\u2211\ni=1\nI(i \u2208 [k]typ,x, xk,h = x) H\u22121\u2211\nj=h\nbi,j,\nwhere c1,k,h is the shorthand-notation for c1(v \u2217 k,h, nk,h). We also define the upper bound Uk,h and Uk,h,x for every k \u2208 [K] , h \u2208 [H ] and x \u2208 S as follows, respectively\nUk,h def = e\nk\u2211\ni=1\nH\u22121\u2211\nj=h\n[bi,j + c1,i,j + c4,i,j ] + (H + 1) \u221a TkL,\nUk,h,x def = e\nk\u2211\ni=1\nH\u22121\u2211\nj=h\n[bi,j + c1,i,j + c4,i,j ] + (H + 1) 3/2 \u221a N \u2032k,h(x)L,"}, {"heading": "B Proof of the Regret Bounds", "text": "Before we start the main analysis we state the following useful lemma that will be used frequently in the analysis:\nLemma 2. letX \u2208 R and Y \u2208 R be two random variables. Then following bound holds for their variances\nVar(X) \u2264 2 [Var(Y ) + Var(X \u2212 Y )] . Proof. The following sequence of inequalities hold\nVar(X) = E(X \u2212 Y \u2212 E(X \u2212 Y ) + Y \u2212 E(Y ))2 \u2264 2E(X \u2212 Y \u2212 E(X \u2212 Y ))2 + 2E(Y \u2212 E(Y ))2. The result follows from the definition of variance.\nWe proceed by proving the following key lemma which shows that proves bound on\u2206k,h under the assumption that Vk,h is UCB w.r.t. V \u2217h . Lemma 3. Let k \u2208 [K] and h \u2208 [H ]. Let the event E and \u2126k,h hold. Then the following bound holds on \u03b4k,h and \u03b4\u0303k,h:\n\u03b4k,h \u2264 \u03b4\u0303k,h \u2264 e H\u22121\u2211\ni=h\n[ \u03b5k,i + 2 \u221a L\u03b5\u0304k,i + c1,k,i + bk,i + c4,k,i ] . (32)\nProof. For the ease of exposition we abuse the notation and drop the dependencies on k, e.g., we write x1, \u03c0 and V1 for xk,1, \u03c0k and Vk,1, respectively. We proceed by bounding \u03b4\u0303h under the event E at every step 0 < h < H :\n\u03b4\u0303h = ThVh+1(xh)\u2212 T \u03c0h V \u03c0h+1(xh) = [P\u0302 \u03c0h Vh+1](xh) + bh \u2212 [P \u03c0h V \u03c0h+1](xh) = bh + [(P\u0302 \u03c0 h \u2212 P \u03c0h )V \u2217h+1](xh) + [(P\u0302 \u03c0h \u2212 P \u03c0h )(Vh+1 \u2212 V \u2217h+1)](xh) + [P \u03c0h (Vh+1 \u2212 V \u03c0h+1)](xh)\n\u2264 \u03b4\u0303h+1 + \u03b5h + bh + c1,h + [(P\u0302 \u03c0h \u2212 P \u03c0h )(Vh+1 \u2212 V \u2217h+1)](xh)\ufe38 \ufe37\ufe37 \ufe38 (a) , (33)\nwhere the last inequality follows from the fact that under the event E we have that [(P\u0302 \u03c0h \u2212 P \u03c0h )V \u2217h+1](xh) \u2264 c1,h. We now bound (a):\n(a) = \u2211\ny\u2208S (P\u0302 \u03c0h (y|xh)\u2212 P \u03c0h (y|xh))(Vh+1(y)\u2212 V \u2217h+1(y))\n(I) \u2264 \u2211\ny\u2208S\n 2 \u221a ph(y)(1\u2212 ph(y))L\nnh +\n4L\n3nh\n \u2206h+1(y)\n\u2264 2 \u221a L \u2211\ny\u2208S\n\u221a ph(y)\nnh \u2206\u0303h+1(y)\n\ufe38 \ufe37\ufe37 \ufe38 (b)\n+ 4SHL\n3nh ,\nwhere (I) holds under the event E . We proceed by bounding (b):\n(b) = \u2211\ny\u2208[y]h\n\u221a ph(y)\nnh \u2206\u0303h+1(y)\n\ufe38 \ufe37\ufe37 \ufe38 (c)\n+ \u2211\ny/\u2208[y]h\n\u221a ph(y)\nnh \u2206\u0303h+1(y)\n\ufe38 \ufe37\ufe37 \ufe38 (d)\n. (34)\nThe term (c) can be bounded as follows\n(c) = \u2211\ny\u2208[y]h\nP \u03c0h (y|xh) \u221a\n1\nnhph(y) \u2206\u0303h+1(y) = \u03b5\u0304h +\n\u221a 1\nnhph(xh+1) I(xh+1 \u2208 [y]h)\u03b4\u0303h+1\n\u2264 \u03b5\u0304h + \u221a 1\n4LH2 \u03b4\u0303h+1, (35)\nwhere in the last line we rely on the definition of [y]h. We now bound (d):\n(d) = \u2211\ny/\u2208[y]h\n\u221a ph(y)nh\nn2h \u2206\u0303h+1(y) \u2264\nSH \u221a 4LH2\nnh . (36)\nBy combining Eq. 35 and Eq. 36 into Eq. 34 we deduce\n(b) \u2264 SH \u221a 4LH2\nnh +\n\u221a 1\n4LH2 \u03b4\u0303h+1 + \u03b5\u0304h. (37)\nBy combining Eq. 37 and Eq. 34 into Eq. B we deduce\n\u03b4\u0303h \u2264 \u03b5h + 2 \u221a L\u03b5\u0304h + bh + c1,h + c4,h + ( 1 + 1\nH\n) \u03b4\u0303h+1.\nLet denote \u03b3h = (1 + 1/H) h. The previous bound combined with an induction argument implies that\n\u03b4\u0303h \u2264 \u2211H\u22121 i=h \u03b3i\u2212h [ \u03b5i + 2 \u221a L\u03b5\u0304i + c1,i + c4,i + bi ] .\nThe inequality log(1 + x) \u2264 x for every x > \u22121 leads to \u03b3h \u2264 \u03b3H \u2264 e for every h \u2208 [H ]. This combined with the assumption that vh \u2265 v\u2217h under the event \u2126h completes the proof.\nLemma 4. Let k \u2208 [k] and h \u2208 [H ]. Let the events E and \u2126k,h hold. Then\nk\u22121\u2211\ni=1\n\u03b4i,h \u2264 k\u22121\u2211\ni=1\n\u03b4\u0303i,h \u2264 e k\u22121\u2211\ni=1\nH\u22121\u2211\nj=h\n[ \u03b5i,j + 2 \u221a L\u03b5\u0304i,j + bi,j + c1,i,j + c4,i,j ] .\nProof. The proof follows by summing up the bounds of Lem. 3 and taking into acoount the fact if \u2126k,h holds then \u2126i,j for all (i, j) \u2208 [k, h]hist hold.\nTo simplify the bound of Lem. 4 we prove bound on sum of the martingales \u03b5k,h and \u03b5\u0304k,h\nLemma 5. Let k \u2208 [k] and h \u2208 [H ]. Let the event E and \u2126k,h hold. Then the following bound holds\nk\u2211\ni=1\nH\u22121\u2211\nj=h\n\u03b5i,j \u2264 H \u221a (H \u2212 h)kL \u2264 H \u221a TkL, (38)\nk\u2211\ni=1\nH\u22121\u2211\nj=h\n\u03b5\u0304i,j \u2264 \u221a (H \u2212 h)k \u2264 \u221a Tk. (39)\nAlso the following bounds holds for every x \u2208 X and h \u2208 H:\nk\u2211\ni=1\nI(xi,h = x) H\u22121\u2211\nj=h\n\u03b5i,j \u2264 H \u221a (H \u2212 h)N \u2032k,h(x)L, (40)\nk\u2211\ni=1\nI(xi,h = x)\nH\u22121\u2211\nj=h\n\u03b5\u0304i,j \u2264 \u221a (H \u2212 h)N \u2032k,h(x). (41)\nProof. The fact that the event E holds implies that the events Eaz(F\u2206\u0303,k,h, H), Eaz(F \u2032\u2206\u0303,k,h, 1\u221a L ) , Eaz(F\u2206\u0303,k,h,x, H) and Eaz(F \u2032\u2206\u0303,x,k,h, 1\u221a L ) hold. Under these events the inequalities of the statement hold. This combined with the fact that (H \u2212 h)k \u2264 Tk completes the proof.\nWe now bound the sum of \u03b4s in terms of the upper-boundU :\nLemma 6. Let k \u2208 [K] and h \u2208 [H ]. Let the event E and \u2126k,h holds. Then the following bounds hold for every h \u2208 [H ] x \u2208 S\nk\u2211\ni=1\n\u03b4i,h \u2264 k\u2211\ni=1\n\u03b4\u0303i,h \u2264 Uk,h \u2264 Uk,1,\nk\u2211\ni=1\nI(xi,h = x)\u03b4i,h \u2264 k\u2211\ni=1\nI(xi,h = x)\u03b4\u0303i,h \u2264 Uk,h,x. \u2264 Uk,1,x.\nProof. The proof follows by incorporating the result of Lem. 5 into Lem. 4 and taking into account that for every h \u2208 [H ] the term Uk,h (Uk,h,x) is a summation of non-negative terms which are also contained in Uk,1 (U1,h,x).\nLemma 7. Let k \u2208 [K] and h \u2208 [H ]. Let the event E and \u2126k,h holds. Then the following bounds hold for every x \u2208 S\nk\u2211\ni=1\nH\u2211\nj=h\n\u03b4i,j \u2264 k\u2211\ni=1\nH\u2211\nj=h\n\u03b4\u0303i,j \u2264 HUk,1,\nk\u2211\ni=1\nI(xi,h = x)\nH\u2211\nj=h\n\u03b4i,j \u2264 k\u2211\ni=1\nI(xi,h = x)\nH\u2211\nj=h\n\u03b4\u0303i,j \u2264 HUk,1,x.\nProof. The proof follows by summing up the bounds of Lem. 6.\nWe now focus on bounding the terms Ck,h (Ck,h,x) and Bk,h (Bk,h,x) in Lem. 11 and Lem. 12, respectively. Before we proceed with the proof of Lem. 11 and Lem. 12. we prove the following key result which bounds sum of the variances of V \u03c0k,h using an LTV argument:\nLemma 8. Let k \u2208 [K] and h \u2208 [H ]. Then under the events E and \u2126k,h the following hold for every x \u2208 S\nk\u2211\ni=1\nH\u22121\u2211\nj=h\nV\u03c0i,j+1 \u2264 TkH + 2 \u221a H4TkL+ 4H3L\n3 ,\nk\u2211\ni=1\nI(xi,h = x)\nH\u22121\u2211\nj=h\nV\u03c0i,j+1 \u2264 N \u2032k,h(x)H2 + 2 \u221a H5N \u2032k,h(x)L+ 4H3L\n3 .\nProof. Under E the events Efr(GV,k,h, H4Tk, H3) and Efr(GV,k,h,x, H5Nk,h(x), H3) hold which then imply:\nk\u2211\ni=1\nH\u22121\u2211\nj=h\nV\u03c0i,j+1 \u2264 k\u2211\ni=1\nE\n  H\u22121\u2211\nj=h\nV\u03c0i,j+1 \u2223\u2223\u2223\u2223\u2223\u2223 Hk,h  + 2 \u221a H4TkL+ 4H3L 3 , (42)\nk\u2211\ni=1\nI(xi,h = x) H\u22121\u2211\nj=h\nV\u03c0i,j+1 \u2264 k\u2211\ni=1\nI(xi,h = x)E\n  H\u22121\u2211\nj=h\nV\u03c0i,j+1 \u2223\u2223\u2223\u2223\u2223\u2223 Hk,h  + 2 \u221a H5N \u2032k,hL+ 4H3L 3 . (43)\nThe LTV argument of Eq. 27 then leads to\nk\u2211\ni=1\nE\n  H\u22121\u2211\nj=h\nV\u03c0i,j+1 \u2223\u2223\u2223\u2223\u2223\u2223 Hi,h   = k\u2211\ni=1\nVar\n  H\u2211\nj=h+1\nr\u03c0i,j\n  \u2264 KH2 = TH (44)\nk\u2211\ni=1\nI(xi,h = x)E\n  H\u22121\u2211\nj=h\nV\u03c0i,j+1 \u2223\u2223\u2223\u2223\u2223\u2223 Hi,h   = k\u2211\ni=1\nI(xi,h = x)Var\n  H\u2211\nj=h+1\nr\u03c0i,j   \u2264 N \u2032k,h(x)H2. (45)\nEq. 42 and Eq. 43 combined with Eq. 44 and Eq. 45, respectively, complete the proof.\nLemma 9. Let k \u2208 [K] and h \u2208 [H ]. Then under the events E and \u2126k,h the following hold for every x \u2208 S\nk\u2211\ni=1\nH\u22121\u2211\nj=h\n( V\u2217i,j+1 \u2212 V\u03c0i,j+1 ) \u2264 2H2Uk,h + 4H2 \u221a TkL, (46)\nk\u2211\ni=1\nI(xi,h = x) H\u22121\u2211\nj=h\n( V\u2217i,j+1 \u2212 V\u03c0i,j+1 ) \u2264 2H2Uk,h,x + \u221a HN \u2032k,h(x, a)L. (47)\nProof. We begin by the following sequence of inequalities:\nk\u2211\ni=1\nH\u22121\u2211\nj=h\nV\u2217i,j+1 \u2212 V\u03c0i,j+1 (I) \u2264 k\u2211\ni=1\nH\u22121\u2211\nj=h\nEy\u223cpi,j [( V \u2217i,j+1(y) )2 \u2212 ( V \u03c0ii,j+1(y) )2]\n= k\u2211\ni=1\nH\u22121\u2211\nj=h\nEy\u223cpi,j [ (V \u2217j+1(y)\u2212 V \u03c0ij+1(y)(V \u2217j+1(y) + V \u03c0ij+1(y)) ]\n\u2264 2H k\u2211\ni=1\nH\u22121\u2211\nj=h\nEy\u223cpi,j ( V \u2217j+1(y)\u2212 V \u03c0ij+1(y) )\n\ufe38 \ufe37\ufe37 \ufe38 (a)\n, (48)\nwhere (I) is obtained from the definition of the variance as well as the fact that V \u2217i,j \u2265 V \u03c0k,h. The last line also follows from the fact that V \u03c0k \u2264 V \u2217h \u2264 H .\nUsing an identical argument we can also prove the following bound for state-dependent difference:\nk\u2211\ni=1\nI(xi,h = x)\nH\u22121\u2211\nj=h\nV\u2217i,j+1 \u2212 V\u03c0i,j+1 \u2264 2H k\u2211\ni=1\nI(xi,h = x)\nH\u22121\u2211\nj=h\nEy\u223cpi,j ( V \u2217j+1(y)\u2212 V \u03c0ij+1(y) )\n\ufe38 \ufe37\ufe37 \ufe38 (b)\n, (49)\nTo bound (a) we use the fact that under the event E the event Eaz(F\u2206\u0303,k,h, H) also holds. This combined with the fact that under the event \u2126k,h the inequality \u03b4k,h \u2264 \u03b4\u0303k,h holds implies that\n(a) \u2264 k\u2211\ni=1\nH\u22121\u2211\nj=h\n\u03b4\u0303i,j+1 + 2H \u221a TkL\n\u2264 HU1,h + 2H \u221a TkL, (50)\nwhere in the last line we rely on the result of Lem. 7. Similarly we can prove the following bound for (b) under the events \u2126k,h and Eaz(F\u2206\u0303,k,h,x, H):\n(b) \u2264 k\u2211\ni=1\nI(xi,h = x)\nH\u22121\u2211\nj=h\n\u2206\u0303i,j+1 + 2H 1.5 \u221a N \u2032k,h(x)L\n\u2264 HUk,h,x + 2H1.5 \u221a N \u2032k,h(x)L. (51)\nThe result then follows by incorporating the results of Eq. 50 and Eq. 51 into Eq. 48 and Eq. 49, respectively.\nLemma 10. Let k \u2208 [K] and h \u2208 [H ]. Then under the events E and \u2126k,h the following hold for every x \u2208 S\nk\u2211\ni=1\nH\u22121\u2211\nj=h\nV\u0302i,j+1 \u2212 V\u03c0i,j+1 \u2264 2H2Uk,1 + 7H2S \u221a ATkL, (52)\nk\u2211\ni=1\nI(xi,h = x) H\u22121\u2211\nj=h\nV\u0302i,j+1 \u2212 V\u03c0i,j+1 \u2264 2H2Uk,h,x + 7H2S \u221a HAN \u2032k,h(x)L. (53)\nProof. Here we only prove the bound on Eq. 52. The proof for the bound of Eq. 53 can be done in a very similar manner, as it is shown in the previous lemmas (the only difference is that HN \u2032k,h(x) and Uk,h,x replace Tk and Uk,1, respectively). The following sequence of inequalities hold:\nk\u2211\ni=1\nH\u22121\u2211\nj=h\nV\u0302i,j+1 \u2212 V\u03c0i,j+1 (I) \u2264 k\u2211\ni=1\nH\u22121\u2211\nj=h\nEy\u223cp\u0302i,j (Vi,j+1(y)) 2 \u2212 Ey\u223cpi,j ( V \u03c0ij+1(y) )2\n= k\u2211\ni=1\nH\u22121\u2211\nj=h\nEy\u223cp\u0302i,j (Vi,j+1(y)) 2 \u2212\nH\u22121\u2211\nj=1\nEy\u223cpi,j (Vi,j+1(y)) 2\n\ufe38 \ufe37\ufe37 \ufe38 (a)\n+ k\u2211\ni=1\nH\u22121\u2211\nj=h\nEy\u223cpi,j [ (Vi,j+1(y)) 2 \u2212 ( V \u03c0ij+1(y) )2]\n\ufe38 \ufe37\ufe37 \ufe38 (b)\n, (54)\nwhere to prove (I) we rely on the definition of the variance as well as the assumption that Vi,j \u2265 V \u03c0ij due to the event \u2126k,h. We now bound (a):\n(a) (I) \u2264 k\u2211\ni=1\nH\u22121\u2211\nj=h\n2H2\n\u221a SL\nnk,h\n(II) \u2264 3H2S \u221a ATkL,\nwhere (I) holds under the event E and (II) holds due to the pigeon-hole argument (see, e.g., Jaksch et al., 2010a, for the proof).\nUsing an identical analysis to the one in Lem. 10 and taking into account that Vi,j \u2265 V \u2217j under the event\u2126k,h and E we can bound (b)\n(b) (I) \u2264 2H   k\u2211\ni=1\nH\u22121\u2211\nj=h\n\u03b4\u0303i,j+1 + 2H \u221a TkL)   \u2264 2H2 ( Uk,1 + 4 \u221a TkL ) ,\nwhere (I) holds since under the event E the event Eaz(F\u2206\u0303,k,h, H) holds. Combining (a) and (b) to bound Eq. 54 proves the result.\nWe now bound Ck,h and Ck,h,x:\nLemma 11. Let k \u2208 [K] and h \u2208 [H ]. Then under the events E and \u2126k,h the following hold for every x \u2208 S\nCk,h \u2264 4 \u221a HSATk + 4 \u221a H2Uk,1SAL2, (55)\nCk,h,x \u2264 4 \u221a H2SANk,h(x) + 4 \u221a H2Uk,h,xSAL2. (56)\nProof. Here we only prove the bound on Eq. 55. The proof for the bound of Eq. 56 can be done in a very similar manner, as it is shown in the previous lemmas (the only difference is that HN \u2032k,h(x) and Uk,h,x replace Tk and Uk,1, respectively). The Cauchy\u2013Schwarz inequality leads to the following sequence of inequalities:\nCk,h =\nk\u2211\ni=1\nI(j \u2208 [k]typ)\n  H\u22121\u2211\nj=h\n2\n\u221a V\u2217i,j+1L\nni,j +\n4HL\n3ni,j\n \n\u2264 2 \u221a L \u221a\u221a\u221a\u221a\u221a\u221a\u221a k\u2211 i=1 H\u22121\u2211 j=h V\u2217i,j+1\n\ufe38 \ufe37\ufe37 \ufe38 (a)\n\u221a\u221a\u221a\u221a\u221a\u221a\u221a k\u2211 i=1 I(i \u2208 [k]typ) H\u22121\u2211 j=h 1\nni,j \ufe38 \ufe37\ufe37 \ufe38\n(b)\n+\nk\u2211\ni=1\nI(i \u2208 [k]typ) H\u22121\u2211\nh=j\n4HL 3ni,j (57)\nWe now prove bounds on (a) and (b) respectively\n(a) =\nk\u2211\ni=1\nH\u22121\u2211\nj=h\nV\u03c0i,j+1\n\ufe38 \ufe37\ufe37 \ufe38 (c)\n+\nk\u2211\ni=1\nH\u22121\u2211\nj=h V\u2217i,j+1 \u2212 V\u03c0i,j+1 \ufe38 \ufe37\ufe37 \ufe38\n(d)\n. (58)\n(c) and (d) can be bounded under the event E and \u2126k,h using the results of Lem. 8 and Lem.9. We then deduce\n(a) \u2264 HTk + 2H2Uk,1 + 6H2 \u221a TkL+ 4H2L\n3\n\u2264 2HTk + 2H2Uk,1, where the last line follows by the fact that for the typical episodes Tk \u2265 75H2S2AL2. Thus if Tk \u2264 H2L the term Ck,h trivially equals to 0 otherwise the higher order terms are bounded by O(HTk).\nWe now bound (b) using a pigeon-hole argument\n(b) \u2264 2 \u2211\n(x,a)\u2208S\u00d7A\nNk(x,a)\u2211\nn=1\n1 n \u2264 2SA\nT\u2211\nn=1\n1 n \u2264 2SA log(3T ).\nPlugging the bound on (a) and (b) into Eq. 57 and taking in to account that for the typical episodes [k]typ we have that T \u2265 H2L completes the proof.\nWe now boundBk,h:\nLemma 12. Let k \u2208 [K] and h \u2208 [H ]. Let the bonus is defined according to Algo. 4. Then under the events E and \u2126k,h the following hold for every x \u2208 S,\nBk,h \u2264 10L \u221a TkHSA+ 7 \u221a H2SAL2Uk,1 + 390H 2S2AL2, (59)\nBk,h,x \u2264 10L \u221a N \u2032k,h(x)HSA+ 7 \u221a H2SAL2Uk,h,x + 390H 2S2AL2, (60)\nProof. Here we only prove the bound on Eq. 59. The proof for the bound of Eq. 60 can be done in a very similar manner, as it is shown in the previous lemmas (the only difference is that HN \u2032k,h(x) and Uk,h,x replace Tk and Uk,1, respectively). We first notice that the following holds:\nBk,h \u2264 k\u2211\ni=1\nI(i \u2208 [k]typ) H\u22121\u2211\nj=h\n\u221a 8V\u0302i,j+1L\nni,j \ufe38 \ufe37\ufe37 \ufe38\n(a)\n+L\nk\u2211\ni=1\nI(i \u2208 [k]typ) H\u22121\u2211\nj=h\n  \u221a\u221a\u221a\u221a 8\nni,j\n\u2211 y\u2208S p\u0302i,j(y)min\n( 652S2H2AL2\nN \u2032i,j+1(y) , H2\n) \n\ufe38 \ufe37\ufe37 \ufe38 (b)\n.\nWe first note that the bound on Bk,h is similar to the bound on Ck,h. The main difference (beside the difference in H.O.Ts)\nis that here V\u2217h+1 is replaced by V\u0302i,j+1. So in our proof we first focus on dealing with this difference. The Cauchy\u2013Schwarz inequality leads to:\n(a) \u2264 \u221a 8L \u221a\u221a\u221a\u221a\u221a\u221a\u221a k\u2211 i=1 H\u22121\u2211 j=h V\u0302i,j+1(x, a)\n\ufe38 \ufe37\ufe37 \ufe38 (c)\n\u221a\u221a\u221a\u221a\u221a\u221a\u221a k\u2211 i=1 I(k \u2208 [k]typ) H\u22121\u2211 j=h 1\nni,j \ufe38 \ufe37\ufe37 \ufe38\n(d)\n,\nThe bound on (d) is identical to the corresponding bound in Lem. 11. So we only focus on bounding (c):\n(c) =\nk\u2211\ni=1\nH\u22121\u2211\nj=h\nV\u03c0i,j+1\n\ufe38 \ufe37\ufe37 \ufe38 (e)\n+\nk\u2211\ni=1\nH\u22121\u2211\nj=h V\u0302i,j+1 \u2212 V\u03c0i,j+1 \ufe38 \ufe37\ufe37 \ufe38\n(f)\n. (61)\n(e) and (f) can be bounded in high probability using the results of Lem. 8 and Lem.10. This implies\n(c) \u2264 HTk + 3H2Uk,1 + 9H2S \u221a ATkL+ 4H2L\n3\n\u2264 2HTk + 3H2Uk,1,\nwhere the last line follows by the fact that for the typical episodes Tk \u2265 100H2S2AL. Thus if Tk \u2264 100H2S2L then Bk,h trivially equals to 0 otherwise the higher order terms are bounded by O(HT ). Combining the bound on (b) and (c) leads to the following bound on (a):\n(a) \u2264 6L \u221a HSATk + 7HL \u221a SAUk,1.\nTo bound (b) we make use of Cauchy-Schwarz inequality again.\n(b) \u2264 \u221a\u221a\u221a\u221a\u221a\u221a\u221a k\u2211 i=1 I(i \u2208 [k]typ) H\u22121\u2211 j=h \u2211 y\u2208S p\u0302i,j(y)b \u2032 i,j+1(y)\n\ufe38 \ufe37\ufe37 \ufe38 (g)\nk\u2211\ni=1\nH\u22121\u2211\nj=h\nI(i \u2208 [k]typ) ni,j\n\ufe38 \ufe37\ufe37 \ufe38 (h)\n.\nThe term (h) bounded by 2SAL using a pigeon-hole argument (see Lem. 11). We proceed by bounding (g):\n(g) \u2264 k\u2211\ni=1\nH\u22121\u2211\nj=h (p\u0302i,j \u2212 pi,j)b\u2032i,j+1 \ufe38 \ufe37\ufe37 \ufe38\n(i)\n+ k\u2211\ni=1\nH\u22121\u2211\nj=h\n(pi,jb \u2032 V \u2212 b\u2032i,j+1(xi,j+1))\n\ufe38 \ufe37\ufe37 \ufe38 (j)\n+ k\u2211\ni=1\nI(i \u2208 [k]typ) H\u22121\u2211\nj=h\nb\u2032i,j+1(xi,j+1)\n\ufe38 \ufe37\ufe37 \ufe38 k\n.\nGiven that the event E holds the term (i) bounded by 2 \u221a 2H2S \u221a ALTk by using the pigeon-hole argument. Under the event\nE the event Eaz(Fb\u2032,k,h, H2) holds. This implies that the term (j) is also bounded by 2H2 \u221a TkL as it is sum of the martingale differences. The term (k) is also bounded by 20000H3S3A3L3 using the pigeon-hole argument. Combining all these bounds together leads to the following bound on (b)\n(b) \u2264 \u221a 16 \u221a 2H2S2 \u221a TkAL3 + 16H2SA \u221a TkL3 + 136000S4H4A2L3.\nCombining this with the bound on (a) and taking into account the fact that we only bound the Bk,h for the typical episodes, in which Tk \u2265 100H2S2AL2, completes the proof.\nLemma 13. Let the bonus is defined according to Algo. 4. Then under the events E and \u2126K,1 the following hold\nRegret(K) \u2264 R\u0303egret(K) \u2264 UK,1 \u2264 14L \u221a HSAT + 11HL \u221a SAUk,1 + 490H 2S2A2L+ 2H \u221a TL. (62)\nProof. We first notice that Regret(K) and Regret(K) are bounded by Uk,1 due to Lem.6. To bound Uk,1 we sum up the regret due to Bk,h and Ck,h from Lem. 11 and Lem. 12. We also bound the sum \u2211K\nk=1 \u2211H h=1 c4,k,h by 2HSAL using a pigeon hole\nargument. We also note that Bk,h and Ck,h only account for the regret of typical episodes in which T \u2265 H2S2A2L. The regret of those episodes which do not belong to the typical set [k]typ, can be bounded by O(H 2S2A2L2), trivially.\nThe following lemma establishes an explicit bound on the regret:\nLemma 14. Let the bonus is defined according to Algo. 4. Then under the events E and \u2126K,1 the following hold\nRegret(K) \u2264 R\u0303egret(K) \u2264 UK,1 \u2264 28L \u221a HSAT + 1225H2S2AL2 + 4H \u221a TL. (63)\nProof. The proof follows by solving the bound of Lem. 13 in terms of Uk,1. which only contributes to the additional regret of O(H2L2SA)\nLemma 15. Let the bonus is defined according to Algo. 3. Then under the events E and \u2126K,1 the following holds\nRegret(K) \u2264 R\u0303egret(K) \u2264 UK,1 \u2264 20H \u221a SATL+ 100H2S2AL2. (64)\nProof. The proof up to Lem. 11 is identical to the proof of Lem. 14. The main difference is to prove bound on Ck,h and Bk,h\nhere we use a loose bound ofO(H \u221a\nSAL nk,h )for both exploration bonus bk,h and the confidence interval c1,k,h and then sum these\nterms using a pigeon-hole argument (The proof is provided in Jaksch et al., 2010a) which leads to a bound of O(H \u221a SATL) on both BK,1 and CK,1. Plugging these results into the bound of Lem. 7 combined with the regret of non-typical episodes complete the proof\nLemma 16. Let the bonus is defined according to Algo. 4. Let k \u2208 [K] and h \u2208 [H ]. Then under the events E and \u2126k,h the following hold for every x \u2208 S,\nRegret(k, x, h) \u2264 R\u0303egret(k, x, h) \u2264 28HL \u221a SAN \u2032k,h(x) + 1225H 2S2AL2 + 4H1.5 \u221a N \u2032k,h(x)L\n\u2264 65H1.5SL \u221a AN \u2032k,h(s).\nProof. The proof is similar to the proof of total regret. Here also we use Lem. 12, Lem. 11 and a pigeon-hole argument to bound the regrets due to Bk,h, Ck,h and c4,k,h. We then incorporate these terms into Lem.6 to bound the regret in terms of Uk,h,x. The result follows by solving the bound w.r.t. the upper bound Uk,h,x.\nLemma 17. Let the bonus b is defined according to Algo. 4. Let k \u2208 [K] and h \u2208 [H ]. Then under the events E and \u2126k,h the following hold for every x \u2208 S\nVk,h(x)\u2212 V \u2217h (x) \u2264 65 \u221a H3S2AL2\nN \u2032k,h(s) .\nProof. From Lem. 16 we have that\n65H1.5SL \u221a AN \u2032k,h(s)\n\u2265 k\u2211\ni=1\nI(xi,h = x)(Vi,h(x)\u2212 V \u03c0ih (x))\n\u2265 (Vk,h(x) \u2212 V \u2217h (x)) k\u2211\ni=1\nI(xi,h = x) = N \u2032 k,h(x)(Vk,h(x) \u2212 V \u2217h (x)),\nwhere the last inequality holds due to the fact that Vk,h by definition is monotonically non-increasing in k. The proof then follows by collecting terms.\nLemma 18. Let the bonus b is defined according to Algo. 3. Then under the event E the set of events {\u2126k,h}k\u2208[K],h\u2208H hold.\nProof. We prove this result by induction. First we notice that for h = H by definition Vk,h = V \u2217 h thus the inequality Vk,h \u2265 V \u2217h trivially holds. Thus to prove this result for h < H we only need to show that if the inequality Vk,h \u2265 V \u2217h holds for h it also holds for h\u2212 1 for every h < H :\nVk,h(x)\u2212 V \u2217h (x) = TkVk,h\u22121(x)\u2212 T V \u2217(x) \u2265 bk(x, \u03c0\u2217h(x)) + P\u0302 \u03c0 \u2217 k,hVk,h+1(x) \u2212 P \u03c0 \u2217 h Vk,h+1(x)\n= bk(x, \u03c0 \u2217 h(x)) + P\u0302 \u03c0\u2217 k,h(Vk,h+1 \u2212 V \u2217h+1)(x) + (P\u0302 \u03c0 \u2217 k,h \u2212 P \u03c0 \u2217 h )V \u2217 h+1(x)\n\u2265 bk(x, \u03c0\u2217h(x)) + (P\u0302 \u03c0 \u2217 k,h \u2212 P \u03c0 \u2217 h )V \u2217 h+1(x),\nwhere the last line follows by the induction condition that Vk,h+1 \u2265 V \u2217h+1. The fact that the event E hols implies that (P \u03c0 \u2217 h \u2212 P\u0302 \u03c0 \u2217\nk,h)V \u2217 h+1(x) \u2264 c1(Nk(x, \u03c0\u2217h(x))) \u2264 bk(x, \u03c0\u2217h(x)), which completes the proof.\nLemma 19. Let the bonus b is defined according to Algo. 4. Then under the event E the set of events {\u2126k,h}k\u2208[K],h\u2208H hold.\nProof. We prove this result by induction. We first notice that in the case of the first episode V1,h = H \u2265 V \u2217h . To prove this result by induction in the case of 1 < k \u2208 [K] we need to show that in the case of h \u2208 [H \u2212 1] if \u2126k,h+1 holds then \u2126k,h also holds. If \u2126k,h\u22121 holds then Vi,j \u2265 V \u2217j for every (i, j) \u2208 [k, h]hist. We can then invoke the result of Lem. 17 which implies\nVk,h+1(x)\u2212 V \u2217h+1(x) \u2264 65 H1.5SL \u221a A\u221a\nN \u2032k,h+1(x) .\nUsing this result which guarantees that Vk,h+1 is close to V \u2217 h+1 we prove that Vk,h \u2212 V \u2217h \u2265 0, that is the event \u2126k,h holds.\nVk,h \u2212 V \u2217h = min(Vk\u22121,h, Tk,hVi,j+1, H)\u2212 V \u2217h\nIf Vk\u22121,h \u2264 Tk,hVi,j+1 the result Vk,h \u2212 V \u2217h = Vk\u22121,h \u2212 V \u2217h \u2265 0 holds trivially. Also if Vk\u22121,h \u2265 H the result trivially holds. So we only need to consider the case that Tk,hVi,j+1 \u2264 Vk\u22121,h \u2264 H in that case we have w\nVk,h(x) \u2212 V \u2217h (x) \u2265 Tk,hVi,j+1(x)\u2212 T V \u2217h+1(x) (I)\n\u2265 bk,h(x, \u03c0\u2217(x, h)) + P\u0302 \u03c0 \u2217 h Vi,j+1(x)\u2212 P \u03c0\u2217h V \u2217h+1(x) = bk,h(x, \u03c0 \u2217(x, h)) + (P\u0302 \u03c0 \u2217 h \u2212 P \u03c0 \u2217 h )V \u2217 h+1(x) + P\u0302 \u03c0\u2217 h (Vi,j+1 \u2212 V \u2217h+1)(x)\n(II)\n\u2265 bk,h(x, \u03c0\u2217(x, h)) + (P\u0302 \u03c0 \u2217 h \u2212 P \u03c0 \u2217 h )V \u2217 h+1(x),\nwhere in (I) we rely on the fact that \u03c0k,h is the greedy policy w.r.t. Vk,h. Thus\nbk,h(x, \u03c0 \u2217(x, h)) + P\u0302 \u03c0 \u2217 h Vi,j+1(x) \u2264 bk,h(x, \u03c0k(x, h)) + P\u0302 \u03c0kh Vi,j+1(x).\nAlso (II) follows from the induction assumption. Under the event E we have\nVk,h \u2212 V \u2217h \u2265 bk,h \u2212 c1(V\u0302\u2217h, Nk)\n\u2265\n\u221a 8V\u0302k,hL\nNk \u2212 2\n\u221a V\u0302\u2217hL\nNk\ufe38 \ufe37\ufe37 \ufe38 (a)\n\u2212 14L 3Nk\n+\n\u221a\u221a\u221a\u221a P\u0302k [ 8min ( 652H3S2A2L2\nN \u2032 k,h+1\n, H2 )]\nNk +\n14L 3Nk .\nWe now prove a lower bound on (a):\n(a) \u2265    \u2212 \u221a 4V\u0302\u2217k,h \u2212 8V\u0302k,h Nk V\u0302k,h \u2264 V\u2217,\n0 otherwise.\nWe proceed by bounding V\u0302\u2217k,h in terms of V\u0302k,h from above:\nV\u0302\u2217k,h (I)\n\u2264 2V\u0302k,h + 2Vary\u223cP\u0302k(Vk,h+1(y)\u2212 V \u2217 h+1(y)) \u2264 2V\u0302k,h + 2 P\u0302k(Vk,h+1 \u2212 V \u2217h+1)2\ufe38 \ufe37\ufe37 \ufe38\n(b)\n,\nwhere (I) is an application of Lem. 2. We now bound (b). Combining this result with the result of Eq. 65 leads to the following bound on (a)\n(a) \u2265    \u2212 \u221a\u221a\u221a\u221a8P\u0302k [ min ( 652H3S2AL2 N \u2032 k,h+1 , H2 )] Nk V\u0302k,h \u2264 V\u0302\u2217,\n0 otherwise,\nwhere the last inequality holds under the event E . The proof is completed by plugging (a) and (b) into Eq. 65 which proves that Vk,h \u2265 V \u2217h thus the event \u2126k,hholds."}, {"heading": "B.1 Proof of Thm. 1", "text": "The result is a direct consequence of Lem. 18 and Lem. 15 and the fact that the high probability event E holds w.p. 1\u2212 \u03b4."}, {"heading": "B.2 Proof of Thm. 2", "text": "The result is a direct consequence of Lem. 19 and Lem. 14 and the fact that the high probability event E holds w.p. 1\u2212 \u03b4."}], "references": [{"title": "Minimax pac bounds on the sample complexity of reinforcement learning", "author": ["M.G. Azar", "R. Munos", "H.J. Kappen"], "venue": null, "citeRegEx": "Azar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2013}, {"title": "REGAL: A regularization based algorithm for reinforcement learning in weakly commu", "author": ["P.L. Bartlett", "A. Tewari"], "venue": null, "citeRegEx": "Bartlett and Tewari,? \\Q2009\\E", "shortCiteRegEx": "Bartlett and Tewari", "year": 2009}, {"title": "R-max - a general polynomial time algorithm for near-optimal reinforcement", "author": ["D.P. edition. Bertsekas", "J.N. Tsitsiklis"], "venue": "Neuro-Dynamic Programming", "citeRegEx": "Bertsekas and Tsitsiklis,? \\Q1996\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1996}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bubeck and Cesa.Bianchi,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi", "year": 2012}, {"title": "Optimal adaptive policies for markov decision processes", "author": ["A.N. 12:1587\u20131627. Burnetas", "M.N. Katehakis"], "venue": null, "citeRegEx": "Burnetas and Katehakis,? \\Q1997\\E", "shortCiteRegEx": "Burnetas and Katehakis", "year": 1997}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Operations Research,", "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "On tail probabilities for martingales", "author": ["D.A. TBA. Freedman"], "venue": "Information Processing Systems,", "citeRegEx": "Freedman,? \\Q1975\\E", "shortCiteRegEx": "Freedman", "year": 1975}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["T. Jaksch", "R. Ortner", "P. Auer"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["T. Jaksch", "R. Ortner", "P. Auer"], "venue": "Learning Research,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "Finite-sample convergence rates for Q-learning and indirect algorithms", "author": ["M. Kearns", "S. Singh"], "venue": "Learning Research,", "citeRegEx": "Kearns and Singh,? \\Q1999\\E", "shortCiteRegEx": "Kearns and Singh", "year": 1999}, {"title": "PAC bounds for discounted MDPs", "author": ["T. 3):209\u2013232. Lattimore", "M. Hutter"], "venue": null, "citeRegEx": "Lattimore and Hutter,? \\Q2012\\E", "shortCiteRegEx": "Lattimore and Hutter", "year": 2012}, {"title": "more) efficient reinforcement learning via posterior sampling", "author": ["I. Osband", "D. Russo", "B. Van Roy"], "venue": null, "citeRegEx": "Osband et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2013}, {"title": "On lower bounds for regret in reinforcement learning. stat, 1050:9", "author": ["I. Osband", "B. Van Roy"], "venue": "Neural Information Processing Systems,", "citeRegEx": "Osband and Roy,? \\Q2016\\E", "shortCiteRegEx": "Osband and Roy", "year": 2016}, {"title": "PAC model-free reinforcement learning", "author": ["A.L. Strehl", "L. Li", "E. Wiewiora", "J. Langford", "M.L. Littman"], "venue": null, "citeRegEx": "Strehl et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2006}, {"title": "A theoretical analysis of model-based interval estimation", "author": ["A.L. Strehl", "M.L. Littman"], "venue": null, "citeRegEx": "Strehl and Littman,? \\Q2005\\E", "shortCiteRegEx": "Strehl and Littman", "year": 2005}, {"title": "A Bayesian framework for reinforcement learning", "author": ["M.J.A. ACM. Strens"], "venue": "international conference on Machine learning,", "citeRegEx": "Strens,? \\Q2000\\E", "shortCiteRegEx": "Strens", "year": 2000}, {"title": "Inequalities for the l1 deviation", "author": ["T. Weissman", "E. Ordentlich", "G. Seroussi", "S. Verdu", "M.J. Weinberger"], "venue": null, "citeRegEx": "Weissman et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Weissman et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 7, "context": "This result improves over the best previous known bound \u00d5(HS \u221a AT ) achieved by the UCRL2 algorithm Jaksch et al. (2010a). The key significance of our new results is that when T \u2265 HSA and SA \u2265 H , it leads to a regret of \u00d5( \u221a HSAT ) that matches the established lower bounds of \u03a9( \u221a HSAT ) up to a logarithmic factor.", "startOffset": 100, "endOffset": 122}, {"referenceID": 4, "context": "1 Introduction We consider the reinforcement learning (RL) problem of an agent interacting with an environment in order to maximize its cumulative rewards through time (Burnetas and Katehakis, 1997; Sutton and Barto, 1998).", "startOffset": 168, "endOffset": 222}, {"referenceID": 13, "context": "Almost all reinforcement learning algorithms with polynomial bounds on sample complexity employ optimism to guide exploration (Kearns and Singh, 2002; Brafman and Tennenholtz, 2002; Strehl et al., 2006; Jaksch et al., 2010a).", "startOffset": 126, "endOffset": 224}, {"referenceID": 15, "context": "The algorithm posterior sampling for reinforcement learning (PSRL) maintains a posterior distribution for MDPs and, at each episode of interaction, follows a policy which is optimal for a single random sample (Strens, 2000).", "startOffset": 209, "endOffset": 223}, {"referenceID": 11, "context": "Recent work has established Bayesian regret bounds for PSRL and even argues for the potential benefits of sampling-based methods over existing optimistic approaches (Osband et al., 2013; Osband and Van Roy, 2016b).", "startOffset": 165, "endOffset": 213}, {"referenceID": 14, "context": "Our algorithm, upper confidence bound value iteration (UCBVI) is similar to model-based interval estimation (MBIE-EB) (Strehl and Littman, 2005) with a delicate alteration to the form of the \u201cexploration bonus\u201d.", "startOffset": 118, "endOffset": 144}, {"referenceID": 1, "context": "This positive result is the first of its kind and helps to address an ongoing question about where the fundamental lower bounds lie for reinforcement learning in finite horizon MDPs (Bartlett and Tewari, 2009; Dann and Brunskill, 2015; Osband and Van Roy, 2016a).", "startOffset": 182, "endOffset": 262}, {"referenceID": 6, "context": "Careful application of the Bernstein and Freedman inequalities (Bernstein, 1927; Freedman, 1975) to the concentration of the optimal value function directly, rather than building confidence sets for the transitions probabilities and rewards, like in UCRL2 (Jaksch et al.", "startOffset": 63, "endOffset": 96}, {"referenceID": 1, "context": "First, we study the setting of episodic, finite horizon MDPs and not the more general setting of weakly communicating systems (Bartlett and Tewari, 2009; Jaksch et al., 2010a).", "startOffset": 126, "endOffset": 175}, {"referenceID": 0, "context": "The use of exploration bonuses based on Bernstein\u2019s inequality, and a recursive Bellman-type Law of Total Variance (LTV) to prove tight bounds on the expected sum of the variances of the value estimates, in a similar spirit to the analysis from Azar et al. (2013); Lattimore and Hutter (2012).", "startOffset": 245, "endOffset": 264}, {"referenceID": 0, "context": "The use of exploration bonuses based on Bernstein\u2019s inequality, and a recursive Bellman-type Law of Total Variance (LTV) to prove tight bounds on the expected sum of the variances of the value estimates, in a similar spirit to the analysis from Azar et al. (2013); Lattimore and Hutter (2012). At a high level, this work addresses the noted shortcomings of existing algorithms for optimistic RL (Osband and Van Roy, 2016b) and demonstrates (contrary to previous assertions) that it is possible to design a simple and computationally efficient optimistic algorithm that does not suffer from these flaws when T is sufficiently large.", "startOffset": 245, "endOffset": 293}, {"referenceID": 2, "context": "MarkovDecision ProblemsWe consider the problem of undiscounted episodic reinforcement learning (RL) (Bertsekas and Tsitsiklis, 1996), where an RL agent interacts with a stochastic environment and this interaction is modeled as a discrete-time MDP.", "startOffset": 100, "endOffset": 132}, {"referenceID": 14, "context": "This algorithm is very related to the model based interval estimation (MBIE-EB) algorithms (Strehl and Littman, 2005).", "startOffset": 91, "endOffset": 117}, {"referenceID": 0, "context": "We then use the fact that the sum of these variances is bounded by the variance of the return (in a spirit similar to (Munos and Moore, 1999; Azar et al., 2013; Lattimore and Hutter, 2012)), which proves that the estimation errors accumulate as \u221a H instead of linearly inH , thus implying the improvedH-dependence.", "startOffset": 118, "endOffset": 188}, {"referenceID": 10, "context": "We then use the fact that the sum of these variances is bounded by the variance of the return (in a spirit similar to (Munos and Moore, 1999; Azar et al., 2013; Lattimore and Hutter, 2012)), which proves that the estimation errors accumulate as \u221a H instead of linearly inH , thus implying the improvedH-dependence.", "startOffset": 118, "endOffset": 188}, {"referenceID": 6, "context": "Theorem 1 is significant in that, for large T , it improves the regret dependence from S to \u221a S, compared to the best known bound of Jaksch et al. (2010b). The main intuition for this improved S-dependence is that we bound the estimation error of the next-state value function directly, instead of the transition probabilities.", "startOffset": 133, "endOffset": 155}, {"referenceID": 6, "context": "Theorem 1 is significant in that, for large T , it improves the regret dependence from S to \u221a S, compared to the best known bound of Jaksch et al. (2010b). The main intuition for this improved S-dependence is that we bound the estimation error of the next-state value function directly, instead of the transition probabilities. More precisely, instead of bounding the estimation error (P\u0302 \u03c0k k \u2212P k)Vk,h+1 by \u2016P\u0302 \u03c0k k \u2212P k\u20161\u2016Vk,h+1\u2016\u221e (as is done in Jaksch et al. (2010b) for example), we bound (P\u0302 \u03c0k k \u2212 P k)V \u2217 h+1 instead (for which a bound with no dependence on S can be achieved since V \u2217 is deterministic) and handle carefully the correction term (P\u0302 \u03c0k k \u2212 P k)(Vk,h+1 \u2212 V \u2217 h+1).", "startOffset": 133, "endOffset": 471}, {"referenceID": 9, "context": "In fact the computational cost of this algorithm is of the same order as that of standard model-based value iteration, which is of \u00d5(HSAmin(T, S)) at every update (Kearns and Singh, 1999).", "startOffset": 163, "endOffset": 187}, {"referenceID": 7, "context": "This technique is common to the literature involves extending the period of fixed optimistic estimates as more data is gathered Jaksch et al. (2010b); Dann and Brunskill (2015).", "startOffset": 128, "endOffset": 150}, {"referenceID": 7, "context": "This technique is common to the literature involves extending the period of fixed optimistic estimates as more data is gathered Jaksch et al. (2010b); Dann and Brunskill (2015). The computational cost of this variant of UCBVI then amounts to \u00d5(SA min(T, S)) as it only needs to update the model \u00d5(SA) times (Jaksch et al.", "startOffset": 128, "endOffset": 177}, {"referenceID": 7, "context": "This technique is common to the literature involves extending the period of fixed optimistic estimates as more data is gathered Jaksch et al. (2010b); Dann and Brunskill (2015). The computational cost of this variant of UCBVI then amounts to \u00d5(SA min(T, S)) as it only needs to update the model \u00d5(SA) times (Jaksch et al., 2010a). We can use similar techniques to further improve the computational complexity of both UCBVI_1 and UCBVI_2 but omit these modifications for clarity in an (already complicated) analysis. In comparison with UCRL2 both variants of our algorithms are up to S-times less computationally expensive than the previous state of the art , despite the improvement in regret scaling for large T . The reason for this improved computational efficiency comes from the structure of UCBVI. Since both algorithms design confidence sets upon the optimal value function directly, rather than the underlying estimates for rewards and transitions, they avoid the need for the computationally intensive extended value iteration Jaksch et al. (2010b).", "startOffset": 128, "endOffset": 1058}, {"referenceID": 7, "context": "In Jaksch et al. (2010b), this issue is addressed by bounding it as \u2016P\u0302 \u03c0k k \u2212 P k\u20161\u2016Vk,h+1\u2016\u221e at the price of an additional \u221a S.", "startOffset": 3, "endOffset": 25}, {"referenceID": 7, "context": "In Jaksch et al. (2010b), this issue is addressed by bounding it as \u2016P\u0302 \u03c0k k \u2212 P k\u20161\u2016Vk,h+1\u2016\u221e at the price of an additional \u221a S. The main contribution of our \u00d5(H \u221a SAT ) bound (which removes a \u221a S factor compared to the previous bound of Jaksch et al. (2010b)) is to handle this term more properly.", "startOffset": 3, "endOffset": 260}, {"referenceID": 7, "context": "Using similar argument as those used in Jaksch et al. (2010b), we have that ak,h \u2264 H2\u2016P\u0302 \u03c0k \u2212 P k\u20161 \u2264 H \u221a SL/nk,h (where nk,h def = Nk(xk,h, \u03c0k(xk,h))).", "startOffset": 40, "endOffset": 62}, {"referenceID": 0, "context": "A recursive application of the law of total variance (see e.g., Munos and Moore, 1999; Azar et al., 2013; Lattimore and Hutter, 2012) shows that this quantity is nothing else than the variance of the return (sum of h rewards) under policy \u03c0k: V (\u2211 h r(xk,h, \u03c0k(xk,h)) ) , which is thus bounded by H.", "startOffset": 53, "endOffset": 133}, {"referenceID": 10, "context": "A recursive application of the law of total variance (see e.g., Munos and Moore, 1999; Azar et al., 2013; Lattimore and Hutter, 2012) shows that this quantity is nothing else than the variance of the return (sum of h rewards) under policy \u03c0k: V (\u2211 h r(xk,h, \u03c0k(xk,h)) ) , which is thus bounded by H.", "startOffset": 53, "endOffset": 133}, {"referenceID": 1, "context": "These results are particularly significant because they help to settle an ongoing debate over whether such bounds were even attainable, or whether the true lower bound should be \u03a9(H \u221a SAT ) Bartlett and Tewari (2009); Osband and Van Roy (2016a).", "startOffset": 190, "endOffset": 217}, {"referenceID": 1, "context": "These results are particularly significant because they help to settle an ongoing debate over whether such bounds were even attainable, or whether the true lower bound should be \u03a9(H \u221a SAT ) Bartlett and Tewari (2009); Osband and Van Roy (2016a).", "startOffset": 190, "endOffset": 245}, {"referenceID": 16, "context": "The following result also holds on l1-normed estimation error of the transition distribution (Weissman et al., 2003), combined with a union bound on Nk(x, a) \u2208 [T ] implies w.", "startOffset": 93, "endOffset": 116}, {"referenceID": 6, "context": "When the sum of the variances \u2211n i=1 Var(Xi|Fi) \u2264 w then the following sharper bound due to Freedman (1975) holds w.", "startOffset": 92, "endOffset": 108}], "year": 2017, "abstractText": "We consider the problem of efficient exploration in finite horizon MDPs. We show that an optimistic modification to modelbased value iteration, can achieve a regret bound \u00d5( \u221a HSAT+HSA+H \u221a T )whereH is the time horizon, S the number of states, A the number of actions and T the time elapsed. This result improves over the best previous known bound \u00d5(HS \u221a AT ) achieved by the UCRL2 algorithm Jaksch et al. (2010a). The key significance of our new results is that when T \u2265 HSA and SA \u2265 H , it leads to a regret of \u00d5( \u221a HSAT ) that matches the established lower bounds of \u03a9( \u221a HSAT ) up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in S), and we use \"exploration bonuses\" based on Bernstein\u2019s inequality, together with using a recursive -Bellman-typeLaw of Total Variance (to improve scaling in H).", "creator": "LaTeX with hyperref package"}}}