{"id": "1703.05820", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2017", "title": "Particle Value Functions", "abstract": "The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.\n\n\n\n\nIn a paper published in The Proceedings of the National Academy of Sciences, We report that the effects of the policy gradients of this objective on the reward value of a given agent's reward can be compared to a fixed probability of the probability of a given agent's performance at a given stage of training.\nThis is consistent with our approach to the reward-sensitivity function. It is suggested that we need to adjust our modeling of the probability distributions of agents to make it more efficient. In this paper, we propose that the goal of this approach is to simulate the probability distribution of agents to avoid any long-term effects that might otherwise occur. It is a major concern to recognize that when agents have been trained for specific behavioral and cognitive tasks, the expected return outcomes are expected to be more similar to those that were trained for specific behavioral and cognitive tasks.\nIt is interesting that the experimental effects of this objective could be interpreted as a positive positive or negative effect on the reward value. The effect of this objective on the reward value of a given agent's performance in terms of a fixed distribution (for example, the probability of a given agent's performance for a given task) has been observed in a sample. It is also noteworthy that the observed effect of the policy gradients of this objective is not necessarily independent of the model used in our model. A similar effect could be seen in the behavioral task at the time of training, in which the reward value is expected to be lower than the expectation of the reward value in the same task. In this case, a different test of the policy gradients of this objective might be observed. In the same scenario, we observe that, as the reward value has been decreased, the performance of the agent is expected to increase significantly with a given agent's performance at a given stage of training.\nThe value of the policy gradients of this objective could", "histories": [["v1", "Thu, 16 Mar 2017 21:08:31 GMT  (299kb,D)", "http://arxiv.org/abs/1703.05820v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["chris j maddison", "dieterich lawson", "george tucker", "nicolas heess", "arnaud doucet", "riy mnih", "yee whye teh"], "accepted": false, "id": "1703.05820"}, "pdf": {"name": "1703.05820.pdf", "metadata": {"source": "CRF", "title": "PARTICLE VALUE FUNCTIONS", "authors": ["Chris J. Maddison", "Dieterich Lawson", "George Tucker", "Nicolas Heess", "Arnaud Doucet", "Andriy Mnih", "Yee Whye Teh"], "emails": ["cmaddis@stats.ox.ac.uk"], "sections": [{"heading": "1 INTRODUCTION", "text": "The expected return objective dominates the field of reinforcement learning, but makes it difficult to express a tolerance for unlikely rewards. This kind of risk sensitivity is desirable, e.g., in real-world settings such as financial trading or safety-critical applications where the risk required to achieve a specific return matters greatly. Even if we ultimately care about the expected return, it may be beneficial during training to tolerate high variance in order to discover high reward strategies.\nIn this paper we introduce a risk-sensitive value function based on a system of interacting trajectories called a particle value function (PVF). This value function is amenable to large-scale reinforcement learning problems with nonlinear function approximation. The idea is inspired by recent advances in variational inference which bound the log marginal likelihood via importance sampling estimators (Burda et al., 2016; Mnih & Rezende, 2016), but takes an orthogonal approach to reward modifications, e.g. (Schmidhuber, 1991; Ng et al., 1999). In Section 2, we review risk sensitivity and a simple decision problem where risk is a consideration. In Section 3, we introduce a particle value function. In Section 4, we highlight its benefits on Cliffworld trained with policy gradients."}, {"heading": "2 RISK SENSITIVITY AND EXPONENTIAL UTILITY", "text": "We look at a finite horizon Markov Decision Process (MDP) setting where Rt is the instantaneous reward generated by an agent following a non-stationary policy \u03c0, see Appendix A. A utility function u : R \u2192 R is an invertible non-decreasing function, which specifies a ranking over possible returns \u2211T t=0Rt. The expected utility E[u( \u2211T t=0Rt)|S0 = s] specifies a ranking over policies (Von Neumann & Morgenstern, 1953). For an agent following u, a natural definition of the \u201cvalue\u201d of a state is the real number V \u03c0T (s, u) whose utility is the expected utility:\nV \u03c0T (s, u) = u \u22121\n( E [ u ( T\u2211 t=1 Rt ) \u2223\u2223\u2223\u2223\u2223 S0 = s ]) . (1)\nNote, when u is the identity we recover the expected return. We consider exponential utilities u(x) = sgn(\u03b2) exp(\u03b2x) where \u03b2 \u2208 R. This choice is well-studied, and it is implied by the assumption that V \u03c0T (s, u) is additive for deterministic translations of the reward function (Pratt, 1964; Howard & Matheson, 1972; Coraluppi, 1997). The corresponding value function is\nV \u03c0T (s, \u03b2) = 1\n\u03b2 logE\n[ exp ( \u03b2\nT\u2211 t=0 Rt ) \u2223\u2223\u2223\u2223\u2223 S0 = s ] , (2)\nar X\niv :1\n70 3.\n05 82\n0v 1\n[ cs\n.L G\n] 1\n6 M\nar 2\n01 7\nand as \u03b2 \u2192 0 we recover the expected return. See Appendix B for details. One way to interpret this value is through the following thought experiment. If the agent is given a choice between a single interaction with the environment and an immediate deterministic return, then V \u03c0T (s, \u03b2) represents the minimum return that our agent would take in exchange for forgoing an interaction. If \u03b2 < 0, then V \u03c0T (s, \u03b2) \u2264 V \u03c0T (s, 0), meaning that the agent is willing to to take a loss relative to the expected return in exchange for certainty. This is a risk-avoiding attitude, which emphasizes low returns. If \u03b2 > 0, then V \u03c0T (s, \u03b2) \u2265 V \u03c0T (s, 0), and the agent would only forgo an interaction for more than it can expect to receive. This is risk-seeking behavior, which emphasizes high returns.\nTo illustrate one effect of risk, consider the two state MDP shown in Figure 1. The agent begins in state 1 and acts for two time steps, choosing between leaving or remaining. Suppose that the agent\u2019s policy is defined by a single parameter p \u2208 [0, 1] that describes the probability of remaining. Then the expected return V p2 (1, 0) = 3 2p\n2 \u2212 2p + 1 has two local maxima at p \u2208 {0, 1} and the solution p = 1 is not a global maximum. Any policy gradient trajectory initialized with p > 2/3 will converge to the suboptimal solution p = 1, but as our risk appetite \u03b2 grows, the basin of attraction to the global maximum of p = 0 expands to the entire unit interval. This sort of state aliasing happens often in reinforcement learning with non-linear function approximation. In these cases, modifying the risk appetite (either towards risk-avoidance or seeking) may favorably modify the convergence of policy gradient algorithms, even if our ultimate objective is the expected return.\nThe risk-seeking variant may be helpful in deterministic environments, where an agent can exactly reproduce a previously experienced trajectory. Rare rewards are rare only due to our current policy, and it may be better to pursue high yield trajectories more aggressively. Note, however, that V \u03c0T (s, \u03b2) is non-decreasing in \u03b2, so in general risk-seeking is not guaranteed to improve the expected return. Note also that the literature on KL regularized control (Todorov, 2006; Kappen, 2005; Tishby & Polani, 2011) gives a different perspective on risk sensitive control, which mirrors the relationship between variational inference and maximum likelihood. See Appendix C for related work."}, {"heading": "3 PARTICLE VALUE FUNCTIONS", "text": "Algorithms for optimizing V \u03c0T (s, \u03b2) may suffer from numerical issues or high variance, see Appendix B. Instead we define a value function that bounds V \u03c0T (s, \u03b2) and approaches it in the infinite sample limit. We call it a particle value function, because it assigns a value to a bootstrap particle filter with K particles representing state-action trajectories. This is distinct, but related to Kantas (2009), which investigates particle filter algorithms for infinite horizon risk-sensitive control.\nBriefly, a bootstrap particle filter can be used to estimate normalizing constants in a hidden Markov model (HMM). Let (Xt, Yt) be the states of an HMM with transitions Xt \u223c p(\u00b7|Xt\u22121) and emissions Yt \u223c q(\u00b7|Xt). Given a sample y0 . . . yT , the probability p({Yt = yt}Tt=0) can be computed with the forward algorithm. The bootstrap particle filter is a stochastic procedure for the forward algorithm that avoids integrating over the state space of the latent variables. It does so by propagating a set of K particles X(i)t with the transition model X (i) t \u223c p(\u00b7|X (i) t\u22121) and a resampling step in propor-\ntion to the potentials q(yt|X(i)t ). The result is an unbiased estimator \u220fT t=0(K \u22121\u2211K i=1 q(yt|X (i) t )) of the desired probability (Del Moral, 2004; Pitt et al., 2012). The insight is that if we treat the state-action pairs (St, At) as the latents of an HMM with emission potentials exp(\u03b2Rt(St, At)) (similar to Toussaint & Storkey, 2006; Rawlik et al., 2010), then a bootstrap particle filter returns an unbiased estimate of E[exp(\u03b2 \u2211T t=0Rt)|S0 = s]. Algorithm 1 summarizes this approach.\nAlgorithm 1 An estimator of the PVF V \u03c0T,K(s(1), . . . , s(K), \u03b2)\n1: for i = 1 : K do 2: S(i)0 = s (i) 3: A(i)0 \u223c \u03c0T (\u00b7|s(i)) 4: W (i)0 = exp(\u03b2R (i) 0 ) 5: end for 6: Z0 = 1 K \u2211K i=1W (i) 0 7: for t = 1 : T do 8: for i = 1 : K do\n9: I \u223c P(I = j) \u221dW (j)t\u22121 # select random parent 10: S(i)t \u223c p(\u00b7|S (I) t\u22121, A (I) t\u22121) # inherit from parent 11: A(i)t \u223c \u03c0T\u2212t(\u00b7|S (i) t ) 12: W (i)t = exp(\u03b2R (i) t ) 13: end for 14: Zt = 1 K \u2211K i=1W (i) t 15: end for 16: return 1\u03b2 \u2211T t=0 logZt\nTaking an expectation over all of the random variables not conditioned on we define the PVF associated with the bootstrap particle filter dynamics:\nV \u03c0T (s (1), . . . , s(K), \u03b2) = E\n[ 1\n\u03b2 T\u2211 t=0 logZt \u2223\u2223\u2223\u2223\u2223 {S(i)0 = s(i)}Ki=1 ] . (3)\nNote, more sophisticated sampling schemes, see Doucet & Johansen (2011), result in distinct PVFs.\nConsider the value if we initialize all particles at s, V \u03c0T,K(s, \u03b2) = V \u03c0 T (s, . . . , s, \u03b2). If \u03b2 > 0, then by Jensen\u2019s inequality and the unbiasedness of the estimator we have that V \u03c0T,K(s, \u03b2) \u2264 V \u03c0T (s, \u03b2). For \u03b2 < 0 the bound is in the opposite direction. It is informative to consider the behaviour of the trajectories for different values of \u03b2. For \u03b2 > 0 this algorithm greedily prefers trajectories that encounter large rewards, and the aggregate return is a per time step soft-max. For \u03b2 < 0 this algorithm prefers trajectories that encounter large negative rewards, and the aggregate return is a per time step soft-min. See Appendix D for the Bellman equation and policy gradient of this PVF."}, {"heading": "4 EXPERIMENTS", "text": "To highlight the benefits of using PVFs we apply them to a variant of the Gridworld task called Cliffworld, see Appendix E for comparison to other methods and more details. We trained time dependent tabular policies using policy gradients from distinct PVFs for \u03b2 \u2208 {\u22121,\u22120.5, 0, 0.5, 1, 2}. We tried K \u2208 {1, . . . , 8} and learning rates \u2208 {1\u00d7 10\u22123, 5\u00d7 10\u22124, 1\u00d7 10\u22124, 5\u00d7 10\u22125}. For the \u03b2 = 0 case we ran K independent non-interacting trajectories and averaged over a policy gradient with estimated baselines. Figure 2 shows the density over the final state of the trained MDP under varying \u03b2 treatments butK = 4. Notice that the higher the risk parameter, the broader the policy, with the agent eventually solving the task. No \u03b2 = 0, corresponding to standard REINFORCE, runs solved this task, even after increasing the number of agents to 64."}, {"heading": "5 CONCLUSION", "text": "We introduced the particle value function, which approximates a risk-sensitive value function for a given MDP. We will seek to address theoretical questions, such as whether the PVF is increasing in \u03b2 and monotonic in the number of particles. Also, the PVF does not have an efficient tabular representation, so understanding the effect of efficient approximations would be valuable. Experimentally, we hope to explore these ideas for complex sequential tasks with non-linear function approximators. One obvious example of such tasks is variational inference over a sequential model."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We thank Re\u0301mi Munos, Theophane Weber, David Silver, Marc G. Bellemare, and Danilo J. Rezende for helpful discussion and support in this project."}, {"heading": "A MARKOV DECISION PROCESSES", "text": "We consider decision problems in which an agent selects actions and receives rewards in a stochastic environment. For the sake of exposition, we consider a finite horizon MDP, which consists of: a finite state space S, a finite action space A, a stationary environmental transition kernel satisfying the Markov property p(\u00b7|St, At, . . . , S0, A0) = p(\u00b7|St, At), and reward functions rT\u2212t : S \u00d7 A \u2192 R. At each time step the agent chooses actions according to a policy \u03c0T\u2212t(\u00b7|St) given the current state. \u03c0T\u2212t(\u00b7|St) is the action distribution and rT\u2212t the reward function when there are T \u2212 t steps remaining. All together the MDP proceeds stochastically producing a sequence of random variables (St, At) according to the following dynamics for T \u2208 N time steps. Let t \u2208 {0, . . . , T},\nS0 = s (4) At \u223c \u03c0T\u2212t(\u00b7|St) (5)\nSt+1 \u223c p(\u00b7|St, At) (6)\nThe agent receives a reward Rt = rT\u2212t(St, At) at each time step. We will call a single realization of the MDP a trajectory. The objective in classical reinforcement learning is to discover the policies \u03c0 = {\u03c0t(\u00b7|s)}Tt=0 that maximize the value function,\nV \u03c0T (s) = E [ T\u2211 t=0 Rt \u2223\u2223\u2223\u2223\u2223 S0 = s ] . (7)\nwhere the expectation is taken with respect to all the stochastic elements not conditioned on.\nAll of the results we present can be simply extended to the infinite horizon case with discounted or episodic returns as well as more general uncountable state and action spaces."}, {"heading": "B RISK-SENSITIVE VALUE FUNCTION DETAILS", "text": "Utility theory gives us a language for describing the relative importance of high or low returns. A utility function u : R \u2192 R is an invertible non-decreasing function, which specifies a ranking over possible returns \u2211T t=0Rt. The expected utility E[u( \u2211T t=0Rt)|S0 = s] specifies a ranking over policies (Von Neumann & Morgenstern, 1953). The expected utility does not necessarily have an interpretable scale, because any affine transformation of the utility function results in the same relative ordering of policies or return outcomes. Therefore we define the value associated with a utility u by returning it to the scale of the rewards defined by the MDP. For an agent following u, the \u201cvalue\u201d of a state is the real number V \u03c0T (s, u) whose utility is the expected utility:\nV \u03c0T (s, u) = u \u22121\n( E [ u ( T\u2211 t=1 Rt ) \u2223\u2223\u2223\u2223\u2223 S0 = s ]) . (8)\nNote that when u is the identity we recover the expected return. Of course for non-decreasing invertible utilities, the value gives the same ranking over policies. One way to interpret this value is through the following thought experiment. If the agent is given a choice between a single interaction with the environment or an immediate deterministic return, then V \u03c0T (s, u) represents the minimum return that our agent would take in exchange for forgoing an interaction. If\nV \u03c0T (s, u) \u2264 E [ T\u2211 t=0 Rt \u2223\u2223\u2223\u2223\u2223S0 = s ]\nthen our agent is willing to take a loss relative to the expected return in exchange for certainty. This is a risk-avoiding attitude, which emphasizes the smallest returns, and one can show that this occurs iff u is concave. If\nV \u03c0T (s, u) \u2265 E [ T\u2211 t=0 Rt \u2223\u2223\u2223\u2223\u2223S0 = s ]\nthen the agent would only forgo an interaction for more than it can expect to receive. This is riskseeking behavior, which emphasizes the largest returns, and one can show that this occurs iff u is\nconvex. The case when u(x) is linear is the risk-neutral case. For these reasons, V \u03c0T (s, u) is also known as the certain equivalent in economics (Howard & Matheson, 1972).\nWe focus on exponential utilities of the form u(x) = sgn(\u03b2) exp(\u03b2x) where \u03b2 \u2208 R. This is a broadly studied choice that is implied by the assumption that the value function V \u03c0T (s, u) is additive for deterministic translations of the return (Pratt, 1964; Howard & Matheson, 1972; Coraluppi, 1997). This assumption is nice, because it preserves the Markov nature of the decision process: if the agent were given a choice at every time step t between continuing the interaction or terminating and taking its value as a deterministic return, then additivity in the value function means that the same decision is made regardless of the return accumulated so far (Howard & Matheson, 1972). The value function corresponding to an exponential utility is\nV \u03c0T (s, \u03b2) = 1\n\u03b2 logE\n[ exp ( \u03b2\nT\u2211 t=0 Rt ) \u2223\u2223\u2223\u2223\u2223 S0 = s ] , (9)\nand as \u03b2 \u2192 0 we recover the expected return. We list a few of its properties.\n1. For \u03b2 near 0\nV \u03c0T (s, \u03b2) = E [ T\u2211 t=0 Rt \u2223\u2223\u2223\u2223\u2223 S0 = s ] + \u03b2 2 var \u03c0 [ T\u2211 t=0 Rt \u2223\u2223\u2223\u2223\u2223 S0 = s ] + o(\u03b22) (10)\n2. lim\u03b2\u2192\u221e V \u03c0T (s, \u03b2) = sup{r|P( \u2211T t=0Rt = r) > 0}\n3. lim\u03b2\u2192\u2212\u221e V \u03c0T (s, \u03b2) = inf{r|P( \u2211T t=0Rt = r) > 0}\n4. V \u03c0T (s, \u03b2) is continous and non-decreasing in \u03b2.\n5. V \u03c0T (s, \u03b2) is risk-seeking for \u03b2 > 0, risk-avoiding for \u03b2 = 0, and risk-neutral for \u03b2 = 0\nFor proofs,\n1. From Coraluppi (1997).\n2. From Coraluppi (1997).\n3. From Coraluppi (1997). 4. V \u03c0T (s, \u03b2) is clearly continuous for all \u03b2 6= 0. If we extend V \u03c0T (s, 0) \u2261 E [\u2211T\nt=0Rt \u2223\u2223\u2223 S0 = s], then 1. gives us the continuity everywhere. For non-decreasing let \u03b1, \u03b2 \u2208 R and \u03b1 6= 0 and \u03b2 6= 0. Furthermore assume \u03b2 \u2265 \u03b1. Then \u03b2/\u03b1 > 0 or \u03b2/\u03b1 \u2265 1. Now,\nE [ exp ( \u03b2\nT\u2211 t=0 Rt ) \u2223\u2223\u2223\u2223\u2223 S0 = s ] = E exp(\u03b1 T\u2211 t=0 Rt )\u03b2/\u03b1 \u2223\u2223\u2223\u2223\u2223\u2223 S0 = s \n\u2265 E [ exp ( \u03b1\nT\u2211 t=0 Rt ) \u2223\u2223\u2223\u2223\u2223 S0 = s ]\u03b2/\u03b1\nsince xp is convex on x > 0 for p \u2265 1 or p < 0, Jensen\u2019s inequality gives us the result. Taking log of both sides gives us the result in that case. In the case that \u03b1 = 0 or \u03b2 = 0, 4. and Jensen\u2019s inequality gives us the result by the concavity of log.\nFrom a practical point of view the value function V \u03c0T (s, \u03b2) behaves like a soft-max or soft-min depending on the sign of \u03b2, emphasizing the avoidance of low returns when \u03b2 < 0 and the pursuit of high returns when \u03b2 > 0. As \u03b2 \u2192\u221e the value V \u03c0T (s, \u03b2) approaches the supremum of the returns over all trajectories with positive probability, a best-case penalty. As \u03b2 \u2192 \u2212\u221e it approaches the infimum, a worst-case value (Coraluppi, 1997). Thus for large positive \u03b2 this value is tolerant of high variance if it can lead to high returns. For large negative \u03b2 it is very intolerant of rare low returns.\nDespite having attractive properties the risk-sensitive value function is not always applicable to reinforcement learning tasks (see also Mihatsch & Neuneier (2002)). The value function satisfies the multiplicative Bellman equation\nexp(\u03b2V \u03c0T (s, \u03b2)) = \u2211 a,s\u2032 \u03c0T (a|s)p(s\u2032|s, a) exp(\u03b2rT (s, a) + \u03b2V \u03c0T\u22121(s, \u03b2)). (11)\nOperating in log-space breaks the ability to exploit this recurrence from Monte Carlo returns generated by a single trajectory, because expectations do not exchange with log. Operating in exp-space is possible for TD learning algorithms, but we must cap the minimum/maximum possible return so that exp(\u03b2V \u03c0T (s, \u03b2)) does not underflow/overflow. This can be an issue when the rewards represent log probabilities as is often the case in variational inference. The policy gradient of V \u03c0T (s, \u03b2) is\n\u2207\u03c0V \u03c0T (s, \u03b2) = E\n[ 1\n\u03b2 T\u2211 t=0 exp(\u03b2Q\u03c0T\u2212t(St, At, \u03b2)\u2212 \u03b2V \u03c0T (St, \u03b2))\u2207 log \u03c0T\u2212t(At|St) \u2223\u2223\u2223\u2223\u2223S0 = s ] (12)\nwhere\nQ\u03c0T (s, a, \u03b2) = 1\n\u03b2 logE\n[ exp ( \u03b2\nT\u2211 t=0 Rt )\u2223\u2223\u2223\u2223\u2223S0 = s,A0 = a ]\n(13)\nEven ignoring underflow/overflow issues, REINFORCE (Williams, 1992) style algorithms would find difficulties, because deriving unbiased estimators of the ratio exp(\u03b2Q\u03c0T\u2212t(St, At, \u03b2) \u2212 \u03b2V \u03c0T (s, \u03b2)) from single trajectories of the MDP may be hard. Lastly, the policy gradient of\nE [ exp ( \u03b2 \u2211T t=0Rt ) \u2223\u2223\u2223 S0 = s] \u2207\u03c0 E [ exp ( \u03b2\nT\u2211 t=0 Rt ) \u2223\u2223\u2223\u2223\u2223 S0 = s ] = E [ T\u2211 t=0 exp ( \u03b2 T\u2211 t=0 Rt ) \u2207 log \u03c0T\u2212t(At|St) \u2223\u2223\u2223\u2223\u2223S0 = s ] ,\n(14)\nThere are particle methods that would address the estimation of this score, e.g. (Kantas et al., 2015), but for large T the estimate suffers from high mean squared errors."}, {"heading": "C RELATED WORK", "text": "Risk sensitivity originates in the study of utility and choice in economics (Von Neumann & Morgenstern, 1953; Pratt, 1964; Arrow, 1974). It has been extensively studied for the control of MDPs (Howard & Matheson, 1972; Coraluppi, 1997; Marcus et al., 1997; Borkar & Meyn, 2002; Mihatsch & Neuneier, 2002; Ba\u0308uerle & Rieder, 2013). In reinforcement learning, risk sensitivity has been studied (Koenig & Simmons, 1994; Neuneier & Mihatsch, 1998; Shen et al., 2014), although none of these consider the direct policy gradient approach considered in this work. Most of the methods considered are variants of a Q learning approach or policy iteration. As well, the idea of treating rewards as emissions of an HMM is not a new idea (Toussaint & Storkey, 2006; Rawlik et al., 2010).\nThe idea of treating reinforcement learning as an inference problem is not a new idea (see e.g. Albertini & Runggaldier, 1988; Dayan & Hinton, 1997; Kappen, 2005; Toussaint & Storkey, 2006; Hoffman et al., 2007; Tishby & Polani, 2011; Kappen et al., 2012). Broadly speaking, all of these works still optimize the expected reward objective V \u03c0T (s, 0) = E[ \u2211T t=0Rt | S0 = s] with or without some regularization penalties on the policy. The ones that share the closest connection to the risk sensitive objective V \u03c0T (s, \u03b2) studied here, are the KL regularized objectives of the form\nV\u0302 \u03c0,\u03c0 \u2032\nT (s, \u03b2) = E \u03c0\u2032 [ T\u2211 t=0 Rt + 1 \u03b2 log \u03c0(At|St) \u03c0\u2032(At|St) \u2223\u2223\u2223\u2223\u2223 S0 = s ]\n(15)\nwhere the MDP dynamics are sampled from \u03c0\u2032. These are studied for example in Albertini & Runggaldier (1988); Kappen (2005); Tishby & Polani (2011); Kappen et al. (2012); Fox et al. (2015);\nRuiz & Kappen (2016). The observation is that in an MDP with fully controllable transition dynamics, optimizing a policy \u03c0\u2032, which completely specifies the transition dynamics, achieves the risk sensitive value at \u03c0:\nmax \u03c0\u2032\nV\u0302 \u03c0,\u03c0 \u2032\nT (s, \u03b2) = V \u03c0 T (s, \u03b2) (16)\nNote that this has an interesting connection to Bayesian inference. Here, \u03c0 plays the role of the prior, \u03c0\u2032 the role of the variational posterior, V\u0302 \u03c0,\u03c0 \u2032\nT (s, \u03b2) the role of the variational lower bound, and V \u03c0T (s, \u03b2) the role of the marginal likelihood. In effect, KL regularized control is like variational inference, where risk sensitive control is like maximum likelihood. Finally, when the environmental dynamics p(\u00b7|s, a) are stochastic, (16) does not necessarily hold, therefore the risk sensitive value is distinct in this case. Yet, in certain special cases, risk sensitive objectives can also be cast as solutions to path integral control problems (Broek et al., 2012).\nTo our knowledge no work has considered using particle filters for risk sensitive control by treating the particle filter\u2019s estimator of the log partition function as a return whose expectation bounds the risk sensitive value and whose policy gradients are cheap to compute."}, {"heading": "D PARTICLE VALUE FUNCTION DETAILS", "text": "Recalling Algorithm 1 and the definition of the MDP in Appendix A, define\nR (i) t = rT\u2212t(S (i) t , A (i) t ) (17)\nthe particle value function associated with the bootstrap particle filter dynamics:\nV \u03c0T (s (1), . . . , s(K), \u03b2) = E\n[ 1\n\u03b2 T\u2211 t=0 logZt \u2223\u2223\u2223\u2223\u2223 {S(i)0 = s(i)}Ki=1 ] . (18)\nWe can also think of this value function as the expected return of an agent whose actions space is the product space AK , in an environment with state space SK whose transition kernel includes the resampling dynamic. Let s(1:K) = (s(1), . . . , s(K)), then the PVF satisfies the Bellman equation\n\u03b2V \u03c0T (s (1:K), \u03b2) = \u2211 a(1:K)\n\u03a0T (a (1:K)|s(1:K)) logZT (a(1:K), s(1:K)) + (19)\u2211\na(1:K) \u2211 \u03c3(1:K) \u03a0T (a (1:K)|s(1:K))PT (\u03c3(1:K)|a(1:K), s(1:K))\u03b2V \u03c0T\u22121(\u03c3(1:K), \u03b2)\n(20)\nwhere\n\u03a0T (a (1:K)|s(1:K)) = K\u220f i=1 \u03c0T (a (i)|s(i)) (21)\nlogZT (a (1:K), s(1:K)) = log\n( 1\nK K\u2211 i=1 exp(\u03b2rT (s (i), a(i)))\n) (22)\nPT (\u03c3 (1:K)|a(1:K), s(1:K)) = K\u220f i=1  K\u2211 j=1 exp(\u03b2rT (s (j), a(j)))\u2211K k=1 exp(\u03b2rT (s (k), a(k))) p(\u03c3(i)|s(j), a(j))  (23) The policy gradient of V \u03c0T (s (1:K), \u03b2) is\n\u2207\u03c0V \u03c0T (s(1:K), \u03b2) = E\n[ 1\n\u03b2 T\u2211 t=0 T\u2211 t\u2032=t K\u2211 i=1 logZt\u2032\u2207 log \u03c0T\u2212t(A(i)t |S (i) t ) \u2223\u2223\u2223\u2223\u2223 {S(i)0 = s(i)}Ki=1 ] (24)\nIn this sense we can think of logZt/\u03b2 as the immediate reward for the whole system of particles and \u2211T t=0 logZt/\u03b2 as the return.\nThe key point is that the use of interacting trajectories to generate the Monte Carlo return ensures that this particle value function defines a bound on V \u03c0T (s, \u03b2). Indeed, consider the particle value function that corresponds to initializing all K trajectories in state s \u2208 S and define, V \u03c0T,K(s, \u03b2) = V \u03c0T (s, . . . , s, \u03b2). Now, for \u03b2 > 0 we have, by Jensen\u2019s inequality,\nV \u03c0T,K(s, \u03b2) \u2264 1\n\u03b2 logE [ T\u220f t=0 Zt \u2223\u2223\u2223\u2223\u2223 {S(i)0 = s}Ki=1 ]\n(25)\nand since the bootstrap particle filter is unbiased (Del Moral, 2004; Pitt et al., 2012),\n= V \u03c0T (s, \u03b2) (26)\nFor \u03b2 < 0 we get the reverse inequality, V \u03c0T,K(s, \u03b2) \u2265 V \u03c0T (s, \u03b2). As K \u2192 \u221e the particle value function converges to V \u03c0T (s, \u03b2) since the estimator is consistent (Del Moral, 2004). We list this and some other properties:\n1. V \u03c0T,K(s, \u03b2) \u2264 V \u03c0T (s, \u03b2). 2. limK\u2192\u221e V \u03c0T,K(s, \u03b2) = V \u03c0 T (s, \u03b2).\n3. lim\u03b1\u21920 V \u03c0T,K(s, \u03b1) = E [\u2211T t=0Rt \u2223\u2223\u2223 S0 = s] = V \u03c0T,1(s, \u03b2). 4. V \u03c0T,K(s, \u03b2) is continuous in \u03b2.\nFor proofs,\n1. \u220fT t=0 Zt is an unbiased estimator of E[exp(\u03b2 \u2211T t=0Rt)|S0 = s] (Del Moral, 2004) and\nthe rest follows from Jensen\u2019s inequality. 2. \u220fT t=0 Zt is a consistent estimator (Del Moral, 2004), and the rest follows from exchanging\nthe limit with an expectation. 3. V \u03c0T,1(s, \u03b2) = E [\u2211T t=0Rt \u2223\u2223\u2223 S0 = s] is clear. Otherwise the limit lim\u03b2\u21920 V \u03c0T,K(s, \u03b2) approaches the algorithm that resamples uniformly and the value under that sampling strategy is\nlim \u03b2\u21920\nV \u03c0T,K(s, \u03b2) = E [ T\u2211 t=0 lim \u03b2\u21920 logZt \u03b2 \u2223\u2223\u2223\u2223\u2223 {S(i)0 = s}Ki=1 ]\n= E [ T\u2211 t=0 K\u2211 i=1 1 K R (i) t \u2223\u2223\u2223\u2223\u2223 {S(i)0 = s}Ki=1 ]\n= T\u2211 t=0 K\u2211 i=1 1 K E [ R (i) t \u2223\u2223\u2223 {S(i)0 = s}Ki=1] Because each R(i)t has a genealogy, which is an MDP trajectory\n= T\u2211 t=0 K\u2211 i=1 1 K E [Rt | S0 = s]\n= E [ T\u2211 t=0 Rt \u2223\u2223\u2223\u2223\u2223 S0 = s ]\n4. V \u03c0T,K(s, \u03b2) is a finite sum of continuous terms, and if we extend the definition of V \u03c0T,K(s, 0) \u2261 lim\u03b2\u21920 V \u03c0T,K(s, \u03b2), then we\u2019re done."}, {"heading": "E CLIFFWORLD DETAILS", "text": "We considered a finite horizon Cliffworld task, which is a variant on Gridworld. The world is 4 rows by 12 columns, and the agent can occupy any grid location. Each episode begins with the\nagent in state (0,0) (marked as S in Figure 3) and ends when 24 timesteps have passed. The actions available to the agent are moving north, east, south, and west, but moving off the grid is prohibited. All environmental transitions are deterministic. The \u2018cliff\u2019 occupies all states between the start and the goal (marked G in Figure 3) along the northern edge of the world. All cliff states are absorbing and when the agent enters any cliff state it initially receives a reward of -100 and receives 0 reward each timestep thereafter. The goal state is also absorbing, and the agent receives a +100 reward upon entering it and 0 reward after. The agent receives a -1 reward for every action that does not transition into a cliff or goal state. The optimal policy for Cliffworld is to hug the cliff and proceed from the start to the goal as speedily as possible, but doing so could incur high variance in reward if the agent falls off the cliff. For a uniform random policy most trajectories result in large negative rewards and occasionally a high positive reward. This means that initially for independent trajectories venturing east is high variance and low reward.\nWe trained non-stationary tabular policies parameterized by parameters \u03b8 of size 4\u00d7 12\u00d7 4\u00d7 24:\n\u03c0T\u2212t(a|s) = exp(\u03b8[s1, s2, a, T \u2212 t])\u22113 a=0 exp(\u03b8[s1, s2, a, T \u2212 t])\nThe policies were trained using policy gradients from distinct PVFs for \u03b2 \u2208 {\u22121,\u22120.5, 0, 0.5, 1, 2}. We triedK \u2208 {1, . . . , 8} and learning rates \u2208 {1\u00d7 10\u22123, 5\u00d7 10\u22124, 1\u00d7 10\u22124, 5\u00d7 10\u22125}. For the \u03b2 = 0 case we ran K independent non-interacting trajectories and averaged over a policy gradient with estimated baselines. For \u03b2 = 0, we used instead a REINFORCE (Williams, 1992) estimator, that was simply estimated from the Monte Carlo returns. For control variates, we used distinct baselines depending on whether \u03b2 = 0 or not. For \u03b2 = 0, we used a baseline that was an exponential moving average with smoothing factor 0.8. The baselines were also non-stationary, and with dimensionality 4 \u00d7 12 \u00d7 24. For \u03b2 6= 0 we used no baseline except for VIMCO\u2019s control variate (Mnih & Rezende, 2016) for the immediate reward. The VIMCO control variate is not applicable for the whole return as future time steps are correlated with the action through the interaction of trajectories.\nWe also compared directly to VIMCO (Mnih & Rezende, 2016). Consider VIMCO\u2019s value function,\nV\u0303 \u03c0T,K(s, \u03b2) = E\n[ 1\n\u03b2 log\n( 1\nK K\u2211 i=1 exp ( T\u2211 t=0 \u03b2R (i) t ))] (27)\nwhere R(i)t is a reward sequence generated by an independent Monte Carlo rollout of the original MDP. VIMCO is also a risk sensitive value function, but it does not decompose over time and so\ndoes not have a temporal Bellman equation. In this case, though, VIMCO policy gradients were able to solve Cliffworld under most of the conditions that the policy gradients of PVF were able to solve. For K = 3 and \u03b2 = 1.0, PVF occasionally solved Cliffworld while VIMCO did not. See Figure 4. However, once in the regime where VIMCO could solve the task, it did so with more reliability than the PVF variant. Note that in no case did REINFORCE on the expected return solve this variant."}], "references": [{"title": "Logarithmic transformations for discrete-time, finite-horizon stochastic control problems", "author": ["Francesca Albertini", "Wolfgang J Runggaldier"], "venue": "Applied mathematics & optimization,", "citeRegEx": "Albertini and Runggaldier.,? \\Q1988\\E", "shortCiteRegEx": "Albertini and Runggaldier.", "year": 1988}, {"title": "Essays in the theory of risk-bearing", "author": ["Kenneth Joseph Arrow"], "venue": null, "citeRegEx": "Arrow.,? \\Q1974\\E", "shortCiteRegEx": "Arrow.", "year": 1974}, {"title": "More risk-sensitive markov decision processes", "author": ["Nicole B\u00e4uerle", "Ulrich Rieder"], "venue": "Mathematics of Operations Research,", "citeRegEx": "B\u00e4uerle and Rieder.,? \\Q2013\\E", "shortCiteRegEx": "B\u00e4uerle and Rieder.", "year": 2013}, {"title": "Risk-sensitive optimal control for markov decision processes with monotone cost", "author": ["Vivek S Borkar", "Sean P Meyn"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Borkar and Meyn.,? \\Q2002\\E", "shortCiteRegEx": "Borkar and Meyn.", "year": 2002}, {"title": "Risk sensitive path integral control", "author": ["Bart van den Broek", "Wim Wiegerinck", "Hilbert Kappen"], "venue": "arXiv preprint arXiv:1203.3523,", "citeRegEx": "Broek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Broek et al\\.", "year": 2012}, {"title": "Importance Weighted Autoencoder", "author": ["Yuri Burda", "Roger Grosse", "Ruslan Salakhutdinov"], "venue": "In ICLR,", "citeRegEx": "Burda et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2016}, {"title": "Optimal control of Markov decision processes for performance and robustness", "author": ["Stefano Coraluppi"], "venue": "PhD thesis, University of Maryland,", "citeRegEx": "Coraluppi.,? \\Q1997\\E", "shortCiteRegEx": "Coraluppi.", "year": 1997}, {"title": "Using expectation-maximization for reinforcement learning", "author": ["Peter Dayan", "Geoffrey E Hinton"], "venue": "Neural Computation,", "citeRegEx": "Dayan and Hinton.,? \\Q1997\\E", "shortCiteRegEx": "Dayan and Hinton.", "year": 1997}, {"title": "Feynman-kac formulae. In Feynman-Kac Formulae, pp. 47\u201393", "author": ["Pierre Del Moral"], "venue": null, "citeRegEx": "Moral.,? \\Q2004\\E", "shortCiteRegEx": "Moral.", "year": 2004}, {"title": "A tutorial on particle filtering and smoothing: fiteen years", "author": ["Arnaud Doucet", "Adam M Johansen"], "venue": null, "citeRegEx": "Doucet and Johansen.,? \\Q2011\\E", "shortCiteRegEx": "Doucet and Johansen.", "year": 2011}, {"title": "Taming the noise in reinforcement learning via soft updates", "author": ["Roy Fox", "Ari Pakman", "Naftali Tishby"], "venue": "arXiv preprint arXiv:1512.08562,", "citeRegEx": "Fox et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2015}, {"title": "On solving general state-space sequential decision problems using inference algorithms", "author": ["Matt Hoffman", "Arnaud Doucet", "Nando De Freitas", "Ajay Jasra"], "venue": "Technical report, Technical Report TR2007-04,", "citeRegEx": "Hoffman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2007}, {"title": "Risk-sensitive Markov decision processes", "author": ["Ronald A Howard", "James E Matheson"], "venue": "Management science,", "citeRegEx": "Howard and Matheson.,? \\Q1972\\E", "shortCiteRegEx": "Howard and Matheson.", "year": 1972}, {"title": "Sequential decision making in general state space models", "author": ["Nikolas Kantas"], "venue": "PhD thesis, Citeseer,", "citeRegEx": "Kantas.,? \\Q2009\\E", "shortCiteRegEx": "Kantas.", "year": 2009}, {"title": "On particle methods for parameter estimation in state-space models", "author": ["Nikolas Kantas", "Arnaud Doucet", "Sumeetpal S Singh", "Jan Maciejowski", "Nicolas Chopin"], "venue": "Statistical science,", "citeRegEx": "Kantas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kantas et al\\.", "year": 2015}, {"title": "Linear theory for control of nonlinear stochastic systems", "author": ["Hilbert J Kappen"], "venue": "Physical review letters,", "citeRegEx": "Kappen.,? \\Q2005\\E", "shortCiteRegEx": "Kappen.", "year": 2005}, {"title": "Optimal control as a graphical model inference problem", "author": ["Hilbert J Kappen", "Vicen\u00e7 G\u00f3mez", "Manfred Opper"], "venue": "Machine learning,", "citeRegEx": "Kappen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kappen et al\\.", "year": 2012}, {"title": "Risk-sensitive planning with probabilistic decision graphs", "author": ["Sven Koenig", "Reid G Simmons"], "venue": "In Proceedings of the 4th international conference on principles of knowledge representation and reasoning,", "citeRegEx": "Koenig and Simmons.,? \\Q1994\\E", "shortCiteRegEx": "Koenig and Simmons.", "year": 1994}, {"title": "Risk sensitive markov decision processes", "author": ["Steven I Marcus", "Emmanual Fern\u00e1ndez-Gaucherand", "Daniel Hern\u00e1ndez-Hernandez", "Stefano Coraluppi", "Pedram Fard"], "venue": "In Systems and control in the twenty-first century,", "citeRegEx": "Marcus et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1997}, {"title": "Risk-sensitive reinforcement learning", "author": ["Oliver Mihatsch", "Ralph Neuneier"], "venue": "Machine learning,", "citeRegEx": "Mihatsch and Neuneier.,? \\Q2002\\E", "shortCiteRegEx": "Mihatsch and Neuneier.", "year": 2002}, {"title": "Variational Inference for Monte Carlo Objectives", "author": ["Andriy Mnih", "Danilo Rezende"], "venue": "In ICML,", "citeRegEx": "Mnih and Rezende.,? \\Q2016\\E", "shortCiteRegEx": "Mnih and Rezende.", "year": 2016}, {"title": "Risk sensitive reinforcement learning", "author": ["Ralph Neuneier", "Oliver Mihatsch"], "venue": "In Proceedings of the 11th International Conference on Neural Information Processing Systems,", "citeRegEx": "Neuneier and Mihatsch.,? \\Q1998\\E", "shortCiteRegEx": "Neuneier and Mihatsch.", "year": 1998}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["Andrew Y Ng", "Daishi Harada", "Stuart Russell"], "venue": "In ICML,", "citeRegEx": "Ng et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "On some properties of markov chain monte carlo simulation methods based on the particle filter", "author": ["Michael K Pitt", "Ralph dos Santos Silva", "Paolo Giordani", "Robert Kohn"], "venue": "Journal of Econometrics,", "citeRegEx": "Pitt et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pitt et al\\.", "year": 2012}, {"title": "Risk aversion in the small and in the large", "author": ["John W Pratt"], "venue": null, "citeRegEx": "Pratt.,? \\Q1964\\E", "shortCiteRegEx": "Pratt.", "year": 1964}, {"title": "An approximate inference approach to temporal optimization in optimal control. In Advances in neural information processing", "author": ["Konrad Rawlik", "Marc Toussaint", "Sethu Vijayakumar"], "venue": null, "citeRegEx": "Rawlik et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rawlik et al\\.", "year": 2011}, {"title": "Particle smoothing for hidden diffusion processes: Adaptive path integral smoother", "author": ["H-Ch Ruiz", "HJ Kappen"], "venue": "arXiv preprint arXiv:1605.00278,", "citeRegEx": "Ruiz and Kappen.,? \\Q2016\\E", "shortCiteRegEx": "Ruiz and Kappen.", "year": 2016}, {"title": "Curious model-building control systems", "author": ["J\u00fcrgen Schmidhuber"], "venue": "In Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q1991\\E", "shortCiteRegEx": "Schmidhuber.", "year": 1991}, {"title": "Risk-sensitive reinforcement learning", "author": ["Yun Shen", "Michael J Tobia", "Tobias Sommer", "Klaus Obermayer"], "venue": "Neural computation,", "citeRegEx": "Shen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Information theory of decisions and actions", "author": ["Naftali Tishby", "Daniel Polani"], "venue": "In Perception-action cycle,", "citeRegEx": "Tishby and Polani.,? \\Q2011\\E", "shortCiteRegEx": "Tishby and Polani.", "year": 2011}, {"title": "Linearly-solvable markov decision problems", "author": ["Emanuel Todorov"], "venue": "In NIPS, pp", "citeRegEx": "Todorov.,? \\Q2006\\E", "shortCiteRegEx": "Todorov.", "year": 2006}, {"title": "Probabilistic inference for solving discrete and continuous state markov decision processes", "author": ["Marc Toussaint", "Amos Storkey"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Toussaint and Storkey.,? \\Q2006\\E", "shortCiteRegEx": "Toussaint and Storkey.", "year": 2006}, {"title": "Theory of games and economic behavior", "author": ["John Von Neumann", "Oskar Morgenstern"], "venue": null, "citeRegEx": "Neumann and Morgenstern.,? \\Q1953\\E", "shortCiteRegEx": "Neumann and Morgenstern.", "year": 1953}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "For control variates, we used distinct baselines depending on whether \u03b2 = 0 or not. For \u03b2 = 0, we used a baseline that was an exponential moving average with smoothing factor 0.8. The baselines were also non-stationary, and with dimensionality 4 \u00d7 12 \u00d7 24. For \u03b2 6= 0 we used no baseline except for VIMCO\u2019s control variate (Mnih", "author": ["Monte Carlo returns"], "venue": null, "citeRegEx": "returns.,? \\Q2016\\E", "shortCiteRegEx": "returns.", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "The idea is inspired by recent advances in variational inference which bound the log marginal likelihood via importance sampling estimators (Burda et al., 2016; Mnih & Rezende, 2016), but takes an orthogonal approach to reward modifications, e.", "startOffset": 140, "endOffset": 182}, {"referenceID": 27, "context": "(Schmidhuber, 1991; Ng et al., 1999).", "startOffset": 0, "endOffset": 36}, {"referenceID": 22, "context": "(Schmidhuber, 1991; Ng et al., 1999).", "startOffset": 0, "endOffset": 36}, {"referenceID": 24, "context": "This choice is well-studied, and it is implied by the assumption that V \u03c0 T (s, u) is additive for deterministic translations of the reward function (Pratt, 1964; Howard & Matheson, 1972; Coraluppi, 1997).", "startOffset": 149, "endOffset": 204}, {"referenceID": 6, "context": "This choice is well-studied, and it is implied by the assumption that V \u03c0 T (s, u) is additive for deterministic translations of the reward function (Pratt, 1964; Howard & Matheson, 1972; Coraluppi, 1997).", "startOffset": 149, "endOffset": 204}, {"referenceID": 30, "context": "Note also that the literature on KL regularized control (Todorov, 2006; Kappen, 2005; Tishby & Polani, 2011) gives a different perspective on risk sensitive control, which mirrors the relationship between variational inference and maximum likelihood.", "startOffset": 56, "endOffset": 108}, {"referenceID": 15, "context": "Note also that the literature on KL regularized control (Todorov, 2006; Kappen, 2005; Tishby & Polani, 2011) gives a different perspective on risk sensitive control, which mirrors the relationship between variational inference and maximum likelihood.", "startOffset": 56, "endOffset": 108}, {"referenceID": 23, "context": "The result is an unbiased estimator \u220fT t=0(K \u22121\u2211K i=1 q(yt|X (i) t )) of the desired probability (Del Moral, 2004; Pitt et al., 2012).", "startOffset": 97, "endOffset": 133}, {"referenceID": 12, "context": "This is distinct, but related to Kantas (2009), which investigates particle filter algorithms for infinite horizon risk-sensitive control.", "startOffset": 33, "endOffset": 47}, {"referenceID": 24, "context": "This is a broadly studied choice that is implied by the assumption that the value function V \u03c0 T (s, u) is additive for deterministic translations of the return (Pratt, 1964; Howard & Matheson, 1972; Coraluppi, 1997).", "startOffset": 161, "endOffset": 216}, {"referenceID": 6, "context": "This is a broadly studied choice that is implied by the assumption that the value function V \u03c0 T (s, u) is additive for deterministic translations of the return (Pratt, 1964; Howard & Matheson, 1972; Coraluppi, 1997).", "startOffset": 161, "endOffset": 216}, {"referenceID": 6, "context": "From Coraluppi (1997). 2.", "startOffset": 5, "endOffset": 22}, {"referenceID": 6, "context": "From Coraluppi (1997). 2. From Coraluppi (1997). 3.", "startOffset": 5, "endOffset": 48}, {"referenceID": 6, "context": "From Coraluppi (1997). 2. From Coraluppi (1997). 3. From Coraluppi (1997). 4.", "startOffset": 5, "endOffset": 74}, {"referenceID": 6, "context": "As \u03b2 \u2192 \u2212\u221e it approaches the infimum, a worst-case value (Coraluppi, 1997).", "startOffset": 56, "endOffset": 73}, {"referenceID": 33, "context": "Even ignoring underflow/overflow issues, REINFORCE (Williams, 1992) style algorithms would find difficulties, because deriving unbiased estimators of the ratio exp(\u03b2QT\u2212t(St, At, \u03b2) \u2212 \u03b2V \u03c0 T (s, \u03b2)) from single trajectories of the MDP may be hard.", "startOffset": 51, "endOffset": 67}, {"referenceID": 14, "context": "(Kantas et al., 2015), but for large T the estimate suffers from high mean squared errors.", "startOffset": 0, "endOffset": 21}, {"referenceID": 24, "context": "Risk sensitivity originates in the study of utility and choice in economics (Von Neumann & Morgenstern, 1953; Pratt, 1964; Arrow, 1974).", "startOffset": 76, "endOffset": 135}, {"referenceID": 1, "context": "Risk sensitivity originates in the study of utility and choice in economics (Von Neumann & Morgenstern, 1953; Pratt, 1964; Arrow, 1974).", "startOffset": 76, "endOffset": 135}, {"referenceID": 6, "context": "It has been extensively studied for the control of MDPs (Howard & Matheson, 1972; Coraluppi, 1997; Marcus et al., 1997; Borkar & Meyn, 2002; Mihatsch & Neuneier, 2002; B\u00e4uerle & Rieder, 2013).", "startOffset": 56, "endOffset": 191}, {"referenceID": 18, "context": "It has been extensively studied for the control of MDPs (Howard & Matheson, 1972; Coraluppi, 1997; Marcus et al., 1997; Borkar & Meyn, 2002; Mihatsch & Neuneier, 2002; B\u00e4uerle & Rieder, 2013).", "startOffset": 56, "endOffset": 191}, {"referenceID": 28, "context": "In reinforcement learning, risk sensitivity has been studied (Koenig & Simmons, 1994; Neuneier & Mihatsch, 1998; Shen et al., 2014), although none of these consider the direct policy gradient approach considered in this work.", "startOffset": 61, "endOffset": 131}, {"referenceID": 15, "context": "The idea of treating reinforcement learning as an inference problem is not a new idea (see e.g. Albertini & Runggaldier, 1988; Dayan & Hinton, 1997; Kappen, 2005; Toussaint & Storkey, 2006; Hoffman et al., 2007; Tishby & Polani, 2011; Kappen et al., 2012).", "startOffset": 86, "endOffset": 255}, {"referenceID": 11, "context": "The idea of treating reinforcement learning as an inference problem is not a new idea (see e.g. Albertini & Runggaldier, 1988; Dayan & Hinton, 1997; Kappen, 2005; Toussaint & Storkey, 2006; Hoffman et al., 2007; Tishby & Polani, 2011; Kappen et al., 2012).", "startOffset": 86, "endOffset": 255}, {"referenceID": 16, "context": "The idea of treating reinforcement learning as an inference problem is not a new idea (see e.g. Albertini & Runggaldier, 1988; Dayan & Hinton, 1997; Kappen, 2005; Toussaint & Storkey, 2006; Hoffman et al., 2007; Tishby & Polani, 2011; Kappen et al., 2012).", "startOffset": 86, "endOffset": 255}, {"referenceID": 14, "context": "These are studied for example in Albertini & Runggaldier (1988); Kappen (2005); Tishby & Polani (2011); Kappen et al.", "startOffset": 65, "endOffset": 79}, {"referenceID": 14, "context": "These are studied for example in Albertini & Runggaldier (1988); Kappen (2005); Tishby & Polani (2011); Kappen et al.", "startOffset": 65, "endOffset": 103}, {"referenceID": 14, "context": "These are studied for example in Albertini & Runggaldier (1988); Kappen (2005); Tishby & Polani (2011); Kappen et al. (2012); Fox et al.", "startOffset": 65, "endOffset": 125}, {"referenceID": 10, "context": "(2012); Fox et al. (2015);", "startOffset": 8, "endOffset": 26}, {"referenceID": 4, "context": "Yet, in certain special cases, risk sensitive objectives can also be cast as solutions to path integral control problems (Broek et al., 2012).", "startOffset": 121, "endOffset": 141}, {"referenceID": 14, "context": "Ruiz & Kappen (2016). The observation is that in an MDP with fully controllable transition dynamics, optimizing a policy \u03c0\u2032, which completely specifies the transition dynamics, achieves the risk sensitive value at \u03c0: max \u03c0\u2032 V\u0302 \u03c0,\u03c0 \u2032 T (s, \u03b2) = V \u03c0 T (s, \u03b2) (16) Note that this has an interesting connection to Bayesian inference.", "startOffset": 7, "endOffset": 21}, {"referenceID": 23, "context": "and since the bootstrap particle filter is unbiased (Del Moral, 2004; Pitt et al., 2012), = V \u03c0 T (s, \u03b2) (26) For \u03b2 < 0 we get the reverse inequality, V \u03c0 T,K(s, \u03b2) \u2265 V \u03c0 T (s, \u03b2).", "startOffset": 52, "endOffset": 88}, {"referenceID": 33, "context": "For \u03b2 = 0, we used instead a REINFORCE (Williams, 1992) estimator, that was simply estimated from the Monte Carlo returns.", "startOffset": 39, "endOffset": 55}], "year": 2017, "abstractText": "The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent\u2019s experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.", "creator": "LaTeX with hyperref package"}}}