{"id": "1703.05908", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2017", "title": "Learning Robust Visual-Semantic Embeddings", "abstract": "Many of the existing methods for learning joint embedding of images and text use only supervised information from paired images and its textual attributes. Taking advantage of the recent success of unsupervised learning in deep neural networks, we propose an end-to-end learning framework that is able to extract more robust multi-modal representations across domains. The proposed method combines representation learning models (i.e., auto-encoders) together with cross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn joint embeddings for semantic and visual features. A novel technique of unsupervised-data adaptation inference is introduced to construct more comprehensive embeddings for both labeled and unlabeled data. We evaluate our method on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with a wide range of applications, including zero and few-shot image recognition and retrieval, from inductive to transductive settings. Empirically, we show that our framework improves over the current state of the art on many of the considered tasks. Our approach highlights that a general approach to learning joint embedding using non-supervised learning could be developed in more recent years, particularly in AI. This approach offers insights into the feasibility and potential of implementing joint embedding in highly-trained systems, particularly in the deep learning field. We demonstrate that our approach to learning joint embedding can be applied in a variety of contexts with different types of data types: machine learning, machine learning, machine learning, machine learning, machine learning. This approach is aimed at reducing the amount of time and resource that is required to implement joint embedding in a highly-trained system. We also show that joint embedding is still possible using multiple-dimensional modeling techniques. In particular, we present a multi-parameter model that allows a model to be split into three dimensions: one dimension can be combined into two dimensions; two dimensions can be used to model multiple data types. We demonstrate that joint embedding can be applied in the deep learning field and in other environments where different inputs are required. We also show that it is possible to embed multi-dimensional modeling techniques. We demonstrate that joint embedding can be applied in a variety of contexts with different types of data types. The approach further shows that joint embedding can be applied in the deep learning field and in other environments where different inputs are required. We also show that joint embedding can be applied in a variety of contexts with different types of data types: machine learning, machine learning", "histories": [["v1", "Fri, 17 Mar 2017 06:59:51 GMT  (8099kb,D)", "http://arxiv.org/abs/1703.05908v1", "12 pages"], ["v2", "Mon, 20 Mar 2017 00:28:07 GMT  (8100kb,D)", "http://arxiv.org/abs/1703.05908v2", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["yao-hung hubert tsai", "liang-kang huang", "ruslan salakhutdinov"], "accepted": false, "id": "1703.05908"}, "pdf": {"name": "1703.05908.pdf", "metadata": {"source": "CRF", "title": "Learning Robust Visual-Semantic Embeddings", "authors": ["Yao-Hung Hubert Tsai", "Liang-Kang Huang", "Ruslan Salakhutdinov"], "emails": ["yaohungt@cs.cmu.edu", "liangkah@andrew.cmu.edu", "rsalakhu@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "Over the past few years, due to the availability of large amount of data and the advancement of the training techniques, learning effective and robust representations directly from images or text becomes feasible [19, 27, 32]. These learned representations have facilitated a number of high-level tasks, such as image recognition [40], sentence generation [17], and object detection [35]. Despite useful representations being developed for specific domains, learning more comprehensive representations across different data modalities remains challenging. In practice, more complex tasks, such as image captioning [45] and image tagging [23] often involve data from different modalities (i.e., images and text). Additionally, the learning process would be faster, requiring fewer labeled examples, and hence more scalable to handling a large number of categories if we could transfer cross-domain knowledge more effectively [9]. This motivates learning multi-modal em-\nbeddings. In this paper, we consider learning robust joint embeddings across visual and textual modalities in an endto-end fashion under zero and few-shot setting.\nZero-shot learning aims at performing specific tasks, such as recognition and retrieval of novel classes, when no label information is available during training [16]. On the other hand, few-shot learning enables us to have few labeled examples in our of-interest categories [38]. In order to compensate the missing information under the zero and fewshot setting, the model should learn to associate novel concepts in image examples with textual attributes and transfer knowledge from training to test classes. A common strategy for deriving the visual-semantic embeddings is to make use of images and textual attributes in a supervised way [41, 2, 48, 49, 50, 22, 7]. Specifically, one can learn transformations of images and textual attributes under the objective that the transformed visual and semantic vectors of the same class should be similar in the joint embeddings space. Despite good performance, this common strategy basically boils down to a supervised learning setting, learning from labeled or paired data only. In this paper, we show that to learn better joint embeddings across different data modalities, it is beneficial to combine supervised and unsupervised learning from both labeled and unlabeled data.\nOur contributions in this work are as follows. First, to extract meaningful feature representations from both labeled and unlabeled data, one possible option is to train an\n1\nar X\niv :1\n70 3.\n05 90\n8v 1\n[ cs\n.C V\n] 1\n7 M\nar 2\n01 7\nauto-encoder [33, 5]. In this way, instead of learning representations directly to align the visual and textual inputs, we choose to learn representations in an auto-encoder using reconstruction objective. Second, we impose a crossmodality distribution matching constraint to require the embeddings learned by the visual and textual auto-decoders to have similar distributions. By minimizing the distributional mismatch between visual and textual domain, we show improved performance on recognition and retrieval tasks. Finally, to achieve better adaptation on the unlabeled data, we perform a novel unsupervised-data adaptation inference technique. We show that by adopting this technique, the accuracy increases significantly not only for our method but also for many of the existing other models. Fig. 1 illustrates our overall end-to-end differentiable model.\nTo summarize, our proposed method successfully combines supervised and unsupervised learning objectives, and learns from both labeled and unlabeled data to construct joint embeddings of visual and textual data. We demonstrate improved performance on Animals with Attributes (AwA) [21] and Caltech-UCSD Birds 200-2011 [47] datasets on both image recognition and image retrieval tasks under zero and few-shot setting."}, {"heading": "2. Related Work", "text": "In this section, we provide an overview of learning multimodal embeddings across visual and textual domain.\nZero and Few-Shot Learning Zero-shot [7, 1, 2] and few-shot learning [8, 39, 20] are related problems, but somewhat different in the setting of the training data. While few-shot learning aims to learn specific classes through one or few examples, zero-shot learning aims to learn even when no examples of the classes are presented. In this setting, zero-shot learning should rely on the side information provided by other domains. In the case of image recognition, this often comes in the form of textual descriptions. Thus, the focus of zero-shot image recognition is to derive joint embeddings of visual and textual data, so that the missing information of specific classes could be transferred from the textual domain.\nSince the relation between raw pixels and text descriptions is non-trivial, most of the previous work relied on learning the embeddings through a large amount of data. Witnessing the success of deep learning in extracting useful representations, much of the existing work mostly applies deep neural networks to first transform raw pixels and text into more informative representations, followed by using various techniques to further identify the relation between them. For example, Socher et al. [41] used deep architectures [13] to learn representations for both images and text, and then used a Bayesian framework to perform classification. Norouzi et al. [29] introduced a simple idea\nthat treated classification scores output by the deep network [19] as weights in convex combination of word vectors. Fu et al. [10] proposed a method that learns projections from low-level visual and textual features to form a hypergraph in the embedding space and performed label propagation for recognition. A number of similar methods learn transformations from input image representations to the semantic space for the recognition or retrieval purposes [2, 49, 1, 48, 7, 50, 51, 30, 37, 6, 12].\nA number of recent approaches also attempt to learn the entire task with deep models in an end-to-end fashion. Frome et al. [9] constructed a deep model that took visual embeddings extracted by CNN [19] and word embeddings as input, and trained the model with the objective that the visual and word embeddings of the same class should be well aligned under linear transformations. Ba et al. [22] predicted the output weights of both the convolutional and fully connected layers in a deep convolutional neural network. Instead of using textual attributes or word embeddings model, Reed et al. [34] proposed to train a neural language model directly from raw text with the goal of encoding only the relevant visual concepts for various categories. Visual and Semantic Knowledge Transfer\nLiu et al. [24] developed multi-task deep visualsemantic embeddings model for selecting video thumbnails based on side semantic information (i.e., title, description, and query). By incorporating knowledge about objects similarities between visual and semantic domains, Tang et al. [44] improved object detection in a semisupervised fashion. Kottur et al. [18] proposed to learn visually grounded word embeddings (vis-w2v) and showed improvements over text only word embeddings (word2vec) on various challenging tasks. Reed et al. designed a text-conditional convolutional GAN architecture to synthesize an image from text. Recently, Wang et al. [46] introduced structure-preserving constraints in learning joint embeddings of images and text for image-to-sentence and sentence-to-image retrieval tasks. Unsupervised Multi-modal Representations Learning\nOne of our key contributions is to effectively combine supervised and unsupervised learning tasks for learning multi-modal embeddings. This is inspired and supported by several previous works that provided evidence of how unsupervised learning tasks could benefit cross-modal feature learning.\nNgiam et al. [28] proposed various models based on Restricted Boltzmann Machine, Deep Belief Network, and Deep Auto-encoder to perform feature learning over multiple modalities. The derived multi-modal features demonstrated an improved performance over single-modal features on the audio-visual speech classification tasks. Srivastava and Salakhutdinov [42] developed a Multimodal Deep\nBoltzmann Machine for fusing together multiple diverse modalities even when some of them are absent. Providing inputs of images and text, their generative model manifested noticeable performance improvement on classification and retrieval tasks."}, {"heading": "3. Proposed Method", "text": "First, we define the problem setting and the corresponding notation. Let Vtr = {v(tr)i } Ntr i=1 denote labeled training images from Ctr classes, Vut = {v(ut)i } Nut i=1 denote unlabeled training images from Cut possibly different classes, and Vte = {v(te)i } Nte i=1 denote test images from Cte novel classes. For each class, following [49, 50, 51, 2, 48, 7], its textual attributes are either provided from human annotated attributes [21] or learned from unsupervised text corpora (Wikipedia) [32]. We denote these class-specific textual attributes as Ttr = {t(tr)c }Ctrc=1, Tut = {t (ut) c }Cutc=1, and Tte = {t(te)c }Ctec=1 for labeled training, unlabeled training, and test classes, respectively.\nUnder zero-shot setting, our goal is to predict labels of the test images coming from novel, previously unseen, classes given textual attributes. That is, for a given test image v(te)i , its label is determined by\nargmax c\u2208{1,...,Cte} P\u03b8\n( c \u2223\u2223\u2223t(te)c , v(te)i ) , (1)\nwhere \u03b8 denotes model parameters. We will also consider a few-shot learning, where a few labeled training images are available in each of the test classes. In the following, we omit the model parameters \u03b8 for brevity."}, {"heading": "3.1. Basic Formulation", "text": "The goal of learning multi-modal embeddings can be formulated as learning transformation functions fv and ft, such that given an image v and a textual attribute t, fv(v) should be similar to ft(t) if v and t are of the same class. Much of the previous work for learning multi-modal embeddings can be generalized to this formulation. For instance, in Cross-Modal Transfer (CMT) [41], fv(\u00b7) can be viewed as a pre-defined feature extraction model followed by a two-layer neural network, while ft(\u00b7) is set to an identity matrix. To be more specific, [41] aim at learning a nonlinear projection directly from visual features to semantic word vectors.\nOver the past few years, deep architectures have been shown to learn useful representations that could embed high-level semantics for both visual and textual data. This gives rise to the attempts of applying successful deep architectures to learn fv(\u00b7) and ft(\u00b7). For example, DeViSE [9] designed fv(\u00b7) as a CNN model followed by a linear transformation matrix. On the other hand, they adopted the well known skip-gram text modeling architecture [27] for learning ft(\u00b7) from raw text on Wikipedia. It is worth noting\nthat, to further take advantage of previous success, these deep models are often pre-trained on large datasets where they have shown to learn effective representations.\nFigure 2 shows the basic formulation of the visualsemantic embeddings model. Our method is built on top of this basic architecture by adding additional components as well as modifying existing ones."}, {"heading": "3.2. Reconstructing Features from Auto-Encoder", "text": "Although the basic architecture provides a way to utilize label information during training, the learning process could further benefit if unlabeled data are provided at the same time. To be more specific, we propose to combine supervised and unsupervised learning objectives together by incorporating auto-encoders [4] for both image and text data. Typical setting of auto-encoders consists of a symmetric encoder-decoder architecture, with the hidden representations in the middle being compact representations that could be used to reconstruct the original input data. In our model, the auto-encoders are added after the image and text data are processed by the pre-trained networks. For learning visual embeddings, we use contractive auto-encoder proposed by [36], which is able to learn more robust visual codes for the images of same class. Given a visual feature vector v\u0303, the contractive auto-encoder maps v\u0303 to a hidden representation v\u0303h, and seeks to reconstruct v\u0303 from v\u0303h. Let us denote the reconstructed vector by rv(\u00b7). Model parameters are thus learned by minimizing the regularized reconstruction error\nLv = 1\nNtr Ntr\u2211 i=1 \u2016v\u0303i \u2212 rv(v\u0303i)\u20162 + \u03b3\u2016J(v\u0303i)\u20162F , (2)\nwhere J(\u00b7) is the Jacobian matrix [36]. On the other hand, for a given semantic feature vector or textual attribute t, we use a vanilla auto-encoder to first encode and then reconstruct from its hidden representation th. We hence minimize the reconstruction error\nLt = 1\nCtr Ctr\u2211 c=1 \u2016tc \u2212 rt(tc)\u20162. (3)\nCombining (2) and (3) gives us the reconstruction loss\nLreconstruct = Lv + Lt. (4)\nIn practice, if we have access to a large unlabeled set or a set of test inputs (without labels), we can easily incorporate them into the reconstruction loss. In our experimental results, we find that with the visual and textual auto-encoders, image and text data are transformed into visual and textual embeddings with more meaningful representations. In order to further transfer the knowledge across modalities, we impose discriminative constraints on the hidden representations (v\u0303h and th) learned by these auto-encoders, as we discuss next."}, {"heading": "3.3. Cross-Modality Distributions Matching", "text": "Distributions matching technique has been proven to be effective for transferring knowledge from one modality to another [31, 25, 14]. A common nonparametric way to analyze and compare distributions is to use Maximum Mean Discrepancy (MMD) [11] criterion. We can view MMD as a two-sample test on v\u0303h and th, and thus its loss can be formulated as\nLMMD = \u2016Ep[\u03c6(v\u0303h)]\u2212Eq[\u03c6(th)]\u20162Hk , (5)\nwhere p, q are the distributions of visual and textual embeddings (i.e., v\u0303h \u223c p and th \u223c q), \u03c6 is the feature map with canonical form \u03c6(x) = k(x, \u00b7), and Hk is the reproducing kernel Hilbert space (RKHS) endowed with a characteristic kernel k. Note that the kernel in the MMD criterion must be a universal kernel, and thus we empirically choose a Gaussian kernel:\nk(x,x\u2032) = exp ( \u2212\u03ba \u2016x\u2212 x\u2032\u20162 ) . (6)\nWe can now minimize the MMD criterion between visual and textual embeddings by minimize eq. (5). This can be further regarded as shrinking the gap between information across two data modalities. In our experiments, we find that the MMD loss helps improve model performance on both recognition and retrieval tasks in zero and few-shot setting."}, {"heading": "3.4. Learning", "text": "After we derive the hidden representations v\u0303h and th, the transformation functions fv(\u00b7) and ft(\u00b7) can be reformulated as\nfv(v) = f \u2032 v(v\u0303h) and ft(t) = f \u2032 t(th), (7)\nwhere f \u2032v(\u00b7) and f \u2032t(\u00b7) are the mapping functions from the hidden representations to the visual and textual output.\nTo leverage the supervised information from labeled training images Vtr and the corresponding textual attributes Ttr, we minimize the binary prediction loss:\nLsupervised = \u2212 1\nNtr Ntr\u2211 i=1 Ctr\u2211 c=1 Ii,c \u2329 f \u2032v(v\u0303 (tr) h,i ), f \u2032 t(t (tr) h,c ) \u232a ,\n(8) where Ii,c indicates a {0, 1} encoding of positive and negative classes and \u3008\u00b7\u3009 denotes a dot-product. It is worth noting that we can adopt different loss functions, including binary cross-entropy loss or multi-class hinge loss. However, empirically, using the simple binary prediction loss results in the best performance of our model.\nSimilar to eq. (8), we adopt the binary prediction loss for unlabeled training images Vut and the attributes Tut:\nLunsupunlab = \u2212 1\nNut Nut\u2211 i=1 Cut\u2211 c=1 I\u0302 (ut) i,c \u2329 f \u2032v(v\u0303 (ut) h,i ), f \u2032 t(t (ut) h,c ) \u232a ,\n(9) where\nI\u0302 (ut) i,c = 1 if c = argmaxc\u2208{1,...,Cut} \u2329 f \u2032v(v\u0303 (ut) h,i ), f \u2032 t(t (ut) h,c ) \u232a 0 otherwise.\n(10) We refer to eq. (9) as unsupervised-data adaptation inference, which acts as a self-reinforcing strategy using the unsupervised data with unknown labels. The intuition is that by minimizing eq. (9), we can further adapt our unlabeled data into the learning of f \u2032v(\u00b7) and f \u2032t(\u00b7) based on the empirical predictions. The choice of \u03bb does influence its effectiveness. However, we find that setting \u03bb = 1.0 works quite well for many methods we considered in this work.\nIn sum, our model learns by minimizing the total loss from both supervised and unsupervised objectives:\nLTotal = Lsupervised + \u03b1Lunsupervised, (11)\nwhere\nLunsupervised = Lreconstruct + \u03bbLunsupunlab + \u03b2LMMD, (12)\nwith \u03b1, \u03bb, and \u03b2 representing the trade-off parameters for different components. Note that we can also view the unsupervised objective here as a regularizer for learning more robust visual and textual representations (see Figure 1 for our overall model architecture).\nBefore computing the loss, we find it useful to perform `2 normalization on the output scores fv(v) and ft(t) along the batch-wise direction. It can be viewed as a mixture of Batch Normalization [15] and Layer Normalization [3]. The idea is simple, we encourage the competence between different instances in the data batch, rather than across different categories."}, {"heading": "4. Experiments", "text": "In the experiments, we denote our proposed method as ReViSE (Robust sEmi-supervised Visual-Semantic Embeddings). Extensive experiments on zero and few-shot image recognition and retrieval tasks are conducted using two benchmark datasets: Animals with Attributes (AwA) [21] and Caltech-UCSD Birds 200-2011 (CUB) [47]. CUB is a fine-grained dataset in which the objects are both visually and semantically very similar, while AwA is a more general concept dataset. We use the same training (+validation)/ test splits as in [2, 48]. Table 1 lists the statistics of the datasets.\nTo verify the performance of our method, we consider two state-of-the-art deep-embeddings methods: CMT [41] and DeViSE [9]. CMT and DeViSE can be viewed as a special case of our proposed method with \u03b1 = 0 (without using unsupervised objective in eq. (11)). The difference between them is that DeViSE learns a nonlinear transformation on raw visual images and textual attributes for the alignment purpose, while CMT only learns the nonlinear transformation from visual to semantic embeddings.\nWe choose GoogLeNet [43] as the CNN model in DeViSE, CMT, and our architecture. For the textual attributes of classes, we consider three alternatives: human annotated attributes (att) [21], Word2Vec attributes (w2v ) [27], and Glove attributes (glo) [32]. att are continuous attributes judged by humans: CUB contains 312 attributes and AwA contains 85 attributes. w2v and glo are unsupervised methods for obtaining distributed text representations of words. We use the pre-extracted Word2Vec and Glove vectors from Wikipedia provided by [2, 48]. Both w2v and glo are 400- dim. features."}, {"heading": "4.1. Network Design and Training Procedure", "text": "Please see Supplementary for the design details of ReViSE and its parameters. Note that we report results averaged over 10 random trials."}, {"heading": "4.2. Zero-Shot Learning", "text": "Following the partitioning strategy of [2, 48], we split AwA dataset into 30/10/10 classes and CUB dataset into 100/50/50 classes for labeled training/ unlabeled training/ test data. We adopt att attributes as a textual description of each class. For zero-shot learning, not only the labels of images are unknown in the unlabeled training and test set, but classes are also disjoint across labeled training/ unlabeled training/ test splits.\nTo verify how unlabeled training data could benefit the learning of ReViSE, we provide four variants: ReViSEa, ReViSEb, ReViSEc, and ReViSE. ReViSEa is when we only consider supervised objective. That is, \u03b1 = 0 in eq. (11). ReViSEb is when we further take unsupervised objective in labeled training data into account; that is, only Lreconstruct and LMMD are considered in Lunsupervised (see eq. (12)) for labeled training data. Next, for ReViSEc, we consider unlabeled training data in Lunsupervised without unsupervised-data adaptation technique (setting \u03b2 = 0). Last, ReViSE denotes our complete training architecture.\nFor completeness, we also consider the technique of unsupervised-data adaptation inference (see section 3.4) for DeViSE [9] and CMT [41]. In other words, we also evaluate how DeViSE and CMT benefit from the unlabeled training data. We adopt the same procedure as in eq. (9) and report results as DeViSE* and CMT*, respectively.\nSimilar to [49, 50, 51], the results and comparisons are reported using top-1 classification accuracy (top-1) (see Table 2) and mean average precision (mAP) (see Table 3) for recognition and retrieval tasks, respectively, on the unlabeled training and test images. To be more specific, we define the prediction score as y\u0302(\u00b7)i,c = \u2329 f \u2032v(v\u0303 (\u00b7) h,i), f \u2032 t(t (\u00b7) h,c) \u232a for a given image v(\u00b7)i and textual attributes t (\u00b7) c for class c. Results are provided after ranking y\u0302(\u00b7)i,c on all unlabeled training or test classes.\nTable 2 and 3 list the results for our zero-shot recognition and retrieval experiments. We first observe that NOT all the methods benefit from using unlabeled training data dur-\ning training. For example, in AwA dataset for test images Vte, there is a 2.7% retrieval deterioration from DeViSE to DeViSE* and a 3.1% recognition deterioration from CMT to CMT*. On the other hand, our proposed method enjoys 2.6% recognition improvement and 0.4% retrieval improvement from ReViSEb to ReViSEc. This shows that the learning method of our proposed architecture can actually benefit from unlabeled training data Vut and Tut.\nNext, we examine different variants in our proposed architecture. Comparing the average results from ReViSEa to ReViSEb, we observe 3.4% recognition improvement and 3.9% retrieval improvement. This indicates that taking unsupervised objectives Lreconstruct and LMMD into account results in learning better feature representations and thus yields a better recognition/ retrieval performance. Moreover, when unsupervised-data adaptation technique is introduced, we enjoy 5.0% average recognition improvement and 5.8% average retrieval improvement from ReViSEc to ReViSE. It is worth noting that the significant performance improvement for unlabeled training images Vut further verifies that our unsupervised-data adaptation technique leads to a more accurate prediction on Vut."}, {"heading": "4.3. Transductive Zero-Shot Learning", "text": "In this subsection, we extend our experiments to a transductive setting, where test data are available during training. Therefore, the test data can now be regarded as the unlabeled training data (Vtr = Vut and Ttr = Tut). To perform the experiments, as in Table 1, we split AwA dataset into 40/10 disjoint classes and CUB dataset into 150/50 disjoint classes for labeled training/ test data.\nIn order to evaluate different components in ReViSE, we further provide two variants: ReViSE\u2020 and ReViSE\u2020\u2020. ReViSE\u2020 is when we consider no distributional matching between the codes across modalities (\u03b2 = 0). ReViSE\u2020\u2020 is when we further consider no contractive loss in our visual auto-encoder (\u03b2 = \u03b3 = 0). Similar to previous subsection, we also consider DeViSE*, CMT*, and ReViSEc to evaluate the effect of our unsupervised-data adaptation inference.\nZero-Shot Recognition: Table 9 reports top-1 classification accuracy. Observe that ReViSE clearly outperforms\nother state-of-the-art methods by a large margin. On average, we have at least 17% gain compared to the methods without using unsupervised objective and 7.5% gain compared to DeViSE* and CMT*. Note that all the methods work better on human annotated attributes (att) than on unsupervised attributes (w2v and glo) in CUB dataset. One possible reason is that for visually and semantically similar classes in a fine-grained dataset (CUB), attributes obtained in an unsupervised way (glo word vectors) cannot fully differentiate between them. Nonetheless, for the more general concept dataset AwA, using either supervised or unsupervised textual attributes, the performance does not differ by that much. For instance, our method achieves comparable performance using att, w2v, and glo (93.4%, 93.5%, and 92.2% top-1 classification accuracy) on AwA dataset.\nThe recognition performance for DeViSE* and CMT* (60.6% and 60.5% on average) compared to DeViSE and CMT (49.3% and 50.5% on average) further verifies that using unsupervised-data adaptation inference technique does benefit transductive zero-shot recognition. Furthermore, all of the variants of ReViSE using unsupervised-data adaptation inference (ReViSE\u2020\u2020, ReViSE\u2020, and ReViSE itself) have noticeable improvement over DeViSE* and CMT*. This shows that the proposed model succeeds in leveraging unsupervised information in test data for constructing more effective cross-modal embeddings.\nNext, we evaluate the effects of different components designed in our architecture. First of all, we compare the results between ReViSE\u2020 (set \u03b2 = 0) and ReViSE. The performance gain (66.8% to 68.1% on average) indicates that minimizing MMD distance between visual and textual codes enables our model to learn more robust visualsemantic embeddings. In other words, we can better associate cross-modal information when we match the distributions across visual and textual domains (please refer to Supplementary for the study of MMD distance). Second, we observe that, without contractive loss, performance slightly drop from 66.8% (ReViSE\u2020) to 65.8% (ReViSE\u2020\u2020). This is not surprising since the contractive auto-encoder aims at learning less varying features/codes with similar visual input, and therefore we can expect to learn more robust visual codes. Finally, similar to the observations found in comparing DeViSE/CMT to DeViSE*/CMT*, the unsupervised-data adaptation inference in ReViSE substantially improves the average top-1 classification accuracy from 53.6% (ReViSEc) to 68.1% (ReViSE). Please see Supplementary material for more detailed comparisons to the following non-deep-embeddings methods: SOC [30], ConSE [29], SSE [49], SJE [2], ESZSL [37], JLSE [50], LatEm [48], Sync [7], MTE [6], TMV [10], and SMS [12].\nZero-Shot Retrieval: In Table 5, we report zero-shot retrieval results by measuring the retrieval performance by mean average precision (mAP). On average, methods that\nleverage unsupervised information yield better performance compared to the methods using no unsupervised objective. However, in few cases, the performance drops when we take unsupervised information into account. For example, on CUB dataset, DeViSE* performs unfavorably compared to DeViSE when w2v and glo word embeddings are used as textual attributes.\nOverall, our method does help improve zero-shot retrieval by at least 14.1% compared to CMT*/DeViSE* and 21.5% compared to CMT/DeViSE. It clearly demonstrates the effectiveness of leveraging unsupervised information for improving zero-shot retrieval (please see Supplementary for the plot of precision-recall curves).\nIn addition to quantitative results, we also provide qualitative results of ReViSE. Fig. 3 is the image retrieval experiments for classes Chestnut sided Warbler and White eyed Vireo. Given a class embedding, the nearest image neighbors are retrieved based on the cosine similarity between transformed visual and textual features. We consider two conditions: images from the same class and images from all test classes. In Chestnut sided Warbler, most of the images (71.7%) are correctly classified, and we also observe that three nearest image neighbors are also in Chestnut sided Warbler. On the other hand, only 43.3% images are correctly classified in White eyed Vireo, and two of the three nearest image neighbors are form wrong class Wilson Warbler.\nAvailability of Unlabeled Test Images: We next evaluate the performance of our method w.r.t. to the availability of\nCUB with human annotated attributes\ntest images for unsupervised objective (see Fig. 4) on CUB dataset with att attributes. We alter the fraction p of unlabeled test images used in the training stage from 0% to 100% by a step size of 10%. That is, in eq. (12), only p portion (randomly chosen) of test images contributes to Lunsupervised. Fig. 4 clearly indicates the performance increases when p increases. That is, with more unsupervised information (test images) available, our model can better associate the supervised and unsupervised data. Another interesting observation is that with only 40% test images available, ReViSE achieves favorable performance on both transductive zero-shot recognition and retrieval. Expand the test-time search space: Note that most of the methods [9, 41, 29, 49, 2, 50, 48, 7, 10] consider that, at test time, queries come from only test classes. For AwA dataset with att attributes, we expand the test-time search space to all training and test classes and perform transductive zero-shot recognition for DeViSE*, CMT*, and ReViSE. We discover severe performance drops from 90.7%, 89.4%, and 93.4% to 47.4%, 45.8%, and 42.5%. Similar results can also be observed in other non-deep-embeddings methods. Although challenging, it remains interesting to consider this generalized zero-/few-shot learning setting in our future work."}, {"heading": "4.4. From Zero to Few-Shot Learning", "text": "In this subsection, we extend our experiments from transductive zero-shot to transductive few-shot learning. Compared to zero-shot learning, few-shot learning allows us to have a few labeled images in our test classes. Here, 3 images are randomly chosen to be labeled per test category. We use the same performance comparison metrics as in Sec. 4.2 to report the results.\nTransductive Few-Shot Recognition and Retrieval: Tables 6 and 7 list the results of transductive few-shot recognition and retrieval tasks. Generally speaking, ReViSE achieves the best performance compared to its variants and other methods. Moreover, as expected, when we compare the results with transductive zero-shot recognition (Table 9) and retrieval (Table 5), every methods perform better when few (i.e., 3) labeled images are observed in the test classes. For example, for CUB dataset with w2v attributes, there is a 22.5% recognition improvement for CMT* and a 32.3%\nretrieval improvement for ReViSE. We also observe that the performance gap between our proposed ReViSE and other methods is reduced compared to transductive zero-shot learning. For instance, in average retrieval performance, ReViSE has 15.5% mAP improvement over DeViSE* under zero-shot experiments, while only 9.3% improvement under few-shot experiments."}, {"heading": "4.5. t-SNE Visualization", "text": "Figure 5 further shows the t-SNE [26] visualization for the original CNN features, the reconstructed visual features rv(v\u0303\n(te)), and the visual codes v\u0303(te)h on AwA dataset with glo attributes under transductive zero-shot setting. First of all, observe that both the reconstructed features and the visual codes have more separate clusters over different classes, which suggest ReViSE has learned useful representations. Another interesting observation is that the affinities between classes might change after learning visual codes. For example, \u201cleopard\u201d images (green dots) are near \u201chumpback whale\u201d images (light purple dots) in the original CNN feature space. However, in the visual code space, leopard images are far from humpback whale images. One possible explanation is that we know leopard is semantically\ndistinct from humpback whale, and thus their semantic attributes must also be very different. This leads to different image clusters in our designed framework.\nNext, we provide the t-SNE visualization on the output visual test scores fv(Vte) for DeViSE*, CMT*, and ReViSE in Fig. 6. Clearly, ReViSE can better separate instances from different classes."}, {"heading": "5. Conclusion", "text": "In this paper, we showed how we can augment a typical supervised formulation with unsupervised techniques for learning joint embeddings of visual and textual data. We empirically evaluate our proposed method on both general and fine-grained image classification datasets, with comparisons against the state-of-the-art methods in zero-shot and few-shot recognition and retrieval tasks, from inductive to transductive setting. In all the experiments, our method consistently outperforms other methods, substantially improving performance in some cases. We believe that this work sheds light on the advantages of combining supervised and unsupervised learning techniques, and makes a step towards learning more useful representations from multimodal data."}, {"heading": "6. Network Design", "text": "Fig. 7 provides an easy-to-understand design of ReViSE. In all of our experiments, GoogLeNet is pre-trained on ImageNet [1] images. Without fine-tuning, we directly extract the top layer activations (1024-dim) as our input image features followed by a common log(1+v) pre-processing step. For the textual attributes, we pre-process them through a standard l2 normalization.\nIn ReViSE, we set \u03b1 = 1.0 in eq. (11), so that we place equal importance on supervised and unsupervised objectives. For the visual auto-encoder, we fix the parameter of the contraction strength \u03b3 = 0.1 in eq. (2). The encoding of visual features is parameterized by a two-hidden layer fullyconnected neural network with architecture dv1\u2212 dv2\u2212 dc, where dv1 = 1024 is the input dimension of the visual features, dv2 = 500 is the intermediate layer, and dc denotes the dimension of the visual codes v\u0303h. To encode textual attributes, we consider a single-hidden layer neural network dt1 \u2212 dc, where dt1 is the input dimension of the textual attributes. We choose dc = 100 when dt1 > 100 and dc = 75 when dt1 < 100. Furthermore, we do not tie the weights to be learned between the decoding and encoding parts. Parameters for associating distributions of visual and textual codes (MMD Loss) in eqs. (5) (12), and (6) are set as \u03b2 = {0.1, 1.0} (chosen by cross-validation) and \u03ba = 32.0. For the remaining part of our model, we set the architecture of visual and textual code mapping as a single-hidden layer fully-connected neural network with dimension dc\u221250. We also adopt a dropout of 0.7.\nDuring the first 100 iterations of training, we set \u03bb = 0 so that no unsupervised-data adaptation is used while still updating I\u0302(ut)i,c . Note that I\u0302 (ut) i,c are the inferred labels for unsupervised data, and not random at each iteration. Beginning with the 101th iteration, we set \u03bb = {0.1, 1.0} (chosen by cross-validation), and the model typically converges within 2000 to 5000 iterations.\nWe implement ReViSE in TensorFlow [2]. We use Adam [3] for optimization with with minibatches of size 1024. We choose tanh for all of our activation functions."}, {"heading": "7. Parameters Choice", "text": "We have four parameters in our architecture: \u03b1, \u03b2, \u03b3, and \u03ba. We fix \u03b1 = 1.0, \u03b3 = 0.1, \u03ba = 32.0 for all the experiments. Then we set \u03bb = 0.0 (no unsuperviseddata adaptation inference), and perform cross-validation on the splitting set as suggested by [3,46] to determine \u03b2 from {0.1, 1.0}. Next, with chosen \u03b2, we perform crossvalidation to choose \u03bb from {0.1, 1.0}. Table 8 lists the statistics of \u03b2 and \u03bb.\nNext, we study the power of unsupervised information. We now take CUB dataset with att attributes to test the advantage of using unsupervised information, which can be\nviewed as tuning the parameter \u03b1 for the unsupervised objective in eq. (11). Originally, \u03b1 was set to 1.0, which equally weights the contribution of supervised and unsupervised loss. We now alter \u03b1 as follows: 0.1 to 1.0 by step size of 0.1 and 0.5 to 5.0 by step size of 0.5. The results are shown in Fig. 8. We observe that when \u03b1 increases from 0.1 to 1.0, the performance increases; however, when \u03b1 increase from 1.0 to 5.0, the performance stays relatively unchanged. Empirically, we find that ReViSE does not perform better when \u03b1 > 1.0, which is expected, since we should not view unsupervised information more important than supervised information."}, {"heading": "8. Precision-Recall Curve", "text": "Fig. 10 is the precision-recall curve for zero-shot retrieval results on CUB dataset with att attributes."}, {"heading": "9. MMD Distance", "text": "MMD distance in eq. (5) can be viewed as the distribution measurement [13] between visual and textual code. For CUB dataset with att attributes under transductive zero-shot experiment, we calculate the MMD distance (on the test codes) in our method with (ReViSE) and without (ReViSE\u2020) LMMD. The results of MMD distance w.r.t. the number of iterations are shown in Fig. 9. We clearly observe that the red curve (ReViSE) has consistently lower value than the blue curve (ReViSE\u2020). Moreover, based on the previous results, ReViSE always performs better than ReViSE\u2020. Hence aligning the distributions across visual and textual codes can better associate cross-modal information and thus lead to more robust visual-semantic embeddings."}, {"heading": "10. Remarks on Contractive Loss", "text": "We find that adding contractive loss to textual autoencoder doesn\u2019t provide much benefit. One possible reason may be the limited number of textual features (200 for CUB). On the other hand, the number of visual features is large (11, 786 for CUB)."}, {"heading": "11. Comparing with recent state-of-the-art methods", "text": "In our main paper, we focus on comparing with deepembeddings methods. In Table 9, we compare other methods for inductive and transductive zero-shot learning. Note that SMSESZSL adopts ESZSL for its initialization."}], "references": [{"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng"], "venue": "IEEE CVPR", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["Abadi"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "ICLR 2015", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "A common strategy for deriving the visual-semantic embeddings is to make use of images and textual attributes in a supervised way [41, 2, 48, 49, 50, 22, 7].", "startOffset": 130, "endOffset": 156}, {"referenceID": 0, "context": "Zero-shot [7, 1, 2] and few-shot learning [8, 39, 20] are related problems, but somewhat different in the setting of the training data.", "startOffset": 10, "endOffset": 19}, {"referenceID": 1, "context": "Zero-shot [7, 1, 2] and few-shot learning [8, 39, 20] are related problems, but somewhat different in the setting of the training data.", "startOffset": 10, "endOffset": 19}, {"referenceID": 1, "context": "A number of similar methods learn transformations from input image representations to the semantic space for the recognition or retrieval purposes [2, 49, 1, 48, 7, 50, 51, 30, 37, 6, 12].", "startOffset": 147, "endOffset": 187}, {"referenceID": 0, "context": "A number of similar methods learn transformations from input image representations to the semantic space for the recognition or retrieval purposes [2, 49, 1, 48, 7, 50, 51, 30, 37, 6, 12].", "startOffset": 147, "endOffset": 187}, {"referenceID": 1, "context": "For each class, following [49, 50, 51, 2, 48, 7], its textual attributes are either provided from human annotated attributes [21] or learned from unsupervised text corpora (Wikipedia) [32].", "startOffset": 26, "endOffset": 48}, {"referenceID": 2, "context": "It can be viewed as a mixture of Batch Normalization [15] and Layer Normalization [3].", "startOffset": 82, "endOffset": 85}, {"referenceID": 1, "context": "We use the same training (+validation)/ test splits as in [2, 48].", "startOffset": 58, "endOffset": 65}, {"referenceID": 1, "context": "We use the pre-extracted Word2Vec and Glove vectors from Wikipedia provided by [2, 48].", "startOffset": 79, "endOffset": 86}, {"referenceID": 1, "context": "Following the partitioning strategy of [2, 48], we split AwA dataset into 30/10/10 classes and CUB dataset into 100/50/50 classes for labeled training/ unlabeled training/ test data.", "startOffset": 39, "endOffset": 46}, {"referenceID": 1, "context": "Please see Supplementary material for more detailed comparisons to the following non-deep-embeddings methods: SOC [30], ConSE [29], SSE [49], SJE [2], ESZSL [37], JLSE [50], LatEm [48], Sync [7], MTE [6], TMV [10], and SMS [12].", "startOffset": 146, "endOffset": 149}, {"referenceID": 1, "context": "Expand the test-time search space: Note that most of the methods [9, 41, 29, 49, 2, 50, 48, 7, 10] consider that, at test time, queries come from only test classes.", "startOffset": 65, "endOffset": 98}], "year": 2017, "abstractText": "Many of the existing methods for learning joint embedding of images and text use only supervised information from paired images and its textual attributes. Taking advantage of the recent success of unsupervised learning in deep neural networks, we propose an end-to-end learning framework that is able to extract more robust multi-modal representations across domains. The proposed method combines representation learning models (i.e., auto-encoders) together with cross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn joint embeddings for semantic and visual features. A novel technique of unsupervised-data adaptation inference is introduced to construct more comprehensive embeddings for both labeled and unlabeled data. We evaluate our method on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with a wide range of applications, including zero and fewshot image recognition and retrieval, from inductive to transductive settings. Empirically, we show that our framework improves over the current state of the art on many of the considered tasks.", "creator": "LaTeX with hyperref package"}}}