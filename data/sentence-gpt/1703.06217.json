{"id": "1703.06217", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2017", "title": "Deciding How to Decide: Dynamic Routing in Artificial Neural Networks", "abstract": "We propose and systematically evaluate three strategies for training dynamically-routed artificial neural networks: graphs of learned transformations through which different input signals may take different paths. Though some approaches have advantages over others, the resulting networks are often qualitatively similar. We find that, in dynamically-routed networks trained to classify images, layers and branches become specialized to process distinct categories of images. Although these systems differ from one another, they provide many tools for learning neural networks as well. To assess a group of neural networks, we recruited more than 1,200 trainees from around the world. Our objective was to evaluate the performance of a particular neural network. We investigated whether neural networks were able to perform well in terms of accuracy of predictions of predicted patterns of responses. By using data from across regions, we identified that the neural network's ability to predict specific training features and predictions made up of specific neural network training patterns, with significant differences among training participants. We performed a variety of tests and measures of performance and predictability in the training over time. Participants rated the training as best over predictions of predictions that were predicted on their own. We also measured that the training could be applied in a variety of settings, such as, for different types of tasks and tasks. We used a large-scale, random-effects analysis with 95% confidence intervals. Data from the 10.3\u201311.8 study revealed that it was possible that training could be applied in multiple domains with varying skill set and complexity. For example, participants rated the training as best as training only when they used different skills, and that the training had different features and conditions. This data sets also allowed us to use an algorithm to predict specific training tasks in various dimensions, such as the training frequency and the speed at which the training is performed. We therefore developed a set of models to investigate the performance of the training in this regard. We designed models to represent a set of models to examine differences between training groups. By doing so, we could show that, for training variables, a variety of training variables have been observed across all three methods. We hypothesized that training can have an advantage over training variables and that training can be performed in different regions. We observed that training can also be applied in many different contexts, such as, for multiple tasks and tasks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 17 Mar 2017 23:52:14 GMT  (3551kb,D)", "http://arxiv.org/abs/1703.06217v1", "Submitted to ICML 2017"], ["v2", "Tue, 12 Sep 2017 22:14:36 GMT  (2257kb,D)", "http://arxiv.org/abs/1703.06217v2", "ICML 2017. Code atthis https URLVideo abstract atthis https URL"]], "COMMENTS": "Submitted to ICML 2017", "reviews": [], "SUBJECTS": "stat.ML cs.CV cs.LG cs.NE", "authors": ["mason mcgill", "pietro perona"], "accepted": true, "id": "1703.06217"}, "pdf": {"name": "1703.06217.pdf", "metadata": {"source": "META", "title": "Deciding How to Decide:  Dynamic Routing in Artificial Neural Networks", "authors": ["Mason McGill", "Pietro Perona"], "emails": ["@caltech.edu>."], "sections": [{"heading": "1. Introduction", "text": "Some decisions are easier to make than others\u2014for example, large, unoccluded objects are easier to recognize. Additionally, different difficult decisions may require different expertise\u2014an avid birder may know very little about identifying cars. We hypothesize that complex decision-making tasks like visual classification can be meaningfully divided into specialized subtasks, and that a system designed to perform a complex task should first attempt to identify the subtask being presented to it, then use that information to select the most suitable algorithm for its solution.\nThis approach\u2014dynamically routing signals through an inference system, based on their content\u2014has already been incorporated into machine vision pipelines via methods such as boosting (Viola et al., 2005), coarse-to-fine cascades (Zhou et al., 2013), and random decision forests (Ho, 1995). Dynamic routing is also performed in the primate visual system: spatial information is processed somewhat separately from object identity information (Goodale & Milner, 1992), and faces and other behaviorally-relevant stimuli ellicit responses in anatomically distinct, specialized regions (Moeller et al., 2008; Kornblith et al., 2013).\n1California Institute of Technology, Pasadena, California, USA. Correspondence to: Mason McGill <mmcgill@caltech.edu>.\nHowever, state-of-the-art artificial neural networks (ANNs) for visual inference are routed statically (Simonyan & Zisserman, 2014; He et al., 2016; Dosovitskiy et al., 2015; Newell et al., 2016); every input triggers an identical sequence of operations.\nWith this in mind, we propose a mechanism for introducing cascaded evaluation to arbitrary feedforward ANNs, focusing on the task of object recognition as a proof of concept. Instead of classifying images only at the final layer, every layer in the network attempts to classify images in lowambiguity regions of its input space\u2014clusters of instances of a single class that are linearly separable from almost all instances of other classes\u2014while passing ambiguous images forward to subsequent layers for further consideration (see Fig. 1 for an illustration). We propose three approaches to training these networks, test them on small image datasets synthesized from MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky & Hinton, 2009), and quantify the accuracy/efficiency trade-off that occurs when the network parameters are tuned to yield more aggressive early classification policies. Additionally, we propose and evaluate methods for appropriating regularization and optimization techniques developed for statically-routed networks."}, {"heading": "2. Related work", "text": "Since the late 1980s, researchers have combined artificial neural networks with decision trees in various\nar X\niv :1\n70 3.\n06 21\n7v 1\n[ st\nat .M\nL ]\n1 7\nM ar\n2 01\n7\nways (Utgoff, 1989) (Sirat & Nadal, 1990). More recently, Kontschieder et al. (2015) performed joint optimization of ANN and decision tree parameters, and Bulo & Kontschieder (2014) used randomized multi-layer networks to compute decision tree split functions.\nTo our knowledge, the family of inference systems we discuss was first described by Denoyer & Gallinari (2014). Additionally, Bengio et al. (2015) explored dynamically skipping layers in neural networks, and Ioannou et al. (2016) explored dynamic routing in networks with equallength paths. Some recently-developed visual detection systems perform cascaded evaluation of convolutional neural network layers (Li et al., 2015; Cai et al., 2015; Girshick, 2015; Ren et al., 2015); though highly specialized for the task of visual detection, these modifications can radically improve efficiency.\nWhile these approaches lend evidence that dynamic routing can be effective, they either ignore the cost of computation, or do not represent it explicitly, and instead use opaque heuristics to trade accuracy for efficiency. We build on this foundation by deriving training procedures from arbitrary application-provided costs of error and computation, comparing one actor-style and two critic-style strategies, and considering regularization and optimization in the context of dynamically-routed networks."}, {"heading": "3. Setup", "text": "In a statically-routed, feedforward artificial neural network, every layer transforms a single input feature vector into a single output feature vector. The output feature vector is then used as the input to the following layer (which we\u2019ll refer to as the current layer\u2019s sink), if it exists, or as the ouptut of the network as a whole, if it does not.\nWe consider networks in which layers may have more than one sink. In such a network, for every n-way junction j a signal reaches, the network must make a decision, dj \u2208 {0..n}, such that the signal will propagate through the ith sink if and only if dj = i (this is illustrated in Fig. 2). We compute dj as the argmax of the score vector sj , a learned function of the last feature vector computed before reaching j. We\u2019ll refer to this rule for generating d from s as the inference routing policy."}, {"heading": "3.1. Multipath architectures for convolutional networks", "text": "Convolutional network layers compute collections of local descriptions of the input signal. It is unreasonable to expect that this kind of feature vector can explicitly encode the global information relevant to deciding how to route the entire signal (e.g., in the case of object recognition, whether the image was taken indoors, whether the image contains an animal, or the prevalence of occlusion in the scene).\nTo address this, instead of computing a 2-dimensional array of local features at each layer, we compute a pyramid of features (resembling the pyramids described by Ke et al. (2016)), with local descriptors at the bottom and global descriptors at the top. At every junction j, the score vector sj is computed by a small routing network operating on the last-computed global descriptor. Our multipath architecture is illustrated in Fig. 3."}, {"heading": "3.2. Balancing accuracy and efficiency", "text": "For a given input, network \u03bd, and set of routing decisions d, we define the cost of performing inference:\ncinf(\u03bd, d) = cerr(\u03bd, d) + ccpt(\u03bd, d), (1)\nwhere cerr(\u03bd, d) is the cost of the inference errors made by the network, and ccpt(\u03bd, d) is the cost of computation. In our experiments, unless stated otherwise, cerr is the crossentropy loss, and\nccpt(\u03bd, d) = kcptnops(\u03bd, d), (2)\nwhere nops(\u03bd, d) is the number of multiply-accumulate operations performed, and kcpt is a scalar hyperparameter. This definition assumes a time- or energy-constrained system\u2014every operation consumes roughly the same amount of time and energy, so every operation is equally expensive. ccpt may be defined differently under other constraints (e.g. memory bandwidth)."}, {"heading": "4. Training", "text": "We propose three approaches to training dynamicallyrouted networks, along with complementary approaches to regularization and optimization, and a method for adapting to changes in the cost of computation.\n4\u00d74\n8\u00d78\n16\u00d716\n32\u00d732\nConvolution, Batch Normalization, Rectification\nLinear Transformation, Batch Normalization, Rectification\nLinear Transformation, Softmax\nActive Path\n\u201cHorse\u201d\nLinear Transformation, Argmax"}, {"heading": "4.1. Training strategy I: actor learning", "text": "Since d is discrete, cinf(\u03bd, d) cannot be minimized via gradient-based methods. However, if d is replaced by a stochastic approximation, d\u0302, during training, we can engineer the gradient of E[cinf(\u03bd, d\u0302)] to be nonzero. We can then learn the routing parameters and classification parameters simultaneously by minimizing the loss\nLac = E[cinf(\u03bd, d\u0302)]. (3)\nIn our experiments, the training routing policy samples d\u0302 such that\nPr(d\u0302j = i) = softmax(sj/\u03c4)i, (4)\nwhere \u03c4 is the network \u201ctemperature\u201d: a scalar hyperparameter that decays over the course of training, converging the training routing policy towards the inference routing policy."}, {"heading": "4.2. Training strategy II: pragmatic critic learning", "text": "Alternatively, we can attempt to learn to predict the expected cost of making every routing decision. In this case, we minimize the loss\nLcr = E cinf(\u03bd, d\u0302) +\u2211 j\u2208J cjcre  , (5)\nwhere J is the set of junctions encountered when making the routing decisions d\u0302, and ccre is the cost regression error cost, defined:\ncjcre = kcre\u2016sj \u2212 uj\u20162, (6)\nwhere\nuij = \u2212cinf(\u03bdij , d), (7)\nkcre is a scalar hyperparameter, and \u03bdij is the subnetwork consisting of the ith child of \u03bdj , and all of its descendants. Since we want to learn the policy indirectly (via cost prediction), d\u0302 is treated as constant with respect to optimization."}, {"heading": "4.3. Training strategy III: optimistic critic learning", "text": "To improve the stability of the loss and hopefully accelerate training, we can adjust the routing utility function u such that, for every junction j, uj is independent of the routing parameters downstream of j. Instead of predicting the cost of making routing decisions given the current downstream routing policy, we can predict the cost of making routing decisions given the optimal downstream routing policy. In this optimistic variant of the critic method,\nuij = \u2212mind\u2032(cinf(\u03bdij , d\u2032)). (8)"}, {"heading": "4.4. Regularization", "text": "Many regularization techniques involve adding a modelcomplexity term, cmod, to the loss function to influence learning, effectively imposing soft constraints upon the network parameters (Hoerl & Kennard, 1970; Rudin et al., 1992; Tibshirani, 1996). However, if such a term affects layers in a way that is independent of the amount of signal routed through them, it will either underconstrain frequently-used layers or overconstrain infrequently-used layers. To support both frequently- and infrequently-used layers, we regularize subnetworks as they are activated by d\u0302, instead of regularizing the entire network directly.\nFor example, to apply L2 regularization to critic networks, we define cmod:\ncmod = E [ kL2\n\u2211 w\u2208W w2\n] , (9)\nwhere W is the set of weights associated with the layers activated by d\u0302, and kL2 is a scalar hyperparameter.\nFor actor networks, we apply an extra term to control the magnitude of s, and therefore the extent to which the net explores subpotimal paths:\ncmod = E kL2 \u2211 w\u2208W w2 + kdec \u2211 j\u2208J \u2016sj\u20162  , (10) where kdec is a scalar hyperparameter indicating the relative cost of decisiveness.\ncmod is added to the loss function in all of our experiments. Within cmod, unless stated otherwise, d\u0302 is treated as constant with respect to optimization."}, {"heading": "4.5. Adjusting learning rates to compensate for throughput variations", "text": "Both training techniques attempt to minimize the expected cost of performing inference with the network, over the training routing policy. With this setup, if we use a constant learning rate for every layer in the network, then layers through which the policy routes examples more frequently will receive larger parameter updates, since they contribute more to the expected cost.\nTo allow every layer to learn as quickly as possible, we scale the learning rate of each layer ` dynamically, by a factor \u03b1`, such that the elementwise variance of the loss gradient with respect to `\u2019s parameters is independent of the amount of probability density routed through it.\nTo derive \u03b1`, we consider an alternative routing policy, d\u2217` , that routes all signals though `, then routes through subse-\nquent layers based on d\u0302. With this policy, at every training interation, mini-batch stochastic gradient descent shifts the parameters associated with layer ` by a vector \u03b4\u2217` , defined:\n\u03b4\u2217` = \u2212\u03bb \u2211 i gi`, (11)\nwhere \u03bb is the global learning rate and gi` is the gradient of the loss with respect to the parameters in `, for training example i, under d\u2217` . Analogously, the scaled parameter adjustment under d\u0302 can be written\n\u03b4` = \u2212\u03b1`\u03bb \u2211 i pi`g i `, (12)\nwhere pi` is the probability with which d\u0302 routes example i through `.\nWe want to select \u03b1` such that\nVar(\u03b4`) = Var(\u03b4 \u2217 ` ). (13)\nSubstituting the definitions of \u03b4` and \u03b4\u2217` ,\nVar ( \u03b1` \u2211 i pi`g i ` ) = Var (\u2211 i gi` ) . (14)\nSince every gi` is sampled independently, we can rewrite this equation:\nnexv`\u03b1 2 `\u2016p`\u20162 = nexv`, (15)\nwhere nex is the number of training examples in the minibatch, and v` is the elementwise variance of gi`, for any i (since every example is sampled via the same mechanism). We can now show that\n\u03b1` = \u2016p`\u2016\u22121. (16)\nSo, for every layer `, we can scale the learning rate by \u2016p`\u2016\u22121, and the variance of the weight updates will be similar thoughout the network. We use this technique, unless otherwise specified, in all of our experiments."}, {"heading": "4.6. Responding to changes in the cost of computation", "text": "We may want a single network to perform well in situations with various degrees of computational resource scarcity (e.g. computation may be more expensive when a device battery is low). To make the network\u2019s routing behavior responsive to a dynamic ccpt, we can concatenate its known\nparameters\u2014in our case, {kcpt}\u2014to the input of every routing subnetwork, to allow them to modulate the routing policy. To match the scale of the image features and facilitate optimization, we express kcpt in units of cost per tenmillion operations."}, {"heading": "4.7. Hyperparameters", "text": "In all of our experiments, we use a mini-batch size, nex, of 128, and run 80,000 training iterations. We perform stochastic gradient descent with initial learning rate 0.1/nex and momentum 0.9. The learning rate decays continuously with a half-life of 10,000 iterations.\nThe weights of the final layers of routing networks are zero-initialized, and we initialize all other weights using the Xavier initialization method (Glorot & Bengio, 2010). All biases are zero-initialized. We perform batch normalization (Ioffe & Szegedy, 2015) before every rectification operation, with an of 1\u00d710\u22126, and an exponential moving average decay constant of 0.9.\n\u03c4 is initialized to 1.0 for actor networks and 0.1 for critic networks, and decays with a half-life of 10,000 iterations. kdec = 0.01, kcre = 0.001, and kL2 = 1 \u00d7 10\u22124. We selected these values (for \u03c4 , kdec, kcre, and kL2) by exploring the hyperparameter space logarithmically, by powers of 10, training and evaluating on the hybrid MNIST/CIFAR-10 dataset (described in section 5.1). At a coarse level, these values are locally optimal\u2014multiplying or dividing any of them by 10 will not improve performance."}, {"heading": "4.8. Data augmentation", "text": "We augment our data using an approach that is popular for use with CIFAR-10 (Lin et al., 2013) (Srivastava et al., 2015) (Clevert et al., 2015). We augment each image by applying vertical and horizontal shifts sampled uniformly from the range [-4,4], and, if the image is from CIFAR-10, flipping it horizontally with probability 0.5. We fill blank pixels introduced by shifts with the mean color of the image (after gamma-decoding)."}, {"heading": "5. Experiments", "text": "We compare approaches to dynamic routing by training 153 networks to classify small images, varying the policy-learning strategy, regularization strategy, optimization strategy, architecture, cost of computation, and details of the task. The results of these experiments are reported in\nFig. 5\u201310."}, {"heading": "5.1. Comparing policy-learning strategies", "text": "To compare routing strategies in the context of a simple dataset with a high degree of difficulty variation, we train\nnetworks to classify images from a small-image dataset synthesized from MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky & Hinton, 2009) (see Fig. 4). Our dataset includes the classes \u201c0\u201d, \u201c1\u201d, \u201c2\u201d, \u201c3\u201d, and \u201c4\u201d from MNIST and \u201cairplane\u201d, \u201cautomobile\u201d, \u201cdeer\u201d, \u201chorse\u201d, and \u201cfrog\u201d from CIFAR-10 (see Fig. 4). The images from MNIST are resized to match the scale of images from CIFAR-10 (32\u00d732), via linear interpolation, and are colormodulated to make them more difficult to trivially distinguish from CIFAR-10 images (MNIST is a grayscale dataset).\nFor a given computational budget, dynamically-routed networks achieve higher accuracy rates than architecturematched statically-routed baselines (networks composed of the first n columns of the architecture illustrated in Fig. 3, for n \u2208 {1..8}). Additionally, dynamically-routed networks tend to avoid routing data along deep paths at the beginning of training (see Fig. 8). This is possibly because the error surfaces of deeper networks are more complicated, or because deeper paths are less stable\u2014changing the parameters in any component layer to better classifying images routed along other, overlapping paths may decrease performance. Whatever the mechanism, this tendency to initially find simpler solutions seems to prevent some of the overfitting that occurs with 7- and 8-layer statically-routed networks.\nCompared to other dynamically-routed networks, optimistic critic networks perform poorly, possibly because optimal routers are a poor approximation for our small, lowcapacity router networks. Actor networks perform better than critic networks, possibly because critic networks are forced to learn a potentially-intractable auxilliary task (i.e. it\u2019s easier to decide who to call to fix your printer than it is to predict exactly how quickly and effectively everyone you know would fix it). Actor networks also consistently achieve higher peak accuracy rates than comparable statically-routed networks, across experiments.\n4 8\n40k\n80k\nE p\no ch\nIn d\nex\nkcpt = 0\n4 8\nkcpt = 1\u00d710\u22129\n4 8\nkcpt = 2\u00d710\u22129\n4 8\nkcpt = 4\u00d710\u22129\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLayer Index\nFigure 8. Dataflow over the course of training. The heatmaps illustrate the fraction of validation images classified at every terminal node in the bottom four networks in Fig. 6, over the course of training.\nAlthough actor networks may be more performant, critic networks are more flexible. Since critic networks don\u2019t require E[cinf(\u03bd, d\u0302)] to be a differentiable function of d\u0302, they can be trained by sampling d\u0302, saving memory, and they support a wider selection of training routing policies (e.g. -greedy) and cinf definitions. In addition to training the standard critic networks, we train networks using a variant of the pragmatic critic training policy, in which we replace\nthe cross-entropy error in the ccre term with the classification error. Although these networks do not perform as well as the original pragmatic critic networks, they still outperform comparable statically-routed networks."}, {"heading": "5.2. Comparing regularization strategies", "text": "Based on our experiments with the hybrid dataset, regularizing d\u0302, as described in section 4.4, discourages networks from routing data along deep paths, reducing peak accuracy. Additionally, some mechanism for encouraging exploration (in our case, a nonzero kdec) appears to be necessary to train effective actor networks."}, {"heading": "5.3. Comparing optimization strategies", "text": "Throughput-adjusting the learning rates (TALR), as described in section 4.5, improves the hybrid dataset performance of both actor and critic networks in computationalresource-abundant, high-accuracy contexts."}, {"heading": "5.4. Comparing architectures", "text": "For a given computational budget, architectures with both 2- and 3-way junctions have a higher capacity than subtrees with only 2-way junctions. On the hybrid dataset, under tight computational constraints, we find that trees with higher degrees of branching achieve higher accuracy rates. Unconstrained, however, they are prone to overfitting.\nIn dynamically-routed networks, early classification layers tend to have high accuracy rates, pushing difficult decisions downstream. Even without energy contraints, terminal layers specialize in detecting instances of certain classes of images. These classes are usually related (they either all come from MNIST or all come from CIFAR-10.) In networks with both 2- and 3-way junctions, branches specialize to an even greater extent. (See Fig. 6 and 7.)"}, {"heading": "5.5. Comparing specialized and adaptive networks", "text": "We train a single actor network to classify images from the hybrid datset under various levels of computational constraints, using the approach described in section 4.6, sampling kcpt randomly from the set mentioned in Fig. 5 for each training example. This network performs comparably to a collection of 8 actor nets trained with various static values of kcpt, over a significant, central region of the accuracy/efficiency curve, with an 8-fold reduction in memory consumption and training time."}, {"heading": "5.6. Exploring the effects of the decision difficulty distribution", "text": "To probe the effect of the inference task\u2019s difficulty distribution on the performance of dynamically-routed net-\nworks, we train networks to classify images from CIFAR10, adjusting the classification task to vary the frequency of difficult decisions (see Fig. 9). We call these variants CIFAR-2\u2014labelling images as \u201chorse\u201d or \u201cother\u201d\u2014 and CIFAR-5\u2014labelling images as \u201ccat\u201d, \u201cdog\u201d, \u201cdeer\u201d, \u201chorse\u201d, or \u201cother\u201d. In this experiment, we compare actor networks (the best-performing networks from the first set of experiments) to architecture-matched statically-routed networks.\nWe find that dynamic routing is more beneficial when the task involves many low-difficulty decisions, allowing the network to route more data along shorter paths. While dynamic routing offers only a slight advantage on CIFAR-10, dynamically-routed networks achieve a higher peak accuracy rate on CIFAR-2 than statically-routed networks, at a third of the computational cost."}, {"heading": "5.7. Exploring the effects of model capacity", "text": "To test whether dynamic routing is advantageous in highercapacity settings, we train actor networks and architecturematched statically-routed networks to classify images from CIFAR-10, varying the width of the networks (see Fig. 10). Increasing the model capacity either increases or does not affect the relative advantage of dynamically-routed networks, suggesting that our approach is applicable to more complicated tasks."}, {"heading": "6. Discussion", "text": "Our experiments suggest that dynamically-routed networks trained under mild computational constraints can operate 2\u20133 times more efficiently than comparable staticallyrouted networks, without sacrificing performance. Additionally, despite their higher capacity, dynamically-routed networks may be less prone to overfitting.\nWhen designing a multipath architecture, we suggest supporting early decision-making wherever possible, since cheap, simple routing networks seem to work well. In convolutional architectures, pyramidal layers appear to be reasonable sites for branching.\nThe actor strategy described in section 4.1 is generally an effective way to learn a routing policy. However, the pragmatic critic strategy described in section 4.2 may be better suited for very large networks (trained via decision sampling to conserve memory) or networks designed for applications with nonsmooth cost-of-inference functions\u2014e.g. one in which kcpt has units errors/operation. Adjusting learning rates to compensate for throughput variations, as described in section 4.5, may improve the performance of deep networks. If the cost of computation is dynamic, a single network, trained with the procedure described in section 5.5, may still be sufficient.\nWhile we test our approach on tasks with some degree of difficulty variation, it is possible that dynamic routing is even more advantageous when performing more complex tasks. For example, video annotation may require specialized modules to recognize locations, objects, faces, human actions, and other scene components or attributes, but having every module constantly operating may be extremely inefficient. A dynamic routing policy could fuse these modules, allowing them to share common components, and activating specialized components as necessary.\nAnother interesting topic for future research is growing and shrinking dynamically-routed networks during training. With such a network, it is not necessary to specify an architecture. The network will instead take shape over the course of training, as computational contraints, memory contraints, and the data dictate."}, {"heading": "Acknowledgements", "text": "This work was funded by a generous grant from Google Inc. We would also like to thank Krzysztof Chalupka, Cristina Segalin, and Oisin Mac Aodha for their thoughtful comments."}], "references": [{"title": "Conditional computation in neural networks for faster models", "author": ["Bengio", "Emmanuel", "Bacon", "Pierre-Luc", "Pineau", "Joelle", "Precup", "Doina"], "venue": "arXiv preprint arXiv:1511.06297,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Neural decision forests for semantic image labelling", "author": ["Bulo", "Samuel", "Kontschieder", "Peter"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Bulo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bulo et al\\.", "year": 2014}, {"title": "Learning complexity-aware cascades for deep pedestrian detection", "author": ["Cai", "Zhaowei", "Saberian", "Mohammad", "Vasconcelos", "Nuno"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Cai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2015}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["Clevert", "Djork-Arn\u00e9", "Unterthiner", "Thomas", "Hochreiter", "Sepp"], "venue": "arXiv preprint arXiv:1511.07289,", "citeRegEx": "Clevert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clevert et al\\.", "year": 2015}, {"title": "Deep sequential neural network", "author": ["Denoyer", "Ludovic", "Gallinari", "Patrick"], "venue": "arXiv preprint arXiv:1410.0510,", "citeRegEx": "Denoyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denoyer et al\\.", "year": 2014}, {"title": "Fast r-cnn", "author": ["Girshick", "Ross"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Girshick and Ross.,? \\Q2015\\E", "shortCiteRegEx": "Girshick and Ross.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In Aistats,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Separate visual pathways for perception and action", "author": ["Goodale", "Melvyn A", "Milner", "A David"], "venue": "Trends in neurosciences,", "citeRegEx": "Goodale et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Goodale et al\\.", "year": 1992}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Random decision forests", "author": ["Ho", "Tin Kam"], "venue": "In Document Analysis and Recognition,", "citeRegEx": "Ho and Kam.,? \\Q1995\\E", "shortCiteRegEx": "Ho and Kam.", "year": 1995}, {"title": "Ridge regression: Biased estimation for nonorthogonal problems", "author": ["Hoerl", "Arthur E", "Kennard", "Robert W"], "venue": null, "citeRegEx": "Hoerl et al\\.,? \\Q1970\\E", "shortCiteRegEx": "Hoerl et al\\.", "year": 1970}, {"title": "Decision forests, convolutional networks and the models in-between", "author": ["Ioannou", "Yani", "Robertson", "Duncan", "Zikic", "Darko", "Kontschieder", "Peter", "Shotton", "Jamie", "Brown", "Matthew", "Criminisi", "Antonio"], "venue": "arXiv preprint arXiv:1603.01250,", "citeRegEx": "Ioannou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ioannou et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Deep neural decision forests", "author": ["Kontschieder", "Peter", "Fiterau", "Madalina", "Criminisi", "Antonio", "Rota Bulo", "Samuel"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "Kontschieder et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kontschieder et al\\.", "year": 2015}, {"title": "A network for scene processing in the macaque temporal lobe", "author": ["Kornblith", "Simon", "Cheng", "Xueqi", "Ohayon", "Shay", "Tsao", "Doris Y"], "venue": null, "citeRegEx": "Kornblith et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kornblith et al\\.", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher JC"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "A convolutional neural network cascade for face detection", "author": ["Li", "Haoxiang", "Lin", "Zhe", "Shen", "Xiaohui", "Brandt", "Jonathan", "Hua", "Gang"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Patches with links: a unified system for processing faces in the macaque temporal lobe", "author": ["Moeller", "Sebastian", "Freiwald", "Winrich A", "Tsao", "Doris Y"], "venue": null, "citeRegEx": "Moeller et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Moeller et al\\.", "year": 2008}, {"title": "Stacked hourglass networks for human pose estimation", "author": ["Newell", "Alejandro", "Yang", "Kaiyu", "Deng", "Jia"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Newell et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Newell et al\\.", "year": 2016}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["Ren", "Shaoqing", "He", "Kaiming", "Girshick", "Ross", "Sun", "Jian"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Nonlinear total variation based noise removal algorithms", "author": ["Rudin", "Leonid I", "Osher", "Stanley", "Fatemi", "Emad"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "Rudin et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Rudin et al\\.", "year": 1992}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Neural trees: a new tool for classification", "author": ["JA Sirat", "Nadal", "JP"], "venue": "Network: Computation in Neural Systems,", "citeRegEx": "Sirat et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Sirat et al\\.", "year": 1990}, {"title": "Training very deep networks", "author": ["Srivastava", "Rupesh K", "Greff", "Klaus", "Schmidhuber", "J\u00fcrgen"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Tibshirani", "Robert"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "citeRegEx": "Tibshirani and Robert.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani and Robert.", "year": 1996}, {"title": "Perceptron trees: A case study in hybrid concept representations", "author": ["Utgoff", "Paul E"], "venue": "Connection Science,", "citeRegEx": "Utgoff and E.,? \\Q1989\\E", "shortCiteRegEx": "Utgoff and E.", "year": 1989}, {"title": "Detecting pedestrians using patterns of motion and appearance", "author": ["Viola", "Paul", "Jones", "Michael J", "Snow", "Daniel"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Viola et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Viola et al\\.", "year": 2005}, {"title": "Extensive facial landmark localization with coarse-to-fine convolutional network cascade", "author": ["Zhou", "Erjin", "Fan", "Haoqiang", "Cao", "Zhimin", "Jiang", "Yuning", "Yin", "Qi"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision Workshops,", "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 27, "context": "This approach\u2014dynamically routing signals through an inference system, based on their content\u2014has already been incorporated into machine vision pipelines via methods such as boosting (Viola et al., 2005), coarse-to-fine cascades (Zhou et al.", "startOffset": 183, "endOffset": 203}, {"referenceID": 28, "context": ", 2005), coarse-to-fine cascades (Zhou et al., 2013), and random decision forests (Ho, 1995).", "startOffset": 33, "endOffset": 52}, {"referenceID": 18, "context": "Dynamic routing is also performed in the primate visual system: spatial information is processed somewhat separately from object identity information (Goodale & Milner, 1992), and faces and other behaviorally-relevant stimuli ellicit responses in anatomically distinct, specialized regions (Moeller et al., 2008; Kornblith et al., 2013).", "startOffset": 290, "endOffset": 336}, {"referenceID": 14, "context": "Dynamic routing is also performed in the primate visual system: spatial information is processed somewhat separately from object identity information (Goodale & Milner, 1992), and faces and other behaviorally-relevant stimuli ellicit responses in anatomically distinct, specialized regions (Moeller et al., 2008; Kornblith et al., 2013).", "startOffset": 290, "endOffset": 336}, {"referenceID": 8, "context": "However, state-of-the-art artificial neural networks (ANNs) for visual inference are routed statically (Simonyan & Zisserman, 2014; He et al., 2016; Dosovitskiy et al., 2015; Newell et al., 2016); every input triggers an identical sequence of operations.", "startOffset": 103, "endOffset": 195}, {"referenceID": 19, "context": "However, state-of-the-art artificial neural networks (ANNs) for visual inference are routed statically (Simonyan & Zisserman, 2014; He et al., 2016; Dosovitskiy et al., 2015; Newell et al., 2016); every input triggers an identical sequence of operations.", "startOffset": 103, "endOffset": 195}, {"referenceID": 16, "context": "We propose three approaches to training these networks, test them on small image datasets synthesized from MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky & Hinton, 2009), and quantify the accuracy/efficiency trade-off that occurs when the network parameters are tuned to yield more aggressive early classification policies.", "startOffset": 113, "endOffset": 133}, {"referenceID": 17, "context": "Some recently-developed visual detection systems perform cascaded evaluation of convolutional neural network layers (Li et al., 2015; Cai et al., 2015; Girshick, 2015; Ren et al., 2015); though highly specialized for the task of visual detection, these modifications can radically improve efficiency.", "startOffset": 116, "endOffset": 185}, {"referenceID": 2, "context": "Some recently-developed visual detection systems perform cascaded evaluation of convolutional neural network layers (Li et al., 2015; Cai et al., 2015; Girshick, 2015; Ren et al., 2015); though highly specialized for the task of visual detection, these modifications can radically improve efficiency.", "startOffset": 116, "endOffset": 185}, {"referenceID": 20, "context": "Some recently-developed visual detection systems perform cascaded evaluation of convolutional neural network layers (Li et al., 2015; Cai et al., 2015; Girshick, 2015; Ren et al., 2015); though highly specialized for the task of visual detection, these modifications can radically improve efficiency.", "startOffset": 116, "endOffset": 185}, {"referenceID": 10, "context": "More recently, Kontschieder et al. (2015) performed joint optimization of ANN and decision tree parameters, and Bulo & Kontschieder (2014) used randomized multi-layer networks to compute decision tree split functions.", "startOffset": 15, "endOffset": 42}, {"referenceID": 10, "context": "More recently, Kontschieder et al. (2015) performed joint optimization of ANN and decision tree parameters, and Bulo & Kontschieder (2014) used randomized multi-layer networks to compute decision tree split functions.", "startOffset": 15, "endOffset": 139}, {"referenceID": 10, "context": "More recently, Kontschieder et al. (2015) performed joint optimization of ANN and decision tree parameters, and Bulo & Kontschieder (2014) used randomized multi-layer networks to compute decision tree split functions. To our knowledge, the family of inference systems we discuss was first described by Denoyer & Gallinari (2014). Additionally, Bengio et al.", "startOffset": 15, "endOffset": 329}, {"referenceID": 0, "context": "Additionally, Bengio et al. (2015) explored dynamically skipping layers in neural networks, and Ioannou et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 0, "context": "Additionally, Bengio et al. (2015) explored dynamically skipping layers in neural networks, and Ioannou et al. (2016) explored dynamic routing in networks with equallength paths.", "startOffset": 14, "endOffset": 118}, {"referenceID": 21, "context": "Regularization Many regularization techniques involve adding a modelcomplexity term, cmod, to the loss function to influence learning, effectively imposing soft constraints upon the network parameters (Hoerl & Kennard, 1970; Rudin et al., 1992; Tibshirani, 1996).", "startOffset": 201, "endOffset": 262}, {"referenceID": 24, "context": ", 2013) (Srivastava et al., 2015) (Clevert et al.", "startOffset": 8, "endOffset": 33}, {"referenceID": 3, "context": ", 2015) (Clevert et al., 2015).", "startOffset": 8, "endOffset": 30}, {"referenceID": 16, "context": "Comparing policy-learning strategies To compare routing strategies in the context of a simple dataset with a high degree of difficulty variation, we train networks to classify images from a small-image dataset synthesized from MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky & Hinton, 2009) (see Fig.", "startOffset": 233, "endOffset": 253}], "year": 2017, "abstractText": "We propose and systematically evaluate three strategies for training dynamically-routed artificial neural networks: graphs of learned transformations through which different input signals may take different paths. Though some approaches have advantages over others, the resulting networks are often qualitatively similar. We find that, in dynamically-routed networks trained to classify images, layers and branches become specialized to process distinct categories of images. Additionally, given a fixed computational budget, dynamically-routed networks tend to perform better than comparable staticallyrouted networks.", "creator": "LaTeX with hyperref package"}}}