{"id": "1703.06275", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2017", "title": "Evolving Game Skill-Depth using General Video Game AI Agents", "abstract": "Most games have, or can be generalised to have, a number of parameters that may be varied in order to provide instances of games that lead to very different player experiences. The space of possible parameter settings can be seen as a search space, and we can therefore use a Random Mutation Hill Climbing algorithm or other search methods to find the parameter settings that induce the best games. One of the hardest parts of this approach is defining a suitable fitness function. In this paper we explore the possibility of using one of a growing set of General Video Game AI agents to perform automatic play-testing. This enables a very general approach to game evaluation based on estimating the skill-depth of a game. Agent-based play-testing is computationally expensive, so we compare two simple but efficient optimisation algorithms: the Random Mutation Hill-Climber and the Multi-Armed Bandit Random Mutation Hill-Climber. For the test game we use a space-battle game in order to provide a suitable balance between simulation speed and potential skill-depth. Results show that both algorithms are able to rapidly evolve game versions with significant skill-depth, but that choosing a suitable resampling number is essential in order to combat the effects of noise. In general, this is a more efficient approach which is a useful one for evaluating games in a very narrow space. In this paper we consider a potential model of the performance of multiple Mutation Hill Climber and the Multi-Armed Bandit Random Mutation Hill-Climber, in order to explore the potential for a suitable balance between simulation speed and potential skill-depth.\n\n\nA number of previous experiments have demonstrated that the ability to efficiently manage multiple game versions of different game models can be achieved by applying a set of parameters of a game to different types of game types. This may have been the case in the last few years, when the most commonly used modal approach for the game game was the modal version, a number of popular games in the game market were made, and a number of popular ones were made. The idea is to allow the player to tailor his or her choices to his or her particular role or setting, such as playing with a small number of players in a multiplayer game that has a high performance, but that is not possible with a large number of players, and that would not be possible with the number of players. This method would be implemented in the context of simulation speed and skill-depth, but in order to achieve the optimal playing", "histories": [["v1", "Sat, 18 Mar 2017 09:04:05 GMT  (1417kb,D)", "http://arxiv.org/abs/1703.06275v1", "9 pages, 17 figures, CEC2017"]], "COMMENTS": "9 pages, 17 figures, CEC2017", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jialin liu", "julian togelius", "diego perez-liebana", "simon m lucas"], "accepted": false, "id": "1703.06275"}, "pdf": {"name": "1703.06275.pdf", "metadata": {"source": "CRF", "title": "Evolving Game Skill-Depth using General Video Game AI Agents", "authors": ["Jialin Liu", "Julian Togelius", "Simon M. Lucas"], "emails": ["jialin.liu@essex.ac.uk", "julian.togelius@nyu.edu", "dperez@essex.ac.uk", "sml@essex.ac.uk"], "sections": [{"heading": null, "text": "Index Terms\u2014Automatic game design, game tuning, optimisation, RMHC, GVG-AI\nI. INTRODUCTION\nDesigning games is an interesting and challenging discipline traditionally demanding creativity and insight into the types of experience which will cause players to enjoy the game or at least play it and replay it. There have been various attempts to automate or part-automate the game generation process, as this is an interesting challenge for AI and computational creativity [1], [2], [3]. So far the quality of the generated games (with some exceptions) do not challenge the skill of human game designers. This is because the generation of complete games is a more challenging task than the more constrained task of generating game content such as levels or maps. Many video games require content to be produced for them, and recent years have seen a surge in AI-based procedural content generation [4].\nThere is another aspect of AI-assisted game design which we believe is hugely under-explored: automatic game tuning. This involves taking an existing game (either human-designed or auto-generated) and performing a comprehensive exploration of the parameter space to find the most interesting game instances.\nRecent work has demonstrated the potential of this approach, automatically generating distinct and novel variants of the minimalist mobile game Flappy Bird [5]. That work involved using a very simple agent to play through each generated game instance. Noise was added to the selected actions, and a game variant was deemed to have an appropriate level of difficulty if a specified number of players achieved a desired score. For Flappy Bird it is straightforward to design an AI agent capable of near-optimal play. Adding noise to the selected actions of this player can be used to provide a less than perfect agent that better represents human reactions. An evolutionary algorithm was used to search for game variants that were as far apart from each other in parameter space as possible but were still playable.\nHowever, for more complex games it is harder to provide a good AI agent, and writing a new game playing agent for each new game would make the process more time consuming. Furthermore, a single hand-crafted agent may be blind to novel aspects of evolved game-play elements that the designer of the AI agent had not considered. This could severely inhibit the utility of the approach. In this work we mitigate these concerns by tapping in to an ever-growing pool of agents designed for the General Video Game AI (GVG-AI) competition1. The idea is that using a rich set of general agents will provide the basis for a robust evaluation process with a higher likelihood of finding skill-depth wherever it may lie in the chosen search space of possible games. In this paper we use one of the sample GVG-AI agents, varying it by changing the rollout budget. This was done by making the game implement a standard GVG-AI game interface, so that any GVG-AI agent can be used with very little effort, allowing the full set of agents to be used in future experiments.\nLiu et al. [6] introduced a two-player space-battle game, derived from the original Spacewar, and performed a study on different parameter settings to bring out some strengths and weaknesses of the various algorithms under test. A key finding is that the rankings of the algorithms depend very much on the details of the game. A mutation of one parameter may lead to a totally different ranking of algorithms. If the game using only a single parameter setting is tested, the conclusions could be less robust and misleading.\n1http://www.gvgai.net/\n978-1-5090-4601-0/17/$31.00 c\u00a92017 IEEE\nar X\niv :1\n70 3.\n06 27\n5v 1\n[ cs\n.A I]\n1 8\nM ar\n2 01\n7\nIn this paper, we adapt the space-battle game introduced by Liu et al. [6] to the GVG-AI framework, then uses the Random Mutation Hill Climber (RMHC) and Multi-Armed Bandit RMHC (MABRMHC) to evolve game parameters to provide some game instances that lead to high winning rates for GVGAI sample MCTS agents. This is used as an approximate measure of skill-depth, the idea being that the smarter MCTS agents should beat unintelligent agents, or that MCTS agents with a high rollout budget should beat those with a low rollout budget.\nThe paper is structured as follows: Section II provides a brief review of the related work on automatic game design, Section III describes the game engine, Section IV introduces the two optimisation algorithms used in this paper, Section V presents the experimental results, finally Section VI concludes and discusses the potential directions in the future."}, {"heading": "II. AUTOMATIC GAME DESIGN AND DEPTH ESTIMATION", "text": "Attempts to automatically design complete games go back to Barney Pell, who generated rules for chess-like games [7]. It did not however become an active research topic until the late 2000\u2019s.\nTogelius et al. [8] evolved racing tracks in a car racing game using a simple multi-objective evolutionary algorithm called Cascading Elitism. The fitness functions attempted to capture various aspects of player experience, using a neural network model of the player. This can be seen as an early form of experience-driven procedural content generation [9], where game content is generated through search in content space using evolutionary computation or some other form of stochastic optimisation. Similar methods have since been used to generate many types of game content, such as particle systems for weapons in a space shooter [10], platform game levels [11] or puzzles [12]. In most of these cases, the fitness functions measure some aspect of problem difficulty, with the assumption that good game content should not make the game too hard nor too easy.\nWhile the research discussed above focuses on generating content for an existing game, there have been several attempts to use the search-based methods to generate new games by searching though spaces of game rules. Togelius and Schmidhuber [1] used a simple hill-climber to generate single-player Pac-Man-like games given a restricted rule search space. The fitness function was based on learnability of the game, operationalised as the capacity of another machine learning algorithm to learn to play the game.\nThis approach was taken further by Cook et al. [13], [3], who used search-based methods to design rulesets, maps and object layouts in tandem for producing simple arcade games via a system called ANGELINA. Further iterations of this system include the automatic selection of media sources, such as images and resources, giving this work a unique flavour. In a similar vein, Browne and Maire [2] developed a system for automatic generation of board games; they also used evolutionary algorithms, and a complex fitness function based on data gathered from dozens of humans playing different\nboard games. Browne\u2019s work is perhaps the only to result in a game of sufficient quality to be sold as a stand-alone product; this is partly a result of working in a constrained space of simple board games.\nA very different approach to game generation was taken by Nelson and Mateas [14], who use reasoning methods to create Wario Ware-style minigames out of verb-noun relations and common minigame design patterns. Conceptnet and Wordnet were used to find suitable roles for game objects.\nQuite recently, some authors have used search-based methods to optimise the parameters of a single game, while keeping both game rules and other parts of the game content constant. In the introduction we discussed the work of Isaksen et al. on generating playable Flappy Bird variants [5]. Similarly, Powley et al. [15] optimise the parameters of an abstract touch-based mobile game, showing that parameter changes to a single ruleset can give rise to what feels and plays like different games.\nOne of the more important properties of a game can be said to be its skill depth, often just called depth. This property is universally considered desirable by game designers, yet it is hard to define properly; some of the definitions build on the idea of a skill chain, where deeper games simply have more things that can be learned [16]. Various attempts have been made to algorithmically estimate depth and use it as a fitness function; some of the research discussed above can be said to embody an implicit notion of depth in their fitness functions. Relative Algorithm Performance Profiles (RAPP) is a more explicit attempt at game depth estimation; the basic idea is that in a deeper game, a better player get relatively better result than a poorer player. Therefore, we can use game-playing agents of different strengths to play the same game, and the bigger the difference in outcome the greater the depth [17].\nIn this paper we use a form of RAPP to try to estimate the depth of variants of a simple two-player game. Using this measure as a fitness function, we optimise the parameters of this game to try to find deeper game variants, using two types of Random-Mutation Hill-Climber. The current work differs from the work discussed above both in the type of game used (two-player physics-based game), the search space (a multidimensional discrete space) and the optimisation method. In particular, compared to previous work by Isaksen et al, the current paper investigates a more complex game and uses a significantly more advanced agent, and also optimizes for skilldepth rather than difficulty. This work is, as far as we know, the first attempt to optimize skill-depth that has had good results."}, {"heading": "III. FRAMEWORK", "text": "We adapt the two-player space-battle game introduced by Liu et al. [6] to the GVG-AI framework, then use RMHC and MABRMHC to evolve game parameters to provide some game instances that lead to high winning rate for GVG-AI sample MCTS agents. The main difference in the modified spacebattle game used in this work is the introduction of weapon system. Each ship has the choice to fire a missile after its cooldown period has finished. From now on, we use the term\n\u201cgame\u201d to refer to a game instance, i.e. a specific configuration of game parameters.\na) Spaceship: Each player/agent controllers a spaceship which has a maximal speed, vs units distance per game tick, and slows down over time. At each game tick, the player can choose to do nothing or to make an action among {RotateClockwise, RotateAnticlockwise, Thrust, Shoot}. A missile is launched while the Shoot action is chosen and its cooldown period is finished, otherwise, no action will be taken (like do nothing). The spaceship is affected by a random recoil force when launching a missile.\nb) Missile: A missile has a maximal speed, vm units distance per game tick, and vanishes into nothing after 30 game tick. It never damages its mother ship.\nEvery spaceship has a radius of 20 pixels and every missile has a radius of 4 pixels in a layout of size 640*480.\nc) Score: Every time a player hits its opponent, it obtains 100 points (reward). Every time a player launches a missile, it is penalized by c points (cost). Given a game state s, the player i \u2208 {1, 2} has a score calculated by:\nscore(i) = 100\u00d7 nbk(i)\u2212 c\u00d7 nbm(i), (1)\nwhere nbk(i) is the number of lives subtracted from the opponent and nbm(i) indicates the number of launched missiles by player i \u2208 {1, 2}.\nd) End condition: A game ends after 500 game ticks. A player wins the game if it has higher score than its opponent after 500 game ticks, and it\u2019s a loss of the other player. If both players have the same score, it\u2019s a draw.\ne) Parameter space: The parameters to be optimised are detailed in Table I. There are in total 14,400 possible games in the 5-dimensional search space. Fig. 1 illustrates briefly how the game changes by varying only the cooldown time for firing missiles.\nThe game is stochastic but fully observable. Each game starts with the agents in the symmetric positions. The two agents make simultaneous moves and in a fair situation. Thus, changing the player id does not change the situation of any player."}, {"heading": "IV. OPTIMISERS", "text": "We compare a Random Mutation Hill-Climber to an MultiArmed Bandit Random Mutation Hill-Climber in evolving instances for space-battle game described previously. This section is organised as follows. Section IV-A briefly recalls\nthe Random Mutation Hill-Climber. Section IV-B presents the Multi-Armed Bandit Random Mutation Hill-Climber and its selection and mutation rules."}, {"heading": "A. Random Mutation Hill-Climber", "text": "The Random Mutation Hill-Climber (RMHC) is a simple but efficient derivative-free optimisation method mostly used in discrete domains [18], [19]. The pseudo-code of RMHC is given in Algorithm 1. At each generation, an offspring is generated based on the only best-so-far genome (parent) by mutating exactly one uniformly randomly chosen gene. The best-so-far genome is updated if the offspring\u2019s fitness value is better or equivalent to the best-so-far."}, {"heading": "B. Multi-Armed Bandit Random Mutation Hill-Climber", "text": "Multi-Armed Bandit Random Mutation Hill-Climber (MABRMHC), derived from the 2-armed bandit-based RMHC [20], [21], uses both UCB-style selection and mutation rules. MABRMHC selects the coordinate (bandit) with the maximal urgency (Equation 2) to mutate, then mutates the parameter in dimension d to the value (arm) which leads to the maximal reward (Equation 3).\nAlgorithm 1 Random Mutation Hill-Climber (RMHC). Require: X : search space Require: D = |X |: problem dimension (genome length) Require: f : X 7\u2192 [0, 1]: fitness function\n1: Randomly initialise a genome x \u2208 X 2: bestF itSoFar \u2190 0 3: M \u2190 0 . Counter for the latest best-so-far genome 4: N \u2190 0 . Total evaluation count so far 5: while time not elapsed do 6: Uniformly randomly select d \u2208 {1, . . . , D} 7: y \u2190 new genome by uniformly randomly mutating\nthe dth gene of x 8: Fitx \u2190 fitness(x) 9: Fity \u2190 fitness(y)\n10: averageF itnessx \u2190 bestF itSoFar\u00d7M+FitxM+1 11: N \u2190 N + 2 . Update evaluation count 12: if Fity \u2265 averageF itnessx then 13: x\u2190 y . Replace the best-so-far genome 14: bestF itSoFar \u2190 Fity 15: M \u2190 1 16: else 17: bestF itSoFar \u2190 averageF itnessx 18: M \u2190M + 1 19: end if 20: end while 21: return x\nFor any multi-armed bandit d \u2208 {1, 2, . . . , D}, its urgencyd is defined as\nurgencyd =\nmin 1\u2264j\u2264Dim(d)\n\u2206d(j) + \u221a 2 log( \u2211Dim(d) k=1 Nd(k))\nNd + \u03c9  , (2)\nwhere Nd(k) is the number of times the kth value is selected when the dth coordinate is selected; Nd is the number of times the dth coordinate is selected to mutate, thus Nd =\u2211Dim(d)\nk=1 Nd(k); \u2206d(k) is the maximal difference between the fitness values if the value k is mutated to when the dth dimension is selected, i.e., the changing of fitness value; \u03c9 denotes a uniformly distributed value between 0 and 1e\u22126 which is used to randomly break ties. Once the coordinate to mutate (eg. d\u2217) is selected, the index of the value to mutated to is determined by\nk\u2217 = argmax 1\u2264k\u2264Dim(d\u2217)\n( \u2206\u0304d\u2217(k) + \u221a 2 log(Nd\u2217)\nNd\u2217(k) + \u03c9\n) , (3)\nwhere \u2206\u0304d\u2217(k) denotes the average changing of fitness value if the value k is mutated to when the dimension d\u2217 is selected.\nThe pseudo-code of MABRMHC is given in Algorithm 2. In this work, we model each of the game parameter to optimise as a bandit, and the legal values for the parameter as the arms of this bandit. The search space is folded in the sense that it takes far less computational cost to mutate and\nAlgorithm 2 Multi-Armed Bandit Random Mutation HillClimber (MABRMHC). Dim(d) returns the number of possible values in dimension d. \u03c9 denotes a uniformly distributed value between 0 and 1e\u22126 which is used to randomly break ties. Require: X : search space Require: D = |X |: problem dimension (genome length) Require: f : X 7\u2192 [0, 1]: fitness function\n1: Randomly initialise a genome x \u2208 X 2: bestF itSoFar \u2190 0 3: M \u2190 0 . Counter for the latest best-so-far genome 4: N \u2190 0 . Total evaluation count so far 5: for d \u2208 {1, . . . , D} do 6: Nd = 0 7: for k \u2208 {1, . . . , Dim(d)} do 8: Nd(k) = 0, \u2206d(k) = 0, \u2206\u0304d(k) = 0 9: end for\n10: end for 11: while time not elapsed do\n12: d\u2217 = argmax 1\u2264d\u2264D\n( min\n1\u2264j\u2264Dim(d) \u2206d(j) +\n\u221a 2 log( \u2211Dim(d) k=1 Nd(k))\nNd + \u03c9 ) . Select the coordinate to mutate (Equation 2)\n13: k\u2217 = argmax 1\u2264k\u2264Dim(d\u2217)\n( \u2206\u0304d\u2217(k) + \u221a 2 log(Nd\u2217 ) Nd\u2217 (k) + \u03c9 )\n.\nSelect the index of value to take (Equation 3) 14: y\u2190 after mutating the element d\u2217 of x to the k\u2217 legal\nvalue 15: Fitx \u2190 fitness(x) 16: Fity \u2190 fitness(y) 17: averageF itness\u2190 bestF itSoFar\u00d7M+FitxM+1 18: N \u2190 N + 2 . Update the counter 19: \u2206 = Fity \u2212 averageF itness 20: Update \u2206d\u2217(k\u2217) and \u2206\u0304d\u2217(k\u2217) . Update the statistic 21: Nd(k)\u2190 Nd(k) + 1, Nd \u2190 Nd + 1 . Update the\ncounters 22: if \u2206 \u2265 0 then 23: x\u2190 y . Replace the best-so-far genome 24: bestF itSoFar \u2190 Fity 25: M \u2190 1 26: else 27: bestF itSoFar \u2190 averageF itness 28: M \u2190M + 1 29: end if 30: end while 31: return x\nevaluate every legal value of each parameter once than to evaluate mutate and evaluate every legal game instance once."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "We firstly use the sample agent using a two-player OpenLoop Monte-Carlo Tree Search algorithm provided by the GVG-AI framework, which uses the difference of scores (Eq. 1) of both players as its heuristic (denoted as OLMCTS), as player 1. No modification or tuning has been performed on this sample agent. We implement a consistently rotate-andshoot agent (denoted as RAS) as the player 2. More precisely, the RAS is a deterministic agent and, by Eq. 1, the OLMCTS aims at maximising (100\u00d7 nbk(1)\u2212 c\u00d7 nbm(1)) \u2212 (100\u00d7 nbk(2)\u2212 c\u00d7 nbm(2)), where nbk(1) and nbk(2) are the numbers of lives subtracted from the RAS and OLMCTS, respectively; nbm(1) and nbm(2) indicates the number of launched missiles by OLMCTS and RAS, respectively. Again, this heuristic is already defined in the sample agent, not by us. Basically, a human player could probably choose to play the game in a passive way by avoiding the missiles and not firing at all, and finally win the game.\nThe landscape of winning rate of OLMCTS against RAS is studied in Section V-A. Section V-B presents the performance of RMHC and MABRMHC with different resampling numbers to generate games in the parameter space detailed previously (Section III-0e) and Section V-C presents their performances in a 5 times larger parameter space."}, {"heading": "A. Winning rate distribution", "text": "We use a OLMCTS agent as the player 1 and a RAS agent as the player 2. At each game tick, 10ms is allocated to each of the agents to decide an action. The average number of iterations performed by OLMCTS is 350. The time to return an action for RAS is negligible.\nThe average winning rates over 11 and 69 repeated trials of all the 14,400 legal game instances played by OLMCTS against RAS are shown in Fig. 2. The winning rate over 69 trials of each games instance varies between 20% and 100%. Among all the legal game instances, the OLMCTS does not achieve a 100% winning rate in more than 5,000 games.\nFig. 3 demonstrates how the winning rate varies along with the changing of each parameter. The maximal ship speed and the thrust speed have negligible influence on the OLMCTS\u2019s average winning rate. Higher the maximal missile speed is or shorter the cooldown time is, higher the average winning rate is. But still, the average winning rate remains above 87%. The most important factor is the cost of firing a missile. It is not surprising, since the RAS fires successively missiles and the number of missiles it fires during each game is constant depending on the cooldown time. the OLMCTS only fires while necessary or it is likely to slash its opponent."}, {"heading": "B. Evolving games by RMHC and MABRMHC using different resampling numbers", "text": "We use the same agents as described in Section V-A. RMHC (Algorithm 1) and MABRMHC (Algorithm 2) are applied to\noptimise the parameters of the space-battle game, aiming at maximising the win probability for the OLMCTS against the RAS. Since the true win probability is unknown, we need to define the fitness of a game using some winning rate by repeating the same game several times, i.e., resampling the game. We define the fitness value of a game g as the winning rate over r repeated games, i.e.,\nfitness(g) = 1\nr r\u2211 i=1 GameV alue(g). (4)\nThe value of game g is defined as\nGameV alue(g) =  1, if OLMCTS wins 0, if RAS wins 0.5, otherwise (a draw).\nA call to fitness(\u00b7) is actually based on independent r realizations of the same game. Due to the internal stochastic effects in the game, each realization may return a different\ngame value. We aim at maximising the fitness f in this work. The empirical winning rates shown in Fig. 2 are two example fitness(\u00b7) with r = 11 (left) and r = 69 (right). The strength of noise decreases while repeating the same game more times, i.e., increasing r.\nA recent work applied the RMHC and a two-armed banditbased RMHC with resamplings to a noisy variant of the OneMax problem, and showed both theoretically and practically the importance of choosing a suitable resampling number to accelerate the convergence to the optimum [22], [20], [21]. As the space-battle game introduced previously is stochastic and the agents can be stochastic as well, it is not trivial to model the noise or provide mathematically any optimal resampling number. Therefore, in this work, some resampling numbers are arbitrarily chosen and compared to give a primary idea about the necessary number of resamplings.\nFigs. 5 and 4 illustrate the overall performance of RMHC and MABRMHC using different resampling numbers over 1,000 optimisation trials with random starting parameters. A number of 5,000 game evaluations is allocated as optimisation budget in each trial. In other words, given a resampling number r, the fitness(\u00b7) (Eq. 4) is called at most 5, 000/r times.\nRMHC and MABRMHC using smaller resampling number achieve a faster move towards to the neighborhood of the optimum at the beginning of optimisation, however, they do not converge to the optimum along with time; despite the slow speed at the beginning, RMHC and MABRMHC using larger resampling number finally succeed in converging to the optimum in the limited budget. A dynamic resampling number which smoothly increases with the number of generations will\nbe favourable. Using smaller budget, MABRMHC reaches the neighborhood of the optimum faster than RMHC. While the current best-so-far fitness is near the optimal fitness value, it\u2019s not surprising to see the jagged curves (Fig. 4, right) while the game evaluation consumed is moderate. The drop to the valley dues to the exploration of MABRMHC, then it manages to return to the previous optimum found or possibly find another optimum. Along with the increment of budget, i.e., game evaluations, the quality of best-so-far games found by MABRMHC remains stable."}, {"heading": "C. Evolving games in a larger search space", "text": "All the 5 parameters considered previously are used for evolving the game rules. In this section, we expand the parameter space by taking into account a parameter for graphical object: the radius of ship. The legal values for ship\u2019s radius are 10, 20, 30, 40 and 50. Thus, the search space is 6-dimensional and the total number of possible games is increase to 72,000 (5 times larger).\nInstead of an intelligent agent and a deterministic agent, we play the same OLMCTS agent (with 350 iterations), which has been used previously in Section V-A and Section V-B, against two of its instances: a OLMCTS with 700 iterations and a OLMCTS with 175 iterations, denoted as OLMCTS700 and OLMCTS175 respectively. The same optimisation process using RMHC and MABRMHC is repeated separately, using 1,000 game evaluation. The resampling numbers used are 5 and 50, the ones which have achieved either fastest convergence at the beginning or provides the best recommendation\nat the end of optimisation (after 1,000 game evaluations), respectively. We aim at verifying if the same algorithms still perform well in a larger parameter space and with smaller optimisation budget.\nFig. 6 shows the average fitness value respected to the number of game evaluations over 11 optimisation trials. Resampling 50 times (black curves in Fig. 6) the same game instance guarantee a more accurate winning rate, while resampling 5 times (red curves in Fig. 6) seems to converge faster.\nTo validate the quality of recommendations, we play each recommended game instance, optimised by playing OLMCTS against OLMCTS175, 100 times using the OLMCTS175 and a random agent, which uniformly randomly returns a legal action. The idea is to verify that the game instances optimised for OLMCTS, are still playable and beneficial for OLMCTS instance with small number of iterations. The statistic is summarised in Table II. The game instances recommended\nby RMHC and MABRMHC after optimising for OLMCTS with more iterations are still beneficial for the OLMCTS with less iterations. The game is still very difficult for the random agent."}, {"heading": "D. But what are the evolved games actually like?", "text": "To understand the results of the optimisation process, we visually inspected a random sample of games that had been found to have high fitness in the optimisation process, and compared these with several games that had low fitness.\nWe can discern some patterns in the high-fitness games. One of them is to simply have a very high cost for firing missiles. This is somewhat disappointing, as it means that the OLMCTS agent will score higher simply by staying far away from the RAS agent. The latter will quickly reach large negative scores.\nA more interesting pattern was to have low missile costs, slow missiles, fast turning speed and fast thrusters. This resulted in a behaviour where the OLMCTS agent coasts around the screen in a mostly straight line, most of the time out of reach of the RAS agent\u2019s missiles. When it gets close to the RAS agent, the OLMCTS turns to intercept and salvo of missiles (which typically all hit), and then flies past.\nIn contrast, several of the low-fitness games have low missile costs and low cool-down times, so that the RAS agent effectively surrounds itself with a wall of missiles. The OLMCTS agent will occasionally attack, but typically loses more score from getting hit and than it gains from hitting the RAS agent. An example of this can be seen in Fig. 3.\nIt appears from this that the high-fitness games, at least\nthose that do not have excessive missile costs, are indeed deeper games in that skilful play is possible."}, {"heading": "VI. CONCLUSION AND FURTHER WORK", "text": "The work described in this paper makes several contributions in different directions. Our main aim in this work is to provide an automatic game tuning method using simple but efficient black-box noisy optimisation algorithms, which can serve as a base-level game generator and part on an AI-assisted game design tool, assisting a human game designer with tuning the game for depth. The baseline game generator can also help with suggesting game variants that a human designer can build on. Conversely, instead of initialising the optimisation with randomly generated parameters in the search space (as what we have done in this paper), human game designers can provide a set of possibly good initial parameters with their knowledge and experiences.\nThe game instance evolving provides a method for automatic game parameter tuning or for automatically designing\nnew games or levels by defining different fitness function used by optimisation algorithms. Even a simple algorithm such as RMHC may be used to automate game tuning. The application of other optimisation algorithms is straightforward.\nThe two tested optimisation algorithms achieve fast convergence towards the optimum even with a small resampling number when optimising for the OLMCTS against the RAS. Using dynamic non-adaptive or adaptive resampling numbers increasing with the generation number, such as the resampling rules discussed in [23], to take the strength of both small and big numbers of resamplings will be favourable.\nThough the primary application of MABRMHC to the space-battle game shows its strength, there is still more to explore. For instance, the selection between parent and offspring is still achieved by resampling each of them several times and comparing their average noise fitness value. However, the classic bandit algorithm stores the average reward and the times that each sampled candidate has been re-evaluated, which is also a form of resampling. We are not making use of this information while making the choice between the parent and offspring at each generation. Using a better recommendation policy (such as UCB or most visited) seems like a fruitful avenue of future work. Another potential issue is the dependencies between the parameters to be optimised in some games or other real world problems. A N-Tuple Bandit Evolutionary Algorithm [24] is proposed to handle such case.\nThe study of the winning rate distribution and landscape over game instances helps us understand more about the game difficulty. Another possible future work is the study of fitness distance correlation across parameters. Isaksen et al. [5] used Euclidean distance for measuring distance between game instances of Flappy Bird and discovered that such a simple measure can be misleading, since the difference between game instances does not always reflect the difference between their parameter values. We observe the same situation when analysing the landscape of fitness value by the possible values of individual game parameter (Fig. 3).\nThough we focus on a discrete domain in this work, it\u2019s obviously applicable to optimise game parameters in continuous domains, either by applying continuous black-box noisy optimisation algorithms or by discretising the continuous parameter space to discrete values. Evolving parameters for some other games, such as the games in GVG-AI framework, is another interesting extension of this work.\nThe approach is currently constrained by the limited intelligence of the GVG-AI agent we used, the proof of which is that on many instances of the game a reasonable human player is able to defeat both rotate-and-shoot (RAS) and the OLMCTS players. This problem will be overcome over time as the set of available GVG-AI agents grows."}], "references": [{"title": "An experiment in automatic game design.", "author": ["J. Togelius", "J. Schmidhuber"], "venue": "Proceedings of the 2008 IEEE Conference on Computational Intelligence and Games,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Evolutionary game design", "author": ["C. Browne", "F. Maire"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games, vol. 2, no. 1, pp. 1\u201316, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Aesthetic considerations for automated platformer design.", "author": ["M. Cook", "S. Colton", "A. Pease"], "venue": "The Eighth Annual AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Procedural content generation in games: A textbook and an overview of current research", "author": ["N. Shaker", "J. Togelius", "M.J. Nelson"], "venue": "Procedural Content Generation in Games: A Textbook and an Overview of Current Research, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Discovering unique game variants", "author": ["A. Isaksen", "D. Gopstein", "J. Togelius", "A. Nealen"], "venue": "Computational Creativity and Games Workshop at the 2015 International Conference on Computational Creativity, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Rolling horizon coevolutionary planning for two-player video games", "author": ["J. Liu", "D. P\u00e9rez-Li\u00e9bana", "S.M. Lucas"], "venue": "Proceedings of the IEEE Computer Science and Electronic Engineering Conference (CEEC), 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Metagame in symmetric chess-like games", "author": ["B. Pell"], "venue": "1992.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1992}, {"title": "Towards automatic personalised content creation for racing games", "author": ["J. Togelius", "R. De Nardi", "S.M. Lucas"], "venue": "2007 IEEE Symposium on Computational Intelligence and Games. IEEE, 2007, pp. 252\u2013259.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Searchbased procedural content generation: A taxonomy and survey", "author": ["J. Togelius", "G.N. Yannakakis", "K.O. Stanley", "C. Browne"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games, vol. 3, no. 3, pp. 172\u2013186, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic content generation in the galactic arms race video game", "author": ["E.J. Hastings", "R.K. Guha", "K.O. Stanley"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games, vol. 1, no. 4, pp. 245\u2013 263, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Towards a generic framework for automated video game level creation", "author": ["N. Sorenson", "P. Pasquier"], "venue": "European Conference on the Applications of Evolutionary Computation. Springer, 2010, pp. 131\u2013 140.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Automatic generation of game elements via evolution", "author": ["D. Ashlock"], "venue": "Proceedings of the 2010 IEEE Conference on Computational Intelligence and Games. IEEE, 2010, pp. 289\u2013296.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-faceted evolution of simple arcade games.", "author": ["M. Cook", "S. Colton"], "venue": "Proceedings of the 2011 IEEE Conference on Computational Intelligence and Games,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Towards automated game design", "author": ["M.J. Nelson", "M. Mateas"], "venue": "Congress of the Italian Association for Artificial Intelligence. Springer, 2007, pp. 626\u2013637.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Automated tweaking of levels for casual creation of mobile games", "author": ["E.J. Powley", "S. Gaudl", "S. Colton", "M.J. Nelson", "R. Saunders", "M. Cook"], "venue": "2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Depth in strategic games", "author": ["F. Lantz", "A. Isaksen", "A. Jaffe", "A. Nealen", "J. Togelius"], "venue": "under review, 2017.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "General video game evaluation using relative algorithm performance profiles", "author": ["T.S. Nielsen", "G.A. Barros", "J. Togelius", "M.J. Nelson"], "venue": "European Conference on the Applications of Evolutionary Computation. Springer, 2015, pp. 369\u2013380.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning DFA: Evolution versus Evidence Driven State Merging", "author": ["S.M. Lucas", "T.J. Reynolds"], "venue": "Evolutionary Computation, 2003. CEC\u201903. The 2003 Congress on, vol. 1. IEEE, 2003, pp. 351\u2013358.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning deterministic finite automata with a smart state labeling evolutionary algorithm", "author": ["\u2014\u2014"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 27, no. 7, pp. 1063\u20131074, 2005.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Bandit-based random mutation hill-climbing", "author": ["J. Liu", "D. Pe\u0155ez-Liebana", "S.M. Lucas"], "venue": "arXiv preprint arXiv:1606.06041, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Bandit-based random mutation hill-climbing", "author": ["D.P.-L. Jialin Liu", "S.M. Lucas"], "venue": "Evolutionary Computation, 2017. CEC\u201917. The 2017 Congress on. IEEE, 2017.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "Optimal resampling for the noisy onemax problem", "author": ["J. Liu", "M. Fairbank", "D. P\u00e9rez-Li\u00e9bana", "S.M. Lucas"], "venue": "arXiv preprint arXiv:1607.06641, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Portfolio methods in uncertain contexts", "author": ["J. Liu"], "venue": "Ph.D. dissertation, Universit\u00e9 Paris-Saclay, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "The n-tuple bandit evolutionary algorithm for automatic game improvement", "author": ["K. Kunanusont", "R.D. Gaina", "J. Liu", "D. Perez-Liebana", "S.M. Lucas"], "venue": "Evolutionary Computation, 2017. CEC\u201917. The 2017 Congress on. IEEE, 2017.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "There have been various attempts to automate or part-automate the game generation process, as this is an interesting challenge for AI and computational creativity [1], [2], [3].", "startOffset": 163, "endOffset": 166}, {"referenceID": 1, "context": "There have been various attempts to automate or part-automate the game generation process, as this is an interesting challenge for AI and computational creativity [1], [2], [3].", "startOffset": 168, "endOffset": 171}, {"referenceID": 2, "context": "There have been various attempts to automate or part-automate the game generation process, as this is an interesting challenge for AI and computational creativity [1], [2], [3].", "startOffset": 173, "endOffset": 176}, {"referenceID": 3, "context": "Many video games require content to be produced for them, and recent years have seen a surge in AI-based procedural content generation [4].", "startOffset": 135, "endOffset": 138}, {"referenceID": 4, "context": "Recent work has demonstrated the potential of this approach, automatically generating distinct and novel variants of the minimalist mobile game Flappy Bird [5].", "startOffset": 156, "endOffset": 159}, {"referenceID": 5, "context": "[6] introduced a two-player space-battle game,", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] to the GVG-AI framework, then uses the Random Mutation Hill Climber (RMHC) and Multi-Armed Bandit RMHC (MABRMHC) to evolve game parameters to provide some game instances that lead to high winning rates for GVGAI sample MCTS agents.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Attempts to automatically design complete games go back to Barney Pell, who generated rules for chess-like games [7].", "startOffset": 113, "endOffset": 116}, {"referenceID": 7, "context": "[8] evolved racing tracks in a car racing game using a simple multi-objective evolutionary algorithm called Cascading Elitism.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "This can be seen as an early form of experience-driven procedural content generation [9], where game content is generated through search in content space using evolutionary computation or some other form of stochastic optimisation.", "startOffset": 85, "endOffset": 88}, {"referenceID": 9, "context": "Similar methods have since been used to generate many types of game content, such as particle systems for weapons in a space shooter [10], platform game levels [11] or puzzles [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 10, "context": "Similar methods have since been used to generate many types of game content, such as particle systems for weapons in a space shooter [10], platform game levels [11] or puzzles [12].", "startOffset": 160, "endOffset": 164}, {"referenceID": 11, "context": "Similar methods have since been used to generate many types of game content, such as particle systems for weapons in a space shooter [10], platform game levels [11] or puzzles [12].", "startOffset": 176, "endOffset": 180}, {"referenceID": 0, "context": "Togelius and Schmidhuber [1] used a simple hill-climber to generate single-player", "startOffset": 25, "endOffset": 28}, {"referenceID": 12, "context": "[13], [3], who used search-based methods to design rulesets, maps and", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[13], [3], who used search-based methods to design rulesets, maps and", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "In a similar vein, Browne and Maire [2] developed a system", "startOffset": 36, "endOffset": 39}, {"referenceID": 13, "context": "Nelson and Mateas [14], who use reasoning methods to create Wario Ware-style minigames out of verb-noun relations and common minigame design patterns.", "startOffset": 18, "endOffset": 22}, {"referenceID": 4, "context": "on generating playable Flappy Bird variants [5].", "startOffset": 44, "endOffset": 47}, {"referenceID": 14, "context": "[15] optimise the parameters of an abstract touch-based mobile game, showing that parameter changes to a single ruleset can give rise to what feels and plays like different games.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "This property is universally considered desirable by game designers, yet it is hard to define properly; some of the definitions build on the idea of a skill chain, where deeper games simply have more things that can be learned [16].", "startOffset": 227, "endOffset": 231}, {"referenceID": 16, "context": "Therefore, we can use game-playing agents of different strengths to play the same game, and the bigger the difference in outcome the greater the depth [17].", "startOffset": 151, "endOffset": 155}, {"referenceID": 5, "context": "[6] to the GVG-AI framework, then use RMHC and MABRMHC to evolve game parameters to provide some game instances that lead to high winning rate for GVG-AI sample", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "The Random Mutation Hill-Climber (RMHC) is a simple but efficient derivative-free optimisation method mostly used in discrete domains [18], [19].", "startOffset": 134, "endOffset": 138}, {"referenceID": 18, "context": "The Random Mutation Hill-Climber (RMHC) is a simple but efficient derivative-free optimisation method mostly used in discrete domains [18], [19].", "startOffset": 140, "endOffset": 144}, {"referenceID": 19, "context": "Multi-Armed Bandit Random Mutation Hill-Climber (MABRMHC), derived from the 2-armed bandit-based RMHC [20], [21], uses both UCB-style selection and mutation rules.", "startOffset": 102, "endOffset": 106}, {"referenceID": 20, "context": "Multi-Armed Bandit Random Mutation Hill-Climber (MABRMHC), derived from the 2-armed bandit-based RMHC [20], [21], uses both UCB-style selection and mutation rules.", "startOffset": 108, "endOffset": 112}, {"referenceID": 0, "context": "Require: X : search space Require: D = |X |: problem dimension (genome length) Require: f : X 7\u2192 [0, 1]: fitness function 1: Randomly initialise a genome x \u2208 X 2: bestF itSoFar \u2190 0 3: M \u2190 0 .", "startOffset": 97, "endOffset": 103}, {"referenceID": 0, "context": "Require: X : search space Require: D = |X |: problem dimension (genome length) Require: f : X 7\u2192 [0, 1]: fitness function 1: Randomly initialise a genome x \u2208 X 2: bestF itSoFar \u2190 0 3: M \u2190 0 .", "startOffset": 97, "endOffset": 103}, {"referenceID": 21, "context": "A recent work applied the RMHC and a two-armed banditbased RMHC with resamplings to a noisy variant of the OneMax problem, and showed both theoretically and practically the importance of choosing a suitable resampling number to accelerate the convergence to the optimum [22], [20], [21].", "startOffset": 270, "endOffset": 274}, {"referenceID": 19, "context": "A recent work applied the RMHC and a two-armed banditbased RMHC with resamplings to a noisy variant of the OneMax problem, and showed both theoretically and practically the importance of choosing a suitable resampling number to accelerate the convergence to the optimum [22], [20], [21].", "startOffset": 276, "endOffset": 280}, {"referenceID": 20, "context": "A recent work applied the RMHC and a two-armed banditbased RMHC with resamplings to a noisy variant of the OneMax problem, and showed both theoretically and practically the importance of choosing a suitable resampling number to accelerate the convergence to the optimum [22], [20], [21].", "startOffset": 282, "endOffset": 286}, {"referenceID": 22, "context": "Using dynamic non-adaptive or adaptive resampling numbers increasing with the generation number, such as the resampling rules discussed in [23], to take the strength of both small and big numbers of resamplings will be favourable.", "startOffset": 139, "endOffset": 143}, {"referenceID": 23, "context": "A N-Tuple Bandit Evolutionary Algorithm [24] is proposed to handle such case.", "startOffset": 40, "endOffset": 44}, {"referenceID": 4, "context": "[5] used Euclidean distance for measuring distance between game instances of Flappy Bird and discovered that such a simple measure can be misleading, since the difference between game instances does not always reflect the difference between their parameter values.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "Most games have, or can be generalised to have, a number of parameters that may be varied in order to provide instances of games that lead to very different player experiences. The space of possible parameter settings can be seen as a search space, and we can therefore use a Random Mutation Hill Climbing algorithm or other search methods to find the parameter settings that induce the best games. One of the hardest parts of this approach is defining a suitable fitness function. In this paper we explore the possibility of using one of a growing set of General Video Game AI agents to perform automatic playtesting. This enables a very general approach to game evaluation based on estimating the skill-depth of a game. Agent-based playtesting is computationally expensive, so we compare two simple but efficient optimisation algorithms: the Random Mutation HillClimber and the Multi-Armed Bandit Random Mutation HillClimber. For the test game we use a space-battle game in order to provide a suitable balance between simulation speed and potential skill-depth. Results show that both algorithms are able to rapidly evolve game versions with significant skill-depth, but that choosing a suitable resampling number is essential in order to combat the effects of noise.", "creator": "LaTeX with hyperref package"}}}