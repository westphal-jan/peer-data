{"id": "1703.06471", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2017", "title": "Multi-Timescale, Gradient Descent, Temporal Difference Learning with Linear Options", "abstract": "Deliberating on large or continuous state spaces have been long standing challenges in reinforcement learning. Temporal Abstraction have somewhat made this possible, but efficiently planing using temporal abstraction still remains an issue. Moreover using spatial abstractions to learn policies for various situations at once while using temporal abstraction models is an open problem, especially in small-scale environments. Therefore, in practice, it is important to consider spatial abstraction as a necessary step for designing and implementing a state in a flexible state model.\n\n\n\n\nTibetty-based models need to be as diverse as they are able to be.\nTibetty-based models are a particularly promising approach for learning theory and design, for example with several frameworks. For example, the first class of model types has a new type of model that contains many other types of model that can be used in the context of a network of networks. This type of model should be modeled using data-driven approach.\n\nTibetty-based models can be easily deployed to a distributed or distributed environment, where the implementation and implementation of any model can be deployed as part of a system, such as a web application or even a web application. The concept of a distributed state model is also important in the design of large or continuous state networks, where the model layer should be structured.\nThis type of model should be modeled using data-driven approach and should be modeled using data-driven approach.\nAnother important advantage of using the data-driven approach in this post is that there is an explicit way for data-driven models to be written as structured or consistent across large-scale environments. Therefore, with a strong focus on a high-level network of networks with low-level state resources and low-level data-driven models, there is a high level of redundancy between the models.\nOne downside to this approach is that for large-scale models to be based on high-level model architectures, the state-oriented model models will need to have a high level of redundancy in order to have a high level of redundancy in place. Therefore, many models cannot be created directly from any of these models, because a model can not be produced directly from any source. For example, for large-scale models, the model layer should have the following two layers:\nThe layer layers should be the most common and most common (as it may not be widely distributed or distributed at all)\nThe layer layers should be the most common (as it may not be widely distributed or", "histories": [["v1", "Sun, 19 Mar 2017 17:31:13 GMT  (578kb,D)", "http://arxiv.org/abs/1703.06471v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["peeyush kumar", "doina precup"], "accepted": false, "id": "1703.06471"}, "pdf": {"name": "1703.06471.pdf", "metadata": {"source": "CRF", "title": "Multi-Timescale, Gradient Descent, Temporal Difference Learning with Linear Options", "authors": ["Peeyush Kumar"], "emails": ["agaron@uw.edu", "dprecup@cs.mcgill.ca"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n06 47\n1v 1"}, {"heading": "1 Introduction", "text": "Decision making involves choosing among different actions over a multiple range of time scales. The use of higher level temporally extended actions allow an AI agent to solve an entire task in smaller number of decision epochs. Like many other AI systems, Temporal abstraction is often used in reinforcement learning to solve large problems efficiently. Various methods have been proposed to reduce computational burden, and reducing data complexity in learning, by allowing planning in AI systems to compute a good policy for many situations at once. In environments with continuously valued observations or others with a large number of states, some form of generalization is necessary. Linear methods are perhaps the most common and best understood class of generalization mechanisms. Linear methods encompass a wide spectrum of approaches including look-up tables, state aggregation methods, radial basis functions with fixed bases, etc. Frameworks for temporal abstractions do not usually contain an intrinsic structure to describe such generalizations. To achieve generalizations usually temporal abstractions are defined over parametric representation of the environment states, among which linear representations are the most dominant approach.\nSince temporal effects are linked to the agents decision method, temporal abstraction framework is tightly coupled with controls. Markov Decision Process (MDP) models provide a formal method of planning in controlled dynamical systems (Sutton and Barto (1998)). The MDP models for planning which are based on primitive actions can be easily extended to SMDP (Semi Markov Decision Process) planning models which include temporally extended actions. The SMDP theory with models based on options framework, provides a formal method for representing temporally abstract information. An option O is a tuple {I, \u00b5, \u03b2}, where I is the initiation set of the option, \u00b5 is the behavior policy of the option O , \u03b2 is the termination condition which defines the time scale of the option. The option models (\u00b5 and \u03b2) are either constructed by learning maximum-likelihood primitive models and computing exactly option models by composing primitive models or learning option models using intraoption learning, importance sampling, or eligibility traces.\nAlthough the option models have been proved to be quite successful in continuous state spaces (one of the central advantages of temporal abstraction), the ability to plan using temporal abstraction methods is still a challenge when the number of states is large. This is in large partly because learning option models for large state spaces is very expensive. Moreover, if the system needs to act in real time, there is not much time for deliberation. In such cases the system needs to make use of many option models over multiple time scales. We propose here that instead of learning option models from the data, the system should rather make use of many option models over multiple time scales even if they are a lot in number. We use the linear option models (Sorg and Singh (2010)) for learning and planning. Learning a policy over many option models does not effect the time complexity because the linear methods scale linearly with the complexity of the value function. Linear option model extends\nthe option framework from a tabular representation to a linear representation. These models provide a general mechanism for providing temporal abstraction over parametric representation of the state space. Behavior of such a model is proved to be sensible while learning and planning. Although linear options provide great benefits towards time efficiency, the SMDP-LSTD algorithm used for policy evaluation using linear options has a quadratic complexity. This makes their convergence slow as compared to the conventional linear complexity of the TD algorithms.\nRecently various temporal difference algorithms were proposed which are compatible with both linear function approximation and off-policy training using primitive action in MDP setting (GTD algorithm Sutton et al. (2001), GTD2 and TDC algorithms Sutton et al. (2009)). These algorithms were shown to converge to the TD fixed point with probability one. We propose a direct extension of the TDC algorithm to the SMDP setup. We show the convergence conditions for the SMDP-TDC algorithm using results from stochastic approximation theory."}, {"heading": "2 Background", "text": "An MDP is a tuple (S,A, P,R, \u03b3) consisting of a state set S, action set A, transition function P : S \u00d7 A \u00d7 S 7\u2192 [0, 1], an expected reward function R : S\u00d7A 7\u2192 IR, and a discount factor \u03b3 \u2208 [0, 1). Every MDP time step t, the agent takes an action at in state st, and the environment responds with a reward rt and resulting next state s\u2032t = st+1. A policy \u03c0 : S\u00d7 7\u2192 [0, 1] is a mapping from states to actions. An action-value function, Q(s, a) is the expected discounted return obtained by taking action a in state s and following \u03c0 thereafter. The Bellman equation defining the optimal action value function Q\u2217 is:\nQ\u2217(s, a) = R(s, a) + \u03b3 \u2211 s\u2032\u2208S P ss \u2032 a max b\u2208A Q\u2217(s\u2032, a)\nA policy that is greedy with respect to a given action value function maps each state to arg maxaQ(s, a). The value function V (s) corresponding to a given action value function is the value of the greedy action from each state: V \u03c0(s) = maxaQ\n\u03c0(s, a). A policy that is greedy with respect to the optimal value function is the optimal policy."}, {"heading": "2.1 Off Policy Temporal Difference Learning", "text": "Off Policy methods are an important classes of algorithms studied in Reinforcement Learning. Any controlled dynamical system is said to be off policy, if the behavior policy (the one which is being followed) is different from the policy being evaluated. Among various methods of learning in RL, temporal difference methods are undoubtedly the most important and novel ones. Temporal Difference methods have the advantage that they can learn directly from the raw data\nwithout any requirement of the environment\u2019s model as well as they can bootstrap by updating estimates based in part on other learned estimates. Among the temporal difference (TD) methods, off policy TD control algorithm known as Q-learning is the most central one. Its simplest form, one-step Q-learning, is defined by\nQ(st, at) = Q(st, at) + \u03b1 [ rt+1 + \u03b3max\na Q(st+1, a)\u2212Q(st, at) ]"}, {"heading": "2.2 Linear Methods for MDPs", "text": "In reinforcement learning, linear methods have long been used for value function approximation. Instead of using tabular representation of states, states are represented in terms of n dimensional feature vectors \u03c6(s) \u2208 IRn. Linear methods approximate the value function for a policy \u03c0 as linear combination of state feature vectors. V \u03c0(s) \u2248 \u03c6(s)T \u03b8\u03c0, where \u03b8\u03c0 is the vector parameter to be learned, \u03b8 \u2208 IRn.\nUnfortunately Q-learning does not show a good behavior when used with function approximations that are linear in the learned parameters. As a result convergence cannot be guaranteed for such methods (Baird (1995)). Several non-gradient-descent approaches to this problem have been developed, but none has been completely satisfactory . Second-order methods, such as LSTD (Bradtke et al. (1996), Boyan (2002)), can be guaranteed stable under general conditions, but their computational complexity is quadratic in the size of the state space. In recent years several gradient descent methods have been proposed (GTD, GTD2, and TDC algorithms ). These methods are shown to have a linear computation complexity and linear memory requirements. Among these methods TDC shows the fastest rate of convergence. The update estimates for TDC are:\n\u03b8k+1 = \u03b8k + \u03b1k(\u03b4k\u03c6k \u2212 \u03b3\u03c6\u2032k)(\u03c6Tkwk) (1)\nwhere \u03b1k and \u03b2k are the learning rate parameters, and wk is updated as,\nwk+1 = wk + \u03b2k(\u03b4k \u2212 \u03c6Tkwk)\u03c6k (2)\nand \u03b4k is the TD error\n\u03b4k = rk + \u03b3\u03b8 T k \u03c6 \u2032 k \u2212 \u03b8Tk \u03c6k (3)\nIt is proved that TDC algorithm converges to the TD fixed point with probability one."}, {"heading": "2.3 Options and SMDP", "text": "Options are temporally extended actions. They can be thought of as fixed policies that can be invoked in certain set of states and which terminate according to a termination condition. Formally an option is a triple (I, \u00b5, \u03b2), where the\nset I \u2282 S is the initiation set of the option, \u00b5 : S 7\u2192 A is the option policy and \u03b2 : S 7\u2192 [0, 1] maps each state to the set [0, 1] which is the probability of termination in each state s \u2208 S. MDP along with the options describe a Semi Markov Decision Process (SMDP). SMDP is like a MDP, except the options take varying amounts of time. In each SMDP time step, the agent selects an option and follows the option\u2019s policy \u00b5 until termination. In any state, one has to choose an option from the set of options available in that state. When the system enters a new subset of the state space, a new set of options becomes available.\nThe Bellman equation for SMDP setup is a direct extension of the Bellman equation for the MDP case\nQ\u2217O(s, o) = r o s + \u2211 s\u2032 P ss \u2032 o max o\u2032\u2208O\u2032s Q\u2217O(s \u2032, o\u2032)\nwhere ros is the expected discounted reward obtained during the execution of the option\nros = E [ rt + \u00b7 \u00b7 \u00b7+ \u03b3k\u22121rt+k\u22121|st = s, ot = o ] and P ss \u2032\no is the option\u2019s transition model Similarly the SMDP version of one step Q learning using options can be\nwritten as follows\nQ(s, o) = Q(s, o) + \u03b1 [ r + \u03b3k max\no\u2032\u2208Os\u2032 Q(s\u2032, o\u2032)\u2212Q(s, o) ] where k is the number of time steps elapsed between s and s\u2032, r denotes the\ncumulative discounted reward until the termination of the option."}, {"heading": "2.4 Linear Options", "text": "Linear options is a direct extension of the option framework from a tabular representation to a linear representation. Linear options are defined over states which are represented as n dimensional feature vector. A linear option is a tuple (I, \u00b5, \u03b2), where I \u2208 IRn is the initiation set, \u00b5 : IRn 7\u2192 A is the option policy, and \u03b2 : IRn 7\u2192 [0, 1] is the termination condition. Sorg and Singh (2010) proposed the idea of linear options as linear representations of options. They proposed an extension of the MDP-LSTD method for policy evaluation in the primitive action case to include options by defining SMDP-LSTD method for behavioral policy evaluation. The solution given by SMDP-LSTD is\n0 = t\u2211 k \u03c6k(rk + \u03b3k\u03c6 \u2032T k \u03b8k \u2212 \u03c6Tk \u03b8k) (4)\n\u03b8\u03c0 =\n[ t\u2211\nk=1\n\u03c6k(\u03c6k \u2212 \u03b3k\u03c6\u2032k)T ]\u22121 [ t\u2211\nk=1\n\u03c6krk\n] (5)\nA linear option expectation model of a behavior policy\u2019s dynamics is denoted by (F\u03c0, b\u03c0) which attempt to satisfy, for all time steps t.\nF\u03c0\u03c6t \u2248 E [\u03b3t\u03c6\u2032t|\u03c6t] (6)\nb\u03c0\u03c6t \u2248 E [rt|\u03c6t] (7)\nwhere F\u03c0\u03c6 can be interpreted as the expected discounted termination feature vector and b\u03c0\u03c6 can be interpreted as the expected discounted reward until termination. A learning and planning agent will need to estimate F\u03c0 and b\u03c0 from data. Given an input feature vector \u03c6, the following recursion defines a LOEM policy evaluation update to the value function parameters \u03b8k:\n\u03b8k+1 = \u03b8k + \u03b1k(b T \u03c0\u03c6+ \u03b8 T k F\u03c0\u03c6\u2212 \u03b8Tk \u03c6)\u03c6 (8)\nLOEM policy evaluation updates is shown to converge to the same behavior value function parameters as does the SMDP-LSTD algorithm."}, {"heading": "3 Gradient Descent TD algorithm for linear op-", "text": "tions\nThe LSTD algorithms developed can be guaranteed stable under general conditions but they are quadratic in computational complexity. We propose here a gradient descent method (SMDP-TDC ) for planning in Reinforcement learning using off-policy TD framework. SMDP-TDC is the direct extension of TDC algorithm (Equations 1, 2 and 3) to the SMDP setting. The option value function is approximated as V \u03c0(s) = maxoQ\n\u03c0(s, o) \u2248 \u03b8T\u03c6s. We aim to minimize the mean-square projected bellman error (MSPBE\u03b8 = \u2016V\u03b8 \u2212\u03a0TV\u03b8\u20162D. For the linear option architecture V\u03b8 = \u03a6\u03b8 where \u03a6 is the matrix whose rows are \u03c6s. \u03a0 is the projection operator, \u03a0 = \u03a6(\u03a6TD\u03c6)\u22121\u03a6TD (where D is the diagonal matrix containing stationary distribution), and T is the SMDP Bellman Operator.\nMSPBE\u03b8\n= \u2016V\u03b8 \u2212\u03a0TV\u03b8\u20162D = \u2016\u03a0(V\u03b8 \u2212 TV\u03b8)\u20162D = (\u03a0(V\u03b8 \u2212 TV\u03b8))TD(\u03a0(V\u03b8 \u2212 TV\u03b8)) = ((V\u03b8 \u2212 TV\u03b8))T\u03a0TD\u03a0((V\u03b8 \u2212 TV\u03b8)) = ((V\u03b8 \u2212 TV\u03b8))TDT\u03a6(\u03a6TD\u03c6)\u22121\u03a6TD((V\u03b8 \u2212 TV\u03b8)) = (\u03a6TD(TV\u03b8 \u2212 V\u03b8))T (\u03a6TD\u03a6)\u22121(\u03a6TD(TV\u03b8 \u2212 V\u03b8)) = E[\u03b4\u03c6]TE[\u03c6\u03c6T ]\u22121E[\u03b4\u03c6]\nNote that E[\u03c6\u03c6T ] = \u2211 s ds\u03c6s\u03c6 T s = \u03a6D\u03a6 T\nE[\u03b4\u03c6] = \u2211 s ds\u03c6s\n( r + \u03b3l\n\u2211 s\u2032 P oss\u2032V\u03b8(s \u2032)\u2212 V\u03b8(s) ) = \u03a6TD(TV\u03b8 \u2212 V\u03b8)\nTherefore,\n\u2212 1 2 \u2207MSPBE\u03b8 = E[(\u03c6\u2212 \u03b3l\u03c6\u2032)\u03c6T ]E[\u03c6\u03c6T ]\u22121E[\u03b4\u03c6] = (E[\u03c6]\u2212 E[\u03b3l\u03c6\u2032\u03c6T ])E[\u03c6\u03c6T ]\u22121E[\u03b4\u03c6] = E[\u03b4\u03c6]\u2212 \u03b3lE[\u03c6\u2032\u03c6T ]E[\u03c6\u03c6T ]\u22121E[\u03b4\u03c6] = E[\u03b4\u03c6]\u2212 \u03b3lE[\u03c6\u2032\u03c6T ]w\nwhere w \u2248 E[\u03c6\u03c6T ]\u22121E[\u03b4\u03c6]. Using this form for the gradient of MSPBE, we can write down the gradient descent expression (parameterized by \u03b1 and \u03b2) to minimize MSPBE as follows:\n\u03b8k+1 = \u03b8k + \u03b1k(\u03b4k\u03c6k \u2212 \u03b3l\u03c6\u2032k)(\u03c6Tkwk) (9)\nwhere wk is updated as,\nwk+1 = wk + \u03b2k(\u03b4k \u2212 \u03c6Tkwk)\u03c6k (10)\nand \u03b4k is the TD error\n\u03b4k = rk + \u03b3 l\u03b8Tk \u03c6 \u2032 k \u2212 \u03b8Tk \u03c6k (11)\nwhere l is the length of the option being executed. Convergence proof of SMDP-TDC can be carried out in a similar way as in Sutton et al. (2009). Hence it can be shown that SMDP-TDC converges to the TD fixed point with probability one."}, {"heading": "3.1 Convergence Proof for SMDP-TDC", "text": "Theorem 1 Consider the iterations 9 and 10 of the SMDP-TDC algorithm. Let the step-size sequences \u03b1k and \u03b2k, k \u2265 0 satisfy in this case \u03b1k, \u03b2k > 0, for all k,\u2211\u221e k=0 \u03b1k = \u2211\u221e k=0 \u03b2k =\u221e, \u2211\u221e k=0 \u03b1 2 k, \u2211\u221e k=0 \u03b2 2 k <\u221e and that \u03b1k \u03b2k \u2192 0 as k \u2192\u221e. Further assume that (\u03c6k, rk, \u03c6 \u2032 k) is an i.i.d sequence with uniformly bounded second moments. Let A = E[\u03c6k(\u03c6k \u2212 \u03b3l\u03c6\u2032k)T ], b = E[rk\u03c6k], and C = E[\u03c6k\u03c6Tk ]. Assume that A and C are non singular. Then the parameter vector \u03b8k converges with probability one to the TD fixed point.\nProof The proof of this theorem is based on a two time scale difference in the step-size schedule {\u03b1k} and {\u03b2k}; refer Borkar (1997) for a convergence analysis of the general two timescale stochastic approximation recursions. The recursions 9 and 10 correspond to the faster and slower recursions respectively. This is because beyond some integer N0 > 0 \u2200 k, the increments in 9 are uniformly larger than those in 10 and hence converge faster. Along faster timescale, i.e, the one corresponding to {\u03b2k}, the associated system ODEs corresponds to\n\u03b8\u0307(t) = 0 (12)\nw\u0307(t) = E[\u03b4t\u03c6t|\u03b8(t)]\u2212 Cw(t) (13)\nThe ODE 12 suggests that \u03b8(t) = \u03b8 from the faster timescale perspective. Indeed, recursion 9 can be rewritten as \u03b8k+1 = \u03b8k + \u03b2k\u03be(k), where from 9, \u03be(k) = ( \u03b1k \u03b2k (\u03b4k\u03c6k \u2212 \u03b3l\u03c6\u2032k\u03c6Tkwk) ) \u2192 0 almost surely as k \u2192 \u221e because \u03b1k\u03b2k \u2192 0 as k \u2192 \u221e. By the Hirsch lemma (Hirsch (1989)), it follow that \u2016\u03b8k \u2212 \u03b8\u2016 \u2192 0 almost surely as k \u2192\u221e for the same \u03b8 that depends on the initial condition \u03b80 of recursion 9.\nConsider now the recursion 10. Let Mk+1 = (\u03b4k\u03c6k \u2212 \u03c6k\u03c6Tkwk)\u2212 E[(\u03b4k\u03c6k \u2212 \u03c6k\u03c6 T kwk)|F(k)], where F(k) = \u03c3(wm, \u03b8m,m \u2264 k;\u03c6s, \u03c6\u2032s, rs, s < k), k \u2265 1 are the sigma fields generated by w0, \u03b80, wm+1, \u03b8m+1, \u03c6m, \u03c6 \u2032 m, 0 \u2264 m < k. It is easy to verify that Mk+1, k \u2265 0 are integrable random variables that satisfy E[Mk+1|F(k)] = 0, \u2200k \u2265 0. Also because rk, \u03c6k and \u03c6\u2032k have uniformly bounded second moments, it can be seen that\nE [ \u2016Mk+1\u20162 |F(k) ] \u2264 c1 ( 1 + \u2016|wk||2 + ||\u03b8k||2 ) ,\nfor all k \u2265 0, for some constant c1 > 0 Now consider the ODE pair 12 and 13. Because \u03b8(t) = \u03b8 from 12, the ODE 13 can written as\nw\u0307(t) = E[\u03b4t\u03c6t|\u03b8]\u2212 Cw(t) (14)\nNow consider the function h(w) = E[\u03b4\u03c6|\u03b8] \u2212 Cw, i.e. the driving vector field of the ODE 14. For 14, w\u2217 = C\u22121E[\u03b4\u03c6|\u03b8] is the unique globally asymptotically stable equilibrium. Let h\u221e(.) be the funciton defined by h\u221e(w) = limr\u2192\u221e h(rw) r . Then h\u221e(w) = \u2212Cw. For the ODE w\u0307(t) = h\u221e(w(t)) = \u2212Cw(t) the origin is a globally asymptotically stable equilibrium because C is a positive definite matrix. The assumptions are now verified and by Borkar and Meyn (2000) Theorem 2.2 we obtain ||wk \u2212 w\u2217|| \u2192 0 almost surely as k \u2192\u221e.\nConsider now the slower time scale recursion 9. Using above, 10 can be rewritten as\n\u03b8k+1 = \u03b8k + \u03b1k(\u03b4k\u03c6k \u2212 \u03b3l\u03c6\u2032k\u03c6TkC\u22121E[\u03b4k\u03c6k|\u03b8k]) (15)\nLet G(k) = \u03c3(\u03b8m,m \u2264 k;\u03c6s, \u03c6\u2032s, rs, s < k) be the sigma fields generated by the quantities \u03b80, \u03b8m+1, \u03c6m, \u03c6 \u2032 m, 0 \u2264 m < k. Let\nZk+1 = (\u03b4k\u03c6k \u2212 \u03b3l\u03c6\u2032k\u03c6kC\u22121E[\u03b4k\u03c6k|\u03b8k]) \u2212 E[(\u03b4k\u03c6k \u2212 \u03b3l\u03c6\u2032k\u03c6TkC\u22121E[\u03b4k\u03c6k \u03b8k])|G(k)] = (\u03b4k\u03c6k \u2212 \u03b3l\u03c6\u2032k\u03c6TkC\u22121E[\u03b4k\u03c6k|\u03b8k]) \u2212 E[\u03b4k\u03c6k|\u03b8k]\u2212 \u03b3lE[\u03c6\u2032k\u03c6Tk ]C\u22121E[\u03b4k\u03c6k|\u03b8k]\nIt is easy to see that Zk, k \u2265 0 are integrable random variables and E [Zk+1|G(k)] = 0,\u2200k \u2265 0. Further,\nE [ \u2016Zk+1\u20162 |G(k) ] \u2264 c2 ( 1 + \u2016\u03b8k\u20162 ) , k \u2265 0\nfor some constant c2 > 0 again because rk, \u03c6k and \u03c6 \u2032 k have uniformly bounded second moments. Consider now the following ODE associated with 9\n\u03b8\u0307(t) = ( I \u2212 E [ \u03b3l\u03c6\u2032\u03c6t ] C\u22121 ) E[\u03b4\u03c6|\u03b8(t)] (16)\nLet h\u0304(\u03b8(t)) be the driving vector field of the ODE 16. Note that h\u0304(\u03b8(t)) = ( I \u2212 E [ \u03b3l\u03c6\u2032\u03c6T ] C\u22121 ) E [\u03b4\u03c6|\u03b8(t)]\n= ( C \u2212 E [ \u03b3l\u03c6\u2032\u03c6T ]) C\u22121E [\u03b4\u03c6|\u03b8(t)]\n= ( E [ \u03c6\u03c6T ] \u2212 E [ \u03b3l\u03c6\u2032\u03c6T ]) C\u22121E [\u03b4\u03c6|\u03b8(t)]\n= ATC\u22121(\u2212A\u03b8(t) + b)\nbecause E[\u03b4\u03c6|\u03b8(t)] = \u2212A\u03b8(t) + b. Now \u03b8\u2217 = A\u22121b can be seen to be the unique globally asymptotically stable\nequilibrium for 16. Let h\u0304\u221e(\u03b8) = limr\u2192\u221e h\u0304(r\u03b8) r . Then h\u0304\u221e(\u03b8) = \u2212A TC\u22121A\u03b8. Consider now the ODE\n\u03b8\u0307(t) = \u2212ATC\u22121A\u03b8(t) (17)\nBecause C\u22121 is positive definite and A has full rank (as it is nonsingular by assumption), the matrix ATC\u22121A is also positive definite. The ODE 17 has the origin as its unique globally asymptotically stable equilibrium. The assumptions are once again verified and the claim follows."}, {"heading": "4 Multiple Time Scales and Real Time RL", "text": "Autonomous systems make decisions by choosing actions at multiple time scales. Although planning for acting at multiple time scales requires more computational time. Real time systems do not have much time for deliberation. Hence a lot of research has been done to construct just the right number of options by optimizing the computational time around models to accommodate multiple time scales (Stolle and Precup (2002), Wolfe and Barto (2005)). Is it necessary\nto be so careful, or can we do around with many option models? The ideas central to planning is searching and learning while searching. We demonstrate empirically that it is much better to search with many option models, rather than using few handcrafted ones.\nWe use the sample based search to search through the state space. The search is done by picking up a quasi-random model and sampling the next state, up to a given depth. At the given depth a guess of the value function is used to greedily pick up the best move. Typically a large number of rollouts is performed, and values lower in the search tree are used to update the values higher up in the search tree (Kocsis and Szepesvri (2006), Veness et al. (2010)). Number of rollouts is a parameter that controls the time for deliberation. The experiment was performed on the gridworld domain shown in Figure 1. We use the hallway going options where the option models are precoded. Values for all options are learned over 50 trials, using off-policy random behavior. The following six experimental conditions were used to describe what to do on the next time step a) Execute the greedy policy using only primitives b) Pick the primitive action suggested by the greedy option policy with the hand-crafted options c) Pick the primitive action suggested by the greedy policy with random options d) Do 100 rollouts of depth 3 using only primitive actions e) Do 100 rollouts of depth 3 using primitive actions and crafted options f) Do 100 rollouts of depth 3, sampling among 6 options from the random set\nAs wee see in Figure 2 search using many option models is better than just using a few handcrafted ones. The results also demonstrate the usefulness of\ntemporal abstraction and search. Search, and learning using many option model does not effect the time, as the computational complexity of the linear option framework does not depend upon the number of options being used. There are different ways to incorporate search in a real-time system a) The planning can be separately threaded b) Trying to anticipate what may happen using the model and planning for different states before the sensation is received ."}, {"heading": "5 Empirical Results", "text": "To assess the practical utility of the proposed framework we demonstrate here some experiments where the system learns policy over multiple time scale linear option models updating estimates using the SMDP-TDC algorithm. We also demonstrate the time scales on which the system acts depending upon its position in the environment and its immediate neighbors. The results are demonstrated over 2 domains. The first domain is a grid world domain as shown in Figure 1 and the second domain is the continuous room domain with large dimension feature representation."}, {"heading": "5.1 Grid World Domain", "text": "The system starts from any state in the state space, it receives a reward of +10 when it reaches the goal state(blue box in Figure 1), while it receives a reward of -3 when it hits the wall and a reward of -1 for all the other transitions. The system has four primitive actions, while acting in the environment with 15% noise. We demonstrate two experimental conditions where option models are composed over multiple time scales and various random policies. The first experiment consists of two time scales - \u03b2 = 0.5, 1 and biased random policies defined as follows - Pick a probability x \u2208 (0, 1), pick uniformly an action a, the system will pick the action a with probability x and all other actions uniformly with probability 1\u2212 x. The sytem has same termination conditions, and pick primitive actions with respect to the same policy for all states. Likewise there are 40 policies, hence the system has 80 options available at all states. The second experimental condition consists of 5 termination conditions - \u03b2 = 0.2, 0.4, 0.50.8, 1 with the same set of policies and hence it has 200 options available at all states. We plot the average return encounterd by the system averaged over various multiple runs in Figure 3.\nImportant observations which we note in the experiments are that a) Far away from the goal, the best options are the ones biased towards the goal, and with a larger time scale. b) Close to the wall, the best options are the ones biased towards the goal and with a shorter time scale. c) Still closer, the best options are biased up or down. d) The system which does not decide on each time step sometimes hits the wall. e) It is much better to search with a lot of random options over multiple time scales rather than a few handcrafted ones .\nIn Figure 4 we compare the performance of SMDP-TDC method with the primitive TDC method. We also compare the same results without using any\nfunction approximation while following a) SMDP Value Learning, and b)MDP Q learning. We observe the convergence of SMDP-TDC algorithm to a recursive optimal solution. In Figure 4 we observe that option models with multiple time scales reach the goal state more often than its primitive counter part."}, {"heading": "5.2 Continuous Domain", "text": "We ran our second set of experiments on a continuous navigation domain as described in Sorg and Singh (2010). The domain (Figure 6), which is 10 \u00d7 10 continuous room world consists of rooms which are separated by walls as shown. In addition the floor of each room is colored with one of the 4 colors (P)urple, (G)reen, (Y)ellow, (B)lue. In figure the floor colors are indicated by the respective first letter, and rooms are seperated by using dashed lines. While\nthe walls are shown using solid lines.\nThe agent controls a circular wheel robot for navigation that is capable of observing its current global position [x, y] \u2208 [0, 10]2 and its current global orientation in terms of the an angle \u03c8 \u2208 [0, 360]. In addition it can also detect the color of the floor beneath it. It has 3 available primitive actions:forward which moves the robot one unit in the forward direction with respect to its current position and orientation, left turns the robot towards left by an angle of 30o at its current location, and right which turns the robot towards right with an angle of 30o at its current location. If the robot hits the wall, its motion is halted in the direction perpendicular to the wall plane. The agent is given a reward of 1 for reaching the purple room on the right side of the domain. After receiving its reward, the agent is transported to the yellow room on the left corner of the building. At all other times, the agent receives a small negative reward of 0.01.\nThe agent represent states using feature vectors \u03c6 of 1204 features. The first four features encode the color of the floor in a binary format. The rest of the features are calculated by placing radial basis functions every one step in the x\u2212y direction and every 30o for \u03c8. The feature values are calculated using \u03c6i = bexp(\u2212 12 (s \u2212 ui) TC(s \u2212 ui)) where s = [x, y, \u03c8], b = 10, C = diag( 11.2 , 1 1.2 , 1 30 ), where ui are the center of the radial basis functions. To make the feature vector sparse, which can help computationally, any feature that would have had a value \u03c6i < 0.1, is set to 0.\nWe tested 2 agents with the above setting, the first agent uses the MDPTDC learning, while the second agent uses the SMDP-TDC learning using 200 randomly generated option models, with 5 different time scales. The average return obtained is reported in Figure 7. While Figure 8 shows that the number\nof goal completions are more when the agent uses option models over multiple time scales as compared to the primitive models."}, {"heading": "6 Conclusion", "text": "Systems which need to act real time, do not have much time for deliberation. For such systems we propose the use of many option models composed over multiple time scale. We also propose an SMDP learning algorithm which is proved to be convergent with the linear option models while achieving a linear time efficiency and memory requirement. We demonstrate our results over the gridworld domain and a continuous room domain. We observe that systems performing while using an approximate representation of the environment states and learning using SMDP TDC algorithm over multiple time scales performs better in all aspects as compared to other methods. They show a recursive optimal behavior which is reasonably good as compared to the optimal solution. Additionally they achieve goals more efficiently as compared to other methods which deliberate at every time step. It will be interesting to see the extension of SMDP TDC algorithm using multiple time-step TD and eligibility traces."}], "references": [{"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L. Baird"], "venue": "In Proceedings of the Twelfth International Conference on Machine Learning, pages 30\u201337. Morgan Kaufmann.", "citeRegEx": "Baird,? 1995", "shortCiteRegEx": "Baird", "year": 1995}, {"title": "Stochastic approximation with two time scales", "author": ["V.S. Borkar"], "venue": "Systems amp; Control Letters.", "citeRegEx": "Borkar,? 1997", "shortCiteRegEx": "Borkar", "year": 1997}, {"title": "The o.d.e. method for convergence of stochastic approximation and reinforcement learning", "author": ["V.S. Borkar", "S.P. Meyn"], "venue": "SIAM J. CONTROL OPTIM", "citeRegEx": "Borkar and Meyn,? \\Q2000\\E", "shortCiteRegEx": "Borkar and Meyn", "year": 2000}, {"title": "Technical update: Least-squares temporal difference learning", "author": ["J.A. Boyan"], "venue": "Machine Learning. 13", "citeRegEx": "Boyan,? 2002", "shortCiteRegEx": "Boyan", "year": 2002}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["S.J. Bradtke", "A.G. Barto", "P. Kaelbling"], "venue": "Machine Learning, pages 22\u2013", "citeRegEx": "Bradtke et al\\.,? 1996", "shortCiteRegEx": "Bradtke et al\\.", "year": 1996}, {"title": "Convergent activation dynamics in continuous time networks", "author": ["M.W. Hirsch"], "venue": "Neural Netw.", "citeRegEx": "Hirsch,? 1989", "shortCiteRegEx": "Hirsch", "year": 1989}, {"title": "Bandit based monte-carlo planning", "author": ["L. Kocsis", "C. Szepesvri"], "venue": "In: ECML-06. Number 4212 in LNCS.", "citeRegEx": "Kocsis and Szepesvri,? 2006", "shortCiteRegEx": "Kocsis and Szepesvri", "year": 2006}, {"title": "Linear options", "author": ["J. Sorg", "S. Singh"], "venue": "AAMAS \u201910.", "citeRegEx": "Sorg and Singh,? 2010", "shortCiteRegEx": "Sorg and Singh", "year": 2010}, {"title": "Learning options in reinforcement learning", "author": ["M. Stolle", "D. Precup"], "venue": "Lecture Notes in Computer Science.", "citeRegEx": "Stolle and Precup,? 2002", "shortCiteRegEx": "Stolle and Precup", "year": 2002}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "MIT Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Fast gradient-descent methods for temporaldifference learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesvri", "E. Wiewiora"], "venue": "In Proceedings of the 26th International Conference on Machine Learning.", "citeRegEx": "Sutton et al\\.,? 2009", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "A convergent o(n) algorithm for off-policy temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "C. Szepesvri", "H.R. Maei"], "venue": "Advances in Neural Information Processing Systems 21. MIT Press.", "citeRegEx": "Sutton et al\\.,? 2001", "shortCiteRegEx": "Sutton et al\\.", "year": 2001}, {"title": "Reinforcement learning via aixi approximation", "author": ["J. Veness", "K.S. Ng", "M. Hutter", "D. Silver"], "venue": "AAAI\u201910.", "citeRegEx": "Veness et al\\.,? 2010", "shortCiteRegEx": "Veness et al\\.", "year": 2010}, {"title": "Identifying useful subgoals in reinforcement learning by local graph partitioning", "author": ["A.P. Wolfe", "A.G. Barto"], "venue": "ICML.", "citeRegEx": "Wolfe and Barto,? 2005", "shortCiteRegEx": "Wolfe and Barto", "year": 2005}], "referenceMentions": [{"referenceID": 8, "context": "Markov Decision Process (MDP) models provide a formal method of planning in controlled dynamical systems (Sutton and Barto (1998)).", "startOffset": 106, "endOffset": 130}, {"referenceID": 7, "context": "We use the linear option models (Sorg and Singh (2010)) for learning and planning.", "startOffset": 33, "endOffset": 55}, {"referenceID": 10, "context": "Recently various temporal difference algorithms were proposed which are compatible with both linear function approximation and off-policy training using primitive action in MDP setting (GTD algorithm Sutton et al. (2001), GTD2 and TDC algorithms Sutton et al.", "startOffset": 200, "endOffset": 221}, {"referenceID": 10, "context": "Recently various temporal difference algorithms were proposed which are compatible with both linear function approximation and off-policy training using primitive action in MDP setting (GTD algorithm Sutton et al. (2001), GTD2 and TDC algorithms Sutton et al. (2009)).", "startOffset": 200, "endOffset": 267}, {"referenceID": 0, "context": "As a result convergence cannot be guaranteed for such methods (Baird (1995)).", "startOffset": 63, "endOffset": 76}, {"referenceID": 0, "context": "As a result convergence cannot be guaranteed for such methods (Baird (1995)). Several non-gradient-descent approaches to this problem have been developed, but none has been completely satisfactory . Second-order methods, such as LSTD (Bradtke et al. (1996), Boyan (2002)), can be guaranteed stable under general conditions, but their computational complexity is quadratic in the size of the state space.", "startOffset": 63, "endOffset": 257}, {"referenceID": 0, "context": "As a result convergence cannot be guaranteed for such methods (Baird (1995)). Several non-gradient-descent approaches to this problem have been developed, but none has been completely satisfactory . Second-order methods, such as LSTD (Bradtke et al. (1996), Boyan (2002)), can be guaranteed stable under general conditions, but their computational complexity is quadratic in the size of the state space.", "startOffset": 63, "endOffset": 271}, {"referenceID": 7, "context": "Sorg and Singh (2010) proposed the idea of linear options as linear representations of options.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "Convergence proof of SMDP-TDC can be carried out in a similar way as in Sutton et al. (2009). Hence it can be shown that SMDP-TDC converges to the TD fixed point with probability one.", "startOffset": 72, "endOffset": 93}, {"referenceID": 1, "context": "Proof The proof of this theorem is based on a two time scale difference in the step-size schedule {\u03b1k} and {\u03b2k}; refer Borkar (1997) for a convergence analysis of the general two timescale stochastic approximation recursions.", "startOffset": 119, "endOffset": 133}, {"referenceID": 5, "context": "By the Hirsch lemma (Hirsch (1989)), it follow that \u2016\u03b8k \u2212 \u03b8\u2016 \u2192 0 almost surely as k \u2192\u221e for the same \u03b8 that depends on the initial condition \u03b80 of recursion 9.", "startOffset": 7, "endOffset": 35}, {"referenceID": 1, "context": "The assumptions are now verified and by Borkar and Meyn (2000) Theorem 2.", "startOffset": 40, "endOffset": 63}, {"referenceID": 8, "context": "Hence a lot of research has been done to construct just the right number of options by optimizing the computational time around models to accommodate multiple time scales (Stolle and Precup (2002), Wolfe and Barto (2005)).", "startOffset": 172, "endOffset": 197}, {"referenceID": 8, "context": "Hence a lot of research has been done to construct just the right number of options by optimizing the computational time around models to accommodate multiple time scales (Stolle and Precup (2002), Wolfe and Barto (2005)).", "startOffset": 172, "endOffset": 221}, {"referenceID": 6, "context": "Typically a large number of rollouts is performed, and values lower in the search tree are used to update the values higher up in the search tree (Kocsis and Szepesvri (2006), Veness et al.", "startOffset": 147, "endOffset": 175}, {"referenceID": 6, "context": "Typically a large number of rollouts is performed, and values lower in the search tree are used to update the values higher up in the search tree (Kocsis and Szepesvri (2006), Veness et al. (2010)).", "startOffset": 147, "endOffset": 197}, {"referenceID": 7, "context": "We ran our second set of experiments on a continuous navigation domain as described in Sorg and Singh (2010). The domain (Figure 6), which is 10 \u00d7 10 continuous room world consists of rooms which are separated by walls as shown.", "startOffset": 87, "endOffset": 109}], "year": 2017, "abstractText": "Deliberating on large or continuous state spaces have been long standing challenges in reinforcement learning. Temporal Abstraction have somewhat made this possible, but efficiently planing using temporal abstraction still remains an issue. Moreover using spatial abstractions to learn policies for various situations at once while using temporal abstraction models is an open problem. We propose here an efficient algorithm which is convergent under linear function approximation while planning using temporally abstract actions. We show how this algorithm can be used along with randomly generated option models over multiple time scales to plan agents which need to act real time. Using these randomly generated option models over multiple time scales are shown to reduce number of decision epochs required to solve the given task, hence effectively reducing the time needed for deliberation. ar X iv :1 70 3. 06 47 1v 1 [ cs .A I] 1 9 M ar 2 01 7", "creator": "LaTeX with hyperref package"}}}