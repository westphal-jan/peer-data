{"id": "1703.06514", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2017", "title": "Recurrent Collective Classification", "abstract": "We propose a new method for training iterative collective classifiers for labeling nodes in network data. The iterative classification algorithm (ICA) is a canonical method for incorporating relational information into classification. Yet, existing methods for training ICA models rely on the assumption that relational features reflect the true labels of the nodes and that there is no direct link between the data set, but also the classification of nodes in the network. The approach, developed by a team of neuropsychologists, combines two models of classification, and is currently used to train a generalized classification algorithm. In order to assess whether neural models of classification could be used to classify systems of classification, we developed an empirical model which has previously defined the use of a new model for classification. The current method of training iterative classifiers uses a method called the \"recurrent\" model that combines recurrent neural models with recurrent neural models. The recurrent model does not require recurrent neural models of classification. The recurrent model does not require recurrent neural models of classification. We are now considering a similar approach to the other approaches of learning classification. However, we do not know how to train an iterative classifier, even if the data are correct. We do not have the same data set as an existing recursive classification algorithm (although we know how to train an iterative classifier).\n\n\n\nA hierarchical representation of all data sets in the network are stored at random. In the first case, there are no partitions and there are no single partitions. This condition is present in both the general population and the trained group. For example, in the first case, there are no partitions. This condition is present in all systems, but there are no partitions. This condition is present in all systems, but there are no partitions. This condition is present in all systems, but there are no partitions. This condition is present in all systems, but there are no partitions. This condition is present in all systems, but there are no partitions. This condition is present in all systems, but there are no partitions. In fact, for all systems, there is a limit in the capacity of the underlying model and a limited range of available model configurations (e.g., the \"cisque\") that allow the model to be able to provide consistent representations of network information. For example, the size of the partition can be reduced to 0.4 g, or 2 g. The size of the partition can be reduced to 0.2 g.\nIn summary, the \"cisque\" model is", "histories": [["v1", "Sun, 19 Mar 2017 21:19:04 GMT  (389kb,D)", "http://arxiv.org/abs/1703.06514v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shuangfei fan", "bert huang"], "accepted": false, "id": "1703.06514"}, "pdf": {"name": "1703.06514.pdf", "metadata": {"source": "CRF", "title": "Recurrent Collective Classification", "authors": ["Shuangfei Fan", "Bert Huang"], "emails": [], "sections": [{"heading": null, "text": "We propose a new method for training iterative collective classifiers for labeling nodes in network data. The iterative classification algorithm (ICA) is a canonical method for incorporating relational information into classification. Yet, existing methods for training ICA models rely on the assumption that relational features reflect the true labels of the nodes. This unrealistic assumption introduces a bias that is inconsistent with the actual prediction algorithm. In this paper, we introduce recurrent collective classification (RCC), a variant of ICA analogous to recurrent neural network prediction. RCC accommodates any differentiable local classifier and relational feature functions. We provide gradient-based strategies for optimizing over model parameters to more directly minimize the loss function. In our experiments, this direct loss minimization translates to improved accuracy and robustness on real network data. We demonstrate the robustness of RCC in settings where local classification is very noisy, settings that are particularly challenging for ICA."}, {"heading": "1 Introduction", "text": "Data science tasks often require reasoning about networks of connected entities, such as social and information networks. In classification tasks, the connections among network nodes can have important effects on node-labeling patterns, so models that perform classification in networks should consider network structure to fully represent the underlying phenomena. For example, when classifying individuals by their personality traits in a social network, a common pattern is that individuals will communicate with like-mined individuals, suggesting that predicted labels should also tend\nto be uniform among connected nodes. Collective classification methods aim to make predictions based on this insight. In this paper, we introduce a collective classification framework that will enable an algorithm to more directly optimize the performance of trained collective classifiers.\nIterative classification is a framework that enables a variety of supervised learning methods to incorporate information from networks. The base machine learning method can be any standard classifier that labels examples based on input features. The iterative classification algorithm (ICA) operates by using previous predictions about neighboring nodes as inputs to the current predictor. This pipeline creates a feedback loop that allows models to pass information through the network and capture the effect of structure on classification. In spite of the feedback loop being the most important aspect of ICA, existing approaches train models in a manner that ignores the feedback-loop structure. In this paper, we introduce recurrent collective classification (RCC), which corrects this discrepancy between the learning and prediction algorithms, incorporating principles used in deep learning and recurrent neural networks into the training process.\nExisting learning algorithms for iterative classification resort to an approximation based on the unrealistic assumption that the predicted labels of neighbors are their true classes (Neville and Jensen, 2000; London and Getoor, 2013). This assumption is overly optimistic. If it were true, iteration would be unnecessary. Because the assumption is overly optimistic, it causes the learned models to cascade and amplify errors when the assumption is broken in early stages of prediction. In contrast, ICA uses predicted neighbor labels as feedback for each subsequent prediction, which means that if the model was trained expecting these predicted labels to be perfect, it will not be robust to situations where predictions are noisy or inaccurate. In this paper, we correct this faulty assumption and develop an approach that trains models for iterative classification by treating the intermediate predictions as latent variables. We compute gradients to the classification loss function using back-propagation through iterative classification. ar X\niv :1\n70 3.\n06 51\n4v 1\n[ cs\n.L G\n] 1\n9 M\nar 2\n01 7\nTo compute gradients for ICA, we break down the ICA process into differentiable (or sub-differentiable) operations. In many cases, the base classifier is differentiable with respect to its parameters. For example, if it is a logistic regression, it has a well-studied gradient. ICA also computes dynamic relational features using the predictions of network neighbors. These relational features are also functions through which gradients can be propagated. Finally, because the same base-classifier parameters should be used at all iterations of ICA, we can use methods for recurrent neural networks such as back-propagation through time (BPTT) (Werbos, 1990) to compute the combined gradient. In contrast with existing strategies for training ICA, the resulting training optimization more closely mimics the actual procedure that ICA uses for prediction.\nThe RCC framework accommodates a variety of base classifiers and relational feature functions. The only restriction is that they must be differentiable. Therefore, RCC is nearly as general as ICA, and its prediction procedure is practically identical to ICA. The key difference is that the view of the algorithm as nested, differentiable functions enables a training procedure that is better aligned with RCC and ICA prediction.\nWe evaluate RCC on data where collective classification has previously been shown to be helpful. We demonstrate that RCC trains classifiers that are robust to situations where local predictions are inaccurate."}, {"heading": "2 Related Work", "text": "Node classification is one of the fundamental tasks in analysis of network data (Getoor and Diehl, 2005; Lindamood et al., 2009). Collective classification addresses this task by making joint classifications of connected nodes (Kong et al., 2011; Taskar et al., 2002; Neville and Jensen, 2003). Gibbs sampling (GS) is another approach for collective classification using the iterative classification framework (McDowell et al., 2007; Sen et al., 2008), that introduces randomization into the iterative classification. ICA and GS have been shown repeatedly to be effective frameworks for collective classification (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003; McDowell et al., 2007). One of the more natural motivations for collective classification comes from the study of social networks, where the phenomenon of homophily\u2014the tendency of individuals to interact with other similar individuals\u2014has been an important concept (McPherson et al., 2001; Bilgic et al., 2007). The types of dependencies that can exist in networks are not limited to assortative relationships, and methods such as ICA enable models to encode both assortative and non-assortative phenomena.\nCollective classification has been studied in both inductive and transductive settings. In inductive settings, a collective classifier is typically trained on a fully labeled training network and evaluated at test time on a completely new network with no known labels. In transductive settings, the classifier is both trained and tested on partially labeled networks, or possibly in the same network with a different set of labels known during each phase (Xiang and Neville, 2011). In this paper, we focus on the inductive setting.\nThrough the interpretation of non-terminal classifications as latent variables, ICA can be related to deep learning methods. Since the same classifier is used to predict each latent layer, ICA is most related to recurrent neural networks (RNNs), which feature a similar feedback loop in which an output of a neural network is used as its input in a subsequent iteration. RNNs were introduced decades ago (Angeline et al., 1994; Connor et al., 1994), but they have recently become prominent because of their effectiveness at modeling sequences, such as those occurring in natural language processing, e.g., (Socher et al., 2011; Mikolov et al., 2010; Graves and Jaitly, 2014; Graves, 2012). A now standard method for gradient optimization of RNN parameters is known as back-propagation through time (Werbos, 1990; Hochreiter et al., 2001; Ishikawa, 1996), which unrolls recurrent networks and computes gradients for the parameters separately before combining them into a single update.\nAfter the first version of our manuscript was published (Fan and Huang, 2016), two other groups have independently and concurrently pursued very similar directions, applying deep learning to collective classification (Pham et al., 2016; Moore and Neville, 2017). Moore and Neville (2017) proposed to use RNNs for nodebased relational classification tasks. They transform each node and its set of neighbors into an unordered sequence and use an LSTM-based RNN to predict the class label as the output of that sequence. Pham et al. (2016) proposed another deep learning model for collective classification. Their approach is fully end-to-end, with a neural network that computes hidden units connected in the same structure as the input network, but the hidden units do not output class probabilities. Instead, they are abstract representations of learned local and relational features. These concurrent studies represent different ideas for bringing the power of neural networks to collective classification."}, {"heading": "3 Iterative Classification", "text": "In this section, we review the iterative classification algorithm (ICA) and the standard method for training its parameters. ICA provides a framework for node clas-\nAlgorithm 1 The Iterative Classification Algorithm\n1: Input: Adj. matrix A, node features X, num. of iterations T , classifier f , and relational feature function g. 2: Initialize labels Y {e.g., uniform probability} 3: for t from 1 to T do 4: R\u2190 g(Y ; A) {Compute relational features}. 5: Y \u2190 f(X,R; \u0398) {Compute new predictions}. 6: end for 7: return labels Y\nsification in networks. ICA is given a decorated graph G = {V,E,X}, where V = {v1, . . . , vn}, E contains pairs of linked nodes (vi, vj) \u2208 E, and each node is associated with a respective feature vector xi \u2208 Rd \u2261 X , where X = {x1, . . . ,xn}. Using these inputs, ICA outputs a set of predictions Y classifying each of the nodes in V into a discrete label space Y. Throughout this paper, we will consider the multi-class setting, in which the labels can take one of k class-label values. ICA makes the label predictions by iteratively classifying nodes by their local features xi and their dynamic relational features ri, which is in a common space ri \u2208 R. In other words, the classifier is a function f that maps X \u00d7R to Y , parameterized by a parameter variable \u0398.\nICA first initializes labels as Y (0), and it then iterates the steps\nR(t\u22121) \u2190 g(Y (t\u22121); A) Y (t) \u2190 f(X,R(t\u22121); \u0398)\n(1)\nfrom iterations t = 1 to t = T .\nThe dynamic relational features R where R = {r1, . . . , rn} are computed based on the current estimated labels of each node. They enable the classifier to reason about patterns of labels among connected nodes. E.g., a common relational feature is the average prediction of neighboring nodes.\nUsing any such aggregation statistic creates a relational feature vector of dimensionality k, where each entry is the occurrence rate of its corresponding label in the node\u2019s neighbors. I.e.,, the relational features are computed by a feature function g that maps Yn to Rn.\nSince the dynamic relational features are computed based on the output of the classifier, the entire process is iterated: (1) all nodes are labeled by the classifier f using the current dynamic relational features, then (2) the dynamic relational features are computed with g based on the new labels. These two phases are repeated either until the predictions converge and do not change between iterations or until a cutoff point.\nThe ICA framework is general in that any classifier\nf can be used and any form of a dynamic relational feature function g can be used. In practice, researchers have used naive Bayes, logistic regression, support vector machines as classifiers, and they have used averages, sums, and presence as relational features (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003). The generality of the framework is a key benefit of ICA\u2014one that we aim to preserve in our proposed framework. In contrast to specially designed methods for collective classification, the modularity of ICA makes it a flexible meta-algorithm.\nIn many settings, we consider real-valued local and relational features, so each node is described by a feature vector created by concatenating its local features with its relational features [xi, ri]. In this case, it is convenient to notate the classification function in matrix form. Let X denote the feature matrix for the graph, such that the ith row of X, i.e., xi is the (transposed) feature vector for node vi. Similarly, let R denote the relational feature matrix, such that the ith row of R is the dynamic relational feature vector of node vi. For convenience, we consider the case where one type of dynamic relational feature is used, meaning the dimensionality of R is n by k (though it is easy to extend both ICA and RCC to multiple relational features).\nThe training procedure for ICA trains the classifiers by computing relational features using the training labels. Given a training set consisting of a set of nodes V , edges E, node features X, and ground-truth labels Y\u0302 , one generates relational features R using the true training labels, creating fully instantiated, fully labeled inputs for the classifier. The model parameters \u0398 are learned by setting R\u0302 = g(Y\u0302 ), then using X and R\u0302 as the input to a supervised training scheme appropriate for fitting the parameters \u0398 of f .\nThis training methodology is only correct in the situation where we expect a perfect-classification fixed-point. In such a fixed point, the relational features are computed using the true labels, and the classifiers perform perfectly, exactly predicting the true labels of the nodes; the relational features are computed, using the perfectly predicted labels, to be exactly the same features that are computed using the true training labels; since the relational features are computed using the exactly predicted true labels, the output is the same as in the first step, and the algorithm converges perfectly to the true labels. Using the matrix form of the relational feature computation above, this absurd fixed point would be\ncharacterized as Y = f ( X, g(Y\u0302 ); \u0398 ) .\nUnfortunately, such a fixed point is unrealistic. In practice, the classifications can be inaccurate or made with low confidence due to a lack of reliable local information. Thus, training the model to expect that the\nAlgorithm 2 RCC Gradient Computation\n1: Input: Graph G = {V,E}, number of iterations T , classifier f , and relational feature function g. 2: Initialize parameter \u0398 set P(0) = 0 3: for t from 1 to T do 4: R(t) \u2190 g(P(t\u22121); A) {Relational features}. 5: P(t) \u2190 f(X,R(t); \u0398) {New predictions} 6: end for 7: \u2206(T) \u2190 L\u2032(P(T)) {Loss gradient for output} 8: for t from T to 2 do 9: for j from 1 to N do\n10: \u03b4 (t\u22121) j \u2190 \u2211 i:(i,j)\u2208E \u03b4 (t) i \u00b7 f \u2032 ( r (t\u22121) i ) \u00b7 g\u2032i ( p (t) j ) 11: end for 12: end for 13: \u2207(\u0398)\u2190 \u2211T t=1 \u2206 (t)f \u2032(\u0398)\nneighbor labels be perfect creates an overconfidence that can lead to cascading errors."}, {"heading": "4 Recurrent Collective Classification", "text": "We propose the recurrent collective classification (RCC) framework to address the previously mentioned deficiencies in ICA training. The RCC framework computes derivatives for the stages of collective classification and provides a general scheme for how to compute the gradient of the loss function with respect to the classifier parameters. Algorithm 2 summarizes RCC gradient computation. This gradient computation enables the training of collective classifiers in a manner that more directly mimics how they will be applied. At test time, a collective classifier is often given only local features of nodes connected in a network. Thus, any relational features are derived from predicted neighbor labels. A classifier considering these neighbor labels should therefore consider common patterns of misclassification of neighbor labels. In contrast to the ICA training procedure, which invites the classifier to become overly reliant on the verity of the neighbor labels.\nPrediction in RCC is analogous to ICA. We initialize P(0) (e.g., setting it to all zeros, or to random values), then iterate the steps R(t\u22121) \u2190 g(P(t\u22121); A), and P(t) \u2190 f(X,R(t\u22121); \u0398) from iterations t = 1 to t = T . This recursive procedure can be interpreted as a recurrent neural network, as illustrated in Figure 1.\nRCC training requires that the local classifier function f is equipped with efficiently computable gradients with respect to the parameters \u0398 and the relational features R, and that the relational feature function g is equipped with an efficiently computable gradient with respect to the current predictions P. Since these functions map matrices to matrices, their gradients\nare computed via matrix calculus. However, as we discuss in Section 4.1, they often have sparse structure that makes their computation efficient. As a shorthand, with some abuse of notation, we refer to these gradients using the following definitions:\nf \u2032(t)(\u0398) := \u2202f(X,R (t\u22121); \u0398)/\u2202\u0398\nf \u2032(R(t\u22121)) := \u2202f(X,R(t\u22121); \u0398)/\u2202R(t\u22121)\ng\u2032(P(t)) := \u2202g(P(t); A)/\u2202P(t).\n(2)\nAt iteration T , we evaluate a loss function L on the final prediction P(T ). RCC admits any differentiable loss function. For example, for multinomial distribution, we can use Softmax classifier, which uses the cross-entropy loss. The cross-entropy loss is to minimize the crossentropy between the estimated distribution q and the \u201dtrue\u201d distribution p as the loss function. It is defined as:\nL = H(p, q) = \u2212 \u2211 x p(x) log q(x) (3)\nDenote the gradient of the loss with respect to the predictions P(t), i.e., \u2206(t) := \u2202L\n\u2202P(t) . Given the gradients\nfor each iteration, the gradient of the loss with respect to the parameters \u0398 is \u2202L\u2202\u0398 = \u2211T t=1 \u2206 (t)f \u2032(\u0398).\nBased on these formulas, RCC is able to use any gradient-based optimization over the parameter space, so long as it is able to compute the loss gradients \u2206. These gradients can be computed using chain rule and dynamic programming, as in back-propagation. The general formula is:\n\u2206(t\u22121) = \u2202L\n\u2202P(t\u22121) =\n( \u2202L\n\u2202P(t)\n)( \u2202P(t)\n\u2202P(t\u22121) ) = \u2206(t) ( \u2202P(t)\n\u2202R(t\u22121)\n)( \u2202R(t\u22121)\n\u2202P(t\u22121)\n) .\n(4)\nThough these matrix gradients can be large for arbitrary matrix functions, collective classification has a particular structure that enables efficient computation."}, {"heading": "4.1 Derivative Structure", "text": "Key aspects in understanding the efficient computability of these gradients are the independence relationships among the input and output variables. These independence relationships are universal to any local classifier and to any relational feature.\nBecause the local classifier operates on each node independently, in each iteration, there is no dependence between the classification of any node and the relational features of any other node. The matrix derivative of the classifier function f is block diagonal, only having nonzero derivative between each node\u2019s relationalfeature row ri and its output-prediction row pi.\nBecause relational features are completely determined by the neighbors of each particular node, there is only dependence between the predictions of any node j and the relational feature of node i if they are neighbors in the graph. The matrix derivative of the relational feature function g therefore has a block structure in the shape of the sparse adjacency matrix.\nConsidering both of these sparse block structures, we can define a new shorthand for the nonzero elements of the matrix derivatives:\nf \u2032 ( r (t\u22121) i ) := \u2202f(x, r (t\u22121) i ; \u0398)/\u2202r (t\u22121) i \u2261 \u2202p (t) i /\u2202r (t\u22121) i\ng\u2032i ( p (t) j ) := \u2202 [ g(p (t) j ; A) ] i /\u2202p (t) j \u2261 \u2202r (t) i /\u2202p (t) j .\n(5)\nFigure 2 illustrates the sparse structure of the full matrix gradients using these definitions. The matrix chain rule then simplifies to\n\u03b4 (t\u22121) j = \u2211 i:(i,j)\u2208E ( \u2202L \u2202pti )( \u2202pti \u2202r (t\u22121) i )( \u2202r (t\u22121) i \u2202p (t\u22121) j )\n\u03b4 (t\u22121) j = \u2211 i:(i,j)\u2208E \u03b4 (t) i \u00b7 f \u2032 ( r (t\u22121) i ) \u00b7 g\u2032i ( p (t) j ) .\n(6)\nwhere the loss derivatives \u03b4 (t) i are gradient vectors and the classifier and feature function derivatives, f \u2032 and g\u2032 are small Jacobian matrices. This structure is significantly sparser and more efficient to compute than the full matrix calculus in Equation (4)."}, {"heading": "4.2 Example Local Classifiers", "text": "One benefit of ICA is that it can use any local classifier and relational feature. Similarly, given Section 4.1, we can also easily use many local classifiers f and relational features g into RCC. In this section, we give some examples of these configurations.\nFirst, we consider linear classifiers with activation functions where the linear product prediction scores are squashed by different activation functions. A common activation function is the logistic sigmoid function:\nf(xi, ri (t\u22121); \u0398) = 1 1+exp ( \u2212 [ xi, r (t\u22121) i ] \u00b7\u0398 ) = pi. (7)\nwhere we write [xi, ri] to indicate the horizontal concatenation of the row vectors xi and ri. Let \u0398 =[\n\u0398x \u0398r\n] , separating the parameters for the relational\nfeatures to submatrix \u0398r. The derivative with respect to r\n(t\u22121) i is the Jacobian matrix f \u2032(ri (t\u22121)) =\ndiag(p (t\u22121) i (1 \u2212 p (t) i ))\u0398 > r . The gradient with respect to the parameters is\nf \u2032(\u0398) = \u2212diag(p(t\u22121)i (1\u2212 p (t) i )) \u00b7\n[ X,R(t\u22121) ] . (8)\nAnother activation function is the tempered softmax function, i.e., a generalization of both the normalized multi-class logistic and the max function:\nf(xi, ri (t\u22121); \u0398) =\nexp (([\nxi, r (t\u22121) i\n] \u00b7\u0398 ) /\u03c4 )\n1> exp (([\nxi, r (t\u22121) i\n] \u00b7\u0398 ) /\u03c4 ) = pi.\n(9)\nwhere \u03c4 is the temperature parameter. As \u03c4 approaches 0, the limit of the softmax is an argmax indicator vector indicating the maximal entry, and at \u03c4 = 1, this is exactly the multi-class logistic class probability.\nThe Jacobian of the softmax with respect to the relational features is f \u2032(ri (t\u22121)) = 1\u03c4 (diag(pi)\u2212pipi >)\u0398>r . And the gradient with respect to the parameters is\nf \u2032(\u0398) = \u22121 \u03c4\n(diag(pi)\u2212 pipi>) \u00b7 [ X,R(t\u22121) ] . (10)\nThe local classifier used in RCC can also seamlessly incorporate non-linear classifiers such as multilayer perceptrons and neural networks that have wellunderstood derivatives."}, {"heading": "4.3 Example Relational Features", "text": "There are many aggregation operators can be used to define the relational features. Commonly used features include the sum of probabilities for each class, the proportion of each class in the neighborhood, mode,\nwhich is the class label with the highest probability among neighbors and exists, which is an indicator for each class label (Sen et al., 2008). The choice of which one to use depends on the application. Here we discuss three of them and provide their derivatives.\nThe sum aggregation function g can be written as ri = g(P (t); ai) = a > i P\n(t), where ai is the adjacency vector of node i. The Jacobian of the sum feature is g\u2032i(p (t) j ) = aijIk, where Ik is the identity matrix.\nThe proportion operator is similar to sum, except that we scale the adjacency matrix A by normalizing each row vector. The operation can be summarized as A\u0302 = A (A \u00b7 1N ), where 1N is the all-ones matrix of size N and N is the total number of nodes. The operator performs element-wise division. The proportion operator uses A\u0302 for the relational feature function ri = g(P (t); a\u0302i) = a\u0302 > i P\n(t), where a\u0302i is the normalized adjacency vector of node i. The Jacobian for the proportion feature function is g\u2032i(p (t) j ) = a\u0302ijIk.\nFor the mode aggregation operator, we can use the tempered softmax function, providing a differentiable\nform ri = g(P (t); ai) =\nexp(a>i P (t)/\u03c4)\n1> exp(a>i P (t)/\u03c4)\nwhere small \u03c4\nvalues closely mimic the true mode. The Jacobian of the mode aggregation relational feature is g\u2032i(p (t) j ) = (1/\u03c4)aij \u00b7 (diag(ri)\u2212 rir>i )."}, {"heading": "4.4 Computational Complexity", "text": "Prediction in RCC (and ICA) requires computing relational features and predicting T times. For most relational features, each node must perform O(k) work per neighbor, amounting to O(|E|k) total computation. Assuming a constant number of relational features, the linear local classifier performs O(d + k) work. Thus each prediction iteration requires O(|E|k+d) time, and the full prediction requires O(T (|E|k + d)) time.\nGradient-based optimization of the RCC objective function is a non-convex program, so the total number of iterations for learning is nontrivial to analyze in general. However, we can analyze the computational complexity of each gradient computation. The computation of the gradient requires three phases: the computation of the classifier and relational feature derivatives, the\ncomputation of the loss derivatives (back-propagation), and the computation of the parameter derivatives.\nThe cost of computing the classifier and relational feature derivatives depends on the chosen classifier and relational features. In the examples provided earlier, the cost is O(k2) for both.\nThe back-propagation formula in Section 4.1 computes a single row of the \u2206(t) derivative, which has N total rows. The inner summation iterates over the neighbors of the current node, performing a vector-matrix product and a matrix-matrix product. The derivatives are Jacobians of size O(k) by O(k), so the total cost is O(k2). Thus, the combined cost is O(T |E|k2) for backpropagating the derivative through T iterations.\nFinally, computation of the gradient for \u0398 also depends on the local classifier. For the linear classifiers described earlier, the cost is O(dk) per iteration, so O(Tdk). Therefore, the entire gradient computation costs O(T (dk+|E|k2)), which is comparable to the cost of prediction itself, since k is often a small quantity."}, {"heading": "5 Experiments", "text": "In this section, we describe experiments that test whether the proposed training method for RCC is able to improve upon existing methods of training iterative classifiers. We explore scenarios where the local classifier produces inaccurate predictions, challenging the faulty assumption implied by training ICA and Gibbs sampling (GS) with relational features computed using the true labels. The results illustrate that our hypothesis is correct, identifying a variety of settings where RCC better optimizes the training objective and produces more accurate predictions on held-out data.\nData We experimented with four data sets, two bibliographic data sets, one social network data set and one image dataset. The Cora data set is a collection of 2,708 machine learning publications categorized into seven classes (Sen et al., 2008). Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 3,703 unique words. The CiteSeer data set is a collec-\ntion of 3,312 research publications crawled from the CiteSeer repository (Sen et al., 2008). It consists of 3,312 scientific publications categorized into six classes. Each publication in the data set is described by a 0/1- valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1,433 unique words. The social network data we use is the Facebook ego network data (McAuley and Leskovec, 2012), which includes users\u2019 personal information, friend lists, and ego networks. We combine the ego networks and node features to form a single connected network, with 4,039 users\u2019 anonymized profile data and links between them. We used the feature \u201ceducation type,\u201d as a label, aiming to predict how many degrees each user has, i.e., the four classes are whether their highest level of education is secondary, undergraduate, or graduate, or unknown. The image dataset we use is the Weizmann horse-image segmentation set (Borenstein and Ullman, 2004). We subsample 20 images of horses on various backgrounds. We use features described by Domke (2013) for each pixel: We expand the RGB values of each pixel and the normalized vertical and horizontal positions into 64 features using sinusoidal expansion. The goal is to identify which pixels are part of a horse and which are background. We pose this task as a collective classification by constructing a grid graph over neighboring pixels. Specifically, we expand the feature vectors by multiplying our basic feature vector with all binary vectors c of the appropriate length and take the sin(c\u00d7 s) and cos(c\u00d7 s) as our final feature vectors, which have 64 features in total.\nSetup For each experiment, we evaluate on four different approaches for node classification: (1) local prediction using only the local features; (2) ICA trained using the true labels; (3) GS trained using the true labels; and (4) RCC trained using back-propagation.They are all trained using a multi-class logistic regression loss function. ICA and GS are trained with the concatenated relational features computed from the true labels in addition to local features as input. RCC is trained using the training labels only in computing the\nloss, but never as input to the classifier in any form.\nFor each of the learning objectives, we optimize using the adagrad approach (Duchi et al., 2011), in which gradients are rescaled based on the magnitude of previously seen gradients. For the gradient g\u03c4 at optimization iteration \u03c4 , one updates the variable \u0398 with \u0398\u03c4 \u2190 \u0398\u03c4\u22121 \u2212 \u03b7 g\u03c4\u221a\u2211\u03c4\ni=1 gi gi , where the gradient divi-\nsion by the historical magnitude is elementwise. Adagrad is one of many approaches that has been shown in practice to accelerate convergence of gradient-based optimization. Training is done with 2,000 iterations of adagrad and an initial learning rate of \u03b7 = 0.1. We evaluate performance of each method using a range of regularization parameter settings from 1\u00d7 10\u22123 to 1.\nFor the document and social network data, we perform snowball sampling to extract a random 1/5 of the nodes to hold out as a isolated test network. We train on the induced graph of the 4/5 remaining nodes, and measure predictive accuracy on both the training graph and testing graph. For the image data, we train on two random splits of 10 training and 10 testing images. The training accuracy should more closely reflect whether each method\u2019s training strategy effectively optimizes the model to fit the observed data, and the testing accuracy should additionally reflect how well the learned model generalizes. We compute both training and testing accuracy by feeding the learned model only the local features and link structure, meaning that though ICA and GS is typically trained with the training labels as input, we do not provide the labels to them when evaluating its training accuracy. Our hypothesis is that by directly computing the gradient of the actual prediction procedure for collective classification, RCC will produce better training performance, which should translate to better testing performance.\nEmpirical Evaluation of Loss To illustrate that RCC training optimizes the training loss function better than training with the true labels, we can compare the training loss associated with the soft-max output loss. We first train model weights using (1) RCC and (2) the ICA approach using the true labels. We then define a tradeoff function l(\u03b1) = L(\u0398RCC + \u03b1(\u0398ICA \u2212\u0398RCC)), where L is the loss function, \u0398RCC is the parameter matrix from the model trained by RCC, \u0398ICA is the weight matrix trained by the ICA strategy. We apply the loss function to different weight matrices on the line that connects \u0398RCC and \u0398ICA. This curve is a crosssection of the high-dimensional function that is the actual training loss. When \u03b1 = 0, the value is the loss of the RCC weights L(\u0398RCC), and (2) when \u03b1 = 1, the value is that of the ICA weights L(\u0398ICA). The results for Cora are shown in Figure 3. For all four data sets, the loss obtained by RCC training L(\u0398RCC) is near a\nCora\n0 0.2 0.4 0.6 0.8 1\n0.4\n0.6\n0.8 1 T ra in in g A cc u ra ci es\nFraction of Deleted Features\nRCC ICA GS Local\n0 0.2 0.4 0.6 0.8 1 0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nT e s t\nA c c u ra\nc ie\ns\nFraction of Deleted Features\nRCC ICA GS Local\nCiteseer\n0 0.2 0.4 0.6 0.8 1 0.4\n0.6\n0.8\n1\nT ra\nin in\ng A\ncc u\nra ci\nes\nFraction of Deleted Features\nRCC ICA GS Local\n0 0.2 0.4 0.6 0.8 1 0.4\n0.5\n0.6\n0.7\n0.8\nT e s t\nA c c u ra\nc ie\ns\nFraction of Deleted Features\nRCC ICA GS Local\nFacebook\n0 0.2 0.4 0.6 0.8 1 0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nT ra\nin in\ng A\ncc u ra\nci es\nFraction of Deleted Features\nRCC ICA GS Local\n0 0.2 0.4 0.6 0.8 1 0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nT e s t\nA c c u ra\nc ie\ns\nFraction of Deleted Features\nRCC ICA GS Local\nWeizmann\n0 0.2 0.4 0.6 0.8 1 0\n0.2\n0.4\n0.6\n0.8\nT ra\nin in\ng F\nm ea\nsu re\nnoise parameter\n0 0.2 0.4 0.6 0.8 1 0\n0.2\n0.4\n0.6\n0.8\nT es\nt F\nm ea\nsu re\nnoise parameter\nFigure 4: Performance of collective classifiers on the four tasks. Each curve plots the average training or testing accuracy or F-measure over the amount of noise (feature removal or salt-and-pepper). RCC dominates all methods on training accuracy, and it performs significantly better in testing than others when there is weak local signal.\nlocal minimum and the loss obtained by ICA training is not. Unsurprisingly, these results suggest that the training procedure of RCC is a better strategy to optimize the loss function than using the true-label relational features, corroborating our proposed approach.\nComparing Relational Features and Classifiers As we discussed before, RCC can be easily plugged in many local classifiers and it can also use many aggregation operators to include different relational features. To prove this, we run experiments with different classifier and aggregation operator for each of the four dataset: Cora, Citeseer, Facebook and Weizmann\u2019s dataset to compare the performance of different settings with RCC. For the local classifier, we choose from logistic sigmoid function and tempered softmax where we set \u03c4 to 0.5. For aggregation operators, we choose from mode, sum and proportion. For these experiments, we compare the average accuracy over 20 splits. The results in Table 1 show that for all these four datasets, there are not much differences between these six experiments. So for the following experiments, we use logistic sigmoid function and proportion.\nComparison of Prediction Accuracies The problem RCC aims to solve is the discrepancy between\ntraining ICA with relational features based on the true labels and the fact that at prediction time, ICA uses estimated labels. To illustrate this discrepancy, we run experiments that consider situations where local classification becomes more and more difficult. For the citation and network data, we generate versions of the data set where different fractions of the features are removed, in the range [0.0, 0.9]. For image data, we add varying amounts of salt-and-pepper noise. In effect, the experiments are run on versions of the data sets where prediction is harder, and more relevantly, where the assumption that the predicted labels are exactly the true labels becomes more and more incorrect. If RCC\u2019s training procedure is truly more robust to this scenario, we expect its improvement over ICA to become more pronounced as (local) prediction becomes more difficult. The results for these experiments are shown in Figure 4 where we plot the average accuracies or F-measures for the best-scoring regularization parameters over 20 splits of network data and 2 splits of image data. We compared RCC, ICA, GS, and the local classifier, which makes predictions only based on local features. The horizontal axis represents the amount of noise, and the vertical axis represents the training or testing accuracies achieved by these algorithms.\nThe results for all data sets suggest that RCC is more robust to weak local signal than ICA, GS, and local classifier. For the Cora and CiteSeer data, as the fraction of deleted features increases, the training accuracies of RCC stays stable until over 80% of local features have been deleted. The training accuracies of ICA, GS, and the local classifier drop earlier and much faster. When 90% of the features are deleted, the training accuracies of ICA, GS, and the local classifier drop to 0.5, however the accuracies of RCC still remains around 0.9, showing that RCC is able to train models to fully utilize the relational structure. The RCC test\naccuracies also show better performance than the other three methods as the number of local features reduces. Especially when over 60% local features are deleted, the local predictors become less reliable which causes ICA\u2019s accuracy to significantly worsen, and RCC is able to withstand the lack of attribute information. The Facebook results follow similar trends, where the training and test accuracies are always better than the ICA and local classifier. The differences in the testing accuracy between RCC and ICA are statistically significant for all fractions of deleted features on the Cora tests, for 0.7 and 0.9 for the CiteSeer tests, and for all fractions 0.2 and higher on the Facebook tests.\nOne interesting effect not often reported in other research is the tendency for ICA trained using the true labels to produce predictors that perform worse than the local classifier, even on training data. This effect is exactly because of the discrepancy between the training regime and the actual prediction algorithm RCC aims to correct. For example, in all three of our data sets, there are settings, especially when the local classifier is noisy, that ICA has worse training accuracy than the local classifier. This effect is especially apparent in the image data, where the faulty assumption made by ICA causes it to consistently cascade local-classification errors, eventually leading ICA, GS, and the local predictor to predict that all pixels are background. RCC avoids this over-reliance on relational features, yet learns to incorporate relational features enough to improve upon the noisy local predictor. Figure 5 contains example images and the predicted segmentations using the various learning algorithms."}, {"heading": "6 Conclusion and Discussion", "text": "We presented recurrent collective classification, a variant of the iterative classification algorithm that uses\ndifferentiable operations, enabling back-propagation of error gradients to directly optimize the model parameters. The concept of collective classification has long been understood to be a principled approach to classifying nodes in networks, but in practice, it often suffers from making only small improvements, or not improving at all. One cause for this could be the faulty training procedure that we correct in this paper. Our experiments demonstrate dramatic improvements in training accuracy, which translate to significant, but less dramatic improvements in testing performance. RCC is a key step toward fully realizing the power of collective classification. Thus, an important aspect to consider to further improve the effectiveness of collective classifiers is the generalization behavior of collective models. One future direction of research is exploring how a more direct training loss-minimization interacts with known generalization analyses, perhaps leading to further algorithm improvements. Another future direction we are exploring is how to apply similar approaches of direct loss minimization in transductive settings and how to expand the flexibility of the RCC framework to incorporate other variants of ICA."}], "references": [{"title": "An evolutionary algorithm that constructs recurrent neural networks", "author": ["Peter J Angeline", "Gregory M Saunders", "Jordan B Pollack"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Angeline et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Angeline et al\\.", "year": 1994}, {"title": "Combining collective classification and link prediction", "author": ["Mustafa Bilgic", "Galileo Mark Namata", "Lise Getoor"], "venue": "In Data Mining Workshops,", "citeRegEx": "Bilgic et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bilgic et al\\.", "year": 2007}, {"title": "Learning to segment", "author": ["Eran Borenstein", "Shimon Ullman"], "venue": "In European conference on computer vision,", "citeRegEx": "Borenstein and Ullman.,? \\Q2004\\E", "shortCiteRegEx": "Borenstein and Ullman.", "year": 2004}, {"title": "Recurrent neural networks and robust time series prediction", "author": ["Jerome T Connor", "R Douglas Martin", "Les E Atlas"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Connor et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Connor et al\\.", "year": 1994}, {"title": "Learning graphical model parameters with approximate marginal inference", "author": ["Justin Domke"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Domke.,? \\Q2013\\E", "shortCiteRegEx": "Domke.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Training iterative collective classifiers with back-propagation", "author": ["Shuangfei Fan", "Bert Huang"], "venue": "In 12th International Workshop on Mining and Learning with Graphs, San Francisco,", "citeRegEx": "Fan and Huang.,? \\Q2016\\E", "shortCiteRegEx": "Fan and Huang.", "year": 2016}, {"title": "Link mining: a survey", "author": ["Lise Getoor", "Christopher P Diehl"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Getoor and Diehl.,? \\Q2005\\E", "shortCiteRegEx": "Getoor and Diehl.", "year": 2005}, {"title": "Supervised sequence labelling", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Graves and Jaitly.,? \\Q2014\\E", "shortCiteRegEx": "Graves and Jaitly.", "year": 2014}, {"title": "Gradient flow in recurrent nets: the difficulty of learning", "author": ["Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi", "J\u00fcrgen Schmidhuber"], "venue": "long-term dependencies,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Structural learning with forgetting", "author": ["Masumi Ishikawa"], "venue": "Neural Networks,", "citeRegEx": "Ishikawa.,? \\Q1996\\E", "shortCiteRegEx": "Ishikawa.", "year": 1996}, {"title": "Why collective inference improves relational classification", "author": ["David Jensen", "Jennifer Neville", "Brian Gallagher"], "venue": "In SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Jensen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Jensen et al\\.", "year": 2004}, {"title": "Multilabel collective classification", "author": ["Xiangnan Kong", "Xiaoxiao Shi", "S Yu Philip"], "venue": "In SDM,", "citeRegEx": "Kong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2011}, {"title": "Inferring private information using social network data", "author": ["Jack Lindamood", "Raymond Heatherly", "Murat Kantarcioglu", "Bhavani Thuraisingham"], "venue": "In Proceedings of the 18th international conference on World wide web,", "citeRegEx": "Lindamood et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lindamood et al\\.", "year": 2009}, {"title": "Collective classification of network data", "author": ["Ben London", "Lise Getoor"], "venue": "Data Classification: Algorithms and Applications. CRC Press,", "citeRegEx": "London and Getoor.,? \\Q2013\\E", "shortCiteRegEx": "London and Getoor.", "year": 2013}, {"title": "Link-based classification", "author": ["Qing Lu", "Lise Getoor"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Lu and Getoor.,? \\Q2003\\E", "shortCiteRegEx": "Lu and Getoor.", "year": 2003}, {"title": "Learning to discover social circles in ego networks", "author": ["J.J. McAuley", "J. Leskovec"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "McAuley and Leskovec.,? \\Q2012\\E", "shortCiteRegEx": "McAuley and Leskovec.", "year": 2012}, {"title": "Cautious inference in collective classification", "author": ["Luke K McDowell", "Kalyan Moy Gupta", "David W Aha"], "venue": "In AAAI,", "citeRegEx": "McDowell et al\\.,? \\Q2007\\E", "shortCiteRegEx": "McDowell et al\\.", "year": 2007}, {"title": "Birds of a feather: Homophily in social networks", "author": ["Miller McPherson", "Lynn Smith-Lovin", "James M Cook"], "venue": "Annual Review of Sociology,", "citeRegEx": "McPherson et al\\.,? \\Q2001\\E", "shortCiteRegEx": "McPherson et al\\.", "year": 2001}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Deep collective inference", "author": ["John Moore", "Jennifer Neville"], "venue": "In Proceedings of the 31st AAAI Conference on Artificial Intelligence,", "citeRegEx": "Moore and Neville.,? \\Q2017\\E", "shortCiteRegEx": "Moore and Neville.", "year": 2017}, {"title": "Iterative classification in relational data", "author": ["Jennifer Neville", "David Jensen"], "venue": "In Proc. AAAI-2000 Workshop on Learning Statistical Models from Relational Data,", "citeRegEx": "Neville and Jensen.,? \\Q2000\\E", "shortCiteRegEx": "Neville and Jensen.", "year": 2000}, {"title": "Collective classification with relational dependency networks", "author": ["Jennifer Neville", "David Jensen"], "venue": "In Proceedings of the Second International Workshop on Multi-Relational Data Mining,", "citeRegEx": "Neville and Jensen.,? \\Q2003\\E", "shortCiteRegEx": "Neville and Jensen.", "year": 2003}, {"title": "Column networks for collective classification", "author": ["Trang Pham", "Truyen Tran", "Dinh Phung", "Svetha Venkatesh"], "venue": "arXiv preprint arXiv:1609.04508,", "citeRegEx": "Pham et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2016}, {"title": "Collective classification in network data", "author": ["Prithviraj Sen", "Galileo Namata", "Mustafa Bilgic", "Lise Getoor", "Brian Galligher", "Tina Eliassi-Rad"], "venue": "AI Magazine,", "citeRegEx": "Sen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sen et al\\.", "year": 2008}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Discriminative probabilistic models for relational data", "author": ["Ben Taskar", "Pieter Abbeel", "Daphne Koller"], "venue": "In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Taskar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2002}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Relational learning with one network: An asymptotic analysis", "author": ["R. Xiang", "J. Neville"], "venue": "In International Conference on Artificial Intelligence & Statistics (AISTATS),", "citeRegEx": "Xiang and Neville.,? \\Q2011\\E", "shortCiteRegEx": "Xiang and Neville.", "year": 2011}], "referenceMentions": [{"referenceID": 22, "context": "Existing learning algorithms for iterative classification resort to an approximation based on the unrealistic assumption that the predicted labels of neighbors are their true classes (Neville and Jensen, 2000; London and Getoor, 2013).", "startOffset": 183, "endOffset": 234}, {"referenceID": 15, "context": "Existing learning algorithms for iterative classification resort to an approximation based on the unrealistic assumption that the predicted labels of neighbors are their true classes (Neville and Jensen, 2000; London and Getoor, 2013).", "startOffset": 183, "endOffset": 234}, {"referenceID": 28, "context": "Finally, because the same base-classifier parameters should be used at all iterations of ICA, we can use methods for recurrent neural networks such as back-propagation through time (BPTT) (Werbos, 1990) to compute the combined gradient.", "startOffset": 188, "endOffset": 202}, {"referenceID": 7, "context": "Node classification is one of the fundamental tasks in analysis of network data (Getoor and Diehl, 2005; Lindamood et al., 2009).", "startOffset": 80, "endOffset": 128}, {"referenceID": 14, "context": "Node classification is one of the fundamental tasks in analysis of network data (Getoor and Diehl, 2005; Lindamood et al., 2009).", "startOffset": 80, "endOffset": 128}, {"referenceID": 13, "context": "Collective classification addresses this task by making joint classifications of connected nodes (Kong et al., 2011; Taskar et al., 2002; Neville and Jensen, 2003).", "startOffset": 97, "endOffset": 163}, {"referenceID": 27, "context": "Collective classification addresses this task by making joint classifications of connected nodes (Kong et al., 2011; Taskar et al., 2002; Neville and Jensen, 2003).", "startOffset": 97, "endOffset": 163}, {"referenceID": 23, "context": "Collective classification addresses this task by making joint classifications of connected nodes (Kong et al., 2011; Taskar et al., 2002; Neville and Jensen, 2003).", "startOffset": 97, "endOffset": 163}, {"referenceID": 18, "context": "Gibbs sampling (GS) is another approach for collective classification using the iterative classification framework (McDowell et al., 2007; Sen et al., 2008), that introduces randomization into the iterative classification.", "startOffset": 115, "endOffset": 156}, {"referenceID": 25, "context": "Gibbs sampling (GS) is another approach for collective classification using the iterative classification framework (McDowell et al., 2007; Sen et al., 2008), that introduces randomization into the iterative classification.", "startOffset": 115, "endOffset": 156}, {"referenceID": 25, "context": "ICA and GS have been shown repeatedly to be effective frameworks for collective classification (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003; McDowell et al., 2007).", "startOffset": 95, "endOffset": 204}, {"referenceID": 12, "context": "ICA and GS have been shown repeatedly to be effective frameworks for collective classification (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003; McDowell et al., 2007).", "startOffset": 95, "endOffset": 204}, {"referenceID": 22, "context": "ICA and GS have been shown repeatedly to be effective frameworks for collective classification (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003; McDowell et al., 2007).", "startOffset": 95, "endOffset": 204}, {"referenceID": 16, "context": "ICA and GS have been shown repeatedly to be effective frameworks for collective classification (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003; McDowell et al., 2007).", "startOffset": 95, "endOffset": 204}, {"referenceID": 18, "context": "ICA and GS have been shown repeatedly to be effective frameworks for collective classification (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003; McDowell et al., 2007).", "startOffset": 95, "endOffset": 204}, {"referenceID": 19, "context": "One of the more natural motivations for collective classification comes from the study of social networks, where the phenomenon of homophily\u2014the tendency of individuals to interact with other similar individuals\u2014has been an important concept (McPherson et al., 2001; Bilgic et al., 2007).", "startOffset": 242, "endOffset": 287}, {"referenceID": 1, "context": "One of the more natural motivations for collective classification comes from the study of social networks, where the phenomenon of homophily\u2014the tendency of individuals to interact with other similar individuals\u2014has been an important concept (McPherson et al., 2001; Bilgic et al., 2007).", "startOffset": 242, "endOffset": 287}, {"referenceID": 29, "context": "In transductive settings, the classifier is both trained and tested on partially labeled networks, or possibly in the same network with a different set of labels known during each phase (Xiang and Neville, 2011).", "startOffset": 186, "endOffset": 211}, {"referenceID": 0, "context": "RNNs were introduced decades ago (Angeline et al., 1994; Connor et al., 1994), but they have recently become prominent because of their effectiveness at modeling sequences, such as those occurring in natural language processing, e.", "startOffset": 33, "endOffset": 77}, {"referenceID": 3, "context": "RNNs were introduced decades ago (Angeline et al., 1994; Connor et al., 1994), but they have recently become prominent because of their effectiveness at modeling sequences, such as those occurring in natural language processing, e.", "startOffset": 33, "endOffset": 77}, {"referenceID": 26, "context": ", (Socher et al., 2011; Mikolov et al., 2010; Graves and Jaitly, 2014; Graves, 2012).", "startOffset": 2, "endOffset": 84}, {"referenceID": 20, "context": ", (Socher et al., 2011; Mikolov et al., 2010; Graves and Jaitly, 2014; Graves, 2012).", "startOffset": 2, "endOffset": 84}, {"referenceID": 9, "context": ", (Socher et al., 2011; Mikolov et al., 2010; Graves and Jaitly, 2014; Graves, 2012).", "startOffset": 2, "endOffset": 84}, {"referenceID": 8, "context": ", (Socher et al., 2011; Mikolov et al., 2010; Graves and Jaitly, 2014; Graves, 2012).", "startOffset": 2, "endOffset": 84}, {"referenceID": 28, "context": "A now standard method for gradient optimization of RNN parameters is known as back-propagation through time (Werbos, 1990; Hochreiter et al., 2001; Ishikawa, 1996), which unrolls recurrent networks and computes gradients for the parameters separately before combining them into a single update.", "startOffset": 108, "endOffset": 163}, {"referenceID": 10, "context": "A now standard method for gradient optimization of RNN parameters is known as back-propagation through time (Werbos, 1990; Hochreiter et al., 2001; Ishikawa, 1996), which unrolls recurrent networks and computes gradients for the parameters separately before combining them into a single update.", "startOffset": 108, "endOffset": 163}, {"referenceID": 11, "context": "A now standard method for gradient optimization of RNN parameters is known as back-propagation through time (Werbos, 1990; Hochreiter et al., 2001; Ishikawa, 1996), which unrolls recurrent networks and computes gradients for the parameters separately before combining them into a single update.", "startOffset": 108, "endOffset": 163}, {"referenceID": 6, "context": "After the first version of our manuscript was published (Fan and Huang, 2016), two other groups have independently and concurrently pursued very similar directions, applying deep learning to collective classification (Pham et al.", "startOffset": 56, "endOffset": 77}, {"referenceID": 24, "context": "After the first version of our manuscript was published (Fan and Huang, 2016), two other groups have independently and concurrently pursued very similar directions, applying deep learning to collective classification (Pham et al., 2016; Moore and Neville, 2017).", "startOffset": 217, "endOffset": 261}, {"referenceID": 21, "context": "After the first version of our manuscript was published (Fan and Huang, 2016), two other groups have independently and concurrently pursued very similar directions, applying deep learning to collective classification (Pham et al., 2016; Moore and Neville, 2017).", "startOffset": 217, "endOffset": 261}, {"referenceID": 24, "context": "Pham et al. (2016) proposed another deep learning model for collective classification.", "startOffset": 0, "endOffset": 19}, {"referenceID": 25, "context": "In practice, researchers have used naive Bayes, logistic regression, support vector machines as classifiers, and they have used averages, sums, and presence as relational features (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003).", "startOffset": 180, "endOffset": 266}, {"referenceID": 12, "context": "In practice, researchers have used naive Bayes, logistic regression, support vector machines as classifiers, and they have used averages, sums, and presence as relational features (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003).", "startOffset": 180, "endOffset": 266}, {"referenceID": 22, "context": "In practice, researchers have used naive Bayes, logistic regression, support vector machines as classifiers, and they have used averages, sums, and presence as relational features (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003).", "startOffset": 180, "endOffset": 266}, {"referenceID": 16, "context": "In practice, researchers have used naive Bayes, logistic regression, support vector machines as classifiers, and they have used averages, sums, and presence as relational features (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003).", "startOffset": 180, "endOffset": 266}, {"referenceID": 25, "context": "which is the class label with the highest probability among neighbors and exists, which is an indicator for each class label (Sen et al., 2008).", "startOffset": 125, "endOffset": 143}, {"referenceID": 25, "context": "The Cora data set is a collection of 2,708 machine learning publications categorized into seven classes (Sen et al., 2008).", "startOffset": 104, "endOffset": 122}, {"referenceID": 25, "context": "tion of 3,312 research publications crawled from the CiteSeer repository (Sen et al., 2008).", "startOffset": 73, "endOffset": 91}, {"referenceID": 17, "context": "The social network data we use is the Facebook ego network data (McAuley and Leskovec, 2012), which includes users\u2019 personal information, friend lists, and ego networks.", "startOffset": 64, "endOffset": 92}, {"referenceID": 2, "context": "The image dataset we use is the Weizmann horse-image segmentation set (Borenstein and Ullman, 2004).", "startOffset": 70, "endOffset": 99}, {"referenceID": 2, "context": "The image dataset we use is the Weizmann horse-image segmentation set (Borenstein and Ullman, 2004). We subsample 20 images of horses on various backgrounds. We use features described by Domke (2013) for each pixel: We expand the RGB values of each pixel and the normalized vertical and horizontal positions into 64 features using sinusoidal expansion.", "startOffset": 71, "endOffset": 200}, {"referenceID": 5, "context": "For each of the learning objectives, we optimize using the adagrad approach (Duchi et al., 2011), in which gradients are rescaled based on the magnitude of previously seen gradients.", "startOffset": 76, "endOffset": 96}], "year": 2017, "abstractText": "We propose a new method for training iterative collective classifiers for labeling nodes in network data. The iterative classification algorithm (ICA) is a canonical method for incorporating relational information into classification. Yet, existing methods for training ICA models rely on the assumption that relational features reflect the true labels of the nodes. This unrealistic assumption introduces a bias that is inconsistent with the actual prediction algorithm. In this paper, we introduce recurrent collective classification (RCC), a variant of ICA analogous to recurrent neural network prediction. RCC accommodates any differentiable local classifier and relational feature functions. We provide gradient-based strategies for optimizing over model parameters to more directly minimize the loss function. In our experiments, this direct loss minimization translates to improved accuracy and robustness on real network data. We demonstrate the robustness of RCC in settings where local classification is very noisy, settings that are particularly challenging for ICA.", "creator": "TeX"}}}