{"id": "1703.06585", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning", "abstract": "We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative 'image guessing' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward. We also provide tools for game models to learn from the experience of these agents to learn the behavior of their own agents. As part of this project, we've developed the framework for a visual task task system that allows users to learn to pick a hidden image from a collection of images, then, when a screen becomes visible.\n\n\n\nThe video will be shown as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of a video tutorial that includes images as part of", "histories": [["v1", "Mon, 20 Mar 2017 03:50:57 GMT  (5750kb,D)", "http://arxiv.org/abs/1703.06585v1", "11 pages, 4 figures, 2 tables, webpage:this http URL"], ["v2", "Tue, 21 Mar 2017 17:41:23 GMT  (5750kb,D)", "http://arxiv.org/abs/1703.06585v2", "11 pages, 4 figures, 2 tables, webpage:this http URL"]], "COMMENTS": "11 pages, 4 figures, 2 tables, webpage:this http URL", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL cs.LG", "authors": ["abhishek das", "satwik kottur", "jos\\'e m f moura", "stefan lee", "dhruv batra"], "accepted": false, "id": "1703.06585"}, "pdf": {"name": "1703.06585.pdf", "metadata": {"source": "CRF", "title": "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning", "authors": ["Abhishek Das", "Satwik Kottur", "Jos\u00e9 M.F. Moura", "Stefan Lee", "Dhruv Batra"], "emails": [], "sections": [{"heading": null, "text": "We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative \u2018image guessing\u2019 game between two agents \u2013 Q-BOT and A-BOT\u2013 who communicate in natural language dialog so that Q-BOT can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end \u2013 from pixels to multi-agent multi-round dialog to game reward. We demonstrate two experimental results. First, as a \u2018sanity check\u2019 demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/size). Thus, we demonstrate the emergence of grounded language and communication among \u2018visual\u2019 dialog agents with no human supervision at all. Second, we conduct large-scale real-image experiments on the VisDial dataset [4], where we pretrain with supervised dialog data and show that the RL \u2018fine-tuned\u2019 agents significantly outperform SL agents. Interestingly, the RL Q-BOT learns to ask questions that A-BOT is good at, ultimately resulting in more informative dialog and a better team."}, {"heading": "1. Introduction", "text": "The focus of this paper is visually-grounded conversational artificial intelligence (AI). Specifically, we would like to develop agents that can \u2018see\u2019 (i.e., understand the contents of an image) and \u2018communicate\u2019 that understanding in natural language (i.e., hold a dialog involving questions and answers about that image). We believe the next generation of intelligent systems will need to posses this ability to hold a dialog about visual content for a variety of applications: e.g., helping visually impaired users understand their surroundings [2] or social media content [35] (\u2018Who is in the photo? Dave. What is he doing?\u2019), enabling analysts to\n*The first two authors (AD, SK) contributed equally.\nsift through large quantities of surveillance data (\u2018Did anyone enter the vault in the last month? Yes, there are 103 recorded instances. Did any of them pick something up?\u2019), and enabling users to interact naturally with intelligent assistants (either embodied as a robot or not) (\u2018Did I leave my phone on my desk? Yes, it\u2019s here. Did I miss any calls?\u2019). Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31\u2013 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent. Two recent works [4, 5] have proposed studying this task of visually-grounded dialog. Perhaps somewhat counterintuitively, both these works treat dialog as a static supervised learning problem, rather than an interactive agent learning problem that it naturally is. Specifically, both\nar X\niv :1\n70 3.\n06 58\n5v 1\n[ cs\n.C V\n] 2\n0 M\nar 2\n01 7\nworks [4, 5] first collect a dataset of human-human dialog, i.e., a sequence of question-answer pairs about an image (q1, a1), . . . , (qT , aT ). Next, a machine (a deep neural network) is provided with the image I , the human dialog recorded till round t\u2212 1, (q1, a1), . . . , (qt\u22121, at\u22121), the follow-up question qt, and is supervised to generate the human response at. Essentially, at each round t, the machine is artificially \u2018injected\u2019 into the conversation between two humans and asked to answer the question qt; but the machine\u2019s answer a\u0302t is thrown away, because at the next round t+1, the machine is again provided with the \u2018ground-truth\u2019 human-human dialog that includes the human response at and not the machine response a\u0302t. Thus, the machine is never allowed to steer the conversation because that would take the dialog out of the dataset, making it non-evaluable. In this paper, we generalize the task of Visual Dialog beyond the necessary first stage of supervised learning \u2013 by posing it as a cooperative \u2018image guessing\u2019 game between two dialog agents. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end \u2013 from pixels to multi-agent multi-round dialog to the game reward. Our setup is illustrated in Fig. 1. We formulate a game between a questioner bot (Q-BOT) and an answerer bot (ABOT). Q-BOT is shown a 1-sentence description (a caption) of an unseen image, and is allowed to communicate in natural language (discrete symbols) with the answering bot (ABOT), who is shown the image. The objective of this fullycooperative game is for Q-BOT to build a mental model of the unseen image purely from the natural language dialog, and then retrieve that image from a lineup of images. Notice that this is a challenging game. Q-BOT must ground the words mentioned in the provided caption (\u2018Two zebra are walking around their pen at the zoo.\u2019), estimate which images from the provided pool contain this content (there will typically be many such images since captions describe only the salient entities), and ask follow-up questions (\u2018Any people in the shot? Are there clouds in the sky? Are they facing each other?\u2019) that help it identify the correct image. Analogously, A-BOT must build a mental model of what QBOT understands, and answer questions (\u2018No, there aren\u2019t any. I can\u2019t see the sky. They aren\u2019t.\u2019) in a precise enough way to allow discrimination between similar images from a pool (that A-BOT does not have access to) while being concise enough to not confuse the imperfect Q-BOT. At every round of dialog, Q-BOT listens to the answer provided by A-BOT, updates its beliefs, and makes a prediction about the visual representation of the unseen image (specifically, the fc7 vector of I), and receives a reward from the environment based on how close Q-BOT\u2019s prediction is to the true fc7 representation of I . The goal of Q-BOT and A-BOT is to communicate to maximize this reward. One critical issue is that both the agents are imperfect and noisy \u2013 both \u2018forget\u2019 things in the past, sometimes repeat them-\nselves, may not stay consistent in their responses, A-BOT does not have access to an external knowledge-base so it cannot answer all questions, etc. Thus, to succeed at the task, they must learn to play to each other\u2019s strengths. An important question to ask is \u2013 why force the two agents to communicate in discrete symbols (English words) as opposed to continuous vectors? The reason is twofold. First, discrete symbols and natural language is interpretable. By forcing the two agents to communicate and understand natural language, we ensure that humans can not only inspect the conversation logs between two agents, but more importantly, communicate with them. After the two bots are trained, we can pair a human questioner with A-BOT to accomplish the goals of visual dialog (aiding visually/situationally impaired users), and pair a human answerer with Q-BOT to play a visual 20-questions game. The second reason to communicate in discrete symbols is to prevent cheating \u2013 if Q-BOT and A-BOT are allowed to exchange continuous vectors, then the trivial solution is for A-BOT to ignore Q-BOT\u2019s question and directly convey the fc7 vector for I , allowing Q-BOT to make a perfect prediction. In essence, discrete natural language is an interpretable lowdimensional \u201cbottleneck\u201d layer between these two agents. Contributions. We introduce a novel goal-driven training for visual question answering and dialog agents. Despite significant popular interest in VQA (over 200 works citing [1] since 2015), all previous approaches have been based on supervised learning, making this the first instance of goaldriven training for visual question answering / dialog. We demonstrate two experimental results. First, as a \u2018sanity check\u2019 demonstration of pure RL (from scratch), we show results on a diagnostic task where perception is perfect \u2013 a synthetic world with \u2018images\u2019 containing a single object defined by three attributes (shape/color/style). In this synthetic world, for Q-BOT to identify an image, it must learn about these attributes. The two bots communicate via an ungrounded vocabulary, i.e., symbols with no pre-specified human-interpretable meanings (\u2018X\u2019, \u2018Y\u2019, \u20181\u2019, \u20182\u2019). When trained end-to-end with RL on this task, we find that the two bots invent their own communication protocol \u2013 Q-BOT starts using certain symbols to query for specific attributes (\u2018X\u2019 for color), and A-BOT starts responding with specific symbols indicating the value of that attribute (\u20181\u2019 for red). Essentially, we demonstrate the automatic emergence of grounded language and communication among \u2018visual\u2019 dialog agents with no human supervision! Second, we conduct large-scale real-image experiments on the VisDial dataset [4]. With imperfect perception on real images, discovering a human-interpretable language and communication strategy from scratch is both tremendously difficult and an unnecessary re-invention of English. Thus, we pretrain with supervised dialog data in VisDial before \u2018fine tuning\u2019 with RL; this alleviates a number of challenges\nin making deep RL converge to something meaningful. We show that these RL fine-tuned bots significantly outperform the supervised bots. Most interestingly, while the supervised Q-BOT attempts to mimic how humans ask questions, the RL trained Q-BOT shifts strategies and asks questions that the A-BOT is better at answering, ultimately resulting in more informative dialog and a better team."}, {"heading": "2. Related Work", "text": "Vision and Language. A number of problems at the intersection of vision and language have recently gained prominence, e.g., image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23]. Most related to this paper are two recent works on visually-grounded dialog [4, 5]. Das et al. [4] proposed the task of Visual Dialog, collected the VisDial dataset by pairing two subjects on Amazon Mechanical Turk to chat about an image (with assigned roles of \u2018Questioner\u2019 and \u2018Answerer\u2019), and trained neural visual dialog answering models. De Vries et al. [5] extended the Referit game [13] to a \u2018GuessWhat\u2019 game, where one person asks questions about an image to guess which object has been \u2018selected\u2019, and the second person answers questions in \u2018yes\u2019/\u2018no\u2019/NA (natural language answers are disallowed). One disadvantage of GuessWhat is that it requires bounding box annotations for objects; our image guessing game does not need any such annotations and thus an unlimited number of game plays may be simulated. Moreover, as described in Sec. 1, both these works unnaturally treat dialog as a static supervised learning problem. Although both datasets contain thousands of human dialogs, they still only represent an incredibly sparse sample of the vast space of visually-grounded questions and answers. Training robust, visually-grounded dialog agents via supervised techniques is still a challenging task. In our work, we take inspiration from the AlphaGo [26] approach of supervision from human-expert games and reinforcement learning from self-play. Similarly, we perform supervised pretraining on human dialog data and fine-tune in an end-to-end goal-driven manner with deep RL. 20 Questions and Lewis Signaling Game. Our proposed image-guessing game is naturally the visual analog of the popular 20-questions game. More formally, it is a generalization of the Lewis Signaling (LS) [16] game, widely studied in economics and game theory. LS is a cooperative game between two players \u2013 a sender and a receiver. In the classical setting, the world can be in a number of finite discrete states {1, 2, . . . , N}, which is known to the sender but not the receiver. The sender can send one of N discrete symbols/signals to the receiver, who upon receiving the signal must take one of N discrete actions. The game is perfectly cooperative, and one simple (though not unique) Nash Equilibrium is the \u2018identity mapping\u2019, where the sender encodes each world state with a bijective signal, and similarly the\nreceiver has a bijective mapping from a signal to an action. Our proposed \u2018image guessing\u2019 game is a generalization of LS with Q-BOT being the receiver and A-BOT the sender. However, in our proposed game, the receiver (Q-BOT) is not passive. It actively solicits information by asking questions. Moreover, the signaling process is not \u2018single shot\u2019, but proceeds over multiple rounds of conversation. Text-only or Classical Dialog. Li et al. [17] have proposed using RL for training dialog systems. However, they hand-define what a \u2018good\u2019 utterance/dialog looks like (nonrepetition, coherence, continuity, etc.). In contrast, taking a cue from adversarial learning [9, 18], we set up a cooperative game between two agents, such that we do not need to hand-define what a \u2018good\u2019 dialog looks like \u2013 a \u2018good\u2019 dialog is one that leads to a successful image-guessing play. Emergence of Language. There is a long history of work on language emergence in multi-agent systems [22]. The more recent resurgence has focused on deep RL [10,15,21]. The high-level ideas of these unpublished concurrent works are similar to our synthetic experiments. For our large-scale real-image results, we do not want our bots to invent their own uninterpretable language and use pretraining on VisDial [4] to achieve \u2018alignment\u2019 with English."}, {"heading": "3. Cooperative Image Guessing Game:", "text": "In Full Generality and a Specific Instantiation\nPlayers and Roles. The game involves two collaborative agents \u2013 a questioner bot (Q-BOT) and an answerer bot (ABOT) \u2013 with an information asymmetry. A-BOT sees an image I , Q-BOT does not. Q-BOT is primed with a 1-sentence description c of the unseen image and asks \u2018questions\u2019 (sequence of discrete symbols over a vocabulary V ), which ABOT answers with another sequence of symbols. The communication occurs for a fixed number of rounds. Game Objective in General. At each round, in addition to communicating, Q-BOT must provide a \u2018description\u2019 y\u0302 of the unknown image I based only on the dialog history and both players receive a reward from the environment inversely proportional to the error in this description under some metric `(y\u0302, ygt). We note that this is a general setting where the \u2018description\u2019 y\u0302 can take on varying levels of specificity \u2013 from image embeddings (or fc7 vectors of I) to textual descriptions to pixel-level image generations. Specific Instantiation. In our experiments, we focus on the setting where Q-BOT is tasked with estimating a vector embedding of the image I . Given some feature extractor (i.e., a pretrained CNN model, say VGG-16), no human annotation is required to produce the target \u2018description\u2019 y\u0302gt (simply forward-prop the image through the CNN). Reward/error can be measured by simple Euclidean distance, and any image may be used as the visual grounding for a dialog. Thus, an unlimited number of \u2018game plays\u2019 may be simulated."}, {"heading": "4. Reinforcement Learning for Dialog Agents", "text": "In this section, we formalize the training of two visual dialog agents (Q-BOT and A-BOT) with Reinforcement Learning (RL) \u2013 describing formally the action, state, environment, reward, policy, and training procedure. We begin by noting that although there are two agents (Q-BOT, A-BOT), since the game is perfectly cooperative, we can without loss of generality view this as a single-agent RL setup where the single \u201cmeta-agent\u201d comprises of two \u201cconstituent agents\u201d communicating via a natural language bottleneck layer. Action. Both agents share a common action space consisting of all possible output sequences under a token vocabulary V . This action space is discrete and in principle, infinitely-large since arbitrary length sequences qt, at may be produced and the dialog may go on forever. In our synthetic experiment, the two agents are given different vocabularies to coax a certain behavior to emerge (details in Sec. 5). In our VisDial experiments, the two agents share a common vocabulary of English tokens. In addition, at each round of the dialog t, Q-BOT also predicts y\u0302t, its current guess about the visual representation of the unseen image. This component of Q-BOT\u2019s action space is continuous. State. Since there is information asymmetry (A-BOT can see the image I , Q-BOT cannot), each agent has its own observed state. For a dialog grounded in image I with caption c, the state of Q-BOT at round t is the caption and dialog history so far sQt = [c, q1, a1, . . . , qt\u22121, at\u22121], and the state of A-BOT also includes the image sAt = [I, c, q1, a1, . . . , qt\u22121, at\u22121, qt]. Policy. We model Q-BOT and A-BOT operating under stochastic policies \u03c0Q(qt | sQt ; \u03b8Q) and \u03c0A(at | sAt ; \u03b8A), such that questions and answers may be sampled from these policies conditioned on the dialog/state history. These policies will be learned by two separate deep neural networks parameterized by \u03b8Q and \u03b8A. In addition, Q-BOT includes a feature regression network f(\u00b7) that produces an image representation prediction after listening to the answer at round t, i.e., y\u0302t = f(s Q t , qt, at; \u03b8f ) = f(s Q t+1; \u03b8f ). Thus, the goal of policy learning is to estimate the parameters \u03b8Q, \u03b8A, \u03b8f . Environment and Reward. The environment is the image I upon which the dialog is grounded. Since this is a purely cooperative setting, both agents receive the same reward. Let `(\u00b7, \u00b7) be a distance metric on image representations (Euclidean distance in our experiments). At each round t, we define the reward for a state-action pair as:\nrt ( sQt\ufe38\ufe37\ufe37\ufe38 state , (qt, at, yt)\ufe38 \ufe37\ufe37 \ufe38 action ) = ` ( y\u0302t\u22121, y gt )\ufe38 \ufe37\ufe37 \ufe38\ndistance at t-1\n\u2212 ` ( y\u0302t, y gt )\ufe38 \ufe37\ufe37 \ufe38\ndistance at t\n(1)\ni.e., the change in distance to the true representation before and after a round of dialog. In this way, we consider a question-answer pair to be low quality (i.e., have a negative reward) if it leads the questioner to make a worse estimate of\nthe target image representation than if the dialog had ended. Note that the total reward summed over all time steps of a dialog is a function of only the initial and final states due to the cancellation of intermediate terms, i.e.,\nT\u2211 t=1 rt ( sQt , (qt, at, yt)) ) = ` ( y\u03020, y gt ) \u2212 ` ( y\u0302T , y gt )\ufe38 \ufe37\ufe37 \ufe38\noverall improvement due to dialog\n(2)\nThis is again intuitive \u2013 \u2018How much do the feature predictions of Q-BOT improve due to the dialog?\u2019 The details of policy learning are described in Sec. 4.2, but before that, let us describe the inner working of the two agents.\n4.1. Policy Networks for Q-BOT and A-BOT\nFig. 2 shows an overview of our policy networks for Q-BOT and A-BOT and their interaction within a single round of dialog. Both the agent policies are modeled via Hierarchical Recurrent Encoder-Decoder neural networks, which have recently been proposed for dialog modeling [4, 24, 25]. Q-BOT consists of the following four components:\n- Fact Encoder: Q-BOT asks a question qt: \u2018Are there any animals?\u2019 and receives an answer at: \u2018Yes, there are two elephants.\u2019. Q-BOT treats this concatenated (qt, at)-pair as a \u2018fact\u2019 it now knows about the unseen image. The fact encoder is an LSTM whose final hidden state FQt \u2208 R512 is used as an embedding of (qt, at).\n- State/History Encoder is an LSTM that takes the encoded fact FQt at each time step to produce an encoding of the prior dialog including time t as SQt \u2208 R512. Notice that this results in a two-level hierarchical encoding of the dialog (qt, at)\u2192 FQt and (F Q 1 , . . . , F Q t )\u2192 S Q t .\n- Question Decoder is an LSTM that takes the state/history encoding from the previous round SQt\u22121 and generates question qt by sequentially sampling words.\n- Feature Regression Network f(\u00b7) is a single fullyconnected layer that produces an image representation prediction y\u0302t from the current encoded state y\u0302t = f(S Q t ).\nEach of these components and their relation to each other are shown on the left side of Fig. 2. We collectively refer to the parameters of the three LSTM models as \u03b8Q and those of the feature regression network as \u03b8f . A-BOT has a similar structure to Q-BOT with slight differences since it also models the image I via a CNN:\n- Question Encoder: A-BOT receives a question qt from Q-BOT and encodes it via an LSTM QAt \u2208 R512.\n- Fact Encoder: Similar to Q-BOT, A-BOT also encodes the (qt, at)-pairs via an LSTM to get FAt \u2208 R512. The purpose of this encoder is for A-BOT to remember what it has already told Q-BOT and be able to understand references to entities already mentioned.\n- State/History Encoder is an LSTM that takes as input at each round t \u2013 the encoded question QAt , the image features from VGG [27] y, and the previous fact encoding FAt\u22121 \u2013 to produce a state encoding, i.e.( (y,QA1 , F A 0 ), . . . , (y,Q A t , F A t\u22121) ) \u2192 SAt . This allows\nthe model to contextualize the current question w.r.t. the history while looking at the image to seek an answer.\n- Answer Decoder is an LSTM that takes the state encoding SAt and generates at by sequentially sampling words.\nOur code will be publicly available. To recap, a dialog round at time t consists of 1) Q-BOT generating a question qt conditioned on its state encoding SQt\u22121, 2) A-BOT encoding qt, updating its state encoding SAt , and generating an answer at, 3) Q-BOT and A-BOT both encoding the completed exchange as FQt and F A t , and 4) Q-BOT updating its state to SQt based on F Q t and making an image representation prediction y\u0302t for the unseen image."}, {"heading": "4.2. Joint Training with Policy Gradients", "text": "In order to train these agents, we use the REINFORCE [34] algorithm that updates policy parameters (\u03b8Q, \u03b8A, \u03b8f ) in response to experienced rewards. In this section, we derive the expressions for the parameter gradients for our setup. Recall that our agents take actions \u2013 communication (qt, at) and feature prediction y\u0302t \u2013 and our objective is to maximize the expected reward under the agents\u2019 policies, summed over the entire dialog:\nmin \u03b8A,\u03b8Q,\u03b8g J(\u03b8A, \u03b8Q, \u03b8g) where, (3)\nJ(\u03b8A, \u03b8Q, \u03b8g) = E \u03c0Q,\u03c0A [ T\u2211 t=1 rt ( sQt , (qt, at, yt) )] (4)\nWhile the above is a natural objective, we find that considering the entire dialog as a single RL episode does not differentiate between individual good or bad exchanges within it. Thus, we update our model based on per-round rewards,\nJ(\u03b8A, \u03b8Q, \u03b8g) = E \u03c0Q,\u03c0A\n[ rt ( sQt , (qt, at, yt) )] (5)\nFollowing the REINFORCE algorithm, we can write the gradient of this expectation as an expectation of a quantity related to the gradient. For \u03b8Q, we derive this explicitly:\n\u2207\u03b8QJ = \u2207\u03b8Q [\nE \u03c0Q,\u03c0A\n[rt (\u00b7)] ] (rt inputs hidden to avoid clutter)\n= \u2207\u03b8Q [\u2211 qt,at \u03c0Q ( qt|sQt\u22121 ) \u03c0A ( at|sAt ) rt (\u00b7) ] =\n\u2211 qt,at \u03c0Q ( qt|sQt\u22121 ) \u2207\u03b8Q log \u03c0Q ( qt|sQt\u22121 ) \u03c0A ( at|sAt ) rt (\u00b7)\n= E \u03c0Q,\u03c0A\n[ rt (\u00b7) \u2207\u03b8Q log \u03c0Q ( qt|sQt\u22121 )] (6)\nSimilarly, gradient w.r.t. \u03b8A, i.e., \u2207\u03b8AJ can be derived as\n\u2207\u03b8AJ = E \u03c0Q,\u03c0A\n[ rt (\u00b7) \u2207\u03b8A log \u03c0A ( at|sAt )] . (7)\nAs is standard practice, we estimate these expectations with sample averages. Specifically, we sample a question from Q-BOT (by sequentially sampling words from the question decoder LSTM till a stop token is produced), sample its answer from A-BOT, compute the scalar reward for this round, multiply that scalar reward to gradient of log-probability of this exchange, propagate backward to compute gradients w.r.t. all parameters \u03b8Q, \u03b8A. This update has an intuitive interpretation \u2013 if a particular (qt, at) is informative (i.e., leads to positive reward), its probabilities will be pushed up (positive gradient). Conversely, a poor exchange leading to negative reward will be pushed down (negative gradient).\nFinally, since the feature regression network f(\u00b7) forms a deterministic policy, its parameters \u03b8f receive \u2018supervised\u2019 gradient updates for differentiable `(\u00b7, \u00b7)."}, {"heading": "5. Emergence of Grounded Dialog", "text": "To succeed at our image guessing game, Q-BOT and A-BOT need to accomplish a number of challenging sub-tasks \u2013 they must learn a common language (do you understand what I mean when I say \u2018person\u2019?) and develop mappings between symbols and image representations (what does \u2018person\u2019 look like?), i.e., A-BOT must learn to ground language in visual perception to answer questions and QBOT must learn to predict plausible image representations \u2013 all in an end-to-end manner from a distant reward function. Before diving in to the full task on real images, we conduct a \u2018sanity check\u2019 on a synthetic dataset with perfect perception to ask \u2013 is this even possible? Setup. As shown in Fig. 3, we consider a synthetic world with \u2018images\u2019 represented as a triplet of attributes \u2013 4 shapes, 4 colors, 4 styles \u2013 for a total of 64 unique images. A-BOT has perfect perception and is given direct access to this representation for an image. Q-BOT is tasked with deducing two attributes of the image in a particular order \u2013 e.g., if the task is (shape, color), Q-BOT would need to output (square, purple) for a (purple, square, filled) image seen by A-BOT (see Fig. 3b). We form all 6 such tasks per image. Vocabulary. We conducted a series of pilot experiments and found the choice of the vocabulary size to be crucial for coaxing non-trivial \u2018non-cheating\u2019 behavior to emerge. For instance, we found that if the A-BOT vocabulary VA is large enough, say |VA| \u2265 64 (#images), the optimal policy learnt simply ignores what Q-BOT asks and A-BOT conveys the entire image in a single token (e.g. token 1 \u2261 (red, square, filled)). As with human communication, an impoverished vocabulary that cannot possibly encode the richness of the visual sensor is necessary for non-trivial dialog to emerge. To ensure at least 2 rounds of dialog, we restrict each agent to only produce a single symbol utterance per round from \u2018minimal\u2019 vocabularies VA = {1, 2, 3, 4} for A-BOT and VQ = {X,Y, Z} for Q-BOT. Since |VA|#rounds < #images,\na non-trivial dialog is necessary to succeed at the task. Policy Learning. Since the action space is discrete and small, we instantiate Q-BOT and A-BOT as fully specified tables of Q-values (state, action, future reward estimate) and apply tabular Q-learning with Monte Carlo estimation over 10k episodes to learn the policies. Updates are done alternately where one bot is frozen while the other is updated. During training, we use -greedy policies [28], ensuring an action probability of 0.6 for the greedy action and split the remaining probability uniformly across other actions. At test time, we default to greedy, deterministic policy obtained from these -greedy policies. The task requires outputting the correct attribute value pair based on the task and image. Since there are a total of 4+ 4+ 4 = 12 unique values across the 3 attributes, Q-BOT\u2019s final action selects one of 12\u00d712=144 attribute-pairs. We use +1 and \u22121 as rewards for right and wrong predictions. Results. Fig. 3d shows the reward achieved by the agents\u2019 policies vs. number of RL iterations (each with 10k episodes/dialogs). We can see that the two quickly learn the optimal policy. Fig. 3b,c show some example exchanges between the trained bots. We find that the two invent their own communication protocol \u2013 Q-BOT consistently uses specific symbols to query for specific attributes: X \u2192 color, Y \u2192 shape, Z \u2192 style. And A-BOT consistently responds with specific symbols to indicate the inquired attribute, e.g., if QBOT emits X (asks for color), A-BOT responds with: 1 \u2192 purple, 2 \u2192 green, 3 \u2192 blue, 4 \u2192 red. Similar mappings exist for responses to other attributes. Essentially, we find the automatic emergence of grounded language and a communication protocol among \u2018visual\u2019 dialog agents without any human supervision!"}, {"heading": "6. Experiments", "text": "Our synthetic experiments in the previous section establish that when faced with a cooperative task where information must be exchanged, two agents with perfect perception are capable of developing a complex communication protocol. In general, with imperfect perception on real images, discovering human-interpretable language and communication\nstrategy from scratch is both tremendously difficult and an unnecessary re-invention of English. We leverage the recently introduced VisDial dataset [4] that contains (as of the publicly released v0.5) human dialogs (10 rounds of question-answer pairs) on 68k images from the COCO dataset, for a total of 680k QA-pairs. Example dialogs from the VisDial dataset are shown in Tab. 1. Image Feature Regression. We consider a specific instantiation of the visual guessing game described in Sec. 3 \u2013 specifically at each round t, Q-BOT needs to regress to the vector embedding y\u0302t of image I corresponding to the fc7 (penultimate fully-connected layer) output from VGG16 [27]. The distance metric used in the reward computation is `2, i.e. rt(\u00b7) = ||ygt \u2212 y\u0302t\u22121|| 2 2 \u2212 ||ygt \u2212 y\u0302t|| 2 2. Training Strategies. We found two training strategies to be crucial to ensure/improve the convergence of the RL framework described in Sec. 4, to produce any meaningful dialog exchanges, and to ground the agents in natural language. 1) Supervised Pretraining. We first train both agents in a supervised manner on the train split of VisDial [4] v0.5 under an MLE objective. Thus, conditioned on human dialog history, Q-BOT is trained to generate the follow-up question by human1, A-BOT is trained to generate the response by human2, and the feature network f(\u00b7) is optimized to regress to y. The CNN in A-BOT is pretrained on ImageNet. This pretraining ensures that the agents can generally recognize some objects/scenes and emit English questions/answers. The space of possible (qt, at) is tremendously large and without pretraining most exchanges result in no information gain about the image. 2) Curriculum Learning. After supervised pretraining, we \u2018smoothly\u2019 transition the agents to RL training according to a curriculum. Specifically, we continue supervised training for the first K (say 9) rounds of dialog and transition to policy-gradient updates for the remaining 10 \u2212K rounds. We start at K = 9 and gradually anneal to 0. This curriculum ensures that the agent team does not suddenly diverge off policy, if one incorrect q or a is generated. Models are pretrained for 15 epochs on VisDial, after which we transition to policy-gradient training by annealing K down by 1 every epoch. All LSTMs are 2-layered with 512- d hidden states. We use Adam [14] with a learning rate of 10\u22123, and clamp gradients to [\u22125, 5] to avoid explosion. All our code will be made publicly available. There is no explicit state-dependent baseline in our training as we initialize from supervised pretraining and have zero-centered reward, which ensures a good proportion of random samples are both positively and negatively reinforced. Model Ablations. We compare to a few natural ablations of our full model, denoted RL-full-QAf. First, we evaluate the purely supervised agents (denoted SL-pretrained), i.e., trained only on VisDial data (no RL). Comparison to these agents establishes how much RL helps over super-\nvised learning. Second, we fix one of Q-BOT or A-BOT to the supervised pretrained initialization and train the other agent (and the regression network f ) with RL; we label these as Frozen-Q or Frozen-A respectively. Comparing to these partially frozen agents tell us the importance of coordinated communication. Finally, we freeze the regression network f to the supervised pretrained initialization while training Q-BOT and A-BOT with RL. This measures improvements from language adaptation alone. We quantify performance of these agents along two dimensions \u2013 how well they perform on the image guessing task (i.e. image retrieval) and how closely they emulate human dialogs (i.e. performance on VisDial dataset [4]). Evaluation: Guessing Game. To assess how well the agents have learned to cooperate at the image guessing task, we setup an image retrieval experiment based on the test split of VisDial v0.5 (\u223c9.5k images), which were never seen by the agents in RL training. We present each image + an automatically generated caption [12] to the agents, and allow them to communicate over 10 rounds of dialog. After each round, Q-BOT predicts a feature representation y\u0302t. We sort the entire test set in ascending distance to this prediction and compute the rank of the source image. Fig. 4a shows the mean percentile rank of the source image for our method and the baselines across the rounds (shaded region indicates standard error). A percentile rank of 95% means that the source image is closer to the prediction than 95% of the images in the set. Tab. 1 shows example exchanges between two humans (from VisDial), the SL-pretrained and the RL-full-QAf agents. We make a few observations:\n\u2022 RL improves image identification. We see that RL-full-QAf significantly outperforms SL-pretrained and all other ablations (e.g., at round 10, improving percentile rank by over 3%), indicating that our training framework is indeed effective at training these agents for image guessing.\n\u2022 All agents \u2018forget\u2019; RL agents forget less. One interesting trend we note in Fig. 4a is that all methods significantly improve from round 0 (caption-based retrieval) to rounds 2 or 3, but beyond that all methods with the exception of RL-full-QAf get worse, even though they have strictly more information. As shown in Tab. 1, agents will often get stuck in infinite repeating loops but this is much rarer for RL agents. Moreover, even when RL agents repeat themselves, it is after longer gaps (2-5 rounds). We conjecture that the goal of helping a partner over multiple rounds encourages longer term memory retention.\n\u2022 RL leads to more informative dialog. SL A-BOT tends to produce \u2018safe\u2019 generic responses (\u2018I don\u2019t know\u2019, \u2018I can\u2019t see\u2019) but RL A-BOT responses are\nmuch more detailed (\u2018It is hard to tell but I think it\u2019s black\u2019). These observations are consistent with recent literature in text-only dialog [17]. Our hypothesis for this improvement is that human responses are diverse and SL trained agents tend to \u2018hedge their bets\u2019 and achieve a reasonable log-likelihood by being non-\ncommittal. In contrast, such \u2018safe\u2019 responses do not help Q-BOT in picking the correct image, thus encouraging an informative RL A-BOT.\nEvaluation: Emulating Human Dialogs. To quantify how well the agents emulate human dialog, we evaluate A-BOT on the retrieval metrics proposed by Das et al. [4]. Specifi-\ncally, every question in VisDial is accompanied by 100 candidate responses. We use the log-likehood assigned by the A-BOT answer decoder to sort these candidates and report the results in Tab. 4b. We find that despite the RL A-BOT\u2019s answer being more informative, the improvements on VisDial metrics are minor. We believe this is because while the answers are correct, they may not necessarily mimic human responses (which is what the answer retrieval metrics check for). In order to dig deeper, we train a variant of Frozen-Q with a multi-task objective \u2013 simultaneous (1) ground truth answer supervision and (2) image guessing reward, to keep A-BOT close to human-like responses. We use a weight of 1.0 for the SL loss and 10.0 for RL. This model, denoted Frozen-Q-multi, performs better than all other approaches on VisDial answering metrics, improving the best reported result on VisDial by 0.7 mean rank (relative improvement of 3%). Note that this gain is entirely \u2018free\u2019 since no additional annotations were required for RL. Human Study. We conducted a human interpretability study to measure (1) whether humans can easily understand the Q-BOT-A-BOT dialog, and (2) how imagediscriminative the interactions are. We show human subjects a pool of 16 images, the agent dialog (10 rounds), and ask humans to pick their top-5 guesses for the image the two agents are talking about. We find that mean rank of the ground-truth image for SL-pretrained agent dialog is 3.70 vs. 2.73 for RL-full-QAf dialog. In terms of MRR, the comparison is 0.518 vs. 0.622 respectively. Thus, under both metrics, humans find it easier to guess the unseen image based on RL-full-QAf dialog exchanges, which shows that agents trained within our framework (1) successfully develop image-discriminative language, and (2) this language is interpretable; they do not deviate off English."}, {"heading": "7. Conclusions", "text": "To summarize, we introduce a novel training framework for visually-grounded dialog agents by posing a cooperative \u2018image guessing\u2019 game between two agents. We use deep reinforcement learning to learn the policies of these agents end-to-end \u2013 from pixels to multi-agent multi-round dialog to game reward. We demonstrate the power of this framework in a completely ungrounded synthetic world, where the agents communicate via symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol without any human supervision. We go on to instantiate this game on the VisDial [4] dataset, where we pretrain with supervised dialog data. We find that the RL \u2018fine-tuned\u2019 agents not only significantly outperform SL agents, but learn to play to each other\u2019s strengths, all the while remaining interpretable to outside humans observers.\nAcknowledgements. We thank Devi Parikh for helpful discussions. This work was funded in part by the following\nawards to DB \u2013 NSF CAREER award, ONR YIP award, ONR Grant N00014-14-1-0679, ARO YIP award, ICTAS Junior Faculty award, Google Faculty Research Award, Amazon Academic Research Award, AWS Cloud Credits for Research, and NVIDIA GPU donations. SK was supported by ONR Grant N00014-12-1-0903, and SL was partially supported by the Bradley Postdoctoral Fellowship. Views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor."}], "references": [{"title": "VQA: Visual Question Answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "ICCV,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "VizWiz: Nearly Real-time Answers to Visual Questions", "author": ["J.P. Bigham", "C. Jayant", "H. Ji", "G. Little", "A. Miller", "R.C. Miller", "R. Miller", "A. Tatarowicz", "B. White", "S. White", "T. Yeh"], "venue": "UIST,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Mind\u2019s Eye: A Recurrent Visual Representation for Image Caption Generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "CVPR,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual Dialog", "author": ["A. Das", "S. Kottur", "K. Gupta", "A. Singh", "D. Yadav", "J.M. Moura", "D. Parikh", "D. Batra"], "venue": "CVPR,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2017}, {"title": "GuessWhat?! visual object discovery through multi-modal dialogue", "author": ["H. de Vries", "F. Strub", "S. Chandar", "O. Pietquin", "H. Larochelle", "A. Courville"], "venue": "In CVPR, 2017", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2017}, {"title": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "From Captions to Visual Concepts and Back", "author": ["H. Fang", "S. Gupta", "F.N. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "G. Zweig"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "NIPS,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative Adversarial Nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Emergence of language with multiagent games: Learning to communicate with sequences of symbols", "author": ["S. Havrylov", "I. Titov"], "venue": "ICLR Workshop,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "DenseCap: Fully Convolutional Localization Networks for Dense Captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "ReferItGame: Referring to Objects in Photographs of Natural Scenes", "author": ["S. Kazemzadeh", "V. Ordonez", "M. Matten", "T.L. Berg"], "venue": "EMNLP,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-agent cooperation and the emergence of (natural) language", "author": ["A. Lazaridou", "A. Peysakhovich", "M. Baroni"], "venue": "10  ICLR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2017}, {"title": "Convention: A philosophical study", "author": ["D. Lewis"], "venue": "John Wiley & Sons,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep Reinforcement Learning for Dialogue Generation", "author": ["J. Li", "W. Monroe", "A. Ritter", "M. Galley", "J. Gao", "D. Jurafsky"], "venue": "EMNLP,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarial learning for neural dialogue generation", "author": ["J. Li", "W. Monroe", "T. Shi", "A. Ritter", "D. Jurafsky"], "venue": "arXiv preprint arXiv:1701.06547,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "ICCV,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Emergence of grounded compositional language in multi-agent populations", "author": ["I. Mordatch", "P. Abbeel"], "venue": "arXiv preprint arXiv:1703.04908,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "Evolution of Communication and Language in Embodied Agents", "author": ["S. Nolfi", "M. Mirolli"], "venue": "Springer Publishing Company, Incorporated, 1st edition,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Exploring Models and Data for Image Question Answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "NIPS,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models", "author": ["I.V. Serban", "A. Sordoni", "Y. Bengio", "A. Courville", "J. Pineau"], "venue": "AAAI,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues", "author": ["I.V. Serban", "A. Sordoni", "R. Lowe", "L. Charlin", "J. Pineau", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1605.06069,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Mastering the game of go with deep neural networks and tree search. Nature, 2016", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot", "S. Dieleman", "D. Grewe", "J. Nham", "N. Kalchbrenner", "I. Sutskever", "T. Lillicrap", "M. Leach", "K. Kavukcuoglu", "T. Graepel", "D. Hassabis"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "MovieQA: Understanding Stories in Movies through Question-Answering", "author": ["M. Tapaswi", "Y. Zhu", "R. Stiefelhagen", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "CVPR,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint Video and Text Parsing for Understanding Events and Answering Queries", "author": ["K. Tu", "M. Meng", "M.W. Lee", "T.E. Choe", "S.C. Zhu"], "venue": "IEEE MultiMedia,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to Sequence - Video to Text", "author": ["S. Venugopalan", "M. Rohrbach", "J. Donahue", "R.J. Mooney", "T. Darrell", "K. Saenko"], "venue": "ICCV,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Translating Videos to Natural Language Using Deep Recurrent Neural Networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R.J. Mooney", "K. Saenko"], "venue": "NAACL HLT,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning, 8(3-4):229\u2013256,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1992}, {"title": "Using artificial intelligence to help blind people \u2018see\u2019 facebook", "author": ["S. Wu", "H. Pique", "J. Wieland"], "venue": "http://newsroom.fb.com/news/2016/04/using-artificialintelligence-to-help-blind-people-see-facebook/,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "Second, we conduct large-scale real-image experiments on the VisDial dataset [4], where we pretrain with supervised dialog data and show that the RL \u2018fine-tuned\u2019 agents significantly outperform SL agents.", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": ", helping visually impaired users understand their surroundings [2] or social media content [35] (\u2018Who is in the photo? Dave.", "startOffset": 64, "endOffset": 67}, {"referenceID": 34, "context": ", helping visually impaired users understand their surroundings [2] or social media content [35] (\u2018Who is in the photo? Dave.", "startOffset": 92, "endOffset": 96}, {"referenceID": 2, "context": "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31\u2013 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.", "startOffset": 108, "endOffset": 127}, {"referenceID": 10, "context": "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31\u2013 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.", "startOffset": 108, "endOffset": 127}, {"referenceID": 32, "context": "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31\u2013 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.", "startOffset": 108, "endOffset": 127}, {"referenceID": 35, "context": "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31\u2013 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.", "startOffset": 108, "endOffset": 127}, {"referenceID": 0, "context": "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31\u2013 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.", "startOffset": 151, "endOffset": 170}, {"referenceID": 19, "context": "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31\u2013 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.", "startOffset": 151, "endOffset": 170}, {"referenceID": 22, "context": "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31\u2013 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.", "startOffset": 151, "endOffset": 170}, {"referenceID": 28, "context": "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31\u2013 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.", "startOffset": 151, "endOffset": 170}, {"referenceID": 29, "context": "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31\u2013 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.", "startOffset": 151, "endOffset": 170}, {"referenceID": 3, "context": "Two recent works [4, 5] have proposed studying this task of visually-grounded dialog.", "startOffset": 17, "endOffset": 23}, {"referenceID": 4, "context": "Two recent works [4, 5] have proposed studying this task of visually-grounded dialog.", "startOffset": 17, "endOffset": 23}, {"referenceID": 3, "context": "works [4, 5] first collect a dataset of human-human dialog, i.", "startOffset": 6, "endOffset": 12}, {"referenceID": 4, "context": "works [4, 5] first collect a dataset of human-human dialog, i.", "startOffset": 6, "endOffset": 12}, {"referenceID": 0, "context": "Despite significant popular interest in VQA (over 200 works citing [1] since 2015), all previous approaches have been based on supervised learning, making this the first instance of goaldriven training for visual question answering / dialog.", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "Second, we conduct large-scale real-image experiments on the VisDial dataset [4].", "startOffset": 77, "endOffset": 80}, {"referenceID": 5, "context": ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].", "startOffset": 19, "endOffset": 33}, {"referenceID": 6, "context": ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].", "startOffset": 19, "endOffset": 33}, {"referenceID": 11, "context": ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].", "startOffset": 19, "endOffset": 33}, {"referenceID": 32, "context": ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].", "startOffset": 19, "endOffset": 33}, {"referenceID": 0, "context": ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].", "startOffset": 71, "endOffset": 89}, {"referenceID": 7, "context": ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].", "startOffset": 71, "endOffset": 89}, {"referenceID": 18, "context": ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].", "startOffset": 71, "endOffset": 89}, {"referenceID": 19, "context": ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].", "startOffset": 71, "endOffset": 89}, {"referenceID": 22, "context": ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].", "startOffset": 71, "endOffset": 89}, {"referenceID": 3, "context": "Most related to this paper are two recent works on visually-grounded dialog [4, 5].", "startOffset": 76, "endOffset": 82}, {"referenceID": 4, "context": "Most related to this paper are two recent works on visually-grounded dialog [4, 5].", "startOffset": 76, "endOffset": 82}, {"referenceID": 3, "context": "[4] proposed the task of Visual Dialog, collected the VisDial dataset by pairing two subjects on Amazon Mechanical Turk to chat about an image (with assigned roles of \u2018Questioner\u2019 and \u2018Answerer\u2019), and trained neural visual dialog answering models.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] extended the Referit game [13] to a \u2018GuessWhat\u2019 game, where one person asks questions about an image to guess which object has been \u2018selected\u2019, and the second person answers questions in \u2018yes\u2019/\u2018no\u2019/NA (natural language answers are disallowed).", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[5] extended the Referit game [13] to a \u2018GuessWhat\u2019 game, where one person asks questions about an image to guess which object has been \u2018selected\u2019, and the second person answers questions in \u2018yes\u2019/\u2018no\u2019/NA (natural language answers are disallowed).", "startOffset": 30, "endOffset": 34}, {"referenceID": 25, "context": "In our work, we take inspiration from the AlphaGo [26] approach of supervision from human-expert games and reinforcement learning from self-play.", "startOffset": 50, "endOffset": 54}, {"referenceID": 15, "context": "More formally, it is a generalization of the Lewis Signaling (LS) [16] game, widely studied in economics and game theory.", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "[17] have proposed using RL for training dialog systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "In contrast, taking a cue from adversarial learning [9, 18], we set up a cooperative game between two agents, such that we do not need to hand-define what a \u2018good\u2019 dialog looks like \u2013 a \u2018good\u2019 dialog is one that leads to a successful image-guessing play.", "startOffset": 52, "endOffset": 59}, {"referenceID": 17, "context": "In contrast, taking a cue from adversarial learning [9, 18], we set up a cooperative game between two agents, such that we do not need to hand-define what a \u2018good\u2019 dialog looks like \u2013 a \u2018good\u2019 dialog is one that leads to a successful image-guessing play.", "startOffset": 52, "endOffset": 59}, {"referenceID": 21, "context": "There is a long history of work on language emergence in multi-agent systems [22].", "startOffset": 77, "endOffset": 81}, {"referenceID": 9, "context": "The more recent resurgence has focused on deep RL [10,15,21].", "startOffset": 50, "endOffset": 60}, {"referenceID": 14, "context": "The more recent resurgence has focused on deep RL [10,15,21].", "startOffset": 50, "endOffset": 60}, {"referenceID": 20, "context": "The more recent resurgence has focused on deep RL [10,15,21].", "startOffset": 50, "endOffset": 60}, {"referenceID": 3, "context": "For our large-scale real-image results, we do not want our bots to invent their own uninterpretable language and use pretraining on VisDial [4] to achieve \u2018alignment\u2019 with English.", "startOffset": 140, "endOffset": 143}, {"referenceID": 3, "context": "Both the agent policies are modeled via Hierarchical Recurrent Encoder-Decoder neural networks, which have recently been proposed for dialog modeling [4, 24, 25].", "startOffset": 150, "endOffset": 161}, {"referenceID": 23, "context": "Both the agent policies are modeled via Hierarchical Recurrent Encoder-Decoder neural networks, which have recently been proposed for dialog modeling [4, 24, 25].", "startOffset": 150, "endOffset": 161}, {"referenceID": 24, "context": "Both the agent policies are modeled via Hierarchical Recurrent Encoder-Decoder neural networks, which have recently been proposed for dialog modeling [4, 24, 25].", "startOffset": 150, "endOffset": 161}, {"referenceID": 26, "context": "- State/History Encoder is an LSTM that takes as input at each round t \u2013 the encoded question Qt , the image features from VGG [27] y, and the previous fact encoding F t\u22121 \u2013 to produce a state encoding, i.", "startOffset": 127, "endOffset": 131}, {"referenceID": 33, "context": "In order to train these agents, we use the REINFORCE [34] algorithm that updates policy parameters (\u03b8Q, \u03b8A, \u03b8f ) in response to experienced rewards.", "startOffset": 53, "endOffset": 57}, {"referenceID": 27, "context": "During training, we use -greedy policies [28], ensuring an action probability of 0.", "startOffset": 41, "endOffset": 45}, {"referenceID": 3, "context": "Image + Caption Human-Human dialog [4] SL-pretrained Q-BOT-A-BOT dialog RL-full-QAf Q-BOT A-BOT dialog", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "We leverage the recently introduced VisDial dataset [4] that contains (as of the publicly released v0.", "startOffset": 52, "endOffset": 55}, {"referenceID": 26, "context": "3 \u2013 specifically at each round t, Q-BOT needs to regress to the vector embedding \u0177t of image I corresponding to the fc7 (penultimate fully-connected layer) output from VGG16 [27].", "startOffset": 174, "endOffset": 178}, {"referenceID": 3, "context": "We first train both agents in a supervised manner on the train split of VisDial [4] v0.", "startOffset": 80, "endOffset": 83}, {"referenceID": 13, "context": "We use Adam [14] with a learning rate of 10\u22123, and clamp gradients to [\u22125, 5] to avoid explosion.", "startOffset": 12, "endOffset": 16}, {"referenceID": 3, "context": "performance on VisDial dataset [4]).", "startOffset": 31, "endOffset": 34}, {"referenceID": 11, "context": "We present each image + an automatically generated caption [12] to the agents, and allow them to communicate over 10 rounds of dialog.", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "These observations are consistent with recent literature in text-only dialog [17].", "startOffset": 77, "endOffset": 81}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "We go on to instantiate this game on the VisDial [4] dataset, where we pretrain with supervised dialog data.", "startOffset": 49, "endOffset": 52}], "year": 2017, "abstractText": "We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative \u2018image guessing\u2019 game between two agents \u2013 Q-BOT and A-BOT\u2013 who communicate in natural language dialog so that Q-BOT can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end \u2013 from pixels to multi-agent multi-round dialog to game reward. We demonstrate two experimental results. First, as a \u2018sanity check\u2019 demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/size). Thus, we demonstrate the emergence of grounded language and communication among \u2018visual\u2019 dialog agents with no human supervision at all. Second, we conduct large-scale real-image experiments on the VisDial dataset [4], where we pretrain with supervised dialog data and show that the RL \u2018fine-tuned\u2019 agents significantly outperform SL agents. Interestingly, the RL Q-BOT learns to ask questions that A-BOT is good at, ultimately resulting in more informative dialog and a better team.", "creator": "LaTeX with hyperref package"}}}