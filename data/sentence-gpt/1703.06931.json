{"id": "1703.06931", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "Learning Correspondence Structures for Person Re-identification", "abstract": "This paper addresses the problem of handling spatial misalignments due to camera-view changes or human-pose variations in person re-identification. We first introduce a boosting-based approach to learn a correspondence structure which indicates the patch-wise matching probabilities between images from a target camera pair. The learned correspondence structure can not only capture the spatial correspondence pattern between cameras but also handle the viewpoint or human-pose variation in individual images, as well as the other possible factors of the spatial misalignment.\n\n\n\n\nThe following results are from a survey of 2,000 users who were unable to read their responses on the web. For each of these results, we conducted two surveys. One of these tests is based on several questions of interest:\n\u201c What are you writing about?\u201c What are you writing about?\u201c What are you writing about?\u201c What are you writing about?\u201c Are you writing about?\u201c Are you writing about?\u201c Are you writing about?\u201c\n\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c\u201c", "histories": [["v1", "Mon, 20 Mar 2017 19:17:14 GMT  (9268kb,D)", "https://arxiv.org/abs/1703.06931v1", "accepted by IEEE Trans. Image Processing. arXiv admin note: text overlap witharXiv:1504.06243"], ["v2", "Wed, 26 Apr 2017 12:31:28 GMT  (9268kb,D)", "http://arxiv.org/abs/1703.06931v2", "IEEE Trans. Image Processing, vol. 26, no. 5, pp. 2438-2453, 2017. The project page for this paper is available atthis http URLarXiv admin note: text overlap witharXiv:1504.06243"], ["v3", "Thu, 27 Apr 2017 16:15:30 GMT  (9268kb,D)", "http://arxiv.org/abs/1703.06931v3", "IEEE Trans. Image Processing, vol. 26, no. 5, pp. 2438-2453, 2017. The project page for this paper is available atthis http URLarXiv admin note: text overlap witharXiv:1504.06243"]], "COMMENTS": "accepted by IEEE Trans. Image Processing. arXiv admin note: text overlap witharXiv:1504.06243", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.MM", "authors": ["weiyao lin", "yang shen", "junchi yan", "mingliang xu", "jianxin wu", "jingdong wang", "ke lu"], "accepted": false, "id": "1703.06931"}, "pdf": {"name": "1703.06931.pdf", "metadata": {"source": "CRF", "title": "Learning Correspondence Structures for Person Re-identification", "authors": ["Weiyao Lin", "Yang Shen", "Junchi Yan", "Mingliang Xu", "Jianxin Wu", "Jingdong Wang", "Ke Lu"], "emails": ["shenyang1715}@sjtu.edu.cn).", "jcyan@sei.ecnu.edu.cn).", "iexumingliang@zzu.edu.cn)", "wujx2001@nju.edu.cn).", "jingdw@microsoft.com).", "luk@ucas.ac.cn)."], "sections": [{"heading": null, "text": "Index Terms\u2014Person Re-identification, Correspondence Structure Learning, Spatial Misalignment\nI. INTRODUCTION\nPerson re-identification (Re-ID) is of increasing importance in visual surveillance. The goal of person Re-ID is to identify a specific person indicated by a probe image from a set of gallery images captured from cross-view cameras (i.e., cameras that are non-overlapping and different from the probe image\u2019s camera) [2], [3], [4]. It remains challenging due to the large appearance changes in different camera views and the interferences from background or object occlusion.\nOne major challenge for person Re-ID is the uncontrolled spatial misalignment between images due to camera-view changes or human-pose variations. For example, in Fig. 1a, the green patch located in the lower part in camera A\u2019s image\nThe basic idea of this paper appeared in our conference version [1]. In this version, we extend our approach by introducing a multi-structure strategy, carry out detailed analysis, and present more performance results.\nW. Lin and Y. Shen are with the Department of Electronic Engineering, Shanghai Jiao Tong University, China (email: {wylin, shenyang1715}@sjtu.edu.cn).\nJ. Yan is with Software Engineering Institute, East China Normal University, Shanghai, China (email: jcyan@sei.ecnu.edu.cn).\nM. Xu is with the School of Information Engineering, Zhengzhou University, China (email: iexumingliang@zzu.edu.cn)\nJ. Wu is with the National Key Laboratory for Novel Software Technology, Nanjing University, China (email: wujx2001@nju.edu.cn).\nJ. Wang is with Microsoft Research, Beijing, China (email: jingdw@microsoft.com).\nK. Lu is with the College of Computer and Information Technology, China Three Gorges University, Yichang, China, also with the University of Chinese Academy of Sciences, Beijing, China (email: luk@ucas.ac.cn).\ncorresponds to patches from the upper part in camera B\u2019s image. However, most existing works [2], [3], [4], [5], [6], [7], [8], [9], [10], [11] focus on handling the overall appearance variations between images, while the spatial misalignment among images\u2019 local patches is not addressed. Although some patch-based methods [12], [13], [14], [15] address the spatial misalignment problem by decomposing images into patches and performing an online patch-level matching, their performances are often restrained by the online matching process which is easily affected by the mismatched patches due to similar appearance or occlusion.\nIn this paper, we argue that due to the stable setting of most cameras (e.g., fixed camera angle or location), each camera has a stable constraint on the spatial configuration of its captured images. For example, images in Fig. 1a and 1b are obtained from the same camera pair: A and B. Due to the constraint from camera angle difference, body parts in camera A\u2019s images are located at lower places than those in camera B, implying a lower-to-upper correspondence pattern between them. Meanwhile, constraints from camera locations can also be observed. Camera A (which monitors an exit region) includes more side-view images, while camera B (monitoring a road) shows more front or back-view images. This further results in a high probability of side-to-front/back correspondence pattern.\nBased on this intuition, we propose to learn a correspondence structure (i.e., a matrix including all patch-wise matching probabilities between a camera pair, as Fig. 1c) to encode the spatial correspondence pattern constrained by a camera pair, and utilize it to guide the patch matching and matching score calculation processes between images. With this correspondence structure, spatial misalignments can be\nar X\niv :1\n70 3.\n06 93\n1v 3\n[ cs\n.C V\n] 2\n7 A\npr 2\n01 7\n2 suitably handled and patch matching results are less interfered by the confusion from appearance or occlusion. In order to model human-pose variations or local viewpoint changes inside a camera view, the correspondence structure for each patch is described by a one-to-many graph whose weights indicate the matching probabilities between patches, as in Fig. 1. Besides, a global constraint is also integrated during the patch matching process, so as to achieve a more reliable matching score between images.\nMoreover, since people often show different poses inside a camera view, the spatial correspondence pattern between a camera pair may be further divided and modeled by a set of sub-patterns according to these pose variations (e.g., Fig. 1 can be divided into a left-to-front correspondence subpattern (a) and a right-to-back correspondence sub-pattern (b)). Therefore, we further extend our approach by introducing a set of local correspondence structures to capture various correspondence sub-patterns between a camera pair. In this way, the spatial misalignments between individual images can be modeled and handled in a more precise way.\nIn summary, our contributions to Re-ID are four folds. 1) We introduce a correspondence structure to encode\ncross-view correspondence pattern between cameras, and develop a global constraint-based matching process by combining a global constraint with the correspondence structure to exclude spatial misalignments between images. These two components establish a novel framework for addressing the Re-ID problem. 2) Under this framework, we propose a boosting-based approach to learn a suitable correspondence structure between a camera pair. The learned correspondence structure can not only capture the spatial correspondence pattern between cameras but also handle the viewpoint or human-pose variation in individual images. 3) We further extend our approach by introducing a multistructure scheme, which first learns a set of local correspondence structures to capture the spatial correspondence sub-patterns between a camera pair, and then adaptively selects suitable correspondence structures to handle the spatial misalignments between individual images. Thus, image-wise spatial misalignments can be modeled and excluded in a more precise way. 4) We release a new and challenging benchmark ROAD DATASET for person Re-ID which includes large variation of human pose and camera angle.\nThe rest of this paper is organized as follows. Section II reviews related works. Section III describes the framework of the proposed approach. Sections IV to V describe the details of our proposed global constraint-based matching process and boosting-based learning approach, respectively. Section VI describes the details of extending our approach with multistructure scheme. Section VII shows the experimental results and Section VIII concludes the paper."}, {"heading": "II. RELATED WORKS", "text": "Many person re-identification methods have been proposed. Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or\nfinding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19]. Recently, due to the effectiveness of deep neural networks in learning discriminative features, many deep Re-ID methods are proposed which utilize deep neural networks to enhance the reliability of feature representations or similarity metrics [20], [21], [22], [23], [24], [25], [26], [27]. Since most of these works do not effectively model the spatial misalignment among local patches inside images, their performances are still impacted by the interferences from viewpoint or human-pose changes.\nIn order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments. In [28], [30], [32], a human body in an image is first parsed into semantic parts (e.g., head and torso) or patch clusters. Then, similarity matching is performed between the corresponding semantic parts or patch clusters. Since these methods are highly dependent on the accuracy of body parser or patch clustering, they have limitations in scenarios where the body parser does not work reliably or the patch clustering results are less satisfactory.\nIn [12], Oreifej et al. divide images into patches according to appearance consistencies and utilize the Earth Movers Distance (EMD) to measure the overall similarity among the extracted patches. However, since the spatial correlation among patches are ignored during similarity calculation, the method is easily affected by the mismatched patches with similar appearance. Although Ma et al. [13] introduce a body prior constraint to avoid mismatching between distant patches, the problem is still not well addressed, especially for the mismatching between closely located patches.\nTo reduce the effect of patch-wise mismatching, some saliency-based approaches [14], [29], [33], [34] are recently proposed, which estimate the saliency distribution relationship between images and utilize it to control the patch-wise matching process. Following the similar line, Bak and Carr [33] introduce a deformable model to obtain a set of weights to guide the patch matching process. Bak et al. [34] further introduce a hand-crafted Epanechnikov kernel to determine the weight distribution of patches. Although these methods consider the correspondence constraint between patches, our approach differs from them in: (1) our approach focuses on constructing a correspondence structure where patch-wise matching parameters are jointly decided by both matched patches. Comparatively, the matching weights in most of the saliency-based approaches [29], [33] are only controlled by patches in the probe-image (probe patch). (2) Our approach models patch-wise correspondence by a one-to-many graph such that each probe patch will trigger multiple matches during the patch matching process. In contrast, the saliencybased approaches only select one best-matched patch for each probe patch. (3) Our approach introduces a global constraint to control the patch-wise matching result while the patch matching result in saliency-based methods is locally decided by choosing the best matched within a neighborhood set.\nBesides patch-based approaches, some other methods are\n3\nalso proposed which aim to simultaneously find discriminative features and handle the spatial misalignment problem. In [35], the correspondence relationship between images is captured by a hierarchical matching process integrated in an end-to-end deep learning framework. In [36], [37], image-wise similarities are measured by kernelized visual word co-occurrence matrices where a latent spatial kernel is integrated to handle spatial displacements. Since these methods give more weights on small displacements and less weights on large displacements, they are more suitable to handle spatial misalignments with relatively small displacements and may have limitations for very large displacements."}, {"heading": "III. OVERVIEW OF THE APPROACH", "text": "The framework of our approach is shown in Fig. 2. During the training process, which is detailed in Section V, we present a boosting-based process to learn the correspondence structure between the target camera pair. During the prediction stage, which is detailed in Section IV, given a probe image and a set of gallery images, we use the correspondence structure to evaluate the patch correlations between the probe image and each gallery image, and find the optimal one-to-one mapping between patches, and accordingly the matching score. The ReID result is achieved by ranking gallery images according to their matching scores."}, {"heading": "IV. PERSON RE-ID VIA CORRESPONDENCE STRUCTURE", "text": "This section introduces the concept of correspondence structure, show the scheme of computing the patch correlation using the correspondence structure, and finally present the patchwise mapping method to compute the matching score between the probe image and a gallery image."}, {"heading": "A. Correspondence structure", "text": "The correspondence structure, \u0398A,B , encodes the spatial correspondence distribution between a pair of cameras, A and B. In our problem, we adopt a discrete distribution, which is a set of patch-wise matching probabilities, \u0398A,B = {Pi,B}NAi=1, where NA is the number of patches of an image in camera A. Pi,B = {Pi1, Pi2, . . . , PiNB} describes the correspondence distribution in an image from camera B for the ith patch xi of an image captured from camera A, where NB is the number of patches of an image in B. The illustrations of\nthe correspondence distribution are shown on the top-right of Fig. 2.\nThe definition of the matching probabilities in the correspondence structure only depends on a camera pair and are independent of specific images. In the correspondence structure, it is possible that one patch in camera A is highly correlated to multiple patches in camera B, so as to handle human-pose variations and local viewpoint changes."}, {"heading": "B. Patch correlation", "text": "Given a probe image U in camera A and a gallery image V in camera B, the patch-wise correlation between U and V , C(xi, yj) (xi \u2208 U , yj \u2208 V ), is computed from both the correspondence structure between two cameras and the visual features:\nC(xi, yj) = \u03bbTc(Pij) \u00b7 log \u03a6(fxi , fyj ;xi, yj). (1)\nHere xi and yj are ith and jth patch in images U and V ; fxi and fyj are the feature vectors for xi and yj , respectively. Pij is the correspondence structure between U and V . \u03bbTc(Pij) = 1, if Pij > Tc, and 0 otherwise, and Tc = 0.05 is a threshold. \u03a6(fxi , fyj ;xi, yj) is the correspondence-structure-controlled similarity between xi and yj ,\n\u03a6(fxi , fyj ;xi, yj) = \u03a6ij(fxi , fyj )Pij , (2)\nwhere \u03a6ij(fxi , fyj ) is the similarity between xi and yj . The correspondence structure Pij in Equations 1 and 2 is used to adjust the appearance similarity \u03a6ij(fxi , fyj ) such that a more reliable patch-wise correlation strength can be achieved. The thresholding term \u03bbTc(Pij) is introduced to exclude the patch-wise correlation with a low correspondence probability, which effectively reduces the interferences from mismatched patches with similar appearance.\nThe patch-wise appearance similarity \u03a6ij(fxi , fyj ) in Eq. 2 can be achieved by many off-the-shelf methods [14], [29], [38]. In this paper, we extract Dense SIFT and Dense Color Histogram [14] from each patch and utilize the KISSME metric [7] as the basic distance metric to compute \u03a6ij(fxi , fyj ). Note that in order to achieve better performance [33], we learn multiple metrics so as to adaptively model the local patchwise correlations at different locations. Specifically, we train an independent KISSME metric \u03a6ij for each set of patch-pair locations {xi, yj}. Besides, it should also be noted that both the feature extraction and distance metric learning parts can be replaced by other state-of-the-art methods. For example, in the experiments in Section VII, we also show the results by replacing KISSME with the kLFDA [8] and KMFA-R\u03c72 [39] metrics to demonstrate the effectiveness of our approach."}, {"heading": "C. Patch-wise mapping", "text": "After obtaining the alignment-enhanced correlation strength C(xi, yj), we can find a best-matched patch in image V for each patch in U and herein calculate the final image matching score. To compute C(xi, yj) of testing image pairs, we only consider the potential matching patches within a searching range R(xi) around the chosen probe patch xi. That is, R(xi) = {yj |d(yi, yj) < Td}, where yi is xi\u2019s co-located\n4 (a) (b)\nFigure 3. Patch matching result (a) by locally finding the largest correlation strength C(xi, yj) for each patch and (b) by using a global constraint. The red dashed lines indicate the final patch matching results and the colored solid lines are the matching probabilities in the correspondence structure. (Best viewed in color)\npatch in camera B, d(yi, yj) is the distance between patches yi and yj (cf. Eq. 6), and Td = 32 in this paper. However, locally finding the largest C(xi, yj) may still create mismatches among patch pairs with high matching probabilities. For example, Fig. 3a shows an image pair U and V containing different people. When locally searching for the largest C(xi, yj), the yellow patch in U will be mismatched to the bold-green patch in V since they have both large appearance similarity and high matching probability. This mismatch unsuitably increases the matching score between U and V .\nTo address this problem, we introduce a global one-to-one mapping constraint and solve the resulting linear assignment task [40] to find the best matching:\n\u2126\u2217U,V = arg max \u2126U,V \u2211 {xi,yj}\u2208\u2126U,V C(xi, yj) (3)\ns.t. xi 6= xs, yj 6= yt \u2200 {xi, yj}, {xs, yt} \u2208 \u2126U,V\nwhere \u2126\u2217U,V is the set of the best patch matching result between images U and V . {xi, yj} and {xs, yt} are two matched patch pairs in \u2126. According to Eq. 3, we want to find the best patch matching result \u2126\u2217U,V that maximizes the total image matching score\n\u03c8U,V = \u2211\n{xi,yj}\u2208\u2126U,V\nC(xi, yj), (4)\ngiven that each patch in U can only be matched to one patch in V and vice versa.\nEq. 3 can be solved by the Hungarian method [40]. Fig. 3b shows an example of the patch matching result by Eq. 3. From Fig. 3b, it is clear that by the inclusion of a global constraint, local mismatches can be effectively reduced and a more reliable image matching score can be achieved. Based on the above process, we can calculate the image matching scores \u03c8 between a probe image and all gallery images in a cross-view camera, and rank the gallery images accordingly to achieve the final Re-ID result [13]."}, {"heading": "V. CORRESPONDENCE STRUCTURE LEARNING", "text": ""}, {"heading": "A. Objective function", "text": "Given a set of probe images {U\u03b1} from camera A and their corresponding cross-view images {V\u03b2} from camera B in the training set, we learn the optimal correspondence structure\n\u0398\u2217A,B between cameras A and B so that the correct match image is ranked before the incorrect match images in terms of the matching scores. The formulation is:\nmin \u0398A,B \u2211 U\u03b1 R(V\u03b1\u2032 ;\u03c8U\u03b1,V\u03b1\u2032 (\u0398A,B),\u03a8U\u03b1,V\u03b2 6=\u03b1\u2032 (\u0398A,B)), (5)\nin which V\u03b1\u2032 is the correct match gallery image of the probe image U\u03b1. \u03c8U\u03b1,V\u03b1\u2032 (\u0398A,B) (as computed from Eq. 4) is the matching score between U\u03b1 and V\u03b1\u2032 and \u03a8U\u03b1,V\u03b2 6=\u03b1\u2032 (\u0398A,B) is the set of matching scores of all incorrect matches. R(V\u03b1\u2032 ;\u03c8U\u03b1,V\u03b1\u2032 (\u0398A,B),\u03a8U\u03b1,V\u03b2 6=\u03b1\u2032 (\u0398A,B)) is the rank of V\u03b1\u2032 among all the gallery images according to the matching scores. Intuitively, the penalty is the smallest if the rank is 1, i.e., the matching score of V\u03b1\u2032 is the greatest.\nAccording to Eq. 5, the correspondence structure that can achieve the best Re-ID result on the training set will be selected as the optimal correspondence structure. However, the optimization is not easy as the matching score (Eq. 4) is complicated. We present an approximate solution, a boostingbased process, to solve this problem, which utilizes an iterative update process to gradually search for better correspondence structures fitting Eq. 5."}, {"heading": "B. Boosting-based learning", "text": "The boosting-based approach utilizes a progressive way to find the best correspondence structure with the help of binary mapping structures. A binary mapping structure is similar to the correspondence structure except that it simply uses 0 or 1 instead of matching probabilities to indicate the connectivity or linkage between patches, cf. Fig. 4a. It can be viewed as a simplified version of the correspondence structure which includes rough information about the crossview correspondence pattern.\nSince binary mapping structures only include simple connectivity information among patches, their optimal solutions are tractable for individual probe images. Therefore, by searching for the optimal binary mapping structures for different probe images and using them to progressively update the correspondence structure, suitable cross-view correspondence patterns can be achieved.\nThe entire boosting-based learning process can be describe by the following steps, which are summarized in Algorithm 1.\nFinding the optimal binary mapping structure. For each training probe image U\u03b1, we aim to find the optimal binary mapping structure M\u03b1 such that the rank order of U\u03b1\u2019s correct match image V\u03b1\u2032 is minimized under M\u03b1. In order to reduce the computation complexity of finding M\u03b1, we introduce an approximate strategy. Specifically, we first create multiple candidate binary mapping structures under different searching ranges (from 26 to 32) by adjacency-constrained search [14], and then select the one which minimizes the rank order of V\u03b1\u2032 and use it as the approximated optimal binary mapping structure M\u03b1. Note that we find one optimal binary mapping structure for each probe image such that the obtained binary mapping structures can include local crossview correspondence clues in different training samples.\n5 (a) (b) (c)\nFigure 4. (a): An example of binary mapping structure (the red lines with weight 1 indicate that the corresponding patches are connected). (b)-(c): Examples of the correspondence structures learned by our approach for the VIPeR [41] data. The line widths in (b)-(c) are proportional to the patch-wise probability values. (Best viewed in color)\nCorrespondence Structure Initialization. In this paper, patch-wise matching probabilities Pij in the correspondence structure are initialized by:\nP 0ij \u221d  0, if d(yi, yj) \u2265 Td 1\nd(yi, yj) + 1 , otherwise\n, (6)\nwhere yi is xi\u2019s co-located patch in camera B, such as the two blue patches in Fig. 5g. d(yi, yj) is the distance between patches yi and yj which is used to measure the distance between patches xi and yj from two different images. The distance d(yi, yj) is defined as the number of strides to move from yi to yj (i.e., the `1 distance). We use d(yi, yj) + 1 to avoid dividing by zero. Td is a threshold which is set to be 32 in this paper. According to Eq. 6, a patch pair whose patches are located close to each other (i.e., small d(yi, yj)) will be initialized with a large P 0ij value. On the contrary, patch pairs with large patch-wise distances will be initialized with small P 0ij values. Moreover, if patches in a patch pair have extremely large distance (i.e., d(yi, yj) \u2265 Td), we will simply set their corresponding matching probability to be 0.\nBinary mapping structure selection. During each iteration k in the learning process, we first apply correspondence structure \u0398k\u22121A,B = {P k\u22121 ij } from the previous iteration to calculate the rank orders of all correct match images V\u03b1\u2032 in the training set. Then, we randomly select 20 V\u03b1\u2032 where half of them are ranked among top 50% (implying better ReID results) and another half are ranked among the last 50% (implying worse Re-ID results). Finally, we extract binary mapping structures corresponding to these selected images and use them to update and boost the correspondence structure.\nNote that we select binary mapping structures for both high- and low-ranked images in order to include a variety of local patch-wise correspondence patterns. In this way, the final obtained correspondence structure can suitably handle the variations in human-pose or local viewpoints.\nCalculating the updated matching probability. With the introduction of the binary mapping structure M\u03b1, we can model the updated matching probability in the correspondence structure by combining the estimated matching probabilities from multiple binary mapping structures:\nP\u0302 kij = \u2211\nM\u03b1\u2208\u0393k P\u0302 (xi, yj |M\u03b1) \u00b7 P (M\u03b1) , (7)\nwhere P\u0302 kij is the updated matching probability between patches xi and yj in the k-th iteration. P\u0302 (xi, yj |M\u03b1) is the estimated matching probability between xi and yj when including the local correspondence pattern information of binary mapping structure M\u03b1. \u0393k is the set of binary mapping structures selected in the k-th iteration. P (M\u03b1) =\nR\u0303n(M\u03b1)\u2211 M\u03b3\u2208\u0393k R\u0303n(M\u03b3) is\nthe prior probability for binary mapping structure M\u03b1, where R\u0303n(M\u03b1) is the CMC score at rank n [42] when using M\u03b1 as the correspondence structure to perform person Re-ID over the training images. n is set to be 5 in our experiments. Similar to C(xi, yj), when calculating matching probabilities, we only consider patch pairs whose distances are within a range (cf. Eq. 6), while probabilities for other patch pairs are simply set to 0.\nAccording to Eq. 7, the updated matching probability P\u0302 kij is calculated by integrating the estimated matching probability under different binary mapping structures (i.e., P\u0302 (xi, yj |M\u03b1)). Moreover, binary mapping structures that have better Re-ID performances (i.e., larger P (M\u03b1)) will have more impacts on the updated matching probability result. P\u0302 (xi, yj |M\u03b1) in Eq. 7 is further calculated by combining a correspondence strength term and a patch importance term:\nP\u0302 (xi, yj |M\u03b1) = P\u0302 (yj |xi,M\u03b1) \u00b7 P\u0302 (xi|M\u03b1) , (8)\nin which P\u0302 (yj |xi,M\u03b1) is the correspondence strength term reflecting the correspondence strength from xi to yj when including M\u03b1. P\u0302 (xi|M\u03b1) is the patch importance term reflecting the impact of M\u03b1 to patch xi. P\u0302 (yj |xi,M\u03b1) is calculated as\nP\u0302 (yj |xi,M\u03b1) \u221d { 1, if mij \u2208M\u03b1 A\u0303yj |xi,M\u03b1 , otherwise , (9)\nin which mij is a patch-wise link connecting xi and yj . A\u0303yj |xi,M\u03b1 =\n\u03a6ij(xi,yj)\u2211 yt,mit\u2208M\u03b1 \u03a6it(xi,yt) , where \u03a6ij(xi, yj) is the\naverage appearance similarity [14], [7] between patches xi and yj over all correct match image pairs in the training set. yt is a patch that is connected to xi in the binary mapping structure M\u03b1.\nAccording to Eq. 8, given a specific binary mapping structure M\u03b1, the estimated matching probability P\u0302 (xi, yj |M\u03b1) between a probe patch xi and a gallery patch yj is calculated by jointly considering the importance of the probe patch xi under M\u03b1 (i.e., P\u0302 (xi|M\u03b1)) and the correspondence strength from the probe patch xi to the gallery patch yj under M\u03b1 (i.e., P\u0302 (yj |xi,M\u03b1)).\nMoreover, from Eq. 9, P\u0302 (yj |xi,M\u03b1) is mainly decided by the relative appearance similarity strength between patch pair {xi, yj} and all patch pairs which are connected to xi in the binary mapping structure M\u03b1. This enables patch pairs with larger appearance similarities to have stronger correspondences. Besides, we also include a constraint that P\u0302 (yj |xi,M\u03b1)\u2019s value will be the largest (equal to 1) if the binary mapping structure M\u03b1 includes a link between xi and yj . In this way, we are able to bring M\u03b1\u2019s impact into full play when estimating the matching probabilities under it.\n6 (a) (b) (c) (d) (e) (f)\n(g) (h) (i) (j) (k) (l)\nFigure 5. The learned correspondence structures for various datasets. (a, d, g, j): The correspondence structures learned by our approach (with the KISSME metric) for the VIPeR [41], PRID 450S [9], 3DPeS [43], and SYSU-sReID [44] datasets, respectively (the correspondence structure for our ROAD dataset is shown in Figure 1). The line widths are proportional to the patch-wise probability values. (b, e, h, k): The complete correspondence structure matrices of (a, d, g, j) learned by our approach. (c, f, i, l): The correspondence structure matrices of (a, d, g, j)\u2019s dataset obtained by the simple-average method. (Patches in (b, e, h, k) and (c, f, i, l) are organized by a row-first scanning order. All the correspondence structure matrices are down-sampled for a clearer illustration of the correspondence pattern). (Best viewed in color)\nAlgorithm 1 Boosting-based Learning Process Input: A set of training probe images {U\u03b1} from camera A and their corresponding cross-view images {V\u03b2} from camera B Output: \u0398A,B = {Pij}, the correspondence structure between {U\u03b1} and {V\u03b2}\n1: Find an optimal binary mapping structure M\u03b1 for each probe image U\u03b1, as described in Sec V-B 2: Set k = 1. Initialize P 0ij by Eq. 6. 3: Use the previous correspondence structure {P k\u22121ij } to perform\nRe-ID on {U\u03b1} and {V\u03b2}, and select 20 binary mapping structures M\u03b1 based on the Re-ID result, as described in Sec V-B\n4: Compute updated matching probability P\u0302 kij by Eq. 7 5: Compute new candidate correspondence structure {P\u0303 kij} by\nEq. 13 6: Use {P\u0303 kij} to calculate the Re-ID result on the training set by\nEq. 5, and compare it with the result of the previous structure {P k\u22121ij } 7: If {P\u0303 kij} has better Re-ID result than {P k\u22121ij }, set {P k ij} =\n{P\u0303 kij}; Otherwise, {P kij} = {P k\u22121ij } 8: Set k = k + 1 and go back to step 3 if not converged or not\nreaching the maximum iteration number 9: Output {Pij}\nFurthermore, the probe patch importance probability P\u0302 (xi|M\u03b1) in Eq. 8 can be further calculated by accumulating the impact of each individual link in M\u03b1 on patch xi:\nP\u0302 (xi|M\u03b1) = \u2211\nmst\u2208M\u03b1\nP\u0302 (xi|mst,M\u03b1)\n\u00b7 P\u0302 (mst|M\u03b1) , (10)\nwhere mst is a patch-wise link in M\u03b1, as the red lines in Fig. 4a. P\u0302 (mst|M\u03b1) is the importance probability of link mst which is defined similar to P (M\u03b1):\nP\u0302 (mst|M\u03b1) = R\u0303n(mst)\u2211\nmhg\u2208M\u03b1 R\u0303n(mhg) , (11)\nwhere R\u0303n(mst) is the rank-n CMC score [42] when only using a single link mst as the correspondence structure to perform Re-ID. P\u0302 (xi|mst,M\u03b1) in Eq. 10 is the impact probability from link mst to patch xi, defined as:\nP\u0302 (xi|mst,M\u03b1) \u221d  0, if d(xi, xs) \u2265 Td 1\nd(xi, xs) + 1 , otherwise\n(12)\nwhere xs is link mst\u2019s end patch in camera A. d(\u00b7) and Td are the same as in Eq. 6.\nAccording to Eq. 10, a probe patch xi\u2019s importance probability P\u0302 (xi|M\u03b1) under M\u03b1 is calculated by integrating the impact probability P\u0302 (xi|mst,M\u03b1) from each individual link in M\u03b1 to xi. Since P\u0302 (xi|mst,M\u03b1) is modeled by the distance between link mst and xi (cf. Eq. 12), it enables patches that are closer to the links in M\u03b1 to have larger importance probabilities. Moreover, Eq. 11 further guarantees links with better Re-ID performances (i.e., larger P\u0302 (mst|M\u03b1)) to have more impacts on the calculated importance probability P\u0302 (xi|M\u03b1).\nCorrespondence structure update. After obtaining the updated matching probability P\u0302 kij in Eq. 7, we can calculate matching probabilities of the new candidate correspondence structure {P\u0303 kij} by:\nP\u0303 kij = (1\u2212 \u03b5)P k\u22121ij + \u03b5P\u0302 k ij , (13)\nwhere P k\u22121ij is the matching probability in iteration k \u2212 1. \u03b5 is the update rate which is set 0.2 in our paper.\nIn order to guarantee that the boosting-based learning approach keeps optimizing Eq. 5 during the iteration process, an evaluation module is further included to decide whether to accept the new candidate correspondence structure {P\u0303 kij}. Specifically, we use {P\u0303 kij} to calculate the Re-ID result on the training set by Eq. 5, and compare it with the Re-ID result of the previous structure {P k\u22121ij }. If {P\u0303 kij} has better Re-ID result\n7 than {P k\u22121ij }, we will select {P\u0303 kij} as the correspondence structure in the k-th iteration (i.e., {P kij} = {P\u0303 kij}); Otherwise, we will still keep the correspondence structure in the previous iteration (i.e., {P kij} = {P k\u22121 ij }).\nFrom Equations 7\u201313, our update process integrates multiple variables (i.e., binary mapping structure, individual links, patch-link correlation) into a unified probability framework. In this way, various information cues that affect the objective function in Equation 5 (such as appearances, ranking results, and patch-wise correspondence patterns) can be effectively included, such that suitable candidate correspondence structures for the objective function can be created during the iterative model updating process. Therefore, by selecting the best structure among these candidate structures (with the evaluation module), a satisfactory correspondence structure can be obtained.\nFigures 1, 4, and 5 show some examples of the correspondence structures learned from different cross-view datasets. From these figures, we can see that the correspondence structures learned by our approach can suitably indicate the matching correspondence between spatial misaligned patches. For example, in Figures 5g- 5h, the large lower-to-upper misalignment between cameras is effectively captured. Besides, the matching probability values in the correspondence structure also indicate the correlation strengths between patch pairs, which are displayed as the colored blocks in Figures 5b, 5e, 5h and 5k.\nFurthermore, comparing Figures 1a and 1b, we can see that human-pose variation is also suitably handled by the learned correspondence structure. More specifically, although images in Fig. 1 have different human poses, patches of camera A in both figures can correctly find their corresponding patches in camera B since the one-to-many matching probability graphs in the correspondence structure suitably embed the local correspondence variation between cameras. Similar observations can also be obtained from Figures 4b and 4c."}, {"heading": "VI. EXTENDING THE APPROACH WITH MULTIPLE CORRESPONDENCE STRUCTURES", "text": "As mentioned before, since people often show different poses inside a camera view, the spatial correspondence pattern between a camera pair can be further divided and modeled by a set of sub-patterns according to these pose variations. Therefore, we extend our approach by introducing a multi-structure scheme, which first learns a set of local correspondence structures to capture various spatial correspondence sub-patterns between a camera pair, and then adaptively selects suitable correspondence structures to handle the spatial misalignments between individual images.\nFig. 6 shows our extended person Re-ID framework with multiple correspondence structures. During the training stage, we first divide images in each camera into different pose groups, where images in each pose group includes people with similar poses. Then, we construct a set of pose-group pairs. Each pose-group pair is composed of two pose groups from different cameras and reflects one spatial correspondence sub-pattern between cameras. Finally, we apply our boostingbased learning process over the constructed pose-group pairs\nFigure 6. Framework of the proposed approach with multiple correspondence structures.\nand obtain a set of local correspondence structures to capture the correspondence sub-patterns between a camera pair.\nDuring the prediction stage, we parse the human pose information for each pair of images being matched, and adaptively select the optimal correspondence structure to calculate the image-wise matching score for person Re-ID. More specifically, for a pair of test images (a probe image and a gallery image), we first parse the human pose information for both images [45] and classify them into their closest pose groups, respectively. Then, we are able to identify the pose-group pair for the test images and use its corresponding correspondence structure as the optimal structure for image matching.\nMoreover, the following settings need to be mentioned about our multi-structure scheme in Fig. 6.\n1) During the prediction stage, since we only need to obtain a rough estimation of human poses (e.g., differentiate front pose, side pose, or back pose), we utilize a simple but effective method which parses human pose from his/her head orientations. Specifically, we first utilize semantic region parsing [45] to obtain head and hair regions from a test image. Then, we construct a feature vector including the size and relative location information of head and hair regions, and apply a linear SVM [46] to classify this feature vector into one of the pose groups. According to our experiments, this method can achieve satisfactory (over 90%) pose classification accuracy on the datasets in our experiments. Note that our framework is general, and in practice, more sophisticated pose estimation methods [47], [48], [34] can be included to handle pose information parsing under more complicated scenarios. 2) During the training stage, we utilize two ways to divide pose groups & pose group pairs: (1) Defining and dividing pose groups & pose group pairs manually; (2) Dividing pose groups & pose group pairs automatically, where we first cluster training images [49] in each camera into a set of pose groups (using the same feature vector as described in point 1), and then form camera-\n8 wise pose group pairs based on the pose group set including the largest number of pose groups. Note that in order to automatically decide the number of clusters, we utilize a spectral clustering scheme [49] which is able to find the optimal cluster number by integrating a clustering-coherency metric to evaluate the results under different cluster numbers. Since the automatic pose group pair division scheme can properly catch the major pose correspondence patterns between a camera\npair, it can also achieve satisfactory performances. Experimental results show that methods with this automatic pose group pair division scheme can achieve similar Re-ID results to the ones with manual pose group pair division (cf. Section VII-C).\n3) Note that in Fig. 6, besides the local correspondence structures learned from pose-group pairs, we also construct a global correspondence structure learned from all training images (cf. Fig. 2). Thus, if a test image pair cannot find suitable local correspondence structure (e.g., a test image cannot be confidently classified into any pose group, or the correspondence structure of a specific pose-group pair is not constructed due to limited training samples), this global correspondence structure will be applied to calculate the matching score of this test image pair."}, {"heading": "VII. EXPERIMENTAL RESULTS", "text": ""}, {"heading": "A. Datasets and Experimental Settings", "text": "We perform experiments on the following five datasets: VIPeR. The VIPeR dataset [41] is a commonly used dataset which contains 632 image pairs for 632 pedestrians, as in Figures 4a-4c and 8d. It is one of the most challenging datasets which includes large differences in viewpoint, pose, and illumination between two camera views. Images from camera A are mainly captured from 0 to 90 degree while camera B mainly from 90 to 180 degree.\nPRID 450S. The PRID 450S dataset [9] consists of 450 person image pairs from two non-overlapping camera views. Some example images in PRID 450S dataset are shown in Fig. 5d. It is also challenging due to low image qualities and viewpoint changes.\n3DPeS. The 3DPeS dataset [43] is comprised of 1012 images from 193 pedestrians captured by eight cameras, where each person has 2 to 26 images, as in Figures 5g and 8a. Note that since there are eight cameras with significantly different views in the dataset, in our experiments, we group cameras with similar views together and form three camera groups. Then, we train a correspondence structure between each pair of camera groups. Finally, three correspondence structures are achieved and utilized to calculate Re-ID performance between different camera groups. For images from the same camera group, we simply utilize adjacency-constrained search [14] to find patch-wise mapping and calculate the image matching score accordingly.\nRoad. The Road dataset is our own constructed dataset which includes 416 image pairs taken by two cameras with camera A monitoring an exit region and camera B monitoring\n(a) VIPeR dataset (b) SYSU dataset\nFigure 7. The convergence curves when using our learning approach to find the correspondence structure in the VIPeR and SYSU datasets. (y-axis: the value of the objective function in Eq. 5 when using the correspondence structure in a certain iteration; x-axis: the iteration number. Red solid curve: our learning approach with the evaluation module; Blue dashed curve: our learning approach without the evaluation module) (Best viewed in color)\na road region, as in Figures 1 and 8g.1 This dataset has large variation of human pose and camera angle. Images in this dataset are taken from a realistic crowd road scene.\nSYSU-sReID. The SYSU-sReID dataset [44] contains 502 individual pairs taken by two disjoint cameras in a campus environment, as in Fig. 5k. This dataset includes more crosscamera pose correspondence patterns.\nFor all of the above datasets, we follow previous methods [5], [8], [2] and perform experiments under 50%-training and 50%-testing. All images are scaled to 128\u00d748. The patch size in our approach is 24\u00d718. The stride size between neighboring patches is 6 horizontally and 8 vertically for probe images, and 3 horizontally and 4 vertically for gallery images. Note that we use smaller stride size in gallery images in order to obtain more patches. In this way, we can have more flexibilities during patch-wise matching.\nFurthermore, in order to save computation complexity, we make the following simplifications on the learning process during our experiments:\n1) The global constraint in Eq. 3 is not applied in the training stage (cf. step 3 in Algorithm 1). Our experiments show that skipping the global constraint in training does not affect the final Re-ID results too much. 2) The evaluation module (cf. step 7 in Algorithm 1) is also skipped in the training stage, i.e., we directly use {P\u0303 kij} in Eq. 13 to be the correspondence structure in the kth iteration without evaluating whether it is better than {P k\u22121ij }. Our experiments show that our learning process without the evaluation module can achieve similar Re-ID results to the one including the evaluation module while still obtaining stable correspondence structures within 300 iterations. For example, Fig. 7 shows the convergence curves when using our learning approach to find the correspondence structure in the VIPeR and SYSU datasets. We can see that our learning approach without the evaluation module (the blue dashed curves) can achieve stable results after 270 iterations, which is similar to the one including the evaluation module (the red solid curves).\n1Available at http://min.sjtu.edu.cn/lwydemo/personReID.htm\n9 (a) (b) (c)\n(d) (e) (f)\n(g) (h) (i)\n(j) (k) (l)\nFigure 8. Comparison of different patch mapping methods. Left column: the adjacency-constrained method; Middle column: the simple-average method; Last column: our approach (with the KISSME metric). The solid lines represent matching probabilities in a correspondence structure and the reddashed lines represent patch matching results. Note that the image pair in (a)-(c) includes the same person (i.e., correct match) while the image pairs in (d)-(l) include different people (i.e., wrong match). (Best viewed in color)"}, {"heading": "B. Results of using a single correspondence structure", "text": "We first evaluate the performance of our approach by using a single correspondence structure to handle the spatial misalignments between a camera pair (cf. Fig. 2). Note that although multiple correspondence structures are used to handle the view differences among the 8 cameras in the 3DPeS dataset (cf. Section VII-A), we only use one correspondence structure for each camera pair. Therefore, it also belongs to the situation of using a single correspondence structure.\n1) Patch matching performances: We compare the patch matching performance of three methods: (1) The adjacencyconstrained search method [14], [29], [15] which finds a best matched patch for each patch in a probe image (probe patch) by searching a fixed neighborhood region around the probe patch\u2019s co-located patch in a gallery image (Adjacencyconstrained). (2) The simple-average method which simply averages the binary mapping structures for different probe images (as in Fig. 4a) to be the correspondence structure and combines it with a global constraint to find the best one-to-one patch matching result (Simple-average). (3) Our approach which employs a boosting-based process to learn the correspondence structure and combines it with a global constraint to find the best one-to-one patch matching result.\nFig. 8 shows the patch mapping results of different methods, where solid lines represent matching probabilities in a correspondence structure and red-dashed lines represent patch matching results. Besides, Figures 5b, 5e, 5h, 5k and Figures 5c, 5f, 5i, 5l also show the correspondence structure matrices obtained by our approach and the simple-average method, respectively. From Figures 5 and 8, we can observe:\n1) Since the adjacency-constrained method searches a fixed neighborhood region without considering the correspondence pattern between cameras, it may easily be interfered by wrong patches with similar appearances in the neighborhood (cf. Figures 8d, 8g, 8j). Comparatively, with the indicative matching probability information in the correspondence structure, the interference from mismatched patches can be effectively reduced (cf. Figures 8f, 8i, 8l). 2) When there are large misalignments between cameras, the adjacency-constrained method may fail to find proper patches as the correct patches may be located outside the neighborhood region, as in Fig. 8a. Comparatively, the large misalignment pattern between cameras can be properly captured by our correspondence structure, resulting in a more accurate patch matching result (cf. Fig. 8c). 3) Comparing Figures 5b, 5e, 5h, 5k and Figures 5c, 5f, 5i, 5l with the last two columns in Fig. 8, it is obvious that the correspondence structures by our approach is better than the simple average method. Specifically, the correspondence structures by the simple average method include many unsuitable matching probabilities which may easily result in wrong patch matches. In contrast, the correspondence structures by our approach are more coherent with the actual spatial correspondence pattern between cameras. This implies that reliable correspondence structure cannot be easily achieved without suitably integrating the information cues between cameras.\nMoreover, in order to further evaluate the effectiveness of our correspondence structure learning process, we also show the learned correspondence structures when using different distance metrics (KISSME [7], kLFDA [8], and KMFAR\u03c72 [39]) to measure the patch-wise similarity (cf. Eq. 2), as in Fig. 9. From Fig. 9, we see that our approach can effectively capture the overall spatial correspondence patterns under different distance metrics (e.g., the large lower-to-upper misalignments between cameras are properly captured by all the correspondence structures in Fig. 9).\n2) Performances of person Re-ID: We evaluate person ReID results by the standard Cumulated Matching Characteristic (CMC) curve [42] which measures the correct match rates within different Re-ID rank ranges. The evaluation protocols are the same as [5]. That is, for each dataset, we perform 10 randomly-partitioned 50%-training and 50%-testing experiments and average the results.\nTo verify the effectiveness of each module of our approach, we compare results of four methods: (1) Not applying correspondence structure and directly using the appearance similarity between co-located patches for person Re-ID, (Nonstructure); (2) Simply averaging the binary mapping structures\n10\nfor different probe images as the correspondence structure and utilizing it for Re-ID (Simple-average); (3) Using a correspondence structure where the correspondence probabilities for all neighboring patch-wise pairs are set as 1 (i.e., Pij = 1, if d(yi, yj) < Td and Pij = 0 otherwise, cf. Eq. 6), and applying a global constraint as Section IV-C to calculate image-wise similarity (AC+global); (4) Using the correspondence structure learned by our approach, but do not include global constraint when performing Re-ID (Non-global); (5) Our approach (Proposed (Single)).\nTables I\u2013V show the CMC results of different methods under three distance metrics: KISSME [7], kLFDA [8], and KMFA-R\u03c72 [39] (note that we use Dense SIFT and Dense Color Histogram features [14] for KISSME & kLFDA metrics, and use RGB, HSV, YCbCr, Lab, YIQ, 16 Gabor texture features [39] for KMFA-R\u03c72 metric). From Tables I\u2013V, we notice that:\n1) Our approach has obviously improved results than the non-structure method. This indicates that proper correspondence structures can effectively improve Re-ID performances by reducing patch-wise misalignments. 2) The simple-average method has similar performance to the no-structure method. This implies that an inappropriate correspondence structure reduces the Re-ID performance. 3) The AC+global method can be viewed as an extended version of the adjacency-constrained search method [14], [29], [15] plus a global constraint. Compared with the AC+global method, our approach still achieves obviously better performance. This indicates that our correspondence structure has stronger capability in handling spatial misalignments than the adjacency-constrained search methods. 4) The non-global method has improved Re-ID performances than the non-structure method. This further demonstrates the effectiveness of the correspondence structure learned by our approach. Meanwhile, our approach also has superior performance than the nonglobal method. This demonstrates the usefulness of introducing global constraint in patch matching process. 5) The improvement of our approach is coherent on different distance metrics (KISSME [7], kLFDA [8], and KMFA-R\u03c72 [39]). This indicates the robustness of our approach in handling spatial misalignments. In practice, our proposed correspondence structure can also be combined with other features or distance metrics to further"}, {"heading": "C. Results of using multiple correspondence structures", "text": "We further evaluate the performance of our approach by using multiple structures to handle spatial misalignments (i.e., the multi-structure strategy described by Fig. 6). Note that since the number of pedestrians for each pose is small in the 3DPeS [43] dataset, which is insufficient to construct reliable local correspondences, we focus on evaluating our multi-structure strategy on the other four datasets: VIPeR [41], PRID 450S [9], Road, and SYSU-sReID [44].\n1) Results for correspondence structure learning: In this section, we evaluate the results of multiple correspondence structure learning based on the manually divided pose group\n11\npairs. Similar results can be observed when automatic pose group pair division (cf. Section VI) is applied.\nSince each of the VIPeR, PRID 450S, and Road dataset includes two major cross-view pose correspondences (e.g., front-to-right and front-to-back for VIPeR, and left-to-left and right-to-right for PRID 450S), we divide the training images in each dataset into two pose group pairs and construct two local correspondence structures from these pose group pairs. Comparatively, since more major cross-view pose correspondences are included in the SYSU-sReID dataset (i.e., left-to-left, rightto-right, front-to-side, and back-to-side), we further divide the training images in this dataset into four pose group pairs and construct four local correspondence structures accordingly.\nFig. 10 shows the multiple correspondence structure learning results of our approach. In Fig. 10, the left and middle\ncolumns show the two local correspondence structures learned for each dataset, and the right column shows the absolute difference matrices between the local correspondence structures in left and middle columns. Note that since the SYSU-sReID dataset includes four local correspondence structures which can form six difference matrices, we only show two local correspondence structures and their corresponding difference matrix to save space (cf. Figs 10j, 10k, 10l).\n12\nFig. 10 indicates that by introducing multiple correspondence structures, the spatial correspondence pattern between cameras can be modeled in a more precise way. Specifically:\n1) The spatial correspondence pattern between a camera pair can be jointly described by a set of local correspondence structures, where each local correspondence structure captures a sub-pattern inside the global correspondence pattern. For example, the local correspondence structure in Fig. 10a can properly emphasize the large spatial displacement due to the cross-view pose translation in the front-to-side correspondence subpattern. At the same time, the local correspondence structure in Fig. 10b also properly captures the frontto-back correspondence sub-pattern. In this way, our approach can have stronger capability to handle spatial misalignments with large variations. 2) Due to the effectiveness of our correspondence structure learning process, the learned local correspondence structures can properly differentiate and highlight the subtle differences among different spatial correspondence subpatterns. For example, the absolute difference matrix in Fig. 10c shows large values in the middle and downright corner. This implies that the major difference between the front-to-side and front-to-back correspondence subpatterns in Figs. 10a and 10b is around the torso and leg regions. Similarly, large difference values appear in the top left and bottom right corners in Fig. 10f, indicating that the major difference between the left-to-left and right-to-right correspondence sub-patterns in Figs. 10d and 10e is around the head and leg regions.\n2) Performances of Person Re-ID: We further perform person Re-ID experiments by using the multiple correspondence structures in Fig. 10. Note that besides the local correspondence structures in Fig. 10, the global correspondence structure learned over all training samples is also included (cf. Fig. 6).\nTables I, II, IV, and V show the CMC results of our approach with multiple correspondence structures where proposed+multi-manu and proposed+multi-auto represent the results of our approach under manually and automatically divided pose group pairs, respectively (cf. Section VI). Moreover, in order to evaluate the impact of pose classification error, we also include the results by using the ground-truth pose classification label2 to select the optimal correspondence structure during person Re-ID (i.e., proposed+multi-GT in Tables I, II, IV, and V ).\nFrom Tables I, II, IV, and V, we can observe:\n1) Our approach with multiple correspondence structures (proposed+multi-manu and proposed+multi-auto) can achieve better results than using a single correspondence structure (proposed+single). This further demonstrates that our multi-structure strategy can model the crossview spatial correspondence in a more precise way, and spatial misalignments can be more effectively handled and excluded by multiple correspondence structures.\n2The ground-truth labels are defined based on the manually divided pose group pairs.\n2) Our approach, which uses automatic pose classification to select the optimal correspondence structure (proposed+multi-manu), can achieve similar results to the method that uses the ground-truth pose classification label for correspondence structure selection (proposed+multi-GT). This implies that the pose classification method used in our approach can provide sufficient classification accuracy to guarantee satisfactory Re-ID performances. 3) Our approach under automatically obtained pose group pairs (proposed+multi-auto) can achieve similar performances to the one under manually divided pose group pairs (proposed+multi-manu). This implies that if we can find proper features to describe human poses, automatic pose grouping can also obtain proper pose group pairs and achieve satisfactory Re-ID results. 4) The improvement of multiple correspondence structures (proposed+multi-manu and proposed+multi-auto) is the most obvious on the SYSU-sReID dataset. This is because the SYSU-sReID dataset includes more crossview pose correspondence patterns. Thus, by introducing more local correspondence structures to handle these pose correspondence patterns, the cross-view spatial misalignments can be more precisely captured, leading to more obvious improvements."}, {"heading": "D. Comparing with the state-of-art methods", "text": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19], Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset; eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.\nNote that since the rotation and orientation parameters used in the TA+W method [34] are derived from camera calibration and human motion information which is not available on SYSU-sReID dataset, we manually label these parameters so as to have a fair comparison with our approach (i.e., guarantee that [34]\u2019s performance will not be degraded due to improper rotation and orientation parameters).\nThe CMC results of different methods are shown in Tables VI\u2013X and Fig. 11. Moreover, since many works also reported fusion results on the VIPeR dataset (i.e., adding multiple Re-ID results together to achieve a higher Re-ID result) [14], [29], [4], we also show a fusion result of our approach (svmml+Proposed+multi-manu-KMFA) and compare it with the other fusion results in Table VI. Tables VI\u2013X show that:\n13\n1) Our approach has better Re-ID performances than most of the state-of-the-art methods. This further demonstrates the effectiveness of our approach. 2) Our approach also achieves similar or better results than some deep-neural-network-based methods (e.g., IDLA [23], JLR [20], and DG-Dropout [26] in Table VI). This implies that spatial misalignment is indeed a significant factor affecting the performance of person Re-ID. If the spatial misalignment problem can be properly handled, satisfactory results can be achieved with relatively simple hand-crafted features. Moreover, since our approach is independent of the specific features used for Re-ID, it can easily accommodate future advances by\ncombining with the deep-neural-network-based features (such as [27]) without changing the core protocols. 3) Since the TA+W method [34] creates multiple weight matrices for pedestrians with different orientations, it can be viewed as another version of using multiple matching structures. However, our approach (proposed+multi-manu) still outperforms the TA+W method [34] (cf. Table X). This further indicates that our multi-structure scheme is effective in modeling the cross-view spatial correspondence.\n14"}, {"heading": "E. Time complexity & Effects of different parameter values", "text": "1) Time complexity: We also evaluate the running time of training & testing excluding dense feature extraction in Table XI. Testing process with (Test+Proposed) and without (Test+Non-global) Hungarian method are both evaluated. Moreover, for the testing process, we list two time complexity values: (1) the running time for the entire process (Test-all image pairs), and (2) the average running time for computing the similarity of a single image pair (Test-per image pair).\nWe can notice that the running time of both training and testing are acceptable. Table XI also reveals that Hungarian method takes the major time cost of testing. This implies that our testing process can be further optimized by replacing Hungarian method with other more efficient algorithms [52].\n2) Effect of different parameter values: Finally, we evaluate the effect of three parameters: (1) Tc which is the threshold of keeping the learned matching probability value Pij (cf. Eq. 1); (2) Td which is the search range of handling spatial displacement (cf. Eq. 6); and (3) Patch size which is the size of the basic element for calculating image-wise similarity.\nFigures 12a to 12b show the rank-1 CMC scores [42] of our approach under different Tc, Td, and patch size values on the Road dataset. Moreover, Fig. 13 further shows the learned correspondence structures under different search ranges Td. Figures 12 and 13 show that:\n1) Basically, Tc should be kept small since a large Tc will exclude many useful matching probabilities in the correspondence structure and lead to decreased Re-ID results. However, when Tc becomes extremely small, the Re-ID performance may also be slightly affected since some noisy matching probabilities may be included in the correspondence structure. According to our experiments, 0.03\u20140.07 is a proper range for Tc and we use Tc = 0.05 throughout our experiments.\n15\n2) Td normally should be set large enough in order to handle large spatial misalignments. However, a large Td will also increase the complexity of person Re-ID. According to our experiments, satisfactory results can be achieved when Td is larger than 32. Therefore, in our experiments, we set Td = 32 to guarantee performance while minimizing computation complexity. 3) The patch size should not be too small or too large. If the patch size is too small, the visual information included in each patch becomes very limited. This may reduce the reliability of patch matching results. On the other hand, if the patch size is too large, the number of patches in an image will be obviously reduced. This will affect the flexibility of our approach to handle spatial misalignments. According to our experiments, 20\u00d715\u2014 28 \u00d7 21 is a proper patch size range for images with 128\u00d7 48 sizes."}, {"heading": "VIII. CONCLUSION", "text": "In this paper, we propose a novel framework for addressing the problem of cross-view spatial misalignments in person Re-ID. Our framework consists of three key ingredients: 1) introducing the idea of correspondence structure and learning this structure via a novel boosting method to adapt to arbitrary camera configurations; 2) a constrained global matching step to control the patch-wise misalignments between images due to local appearance ambiguity; 3) a multi-structure strategy to handle spatial misalignments in a more precise way. Extensive experimental results on benchmark show that our approach achieves the state-of-the-art performance."}], "references": [{"title": "Person reidentification with correspondence structure learning", "author": ["Y. Shen", "W. Lin", "J. Yan", "M. Xu", "J. Wu", "J. Wang"], "venue": "IEEE Intl. Conf. Comp. Vision, 2015, pp. 3200\u20133208.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Salient color names for person re-identification", "author": ["Y. Yang", "J. Yang", "J. Yan", "S. Liao", "D. Yi", "S.Z. Li"], "venue": "Eur. Conf. Comp. Vision, 2014, pp. 536\u2013551.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Person re-identification using semantic color names and rankboost", "author": ["C.-H. Kuo", "S. Khamis", "V. Shet"], "venue": "IEEE Winter Conf. Application of Computer Vision, 2013, pp. 281\u2013287.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning invariant color features for person re-identification", "author": ["R. Varior", "G. Wang", "J. Lu", "T. Liu"], "venue": "IEEE Trans. Image Process., 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Viewpoint invariant pedestrian recognition with an ensemble of localized features", "author": ["D. Gray", "H. Tao"], "venue": "Eur. Conf. Comp. Vision, 2008, pp. 262\u2013275.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Person re-identification by efficient impostor-based metric learning", "author": ["M. Hirzer", "P.M. Roth", "H. Bischof"], "venue": "IEEE Conf. Advanced Video and Signal-based Surveillance, 2012, pp. 203\u2013208.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Large scale metric learning from equivalence constraints", "author": ["M. Kostinger", "M. Hirzer", "P. Wohlhart", "P.M. Roth", "H. Bischof"], "venue": "IEEE Comp. Vision and Patt. Recog., 2012, pp. 2288\u20132295.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Person re-identification using kernel-based metric learning methods", "author": ["F. Xiong", "M. Gou", "O. Camps", "M. Sznaier"], "venue": "Eur. Conf. Comp. Vision, 2014, pp. 1\u201316.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Mahalanobis distance learning for person re-identification", "author": ["P.M. Roth", "M. Hirzer", "M. K\u00f6stinger", "C. Beleznai", "H. Bischof"], "venue": "Person Re-Identification. Springer, 2014, pp. 247\u2013267.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Scalable person re-identification: A benchmark", "author": ["L. Zheng", "L.-Y. Sheng", "L. Tian", "S.-J. Wang", "J.-D. Wang", "Q. Tian"], "venue": "IEEE Intl. Conf. Comp. Vision, 2015, pp. 1116\u20131124.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Similarity learning on an explicit polynomial kernel feature map for person re-identification", "author": ["D.-P. Chen", "Z.-J. Yuan", "G. Hua", "N.-N. Zheng", "J.-D. Wang"], "venue": "IEEE Comp. Vision and Patt. Recog., 2015, pp. 1565\u20131573.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Human identity recognition in aerial images", "author": ["O. Oreifej", "R. Mehran", "M. Shah"], "venue": "IEEE Comp. Vision and Patt. Recog., 2010, pp. 709\u2013 716.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "A generalized emd with body prior for pedestrian identification", "author": ["L. Ma", "X. Yang", "Y. Xu", "J. Zhu"], "venue": "Journal of Visual Communication and Image Representation, vol. 24, no. 6, pp. 708\u2013716, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised salience learning for person re-identification", "author": ["R. Zhao", "W. Ouyang", "X. Wang"], "venue": "IEEE Comp. Vision and Patt. Recog., 2013, pp. 3586\u20133593.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Dense invariant feature based support vector ranking for cross-camera person re-identification", "author": ["S. Tan", "F. Zheng", "L. Liu", "J. Han", "L. Shao"], "venue": "IEEE Trans. Circuits Syst. Video Technol., pp. 687\u2013691, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "On-the-fly feature importance mining for person re-identification", "author": ["C. Liu", "S. Gong", "C.C. Loy"], "venue": "Patt. Recog., vol. 47, no. 4, pp. 1602\u20131615, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Person re-identification over camera networks using multi-task distance metric learning", "author": ["L. Ma", "X. Yang", "D. Tao"], "venue": "IEEE Trans. Image Process., vol. 23, no. 8, pp. 3656\u20133670, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Relevance metric learning for person re-identification by exploiting listwise similarities", "author": ["J. Chen", "Z. Zhang", "Y. Wang"], "venue": "IEEE Trans. Image Process., vol. 24, no. 12, pp. 4741\u20134755, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a discriminative null space for person re-identification", "author": ["L. Zhang", "T. Xiang", "S. Gong"], "venue": "IEEE Comp. Vision and Patt. Recog., 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint learning of single-image and cross-image representations for person reidentification", "author": ["F.-Q. Wang", "W.-M. Zuo", "L. Lin", "D. Zhang", "L. Zhang"], "venue": "IEEE Comp. Vision and Patt. Recog., 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Bit-scalable deep hashing with regularized similarity learning for image retrieval and person re-identification", "author": ["R. Zhang", "L. Lin", "R. Zhang", "W. Zuo", "L. Zhang"], "venue": "IEEE Trans. Image Process., vol. 24, no. 12, pp. 4766\u20134779, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep ranking for person reidentification via joint representation learning", "author": ["S.-Z. Chen", "C.-C. Guo", "J.-H. Lai"], "venue": "IEEE Trans. Image Process., 2016.  16", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "An improved deep learning architecture for person re-identification", "author": ["E. Ahmed", "M. Jones", "T.-K. Marks"], "venue": "IEEE Comp. Vision and Patt. Recog., 2015, pp. 3908\u20133916.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep feature learning with relative distance comparison for person re-identification", "author": ["S.-Y. Ding", "L. Lin", "G.-R. Wang", "H.-Y. Chao"], "venue": "Patt. Recog., vol. 48, no. 10, pp. 2993\u20133003, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Person reidentification by multi-channel parts-based cnn with improved triplet loss function", "author": ["D. Cheng", "Y. Gong", "S. Zhou", "J. Wang", "N. Zheng"], "venue": "IEEE Comp. Vision and Patt. Recog., 2016, pp. 1335\u2013 1344.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deep feature representations with domain guided dropout for person re-identification", "author": ["T. Xiao", "H. Li", "W. Ouyang", "X. Wang"], "venue": "IEEE Comp. Vision and Patt. Recog., 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "An enhanced deep feature representation for person re-identification", "author": ["S. Wu", "Y.-C. Chen", "X. Li", "A.-C. Wu", "J.-J. You", "W.-S. Zheng"], "venue": "IEEE Winter Conf. App. of Comp. Vision, 2016, pp. 1\u20138.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Human re-identification by matching compositional template with cluster sampling", "author": ["Y. Xu", "L. Lin", "W.-S. Zheng", "X. Liu"], "venue": "IEEE Intl. Conf. Comp. Vision, 2013, pp. 3152\u20133159.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Person re-identification by salience learning", "author": ["R. Zhao", "W. Ouyang", "X. Wang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Custom pictorial structures for re-identification", "author": ["D. Cheng", "M. Cristani", "M. Stoppa", "L. Bazzani", "V. Murino"], "venue": "British Machive Vision Conference, 2011, pp. 1\u20136.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised learning of generative topic saliency for person re-identification", "author": ["H. Wang", "S. Gong", "T. Xiang"], "venue": "British Machive Vision Conference, 2014, pp. 1\u201311.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Person reidentification via learning visual similarity on corresponding patch pairs", "author": ["H. Sheng", "Y. Huang", "Y. Zheng", "J. Chen", "Z. Xiong"], "venue": "Intl. Conf. Knowledge Science, Engineering and Management, 2015, pp. 787\u2013798.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Person re-identification using deformable patch metric learning", "author": ["S. Bak", "P. Carr"], "venue": "IEEE Winter Conf. App. of Comp. Vision, 2016, pp. 1\u20139.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Improving person reidentification by viewpoint cues", "author": ["S. Bak", "S. Zaidenberg", "B. Boulay"], "venue": "IEEE Conf. Advanced Video and Signal Based Surveillance, 2014, pp. 175\u2013180.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantics-aware deep correspondence structure learning for robust person re-identification", "author": ["Y. Zhang", "X. Li", "L. Zhao", "Z. Zhang"], "venue": "Intl. Joint Conf. Artificial Intelligence, 2016, pp. 3545\u20133551.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "A novel visual word cooccurrence model for person re-identification", "author": ["Z.-M. Zhang", "Y.-T. Chen", "V. Saligrama"], "venue": "Eur. Conf. Comp. Vision Workshops, 2014, pp. 122\u2013133.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Prism: Person re-identification via structured matching", "author": ["Z.-M. Zhang", "V. Saligrama"], "venue": "IEEE Trans. Circuits and Systems for Video Technology, pp. 1\u201313, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "The generalized patchmatch correspondence algorithm", "author": ["C. Barnes", "E. Shechtman", "D.B. Goldman", "A. Finkelstein"], "venue": "Eur. Conf. Comp. Vision, 2010, pp. 29\u201343.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Mirror representation for modeling view-specific transform in person re-identification", "author": ["Y.-C. Chen", "W.-S. Zheng", "J. Lai"], "venue": "Intl. Joint Conf. Artificial Intelligence, 2015, pp. 3402\u20133408.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "The hungarian method for the assignment problem", "author": ["H.W. Kuhn"], "venue": "Naval Research Logistics Quarterly, vol. 2, no. 1-2, pp. 83\u201397, 1955.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1955}, {"title": "Evaluating appearance models for recognition, reacquisition, and tracking", "author": ["D. Gray", "S. Brennan", "H. Tao"], "venue": "Performance Evaluation of Tracking and Surveillance (PETS), vol. 3, no. 5, 2007.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2007}, {"title": "Shape and appearance context modeling", "author": ["X. Wang", "G. Doretto", "T. Sebastian", "J. Rittscher", "P. Tu"], "venue": "IEEE Intl. Conf. Comp. Vision, 2007, pp. 1\u20138.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2007}, {"title": "3dpes: 3d people dataset for surveillance and forensics", "author": ["D. Baltieri", "R. Vezzani", "R. Cucchiara"], "venue": "ACM workshop Human gesture and behavior understanding, 2011, pp. 59\u201364.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "Person re-identification with multi-level adaptive correspondence models", "author": ["S.-C. Shia", "C.-C. Guoa", "J.-H. Lai", "S.-Z. Chen", "X.-J. Hua"], "venue": "Neurocomputing, vol. 168, pp. 550\u2013559, 2015.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Pedestrian parsing via deep decompositional network", "author": ["L. Ping", "X.-G. Wang", "X.-O. Tang"], "venue": "IEEE Intl. Conf. Comp. Vision, 2013, pp. 2648\u20132655.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "Libsvm: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Trans. Intell. Syst. Tech., vol. 2, pp. 1\u201327, 2011.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning descriptors for object recognition and 3d pose estimation", "author": ["P. Wohlhart", "V. Lepetit"], "venue": "IEEE Comp. Vision and Patt. Recog., 2015, pp. 3041\u20133048.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Human pose estimation using body parts dependent joint regressors", "author": ["M. Dantone", "J. Gall", "C. Leistner", "L.-V. Gool"], "venue": "IEEE Comp. Vision and Patt. Recog., 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Inferring user image search goals under the implicit guidance of users", "author": ["Z. Lu", "X. Yang", "W. Lin", "H. Zha", "X. Chen"], "venue": "IEEE Trans. Circuits and Systems for Video Technology, vol. 24, no. 3, pp. 394\u2013406, 2014.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning locally-adaptive decision functions for person verification", "author": ["Z. Li", "S. Chang", "F. Liang", "T.S. Huang", "L. Cao", "J.R. Smith"], "venue": "IEEE Comp. Vision and Patt. Recog., 2013, pp. 3610\u20133617.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2013}, {"title": "PCCA: A new approach for distance learning from sparse pairwise constraints", "author": ["A. Mignon", "F. Jurie"], "venue": "IEEE Comp. Vision and Patt. Recog., 2012, pp. 2666\u20132672.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}, {"title": "The dynamic hungarian algorithm for the assignment problem with changing costs", "author": ["G.A. Mills-Tettey", "A. Stentz", "M.B. Dias"], "venue": "Carnegie Mellon University, 2007.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 1, "context": ", cameras that are non-overlapping and different from the probe image\u2019s camera) [2], [3], [4].", "startOffset": 80, "endOffset": 83}, {"referenceID": 2, "context": ", cameras that are non-overlapping and different from the probe image\u2019s camera) [2], [3], [4].", "startOffset": 85, "endOffset": 88}, {"referenceID": 3, "context": ", cameras that are non-overlapping and different from the probe image\u2019s camera) [2], [3], [4].", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "The basic idea of this paper appeared in our conference version [1].", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "However, most existing works [2], [3], [4], [5], [6], [7],", "startOffset": 29, "endOffset": 32}, {"referenceID": 2, "context": "However, most existing works [2], [3], [4], [5], [6], [7],", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "However, most existing works [2], [3], [4], [5], [6], [7],", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "However, most existing works [2], [3], [4], [5], [6], [7],", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "However, most existing works [2], [3], [4], [5], [6], [7],", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "However, most existing works [2], [3], [4], [5], [6], [7],", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "[8], [9], [10], [11] focus on handling the overall appearance variations between images, while the spatial misalignment among images\u2019 local patches is not addressed.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[8], [9], [10], [11] focus on handling the overall appearance variations between images, while the spatial misalignment among images\u2019 local patches is not addressed.", "startOffset": 5, "endOffset": 8}, {"referenceID": 9, "context": "[8], [9], [10], [11] focus on handling the overall appearance variations between images, while the spatial misalignment among images\u2019 local patches is not addressed.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "[8], [9], [10], [11] focus on handling the overall appearance variations between images, while the spatial misalignment among images\u2019 local patches is not addressed.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "Although some patch-based methods [12], [13], [14], [15] address the spatial misalignment problem by decomposing images into patches", "startOffset": 34, "endOffset": 38}, {"referenceID": 12, "context": "Although some patch-based methods [12], [13], [14], [15] address the spatial misalignment problem by decomposing images into patches", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "Although some patch-based methods [12], [13], [14], [15] address the spatial misalignment problem by decomposing images into patches", "startOffset": 46, "endOffset": 50}, {"referenceID": 14, "context": "Although some patch-based methods [12], [13], [14], [15] address the spatial misalignment problem by decomposing images into patches", "startOffset": 52, "endOffset": 56}, {"referenceID": 1, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 91, "endOffset": 94}, {"referenceID": 2, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 96, "endOffset": 99}, {"referenceID": 3, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 106, "endOffset": 109}, {"referenceID": 15, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 5, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 206, "endOffset": 209}, {"referenceID": 6, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 211, "endOffset": 214}, {"referenceID": 7, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 216, "endOffset": 219}, {"referenceID": 8, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 221, "endOffset": 224}, {"referenceID": 16, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 226, "endOffset": 230}, {"referenceID": 17, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 232, "endOffset": 236}, {"referenceID": 18, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 238, "endOffset": 242}, {"referenceID": 19, "context": "reliability of feature representations or similarity metrics [20], [21], [22], [23], [24], [25], [26], [27].", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "reliability of feature representations or similarity metrics [20], [21], [22], [23], [24], [25], [26], [27].", "startOffset": 67, "endOffset": 71}, {"referenceID": 21, "context": "reliability of feature representations or similarity metrics [20], [21], [22], [23], [24], [25], [26], [27].", "startOffset": 73, "endOffset": 77}, {"referenceID": 22, "context": "reliability of feature representations or similarity metrics [20], [21], [22], [23], [24], [25], [26], [27].", "startOffset": 79, "endOffset": 83}, {"referenceID": 23, "context": "reliability of feature representations or similarity metrics [20], [21], [22], [23], [24], [25], [26], [27].", "startOffset": 85, "endOffset": 89}, {"referenceID": 24, "context": "reliability of feature representations or similarity metrics [20], [21], [22], [23], [24], [25], [26], [27].", "startOffset": 91, "endOffset": 95}, {"referenceID": 25, "context": "reliability of feature representations or similarity metrics [20], [21], [22], [23], [24], [25], [26], [27].", "startOffset": 97, "endOffset": 101}, {"referenceID": 26, "context": "reliability of feature representations or similarity metrics [20], [21], [22], [23], [24], [25], [26], [27].", "startOffset": 103, "endOffset": 107}, {"referenceID": 27, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 92, "endOffset": 96}, {"referenceID": 11, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 104, "endOffset": 108}, {"referenceID": 12, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 116, "endOffset": 120}, {"referenceID": 28, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 122, "endOffset": 126}, {"referenceID": 29, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 128, "endOffset": 132}, {"referenceID": 30, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 134, "endOffset": 138}, {"referenceID": 31, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 140, "endOffset": 144}, {"referenceID": 32, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 146, "endOffset": 150}, {"referenceID": 27, "context": "In [28], [30], [32], a human body in an image is first parsed into semantic parts (e.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "In [28], [30], [32], a human body in an image is first parsed into semantic parts (e.", "startOffset": 9, "endOffset": 13}, {"referenceID": 31, "context": "In [28], [30], [32], a human body in an image is first parsed into semantic parts (e.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "In [12], Oreifej et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "[13] introduce a body prior constraint to avoid mismatching between distant patches, the problem is still not well addressed, especially for the mismatching between closely located patches.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "saliency-based approaches [14], [29], [33], [34] are recently proposed, which estimate the saliency distribution relationship between images and utilize it to control the patch-wise matching process.", "startOffset": 26, "endOffset": 30}, {"referenceID": 28, "context": "saliency-based approaches [14], [29], [33], [34] are recently proposed, which estimate the saliency distribution relationship between images and utilize it to control the patch-wise matching process.", "startOffset": 32, "endOffset": 36}, {"referenceID": 32, "context": "saliency-based approaches [14], [29], [33], [34] are recently proposed, which estimate the saliency distribution relationship between images and utilize it to control the patch-wise matching process.", "startOffset": 38, "endOffset": 42}, {"referenceID": 33, "context": "saliency-based approaches [14], [29], [33], [34] are recently proposed, which estimate the saliency distribution relationship between images and utilize it to control the patch-wise matching process.", "startOffset": 44, "endOffset": 48}, {"referenceID": 32, "context": "Following the similar line, Bak and Carr [33] introduce a deformable model to obtain a set of weights to guide the patch matching process.", "startOffset": 41, "endOffset": 45}, {"referenceID": 33, "context": "[34] further", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Comparatively, the matching weights in most of the saliency-based approaches [29], [33] are only controlled by patches in the probe-image (probe patch).", "startOffset": 77, "endOffset": 81}, {"referenceID": 32, "context": "Comparatively, the matching weights in most of the saliency-based approaches [29], [33] are only controlled by patches in the probe-image (probe patch).", "startOffset": 83, "endOffset": 87}, {"referenceID": 34, "context": "In [35],", "startOffset": 3, "endOffset": 7}, {"referenceID": 35, "context": "In [36], [37], image-wise similarities are measured by kernelized visual word co-occurrence matrices where a latent spatial kernel is integrated to handle spatial displacements.", "startOffset": 3, "endOffset": 7}, {"referenceID": 36, "context": "In [36], [37], image-wise similarities are measured by kernelized visual word co-occurrence matrices where a latent spatial kernel is integrated to handle spatial displacements.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "2 can be achieved by many off-the-shelf methods [14], [29], [38].", "startOffset": 48, "endOffset": 52}, {"referenceID": 28, "context": "2 can be achieved by many off-the-shelf methods [14], [29], [38].", "startOffset": 54, "endOffset": 58}, {"referenceID": 37, "context": "2 can be achieved by many off-the-shelf methods [14], [29], [38].", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "In this paper, we extract Dense SIFT and Dense Color Histogram [14] from each patch and utilize the KISSME metric [7] as the basic distance metric to compute \u03a6ij(fxi , fyj ).", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "In this paper, we extract Dense SIFT and Dense Color Histogram [14] from each patch and utilize the KISSME metric [7] as the basic distance metric to compute \u03a6ij(fxi , fyj ).", "startOffset": 114, "endOffset": 117}, {"referenceID": 32, "context": "Note that in order to achieve better performance [33], we learn multiple metrics so as to adaptively model the local patch-", "startOffset": 49, "endOffset": 53}, {"referenceID": 7, "context": "replacing KISSME with the kLFDA [8] and KMFA-R\u03c72 [39] metrics to demonstrate the effectiveness of our approach.", "startOffset": 32, "endOffset": 35}, {"referenceID": 38, "context": "replacing KISSME with the kLFDA [8] and KMFA-R\u03c72 [39] metrics to demonstrate the effectiveness of our approach.", "startOffset": 49, "endOffset": 53}, {"referenceID": 39, "context": "To address this problem, we introduce a global one-to-one mapping constraint and solve the resulting linear assignment task [40] to find the best matching:", "startOffset": 124, "endOffset": 128}, {"referenceID": 39, "context": "3 can be solved by the Hungarian method [40].", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "Based on the above process, we can calculate the image matching scores \u03c8 between a probe image and all gallery images in a cross-view camera, and rank the gallery images accordingly to achieve the final Re-ID result [13].", "startOffset": 216, "endOffset": 220}, {"referenceID": 13, "context": "searching ranges (from 26 to 32) by adjacency-constrained search [14], and then select the one which minimizes the rank order of V\u03b1\u2032 and use it as the approximated optimal binary mapping structure M\u03b1.", "startOffset": 65, "endOffset": 69}, {"referenceID": 40, "context": "(b)-(c): Examples of the correspondence structures learned by our approach for the VIPeR [41] data.", "startOffset": 89, "endOffset": 93}, {"referenceID": 41, "context": "P (M\u03b1) = R\u0303n(M\u03b1) \u2211 M\u03b3\u2208\u0393k R\u0303n(M\u03b3) is the prior probability for binary mapping structure M\u03b1, where R\u0303n(M\u03b1) is the CMC score at rank n [42] when using M\u03b1 as the correspondence structure to perform person Re-ID over the", "startOffset": 132, "endOffset": 136}, {"referenceID": 13, "context": "average appearance similarity [14], [7] between patches xi and yj over all correct match image pairs in the training set.", "startOffset": 30, "endOffset": 34}, {"referenceID": 6, "context": "average appearance similarity [14], [7] between patches xi and yj over all correct match image pairs in the training set.", "startOffset": 36, "endOffset": 39}, {"referenceID": 40, "context": "(a, d, g, j): The correspondence structures learned by our approach (with the KISSME metric) for the VIPeR [41], PRID 450S [9], 3DPeS [43], and SYSU-sReID [44] datasets, respectively (the correspondence structure for our ROAD dataset is shown in Figure 1).", "startOffset": 107, "endOffset": 111}, {"referenceID": 8, "context": "(a, d, g, j): The correspondence structures learned by our approach (with the KISSME metric) for the VIPeR [41], PRID 450S [9], 3DPeS [43], and SYSU-sReID [44] datasets, respectively (the correspondence structure for our ROAD dataset is shown in Figure 1).", "startOffset": 123, "endOffset": 126}, {"referenceID": 42, "context": "(a, d, g, j): The correspondence structures learned by our approach (with the KISSME metric) for the VIPeR [41], PRID 450S [9], 3DPeS [43], and SYSU-sReID [44] datasets, respectively (the correspondence structure for our ROAD dataset is shown in Figure 1).", "startOffset": 134, "endOffset": 138}, {"referenceID": 43, "context": "(a, d, g, j): The correspondence structures learned by our approach (with the KISSME metric) for the VIPeR [41], PRID 450S [9], 3DPeS [43], and SYSU-sReID [44] datasets, respectively (the correspondence structure for our ROAD dataset is shown in Figure 1).", "startOffset": 155, "endOffset": 159}, {"referenceID": 41, "context": "P\u0302 (mst|M\u03b1) = R\u0303n(mst) \u2211 mhg\u2208M\u03b1 R\u0303n(mhg) , (11) where R\u0303n(mst) is the rank-n CMC score [42] when only using a single link mst as the correspondence structure to perform Re-ID.", "startOffset": 87, "endOffset": 91}, {"referenceID": 44, "context": "More specifically, for a pair of test images (a probe image and a gallery image), we first parse the human pose information for both images [45] and classify them into their closest pose groups,", "startOffset": 140, "endOffset": 144}, {"referenceID": 44, "context": "Specifically, we first utilize semantic region parsing [45] to obtain head and hair regions from a test image.", "startOffset": 55, "endOffset": 59}, {"referenceID": 45, "context": "mation of head and hair regions, and apply a linear SVM [46] to classify this feature vector into one of the pose groups.", "startOffset": 56, "endOffset": 60}, {"referenceID": 46, "context": "ticated pose estimation methods [47], [48], [34] can be included to handle pose information parsing under more complicated scenarios.", "startOffset": 32, "endOffset": 36}, {"referenceID": 47, "context": "ticated pose estimation methods [47], [48], [34] can be included to handle pose information parsing under more complicated scenarios.", "startOffset": 38, "endOffset": 42}, {"referenceID": 33, "context": "ticated pose estimation methods [47], [48], [34] can be included to handle pose information parsing under more complicated scenarios.", "startOffset": 44, "endOffset": 48}, {"referenceID": 48, "context": "Dividing pose groups & pose group pairs automatically, where we first cluster training images [49] in each camera into a set of pose groups (using the same feature vector as described in point 1), and then form camera-", "startOffset": 94, "endOffset": 98}, {"referenceID": 48, "context": "Note that in order to automatically decide the number of clusters, we utilize a spectral clustering scheme [49] which is able to find the optimal cluster number by integrating a clustering-coherency metric to evaluate the results under different cluster numbers.", "startOffset": 107, "endOffset": 111}, {"referenceID": 40, "context": "The VIPeR dataset [41] is a commonly used dataset which contains 632 image pairs for 632 pedestrians, as in Figures 4a-4c and 8d.", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "The PRID 450S dataset [9] consists of 450", "startOffset": 22, "endOffset": 25}, {"referenceID": 42, "context": "The 3DPeS dataset [43] is comprised of 1012", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "For images from the same camera group, we simply utilize adjacency-constrained search [14] to find patch-wise mapping and calculate the image matching", "startOffset": 86, "endOffset": 90}, {"referenceID": 43, "context": "The SYSU-sReID dataset [44] contains 502 individual pairs taken by two disjoint cameras in a campus environment, as in Fig.", "startOffset": 23, "endOffset": 27}, {"referenceID": 4, "context": "[5], [8], [2] and perform experiments under 50%-training and 50%-testing.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[5], [8], [2] and perform experiments under 50%-training and 50%-testing.", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "[5], [8], [2] and perform experiments under 50%-training and 50%-testing.", "startOffset": 10, "endOffset": 13}, {"referenceID": 13, "context": "1) Patch matching performances: We compare the patch matching performance of three methods: (1) The adjacencyconstrained search method [14], [29], [15] which finds a best matched patch for each patch in a probe image (probe patch) by searching a fixed neighborhood region around the", "startOffset": 135, "endOffset": 139}, {"referenceID": 28, "context": "1) Patch matching performances: We compare the patch matching performance of three methods: (1) The adjacencyconstrained search method [14], [29], [15] which finds a best matched patch for each patch in a probe image (probe patch) by searching a fixed neighborhood region around the", "startOffset": 141, "endOffset": 145}, {"referenceID": 14, "context": "1) Patch matching performances: We compare the patch matching performance of three methods: (1) The adjacencyconstrained search method [14], [29], [15] which finds a best matched patch for each patch in a probe image (probe patch) by searching a fixed neighborhood region around the", "startOffset": 147, "endOffset": 151}, {"referenceID": 6, "context": "Moreover, in order to further evaluate the effectiveness of our correspondence structure learning process, we also show the learned correspondence structures when using different distance metrics (KISSME [7], kLFDA [8], and KMFAR\u03c72 [39]) to measure the patch-wise similarity (cf.", "startOffset": 204, "endOffset": 207}, {"referenceID": 7, "context": "Moreover, in order to further evaluate the effectiveness of our correspondence structure learning process, we also show the learned correspondence structures when using different distance metrics (KISSME [7], kLFDA [8], and KMFAR\u03c72 [39]) to measure the patch-wise similarity (cf.", "startOffset": 215, "endOffset": 218}, {"referenceID": 38, "context": "Moreover, in order to further evaluate the effectiveness of our correspondence structure learning process, we also show the learned correspondence structures when using different distance metrics (KISSME [7], kLFDA [8], and KMFAR\u03c72 [39]) to measure the patch-wise similarity (cf.", "startOffset": 232, "endOffset": 236}, {"referenceID": 41, "context": "ID results by the standard Cumulated Matching Characteristic (CMC) curve [42] which measures the correct match rates within different Re-ID rank ranges.", "startOffset": 73, "endOffset": 77}, {"referenceID": 4, "context": "The evaluation protocols are the same as [5].", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "Comparison of correspondence structures for the Road dataset when using different distance metrics (KISSME [7], kLFDA [8] and KMFA-R\u03c72 [39]) to measure the patch-wise similarity.", "startOffset": 107, "endOffset": 110}, {"referenceID": 7, "context": "Comparison of correspondence structures for the Road dataset when using different distance metrics (KISSME [7], kLFDA [8] and KMFA-R\u03c72 [39]) to measure the patch-wise similarity.", "startOffset": 118, "endOffset": 121}, {"referenceID": 38, "context": "Comparison of correspondence structures for the Road dataset when using different distance metrics (KISSME [7], kLFDA [8] and KMFA-R\u03c72 [39]) to measure the patch-wise similarity.", "startOffset": 135, "endOffset": 139}, {"referenceID": 6, "context": "Tables I\u2013V show the CMC results of different methods under three distance metrics: KISSME [7], kLFDA [8], and KMFA-R\u03c72 [39] (note that we use Dense SIFT and Dense Color Histogram features [14] for KISSME & kLFDA metrics, and use RGB, HSV, YCbCr, Lab, YIQ, 16 Gabor texture", "startOffset": 90, "endOffset": 93}, {"referenceID": 7, "context": "Tables I\u2013V show the CMC results of different methods under three distance metrics: KISSME [7], kLFDA [8], and KMFA-R\u03c72 [39] (note that we use Dense SIFT and Dense Color Histogram features [14] for KISSME & kLFDA metrics, and use RGB, HSV, YCbCr, Lab, YIQ, 16 Gabor texture", "startOffset": 101, "endOffset": 104}, {"referenceID": 38, "context": "Tables I\u2013V show the CMC results of different methods under three distance metrics: KISSME [7], kLFDA [8], and KMFA-R\u03c72 [39] (note that we use Dense SIFT and Dense Color Histogram features [14] for KISSME & kLFDA metrics, and use RGB, HSV, YCbCr, Lab, YIQ, 16 Gabor texture", "startOffset": 119, "endOffset": 123}, {"referenceID": 13, "context": "Tables I\u2013V show the CMC results of different methods under three distance metrics: KISSME [7], kLFDA [8], and KMFA-R\u03c72 [39] (note that we use Dense SIFT and Dense Color Histogram features [14] for KISSME & kLFDA metrics, and use RGB, HSV, YCbCr, Lab, YIQ, 16 Gabor texture", "startOffset": 188, "endOffset": 192}, {"referenceID": 38, "context": "features [39] for KMFA-R\u03c72 metric).", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "3) The AC+global method can be viewed as an extended version of the adjacency-constrained search method [14], [29], [15] plus a global constraint.", "startOffset": 104, "endOffset": 108}, {"referenceID": 28, "context": "3) The AC+global method can be viewed as an extended version of the adjacency-constrained search method [14], [29], [15] plus a global constraint.", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "3) The AC+global method can be viewed as an extended version of the adjacency-constrained search method [14], [29], [15] plus a global constraint.", "startOffset": 116, "endOffset": 120}, {"referenceID": 6, "context": "5) The improvement of our approach is coherent on different distance metrics (KISSME [7], kLFDA [8], and", "startOffset": 85, "endOffset": 88}, {"referenceID": 7, "context": "5) The improvement of our approach is coherent on different distance metrics (KISSME [7], kLFDA [8], and", "startOffset": 96, "endOffset": 99}, {"referenceID": 38, "context": "KMFA-R\u03c72 [39]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 42, "context": "Note that since the number of pedestrians for each pose is small in the 3DPeS [43] dataset, which is insufficient to construct reliable local correspondences, we focus on evaluating our multi-structure strategy on the other four datasets: VIPeR [41], PRID 450S [9], Road, and SYSU-sReID [44].", "startOffset": 78, "endOffset": 82}, {"referenceID": 40, "context": "Note that since the number of pedestrians for each pose is small in the 3DPeS [43] dataset, which is insufficient to construct reliable local correspondences, we focus on evaluating our multi-structure strategy on the other four datasets: VIPeR [41], PRID 450S [9], Road, and SYSU-sReID [44].", "startOffset": 245, "endOffset": 249}, {"referenceID": 8, "context": "Note that since the number of pedestrians for each pose is small in the 3DPeS [43] dataset, which is insufficient to construct reliable local correspondences, we focus on evaluating our multi-structure strategy on the other four datasets: VIPeR [41], PRID 450S [9], Road, and SYSU-sReID [44].", "startOffset": 261, "endOffset": 264}, {"referenceID": 43, "context": "Note that since the number of pedestrians for each pose is small in the 3DPeS [43] dataset, which is insufficient to construct reliable local correspondences, we focus on evaluating our multi-structure strategy on the other four datasets: VIPeR [41], PRID 450S [9], Road, and SYSU-sReID [44].", "startOffset": 287, "endOffset": 291}, {"referenceID": 2, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 113, "endOffset": 116}, {"referenceID": 8, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 125, "endOffset": 128}, {"referenceID": 28, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 139, "endOffset": 143}, {"referenceID": 14, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 154, "endOffset": 158}, {"referenceID": 49, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 166, "endOffset": 170}, {"referenceID": 17, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 176, "endOffset": 180}, {"referenceID": 7, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 188, "endOffset": 191}, {"referenceID": 7, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 197, "endOffset": 200}, {"referenceID": 22, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 207, "endOffset": 211}, {"referenceID": 19, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 217, "endOffset": 221}, {"referenceID": 36, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 229, "endOffset": 233}, {"referenceID": 25, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 246, "endOffset": 250}, {"referenceID": 32, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 257, "endOffset": 261}, {"referenceID": 18, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 268, "endOffset": 272}, {"referenceID": 38, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 18, "endOffset": 22}, {"referenceID": 34, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 29, "endOffset": 33}, {"referenceID": 31, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 42, "endOffset": 46}, {"referenceID": 24, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 55, "endOffset": 59}, {"referenceID": 26, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 103, "endOffset": 106}, {"referenceID": 5, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 140, "endOffset": 143}, {"referenceID": 49, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 151, "endOffset": 155}, {"referenceID": 7, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 161, "endOffset": 164}, {"referenceID": 7, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 172, "endOffset": 175}, {"referenceID": 38, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 195, "endOffset": 199}, {"referenceID": 26, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 209, "endOffset": 213}, {"referenceID": 50, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 245, "endOffset": 249}, {"referenceID": 7, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 257, "endOffset": 260}, {"referenceID": 49, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 268, "endOffset": 272}, {"referenceID": 7, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 278, "endOffset": 281}, {"referenceID": 7, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 293, "endOffset": 296}, {"referenceID": 13, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 9, "endOffset": 13}, {"referenceID": 49, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 31, "endOffset": 34}, {"referenceID": 7, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 46, "endOffset": 49}, {"referenceID": 13, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 84, "endOffset": 88}, {"referenceID": 49, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 96, "endOffset": 100}, {"referenceID": 7, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 117, "endOffset": 120}, {"referenceID": 43, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 128, "endOffset": 132}, {"referenceID": 33, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 143, "endOffset": 147}, {"referenceID": 33, "context": "Note that since the rotation and orientation parameters used in the TA+W method [34] are derived from camera calibration", "startOffset": 80, "endOffset": 84}, {"referenceID": 33, "context": ", guarantee that [34]\u2019s performance will not be degraded due to improper rotation and orientation parameters).", "startOffset": 17, "endOffset": 21}, {"referenceID": 13, "context": ", adding multiple Re-ID results together to achieve a higher Re-ID result) [14], [29], [4], we also show a fusion result of our approach (svmml+Proposed+multi-manu-KMFA) and compare it with the other fusion results in Table VI.", "startOffset": 75, "endOffset": 79}, {"referenceID": 28, "context": ", adding multiple Re-ID results together to achieve a higher Re-ID result) [14], [29], [4], we also show a fusion result of our approach (svmml+Proposed+multi-manu-KMFA) and compare it with the other fusion results in Table VI.", "startOffset": 81, "endOffset": 85}, {"referenceID": 3, "context": ", adding multiple Re-ID results together to achieve a higher Re-ID result) [14], [29], [4], we also show a fusion result of our approach (svmml+Proposed+multi-manu-KMFA) and compare it with the other fusion results in Table VI.", "startOffset": 87, "endOffset": 90}, {"referenceID": 22, "context": ", IDLA [23], JLR [20], and DG-Dropout [26] in Ta-", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": ", IDLA [23], JLR [20], and DG-Dropout [26] in Ta-", "startOffset": 17, "endOffset": 21}, {"referenceID": 25, "context": ", IDLA [23], JLR [20], and DG-Dropout [26] in Ta-", "startOffset": 38, "endOffset": 42}, {"referenceID": 26, "context": "for Re-ID, it can easily accommodate future advances by combining with the deep-neural-network-based features (such as [27]) without changing the core protocols.", "startOffset": 119, "endOffset": 123}, {"referenceID": 33, "context": "3) Since the TA+W method [34] creates multiple weight matrices for pedestrians with different orientations, it can be viewed as another version of using mul-", "startOffset": 25, "endOffset": 29}, {"referenceID": 33, "context": "However, our approach (proposed+multi-manu) still outperforms the TA+W method [34] (cf.", "startOffset": 78, "endOffset": 82}, {"referenceID": 2, "context": "Rank 1 5 10 20 30 RankBoost [3] 23.", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "7 JLCF [4] 26.", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": "1 - KISSME [9] 27 - 70 83 SalMatch [29] 28.", "startOffset": 11, "endOffset": 14}, {"referenceID": 28, "context": "1 - KISSME [9] 27 - 70 83 SalMatch [29] 28.", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "3 - - DSVR FSA [15] 29.", "startOffset": 15, "endOffset": 19}, {"referenceID": 49, "context": "9 svmml [50] 30.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "1 ELS [18] 31.", "startOffset": 6, "endOffset": 10}, {"referenceID": 7, "context": "7 kLFDA [8] 32.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "9 MFA [8] 32.", "startOffset": 6, "endOffset": 9}, {"referenceID": 22, "context": "6 IDLA [23] 34.", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": "5 - - JLR [20] 35.", "startOffset": 10, "endOffset": 14}, {"referenceID": 36, "context": "0 - - PRISM [37] 36.", "startOffset": 12, "endOffset": 16}, {"referenceID": 3, "context": "2 JLCF-fusing [4] 38.", "startOffset": 14, "endOffset": 17}, {"referenceID": 25, "context": "0 - DG-Dropout [26] 38.", "startOffset": 15, "endOffset": 19}, {"referenceID": 32, "context": "6 - - - DPML [33] 41.", "startOffset": 13, "endOffset": 17}, {"referenceID": 18, "context": "5 LOMO [19] 42.", "startOffset": 7, "endOffset": 11}, {"referenceID": 38, "context": "1 Mirror-KMFA(R\u03c72 ) [39] 43.", "startOffset": 20, "endOffset": 24}, {"referenceID": 34, "context": "DCSL [35] 44.", "startOffset": 5, "endOffset": 9}, {"referenceID": 28, "context": "6 - svmml+SalMatch [29] 44.", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "4 - - deCPPs [32] 44.", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "8 - deCPPs+MER [32] 47.", "startOffset": 15, "endOffset": 19}, {"referenceID": 24, "context": "8 - MCP-CNN [25] 47.", "startOffset": 12, "endOffset": 16}, {"referenceID": 26, "context": "3 FNN [27] 51.", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "9 LOMO-fusing [19] 51.", "startOffset": 14, "endOffset": 18}, {"referenceID": 8, "context": "Rank 1 5 10 20 30 KISSME [9] 33 - 71 79 EIML [6] 35 - 68 77 SCNCD [2] 41.", "startOffset": 25, "endOffset": 28}, {"referenceID": 5, "context": "Rank 1 5 10 20 30 KISSME [9] 33 - 71 79 EIML [6] 35 - 68 77 SCNCD [2] 41.", "startOffset": 45, "endOffset": 48}, {"referenceID": 1, "context": "Rank 1 5 10 20 30 KISSME [9] 33 - 71 79 EIML [6] 35 - 68 77 SCNCD [2] 41.", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "4 SCNCDFinal [2] 41.", "startOffset": 13, "endOffset": 16}, {"referenceID": 49, "context": "8 svmml [50] 42.", "startOffset": 8, "endOffset": 12}, {"referenceID": 7, "context": "2 MFA [8] 44.", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "9 kLFDA [8] 46.", "startOffset": 8, "endOffset": 11}, {"referenceID": 38, "context": "4 Mirror-KMFA(R\u03c72 ) [39] 55.", "startOffset": 20, "endOffset": 24}, {"referenceID": 26, "context": "FNN [27] 66.", "startOffset": 4, "endOffset": 8}, {"referenceID": 49, "context": "Rank 1 5 10 20 30 svmml [50] 34.", "startOffset": 24, "endOffset": 28}, {"referenceID": 50, "context": "5 PCCA [51] 41.", "startOffset": 7, "endOffset": 11}, {"referenceID": 7, "context": "4 rPCCA [8] 47.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "9 MFA [8] 48.", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "8 kLFDA [8] 54.", "startOffset": 8, "endOffset": 11}, {"referenceID": 25, "context": "4 DG-Dropout [26] 56.", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "Rank 1 5 10 20 30 eSDC-knn [14] 52.", "startOffset": 27, "endOffset": 31}, {"referenceID": 49, "context": "8 svmml [50] 57.", "startOffset": 8, "endOffset": 12}, {"referenceID": 7, "context": "6 MFA [8] 58.", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "3 kLFDA [8] 59.", "startOffset": 8, "endOffset": 11}, {"referenceID": 8, "context": "Rank 1 5 10 20 30 KISSME [9] 28.", "startOffset": 25, "endOffset": 28}, {"referenceID": 49, "context": "0 svmml [50] 30.", "startOffset": 8, "endOffset": 12}, {"referenceID": 7, "context": "3 MFA [8] 32.", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "4 kLFDA [8] 33.", "startOffset": 8, "endOffset": 11}, {"referenceID": 43, "context": "0 MLACM [44] 32.", "startOffset": 8, "endOffset": 12}, {"referenceID": 33, "context": "2 \u2013 TA-W [34] 43.", "startOffset": 9, "endOffset": 13}, {"referenceID": 51, "context": "This implies that our testing process can be further optimized by replacing Hungarian method with other more efficient algorithms [52].", "startOffset": 130, "endOffset": 134}, {"referenceID": 41, "context": "Figures 12a to 12b show the rank-1 CMC scores [42] of", "startOffset": 46, "endOffset": 50}], "year": 2017, "abstractText": "This paper addresses the problem of handling spatial misalignments due to camera-view changes or humanpose variations in person re-identification. We first introduce a boosting-based approach to learn a correspondence structure which indicates the patch-wise matching probabilities between images from a target camera pair. The learned correspondence structure can not only capture the spatial correspondence pattern between cameras but also handle the viewpoint or humanpose variation in individual images. We further introduce a global constraint-based matching process. It integrates a global matching constraint over the learned correspondence structure to exclude cross-view misalignments during the image patch matching process, hence achieving a more reliable matching score between images. Finally, we also extend our approach by introducing a multi-structure scheme, which learns a set of local correspondence structures to capture the spatial correspondence sub-patterns between a camera pair, so as to handle the spatial misalignments between individual images in a more precise way. Experimental results on various datasets demonstrate the effectiveness of our approach. The project page for this paper is available at http://min.sjtu.edu.cn/lwydemo/personReID.htm", "creator": "LaTeX with hyperref package"}}}