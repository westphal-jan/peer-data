{"id": "1703.06971", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "Active Decision Boundary Annotation with Deep Generative Models", "abstract": "This paper is on active learning where the goal is to reduce the data annotation burden by interacting with a (human) oracle during training. Standard active learning methods ask the oracle to annotate data samples. Instead, we take a profoundly different approach: we ask for annotations of the decision boundary in the design. The choice is based on the choice of annotation choice and its annotation type.\n\n\n\nThe choice of annotation type is based on the choice of annotation type. What we want to see in the choice of annotations is how the choice may affect the choice of annotation type. We can see here that we may interpret an annotation type as one that we expect, and that our chosen annotation type is a choice that is considered to be appropriate and can also be interpreted as a choice that is considered to be appropriate and can also be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is considered to be appropriate and can be interpreted as a choice that is", "histories": [["v1", "Mon, 20 Mar 2017 21:20:21 GMT  (971kb,D)", "http://arxiv.org/abs/1703.06971v1", "ICCV submission"], ["v2", "Wed, 2 Aug 2017 09:36:55 GMT  (993kb,D)", "http://arxiv.org/abs/1703.06971v2", "ICCV 2017"]], "COMMENTS": "ICCV submission", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["miriam w huijser", "jan c van gemert"], "accepted": false, "id": "1703.06971"}, "pdf": {"name": "1703.06971.pdf", "metadata": {"source": "CRF", "title": "Active Decision Boundary Annotation with Deep Generative Models", "authors": ["Miriam W. Huijser", "Jan C. van Gemert"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "If data is king, then annotation labels are its crown jewels. Big image data sets are relatively easy to obtain; it\u2019s the ground truth labels that are expensive [8, 21]. With the huge success of deep learning methods critically depending on large annotated datasets, there is a strong demand for reducing the annotation effort [17, 23, 25].\nIn active learning [27] the goal is to train a good predictor while minimizing the human annotation effort for large unlabeled data sets. During training, the model can interact with a human oracle who provides ground truth annotations on demand. The challenge is to have the model automatically select a small set of the most informative annotations so that prediction performance is maximized.\nIn this paper we exploit the power of deep generative models for active learning. Existing active learning methods [19, 27, 31] typically ask the oracle to label an existing data sample. Instead, we take a radically different approach: we ask the oracle to directly annotate the decision boundary itself. To achieve this, we first use all unlabeled images to learn a K-dimensional embedding. In this K-dimensional embedding we select a 1-dimensional query line and em-\nploy a deep generative model to generate visual samples along this line. We visualize the 1-dimensional line as an ordered row of images and simply ask the oracle to visually annotate the point where the generated visual samples changes classes. Since the generated images are ordered, the oracle does not need to examine and annotate each and every image, merely identifying the change point is enough. The point between two samples of different classes is a point that lies close to the decision boundary and we use that point to improve the classification model. In figure 1 we show an illustration of our approach.\n1\nar X\niv :1\n70 3.\n06 97\n1v 1\n[ cs\n.C V\n] 2\n0 M\nar 2\n01 7\nWe make the following contributions. First, we use a deep generative model to present a 1-dimensional query line to the oracle. Second, we directly annotate the decision boundary instead of querying and labeling data samples. Third, we learn a decision boundary from both labeled instances and boundary annotations. Fourth, we evaluate if the generative model is good enough to construct query lines that a human oracle can annotate, how much noise the decision boundary annotations allow, and how our decision boundary annotation version of active learning compares to traditional active learning by data sample labeling."}, {"heading": "2. Related work", "text": "Active learning [27] is an iterative framework and starts with the initialization of a prediction model either by training on a small set of labeled samples or by random initialization. Subsequently, a query strategy is used to interactively query an oracle, which can be another model or a human annotator. The annotated query is then used to retrain the model and starts the next iteration. Active learning in computer vision includes work on selecting the most influential images [12], refraining from labeling unclear visuals [16], zero-shot transfer learning [13], multi-label active learning [32]. Similar to these methods, our paper uses active learning in the visual domain to minimize the number of iterations while maximizing prediction accuracy.\nThere are several settings of active learning. A poolbased setting [19, 31] assumes the availability of a large set of unlabeled instances. Instead, a stream-based setting [3, 7] is favorable for online learning where a query is selectively sampled from an incoming data stream. Alternatively, in a query synthesis setting [2, 4, 36] the system is input with a data distribution and queries to the oracle are generated to lie in the input space. Recently, [1] and [6] proposed methods for efficiently learning halfspaces, i.e. linear classifiers, using synthesized instances. Our paper proposes a new active learning setting: active boundary annotation. All other active learners use sample instances to query the oracle. Instead, we generate a row of instances and query the point where the instances change class label. We do not query an annotation of a sample, we query an annotation of the decision boundary.\nQuery strategies in active learning are the informativeness measures used to select or generate new queries. Much work has been done on this topic [27], here we describe a few prominent strategies. Uncertainty sampling [19] is a query strategy that selects the unlabeled sample of which the current model is least certain. This could be obtained by sampling closest to the decision boundary [5, 31] or based on entropy [20]. The Uncertainty-dense sampling method [28, 35] aims to correct for the problems associated with uncertainty sampling by selecting samples that are not only uncertain but that also lie in dense areas of\nthe data distribution and are thus representative of the data. In Batch methods [15, 30, 33], not a single sample is queried at each iteration, but a set of samples. In Queryby-committee [29] multiple models are used as a committee and queries the samples where the disagreement is highest. Our active boundary annotation method can plug-in any sampling method and thus does not depend on a particular query strategy. We will experimentally demonstrate that our boundary annotation method can readily be applied to various query strategies.\nIn our active learning approach we make use of deep generative models. Probabilistic generative models such as variational autoencoders [17, 18], learn an inference model to map images to a latent space and a decoder to map from latent space back again to image space. Unfortunately the generated images are sometimes not very sharp. Non-probabilistic models such as the generative adversarial nets [14], produce higher-quality images than variational autoencoders [9, 26], but can only map from latent space to image space. These models randomly sample latent vectors from a predefined distribution and then learn a mapping from these latent vectors to images; there is no mapping that can embed real images in the latent space. To perform classification on real images in the embedding we need an inference step to map images to the latent space. Fortunately, recent generative adversarial models can produce high-quality images and provide efficient inference [10, 11]. In this paper we use such a GAN model."}, {"heading": "3. Active decision boundary annotation", "text": "We have N data samples x \u2208 RD, each sample is paired with a class label (X,Y) = {(x1, y1), . . . , (xN , yN )} where for clarity we focus on the binary case y \u2208 {\u22121, 1} and a linear classification model. As often done in active learning we assume an initial set A which contains a handful of annotated images. Each data sample xi has a corresponding latent variable zi \u2208 RK . For clarity we omit the index i whenever it is clear that we refer to a single data point. The tightest hypersphere that contains all latent variables zi is denoted by \u2126. Every iteration in active learning estimates a decision boundary \u03b8\u0302 where the goal is to best approximate the real decision boundary \u03b8 while minimizing the number of iterations.\nIn figure 2 we show an overview of our method. Each image x is embedded in a manifold as a latent variable Gz(x) = z. In this embedding we use a standard active learning query strategy to select the most informative query sample z\u2217. We then construct a query line q in such a way that it intersects the query sample z\u2217 and is perpendicular to the current estimate of the decision boundary \u03b8\u0302 at point zp. We uniformly sample latent points z along the 1- dimensional query line q and for each point on the line generate an estimate of the corresponding image Gx(z) = x\u0302.\nOn this generated row of images we ask the oracle to provide the point where the images change class, this is where the decision boundary intersects the query line q \u2229 \u03b8 and the latent variable zq\u2229\u03b8 is a decision boundary annotation. Using the boundary annotation we can assign a label to the query sample z\u2217 which we add to the set of annotated samples A. All annotated decision boundary points are stored in the set B. The estimate of the decision boundary \u03b8\u0302 is found by optimizing a joint classification/regression loss. The classification loss is computed on the labeled samples A while at the same time the regression aims to fit the decision boundary through the annotations in B.\nDeep generative embedding. We make use of GANs (Generative Adversarial Nets) [14] to obtain a high-quality embedding. In GANs, a generative model G can create realistic-looking images from a latent random variable G(z) = x\u0302. The generative model is trained by making use\nof a strong discriminator D that tries to separate synthetic generated images from real images. Once the discriminator cannot tell synthetic images from real images, the generator is well trained. The generator and discriminator are simultaneously trained by playing a two-player minimax game, for details see [14].\nDeep generative inference. Because we perform classification in the embedding we need an encoder to map the image samples to the latent space. Thus, in addition to a mapping from latent space to images (decoding) as in a standard GAN, we also need an inference model to map images to latent space (encoding) [10, 11]. This is done by optimizing two joint distributions over (x, z): one for the encoder q(x, z) = q(x)q(z|x) and one for the decoder p(x, z) = p(z)p(x|z). Since both marginals are known, we can sample from them. The encoder marginal q(x) is the empirical data distribution and the decoder marginal is defined as p(z) = N (0, I). The encoder and the decoder are trained together to fool the discriminator. This is achieved by playing an adversarial game to try and match the encoder and decoder joint distributions where the adversarial game is played between the encoder and decoder on the one side and the discriminator on the other side, for details see [11].\nQuery strategy. To make our method compatible with standard active learning methods we allow plugging in any query strategy that selects an informative query sample z\u2217. We make such a plug in possible by making sure that the query sample z\u2217 is part of the query line q. An example of a popular query sample strategy is uncertainty sampling that selects the sample whose prediction is the least confident: z\u2217 = arg maxz 1 \u2212 P\u03b8\u0302(y\u0302|z), where y\u0302 is the class label with the highest posterior probability under the current prediction model \u03b8\u0302. This can be interpreted as the sample where the model is least certain about. In the experiments we will show that our method is compatible with common query strategies.\nConstructing the query line. The query line q determines which images will be shown to the oracle. To make decision boundary annotation possible these images should undergo a class-change. A reasonable strategy is then to make sure the query line intersects the current estimate of the decision boundary \u03b8\u0302. A crisp class change will make annotating the point of change easier. Thus, we increase the likelihood of a crisp class-change by ensuring that q is perpendicular to \u03b8\u0302. Since the query sample z\u2217 lies on the query line and q \u22a5 \u03b8\u0302 this is enough information to construct the query line q. Let the current estimation of the linear decision boundary \u03b8\u0302 be a hyperplane which is parametrized by a vector w\u0302 perpendicular to the plane and offset with a bias b\u0302. Then the query line is given by q(t) = zp + (z\u2217 \u2212 zp)t, where zp is the projection of the query point z\u2217 on the current decision boundary \u03b8\u0302 and is defined as zp = z\u2217 \u2212 (w\u0302\n\u1d40z\u2217+b\u0302) w\u0302\u1d40w\u0302 w\u0302.\nConstructing the image row. We have to make a choice about which image samples along the query line q to show to the oracle. We first restrict the size of q to lie within the tightest hypersphere \u2126 that captures all latent data samples. This is defined as \u2126 = {z \u2208 RK : ||z \u2212 z\u0304|| \u2264 r} where r = maxNi=0 ||zi \u2212 z\u0304|| and z\u0304 is the average vector over all latent samples z. As a second step we have to choose a sub-sampling of q. We uniformly sample s samples from q within \u2126, where s depends on the sampling resolution rs and the length of the line as follows: s = length q within \u2126rs + 1 This sampling yields an ordered list of latent variables\nIz = ( z \u2208 RK : z \u2208 \u2126 \u2229 { zp + (z\u2217 \u2212 zp)t,\nt \u2208 (tmin, ..., tmin + tmax \u2212 tmin s\u2212 1\n..., tmax) }) , (1)\nwhere tmin and tmax correspond to the points zmin and zmax where q intersects the hypersphere \u2126. Each latent variable in this list is then decoded to an ordered list of images with the deep generative model G(z) = x\u0302.\nAnnotating the decision boundary. Annotating the decision boundary can be done by a human oracle or by a given model. A model is often used as a convenience to do large scale experiments where human experiments are too time consuming. For example, large scale experiments for active learning methods that require query sample annotation are typically performed by replacing the human annotation by the ground truth annotation. This works under the assumption that the human will annotate the same as the ground truth. This assumption may easily be violated especially when samples are close to the decision boundary. To do large scale experiments for our decision boundary annotation method we also use a model based on the ground truth, like commonly done for query sample annotation. We train a oracle-classifier on ground truth labels and use the that model as a ground truth decision boundary where the intersection zq\u2229\u03b8 = q \u2229 \u03b8 can be computed. For both oracle types \u2013the oracle-classifier and the oracle human annotations\u2013 we store the decision boundary annotations in B and we also ask the annotators for the label y of the query point z\u2217, for which we store the pair (z\u2217, y) in A. We experimentally evaluate human and model-based annotations.\nModel optimization using boundary annotations. At every iteration of active learning we update \u03b8\u0302. We use a classification loss on the labeled samples in A while at the same time we optimize a regression loss to fit the decision boundary through the annotations in B. In this paper we restrict ourselves to the linear case and parametrize \u03b8\u0302 with a linear model w\u0302\u1d40z + b\u0302 = 0.\nFor the classification loss we use a standard SVM hinge loss over the labeled samples (z, y) in A as\nLclass = 1 |A| \u2211\n(z,y)\u2208A\nmax ( 0, 1\u2212 y(w\u0302\u1d40z + b\u0302) ) . (2)\nFor the regression, we use a simple squared loss to fit the model w\u0302 + b\u0302 to the annotations in B\nLregress = 1 |B| \u2211 z\u2208B ( w\u0302\u1d40z + b\u0302 )2 . (3)\nThe final loss L jointly weights the classification and regression losses equally and simply becomes\nL = 1 2 Lclass + 1 2 Lregress + \u03bb||w\u0302||2, (4)\nwhere the parameter \u03bb controls the influence of the regularization term ||w\u0302||2 where we use \u03bb = 1 in all experiments. Note that because Lclass and Lregress are both convex losses, the joint loss L is convex as well."}, {"heading": "4. Experiments", "text": "We perform active learning experiments on three datasets. MNIST contains 60,000 binary digit images, 50k to train and 10k in the test set. The SVHN dataset [22] contains challenging digit images from Google streetview, it has 73,257 train and 26,032 test images. In addition, the SVHN dataset has 531,131 extra images, which we use to train the embedding. For the third dataset we want to evaluate our method on more variable images than digits. We create the Shoe-Bag dataset of 40,000 train and 14,000 test images by taking subsets from the Handbags dataset [37] and the Shoes dataset [34].\nFor every dataset we train a deep generative embedding following [11]. For MNIST and SVHN we train an embedding with 100 dimensions, for the more varied Shoe-Bag we train an embedding of 256 dimensions. For the training of the embeddings we set dropout = 0.4 for the layers of the discriminator. All embeddings are trained on the train data, except for the SVHN embedding; which is trained on the larger \u201cextra\u201d dataset following [11]. We will make our code available. All learning is done in the embedding and the experiments that do not use a human oracle use an SVM trained on all labels as the oracle.\nWe evaluate active learning with the Area Under the (accuracy) Learning Curve (AULC) measure [24, 28]. The Area under the Learning Curve is computed by integrating over the test accuracy scores of N active learning iterations using the trapezoidal rule:\nAULC = N\u2211 i=1 1 2 (acci\u22121 + acci), (5)\nwhere acc0 is the test accuracy of the initial classifier before the first query. The AULC score is higher for active learning methods that quickly learn high-performing models with few queries, i.e. in few iterations, because these have a larger area under their accuracy curves. We adopt the AULC measure for our evaluation.\nWe first evaluate essential properties of our method on the three datasets where for MNIST and SVHN we use a single pair: \u20180\u2019 versus \u20188\u2019. Later we evaluate on the full sets."}, {"heading": "4.1. Exp 1: Evaluating various query strategies", "text": "Our method can re-use standard active learning query strategies that select a sample for oracle annotation. We evaluate three sample-based query strategies. Uncertainty sampling selects the least confident sample point [19]. Uncertainty-dense sampling [28] selects samples that are not only uncertain but that also lie in dense areas of the data distribution. The K-cluster centroid [30] uses batches of K samples, where we set K = 5. We plug in each query sample strategy in our line query construction approach used for decision boundary annotation.\nThe results in Table 1 show that for all three datasets and for all three query strategies our boundary annotations outperform sample annotations. For the uncertainty-dense method the improvement is the largest, which may be due to this method sampling from dense areas of the distribution, and boundary annotations add complementary information. The uncertainty sampling gives best results for both active learning methods and all three datasets. It is also the strategy where our method improves the least, and is thus the most challenging to improve upon. We select uncertainty sampling for the remainder of the experiments."}, {"heading": "4.2. Exp 2: Evaluating generative model quality", "text": "The generative model that we plug into our method should be able to construct recognizable line queries so that human oracles can annotate them. In figure 3 we show some line queries generated for all three datasets by our active\nlearning method with uncertainty sampling. Some query lines are difficult to annotate, as shown in figure 3(b) and others are of good quality as shown in figure 3(a).\nWe quantitatively evaluate the generation quality per dataset by letting 10 humans each annotate the same 10 line queries. Because the line queries intersect \u2126 at different locations, they very in length. Line queries are subsampled to have a sample resolution rs = 0.25, i.e. the distance between each image on the line. The human annotators are thus presented with more images for longer query lines and fewer images for shorter query lines. For all 10 line queries we evaluate the inter-human annotation consistency. A higher inter-human annotation consistency suggests that the query line is well-recognizable and thus that the generative model has a good image generation quality. We instructed the human oracles to annotate the point where they saw a class change; or indicate if they see no change, this happens for 8 out of the 30 lines.\nIn Table 2 we show the results for the inter-human annotation consistency. The Shoe-Bag embedding does not seem to be properly trained because the human annotators see no change in half of the query lines. In addition, the variance\nbetween the images make the consistency lower. MNIST has a deviation of 4 images and 2 lines were reported with no change. SVHN provides the highest quality query lines - the human annotators agreed collectively on the inadequacy of only one query line and the human annotators are most consistent for this dataset."}, {"heading": "4.3. Exp 3: Evaluating annotation noise", "text": "In experiment 2 we show that there is variation in the annotations between human oracles. Here we aim to answer\nthe question if that variation matters. We evaluate the effect of query line annotation noise on the classification performance. We vary the degree of additive line annotation noise with respect to SVM oracle decision boundary annotations on the 1-dimensional line query. We vary the standard deviation \u03c3 of Gaussian noise to \u03c3 \u2208 {1, . . . , 5} image samples away from the oracle.\nThe results in Table 3 show that adding sampling noise of up to about \u03c3 = 4 images to the SVM oracle annotations has a slight negative effect on the performance of our\nboundary annotation method, but it is still significantly better than sample annotation. Comparing these results to the inter-human annotation consistency results in Table 2 shows that Shoe-Bag annotation variation is around 9, and thus the quality of the generator will likely have a negative performance on accuracy. For MNIST and SVHN the human consistency is around or below 4 images, which our method thus should be able to handle."}, {"heading": "4.4. Exp 4: Evaluating a human oracle", "text": "In this experiment we evaluate classification performance with a human oracle. For all three datasets we have a human oracle annotate the first 10 line queries, selected using uncertainty sampling. We repeat the same experimental setup for sample-based active learning. The results are averaged over 15 repetitions.\nThe results in Table 4 show that an oracle-SVM outperforms a human annotator. This is probably because the active learner method that is being trained is also an SVM, and since the oracle is also an SVM it will choose the perfect samples. For humans, boundary annotation always improves over sample annotation. Yet, for SVHN and ShoeBag this improvement is not significant. This is probably\ndue to the small number of queries, where our method after only 10 iterations has not yet achieved peak performance as corroborated by the learning curves in figure 4."}, {"heading": "4.5. Exp 5: Generalization over classes", "text": "Up to now we have shown that our method outperforms sample-based active learning on a subset of MNIST and SVHN. To see whether our method generalizes to the other classes we evaluate the performance averaged over all the SVHN and MNIST class pairs using uncertainty sampling as query strategy. We show results in Table 5 and plot the learning curves in figure 4. Results show that our method indeed generalizes to the other SVHN and MNIST classes. Averaged over all datasets and class pairs our method is significantly better than the sample-based approach."}, {"heading": "5. Discussion", "text": "We extend active learning with a method for decision boundary annotation. We use a deep generative model to synthesize new images along a 1-dimensional query line, and ask an oracle to annotate the point where the images change class: this point is an annotation of the decision boundary.\nOne disadvantage of our method is that it is very easy to annotate changes visually, but this is not so straightforward in other domains. The core of our method can in principle also be used on any input data, but actually using a human oracle to detect a class change for non-visual data would become tedious fast. For example, having a human annotate a class-change for raw sensor data, speech or text documents would be quite difficult in practice.\nAnother problem is precisely annotating the boundary when the margin between classes is large. With a large margin the generated samples may all look similar to each other and it is difficult to annotate the class change. One direction for future work could be to annotate the margin on each side instead of the boundary.\nOur method depends critically on the quality of the generative model. We specifically evaluated this by including the Shoe-Bag dataset where the quality of the generated samples impairs the consistency of human annotation. If the generative model is of low quality, our method will fail as well. GANs is an active area of reserach, so we are con-\nfident that the quality of generative models will improve. One possible direction for future work could be to exploit the knowledge of the annotated decision boundary to update the generative model.\nIn this work we consider linear models only. For nonlinear models, a non-linear 1-dimensional query line could perhaps work better. Also, when data sets are not linearly separable we may require more than one annotation of the decision boundary for 1 query line. This is left for future work.\nOur paper showed that boundary annotation for visual data is possible and improves results over only labeling query samples. We show that our method can plug-in existing active learning strategies, that humans can consistently annotate the boundary if the generative model is good enough, that our method is robust to noise, and that it significantly outperforms sample-based methods for all classes and data sets."}], "references": [{"title": "Efficient active learning of halfspaces via query synthesis", "author": ["I.M. Alabdulmohsin", "X. Gao", "X. Zhang"], "venue": "In AAAI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Queries and concept learning", "author": ["D. Angluin"], "venue": "Machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1988}, {"title": "Training connectionist networks with queries and selective sampling", "author": ["L.E. Atlas", "D.A. Cohn", "R.E. Ladner", "M.A. El-Sharkawi", "R.J. Marks", "M. Aggoune", "D. Park"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1989}, {"title": "Query learning can work poorly when a human oracle is used", "author": ["E.B. Baum", "K. Lang"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1992}, {"title": "Query learning with large margin classifiers", "author": ["C. Campbell", "N. Cristianini", "A. Smola"], "venue": "In ICML, pages 111\u2013118,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Dimension coupling: Optimal active learning of halfspaces via query synthesis", "author": ["L. Chen", "H. Hassani", "A. Karbasi"], "venue": "arXiv preprint arXiv:1603.03515,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Improving generalization with active learning", "author": ["D. Cohn", "L. Atlas", "R. Ladner"], "venue": "Machine learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1994}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "In CVPR. IEEE,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E.L. Denton", "S. Chintala", "a. szlam", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Adversarial feature learning", "author": ["J. Donahue", "P. Kr\u00e4henb\u00fchl", "T. Darrell"], "venue": "arXiv preprint arXiv:1605.09782,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Adversarially learned inference", "author": ["V. Dumoulin", "I. Belghazi", "B. Poole", "A. Lamb", "M. Arjovsky", "O. Mastropietro", "A. Courville"], "venue": "arXiv preprint arXiv:1606.00704,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Selecting influential examples: Active learning with expected model output changes", "author": ["A. Freytag", "E. Rodner", "J. Denzler"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Active transfer learning with zero-shot priors: Reusing past datasets for future tasks", "author": ["E. Gavves", "T. Mensink", "T. Tommasi", "C.G.M. Snoek", "T. Tuytelaars"], "venue": "In ICCV,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Discriminative batch mode active learning", "author": ["Y. Guo", "D. Schuurmans"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Active learning and discovery of object categories in the presence of unnameable instances", "author": ["C. Kading", "A. Freytag", "E. Rodner", "P. Bodesheim", "J. Denzler"], "venue": "In CVPR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "S. Mohamed", "D.J. Rezende", "M. Welling"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "ICLR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "A sequential algorithm for training text classifiers", "author": ["D. Lewis", "G.A. William"], "venue": "In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}, {"title": "Heterogeneous uncertainty sampling for supervised learning", "author": ["D.D. Lewis", "J. Catlett"], "venue": "In Proceedings of the eleventh international conference on machine learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In ECCV,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Ambient sound provides supervision for visual learning", "author": ["A. Owens", "J. Wu", "J.H. McDermott", "W.T. Freeman", "A. Torralba"], "venue": "In ECCV. Springer,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Model-free and model-based active learning for regression", "author": ["J. ONeill", "S.J. Delany", "B. MacNamee"], "venue": "In Advances in Computational Intelligence Systems,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2017}, {"title": "Context encoders: Feature learning by inpainting", "author": ["D. Pathak", "P. Krahenbuhl", "J. Donahue", "T. Darrell", "A.A. Efros"], "venue": "In CVPR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "University of Wisconsin, Madison,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "An analysis of active learning strategies for sequence labeling tasks. In Proceedings of the conference on empirical methods in natural language processing, pages 1070\u20131079", "author": ["B. Settles", "M. Craven"], "venue": "Association for Computational Linguistics,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Query by committee", "author": ["H.S. Seung", "M. Opper", "H. Sompolinsky"], "venue": "In Proceedings of the fifth annual workshop on Computational learning theory,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1992}, {"title": "Active feedback in ad hoc information retrieval", "author": ["X. Shen", "C. Zhai"], "venue": "In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2005}, {"title": "Support vector machine active learning with applications to text classification", "author": ["S. Tong", "D. Koller"], "venue": "Journal of machine learning research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2001}, {"title": "Multi-label active learning based on maximum correntropy criterion: Towards robust and discriminative labeling", "author": ["Z. Wang", "B. Du", "L. Zhang", "M. Fang", "D. Tao"], "venue": "In ECCV,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Incorporating diversity and density in active learning for relevance feedback", "author": ["Z. Xu", "R. Akella", "Y. Zhang"], "venue": "In European Conference on Information Retrieval,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "Fine-grained visual comparisons with local learning", "author": ["A. Yu", "K. Grauman"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Active learning with sampling by uncertainty and density for data annotations", "author": ["J. Zhu", "H. Wang", "B.K. Tsou", "M. Ma"], "venue": "IEEE Transactions on audio, speech, and language processing,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Generative adversarial active learning", "author": ["J.-J. Zhu", "J. Bento"], "venue": "arXiv preprint arXiv:1702.07956,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2017}, {"title": "Generative visual manipulation on the natural image manifold", "author": ["J.-Y. Zhu", "P. Kr\u00e4henb\u00fchl", "E. Shechtman", "A.A. Efros"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "Big image data sets are relatively easy to obtain; it\u2019s the ground truth labels that are expensive [8, 21].", "startOffset": 99, "endOffset": 106}, {"referenceID": 20, "context": "Big image data sets are relatively easy to obtain; it\u2019s the ground truth labels that are expensive [8, 21].", "startOffset": 99, "endOffset": 106}, {"referenceID": 16, "context": "With the huge success of deep learning methods critically depending on large annotated datasets, there is a strong demand for reducing the annotation effort [17, 23, 25].", "startOffset": 157, "endOffset": 169}, {"referenceID": 22, "context": "With the huge success of deep learning methods critically depending on large annotated datasets, there is a strong demand for reducing the annotation effort [17, 23, 25].", "startOffset": 157, "endOffset": 169}, {"referenceID": 24, "context": "With the huge success of deep learning methods critically depending on large annotated datasets, there is a strong demand for reducing the annotation effort [17, 23, 25].", "startOffset": 157, "endOffset": 169}, {"referenceID": 26, "context": "In active learning [27] the goal is to train a good predictor while minimizing the human annotation effort for large unlabeled data sets.", "startOffset": 19, "endOffset": 23}, {"referenceID": 18, "context": "Existing active learning methods [19, 27, 31] typically ask the oracle to label an existing data sample.", "startOffset": 33, "endOffset": 45}, {"referenceID": 26, "context": "Existing active learning methods [19, 27, 31] typically ask the oracle to label an existing data sample.", "startOffset": 33, "endOffset": 45}, {"referenceID": 30, "context": "Existing active learning methods [19, 27, 31] typically ask the oracle to label an existing data sample.", "startOffset": 33, "endOffset": 45}, {"referenceID": 26, "context": "Active learning [27] is an iterative framework and starts with the initialization of a prediction model either by training on a small set of labeled samples or by random initialization.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "Active learning in computer vision includes work on selecting the most influential images [12], refraining from labeling unclear visuals [16], zero-shot transfer learning [13], multi-label active learning [32].", "startOffset": 90, "endOffset": 94}, {"referenceID": 15, "context": "Active learning in computer vision includes work on selecting the most influential images [12], refraining from labeling unclear visuals [16], zero-shot transfer learning [13], multi-label active learning [32].", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "Active learning in computer vision includes work on selecting the most influential images [12], refraining from labeling unclear visuals [16], zero-shot transfer learning [13], multi-label active learning [32].", "startOffset": 171, "endOffset": 175}, {"referenceID": 31, "context": "Active learning in computer vision includes work on selecting the most influential images [12], refraining from labeling unclear visuals [16], zero-shot transfer learning [13], multi-label active learning [32].", "startOffset": 205, "endOffset": 209}, {"referenceID": 18, "context": "A poolbased setting [19, 31] assumes the availability of a large set of unlabeled instances.", "startOffset": 20, "endOffset": 28}, {"referenceID": 30, "context": "A poolbased setting [19, 31] assumes the availability of a large set of unlabeled instances.", "startOffset": 20, "endOffset": 28}, {"referenceID": 2, "context": "Instead, a stream-based setting [3, 7] is favorable for online learning where a query is selectively sampled from an incoming data stream.", "startOffset": 32, "endOffset": 38}, {"referenceID": 6, "context": "Instead, a stream-based setting [3, 7] is favorable for online learning where a query is selectively sampled from an incoming data stream.", "startOffset": 32, "endOffset": 38}, {"referenceID": 1, "context": "Alternatively, in a query synthesis setting [2, 4, 36] the system is input with a data distribution and queries to the oracle are generated to lie in the input space.", "startOffset": 44, "endOffset": 54}, {"referenceID": 3, "context": "Alternatively, in a query synthesis setting [2, 4, 36] the system is input with a data distribution and queries to the oracle are generated to lie in the input space.", "startOffset": 44, "endOffset": 54}, {"referenceID": 35, "context": "Alternatively, in a query synthesis setting [2, 4, 36] the system is input with a data distribution and queries to the oracle are generated to lie in the input space.", "startOffset": 44, "endOffset": 54}, {"referenceID": 0, "context": "Recently, [1] and [6] proposed methods for efficiently learning halfspaces, i.", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "Recently, [1] and [6] proposed methods for efficiently learning halfspaces, i.", "startOffset": 18, "endOffset": 21}, {"referenceID": 26, "context": "Much work has been done on this topic [27], here we describe a few prominent strategies.", "startOffset": 38, "endOffset": 42}, {"referenceID": 18, "context": "Uncertainty sampling [19] is a query strategy that selects the unlabeled sample of which the current model is least certain.", "startOffset": 21, "endOffset": 25}, {"referenceID": 4, "context": "This could be obtained by sampling closest to the decision boundary [5, 31] or based on entropy [20].", "startOffset": 68, "endOffset": 75}, {"referenceID": 30, "context": "This could be obtained by sampling closest to the decision boundary [5, 31] or based on entropy [20].", "startOffset": 68, "endOffset": 75}, {"referenceID": 19, "context": "This could be obtained by sampling closest to the decision boundary [5, 31] or based on entropy [20].", "startOffset": 96, "endOffset": 100}, {"referenceID": 27, "context": "The Uncertainty-dense sampling method [28, 35] aims to correct for the problems associated with uncertainty sampling by selecting samples that are not only uncertain but that also lie in dense areas of the data distribution and are thus representative of the data.", "startOffset": 38, "endOffset": 46}, {"referenceID": 34, "context": "The Uncertainty-dense sampling method [28, 35] aims to correct for the problems associated with uncertainty sampling by selecting samples that are not only uncertain but that also lie in dense areas of the data distribution and are thus representative of the data.", "startOffset": 38, "endOffset": 46}, {"referenceID": 14, "context": "In Batch methods [15, 30, 33], not a single sample is queried at each iteration, but a set of samples.", "startOffset": 17, "endOffset": 29}, {"referenceID": 29, "context": "In Batch methods [15, 30, 33], not a single sample is queried at each iteration, but a set of samples.", "startOffset": 17, "endOffset": 29}, {"referenceID": 32, "context": "In Batch methods [15, 30, 33], not a single sample is queried at each iteration, but a set of samples.", "startOffset": 17, "endOffset": 29}, {"referenceID": 28, "context": "In Queryby-committee [29] multiple models are used as a committee and queries the samples where the disagreement is highest.", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "Probabilistic generative models such as variational autoencoders [17, 18], learn an inference model to map images to a latent space and a decoder to map from latent space back again to image space.", "startOffset": 65, "endOffset": 73}, {"referenceID": 17, "context": "Probabilistic generative models such as variational autoencoders [17, 18], learn an inference model to map images to a latent space and a decoder to map from latent space back again to image space.", "startOffset": 65, "endOffset": 73}, {"referenceID": 13, "context": "Non-probabilistic models such as the generative adversarial nets [14], produce higher-quality images than variational autoencoders [9, 26], but can only map from latent space to image space.", "startOffset": 65, "endOffset": 69}, {"referenceID": 8, "context": "Non-probabilistic models such as the generative adversarial nets [14], produce higher-quality images than variational autoencoders [9, 26], but can only map from latent space to image space.", "startOffset": 131, "endOffset": 138}, {"referenceID": 25, "context": "Non-probabilistic models such as the generative adversarial nets [14], produce higher-quality images than variational autoencoders [9, 26], but can only map from latent space to image space.", "startOffset": 131, "endOffset": 138}, {"referenceID": 9, "context": "Fortunately, recent generative adversarial models can produce high-quality images and provide efficient inference [10, 11].", "startOffset": 114, "endOffset": 122}, {"referenceID": 10, "context": "Fortunately, recent generative adversarial models can produce high-quality images and provide efficient inference [10, 11].", "startOffset": 114, "endOffset": 122}, {"referenceID": 13, "context": "We make use of GANs (Generative Adversarial Nets) [14] to obtain a high-quality embedding.", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "The generator and discriminator are simultaneously trained by playing a two-player minimax game, for details see [14].", "startOffset": 113, "endOffset": 117}, {"referenceID": 9, "context": "Thus, in addition to a mapping from latent space to images (decoding) as in a standard GAN, we also need an inference model to map images to latent space (encoding) [10, 11].", "startOffset": 165, "endOffset": 173}, {"referenceID": 10, "context": "Thus, in addition to a mapping from latent space to images (decoding) as in a standard GAN, we also need an inference model to map images to latent space (encoding) [10, 11].", "startOffset": 165, "endOffset": 173}, {"referenceID": 10, "context": "This is achieved by playing an adversarial game to try and match the encoder and decoder joint distributions where the adversarial game is played between the encoder and decoder on the one side and the discriminator on the other side, for details see [11].", "startOffset": 251, "endOffset": 255}, {"referenceID": 21, "context": "The SVHN dataset [22] contains challenging digit images from Google streetview, it has 73,257 train and 26,032 test images.", "startOffset": 17, "endOffset": 21}, {"referenceID": 36, "context": "We create the Shoe-Bag dataset of 40,000 train and 14,000 test images by taking subsets from the Handbags dataset [37] and the Shoes dataset [34].", "startOffset": 114, "endOffset": 118}, {"referenceID": 33, "context": "We create the Shoe-Bag dataset of 40,000 train and 14,000 test images by taking subsets from the Handbags dataset [37] and the Shoes dataset [34].", "startOffset": 141, "endOffset": 145}, {"referenceID": 10, "context": "For every dataset we train a deep generative embedding following [11].", "startOffset": 65, "endOffset": 69}, {"referenceID": 10, "context": "All embeddings are trained on the train data, except for the SVHN embedding; which is trained on the larger \u201cextra\u201d dataset following [11].", "startOffset": 134, "endOffset": 138}, {"referenceID": 23, "context": "We evaluate active learning with the Area Under the (accuracy) Learning Curve (AULC) measure [24, 28].", "startOffset": 93, "endOffset": 101}, {"referenceID": 27, "context": "We evaluate active learning with the Area Under the (accuracy) Learning Curve (AULC) measure [24, 28].", "startOffset": 93, "endOffset": 101}, {"referenceID": 18, "context": "Uncertainty sampling selects the least confident sample point [19].", "startOffset": 62, "endOffset": 66}, {"referenceID": 27, "context": "Uncertainty-dense sampling [28] selects samples that are not only uncertain but that also lie in dense areas of the data distribution.", "startOffset": 27, "endOffset": 31}, {"referenceID": 29, "context": "The K-cluster centroid [30] uses batches of K samples, where we set K = 5.", "startOffset": 23, "endOffset": 27}], "year": 2017, "abstractText": "This paper is on active learning where the goal is to reduce the data annotation burden by interacting with a (human) oracle during training. Standard active learning methods ask the oracle to annotate data samples. Instead, we take a profoundly different approach: we ask for annotations of the decision boundary. We achieve this using a deep generative model to create novel instances along a 1d line. A point on the decision boundary is revealed where the instances change class. Experimentally we show on three data sets that our method can be plugged-in to other active learning schemes, that human oracles can effectively annotate points on the decision boundary, that our method is robust to annotation noise, and that decision boundary annotations improve over annotating data samples.", "creator": "LaTeX with hyperref package"}}}