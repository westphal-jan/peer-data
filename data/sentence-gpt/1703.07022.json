{"id": "1703.07022", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "Recurrent Topic-Transition GAN for Visual Paragraph Generation", "abstract": "A natural image usually conveys rich semantic content and can be viewed from different angles. Existing image description methods are largely restricted by small sets of biased visual paragraph annotations, and fail to cover rich underlying semantics. In this paper, we investigate a semi-supervised paragraph generative framework that is able to synthesize diverse and semantically coherent paragraph descriptions by reasoning over local semantic regions and exploiting linguistic knowledge. The proposed Recurrent Topic-Transition Generative Adversarial Network (RTT-GAN) builds an adversarial framework between a structured paragraph generator and multi-level paragraph discriminators. The paragraph generator generates sentences recurrently by incorporating region-based visual and language attention mechanisms at each step. The quality of generated paragraph sentences is assessed by multi-level adversarial discriminators from two aspects, namely, plausibility at sentence level and topic-transition coherence at paragraph level. The joint adversarial training of RTT-GAN drives the model to generate realistic paragraphs with smooth logical transition between sentence topics. Extensive quantitative experiments on image and video paragraph datasets demonstrate the effectiveness of our RTT-GAN in both supervised and semi-supervised settings. Qualitative results on telling diverse stories for an image also verify the interpretability of RTT-GAN.\n\n\n\n\nIn this paper, we present the first generation of text-based RTT-GAN networks, that demonstrate their flexibility and effectiveness. The network is also able to use the RTT-GAN to classify a single paragraph to generate large text-based sentences that are both easily recognizable and accurately labeled. The output of the sentences is then read aloud, as well as for a set of sentences. The first generation output of the sentence is generated from the following sentence:\nThe sentence text (1), 2) is described as 'hippable and useful', and the sentence text is given an unsupervised sentence description that is well suited to describing a particular sentence (2). In this sentence, the sentence text is identified with a 'hippable' character, meaning it can be easily identified with a 'hippable' character, which is more similar to the 'hippable' character. This model can be applied in real-world situations to the network using multiple sentence classes.\n\nThe second generation output of the sentence is generated by a supervised model called 'hippable' (i.e., the network in which the words are written), which is then read aloud, and the sentence text is given an unsupervised sentence description that is", "histories": [["v1", "Tue, 21 Mar 2017 01:43:12 GMT  (2327kb,D)", "https://arxiv.org/abs/1703.07022v1", "10 pages, 6 figures"], ["v2", "Thu, 23 Mar 2017 20:06:15 GMT  (3728kb,D)", "http://arxiv.org/abs/1703.07022v2", "10 pages, 6 figures"]], "COMMENTS": "10 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["xiaodan liang", "zhiting hu", "hao zhang", "chuang gan", "eric p xing"], "accepted": false, "id": "1703.07022"}, "pdf": {"name": "1703.07022.pdf", "metadata": {"source": "CRF", "title": "Recurrent Topic-Transition GAN for Visual Paragraph Generation", "authors": ["Xiaodan Liang", "Zhiting Hu", "Hao Zhang", "Chuang Gan", "Eric P. Xing"], "emails": ["xiaodan1@cs.cmu.edu", "zhitingh@cs.cmu.edu", "hao@cs.cmu.edu", "ganchuang1990@gmail.com", "epxing@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "Describing visual content with natural language is an emerging interdisciplinary problem at the intersection of computer vision, natural language processing, and artificial intelligence. Recently, great advances [18, 3, 4, 31, 23, 33] have been achieved in describing images and videos using a single high-level sentence, owing to the advent of large\ndatasets [22, 34, 17] pairing images with natural language descriptions. Despite the encouraging progress in image captioning [30, 33, 23, 31], most current systems tend to capture the scene-level gist rather than fine-grained entities, which largely undermines their applications in realworld scenarios such as blind navigation, video retrieval, and automatic video subtitling. One of the recent alternatives to sentence-level captioning is visual paragraph generation [10, 16, 35, 29], which aims to provide a coherent and detailed description, like telling stories for images/videos.\nGenerating a full paragraph description for an image/video is challenging. First, paragraph descriptions tend to be diverse, just like different individuals can tell stories from personalized perspectives. As illustrated in Figure 1, users may describe the image starting from different viewpoints and objects. Existing methods [16, 35, 19] deterministically optimizing over single annotated paragraph thus suffer from losing massive information expressed in the image. It is desirable to enable diverse generation through simple manipulations. Second, annotating images/videos with long paragraphs is labor-expensive, leading to only\n1\nar X\niv :1\n70 3.\n07 02\n2v 2\n[ cs\n.C V\n] 2\n3 M\nar 2\nsmall scale image-paragraph pairs which limits the model generalization. Finally, different from single-sentence captioning, visual paragraphing requires to capture more detailed and richer semantic content. It is necessary to perform long-term visual and language reasoning to incorporate fine-grained cues while ensuring coherent paragraphs.\nTo overcome the above challenges, we propose a semisupervised visual paragraph generative model, Recurrent Topic-Transition GAN (RTT-GAN), which generates diverse and semantically coherent paragraphs by reasoning over both local semantic regions and global paragraph context. Inspired by Generative Adversarial Networks (GANs) [6], we establish an adversarial training mechanism between a structured paragraph generator and multi-level paragraph discriminators, where the discriminators learn to distinguish between real and synthesized paragraphs while the generator aims to fool the discriminators by generating diverse and realistic paragraphs.\nThe paragraph generator is built upon dense semantic regions of the image, and selectively attends over the regional content details to construct meaningful and coherent paragraphs. To enable long-term visual and language reasoning spanning multiple sentences, the generator recurrently maintains context states of different granularities, ranging from paragraph to sentences and words. Conditioned on current state, a spatial visual attention mechanism selectively incorporates visual cues of local semantic regions to manifest a topic vector for next sentence, and a language attention mechanism incorporates linguistic information of regional phrases to generate precise text descriptions. We pair the generator with rival discriminators which assess synthesized paragraphs in terms of plausibility at sentence level as well as topic-transition coherence at paragraph level. Our model allows diverse descriptions from a single image by manipulating the first sentence which guides the topic of the whole paragraph. Semi-supervised learning is enabled in the sense that only single-sentence caption annotation is required for model training, while the linguistic knowledge for constructing long paragraphs is transfered from standalone text paragraphs without paired images.\nWe compare RTT-GAN with state-of-the-art methods on both image-paragraph and video-paragraph datasets, and verify the superiority of our method in both supervised and semi-supervised settings. Using only the single-sentence COCO captioning dataset, our model generates highly plausible multi-sentence paragraphs. Given these synthesized paragraphs for COCO image, we can considerably enlarge the existing small paragraph dataset to further improve the paragraph generation capability of our RTT-GAN. Qualitative results on personalized paragraph generation also shows the flexibility and applicability of our model."}, {"heading": "2. Related Work", "text": "Visual Captioning. Image captioning is posed as a longstanding and holy-grail goal in computer vision, targeting at bridging visual and linguistic domain. Early works that posed this problem as a ranking and template retrieval tasks [5, 8, 14] performed poorly since it is hard to enumerate all possibilities in one collected dataset due to the compositional nature of language. Therefore, some recent works [18, 3, 4, 31, 23, 33, 20] focus on directly generating captions by modeling the semantic mapping from visual cues to language descriptions. Among all these research lines, advanced methods that train recurrent neural network language models conditioned on image features [3, 4, 31, 33] achieve great success by taking advantages of large-scale image captioning dataset. Similar success has been already seen in video captioning fields [4, 32]. Though generating high-level sentences for images is encouraging, massive underlying information, such as relationships between objects, attributes, and entangled geometric structures conveyed in the image, would be missed if only summarizing them with a coarse sentence. Dense captioning [12] is recently proposed to describe each region of interest with a short phrase, considering more details than standard image captioning. However, local phrases can not provide a comprehensive and logical description for the entire image.\nVisual Paragraph Generation. Paragraph generation overcomes shortcomings of standard captioning and dense captioning by producing a coherent and fine-grained natural language description. To reason about long-term linguistic structures with multiple sentences, hierarchical recurrent network [19, 21, 35, 16] has been widely used to directly simulate the hierarchy of language. For example, Yu et al. [35] generate multi-sentence video descriptions for cooking videos to capture strong temporal dependencies. Krause et al. [16] combine semantics of all regions of interest to produce a generic paragraph for an image. However, all these methods suffer from the overfitting problem due to the lack of sufficient paragraph descriptions. In contrast, we propose a generative model to automatically synthesize a large amount of diverse and reasonable paragraph descriptions by learning the implicit linguistic interplay between sentences. Our RTT-GAN has better interpretability by imposing the sentence plausibility and topic-transition coherence on the generator with two adversarial discriminators. The generator selectively incorporates visual and language cues of semantic regions to produce each sentence."}, {"heading": "3. Recurrent Topic-Transition GAN", "text": "The proposed Recurrent Topic-Transition GAN (RTTGAN) aims to describe the rich content of a given image/video by generating a natural language paragraph. Fig-\nure 2 provides an overview of the framework. Given an input image, we first detect a set of semantic regions using dense captioning method [12]. Each semantic region is represented with a visual feature vector and a short text phrase (e.g. person riding a horse). The paragraph generator then sequentially generates meaningful sentences by incorporating the fine-grained visual and textual cues in a selective way. To ensure high-quality individual sentences and coherent whole paragraph, we apply a sentence discriminator and a topic-transition discriminator on each generated sentence, respectively, to measure the plausibility and smoothness of semantic transition with preceding sentences. The generator and multi-level discriminators are learned jointly within an adversarial framework. RTT-GAN supports not only supervised setting with annotated image-paragraph pairs, but also semi-supervised setting where only a single sentence caption is provided for each image and the knowledge of long paragraph construction is learned from a standalone paragraph corpus.\nIn next sections, we first derive the adversarial framework of our RTT-GAN, then describe detailed model design of the paragraph generator and the multi-level discriminators, respectively."}, {"heading": "3.1. Adversarial Objective", "text": "We construct an adversarial game between the generator and discriminators to drive the model learning. Specifically, the sentence and topic-transition discriminators learn a critic between real and generated samples, while the generator attempts to confuse the discriminators by generating realistic paragraphs that satisfy linguistic characteristics (i.e., sentence plausibility and topic-transition coherence). The generative neural architecture ensures the paragraph captures adequate semantic content of the image, which we describe in detail in the next sections. Formally, let G denote the paragraph generator, and let Ds and Dr denote the sentence and topic-transition discriminators, respectively.\nAt each time step t, conditioned on preceding sentences s1:t\u22121 and local semantic regions V of the image, the generator G recurrently produces a single sentence st, where each sentence st = {wt,i} consists of a sequence of Nt words wt,i (i = 1, . . . ,Nt):\nPG(st|s1:t\u22121,V) = Nt\u220f i=1 PG(wt,i|wt,1:i\u22121, s1:t\u22121,V). (1)\nThe discriminators learn to differentiate real sentences s\u0302 within a true paragraph P\u0302 from the synthesized ones st. The generator G tries to generate realistic visual paragraph by minimizing against the discriminators\u2019 chance of correctly telling apart the sample source. As the original GAN [6] that optimizes over binary probability distance suffers from mode collapse and instable convergence, we follow the new Wasserstein GAN [1] method that theoretically remedies this by minimizing an approximated Wasserstein distance. The objective of the adversarial framework is thus written as:\nmin G max Ds,Dr\nL(G,Ds, Dr) =\nEs\u0302\u223cpdata(s\u0302) [ Ds(s\u0302) ] \u2212 Es1:t\u223cpG(s1:t|V) [ Ds(st) ] +\nEP\u0302\u223cpdata(P\u0302) [ Dr(P\u0302) ] \u2212 Es1:t\u223cpG(s1:t|V) [ Dr(s1:t) ] , (2)\nwhere pdata(s\u0302) and pdata(P\u0302) denote the true data distributions of sentences and paragraphs, respectively, which are empirically constructed from a paragraph description corpus. The second line of the equation is the objective of the sentence discriminator Ds that optimizes a critic between real/fake sentences, while the third line is the objective of the topictransition discriminator Dr. Here pG(s1:t|V) indicates the distribution of generated sentences by the generator G.\nTo leverage existing image-paragraph pair dataset in the supervised setting, or image captioning dataset in the semisupervised setting, we also incorporate the traditional word reconstruction loss for generator optimization, which is de-\nfined as:\nLc(G) = \u2212 T\u2211\nt=1 Nt\u2211 i=1 logPG(wt,i|wt,1:i\u22121, s1:t\u22121,V). (3)\nNote that the reconstruction loss is only used for supervised examples with paragraph annotations, and semi-supervised examples with single-sentence caption (where we set T = 1). Combining Eqs.(2)-(3), the joint objective for the generator G is thus:\nG\u2217 = argmin G max Ds,Dr \u03bbL(G,Ds, Dr) + Lc(G), (4)\nwhere \u03bb is the balancing parameter fixed to 0.001 in our implementation. The optimization of the generator and discriminators (i.e., Eq.(4) and Eq.(2), respectively) is performed in an alternating min-max manner. We describe the training details in section 3.4.\nThe discrete nature of text samples hinders gradient back-propagation from the discriminators to the generator [9]. We address this issue following SeqGAN [36]. The state is the current produced words and sentences, and the action is the next word to select. we apply Monte Carlo search with a roll-out policy to sample the remaining words until it sees an END token for each sentence and maximal number of sentences. The roll-out policy is the same with the generator, elaborated in Section 3.2. The discriminator is trained by providing true paragraphs from the text corpus and synthetic ones from the generator. The generator is updated by employing a policy gradient based on the expected reward received from the discriminator and the reconstruction loss for fully-supervised and semi-supervised settings, defined in Eq. 4. To reduce the variance of the action values, we run the roll-out policy starting from current state till the\nend of the paragraph for five times to get a batch of output samples. The signals that come from the word prediction for labeled sentences (defined in Eq. 3)) can be regarded as the intermediate reward. The gradients are passed back to the intermediate action value via Monte Carlo search [36]."}, {"heading": "3.2. Paragraph Generator", "text": "Figure 3 shows the architecture of the generator G, which recurrently retains different levels of context states with a hierarchy constructed by a paragraph RNN, a sentence RNN, and a word RNN, and two attention modules. First, the paragraph RNN encodes the current paragraph state based on all preceding sentences. Second, the spatial visual attention module selectively focuses on semantic regions with the guidance of current paragraph state to produce the visual representation of the sentence. The sentence RNN is thus able to encode a topic vector for the new sentence. Third, the language attention module learns to incorporate linguistic knowledge embedded in local phrases of focused semantic regions to facilitate word generation by the word RNN.\nRegion Representation. Given an input image, we adopt the dense captioning model [12, 16] to detect semantic regions of the image and generate their local phrases. Each region Rj (j \u2208 1, . . . ,M) has a visual feature vector vj and a local text phrase (i.e., region captioning) srj = {wrj,i} consisting of Nj words. In practice, we use the top M = 50 regions.\nParagraph RNN. The paragraph RNN keeps track of the paragraph state by summarizing preceding sentences. At each t-th step (t = 1, . . . , T ), the paragraph RNN takes the embedding of generated sentence in previous step as input, and in turn produces the paragraph hidden state hPt .\nThe sentence embedding is obtained by simply averaging over the embedding vectors of the words in the sentence. This strategy enables our model to support the manipulation of the first sentence to initialize the paragraph RNN and generate personalized follow-up descriptions.\nSentence RNN with Spatial Visual Attention. The visual attentive sentence RNN controls the topic of the next sentence st by selectively focusing on relevant regions of the image. Specifically, given the paragraph states hPt from the paragraph RNN and previous hidden states hSt\u22121 of the sentence RNN, we apply an attention mechanism on the visual features V = {v1, . . . ,vM} of all semantic regions, and construct a visual context vector fvt that represents the next sentence at t-th step:\nfvt = att v(V,hPt ,h S t\u22121)\n= M\u2211 j=1\n\u03b1(vj , \u03b2(h P t ,h S t\u22121))\u2211M\nj\u2032=1 \u03b1(vj\u2032 , \u03b2(h P t ,h S t\u22121))\nvj\n:= M\u2211 j=1 ajvj ,\n(5)\nwhere \u03b2(hPt ,h S t\u22121) is a linear layer that transforms the concatenation of hPt and h S t\u22121 into a compact vector with the same dimension as vj ; the function \u03b1(\u00b7) is to compute the weight of each region and is implemented with a single linear layer. For notational simplicity, we use aj to denote the normalized attentive weight of each region Rj .\nGiven the visual representation fvt , the sentence RNN is responsible for determining the number of sentences that should be in the paragraph and producing a topic vector of each sentence. Specifically, each hidden state hSt is first passed into a linear layer to produce a probability over the two states {CONTINUE=0, STOP=1} which determine whether t-th sentence is the last sentence. The updated hSt is treated as the topic vector of the sentence.\nWord RNN with Language Attention. To generate meaningful paragraphs relevant to the image, the model is desired to recognize and describe substantial details such as objects, attributes, and relationships. The text phrases of semantic regions that express such local semantics are leveraged by a language attention module to help with the recurrent word generation. For example, the word RNN can conveniently copy precise concepts (e.g., baseball, helmet) from the local phrases. Following the copy mechanism [7] firstly proposed in natural language processing, we selectively incorporate the embeddings of local phrases based on the topic vector hSt and preceding word state hwt,i\u22121, i \u2208 {1, . . . ,Nt} by the word RNN to generate the next word representation f lt,i. Since each local phrase s r j semantically relates to respective visual feature vj , we thus reuse the visual attentive weights {aj}Mj=1 to enhance the language attention. Representing each word with an embed-\nding vector wri,j , the language representation f l t,i for each word prediction at i-th step is formulated as\nf lt,i = att l(Sr,hSt ,h w t,i\u22121)\n= M\u2211 j=1 Nj\u2211 i\u2032=1\n\u03b1(wri\u2032,j , \u03b2(h S t ,h w t,i\u22121))\u2211M\nj\u2032=1 \u2211Nj\u2032 i\u2032\u2032=1 \u03b1(w r i\u2032\u2032,j\u2032 , \u03b2(h S t ,h w t,i\u2032\u2032\u22121)) ajw r i\u2032,j .\n(6)\nGiven the language representation f lt,i as the input at i-th step, the word RNN computes a hidden states hwt,i which is then used to predict a distribution over the words in the vocabulary. After obtaining all words of each sentence, these sentences are finally concatenated to form the generated paragraph."}, {"heading": "3.3. Paragraph Discriminators", "text": "The paragraph discriminators {Ds, Dr} aim to distinguish between real paragraphs and synthesized ones based on the linguistic characteristics of a natural paragraph description. In particular, the sentence discriminator Ds evaluates the plausibility of individual sentences, while the topic-transition discriminator Dr evaluates the topic coherence of all sentences generated so far. With such multi-level assessment, the model is able to generate long yet realistic descriptions. Specifically, the sentence discriminator Ds is an LSTM RNN that recurrently takes each word embedding within a sentence as the input, and produces a realvalue plausibility score of the synthesized sentence. The topic-transition discriminator Dr is another LSTM RNN which recurrently takes the sentence embeddings of all preceding sentences as inputs and computes the topic smoothness value of the current constructed paragraph description at each recurrent step."}, {"heading": "3.4. Implementation Details", "text": "The discriminators Ds and Dr are both implemented as a single-layer LSTM with hidden dimension of 512. For the generator, the paragraph RNN is a single-layer LSTM with hidden size of 512 and the initial hidden and memory cells set to zero. Similarly, the sentence RNN and word RNN are single-layer LSTMs with hidden dimension of 1024 and 512, respectively. Each input word is encoded as a embedding vector of 512 dimension. The visual feature vector vj of each semantic region has dimension of 4096.\nThe adversarial framework is trained following the Wasserstein GAN (WGAN) [1] in which we alternate between the optimization of {Ds, Dr}with Eq.(2) and the optimization of G with Eq.(4). In particular, we perform one gradient descent step on G every time after 5 gradient steps on {Ds, Dr}. We use minibatch SGD and apply the RMSprop solver [27] with the initial learning rate set to 0.0001. For stable training, we apply batch normalization [11] and\nset the batch size to 1 (i.e., \u201cinstance normalization\u201d). In order to make the parameters of Ds and Dr lie in a compact space, we clamp the weights to a fixed box [\u22120.01, 0.01] after each gradient update. In the semi-supervised setting where only single-sentence captioning for images and standalone paragraph corpus are available, we set the maximal number of sentences in the generated paragraph to 6 for all images. In the fully-supervised setting, the groundtruth sentence number in each visual paragraph is used to train the sentence-RNN for learning how many sentences are needed. We train the models to converge for 40 epochs. The implementations are based on the public Torch7 platform on a single NVIDIA GeForce GTX 1080."}, {"heading": "4. Experiments", "text": "In this section, we perform detailed comparisons with state-of-the-art methods on the visual paragraph generation task in both supervised and semi-supervised settings."}, {"heading": "4.1. Experimental Settings", "text": "To generate a paragraph for an image, we run the paragraph generator forward until the STOP sentence state is predicted or after Smax = 6 sentences, whichever comes first. The word RNN is recurrently forwarded to sample the most likely word at each time step, and stops after choosing the STOP token or after Nmax = 30 words. We use beam search with beam size 2 for generating paragraph descriptions. Training details are presented in Section 3.4, and all models are implemented in Torch platform. In terms of the fully-supervised setting, to make a fair comparison with the state-of-the-art methods [13, 16], the experiments are conducted on the public image paragraph dataset [16], where 14,575 image-paragraph pairs are used for training, 2,487 for validation and 2,489 for testing. In terms of semi-supervised setting, our RTT-GAN is trained with the single sentence annotations provided in MSCOCO image captioning dataset [2] which contains 123,000 images. The image-paragraph validation set is used for validating the semi-supervised paragraph generation. The paragraph generation performance is also evaluated on 2,489 paragraph testing samples. For both fully-supervised and semi-supervised settings, we use the word vocabulary of image-paragraph dataset as [16] does and the 14,575 paragraph descriptions on public image paragraph dataset [16] are adopted as the standalone paragraph corpus for training discriminators. We report six widely used automatic evaluation metrics, BLEU-1, BLEU-2, BLEU-3, BLEU-4, METEOR, and CIDEr. The model checkpoint selection is based on the best combined METEOR and CIDEr score on the validation set. Table 1 reports the performance of all baselines and our models."}, {"heading": "4.2. Comparison with the State-of-the-arts", "text": "We obtain the results of all four baselines from [16]. Specifically, Sentence-Concat samples and concatenates five sentence captions from the model trained on MS COCO captions, in which the first sentence uses beam search and the rest are samples. Image-Flat [13] directly decodes an image into a paragraph token by token. Template predicts the text via a handful of manually specified templates. And Region-Hierarchical [16] uses a hierarchical recurrent neural network to decompose the paragraphs into the corresponding sentences. Same with all baselines, we adopt VGG-16 net [26] to encode the visual representation of an image. Note that our RTT-GAN and RegionHierarchical [16] use the same dense captioning model [12] to extract semantic regions. Human shows the results by collecting an additional paragraph for 500 randomly chosen images as [16]. As expected, humans produce superior descriptions over any automatic method and the large gaps on CIDEr and METEOR verify that CIDEr and METEOR metrics align better with human judgment than BLEU scores.\nFully-supervised Setting. We can see that our RTTGAN (Fully-) model significantly outperforms all baselines on all metrics; particularly, 3.35% over RegionHierarchical and 5.81% over Image-Flat in terms of CIDEr. It clearly demonstrates the effectiveness of our region-based attention mechanism that selectively incorporate visual and language cues, and the adversarial multi-level discriminators that provide a better holistic, semantic regularization of the generated sentences in a paragraph. The inferiority of Image-Flat compared to the hierarchical networks of RTTGAN and Region-Hierarchical demonstrates the advantage of performing hierarchical sentence predictions for a long paragraph description.\nSemi-supervised Setting. The main advantage of our RTT-GAN compared to prior works is the capability of generating realistic paragraph descriptions coordinating with the natural linguistic properties, given only singe sentence annotations. It is demonstrated by the effectiveness of\nour semi-supervised model RTT-GAN (Semi-) that only uses the single sentence annotations of MSCOCO captions, and imposes the linguistic characteristics on the rest sentence predictions using adversarial discriminators that are trained with the standalone paragraph corpus. Specifically, RTT-GAN (Semi-) achieves comparable performance with the fully-supervised Regions-Hierarchical without using any groundtruth image-paragraph pairs. After augmenting the image paragraph dataset with the synthesized paragraph descriptions by RTT-GAN (Semi-), RTT-GAN (Semi+Fully) dramatically outperforms RTT-GAN (Fully) and other baselines, e.g. 6.84% increase over RegionsHierarchical on CIDEr. We also show some qualitative results of generated paragraphs by our RTT-GAN (Semi-) in\nFigure 5. These promising results further verify the capability of our RTT-GAN on reasoning a long description that has coherent topics and plausible sentences without the presence of ground-truth image paragraph pairs."}, {"heading": "4.3. The Importance of Adversarial Training", "text": "After eliminating the discriminators during the model optimization in both fully- and semi-supervised settings (i.e. RTT-GAN (Fully- w/o discriminator) and RTT-GAN (Semi- w/o discriminator)), we observe consistent performance drops on all metrics compared to the full models, i.e. 1.80% and 4.11% on CIDEr, respectively. RTT-GAN (Semi- w/o discriminator) can be regarded as a image captioning model due to the lack of adversarial loss, similar to Sentence-Concat. It justifies that the sentence plausibility and topic coherences with preceding sentences are very critical for generating long, convincing stories. Moreover, the pure word prediction loss largely hinders the model\u2019s extension to unsupervised or semi-supervised generative modeling. Training adversarial discriminators that explicitly enforce the linguistic characteristics of a good description can effectively impose high-level and semantic constraints on sentence predictions by the generator.\nFurthermore, we break down our design of discriminators in order to compare the effect of the sentence discriminator and recurrent topic-transition discriminator, as RTTGAN (Semi- w/o sentence D) and RTT-GAN (Semi- w/o topic-transition D), respectively. It can be observed that although both discriminators help bring the significant improvement, the sentence discriminator seems to play a more critical role by addressing the plausibility of each sentence."}, {"heading": "4.4. The Importance of Region-based Attention", "text": "We also evaluate the effectiveness of the spatial visual attention and language attention mechanisms to facilitate the paragraph prediction, as reported in Table 2. RTT-GAN (Fully- w/o att) directly pools the visual features of all regions into a compact representation for sequential sentence prediction, like Region-Hierarchical. RTT-GAN (Fully- w/o phrase att) represents the variant that removes the language attention module. It can be observed that the attention mechanism effectively facilitates the prediction of RTT-GAN by selectively incorporating appropriate visual and language cues. Particularly, the advantages of explicitly leveraging words from local phrases suggest that transferring visual-language knowledges from more fundamental tasks (e.g. detection) is beneficial for generating highlevel and holistic descriptions.\nAs an exploratory experiment, we investigate generating paragraphs from a smaller number of regions (10 and 20) than 50 used in previous models, denoted as RTT-GAN (Fully- 10 regions) and RTT-GAN (Fully- 20 regions). Although these results are worse than our full model, the\nperformance of using only top 10 regions is still reasonably good. Figure 4 gives some visualization results of our region-based attention mechanism. For generating the sentence at each step, our model selectively focuses on distinct regions and their distinct corresponding words in local phrases to facilitate the sentence prediction."}, {"heading": "4.5. Personalized Paragraph Generation", "text": "Different from prior works, our model supports the personalized paragraph generation which produces diverse descriptions by manipulating first sentences. It can be conveniently achieved by initializing the paragraph RNN with the sentence embedding of a predefined first sentence. The generator can sequentially output diverse and topic-coherent sentences to form a personalized paragraph for an image. We present qualitative results of our model in Figure 6. Some interesting properties of our predictions include its usage of coreference in the first sentence and its ability to capture topic relationships with preceding sentences. Given the first sentences, subsequent sentences give some details about scene elements mentioned earlier in the description and also connect to other related content. We also report the human evaluation results in Table 3 on randomly chosen 100 testing images, where three model variants are compared, i.e. RTT-GAN (Semi- w/o discriminator), RTT-GAN (Semi-), RTT-GAN (Semi + Fully). For each image, given two first sentences with distinct topics, each model produces two personalized paragraphs accordingly. Regarding to each first sentence of the image, we present three paragraphs by three models in a random order to judges, and ask them to select the most convincing ones. The results in Table 3 indicate that 87.4% of the judges think the paragraphs generated by the models (i.e. RTT-GAN (Semi-), RTT-GAN (Semi + Fully)) that incorporate two adversarial discriminators, look more convincing than those by RTT-GAN (Semiw/o discriminator)."}, {"heading": "4.6. Extension to Video Domain", "text": "As in Table 4, we also extend our RTT-GAN to the task of video paragraph generation and evaluate it on TACoSMultiLevel dataset [24] that contains 185 long videos filmed in an indoor environment, following [35]. To model spatial appearance, we also extract 50 semantic regions for the frames in every second. To capture the motion patterns, we enhance the feature representation with motion features. Similar to [35], we use the pre-trained C3D [28] model on the Sport1M dataset [15], which outputs a fixed-length feature vector every 16 frames. We then perform a mean pooling over all features to generate a compact motion repre-\nsentation, which are used as additional inputs in every visual attention step. Our model significantly outperforms all state-of-the-arts, demonstrating its good generalization capability in video domain."}, {"heading": "5. Conclusion and Future Work", "text": "In this paper, we propose a Recurrent Topic-Transition GAN (RTT-GAN) for visual paragraph generation. Thanks to the adversarial generative modeling, our RTT-GAN is capable of generating diverse paragraphs when only first sentence annotations are given for training. The generator incorporates visual attention and language attention mechanisms to recurrently reason about fine-grained semantic regions. Two discriminators assess the quality of generated paragraphs from two aspects: sentence plausibility and topic-transition coherence. Extensive experiments show the effectiveness of our model in both fully-supervised and semi-supervised settings. In future, we will extend our generative model into other vision tasks that require jointly visual and language modeling."}], "references": [{"title": "Wasserstein gan", "author": ["M. Arjovsky", "S. Chintala", "L. Bottou"], "venue": "arXiv preprint arXiv:1701.07875,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2017}, {"title": "Microsoft coco captions: Data collection and evaluation server", "author": ["X. Chen", "H. Fang", "T.-Y. Lin", "R. Vedantam", "S. Gupta", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1504.00325,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C. Lawrence Zitnick"], "venue": "ICCV, pages 2422\u20132431,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual  recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR, pages 2625\u20132634,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "ECCV, pages 15\u201329,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS, pages 2672\u20132680,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["J. Gu", "Z. Lu", "H. Li", "V.O. Li"], "venue": "ACL,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "Journal of Artificial Intelligence Research, 47:853\u2013 899,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Controllable text generation", "author": ["Z. Hu", "Z. Yang", "X. Liang", "R. Salakhutdinov", "E.P. Xing"], "venue": "arXiv preprint arXiv:1703.00955,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2017}, {"title": "Visual storytelling", "author": ["T.K. Huang", "F. Ferraro", "N. Mostafazadeh", "I. Misra", "A. Agrawal", "J. Devlin", "R.B. Girshick", "X. He", "P. Kohli", "D. Batra", "C.L. Zitnick", "D. Parikh", "L. Vanderwende", "M. Galley", "M. Mitchell"], "venue": "NAACL- HLT, pages 1233\u20131239,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "ICML,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "CVPR, pages 4565\u20134574,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR, pages 3128\u20133137,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["A. Karpathy", "A. Joulin", "F.F.F. Li"], "venue": "NIPS, pages 1889\u20131897,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 1725\u20131732,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "A hierarchical approach for generating descriptive image paragraphs", "author": ["J. Krause", "J. Johnson", "R. Krishna", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.-J. Li", "D.A. Shamma"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "CVPR,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["J. Li", "M.-T. Luong", "D. Jurafsky"], "venue": "ACL,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep variation-structured reinforcement learning for visual relationship and attribute detection", "author": ["X. Liang", "L. Lee", "E.P. Xing"], "venue": "arXiv preprint arXiv:1703.03054,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2017}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["R. Lin", "S. Liu", "M. Yang", "M. Li", "M. Zhou", "S. Li"], "venue": "EMNLP, pages 899\u2013907,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV, pages 740\u2013755. Springer,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Coherent multi-sentence video description with variable level of detail", "author": ["A. Rohrbach", "M. Rohrbach", "W. Qiu", "A. Friedrich", "M. Pinkal", "B. Schiele"], "venue": "German Conference on Pattern Recognition, pages 184\u2013195. Springer,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Translating video content to natural language descriptions", "author": ["M. Rohrbach", "W. Qiu", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele"], "venue": "ICCV, pages 433\u2013440,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.  COURSERA: Neural networks for machine learning", "author": ["T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "C3D: generic features for video analysis", "author": ["D. Tran", "L.D. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "CoRR, abs/1412.0767, 2:7,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Msr-vtt: A large video description dataset for bridging video and language", "author": ["J. Xu", "T. Mei", "T. Yao", "Y. Rui"], "venue": "CVPR, pages 5288\u20135296,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "ICML, volume 14, pages 77\u201381,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Review networks for caption generation", "author": ["Z. Yang", "Y. Yuan", "Y. Wu", "W.W. Cohen", "R.R. Salakhutdinov"], "venue": "NIPS, pages 2361\u20132369,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "ICCV, pages 4507\u20134515,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Image captioning with semantic attention", "author": ["Q. You", "H. Jin", "Z. Wang", "C. Fang", "J. Luo"], "venue": "CVPR, pages 4651\u20134659,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "Transactions of the Association for Computational Linguistics, 2:67\u2013 78,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["H. Yu", "J. Wang", "Z. Huang", "Y. Yang", "W. Xu"], "venue": "CVPR, pages 4584\u20134593,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Seqgan: sequence generative adversarial nets with policy gradient", "author": ["L. Yu", "W. Zhang", "J. Wang", "Y. Yu"], "venue": "arXiv preprint arXiv:1609.05473,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "Recently, great advances [18, 3, 4, 31, 23, 33] have been achieved in describing images and videos using a single high-level sentence, owing to the advent of large A group of people are sitting around a living", "startOffset": 25, "endOffset": 47}, {"referenceID": 2, "context": "Recently, great advances [18, 3, 4, 31, 23, 33] have been achieved in describing images and videos using a single high-level sentence, owing to the advent of large A group of people are sitting around a living", "startOffset": 25, "endOffset": 47}, {"referenceID": 3, "context": "Recently, great advances [18, 3, 4, 31, 23, 33] have been achieved in describing images and videos using a single high-level sentence, owing to the advent of large A group of people are sitting around a living", "startOffset": 25, "endOffset": 47}, {"referenceID": 30, "context": "Recently, great advances [18, 3, 4, 31, 23, 33] have been achieved in describing images and videos using a single high-level sentence, owing to the advent of large A group of people are sitting around a living", "startOffset": 25, "endOffset": 47}, {"referenceID": 22, "context": "Recently, great advances [18, 3, 4, 31, 23, 33] have been achieved in describing images and videos using a single high-level sentence, owing to the advent of large A group of people are sitting around a living", "startOffset": 25, "endOffset": 47}, {"referenceID": 32, "context": "Recently, great advances [18, 3, 4, 31, 23, 33] have been achieved in describing images and videos using a single high-level sentence, owing to the advent of large A group of people are sitting around a living", "startOffset": 25, "endOffset": 47}, {"referenceID": 21, "context": "datasets [22, 34, 17] pairing images with natural language descriptions.", "startOffset": 9, "endOffset": 21}, {"referenceID": 33, "context": "datasets [22, 34, 17] pairing images with natural language descriptions.", "startOffset": 9, "endOffset": 21}, {"referenceID": 16, "context": "datasets [22, 34, 17] pairing images with natural language descriptions.", "startOffset": 9, "endOffset": 21}, {"referenceID": 29, "context": "Despite the encouraging progress in image captioning [30, 33, 23, 31], most current systems tend to capture the scene-level gist rather than fine-grained entities, which largely undermines their applications in realworld scenarios such as blind navigation, video retrieval, and automatic video subtitling.", "startOffset": 53, "endOffset": 69}, {"referenceID": 32, "context": "Despite the encouraging progress in image captioning [30, 33, 23, 31], most current systems tend to capture the scene-level gist rather than fine-grained entities, which largely undermines their applications in realworld scenarios such as blind navigation, video retrieval, and automatic video subtitling.", "startOffset": 53, "endOffset": 69}, {"referenceID": 22, "context": "Despite the encouraging progress in image captioning [30, 33, 23, 31], most current systems tend to capture the scene-level gist rather than fine-grained entities, which largely undermines their applications in realworld scenarios such as blind navigation, video retrieval, and automatic video subtitling.", "startOffset": 53, "endOffset": 69}, {"referenceID": 30, "context": "Despite the encouraging progress in image captioning [30, 33, 23, 31], most current systems tend to capture the scene-level gist rather than fine-grained entities, which largely undermines their applications in realworld scenarios such as blind navigation, video retrieval, and automatic video subtitling.", "startOffset": 53, "endOffset": 69}, {"referenceID": 9, "context": "One of the recent alternatives to sentence-level captioning is visual paragraph generation [10, 16, 35, 29], which aims to provide a coherent and detailed description, like telling stories for images/videos.", "startOffset": 91, "endOffset": 107}, {"referenceID": 15, "context": "One of the recent alternatives to sentence-level captioning is visual paragraph generation [10, 16, 35, 29], which aims to provide a coherent and detailed description, like telling stories for images/videos.", "startOffset": 91, "endOffset": 107}, {"referenceID": 34, "context": "One of the recent alternatives to sentence-level captioning is visual paragraph generation [10, 16, 35, 29], which aims to provide a coherent and detailed description, like telling stories for images/videos.", "startOffset": 91, "endOffset": 107}, {"referenceID": 28, "context": "One of the recent alternatives to sentence-level captioning is visual paragraph generation [10, 16, 35, 29], which aims to provide a coherent and detailed description, like telling stories for images/videos.", "startOffset": 91, "endOffset": 107}, {"referenceID": 15, "context": "Existing methods [16, 35, 19] deterministically optimizing over single annotated paragraph thus suffer from losing massive information expressed in the image.", "startOffset": 17, "endOffset": 29}, {"referenceID": 34, "context": "Existing methods [16, 35, 19] deterministically optimizing over single annotated paragraph thus suffer from losing massive information expressed in the image.", "startOffset": 17, "endOffset": 29}, {"referenceID": 18, "context": "Existing methods [16, 35, 19] deterministically optimizing over single annotated paragraph thus suffer from losing massive information expressed in the image.", "startOffset": 17, "endOffset": 29}, {"referenceID": 5, "context": "Inspired by Generative Adversarial Networks (GANs) [6], we establish an adversarial training mechanism between a structured paragraph generator and multi-level paragraph discriminators, where the discriminators learn to distinguish between real and synthesized paragraphs while the generator aims to fool the discriminators by generating diverse and realistic paragraphs.", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "Early works that posed this problem as a ranking and template retrieval tasks [5, 8, 14] performed poorly since it is hard to enumerate all possibilities in one collected dataset due to the compositional nature of language.", "startOffset": 78, "endOffset": 88}, {"referenceID": 7, "context": "Early works that posed this problem as a ranking and template retrieval tasks [5, 8, 14] performed poorly since it is hard to enumerate all possibilities in one collected dataset due to the compositional nature of language.", "startOffset": 78, "endOffset": 88}, {"referenceID": 13, "context": "Early works that posed this problem as a ranking and template retrieval tasks [5, 8, 14] performed poorly since it is hard to enumerate all possibilities in one collected dataset due to the compositional nature of language.", "startOffset": 78, "endOffset": 88}, {"referenceID": 17, "context": "Therefore, some recent works [18, 3, 4, 31, 23, 33, 20] focus on directly generating captions by modeling the semantic mapping from visual cues to language descriptions.", "startOffset": 29, "endOffset": 55}, {"referenceID": 2, "context": "Therefore, some recent works [18, 3, 4, 31, 23, 33, 20] focus on directly generating captions by modeling the semantic mapping from visual cues to language descriptions.", "startOffset": 29, "endOffset": 55}, {"referenceID": 3, "context": "Therefore, some recent works [18, 3, 4, 31, 23, 33, 20] focus on directly generating captions by modeling the semantic mapping from visual cues to language descriptions.", "startOffset": 29, "endOffset": 55}, {"referenceID": 30, "context": "Therefore, some recent works [18, 3, 4, 31, 23, 33, 20] focus on directly generating captions by modeling the semantic mapping from visual cues to language descriptions.", "startOffset": 29, "endOffset": 55}, {"referenceID": 22, "context": "Therefore, some recent works [18, 3, 4, 31, 23, 33, 20] focus on directly generating captions by modeling the semantic mapping from visual cues to language descriptions.", "startOffset": 29, "endOffset": 55}, {"referenceID": 32, "context": "Therefore, some recent works [18, 3, 4, 31, 23, 33, 20] focus on directly generating captions by modeling the semantic mapping from visual cues to language descriptions.", "startOffset": 29, "endOffset": 55}, {"referenceID": 19, "context": "Therefore, some recent works [18, 3, 4, 31, 23, 33, 20] focus on directly generating captions by modeling the semantic mapping from visual cues to language descriptions.", "startOffset": 29, "endOffset": 55}, {"referenceID": 2, "context": "Among all these research lines, advanced methods that train recurrent neural network language models conditioned on image features [3, 4, 31, 33] achieve great success by taking advantages of large-scale image captioning dataset.", "startOffset": 131, "endOffset": 145}, {"referenceID": 3, "context": "Among all these research lines, advanced methods that train recurrent neural network language models conditioned on image features [3, 4, 31, 33] achieve great success by taking advantages of large-scale image captioning dataset.", "startOffset": 131, "endOffset": 145}, {"referenceID": 30, "context": "Among all these research lines, advanced methods that train recurrent neural network language models conditioned on image features [3, 4, 31, 33] achieve great success by taking advantages of large-scale image captioning dataset.", "startOffset": 131, "endOffset": 145}, {"referenceID": 32, "context": "Among all these research lines, advanced methods that train recurrent neural network language models conditioned on image features [3, 4, 31, 33] achieve great success by taking advantages of large-scale image captioning dataset.", "startOffset": 131, "endOffset": 145}, {"referenceID": 3, "context": "Similar success has been already seen in video captioning fields [4, 32].", "startOffset": 65, "endOffset": 72}, {"referenceID": 31, "context": "Similar success has been already seen in video captioning fields [4, 32].", "startOffset": 65, "endOffset": 72}, {"referenceID": 11, "context": "Dense captioning [12] is recently proposed to describe each region of interest with a short phrase, considering more details than standard image captioning.", "startOffset": 17, "endOffset": 21}, {"referenceID": 18, "context": "To reason about long-term linguistic structures with multiple sentences, hierarchical recurrent network [19, 21, 35, 16] has been widely used to directly simulate the hierarchy of language.", "startOffset": 104, "endOffset": 120}, {"referenceID": 20, "context": "To reason about long-term linguistic structures with multiple sentences, hierarchical recurrent network [19, 21, 35, 16] has been widely used to directly simulate the hierarchy of language.", "startOffset": 104, "endOffset": 120}, {"referenceID": 34, "context": "To reason about long-term linguistic structures with multiple sentences, hierarchical recurrent network [19, 21, 35, 16] has been widely used to directly simulate the hierarchy of language.", "startOffset": 104, "endOffset": 120}, {"referenceID": 15, "context": "To reason about long-term linguistic structures with multiple sentences, hierarchical recurrent network [19, 21, 35, 16] has been widely used to directly simulate the hierarchy of language.", "startOffset": 104, "endOffset": 120}, {"referenceID": 34, "context": "[35] generate multi-sentence video descriptions for cooking videos to capture strong temporal dependencies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] combine semantics of all regions of interest to produce a generic paragraph for an image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Given an input image, we first detect a set of semantic regions using dense captioning method [12].", "startOffset": 94, "endOffset": 98}, {"referenceID": 5, "context": "As the original GAN [6] that optimizes over binary probability distance suffers from mode collapse and instable convergence, we follow the new Wasserstein GAN [1] method that theoretically remedies this by minimizing an approximated Wasserstein distance.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "As the original GAN [6] that optimizes over binary probability distance suffers from mode collapse and instable convergence, we follow the new Wasserstein GAN [1] method that theoretically remedies this by minimizing an approximated Wasserstein distance.", "startOffset": 159, "endOffset": 162}, {"referenceID": 8, "context": "The discrete nature of text samples hinders gradient back-propagation from the discriminators to the generator [9].", "startOffset": 111, "endOffset": 114}, {"referenceID": 35, "context": "We address this issue following SeqGAN [36].", "startOffset": 39, "endOffset": 43}, {"referenceID": 35, "context": "The gradients are passed back to the intermediate action value via Monte Carlo search [36].", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "Given an input image, we adopt the dense captioning model [12, 16] to detect semantic regions of the image and generate their local phrases.", "startOffset": 58, "endOffset": 66}, {"referenceID": 15, "context": "Given an input image, we adopt the dense captioning model [12, 16] to detect semantic regions of the image and generate their local phrases.", "startOffset": 58, "endOffset": 66}, {"referenceID": 6, "context": "Following the copy mechanism [7] firstly proposed in natural language processing, we selectively incorporate the embeddings of local phrases based on the topic vector ht and preceding word state ht,i\u22121, i \u2208 {1, .", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "The adversarial framework is trained following the Wasserstein GAN (WGAN) [1] in which we alternate between the optimization of {D, D}with Eq.", "startOffset": 74, "endOffset": 77}, {"referenceID": 26, "context": "We use minibatch SGD and apply the RMSprop solver [27] with the initial learning rate set to 0.", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "For stable training, we apply batch normalization [11] and", "startOffset": 50, "endOffset": 54}, {"referenceID": 12, "context": "In terms of the fully-supervised setting, to make a fair comparison with the state-of-the-art methods [13, 16], the experiments are conducted on the public image paragraph dataset [16], where 14,575 image-paragraph pairs are used for training, 2,487 for validation and 2,489 for testing.", "startOffset": 102, "endOffset": 110}, {"referenceID": 15, "context": "In terms of the fully-supervised setting, to make a fair comparison with the state-of-the-art methods [13, 16], the experiments are conducted on the public image paragraph dataset [16], where 14,575 image-paragraph pairs are used for training, 2,487 for validation and 2,489 for testing.", "startOffset": 102, "endOffset": 110}, {"referenceID": 15, "context": "In terms of the fully-supervised setting, to make a fair comparison with the state-of-the-art methods [13, 16], the experiments are conducted on the public image paragraph dataset [16], where 14,575 image-paragraph pairs are used for training, 2,487 for validation and 2,489 for testing.", "startOffset": 180, "endOffset": 184}, {"referenceID": 1, "context": "In terms of semi-supervised setting, our RTT-GAN is trained with the single sentence annotations provided in MSCOCO image captioning dataset [2] which contains 123,000 images.", "startOffset": 141, "endOffset": 144}, {"referenceID": 15, "context": "For both fully-supervised and semi-supervised settings, we use the word vocabulary of image-paragraph dataset as [16] does and the 14,575 paragraph descriptions on public image paragraph dataset [16] are adopted as the standalone paragraph corpus for training discriminators.", "startOffset": 113, "endOffset": 117}, {"referenceID": 15, "context": "For both fully-supervised and semi-supervised settings, we use the word vocabulary of image-paragraph dataset as [16] does and the 14,575 paragraph descriptions on public image paragraph dataset [16] are adopted as the standalone paragraph corpus for training discriminators.", "startOffset": 195, "endOffset": 199}, {"referenceID": 15, "context": "We obtain the results of all four baselines from [16].", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "Image-Flat [13] directly decodes an image into a paragraph token by token.", "startOffset": 11, "endOffset": 15}, {"referenceID": 15, "context": "And Region-Hierarchical [16] uses a hierarchical recurrent neural network to decompose the paragraphs into the corresponding sentences.", "startOffset": 24, "endOffset": 28}, {"referenceID": 25, "context": "Same with all baselines, we adopt VGG-16 net [26] to encode the visual representation of an image.", "startOffset": 45, "endOffset": 49}, {"referenceID": 15, "context": "Note that our RTT-GAN and RegionHierarchical [16] use the same dense captioning model [12] to extract semantic regions.", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "Note that our RTT-GAN and RegionHierarchical [16] use the same dense captioning model [12] to extract semantic regions.", "startOffset": 86, "endOffset": 90}, {"referenceID": 15, "context": "Human shows the results by collecting an additional paragraph for 500 randomly chosen images as [16].", "startOffset": 96, "endOffset": 100}, {"referenceID": 12, "context": "38 Image-Flat [13] 12.", "startOffset": 14, "endOffset": 18}, {"referenceID": 15, "context": "71 Regions-Hierarchical [16] 15.", "startOffset": 24, "endOffset": 28}, {"referenceID": 23, "context": "As in Table 4, we also extend our RTT-GAN to the task of video paragraph generation and evaluate it on TACoSMultiLevel dataset [24] that contains 185 long videos filmed in an indoor environment, following [35].", "startOffset": 127, "endOffset": 131}, {"referenceID": 34, "context": "As in Table 4, we also extend our RTT-GAN to the task of video paragraph generation and evaluate it on TACoSMultiLevel dataset [24] that contains 185 long videos filmed in an indoor environment, following [35].", "startOffset": 205, "endOffset": 209}, {"referenceID": 34, "context": "Similar to [35], we use the pre-trained C3D [28] model on the Sport1M dataset [15], which outputs a fixed-length feature vector every 16 frames.", "startOffset": 11, "endOffset": 15}, {"referenceID": 27, "context": "Similar to [35], we use the pre-trained C3D [28] model on the Sport1M dataset [15], which outputs a fixed-length feature vector every 16 frames.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "Similar to [35], we use the pre-trained C3D [28] model on the Sport1M dataset [15], which outputs a fixed-length feature vector every 16 frames.", "startOffset": 78, "endOffset": 82}, {"referenceID": 24, "context": "Method BLEU-4 METEOR CIDEr CRF-T [25] 25.", "startOffset": 33, "endOffset": 37}, {"referenceID": 23, "context": "8 CRF-M [24] 27.", "startOffset": 8, "endOffset": 12}, {"referenceID": 3, "context": "7 LRCN [4] 29.", "startOffset": 7, "endOffset": 10}, {"referenceID": 34, "context": "4 h-RNN [35] 30.", "startOffset": 8, "endOffset": 12}], "year": 2017, "abstractText": "A natural image usually conveys rich semantic content and can be viewed from different angles. Existing image description methods are largely restricted by small sets of biased visual paragraph annotations, and fail to cover rich underlying semantics. In this paper, we investigate a semi-supervised paragraph generative framework that is able to synthesize diverse and semantically coherent paragraph descriptions by reasoning over local semantic regions and exploiting linguistic knowledge. The proposed Recurrent Topic-Transition Generative Adversarial Network (RTT-GAN) builds an adversarial framework between a structured paragraph generator and multi-level paragraph discriminators. The paragraph generator generates sentences recurrently by incorporating region-based visual and language attention mechanisms at each step. The quality of generated paragraph sentences is assessed by multi-level adversarial discriminators from two aspects, namely, plausibility at sentence level and topic-transition coherence at paragraph level. The joint adversarial training of RTT-GAN drives the model to generate realistic paragraphs with smooth logical transition between sentence topics. Extensive quantitative experiments on image and video paragraph datasets demonstrate the effectiveness of our RTT-GAN in both supervised and semi-supervised settings. Qualitative results on telling diverse stories for an image also verify the interpretability of RTT-GAN.", "creator": "LaTeX with hyperref package"}}}