{"id": "1703.07432", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "Efficient PAC Learning from the Crowd", "abstract": "In recent years crowdsourcing has become the method of choice for gathering labeled training data for learning algorithms. Standard approaches to crowdsourcing view the process of acquiring labeled data separately from the process of learning a classifier from the gathered data. This can give rise to computational and statistical challenges. For example, in most cases there are no known computationally efficient learning algorithms that are robust to the high level of noise that exists in crowdsourced data, and efforts to eliminate noise through voting often require a large number of queries per example.\n\n\n\nThe data that the researchers gathered is now used in many ways, but with this approach, they do not rely on either the machine or the training tools in-memory to produce the desired performance results. Instead, the technique has been designed to generate an \"optimal\" performance score based on a dataset. A training dataset, like a high-level training dataset, can perform only one training task at a time, and can only do several training tasks at a time, while the training data should be collected only to be available when all training tasks are completed.\nThe researchers' work suggests that this approach could be used as a way to provide training data for algorithms in a more intuitive way. The findings of the latest study are published online in Physical Review Letters in the journal Proceedings of the National Academy of Sciences.\nAs shown in the article, the authors estimate that the data collected by the researchers in the previous study will be available only in memory for training and to be collected by the machine, but will be able to use only a subset of those training tasks. Therefore, the findings indicate that the data collected by the researchers will be in an efficient way.\n\"Given how many people have used the training software, this approach has a lot of potential,\" said Robert D. Dennison, an associate professor of information technology at the University of Texas at Austin. \"As a result, this approach is very effective, but can also help developers to understand the problems that could arise as a result of a training dataset. The authors point out that a training dataset can also provide training data for algorithms in a more intuitive way. The data collected by the researchers will be available only in memory for training and to be collected by the machine, but will be able to use only a subset of those training tasks. Therefore, the findings indicate that the data collected by the researchers will be in an efficient way. The researchers propose that the data collected by the researchers will be in an efficient way. The findings indicate that the", "histories": [["v1", "Tue, 21 Mar 2017 21:05:27 GMT  (28kb)", "https://arxiv.org/abs/1703.07432v1", null], ["v2", "Thu, 13 Apr 2017 21:21:41 GMT  (28kb)", "http://arxiv.org/abs/1703.07432v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["pranjal awasthi", "avrim blum", "nika haghtalab", "yishay mansour"], "accepted": false, "id": "1703.07432"}, "pdf": {"name": "1703.07432.pdf", "metadata": {"source": "CRF", "title": "Efficient PAC Learning from the Crowd", "authors": ["Pranjal Awasthi", "Avrim Blum", "Nika Haghtalab", "Yishay Mansour"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n07 43\n2v 2\n[ cs\n.L G\n] 1\n3 A\npr 2\n01 7\nIn recent years crowdsourcing has become the method of choice for gathering labeled training data for learning algorithms. Standard approaches to crowdsourcing view the process of acquiring labeled data separately from the process of learning a classifier from the gathered data. This can give rise to computational and statistical challenges. For example, in most cases there are no known computationally efficient learning algorithms that are robust to the high level of noise that exists in crowdsourced data, and efforts to eliminate noise through voting often require a large number of queries per example.\nIn this paper, we show how by interleaving the process of labeling and learning, we can attain computational efficiency with much less overhead in the labeling cost. In particular, we consider the realizable setting where there exists a true target function in F and consider a pool of labelers. When a noticeable fraction of the labelers are perfect, and the rest behave arbitrarily, we show that any F that can be efficiently learned in the traditional realizable PAC model can be learned in a computationally efficient manner by querying the crowd, despite high amounts of noise in the responses. Moreover, we show that this can be done while each labeler only labels a constant number of examples and the number of labels requested per example, on average, is a constant. When no perfect labelers exist, a related task is to find a set of the labelers which are good but not perfect. We show that we can identify all good labelers, when at least the majority of labelers are good."}, {"heading": "1 Introduction", "text": "Over the last decade, research in machine learning and AI has seen tremendous growth, partly due to the ease\nwith which we can collect and annotate massive amounts of data across various domains. This rate of data annotation has been made possible due to crowdsourcing tools, such as Amazon Mechanical TurkTM, that\nfacilitate individuals\u2019 participation in a labeling task. In the context of classification, a crowdsourced model\nuses a large pool of workers to gather labels for a given training data set that will be used for the purpose\nof learning a good classifier. Such learning environments that involve the crowd give rise to a multitude of\ndesign choices that do not appear in traditional learning environments. These include: How does the goal of learning from the crowd differs from the goal of annotating data by the crowd? What challenges does\nthe high amount of noise typically found in curated data sets [Wais et al., 2010, Kittur et al., 2008, Ipeirotis\net al., 2010] pose to the learning algorithms? How do learning and labeling processes interplay? How many\nlabels are we willing to take per example? And, how much load can a labeler handle?\n\u2217Rutgers University, pranjal.awasthi@rutgers.edu \u2020Carnegie Mellon University, avrim@cs.cmu.edu. Supported in part by NSF grants CCF-1525971 and CCF-1535967. This work was done in part while the author was visiting the Simons Institute for the Theory of Computing. \u2021Carnegie Mellon University, nhaghtal@cs.cmu.edu. Supported in part by NSF grants CCF-1525971 and CCF-1535967 and a Microsoft Research Ph.D. Fellowship. This work was done in part while the author was visiting the Simons Institute for the Theory of Computing. \u00a7Blavatnik School of Computer Science, Tel-Aviv University, mansour@tau.ac.il. This work was done while the author was at Microsoft Research, Herzliya. Supported in part by a grant from the Science Foundation (ISF), by a grant from United States-Israel Binational Science Foundation (BSF), and by The Israeli Centers of Research Excellence (I-CORE) program (Center No. 4/11).\nIn recent years, there have been many exciting works addressing various theoretical aspects of these\nand other questions [Slivkins and Vaughan, 2014], such as reducing noise in crowdsourced data [Dekel and\nShamir, 2009], task assignment [Badanidiyuru et al., 2013, Tran-Thanh et al., 2014] in online or offline\nsettings [Karger et al., 2014], and the role of incentives [Ho et al., 2013]. In this paper we focus on one\nsuch aspect, namely, how to efficiently learn and generalize from the crowd with minimal cost? The standard\napproach is to view the process of acquiring labeled data through crowdsourcing and the process of learning\na classifier in isolation. In other words, a typical learning process involves collecting data labeled by many\nlabelers via a crowdsourcing platform followed by running a passive learning algorithm to extract a good\nhypothesis from the labeled data. As a result, approaches to crowdsourcing focus on getting high quality\nlabels per example and not so much on the task further down in the pipeline. Naive techniques such as taking majority votes to obtain almost perfect labels have a cost per labeled example that scales with the data size, namely log(m\u03b4 ) queries per label where m is the training data size and \u03b4 is the desired failure probability. This is undesirable in many scenarios when data size is large. Furthermore, if only a small fraction of the\nlabelers in the crowd are perfect, such approaches will inevitably fail. An alternative is to feed the noisy\nlabeled data to existing passive learning algorithms. However, we currently lack computationally efficient\nPAC learning algorithms that are provably robust to high amounts of noise that exists in crowdsourced data.\nHence separating the learning process from the data annotation process results in high labeling costs or\nsuboptimal learning algorithms.\nIn light of the above, we initiate the study of designing efficient PAC learning algorithms in a crowd-\nsourced setting where learning and acquiring labels are done in tandem. We consider a natural model of\ncrowdsourcing and ask the fundamental question of whether efficient learning with little overhead in labeling cost is possible in this scenario. We focus on the classical PAC setting of Valiant [1984] where there exists a true target classifier f\u2217 \u2208 F and the goal is to learn F from a finite training set generated from the underlying distribution. We assume that one has access to a large pool of labelers that can provide (noisy)\nlabels for the training set. We seek algorithms that run in polynomial time and produce a hypothesis with\nsmall error. We are especially interested in settings where there are computationally efficient algorithms for learning F in the consistency model, i.e. the realizable PAC setting. Additionally, we also want our algorithms to make as few label queries as possible, ideally requesting a total number of labels that is within a constant factor of the amount of labeled data needed in the realizable PAC setting. We call this O(1) overhead or cost per labeled example. Furthermore, in a realistic scenario each labeler can only provide labels\nfor a constant number of examples, hence we cannot ask too many queries to a single labeler. We call the\nnumber of queries asked to a particular labeler the load of that labeler.\nPerhaps surprisingly, we show that when a noticeable fraction of the labelers in our pool are perfect all of the above objectives can be achieved simultaneously. That is, if F can be efficiently PAC learned in the realizable PAC model, then it can be efficiently PAC learned in the noisy crowdsourcing model with a\nconstant cost per labeled example. In other words, the ratio of the number of label requests in the noisy\ncrowdsourcing model to the number of labeled examples needed in the traditional PAC model with a perfect\nlabeler is a constant and does not increase with the size of the data set. Additionally, each labeler is asked to label only a constant number of examples, i.e., O(1) load per labeler. Our results also answer an open question of Dekel and Shamir [2009] regarding the possibility of efficient noise robust PAC learning by\nperforming labeling and learning simultaneously. When no perfect labelers exist, a related task is to find a\nset of the labelers which are good but not perfect. We show that we can identify the set of all good labelers,\nwhen at least the majority of labelers are good."}, {"heading": "1.1 Overview of Results", "text": "We study various versions of the model described above. In the most basic setting we assume that a large percentage, say 70% of the labelers are perfect, i.e., they always label according to the target function f\u2217.\nThe remaining 30% of the labelers could behave arbitrarily and we make no assumptions on them. Since the perfect labelers are in strong majority, a straightforward approach is to label each example with the\nmajority vote over a few randomly chosen labelers, to produce the correct label on every instance with high probability. However, such an approach leads to a query bound of O(log m\u03b4 ) per labeled example, where m is the size of the training set and \u03b4 is the acceptable probability of failure. In other words, the cost per labeled example is O(log m\u03b4 ) and scales with the size of the data set. Another easy approach is to pick a few labelers at random and ask them to label all the examples. Here, the cost per labeled example is a constant but the\napproach is infeasible in a crowdsourcing environment since it requires a single or a constant number of\nlabelers to label the entire data set. Yet another approach is to label each example with the majority vote of O(log 1\u01eb ) labelers. While the labeled sample set created in this way only has error of \u01eb, it is still unsuitable for being used with PAC learning algorithms as they are not robust to even small amounts of noise, if the noise is\nheterogeneous. So, the computational challenges still persist. Nevertheless, we introduce an algorithm that performs efficient learning with O(1) cost per labeled example and O(1) load per labeler.\nTheorem 4.3 (Informal) Let F be a hypothesis class that can be PAC learned in polynomial time to \u01eb error with probability 1 \u2212 \u03b4 using m\u01eb,\u03b4 samples. Then F can be learned in polynomial time using O(m\u01eb,\u03b4) samples in a crowdsourced setting with O(1) cost per labeled example, provided a 12 +\u0398(1) fraction of the labelers are perfect. Furthermore, every labeler is asked to label only 1 example.\nNotice that the above theorem immediately implies that each example is queried only O(1) times on average as opposed to the data size dependent O(log(m\u03b4 )) cost incurred by the naive majority vote style procedures. We next extend our result to the setting where the fraction of perfect labelers is significant but might be less than 12 , say 0.4. Here we again show that F can be efficiently PAC learned using O(m\u01eb,\u03b4) queries provided we have access to an \u201cexpert\u201d that can correctly label a constant number of examples. We call such queries that are made to an expert golden queries. When the fraction of perfect labelers is close to 1 2 , say 0.4, we show that just one golden query is enough to learn. More generally, when the fraction of the perfect labelers is some \u03b1, we show that O(1/\u03b1) golden queries is sufficient to learn a classifier efficiently. We describe our results in terms of \u03b1, but we are particularly interested in regimes where \u03b1 = \u0398(1).\nTheorem 4.13 (Informal) Let F be a hypothesis class that can be PAC learned in polynomial time to \u01eb error with probability 1 \u2212 \u03b4 using m\u01eb,\u03b4 samples. Then F can be learned in polynomial time using O(m\u01eb,\u03b4) samples in a crowdsourced setting with O( 1\u03b1) cost per labeled example, provided more than an \u03b1 fraction of the labelers are perfect for some constant \u03b1 > 0. Furthermore, every labeler is asked to label only O( 1\u03b1 ) examples and the algorithm uses at most 2\u03b1 golden queries.\nThe above two theorems highlight the importance of incorporating the structure of the crowd in al-\ngorithm design. Being oblivious to the labelers will result in noise models that are notoriously hard. For\ninstance, if one were to assume that each example is labeled by a single random labeler drawn from the\ncrowd, one would recover the Malicious Misclassification Noise of Rivest and Sloan [1994]. Getting computationally efficient learning algorithms even for very simple hypothesis classes has been a long standing\nopen problem in this space. Our results highlight that by incorporating the structure of the crowd, one can\nefficiently learn any hypothesis class with a small overhead.\nFinally, we study the scenario when none of the labelers are perfect. Here we assume that the majority of the labelers are \u201cgood\u201d, that is they provide labels according to functions that are all \u01eb-close to the target function. In this scenario generating a hypothesis of low error is as hard as agnostic learning1. Nonetheless, we show that one can detect all of the good labelers using expected O(1\u01eb log(n)) queries per labeler, where n is the target number of labelers desired in the pool.\nTheorem 5.1 (Informal) Assume we have a target set of n labelers that are partitioned into two sets, good and bad. Furthermore, assume that there are at least n2 good labelers who always provide labels according to\n1This can happen for instance when all the labelers label according to a single function f that is \u01eb-far from f\u2217.\nfunctions that are \u01eb-close to a target function f\u2217. The set of bad labelers always provide labels according to functions that are at least 4\u01eb away from the target. Then there is a polynomial time algorithm that identifies, with probability at least 1\u2212\u03b4, all the good labelers and none of the bad labelers using expected O(1\u01eb log(n\u03b4 )) queries per labeler."}, {"heading": "1.2 Related Work", "text": "Crowdsourcing has received significant attention in the machine learning community. As mentioned in the\nintroduction, crowdsourcing platforms require one to address several questions that are not present in tradi-\ntional modes of learning.\nThe work of Dekel and Shamir [2009] shows how to use crowdsourcing to reduce the noise in a training\nset before feeding it to a learning algorithm. Our results answer an open question in their work by showing\nthat performing data labeling and learning in tandem can lead to significant benefits.\nA large body of work in crowdsourcing has focused on the problem of task assignment. Here, workers\narrive in an online fashion and a requester has to choose to assign specific tasks to specific workers. Ad-\nditionally, workers might have different abilities and might charge differently for the same task. The goal\nfrom the requester\u2019s point of view is to finish multiple tasks within a given budget while maintaining a certain minimum quality [Ho et al., 2013, Tran-Thanh et al., 2014]. There is also significant work on dynamic\nprocurement where the focus is on assigning prices to the given tasks so as to provide incentive to the crowd\nto perform as many of them as possible within a given budget [Badanidiyuru et al., 2012, 2013, Singla and\nKrause, 2013]. Unlike our setting, the goal in these works is not to obtain a generalization guarantee or learn\na function, but rather to complete as many tasks as possible within the budget.\nThe work of Karger et al. [2011, 2014] also studies the problem of task assignment in offline and on-\nline settings. In the offline setting, the authors provide an algorithm based on belief propagation that infers\nthe correct answers for each task by pooling together the answers from each worker. They show that their\napproach performs better than simply taking majority votes. Unlike our setting, their goal is to get an approx-\nimately correct set of answers for the given data set and not to generalize from the answers. Furthermore,\ntheir model assumes that each labeler makes an error at random independently with a certain probability. We, on the other hand, make no assumptions on the nature of the bad labelers.\nAnother related model is the recent work of Steinhardt et al. [2016]. Here the authors look at the problem\nof extracting top rated items by a group of labelers among whom a constant fraction are consistent with the\ntrue ratings of the items. The authors use ideas from matrix completion to design an algorithm that can recover the top rated items with an \u01eb fraction of the noise provided every labeler rates \u223c 1 \u01eb4 items and one has access to \u223c 1 \u01eb2 ratings from a trusted expert. Their model is incomparable to ours since their goal is to recover the top rated items and not to learn a hypothesis that generalizes to a test set.\nOur results also shed insights into the notorious problem of PAC learning with noise. Despite decades\nof research into PAC learning, noise tolerant polynomial time learning algorithms remain elusive. There\nhas been substantial work on PAC learning under realistic noise models such as the Massart noise or the\nTsybakov noise models [Bousquet et al., 2005]. However, computationally efficient algorithms for such models are known in very restricted cases [Awasthi et al., 2015, 2016]. In contrast, we show that by using\nthe structure of the crowd, one can indeed design polynomial time PAC learning algorithms even when the\nnoise is of the type mentioned above.\nMore generally, interactive models of learning have been studied in the machine learning commu-\nnity [Cohn et al., 1994, Dasgupta, 2005, Balcan et al., 2006, Koltchinskii, 2010, Hanneke, 2011, Zhang\nand Chaudhuri, 2015, Yan et al., 2016]. We describe some of these works in Appendix A."}, {"heading": "2 Model and Notations", "text": "Let X be an instance space and Y = {+1,\u22121} be the set of possible labels. A hypothesis is a function f : X \u2192 Y that maps an instance x \u2208 X to its classification y. We consider the realizable setting where there is a distribution over X \u00d7 Y and a true target function in hypothesis class F . More formally, we consider a distribution D over X \u00d7Y and an unknown hypothesis f\u2217 \u2208 F , where errD(f\u2217) = 0. We denote the marginal of D over X by D|X . The error of a hypothesis f with respect to distribution D is defined as errD(f) = Pr(x,f\u2217(x))\u223cD[f(x) 6= f\u2217(x)].\nIn order to achieve our goal of learning f\u2217 well with respect to distribution D, we consider having access to a large pool of labelers, some of whom label according to f\u2217 and some who do not. Formally, labeler i is defined by its corresponding classification function gi : X \u2192 Y . We say that gi is perfect if errD(gi) = 0. We consider a distribution P that is uniform over all labelers and let \u03b1 = Pri\u223cP [errD(gi) = 0] be the fraction of perfect labelers. We allow an algorithm to query labelers on instances drawn from D|X . Our goal is to design learning algorithms that efficiently learn a low error classifier while maintaining a small\noverhead in the number of labels. We compare the computational and statistical aspects of our algorithms to\ntheir PAC counterparts in the realizable setting.\nIn the traditional PAC setting with a realizable distribution, m\u01eb,\u03b4 denotes the number of samples needed for learning F . That is, m\u01eb,\u03b4 is the total number of labeled samples drawn from the realizable distribution D needed to output a classifier f that has errD(f) \u2264 \u01eb, with probability 1 \u2212 \u03b4. We know from the VC theory [Anthony and Bartlett, 1999], that for a hypothesis class F with VC-dimension d and no additional assumptions on F , m\u01eb,\u03b4 \u2208 O ( \u01eb\u22121 ( d ln ( 1 \u01eb ) + ln ( 1 \u03b4 )))\n. Furthermore, we assume that efficient algorithms for the realizable setting exist. That is, we consider an oracleOF that for a set of labeled instances S, returns a function f \u2208 F that is consistent with the labels in S, if one such function exists, and outputs \u201cNone\u201d otherwise.\nGiven an algorithm in the noisy crowd-sourcing setting, we define the average cost per labeled example of the algorithm, denoted by \u039b, to be the ratio of the number of label queries made by the algorithm to the number of labeled examples needed in the traditional realizable PAC model,m\u01eb,\u03b4. The load of an algorithm, denoted by \u03bb, is the maximum number of label queries that have to be answered by an individual labeler. In other words, \u03bb is the maximum number of labels queried from one labeler, when P has an infinitely large support. 2 When the number of labelers is fixed, such as in Section 5, we define the load to simply be the\nnumber of queries answered by a single labeler. Moreover, we allow an algorithm to directly query the target hypothesis f\u2217 on a few, e.g., O(1), instances drawn from D|X . We call these \u201cgolden queries\u201d and denote their total number by \u0393.\nGiven a set of labelers L and an instance x \u2208 X , we define MajL(x) to be the label assigned to x by the majority of labelers in L. Moreover, we denote by Maj-sizeL(x) the fraction of the labelers in L that agree with the label MajL(x). Given a set of classifiers H , we denote by MAJ (H) the classifier that for each x returns prediction MajH(x). Given a distribution P over labelers and a set of labeled examples S, we denote by P|S the distribution P conditioned on labelers that agree with labeled samples (x, y) \u2208 S. We consider S to be small, typically of size O(1). Note that we can draw a labeler from P|S by first drawing a labeler according to P and querying it on all the labeled instances in S. Therefore, when P has infinitely large support, the load of an algorithm is the maximum size of S that P is ever conditioned on.\n2The concepts of total number of queries and load may be seen as analogous to work and depth in parallel algorithms, where work is the total number of operations performed by an algorithm and depth is the maximum number of operations that one processor has to perform in a system with infinitely many processors."}, {"heading": "3 A Baseline Algorithm and a Road-map for Improvement", "text": "In this section, we briefly describe a simple algorithm and the approach we use to improve over it. Consider a very simple baseline algorithm for the case of \u03b1 > 12 :\nBASELINE: Draw a sample of size m = m\u01eb,\u03b4 from D|X and label each x \u2208 S by MajL(x), where L \u223c P k for k = O ( (\u03b1\u2212 0.5)\u22122 ln (\nm \u03b4\n))\nis a set of randomly drawn labelers. Return\nclassifier OF (S). That is, the baseline algorithm queries enough labelers on each sample such that with probability 1 \u2212 \u03b4 all the labels are correct. Then, it learns a classifier using this labeled set. It is clear that the performance of BASELINE is far from being desirable. First, this approach takes log(m/\u03b4) more labels than it requires samples, leading to an average cost per labeled example that increases with the size of the sample set. Moreover, when perfect labelers form a small majority of the labelers, i.e., \u03b1 = 12 + o(1), the number of labels needed to correctly label an instance increases drastically. Perhaps even more troubling is that if the perfect labelers are in minority, i.e., \u03b1 < 12 , S may be mislabeled andOF (S) may return a classifier that has large error, or no classifier at all. In this work, we improve over BASELINE in both aspects.\nIn Section 4, we improve the log(m/\u03b4) average cost per labeled example by interleaving the two processes responsible for learning a classifier and querying labels. In particular, BASELINE first finds high\nquality labels, i.e., labels that are correct with high probability, and then learns a classifier that is consistent\nwith those labeled samples. However, interleaving the process of learning and acquiring high quality labels can make both processes more efficient. At a high level, for a given classifier h that has a larger than desirable error, one may be able to find regions where h performs particularly poorly. That is, the classifications provided by h may differ from the correct label of the instances. In turn, by focusing our effort for getting high quality labels on these regions we can output a correctly labeled sample set using less label queries overall. These additional correctly labeled instances from regions where h performs poorly can help us improve the error rate of h in return. In Section 4, we introduce an algorithm that draws on ideas from boosting and a probabilistic filtering approach that we develop in this work to facilitate interactions between learning\nand querying.\nIn Section 4.1, we remove the dependence of label complexity on (\u03b1 \u2212 0.5)\u22122 using O(1/\u03b1) golden queries. At a high level, instances where only a small majority of labelers agree are difficult to label using\nqueries asked from labelers. But, these instances are great test cases that help us identify a large fraction of\nimperfect labelers. That is, we can first ask a golden query on one such instance to get its correct label and\nfrom then on only consider labelers that got this label correctly. In other words, we first test the labelers on\none or very few tests questions, if they pass the tests, then we ask them real label queries for the remainder\nof the algorithm, if not, we never consider them again."}, {"heading": "4 An Interleaving Algorithm", "text": "In this section, we improve over the average cost per labeled example of the BASELINE algorithm, by inter-\nleaving the process of learning and acquiring high quality labels. Our Algorithm 2 facilitates the interactions\nbetween the learning process and the querying process using ideas from classical PAC learning and adaptive techniques we develop in this work. For ease of presentation, we first consider the case where \u03b1 = 12+\u0398(1), say \u03b1 \u2265 0.7, and introduce an algorithm and techniques that work in this regime. In Section 4.1, we show how our algorithm can be modified to work with any value of \u03b1. For convenience, we assume in the analysis below that distribution D is over a discrete space. This is in fact without loss of generality, since using uniform convergence one can instead work with the uniform distribution over an unlabeled sample multiset of size O( d \u01eb2 ) drawn from D|X .\nHere, we provide an overview of the techniques and ideas used in this algorithm.\nBoosting: In general, boosting algorithms [Schapire, 1990, Freund, 1990, Freund and Schapire, 1995] provide a mechanism for producing a classifier of error \u01eb using learning algorithms that are only capable of producing classifiers with considerably larger error rates, typically of error p = 12 \u2212 \u03b3 for small \u03b3. In particular, early work of Schapire [1990] in this space shows how one can combine 3 classifiers of error p to get a classifier of error O(p2), for any p > 0.\nTheorem 4.1 (Schapire [1990]). For any p > 0 and distributionD, consider three classifiers: 1) classifier h1 such that errD(h1) \u2264 p; 2) classifier h2 such that errD2(h2) \u2264 p, whereD2 = 12DC+ 12DI for distributions DC and DI that denote distribution D conditioned on {x | h1(x) = f\u2217(x)} and {x | h1(x) 6= f\u2217(x)}, respectively; 3) classifier h3 such that errD3(h3) \u2264 p, where D3 is D conditioned on {x | h1(x) 6= h2(x)}. Then, errD(MAJ (h1, h2, h3)) \u2264 3p2 \u2212 2p3.\nAs opposed to the main motivation for boosting where the learner only has access to a learning algorithm of error p = 12 \u2212 \u03b3, in our setting we can learn a classifier to any desired error rate p as long as we have a sample set of mp,\u03b4 correctly labeled instances. The larger the error rate p, the smaller the total number of label queries needed for producing a correctly labeled set of the appropriate size. We use this idea in Algorithm 2. In particular, we learn classifiers of error O( \u221a \u01eb) using sample sets of size O(m\u221a\u01eb,\u03b4) that are labeled by majority vote of O(log(m\u221a\u01eb,\u03b4)) labelers, using fewer label queries overall than BASELINE.\nProbabilistic Filtering: Given classifier h1, the second step of the classical boosting algorithm requires distribution D to be reweighed based on the correctness of h1. This step can be done by a filtering process as follows: Take a large set of labeled samples fromD and divide them to two sets depending on whether or not the instances are mislabeled by h1. Distribution D2, in which instances mislabeled by h1 make up half of the weight, can be simulated by picking each set with probability 12 and taking an instance from that set uniformly at random. To implement filtering in our setting, however, we would need to first get high quality labels for the set of instances used for simulating D2. Furthermore, this sample set is typically large, since at least 1pmp,\u03b4 random samples from D are needed to simulate D2 that has half of its weight on the points that\nh1 mislabels (which is a p fraction of the total points). In our case where p = O( \u221a \u01eb), getting high quality labels for such a large sample set requires O ( m\u01eb,\u03b4 ln (m\u01eb,\u03b4\n\u03b4\n))\nlabel queries, which is as large as the total\nnumber of labels queried by BASELINE.\nAlgorithm 1 FILTER(S, h) Let SI = \u2205 and N = log ( 1 \u01eb ) . for x \u2208 S do for t = 1, . . . , N do\nDraw a random labeler i \u223c P and let yt = gi(x) If t is odd and Maj(y1:t) = h(x), then break.\nend\nLet SI = SI \u222a {x}. // Reaches this step when for all t, Maj(y1:t) 6= h(x) end return SI\nIn this work, we introduce a probabilistic filtering approach, called FILTER, that only requires O (m\u01eb,\u03b4) label queries, i.e., O(1) cost per labeled example. Given classifier h1 and an unlabeled sample set S, FILTER(S, h1) returns a set SI \u2286 S such that for any x \u2208 S that is mislabeled by h1, x \u2208 SI with probability at least \u0398(1). Moreover, any x that is correctly labeled by h1 is most likely not included in SI . This procedure is described in detail in Algorithm 1. Here, we provide a brief description of its working: For any x \u2208 S, FILTER queries one labeler at a time, drawn at random, until the majority of the labels it has\nacquired so far agree with h1(x), at which point FILTER removes x from consideration. On the other hand, if the majority of the labels never agree with h1(x), FILTER adds x to the output set SI . Consider x \u2208 S that is correctly labeled by h. Since each additional label agrees with h1(x) = f\n\u2217(x) with probability \u2265 0.7, with high probability the majority of the labels on x will agree with f\u2217(x) at some point, in which case FILTER stops asking for more queries and removes x. As we show in Lemma 4.9 this happens within O(1) queries most of the time. On the other hand, for x that is mislabeled by h, a labeler agrees with h1(x) with probability \u2264 0.3. Clearly, for one set of random labelers \u2014one snapshot of the labels queried by FILTER\u2014 the majority label agrees with h1(x) with a very small probability. As we show in Lemma 4.6, even when considering the progression of all labels queried by FILTER throughout the process, with probability \u0398(1) the majority label never agrees with h1(x). Therefore, x is added to SI with probability \u0398(1).\nSuper-sampling: Another key technique we use in this work is super-sampling. In short, this means that\nas long as we have the correct label of the sampled points and we are in the realizable setting, more samples\nnever hurt the algorithm. Although this seems trivial at first, it does play an important role in our approach. In particular, our probabilistic filtering procedure does not necessarily simulate D2 but a distribution D \u2032, such that \u0398(1)d2(x) \u2264 d\u2032(x) for all x, where d2 and d\u2032 are the densities of D2 and D\u2032, respectively. At a high level, sampling \u0398(m) instances fromD\u2032 simulates a super-sampling process that samples m instances from D2 and then adds in some arbitrary instances. This is formally stated below and is proved in Appendix B.\nLemma 4.2. Given a hypothesis class F consider any two discrete distributions D and D\u2032 such that for all x, d\u2032(x) \u2265 c \u00b7 d(x) for an absolute constant c > 0, and both distributions are labeled according to f\u2217 \u2208 F . There exists a constant c\u2032 > 1 such that for any \u01eb and \u03b4, with probability 1\u2212 \u03b4 over a labeled sample set S of size c\u2032m\u01eb,\u03b4 drawn from D\u2032, OF (S) has error of at most \u01eb with respect to distribution D.\nWith these techniques at hand, we present Algorithm 2. At a high level, the algorithm proceeds in three phases, one for each classifier used by Theorem 4.1. In Phase 1, the algorithm learns h1 such that errD(h1) \u2264 12 \u221a \u01eb. In Phase 2, the algorithm first filters a set of size O(m\u01eb,\u03b4) into the set SI and takes an additional set SC of \u0398(m\u221a\u01eb,\u03b4) samples. Then, it queries O(log( m\u01eb,\u03b4 \u03b4 )) labelers on each instance in SI and SC to get their correct labels with high probability. Next, it partitions these instances to two different sets based on whether or not h1 made a mistake on them. It then learns h2 on a sample set W that is drawn by weighting these two sets equally. As we show in Lemma 4.8, errD2(h2) \u2264 12 \u221a \u01eb. In phase 3, the algorithm learns h3 on a sample set S3 drawn from D|X conditioned on h1 and h2 disagreeing. Finally, the algorithm returns MAJ (h1, h2, h3).\nTheorem 4.3 (\u03b1 = 1 2 + \u0398(1) case). Algorithm 2 uses oracle OF , runs in time poly(d, 1\u01eb , ln(1\u03b4 )) and with probability 1 \u2212 \u03b4 returns f \u2208 F with errD(f) \u2264 \u01eb, using \u039b = O (\u221a \u01eb log ( m\u221a\u01eb,\u03b4 \u03b4 ) + 1 ) cost per labeled example, \u0393 = 0 golden queries, and \u03bb = 1 load. Note that when 1\u221a \u01eb \u2265 log ( m\u221a\u01eb,\u03b4 \u03b4 ) , the above cost per labeled sample is O(1).\nWe start our analysis of Algorithm 2 by stating that CORRECT-LABEL(S, \u03b4) labels S correctly, with probability 1\u2212 \u03b4. This is direct application of the Hoeffding bound and its proof is omitted.\nLemma 4.4. For any unlabeled sample set S, \u03b4 > 0, and S = CORRECT-LABEL(S, \u03b4), with probability 1\u2212 \u03b4, for all (x, y) \u2208 S, y = f\u2217(x).\nNote that as a direct consequence of the above lemma, Phase 1 of Algorithm 2 achieves error of O( \u221a \u01eb).\nLemma 4.5. In Algorithm 2, with probability 1\u2212 \u03b43 , errD(h1) \u2264 12 \u221a \u01eb.\nAlgorithm 2 INTERLEAVING: BOOSTING BY PROBABILISTIC FILTERING FOR \u03b1 = 12 +\u0398(1) Input: Given a distribution D|X , a class of hypotheses F , parameters \u01eb and \u03b4. Phase 1:\nLet S1 = CORRECT-LABEL(S1, \u03b4/6), for a set of sample S1 of size 2m\u221a\u01eb,\u03b4/6 from D|X .\nLet h1 = OF (S1). Phase 2:\nLet SI = FILTER(S2, h1), for a set of samples S2 of size \u0398(m\u01eb,\u03b4) drawn from D|X . Let SC be a sample set of size \u0398(m\u221a\u01eb,\u03b4) drawn from D|X . Let SAll = CORRECT-LABEL(SI \u222a SC , \u03b4/6). LetWI = {(x, y) \u2208 SAll | y 6= h1(x)} and LetWC = SAll \\WI . Draw a sample set W of size \u0398(m\u221a\u01eb,\u03b4) from a distribution that equally weights WI and WC .\nLet h2 = OF (W ). Phase 3:\nLet S3 = CORRECT-LABEL(S3, \u03b4/6), for a sample set S3 of size 2m\u221a\u01eb,\u03b4/6 drawn fromD|X conditioned on h1(x) 6= h2(x). Let h3 = OF (S3).\nreturn Maj(h1, h2, h3).\nCORRECT-LABEL(S, \u03b4): for x \u2208 S do Let L \u223c P k for a set of k = O(log( |S|\u03b4 )) labelers drawn from P and S \u2190 S \u222a {(x,MajL(x))}. end\nreturn S.\nNext, we prove that FILTER removes instances that are correctly labeled by h1 with good probability and retains instances that are mislabeled by h1 with at least a constant probability.\nLemma 4.6. Given any sample set S and classifier h, for every x \u2208 S 1. If h(x) = f\u2217(x), then x \u2208 FILTER(S, h) with probability < \u221a\u01eb. 2. If h(x) 6= f\u2217(x), then x \u2208 FILTER(S, h) with probability \u2265 0.5.\nProof. For the first claim, note that x \u2208 SI only if Maj(y1:t) 6= h(x) for all t \u2264 N . Consider t = N time step. Since each random query agrees with f\u2217(x) = h(x) with probability \u2265 0.7 independently, majority of N = O(log(1/ \u221a \u01eb)) labels are correct with probability at least 1 \u2212 \u221a\u01eb. Therefore, the probability that the majority label disagrees with h(x) = f\u2217(x) at every time step is at most \u221a \u01eb.\nIn the second claim, we are interested in the probability that there exists some t \u2264 N , for which Maj(y1:t) = h(x) 6= f\u2217(x). This is the same as the probability of return in biased random walks, also called the probability of ruin in gambling [Feller, 2008], where we are given a random walk that takes a step to the right with probability \u2265 0.7 and takes a step to the left with the remaining probability and we are interested in the probability that this walk ever crosses the origin to the left while taking N or even infinitely many steps. Using the probability of return for biased random walks (see Theorem E.1), the probability that Maj(y1:t) 6= f\u2217(x) ever is at most ( 1\u2212 ( 0.7 1\u22120.7 )N ) / ( 1\u2212 ( 0.7 1\u22120.7 )N+1 ) < 37 . Therefore, for each x such that h(x) 6= f\u2217(x), x \u2208 SI with probability at least 4/7.\nIn the remainder of the proof, for ease of exposition we assume that not only errD(h1) \u2264 12 \u221a \u01eb as per\nLemma 4.5, but in fact errD(h1) = 1 2\n\u221a \u01eb. This assumption is not needed for the correctness of the results\nbut it helps simplify the notation and analysis. As a direct consequence of Lemma 4.6 and application of the\nChernoff bound, we deduce that with high probability W I , WC , and SI all have size \u0398(m\u221a\u01eb,\u03b4). The next lemma, whose proof appears in Appendix C, formalizes this claim.\nLemma 4.7. With probability 1\u2212 exp(\u2212\u2126(m\u221a\u01eb,\u03b4)), W I , WC , and SI all have size \u0398(m\u221a\u01eb,\u03b4).\nThe next lemma combines the probabilistic filtering and super-sampling techniques to show that h2 has the desired error O( \u221a \u01eb) on D2.\nLemma 4.8. Let DC and DI denote distribution D when it is conditioned on {x | h1(x) = f\u2217(x)} and {x | h1(x) 6= f\u2217(x)}, respectively, and letD2 = 12DI+ 12DC . With probability 1\u22122\u03b4/3, errD2(h2) \u2264 12 \u221a \u01eb.\nProof. Consider distribution D\u2032 that has equal probability on the distributions induced by WI and WC and let d\u2032(x) denote the density of point x in this distribution. Relying on our super-sampling technique (see Lemma 4.2), it is sufficient to show that for any x, d\u2032(x) = \u0398(d2(x)).\nFor ease of presentation, we assume that Lemma 4.5 holds with equality, i.e., errD(h1) is exactly 1 2\n\u221a \u01eb\nwith probability 1\u2212 \u03b4/3. Let d(x), d2(x), dC(x), and dI(x) be the density of instance x in distributions D, D2, DC , and DI , respectively. Note that, for any x such that h1(x) = f\n\u2217(x), we have d(x) = dC(x)(1 \u2212 1 2 \u221a \u01eb). Similarly, for any x such that h1(x) 6= f\u2217(x), we have d(x) = dI(x)12 \u221a \u01eb. LetNC(x),NI(x),MC(x) andMI(x) be the number of occurrences of x in the sets SC , SI ,WC andWI , respectively. For any x, there are two cases:\nIf h1(x) = f \u2217(x): Then, there exist absolute constants c1 and c2 according to Lemma 4.7, such that\nd\u2032(x) = 1\n2 E\n[\nMC(x)\n|WC |\n]\n\u2265 E[MC(x)] c1 \u00b7m\u221a\u01eb,\u03b4 \u2265 E[NC(x)] c1 \u00b7m\u221a\u01eb,\u03b4 = |SC | \u00b7 d(x) c1 \u00b7m\u221a\u01eb,\u03b4\n= |SC | \u00b7 dC(x) \u00b7 (1\u2212 12\n\u221a \u01eb)\nc1 \u00b7m\u221a\u01eb,\u03b4 \u2265 c2dC(x) =\nc2d2(x)\n2 ,\nwhere the second and sixth transitions are by the sizes ofWC and |SC | and the third transition is by the fact that if h(x) = f\u2217(x), MC(x) > NC(x). If h1(x) 6= f\u2217(x): Then, there exist absolute constants c\u20321 and c\u20322 according to Lemma 4.7, such that\nd\u2032(x) = 1\n2 E\n[\nMI(x)\n|WI |\n]\n\u2265 E[MI(x)] c\u20321 \u00b7m\u221a\u01eb,\u03b4 \u2265 E[NI(x)] c\u20321 \u00b7m\u221a\u01eb,\u03b4 \u2265 4 7 d(x)|S2| c\u20321 \u00b7m\u221a\u01eb,\u03b4\n= 4 7 dI(x) 1 2\n\u221a \u01eb \u00b7 |S2|\nc\u20321 \u00b7m\u221a\u01eb,\u03b4 \u2265 c\u20322dI(x) = c\u20322d2(x) 2 ,\nwhere the second and sixth transitions are by the sizes of WI and |S2|, the third transition is by the fact that if h(x) 6= f\u2217(x), MI(x) > NI(x), and the fourth transition holds by part 2 of Lemma 4.6.\nUsing the super-sampling guarantees of Lemma 4.2, with probability 1\u22122\u03b4/3, errD2(h2) \u2264 \u221a \u01eb/2.\nThe next claim shows that the probabilistic filtering step queries a few labels only. At a high level, this is achieved by showing that any instance x for which h1(x) = f \u2217(x) contributes only O(1) queries, with high probability. On the other hand, instances that h1 mislabeled may each get log( 1 \u01eb ) queries. But, because there are only few such points, the total number of queries these instances require is a lower order term. Lemma 4.9. Let S be a sample set drawn from distribution D and let h be such that errD(h) \u2264 \u221a \u01eb. With\nprobability 1\u2212 exp(\u2212\u2126(|S|\u221a\u01eb)), FILTER(S, h) makes O(|S|) label queries.\nProof. Using Chernoff bound, with probability 1 \u2212 exp (\u2212|S|\u221a\u01eb) the total number of points in S where h disagrees with f\u2217 isO(|S|\u221a\u01eb). The number of queries spent on these points is at mostO (|S|\u221a\u01eb log(1/\u01eb)) \u2264 O(|S|).\nNext, we show that for each x such that h(x) = f\u2217(x), the number of queries taken until a majority of them agree with h(x) is a constant. Let us first show that this is the case in expectation. Let Ni be the expected number of labels queried until we have i more correct labels than incorrect ones. Then N1 \u2264 0.7(1) + 0.3(N2 + 1), since with probability at least \u03b1 \u2265 0.7, we receive one more correct label and stop, and with probability \u2264 0.3 we get a wrong label in which case we have to get two more correct labels in future. Moreover, N2 = 2N1, since we have to get one more correct label to move from N2 to N1 and then one more. Solving these, we have that N1 \u2264 2.5. Therefore, the expected total number of queries is at most O(|S|). Next, we show that this random variable is also well-concentrated. Let Lx be a random variable that indicates the total number of queries on x before we have one more correct label than incorrect labels. Note that Lx is an unbounded random variable, therefore concentration bounds such as Hoeffding or Chernoff do not work here. Instead, to show that Lx is well-concentrated, we prove that the Bernstein inequality (see Theorem E.2) holds. That is, as we show in Appendix D, for any x, the Bernstein inequality is statisfied by the fact that for any i > 1, E[(Lx \u2212 E[Lx])i] \u2264 50(i + 1)! e4i. Therefore, over all instances in S, \u2211\nx\u2208S Lx \u2208 O(|S|) with probability 1\u2212 exp(\u2212|S|).\nFinally, we have all of the ingredients needed for proving our main theorem.\nProof of Theorem 4.3. We first discuss the number of label queries Algorithm 2 makes. The total number of labels queried by Phases 1 and 3 is attributed to the labels queried by CORRECT-LABEL(S1, \u03b4) and CORRECT-LABEL(S3, \u03b4/6), which is O ( m\u221a\u01eb,\u03b4 log(m\u221a\u01eb,\u03b4/\u03b4) )\n. By Lemma 4.7, |SI \u222a SC | \u2264 O(m\u221a\u01eb,\u03b4) almost surely. Therefore, CORRECT-LABEL(SI \u222a SC , \u03b4/6) contributes O ( m\u221a\u01eb,\u03b4 log(m\u221a\u01eb,\u03b4/\u03b4) ) labels. Moreover, as we showed in Lemma 4.9, FILTER(S2, h1) queries O(m\u01eb,\u03b4) labels, almost surely. So, the total number of labels queried by Algorithm 2 is at most O ( m\u221a\u01eb,\u03b4 log ( m\u221a\u01eb,\u03b4 \u03b4 ) +m\u01eb,\u03b4 ) . This leads to \u039b = O (\u221a \u01eb log (\nm\u221a\u01eb,\u03b4 \u03b4 ) + 1 ) cost per labeled example.\nIt remains to show that MAJ (h1, h2, h3) has error \u2264 \u01eb on D. Since CORRECT-LABEL(S1, \u03b4/6) and CORRECT-LABEL(S3, \u03b4/6) return correctly labeled sets , errD(h1) \u2264 12 \u221a \u01eb and errD3(h3) \u2264 12 \u221a \u01eb, where\nD3 is distribution D conditioned on {x | h1(x) 6= h2(x)}. As we showed in Lemma 4.8, errD2(h2) \u2264 12 \u221a \u01eb with probability 1 \u2212 2\u03b4/3. Using the boosting technique of Schapire [1990] described in Theorem 4.1, we conclude that MAJ (h1, h2, h3) has error \u2264 \u01eb on D."}, {"heading": "4.1 The General Case of Any \u03b1", "text": "In this section, we extend Algorithm 2 to handle any value of \u03b1, that does not necessarily satisfy \u03b1 > 1 2 + \u0398(1). We show that by using O( 1 \u03b1) golden queries, it is possible to efficiently learn any function class with a small overhead.\nThere are two key challenges that one needs to overcome when \u03b1 < 12 + o(1). First, we can no longer assume that by taking the majority vote over a few random labelers we get the correct label of an instance. Therefore, CORRECT-LABEL(S, \u03b4) may return a highly noisy labeled sample set. This is problematic, since efficiently learning h1, h2, and h3 using oracle OF crucially depends on the correctness of the input labeled set. Second, FILTER(S, h1) no longer \u201cfilters\u201d the instances correctly based on the classification error of h1. In particular, FILTER may retain a constant fraction of instances where h1 is in fact correct, and it may throw out instances where h1 was incorrect with high probability. Therefore, the per-instance guarantees of Lemma 4.6 fall apart, immediately.\nWe overcome both of these challenges by using two key ideas outlined below.\nPruning: As we alluded to in Section 3, instances where only a small majority of labelers are in agreement\nare great for identifying and pruning away a noticeable fraction of the bad labelers. We call these instances good test cases. In particular, if we ever encounter a good test case x, we can ask a golden query y = f\u2217(x) and from then on only consider the labelers who got this test correctly, i.e., P \u2190 P|{(x,y)}. Note that if we make our golden queries when Maj-sizeP (x) \u2264 1 \u2212 \u03b12 , at least an \u03b12 fraction of the labelers would be pruned. This can be repeated at mostO( 1\u03b1 ) times before the number of good labelers form a strong majority, in which case Algorithm 2 succeeds. The natural question is how would we measure Maj-sizeP (x) using few label queries? Interestingly, CORRECT-LABEL(S, \u03b4) can be modified to detect such good test cases by measuring the empirical agreement rate on a set L of O( 1 \u03b12\nlog( |S|\u03b4 )) labelers. This is shown in procedure PRUNE-AND-LABEL as part Algorithm 3. That is, if Maj-sizeL(x) > 1\u2212 \u03b1/4, we take MajL(x) to be the label, otherwise we test and prune the labelers, and then restart the procedure. This ensures that whenever\nwe use a sample set that is labeled by PRUNE-AND-LABEL, we can be certain of the correctness of the\nlabels. This is stated in the following lemma, and proved in Appendix F.1.\nLemma 4.10. For any unlabeled sample set S, \u03b4 > 0, with probability 1\u2212\u03b4, either PRUNE-AND-LABEL(S, \u03b4) prunes the set of labelers or S = PRUNE-AND-LABEL(S, \u03b4) is such that for all (x, y) \u2208 S, y = f\u2217(x).\nAs an immediate result, the first phase of Algorithm 3 succeeds in computing h1, such that errD(h1) \u2264 1 2 \u221a \u01eb. Moreover, every time PRUNE-AND-LABEL prunes the set of labelers, the total fraction of good labeler among all remaining labelers increase. As we show, after O(1/\u03b1) prunings, the set of good labelers is guaranteed to form a large majority, in which case Algorithm 2 for the case of \u03b1 = 12 + \u0398(1) can be used. This is stated in the next lemma and proved in Appendix F.2.\nLemma 4.11. For any \u03b4, with probability 1\u2212 \u03b4, the total number of times that Algorithm 3 is restarted as a result of pruning is O( 1\u03b1).\nRobust Super-sampling: The filtering step faces a completely different challenge: Any point that is a good\ntest case can be filtered the wrong way. However, instances where still a strong majority of the labelers\nagree are not affected by this problem and will be filtered correctly. Therefore, as a first step we ensure that\nthe total number of good test cases that were not caught before FILTER starts is small. For this purpose, we start the algorithm by calling CORRECT-LABEL on a sample of size O(1\u01eb log( 1 \u03b4 )), and if no test points were found in this set, then with high probability the total fraction of good test cases in the underlying distribution is at most \u01eb2 . Since the fraction of good test cases is very small, one can show that except for\nan \u221a \u01eb fraction, the noisy distribution constructed by the filtering process will, for the purposes of boosting,\nsatisfy the conditions needed for the super-sampling technique. Here, we introduce a robust version of the super-sampling technique to argue that the filtering step will indeed produce h2 of error O( \u221a \u01eb).\nLemma 4.12 (Robust Super-Sampling Lemma). Given a hypothesis class F consider any two discrete distributions D and D\u2032 such that except for an \u01eb fraction of the mass under D, we have that for all x, d\u2032(x) \u2265 c \u00b7 d(x) for an absolute constant c > 0 and both distributions are labeled according to f\u2217 \u2208 F . There exists a constant c\u2032 > 1 such that for any \u01eb and \u03b4, with probability 1\u2212 \u03b4 over a labeled sample set S of size c\u2032m\u01eb,\u03b4 drawn from D\u2032, OF (S) has error of at most 2\u01eb with respect toD.\nBy combining these techniques at every execution of our algorithm we ensure that if a good test case\nis ever detected we prune a small fraction of the bad labelers and restart the algorithm, and if it is never detected, our algorithm returns a classifier of error \u01eb.\nTheorem 4.13 (Any \u03b1). Suppose the fraction of the perfect labelers is \u03b1 and let \u03b4\u2032 = c\u03b1\u03b4 for small enough constant c > 0. Algorithm 3 uses oracle OF , runs in time poly(d, 1\u03b1 , 1\u01eb , ln(1\u03b4 )), uses a training set of size\nAlgorithm 3 BOOSTING BY PROBABILISTIC FILTERING FOR ANY \u03b1 Input: Given a distribution D|X and P , a class of hypothesis F , parameters \u01eb, \u03b4, and \u03b1. Phase 0:\nIf \u03b1 > 34 , run Algorithm 2 and quit. Let \u03b4\u2032 = c\u03b1\u03b4 for small enough c > 0 and draw S0 of O(1\u01eb log( 1 \u03b4\u2032 )) examples from the distribution D.\nPRUNE-AND-LABEL(S0, \u03b4 \u2032).\nPhase 1:\nLet S1 = PRUNE-AND-LABEL(S1, \u03b4 \u2032), for a set of sample S1 of size 2m\u221a\u01eb,\u03b4\u2032 from D.\nLet h1 = OF (S1). Phase 2:\nLet SI = FILTER(S2, h1), for a set of samples S2 of size \u0398(m\u01eb,\u03b4\u2032) drawn from D. Let SC be a sample set of size \u0398(m\u221a\u01eb,\u03b4\u2032) drawn from D. Let SAll = PRUNE-AND-LABEL(SI \u222a SC , \u03b4\u2032). Let WI = {(x, y) \u2208 SAll | y 6= h1(x)} and LetWC = SAll \\WI . Draw a sample set W of size \u0398(m\u221a\u01eb,\u03b4\u2032) from a distribution that equally weights WI and WC .\nLet h2 = OF (W ). Phase 3:\nLet S3 = PRUNE-AND-LABEL(S3, \u03b4 \u2032), for a sample set S3 of size 2m\u221a\u01eb,\u03b4\u2032 drawn from D conditioned\non h1(x) 6= h2(x). Let h3 = OF (S3). return Maj(h1, h2, h3).\nPRUNE-AND-LABEL(S, \u03b4): for x \u2208 S do Let L \u223c P k for a set of k = O( 1\u03b12 log( |S| \u03b4 )) labelers drawn from P .\nif Maj-sizeL(x) \u2264 1\u2212 \u03b14 then Get a golden query y\u2217 = f\u2217(x), Restart Algorithm 3 with distribution P \u2190 P|{(x,y\u2217)} and \u03b1 \u2190 \u03b11\u2212\u03b1\n8\n.\nelse\nS \u2190 S \u222a {(x,MajL(x))}. end\nend\nreturn S.\nO( 1\u03b1m\u01eb,\u03b4\u2032) size and with probability 1 \u2212 \u03b4 returns f \u2208 F with errD(f) \u2264 \u01eb using O( 1\u03b1) golden queries, load of 1\u03b1 per labeler, and a total number of queries\nO\n(\n1 \u03b1 m\u01eb,\u03b4\u2032 + 1 \u03b1\u01eb log( 1 \u03b4\u2032 ) log( 1 \u01eb\u03b4\u2032 ) + 1 \u03b13 m\u221a\u01eb,\u03b4\u2032 log(\nm\u221a\u01eb,\u03b4\u2032\n\u03b4\u2032 )\n)\n.\nNote that when 1 \u03b12 \u221a \u01eb \u2265 log\n( m\u221a\u01eb,\u03b4 \u03b1\u03b4 ) and log( 1\u03b1\u03b4 ) < d, the cost per labeled query is O( 1 \u03b1 ).\nProof Sketch. Let B = {x | Maj-sizeP (x) \u2264 1 \u2212 \u03b1/2} be the set of good test cases and let \u03b2 = D[B] be the total density on such points. Note that if \u03b2 > \u01eb4 , with high probability S0 includes one such point, in which case PRUNE-AND-LABEL identifies it and prunes the set of labelers. Therefore, we can assume that \u03b2 \u2264 \u01eb4 . By Lemma 4.10, it is easy to see that Phase 1 and Phase 3 of Algorithm 3 succeed in producing h1 and\nh3 such that errD(h1) \u2264 12 \u221a \u01eb and errD3(h3) \u2264 12 \u221a \u01eb. It remains to show that Phase 2 of Algorithm 3 also\nproduces h2 such that errD2(h2) \u2264 12 \u221a \u01eb.\nConsider the filtering step of Phase 2. First note that for any x /\u2208 B, the per-point guarantees of FILTER expressed in Lemma 4.6 still hold. Let D\u2032 be the distribution that has equal probability on the distributions induced by WI and WC , and is used for simulating D2. Similarly as in Lemma 4.8 one can show that for any x 6\u2208 B, d\u2032(x) = \u0398(d2(x)). Since D[B] \u2264 \u01eb4 , we have that D2[B] \u2264 14 \u221a \u01eb. Therefore, D\u2032 andD2 satisfy the conditions of the robust super-sampling lemma (Lemma 4.12) where the fraction of bad points is at most\u221a \u01eb 4 . Hence, we can argue that errD2(h2) \u2264 \u221a \u01eb 2 .\nThe remainder of the proof follows by using the boosting technique of Schapire [1990] described in\nTheorem 4.1."}, {"heading": "5 No Perfect Labelers", "text": "In this section, we consider a scenario where our pool of labelers does not include any perfect labelers. Unfortunately, learning f\u2217 in this setting reduces to the notoriously difficult agnostic learning problem. A related task is to find a set of the labelers which are good but not perfect. In this section, we show how to\nidentify the set of all good labelers, when at least the majority of the labelers are good.\nWe consider a setting where the fraction of the perfect labelers, \u03b1, is arbitrarily small or 0. We further assume that at least half of the labelers are good, while others have considerably worst performance. More formally, we are given a set of labelers g1, . . . , gn and a distribution D with an unknown target classifier f\u2217 \u2208 F . We assume that more than half of these labelers are \u201cgood\u201d, that is they have error of \u2264 \u01eb on distribution D. On the other hand, the remaining labelers, which we call \u201cbad\u201d, have error rates \u2265 4\u01eb on distribution D. We are interested in identifying all of the good labelers with high probability by querying the labelers on an unlabeled sample set drawn from D|X .\nThis model presents an interesting community structure: Two good labelers agree on at least 1 \u2212 2\u01eb fraction of the data, while a bad and a good labeler agree on at most 1 \u2212 3\u01eb of the data. Note that the rate of agreement between two bad labelers can be arbitrary. This is due to the fact that there can be multiple\nbad labelers with the same classification function, in which case they completely agree with each other, or two bad labelers who disagree on the classification of every instance. This structure serves as the basis of\nAlgorithm 4 and its analysis. Here we provide an overview of its working and analysis.\nAlgorithm 4 GOOD LABELER DETECTION\nInput: Given n labelers, parameters \u01eb and \u03b4 Let G = ([n], \u2205) be a graph on n vertices with no edges. Take set Q of 16 ln(2)n random pairs of nodes from G.\n1 for (i, j) \u2208 Q do if DISAGREE(i, j) < 2.5\u01eb then add edge (i, j) to G;\nend\n2 Let C be the set of connected components of G each with \u2265 n/4 nodes. 3 for i \u2208 [n] \\ ( \u22c3\nC\u2208C C ) and C \u2208 C do Take one node j \u2208 C , if DISAGREE(i, j) < 2.5\u01eb add edge (i, j) to G.\nend return The largest connected component of G\nDISAGREE(i, j): Take set S of \u0398(1\u01eb ln( n \u03b4 )) samples from D.\nreturn 1|S| \u2211 x\u2208S 1(gi(x)6=gj(x)).\nTheorem 5.1 (Informal). Suppose that any good labeler i is such that errD(gi) \u2264 \u01eb. Furthermore, assume that errD(gj) 6\u2208 (\u01eb, 4\u01eb) for any j \u2208 [n]. And let the number of good labelers be at least \u230an2 \u230b + 1. Then, Algorithm 4, returns the set of all good labeler with probability 1 \u2212 \u03b4, using an expected load of \u03bb = O (\n1 \u01eb ln\n(\nn \u03b4\n))\nper labeler.\nWe view the labelers as nodes in a graph that has no edges at the start of the algorithm. In step 1, the algorithm takesO(n) random pairs of labelers and estimates their level of disagreement by querying them on an unlabeled sample set of size O (\n1 \u01eb ln\n(\nn \u03b4\n))\nand measuring their empirical disagreement. By an application\nof Chernoff bound, we know that with probability 1\u2212 \u03b4, for any i, j \u2208 [n], \u2223\n\u2223 \u2223 \u2223 DISAGREE(i, j) \u2212 Pr x\u223cD|X\n[gi(x) 6= gj(x)] \u2223 \u2223 \u2223\n\u2223\n< \u01eb\n2 .\nTherefore, for any pair of good labelers i and j tested by the algorithm, DISAGREE(i, j) < 2.5\u01eb, and for any pair of labelers i and j that one is good and the other is bad, DISAGREE(i, j) \u2265 2.5\u01eb. Therefore, the connected components of such a graph only include labelers from a single community.\nNext, we show that at step 2 of Algorithm 4 with probability 1 \u2212 \u03b4 there exists at least one connected component of size n/4 of good labelers.\nTo see this we first prove that for any two good labelers i and j, the probability of (i, j) existing is at least \u0398(1/n). Let Vg be the set of nodes corresponding to good labelers. For i, j \u2208 Vg, we have\nPr[(i, j) \u2208 G] = 1\u2212 ( 1\u2212 1 n2\n)4 ln(2)n\n\u2248 4 ln(2) n \u2265 2 ln(2)|Vg| .\nBy the properties of random graphs, with very high probability there is a component of size \u03b2|Vg| in a random graph whose edges exists with probability c/|Vg|, for \u03b2+e\u2212\u03b2c = 1 [Janson et al., 2011]. Therefore, with probability 1\u2212 \u03b4, there is a component of size |Vg|/2 > n/4 over the vertices in Vg.\nFinally, at step 3 the algorithm considers smaller connected components and tests whether they join\nany of the bigger components, by measuring the disagreement of two arbitrary labelers from these components.,At this point, all good labelers form one single connected component of size > n2 . So, the algorithm succeeds in identifying all good labelers.\nNext, we briefly discuss the expected load per labeler in Algorithm 4. Each labeler participates in O(1) pairs of disagreement tests in expectation, each requiring O(1\u01eb ln(n/\u03b4)) queries. So, in expectation each labeler labels O(1\u01eb ln(n/\u03b4)) instances."}, {"heading": "A Additional Related Work", "text": "More generally, interactive models of learning have been studied in the machine learning community. The\nmost popular among them is the area of active learning [Cohn et al., 1994, Dasgupta, 2005, Balcan et al.,\n2006, Koltchinskii, 2010, Hanneke, 2011]. In this model, the learning algorithm can adaptively query for\nthe labels of a few examples in the training set and use them to produce an accurate hypothesis. The goal\nis to use as few label queries as possible. The number of labeled queries used is called the label complexity\nof the algorithm. It is known that certain hypothesis classes can be learned in this model using much fewer\nlabeled queries than predicted by the VC theory. In particular, in many instances the label complexity scales only logarithmically in 1\u01eb as opposed to linearly in 1 \u01eb . However, to achieve computational efficiency, the algorithms in this model rely on the fact that one can get perfect labels for every example queried. This would be hard to achieve in our model since in the worst case it would lead to each labeler answering log(d\u01eb ) many queries. In contrast, we want to keep the query load of a labeler to a constant and hence the techniques\ndeveloped for active learning are insufficient for our purposes. Furthermore, in noisy settings most work\non efficient active learning algorithms assumes the existence of an empirical risk minimizer (ERM) oracle\nthat can minimize training error even when the instances aren\u2019t labeled according to the target classifier.\nHowever, in most cases such an ERM oracle is hard to implement and the improvements obtained in the\nlabel complexity are less drastic in such noisy scenarios.\nAnother line of work initiated by Zhang and Chaudhuri [2015] models related notions of weak and\nstrong labelers in the context of active learning. The authors study scenarios where the label queries to the\nstrong labeler can be reduced by querying the weak and potentially noisy labelers more often. However, as\ndiscussed above, the model does not yield relevant algorithms for our setting as in the worst case one might end up querying for d\u01eb high quality labels leading to a prohibitively large load per labeler in our setting. The work of Yan et al. [2016] studies a model of active learning where the labeler abstains from providing a label\nprediction more often on instances that are closer to the decision boundary. The authors then show how to\nuse the abstentions in order to approximate the decision boundary. Our setting is inherently different, since\nwe make no assumptions on the bad labelers."}, {"heading": "B Proof of Lemma 4.2", "text": "First, notice that because D and D\u2032 are both labeled according to f\u2217 \u2208 F , for any f \u2208 F we have,\nerrD\u2032(f) = \u2211\nx\nd\u2032(x)1f(x)6=f\u2217(x) \u2265 \u2211\nx\nc \u00b7 d(x)1f(x)6=f\u2217(x) = c \u00b7 errD(f).\nTherefore, if errD\u2032(f) \u2264 c\u01eb, then errD(f) \u2264 \u01eb. Let m\u2032 = mc\u01eb,\u03b4, we have\n\u03b4 > Pr S\u2032\u223cD\u2032m\u2032\n[\u2203f \u2208 F , s.t. errS\u2032(f) = 0 \u2227 errD\u2032(f) \u2265 c\u01eb]\n\u2265 Pr S\u2032\u223cD\u2032m\u2032 [\u2203f \u2208 F , s.t. errS\u2032(f) = 0 \u2227 errD(f) \u2265 \u01eb].\nThe claim follows by the fact that mc\u01eb,\u03b4 = O ( 1 cm\u01eb,\u03b4 ) ."}, {"heading": "C Proof of Lemma 4.7", "text": "Let us first consider the expected size of sets SI ,W I , and WC . Using Lemma 4.6, we have\nO(m\u221a\u01eb,\u03b4) \u2265 1\n2\n\u221a \u01eb|S2|+ \u221a \u01eb|S2| \u2265 E[|SI |] \u2265 1\n2\n(\n1\n2\n\u221a \u01eb )\n|S2| \u2265 \u2126(m\u221a\u01eb,\u03b4).\nSimilarly,\nO(m\u221a\u01eb,\u03b4) \u2265 E[SI ] + |SC | \u2265 E[W I ] \u2265 1\n2\n(\n1\n2\n\u221a \u01eb )\n|S2| \u2265 \u2126(m\u221a\u01eb,\u03b4).\nSimilarly,\nO(m\u221a\u01eb,\u03b4) \u2265 E[SI ] + |SC | \u2265 E[WC ] \u2265 ( 1\u2212 1 2 \u221a \u01eb ) |SC | \u2265 \u2126(m\u221a\u01eb,\u03b4).\nThe claim follows by the Chernoff bound."}, {"heading": "D Remainder of the Proof of Lemma 4.9", "text": "We prove that the Bernstein inequality holds for the total number of queries y1, y2, . . . , made before their majority agrees with f\u2217(x). Let Lx be the random variable denoting the number of queries the algorithm makes on instance x for which h(x) = f\u2217(x). Consider the probability that Lx = 2k+1 for some k. That is, Maj(y1:t) = f\n\u2217(x) for the first time when t = 2k + 1. This is at most the probability that Maj(y1:2k\u22121) 6= f\u2217(x). By Chernoff bound, we have that\nPr[Lx = 2k + 1] \u2264 Pr[Maj(y1:2k\u22121) 6= f\u2217(x)] \u2264 exp ( \u22120.7(2k \u2212 1)(2 7 )2/2 )\n\u2264 exp (\u22120.02(2k \u2212 1)) .\nFor each i > 1, we have\nE[(Lx \u2212 E[Lx])i] \u2264 \u221e \u2211\nk=0\nPr[Lx = 2k + 1](2k + 1\u2212 E[Lx])i\n\u2264 \u221e \u2211\nk=0\ne\u22120.02(2k\u22121)(2k + 1)i\n\u2264 e0.04 \u221e \u2211\nk=0\ne\u22120.02(2k+1)(2k + 1)i\n\u2264 e0.04 \u221e \u2211\nk=0\ne\u22120.02kki\n\u2264 50(i + 1)! e4i+0.04,\nwhere the last inequality is done by integration. This satisfies the Bernstein condition stated in Theorem E.2.\nTherefore,\nPr\n[\n\u2211 x\u2208S Lx \u2212 |S|E[Lx] \u2265 O(|S|)]\n]\n\u2264 exp (\u2212|S|) .\nTherefore, the total number of queries over all points in x \u2208 S where h(x) = f\u2217(x) is at most O(|S|) with very high probability."}, {"heading": "E Probability Lemmas", "text": "Theorem E.1 (Probability of Ruin [Feller, 2008]). Consider a player who starts with i dollars against an adversary that has N dollars. The player bets one dollar in each gamble, which he wins with probability p.\nThe probability that the player ends up with no money at any point in the game is\n1\u2212 (\np 1\u2212p\n)N\n1\u2212 (\np 1\u2212p\n)N+i .\nTheorem E.2 (Bernstein Inequality). LetX1, . . . ,Xn be independent random variables with expectation \u00b5. Supposed that for some positive real number L and every k > 1,\nE[(Xi \u2212 \u00b5)k] \u2264 1 2 E[(Xi \u2212 \u00b5)2]Lk\u22122k!.\nThen,\nPr\n\n\nn \u2211\ni=1\nXi \u2212 n\u00b5 \u2265 2t\n\u221a \u221a \u221a \u221a n \u2211\ni=1\nE[(Xi \u2212 \u00b5)2]\n\n < exp(\u2212t2), for 0 < t \u2264 1 2L \u221a E[(Xi \u2212 \u00b5)2]."}, {"heading": "F Omitted Proofs from Section 4.1", "text": "In this section, we prove Theorem 4.13 and present the proofs that were omitted from Section 4.1. Theorem 4.13 (restated) Suppose the fraction of the perfect labelers is \u03b1 and let \u03b4\u2032 = \u0398(\u03b1\u03b4). Algorithm 3 uses oracle OF , runs in time poly(d, 1\u03b1 , 1\u01eb , ln(1\u03b4 )), uses a training set of size O( 1\u03b1m\u01eb,\u03b4\u2032) size and with probability 1\u2212 \u03b4 returns f \u2208 F with errD(f) \u2264 \u01eb using O( 1\u03b1 ) golden queries, load of 1\u03b1 per labeler, and a total number of queries\nO\n(\n1 \u03b1 m\u01eb,\u03b4\u2032 + 1 \u03b1\u01eb log( 1 \u03b4\u2032 ) log( 1 \u01eb\u03b4\u2032 ) + 1 \u03b13 m\u221a\u01eb,\u03b4\u2032 log(\nm\u221a\u01eb,\u03b4\u2032\n\u03b4\u2032 )\n)\n.\nNote that when 1 \u03b12 \u221a \u01eb \u2265 log\n( m\u221a\u01eb,\u03b4 \u03b1\u03b4 ) and log( 1\u03b1\u03b4 ) < d, the cost per labeled query is O( 1 \u03b1 ).\nF.1 Proof of Lemma 4.10\nBy Chernoff bound, with probability \u2265 1\u2212 \u03b4, for every x \u2208 S we have that |Maj-sizeP (x)\u2212Maj-sizeL(x)| \u2264 \u03b1\n8 ,\nwhereL is the set of labelers PRUNE-AND-LABEL(S, \u03b4) queries on x. Hence, if x is such thatMaj-sizeP (x) \u2264 1\u2212 \u03b12 , then it will be identified and the set of labelers is pruned. Otherwise, MajL(x) agrees with the good labelers and x gets labeled correctly according to the target function.\nF.2 Proof of Lemma 4.11\nRecall that \u03b4\u2032 = c \u00b7 \u03b1\u03b4 for some small enough constant c > 0. Each time PRUNE-AND-LABEL(S, \u03b4\u2032) is called, by Hoeffding bound, it is guaranteed that with probability \u2265 1\u2212 \u03b4\u2032, for each x \u2208 S,\n|Maj-sizeP (x)\u2212Maj-sizeL(x)| \u2264 \u03b1\n8 ,\nwhere L is the set of labelers PRUNE-AND-LABEL(S, \u03b4\u2032) queries on x. Hence, when we issue a golden query for x such that Maj-sizeL(x) \u2264 1 \u2212 \u03b14 and prune away bad labelers, we are guaranteed to remove at least an \u03b18 fraction of the labelers. Furthermore, no good labeler is ever removed. Hence, the fraction of good labelers increases from \u03b1 to \u03b1/(1 \u2212 \u03b18 ). So, in O( 1\u03b1) calls, the fraction of the good labelers surpasses 3 4 and we switch to using Algorithm 2. Therefore, with probability 1\u2212 \u03b4 overall, the total number of golden queries is O(1/\u03b1).\nF.3 Proof of Lemma 4.12\nLet B be the set of points that do not satisfy the condition that d\u2032(x) \u2265 c \u00b7 d(x). Notice that because D and D\u2032 are both labeled according to f\u2217 \u2208 F , for any f \u2208 F we have,\nerrD\u2032(f) = \u2211 x\u2208B d\u2032(x)1f(x)6=f\u2217(x) + \u2211 x/\u2208B d\u2032(x)1f(x)6=f\u2217(x) \u2265 \u2211 x/\u2208B c \u00b7 d(x)1f(x)6=f\u2217(x) \u2265 c \u00b7 (errD(f)\u2212 \u01eb).\nTherefore, if errD\u2032(f) \u2264 c\u01eb, then errD(f) \u2264 2\u01eb. Let m\u2032 = mc\u01eb,\u03b4, we have\n\u03b4 > Pr S\u2032\u223cD\u2032m\u2032\n[\u2203f \u2208 F , s.t. errS\u2032(f) = 0 \u2227 errD\u2032(f) \u2265 c\u01eb]\n\u2265 Pr S\u2032\u223cD\u2032m\u2032 [\u2203f \u2208 F , s.t. errS\u2032(f) = 0 \u2227 errD(f) \u2265 2\u01eb].\nThe claim follows by the fact that mc\u01eb,\u03b4 = O ( 1 cm\u01eb,\u03b4 ) .\nF.4 Proof of Theorem 4.13\nRecall that \u03b4\u2032 = c \u00b7 \u03b1\u03b4 for a small enough constant c > 0. Let B = {x | Maj-sizeP (x) \u2264 1 \u2212 \u03b1/2} be the set of good test cases and and let \u03b2 = D[B] be the total density on such points. Note that if \u03b2 > \u01eb4 , with high probability S0 includes one such point, in which case PRUNE-AND-LABEL identifies it and prunes the set of labelers. Therefore, we can assume that \u03b2 \u2264 \u01eb4 . By Lemma 4.10, it is easy to see that errD(h1) \u2264 12 \u221a \u01eb.\nWe now analyze the filtering step of Phase 2. As in Section 4, our goal is to argue that errD2(h2) \u2264 12 \u221a \u01eb. Consider distribution D\u2032 that has equal probability on the distributions induced by WI and WC and let d\u2032(x) denote the density of point x in this distribution. We will show that for any x /\u2208 B we have that d\u2032(x) = \u0398(d2(x)). Since D[B] \u2264 \u01eb4 , we have that D2[B] \u2264 14 \u221a \u01eb. Therefore, D\u2032 and D2 satisfy the conditions of the robust super-sampling lemma (Lemma 4.12) where the fraction of bad points is at most\u221a \u01eb 4 . Hence, errD2(h2) \u2264 12 \u221a \u01eb.\nWe now show that for any x \u2208 B, d\u2032(x) = \u0398(d2(x)). The proof is identical to the one in Lemma 4.8. For ease of representation, we assume that errD(h1) is exactly 1 2 \u221a \u01eb. Let d(x), d2(x), dC(x), and dI(x) be the density of instance x in distributions D, D2, DC , and DI , respectively. Note that, for any x such that h1(x) = f \u2217(x), we have d(x) = dC(x)(1 \u2212 12 \u221a \u01eb). Similarly, for any x such that h1(x) 6= f\u2217(x), we have d(x) = dI(x) 1 2 \u221a \u01eb. Let NC(x), NI(x), MC(x) and MI(x) be the number of occurrences of x in the sets SC , SI ,WC and WI , respectively. For any x, there are two cases: If h1(x) = f \u2217(x): Then, there exist absolute constants c1 and c2 according to Lemma 4.7, such that\nd\u2032(x) = 1\n2 E\n[\nMC(x)\n|WC |\n]\n\u2265 E[MC(x)] c1 \u00b7m\u221a\u01eb,\u03b4 \u2265 E[NC(x)] c1 \u00b7m\u221a\u01eb,\u03b4 = |SC | \u00b7 d(x) c1 \u00b7m\u221a\u01eb,\u03b4\n= |SC | \u00b7 dC(x) \u00b7 (1\u2212 12\n\u221a \u01eb)\nc1 \u00b7m\u221a\u01eb,\u03b4 \u2265 c2dC(x) =\nc2d2(x)\n2 ,\nwhere the second and sixth transitions are by the sizes ofWC and |SC | and the third transition is by the fact that if h(x) = f\u2217(x), MC(x) > NC(x).\nIf h1(x) 6= f\u2217(x): Then, there exist absolute constants c\u20321 and c\u20322 according to Lemma 4.7, such that\nd\u2032(x) = 1\n2 E\n[\nMI(x)\n|WI |\n]\n\u2265 E[MI(x)] c\u20321 \u00b7m\u221a\u01eb,\u03b4 \u2265 E[NI(x)] c\u20321 \u00b7m\u221a\u01eb,\u03b4 \u2265 0.5 d(x)|S2| c\u20321 \u00b7m\u221a\u01eb,\u03b4\n= 0.5 dI(x)\n1 2 \u221a \u01eb \u00b7 |S2|\nc\u20321 \u00b7m\u221a\u01eb,\u03b4 = c\u20322dI(x) = c\u20322d2(x) 2 ,\nwhere the second and sixth transitions are by the sizes of WI and |S2|, the third transition is by the fact that if h(x) 6= f\u2217(x), MI(x) > NI(x), and the fourth transition holds by part 2 of Lemma 4.6.\nFinally, we have that errD3(h3) \u2264 12 \u221a \u01eb, where D3 is distribution D conditioned on {x | h1(x) 6=\nh2(x)}. Using the boosting technique of Schapire [1990] describe in Theorem 4.1, we conclude thatMAJ (h1, h2, h3) has error \u2264 \u01eb onD.\nThe label complexity claim follows by the fact that we restart Algorithm 3 at mostO(1/\u03b1) times, take an additional O(1\u01eb log( 1 \u03b4\u2032 )) high quality labeled set, and each run of Algorithm 3 uses the same label complexity as in Theorem 4.3 before getting restarted."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "In recent years crowdsourcing has become the method of choice for gathering labeled training data for learning algorithms. Standard approaches to crowdsourcing view the process of acquiring labeled data separately from the process of learning a classifier from the gathered data. This can give rise to computational and statistical challenges. For example, in most cases there are no known computationally efficient learning algorithms that are robust to the high level of noise that exists in crowdsourced data, and efforts to eliminate noise through voting often require a large number of queries per example. In this paper, we show how by interleaving the process of labeling and learning, we can attain computational efficiency with much less overhead in the labeling cost. In particular, we consider the realizable setting where there exists a true target function in F and consider a pool of labelers. When a noticeable fraction of the labelers are perfect, and the rest behave arbitrarily, we show that any F that can be efficiently learned in the traditional realizable PAC model can be learned in a computationally efficient manner by querying the crowd, despite high amounts of noise in the responses. Moreover, we show that this can be done while each labeler only labels a constant number of examples and the number of labels requested per example, on average, is a constant. When no perfect labelers exist, a related task is to find a set of the labelers which are good but not perfect. We show that we can identify all good labelers, when at least the majority of labelers are good.", "creator": "LaTeX with hyperref package"}}}